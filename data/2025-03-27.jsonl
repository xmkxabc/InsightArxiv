{"id": "2503.19979", "pdf": "https://arxiv.org/pdf/2503.19979", "abs": "https://arxiv.org/abs/2503.19979", "authors": ["Enora Rice", "Ali Marashian", "Hannah Haynie", "Katharina von der Wense", "Alexis Palmer"], "title": "Untangling the Influence of Typology, Data and Model Architecture on Ranking Transfer Languages for Cross-Lingual POS Tagging", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Workshop Language Models for Underserved\n  Communities", "summary": "Cross-lingual transfer learning is an invaluable tool for overcoming data\nscarcity, yet selecting a suitable transfer language remains a challenge. The\nprecise roles of linguistic typology, training data, and model architecture in\ntransfer language choice are not fully understood. We take a holistic approach,\nexamining how both dataset-specific and fine-grained typological features\ninfluence transfer language selection for part-of-speech tagging, considering\ntwo different sources for morphosyntactic features. While previous work\nexamines these dynamics in the context of bilingual biLSTMS, we extend our\nanalysis to a more modern transfer learning pipeline: zero-shot prediction with\npretrained multilingual models. We train a series of transfer language ranking\nsystems and examine how different feature inputs influence ranker performance\nacross architectures. Word overlap, type-token ratio, and genealogical distance\nemerge as top features across all architectures. Our findings reveal that a\ncombination of typological and dataset-dependent features leads to the best\nrankings, and that good performance can be obtained with either feature group\non its own."}
{"id": "2503.20007", "pdf": "https://arxiv.org/pdf/2503.20007", "abs": "https://arxiv.org/abs/2503.20007", "authors": ["Maksim Borisov", "Zhanibek Kozhirbayev", "Valentin Malykh"], "title": "Low-resource Machine Translation for Code-switched Kazakh-Russian Language Pair", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Machine translation for low resource language pairs is a challenging task.\nThis task could become extremely difficult once a speaker uses code switching.\nWe propose a method to build a machine translation model for code-switched\nKazakh-Russian language pair with no labeled data. Our method is basing on\ngeneration of synthetic data. Additionally, we present the first codeswitching\nKazakh-Russian parallel corpus and the evaluation results, which include a\nmodel achieving 16.48 BLEU almost reaching an existing commercial system and\nbeating it by human evaluation."}
{"id": "2503.20062", "pdf": "https://arxiv.org/pdf/2503.20062", "abs": "https://arxiv.org/abs/2503.20062", "authors": ["Jinsook Lee", "AJ Alvero", "Thorsten Joachims", "René Kizilcec"], "title": "Poor Alignment and Steerability of Large Language Models: Evidence from College Admission Essays", "categories": ["cs.CL"], "comment": "48 pages, 10 figures, 6 tables", "summary": "People are increasingly using technologies equipped with large language\nmodels (LLM) to write texts for formal communication, which raises two\nimportant questions at the intersection of technology and society: Who do LLMs\nwrite like (model alignment); and can LLMs be prompted to change who they write\nlike (model steerability). We investigate these questions in the high-stakes\ncontext of undergraduate admissions at a selective university by comparing\nlexical and sentence variation between essays written by 30,000 applicants to\ntwo types of LLM-generated essays: one prompted with only the essay question\nused by the human applicants; and another with additional demographic\ninformation about each applicant. We consistently find that both types of\nLLM-generated essays are linguistically distinct from human-authored essays,\nregardless of the specific model and analytical approach. Further, prompting a\nspecific sociodemographic identity is remarkably ineffective in aligning the\nmodel with the linguistic patterns observed in human writing from this identity\ngroup. This holds along the key dimensions of sex, race, first-generation\nstatus, and geographic location. The demographically prompted and unprompted\nsynthetic texts were also more similar to each other than to the human text,\nmeaning that prompting did not alleviate homogenization. These issues of model\nalignment and steerability in current LLMs raise concerns about the use of LLMs\nin high-stakes contexts."}
{"id": "2503.20083", "pdf": "https://arxiv.org/pdf/2503.20083", "abs": "https://arxiv.org/abs/2503.20083", "authors": ["Benjamin Minixhofer", "Edoardo Maria Ponti", "Ivan Vulić"], "title": "Cross-Tokenizer Distillation via Approximate Likelihood Matching", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Distillation has shown remarkable success in transferring knowledge from a\nLarge Language Model (LLM) teacher to a student LLM. However, current\ndistillation methods predominantly require the same tokenizer between the\nteacher and the student, restricting their applicability to only a small subset\nof teacher-student pairs. In this work, we develop a cross-tokenizer\ndistillation method to solve this crucial deficiency. Our method is the first\nto enable cross-tokenizer distillation without a next-token prediction loss as\nthe main objective, instead purely maximizing the student predictions'\nsimilarity to the teacher's predictions (known as pure distillation), while\nalso being robust to large mismatches between the teacher and the student\ntokenizer function and vocabulary. Empirically, our method enables\nsubstantially improved performance as tested on two use cases. First, we show\nthat viewing tokenizer transfer as self-distillation enables unprecedently\neffective transfer across tokenizers. We transfer (subword-level) Llama and\nGemma models to byte-level tokenization more effectively than prior methods\ntransfer to a similar subword tokenizer under a comparable training budget.\nTransferring different base models to the same tokenizer also enables\nensembling them (e.g., via averaging their predicted probabilities) which\nboosts performance. Second, we use our cross-tokenizer distillation method to\ndistil a large maths-specialized LLM into a smaller model, achieving\ncompetitive maths problem-solving performance. Overall, our results make\nsubstantial strides toward better adaptability and enhanced interaction between\ndifferent LLMs."}
{"id": "2503.19917", "pdf": "https://arxiv.org/pdf/2503.19917", "abs": "https://arxiv.org/abs/2503.19917", "authors": ["Atsushi Simojo", "Harumi Haraguchi"], "title": "A Study on the Matching Rate of Dance Movements Using 2D Skeleton Detection and 3D Pose Estimation: Why Is SEVENTEEN's Performance So Bita-Zoroi (Perfectly Synchronized)?", "categories": ["cs.CV"], "comment": "14 pages, 11 figures and 20 tables", "summary": "SEVENTEEN is a K-pop group with a large number of members 13 in total and the\nsignificant physical disparity between the tallest and shortest members among\nK-pop groups. However, despite their large numbers and physical differences,\ntheir dance performances exhibit unparalleled unity in the K-pop industry.\nAccording to one theory, their dance synchronization rate is said to be 90% or\neven 97%. However, there is little concrete data to substantiate this\nsynchronization rate. In this study, we analyzed SEVENTEEN's dance performances\nusing videos available on YouTube. We applied 2D skeleton detection and 3D pose\nestimation to evaluate joint angles, body part movements, and jumping and\ncrouching motions to investigate the factors contributing to their performance\nunity. The analysis revealed exceptionally high consistency in the movement\ndirection of body parts, as well as in the ankle and head positions during\njumping movements and the head position during crouching movements. These\nfindings suggested that SEVENTEEN's high synchronization rate can be attributed\nto the consistency of movement direction and the synchronization of ankle and\nhead heights during jumping and crouching movements."}
{"id": "2503.20088", "pdf": "https://arxiv.org/pdf/2503.20088", "abs": "https://arxiv.org/abs/2503.20088", "authors": ["Sophie Hao"], "title": "Generative Linguistics, Large Language Models, and the Social Nature of Scientific Success", "categories": ["cs.CL"], "comment": "To appear in the Italian Journal of Linguistics. This is a response\n  to Chesi (2024): arXiv:2412.12797", "summary": "Chesi's (forthcoming) target paper depicts a generative linguistics in\ncrisis, foreboded by Piantadosi's (2023) declaration that \"modern language\nmodels refute Chomsky's approach to language.\" In order to survive, Chesi\nwarns, generativists must hold themselves to higher standards of formal and\nempirical rigor. This response argues that the crisis described by Chesi and\nPiantadosi actually has little to do with rigor, but is rather a reflection of\ngenerativists' limited social ambitions. Chesi ties the fate of generative\nlinguistics to its intellectual merits, but the current success of language\nmodel research is social in nature as much as it is intellectual. In order to\nthrive, then, generativists must do more than heed Chesi's call for rigor; they\nmust also expand their ambitions by giving outsiders a stake in their future\nsuccess."}
{"id": "2503.19929", "pdf": "https://arxiv.org/pdf/2503.19929", "abs": "https://arxiv.org/abs/2503.19929", "authors": ["Pinhao Song"], "title": "Robust Object Detection of Underwater Robot based on Domain Generalization", "categories": ["cs.CV", "cs.LG"], "comment": "Master's thesis, in Chinese language", "summary": "Object detection aims to obtain the location and the category of specific\nobjects in a given image, which includes two tasks: classification and\nlocation. In recent years, researchers tend to apply object detection to\nunderwater robots equipped with vision systems to complete tasks including\nseafood fishing, fish farming, biodiversity monitoring and so on. However, the\ndiversity and complexity of underwater environments bring new challenges to\nobject detection. First, aquatic organisms tend to live together, which leads\nto severe occlusion. Second, theaquatic organisms are good at hiding\nthemselves, which have a similar color to the background. Third, the various\nwater quality and changeable and extreme lighting conditions lead to the\ndistorted, low contrast, blue or green images obtained by the underwater\ncamera, resulting in domain shift. And the deep model is generally vulnerable\nto facing domain shift. Fourth, the movement of the underwater robot leads to\nthe blur of the captured image and makes the water muddy, which results in low\nvisibility of the water. This paper investigates the problems brought by the\nunderwater environment mentioned above, and aims to design a high-performance\nand robust underwater object detector."}
{"id": "2503.20103", "pdf": "https://arxiv.org/pdf/2503.20103", "abs": "https://arxiv.org/abs/2503.20103", "authors": ["Changye Li", "Weizhe Xu", "Serguei Pakhomov", "Ellen Bradley", "Dror Ben-Zeev", "Trevor Cohen"], "title": "Bigger But Not Better: Small Neural Language Models Outperform Large Language Models in Detection of Thought Disorder", "categories": ["cs.CL"], "comment": "Accepted to CL Psych 2025 workshop, co-located with NAACL 2025", "summary": "Disorganized thinking is a key diagnostic indicator of schizophrenia-spectrum\ndisorders. Recently, clinical estimates of the severity of disorganized\nthinking have been shown to correlate with measures of how difficult speech\ntranscripts would be for large language models (LLMs) to predict. However,\nLLMs' deployment challenges -- including privacy concerns, computational and\nfinancial costs, and lack of transparency of training data -- limit their\nclinical utility. We investigate whether smaller neural language models can\nserve as effective alternatives for detecting positive formal thought disorder,\nusing the same sliding window based perplexity measurements that proved\neffective with larger models. Surprisingly, our results show that smaller\nmodels are more sensitive to linguistic differences associated with formal\nthought disorder than their larger counterparts. Detection capability declines\nbeyond a certain model size and context length, challenging the common\nassumption of ``bigger is better'' for LLM-based applications. Our findings\ngeneralize across audio diaries and clinical interview speech samples from\nindividuals with psychotic symptoms, suggesting a promising direction for\ndeveloping efficient, cost-effective, and privacy-preserving screening tools\nthat can be deployed in both clinical and naturalistic settings."}
{"id": "2503.19936", "pdf": "https://arxiv.org/pdf/2503.19936", "abs": "https://arxiv.org/abs/2503.19936", "authors": ["Kelaiti Xiao", "Liang Yang", "Paerhati Tulajiang", "Hongfei Lin"], "title": "VisualQuest: A Diverse Image Dataset for Evaluating Visual Recognition in LLMs", "categories": ["cs.CV"], "comment": null, "summary": "This paper introduces VisualQuest, a novel image dataset designed to assess\nthe ability of large language models (LLMs) to interpret non-traditional,\nstylized imagery. Unlike conventional photographic benchmarks, VisualQuest\nchallenges models with images that incorporate abstract, symbolic, and\nmetaphorical elements, requiring the integration of domain-specific knowledge\nand advanced reasoning. The dataset was meticulously curated through multiple\nstages of filtering, annotation, and standardization to ensure high quality and\ndiversity. Our evaluations using several state-of-the-art multimodal LLMs\nreveal significant performance variations that underscore the importance of\nboth factual background knowledge and inferential capabilities in visual\nrecognition tasks. VisualQuest thus provides a robust and comprehensive\nbenchmark for advancing research in multimodal reasoning and model architecture\ndesign."}
{"id": "2503.20104", "pdf": "https://arxiv.org/pdf/2503.20104", "abs": "https://arxiv.org/abs/2503.20104", "authors": ["Changye Li", "Zhecheng Sheng", "Trevor Cohen", "Serguei Pakhomov"], "title": "\"Is There Anything Else?'': Examining Administrator Influence on Linguistic Features from the Cookie Theft Picture Description Cognitive Test", "categories": ["cs.CL"], "comment": "Accepted to CMCL 2025 workshop, co-located with NAACL 2025", "summary": "Alzheimer's Disease (AD) dementia is a progressive neurodegenerative disease\nthat negatively impacts patients' cognitive ability. Previous studies have\ndemonstrated that changes in naturalistic language samples can be useful for\nearly screening of AD dementia. However, the nature of language deficits often\nrequires test administrators to use various speech elicitation techniques\nduring spontaneous language assessments to obtain enough propositional\nutterances from dementia patients. This could lead to the ``observer's effect''\non the downstream analysis that has not been fully investigated. Our study\nseeks to quantify the influence of test administrators on linguistic features\nin dementia assessment with two English corpora the ``Cookie Theft'' picture\ndescription datasets collected at different locations and test administrators\nshow different levels of administrator involvement. Our results show that the\nlevel of test administrator involvement significantly impacts observed\nlinguistic features in patient speech. These results suggest that many of\nsignificant linguistic features in the downstream classification task may be\npartially attributable to differences in the test administration practices\nrather than solely to participants' cognitive status. The variations in test\nadministrator behavior can lead to systematic biases in linguistic data,\npotentially confounding research outcomes and clinical assessments. Our study\nsuggests that there is a need for a more standardized test administration\nprotocol in the development of responsible clinical speech analytics\nframeworks."}
{"id": "2503.19937", "pdf": "https://arxiv.org/pdf/2503.19937", "abs": "https://arxiv.org/abs/2503.19937", "authors": ["Zhiyao Ren", "Yibing Zhan", "Baosheng Yu", "Dacheng Tao"], "title": "Reverse Prompt: Cracking the Recipe Inside Text-to-Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Text-to-image generation has become increasingly popular, but achieving the\ndesired images often requires extensive prompt engineering. In this paper, we\nexplore how to decode textual prompts from reference images, a process we refer\nto as image reverse prompt engineering. This technique enables us to gain\ninsights from reference images, understand the creative processes of great\nartists, and generate impressive new images. To address this challenge, we\npropose a method known as automatic reverse prompt optimization (ARPO).\nSpecifically, our method refines an initial prompt into a high-quality prompt\nthrough an iteratively imitative gradient prompt optimization process: 1)\ngenerating a recreated image from the current prompt to instantiate its\nguidance capability; 2) producing textual gradients, which are candidate\nprompts intended to reduce the difference between the recreated image and the\nreference image; 3) updating the current prompt with textual gradients using a\ngreedy search method to maximize the CLIP similarity between prompt and\nreference image. We compare ARPO with several baseline methods, including\nhandcrafted techniques, gradient-based prompt tuning methods, image captioning,\nand data-driven selection method. Both quantitative and qualitative results\ndemonstrate that our ARPO converges quickly to generate high-quality reverse\nprompts. More importantly, we can easily create novel images with diverse\nstyles and content by directly editing these reverse prompts. Code will be made\npublicly available."}
{"id": "2503.20110", "pdf": "https://arxiv.org/pdf/2503.20110", "abs": "https://arxiv.org/abs/2503.20110", "authors": ["Pin-Jie Lin", "Rishab Balasubramanian", "Fengyuan Liu", "Nikhil Kandpal", "Tu Vu"], "title": "Efficient Model Development through Fine-tuning Transfer", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "21 pages, 4 figures, 13 tables", "summary": "Modern LLMs struggle with efficient updates, as each new pretrained model\nversion requires repeating expensive alignment processes. This challenge also\napplies to domain- or language-specific models, where fine-tuning on\nspecialized data must be redone for every new base model release. In this\npaper, we explore the transfer of fine-tuning updates between model versions.\nSpecifically, we derive the diff vector from one source model version, which\nrepresents the weight changes from fine-tuning, and apply it to the base model\nof a different target version. Through empirical evaluations on various\nopen-weight model versions, we show that transferring diff vectors can\nsignificantly improve the target base model, often achieving performance\ncomparable to its fine-tuned counterpart. For example, reusing the fine-tuning\nupdates from Llama 3.0 8B leads to an absolute accuracy improvement of 10.7% on\nGPQA over the base Llama 3.1 8B without additional training, surpassing Llama\n3.1 8B Instruct. In a multilingual model development setting, we show that this\napproach can significantly increase performance on target-language tasks\nwithout retraining, achieving an absolute improvement of 4.7% and 15.5% on\nGlobal MMLU for Malagasy and Turkish, respectively, compared to Llama 3.1 8B\nInstruct. Our controlled experiments reveal that fine-tuning transfer is most\neffective when the source and target models are linearly connected in the\nparameter space. Additionally, we demonstrate that fine-tuning transfer offers\na stronger and more computationally efficient starting point for further\nfine-tuning. Finally, we propose an iterative recycling-then-finetuning\napproach for continuous model development, which improves both efficiency and\neffectiveness. Our findings suggest that fine-tuning transfer is a viable\nstrategy to reduce training costs while maintaining model performance."}
{"id": "2503.19947", "pdf": "https://arxiv.org/pdf/2503.19947", "abs": "https://arxiv.org/abs/2503.19947", "authors": ["Paul Koch", "Jörg Krüger", "Ankit Chowdhury", "Oliver Heimann"], "title": "Vanishing Depth: A Depth Adapter with Positional Depth Encoding for Generalized Image Encoders", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint", "summary": "Generalized metric depth understanding is critical for precise vision-guided\nrobotics, which current state-of-the-art (SOTA) vision-encoders do not support.\nTo address this, we propose Vanishing Depth, a self-supervised training\napproach that extends pretrained RGB encoders to incorporate and align metric\ndepth into their feature embeddings. Based on our novel positional depth\nencoding, we enable stable depth density and depth distribution invariant\nfeature extraction. We achieve performance improvements and SOTA results across\na spectrum of relevant RGBD downstream tasks - without the necessity of\nfinetuning the encoder. Most notably, we achieve 56.05 mIoU on SUN-RGBD\nsegmentation, 88.3 RMSE on Void's depth completion, and 83.8 Top 1 accuracy on\nNYUv2 scene classification. In 6D-object pose estimation, we outperform our\npredecessors of DinoV2, EVA-02, and Omnivore and achieve SOTA results for\nnon-finetuned encoders in several related RGBD downstream tasks."}
{"id": "2503.20179", "pdf": "https://arxiv.org/pdf/2503.20179", "abs": "https://arxiv.org/abs/2503.20179", "authors": ["Shijia Zhang", "Xiyu Ding", "Kai Ding", "Jacob Zhang", "Kevin Galinsky", "Mengrui Wang", "Ryan P. Mayers", "Zheyu Wang", "Hadi Kharrazi"], "title": "ProtoBERT-LoRA: Parameter-Efficient Prototypical Finetuning for Immunotherapy Study Identification", "categories": ["cs.CL", "cs.IR", "q-bio.QM"], "comment": "Submitted to AMIA 2025 Annual Symposium", "summary": "Identifying immune checkpoint inhibitor (ICI) studies in genomic repositories\nlike Gene Expression Omnibus (GEO) is vital for cancer research yet remains\nchallenging due to semantic ambiguity, extreme class imbalance, and limited\nlabeled data in low-resource settings. We present ProtoBERT-LoRA, a hybrid\nframework that combines PubMedBERT with prototypical networks and Low-Rank\nAdaptation (LoRA) for efficient fine-tuning. The model enforces class-separable\nembeddings via episodic prototype training while preserving biomedical domain\nknowledge. Our dataset was divided as: Training (20 positive, 20 negative),\nPrototype Set (10 positive, 10 negative), Validation (20 positive, 200\nnegative), and Test (71 positive, 765 negative). Evaluated on test dataset,\nProtoBERT-LoRA achieved F1-score of 0.624 (precision: 0.481, recall: 0.887),\noutperforming the rule-based system, machine learning baselines and finetuned\nPubMedBERT. Application to 44,287 unlabeled studies reduced manual review\nefforts by 82%. Ablation studies confirmed that combining prototypes with LoRA\nimproved performance by 29% over stand-alone LoRA."}
{"id": "2503.19948", "pdf": "https://arxiv.org/pdf/2503.19948", "abs": "https://arxiv.org/abs/2503.19948", "authors": ["Alexander Gambashidze", "Konstantin Sobolev", "Andrey Kuznetsov", "Ivan Oseledets"], "title": "Test-Time Reasoning Through Visual Human Preferences with VLMs and Soft Rewards", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Can Visual Language Models (VLMs) effectively capture human visual\npreferences? This work addresses this question by training VLMs to think about\npreferences at test time, employing reinforcement learning methods inspired by\nDeepSeek R1 and OpenAI O1. Using datasets such as ImageReward and Human\nPreference Score v2 (HPSv2), our models achieve accuracies of 64.9% on the\nImageReward test set (trained on ImageReward official split) and 65.4% on HPSv2\n(trained on approximately 25% of its data). These results match traditional\nencoder-based models while providing transparent reasoning and enhanced\ngeneralization. This approach allows to use not only rich VLM world knowledge,\nbut also its potential to think, yielding interpretable outcomes that help\ndecision-making processes. By demonstrating that human visual preferences\nreasonable by current VLMs, we introduce efficient soft-reward strategies for\nimage ranking, outperforming simplistic selection or scoring methods. This\nreasoning capability enables VLMs to rank arbitrary images-regardless of aspect\nratio or complexity-thereby potentially amplifying the effectiveness of visual\nPreference Optimization. By reducing the need for extensive markup while\nimproving reward generalization and explainability, our findings can be a\nstrong mile-stone that will enhance text-to-vision models even further."}
{"id": "2503.20182", "pdf": "https://arxiv.org/pdf/2503.20182", "abs": "https://arxiv.org/abs/2503.20182", "authors": ["Huanhuan Ma", "Haisong Gong", "Xiaoyuan Yi", "Xing Xie", "Dongkuan Xu"], "title": "Leveraging Implicit Sentiments: Enhancing Reliability and Validity in Psychological Trait Evaluation of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Code available via https://github.com/dependentsign/CSI", "summary": "Recent advancements in Large Language Models (LLMs) have led to their\nincreasing integration into human life. With the transition from mere tools to\nhuman-like assistants, understanding their psychological aspects-such as\nemotional tendencies and personalities-becomes essential for ensuring their\ntrustworthiness. However, current psychological evaluations of LLMs, often\nbased on human psychological assessments like the BFI, face significant\nlimitations. The results from these approaches often lack reliability and have\nlimited validity when predicting LLM behavior in real-world scenarios. In this\nwork, we introduce a novel evaluation instrument specifically designed for\nLLMs, called Core Sentiment Inventory (CSI). CSI is a bilingual tool, covering\nboth English and Chinese, that implicitly evaluates models' sentiment\ntendencies, providing an insightful psychological portrait of LLM across three\ndimensions: optimism, pessimism, and neutrality. Through extensive experiments,\nwe demonstrate that: 1) CSI effectively captures nuanced emotional patterns,\nrevealing significant variation in LLMs across languages and contexts; 2)\nCompared to current approaches, CSI significantly improves reliability,\nyielding more consistent results; and 3) The correlation between CSI scores and\nthe sentiment of LLM's real-world outputs exceeds 0.85, demonstrating its\nstrong validity in predicting LLM behavior. We make CSI public available via:\nhttps://github.com/dependentsign/CSI."}
{"id": "2503.19951", "pdf": "https://arxiv.org/pdf/2503.19951", "abs": "https://arxiv.org/abs/2503.19951", "authors": ["Yudong Yang", "Jimin Zhuang", "Guangzhi Sun", "Changli Tang", "Yixuan Li", "Peihan Li", "Yifan Jiang", "Wei Li", "Zejun Ma", "Chao Zhang"], "title": "ACVUBench: Audio-Centric Video Understanding Benchmark", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Audio often serves as an auxiliary modality in video understanding tasks of\naudio-visual large language models (LLMs), merely assisting in the\ncomprehension of visual information. However, a thorough understanding of\nvideos significantly depends on auditory information, as audio offers critical\ncontext, emotional cues, and semantic meaning that visual data alone often\nlacks. This paper proposes an audio-centric video understanding benchmark\n(ACVUBench) to evaluate the video comprehension capabilities of multimodal LLMs\nwith a particular focus on auditory information. Specifically, ACVUBench\nincorporates 2,662 videos spanning 18 different domains with rich auditory\ninformation, together with over 13k high-quality human annotated or validated\nquestion-answer pairs. Moreover, ACVUBench introduces a suite of carefully\ndesigned audio-centric tasks, holistically testing the understanding of both\naudio content and audio-visual interactions in videos. A thorough evaluation\nacross a diverse range of open-source and proprietary multimodal LLMs is\nperformed, followed by the analyses of deficiencies in audio-visual LLMs. Demos\nare available at https://github.com/lark-png/ACVUBench."}
{"id": "2503.20194", "pdf": "https://arxiv.org/pdf/2503.20194", "abs": "https://arxiv.org/abs/2503.20194", "authors": ["Zhouhong Gu", "Xingzhou Chen", "Xiaoran Shi", "Tao Wang", "Suhang Zheng", "Tianyu Li", "Hongwei Feng", "Yanghua Xiao"], "title": "GAPO: Learning Preferential Prompt through Generative Adversarial Policy Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models have highlighted the critical need\nfor precise control over model outputs through predefined constraints. While\nexisting methods attempt to achieve this through either direct\ninstruction-response synthesis or preferential response optimization, they\noften struggle with constraint understanding and adaptation. This limitation\nbecomes particularly evident when handling fine-grained constraints, leading to\neither hallucination or brittle performance. We introduce Generative\nAdversarial Policy Optimization (GAPO), a novel framework that combines\nGAN-based training dynamics with an encoder-only reward model to progressively\nlearn and adapt to increasingly complex constraints. GAPO leverages adversarial\ntraining to automatically generate training samples of varying difficulty while\nutilizing the encoder-only architecture to better capture prompt-response\nrelationships. Extensive experiments demonstrate GAPO's superior performance\nacross multiple benchmarks, particularly in scenarios requiring fine-grained\nconstraint handling, where it significantly outperforms existing methods like\nPPO, DPO, and KTO. Our results suggest that GAPO's unique approach to\npreferential prompt learning offers a more robust and effective solution for\ncontrolling LLM outputs. Code is avaliable in\nhttps://github.com/MikeGu721/GAPO."}
{"id": "2503.19953", "pdf": "https://arxiv.org/pdf/2503.19953", "abs": "https://arxiv.org/abs/2503.19953", "authors": ["Stefan Stojanov", "David Wendt", "Seungwoo Kim", "Rahul Venkatesh", "Kevin Feigelis", "Jiajun Wu", "Daniel LK Yamins"], "title": "Self-Supervised Learning of Motion Concepts by Optimizing Counterfactuals", "categories": ["cs.CV"], "comment": "Project webpage: https://neuroailab.github.io/opt_cwm_page/", "summary": "Estimating motion in videos is an essential computer vision problem with many\ndownstream applications, including controllable video generation and robotics.\nCurrent solutions are primarily trained using synthetic data or require tuning\nof situation-specific heuristics, which inherently limits these models'\ncapabilities in real-world contexts. Despite recent developments in large-scale\nself-supervised learning from videos, leveraging such representations for\nmotion estimation remains relatively underexplored. In this work, we develop\nOpt-CWM, a self-supervised technique for flow and occlusion estimation from a\npre-trained next-frame prediction model. Opt-CWM works by learning to optimize\ncounterfactual probes that extract motion information from a base video model,\navoiding the need for fixed heuristics while training on unrestricted video\ninputs. We achieve state-of-the-art performance for motion estimation on\nreal-world videos while requiring no labeled data."}
{"id": "2503.20202", "pdf": "https://arxiv.org/pdf/2503.20202", "abs": "https://arxiv.org/abs/2503.20202", "authors": ["Nan Gao", "Yihua Bao", "Dongdong Weng", "Jiayi Zhao", "Jia Li", "Yan Zhou", "Pengfei Wan", "Di Zhang"], "title": "SARGes: Semantically Aligned Reliable Gesture Generation via Intent Chain", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.RO"], "comment": null, "summary": "Co-speech gesture generation enhances human-computer interaction realism\nthrough speech-synchronized gesture synthesis. However, generating semantically\nmeaningful gestures remains a challenging problem. We propose SARGes, a novel\nframework that leverages large language models (LLMs) to parse speech content\nand generate reliable semantic gesture labels, which subsequently guide the\nsynthesis of meaningful co-speech gestures.First, we constructed a\ncomprehensive co-speech gesture ethogram and developed an LLM-based intent\nchain reasoning mechanism that systematically parses and decomposes gesture\nsemantics into structured inference steps following ethogram criteria,\neffectively guiding LLMs to generate context-aware gesture labels.\nSubsequently, we constructed an intent chain-annotated text-to-gesture label\ndataset and trained a lightweight gesture label generation model, which then\nguides the generation of credible and semantically coherent co-speech gestures.\nExperimental results demonstrate that SARGes achieves highly\nsemantically-aligned gesture labeling (50.2% accuracy) with efficient\nsingle-pass inference (0.4 seconds). The proposed method provides an\ninterpretable intent reasoning pathway for semantic gesture synthesis."}
{"id": "2503.19982", "pdf": "https://arxiv.org/pdf/2503.19982", "abs": "https://arxiv.org/abs/2503.19982", "authors": ["Pei-Kai Huang", "Jun-Xiong Chong", "Cheng-Hsuan Chiang", "Tzu-Hsien Chen", "Tyng-Luh Liu", "Chiou-Ting Hsu"], "title": "SLIP: Spoof-Aware One-Class Face Anti-Spoofing with Language Image Pretraining", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2025", "summary": "Face anti-spoofing (FAS) plays a pivotal role in ensuring the security and\nreliability of face recognition systems. With advancements in vision-language\npretrained (VLP) models, recent two-class FAS techniques have leveraged the\nadvantages of using VLP guidance, while this potential remains unexplored in\none-class FAS methods. The one-class FAS focuses on learning intrinsic liveness\nfeatures solely from live training images to differentiate between live and\nspoof faces. However, the lack of spoof training data can lead one-class FAS\nmodels to inadvertently incorporate domain information irrelevant to the\nlive/spoof distinction (e.g., facial content), causing performance degradation\nwhen tested with a new application domain. To address this issue, we propose a\nnovel framework called Spoof-aware one-class face anti-spoofing with Language\nImage Pretraining (SLIP). Given that live faces should ideally not be obscured\nby any spoof-attack-related objects (e.g., paper, or masks) and are assumed to\nyield zero spoof cue maps, we first propose an effective language-guided spoof\ncue map estimation to enhance one-class FAS models by simulating whether the\nunderlying faces are covered by attack-related objects and generating\ncorresponding nonzero spoof cue maps. Next, we introduce a novel prompt-driven\nliveness feature disentanglement to alleviate live/spoof-irrelative domain\nvariations by disentangling live/spoof-relevant and domain-dependent\ninformation. Finally, we design an effective augmentation strategy by fusing\nlatent features from live images and spoof prompts to generate spoof-like image\nfeatures and thus diversify latent spoof features to facilitate the learning of\none-class FAS. Our extensive experiments and ablation studies support that SLIP\nconsistently outperforms previous one-class FAS methods."}
{"id": "2503.20212", "pdf": "https://arxiv.org/pdf/2503.20212", "abs": "https://arxiv.org/abs/2503.20212", "authors": ["Yangyang Meng", "Jinpeng Li", "Guodong Lin", "Yu Pu", "Guanbo Wang", "Hu Du", "Zhiming Shao", "Yukai Huang", "Ke Li", "Wei-Qiang Zhang"], "title": "Dolphin: A Large-Scale Automatic Speech Recognition Model for Eastern Languages", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "This report introduces Dolphin, a large-scale multilingual automatic speech\nrecognition (ASR) model that extends the Whisper architecture to support a\nwider range of languages. Our approach integrates in-house proprietary and\nopen-source datasets to refine and optimize Dolphin's performance. The model is\nspecifically designed to achieve notable recognition accuracy for 40 Eastern\nlanguages across East Asia, South Asia, Southeast Asia, and the Middle East,\nwhile also supporting 22 Chinese dialects. Experimental evaluations show that\nDolphin significantly outperforms current state-of-the-art open-source models\nacross various languages. To promote reproducibility and community-driven\ninnovation, we are making our trained models and inference source code publicly\navailable."}
{"id": "2503.20000", "pdf": "https://arxiv.org/pdf/2503.20000", "abs": "https://arxiv.org/abs/2503.20000", "authors": ["Jonathan Sauder", "Viktor Domazetoski", "Guilhem Banc-Prandi", "Gabriela Perna", "Anders Meibom", "Devis Tuia"], "title": "The Coralscapes Dataset: Semantic Scene Understanding in Coral Reefs", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Coral reefs are declining worldwide due to climate change and local\nstressors. To inform effective conservation or restoration, monitoring at the\nhighest possible spatial and temporal resolution is necessary. Conventional\ncoral reef surveying methods are limited in scalability due to their reliance\non expert labor time, motivating the use of computer vision tools to automate\nthe identification and abundance estimation of live corals from images.\nHowever, the design and evaluation of such tools has been impeded by the lack\nof large high quality datasets. We release the Coralscapes dataset, the first\ngeneral-purpose dense semantic segmentation dataset for coral reefs, covering\n2075 images, 39 benthic classes, and 174k segmentation masks annotated by\nexperts. Coralscapes has a similar scope and the same structure as the widely\nused Cityscapes dataset for urban scene segmentation, allowing benchmarking of\nsemantic segmentation models in a new challenging domain which requires expert\nknowledge to annotate. We benchmark a wide range of semantic segmentation\nmodels, and find that transfer learning from Coralscapes to existing smaller\ndatasets consistently leads to state-of-the-art performance. Coralscapes will\ncatalyze research on efficient, scalable, and standardized coral reef surveying\nmethods based on computer vision, and holds the potential to streamline the\ndevelopment of underwater ecological robotics."}
{"id": "2503.20215", "pdf": "https://arxiv.org/pdf/2503.20215", "abs": "https://arxiv.org/abs/2503.20215", "authors": ["Jin Xu", "Zhifang Guo", "Jinzheng He", "Hangrui Hu", "Ting He", "Shuai Bai", "Keqin Chen", "Jialin Wang", "Yang Fan", "Kai Dang", "Bin Zhang", "Xiong Wang", "Yunfei Chu", "Junyang Lin"], "title": "Qwen2.5-Omni Technical Report", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model\ndesigned to perceive diverse modalities, including text, images, audio, and\nvideo, while simultaneously generating text and natural speech responses in a\nstreaming manner. To enable the streaming of multimodal information inputs,\nboth audio and visual encoders utilize a block-wise processing approach. To\nsynchronize the timestamps of video inputs with audio, we organize the audio\nand video sequentially in an interleaved manner and propose a novel position\nembedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently\ngenerate text and speech while avoiding interference between the two\nmodalities, we propose \\textbf{Thinker-Talker} architecture. In this framework,\nThinker functions as a large language model tasked with text generation, while\nTalker is a dual-track autoregressive model that directly utilizes the hidden\nrepresentations from the Thinker to produce audio tokens as output. Both the\nThinker and Talker models are designed to be trained and inferred in an\nend-to-end manner. For decoding audio tokens in a streaming manner, we\nintroduce a sliding-window DiT that restricts the receptive field, aiming to\nreduce the initial package delay. Qwen2.5-Omni is comparable with the similarly\nsized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni\nachieves state-of-the-art performance on multimodal benchmarks like Omni-Bench.\nNotably, Qwen2.5-Omni's performance in end-to-end speech instruction following\nis comparable to its capabilities with text inputs, as evidenced by benchmarks\nsuch as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming\nTalker outperforms most existing streaming and non-streaming alternatives in\nrobustness and naturalness."}
{"id": "2503.20011", "pdf": "https://arxiv.org/pdf/2503.20011", "abs": "https://arxiv.org/abs/2503.20011", "authors": ["Luke Chen", "Junyao Wang", "Trier Mortlock", "Pramod Khargonekar", "Mohammad Abdullah Al Faruque"], "title": "Hyperdimensional Uncertainty Quantification for Multimodal Uncertainty Fusion in Autonomous Vehicles Perception", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted at CVPR 2025", "summary": "Uncertainty Quantification (UQ) is crucial for ensuring the reliability of\nmachine learning models deployed in real-world autonomous systems. However,\nexisting approaches typically quantify task-level output prediction uncertainty\nwithout considering epistemic uncertainty at the multimodal feature fusion\nlevel, leading to sub-optimal outcomes. Additionally, popular uncertainty\nquantification methods, e.g., Bayesian approximations, remain challenging to\ndeploy in practice due to high computational costs in training and inference.\nIn this paper, we propose HyperDUM, a novel deterministic uncertainty method\n(DUM) that efficiently quantifies feature-level epistemic uncertainty by\nleveraging hyperdimensional computing. Our method captures the channel and\nspatial uncertainties through channel and patch -wise projection and bundling\ntechniques respectively. Multimodal sensor features are then adaptively\nweighted to mitigate uncertainty propagation and improve feature fusion. Our\nevaluations show that HyperDUM on average outperforms the state-of-the-art\n(SOTA) algorithms by up to 2.01%/1.27% in 3D Object Detection and up to 1.29%\nimprovement over baselines in semantic segmentation tasks under various types\nof uncertainties. Notably, HyperDUM requires 2.36x less Floating Point\nOperations and up to 38.30x less parameters than SOTA methods, providing an\nefficient solution for real-world autonomous systems."}
{"id": "2503.20227", "pdf": "https://arxiv.org/pdf/2503.20227", "abs": "https://arxiv.org/abs/2503.20227", "authors": ["Tianhao Wu", "Yu Wang", "Ngoc Quach"], "title": "Advancements in Natural Language Processing: Exploring Transformer-Based Architectures for Text Understanding", "categories": ["cs.CL", "cs.AI"], "comment": "This paper has been accepted by the 5th International Conference on\n  Artificial Intelligence and Industrial Technology Applications (AIITA 2025)", "summary": "Natural Language Processing (NLP) has witnessed a transformative leap with\nthe advent of transformer-based architectures, which have significantly\nenhanced the ability of machines to understand and generate human-like text.\nThis paper explores the advancements in transformer models, such as BERT and\nGPT, focusing on their superior performance in text understanding tasks\ncompared to traditional methods like recurrent neural networks (RNNs). By\nanalyzing statistical properties through visual representations-including\nprobability density functions of text length distributions and feature space\nclassifications-the study highlights the models' proficiency in handling\nlong-range dependencies, adapting to conditional shifts, and extracting\nfeatures for classification, even with overlapping classes. Drawing on recent\n2024 research, including enhancements in multi-hop knowledge graph reasoning\nand context-aware chat interactions, the paper outlines a methodology involving\ndata preparation, model selection, pretraining, fine-tuning, and evaluation.\nThe results demonstrate state-of-the-art performance on benchmarks like GLUE\nand SQuAD, with F1 scores exceeding 90%, though challenges such as high\ncomputational costs persist. This work underscores the pivotal role of\ntransformers in modern NLP and suggests future directions, including efficiency\noptimization and multimodal integration, to further advance language-based AI\nsystems."}
{"id": "2503.20068", "pdf": "https://arxiv.org/pdf/2503.20068", "abs": "https://arxiv.org/abs/2503.20068", "authors": ["Naitik Jain", "Amogh Joshi", "Mason Earles"], "title": "iNatAg: Multi-Class Classification Models Enabled by a Large-Scale Benchmark Dataset with 4.7M Images of 2,959 Crop and Weed Species", "categories": ["cs.CV"], "comment": null, "summary": "Accurate identification of crop and weed species is critical for precision\nagriculture and sustainable farming. However, it remains a challenging task due\nto a variety of factors -- a high degree of visual similarity among species,\nenvironmental variability, and a continued lack of large, agriculture-specific\nimage data. We introduce iNatAg, a large-scale image dataset which contains\nover 4.7 million images of 2,959 distinct crop and weed species, with precise\nannotations along the taxonomic hierarchy from binary crop/weed labels to\nspecific species labels. Curated from the broader iNaturalist database, iNatAg\ncontains data from every continent and accurately reflects the variability of\nnatural image captures and environments. Enabled by this data, we train\nbenchmark models built upon the Swin Transformer architecture and evaluate the\nimpact of various modifications such as the incorporation of geospatial data\nand LoRA finetuning. Our best models achieve state-of-the-art performance\nacross all taxonomic classification tasks, achieving 92.38\\% on crop and weed\nclassification. Furthermore, the scale of our dataset enables us to explore\nincorrect misclassifications and unlock new analytic possiblities for plant\nspecies. By combining large-scale species coverage, multi-task labels, and\ngeographic diversity, iNatAg provides a new foundation for building robust,\ngeolocation-aware agricultural classification systems. We release the iNatAg\ndataset publicly through AgML (https://github.com/Project-AgML/AgML), enabling\ndirect access and integration into agricultural machine learning workflows."}
{"id": "2503.20279", "pdf": "https://arxiv.org/pdf/2503.20279", "abs": "https://arxiv.org/abs/2503.20279", "authors": ["Sejin Lee", "Jian Kim", "Haon Park", "Ashkan Yousefpour", "Sangyoon Yu", "Min Song"], "title": "sudo rm -rf agentic_security", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed as computer-use\nagents, autonomously performing tasks within real desktop or web environments.\nWhile this evolution greatly expands practical use cases for humans, it also\ncreates serious security exposures. We present SUDO (Screen-based Universal\nDetox2Tox Offense), a novel attack framework that systematically bypasses\nrefusal trained safeguards in commercial computer-use agents, such as Claude\nComputer Use. The core mechanism, Detox2Tox, transforms harmful requests (that\nagents initially reject) into seemingly benign requests via detoxification,\nsecures detailed instructions from advanced vision language models (VLMs), and\nthen reintroduces malicious content via toxification just before execution.\nUnlike conventional jailbreaks, SUDO iteratively refines its attacks based on a\nbuilt-in refusal feedback, making it increasingly effective against robust\npolicy filters. In extensive tests spanning 50 real-world tasks and multiple\nstate-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with\nno refinement), and up to 41% (by its iterative refinement) in Claude Computer\nUse. By revealing these vulnerabilities and demonstrating the ease with which\nthey can be exploited in real-world computing environments, this paper\nhighlights an immediate need for robust, context-aware safeguards. WARNING:\nThis paper includes harmful or offensive model outputs."}
{"id": "2503.20084", "pdf": "https://arxiv.org/pdf/2503.20084", "abs": "https://arxiv.org/abs/2503.20084", "authors": ["Simiao Ren", "Yao Yao", "Kidus Zewde", "Zisheng Liang", "Tsang", "Ng", "Ning-Yau Cheng", "Xiaoou Zhan", "Qinzhe Liu", "Yifei Chen", "Hengwei Xu"], "title": "Can Multi-modal (reasoning) LLMs work as deepfake detectors?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Deepfake detection remains a critical challenge in the era of advanced\ngenerative models, particularly as synthetic media becomes more sophisticated.\nIn this study, we explore the potential of state of the art multi-modal\n(reasoning) large language models (LLMs) for deepfake image detection such as\n(OpenAI O1/4o, Gemini thinking Flash 2, Deepseek Janus, Grok 3, llama 3.2, Qwen\n2/2.5 VL, Mistral Pixtral, Claude 3.5/3.7 sonnet) . We benchmark 12 latest\nmulti-modal LLMs against traditional deepfake detection methods across multiple\ndatasets, including recently published real-world deepfake imagery. To enhance\nperformance, we employ prompt tuning and conduct an in-depth analysis of the\nmodels' reasoning pathways to identify key contributing factors in their\ndecision-making process. Our findings indicate that best multi-modal LLMs\nachieve competitive performance with promising generalization ability with zero\nshot, even surpass traditional deepfake detection pipelines in\nout-of-distribution datasets while the rest of the LLM families performs\nextremely disappointing with some worse than random guess. Furthermore, we\nfound newer model version and reasoning capabilities does not contribute to\nperformance in such niche tasks of deepfake detection while model size do help\nin some cases. This study highlights the potential of integrating multi-modal\nreasoning in future deepfake detection frameworks and provides insights into\nmodel interpretability for robustness in real-world scenarios."}
{"id": "2503.20302", "pdf": "https://arxiv.org/pdf/2503.20302", "abs": "https://arxiv.org/abs/2503.20302", "authors": ["Sunayana Sitaram", "Adrian de Wynter", "Isobel McCrum", "Qilong Gu", "Si-Qing Chen"], "title": "A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Misgendering is the act of referring to someone by a gender that does not\nmatch their chosen identity. It marginalizes and undermines a person's sense of\nself, causing significant harm. English-based approaches have clear-cut\napproaches to avoiding misgendering, such as the use of the pronoun ``they''.\nHowever, other languages pose unique challenges due to both grammatical and\ncultural constructs. In this work we develop methodologies to assess and\nmitigate misgendering across 42 languages and dialects using a\nparticipatory-design approach to design effective and appropriate guardrails\nacross all languages. We test these guardrails in a standard large language\nmodel-based application (meeting transcript summarization), where both the data\ngeneration and the annotation steps followed a human-in-the-loop approach. We\nfind that the proposed guardrails are very effective in reducing misgendering\nrates across all languages in the summaries generated, and without incurring\nloss of quality. Our human-in-the-loop approach demonstrates a method to\nfeasibly scale inclusive and responsible AI-based solutions across multiple\nlanguages and cultures."}
{"id": "2503.20101", "pdf": "https://arxiv.org/pdf/2503.20101", "abs": "https://arxiv.org/abs/2503.20101", "authors": ["Albert W Reed", "Connor Hashemi", "Dennis Melamed", "Nitesh Menon", "Keigo Hirakawa", "Scott McCloskey"], "title": "EBS-EKF: Accurate and High Frequency Event-based Star Tracking", "categories": ["cs.CV"], "comment": "Accepted into the proceedings of the Conference on Computer Vision\n  and Pattern Recognition (CVPR) for 2025. Link to code and dataset is\n  https://gitlab.kitware.com/nest-public/kw_ebs_star_tracking#", "summary": "Event-based sensors (EBS) are a promising new technology for star tracking\ndue to their low latency and power efficiency, but prior work has thus far been\nevaluated exclusively in simulation with simplified signal models. We propose a\nnovel algorithm for event-based star tracking, grounded in an analysis of the\nEBS circuit and an extended Kalman filter (EKF). We quantitatively evaluate our\nmethod using real night sky data, comparing its results with those from a\nspace-ready active-pixel sensor (APS) star tracker. We demonstrate that our\nmethod is an order-of-magnitude more accurate than existing methods due to\nimproved signal modeling and state estimation, while providing more frequent\nupdates and greater motion tolerance than conventional APS trackers. We provide\nall code and the first dataset of events synchronized with APS solutions."}
{"id": "2503.20320", "pdf": "https://arxiv.org/pdf/2503.20320", "abs": "https://arxiv.org/abs/2503.20320", "authors": ["Shih-Wen Ke", "Guan-Yu Lai", "Guo-Lin Fang", "Hsi-Yuan Kao"], "title": "Iterative Prompting with Persuasion Skills in Jailbreaking Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.ET"], "comment": null, "summary": "Large language models (LLMs) are designed to align with human values in their\nresponses. This study exploits LLMs with an iterative prompting technique where\neach prompt is systematically modified and refined across multiple iterations\nto enhance its effectiveness in jailbreaking attacks progressively. This\ntechnique involves analyzing the response patterns of LLMs, including GPT-3.5,\nGPT-4, LLaMa2, Vicuna, and ChatGLM, allowing us to adjust and optimize prompts\nto evade the LLMs' ethical and security constraints. Persuasion strategies\nenhance prompt effectiveness while maintaining consistency with malicious\nintent. Our results show that the attack success rates (ASR) increase as the\nattacking prompts become more refined with the highest ASR of 90% for GPT4 and\nChatGLM and the lowest ASR of 68% for LLaMa2. Our technique outperforms\nbaseline techniques (PAIR and PAP) in ASR and shows comparable performance with\nGCG and ArtPrompt."}
{"id": "2503.20108", "pdf": "https://arxiv.org/pdf/2503.20108", "abs": "https://arxiv.org/abs/2503.20108", "authors": ["Xavier Merino", "Gabriella Pangelinan", "Samuel Langborgh", "Michael C. King", "Kevin W. Bowyer"], "title": "Peepers & Pixels: Human Recognition Accuracy on Low Resolution Faces", "categories": ["cs.CV"], "comment": "10 pages, 3 figures", "summary": "Automated one-to-many (1:N) face recognition is a powerful investigative tool\ncommonly used by law enforcement agencies. In this context, potential matches\nresulting from automated 1:N recognition are reviewed by human examiners prior\nto possible use as investigative leads. While automated 1:N recognition can\nachieve near-perfect accuracy under ideal imaging conditions, operational\nscenarios may necessitate the use of surveillance imagery, which is often\ndegraded in various quality dimensions. One important quality dimension is\nimage resolution, typically quantified by the number of pixels on the face. The\ncommon metric for this is inter-pupillary distance (IPD), which measures the\nnumber of pixels between the pupils. Low IPD is known to degrade the accuracy\nof automated face recognition. However, the threshold IPD for reliability in\nhuman face recognition remains undefined. This study aims to explore the\nboundaries of human recognition accuracy by systematically testing accuracy\nacross a range of IPD values. We find that at low IPDs (10px, 5px), human\naccuracy is at or below chance levels (50.7%, 35.9%), even as confidence in\ndecision-making remains relatively high (77%, 70.7%). Our findings indicate\nthat, for low IPD images, human recognition ability could be a limiting factor\nto overall system accuracy."}
{"id": "2503.20417", "pdf": "https://arxiv.org/pdf/2503.20417", "abs": "https://arxiv.org/abs/2503.20417", "authors": ["Zhenghan Yu", "Xinyu Hu", "Xiaojun Wan"], "title": "CFunModel: A \"Funny\" Language Model Capable of Chinese Humor Generation and Processing", "categories": ["cs.CL"], "comment": "9 pages", "summary": "Humor plays a significant role in daily language communication. With the\nrapid development of large language models (LLMs), natural language processing\nhas made significant strides in understanding and generating various genres of\ntexts. However, most LLMs exhibit poor performance in generating and processing\nChinese humor. In this study, we introduce a comprehensive Chinese\nhumor-related dataset, the Chinese Fun Set (CFunSet). This dataset aggregates\nexisting Chinese humor datasets and includes over 20,000 jokes collected from\nTieba-JokeBar, a Chinese online platform known for joke sharing. The resulting\ncorpus comprises more than 160,000 entries. Leveraging CFunSet, we developed\nthe Chinese Fun Model (CFunModel), the first large language model designed to\nhandle various Chinese humor-related tasks including Crosstalk Response\nSelection, Humor Recognition, Joke Generation, etc. Experimental results\ndemonstrate that CFunModel outperforms popular large language models in these\ntasks. Our CFunSet is available at\nhttps://huggingface.co/datasets/ZhenghanYU/CFunSet and CFunModel is available\nat https://huggingface.co/ZhenghanYU/CFunModel. A demostration video of our\nwork is available at https://youtu.be/MOsISOJ66Ms."}
{"id": "2503.20168", "pdf": "https://arxiv.org/pdf/2503.20168", "abs": "https://arxiv.org/abs/2503.20168", "authors": ["Sheng Miao", "Jiaxin Huang", "Dongfeng Bai", "Xu Yan", "Hongyu Zhou", "Yue Wang", "Bingbing Liu", "Andreas Geiger", "Yiyi Liao"], "title": "EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View Synthesis", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Novel view synthesis of urban scenes is essential for autonomous\ndriving-related applications.Existing NeRF and 3DGS-based methods show\npromising results in achieving photorealistic renderings but require slow,\nper-scene optimization. We introduce EVolSplat, an efficient 3D Gaussian\nSplatting model for urban scenes that works in a feed-forward manner. Unlike\nexisting feed-forward, pixel-aligned 3DGS methods, which often suffer from\nissues like multi-view inconsistencies and duplicated content, our approach\npredicts 3D Gaussians across multiple frames within a unified volume using a 3D\nconvolutional network. This is achieved by initializing 3D Gaussians with noisy\ndepth predictions, and then refining their geometric properties in 3D space and\npredicting color based on 2D textures. Our model also handles distant views and\nthe sky with a flexible hemisphere background model. This enables us to perform\nfast, feed-forward reconstruction while achieving real-time rendering.\nExperimental evaluations on the KITTI-360 and Waymo datasets show that our\nmethod achieves state-of-the-art quality compared to existing feed-forward\n3DGS- and NeRF-based methods."}
{"id": "2503.20421", "pdf": "https://arxiv.org/pdf/2503.20421", "abs": "https://arxiv.org/abs/2503.20421", "authors": ["Tom Kempton", "Stuart Burrell", "Connor Cheverall"], "title": "TempTest: Local Normalization Distortion and the Detection of Machine-generated Text", "categories": ["cs.CL", "cs.LG", "math.DS"], "comment": null, "summary": "Existing methods for the zero-shot detection of machine-generated text are\ndominated by three statistical quantities: log-likelihood, log-rank, and\nentropy. As language models mimic the distribution of human text ever closer,\nthis will limit our ability to build effective detection algorithms. To combat\nthis, we introduce a method for detecting machine-generated text that is\nentirely agnostic of the generating language model. This is achieved by\ntargeting a defect in the way that decoding strategies, such as temperature or\ntop-k sampling, normalize conditional probability measures. This method can be\nrigorously theoretically justified, is easily explainable, and is conceptually\ndistinct from existing methods for detecting machine-generated text. We\nevaluate our detector in the white and black box settings across various\nlanguage models, datasets, and passage lengths. We also study the effect of\nparaphrasing attacks on our detector and the extent to which it is biased\nagainst non-native speakers. In each of these settings, the performance of our\ntest is at least comparable to that of other state-of-the-art text detectors,\nand in some cases, we strongly outperform these baselines."}
{"id": "2503.20172", "pdf": "https://arxiv.org/pdf/2503.20172", "abs": "https://arxiv.org/abs/2503.20172", "authors": ["Mengqing Xue", "Yifei Liu", "Ling Guo", "Shaoli Huang", "Changxing Ding"], "title": "Guiding Human-Object Interactions with Rich Geometry and Relations", "categories": ["cs.CV"], "comment": "CVPR 2025.Project website: https://lalalfhdh.github.io/rog_page/", "summary": "Human-object interaction (HOI) synthesis is crucial for creating immersive\nand realistic experiences for applications such as virtual reality. Existing\nmethods often rely on simplified object representations, such as the object's\ncentroid or the nearest point to a human, to achieve physically plausible\nmotions. However, these approaches may overlook geometric complexity, resulting\nin suboptimal interaction fidelity. To address this limitation, we introduce\nROG, a novel diffusion-based framework that models the spatiotemporal\nrelationships inherent in HOIs with rich geometric detail. For efficient object\nrepresentation, we select boundary-focused and fine-detail key points from the\nobject mesh, ensuring a comprehensive depiction of the object's geometry. This\nrepresentation is used to construct an interactive distance field (IDF),\ncapturing the robust HOI dynamics. Furthermore, we develop a diffusion-based\nrelation model that integrates spatial and temporal attention mechanisms,\nenabling a better understanding of intricate HOI relationships. This relation\nmodel refines the generated motion's IDF, guiding the motion generation process\nto produce relation-aware and semantically aligned movements. Experimental\nevaluations demonstrate that ROG significantly outperforms state-of-the-art\nmethods in the realism and semantic accuracy of synthesized HOIs."}
{"id": "2503.20496", "pdf": "https://arxiv.org/pdf/2503.20496", "abs": "https://arxiv.org/abs/2503.20496", "authors": ["Aishik Mandal", "Dana Atzil-Slonim", "Thamar Solorio", "Iryna Gurevych"], "title": "Enhancing Depression Detection via Question-wise Modality Fusion", "categories": ["cs.CL"], "comment": "18 pages, 5 figures, The 10th Workshop on Computational Linguistics\n  and Clinical Psychology", "summary": "Depression is a highly prevalent and disabling condition that incurs\nsubstantial personal and societal costs. Current depression diagnosis involves\ndetermining the depression severity of a person through self-reported\nquestionnaires or interviews conducted by clinicians. This often leads to\ndelayed treatment and involves substantial human resources. Thus, several works\ntry to automate the process using multimodal data. However, they usually\noverlook the following: i) The variable contribution of each modality for each\nquestion in the questionnaire and ii) Using ordinal classification for the\ntask. This results in sub-optimal fusion and training methods. In this work, we\npropose a novel Question-wise Modality Fusion (QuestMF) framework trained with\na novel Imbalanced Ordinal Log-Loss (ImbOLL) function to tackle these issues.\nThe performance of our framework is comparable to the current state-of-the-art\nmodels on the E-DAIC dataset and enhances interpretability by predicting scores\nfor each question. This will help clinicians identify an individual's symptoms,\nallowing them to customise their interventions accordingly. We also make the\ncode for the QuestMF framework publicly available."}
{"id": "2503.20174", "pdf": "https://arxiv.org/pdf/2503.20174", "abs": "https://arxiv.org/abs/2503.20174", "authors": ["Shihao Zhou", "Dayu Li", "Jinshan Pan", "Juncheng Zhou", "Jinglei Shi", "Jufeng Yang"], "title": "Devil is in the Uniformity: Exploring Diverse Learners within Transformer for Image Restoration", "categories": ["cs.CV"], "comment": "11 pages, 10 figures", "summary": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials."}
{"id": "2503.20508", "pdf": "https://arxiv.org/pdf/2503.20508", "abs": "https://arxiv.org/abs/2503.20508", "authors": ["Leonor Barreiros", "Isabel Coutinho", "Gonçalo M. Correia", "Bruno Martins"], "title": "Explainable ICD Coding via Entity Linking", "categories": ["cs.CL"], "comment": "Accepted at CL4Health at NAACL 2025", "summary": "Clinical coding is a critical task in healthcare, although traditional\nmethods for automating clinical coding may not provide sufficient explicit\nevidence for coders in production environments. This evidence is crucial, as\nmedical coders have to make sure there exists at least one explicit passage in\nthe input health record that justifies the attribution of a code. We therefore\npropose to reframe the task as an entity linking problem, in which each\ndocument is annotated with its set of codes and respective textual evidence,\nenabling better human-machine collaboration. By leveraging parameter-efficient\nfine-tuning of Large Language Models (LLMs), together with constrained\ndecoding, we introduce three approaches to solve this problem that prove\neffective at disambiguating clinical mentions and that perform well in few-shot\nscenarios."}
{"id": "2503.20184", "pdf": "https://arxiv.org/pdf/2503.20184", "abs": "https://arxiv.org/abs/2503.20184", "authors": ["M. Kerem Aydin", "Yi-Chun Hung", "Jaclyn Pytlarz", "Qi Guo", "Emma Alexander"], "title": "Spectrum from Defocus: Fast Spectral Imaging with Chromatic Focal Stack", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Hyperspectral cameras face harsh trade-offs between spatial, spectral, and\ntemporal resolution in an inherently low-photon regime. Computational imaging\nsystems break through these trade-offs with compressive sensing, but require\ncomplex optics and/or extensive compute. We present Spectrum from Defocus\n(SfD), a chromatic focal sweep method that recovers state-of-the-art\nhyperspectral images with a small system of off-the-shelf optics and < 1 second\nof compute. Our camera uses two lenses and a grayscale sensor to preserve\nnearly all incident light in a chromatically-aberrated focal stack. Our\nphysics-based iterative algorithm efficiently demixes, deconvolves, and\ndenoises the blurry grayscale focal stack into a sharp spectral image. The\ncombination of photon efficiency, optical simplicity, and physical modeling\nmakes SfD a promising solution for fast, compact, interpretable hyperspectral\nimaging."}
{"id": "2503.20527", "pdf": "https://arxiv.org/pdf/2503.20527", "abs": "https://arxiv.org/abs/2503.20527", "authors": ["Zhicheng Guo", "Sijie Cheng", "Yuchen Niu", "Hao Wang", "Sicheng Zhou", "Wenbing Huang", "Yang Liu"], "title": "StableToolBench-MirrorAPI: Modeling Tool Environments as Mirrors of 7,000+ Real-World APIs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has spurred significant\ninterest in tool learning, where LLMs are augmented with external tools to\ntackle complex tasks. However, existing tool environments face challenges in\nbalancing stability, scalability, and realness, particularly for benchmarking\npurposes. To address this problem, we propose MirrorAPI, a novel framework that\ntrains specialized LLMs to accurately simulate real API responses, effectively\nacting as \"mirrors\" to tool environments. Using a comprehensive dataset of\nrequest-response pairs from 7,000+ APIs, we employ supervised fine-tuning and\nchain-of-thought reasoning to enhance simulation fidelity. MirrorAPI achieves\nsuperior accuracy and stability compared to state-of-the-art methods, as\ndemonstrated by its performance on the newly constructed MirrorAPI-Bench and\nits integration into StableToolBench."}
{"id": "2503.20188", "pdf": "https://arxiv.org/pdf/2503.20188", "abs": "https://arxiv.org/abs/2503.20188", "authors": ["Xiao Guo", "Xiufeng Song", "Yue Zhang", "Xiaohong Liu", "Xiaoming Liu"], "title": "Rethinking Vision-Language Model in Face Forensics: Multi-Modal Interpretable Forged Face Detector", "categories": ["cs.CV"], "comment": "8 figures; 6 tables", "summary": "Deepfake detection is a long-established research topic vital for mitigating\nthe spread of malicious misinformation. Unlike prior methods that provide\neither binary classification results or textual explanations separately, we\nintroduce a novel method capable of generating both simultaneously. Our method\nharnesses the multi-modal learning capability of the pre-trained CLIP and the\nunprecedented interpretability of large language models (LLMs) to enhance both\nthe generalization and explainability of deepfake detection. Specifically, we\nintroduce a multi-modal face forgery detector (M2F2-Det) that employs tailored\nface forgery prompt learning, incorporating the pre-trained CLIP to improve\ngeneralization to unseen forgeries. Also, M2F2-Det incorporates an LLM to\nprovide detailed textual explanations of its detection decisions, enhancing\ninterpretability by bridging the gap between natural language and subtle cues\nof facial forgeries. Empirically, we evaluate M2F2-Det on both detection and\nexplanation generation tasks, where it achieves state-of-the-art performance,\ndemonstrating its effectiveness in identifying and explaining diverse\nforgeries."}
{"id": "2503.20533", "pdf": "https://arxiv.org/pdf/2503.20533", "abs": "https://arxiv.org/abs/2503.20533", "authors": ["Yijiong Yu"], "title": "Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence", "categories": ["cs.CL"], "comment": "Our code is available in\n  https://github.com/yuyijiong/parallel-decoding-in-one-sequence", "summary": "Recent advances in reasoning models have demonstrated significant\nimprovements in accuracy, particularly for complex tasks such as mathematical\nreasoning, by employing detailed and comprehensive reasoning processes.\nHowever, generating these lengthy reasoning sequences is computationally\nexpensive and time-consuming. To address this inefficiency, we leverage the\ninherent parallelizability of certain tasks to accelerate the reasoning\nprocess. Specifically, when multiple parallel reasoning branches exist, we\ndecode multiple tokens per step using a specialized attention mask, processing\nthem within a single sequence. Experimental results show that our method\nachieves over 100% speedup in decoding time while basically maintaining\naccuracy."}
{"id": "2503.20190", "pdf": "https://arxiv.org/pdf/2503.20190", "abs": "https://arxiv.org/abs/2503.20190", "authors": ["Yuxuan Chen", "Jiawen Li", "Jiali Hu", "Xitong Ling", "Tian Guan", "Anjia Han", "Yonghong He"], "title": "Cross-Modal Prototype Allocation: Unsupervised Slide Representation Learning via Patch-Text Contrast in Computational Pathology", "categories": ["cs.CV"], "comment": "11pages,3 figures", "summary": "With the rapid advancement of pathology foundation models (FMs), the\nrepresentation learning of whole slide images (WSIs) attracts increasing\nattention. Existing studies develop high-quality patch feature extractors and\nemploy carefully designed aggregation schemes to derive slide-level\nrepresentations. However, mainstream weakly supervised slide representation\nlearning methods, primarily based on multiple instance learning (MIL), are\ntailored to specific downstream tasks, which limits their generalizability. To\naddress this issue, some studies explore unsupervised slide representation\nlearning. However, these approaches focus solely on the visual modality of\npatches, neglecting the rich semantic information embedded in textual data. In\nthis work, we propose ProAlign, a cross-modal unsupervised slide representation\nlearning framework. Specifically, we leverage a large language model (LLM) to\ngenerate descriptive text for the prototype types present in a WSI, introducing\npatch-text contrast to construct initial prototype embeddings. Furthermore, we\npropose a parameter-free attention aggregation strategy that utilizes the\nsimilarity between patches and these prototypes to form unsupervised slide\nembeddings applicable to a wide range of downstream tasks. Extensive\nexperiments on four public datasets show that ProAlign outperforms existing\nunsupervised frameworks and achieves performance comparable to some weakly\nsupervised models."}
{"id": "2503.20556", "pdf": "https://arxiv.org/pdf/2503.20556", "abs": "https://arxiv.org/abs/2503.20556", "authors": ["Andrei Niculae", "Adrian Cosma", "Emilian Radoi"], "title": "A Retrieval-Based Approach to Medical Procedure Matching in Romanian", "categories": ["cs.CL"], "comment": null, "summary": "Accurately mapping medical procedure names from healthcare providers to\nstandardized terminology used by insurance companies is a crucial yet complex\ntask. Inconsistencies in naming conventions lead to missclasified procedures,\ncausing administrative inefficiencies and insurance claim problems in private\nhealthcare settings. Many companies still use human resources for manual\nmapping, while there is a clear opportunity for automation. This paper proposes\na retrieval-based architecture leveraging sentence embeddings for medical name\nmatching in the Romanian healthcare system. This challenge is significantly\nmore difficult in underrepresented languages such as Romanian, where existing\npretrained language models lack domain-specific adaptation to medical text. We\nevaluate multiple embedding models, including Romanian, multilingual, and\nmedical-domain-specific representations, to identify the most effective\nsolution for this task. Our findings contribute to the broader field of medical\nNLP for low-resource languages such as Romanian."}
{"id": "2503.20198", "pdf": "https://arxiv.org/pdf/2503.20198", "abs": "https://arxiv.org/abs/2503.20198", "authors": ["Alex Jinpeng Wang", "Linjie Li", "Zhengyuan Yang", "Lijuan Wang", "Min Li"], "title": "Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models", "categories": ["cs.CV"], "comment": "16 pages", "summary": "Recent advancements in autoregressive and diffusion models have led to strong\nperformance in image generation with short scene text words. However,\ngenerating coherent, long-form text in images, such as paragraphs in slides or\ndocuments, remains a major challenge for current generative models. We present\nthe first work specifically focused on long text image generation, addressing a\ncritical gap in existing text-to-image systems that typically handle only brief\nphrases or single sentences. Through comprehensive analysis of state-of-the-art\nautoregressive generation models, we identify the image tokenizer as a critical\nbottleneck in text generating quality. To address this, we introduce a novel\ntext-focused, binary tokenizer optimized for capturing detailed scene text\nfeatures. Leveraging our tokenizer, we develop \\ModelName, a multimodal\nautoregressive model that excels in generating high-quality long-text images\nwith unprecedented fidelity. Our model offers robust controllability, enabling\ncustomization of text properties such as font style, size, color, and\nalignment. Extensive experiments demonstrate that \\ModelName~significantly\noutperforms SD3.5 Large~\\cite{sd3} and GPT4o~\\cite{gpt4o} with DALL-E\n3~\\cite{dalle3} in generating long text accurately, consistently, and flexibly.\nBeyond its technical achievements, \\ModelName~opens up exciting opportunities\nfor innovative applications like interleaved document and PowerPoint\ngeneration, establishing a new frontier in long-text image generating."}
{"id": "2503.20568", "pdf": "https://arxiv.org/pdf/2503.20568", "abs": "https://arxiv.org/abs/2503.20568", "authors": ["Soumitra Ghosh", "Begona Altuna", "Saeed Farzi", "Pietro Ferrazzi", "Alberto Lavelli", "Giulia Mezzanotte", "Manuela Speranza", "Bernardo Magnini"], "title": "Low-resource Information Extraction with the European Clinical Case Corpus", "categories": ["cs.CL"], "comment": null, "summary": "We present E3C-3.0, a multilingual dataset in the medical domain, comprising\nclinical cases annotated with diseases and test-result relations. The dataset\nincludes both native texts in five languages (English, French, Italian, Spanish\nand Basque) and texts translated and projected from the English source into\nfive target languages (Greek, Italian, Polish, Slovak, and Slovenian). A\nsemi-automatic approach has been implemented, including automatic annotation\nprojection based on Large Language Models (LLMs) and human revision. We present\nseveral experiments showing that current state-of-the-art LLMs can benefit from\nbeing fine-tuned on the E3C-3.0 dataset. We also show that transfer learning in\ndifferent languages is very effective, mitigating the scarcity of data.\nFinally, we compare performance both on native data and on projected data. We\nrelease the data at\nhttps://huggingface.co/collections/NLP-FBK/e3c-projected-676a7d6221608d60e4e9fd89 ."}
{"id": "2503.20199", "pdf": "https://arxiv.org/pdf/2503.20199", "abs": "https://arxiv.org/abs/2503.20199", "authors": ["Mélisande Teng", "Arthur Ouaknine", "Etienne Laliberté", "Yoshua Bengio", "David Rolnick", "Hugo Larochelle"], "title": "Assessing SAM for Tree Crown Instance Segmentation from Drone Imagery", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICLR 2025 ML4RS workshop", "summary": "The potential of tree planting as a natural climate solution is often\nundermined by inadequate monitoring of tree planting projects. Current\nmonitoring methods involve measuring trees by hand for each species, requiring\nextensive cost, time, and labour. Advances in drone remote sensing and computer\nvision offer great potential for mapping and characterizing trees from aerial\nimagery, and large pre-trained vision models, such as the Segment Anything\nModel (SAM), may be a particularly compelling choice given limited labeled\ndata. In this work, we compare SAM methods for the task of automatic tree crown\ninstance segmentation in high resolution drone imagery of young tree\nplantations. We explore the potential of SAM for this task, and find that\nmethods using SAM out-of-the-box do not outperform a custom Mask R-CNN, even\nwith well-designed prompts, but that there is potential for methods which tune\nSAM further. We also show that predictions can be improved by adding Digital\nSurface Model (DSM) information as an input."}
{"id": "2503.20588", "pdf": "https://arxiv.org/pdf/2503.20588", "abs": "https://arxiv.org/abs/2503.20588", "authors": ["Frances Yung", "Varsha Suresh", "Zaynab Reza", "Mansoor Ahmad", "Vera Demberg"], "title": "Synthetic Data Augmentation for Cross-domain Implicit Discourse Relation Recognition", "categories": ["cs.CL"], "comment": null, "summary": "Implicit discourse relation recognition (IDRR) -- the task of identifying the\nimplicit coherence relation between two text spans -- requires deep semantic\nunderstanding. Recent studies have shown that zero- or few-shot approaches\nsignificantly lag behind supervised models, but LLMs may be useful for\nsynthetic data augmentation, where LLMs generate a second argument following a\nspecified coherence relation. We applied this approach in a cross-domain\nsetting, generating discourse continuations using unlabelled target-domain data\nto adapt a base model which was trained on source-domain labelled data.\nEvaluations conducted on a large-scale test set revealed that different\nvariations of the approach did not result in any significant improvements. We\nconclude that LLMs often fail to generate useful samples for IDRR, and\nemphasize the importance of considering both statistical significance and\ncomparability when evaluating IDRR models."}
{"id": "2503.20207", "pdf": "https://arxiv.org/pdf/2503.20207", "abs": "https://arxiv.org/abs/2503.20207", "authors": ["Peiyuan Ni", "Chee Meng Chew", "Marcelo H. Ang Jr.", "Gregory S. Chirikjian"], "title": "Reasoning and Learning a Perceptual Metric for Self-Training of Reflective Objects in Bin-Picking with a Low-cost Camera", "categories": ["cs.CV", "cs.RO"], "comment": "9 pages, 10 figures", "summary": "Bin-picking of metal objects using low-cost RGB-D cameras often suffers from\nsparse depth information and reflective surface textures, leading to errors and\nthe need for manual labeling. To reduce human intervention, we propose a\ntwo-stage framework consisting of a metric learning stage and a self-training\nstage. Specifically, to automatically process data captured by a low-cost\ncamera (LC), we introduce a Multi-object Pose Reasoning (MoPR) algorithm that\noptimizes pose hypotheses under depth, collision, and boundary constraints. To\nfurther refine pose candidates, we adopt a Symmetry-aware Lie-group based\nBayesian Gaussian Mixture Model (SaL-BGMM), integrated with the\nExpectation-Maximization (EM) algorithm, for symmetry-aware filtering.\nAdditionally, we propose a Weighted Ranking Information Noise Contrastive\nEstimation (WR-InfoNCE) loss to enable the LC to learn a perceptual metric from\nreconstructed data, supporting self-training on untrained or even unseen\nobjects. Experimental results show that our approach outperforms several\nstate-of-the-art methods on both the ROBI dataset and our newly introduced\nSelf-ROBI dataset."}
{"id": "2503.20623", "pdf": "https://arxiv.org/pdf/2503.20623", "abs": "https://arxiv.org/abs/2503.20623", "authors": ["Alessandro Maisto"], "title": "Collaborative Storytelling and LLM: A Linguistic Analysis of Automatically-Generated Role-Playing Game Sessions", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Role-playing games (RPG) are games in which players interact with one another\nto create narratives. The role of players in the RPG is largely based on the\ninteraction between players and their characters. This emerging form of shared\nnarrative, primarily oral, is receiving increasing attention. In particular,\nmany authors investigated the use of an LLM as an actor in the game. In this\npaper, we aim to discover to what extent the language of Large Language Models\n(LLMs) exhibit oral or written features when asked to generate an RPG session\nwithout human interference. We will conduct a linguistic analysis of the\nlexical and syntactic features of the generated texts and compare the results\nwith analyses of conversations, transcripts of human RPG sessions, and books.\nWe found that LLMs exhibit a pattern that is distinct from all other text\ncategories, including oral conversations, human RPG sessions and books. Our\nanalysis has shown how training influences the way LLMs express themselves and\nprovides important indications of the narrative capabilities of these tools."}
{"id": "2503.20209", "pdf": "https://arxiv.org/pdf/2503.20209", "abs": "https://arxiv.org/abs/2503.20209", "authors": ["Chengyang Hu", "Yuduo Chen", "Lizhuang Ma"], "title": "BEAR: A Video Dataset For Fine-grained Behaviors Recognition Oriented with Action and Environment Factors", "categories": ["cs.CV"], "comment": "Accept by ICME2025", "summary": "Behavior recognition is an important task in video representation learning.\nAn essential aspect pertains to effective feature learning conducive to\nbehavior recognition. Recently, researchers have started to study fine-grained\nbehavior recognition, which provides similar behaviors and encourages the model\nto concern with more details of behaviors with effective features for\ndistinction. However, previous fine-grained behaviors limited themselves to\ncontrolling partial information to be similar, leading to an unfair and not\ncomprehensive evaluation of existing works. In this work, we develop a new\nvideo fine-grained behavior dataset, named BEAR, which provides fine-grained\n(i.e. similar) behaviors that uniquely focus on two primary factors defining\nbehavior: Environment and Action. It includes two fine-grained behavior\nprotocols including Fine-grained Behavior with Similar Environments and\nFine-grained Behavior with Similar Actions as well as multiple sub-protocols as\ndifferent scenarios. Furthermore, with this new dataset, we conduct multiple\nexperiments with different behavior recognition models. Our research primarily\nexplores the impact of input modality, a critical element in studying the\nenvironmental and action-based aspects of behavior recognition. Our\nexperimental results yield intriguing insights that have substantial\nimplications for further research endeavors."}
{"id": "2503.20639", "pdf": "https://arxiv.org/pdf/2503.20639", "abs": "https://arxiv.org/abs/2503.20639", "authors": ["Jeffery L Painter", "Gregory E Powell", "Andrew Bate"], "title": "PVLens: Enhancing Pharmacovigilance Through Automated Label Extraction", "categories": ["cs.CL", "cs.LG", "J.3; H.3.1; D.2.12"], "comment": null, "summary": "Reliable drug safety reference databases are essential for pharmacovigilance,\nyet existing resources like SIDER are outdated and static. We introduce PVLens,\nan automated system that extracts labeled safety information from FDA\nStructured Product Labels (SPLs) and maps terms to MedDRA. PVLens integrates\nautomation with expert oversight through a web-based review tool. In validation\nagainst 97 drug labels, PVLens achieved an F1 score of 0.882, with high recall\n(0.983) and moderate precision (0.799). By offering a scalable, more accurate\nand continuously updated alternative to SIDER, PVLens enhances real-time\npharamcovigilance with improved accuracy and contemporaneous insights."}
{"id": "2503.20211", "pdf": "https://arxiv.org/pdf/2503.20211", "abs": "https://arxiv.org/abs/2503.20211", "authors": ["Weilong Yan", "Ming Li", "Haipeng Li", "Shuwei Shao", "Robby T. Tan"], "title": "Synthetic-to-Real Self-supervised Robust Depth Estimation via Learning with Motion and Structure Priors", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Self-supervised depth estimation from monocular cameras in diverse outdoor\nconditions, such as daytime, rain, and nighttime, is challenging due to the\ndifficulty of learning universal representations and the severe lack of labeled\nreal-world adverse data. Previous methods either rely on synthetic inputs and\npseudo-depth labels or directly apply daytime strategies to adverse conditions,\nresulting in suboptimal results. In this paper, we present the first\nsynthetic-to-real robust depth estimation framework, incorporating motion and\nstructure priors to capture real-world knowledge effectively. In the synthetic\nadaptation, we transfer motion-structure knowledge inside cost volumes for\nbetter robust representation, using a frozen daytime model to train a depth\nestimator in synthetic adverse conditions. In the innovative real adaptation,\nwhich targets to fix synthetic-real gaps, models trained earlier identify the\nweather-insensitive regions with a designed consistency-reweighting strategy to\nemphasize valid pseudo-labels. We introduce a new regularization by gathering\nexplicit depth distributions to constrain the model when facing real-world\ndata. Experiments show that our method outperforms the state-of-the-art across\ndiverse conditions in multi-frame and single-frame evaluations. We achieve\nimprovements of 7.5% and 4.3% in AbsRel and RMSE on average for nuScenes and\nRobotcar datasets (daytime, nighttime, rain). In zero-shot evaluation of\nDrivingStereo (rain, fog), our method generalizes better than the previous\nones."}
{"id": "2503.20641", "pdf": "https://arxiv.org/pdf/2503.20641", "abs": "https://arxiv.org/abs/2503.20641", "authors": ["Han Wu", "Yuxuan Yao", "Shuqi Liu", "Zehua Liu", "Xiaojin Fu", "Xiongwei Han", "Xing Li", "Hui-Ling Zhen", "Tao Zhong", "Mingxuan Yuan"], "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging", "categories": ["cs.CL"], "comment": "Work in progress; technical report", "summary": "The transition from System 1 to System 2 reasoning in large language models\n(LLMs) has marked significant advancements in handling complex tasks through\ndeliberate, iterative thinking. However, this progress often comes at the cost\nof efficiency, as models tend to overthink, generating redundant reasoning\nsteps without proportional improvements in output quality. Long-to-Short (L2S)\nreasoning has emerged as a promising solution to this challenge, aiming to\nbalance reasoning depth with practical efficiency. While existing approaches,\nsuch as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt\nengineering, have shown potential, they are either computationally expensive or\nunstable. Model merging, on the other hand, offers a cost-effective and robust\nalternative by integrating the quick-thinking capabilities of System 1 models\nwith the methodical reasoning of System 2 models. In this work, we present a\ncomprehensive empirical study on model merging for L2S reasoning, exploring\ndiverse methodologies, including task-vector-based, SVD-based, and\nactivation-informed merging. Our experiments reveal that model merging can\nreduce average response length by up to 55% while preserving or even improving\nbaseline performance. We also identify a strong correlation between model scale\nand merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models.\nFurthermore, we investigate the merged model's ability to self-critique and\nself-correct, as well as its adaptive response length based on task complexity.\nOur findings highlight model merging as a highly efficient and effective\nparadigm for L2S reasoning, offering a practical solution to the overthinking\nproblem while maintaining the robustness of System 2 reasoning. This work can\nbe found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging."}
{"id": "2503.20218", "pdf": "https://arxiv.org/pdf/2503.20218", "abs": "https://arxiv.org/abs/2503.20218", "authors": ["Haiyang Liu", "Zhan Xu", "Fa-Ting Hong", "Hsin-Ping Huang", "Yi Zhou", "Yang Zhou"], "title": "Video Motion Graphs", "categories": ["cs.CV"], "comment": "14 pages,10 figures", "summary": "We present Video Motion Graphs, a system designed to generate realistic human\nmotion videos. Using a reference video and conditional signals such as music or\nmotion tags, the system synthesizes new videos by first retrieving video clips\nwith gestures matching the conditions and then generating interpolation frames\nto seamlessly connect clip boundaries. The core of our approach is HMInterp, a\nrobust Video Frame Interpolation (VFI) model that enables seamless\ninterpolation of discontinuous frames, even for complex motion scenarios like\ndancing. HMInterp i) employs a dual-branch interpolation approach, combining a\nMotion Diffusion Model for human skeleton motion interpolation with a\ndiffusion-based video frame interpolation model for final frame generation. ii)\nadopts condition progressive training to effectively leverage identity strong\nand weak conditions, such as images and pose. These designs ensure both high\nvideo texture quality and accurate motion trajectory. Results show that our\nVideo Motion Graphs outperforms existing generative- and retrieval-based\nmethods for multi-modal conditioned human motion video generation. Project page\ncan be found at https://h-liu1997.github.io/Video-Motion-Graphs/"}
{"id": "2503.20648", "pdf": "https://arxiv.org/pdf/2503.20648", "abs": "https://arxiv.org/abs/2503.20648", "authors": ["Raj Sanjay Shah", "Lei Xu", "Qianchu Liu", "Jon Burnsky", "Drew Bertagnolli", "Chaitanya Shivade"], "title": "TN-Eval: Rubric and Evaluation Protocols for Measuring the Quality of Behavioral Therapy Notes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Behavioral therapy notes are important for both legal compliance and patient\ncare. Unlike progress notes in physical health, quality standards for\nbehavioral therapy notes remain underdeveloped. To address this gap, we\ncollaborated with licensed therapists to design a comprehensive rubric for\nevaluating therapy notes across key dimensions: completeness, conciseness, and\nfaithfulness. Further, we extend a public dataset of behavioral health\nconversations with therapist-written notes and LLM-generated notes, and apply\nour evaluation framework to measure their quality. We find that: (1) A\nrubric-based manual evaluation protocol offers more reliable and interpretable\nresults than traditional Likert-scale annotations. (2) LLMs can mimic human\nevaluators in assessing completeness and conciseness but struggle with\nfaithfulness. (3) Therapist-written notes often lack completeness and\nconciseness, while LLM-generated notes contain hallucination. Surprisingly, in\na blind test, therapists prefer and judge LLM-generated notes to be superior to\ntherapist-written notes."}
{"id": "2503.20220", "pdf": "https://arxiv.org/pdf/2503.20220", "abs": "https://arxiv.org/abs/2503.20220", "authors": ["Weijie Guo", "Guofeng Zhang", "Wufei Ma", "Alan Yuille"], "title": "DINeMo: Learning Neural Mesh Models with no 3D Annotations", "categories": ["cs.CV"], "comment": "Technical report", "summary": "Category-level 3D/6D pose estimation is a crucial step towards comprehensive\n3D scene understanding, which would enable a broad range of applications in\nrobotics and embodied AI. Recent works explored neural mesh models that\napproach a range of 2D and 3D tasks from an analysis-by-synthesis perspective.\nDespite the largely enhanced robustness to partial occlusion and domain shifts,\nthese methods depended heavily on 3D annotations for part-contrastive learning,\nwhich confines them to a narrow set of categories and hinders efficient\nscaling. In this work, we present DINeMo, a novel neural mesh model that is\ntrained with no 3D annotations by leveraging pseudo-correspondence obtained\nfrom large visual foundation models. We adopt a bidirectional\npseudo-correspondence generation method, which produce pseudo correspondence\nutilize both local appearance features and global context information.\nExperimental results on car datasets demonstrate that our DINeMo outperforms\nprevious zero- and few-shot 3D pose estimation by a wide margin, narrowing the\ngap with fully-supervised methods by 67.3%. Our DINeMo also scales effectively\nand efficiently when incorporating more unlabeled images during training, which\ndemonstrate the advantages over supervised learning methods that rely on 3D\nannotations. Our project page is available at\nhttps://analysis-by-synthesis.github.io/DINeMo/."}
{"id": "2503.20701", "pdf": "https://arxiv.org/pdf/2503.20701", "abs": "https://arxiv.org/abs/2503.20701", "authors": ["Zhendong Chu", "Jian Xie", "Shen Wang", "Zichao Wang", "Qingsong Wen"], "title": "UniEDU: A Unified Language and Vision Assistant for Education Applications", "categories": ["cs.CL"], "comment": null, "summary": "Education materials for K-12 students often consist of multiple modalities,\nsuch as text and images, posing challenges for models to fully understand\nnuanced information in these materials. In this paper, we propose a unified\nlanguage and vision assistant UniEDU designed for various educational\napplications, including knowledge recommendation, knowledge tracing, time cost\nprediction, and user answer prediction, all within a single model. Unlike\nconventional task-specific models, UniEDU offers a unified solution that excels\nacross multiple educational tasks while maintaining strong generalization\ncapabilities. Its adaptability makes it well-suited for real-world deployment\nin diverse learning environments. Furthermore, UniEDU is optimized for\nindustry-scale deployment by significantly reducing computational\noverhead-achieving approximately a 300\\% increase in efficiency-while\nmaintaining competitive performance with minimal degradation compared to fully\nfine-tuned models. This work represents a significant step toward creating\nversatile AI systems tailored to the evolving demands of education."}
{"id": "2503.20221", "pdf": "https://arxiv.org/pdf/2503.20221", "abs": "https://arxiv.org/abs/2503.20221", "authors": ["Taorui Wang", "Zitong Yu", "Yong Xu"], "title": "TC-GS: Tri-plane based compression for 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted by ICME 2025", "summary": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a prominent framework\nfor novel view synthesis, providing high fidelity and rapid rendering speed.\nHowever, the substantial data volume of 3DGS and its attributes impede its\npractical utility, requiring compression techniques for reducing memory cost.\nNevertheless, the unorganized shape of 3DGS leads to difficulties in\ncompression. To formulate unstructured attributes into normative distribution,\nwe propose a well-structured tri-plane to encode Gaussian attributes,\nleveraging the distribution of attributes for compression. To exploit the\ncorrelations among adjacent Gaussians, K-Nearest Neighbors (KNN) is used when\ndecoding Gaussian distribution from the Tri-plane. We also introduce Gaussian\nposition information as a prior of the position-sensitive decoder.\nAdditionally, we incorporate an adaptive wavelet loss, aiming to focus on the\nhigh-frequency details as iterations increase. Our approach has achieved\nresults that are comparable to or surpass that of SOTA 3D Gaussians Splatting\ncompression work in extensive experiments across multiple datasets. The codes\nare released at https://github.com/timwang2001/TC-GS."}
{"id": "2503.20715", "pdf": "https://arxiv.org/pdf/2503.20715", "abs": "https://arxiv.org/abs/2503.20715", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Vijay Mago"], "title": "From Annotation to Adaptation: Metrics, Synthetic Data, and Aspect Extraction for Aspect-Based Sentiment Analysis with Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to NAACL SRW 2025", "summary": "This study examines the performance of Large Language Models (LLMs) in\nAspect-Based Sentiment Analysis (ABSA), with a focus on implicit aspect\nextraction in a novel domain. Using a synthetic sports feedback dataset, we\nevaluate open-weight LLMs' ability to extract aspect-polarity pairs and propose\na metric to facilitate the evaluation of aspect extraction with generative\nmodels. Our findings highlight both the potential and limitations of LLMs in\nthe ABSA task."}
{"id": "2503.20230", "pdf": "https://arxiv.org/pdf/2503.20230", "abs": "https://arxiv.org/abs/2503.20230", "authors": ["Ugochukwu Ejike Akpudo", "Yongsheng Gao", "Jun Zhou", "Andrew Lewis"], "title": "TraNCE: Transformative Non-linear Concept Explainer for CNNs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Convolutional neural networks (CNNs) have succeeded remarkably in various\ncomputer vision tasks. However, they are not intrinsically explainable. While\nthe feature-level understanding of CNNs reveals where the models looked,\nconcept-based explainability methods provide insights into what the models saw.\nHowever, their assumption of linear reconstructability of image activations\nfails to capture the intricate relationships within these activations. Their\nFidelity-only approach to evaluating global explanations also presents a new\nconcern. For the first time, we address these limitations with the novel\nTransformative Nonlinear Concept Explainer (TraNCE) for CNNs. Unlike linear\nreconstruction assumptions made by existing methods, TraNCE captures the\nintricate relationships within the activations. This study presents three\noriginal contributions to the CNN explainability literature: (i) An automatic\nconcept discovery mechanism based on variational autoencoders (VAEs). This\ntransformative concept discovery process enhances the identification of\nmeaningful concepts from image activations. (ii) A visualization module that\nleverages the Bessel function to create a smooth transition between\nprototypical image pixels, revealing not only what the CNN saw but also what\nthe CNN avoided, thereby mitigating the challenges of concept duplication as\ndocumented in previous works. (iii) A new metric, the Faith score, integrates\nboth Coherence and Fidelity for a comprehensive evaluation of explainer\nfaithfulness and consistency."}
{"id": "2503.20737", "pdf": "https://arxiv.org/pdf/2503.20737", "abs": "https://arxiv.org/abs/2503.20737", "authors": ["Jeffery L Painter", "François Haguinet", "Gregory E Powell", "Andrew Bate"], "title": "Ontology-based Semantic Similarity Measures for Clustering Medical Concepts in Drug Safety", "categories": ["cs.CL", "I.2.4; G.3; H.3.3"], "comment": null, "summary": "Semantic similarity measures (SSMs) are widely used in biomedical research\nbut remain underutilized in pharmacovigilance. This study evaluates six\nontology-based SSMs for clustering MedDRA Preferred Terms (PTs) in drug safety\ndata. Using the Unified Medical Language System (UMLS), we assess each method's\nability to group PTs around medically meaningful centroids. A high-throughput\nframework was developed with a Java API and Python and R interfaces support\nlarge-scale similarity computations. Results show that while path-based methods\nperform moderately with F1 scores of 0.36 for WUPALMER and 0.28 for LCH,\nintrinsic information content (IC)-based measures, especially INTRINSIC-LIN and\nSOKAL, consistently yield better clustering accuracy (F1 score of 0.403).\nValidated against expert review and standard MedDRA queries (SMQs), our\nfindings highlight the promise of IC-based SSMs in enhancing pharmacovigilance\nworkflows by improving early signal detection and reducing manual review."}
{"id": "2503.20235", "pdf": "https://arxiv.org/pdf/2503.20235", "abs": "https://arxiv.org/abs/2503.20235", "authors": ["Ahyun Seo", "Minsu Cho"], "title": "Leveraging 3D Geometric Priors in 2D Rotation Symmetry Detection", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Symmetry plays a vital role in understanding structural patterns, aiding\nobject recognition and scene interpretation. This paper focuses on rotation\nsymmetry, where objects remain unchanged when rotated around a central axis,\nrequiring detection of rotation centers and supporting vertices. Traditional\nmethods relied on hand-crafted feature matching, while recent segmentation\nmodels based on convolutional neural networks detect rotation centers but\nstruggle with 3D geometric consistency due to viewpoint distortions. To\novercome this, we propose a model that directly predicts rotation centers and\nvertices in 3D space and projects the results back to 2D while preserving\nstructural integrity. By incorporating a vertex reconstruction stage enforcing\n3D geometric priors -- such as equal side lengths and interior angles -- our\nmodel enhances robustness and accuracy. Experiments on the DENDI dataset show\nsuperior performance in rotation axis detection and validate the impact of 3D\npriors through ablation studies."}
{"id": "2503.20749", "pdf": "https://arxiv.org/pdf/2503.20749", "abs": "https://arxiv.org/abs/2503.20749", "authors": ["Yuxuan Lu", "Jing Huang", "Yan Han", "Bennet Bei", "Yaochen Xie", "Dakuo Wang", "Jessie Wang", "Qi He"], "title": "Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Recent research shows that LLMs can simulate ``believable'' human behaviors\nto power LLM agents via prompt-only methods. In this work, we focus on\nevaluating and improving LLM's objective ``accuracy'' rather than the\nsubjective ``believability'' in the web action generation task, leveraging a\nlarge-scale, real-world dataset collected from online shopping human actions.\nWe present the first comprehensive quantitative evaluation of state-of-the-art\nLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action\ngeneration. Our results show that fine-tuning LLMs on real-world behavioral\ndata substantially improves their ability to generate actions compared to\nprompt-only methods. Furthermore, incorporating synthesized reasoning traces\ninto model training leads to additional performance gains, demonstrating the\nvalue of explicit rationale in behavior modeling. This work establishes a new\nbenchmark for evaluating LLMs in behavior simulation and offers actionable\ninsights into how real-world action data and reasoning augmentation can enhance\nthe fidelity of LLM agents."}
{"id": "2503.20240", "pdf": "https://arxiv.org/pdf/2503.20240", "abs": "https://arxiv.org/abs/2503.20240", "authors": ["Prin Phunyaphibarn", "Phillip Y. Lee", "Jaihoon Kim", "Minhyuk Sung"], "title": "Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Classifier-Free Guidance (CFG) is a fundamental technique in training\nconditional diffusion models. The common practice for CFG-based training is to\nuse a single network to learn both conditional and unconditional noise\nprediction, with a small dropout rate for conditioning. However, we observe\nthat the joint learning of unconditional noise with limited bandwidth in\ntraining results in poor priors for the unconditional case. More importantly,\nthese poor unconditional noise predictions become a serious reason for\ndegrading the quality of conditional generation. Inspired by the fact that most\nCFG-based conditional models are trained by fine-tuning a base model with\nbetter unconditional generation, we first show that simply replacing the\nunconditional noise in CFG with that predicted by the base model can\nsignificantly improve conditional generation. Furthermore, we show that a\ndiffusion model other than the one the fine-tuned model was trained on can be\nused for unconditional noise replacement. We experimentally verify our claim\nwith a range of CFG-based conditional models for both image and video\ngeneration, including Zero-1-to-3, Versatile Diffusion, DiT, DynamiCrafter, and\nInstructPix2Pix."}
{"id": "2503.20756", "pdf": "https://arxiv.org/pdf/2503.20756", "abs": "https://arxiv.org/abs/2503.20756", "authors": ["Chenxi Wang", "Jizhan Fang", "Xiang Chen", "Bozhong Tian", "Ziwen Xu", "Huajun Chen", "Ningyu Zhang"], "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit."}
{"id": "2503.20248", "pdf": "https://arxiv.org/pdf/2503.20248", "abs": "https://arxiv.org/abs/2503.20248", "authors": ["Mingfu Liang", "Jiahuan Zhou", "Xu Zou", "Ying Wu"], "title": "Incremental Object Keypoint Learning", "categories": ["cs.CV", "cs.LG"], "comment": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "summary": "Existing progress in object keypoint estimation primarily benefits from the\nconventional supervised learning paradigm based on numerous data labeled with\npre-defined keypoints. However, these well-trained models can hardly detect the\nundefined new keypoints in test time, which largely hinders their feasibility\nfor diverse downstream tasks. To handle this, various solutions are explored\nbut still suffer from either limited generalizability or transferability.\nTherefore, in this paper, we explore a novel keypoint learning paradigm in that\nwe only annotate new keypoints in the new data and incrementally train the\nmodel, without retaining any old data, called Incremental object Keypoint\nLearning (IKL). A two-stage learning scheme as a novel baseline tailored to IKL\nis developed. In the first Knowledge Association stage, given the data labeled\nwith only new keypoints, an auxiliary KA-Net is trained to automatically\nassociate the old keypoints to these new ones based on their spatial and\nintrinsic anatomical relations. In the second Mutual Promotion stage, based on\na keypoint-oriented spatial distillation loss, we jointly leverage the\nauxiliary KA-Net and the old model for knowledge consolidation to mutually\npromote the estimation of all old and new keypoints. Owing to the investigation\nof the correlations between new and old keypoints, our proposed method can not\njust effectively mitigate the catastrophic forgetting of old keypoints, but may\neven further improve the estimation of the old ones and achieve a positive\ntransfer beyond anti-forgetting. Such an observation has been solidly verified\nby extensive experiments on different keypoint datasets, where our method\nexhibits superiority in alleviating the forgetting issue and boosting\nperformance while enjoying labeling efficiency even under the low-shot data\nregime."}
{"id": "2503.20757", "pdf": "https://arxiv.org/pdf/2503.20757", "abs": "https://arxiv.org/abs/2503.20757", "authors": ["Yunhai Hu", "Yilun Zhao", "Chen Zhao", "Arman Cohan"], "title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search", "categories": ["cs.CL"], "comment": null, "summary": "We introduce MCTS-RAG, a novel approach that enhances the reasoning\ncapabilities of small language models on knowledge-intensive tasks by\nleveraging retrieval-augmented generation (RAG) to provide relevant context and\nMonte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically\nintegrates retrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically retrieve information\nindependently from reasoning and thus integrate knowledge suboptimally, or\nconventional MCTS reasoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured reasoning with adaptive\nretrieval. This integrated approach enhances decision-making, reduces\nhallucinations, and ensures improved factual accuracy and response consistency.\nThe experimental results on multiple reasoning and knowledge-intensive datasets\ndatasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method\nenables small-scale LMs to achieve performance comparable to frontier LLMs like\nGPT-4o by effectively scaling inference-time compute, setting a new standard\nfor reasoning in small-scale models."}
{"id": "2503.20252", "pdf": "https://arxiv.org/pdf/2503.20252", "abs": "https://arxiv.org/abs/2503.20252", "authors": ["Yejin Kwon", "Daeun Moon", "Youngje Oh", "Hyunsoo Yoon"], "title": "LogicQA: Logical Anomaly Detection with Vision Language Model Generated Questions", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Anomaly Detection (AD) focuses on detecting samples that differ from the\nstandard pattern, making it a vital tool in process control. Logical anomalies\nmay appear visually normal yet violate predefined constraints on object\npresence, arrangement, or quantity, depending on reasoning and explainability.\nWe introduce LogicQA, a framework that enhances AD by providing industrial\noperators with explanations for logical anomalies. LogicQA compiles\nautomatically generated questions into a checklist and collects responses to\nidentify violations of logical constraints. LogicQA is training-free,\nannotation-free, and operates in a few-shot setting. We achieve\nstate-of-the-art (SOTA) Logical AD performance on public benchmarks, MVTec LOCO\nAD, with an AUROC of 87.6 percent and an F1-max of 87.0 percent along with the\nexplanations of anomalies. Also, our approach has shown outstanding performance\non semiconductor SEM corporate data, further validating its effectiveness in\nindustrial applications."}
{"id": "2503.20786", "pdf": "https://arxiv.org/pdf/2503.20786", "abs": "https://arxiv.org/abs/2503.20786", "authors": ["Sondos Mahmoud Bsharat", "Mukul Ranjan", "Aidar Myrzakhan", "Jiacheng Liu", "Bowei Guo", "Shengkun Tang", "Zhuang Liu", "Yuanzhi Li", "Zhiqiang Shen"], "title": "Mobile-MMLU: A Mobile Intelligence Language Understanding Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": "An order-invariant and mobile-centric benchmark. Code and data are\n  available at: https://github.com/VILA-Lab/Mobile-MMLU", "summary": "Rapid advancements in large language models (LLMs) have increased interest in\ndeploying them on mobile devices for on-device AI applications. Mobile users\ninteract differently with LLMs compared to desktop users, creating unique\nexpectations and data biases. Current benchmark datasets primarily target at\nserver and desktop environments, and there is a notable lack of extensive\ndatasets specifically designed for mobile contexts. Additionally, mobile\ndevices face strict limitations in storage and computing resources,\nconstraining model size and capabilities, thus requiring optimized efficiency\nand prioritized knowledge. To address these challenges, we introduce\nMobile-MMLU, a large-scale benchmark dataset tailored for mobile intelligence.\nIt consists of 16,186 questions across 80 mobile-related fields, designed to\nevaluate LLM performance in realistic mobile scenarios. A challenging subset,\nMobile-MMLU-Pro, provides advanced evaluation similar in size to MMLU-Pro but\nsignificantly more difficult than our standard full set. Both benchmarks use\nmultiple-choice, order-invariant questions focused on practical mobile\ninteractions, such as recipe suggestions, travel planning, and essential daily\ntasks. The dataset emphasizes critical mobile-specific metrics like inference\nlatency, energy consumption, memory usage, and response quality, offering\ncomprehensive insights into model performance under mobile constraints.\nMoreover, it prioritizes privacy and adaptability, assessing models' ability to\nperform on-device processing, maintain user privacy, and adapt to personalized\nusage patterns. Mobile-MMLU family offers a standardized framework for\ndeveloping and comparing mobile-optimized LLMs, enabling advancements in\nproductivity and decision-making within mobile computing environments. Our code\nand data are available at: https://github.com/VILA-Lab/Mobile-MMLU."}
{"id": "2503.20258", "pdf": "https://arxiv.org/pdf/2503.20258", "abs": "https://arxiv.org/abs/2503.20258", "authors": ["Jiaheng Zhou", "Yanfeng Zhou", "Wei Fang", "Yuxing Tang", "Le Lu", "Ge Yang"], "title": "Mamba-3D as Masked Autoencoders for Accurate and Data-Efficient Analysis of Medical Ultrasound Videos", "categories": ["cs.CV"], "comment": null, "summary": "Ultrasound videos are an important form of clinical imaging data, and deep\nlearning-based automated analysis can improve diagnostic accuracy and clinical\nefficiency. However, the scarcity of labeled data and the inherent challenges\nof video analysis have impeded the advancement of related methods. In this\nwork, we introduce E-ViM$^3$, a data-efficient Vision Mamba network that\npreserves the 3D structure of video data, enhancing long-range dependencies and\ninductive biases to better model space-time correlations. With our design of\nEnclosure Global Tokens (EGT), the model captures and aggregates global\nfeatures more effectively than competing methods. To further improve data\nefficiency, we employ masked video modeling for self-supervised pre-training,\nwith the proposed Spatial-Temporal Chained (STC) masking strategy designed to\nadapt to various video scenarios. Experiments demonstrate that E-ViM$^3$\nperforms as the state-of-the-art in two high-level semantic analysis tasks\nacross four datasets of varying sizes: EchoNet-Dynamic, CAMUS, MICCAI-BUV, and\nWHBUS. Furthermore, our model achieves competitive performance with limited\nlabels, highlighting its potential impact on real-world clinical applications."}
{"id": "2503.19950", "pdf": "https://arxiv.org/pdf/2503.19950", "abs": "https://arxiv.org/abs/2503.19950", "authors": ["Han Chen", "Zicong Jiang", "Zining Zhang", "Bingsheng He", "Pingyi Luo", "Mian Lu", "Yuqiang Chen"], "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior Accuracy Preservation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by ICLR 2025 Workshop on Sparsity in LLMs (SLLM)", "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV."}
{"id": "2503.20268", "pdf": "https://arxiv.org/pdf/2503.20268", "abs": "https://arxiv.org/abs/2503.20268", "authors": ["Ziran Zhang", "Xiaohui Li", "Yihao Liu", "Yujin Wang", "Yueting Chen", "Tianfan Xue", "Shi Guo"], "title": "EGVD: Event-Guided Video Diffusion Model for Physically Realistic Large-Motion Frame Interpolation", "categories": ["cs.CV"], "comment": null, "summary": "Video frame interpolation (VFI) in scenarios with large motion remains\nchallenging due to motion ambiguity between frames. While event cameras can\ncapture high temporal resolution motion information, existing event-based VFI\nmethods struggle with limited training data and complex motion patterns. In\nthis paper, we introduce Event-Guided Video Diffusion Model (EGVD), a novel\nframework that leverages the powerful priors of pre-trained stable video\ndiffusion models alongside the precise temporal information from event cameras.\nOur approach features a Multi-modal Motion Condition Generator (MMCG) that\neffectively integrates RGB frames and event signals to guide the diffusion\nprocess, producing physically realistic intermediate frames. We employ a\nselective fine-tuning strategy that preserves spatial modeling capabilities\nwhile efficiently incorporating event-guided temporal information. We\nincorporate input-output normalization techniques inspired by recent advances\nin diffusion modeling to enhance training stability across varying noise\nlevels. To improve generalization, we construct a comprehensive dataset\ncombining both real and simulated event data across diverse scenarios.\nExtensive experiments on both real and simulated datasets demonstrate that EGVD\nsignificantly outperforms existing methods in handling large motion and\nchallenging lighting conditions, achieving substantial improvements in\nperceptual quality metrics (27.4% better LPIPS on Prophesee and 24.1% on BSRGB)\nwhile maintaining competitive fidelity measures. Code and datasets available\nat: https://github.com/OpenImagingLab/EGVD."}
{"id": "2503.20201", "pdf": "https://arxiv.org/pdf/2503.20201", "abs": "https://arxiv.org/abs/2503.20201", "authors": ["Salaheddin Alzubi", "Creston Brooks", "Purva Chiniya", "Edoardo Contente", "Chiara von Gerlach", "Lucas Irwin", "Yihan Jiang", "Arda Kaz", "Windsor Nguyen", "Sewoong Oh", "Himanshu Tyagi", "Pramod Viswanath"], "title": "Open Deep Search: Democratizing Search with Open-source Reasoning Agents", "categories": ["cs.LG", "cs.CL", "cs.IR"], "comment": "27 pages, 8 figures, 4 tables", "summary": "We introduce Open Deep Search (ODS) to close the increasing gap between the\nproprietary search AI solutions, such as Perplexity's Sonar Reasoning Pro and\nOpenAI's GPT-4o Search Preview, and their open-source counterparts. The main\ninnovation introduced in ODS is to augment the reasoning capabilities of the\nlatest open-source LLMs with reasoning agents that can judiciously use web\nsearch tools to answer queries. Concretely, ODS consists of two components that\nwork with a base LLM chosen by the user: Open Search Tool and Open Reasoning\nAgent. Open Reasoning Agent interprets the given task and completes it by\norchestrating a sequence of actions that includes calling tools, one of which\nis the Open Search Tool. Open Search Tool is a novel web search tool that\noutperforms proprietary counterparts. Together with powerful open-source\nreasoning LLMs, such as DeepSeek-R1, ODS nearly matches and sometimes surpasses\nthe existing state-of-the-art baselines on two benchmarks: SimpleQA and FRAMES.\nFor example, on the FRAMES evaluation benchmark, ODS improves the best existing\nbaseline of the recently released GPT-4o Search Preview by 9.7% in accuracy.\nODS is a general framework for seamlessly augmenting any LLMs -- for example,\nDeepSeek-R1 that achieves 82.4% on SimpleQA and 30.1% on FRAMES -- with search\nand reasoning capabilities to achieve state-of-the-art performance: 88.3% on\nSimpleQA and 75.3% on FRAMES."}
{"id": "2503.20271", "pdf": "https://arxiv.org/pdf/2503.20271", "abs": "https://arxiv.org/abs/2503.20271", "authors": ["Haoqin Tu", "Weitao Feng", "Hardy Chen", "Hui Liu", "Xianfeng Tang", "Cihang Xie"], "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\nand data."}
{"id": "2503.20228", "pdf": "https://arxiv.org/pdf/2503.20228", "abs": "https://arxiv.org/abs/2503.20228", "authors": ["Xiao Lin", "Manoj Acharya", "Anirban Roy", "Susmit Jha"], "title": "TeleLoRA: Teleporting Model-Specific Alignment Across LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Mitigating Trojans in Large Language Models (LLMs) is one of many tasks where\nalignment data is LLM specific, as different LLMs have different Trojan\ntriggers and trigger behaviors to be removed. In this paper, we introduce\nTeleLoRA (Teleporting Low-Rank Adaptation), a novel framework that synergizes\nmodel-specific alignment data across multiple LLMs to enable zero-shot Trojan\nmitigation on unseen LLMs without alignment data. TeleLoRA learns a unified\ngenerator of LoRA adapter weights by leveraging local activation information\nacross multiple LLMs. This generator is designed to be permutation symmetric to\ngeneralize across models with different architectures and sizes. We optimize\nthe model design for memory efficiency, making it feasible to learn with\nlarge-scale LLMs with minimal computational resources. Experiments on LLM\nTrojan mitigation benchmarks demonstrate that TeleLoRA effectively reduces\nattack success rates while preserving the benign performance of the models."}
{"id": "2503.20282", "pdf": "https://arxiv.org/pdf/2503.20282", "abs": "https://arxiv.org/abs/2503.20282", "authors": ["Kwonyoung Kim", "Jungin Park", "Jin Kim", "Hyeongjun Kwon", "Kwanghoon Sohn"], "title": "Faster Parameter-Efficient Tuning with Token Redundancy Reduction", "categories": ["cs.CV", "cs.AI"], "comment": "CVPR 2025 Camera-ready", "summary": "Parameter-efficient tuning (PET) aims to transfer pre-trained foundation\nmodels to downstream tasks by learning a small number of parameters. Compared\nto traditional fine-tuning, which updates the entire model, PET significantly\nreduces storage and transfer costs for each task regardless of exponentially\nincreasing pre-trained model capacity. However, most PET methods inherit the\ninference latency of their large backbone models and often introduce additional\ncomputational overhead due to additional modules (e.g. adapters), limiting\ntheir practicality for compute-intensive applications. In this paper, we\npropose Faster Parameter-Efficient Tuning (FPET), a novel approach that\nenhances inference speed and training efficiency while maintaining high storage\nefficiency. Specifically, we introduce a plug-and-play token redundancy\nreduction module delicately designed for PET. This module refines tokens from\nthe self-attention layer using an adapter to learn the accurate similarity\nbetween tokens and cuts off the tokens through a fully-differentiable token\nmerging strategy, which uses a straight-through estimator for optimal token\nreduction. Experimental results prove that our FPET achieves faster inference\nand higher memory efficiency than the pre-trained backbone while keeping\ncompetitive performance on par with state-of-the-art PET methods."}
{"id": "2503.20271", "pdf": "https://arxiv.org/pdf/2503.20271", "abs": "https://arxiv.org/abs/2503.20271", "authors": ["Haoqin Tu", "Weitao Feng", "Hardy Chen", "Hui Liu", "Xianfeng Tang", "Cihang Xie"], "title": "ViLBench: A Suite for Vision-Language Process Reward Modeling", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Process-supervised reward models serve as a fine-grained function that\nprovides detailed step-wise feedback to model responses, facilitating effective\nselection of reasoning trajectories for complex tasks. Despite its advantages,\nevaluation on PRMs remains less explored, especially in the multimodal domain.\nTo address this gap, this paper first benchmarks current vision large language\nmodels (VLLMs) as two types of reward models: output reward models (ORMs) and\nprocess reward models (PRMs) on multiple vision-language benchmarks, which\nreveal that neither ORM nor PRM consistently outperforms across all tasks, and\nsuperior VLLMs do not necessarily yield better rewarding performance. To\nfurther advance evaluation, we introduce ViLBench, a vision-language benchmark\ndesigned to require intensive process reward signals. Notably, OpenAI's GPT-4o\nwith Chain-of-Thought (CoT) achieves only 27.3% accuracy, indicating the\nbenchmark's challenge for current VLLMs. Lastly, we preliminarily showcase a\npromising pathway towards bridging the gap between general VLLMs and reward\nmodels -- by collecting 73.6K vision-language process reward data using an\nenhanced tree-search algorithm, our 3B model is able to achieve an average\nimprovement of 3.3% over standard CoT and up to 2.5% compared to its untrained\ncounterpart on ViLBench by selecting OpenAI o1's generations. We release the\nimplementations at https://ucsc-vlaa.github.io/ViLBench with our code, model,\nand data."}
{"id": "2503.20287", "pdf": "https://arxiv.org/pdf/2503.20287", "abs": "https://arxiv.org/abs/2503.20287", "authors": ["Yuhui Wu", "Liyi Chen", "Ruibin Li", "Shihao Wang", "Chenxi Xie", "Lei Zhang"], "title": "InsViE-1M: Effective Instruction-based Video Editing with Elaborate Dataset Construction", "categories": ["cs.CV"], "comment": null, "summary": "Instruction-based video editing allows effective and interactive editing of\nvideos using only instructions without extra inputs such as masks or\nattributes. However, collecting high-quality training triplets (source video,\nedited video, instruction) is a challenging task. Existing datasets mostly\nconsist of low-resolution, short duration, and limited amount of source videos\nwith unsatisfactory editing quality, limiting the performance of trained\nediting models. In this work, we present a high-quality Instruction-based Video\nEditing dataset with 1M triplets, namely InsViE-1M. We first curate\nhigh-resolution and high-quality source videos and images, then design an\neffective editing-filtering pipeline to construct high-quality editing triplets\nfor model training. For a source video, we generate multiple edited samples of\nits first frame with different intensities of classifier-free guidance, which\nare automatically filtered by GPT-4o with carefully crafted guidelines. The\nedited first frame is propagated to subsequent frames to produce the edited\nvideo, followed by another round of filtering for frame quality and motion\nevaluation. We also generate and filter a variety of video editing triplets\nfrom high-quality images. With the InsViE-1M dataset, we propose a multi-stage\nlearning strategy to train our InsViE model, progressively enhancing its\ninstruction following and editing ability. Extensive experiments demonstrate\nthe advantages of our InsViE-1M dataset and the trained model over\nstate-of-the-art works. Codes are available at InsViE."}
{"id": "2503.20290", "pdf": "https://arxiv.org/pdf/2503.20290", "abs": "https://arxiv.org/abs/2503.20290", "authors": ["Siyin Wang", "Wenyi Yu", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Yu Tsao", "Junichi Yamagishi", "Yuxuan Wang", "Chao Zhang"], "title": "QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "23 pages, 16 figures", "summary": "This paper explores a novel perspective to speech quality assessment by\nleveraging natural language descriptions, offering richer, more nuanced\ninsights than traditional numerical scoring methods. Natural language feedback\nprovides instructive recommendations and detailed evaluations, yet existing\ndatasets lack the comprehensive annotations needed for this approach. To bridge\nthis gap, we introduce QualiSpeech, a comprehensive low-level speech quality\nassessment dataset encompassing 11 key aspects and detailed natural language\ncomments that include reasoning and contextual insights. Additionally, we\npropose the QualiSpeech Benchmark to evaluate the low-level speech\nunderstanding capabilities of auditory large language models (LLMs).\nExperimental results demonstrate that finetuned auditory LLMs can reliably\ngenerate detailed descriptions of noise and distortion, effectively identifying\ntheir types and temporal characteristics. The results further highlight the\npotential for incorporating reasoning to enhance the accuracy and reliability\nof quality assessments. The dataset will be released at\nhttps://huggingface.co/datasets/tsinghua-ee/QualiSpeech."}
{"id": "2503.20289", "pdf": "https://arxiv.org/pdf/2503.20289", "abs": "https://arxiv.org/abs/2503.20289", "authors": ["Kaifan Sun", "Bingchen Yang", "Peter Wonka", "Jun Xiao", "Haiyong Jiang"], "title": "RelTriple: Learning Plausible Indoor Layouts by Integrating Relationship Triples into the Diffusion Process", "categories": ["cs.CV"], "comment": null, "summary": "The generation of indoor furniture layouts has significant applications in\naugmented reality, smart homes, and architectural design. Successful furniture\narrangement requires proper physical relationships (e.g., collision avoidance)\nand spacing relationships between furniture and their functional zones to be\nrespected. However, manually defined relationships are almost always incomplete\nand can produce unrealistic layouts. This work instead extracts spacing\nrelationships automatically based on a hierarchical analysis and adopts the\nDelaunay Triangulation to produce important triple relationships. Compared to\npairwise relationship modeling, triple relationships account for interactions\nand space utilization among multiple objects. To this end, we introduce\nRelTriple, a novel approach that enhances furniture distribution by learning\nspacing relationships between objects and regions. We formulate triple\nrelationships as object-to-object (O2O) losses and object-to-region (O2R)\nlosses and integrate them directly into the training process of generative\ndiffusion. Our approach consistently improves over existing state-of-the-art\nmethods in visual results evaluation metrics on unconditional layout\ngeneration, floorplan-conditioned layout generation, and scene rearrangement,\nachieving at least 12% on the introduced spatial relationship metric and\nsuperior spatial coherence and practical usability."}
{"id": "2503.20348", "pdf": "https://arxiv.org/pdf/2503.20348", "abs": "https://arxiv.org/abs/2503.20348", "authors": ["Felix Vogel", "Walid Bousselham", "Anna Kukleva", "Nina Shvetsova", "Hilde Kuehne"], "title": "VideoGEM: Training-free Action Grounding in Videos", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-language foundation models have shown impressive capabilities across\nvarious zero-shot tasks, including training-free localization and grounding,\nprimarily focusing on localizing objects in images. However, leveraging those\ncapabilities to localize actions and events in videos is challenging, as\nactions have less physical outline and are usually described by higher-level\nconcepts. In this work, we propose VideoGEM, the first training-free spatial\naction grounding method based on pretrained image- and video-language\nbackbones. Namely, we adapt the self-self attention formulation of GEM to\nspatial activity grounding. We observe that high-level semantic concepts, such\nas actions, usually emerge in the higher layers of the image- and\nvideo-language models. We, therefore, propose a layer weighting in the\nself-attention path to prioritize higher layers. Additionally, we introduce a\ndynamic weighting method to automatically tune layer weights to capture each\nlayer`s relevance to a specific prompt. Finally, we introduce a prompt\ndecomposition, processing action, verb, and object prompts separately,\nresulting in a better spatial localization of actions. We evaluate the proposed\napproach on three image- and video-language backbones, CLIP, OpenCLIP, and\nViCLIP, and on four video grounding datasets, V-HICO, DALY,\nYouCook-Interactions, and GroundingYouTube, showing that the proposed\ntraining-free approach is able to outperform current trained state-of-the-art\napproaches for spatial video grounding."}
{"id": "2503.20291", "pdf": "https://arxiv.org/pdf/2503.20291", "abs": "https://arxiv.org/abs/2503.20291", "authors": ["Chenwei Zhang", "Anne Condon", "Khanh Dao Duc"], "title": "CryoSAMU: Enhancing 3D Cryo-EM Density Maps of Protein Structures at Intermediate Resolution with Structure-Aware Multimodal U-Nets", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.BM"], "comment": "18 pages, 6 main figures, 2 supplementary figures, 3 main tables, 4\n  supplementary tables", "summary": "Enhancing cryogenic electron microscopy (cryo-EM) 3D density maps at\nintermediate resolution (4-8 {\\AA}) is crucial in protein structure\ndetermination. Recent advances in deep learning have led to the development of\nautomated approaches for enhancing experimental cryo-EM density maps. Yet,\nthese methods are not optimized for intermediate-resolution maps and rely on\nmap density features alone. To address this, we propose CryoSAMU, a novel\nmethod designed to enhance 3D cryo-EM density maps of protein structures using\nstructure-aware multimodal U-Nets and trained on curated\nintermediate-resolution density maps. We comprehensively evaluate CryoSAMU\nacross various metrics and demonstrate its competitive performance compared to\nstate-of-the-art methods. Notably, CryoSAMU achieves significantly faster\nprocessing speed, showing promise for future practical applications. Our code\nis available at https://github.com/chenwei-zhang/CryoSAMU."}
{"id": "2503.20491", "pdf": "https://arxiv.org/pdf/2503.20491", "abs": "https://arxiv.org/abs/2503.20491", "authors": ["Jiale Cheng", "Ruiliang Lyu", "Xiaotao Gu", "Xiao Liu", "Jiazheng Xu", "Yida Lu", "Jiayan Teng", "Zhuoyi Yang", "Yuxiao Dong", "Jie Tang", "Hongning Wang", "Minlie Huang"], "title": "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Video generation models have achieved remarkable progress in text-to-video\ntasks. These models are typically trained on text-video pairs with highly\ndetailed and carefully crafted descriptions, while real-world user inputs\nduring inference are often concise, vague, or poorly structured. This gap makes\nprompt optimization crucial for generating high-quality videos. Current methods\noften rely on large language models (LLMs) to refine prompts through in-context\nlearning, but suffer from several limitations: they may distort user intent,\nomit critical details, or introduce safety risks. Moreover, they optimize\nprompts without considering the impact on the final video quality, which can\nlead to suboptimal results. To address these issues, we introduce VPO, a\nprincipled framework that optimizes prompts based on three core principles:\nharmlessness, accuracy, and helpfulness. The generated prompts faithfully\npreserve user intents and, more importantly, enhance the safety and quality of\ngenerated videos. To achieve this, VPO employs a two-stage optimization\napproach. First, we construct and refine a supervised fine-tuning (SFT) dataset\nbased on principles of safety and alignment. Second, we introduce both\ntext-level and video-level feedback to further optimize the SFT model with\npreference learning. Our extensive experiments demonstrate that VPO\nsignificantly improves safety, alignment, and video quality compared to\nbaseline methods. Moreover, VPO shows strong generalization across video\ngeneration models. Furthermore, we demonstrate that VPO could outperform and be\ncombined with RLHF methods on video generation models, underscoring the\neffectiveness of VPO in aligning video generation models. Our code and data are\npublicly available at https://github.com/thu-coai/VPO."}
{"id": "2503.20294", "pdf": "https://arxiv.org/pdf/2503.20294", "abs": "https://arxiv.org/abs/2503.20294", "authors": ["Xinghao Wang", "Changtao Miao", "Dianmo Sheng", "Tao Gong", "Qi Chu", "Bin Liu", "Nenghai Yu"], "title": "Context-Aware Weakly Supervised Image Manipulation Localization with SAM Refinement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Malicious image manipulation poses societal risks, increasing the importance\nof effective image manipulation detection methods. Recent approaches in image\nmanipulation detection have largely been driven by fully supervised approaches,\nwhich require labor-intensive pixel-level annotations. Thus, it is essential to\nexplore weakly supervised image manipulation localization methods that only\nrequire image-level binary labels for training. However, existing weakly\nsupervised image manipulation methods overlook the importance of edge\ninformation for accurate localization, leading to suboptimal localization\nperformance. To address this, we propose a Context-Aware Boundary Localization\n(CABL) module to aggregate boundary features and learn context-inconsistency\nfor localizing manipulated areas. Furthermore, by leveraging Class Activation\nMapping (CAM) and Segment Anything Model (SAM), we introduce the CAM-Guided SAM\nRefinement (CGSR) module to generate more accurate manipulation localization\nmaps. By integrating two modules, we present a novel weakly supervised\nframework based on a dual-branch Transformer-CNN architecture. Our method\nachieves outstanding localization performance across multiple datasets."}
{"id": "2503.20518", "pdf": "https://arxiv.org/pdf/2503.20518", "abs": "https://arxiv.org/abs/2503.20518", "authors": ["Liza Darwesh", "Jaspreet Singh", "Marin Marian", "Eduard Alexa", "Koen Hindriks", "Kim Baraka"], "title": "Exploring the Effect of Robotic Embodiment and Empathetic Tone of LLMs on Empathy Elicitation", "categories": ["cs.HC", "cs.CL", "cs.RO", "I.2.9, I.2.7, H.5.2"], "comment": "*Liza Darwesh, Jaspreet Singh, Marin Marian, and Eduard Alexa\n  contributed equally to this work.*", "summary": "This study investigates the elicitation of empathy toward a third party\nthrough interaction with social agents. Participants engaged with either a\nphysical robot or a voice-enabled chatbot, both driven by a large language\nmodel (LLM) programmed to exhibit either an empathetic tone or remain neutral.\nThe interaction is focused on a fictional character, Katie Banks, who is in a\nchallenging situation and in need of financial donations. The willingness to\nhelp Katie, measured by the number of hours participants were willing to\nvolunteer, along with their perceptions of the agent, were assessed for 60\nparticipants. Results indicate that neither robotic embodiment nor empathetic\ntone significantly influenced participants' willingness to volunteer. While the\nLLM effectively simulated human empathy, fostering genuine empathetic responses\nin participants proved challenging."}
{"id": "2503.20297", "pdf": "https://arxiv.org/pdf/2503.20297", "abs": "https://arxiv.org/abs/2503.20297", "authors": ["Yuhan Wang", "Suzhi Bi", "Ying-Jun Angela Zhang", "Xiaojun Yuan"], "title": "Traversing Distortion-Perception Tradeoff using a Single Score-Based Generative Model", "categories": ["cs.CV"], "comment": "Accepted by IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2025", "summary": "The distortion-perception (DP) tradeoff reveals a fundamental conflict\nbetween distortion metrics (e.g., MSE and PSNR) and perceptual quality. Recent\nresearch has increasingly concentrated on evaluating denoising algorithms\nwithin the DP framework. However, existing algorithms either prioritize\nperceptual quality by sacrificing acceptable distortion, or focus on minimizing\nMSE for faithful restoration. When the goal shifts or noisy measurements vary,\nadapting to different points on the DP plane needs retraining or even\nre-designing the model. Inspired by recent advances in solving inverse problems\nusing score-based generative models, we explore the potential of flexibly and\noptimally traversing DP tradeoffs using a single pre-trained score-based model.\nSpecifically, we introduce a variance-scaled reverse diffusion process and\ntheoretically characterize the marginal distribution. We then prove that the\nproposed sample process is an optimal solution to the DP tradeoff for\nconditional Gaussian distribution. Experimental results on two-dimensional and\nimage datasets illustrate that a single score network can effectively and\nflexibly traverse the DP tradeoff for general denoising problems."}
{"id": "2503.20576", "pdf": "https://arxiv.org/pdf/2503.20576", "abs": "https://arxiv.org/abs/2503.20576", "authors": ["Siyuan Guo", "Huiwu Liu", "Xiaolong Chen", "Yuming Xie", "Liang Zhang", "Tao Han", "Hechang Chen", "Yi Chang", "Jun Wang"], "title": "Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": null, "summary": "In this work, we explore the potential of large language models (LLMs) for\ngenerating functional test scripts, which necessitates understanding the\ndynamically evolving code structure of the target software. To achieve this, we\npropose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e.,\nretrieve, reuse, revise, and retain), which maintains and leverages a case bank\nof test intent descriptions and corresponding test scripts to facilitate LLMs\nfor test script generation. To improve user experience further, we introduce\nRe4, an optimization method for the CBR system, comprising reranking-based\nretrieval finetuning and reinforced reuse finetuning. Specifically, we first\nidentify positive examples with high semantic and script similarity, providing\nreliable pseudo-labels for finetuning the retriever model without costly\nlabeling. Then, we apply supervised finetuning, followed by a reinforcement\nlearning finetuning stage, to align LLMs with our production scenarios,\nensuring the faithful reuse of retrieved cases. Extensive experimental results\non two product development units from Huawei Datacom demonstrate the\nsuperiority of the proposed CBR+Re4. Notably, we also show that the proposed\nRe4 method can help alleviate the repetitive generation issues with LLMs."}
{"id": "2503.20301", "pdf": "https://arxiv.org/pdf/2503.20301", "abs": "https://arxiv.org/abs/2503.20301", "authors": ["Jianyang Zhang", "Qianli Luo", "Guowu Yang", "Wenjing Yang", "Weide Liu", "Guosheng Lin", "Fengmao Lv"], "title": "Attribute-formed Class-specific Concept Space: Endowing Language Bottleneck Model with Better Interpretability and Scalability", "categories": ["cs.CV"], "comment": "This paper has been accepted to CVPR 2025", "summary": "Language Bottleneck Models (LBMs) are proposed to achieve interpretable image\nrecognition by classifying images based on textual concept bottlenecks.\nHowever, current LBMs simply list all concepts together as the bottleneck\nlayer, leading to the spurious cue inference problem and cannot generalized to\nunseen classes. To address these limitations, we propose the Attribute-formed\nLanguage Bottleneck Model (ALBM). ALBM organizes concepts in the\nattribute-formed class-specific space, where concepts are descriptions of\nspecific attributes for specific classes. In this way, ALBM can avoid the\nspurious cue inference problem by classifying solely based on the essential\nconcepts of each class. In addition, the cross-class unified attribute set also\nensures that the concept spaces of different classes have strong correlations,\nas a result, the learned concept classifier can be easily generalized to unseen\nclasses. Moreover, to further improve interpretability, we propose Visual\nAttribute Prompt Learning (VAPL) to extract visual features on fine-grained\nattributes. Furthermore, to avoid labor-intensive concept annotation, we\npropose the Description, Summary, and Supplement (DSS) strategy to\nautomatically generate high-quality concept sets with a complete and precise\nattribute. Extensive experiments on 9 widely used few-shot benchmarks\ndemonstrate the interpretability, transferability, and performance of our\napproach. The code and collected concept sets are available at\nhttps://github.com/tiggers23/ALBM."}
{"id": "2503.20666", "pdf": "https://arxiv.org/pdf/2503.20666", "abs": "https://arxiv.org/abs/2503.20666", "authors": ["Huimin Xu", "Seungjun Yi", "Terence Lim", "Jiawei Xu", "Andrew Well", "Carlos Mery", "Aidong Zhang", "Yuji Zhang", "Heng Ji", "Keshav Pingali", "Yan Leng", "Ying Ding"], "title": "TAMA: A Human-AI Collaborative Thematic Analysis Framework Using Multi-Agent LLMs for Clinical Interviews", "categories": ["cs.HC", "cs.CL"], "comment": "Submitted to the American Medical Informatics Association (AMIA) 2025\n  Annual Symposium, 10 pages", "summary": "Thematic analysis (TA) is a widely used qualitative approach for uncovering\nlatent meanings in unstructured text data. TA provides valuable insights in\nhealthcare but is resource-intensive. Large Language Models (LLMs) have been\nintroduced to perform TA, yet their applications in healthcare remain\nunexplored. Here, we propose TAMA: A Human-AI Collaborative Thematic Analysis\nframework using Multi-Agent LLMs for clinical interviews. We leverage the\nscalability and coherence of multi-agent systems through structured\nconversations between agents and coordinate the expertise of cardiac experts in\nTA. Using interview transcripts from parents of children with Anomalous Aortic\nOrigin of a Coronary Artery (AAOCA), a rare congenital heart disease, we\ndemonstrate that TAMA outperforms existing LLM-assisted TA approaches,\nachieving higher thematic hit rate, coverage, and distinctiveness. TAMA\ndemonstrates strong potential for automated TA in clinical settings by\nleveraging multi-agent LLM systems with human-in-the-loop integration by\nenhancing quality while significantly reducing manual workload."}
{"id": "2503.20309", "pdf": "https://arxiv.org/pdf/2503.20309", "abs": "https://arxiv.org/abs/2503.20309", "authors": ["Zitian Wang", "Yue Liao", "Kang Rong", "Fengyun Rao", "Yibo Yang", "Si Liu"], "title": "Instruction-Oriented Preference Alignment for Enhancing Multi-Modal Comprehension Capability of MLLMs", "categories": ["cs.CV"], "comment": "Technical report", "summary": "Preference alignment has emerged as an effective strategy to enhance the\nperformance of Multimodal Large Language Models (MLLMs) following supervised\nfine-tuning. While existing preference alignment methods predominantly target\nhallucination factors, they overlook the factors essential for multi-modal\ncomprehension capabilities, often narrowing their improvements on hallucination\nmitigation. To bridge this gap, we propose Instruction-oriented Preference\nAlignment (IPA), a scalable framework designed to automatically construct\nalignment preferences grounded in instruction fulfillment efficacy. Our method\ninvolves an automated preference construction coupled with a dedicated\nverification process that identifies instruction-oriented factors, avoiding\nsignificant variability in response representations. Additionally, IPA\nincorporates a progressive preference collection pipeline, further recalling\nchallenging samples through model self-evolution and reference-guided\nrefinement. Experiments conducted on Qwen2VL-7B demonstrate IPA's effectiveness\nacross multiple benchmarks, including hallucination evaluation, visual question\nanswering, and text understanding tasks, highlighting its capability to enhance\ngeneral comprehension."}
{"id": "2503.20680", "pdf": "https://arxiv.org/pdf/2503.20680", "abs": "https://arxiv.org/abs/2503.20680", "authors": ["Han Wang", "Yongjie Ye", "Bingru Li", "Yuxiang Nie", "Jinghui Lu", "Jingqun Tang", "Yanjie Wang", "Can Huang"], "title": "Vision as LoRA", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce Vision as LoRA (VoRA), a novel paradigm for transforming an LLM\ninto an MLLM. Unlike prevalent MLLM architectures that rely on external vision\nmodules for vision encoding, VoRA internalizes visual capabilities by\nintegrating vision-specific LoRA layers directly into the LLM. This design\nallows the added parameters to be seamlessly merged into the LLM during\ninference, eliminating structural complexity and minimizing computational\noverhead. Moreover, inheriting the LLM's ability of handling flexible context,\nVoRA can process inputs at arbitrary resolutions.\n  To further strengthen VoRA's visual capabilities, we introduce a block-wise\ndistillation method that transfers visual priors from a pre-trained ViT into\nthe LoRA layers, effectively accelerating training by injecting visual\nknowledge. Additionally, we apply bi-directional attention masks to better\ncapture the context information of an image. We successfully demonstrate that\nwith additional pre-training data, VoRA can perform comparably with\nconventional encode-based MLLMs. All training data, codes, and model weights\nwill be released at https://github.com/Hon-Wong/VoRA."}
{"id": "2503.20310", "pdf": "https://arxiv.org/pdf/2503.20310", "abs": "https://arxiv.org/abs/2503.20310", "authors": ["Tao Wu", "Tie Luo"], "title": "Enabling Heterogeneous Adversarial Transferability via Feature Permutation Attacks", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": "PAKDD 2025. Main Track", "summary": "Adversarial attacks in black-box settings are highly practical, with\ntransfer-based attacks being the most effective at generating adversarial\nexamples (AEs) that transfer from surrogate models to unseen target models.\nHowever, their performance significantly degrades when transferring across\nheterogeneous architectures -- such as CNNs, MLPs, and Vision Transformers\n(ViTs) -- due to fundamental architectural differences. To address this, we\npropose Feature Permutation Attack (FPA), a zero-FLOP, parameter-free method\nthat enhances adversarial transferability across diverse architectures. FPA\nintroduces a novel feature permutation (FP) operation, which rearranges pixel\nvalues in selected feature maps to simulate long-range dependencies,\neffectively making CNNs behave more like ViTs and MLPs. This enhances feature\ndiversity and improves transferability both across heterogeneous architectures\nand within homogeneous CNNs. Extensive evaluations on 14 state-of-the-art\narchitectures show that FPA achieves maximum absolute gains in attack success\nrates of 7.68% on CNNs, 14.57% on ViTs, and 14.48% on MLPs, outperforming\nexisting black-box attacks. Additionally, FPA is highly generalizable and can\nseamlessly integrate with other transfer-based attacks to further boost their\nperformance. Our findings establish FPA as a robust, efficient, and\ncomputationally lightweight strategy for enhancing adversarial transferability\nacross heterogeneous architectures."}
{"id": "2503.20783", "pdf": "https://arxiv.org/pdf/2503.20783", "abs": "https://arxiv.org/abs/2503.20783", "authors": ["Zichen Liu", "Changyu Chen", "Wenjun Li", "Penghui Qi", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Understanding R1-Zero-Like Training: A Critical Perspective", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can\ndirectly enhance the reasoning capabilities of LLMs without supervised\nfine-tuning. In this work, we critically examine R1-Zero-like training by\nanalyzing its two core components: base models and RL. We investigate a wide\nrange of base models, including DeepSeek-V3-Base, to understand how pretraining\ncharacteristics influence RL performance. Our analysis reveals that\nDeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models\ndemonstrate strong reasoning capabilities even without prompt templates,\nsuggesting potential pretraining biases. Additionally, we identify an\noptimization bias in Group Relative Policy Optimization (GRPO), which\nartificially increases response length (especially for incorrect outputs)\nduring training. To address this, we introduce Dr. GRPO, an unbiased\noptimization method that improves token efficiency while maintaining reasoning\nperformance. Leveraging these insights, we present a minimalist R1-Zero recipe\nthat achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a\nnew state-of-the-art. Our code is available at\nhttps://github.com/sail-sg/understand-r1-zero."}
{"id": "2503.20314", "pdf": "https://arxiv.org/pdf/2503.20314", "abs": "https://arxiv.org/abs/2503.20314", "authors": ["WanTeam", ":", "Ang Wang", "Baole Ai", "Bin Wen", "Chaojie Mao", "Chen-Wei Xie", "Di Chen", "Feiwu Yu", "Haiming Zhao", "Jianxiao Yang", "Jianyuan Zeng", "Jiayu Wang", "Jingfeng Zhang", "Jingren Zhou", "Jinkai Wang", "Jixuan Chen", "Kai Zhu", "Kang Zhao", "Keyu Yan", "Lianghua Huang", "Mengyang Feng", "Ningyi Zhang", "Pandeng Li", "Pingyu Wu", "Ruihang Chu", "Ruili Feng", "Shiwei Zhang", "Siyang Sun", "Tao Fang", "Tianxing Wang", "Tianyi Gui", "Tingyu Weng", "Tong Shen", "Wei Lin", "Wei Wang", "Wei Wang", "Wenmeng Zhou", "Wente Wang", "Wenting Shen", "Wenyuan Yu", "Xianzhong Shi", "Xiaoming Huang", "Xin Xu", "Yan Kou", "Yangyu Lv", "Yifei Li", "Yijing Liu", "Yiming Wang", "Yingya Zhang", "Yitong Huang", "Yong Li", "You Wu", "Yu Liu", "Yulin Pan", "Yun Zheng", "Yuntao Hong", "Yupeng Shi", "Yutong Feng", "Zeyinzi Jiang", "Zhen Han", "Zhi-Fan Wu", "Ziyu Liu"], "title": "Wan: Open and Advanced Large-Scale Video Generative Models", "categories": ["cs.CV"], "comment": "60 pages, 33 figures", "summary": "This report presents Wan, a comprehensive and open suite of video foundation\nmodels designed to push the boundaries of video generation. Built upon the\nmainstream diffusion transformer paradigm, Wan achieves significant\nadvancements in generative capabilities through a series of innovations,\nincluding our novel VAE, scalable pre-training strategies, large-scale data\ncuration, and automated evaluation metrics. These contributions collectively\nenhance the model's performance and versatility. Specifically, Wan is\ncharacterized by four key features: Leading Performance: The 14B model of Wan,\ntrained on a vast dataset comprising billions of images and videos,\ndemonstrates the scaling laws of video generation with respect to both data and\nmodel size. It consistently outperforms the existing open-source models as well\nas state-of-the-art commercial solutions across multiple internal and external\nbenchmarks, demonstrating a clear and significant performance superiority.\nComprehensiveness: Wan offers two capable models, i.e., 1.3B and 14B\nparameters, for efficiency and effectiveness respectively. It also covers\nmultiple downstream applications, including image-to-video, instruction-guided\nvideo editing, and personal video generation, encompassing up to eight tasks.\nConsumer-Grade Efficiency: The 1.3B model demonstrates exceptional resource\nefficiency, requiring only 8.19 GB VRAM, making it compatible with a wide range\nof consumer-grade GPUs. Openness: We open-source the entire series of Wan,\nincluding source code and all models, with the goal of fostering the growth of\nthe video generation community. This openness seeks to significantly expand the\ncreative possibilities of video production in the industry and provide academia\nwith high-quality video foundation models. All the code and models are\navailable at https://github.com/Wan-Video/Wan2.1."}
{"id": "2503.20315", "pdf": "https://arxiv.org/pdf/2503.20315", "abs": "https://arxiv.org/abs/2503.20315", "authors": ["Hanwen Liang", "Xian Zhong", "Wenxuan Liu", "Yajing Zheng", "Wenxin Huang", "Zhaofei Yu", "Tiejun Huang"], "title": "SpikeDerain: Unveiling Clear Videos from Rainy Sequences Using Color Spike Streams", "categories": ["cs.CV"], "comment": null, "summary": "Restoring clear frames from rainy videos presents a significant challenge due\nto the rapid motion of rain streaks. Traditional frame-based visual sensors,\nwhich capture scene content synchronously, struggle to capture the fast-moving\ndetails of rain accurately. In recent years, neuromorphic sensors have\nintroduced a new paradigm for dynamic scene perception, offering microsecond\ntemporal resolution and high dynamic range. However, existing multimodal\nmethods that fuse event streams with RGB images face difficulties in handling\nthe complex spatiotemporal interference of raindrops in real scenes, primarily\ndue to hardware synchronization errors and computational redundancy. In this\npaper, we propose a Color Spike Stream Deraining Network (SpikeDerain), capable\nof reconstructing spike streams of dynamic scenes and accurately removing rain\nstreaks. To address the challenges of data scarcity in real continuous rainfall\nscenes, we design a physically interpretable rain streak synthesis model that\ngenerates parameterized continuous rain patterns based on arbitrary background\nimages. Experimental results demonstrate that the network, trained with this\nsynthetic data, remains highly robust even under extreme rainfall conditions.\nThese findings highlight the effectiveness and robustness of our method across\nvarying rainfall levels and datasets, setting new standards for video deraining\ntasks. The code will be released soon."}
{"id": "2503.20318", "pdf": "https://arxiv.org/pdf/2503.20318", "abs": "https://arxiv.org/abs/2503.20318", "authors": ["Qian Wang", "Aleksandar Cvejic", "Abdelrahman Eldesokey", "Peter Wonka"], "title": "EditCLIP: Representation Learning for Image Editing", "categories": ["cs.CV"], "comment": "Project page: https://qianwangx.github.io/EditCLIP/", "summary": "We introduce EditCLIP, a novel representation-learning approach for image\nediting. Our method learns a unified representation of edits by jointly\nencoding an input image and its edited counterpart, effectively capturing their\ntransformation. To evaluate its effectiveness, we employ EditCLIP to solve two\ntasks: exemplar-based image editing and automated edit evaluation. In\nexemplar-based image editing, we replace text-based instructions in\nInstructPix2Pix with EditCLIP embeddings computed from a reference exemplar\nimage pair. Experiments demonstrate that our approach outperforms\nstate-of-the-art methods while being more efficient and versatile. For\nautomated evaluation, EditCLIP assesses image edits by measuring the similarity\nbetween the EditCLIP embedding of a given image pair and either a textual\nediting instruction or the EditCLIP embedding of another reference image pair.\nExperiments show that EditCLIP aligns more closely with human judgments than\nexisting CLIP-based metrics, providing a reliable measure of edit quality and\nstructural preservation."}
{"id": "2503.20321", "pdf": "https://arxiv.org/pdf/2503.20321", "abs": "https://arxiv.org/abs/2503.20321", "authors": ["Jaeah Lee", "Changwoon Choi", "Young Min Kim", "Jaesik Park"], "title": "Recovering Dynamic 3D Sketches from Videos", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Understanding 3D motion from videos presents inherent challenges due to the\ndiverse types of movement, ranging from rigid and deformable objects to\narticulated structures. To overcome this, we propose Liv3Stroke, a novel\napproach for abstracting objects in motion with deformable 3D strokes. The\ndetailed movements of an object may be represented by unstructured motion\nvectors or a set of motion primitives using a pre-defined articulation from a\ntemplate model. Just as a free-hand sketch can intuitively visualize scenes or\nintentions with a sparse set of lines, we utilize a set of parametric 3D curves\nto capture a set of spatially smooth motion elements for general objects with\nunknown structures. We first extract noisy, 3D point cloud motion guidance from\nvideo frames using semantic features, and our approach deforms a set of curves\nto abstract essential motion features as a set of explicit 3D representations.\nSuch abstraction enables an understanding of prominent components of motions\nwhile maintaining robustness to environmental factors. Our approach allows\ndirect analysis of 3D object movements from video, tackling the uncertainty\nthat typically occurs when translating real-world motion into recorded footage.\nThe project page is accessible via: https://jaeah.me/liv3stroke_web}"}
{"id": "2503.20322", "pdf": "https://arxiv.org/pdf/2503.20322", "abs": "https://arxiv.org/abs/2503.20322", "authors": ["Hao Ai", "Kunyi Wang", "Zezhou Wang", "Hao Lu", "Jin Tian", "Yaxin Luo", "Peng Xing", "Jen-Yuan Huang", "Huaxia Li", "Gen luo"], "title": "Dynamic Pyramid Network for Efficient Multimodal Large Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated impressive\nperformance in various vision-language (VL) tasks, but their expensive\ncomputations still limit the real-world application. To address this issue,\nrecent efforts aim to compress the visual features to save the computational\ncosts of MLLMs. However, direct visual compression methods, e.g. efficient\nprojectors, inevitably destroy the visual semantics in MLLM, especially in\ndifficult samples. To overcome this shortcoming, we propose a novel dynamic\npyramid network (DPN) for efficient MLLMs. Specifically, DPN formulates MLLM as\na hierarchical structure where visual features are gradually compressed with\nincreasing depth. In this case, even with a high compression ratio,\nfine-grained visual information can still be perceived in shallow layers. To\nmaximize the benefit of DPN, we further propose an innovative Dynamic Pooling\nExperts (DPE) that can dynamically choose the optimal visual compression rate\naccording to input features. With this design, harder samples will be assigned\nlarger computations, thus preserving the model performance. To validate our\napproach, we conduct extensive experiments on two popular MLLMs and ten\nbenchmarks. Experimental results show that DPN can save up to 56% average FLOPs\non LLaVA while further achieving +0.74% performance gains. Besides, the\ngeneralization ability of DPN is also validated on the existing high-resolution\nMLLM called LLaVA-HR. Our source codes are anonymously released at\nhttps://github.com/aihao2000/DPN-LLaVA."}
{"id": "2503.20337", "pdf": "https://arxiv.org/pdf/2503.20337", "abs": "https://arxiv.org/abs/2503.20337", "authors": ["Wei Long", "Xingyu Zhou", "Leheng Zhang", "Shuhang Gu"], "title": "Progressive Focused Transformer for Single Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Transformer-based methods have achieved remarkable results in image\nsuper-resolution tasks because they can capture non-local dependencies in\nlow-quality input images. However, this feature-intensive modeling approach is\ncomputationally expensive because it calculates the similarities between\nnumerous features that are irrelevant to the query features when obtaining\nattention weights. These unnecessary similarity calculations not only degrade\nthe reconstruction performance but also introduce significant computational\noverhead. How to accurately identify the features that are important to the\ncurrent query features and avoid similarity calculations between irrelevant\nfeatures remains an urgent problem. To address this issue, we propose a novel\nand effective Progressive Focused Transformer (PFT) that links all isolated\nattention maps in the network through Progressive Focused Attention (PFA) to\nfocus attention on the most important tokens. PFA not only enables the network\nto capture more critical similar features, but also significantly reduces the\ncomputational cost of the overall network by filtering out irrelevant features\nbefore calculating similarities. Extensive experiments demonstrate the\neffectiveness of the proposed method, achieving state-of-the-art performance on\nvarious single image super-resolution benchmarks."}
{"id": "2503.20348", "pdf": "https://arxiv.org/pdf/2503.20348", "abs": "https://arxiv.org/abs/2503.20348", "authors": ["Felix Vogel", "Walid Bousselham", "Anna Kukleva", "Nina Shvetsova", "Hilde Kuehne"], "title": "VideoGEM: Training-free Action Grounding in Videos", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-language foundation models have shown impressive capabilities across\nvarious zero-shot tasks, including training-free localization and grounding,\nprimarily focusing on localizing objects in images. However, leveraging those\ncapabilities to localize actions and events in videos is challenging, as\nactions have less physical outline and are usually described by higher-level\nconcepts. In this work, we propose VideoGEM, the first training-free spatial\naction grounding method based on pretrained image- and video-language\nbackbones. Namely, we adapt the self-self attention formulation of GEM to\nspatial activity grounding. We observe that high-level semantic concepts, such\nas actions, usually emerge in the higher layers of the image- and\nvideo-language models. We, therefore, propose a layer weighting in the\nself-attention path to prioritize higher layers. Additionally, we introduce a\ndynamic weighting method to automatically tune layer weights to capture each\nlayer`s relevance to a specific prompt. Finally, we introduce a prompt\ndecomposition, processing action, verb, and object prompts separately,\nresulting in a better spatial localization of actions. We evaluate the proposed\napproach on three image- and video-language backbones, CLIP, OpenCLIP, and\nViCLIP, and on four video grounding datasets, V-HICO, DALY,\nYouCook-Interactions, and GroundingYouTube, showing that the proposed\ntraining-free approach is able to outperform current trained state-of-the-art\napproaches for spatial video grounding."}
{"id": "2503.20349", "pdf": "https://arxiv.org/pdf/2503.20349", "abs": "https://arxiv.org/abs/2503.20349", "authors": ["Weiyi You", "Mingyang Zhang", "Leheng Zhang", "Kexuan Shi", "Xingyu Zhou", "Shuhang Gu"], "title": "Consistency Trajectory Matching for One-Step Generative Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Current diffusion-based super-resolution (SR) approaches achieve commendable\nperformance at the cost of high inference overhead. Therefore, distillation\ntechniques are utilized to accelerate the multi-step teacher model into\none-step student model. Nevertheless, these methods significantly raise\ntraining costs and constrain the performance of the student model by the\nteacher model. To overcome these tough challenges, we propose Consistency\nTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategy\nthat is able to generate photo-realistic SR results in one step. Concretely, we\nfirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)\ntrajectory to establish a deterministic mapping from low-resolution (LR) images\nwith noise to high-resolution (HR) images. Then we apply the Consistency\nTraining (CT) strategy to directly learn the mapping in one step, eliminating\nthe necessity of pre-trained diffusion model. To further enhance the\nperformance and better leverage the ground-truth during the training process,\nwe aim to align the distribution of SR results more closely with that of the\nnatural images. To this end, we propose to minimize the discrepancy between\ntheir respective PF-ODE trajectories from the LR image distribution by our\nmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting in\nimproved realism of our recovered HR images. Comprehensive experimental results\ndemonstrate that the proposed methods can attain comparable or even superior\ncapabilities on both synthetic and real datasets while maintaining minimal\ninference latency."}
{"id": "2503.20354", "pdf": "https://arxiv.org/pdf/2503.20354", "abs": "https://arxiv.org/abs/2503.20354", "authors": ["Ke Ma", "Jiaqi Tang", "Bin Guo", "Fan Dang", "Sicong Liu", "Zhui Zhu", "Lei Wu", "Cheng Fang", "Ying-Cong Chen", "Zhiwen Yu", "Yunhao Liu"], "title": "SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic Activation Sparsity", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "Despite the growing integration of deep models into mobile terminals, the\naccuracy of these models declines significantly due to various deployment\ninterferences. Test-time adaptation (TTA) has emerged to improve the\nperformance of deep models by adapting them to unlabeled target data online.\nYet, the significant memory cost, particularly in resource-constrained\nterminals, impedes the effective deployment of most backward-propagation-based\nTTA methods. To tackle memory constraints, we introduce SURGEON, a method that\nsubstantially reduces memory cost while preserving comparable accuracy\nimprovements during fully test-time adaptation (FTTA) without relying on\nspecific network architectures or modifications to the original training\nprocedure. Specifically, we propose a novel dynamic activation sparsity\nstrategy that directly prunes activations at layer-specific dynamic ratios\nduring adaptation, allowing for flexible control of learning ability and memory\ncost in a data-sensitive manner. Among this, two metrics, Gradient Importance\nand Layer Activation Memory, are considered to determine the layer-wise pruning\nratios, reflecting accuracy contribution and memory efficiency, respectively.\nExperimentally, our method surpasses the baselines by not only reducing memory\nusage but also achieving superior accuracy, delivering SOTA performance across\ndiverse datasets, architectures, and tasks."}
{"id": "2503.20362", "pdf": "https://arxiv.org/pdf/2503.20362", "abs": "https://arxiv.org/abs/2503.20362", "authors": ["Joao Pereira", "Vasco Lopes", "David Semedo", "Joao Neves"], "title": "Self-ReS: Self-Reflection in Large Vision-Language Models for Long Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) demonstrate remarkable performance in\nshort-video tasks such as video question answering, but struggle in long-video\nunderstanding. The linear frame sampling strategy, conventionally used by\nLVLMs, fails to account for the non-linear distribution of key events in video\ndata, often introducing redundant or irrelevant information in longer contexts\nwhile risking the omission of critical events in shorter ones. To address this,\nwe propose SelfReS, a non-linear spatiotemporal self-reflective sampling method\nthat dynamically selects key video fragments based on user prompts. Unlike\nprior approaches, SelfReS leverages the inherently sparse attention maps of\nLVLMs to define reflection tokens, enabling relevance-aware token selection\nwithout requiring additional training or external modules. Experiments\ndemonstrate that SelfReS can be seamlessly integrated into strong base LVLMs,\nimproving long-video task accuracy and achieving up to 46% faster inference\nspeed within the same GPU memory budget."}
{"id": "2503.20368", "pdf": "https://arxiv.org/pdf/2503.20368", "abs": "https://arxiv.org/abs/2503.20368", "authors": ["Hongda Liu", "Longguang Wang", "Weijun Guan", "Ye Zhang", "Yulan Guo"], "title": "Pluggable Style Representation Learning for Multi-Style Transfer", "categories": ["cs.CV"], "comment": "18 pages, 13 figures, 2 tables", "summary": "Due to the high diversity of image styles, the scalability to various styles\nplays a critical role in real-world applications. To accommodate a large amount\nof styles, previous multi-style transfer approaches rely on enlarging the model\nsize while arbitrary-style transfer methods utilize heavy backbones. However,\nthe additional computational cost introduced by more model parameters hinders\nthese methods to be deployed on resource-limited devices. To address this\nchallenge, in this paper, we develop a style transfer framework by decoupling\nthe style modeling and transferring. Specifically, for style modeling, we\npropose a style representation learning scheme to encode the style information\ninto a compact representation. Then, for style transferring, we develop a\nstyle-aware multi-style transfer network (SaMST) to adapt to diverse styles\nusing pluggable style representations. In this way, our framework is able to\naccommodate diverse image styles in the learned style representations without\nintroducing additional overhead during inference, thereby maintaining\nefficiency. Experiments show that our style representation can extract accurate\nstyle information. Moreover, qualitative and quantitative results demonstrate\nthat our method achieves state-of-the-art performance in terms of both accuracy\nand efficiency. The codes are available in\nhttps://github.com/The-Learning-And-Vision-Atelier-LAVA/SaMST."}
{"id": "2503.20382", "pdf": "https://arxiv.org/pdf/2503.20382", "abs": "https://arxiv.org/abs/2503.20382", "authors": ["Chunshan Li", "Rong Wang", "Xiaofei Yang", "Dianhui Chu"], "title": "RSRWKV: A Linear-Complexity 2D Attention Mechanism for Efficient Remote Sensing Vision Task", "categories": ["cs.CV"], "comment": null, "summary": "High-resolution remote sensing analysis faces challenges in global context\nmodeling due to scene complexity and scale diversity. While CNNs excel at local\nfeature extraction via parameter sharing, their fixed receptive fields\nfundamentally restrict long-range dependency modeling. Vision Transformers\n(ViTs) effectively capture global semantic relationships through self-attention\nmechanisms but suffer from quadratic computational complexity relative to image\nresolution, creating critical efficiency bottlenecks for high-resolution\nimagery. The RWKV model's linear-complexity sequence modeling achieves\nbreakthroughs in NLP but exhibits anisotropic limitations in vision tasks due\nto its 1D scanning mechanism. To address these challenges, we propose RSRWKV,\nfeaturing a novel 2D-WKV scanning mechanism that bridges sequential processing\nand 2D spatial reasoning while maintaining linear complexity. This enables\nisotropic context aggregation across multiple directions. The MVC-Shift module\nenhances multi-scale receptive field coverage, while the ECA module strengthens\ncross-channel feature interaction and semantic saliency modeling. Experimental\nresults demonstrate RSRWKV's superior performance over CNN and Transformer\nbaselines in classification, detection, and segmentation tasks on NWPU\nRESISC45, VHR-10.v2, and GLH-Water datasets, offering a scalable solution for\nhigh-resolution remote sensing analysis."}
{"id": "2503.20418", "pdf": "https://arxiv.org/pdf/2503.20418", "abs": "https://arxiv.org/abs/2503.20418", "authors": ["Ji Woo Hong", "Tri Ton", "Trung X. Pham", "Gwanhyeong Koo", "Sunjae Yoon", "Chang D. Yoo"], "title": "ITA-MDT: Image-Timestep-Adaptive Masked Diffusion Transformer Framework for Image-Based Virtual Try-On", "categories": ["cs.CV"], "comment": "CVPR 2025, Project Page: https://jiwoohong93.github.io/ita-mdt/", "summary": "This paper introduces ITA-MDT, the Image-Timestep-Adaptive Masked Diffusion\nTransformer Framework for Image-Based Virtual Try-On (IVTON), designed to\novercome the limitations of previous approaches by leveraging the Masked\nDiffusion Transformer (MDT) for improved handling of both global garment\ncontext and fine-grained details. The IVTON task involves seamlessly\nsuperimposing a garment from one image onto a person in another, creating a\nrealistic depiction of the person wearing the specified garment. Unlike\nconventional diffusion-based virtual try-on models that depend on large\npre-trained U-Net architectures, ITA-MDT leverages a lightweight, scalable\ntransformer-based denoising diffusion model with a mask latent modeling scheme,\nachieving competitive results while reducing computational overhead. A key\ncomponent of ITA-MDT is the Image-Timestep Adaptive Feature Aggregator (ITAFA),\na dynamic feature aggregator that combines all of the features from the image\nencoder into a unified feature of the same size, guided by diffusion timestep\nand garment image complexity. This enables adaptive weighting of features,\nallowing the model to emphasize either global information or fine-grained\ndetails based on the requirements of the denoising stage. Additionally, the\nSalient Region Extractor (SRE) module is presented to identify complex region\nof the garment to provide high-resolution local information to the denoising\nmodel as an additional condition alongside the global information of the full\ngarment image. This targeted conditioning strategy enhances detail preservation\nof fine details in highly salient garment regions, optimizing computational\nresources by avoiding unnecessarily processing entire garment image.\nComparative evaluations confirms that ITA-MDT improves efficiency while\nmaintaining strong performance, reaching state-of-the-art results in several\nmetrics."}
{"id": "2503.20419", "pdf": "https://arxiv.org/pdf/2503.20419", "abs": "https://arxiv.org/abs/2503.20419", "authors": ["Andreas Gilson", "Peter Pietrzyk", "Chiara Paglia", "Annika Killer", "Fabian Keil", "Lukas Meyer", "Dominikus Kittemann", "Patrick Noack", "Oliver Scholz"], "title": "Cherry Yield Forecast: Harvest Prediction for Individual Sweet Cherry Trees", "categories": ["cs.CV"], "comment": null, "summary": "This paper is part of a publication series from the For5G project that has\nthe goal of creating digital twins of sweet cherry trees. At the beginning a\nbrief overview of the revious work in this project is provided. Afterwards the\nfocus shifts to a crucial problem in the fruit farming domain: the difficulty\nof making reliable yield predictions early in the season. Following three Satin\nsweet cherry trees along the year 2023 enabled the collection of accurate\nground truth data about the development of cherries from dormancy until\nharvest. The methodology used to collect this data is presented, along with its\nvaluation and visualization. The predictive power of counting objects at all\nrelevant vegetative stages of the fruit development cycle in cherry trees with\nregards to yield predictions is investigated. It is found that all investigated\nfruit states are suitable for yield predictions based on linear regression.\nConceptionally, there is a trade-off between earliness and external events with\nthe potential to invalidate the prediction. Considering this, two optimal\ntimepoints are suggested that are opening cluster stage before the start of the\nflowering and the early fruit stage right after the second fruit drop. However,\nboth timepoints are challenging to solve with automated procedures based on\nimage data. Counting developing cherries based on images is exceptionally\ndifficult due to the small fruit size and their tendency to be occluded by\nleaves. It was not possible to obtain satisfying results relying on a\nstate-of-the-art fruit-counting method. Counting the elements within a bursting\nbud is also challenging, even when using high resolution cameras. It is\nconcluded that accurate yield prediction for sweet cherry trees is possible\nwhen objects are manually counted and that automated features extraction with\nsimilar accuracy remains an open problem yet to be solved."}
{"id": "2503.20428", "pdf": "https://arxiv.org/pdf/2503.20428", "abs": "https://arxiv.org/abs/2503.20428", "authors": ["F. Xavier Gaya-Morey", "Cristina Manresa-Yee", "Célia Martinie", "Jose M. Buades-Rubio"], "title": "Evaluating Facial Expression Recognition Datasets for Deep Learning: A Benchmark Study with Novel Similarity Metrics", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This study investigates the key characteristics and suitability of widely\nused Facial Expression Recognition (FER) datasets for training deep learning\nmodels. In the field of affective computing, FER is essential for interpreting\nhuman emotions, yet the performance of FER systems is highly contingent on the\nquality and diversity of the underlying datasets. To address this issue, we\ncompiled and analyzed 24 FER datasets, including those targeting specific age\ngroups such as children, adults, and the elderly, and processed them through a\ncomprehensive normalization pipeline. In addition, we enriched the datasets\nwith automatic annotations for age and gender, enabling a more nuanced\nevaluation of their demographic properties. To further assess dataset efficacy,\nwe introduce three novel metricsLocal, Global, and Paired Similarity, which\nquantitatively measure dataset difficulty, generalization capability, and\ncross-dataset transferability. Benchmark experiments using state-of-the-art\nneural networks reveal that large-scale, automatically collected datasets\n(e.g., AffectNet, FER2013) tend to generalize better, despite issues with\nlabeling noise and demographic biases, whereas controlled datasets offer higher\nannotation quality but limited variability. Our findings provide actionable\nrecommendations for dataset selection and design, advancing the development of\nmore robust, fair, and effective FER systems."}
{"id": "2503.20429", "pdf": "https://arxiv.org/pdf/2503.20429", "abs": "https://arxiv.org/abs/2503.20429", "authors": ["Guilherme Fernandes", "Vasco Ramos", "Regev Cohen", "Idan Szpektor", "João Magalhães"], "title": "Latent Beam Diffusion Models for Decoding Image Sequences", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models excel at generating high-quality images from text\nprompts, they struggle with visual consistency in image sequences. Existing\nmethods generate each image independently, leading to disjointed narratives - a\nchallenge further exacerbated in non-linear storytelling, where scenes must\nconnect beyond adjacent frames. We introduce a novel beam search strategy for\nlatent space exploration, enabling conditional generation of full image\nsequences with beam search decoding. Unlike prior approaches that use fixed\nlatent priors, our method dynamically searches for an optimal sequence of\nlatent representations, ensuring coherent visual transitions. To address beam\nsearch's quadratic complexity, we integrate a cross-attention mechanism that\nefficiently scores search paths and enables pruning, prioritizing alignment\nwith both textual prompts and visual context. Human evaluations confirm that\nour approach outperforms baseline methods, producing full sequences with\nsuperior coherence, visual continuity, and textual alignment. By bridging\nadvances in search optimization and latent space refinement, this work sets a\nnew standard for structured image sequence generation."}
{"id": "2503.20436", "pdf": "https://arxiv.org/pdf/2503.20436", "abs": "https://arxiv.org/abs/2503.20436", "authors": ["Muxin Pu", "Mei Kuan Lim", "Chun Yong Chong"], "title": "Siformer: Feature-isolated Transformer for Efficient Skeleton-based Sign Language Recognition", "categories": ["cs.CV"], "comment": "10 pages, ACM Multimedia", "summary": "Sign language recognition (SLR) refers to interpreting sign language glosses\nfrom given videos automatically. This research area presents a complex\nchallenge in computer vision because of the rapid and intricate movements\ninherent in sign languages, which encompass hand gestures, body postures, and\neven facial expressions. Recently, skeleton-based action recognition has\nattracted increasing attention due to its ability to handle variations in\nsubjects and backgrounds independently. However, current skeleton-based SLR\nmethods exhibit three limitations: 1) they often neglect the importance of\nrealistic hand poses, where most studies train SLR models on non-realistic\nskeletal representations; 2) they tend to assume complete data availability in\nboth training or inference phases, and capture intricate relationships among\ndifferent body parts collectively; 3) these methods treat all sign glosses\nuniformly, failing to account for differences in complexity levels regarding\nskeletal representations. To enhance the realism of hand skeletal\nrepresentations, we present a kinematic hand pose rectification method for\nenforcing constraints. Mitigating the impact of missing data, we propose a\nfeature-isolated mechanism to focus on capturing local spatial-temporal\ncontext. This method captures the context concurrently and independently from\nindividual features, thus enhancing the robustness of the SLR model.\nAdditionally, to adapt to varying complexity levels of sign glosses, we develop\nan input-adaptive inference approach to optimise computational efficiency and\naccuracy. Experimental results demonstrate the effectiveness of our approach,\nas evidenced by achieving a new state-of-the-art (SOTA) performance on WLASL100\nand LSA64. For WLASL100, we achieve a top-1 accuracy of 86.50\\%, marking a\nrelative improvement of 2.39% over the previous SOTA. For LSA64, we achieve a\ntop-1 accuracy of 99.84%."}
{"id": "2503.20472", "pdf": "https://arxiv.org/pdf/2503.20472", "abs": "https://arxiv.org/abs/2503.20472", "authors": ["Yucheng Suo", "Fan Ma", "Linchao Zhu", "Tianyi Wang", "Fengyun Rao", "Yi Yang"], "title": "From Trial to Triumph: Advancing Long Video Understanding via Visual Context Sample Scaling and Self-reward Alignment", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-modal Large language models (MLLMs) show remarkable ability in video\nunderstanding. Nevertheless, understanding long videos remains challenging as\nthe models can only process a finite number of frames in a single inference,\npotentially omitting crucial visual information. To address the challenge, we\npropose generating multiple predictions through visual context sampling,\nfollowed by a scoring mechanism to select the final prediction. Specifically,\nwe devise a bin-wise sampling strategy that enables MLLMs to generate diverse\nanswers based on various combinations of keyframes, thereby enriching the\nvisual context. To determine the final prediction from the sampled answers, we\nemploy a self-reward by linearly combining three scores: (1) a frequency score\nindicating the prevalence of each option, (2) a marginal confidence score\nreflecting the inter-intra sample certainty of MLLM predictions, and (3) a\nreasoning score for different question types, including clue-guided answering\nfor global questions and temporal self-refocusing for local questions. The\nfrequency score ensures robustness through majority correctness, the\nconfidence-aligned score reflects prediction certainty, and the typed-reasoning\nscore addresses cases with sparse key visual information using tailored\nstrategies. Experiments show that this approach covers the correct answer for a\nhigh percentage of long video questions, on seven datasets show that our method\nimproves the performance of three MLLMs."}
{"id": "2503.20483", "pdf": "https://arxiv.org/pdf/2503.20483", "abs": "https://arxiv.org/abs/2503.20483", "authors": ["Yingdong Shi", "Changming Li", "Yifan Wang", "Yongxiang Zhao", "Anqi Pang", "Sibei Yang", "Jingyi Yu", "Kan Ren"], "title": "Dissecting and Mitigating Diffusion Bias via Mechanistic Interpretability", "categories": ["cs.CV", "cs.LG"], "comment": "CVPR 2025; Project Page:\n  https://foundation-model-research.github.io/difflens", "summary": "Diffusion models have demonstrated impressive capabilities in synthesizing\ndiverse content. However, despite their high-quality outputs, these models\noften perpetuate social biases, including those related to gender and race.\nThese biases can potentially contribute to harmful real-world consequences,\nreinforcing stereotypes and exacerbating inequalities in various social\ncontexts. While existing research on diffusion bias mitigation has\npredominantly focused on guiding content generation, it often neglects the\nintrinsic mechanisms within diffusion models that causally drive biased\noutputs. In this paper, we investigate the internal processes of diffusion\nmodels, identifying specific decision-making mechanisms, termed bias features,\nembedded within the model architecture. By directly manipulating these\nfeatures, our method precisely isolates and adjusts the elements responsible\nfor bias generation, permitting granular control over the bias levels in the\ngenerated content. Through experiments on both unconditional and conditional\ndiffusion models across various social bias attributes, we demonstrate our\nmethod's efficacy in managing generation distribution while preserving image\nquality. We also dissect the discovered model mechanism, revealing different\nintrinsic features controlling fine-grained aspects of generation, boosting\nfurther research on mechanistic interpretability of diffusion models."}
{"id": "2503.20484", "pdf": "https://arxiv.org/pdf/2503.20484", "abs": "https://arxiv.org/abs/2503.20484", "authors": ["Qi Si", "Bo Wang", "Zhao Zhang"], "title": "Contrastive Learning Guided Latent Diffusion Model for Image-to-Image Translation", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 13 figures", "summary": "The diffusion model has demonstrated superior performance in synthesizing\ndiverse and high-quality images for text-guided image translation. However,\nthere remains room for improvement in both the formulation of text prompts and\nthe preservation of reference image content. First, variations in target text\nprompts can significantly influence the quality of the generated images, and it\nis often challenging for users to craft an optimal prompt that fully captures\nthe content of the input image. Second, while existing models can introduce\ndesired modifications to specific regions of the reference image, they\nfrequently induce unintended alterations in areas that should remain unchanged.\nTo address these challenges, we propose pix2pix-zeroCon, a zero-shot\ndiffusion-based method that eliminates the need for additional training by\nleveraging patch-wise contrastive loss. Specifically, we automatically\ndetermine the editing direction in the text embedding space based on the\nreference image and target prompts. Furthermore, to ensure precise content and\nstructural preservation in the edited image, we introduce cross-attention\nguiding loss and patch-wise contrastive loss between the generated and original\nimage embeddings within a pre-trained diffusion model. Notably, our approach\nrequires no additional training and operates directly on a pre-trained\ntext-to-image diffusion model. Extensive experiments demonstrate that our\nmethod surpasses existing models in image-to-image translation, achieving\nenhanced fidelity and controllability."}
{"id": "2503.20491", "pdf": "https://arxiv.org/pdf/2503.20491", "abs": "https://arxiv.org/abs/2503.20491", "authors": ["Jiale Cheng", "Ruiliang Lyu", "Xiaotao Gu", "Xiao Liu", "Jiazheng Xu", "Yida Lu", "Jiayan Teng", "Zhuoyi Yang", "Yuxiao Dong", "Jie Tang", "Hongning Wang", "Minlie Huang"], "title": "VPO: Aligning Text-to-Video Generation Models with Prompt Optimization", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Video generation models have achieved remarkable progress in text-to-video\ntasks. These models are typically trained on text-video pairs with highly\ndetailed and carefully crafted descriptions, while real-world user inputs\nduring inference are often concise, vague, or poorly structured. This gap makes\nprompt optimization crucial for generating high-quality videos. Current methods\noften rely on large language models (LLMs) to refine prompts through in-context\nlearning, but suffer from several limitations: they may distort user intent,\nomit critical details, or introduce safety risks. Moreover, they optimize\nprompts without considering the impact on the final video quality, which can\nlead to suboptimal results. To address these issues, we introduce VPO, a\nprincipled framework that optimizes prompts based on three core principles:\nharmlessness, accuracy, and helpfulness. The generated prompts faithfully\npreserve user intents and, more importantly, enhance the safety and quality of\ngenerated videos. To achieve this, VPO employs a two-stage optimization\napproach. First, we construct and refine a supervised fine-tuning (SFT) dataset\nbased on principles of safety and alignment. Second, we introduce both\ntext-level and video-level feedback to further optimize the SFT model with\npreference learning. Our extensive experiments demonstrate that VPO\nsignificantly improves safety, alignment, and video quality compared to\nbaseline methods. Moreover, VPO shows strong generalization across video\ngeneration models. Furthermore, we demonstrate that VPO could outperform and be\ncombined with RLHF methods on video generation models, underscoring the\neffectiveness of VPO in aligning video generation models. Our code and data are\npublicly available at https://github.com/thu-coai/VPO."}
{"id": "2503.20492", "pdf": "https://arxiv.org/pdf/2503.20492", "abs": "https://arxiv.org/abs/2503.20492", "authors": ["Fanhu Zeng", "Zhen Cheng", "Fei Zhu", "Xu-Yao Zhang"], "title": "Towards Efficient and General-Purpose Few-Shot Misclassification Detection for Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "preprint", "summary": "Reliable prediction by classifiers is crucial for their deployment in high\nsecurity and dynamically changing situations. However, modern neural networks\noften exhibit overconfidence for misclassified predictions, highlighting the\nneed for confidence estimation to detect errors. Despite the achievements\nobtained by existing methods on small-scale datasets, they all require training\nfrom scratch and there are no efficient and effective misclassification\ndetection (MisD) methods, hindering practical application towards large-scale\nand ever-changing datasets. In this paper, we pave the way to exploit vision\nlanguage model (VLM) leveraging text information to establish an efficient and\ngeneral-purpose misclassification detection framework. By harnessing the power\nof VLM, we construct FSMisD, a Few-Shot prompt learning framework for MisD to\nrefrain from training from scratch and therefore improve tuning efficiency. To\nenhance misclassification detection ability, we use adaptive pseudo sample\ngeneration and a novel negative loss to mitigate the issue of overconfidence by\npushing category prompts away from pseudo features. We conduct comprehensive\nexperiments with prompt learning methods and validate the generalization\nability across various datasets with domain shift. Significant and consistent\nimprovement demonstrates the effectiveness, efficiency and generalizability of\nour approach."}
{"id": "2503.20502", "pdf": "https://arxiv.org/pdf/2503.20502", "abs": "https://arxiv.org/abs/2503.20502", "authors": ["Yiwei Ma", "Guohai Xu", "Xiaoshuai Sun", "Jiayi Ji", "Jie Lou", "Debing Zhang", "Rongrong Ji"], "title": "MLLM-Selector: Necessity and Diversity-driven High-Value Data Selection for Enhanced Visual Instruction Tuning", "categories": ["cs.CV"], "comment": "Tech Report", "summary": "Visual instruction tuning (VIT) has emerged as a crucial technique for\nenabling multi-modal large language models (MLLMs) to follow user instructions\nadeptly. Yet, a significant gap persists in understanding the attributes of\nhigh-quality instruction tuning data and frameworks for its automated\nselection. To address this, we introduce MLLM-Selector, an automated approach\nthat identifies valuable data for VIT by weighing necessity and diversity. Our\nprocess starts by randomly sampling a subset from the VIT data pool to\nfine-tune a pretrained model, thus creating a seed model with an initial\nability to follow instructions. Then, leveraging the seed model, we calculate\nnecessity scores for each sample in the VIT data pool to identify samples\npivotal for enhancing model performance. Our findings underscore the importance\nof mixing necessity and diversity in data choice, leading to the creation of\nMLLM-Selector, our methodology that fuses necessity scoring with strategic\nsampling for superior data refinement. Empirical results indicate that within\nidentical experimental conditions, MLLM-Selector surpasses LLaVA-1.5 in some\nbenchmarks with less than 1% of the data and consistently exceeds performance\nacross all validated benchmarks when using less than 50%."}
{"id": "2503.20504", "pdf": "https://arxiv.org/pdf/2503.20504", "abs": "https://arxiv.org/abs/2503.20504", "authors": ["Zehui Liao", "Shishuai Hu", "Ke Zou", "Huazhu Fu", "Liangli Zhen", "Yong Xia"], "title": "Vision-Amplified Semantic Entropy for Hallucination Detection in Medical Visual Question Answering", "categories": ["cs.CV"], "comment": "11 pages, 2 figures", "summary": "Multimodal large language models (MLLMs) have demonstrated significant\npotential in medical Visual Question Answering (VQA). Yet, they remain prone to\nhallucinations-incorrect responses that contradict input images, posing\nsubstantial risks in clinical decision-making. Detecting these hallucinations\nis essential for establishing trust in MLLMs among clinicians and patients,\nthereby enabling their real-world adoption. Current hallucination detection\nmethods, especially semantic entropy (SE), have demonstrated promising\nhallucination detection capacity for LLMs. However, adapting SE to medical\nMLLMs by incorporating visual perturbations presents a dilemma. Weak\nperturbations preserve image content and ensure clinical validity, but may be\noverlooked by medical MLLMs, which tend to over rely on language priors. In\ncontrast, strong perturbations can distort essential diagnostic features,\ncompromising clinical interpretation. To address this issue, we propose Vision\nAmplified Semantic Entropy (VASE), which incorporates weak image\ntransformations and amplifies the impact of visual input, to improve\nhallucination detection in medical VQA. We first estimate the semantic\npredictive distribution under weak visual transformations to preserve clinical\nvalidity, and then amplify visual influence by contrasting this distribution\nwith that derived from a distorted image. The entropy of the resulting\ndistribution is estimated as VASE. Experiments on two medical open-ended VQA\ndatasets demonstrate that VASE consistently outperforms existing hallucination\ndetection methods."}
{"id": "2503.20516", "pdf": "https://arxiv.org/pdf/2503.20516", "abs": "https://arxiv.org/abs/2503.20516", "authors": ["Mahya Nikouei", "Bita Baroutian", "Shahabedin Nabavi", "Fateme Taraghi", "Atefe Aghaei", "Ayoob Sajedi", "Mohsen Ebrahimi Moghaddam"], "title": "Small Object Detection: A Comprehensive Survey on Challenges, Techniques and Real-World Applications", "categories": ["cs.CV"], "comment": null, "summary": "Small object detection (SOD) is a critical yet challenging task in computer\nvision, with applications like spanning surveillance, autonomous systems,\nmedical imaging, and remote sensing. Unlike larger objects, small objects\ncontain limited spatial and contextual information, making accurate detection\ndifficult. Challenges such as low resolution, occlusion, background\ninterference, and class imbalance further complicate the problem. This survey\nprovides a comprehensive review of recent advancements in SOD using deep\nlearning, focusing on articles published in Q1 journals during 2024-2025. We\nanalyzed challenges, state-of-the-art techniques, datasets, evaluation metrics,\nand real-world applications. Recent advancements in deep learning have\nintroduced innovative solutions, including multi-scale feature extraction,\nSuper-Resolution (SR) techniques, attention mechanisms, and transformer-based\narchitectures. Additionally, improvements in data augmentation, synthetic data\ngeneration, and transfer learning have addressed data scarcity and domain\nadaptation issues. Furthermore, emerging trends such as lightweight neural\nnetworks, knowledge distillation (KD), and self-supervised learning offer\npromising directions for improving detection efficiency, particularly in\nresource-constrained environments like Unmanned Aerial Vehicles (UAV)-based\nsurveillance and edge computing. We also review widely used datasets, along\nwith standard evaluation metrics such as mean Average Precision (mAP) and\nsize-specific AP scores. The survey highlights real-world applications,\nincluding traffic monitoring, maritime surveillance, industrial defect\ndetection, and precision agriculture. Finally, we discuss open research\nchallenges and future directions, emphasizing the need for robust domain\nadaptation techniques, better feature fusion strategies, and real-time\nperformance optimization."}
{"id": "2503.20519", "pdf": "https://arxiv.org/pdf/2503.20519", "abs": "https://arxiv.org/abs/2503.20519", "authors": ["Jinnan Chen", "Lingting Zhu", "Zeyu Hu", "Shengju Qian", "Yugang Chen", "Xin Wang", "Gim Hee Lee"], "title": "MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D Generation", "categories": ["cs.CV"], "comment": "Aceepted to CVPR 2025", "summary": "Recent advances in auto-regressive transformers have revolutionized\ngenerative modeling across different domains, from language processing to\nvisual generation, demonstrating remarkable capabilities. However, applying\nthese advances to 3D generation presents three key challenges: the unordered\nnature of 3D data conflicts with sequential next-token prediction paradigm,\nconventional vector quantization approaches incur substantial compression loss\nwhen applied to 3D meshes, and the lack of efficient scaling strategies for\nhigher resolution latent prediction. To address these challenges, we introduce\nMAR-3D, which integrates a pyramid variational autoencoder with a cascaded\nmasked auto-regressive transformer (Cascaded MAR) for progressive latent\nupscaling in the continuous space. Our architecture employs random masking\nduring training and auto-regressive denoising in random order during inference,\nnaturally accommodating the unordered property of 3D latent tokens.\nAdditionally, we propose a cascaded training strategy with condition\naugmentation that enables efficiently up-scale the latent token resolution with\nfast convergence. Extensive experiments demonstrate that MAR-3D not only\nachieves superior performance and generalization capabilities compared to\nexisting methods but also exhibits enhanced scaling capabilities compared to\njoint distribution modeling approaches (e.g., diffusion transformers)."}
{"id": "2503.20523", "pdf": "https://arxiv.org/pdf/2503.20523", "abs": "https://arxiv.org/abs/2503.20523", "authors": ["Lloyd Russell", "Anthony Hu", "Lorenzo Bertoni", "George Fedoseev", "Jamie Shotton", "Elahe Arani", "Gianluca Corrado"], "title": "GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Technical Report", "summary": "Generative models offer a scalable and flexible paradigm for simulating\ncomplex environments, yet current approaches fall short in addressing the\ndomain-specific requirements of autonomous driving - such as multi-agent\ninteractions, fine-grained control, and multi-camera consistency. We introduce\nGAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies\nthese capabilities within a single generative framework. GAIA-2 supports\ncontrollable video generation conditioned on a rich set of structured inputs:\nego-vehicle dynamics, agent configurations, environmental factors, and road\nsemantics. It generates high-resolution, spatiotemporally consistent\nmulti-camera videos across geographically diverse driving environments (UK, US,\nGermany). The model integrates both structured conditioning and external latent\nembeddings (e.g., from a proprietary driving model) to facilitate flexible and\nsemantically grounded scene synthesis. Through this integration, GAIA-2 enables\nscalable simulation of both common and rare driving scenarios, advancing the\nuse of generative world models as a core tool in the development of autonomous\nsystems. Videos are available at https://wayve.ai/thinking/gaia-2."}
{"id": "2503.20537", "pdf": "https://arxiv.org/pdf/2503.20537", "abs": "https://arxiv.org/abs/2503.20537", "authors": ["Ziying Zhang", "Xiang Gao", "Zhixin Wang", "Qiang hu", "Xiaoyun Zhang"], "title": "TD-BFR: Truncated Diffusion Model for Efficient Blind Face Restoration", "categories": ["cs.CV"], "comment": "Accepted by ICME 2025", "summary": "Diffusion-based methodologies have shown significant potential in blind face\nrestoration (BFR), leveraging their robust generative capabilities. However,\nthey are often criticized for two significant problems: 1) slow training and\ninference speed, and 2) inadequate recovery of fine-grained facial details. To\naddress these problems, we propose a novel Truncated Diffusion model for\nefficient Blind Face Restoration (TD-BFR), a three-stage paradigm tailored for\nthe progressive resolution of degraded images. Specifically, TD-BFR utilizes an\ninnovative truncated sampling method, starting from low-quality (LQ) images at\nlow resolution to enhance sampling speed, and then introduces an adaptive\ndegradation removal module to handle unknown degradations and connect the\ngeneration processes across different resolutions. Additionally, we further\nadapt the priors of pre-trained diffusion models to recover rich facial\ndetails. Our method efficiently restores high-quality images in a\ncoarse-to-fine manner and experimental results demonstrate that TD-BFR is, on\naverage, \\textbf{4.75$\\times$} faster than current state-of-the-art\ndiffusion-based BFR methods while maintaining competitive quality."}
{"id": "2503.20540", "pdf": "https://arxiv.org/pdf/2503.20540", "abs": "https://arxiv.org/abs/2503.20540", "authors": ["Dingchen Yang", "Bowen Cao", "Anran Zhang", "Weibo Gu", "Winston Hu", "Guang Chen"], "title": "Beyond Intermediate States: Explaining Visual Redundancy through Language", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modal Large Langue Models (MLLMs) often process thousands of visual\ntokens, which consume a significant portion of the context window and impose a\nsubstantial computational burden. Prior work has empirically explored visual\ntoken pruning methods based on MLLMs' intermediate states (e.g., attention\nscores). However, they have limitations in precisely defining visual redundancy\ndue to their inability to capture the influence of visual tokens on MLLMs'\nvisual understanding (i.e., the predicted probabilities for textual token\ncandidates). To address this issue, we manipulate the visual input and\ninvestigate variations in the textual output from both token-centric and\ncontext-centric perspectives, achieving intuitive and comprehensive analysis.\nExperimental results reveal that visual tokens with low ViT-[cls] association\nand low text-to-image attention scores can contain recognizable information and\nsignificantly contribute to images' overall information. To develop a more\nreliable method for identifying and pruning redundant visual tokens, we\nintegrate these two perspectives and introduce a context-independent condition\nto identify redundant prototypes from training images, which probes the\nredundancy of each visual token during inference. Extensive experiments on\nsingle-image, multi-image and video comprehension tasks demonstrate the\neffectiveness of our method, notably achieving 90% to 110% of the performance\nwhile pruning 80% to 90% of visual tokens."}
{"id": "2503.20563", "pdf": "https://arxiv.org/pdf/2503.20563", "abs": "https://arxiv.org/abs/2503.20563", "authors": ["Carlos Gomes", "Benedikt Blumenstiel", "Joao Lucas de Sousa Almeida", "Pedro Henrique de Oliveira", "Paolo Fraccaro", "Francesc Marti Escofet", "Daniela Szwarcman", "Naomi Simumba", "Romeo Kienzler", "Bianca Zadrozny"], "title": "TerraTorch: The Geospatial Foundation Models Toolkit", "categories": ["cs.CV", "cs.LG"], "comment": "IGARSS 2025", "summary": "TerraTorch is a fine-tuning and benchmarking toolkit for Geospatial\nFoundation Models built on PyTorch Lightning and tailored for satellite,\nweather, and climate data. It integrates domain-specific data modules,\npre-defined tasks, and a modular model factory that pairs any backbone with\ndiverse decoder heads. These components allow researchers and practitioners to\nfine-tune supported models in a no-code fashion by simply editing a training\nconfiguration. By consolidating best practices for model development and\nincorporating the automated hyperparameter optimization extension Iterate,\nTerraTorch reduces the expertise and time required to fine-tune or benchmark\nmodels on new Earth Observation use cases. Furthermore, TerraTorch directly\nintegrates with GEO-Bench, allowing for systematic and reproducible\nbenchmarking of Geospatial Foundation Models. TerraTorch is open sourced under\nApache 2.0, available at https://github.com/IBM/terratorch, and can be\ninstalled via pip install terratorch."}
{"id": "2503.20612", "pdf": "https://arxiv.org/pdf/2503.20612", "abs": "https://arxiv.org/abs/2503.20612", "authors": ["Hao Fu", "Hanbin Zhao", "Jiahua Dong", "Chao Zhang", "Hui Qian"], "title": "IAP: Improving Continual Learning of Vision-Language Models via Instance-Aware Prompting", "categories": ["cs.CV"], "comment": "Code can be found at https://github.com/FerdinandZJU/IAP", "summary": "Recent pre-trained vision-language models (PT-VLMs) often face a Multi-Domain\nClass-Incremental Learning (MCIL) scenario in practice, where several classes\nand domains of multi-modal tasks are incrementally arrived. Without access to\npreviously learned tasks and unseen tasks, memory-constrained MCIL suffers from\nforward and backward forgetting. To alleviate the above challenges,\nparameter-efficient fine-tuning techniques (PEFT), such as prompt tuning, are\nemployed to adapt the PT-VLM to the diverse incrementally learned tasks. To\nachieve effective new task adaptation, existing methods only consider the\neffect of PEFT strategy selection, but neglect the influence of PEFT parameter\nsetting (e.g., prompting). In this paper, we tackle the challenge of optimizing\nprompt designs for diverse tasks in MCIL and propose an Instance-Aware\nPrompting (IAP) framework. Specifically, our Instance-Aware Gated Prompting\n(IA-GP) module enhances adaptation to new tasks while mitigating forgetting by\ndynamically assigning prompts across transformer layers at the instance level.\nOur Instance-Aware Class-Distribution-Driven Prompting (IA-CDDP) improves the\ntask adaptation process by determining an accurate task-label-related\nconfidence score for each instance. Experimental evaluations across 11\ndatasets, using three performance metrics, demonstrate the effectiveness of our\nproposed method. Code can be found at https://github.com/FerdinandZJU/IAP."}
{"id": "2503.20644", "pdf": "https://arxiv.org/pdf/2503.20644", "abs": "https://arxiv.org/abs/2503.20644", "authors": ["Jiepeng Wang", "Zhaoqing Wang", "Hao Pan", "Yuan Liu", "Dongdong Yu", "Changhu Wang", "Wenping Wang"], "title": "MMGen: Unified Multi-modal Image Generation and Understanding in One Go", "categories": ["cs.CV"], "comment": "Our project page: https://jiepengwang.github.io/MMGen/", "summary": "A unified diffusion framework for multi-modal generation and understanding\nhas the transformative potential to achieve seamless and controllable image\ndiffusion and other cross-modal tasks. In this paper, we introduce MMGen, a\nunified framework that integrates multiple generative tasks into a single\ndiffusion model. This includes: (1) multi-modal category-conditioned\ngeneration, where multi-modal outputs are generated simultaneously through a\nsingle inference process, given category information; (2) multi-modal visual\nunderstanding, which accurately predicts depth, surface normals, and\nsegmentation maps from RGB images; and (3) multi-modal conditioned generation,\nwhich produces corresponding RGB images based on specific modality conditions\nand other aligned modalities. Our approach develops a novel diffusion\ntransformer that flexibly supports multi-modal output, along with a simple\nmodality-decoupling strategy to unify various tasks. Extensive experiments and\napplications demonstrate the effectiveness and superiority of MMGen across\ndiverse tasks and conditions, highlighting its potential for applications that\nrequire simultaneous generation and understanding."}
{"id": "2503.20652", "pdf": "https://arxiv.org/pdf/2503.20652", "abs": "https://arxiv.org/abs/2503.20652", "authors": ["Theo Di Piazza", "Carole Lazarus", "Olivier Nempont", "Loic Boussel"], "title": "Imitating Radiological Scrolling: A Global-Local Attention Model for 3D Chest CT Volumes Multi-Label Anomaly Classification", "categories": ["cs.CV"], "comment": "13 pages, 4 figures, under review for MIDL 2025", "summary": "The rapid increase in the number of Computed Tomography (CT) scan\nexaminations has created an urgent need for automated tools, such as organ\nsegmentation, anomaly classification, and report generation, to assist\nradiologists with their growing workload. Multi-label classification of\nThree-Dimensional (3D) CT scans is a challenging task due to the volumetric\nnature of the data and the variety of anomalies to be detected. Existing deep\nlearning methods based on Convolutional Neural Networks (CNNs) struggle to\ncapture long-range dependencies effectively, while Vision Transformers require\nextensive pre-training, posing challenges for practical use. Additionally,\nthese existing methods do not explicitly model the radiologist's navigational\nbehavior while scrolling through CT scan slices, which requires both global\ncontext understanding and local detail awareness. In this study, we present\nCT-Scroll, a novel global-local attention model specifically designed to\nemulate the scrolling behavior of radiologists during the analysis of 3D CT\nscans. Our approach is evaluated on two public datasets, demonstrating its\nefficacy through comprehensive experiments and an ablation study that\nhighlights the contribution of each model component."}
{"id": "2503.20654", "pdf": "https://arxiv.org/pdf/2503.20654", "abs": "https://arxiv.org/abs/2503.20654", "authors": ["Xiangwen Zhang", "Qian Zhang", "Longfei Han", "Qiang Qu", "Xiaoming Chen"], "title": "AccidentSim: Generating Physically Realistic Vehicle Collision Videos from Real-World Accident Reports", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Collecting real-world vehicle accident videos for autonomous driving research\nis challenging due to their rarity and complexity. While existing driving video\ngeneration methods may produce visually realistic videos, they often fail to\ndeliver physically realistic simulations because they lack the capability to\ngenerate accurate post-collision trajectories. In this paper, we introduce\nAccidentSim, a novel framework that generates physically realistic vehicle\ncollision videos by extracting and utilizing the physical clues and contextual\ninformation available in real-world vehicle accident reports. Specifically,\nAccidentSim leverages a reliable physical simulator to replicate post-collision\nvehicle trajectories from the physical and contextual information in the\naccident reports and to build a vehicle collision trajectory dataset. This\ndataset is then used to fine-tune a language model, enabling it to respond to\nuser prompts and predict physically consistent post-collision trajectories\nacross various driving scenarios based on user descriptions. Finally, we employ\nNeural Radiance Fields (NeRF) to render high-quality backgrounds, merging them\nwith the foreground vehicles that exhibit physically realistic trajectories to\ngenerate vehicle collision videos. Experimental results demonstrate that the\nvideos produced by AccidentSim excel in both visual and physical authenticity."}
{"id": "2503.20662", "pdf": "https://arxiv.org/pdf/2503.20662", "abs": "https://arxiv.org/abs/2503.20662", "authors": ["Sadaf Khademi", "Mehran Shabanpour", "Reza Taleei", "Anastasia Oikonomou", "Arash Mohammadi"], "title": "AutoRad-Lung: A Radiomic-Guided Prompting Autoregressive Vision-Language Model for Lung Nodule Malignancy Prediction", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Lung cancer remains one of the leading causes of cancer-related mortality\nworldwide. A crucial challenge for early diagnosis is differentiating uncertain\ncases with similar visual characteristics and closely annotation scores. In\nclinical practice, radiologists rely on quantitative, hand-crafted Radiomic\nfeatures extracted from Computed Tomography (CT) images, while recent research\nhas primarily focused on deep learning solutions. More recently,\nVision-Language Models (VLMs), particularly Contrastive Language-Image\nPre-Training (CLIP)-based models, have gained attention for their ability to\nintegrate textual knowledge into lung cancer diagnosis. While CLIP-Lung models\nhave shown promising results, we identified the following potential\nlimitations: (a) dependence on radiologists' annotated attributes, which are\ninherently subjective and error-prone, (b) use of textual information only\nduring training, limiting direct applicability at inference, and (c)\nConvolutional-based vision encoder with randomly initialized weights, which\ndisregards prior knowledge. To address these limitations, we introduce\nAutoRad-Lung, which couples an autoregressively pre-trained VLM, with prompts\ngenerated from hand-crafted Radiomics. AutoRad-Lung uses the vision encoder of\nthe Large-Scale Autoregressive Image Model (AIMv2), pre-trained using a\nmulti-modal autoregressive objective. Given that lung tumors are typically\nsmall, irregularly shaped, and visually similar to healthy tissue, AutoRad-Lung\noffers significant advantages over its CLIP-based counterparts by capturing\npixel-level differences. Additionally, we introduce conditional context\noptimization, which dynamically generates context-specific prompts based on\ninput Radiomics, improving cross-modal alignment."}
{"id": "2503.20663", "pdf": "https://arxiv.org/pdf/2503.20663", "abs": "https://arxiv.org/abs/2503.20663", "authors": ["Mingze Sun", "Shiwei Mao", "Keyi Chen", "Yurun Chen", "Shunlin Lu", "Jingbo Wang", "Junting Dong", "Ruqi Huang"], "title": "ARMO: Autoregressive Rigging for Multi-Category Objects", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in large-scale generative models have significantly\nimproved the quality and diversity of 3D shape generation. However, most\nexisting methods focus primarily on generating static 3D models, overlooking\nthe potentially dynamic nature of certain shapes, such as humanoids, animals,\nand insects. To address this gap, we focus on rigging, a fundamental task in\nanimation that establishes skeletal structures and skinning for 3D models. In\nthis paper, we introduce OmniRig, the first large-scale rigging dataset,\ncomprising 79,499 meshes with detailed skeleton and skinning information.\nUnlike traditional benchmarks that rely on predefined standard poses (e.g.,\nA-pose, T-pose), our dataset embraces diverse shape categories, styles, and\nposes. Leveraging this rich dataset, we propose ARMO, a novel rigging framework\nthat utilizes an autoregressive model to predict both joint positions and\nconnectivity relationships in a unified manner. By treating the skeletal\nstructure as a complete graph and discretizing it into tokens, we encode the\njoints using an auto-encoder to obtain a latent embedding and an autoregressive\nmodel to predict the tokens. A mesh-conditioned latent diffusion model is used\nto predict the latent embedding for conditional skeleton generation. Our method\naddresses the limitations of regression-based approaches, which often suffer\nfrom error accumulation and suboptimal connectivity estimation. Through\nextensive experiments on the OmniRig dataset, our approach achieves\nstate-of-the-art performance in skeleton prediction, demonstrating improved\ngeneralization across diverse object categories. The code and dataset will be\nmade public for academic use upon acceptance."}
{"id": "2503.20672", "pdf": "https://arxiv.org/pdf/2503.20672", "abs": "https://arxiv.org/abs/2503.20672", "authors": ["Yuyang Peng", "Shishi Xiao", "Keming Wu", "Qisheng Liao", "Bohan Chen", "Kevin Lin", "Danqing Huang", "Ji Li", "Yuhui Yuan"], "title": "BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project Page: https://bizgen-msra.github.io", "summary": "Recently, state-of-the-art text-to-image generation models, such as Flux and\nIdeogram 2.0, have made significant progress in sentence-level visual text\nrendering. In this paper, we focus on the more challenging scenarios of\narticle-level visual text rendering and address a novel task of generating\nhigh-quality business content, including infographics and slides, based on user\nprovided article-level descriptive prompts and ultra-dense layouts. The\nfundamental challenges are twofold: significantly longer context lengths and\nthe scarcity of high-quality business content data.\n  In contrast to most previous works that focus on a limited number of\nsub-regions and sentence-level prompts, ensuring precise adherence to\nultra-dense layouts with tens or even hundreds of sub-regions in business\ncontent is far more challenging. We make two key technical contributions: (i)\nthe construction of scalable, high-quality business content dataset, i.e.,\nInfographics-650K, equipped with ultra-dense layouts and prompts by\nimplementing a layer-wise retrieval-augmented infographic generation scheme;\nand (ii) a layout-guided cross attention scheme, which injects tens of\nregion-wise prompts into a set of cropped region latent space according to the\nultra-dense layouts, and refine each sub-regions flexibly during inference\nusing a layout conditional CFG.\n  We demonstrate the strong results of our system compared to previous SOTA\nsystems such as Flux and SD3 on our BizEval prompt set. Additionally, we\nconduct thorough ablation experiments to verify the effectiveness of each\ncomponent. We hope our constructed Infographics-650K and BizEval can encourage\nthe broader community to advance the progress of business content generation."}
{"id": "2503.20673", "pdf": "https://arxiv.org/pdf/2503.20673", "abs": "https://arxiv.org/abs/2503.20673", "authors": ["Yinan Sun", "Xiongkuo Min", "Zicheng Zhang", "Yixuan Gao", "Yuqin Cao", "Guangtao Zhai"], "title": "Mitigating Low-Level Visual Hallucinations Requires Self-Awareness: Database, Model and Training Strategy", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of multimodal large language models has resulted in\nremarkable advancements in visual perception and understanding, consolidating\nseveral tasks into a single visual question-answering framework. However, these\nmodels are prone to hallucinations, which limit their reliability as artificial\nintelligence systems. While this issue is extensively researched in natural\nlanguage processing and image captioning, there remains a lack of investigation\nof hallucinations in Low-level Visual Perception and Understanding (HLPU),\nespecially in the context of image quality assessment tasks. We consider that\nthese hallucinations arise from an absence of clear self-awareness within the\nmodels. To address this issue, we first introduce the HLPU instruction\ndatabase, the first instruction database specifically focused on hallucinations\nin low-level vision tasks. This database contains approximately 200K\nquestion-answer pairs and comprises four subsets, each covering different types\nof instructions. Subsequently, we propose the Self-Awareness Failure\nElimination (SAFEQA) model, which utilizes image features, salient region\nfeatures and quality features to improve the perception and comprehension\nabilities of the model in low-level vision tasks. Furthermore, we propose the\nEnhancing Self-Awareness Preference Optimization (ESA-PO) framework to increase\nthe model's awareness of knowledge boundaries, thereby mitigating the incidence\nof hallucination. Finally, we conduct comprehensive experiments on low-level\nvision tasks, with the results demonstrating that our proposed method\nsignificantly enhances self-awareness of the model in these tasks and reduces\nhallucinations. Notably, our proposed method improves both accuracy and\nself-awareness of the proposed model and outperforms close-source models in\nterms of various evaluation metrics."}
{"id": "2503.20680", "pdf": "https://arxiv.org/pdf/2503.20680", "abs": "https://arxiv.org/abs/2503.20680", "authors": ["Han Wang", "Yongjie Ye", "Bingru Li", "Yuxiang Nie", "Jinghui Lu", "Jingqun Tang", "Yanjie Wang", "Can Huang"], "title": "Vision as LoRA", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce Vision as LoRA (VoRA), a novel paradigm for transforming an LLM\ninto an MLLM. Unlike prevalent MLLM architectures that rely on external vision\nmodules for vision encoding, VoRA internalizes visual capabilities by\nintegrating vision-specific LoRA layers directly into the LLM. This design\nallows the added parameters to be seamlessly merged into the LLM during\ninference, eliminating structural complexity and minimizing computational\noverhead. Moreover, inheriting the LLM's ability of handling flexible context,\nVoRA can process inputs at arbitrary resolutions.\n  To further strengthen VoRA's visual capabilities, we introduce a block-wise\ndistillation method that transfers visual priors from a pre-trained ViT into\nthe LoRA layers, effectively accelerating training by injecting visual\nknowledge. Additionally, we apply bi-directional attention masks to better\ncapture the context information of an image. We successfully demonstrate that\nwith additional pre-training data, VoRA can perform comparably with\nconventional encode-based MLLMs. All training data, codes, and model weights\nwill be released at https://github.com/Hon-Wong/VoRA."}
{"id": "2503.20682", "pdf": "https://arxiv.org/pdf/2503.20682", "abs": "https://arxiv.org/abs/2503.20682", "authors": ["Xingyu Peng", "Si Liu", "Chen Gao", "Yan Bai", "Beipeng Mu", "Xiaofei Wang", "Huaxia Xia"], "title": "GLRD: Global-Local Collaborative Reason and Debate with PSL for 3D Open-Vocabulary Detection", "categories": ["cs.CV"], "comment": "15 pages", "summary": "The task of LiDAR-based 3D Open-Vocabulary Detection (3D OVD) requires the\ndetector to learn to detect novel objects from point clouds without\noff-the-shelf training labels. Previous methods focus on the learning of\nobject-level representations and ignore the scene-level information, thus it is\nhard to distinguish objects with similar classes. In this work, we propose a\nGlobal-Local Collaborative Reason and Debate with PSL (GLRD) framework for the\n3D OVD task, considering both local object-level information and global\nscene-level information. Specifically, LLM is utilized to perform common sense\nreasoning based on object-level and scene-level information, where the\ndetection result is refined accordingly. To further boost the LLM's ability of\nprecise decisions, we also design a probabilistic soft logic solver (OV-PSL) to\nsearch for the optimal solution, and a debate scheme to confirm the class of\nconfusable objects. In addition, to alleviate the uneven distribution of\nclasses, a static balance scheme (SBC) and a dynamic balance scheme (DBC) are\ndesigned. In addition, to reduce the influence of noise in data and training,\nwe further propose Reflected Pseudo Labels Generation (RPLG) and\nBackground-Aware Object Localization (BAOL). Extensive experiments conducted on\nScanNet and SUN RGB-D demonstrate the superiority of GLRD, where absolute\nimprovements in mean average precision are $+2.82\\%$ on SUN RGB-D and $+3.72\\%$\non ScanNet in the partial open-vocabulary setting. In the full open-vocabulary\nsetting, the absolute improvements in mean average precision are $+4.03\\%$ on\nScanNet and $+14.11\\%$ on SUN RGB-D."}
{"id": "2503.20685", "pdf": "https://arxiv.org/pdf/2503.20685", "abs": "https://arxiv.org/abs/2503.20685", "authors": ["Yuhao Huang", "Ao Chang", "Haoran Dou", "Xing Tao", "Xinrui Zhou", "Yan Cao", "Ruobing Huang", "Alejandro F Frangi", "Lingyun Bao", "Xin Yang", "Dong Ni"], "title": "Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by Medical Image Analysis. 24 pages, 13 figures, 18 tabels", "summary": "Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3D\nautomated breast ultrasound (ABUS) is crucial for clinical diagnosis and\ntreatment planning. Therefore, developing an automated system for nodule\nsegmentation can enhance user independence and expedite clinical analysis.\nUnlike fully-supervised learning, weakly-supervised segmentation (WSS) can\nstreamline the laborious and intricate annotation process. However, current WSS\nmethods face challenges in achieving precise nodule segmentation, as many of\nthem depend on inaccurate activation maps or inefficient pseudo-mask generation\nalgorithms. In this study, we introduce a novel multi-agent reinforcement\nlearning-based WSS framework called Flip Learning, which relies solely on 2D/3D\nboxes for accurate segmentation. Specifically, multiple agents are employed to\nerase the target from the box to facilitate classification tag flipping, with\nthe erased region serving as the predicted segmentation mask. The key\ncontributions of this research are as follows: (1) Adoption of a\nsuperpixel/supervoxel-based approach to encode the standardized environment,\ncapturing boundary priors and expediting the learning process. (2) Introduction\nof three meticulously designed rewards, comprising a classification score\nreward and two intensity distribution rewards, to steer the agents' erasing\nprocess precisely, thereby avoiding both under- and over-segmentation. (3)\nImplementation of a progressive curriculum learning strategy to enable agents\nto interact with the environment in a progressively challenging manner, thereby\nenhancing learning efficiency. Extensively validated on the large in-house BUS\nand ABUS datasets, our Flip Learning method outperforms state-of-the-art WSS\nmethods and foundation models, and achieves comparable performance as\nfully-supervised learning algorithms."}
{"id": "2503.20698", "pdf": "https://arxiv.org/pdf/2503.20698", "abs": "https://arxiv.org/abs/2503.20698", "authors": ["Saron Samuel", "Dan DeGenaro", "Jimena Guallar-Blasco", "Kate Sanders", "Oluwaseun Eisape", "Arun Reddy", "Alexander Martin", "Andrew Yates", "Eugene Yang", "Cameron Carpenter", "David Etter", "Efsun Kayi", "Matthew Wiesner", "Kenton Murray", "Reno Kriz"], "title": "MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Videos inherently contain multiple modalities, including visual events, text\noverlays, sounds, and speech, all of which are important for retrieval.\nHowever, state-of-the-art multimodal language models like VAST and LanguageBind\nare built on vision-language models (VLMs), and thus overly prioritize visual\nsignals. Retrieval benchmarks further reinforce this bias by focusing on visual\nqueries and neglecting other modalities. We create a search system MMMORRF that\nextracts text and features from both visual and audio modalities and integrates\nthem with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is\nboth effective and efficient, demonstrating practicality in searching videos\nbased on users' information needs instead of visual descriptive queries. We\nevaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed\nfor more targeted information needs, and find that it improves nDCG@20 by 81%\nover leading multimodal encoders and 37% over single-modality retrieval,\ndemonstrating the value of integrating diverse modalities."}
{"id": "2503.20722", "pdf": "https://arxiv.org/pdf/2503.20722", "abs": "https://arxiv.org/abs/2503.20722", "authors": ["A. Candito", "A. Dragan", "R. Holbrey", "A. Ribeiro", "R. Donners", "C. Messiou", "N. Tunariu", "D. -M. Koh", "M. D. Blackledge", "The Institute of Cancer Research", "London", "United Kingdom", "The Royal Marsden NHS Foundation Trust", "London", "United Kingdom", "University Hospital Basel", "Basel", "Switzerland"], "title": "A weakly-supervised deep learning model for fast localisation and delineation of the skeleton, internal organs, and spinal canal on Whole-Body Diffusion-Weighted MRI (WB-DWI)", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Background: Apparent Diffusion Coefficient (ADC) values and Total Diffusion\nVolume (TDV) from Whole-body diffusion-weighted MRI (WB-DWI) are recognized\ncancer imaging biomarkers. However, manual disease delineation for ADC and TDV\nmeasurements is unfeasible in clinical practice, demanding automation. As a\nfirst step, we propose an algorithm to generate fast and reproducible\nprobability maps of the skeleton, adjacent internal organs (liver, spleen,\nurinary bladder, and kidneys), and spinal canal. Methods: We developed an\nautomated deep-learning pipeline based on a 3D patch-based Residual U-Net\narchitecture that localizes and delineates these anatomical structures on\nWB-DWI. The algorithm was trained using \"soft-labels\" (non-binary\nsegmentations) derived from a computationally intensive atlas-based approach.\nFor training and validation, we employed a multi-center WB-DWI dataset\ncomprising 532 scans from patients with Advanced Prostate Cancer (APC) or\nMultiple Myeloma (MM), with testing on 45 patients. Results: Our\nweakly-supervised deep learning model achieved an average dice\nscore/precision/recall of 0.66/0.6/0.73 for skeletal delineations,\n0.8/0.79/0.81 for internal organs, and 0.85/0.79/0.94 for spinal canal, with\nsurface distances consistently below 3 mm. Relative median ADC and\nlog-transformed volume differences between automated and manual expert-defined\nfull-body delineations were below 10% and 4%, respectively. The computational\ntime for generating probability maps was 12x faster than the atlas-based\nregistration algorithm (25 s vs. 5 min). An experienced radiologist rated the\nmodel's accuracy \"good\" or \"excellent\" on test datasets. Conclusion: Our model\noffers fast and reproducible probability maps for localizing and delineating\nbody regions on WB-DWI, enabling ADC and TDV quantification, potentially\nsupporting clinicians in disease staging and treatment response assessment."}
{"id": "2503.20724", "pdf": "https://arxiv.org/pdf/2503.20724", "abs": "https://arxiv.org/abs/2503.20724", "authors": ["Nan Jiang", "Hongjie Li", "Ziye Yuan", "Zimo He", "Yixin Chen", "Tengyu Liu", "Yixin Zhu", "Siyuan Huang"], "title": "Dynamic Motion Blending for Versatile Motion Editing", "categories": ["cs.CV"], "comment": null, "summary": "Text-guided motion editing enables high-level semantic control and iterative\nmodifications beyond traditional keyframe animation. Existing methods rely on\nlimited pre-collected training triplets, which severely hinders their\nversatility in diverse editing scenarios. We introduce MotionCutMix, an online\ndata augmentation technique that dynamically generates training triplets by\nblending body part motions based on input text. While MotionCutMix effectively\nexpands the training distribution, the compositional nature introduces\nincreased randomness and potential body part incoordination. To model such a\nrich distribution, we present MotionReFit, an auto-regressive diffusion model\nwith a motion coordinator. The auto-regressive architecture facilitates\nlearning by decomposing long sequences, while the motion coordinator mitigates\nthe artifacts of motion composition. Our method handles both spatial and\ntemporal motion edits directly from high-level human instructions, without\nrelying on additional specifications or Large Language Models. Through\nextensive experiments, we show that MotionReFit achieves state-of-the-art\nperformance in text-guided motion editing."}
{"id": "2503.20734", "pdf": "https://arxiv.org/pdf/2503.20734", "abs": "https://arxiv.org/abs/2503.20734", "authors": ["Ziyu Zhou", "Keyan Hu", "Yutian Fang", "Xiaoping Rui"], "title": "SChanger: Change Detection from a Semantic Change and Spatial Consistency Perspective", "categories": ["cs.CV"], "comment": null, "summary": "Change detection is a key task in Earth observation applications. Recently,\ndeep learning methods have demonstrated strong performance and widespread\napplication. However, change detection faces data scarcity due to the\nlabor-intensive process of accurately aligning remote sensing images of the\nsame area, which limits the performance of deep learning algorithms. To address\nthe data scarcity issue, we develop a fine-tuning strategy called the Semantic\nChange Network (SCN). We initially pre-train the model on single-temporal\nsupervised tasks to acquire prior knowledge of instance feature extraction. The\nmodel then employs a shared-weight Siamese architecture and extended Temporal\nFusion Module (TFM) to preserve this prior knowledge and is fine-tuned on\nchange detection tasks. The learned semantics for identifying all instances is\nchanged to focus on identifying only the changes. Meanwhile, we observe that\nthe locations of changes between the two images are spatially identical, a\nconcept we refer to as spatial consistency. We introduce this inductive bias\nthrough an attention map that is generated by large-kernel convolutions and\napplied to the features from both time points. This enhances the modeling of\nmulti-scale changes and helps capture underlying relationships in change\ndetection semantics. We develop a binary change detection model utilizing these\ntwo strategies. The model is validated against state-of-the-art methods on six\ndatasets, surpassing all benchmark methods and achieving F1 scores of 92.87%,\n86.43%, 68.95%, 97.62%, 84.58%, and 93.20% on the LEVIR-CD, LEVIR-CD+,\nS2Looking, CDD, SYSU-CD, and WHU-CD datasets, respectively."}
{"id": "2503.20739", "pdf": "https://arxiv.org/pdf/2503.20739", "abs": "https://arxiv.org/abs/2503.20739", "authors": ["Swetha Kambham", "Hubert Jhonson", "Sai Prathap Reddy Kambham"], "title": "Emotion Detection and Music Recommendation System", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As artificial intelligence becomes more and more ingrained in daily life, we\npresent a novel system that uses deep learning for music recommendation and\nemotion-based detection. Through the use of facial recognition and the DeepFace\nframework, our method analyses human emotions in real-time and then plays music\nthat reflects the mood it has discovered. The system uses a webcam to take\npictures, analyses the most common facial expression, and then pulls a playlist\nfrom local storage that corresponds to the mood it has detected. An engaging\nand customised experience is ensured by allowing users to manually change the\nsong selection via a dropdown menu or navigation buttons. By continuously\nlooping over the playlist, the technology guarantees continuity. The objective\nof our system is to improve emotional well-being through music therapy by\noffering a responsive and automated music-selection experience."}
{"id": "2503.20744", "pdf": "https://arxiv.org/pdf/2503.20744", "abs": "https://arxiv.org/abs/2503.20744", "authors": ["Guoqiang Zhang", "Kenta Niwa", "J. P. Lewis", "Cedric Mesnage", "W. Bastiaan Kleijn"], "title": "High Quality Diffusion Distillation on a Single GPU with Relative and Absolute Position Matching", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce relative and absolute position matching (RAPM), a diffusion\ndistillation method resulting in high quality generation that can be trained\nefficiently on a single GPU. Recent diffusion distillation research has\nachieved excellent results for high-resolution text-to-image generation with\nmethods such as phased consistency models (PCM) and improved distribution\nmatching distillation (DMD2). However, these methods generally require many\nGPUs (e.g.~8-64) and significant batchsizes (e.g.~128-2048) during training,\nresulting in memory and compute requirements that are beyond the resources of\nsome researchers. RAPM provides effective single-GPU diffusion distillation\ntraining with a batchsize of 1. The new method attempts to mimic the sampling\ntrajectories of the teacher model by matching the relative and absolute\npositions. The design of relative positions is inspired by PCM. Two\ndiscriminators are introduced accordingly in RAPM, one for matching relative\npositions and the other for absolute positions. Experimental results on\nStableDiffusion (SD) V1.5 and SDXL indicate that RAPM with 4 timesteps produces\ncomparable FID scores as the best method with 1 timestep under very limited\ncomputational resources."}
{"id": "2503.20745", "pdf": "https://arxiv.org/pdf/2503.20745", "abs": "https://arxiv.org/abs/2503.20745", "authors": ["Yanpeng Sun", "Shan Zhang", "Wei Tang", "Aotian Chen", "Piotr Koniusz", "Kai Zou", "Yuan Xue", "Anton van den Hengel"], "title": "MATHGLANCE: Multimodal Large Language Models Do Not Know Where to Look in Mathematical Diagrams", "categories": ["cs.CV"], "comment": null, "summary": "Diagrams serve as a fundamental form of visual language, representing complex\nconcepts and their inter-relationships through structured symbols, shapes, and\nspatial arrangements. Unlike natural images, their inherently symbolic and\nabstract nature poses significant challenges for Multimodal Large Language\nModels (MLLMs). However, current benchmarks conflate perceptual and reasoning\ntasks, making it difficult to assess whether MLLMs genuinely understand\nmathematical diagrams beyond superficial pattern recognition. To address this\ngap, we introduce MATHGLANCE, a benchmark specifically designed to isolate and\nevaluate mathematical perception in MLLMs. MATHGLANCE comprises 1.2K images and\n1.6K carefully curated questions spanning four perception tasks: shape\nclassification, object counting, relationship identification, and object\ngrounding, covering diverse domains including plane geometry, solid geometry,\nand graphical representations. Our evaluation of MLLMs reveals that their\nability to understand diagrams is notably limited, particularly in fine-grained\ngrounding tasks. In response, we construct GeoPeP, a perception-oriented\ndataset of 200K structured geometry image-text pairs explicitly annotated with\ngeometric primitives and precise spatial relationships. Training MLLM on GeoPeP\nleads to significant gains in perceptual accuracy, which in turn substantially\nimproves mathematical reasoning. Our benchmark and dataset establish critical\nstandards for evaluating and advancing multimodal mathematical understanding,\nproviding valuable resources and insights to foster future MLLM research."}
{"id": "2503.20746", "pdf": "https://arxiv.org/pdf/2503.20746", "abs": "https://arxiv.org/abs/2503.20746", "authors": ["Boyuan Chen", "Hanxiao Jiang", "Shaowei Liu", "Saurabh Gupta", "Yunzhu Li", "Hao Zhao", "Shenlong Wang"], "title": "PhysGen3D: Crafting a Miniature Interactive World from a Single Image", "categories": ["cs.CV"], "comment": "CVPR 2025, Project page: https://by-luckk.github.io/PhysGen3D", "summary": "Envisioning physically plausible outcomes from a single image requires a deep\nunderstanding of the world's dynamics. To address this, we introduce PhysGen3D,\na novel framework that transforms a single image into an amodal,\ncamera-centric, interactive 3D scene. By combining advanced image-based\ngeometric and semantic understanding with physics-based simulation, PhysGen3D\ncreates an interactive 3D world from a static image, enabling us to \"imagine\"\nand simulate future scenarios based on user input. At its core, PhysGen3D\nestimates 3D shapes, poses, physical and lighting properties of objects,\nthereby capturing essential physical attributes that drive realistic object\ninteractions. This framework allows users to specify precise initial\nconditions, such as object speed or material properties, for enhanced control\nover generated video outcomes. We evaluate PhysGen3D's performance against\nclosed-source state-of-the-art (SOTA) image-to-video models, including Pika,\nKling, and Gen-3, showing PhysGen3D's capacity to generate videos with\nrealistic physics while offering greater flexibility and fine-grained control.\nOur results show that PhysGen3D achieves a unique balance of photorealism,\nphysical plausibility, and user-driven interactivity, opening new possibilities\nfor generating dynamic, physics-grounded video from an image."}
{"id": "2503.20748", "pdf": "https://arxiv.org/pdf/2503.20748", "abs": "https://arxiv.org/abs/2503.20748", "authors": ["Chen Tang", "Xinzhu Ma", "Encheng Su", "Xiufeng Song", "Xiaohong Liu", "Wei-Hong Li", "Lei Bai", "Wanli Ouyang", "Xiangyu Yue"], "title": "UniSTD: Towards Unified Spatio-Temporal Learning across Diverse Disciplines", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Traditional spatiotemporal models generally rely on task-specific\narchitectures, which limit their generalizability and scalability across\ndiverse tasks due to domain-specific design requirements. In this paper, we\nintroduce \\textbf{UniSTD}, a unified Transformer-based framework for\nspatiotemporal modeling, which is inspired by advances in recent foundation\nmodels with the two-stage pretraining-then-adaption paradigm. Specifically, our\nwork demonstrates that task-agnostic pretraining on 2D vision and vision-text\ndatasets can build a generalizable model foundation for spatiotemporal\nlearning, followed by specialized joint training on spatiotemporal datasets to\nenhance task-specific adaptability. To improve the learning capabilities across\ndomains, our framework employs a rank-adaptive mixture-of-expert adaptation by\nusing fractional interpolation to relax the discrete variables so that can be\noptimized in the continuous space. Additionally, we introduce a temporal module\nto incorporate temporal dynamics explicitly. We evaluate our approach on a\nlarge-scale dataset covering 10 tasks across 4 disciplines, demonstrating that\na unified spatiotemporal model can achieve scalable, cross-task learning and\nsupport up to 10 tasks simultaneously within one model while reducing training\ncosts in multi-domain applications. Code will be available at\nhttps://github.com/1hunters/UniSTD."}
{"id": "2503.20752", "pdf": "https://arxiv.org/pdf/2503.20752", "abs": "https://arxiv.org/abs/2503.20752", "authors": ["Huajie Tan", "Yuheng Ji", "Xiaoshuai Hao", "Minglan Lin", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "35 pages, 22 figures", "summary": "Visual reasoning abilities play a crucial role in understanding complex\nmultimodal data, advancing both domain-specific applications and artificial\ngeneral intelligence (AGI). Existing methods improve VLM reasoning via\nChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotated\ntraining data to enhance visual reasoning capabilities. However, this training\nparadigm may lead to overfitting and cognitive rigidity, restricting the\nmodel's ability to transfer visual reasoning skills across domains and limiting\nits real-world applicability. To address these limitations, we propose\nReason-RFT, a novel reinforcement fine-tuning framework that significantly\nenhances generalization capabilities in visual reasoning tasks. Reason-RFT\nintroduces a two-phase training framework for visual reasoning: (1) Supervised\nFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates the\nreasoning potential of Vision-Language Models (VLMs), followed by (2) Group\nRelative Policy Optimization (GRPO)-based reinforcement learning that generates\nmultiple reasoning-response pairs, significantly enhancing generalization in\nvisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,\nwe reconstructed a comprehensive dataset spanning visual counting, structure\nperception, and spatial transformation.cExperimental results demonstrate\nReasoning-RFT's three key advantages: (1) Performance Enhancement: achieving\nstate-of-the-art results across multiple tasks, outperforming most mainstream\nopen-source and proprietary models; (2) Generalization Superiority:\nconsistently maintaining robust performance across diverse tasks and domains,\noutperforming alternative training paradigms; (3) Data Efficiency: excelling in\nfew-shot learning scenarios while surpassing full-dataset SFT baselines."}
{"id": "2503.20771", "pdf": "https://arxiv.org/pdf/2503.20771", "abs": "https://arxiv.org/abs/2503.20771", "authors": ["Masoumeh Sharafi", "Emma Ollivier", "Muhammad Osama Zeeshan", "Soufiane Belharbi", "Marco Pedersoli", "Alessandro Lameiras Koerich", "Simon Bacon", "Eric~Granger"], "title": "Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data", "categories": ["cs.CV"], "comment": null, "summary": "Facial Expression Recognition (FER) from videos is a crucial task in various\napplication areas, such as human-computer interaction and health monitoring\n(e.g., pain, depression, fatigue, and stress). Beyond the challenges of\nrecognizing subtle emotional or health states, the effectiveness of deep FER\nmodels is often hindered by the considerable variability of expressions among\nsubjects. Source-free domain adaptation (SFDA) methods are employed to adapt a\npre-trained source model using only unlabeled target domain data, thereby\navoiding data privacy and storage issues. Typically, SFDA methods adapt to a\ntarget domain dataset corresponding to an entire population and assume it\nincludes data from all recognition classes. However, collecting such\ncomprehensive target data can be difficult or even impossible for FER in\nhealthcare applications. In many real-world scenarios, it may be feasible to\ncollect a short neutral control video (displaying only neutral expressions) for\ntarget subjects before deployment. These videos can be used to adapt a model to\nbetter handle the variability of expressions among subjects. This paper\nintroduces the Disentangled Source-Free Domain Adaptation (DSFDA) method to\naddress the SFDA challenge posed by missing target expression data. DSFDA\nleverages data from a neutral target control video for end-to-end generation\nand adaptation of target data with missing non-neutral data. Our method learns\nto disentangle features related to expressions and identity while generating\nthe missing non-neutral target data, thereby enhancing model accuracy.\nAdditionally, our self-supervision strategy improves model adaptation by\nreconstructing target images that maintain the same identity and source\nexpression."}
{"id": "2503.20776", "pdf": "https://arxiv.org/pdf/2503.20776", "abs": "https://arxiv.org/abs/2503.20776", "authors": ["Shijie Zhou", "Hui Ren", "Yijia Weng", "Shuwang Zhang", "Zhen Wang", "Dejia Xu", "Zhiwen Fan", "Suya You", "Zhangyang Wang", "Leonidas Guibas", "Achuta Kadambi"], "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in 2D and multimodal models have achieved remarkable\nsuccess by leveraging large-scale training on extensive datasets. However,\nextending these achievements to enable free-form interactions and high-level\nsemantic operations with complex 3D/4D scenes remains challenging. This\ndifficulty stems from the limited availability of large-scale, annotated 3D/4D\nor multi-view datasets, which are crucial for generalizable vision and language\ntasks such as open-vocabulary and prompt-based segmentation, language-guided\nediting, and visual question answering (VQA). In this paper, we introduce\nFeature4X, a universal framework designed to extend any functionality from 2D\nvision foundation model into the 4D realm, using only monocular video input,\nwhich is widely available from user-generated content. The \"X\" in Feature4X\nrepresents its versatility, enabling any task through adaptable,\nmodel-conditioned 4D feature field distillation. At the core of our framework\nis a dynamic optimization strategy that unifies multiple model capabilities\ninto a single representation. Additionally, to the best of our knowledge,\nFeature4X is the first method to distill and lift the features of video\nfoundation models (e.g. SAM2, InternVideo2) into an explicit 4D feature field\nusing Gaussian Splatting. Our experiments showcase novel view segment anything,\ngeometric and appearance scene editing, and free-form VQA across all time\nsteps, empowered by LLMs in feedback loops. These advancements broaden the\nscope of agentic AI applications by providing a foundation for scalable,\ncontextually and spatiotemporally aware systems capable of immersive dynamic 4D\nscene interaction."}
{"id": "2503.20781", "pdf": "https://arxiv.org/pdf/2503.20781", "abs": "https://arxiv.org/abs/2503.20781", "authors": ["Yulu Pan", "Ce Zhang", "Gedas Bertasius"], "title": "BASKET: A Large-Scale Video Dataset for Fine-Grained Skill Estimation", "categories": ["cs.CV"], "comment": null, "summary": "We present BASKET, a large-scale basketball video dataset for fine-grained\nskill estimation. BASKET contains 4,477 hours of video capturing 32,232\nbasketball players from all over the world. Compared to prior skill estimation\ndatasets, our dataset includes a massive number of skilled participants with\nunprecedented diversity in terms of gender, age, skill level, geographical\nlocation, etc. BASKET includes 20 fine-grained basketball skills, challenging\nmodern video recognition models to capture the intricate nuances of player\nskill through in-depth video analysis. Given a long highlight video (8-10\nminutes) of a particular player, the model needs to predict the skill level\n(e.g., excellent, good, average, fair, poor) for each of the 20 basketball\nskills. Our empirical analysis reveals that the current state-of-the-art video\nmodels struggle with this task, significantly lagging behind the human\nbaseline. We believe that BASKET could be a useful resource for developing new\nvideo models with advanced long-range, fine-grained recognition capabilities.\nIn addition, we hope that our dataset will be useful for domain-specific\napplications such as fair basketball scouting, personalized player development,\nand many others. Dataset and code are available at\nhttps://github.com/yulupan00/BASKET."}
{"id": "2503.20782", "pdf": "https://arxiv.org/pdf/2503.20782", "abs": "https://arxiv.org/abs/2503.20782", "authors": ["Yan-Bo Lin", "Kevin Lin", "Zhengyuan Yang", "Linjie Li", "Jianfeng Wang", "Chung-Ching Lin", "Xiaofei Wang", "Gedas Bertasius", "Lijuan Wang"], "title": "Zero-Shot Audio-Visual Editing via Cross-Modal Delta Denoising", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "Project page: https://genjib.github.io/project_page/AVED/index.html", "summary": "In this paper, we introduce zero-shot audio-video editing, a novel task that\nrequires transforming original audio-visual content to align with a specified\ntextual prompt without additional model training. To evaluate this task, we\ncurate a benchmark dataset, AvED-Bench, designed explicitly for zero-shot\naudio-video editing. AvED-Bench includes 110 videos, each with a 10-second\nduration, spanning 11 categories from VGGSound. It offers diverse prompts and\nscenarios that require precise alignment between auditory and visual elements,\nenabling robust evaluation. We identify limitations in existing zero-shot audio\nand video editing methods, particularly in synchronization and coherence\nbetween modalities, which often result in inconsistent outcomes. To address\nthese challenges, we propose AvED, a zero-shot cross-modal delta denoising\nframework that leverages audio-video interactions to achieve synchronized and\ncoherent edits. AvED demonstrates superior results on both AvED-Bench and the\nrecent OAVE dataset to validate its generalization capabilities. Results are\navailable at https://genjib.github.io/project_page/AVED/index.html"}
{"id": "2503.20784", "pdf": "https://arxiv.org/pdf/2503.20784", "abs": "https://arxiv.org/abs/2503.20784", "authors": ["Jinwei Li", "Huan-ang Gao", "Wenyi Li", "Haohan Chi", "Chenyu Liu", "Chenxi Du", "Yiqian Liu", "Mingju Gao", "Guiyu Zhang", "Zongzheng Zhang", "Li Yi", "Yao Yao", "Jingwei Zhao", "Hongyang Li", "Yikai Wang", "Hao Zhao"], "title": "FB-4D: Spatial-Temporal Coherent Dynamic 3D Content Generation with Feature Banks", "categories": ["cs.CV"], "comment": "Project page:https://fb-4d.c7w.tech/", "summary": "With the rapid advancements in diffusion models and 3D generation techniques,\ndynamic 3D content generation has become a crucial research area. However,\nachieving high-fidelity 4D (dynamic 3D) generation with strong spatial-temporal\nconsistency remains a challenging task. Inspired by recent findings that\npretrained diffusion features capture rich correspondences, we propose FB-4D, a\nnovel 4D generation framework that integrates a Feature Bank mechanism to\nenhance both spatial and temporal consistency in generated frames. In FB-4D, we\nstore features extracted from previous frames and fuse them into the process of\ngenerating subsequent frames, ensuring consistent characteristics across both\ntime and multiple views. To ensure a compact representation, the Feature Bank\nis updated by a proposed dynamic merging mechanism. Leveraging this Feature\nBank, we demonstrate for the first time that generating additional reference\nsequences through multiple autoregressive iterations can continuously improve\ngeneration performance. Experimental results show that FB-4D significantly\noutperforms existing methods in terms of rendering quality, spatial-temporal\nconsistency, and robustness. It surpasses all multi-view generation tuning-free\napproaches by a large margin and achieves performance on par with\ntraining-based methods."}
{"id": "2503.20785", "pdf": "https://arxiv.org/pdf/2503.20785", "abs": "https://arxiv.org/abs/2503.20785", "authors": ["Tianqi Liu", "Zihao Huang", "Zhaoxi Chen", "Guangcong Wang", "Shoukang Hu", "Liao Shen", "Huiqiang Sun", "Zhiguo Cao", "Wei Li", "Ziwei Liu"], "title": "Free4D: Tuning-free 4D Scene Generation with Spatial-Temporal Consistency", "categories": ["cs.CV"], "comment": "Project Page: https://free4d.github.io/ , Code:\n  https://github.com/TQTQliu/Free4D", "summary": "We present Free4D, a novel tuning-free framework for 4D scene generation from\na single image. Existing methods either focus on object-level generation,\nmaking scene-level generation infeasible, or rely on large-scale multi-view\nvideo datasets for expensive training, with limited generalization ability due\nto the scarcity of 4D scene data. In contrast, our key insight is to distill\npre-trained foundation models for consistent 4D scene representation, which\noffers promising advantages such as efficiency and generalizability. 1) To\nachieve this, we first animate the input image using image-to-video diffusion\nmodels followed by 4D geometric structure initialization. 2) To turn this\ncoarse structure into spatial-temporal consistent multiview videos, we design\nan adaptive guidance mechanism with a point-guided denoising strategy for\nspatial consistency and a novel latent replacement strategy for temporal\ncoherence. 3) To lift these generated observations into consistent 4D\nrepresentation, we propose a modulation-based refinement to mitigate\ninconsistencies while fully leveraging the generated information. The resulting\n4D representation enables real-time, controllable rendering, marking a\nsignificant advancement in single-image-based 4D scene generation."}
{"id": "2503.19923", "pdf": "https://arxiv.org/pdf/2503.19923", "abs": "https://arxiv.org/abs/2503.19923", "authors": ["Cesare Maria Dalbagno", "Manuel de Castro Ribeiro Jardim", "Mihnea Angheluţă"], "title": "Mapping fMRI Signal and Image Stimuli in an Artificial Neural Network Latent Space: Bringing Artificial and Natural Minds Together", "categories": ["q-bio.NC", "cs.CV", "92B20, 68T07, 62P10, 94A12", "I.2.6; I.5.1; J.3"], "comment": "5 pages, 3 figures", "summary": "The goal of this study is to investigate whether latent space representations\nof visual stimuli and fMRI data share common information. Decoding and\nreconstructing stimuli from fMRI data remains a challenge in AI and\nneuroscience, with significant implications for understanding neural\nrepresentations and improving the interpretability of Artificial Neural\nNetworks (ANNs). In this preliminary study, we investigate the feasibility of\nsuch reconstruction by examining the similarity between the latent spaces of\none autoencoder (AE) and one vision transformer (ViT) trained on fMRI and image\ndata, respectively. Using representational similarity analysis (RSA), we found\nthat the latent spaces of the two domains appear different. However, these\ninitial findings are inconclusive, and further research is needed to explore\nthis relationship more thoroughly."}
{"id": "2503.19945", "pdf": "https://arxiv.org/pdf/2503.19945", "abs": "https://arxiv.org/abs/2503.19945", "authors": ["Daniel G. P. Petrini", "Hae Yong Kim"], "title": "Optimizing Breast Cancer Detection in Mammograms: A Comprehensive Study of Transfer Learning, Resolution Reduction, and Multi-View Classification", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "8 pages", "summary": "This study explores open questions in the application of machine learning for\nbreast cancer detection in mammograms. Current approaches often employ a\ntwo-stage transfer learning process: first, adapting a backbone model trained\non natural images to develop a patch classifier, which is then used to create a\nsingle-view whole-image classifier. Additionally, many studies leverage both\nmammographic views to enhance model performance. In this work, we\nsystematically investigate five key questions: (1) Is the intermediate patch\nclassifier essential for optimal performance? (2) Do backbone models that excel\nin natural image classification consistently outperform others on mammograms?\n(3) When reducing mammogram resolution for GPU processing, does the\nlearn-to-resize technique outperform conventional methods? (4) Does\nincorporating both mammographic views in a two-view classifier significantly\nimprove detection accuracy? (5) How do these findings vary when analyzing\nlow-quality versus high-quality mammograms? By addressing these questions, we\ndeveloped models that outperform previous results for both single-view and\ntwo-view classifiers. Our findings provide insights into model architecture and\ntransfer learning strategies contributing to more accurate and efficient\nmammogram analysis."}
{"id": "2503.19976", "pdf": "https://arxiv.org/pdf/2503.19976", "abs": "https://arxiv.org/abs/2503.19976", "authors": ["Navami Kairanda", "Marc Habermann", "Shanthika Naik", "Christian Theobalt", "Vladislav Golyanik"], "title": "Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking with Neural Deformation Fields", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "15 pages, 12 figures and 3 tables; project page:\n  https://4dqv.mpiinf.mpg.de/ThinShellSfT; CVPR 2025", "summary": "3D reconstruction of highly deformable surfaces (e.g. cloths) from monocular\nRGB videos is a challenging problem, and no solution provides a consistent and\naccurate recovery of fine-grained surface details. To account for the ill-posed\nnature of the setting, existing methods use deformation models with\nstatistical, neural, or physical priors. They also predominantly rely on\nnonadaptive discrete surface representations (e.g. polygonal meshes), perform\nframe-by-frame optimisation leading to error propagation, and suffer from poor\ngradients of the mesh-based differentiable renderers. Consequently, fine\nsurface details such as cloth wrinkles are often not recovered with the desired\naccuracy. In response to these limitations, we propose ThinShell-SfT, a new\nmethod for non-rigid 3D tracking that represents a surface as an implicit and\ncontinuous spatiotemporal neural field. We incorporate continuous thin shell\nphysics prior based on the Kirchhoff-Love model for spatial regularisation,\nwhich starkly contrasts the discretised alternatives of earlier works. Lastly,\nwe leverage 3D Gaussian splatting to differentiably render the surface into\nimage space and optimise the deformations based on analysis-bysynthesis\nprinciples. Our Thin-Shell-SfT outperforms prior works qualitatively and\nquantitatively thanks to our continuous surface formulation in conjunction with\na specially tailored simulation prior and surface-induced 3D Gaussians. See our\nproject page at https://4dqv.mpiinf.mpg.de/ThinShellSfT."}
{"id": "2503.20047", "pdf": "https://arxiv.org/pdf/2503.20047", "abs": "https://arxiv.org/abs/2503.20047", "authors": ["Yu Xin", "Gorkem Can Ates", "Kuang Gong", "Wei Shao"], "title": "Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image Analysis", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Vision-language models (VLMs) have shown promise in 2D medical image\nanalysis, but extending them to 3D remains challenging due to the high\ncomputational demands of volumetric data and the difficulty of aligning 3D\nspatial features with clinical text. We present Med3DVLM, a 3D VLM designed to\naddress these challenges through three key innovations: (1) DCFormer, an\nefficient encoder that uses decomposed 3D convolutions to capture fine-grained\nspatial features at scale; (2) SigLIP, a contrastive learning strategy with\npairwise sigmoid loss that improves image-text alignment without relying on\nlarge negative batches; and (3) a dual-stream MLP-Mixer projector that fuses\nlow- and high-level image features with text embeddings for richer multi-modal\nrepresentations. We evaluate our model on the M3D dataset, which includes\nradiology reports and VQA data for 120,084 3D medical images. Results show that\nMed3DVLM achieves superior performance across multiple benchmarks. For\nimage-text retrieval, it reaches 61.00% R@1 on 2,000 samples, significantly\noutperforming the current state-of-the-art M3D model (19.10%). For report\ngeneration, it achieves a METEOR score of 36.42% (vs. 14.38%). In open-ended\nvisual question answering (VQA), it scores 36.76% METEOR (vs. 33.58%), and in\nclosed-ended VQA, it achieves 79.95% accuracy (vs. 75.78%). These results\nhighlight Med3DVLM's ability to bridge the gap between 3D imaging and language,\nenabling scalable, multi-task reasoning across clinical applications. Our code\nis publicly available at https://github.com/mirthAI/Med3DVLM."}
{"id": "2503.20066", "pdf": "https://arxiv.org/pdf/2503.20066", "abs": "https://arxiv.org/abs/2503.20066", "authors": ["Zhirui Dai", "Hojoon Shin", "Yulun Tian", "Ki Myung Brian Lee", "Nikolay Atanasov"], "title": "Learning Scene-Level Signed Directional Distance Function with Ellipsoidal Priors and Neural Residuals", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Dense geometric environment representations are critical for autonomous\nmobile robot navigation and exploration. Recent work shows that implicit\ncontinuous representations of occupancy, signed distance, or radiance learned\nusing neural networks offer advantages in reconstruction fidelity, efficiency,\nand differentiability over explicit discrete representations based on meshes,\npoint clouds, and voxels. In this work, we explore a directional formulation of\nsigned distance, called signed directional distance function (SDDF). Unlike\nsigned distance function (SDF) and similar to neural radiance fields (NeRF),\nSDDF has a position and viewing direction as input. Like SDF and unlike NeRF,\nSDDF directly provides distance to the observed surface along the direction,\nrather than integrating along the view ray, allowing efficient view synthesis.\nTo learn and predict scene-level SDDF efficiently, we develop a differentiable\nhybrid representation that combines explicit ellipsoid priors and implicit\nneural residuals. This approach allows the model to effectively handle large\ndistance discontinuities around obstacle boundaries while preserving the\nability for dense high-fidelity prediction. We show that SDDF is competitive\nwith the state-of-the-art neural implicit scene models in terms of\nreconstruction accuracy and rendering efficiency, while allowing differentiable\nview prediction for robot trajectory optimization."}
{"id": "2503.20118", "pdf": "https://arxiv.org/pdf/2503.20118", "abs": "https://arxiv.org/abs/2503.20118", "authors": ["Yuke Lou", "Yiming Wang", "Zhen Wu", "Rui Zhao", "Wenjia Wang", "Mingyi Shi", "Taku Komura"], "title": "Zero-Shot Human-Object Interaction Synthesis with Multimodal Priors", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Human-object interaction (HOI) synthesis is important for various\napplications, ranging from virtual reality to robotics. However, acquiring 3D\nHOI data is challenging due to its complexity and high cost, limiting existing\nmethods to the narrow diversity of object types and interaction patterns in\ntraining datasets. This paper proposes a novel zero-shot HOI synthesis\nframework without relying on end-to-end training on currently limited 3D HOI\ndatasets. The core idea of our method lies in leveraging extensive HOI\nknowledge from pre-trained Multimodal Models. Given a text description, our\nsystem first obtains temporally consistent 2D HOI image sequences using image\nor video generation models, which are then uplifted to 3D HOI milestones of\nhuman and object poses. We employ pre-trained human pose estimation models to\nextract human poses and introduce a generalizable category-level 6-DoF\nestimation method to obtain the object poses from 2D HOI images. Our estimation\nmethod is adaptive to various object templates obtained from text-to-3D models\nor online retrieval. A physics-based tracking of the 3D HOI kinematic milestone\nis further applied to refine both body motions and object poses, yielding more\nphysically plausible HOI generation results. The experimental results\ndemonstrate that our method is capable of generating open-vocabulary HOIs with\nphysical realism and semantic diversity."}
{"id": "2503.20187", "pdf": "https://arxiv.org/pdf/2503.20187", "abs": "https://arxiv.org/abs/2503.20187", "authors": ["Pirzada Suhail", "Amit Sethi"], "title": "Network Inversion for Generating Confidently Classified Counterfeits", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "In machine learning, especially with vision classifiers, generating inputs\nthat are confidently classified by the model is essential for understanding its\ndecision boundaries and behavior. However, creating such samples that are\nconfidently classified yet distinct from the training data distribution is a\nchallenge. Traditional methods often modify existing inputs, but they don't\nalways ensure confident classification. In this work, we extend network\ninversion techniques to generate Confidently Classified Counterfeits-synthetic\nsamples that are confidently classified by the model despite being\nsignificantly different from the training data. We achieve this by modifying\nthe generator's conditioning mechanism from soft vector conditioning to one-hot\nvector conditioning and applying Kullback-Leibler divergence (KLD) between the\none-hot vectors and the classifier's output distribution. This encourages the\ngenerator to produce samples that are both plausible and confidently\nclassified. Generating Confidently Classified Counterfeits is crucial for\nensuring the safety and reliability of machine learning systems, particularly\nin safety-critical applications where models must exhibit confidence only on\ndata within the training distribution. By generating such counterfeits, we\nchallenge the assumption that high-confidence predictions are always indicative\nof in-distribution data, providing deeper insights into the model's limitations\nand decision-making process."}
{"id": "2503.20215", "pdf": "https://arxiv.org/pdf/2503.20215", "abs": "https://arxiv.org/abs/2503.20215", "authors": ["Jin Xu", "Zhifang Guo", "Jinzheng He", "Hangrui Hu", "Ting He", "Shuai Bai", "Keqin Chen", "Jialin Wang", "Yang Fan", "Kai Dang", "Bin Zhang", "Xiong Wang", "Yunfei Chu", "Junyang Lin"], "title": "Qwen2.5-Omni Technical Report", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model\ndesigned to perceive diverse modalities, including text, images, audio, and\nvideo, while simultaneously generating text and natural speech responses in a\nstreaming manner. To enable the streaming of multimodal information inputs,\nboth audio and visual encoders utilize a block-wise processing approach. To\nsynchronize the timestamps of video inputs with audio, we organize the audio\nand video sequentially in an interleaved manner and propose a novel position\nembedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently\ngenerate text and speech while avoiding interference between the two\nmodalities, we propose \\textbf{Thinker-Talker} architecture. In this framework,\nThinker functions as a large language model tasked with text generation, while\nTalker is a dual-track autoregressive model that directly utilizes the hidden\nrepresentations from the Thinker to produce audio tokens as output. Both the\nThinker and Talker models are designed to be trained and inferred in an\nend-to-end manner. For decoding audio tokens in a streaming manner, we\nintroduce a sliding-window DiT that restricts the receptive field, aiming to\nreduce the initial package delay. Qwen2.5-Omni is comparable with the similarly\nsized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni\nachieves state-of-the-art performance on multimodal benchmarks like Omni-Bench.\nNotably, Qwen2.5-Omni's performance in end-to-end speech instruction following\nis comparable to its capabilities with text inputs, as evidenced by benchmarks\nsuch as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming\nTalker outperforms most existing streaming and non-streaming alternatives in\nrobustness and naturalness."}
{"id": "2503.20306", "pdf": "https://arxiv.org/pdf/2503.20306", "abs": "https://arxiv.org/abs/2503.20306", "authors": ["Bargava Subramanian", "Naveen Kumarasami", "Praveen Shastry", "Kalyan Sivasailam", "Anandakumar D", "Elakkiya R", "Harsha KG", "Rithanya V", "Harini T", "Afshin Hussain", "Kishore Prasath Venkatesh"], "title": "3D Convolutional Neural Networks for Improved Detection of Intracranial bleeding in CT Imaging", "categories": ["eess.IV", "cs.CV", "68T07"], "comment": "12 pages,4 figures", "summary": "Background: Intracranial bleeding (IB) is a life-threatening condition caused\nby traumatic brain injuries, including epidural, subdural, subarachnoid, and\nintraparenchymal hemorrhages. Rapid and accurate detection is crucial to\nprevent severe complications. Traditional imaging can be slow and prone to\nvariability, especially in high-pressure scenarios. Artificial Intelligence\n(AI) provides a solution by quickly analyzing medical images, identifying\nsubtle hemorrhages, and flagging urgent cases. By enhancing diagnostic speed\nand accuracy, AI improves workflows and patient care. This article explores\nAI's role in transforming IB detection in emergency settings.\n  Methods: A U-shaped 3D Convolutional Neural Network (CNN) automates IB\ndetection and classification in volumetric CT scans. Advanced preprocessing,\nincluding CLAHE and intensity normalization, enhances image quality. The\narchitecture preserves spatial and contextual details for precise segmentation.\nA dataset of 2,912 annotated CT scans was used for training and evaluation.\n  Results: The model achieved high performance across major bleed types, with\nprecision, recall, and accuracy exceeding 90 percent in most cases 96 percent\nprecision for epidural hemorrhages and 94 percent accuracy for subarachnoid\nhemorrhages. Its ability to classify and localize hemorrhages highlights its\nclinical reliability.\n  Conclusion: This U-shaped 3D CNN offers a scalable solution for automating IB\ndetection, reducing diagnostic delays, and improving emergency care outcomes.\nFuture work will expand dataset diversity, optimize real-time processing, and\nintegrate multimodal data for enhanced clinical applicability."}
{"id": "2503.20308", "pdf": "https://arxiv.org/pdf/2503.20308", "abs": "https://arxiv.org/abs/2503.20308", "authors": ["Lee Chae-Yeon", "Oh Hyun-Bin", "Han EunGi", "Kim Sung-Bin", "Suekyeong Nam", "Tae-Hyun Oh"], "title": "Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Recent advancements in speech-driven 3D talking head generation have made\nsignificant progress in lip synchronization. However, existing models still\nstruggle to capture the perceptual alignment between varying speech\ncharacteristics and corresponding lip movements. In this work, we claim that\nthree criteria -- Temporal Synchronization, Lip Readability, and Expressiveness\n-- are crucial for achieving perceptually accurate lip movements. Motivated by\nour hypothesis that a desirable representation space exists to meet these three\ncriteria, we introduce a speech-mesh synchronized representation that captures\nintricate correspondences between speech signals and 3D face meshes. We found\nthat our learned representation exhibits desirable characteristics, and we plug\nit into existing models as a perceptual loss to better align lip movements to\nthe given speech. In addition, we utilize this representation as a perceptual\nmetric and introduce two other physically grounded lip synchronization metrics\nto assess how well the generated 3D talking heads align with these three\ncriteria. Experiments show that training 3D talking head generation models with\nour perceptual loss significantly improve all three aspects of perceptually\naccurate lip synchronization. Codes and datasets are available at\nhttps://perceptual-3d-talking-head.github.io/."}
{"id": "2503.20316", "pdf": "https://arxiv.org/pdf/2503.20316", "abs": "https://arxiv.org/abs/2503.20316", "authors": ["Bargava Subramanian", "Naveen Kumarasami", "Praveen Shastry", "Raghotham Sripadraj", "Kalyan Sivasailam", "Anandakumar D", "Abinaya Ramachandran", "Sudhir MP", "Gunakutti G", "Kishore Prasath Venkatesh"], "title": "AI-Driven MRI Spine Pathology Detection: A Comprehensive Deep Learning Approach for Automated Diagnosis in Diverse Clinical Settings", "categories": ["eess.IV", "cs.CV", "68T07"], "comment": "20 pages , 3 figurea", "summary": "Study Design This study presents the development of an autonomous AI system\nfor MRI spine pathology detection, trained on a dataset of 2 million MRI spine\nscans sourced from diverse healthcare facilities across India. The AI system\nintegrates advanced architectures, including Vision Transformers, U-Net with\ncross-attention, MedSAM, and Cascade R-CNN, enabling comprehensive\nclassification, segmentation, and detection of 43 distinct spinal pathologies.\nThe dataset is balanced across age groups, genders, and scanner manufacturers\nto ensure robustness and adaptability. Subgroup analyses were conducted to\nvalidate the model's performance across different patient demographics, imaging\nconditions, and equipment types.\n  Performance The AI system achieved up to 97.9 percent multi-pathology\ndetection, demonstrating consistent performance across age, gender, and\nmanufacturer subgroups. The normal vs. abnormal classification achieved 98.0\npercent accuracy, and the system was deployed across 13 major healthcare\nenterprises in India, encompassing diagnostic centers, large hospitals, and\ngovernment facilities. During deployment, it processed approximately 100,000\nplus MRI spine scans, leading to reduced reporting times and increased\ndiagnostic efficiency by automating the identification of common spinal\nconditions.\n  Conclusion The AI system's high precision and recall validate its capability\nas a reliable tool for autonomous normal/abnormal classification, pathology\nsegmentation, and detection. Its scalability and adaptability address critical\ndiagnostic gaps, optimize radiology workflows, and improve patient care across\nvaried healthcare environments in India."}
{"id": "2503.20328", "pdf": "https://arxiv.org/pdf/2503.20328", "abs": "https://arxiv.org/abs/2503.20328", "authors": ["Antoine Bottenmuller", "Florent Magaud", "Arnaud Demortière", "Etienne Decencière", "Petr Dokladal"], "title": "Euclidean Distance to Convex Polyhedra and Application to Class Representation in Spectral Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "With the aim of estimating the abundance map from observations only, linear\nunmixing approaches are not always suitable to spectral images, especially when\nthe number of bands is too small or when the spectra of the observed data are\ntoo correlated. To address this issue in the general case, we present a novel\napproach which provides an adapted spatial density function based on any\narbitrary linear classifier. A robust mathematical formulation for computing\nthe Euclidean distance to polyhedral sets is presented, along with an efficient\nalgorithm that provides the exact minimum-norm point in a polyhedron. An\nempirical evaluation on the widely-used Samson hyperspectral dataset\ndemonstrates that the proposed method surpasses state-of-the-art approaches in\nreconstructing abundance maps. Furthermore, its application to spectral images\nof a Lithium-ion battery, incompatible with linear unmixing models, validates\nthe method's generality and effectiveness."}
{"id": "2503.20446", "pdf": "https://arxiv.org/pdf/2503.20446", "abs": "https://arxiv.org/abs/2503.20446", "authors": ["Farzan Moodi", "Fereshteh Khodadadi Shoushtari", "Gelareh Valizadeh", "Dornaz Mazinani", "Hanieh Mobarak Salari", "Hamidreza Saligheh Rad"], "title": "Attention Xception UNet (AXUNet): A Novel Combination of CNN and Self-Attention for Brain Tumor Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Accurate segmentation of glioma brain tumors is crucial for diagnosis and\ntreatment planning. Deep learning techniques offer promising solutions, but\noptimal model architectures remain under investigation. We used the BraTS 2021\ndataset, selecting T1 with contrast enhancement (T1CE), T2, and\nFluid-Attenuated Inversion Recovery (FLAIR) sequences for model development.\nThe proposed Attention Xception UNet (AXUNet) architecture integrates an\nXception backbone with dot-product self-attention modules, inspired by\nstate-of-the-art (SOTA) large language models such as Google Bard and OpenAI\nChatGPT, within a UNet-shaped model. We compared AXUNet with SOTA models.\nComparative evaluation on the test set demonstrated improved results over\nbaseline models. Inception-UNet and Xception-UNet achieved mean Dice scores of\n90.88 and 93.24, respectively. Attention ResUNet (AResUNet) attained a mean\nDice score of 92.80, with the highest score of 84.92 for enhancing tumor (ET)\namong all models. Attention Gate UNet (AGUNet) yielded a mean Dice score of\n90.38. AXUNet outperformed all models with a mean Dice score of 93.73. It\ndemonstrated superior Dice scores across whole tumor (WT) and tumor core (TC)\nregions, achieving 92.59 for WT, 86.81 for TC, and 84.89 for ET. The\nintegration of the Xception backbone and dot-product self-attention mechanisms\nin AXUNet showcases enhanced performance in capturing spatial and contextual\ninformation. The findings underscore the potential utility of AXUNet in\nfacilitating precise tumor delineation."}
{"id": "2503.20454", "pdf": "https://arxiv.org/pdf/2503.20454", "abs": "https://arxiv.org/abs/2503.20454", "authors": ["Yangqi Feng", "Shing-Ho J. Lin", "Baoyuan Gao", "Xian Wei"], "title": "Lipschitz Constant Meets Condition Number: Learning Robust and Compact Deep Neural Networks", "categories": ["cs.LG", "cs.CV"], "comment": "13 pages, 6 figures", "summary": "Recent research has revealed that high compression of Deep Neural Networks\n(DNNs), e.g., massive pruning of the weight matrix of a DNN, leads to a severe\ndrop in accuracy and susceptibility to adversarial attacks. Integration of\nnetwork pruning into an adversarial training framework has been proposed to\npromote adversarial robustness. It has been observed that a highly pruned\nweight matrix tends to be ill-conditioned, i.e., increasing the condition\nnumber of the weight matrix. This phenomenon aggravates the vulnerability of a\nDNN to input noise. Although a highly pruned weight matrix is considered to be\nable to lower the upper bound of the local Lipschitz constant to tolerate large\ndistortion, the ill-conditionedness of such a weight matrix results in a\nnon-robust DNN model. To overcome this challenge, this work develops novel\njoint constraints to adjust the weight distribution of networks, namely, the\nTransformed Sparse Constraint joint with Condition Number Constraint (TSCNC),\nwhich copes with smoothing distribution and differentiable constraint functions\nto reduce condition number and thus avoid the ill-conditionedness of weight\nmatrices. Furthermore, our theoretical analyses unveil the relevance between\nthe condition number and the local Lipschitz constant of the weight matrix,\nnamely, the sharply increasing condition number becomes the dominant factor\nthat restricts the robustness of over-sparsified models. Extensive experiments\nare conducted on several public datasets, and the results show that the\nproposed constraints significantly improve the robustness of a DNN with high\npruning rates."}
{"id": "2503.20571", "pdf": "https://arxiv.org/pdf/2503.20571", "abs": "https://arxiv.org/abs/2503.20571", "authors": ["Vinzenz Uhr", "Ivan Diaz", "Christian Rummel", "Richard McKinley"], "title": "Exploring Robustness of Cortical Morphometry in the presence of white matter lesions, using Diffusion Models for Lesion Filling", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Cortical thickness measurements from magnetic resonance imaging, an important\nbiomarker in many neurodegenerative and neurological disorders, are derived by\nmany tools from an initial voxel-wise tissue segmentation. White matter (WM)\nhypointensities in T1-weighted imaging, such as those arising from multiple\nsclerosis or small vessel disease, are known to affect the output of brain\nsegmentation methods and therefore bias cortical thickness measurements. These\neffects are well-documented among traditional brain segmentation tools but have\nnot been studied extensively in tools based on deep-learning segmentations,\nwhich promise to be more robust. In this paper, we explore the potential of\ndeep learning to enhance the accuracy and efficiency of cortical thickness\nmeasurement in the presence of WM lesions, using a high-quality lesion filling\nalgorithm leveraging denoising diffusion networks.\n  A pseudo-3D U-Net architecture trained on the OASIS dataset to generate\nsynthetic healthy tissue, conditioned on binary lesion masks derived from the\nMSSEG dataset, allows realistic removal of white matter lesions in multiple\nsclerosis patients. By applying morphometry methods to patient images before\nand after lesion filling, we analysed robustness of global and regional\ncortical thickness measurements in the presence of white matter lesions.\nMethods based on a deep learning-based segmentation of the brain (Fastsurfer,\nDL+DiReCT, ANTsPyNet) exhibited greater robustness than those using classical\nsegmentation methods (Freesurfer, ANTs)."}
{"id": "2503.20595", "pdf": "https://arxiv.org/pdf/2503.20595", "abs": "https://arxiv.org/abs/2503.20595", "authors": ["Trung Duc Ha", "Sidney Bender"], "title": "Diffusion Counterfactuals for Image Regressors", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": "24 Pages, 5 Figures, Accepted at 3rd World Conference on eXplainable\n  Artificial Intelligence (xAI-2025), Code and reproduction instructions\n  available on GitHub, see\n  https://github.com/DevinTDHa/Diffusion-Counterfactuals-for-Image-Regressors", "summary": "Counterfactual explanations have been successfully applied to create human\ninterpretable explanations for various black-box models. They are handy for\ntasks in the image domain, where the quality of the explanations benefits from\nrecent advances in generative models. Although counterfactual explanations have\nbeen widely applied to classification models, their application to regression\ntasks remains underexplored. We present two methods to create counterfactual\nexplanations for image regression tasks using diffusion-based generative models\nto address challenges in sparsity and quality: 1) one based on a Denoising\nDiffusion Probabilistic Model that operates directly in pixel-space and 2)\nanother based on a Diffusion Autoencoder operating in latent space. Both\nproduce realistic, semantic, and smooth counterfactuals on CelebA-HQ and a\nsynthetic data set, providing easily interpretable insights into the\ndecision-making process of the regression model and reveal spurious\ncorrelations. We find that for regression counterfactuals, changes in features\ndepend on the region of the predicted value. Large semantic changes are needed\nfor significant changes in predicted values, making it harder to find sparse\ncounterfactuals than with classifiers. Moreover, pixel space counterfactuals\nare more sparse while latent space counterfactuals are of higher quality and\nallow bigger semantic changes."}
{"id": "2503.20631", "pdf": "https://arxiv.org/pdf/2503.20631", "abs": "https://arxiv.org/abs/2503.20631", "authors": ["Andy Chu", "Rashik Shrestha", "Yu Gu", "Jason N. Gross"], "title": "Robust Flower Cluster Matching Using The Unscented Transform", "categories": ["cs.RO", "cs.CV"], "comment": "*CASE2025 Under Review*", "summary": "Monitoring flowers over time is essential for precision robotic pollination\nin agriculture. To accomplish this, a continuous spatial-temporal observation\nof plant growth can be done using stationary RGB-D cameras. However, image\nregistration becomes a serious challenge due to changes in the visual\nappearance of the plant caused by the pollination process and occlusions from\ngrowth and camera angles. Plants flower in a manner that produces distinct\nclusters on branches. This paper presents a method for matching flower clusters\nusing descriptors generated from RGB-D data and considers allowing for spatial\nuncertainty within the cluster. The proposed approach leverages the Unscented\nTransform to efficiently estimate plant descriptor uncertainty tolerances,\nenabling a robust image-registration process despite temporal changes. The\nUnscented Transform is used to handle the nonlinear transformations by\npropagating the uncertainty of flower positions to determine the variations in\nthe descriptor domain. A Monte Carlo simulation is used to validate the\nUnscented Transform results, confirming our method's effectiveness for flower\ncluster matching. Therefore, it can facilitate improved robotics pollination in\ndynamic environments."}
{"id": "2503.20653", "pdf": "https://arxiv.org/pdf/2503.20653", "abs": "https://arxiv.org/abs/2503.20653", "authors": ["Antoine Schieb", "Bilal Hadjadji", "Daniel Tshokola Mweze", "Natalia Fernanda Valderrama", "Valentin Derangère", "Laurent Arnould", "Sylvain Ladoire", "Alain Lalande", "Louis-Oscar Morel", "Nathan Vinçon"], "title": "UWarp: A Whole Slide Image Registration Pipeline to Characterize Scanner-Induced Local Domain Shift", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Histopathology slide digitization introduces scanner-induced domain shift\nthat can significantly impact computational pathology models based on deep\nlearning methods. In the state-of-the-art, this shift is often characterized at\na broad scale (slide-level or dataset-level) but not patch-level, which limits\nour comprehension of the impact of localized tissue characteristics on the\naccuracy of the deep learning models. To address this challenge, we present a\ndomain shift analysis framework based on UWarp, a novel registration tool\ndesigned to accurately align histological slides scanned under varying\nconditions. UWarp employs a hierarchical registration approach, combining\nglobal affine transformations with fine-grained local corrections to achieve\nrobust tissue patch alignment. We evaluate UWarp using two private datasets,\nCypathLung and BosomShieldBreast, containing whole slide images scanned by\nmultiple devices. Our experiments demonstrate that UWarp outperforms existing\nopen-source registration methods, achieving a median target registration error\n(TRE) of less than 4 pixels (<1 micrometer at 40x magnification) while\nsignificantly reducing computational time. Additionally, we apply UWarp to\ncharacterize scanner-induced local domain shift in the predictions of\nBreast-NEOprAIdict, a deep learning model for breast cancer pathological\nresponse prediction. We find that prediction variability is strongly correlated\nwith tissue density on a given patch. Our findings highlight the importance of\nlocalized domain shift analysis and suggest that UWarp can serve as a valuable\ntool for improving model robustness and domain adaptation strategies in\ncomputational pathology."}
{"id": "2503.20681", "pdf": "https://arxiv.org/pdf/2503.20681", "abs": "https://arxiv.org/abs/2503.20681", "authors": ["Shuaikai Shi", "Qijun Zong"], "title": "Benchmarking Machine Learning Methods for Distributed Acoustic Sensing", "categories": ["eess.AS", "cs.CV", "cs.LG", "cs.SD"], "comment": null, "summary": "Distributed acoustic sensing (DAS) technology represents an innovative\nfiber-optic-based sensing methodology that enables real-time acoustic signal\nmonitoring through the detection of minute perturbations along optical fibers.\nThis sensing approach offers compelling advantages, including extensive\nmeasurement ranges, exceptional spatial resolution, and an expansive dynamic\nmeasurement spectrum.\n  The integration of machine learning (ML) paradigms presents transformative\npotential for DAS technology, encompassing critical domains such as data\naugmentation, sophisticated preprocessing techniques, and advanced acoustic\nevent classification and recognition. By leveraging ML algorithms, DAS systems\ncan transition from traditional data processing methodologies to more automated\nand intelligent analytical frameworks.\n  The computational intelligence afforded by ML-enhanced DAS technologies\nfacilitates unprecedented monitoring capabilities across diverse critical\ninfrastructure sectors. Particularly noteworthy are the technology's\napplications in transportation infrastructure, energy management systems, and\nNatural disaster monitoring frameworks, where the precision of data acquisition\nand the reliability of intelligent decision-making mechanisms are paramount.\n  This research critically examines the comparative performance characteristics\nof classical machine learning methodologies and state-of-the-art deep learning\nmodels in the context of DAS data recognition and interpretation, offering\ncomprehensive insights into the evolving landscape of intelligent sensing\ntechnologies."}
{"id": "2503.20711", "pdf": "https://arxiv.org/pdf/2503.20711", "abs": "https://arxiv.org/abs/2503.20711", "authors": ["Giovanni Compiani", "Ilya Morozov", "Stephan Seiler"], "title": "Demand Estimation with Text and Image Data", "categories": ["econ.GN", "cs.CV", "cs.LG", "q-fin.EC"], "comment": null, "summary": "We propose a demand estimation method that leverages unstructured text and\nimage data to infer substitution patterns. Using pre-trained deep learning\nmodels, we extract embeddings from product images and textual descriptions and\nincorporate them into a random coefficients logit model. This approach enables\nresearchers to estimate demand even when they lack data on product attributes\nor when consumers value hard-to-quantify attributes, such as visual design or\nfunctional benefits. Using data from a choice experiment, we show that our\napproach outperforms standard attribute-based models in counterfactual\npredictions of consumers' second choices. We also apply it across 40 product\ncategories on Amazon.com and consistently find that text and image data help\nidentify close substitutes within each category."}
{"id": "2503.20756", "pdf": "https://arxiv.org/pdf/2503.20756", "abs": "https://arxiv.org/abs/2503.20756", "authors": ["Chenxi Wang", "Jizhan Fang", "Xiang Chen", "Bozhong Tian", "Ziwen Xu", "Huajun Chen", "Ningyu Zhang"], "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit."}
{"id": "2503.20758", "pdf": "https://arxiv.org/pdf/2503.20758", "abs": "https://arxiv.org/abs/2503.20758", "authors": ["Shakiba Rahimiaghdam", "Hande Alemdar"], "title": "MindfulLIME: A Stable Solution for Explanations of Machine Learning Models with Enhanced Localization Precision -- A Medical Image Case Study", "categories": ["cs.LG", "cs.CV", "eess.IV"], "comment": null, "summary": "Ensuring transparency in machine learning decisions is critically important,\nespecially in sensitive sectors such as healthcare, finance, and justice.\nDespite this, some popular explainable algorithms, such as Local Interpretable\nModel-agnostic Explanations (LIME), often produce unstable explanations due to\nthe random generation of perturbed samples. Random perturbation introduces\nsmall changes or noise to modified instances of the original data, leading to\ninconsistent explanations. Even slight variations in the generated samples\nsignificantly affect the explanations provided by such models, undermining\ntrust and hindering the adoption of interpretable models. To address this\nchallenge, we propose MindfulLIME, a novel algorithm that intelligently\ngenerates purposive samples using a graph-based pruning algorithm and\nuncertainty sampling. MindfulLIME substantially improves the consistency of\nvisual explanations compared to random sampling approaches. Our experimental\nevaluation, conducted on a widely recognized chest X-ray dataset, confirms\nMindfulLIME's stability with a 100% success rate in delivering reliable\nexplanations under identical conditions. Additionally, MindfulLIME improves the\nlocalization precision of visual explanations by reducing the distance between\nthe generated explanations and the actual local annotations compared to LIME.\nWe also performed comprehensive experiments considering various segmentation\nalgorithms and sample numbers, focusing on stability, quality, and efficiency.\nThe results demonstrate the outstanding performance of MindfulLIME across\ndifferent segmentation settings, generating fewer high-quality samples within a\nreasonable processing time. By addressing the stability limitations of LIME in\nimage data, MindfulLIME enhances the trustworthiness and interpretability of\nmachine learning models in specific medical imaging applications, a critical\ndomain."}
