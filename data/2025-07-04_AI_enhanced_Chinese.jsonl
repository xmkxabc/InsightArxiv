{"id": "2507.02057", "title": "MGC: A Compiler Framework Exploiting Compositional Blindness in Aligned LLMs for Malware Generation", "authors": ["Lu Yan", "Zhuo Zhang", "Xiangzhe Xu", "Shengwei An", "Guangyu Shen", "Zhou Xuan", "Xuan Chen", "Xiangyu Zhang"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02057v1", "summary": "Large language models (LLMs) have democratized software development, reducing\nthe expertise barrier for programming complex applications. This accessibility\nextends to malicious software development, raising significant security\nconcerns. While LLM providers have implemented alignment mechanisms to prevent\ndirect generation of overtly malicious code, these safeguards predominantly\nevaluate individual prompts in isolation, overlooking a critical vulnerability:\nmalicious operations can be systematically decomposed into benign-appearing\nsub-tasks. In this paper, we introduce the Malware Generation Compiler (MGC), a\nnovel framework that leverages this vulnerability through modular decomposition\nand alignment-evasive generation. MGC employs a specialized Malware Description\nIntermediate Representation (MDIR) to bridge high-level malicious intents and\nbenign-appearing code snippets. Extensive evaluation demonstrates that our\nattack reliably generates functional malware across diverse task specifications\nand categories, outperforming jailbreaking methods by +365.79% and underground\nservices by +78.07% in correctness on three benchmark datasets. Case studies\nfurther show that MGC can reproduce and even enhance 16 real-world malware\nsamples. This work provides critical insights for security researchers by\nexposing the risks of compositional attacks against aligned AI systems.\nDemonstrations are available at\nhttps://sites.google.com/view/malware-generation-compiler.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02057v1", "cate": "cs.CR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "MGC：一种利用对齐LLMs组合盲点生成恶意软件的编译器框架", "tldr": "MGC是一个编译器框架，它通过将恶意任务分解为无害子任务来规避LLM的安全对齐机制，从而高效生成恶意软件。", "motivation": "大型语言模型（LLMs）降低了软件开发门槛，也促进了恶意软件开发，引发了重大安全担忧。尽管LLM供应商已实施对齐机制来阻止恶意代码生成，但这些机制主要孤立评估单个提示，忽视了恶意操作可被系统分解为看似良性的子任务这一关键漏洞。", "method": "本文引入了恶意软件生成编译器（MGC），这是一个新颖的框架，通过模块化分解和规避对齐的生成来利用LLMs的组合盲点。MGC采用专门的恶意软件描述中间表示（MDIR），以连接高层恶意意图和看似无害的代码片段。", "result": "广泛评估表明，MGC的攻击能够在不同任务规范和类别下可靠地生成功能性恶意软件，在三个基准数据集上的正确性方面，比越狱方法高出365.79%，比地下服务高出78.07%。案例研究进一步表明，MGC可以复现甚至增强16个现实世界的恶意软件样本。", "conclusion": "这项工作通过揭示针对对齐AI系统的组合攻击风险，为安全研究人员提供了重要的见解。", "translation": "大型语言模型（LLMs）使软件开发大众化，降低了编程复杂应用程序的专业障碍。这种可访问性延伸到恶意软件开发，引发了重大的安全担忧。虽然LLM供应商已实施对齐机制以防止直接生成公然的恶意代码，但这些安全措施主要孤立地评估单个提示，忽视了一个关键漏洞：恶意操作可以系统地分解为看似良性的子任务。在本文中，我们引入了恶意软件生成编译器（MGC），一个新颖的框架，通过模块化分解和规避对齐的生成来利用这一漏洞。MGC采用专门的恶意软件描述中间表示（MDIR）来连接高层恶意意图和看似良性的代码片段。广泛的评估表明，我们的攻击能够在不同的任务规范和类别下可靠地生成功能性恶意软件，在三个基准数据集上的正确性方面，比越狱方法高出365.79%，比地下服务高出78.07%。案例研究进一步表明，MGC可以复现甚至增强16个现实世界的恶意软件样本。这项工作通过揭示针对对齐AI系统的组合攻击风险，为安全研究人员提供了重要的见解。演示可在https://sites.google.com/view/malware-generation-compiler 获取。", "summary": "本文介绍了MGC（恶意软件生成编译器），这是一个新颖的框架，旨在利用大型语言模型（LLMs）在处理分解的恶意任务时的组合盲点。MGC通过将恶意意图分解为看似无害的子任务，并利用专门的MDIR来规避LLM的安全对齐机制，从而高效生成功能性恶意软件。实验证明，MGC在生成恶意软件的正确性上显著优于现有越狱方法和地下服务，并能复现及增强现实世界的恶意软件样本。这项研究强调了对齐AI系统面临的组合攻击风险，为安全研究提供了重要见解。", "keywords": "LLM安全, 恶意软件生成, 组合攻击, 对齐规避, MGC", "comments": "本文创新性地揭示了LLM安全对齐机制的一个深层漏洞——“组合盲点”，即当恶意任务被分解为看似无害的子任务时，LLM难以识别其最终的恶意意图。MGC框架通过系统化的分解和MDIR设计，提供了一种高效生成恶意软件的攻击方法，其性能远超现有技术。这项工作对于LLM的安全性评估和防御机制的开发具有极其重要的指导意义，提醒研究人员不能仅仅关注单个提示的安全性，而应考虑更复杂的、基于任务分解的攻击向量。"}}
{"id": "2507.02125", "title": "Can Artificial Intelligence solve the blockchain oracle problem? Unpacking the Challenges and Possibilities", "authors": ["Giulio Caldarelli"], "categories": ["cs.CR", "cs.AI", "cs.CY", "cs.GT", "cs.LG", "11, 62, 68, 90, 91", "F.0; F.4; H.4; H.5; I.2"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02125v1", "summary": "The blockchain oracle problem, which refers to the challenge of injecting\nreliable external data into decentralized systems, remains a fundamental\nlimitation to the development of trustless applications. While recent years\nhave seen a proliferation of architectural, cryptographic, and economic\nstrategies to mitigate this issue, no one has yet fully resolved the\nfundamental question of how a blockchain can gain knowledge about the off-chain\nworld. In this position paper, we critically assess the role artificial\nintelligence (AI) can play in tackling the oracle problem. Drawing from both\nacademic literature and practitioner implementations, we examine how AI\ntechniques such as anomaly detection, language-based fact extraction, dynamic\nreputation modeling, and adversarial resistance can enhance oracle systems. We\nobserve that while AI introduces powerful tools for improving data quality,\nsource selection, and system resilience, it cannot eliminate the reliance on\nunverifiable off-chain inputs. Therefore, this study supports the idea that AI\nshould be understood as a complementary layer of inference and filtering within\na broader oracle design, not a substitute for trust assumptions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02125v1", "cate": "cs.CR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "人工智能能否解决区块链预言机问题？挑战与可能性分析", "tldr": "本文探讨了人工智能在解决区块链预言机问题中的作用，认为AI可以增强预言机系统，但不能完全消除对链下输入的信任依赖，应作为补充层而非替代品。", "motivation": "区块链预言机问题是去中心化系统注入可靠外部数据的挑战，严重限制了去信任化应用的发展。尽管已有多种缓解策略，但尚未完全解决区块链如何获取链下世界知识的根本问题。", "method": "本文作为一篇立场论文，批判性地评估了人工智能在解决预言机问题中的作用。通过借鉴学术文献和实践实现，论文探讨了异常检测、基于语言的事实提取、动态声誉建模和对抗性抵抗等AI技术如何增强预言机系统。", "result": "研究发现，人工智能为提高数据质量、源选择和系统弹性引入了强大的工具。然而，它不能消除对不可验证的链下输入的依赖。", "conclusion": "人工智能应被理解为更广泛的预言机设计中的一个补充推理和过滤层，而不是信任假设的替代品。", "translation": "区块链预言机问题指的是将可靠的外部数据注入去中心化系统所面临的挑战，这仍然是去信任化应用发展的一个根本限制。尽管近年来出现了许多架构、密码学和经济策略来缓解这个问题，但没有人能够完全解决区块链如何获取链下世界知识的根本问题。在这篇立场论文中，我们批判性地评估了人工智能（AI）在解决预言机问题中可以发挥的作用。我们借鉴学术文献和实践实现，探讨了异常检测、基于语言的事实提取、动态声誉建模和对抗性抵抗等AI技术如何增强预言机系统。我们观察到，虽然AI为提高数据质量、源选择和系统弹性引入了强大的工具，但它不能消除对不可验证的链下输入的依赖。因此，本研究支持这样的观点：AI应被理解为更广泛的预言机设计中的一个补充推理和过滤层，而不是信任假设的替代品。", "summary": "本文探讨了人工智能在解决区块链预言机问题中的潜力与局限性。预言机问题是去中心化系统获取可靠链下数据的核心挑战。文章评估了异常检测、事实提取、声誉建模等AI技术如何增强预言机系统的数据质量、源选择和弹性。结论指出，AI虽能提供强大工具，但无法完全消除对链下数据信任的依赖，因此应作为预言机设计的补充层，而非信任假设的替代品。", "keywords": "区块链预言机, 人工智能, 去中心化系统, 数据质量, 信任假设", "comments": "这是一篇立场论文，而非基于实证研究的论文。它创新性地将人工智能作为解决区块链预言机问题的潜在方案进行探讨，并清晰地指出了AI的辅助性作用和局限性，即它不能替代对链下数据的信任假设。这对于理解AI在区块链领域中的实际应用边界具有重要指导意义。"}}
{"id": "2507.02177", "title": "ARMOUR US: Android Runtime Zero-permission Sensor Usage Monitoring from User Space", "authors": ["Yan Long", "Jiancong Cui", "Yuqing Yang", "Tobias Alam", "Zhiqiang Lin", "Kevin Fu"], "categories": ["cs.CR", "K.6.5; D.4.6"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02177v1", "summary": "This work investigates how to monitor access to Android zero-permission\nsensors which could cause privacy leakage to users. Moreover, monitoring such\nsensitive access allows security researchers to characterize potential sensor\nabuse patterns. Zero-permission sensors such as accelerometers have become an\nindispensable part of Android devices. The critical information they provide\nhas attracted extensive research investigating how data collectors could\ncapture more sensor data to enable both benign and exploitative applications.\nIn contrast, little work has explored how to enable data providers, such as end\nusers, to understand sensor usage. While existing methods such as static\nanalysis and hooking-based dynamic analysis face challenges of requiring\ncomplicated development chains, rooting privilege, and app-specific reverse\nengineering analysis, our work aims to bridge this gap by developing ARMOUR for\nuser-space runtime monitoring, leveraging the intrinsic sampling rate variation\nand convergence behaviors of Android. ARMOUR enables privacy-aware users to\neasily monitor how third-party apps use sensor data and support security\nresearchers to perform rapid app-agnostic sensor access analysis. Our\nevaluation with 1,448 commercial applications shows the effectiveness of ARMOUR\nin detecting sensor usage in obfuscated code and other conditions, and observes\nsalient sensor abuse patterns such as 50% of apps from seemingly\nsensor-independent categories accessing data of multiple zero-permission\nsensors. We analyze the impact of Android's recent policy changes on\nzero-permission sensors and remaining technical and regulatory problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02177v1", "cate": "cs.CR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "ARMOUR US: 从用户空间监控安卓零权限传感器使用", "tldr": "ARMOUR US 是一种用户空间运行时监控工具，用于检测安卓零权限传感器的使用，解决现有方法需要复杂开发链、root权限和特定应用逆向工程的挑战，并揭示了显著的传感器滥用模式。", "motivation": "本研究旨在解决如何监控安卓零权限传感器的访问问题，这些传感器可能导致用户隐私泄露。虽然已有大量研究关注数据收集者如何获取传感器数据，但很少有工作探讨如何让数据提供者（如终端用户）了解传感器使用情况。现有方法（如静态分析和基于hook的动态分析）面临着需要复杂开发链、root权限和特定应用逆向工程分析的挑战。", "method": "通过开发ARMOUR，利用安卓固有的采样率变化和收敛行为，实现了用户空间运行时监控。ARMOUR旨在弥补现有方法的不足，使隐私意识强的用户能够轻松监控第三方应用如何使用传感器数据，并支持安全研究人员进行快速的应用无关传感器访问分析。", "result": "对1,448个商业应用的评估表明，ARMOUR在检测混淆代码和其他条件下的传感器使用方面是有效的。研究观察到显著的传感器滥用模式，例如50%来自看似与传感器无关类别的应用访问了多个零权限传感器的数据。", "conclusion": "ARMOUR有效检测了安卓零权限传感器的使用，并揭示了普遍的滥用模式。研究还分析了安卓近期政策变化对零权限传感器的影响以及剩余的技术和监管问题。", "translation": "这项工作研究了如何监控安卓零权限传感器的访问，这些传感器可能导致用户隐私泄露。此外，监控此类敏感访问使安全研究人员能够识别潜在的传感器滥用模式。加速度计等零权限传感器已成为安卓设备不可或缺的一部分。它们提供的关键信息吸引了广泛的研究，探讨数据收集者如何捕获更多传感器数据，以支持良性和恶意应用程序。相比之下，很少有工作探索如何使数据提供者（如终端用户）了解传感器使用情况。虽然现有方法，如静态分析和基于hook的动态分析，面临着需要复杂开发链、root权限和特定应用逆向工程分析的挑战，但我们的工作旨在通过开发ARMOUR进行用户空间运行时监控来弥补这一差距，利用安卓固有的采样率变化和收敛行为。ARMOUR使具有隐私意识的用户能够轻松监控第三方应用如何使用传感器数据，并支持安全研究人员执行快速的应用无关传感器访问分析。我们对1,448个商业应用的评估表明，ARMOUR在检测混淆代码和其他条件下的传感器使用方面是有效的，并观察到显著的传感器滥用模式，例如50%来自看似与传感器无关类别的应用访问了多个零权限传感器的数据。我们分析了安卓近期政策变化对零权限传感器的影响以及剩余的技术和监管问题。", "summary": "ARMOUR US是一种新颖的用户空间运行时监控工具，专门用于检测安卓零权限传感器的使用，以解决隐私泄露问题。与需要root权限或复杂逆向工程的现有方法不同，ARMOUR利用安卓固有的传感器采样行为，实现了对1,448个商业应用的有效监控。研究不仅验证了其在检测混淆代码中传感器使用的能力，还揭示了50%看似无关应用滥用多零权限传感器的数据模式，并讨论了安卓政策变化及未来挑战。", "keywords": "安卓, 零权限传感器, 隐私监控, 用户空间, 传感器滥用", "comments": "ARMOUR的创新之处在于其用户空间运行时监控方法，避免了传统方法对root权限和复杂逆向工程的依赖，大大降低了使用门槛。这对于普通用户了解应用行为和安全研究员进行大规模分析都具有重要意义。它揭示的零权限传感器滥用模式，尤其是看似无关应用的大量访问，凸显了当前移动隐私保护的严峻挑战和该研究的重要性。其局限性可能在于对安卓系统更新的适应性以及如何应对未来更复杂的规避技术。"}}
{"id": "2507.02181", "title": "Extended c-differential distinguishers of full 9 and reduced-round Kuznyechik cipher", "authors": ["Pantelimon Stanica", "Ranit Dutta", "Bimal Mandal"], "categories": ["cs.CR", "cs.IT", "math.IT", "94A60, 11T71, 12E20, 68P25, 62P99"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02181v1", "summary": "This paper introduces {\\em truncated inner $c$-differential cryptanalysis}, a\nnovel technique that for the first time enables the practical application of\n$c$-differential uniformity to block ciphers. While Ellingsen et al. (IEEE\nTrans. Inf. Theory, 2020) established the notion of $c$-differential uniformity\nusing $(F(x\\oplus a), cF(x))$, a key challenge remained: multiplication by $c$\ndisrupts the structural properties essential for block cipher analysis,\nparticularly key addition.\n  We resolve this challenge by developing an \\emph{inner} $c$-differential\napproach where multiplication by $c$ affects the input: $(F(cx\\oplus a),\nF(x))$. We prove that the inner $c$-differential uniformity of a function $F$\nequals the outer $c$-differential uniformity of $F^{-1}$, establishing a\nfundamental duality. This modification preserves cipher structure while\nenabling practical cryptanalytic applications.\n  Our main contribution is a comprehensive multi-faceted\nstatistical-computational framework, implementing truncated $c$-differential\nanalysis against the full 9-round Kuznyechik cipher (the inner\n$c$-differentials are immune to the key whitening at the backend). Through\nextensive computational analysis involving millions of differential pairs, we\ndemonstrate statistically significant non-randomness across all tested round\ncounts. For the full 9-round cipher, we identify multiple configurations\ntriggering critical security alerts, with bias ratios reaching $1.7\\times$ and\ncorrected p-values as low as $1.85 \\times 10^{-3}$, suggesting insufficient\nsecurity margin against this new attack vector. This represents the first\npractical distinguisher against the full 9-round Kuznyechik.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02181v1", "cate": "cs.CR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "针对完整9轮和简化轮Kuznyechik密码的扩展c-差分区分器", "tldr": "本文引入了一种新的“截断内c-差分密码分析”技术，首次实现了c-差分均匀性在分组密码上的实际应用，并成功将其应用于完整的9轮Kuznyechik密码，揭示了其安全性不足，并提供了首个实用区分器。", "motivation": "本文旨在解决c-差分均匀性在应用于分组密码时因密钥加法破坏结构特性而面临的挑战，从而首次实现其在分组密码上的实际应用。", "method": "引入了“截断内c-差分密码分析”新方法，通过修改c乘法影响输入为$(F(cx\text{⊕}a), F(x))$，证明了其与逆函数外c-差分均匀性的对偶性，从而保留了密码结构。在此基础上，开发了一个全面的多方面统计计算框架，并将其应用于完整的9轮Kuznyechik密码。", "result": "通过对数百万个差分对的广泛计算分析，在Kuznyechik密码的所有测试轮数中都发现了统计上显著的非随机性。对于完整的9轮密码，识别出多种触发安全警报的配置，偏置比高达1.7倍，校正p值低至$1.85 \times 10^{-3}$。", "conclusion": "Kuznyechik密码对抗这种新的c-差分攻击向量的安全性裕度不足。本研究首次提供了针对完整的9轮Kuznyechik密码的实用区分器。", "translation": "本文引入了“截断内c-差分密码分析”，这是一种新颖的技术，首次使c-差分均匀性能够实际应用于分组密码。虽然Ellingsen等人（IEEE Trans. Inf. Theory, 2020）建立了使用$(F(x\text{⊕}a), cF(x))$的c-差分均匀性概念，但一个关键挑战依然存在：与c相乘会破坏分组密码分析所必需的结构特性，特别是密钥加法。\n我们通过开发一种“内”c-差分方法解决了这一挑战，其中与c相乘影响输入：$(F(cx\text{⊕}a), F(x))$。我们证明了函数F的内c-差分均匀性等于$F^{-1}$的外c-差分均匀性，建立了基本的对偶性。这种修改保留了密码结构，同时实现了实际的密码分析应用。\n我们的主要贡献是一个全面的多方面统计计算框架，对完整的9轮Kuznyechik密码实施截断c-差分分析（内部c-差分对后端密钥白化免疫）。通过涉及数百万个差分对的广泛计算分析，我们证明了在所有测试轮数中都存在统计上显著的非随机性。对于完整的9轮密码，我们识别出多种触发关键安全警报的配置，偏置比达到$1.7\times$，校正p值低至$1.85 \times 10^{-3}$，这表明对抗这种新攻击向量的安全性裕度不足。这代表了针对完整9轮Kuznyechik的第一个实用区分器。", "summary": "本文首次引入了“截断内c-差分密码分析”这一新颖技术，解决了c-差分均匀性在分组密码中实际应用的关键挑战。通过将c乘法作用于输入并证明其与逆函数的对偶性，该方法成功保留了密码结构。研究人员开发了一个统计计算框架，并将其应用于完整的9轮Kuznyechik密码，通过大量计算分析揭示了其统计上的非随机性，并识别出多种具有显著偏置和低p值的配置，从而首次为完整的9轮Kuznyechik提供了实用的区分器，指出其安全性裕度不足。", "keywords": "c-差分密码分析, 分组密码, Kuznyechik, 区分器, 安全性裕度", "comments": "本文通过引入“内c-差分”概念，创新性地解决了c-差分密码分析在分组密码中应用的关键难题，使其首次具备实用性。成功地针对完整的9轮Kuznyechik密码提供了首个实用区分器，证明了该密码在面对这种新型攻击向量时存在安全缺陷，对密码学界评估和设计安全的分组密码具有重要意义。"}}
{"id": "2507.02002", "title": "Dynamic Strategy Adaptation in Multi-Agent Environments with Large Language Models", "authors": ["Shaurya Mallampati", "Rashed Shelim", "Walid Saad", "Naren Ramakrishnan"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "Comments:      20 pages, 11 figures Submitted to GameSec 2025 (under review)", "url": "http://arxiv.org/abs/2507.02002v1", "summary": "Large language models (LLMs) demonstrate strong reasoning abilities across\nmathematical, strategic, and linguistic tasks, yet little is known about how\nwell they reason in dynamic, real-time, multi-agent scenarios, such as\ncollaborative environments in which agents continuously adapt to each other's\nbehavior, as in cooperative gameplay settings. In this paper, we bridge this\ngap by combining LLM-driven agents with strategic reasoning and real-time\nadaptation in cooperative, multi-agent environments grounded in game-theoretic\nprinciples such as belief consistency and Nash equilibrium. The proposed\nframework applies broadly to dynamic scenarios in which agents coordinate,\ncommunicate, and make decisions in response to continuously changing\nconditions. We provide real-time strategy refinement and adaptive feedback\nmechanisms that enable agents to dynamically adjust policies based on immediate\ncontextual interactions, in contrast to previous efforts that evaluate LLM\ncapabilities in static or turn-based settings. Empirical results show that our\nmethod achieves up to a 26\\% improvement in return over PPO baselines in\nhigh-noise environments, while maintaining real-time latency under 1.05\nmilliseconds. Our approach improves collaboration efficiency, task completion\nrates, and flexibility, illustrating that game-theoretic guidance integrated\nwith real-time feedback enhances LLM performance, ultimately fostering more\nresilient and flexible strategic multi-agent systems.", "comment": "20 pages, 11 figures Submitted to GameSec 2025 (under review)", "pdf_url": "http://arxiv.org/pdf/2507.02002v1", "cate": "cs.MA", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "大型语言模型在多智能体环境中的动态策略适应", "tldr": "本研究探索了大型语言模型（LLMs）在动态、实时、多智能体协作环境中的推理能力，提出了一种结合博弈论和实时反馈的框架，显著提高了LLM在复杂环境中的策略适应性和协作效率。", "motivation": "尽管大型语言模型（LLMs）在数学、策略和语言任务中展现出强大的推理能力，但它们在动态、实时、多智能体场景（如协作博弈）中的推理表现却知之甚少。本文旨在弥补这一空白。", "method": "本文通过将LLM驱动的智能体与基于博弈论原则（如信念一致性和纳什均衡）的战略推理和实时适应相结合，构建了一个在合作多智能体环境中的框架。该框架提供了实时策略优化和自适应反馈机制，使智能体能够根据即时上下文交互动态调整策略，这与之前在静态或回合制设置中评估LLM能力的方法不同。", "result": "实证结果表明，在高度噪声环境中，我们的方法相对于PPO基线在回报方面实现了高达26%的提升，同时将实时延迟保持在1.05毫秒以下。我们的方法提高了协作效率、任务完成率和灵活性。", "conclusion": "博弈论指导与实时反馈的结合能够增强大型语言模型的性能，最终促进更具弹性和灵活性的战略多智能体系统。", "translation": "大型语言模型（LLMs）在数学、策略和语言任务中展现出强大的推理能力，然而，它们在动态、实时、多智能体场景中（例如智能体持续适应彼此行为的协作环境，如合作博弈设置）的推理能力却鲜为人知。在本文中，我们通过将LLM驱动的智能体与基于信念一致性和纳什均衡等博弈论原则的战略推理和实时适应相结合，弥补了这一空白。所提出的框架广泛适用于智能体在不断变化的条件下进行协调、沟通和决策的动态场景。我们提供了实时策略优化和自适应反馈机制，使智能体能够根据即时上下文交互动态调整策略，这与之前在静态或回合制设置中评估LLM能力的方法不同。实证结果表明，在高度噪声环境中，我们的方法相对于PPO基线在回报方面实现了高达26%的提升，同时将实时延迟保持在1.05毫秒以下。我们的方法提高了协作效率、任务完成率和灵活性，这表明博弈论指导与实时反馈的结合增强了LLM的性能，最终促进了更具弹性和灵活性的战略多智能体系统。", "summary": "本论文研究了大型语言模型（LLMs）在动态、实时、多智能体环境中的策略适应能力。作者提出了一个将LLM驱动的智能体与博弈论原则相结合的框架，引入了实时策略优化和自适应反馈机制，使智能体能根据即时上下文动态调整行为。实验证明，该方法在噪声环境中相较于PPO基线，将回报率提高了26%，并保持了低延迟，显著提升了协作效率和任务完成率，表明博弈论与实时反馈能有效增强LLM在复杂多智能体系统中的性能。", "keywords": "大型语言模型, 多智能体系统, 动态策略适应, 博弈论, 实时反馈", "comments": "这篇论文的创新点在于将大型语言模型的能力从静态或回合制环境扩展到动态、实时的多智能体协作场景。通过引入博弈论原则和实时反馈机制，该研究成功解决了LLM在复杂交互中策略适应性的挑战。其在回报率提升和低延迟方面的实证结果，展示了该框架在构建更智能、更具弹性的多智能体系统方面的巨大潜力。这对于LLM在实际应用中的部署，尤其是在需要快速决策和协作的领域，具有重要意义。"}}
{"id": "2507.02156", "title": "StorySpace: Technology supporting reflection, expression, and discourse in classroom narrative", "authors": ["Benjamin Watson", "Janet Kim", "Tim McEneany", "Tom Moher", "Claudia Hindo", "Louis Gomez", "Stephen Fransen"], "categories": ["cs.HC", "cs.ET"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02156v1", "summary": "The StorySpace project studies the role new interface technologies might play\nin high school education. With this approach in mind, StorySpace is\nspecifically designed to support and enhance classroom narrative, an already\nwell-established classroom activity. StorySpace strives to achieve this through\nadherence to three design goals. The first is to trigger student reflection and\ninterpretation. The narrative medium created by StorySpace should represent the\ntopic of classroom discussion and learning in all its complexity. In building\ntheir representation, the students will then be confronted with that same\ncomplexity. The medium should also itself be exciting and compelling, making\nclassroom narrative interesting and fun.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02156v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "StorySpace：支持课堂叙事中反思、表达和讨论的技术", "tldr": "StorySpace是一个旨在通过新界面技术支持和增强高中课堂叙事的项目，通过触发学生反思和使叙事媒介有趣来促进学习。", "motivation": "该论文旨在研究新界面技术在高中教育中的作用，特别是如何支持和增强课堂叙事活动。", "method": "StorySpace项目通过设计一个叙事媒介来实现目标，该媒介旨在触发学生的反思和解读，并以其复杂性呈现课堂讨论和学习的主题。此外，该媒介本身应具有吸引力和趣味性，使课堂叙事变得有趣。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "StorySpace项目研究了新界面技术在高中教育中可能扮演的角色。秉持这一理念，StorySpace专门设计用于支持和增强课堂叙事，这是一项已经成熟的课堂活动。StorySpace通过遵循三个设计目标来努力实现这一目标。首先是激发学生的反思和解读。StorySpace创建的叙事媒介应以其所有复杂性来呈现课堂讨论和学习的主题。在构建他们的表达时，学生将面临同样的复杂性。该媒介本身也应令人兴奋和引人入胜，使课堂叙事变得有趣和好玩。", "summary": "StorySpace项目探索新界面技术在高中教育中的应用，旨在通过设计一个引人入胜的叙事媒介来支持和增强课堂叙事。该媒介旨在激发学生的批判性思维和反思，同时以其复杂性呈现学习主题，并确保学习过程的趣味性。", "keywords": "StorySpace, 课堂叙事, 教育技术, 反思, 新界面", "comments": "该论文提出了一个创新的教育技术项目StorySpace，专注于利用新界面技术提升高中课堂叙事体验。其核心创新在于强调通过数字媒介触发学生的深度反思和批判性思维，同时兼顾学习的趣味性。这种将复杂性呈现与趣味性结合的设计理念，对于未来的教育技术发展具有启发意义。局限性在于抽象中未提及具体技术实现细节或实验结果。"}}
{"id": "2507.02068", "title": "How do Software Engineering Candidates Prepare for Technical Interviews?", "authors": ["Brian Bell", "Teresa Thomas", "Sang Won Lee", "Chris Brown"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02068v1", "summary": "To obtain employment, aspiring software engineers must complete technical\ninterviews -- a hiring process which involves candidates writing code while\ncommunicating to an audience. However, the complexities of tech interviews are\ndifficult to prepare for and seldom faced in computing curricula. To this end,\nwe seek to understand how candidates prepare for technical interviews,\ninvestigating the effects of preparation methods and the role of education. We\ndistributed a survey to candidates (n = 131) actively preparing for technical\ninterviews. Our results suggest candidates rarely train in authentic settings\nand courses fail to support preparation efforts -- leading to stress and\nunpreparedness. Based on our findings, we provide implications for stakeholders\nto enhance tech interview preparation for candidates pursuing software\nengineering roles.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02068v1", "cate": "cs.SE", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "软件工程求职者如何准备技术面试？", "tldr": "软件工程求职者在技术面试准备上存在困难，因为缺乏真实的训练环境和课程支持，导致压力和准备不足。", "motivation": "技术面试对于软件工程师求职至关重要，但其复杂性使得准备困难，且现有计算机课程很少涉及。因此，本研究旨在了解求职者如何准备技术面试，并探讨准备方法的效果以及教育在其中的作用。", "method": "研究向131名正在积极准备技术面试的求职者发放了一项调查问卷，以了解他们的准备方式、准备方法的效果以及教育所扮演的角色。", "result": "研究结果表明，求职者很少在真实场景中进行训练，并且现有课程未能有效支持他们的准备工作，这导致了求职者的压力和准备不足。", "conclusion": "基于研究发现，论文为相关利益方提供了建议，以期改进软件工程求职者的技术面试准备。", "translation": "为了获得就业机会，有抱负的软件工程师必须完成技术面试——这是一个涉及求职者在与听众交流的同时编写代码的招聘过程。然而，技术面试的复杂性难以准备，且在计算机课程中很少遇到。为此，我们旨在了解求职者如何准备技术面试，调查准备方法的效果以及教育的作用。我们向131名正在积极准备技术面试的求职者发放了一项调查问卷。我们的结果表明，求职者很少在真实环境中进行训练，并且课程未能支持准备工作——导致压力和准备不足。基于我们的发现，我们为利益相关者提供了启示，以增强追求软件工程职位的求职者的技术面试准备。", "summary": "本研究探讨了软件工程求职者如何准备技术面试，这些面试通常复杂且在计算机课程中很少涉及。通过对131名求职者进行调查，研究发现求职者很少在真实环境中进行训练，且现有课程未能有效支持他们的准备工作，这导致了求职者的压力和准备不足。论文根据这些发现为相关利益方提供了改进技术面试准备的建议。", "keywords": "技术面试, 软件工程, 求职准备, 调查, 教育", "comments": "本论文揭示了软件工程学术准备与行业需求之间的一个关键差距。其基于调查的方法提供了对求职者经验和挑战的宝贵见解。研究结果为教育者和企业提供了实用的建议，以更好地支持未来的工程师。"}}
{"id": "2507.02016", "title": "Effective Explanations for Belief-Desire-Intention Robots: When and What to Explain", "authors": ["Cong Wang", "Roberto Calandra", "Verena Klös"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Paper accepted at IEEE RO-MAN 2025; 6 pages", "url": "http://arxiv.org/abs/2507.02016v1", "summary": "When robots perform complex and context-dependent tasks in our daily lives,\ndeviations from expectations can confuse users. Explanations of the robot's\nreasoning process can help users to understand the robot intentions. However,\nwhen to provide explanations and what they contain are important to avoid user\nannoyance. We have investigated user preferences for explanation demand and\ncontent for a robot that helps with daily cleaning tasks in a kitchen. Our\nresults show that users want explanations in surprising situations and prefer\nconcise explanations that clearly state the intention behind the confusing\naction and the contextual factors that were relevant to this decision. Based on\nthese findings, we propose two algorithms to identify surprising actions and to\nconstruct effective explanations for Belief-Desire-Intention (BDI) robots. Our\nalgorithms can be easily integrated in the BDI reasoning process and pave the\nway for better human-robot interaction with context- and user-specific\nexplanations.", "comment": "Paper accepted at IEEE RO-MAN 2025; 6 pages", "pdf_url": "http://arxiv.org/pdf/2507.02016v1", "cate": "cs.RO", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "信念-欲望-意图机器人有效解释：何时以及解释什么", "tldr": "用户在机器人行为出乎意料时需要解释，且偏好简洁地说明意图和相关上下文的解释。本文提出两种算法，用于BDI机器人识别意外行为并生成有效解释。", "motivation": "当机器人在日常生活中执行复杂且依赖上下文的任务时，其行为与用户预期不符会引起困惑。解释机器人的推理过程有助于用户理解其意图，但需要考虑何时提供解释以及解释包含什么内容，以避免用户烦恼。", "method": "本文通过调查用户对在厨房执行日常清洁任务的机器人的解释需求和内容偏好，得出了用户偏好。基于这些发现，作者提出了两种算法，用于识别意外行为并为信念-欲望-意图（BDI）机器人构建有效解释。", "result": "研究结果表明，用户在意外情况下希望获得解释，并偏好简洁明了地说明令人困惑行为背后的意图以及与决策相关的上下文因素的解释。", "conclusion": "本文提出的算法可以轻松集成到BDI推理过程中，为实现更好的、上下文和用户特定的机器人解释的人机交互铺平道路。", "translation": "当机器人在我们的日常生活中执行复杂且依赖上下文的任务时，与预期不符的行为可能会使用户感到困惑。解释机器人的推理过程可以帮助用户理解机器人的意图。然而，何时提供解释以及解释中包含什么内容对于避免用户烦恼至关重要。我们调查了用户对厨房日常清洁任务机器人的解释需求和内容的偏好。我们的研究结果表明，用户在意外情况下希望获得解释，并且偏好简洁的解释，这些解释清楚地说明了令人困惑行为背后的意图以及与该决策相关的上下文因素。基于这些发现，我们提出了两种算法，用于识别意外行为并为信念-欲望-意图（BDI）机器人构建有效解释。我们的算法可以轻松集成到BDI推理过程中，为实现更好的、上下文和用户特定的解释的人机交互铺平道路。", "summary": "本文研究了用户对日常任务机器人解释的需求和偏好，发现用户在意外情况下降需要解释，并倾向于简洁地说明意图和相关上下文的解释。基于此，作者提出了两种算法，旨在帮助信念-欲望-意图（BDI）机器人识别意外行为并生成有效的、上下文和用户特定的解释，以提升人机交互。", "keywords": "机器人解释, 人机交互, BDI机器人, 用户偏好, 意外检测", "comments": "这篇论文解决了人机交互中一个关键问题：如何有效地向用户解释机器人的行为。其创新之处在于通过用户研究确定了“何时解释”和“解释什么”的具体偏好，并基于这些发现提出了针对BDI机器人的实用算法。这对于提高机器人系统的透明度、可信度以及用户接受度具有重要意义，尤其是在机器人日益融入日常生活的背景下。"}}
{"id": "2507.02122", "title": "PAL: Designing Conversational Agents as Scalable, Cooperative Patient Simulators for Palliative-Care Training", "authors": ["Neil K. R. Sehgal", "Hita Kambhamettu", "Allen Chang", "Andrew Zhu", "Lyle Ungar", "Sharath Chandra Guntuku"], "categories": ["cs.HC", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02122v1", "summary": "Effective communication in serious illness and palliative care is essential\nbut often under-taught due to limited access to training resources like\nstandardized patients. We present PAL (Palliative Assisted Learning-bot), a\nconversational system that simulates emotionally nuanced patient interactions\nand delivers structured feedback grounded in an existing empathy-based\nframework. PAL supports text and voice modalities and is designed to scaffold\nclinical skill-building through repeated, low-cost practice. Through a\nmixed-methods study with 17 U.S. medical trainees and clinicians, we explore\nuser engagement with PAL, evaluate usability, and examine design tensions\naround modalities, emotional realism, and feedback delivery. Participants found\nPAL helpful for reflection and skill refinement, though some noted limitations\nin emotional authenticity and the adaptability of feedback. We contribute: (1)\nempirical evidence that large language models can support palliative\ncommunication training; (2) design insights for modality-aware, emotionally\nsensitive simulation tools; and (3) implications for systems that support\nemotional labor, cooperative learning, and AI-augmented training in high-stakes\ncare settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02122v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "PAL：设计可扩展、协作式患者模拟对话代理用于姑息治疗培训", "tldr": "PAL是一个基于大语言模型的对话系统，旨在模拟姑息治疗中的患者互动，提供反馈，以帮助医疗培训生和临床医生练习沟通技巧。", "motivation": "由于标准化患者等培训资源有限，严重疾病和姑息治疗中的有效沟通技能往往得不到充分教授。", "method": "研究团队提出了PAL（Palliative Assisted Learning-bot），一个模拟情感细致患者互动的对话系统，并提供基于现有同理心框架的结构化反馈。PAL支持文本和语音模式，旨在通过重复、低成本的练习来辅助临床技能培养。通过一项针对17名美国医疗培训生和临床医生的混合方法研究，探讨了用户对PAL的参与度，评估了可用性，并检查了模式、情感真实性和反馈交付方面的设计张力。", "result": "参与者发现PAL有助于反思和技能完善，尽管一些人指出了情感真实性和反馈适应性方面的局限性。", "conclusion": "研究贡献包括：(1) 大语言模型可以支持姑息沟通培训的实证证据；(2) 针对模式感知、情感敏感模拟工具的设计见解；(3) 对支持情感劳动、协作学习和高风险护理环境中AI增强培训系统的启示。", "translation": "在严重疾病和姑息治疗中，有效的沟通至关重要，但由于标准化患者等培训资源有限，往往教学不足。我们提出了PAL（Palliative Assisted Learning-bot），一个模拟情感细致患者互动的对话系统，并提供基于现有同理心框架的结构化反馈。PAL支持文本和语音模式，旨在通过重复、低成本的练习来辅助临床技能培养。通过一项针对17名美国医疗培训生和临床医生的混合方法研究，我们探讨了用户对PAL的参与度，评估了可用性，并检查了模式、情感真实性和反馈交付方面的设计张力。参与者发现PAL有助于反思和技能完善，尽管一些人指出了情感真实性和反馈适应性方面的局限性。我们的贡献包括：(1) 大语言模型可以支持姑息沟通培训的实证证据；(2) 针对模式感知、情感敏感模拟工具的设计见解；(3) 对支持情感劳动、协作学习和高风险护理环境中AI增强培训系统的启示。", "summary": "本研究介绍了PAL（Palliative Assisted Learning-bot），一个利用大语言模型设计的对话系统，旨在为姑息治疗提供可扩展、低成本的患者模拟训练。PAL支持文本和语音交互，并提供基于同理心框架的结构化反馈，以帮助医疗培训生和临床医生提升沟通技能。通过一项混合方法研究，发现PAL有助于技能反思和完善，尽管在情感真实性和反馈适应性方面仍有改进空间。该研究证明了LLMs在姑息沟通培训中的潜力，并提供了情感敏感模拟工具的设计见解。", "keywords": "姑息治疗, 对话代理, 患者模拟, 沟通培训, 大语言模型", "comments": "该论文的创新之处在于利用大型语言模型（LLMs）来创建可扩展的、情感细致的患者模拟器，以解决姑息治疗沟通培训中资源有限的问题。其重要性在于为高风险医疗环境中的沟通技能培训提供了一种新的、低成本的解决方案，并为AI在情感劳动和协作学习中的应用提供了实证。局限性可能在于其情感真实性和反馈适应性仍需进一步优化，且研究样本量较小。"}}
{"id": "2507.02067", "title": "Advanced Printed Sensors for Environmental Applications: A Path Towards Sustainable Monitoring Solutions", "authors": ["Nikolaos Papanikolaou", "Doha Touhafi", "Jurgen Vandendriessche", "Danial Karimi", "Sohail Fatimi", "Gianluca Cornetta", "Abdellah Touhafi"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02067v1", "summary": "Printed sensors represent a transformative advancement in sensor technology,\nutilizing innovative printing techniques to create flexible, cost-effective,\nand highly customizable sensing devices. Their versatility allows integration\ninto numerous applications across diverse fields such as monitoring a wide\nrange of environmental factors e.g. air and water quality, soil conditions, and\natmospheric changes among others. These sensors demonstrate high sensitivity\nand accuracy in detecting pollutants, temperature variations, humidity levels,\nand other critical parameters essential for environmental assessment and\nprotection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02067v1", "cate": "cs.AR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "先进印刷传感器在环境应用中的发展：通向可持续监测解决方案的路径", "tldr": "印刷传感器为多样化的环境监测提供了灵活、经济且准确的解决方案，包括空气/水质和土壤条件。", "motivation": "利用先进的印刷传感器进行环境应用，提供可持续、灵活、经济且高度可定制的监测解决方案。", "method": "Not mentioned in abstract", "result": "这些传感器在检测污染物、温度变化、湿度水平以及其他对环境评估和保护至关重要的参数方面表现出高灵敏度和准确性。", "conclusion": "印刷传感器是传感器技术的一项变革性进步，为各种环境参数的可持续监测提供了高灵敏度和准确的解决方案。", "translation": "印刷传感器代表了传感器技术的一项变革性进步，它利用创新的印刷技术来制造柔性、经济且高度可定制的传感设备。它们的通用性使其能够集成到各个领域的众多应用中，例如监测各种环境因素，包括空气和水质、土壤条件以及大气变化等。这些传感器在检测污染物、温度变化、湿度水平以及其他对环境评估和保护至关重要的参数方面表现出高灵敏度和准确性。", "summary": "印刷传感器是传感器技术的一项重大进步，通过创新的印刷技术实现了柔性、经济且可定制的设备。它们在环境监测中具有广泛的通用性，能够准确检测污染物、温度和湿度，这对于环境评估至关重要。", "keywords": "印刷传感器, 环境监测, 柔性传感器, 可持续解决方案, 传感器技术", "comments": "该论文的创新之处在于利用印刷技术制造柔性、经济且可定制的传感器。其重要性在于能够实现各种环境参数的可持续和准确监测。"}}
{"id": "2507.01978", "title": "Recommendation Algorithms on Social Media: Unseen Drivers of Political Opinion", "authors": ["Waseq Billah"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      3 tables in main text, 2 tables in appendix. The article was presented at the Western Political Science Association (WPSA) Annual Meeting held during April 16-19, 2025 at Seattle, WA", "url": "http://arxiv.org/abs/2507.01978v1", "summary": "Social media broadly refers to digital platforms and applications that\nsimulate social interactions online. This study investigates the impact of\nsocial media platforms and their algorithms on political interest among users.\nAs social media usage continues to rise, platforms like Facebook and X\n(formerly Twitter) play increasingly pivotal roles in shaping political\ndiscourse. By employing statistical analyses on data collected from over 3,300\nparticipants, this research identifies significant differences in how various\nsocial media platforms influence political interest. Findings reveal that\nmoderate Facebook users demonstrate decreased political engagement, whereas\neven minimal engagement with X significantly boosts political interest. The\nstudy further identifies demographic variations, noting that males, older\nindividuals, Black or African American users, those with higher incomes show\ngreater political interest. The demographic analysis highlights that\nRepublicans are particularly active on social media - potentially influencing\ntheir social media engagement patterns. However, the study acknowledges a\ncrucial limitation - the lack of direct data regarding the content users are\nexposed to which is shaping their social media experiences. Future research\nshould explore these influences and consider additional popular platforms to\nenhance the understanding of social media's political impact. Addressing these\ngaps can provide deeper insights into digital political mobilization, aiding\npolicymakers, educators, and platform designers in fostering healthier\ndemocratic engagement.", "comment": "3 tables in main text, 2 tables in appendix. The article was\n  presented at the Western Political Science Association (WPSA) Annual Meeting\n  held during April 16-19, 2025 at Seattle, WA", "pdf_url": "http://arxiv.org/pdf/2507.01978v1", "cate": "cs.SI", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "社交媒体上的推荐算法：政治观点背后看不见的驱动力", "tldr": "本研究调查了社交媒体平台及其算法对用户政治兴趣的影响，发现不同平台影响不同，例如Facebook用户政治参与度下降，而X用户政治兴趣显著提升。", "motivation": "随着社交媒体使用量持续增长，平台在塑造政治话语中扮演着关键角色，因此本研究旨在探究社交媒体平台及其算法对用户政治兴趣的影响。", "method": "通过对超过3,300名参与者收集的数据进行统计分析。", "result": "研究发现不同社交媒体平台对政治兴趣的影响存在显著差异：适度使用Facebook的用户政治参与度降低，而即使少量使用X也能显著提升政治兴趣。人口统计学分析显示，男性、老年人、非裔美国用户和高收入人群表现出更高的政治兴趣，且共和党人在社交媒体上尤其活跃。", "conclusion": "本研究揭示了社交媒体平台及其算法对政治兴趣的复杂影响，并指出缺乏用户接触内容数据的局限性。未来的研究应探索这些影响，以加深对数字政治动员的理解，从而帮助政策制定者、教育工作者和平台设计者促进健康的民主参与。", "translation": "社交媒体广义上指模拟在线社交互动的数字平台和应用程序。本研究调查了社交媒体平台及其算法对用户政治兴趣的影响。随着社交媒体使用量持续增长，Facebook和X（前身为Twitter）等平台在塑造政治话语方面发挥着越来越关键的作用。通过对从3,300多名参与者收集的数据进行统计分析，本研究发现了不同社交媒体平台影响政治兴趣的显著差异。研究结果显示，适度使用Facebook的用户政治参与度下降，而即使少量使用X也能显著提升政治兴趣。研究进一步确定了人口统计学差异，指出男性、老年人、黑人或非裔美国用户以及收入较高的人表现出更大的政治兴趣。人口统计学分析强调，共和党人在社交媒体上尤其活跃——这可能影响他们的社交媒体参与模式。然而，本研究承认一个关键局限性——缺乏关于用户接触到的内容（这些内容正在塑造他们的社交媒体体验）的直接数据。未来的研究应探索这些影响，并考虑其他流行平台，以增强对社交媒体政治影响的理解。解决这些空白可以为数字政治动员提供更深入的见解，从而帮助政策制定者、教育工作者和平台设计者促进健康的民主参与。", "summary": "本研究通过对3,300多名参与者的数据进行统计分析，探讨了社交媒体平台及其算法对用户政治兴趣的影响。研究发现，不同平台的影响存在显著差异，例如Facebook的适度使用导致政治参与度下降，而X的少量使用则显著提升政治兴趣。此外，研究还识别了人口统计学差异，指出男性、老年人、非裔美国用户和高收入人群表现出更高的政治兴趣，且共和党人更活跃。研究强调了缺乏用户内容接触数据的局限性，并建议未来研究深入探索这些影响，以更好地理解数字政治动员。", "keywords": "社交媒体, 推荐算法, 政治兴趣, 数字动员, 平台影响", "comments": "本研究创新性地探讨了社交媒体平台（特别是推荐算法）对政治兴趣的“看不见的驱动力”，通过对比不同平台（Facebook vs. X）的影响揭示了细致的差异。其重要性在于揭示了社交媒体在塑造政治话语和公民参与方面的深远影响。然而，研究明确指出了一个关键局限性，即缺乏用户实际接触内容的数据，这限制了对算法如何具体影响用户体验和政治兴趣的深入机制理解。未来的研究若能克服这一数据障碍，将能提供更全面的见解。"}}
{"id": "2507.01976", "title": "A Comprehensive Survey on Network Traffic Synthesis: From Statistical Models to Deep Learning", "authors": ["Nirhoshan Sivaroopan", "Kaushitha Silva", "Chamara Madarasingha", "Thilini Dahanayaka", "Guillaume Jourjon", "Anura Jayasumana", "Kanchana Thilakarathna"], "categories": ["cs.NI", "cs.LG"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01976v1", "summary": "Synthetic network traffic generation has emerged as a promising alternative\nfor various data-driven applications in the networking domain. It enables the\ncreation of synthetic data that preserves real-world characteristics while\naddressing key challenges such as data scarcity, privacy concerns, and purity\nconstraints associated with real data. In this survey, we provide a\ncomprehensive review of synthetic network traffic generation approaches,\ncovering essential aspects such as data types, generation models, and\nevaluation methods. With the rapid advancements in AI and machine learning, we\nfocus particularly on deep learning-based techniques while also providing a\ndetailed discussion of statistical methods and their extensions, including\ncommercially available tools. Furthermore, we highlight open challenges in this\ndomain and discuss potential future directions for further research and\ndevelopment. This survey serves as a foundational resource for researchers and\npractitioners, offering a structured analysis of existing methods, challenges,\nand opportunities in synthetic network traffic generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01976v1", "cate": "cs.NI", "date": "2025-06-23", "updated": "2025-06-23", "AI": {"title_translation": "网络流量合成的综合调查：从统计模型到深度学习", "tldr": "本调查全面回顾了合成网络流量生成方法，重点关注深度学习技术，同时涵盖统计模型，并讨论了挑战和未来方向。", "motivation": "合成网络流量生成是网络领域数据驱动应用的一种有前景的替代方案，旨在解决真实数据的数据稀缺性、隐私问题和纯度约束等关键挑战。", "method": "本文是一项综合性调查，全面回顾了合成网络流量生成方法，涵盖数据类型、生成模型（包括统计方法和基于深度学习的技术）和评估方法，并讨论了该领域的开放挑战和未来研究方向。", "result": "该调查对合成网络流量生成中的现有方法、挑战和机遇进行了结构化分析，为研究人员和从业者提供了基础资源。", "conclusion": "合成网络流量生成对网络应用至关重要。本调查强调了深度学习技术的重要性，认可了统计方法，并指出了开放挑战和未来的研究方向。", "translation": "合成网络流量生成已成为网络领域各种数据驱动应用的一种有前景的替代方案。它能够创建保留真实世界特征的合成数据，同时解决与真实数据相关的数据稀缺、隐私问题和纯度约束等关键挑战。在这项调查中，我们对合成网络流量生成方法进行了全面回顾，涵盖了数据类型、生成模型和评估方法等基本方面。随着人工智能和机器学习的快速发展，我们特别关注基于深度学习的技术，同时还详细讨论了统计方法及其扩展，包括商业可用工具。此外，我们强调了该领域的开放挑战，并讨论了未来研究和开发的潜在方向。这项调查为研究人员和从业者提供了基础资源，对合成网络流量生成中的现有方法、挑战和机遇进行了结构化分析。", "summary": "本文对合成网络流量生成进行了全面调查，这是一项对数据驱动网络应用至关重要的技术，旨在克服真实数据的局限性。它彻底回顾了各种方法，强调了深度学习方法以及统计模型，并涵盖了数据类型、生成模型和评估方法。该调查还指出了当前的挑战和未来的研究方向，旨在成为该领域的基础资源。", "keywords": "网络流量合成, 深度学习, 统计模型, 数据生成, 综述", "comments": "这项调查非常重要，因为它整合了新兴关键领域（合成网络流量生成）的知识，提供了从传统统计方法到前沿深度学习技术的结构化概述，对研究人员和从业者都具有宝贵的价值。其对开放挑战和未来方向的识别尤其有用。"}}
{"id": "2507.02124", "title": "SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan", "authors": ["Fumikazu Konishi"], "categories": ["cs.DC", "cs.NI", "C.5.5; B.8.2"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      13 pages, 2 Figures, 10 tables", "url": "http://arxiv.org/abs/2507.02124v1", "summary": "SAKURAONE is a managed high performance computing (HPC) cluster developed and\noperated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU\nPHY'' configuration of bare-metal GPU servers and is designed as a cluster\ncomputing resource optimized for advanced workloads, including large language\nmodel (LLM) training.\n  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked\n\\textbf{49th} in the world based on its High Performance Linpack (HPL) score,\ndemonstrating its global competitiveness. In particular, it is the \\textbf{only\nsystem within the top 100} that employs a fully open networking stack based on\n\\textbf{800~GbE (Gigabit Ethernet)} and the \\textbf{SONiC (Software for Open\nNetworking in the Cloud)} operating system, highlighting the viability of open\nand vendor-neutral technologies in large-scale HPC infrastructure.\n  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL\nbenchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate\nGradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets\nlow-precision workloads representative of AI applications, SAKURAONE delivered\nan impressive 339.86~PFLOP/s using FP8 precision.\n  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100\nGPUs. It is supported by an all-flash Lustre storage subsystem with a total\nphysical capacity of 2~petabytes, providing high-throughput and low-latency\ndata access. Internode communication is enabled by a full-bisection bandwidth\ninterconnect based on a Rail-Optimized topology, where the Leaf and Spine\nlayers are interconnected via 800~GbE links. This topology, in combination with\nRoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless\ndata transfers and mitigates communication bottlenecks in large-scale parallel\nworkloads.", "comment": "13 pages, 2 Figures, 10 tables", "pdf_url": "http://arxiv.org/pdf/2507.02124v1", "cate": "cs.DC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "SAKURAONE：通过日本私营部门HPC投资赋能透明开放的AI平台", "tldr": "SAKURAONE是一个由日本私营部门投资的HPC集群，在TOP500排名第49位，其独特的800GbE全开放网络堆栈展示了开放技术在大型HPC中的可行性，并为AI工作负载提供了卓越性能。", "motivation": "旨在通过私营部门的HPC投资，赋能透明和开放的AI平台。", "method": "SAKURAONE是一个托管式高性能计算（HPC）集群，基于“KOKARYOKU PHY”配置的裸金属GPU服务器，优化用于包括大型语言模型（LLM）训练在内的高级工作负载。它采用全开放网络堆栈，基于800 GbE和SONiC操作系统，包含100个计算节点，每个节点配备8个NVIDIA H100 GPU，配有2PB全闪存Lustre存储子系统，并通过基于Rail-Optimized拓扑和RoCEv2的800 GbE互连实现节点间通信。", "result": "在ISC 2025 TOP500榜单中排名全球第49位。HPL基准测试（Rmax）持续性能达到33.95 PFLOP/s。HPCG基准测试达到396.295 TFLOP/s。HPL-MxP基准测试在FP8精度下达到339.86 PFLOP/s。它是TOP100中唯一采用基于800 GbE和SONiC的全开放网络堆栈的系统。", "conclusion": "SAKURAONE的成功证明了开放和供应商中立技术在大型HPC基础设施中的可行性，并为AI应用提供了强大的计算资源。", "translation": "SAKURAONE是由SAKURA互联网研究中心开发和运营的托管式高性能计算（HPC）集群。它强化了裸金属GPU服务器的“KOKARYOKU PHY”配置，并被设计为针对包括大型语言模型（LLM）训练在内的高级工作负载进行优化的集群计算资源。\n在ISC 2025 TOP500榜单中，SAKURAONE凭借其高性能Linpack（HPL）得分排名全球第49位，展示了其全球竞争力。特别是，它是前100名中唯一采用基于800 GbE（千兆以太网）和SONiC（云端开放网络软件）操作系统的全开放网络堆栈的系统，突显了开放和供应商中立技术在大型HPC基础设施中的可行性。\nSAKURAONE在HPL基准测试（Rmax）中实现了33.95 PFLOP/s的持续性能，在高性能共轭梯度（HPCG）基准测试中实现了396.295 TFLOP/s。对于针对代表AI应用的低精度工作负载的HPL-MxP基准测试，SAKURAONE使用FP8精度提供了令人印象深刻的339.86 PFLOP/s的性能。\n该系统包含100个计算节点，每个节点配备8个NVIDIA H100 GPU。它由一个总物理容量为2拍字节的全闪存Lustre存储子系统提供支持，提供高吞吐量和低延迟的数据访问。节点间通信通过基于Rail-Optimized拓扑的全二分带宽互连实现，其中Leaf和Spine层通过800 GbE链路互连。这种拓扑结构与RoCEv2（基于融合以太网的RDMA版本2）相结合，实现了高速、无损的数据传输，并缓解了大规模并行工作负载中的通信瓶颈。", "summary": "SAKURAONE是由SAKURA互联网研究中心开发和运营的HPC集群，旨在通过私营部门投资支持透明开放的AI平台。该系统在ISC 2025 TOP500中排名第49位，其独特之处在于作为前100名中唯一采用800 GbE和SONiC全开放网络堆栈的系统，展示了开放技术在大型HPC中的潜力。它为LLM训练等高级AI工作负载提供了卓越性能，HPL Rmax达到33.95 PFLOP/s，HPL-MxP FP8达到339.86 PFLOP/s。系统由100个配备NVIDIA H100 GPU的节点、2PB全闪存存储和800 GbE/RoCEv2互连组成，确保高效的数据传输和通信。", "keywords": "高性能计算, AI平台, SAKURAONE, 开放网络, 大语言模型", "comments": "SAKURAONE的创新之处在于其在大型HPC集群中成功部署了完全开放的800 GbE和SONiC网络堆栈，这挑战了传统上由少数供应商主导的HPC网络解决方案。这不仅降低了成本，也提高了透明度和灵活性，为未来大规模AI基础设施的构建提供了有价值的参考。其在全球TOP500中的高排名及其在AI工作负载上的卓越性能，进一步证明了这种开放架构的实用性和竞争力。"}}
{"id": "2507.02183", "title": "Computer Science Education in the Age of Generative AI", "authors": ["Russell Beale"], "categories": ["cs.CY", "cs.HC", "H.5.0; K.3.1; K.3.2"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02183v1", "summary": "Generative AI tools - most notably large language models (LLMs) like ChatGPT\nand Codex - are rapidly revolutionizing computer science education. These tools\ncan generate, debug, and explain code, thereby transforming the landscape of\nprogramming instruction. This paper examines the profound opportunities that AI\noffers for enhancing computer science education in general, from coding\nassistance to fostering innovative pedagogical practices and streamlining\nassessments. At the same time, it highlights challenges including academic\nintegrity concerns, the risk of over-reliance on AI, and difficulties in\nverifying originality. We discuss what computer science educators should teach\nin the AI era, how to best integrate these technologies into curricula, and the\nbest practices for assessing student learning in an environment where AI can\ngenerate code, prototypes and user feedback. Finally, we propose a set of\npolicy recommendations designed to harness the potential of generative AI while\npreserving the integrity and rigour of computer science education. Empirical\ndata and emerging studies are used throughout to support our arguments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02183v1", "cate": "cs.CY", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "生成式AI时代的计算机科学教育", "tldr": "本文探讨了生成式AI工具，特别是大型语言模型，在计算机科学教育中带来的机遇与挑战，并提出了教学内容、整合方法、评估实践和政策建议。", "motivation": "生成式AI工具（如LLMs）正在迅速革新计算机科学教育，它们能够生成、调试和解释代码，改变了编程教学的格局。本文旨在探讨AI对计算机科学教育的深刻机遇、挑战以及如何应对。", "method": "本文通过审视生成式AI工具在计算机科学教育中的应用，讨论了其带来的机遇（如编码辅助、创新教学实践、简化评估）和挑战（如学术诚信、过度依赖、原创性验证）。文章还讨论了教师应教什么、如何整合技术以及如何评估学生学习，并提出了政策建议。文中使用了经验数据和新兴研究来支持论点。", "result": "生成式AI工具为计算机科学教育带来了增强编码辅助、促进创新教学实践和简化评估的巨大机遇。同时，也带来了学术诚信、过度依赖AI和验证原创性的挑战。文章讨论了在AI时代计算机科学教育应教的内容、如何将这些技术整合到课程中，以及在AI可以生成代码的环境中评估学生学习的最佳实践。", "conclusion": "为了利用生成式AI的潜力并同时保持计算机科学教育的完整性和严谨性，需要制定一套政策建议，并重新思考教学内容、课程整合和学生评估的最佳实践。", "translation": "生成式AI工具——最著名的是像ChatGPT和Codex这样的大型语言模型（LLMs）——正在迅速革新计算机科学教育。这些工具能够生成、调试和解释代码，从而改变了编程教学的格局。本文探讨了AI在总体上提升计算机科学教育所提供的深刻机遇，从编码辅助到促进创新教学实践和简化评估。同时，它也强调了挑战，包括学术诚信问题、过度依赖AI的风险以及验证原创性的困难。我们讨论了在AI时代计算机科学教育工作者应该教授什么，如何最好地将这些技术整合到课程中，以及在AI可以生成代码、原型和用户反馈的环境中评估学生学习的最佳实践。最后，我们提出了一套旨在利用生成式AI潜力同时保持计算机科学教育的完整性和严谨性的政策建议。本文全程使用经验数据和新兴研究来支持我们的论点。", "summary": "本文探讨了生成式AI工具（如LLMs）对计算机科学教育的深远影响，分析了其带来的机遇（如编码辅助、教学创新）和挑战（如学术诚信、过度依赖）。文章讨论了在AI时代计算机科学教育的教学内容、技术整合和评估方法，并提出了一系列旨在利用AI潜力并维护教育质量的政策建议，其论点得到了经验数据和新兴研究的支持。", "keywords": "生成式AI, 计算机科学教育, 大型语言模型, 教学实践, 学术诚信", "comments": "本文抓住了当前计算机科学教育领域最紧迫和热门的话题——生成式AI的影响。其创新之处在于不仅指出AI带来的机遇，更深入探讨了学术诚信、过度依赖等现实挑战，并试图提供全面的解决方案，包括教学内容、课程整合、评估方法和政策建议，这对于教育工作者和政策制定者都具有重要的指导意义。"}}
{"id": "2507.02005", "title": "Discovery of Fatigue Strength Models via Feature Engineering and automated eXplainable Machine Learning applied to the welded Transverse Stiffener", "authors": ["Michael A. Kraus", "Helen Bartsch"], "categories": ["cs.CE", "cs.AI"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02005v1", "summary": "This research introduces a unified approach combining Automated Machine\nLearning (AutoML) with Explainable Artificial Intelligence (XAI) to predict\nfatigue strength in welded transverse stiffener details. It integrates\nexpert-driven feature engineering with algorithmic feature creation to enhance\naccuracy and explainability.\n  Based on the extensive fatigue test database regression models - gradient\nboosting, random forests, and neural networks - were trained using AutoML under\nthree feature schemes: domain-informed, algorithmic, and combined. This allowed\na systematic comparison of expert-based versus automated feature selection.\n  Ensemble methods (e.g. CatBoost, LightGBM) delivered top performance. The\ndomain-informed model $\\mathcal M_2$ achieved the best balance: test RMSE\n$\\approx$ 30.6 MPa and $R^2 \\approx 0.780% over the full $\\Delta\n\\sigma_{c,50\\%}$ range, and RMSE $\\approx$ 13.4 MPa and $R^2 \\approx 0.527%\nwithin the engineering-relevant 0 - 150 MPa domain. The denser-feature model\n($\\mathcal M_3$) showed minor gains during training but poorer generalization,\nwhile the simpler base-feature model ($\\mathcal M_1$) performed comparably,\nconfirming the robustness of minimalist designs.\n  XAI methods (SHAP and feature importance) identified stress ratio $R$, stress\nrange $\\Delta \\sigma_i$, yield strength $R_{eH}$, and post-weld treatment (TIG\ndressing vs. as-welded) as dominant predictors. Secondary geometric factors -\nplate width, throat thickness, stiffener height - also significantly affected\nfatigue life.\n  This framework demonstrates that integrating AutoML with XAI yields accurate,\ninterpretable, and robust fatigue strength models for welded steel structures.\nIt bridges data-driven modeling with engineering validation, enabling\nAI-assisted design and assessment. Future work will explore probabilistic\nfatigue life modeling and integration into digital twin environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02005v1", "cate": "cs.CE", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "焊接横向加劲肋疲劳强度模型的特征工程与自动化可解释机器学习发现", "tldr": "本研究结合自动化机器学习（AutoML）和可解释人工智能（XAI），为焊接横向加劲肋建立了准确、可解释且鲁棒的疲劳强度预测模型。", "motivation": "本研究旨在通过结合自动化机器学习（AutoML）与可解释人工智能（XAI）来预测焊接横向加劲肋细节的疲劳强度。其动机在于通过整合专家驱动的特征工程和算法特征创建，提高预测模型的准确性和可解释性，并弥合数据驱动建模与工程验证之间的鸿沟。", "method": "本研究提出了一种结合自动化机器学习（AutoML）和可解释人工智能（XAI）的统一方法。利用广泛的疲劳测试数据库，在领域知识、算法生成和组合三种特征方案下，通过AutoML训练了梯度提升、随机森林和神经网络等回归模型。研究中采用了集成方法（如CatBoost、LightGBM）以优化性能，并使用SHAP和特征重要性等XAI方法来识别主导预测因子。", "result": "集成方法（如CatBoost、LightGBM）表现出最佳性能。领域知识模型 ($\\mathcal M_2$) 实现了最佳平衡，在全 $\\Delta \\sigma_{c,50\\%}$ 范围内测试RMSE $\\approx$ 30.6 MPa，$R^2 \\approx 0.780$，在工程相关0-150 MPa范围内RMSE $\\approx$ 13.4 MPa，$R^2 \\approx 0.527$。更密集特征的模型($\\mathcal M_3$)在训练时有小幅提升但泛化能力较差，而更简单的基础特征模型($\\mathcal M_1$)表现相当，证实了极简设计的鲁棒性。XAI方法识别出应力比 $R$、应力范围 $\\Delta \\sigma_i$、屈服强度 $R_{eH}$ 和焊后处理（TIG修整与焊态）为主要预测因子，次要几何因素（板宽、焊喉厚度、加劲肋高度）也显著影响疲劳寿命。", "conclusion": "该框架证明了将自动化机器学习（AutoML）与可解释人工智能（XAI）相结合可以为焊接钢结构提供准确、可解释且鲁棒的疲劳强度模型。它成功弥合了数据驱动建模与工程验证之间的鸿沟，从而实现AI辅助设计和评估。", "translation": "本研究引入了一种结合自动化机器学习（AutoML）和可解释人工智能（XAI）的统一方法，用于预测焊接横向加劲肋细节的疲劳强度。它将专家驱动的特征工程与算法特征创建相结合，以提高准确性和可解释性。\n基于广泛的疲劳测试数据库，在三种特征方案（领域知识、算法生成和组合）下，使用AutoML训练了回归模型——梯度提升、随机森林和神经网络。这允许系统地比较基于专家的方法与自动化特征选择。\n集成方法（例如CatBoost、LightGBM）表现出最佳性能。领域知识模型 $\\mathcal M_2$ 实现了最佳平衡：在完整的 $\\Delta \\sigma_{c,50\\%}$ 范围内，测试RMSE $\\approx$ 30.6 MPa，$R^2 \\approx 0.780$；在工程相关的0-150 MPa范围内，RMSE $\\approx$ 13.4 MPa，$R^2 \\approx 0.527$。密集特征模型（$\\mathcal M_3$）在训练期间显示出微小收益，但泛化能力较差，而更简单的基础特征模型（$\\mathcal M_1$）表现相当，证实了极简设计的鲁棒性。\nXAI方法（SHAP和特征重要性）将应力比 $R$、应力范围 $\\Delta \\sigma_i$、屈服强度 $R_{eH}$ 和焊后处理（TIG修整与焊态）确定为主要预测因子。次要几何因素——板宽、焊喉厚度、加劲肋高度——也显著影响疲劳寿命。\n该框架表明，将AutoML与XAI相结合可以为焊接钢结构提供准确、可解释且鲁棒的疲劳强度模型。它弥合了数据驱动建模与工程验证之间的鸿沟，从而实现AI辅助设计和评估。未来的工作将探索概率疲劳寿命建模并集成到数字孪生环境中。", "summary": "本文提出了一种结合自动化机器学习（AutoML）和可解释人工智能（XAI）的统一框架，用于预测焊接横向加劲肋的疲劳强度。研究利用大规模疲劳测试数据，通过AutoML训练了多种回归模型，并比较了专家驱动和算法生成的特征方案。结果表明，集成方法表现最佳，其中领域知识模型在准确性和鲁棒性之间达到良好平衡。XAI方法成功识别出疲劳强度的关键影响因素，包括应力比、应力范围、屈服强度和焊后处理。该研究验证了AutoML与XAI结合在构建可解释、准确的疲劳强度模型方面的有效性，为AI辅助的工程设计和评估提供了新途径。", "keywords": "疲劳强度, 自动化机器学习, 可解释人工智能, 焊接结构, 特征工程", "comments": "这项研究的创新之处在于其将AutoML与XAI相结合的统一框架，不仅提高了疲劳强度预测的准确性，还增强了模型的可解释性，解决了传统黑盒模型在工程应用中的挑战。通过系统比较不同特征工程方案，并利用XAI识别关键影响因素，为焊接结构的AI辅助设计提供了坚实的基础。其结果证实了在某些情况下，简约模型也能达到与复杂模型相当的性能，这对于实际工程应用具有重要指导意义。"}}
{"id": "2507.02004", "title": "STELLA: Self-Evolving LLM Agent for Biomedical Research", "authors": ["Ruofan Jin", "Zaixi Zhang", "Mengdi Wang", "Le Cong"], "categories": ["cs.AI", "cs.CL", "q-bio.BM"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02004v1", "summary": "The rapid growth of biomedical data, tools, and literature has created a\nfragmented research landscape that outpaces human expertise. While AI agents\noffer a solution, they typically rely on static, manually curated toolsets,\nlimiting their ability to adapt and scale. Here, we introduce STELLA, a\nself-evolving AI agent designed to overcome these limitations. STELLA employs a\nmulti-agent architecture that autonomously improves its own capabilities\nthrough two core mechanisms: an evolving Template Library for reasoning\nstrategies and a dynamic Tool Ocean that expands as a Tool Creation Agent\nautomatically discovers and integrates new bioinformatics tools. This allows\nSTELLA to learn from experience. We demonstrate that STELLA achieves\nstate-of-the-art accuracy on a suite of biomedical benchmarks, scoring\napproximately 26\\% on Humanity's Last Exam: Biomedicine, 54\\% on LAB-Bench:\nDBQA, and 63\\% on LAB-Bench: LitQA, outperforming leading models by up to 6\npercentage points. More importantly, we show that its performance\nsystematically improves with experience; for instance, its accuracy on the\nHumanity's Last Exam benchmark almost doubles with increased trials. STELLA\nrepresents a significant advance towards AI Agent systems that can learn and\ngrow, dynamically scaling their expertise to accelerate the pace of biomedical\ndiscovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02004v1", "cate": "cs.AI", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "STELLA：生物医学研究的自进化LLM智能体", "tldr": "STELLA是一个自进化的AI智能体，通过动态更新推理策略和工具集，在生物医学基准测试上实现了最先进的性能，并能随经验提升。", "motivation": "生物医学数据、工具和文献的快速增长导致研究领域碎片化，超出了人类专业知识的范围。现有AI智能体依赖静态、手动管理的工具集，限制了其适应性和扩展能力。", "method": "STELLA采用多智能体架构，通过两种核心机制自主提升能力：一个不断进化的“模板库”用于推理策略，以及一个动态的“工具海洋”，通过“工具创建智能体”自动发现和整合新的生物信息学工具来扩展。这使得STELLA能够从经验中学习。", "result": "STELLA在生物医学基准测试中取得了最先进的准确性，在“人类的最后一次考试：生物医学”中得分约26%，在“LAB-Bench：DBQA”中得分54%，在“LAB-Bench：LitQA”中得分63%，比领先模型高出最多6个百分点。其性能随经验系统性提升，例如，“人类的最后一次考试”基准测试的准确性在增加试验次数后几乎翻倍。", "conclusion": "STELLA代表了AI智能体系统在学习和成长方面的重大进步，能够动态扩展其专业知识以加速生物医学发现的步伐。", "translation": "生物医学数据、工具和文献的快速增长导致研究领域碎片化，其发展速度超越了人类的专业知识。尽管AI智能体提供了一种解决方案，但它们通常依赖静态、手动管理的工具集，这限制了它们的适应性和扩展能力。在此，我们介绍了STELLA，一个旨在克服这些限制的自进化AI智能体。STELLA采用多智能体架构，通过两种核心机制自主提升自身能力：一个用于推理策略的不断进化的“模板库”，以及一个动态的“工具海洋”，该工具海洋随着“工具创建智能体”自动发现和整合新的生物信息学工具而扩展。这使得STELLA能够从经验中学习。我们证明了STELLA在生物医学基准测试套件上实现了最先进的准确性，在“人类的最后一次考试：生物医学”中得分约26%，在“LAB-Bench：DBQA”中得分54%，在“LAB-Bench：LitQA”中得分63%，比领先模型高出最多6个百分点。更重要的是，我们表明其性能随经验系统性提升；例如，其在“人类的最后一次考试”基准测试上的准确性随着试验次数的增加几乎翻倍。STELLA代表了AI智能体系统在学习和成长方面的重大进步，能够动态扩展其专业知识以加速生物医学发现的步伐。", "summary": "本文介绍了STELLA，一个用于生物医学研究的自进化AI智能体。针对现有AI智能体依赖静态工具集、适应性差的问题，STELLA采用多智能体架构，通过动态更新推理策略的“模板库”和自动扩展工具的“工具海洋”来提升自身能力并从经验中学习。实验证明，STELLA在多个生物医学基准测试中取得了最先进的性能，并能随经验积累持续提升准确性，预示着其在加速生物医学发现方面的重要潜力。", "keywords": "自进化AI智能体, 生物医学研究, LLM, 工具学习, 多智能体系统", "comments": "STELLA的创新之处在于其“自进化”能力，通过动态的模板库和工具海洋机制，使其能够自主学习和适应，克服了传统AI智能体依赖静态工具集的局限性。这种能够从经验中持续学习并提升性能的特性，对于快速发展的生物医学领域尤为重要，有望显著加速新知识的发现和应用。"}}
{"id": "2507.02074", "title": "Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02074v1", "summary": "Crash detection from video feeds is a critical problem in intelligent\ntransportation systems. Recent developments in large language models (LLMs) and\nvision-language models (VLMs) have transformed how we process, reason about,\nand summarize multimodal information. This paper surveys recent methods\nleveraging LLMs for crash detection from video data. We present a structured\ntaxonomy of fusion strategies, summarize key datasets, analyze model\narchitectures, compare performance benchmarks, and discuss ongoing challenges\nand opportunities. Our review provides a foundation for future research in this\nfast-growing intersection of video understanding and foundation models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02074v1", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "视频中碰撞检测的大型语言模型：方法、数据集和挑战综述", "tldr": "综述了利用大型语言模型进行视频碰撞检测的方法、数据集、挑战和机遇。", "motivation": "视频中的碰撞检测是智能交通系统中的一个关键问题。大型语言模型（LLMs）和视觉-语言模型（VLMs）的最新发展改变了多模态信息的处理、推理和总结方式。", "method": "本文综述了利用大型语言模型从视频数据中进行碰撞检测的最新方法。提出了融合策略的结构化分类法，总结了关键数据集，分析了模型架构，比较了性能基准，并讨论了持续的挑战和机遇。", "result": "论文提供了融合策略的结构化分类法，总结了关键数据集，分析了模型架构，比较了性能基准，并讨论了持续的挑战和机遇。", "conclusion": "本综述为视频理解和基础模型这一快速发展的交叉领域中的未来研究奠定了基础。", "translation": "视频中的碰撞检测是智能交通系统中的一个关键问题。大型语言模型（LLMs）和视觉-语言模型（VLMs）的最新发展改变了我们处理、推理和总结多模态信息的方式。本文综述了利用LLMs从视频数据中进行碰撞检测的最新方法。我们提出了融合策略的结构化分类法，总结了关键数据集，分析了模型架构，比较了性能基准，并讨论了持续的挑战和机遇。我们的综述为视频理解和基础模型这一快速发展的交叉领域中的未来研究奠定了基础。", "summary": "本文综述了利用大型语言模型（LLMs）和视觉-语言模型（VLMs）在视频中进行碰撞检测的最新进展。它系统地分类了融合策略，概述了数据集，分析了模型架构和性能，并指出了该领域面临的挑战和未来机遇，旨在为相关研究奠定基础。", "keywords": "大型语言模型, 碰撞检测, 视频理解, 智能交通系统, 综述", "comments": "这篇综述论文的重要性在于它系统地整理了LLMs在视频碰撞检测这一新兴交叉领域的应用，为研究人员提供了全面的视角，有助于识别当前的研究空白和未来发展方向。"}}
{"id": "2507.02078", "title": "Enhancing Power Flow Estimation with Topology-Aware Gated Graph Neural Networks", "authors": ["Shrenik Jadhav", "Birva Sevak", "Srijita Das", "Wencong Su", "Van-Hai Bui"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02078v1", "summary": "Accurate and scalable surrogate models for AC power flow are essential for\nreal-time grid monitoring, contingency analysis, and decision support in\nincreasingly dynamic and inverter-dominated power systems. However, most\nexisting surrogates fall short of practical deployment due to their limited\ncapacity to capture long-range nonlinear dependencies in meshed transmission\nnetworks and their weak enforcement of physical laws. These models often\nrequire extensive hyperparameter tuning, exhibit poor generalization under\ntopology changes or large load swings, and typically do not quantify\nuncertainty or scale well beyond a few hundred buses. To address these\nchallenges, this paper proposes a \\textit{gated graph neural network (GGNN)}\nsurrogate for AC power-flow estimation under topological uncertainty. The model\nis trained across multiple IEEE benchmark networks of varying size and\ncomplexity, each incorporating randomized line contingencies and up to 40\\%\nload variation. To improve robustness and generalization, we explore both\nconventional supervised learning and physics-informed self-supervised training\nstrategies. Comparative evaluations show that the proposed GGNN consistently\noutperforms prior GNN-based surrogates, achieving predictions closely aligned\nwith Newton--Raphson solutions. By embedding operational constraints directly\ninto the architecture and loss function, the model ensures physical consistency\nand delivers a lightweight, accurate, and scalable tool for real-time grid\noperations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02078v1", "cate": "eess.SY", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "拓扑感知门控图神经网络增强潮流估计", "tldr": "本文提出一种拓扑感知的门控图神经网络（GGNN）代理模型，用于在拓扑不确定性下进行交流潮流估计，性能优于现有GNN模型，并确保物理一致性。", "motivation": "现有交流潮流代理模型在捕捉长距离非线性依赖、强制物理定律、泛化能力、不确定性量化和可扩展性方面存在局限性，无法满足实时电网监控和决策支持的需求。", "method": "本文提出一个门控图神经网络（GGNN）代理模型，用于在拓扑不确定性下进行交流潮流估计。模型在不同规模和复杂度的IEEE基准网络上训练，包含随机线路故障和高达40%的负荷变化。为提高鲁棒性和泛化能力，探索了传统的监督学习和物理信息自监督训练策略，并将操作约束直接嵌入模型架构和损失函数中。", "result": "所提出的GGNN模型在性能上始终优于先前的基于GNN的代理模型，其预测结果与牛顿-拉弗森（Newton-Raphson）解高度吻合。", "conclusion": "通过将操作约束直接嵌入模型架构和损失函数，该模型确保了物理一致性，提供了一个轻量级、准确且可扩展的实时电网操作工具。", "translation": "交流潮流的精确和可扩展代理模型对于日益动态化和逆变器主导的电力系统中的实时电网监控、应急分析和决策支持至关重要。然而，大多数现有代理模型由于其捕获网状输电网络中长距离非线性依赖的能力有限，以及对物理定律的弱强制执行，未能实现实际部署。这些模型通常需要大量的超参数调优，在拓扑变化或大负荷波动下表现出较差的泛化能力，并且通常不量化不确定性或难以扩展到数百个母线以上。为了解决这些挑战，本文提出了一种在拓扑不确定性下用于交流潮流估计的门控图神经网络（GGNN）代理模型。该模型在不同规模和复杂度的多个IEEE基准网络上进行训练，每个网络都包含随机线路故障和高达40%的负荷变化。为了提高鲁棒性和泛化能力，我们探索了传统的监督学习和物理信息自监督训练策略。对比评估表明，所提出的GGNN始终优于先前的基于GNN的代理模型，实现了与牛顿-拉弗森解高度一致的预测。通过将操作约束直接嵌入架构和损失函数中，该模型确保了物理一致性，并为实时电网操作提供了一个轻量级、准确且可扩展的工具。", "summary": "本文提出一种基于门控图神经网络（GGNN）的交流潮流估计代理模型，旨在解决现有模型在处理长距离非线性依赖、物理一致性、泛化能力及可扩展性方面的不足。该模型在包含拓扑不确定性和负荷变化的多个IEEE基准网络上进行训练，并结合了监督学习和物理信息自监督策略。实验结果表明，所提出的GGNN模型在预测精度和物理一致性方面均优于现有GNN代理模型，为实时电网操作提供了一个高效、准确且可扩展的解决方案。", "keywords": "潮流估计, 门控图神经网络, 拓扑不确定性, 物理信息学习, 代理模型", "comments": "本文的创新之处在于提出了一种拓扑感知的门控图神经网络（GGNN）模型，有效解决了现有潮流代理模型在处理复杂电网拓扑变化和确保物理一致性方面的挑战。通过结合物理信息自监督训练和将操作约束嵌入模型，显著提升了模型的鲁棒性、泛化能力和预测精度，使其更适用于实际的实时电网操作。"}}
{"id": "2507.02243", "title": "Derivative-Free Optimization-Empowered Wireless Channel Reconfiguration for 6G", "authors": ["Peilan Wang", "Jun Fang", "Xianlong Zeng", "Bin Wang", "Zhi Chen", "Yonina C. Eldar"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      7 pages", "url": "http://arxiv.org/abs/2507.02243v1", "summary": "Reconfigurable antennas, including reconfigurable intelligent surface (RIS),\nmovable antenna (MA), fluid antenna (FA), and other advanced antenna\ntechniques, have been studied extensively in the context of reshaping wireless\npropagation environments for 6G and beyond wireless communications.\nNevertheless, how to reconfigure/optimize the real-time controllable\ncoefficients to achieve a favorable end-to-end wireless channel remains a\nsubstantial challenge, as it usually requires accurate modeling of the complex\ninteraction between the reconfigurable devices and the electromagnetic waves,\nas well as knowledge of implicit channel propagation parameters. In this paper,\nwe introduce a derivative-free optimization (a.k.a., zeroth-order (ZO)\noptimization) technique to directly optimize reconfigurable coefficients to\nshape the wireless end-to-end channel, without the need of channel modeling and\nestimation of the implicit environmental propagation parameters. We present the\nfundamental principles of ZO optimization and discuss its potential advantages\nin wireless channel reconfiguration. Two case studies for RIS and movable\nantenna-enabled single-input single-output (SISO) systems are provided to show\nthe superiority of ZO-based methods as compared to state-of-the-art techniques.\nFinally, we outline promising future research directions and offer concluding\ninsights on derivative-free optimization for reconfigurable antenna\ntechnologies.", "comment": "7 pages", "pdf_url": "http://arxiv.org/pdf/2507.02243v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "面向6G的无导数优化赋能无线信道重构", "tldr": "本文提出一种无导数优化技术，用于直接优化可重构天线（如RIS、MA）的系数以重构无线信道，无需复杂的信道建模和参数估计，并在案例研究中证明其优越性。", "motivation": "在6G及未来无线通信中，如何优化可重构天线（如RIS、MA、FA）的实时可控系数以实现有利的端到端无线信道是一个巨大挑战，因为它通常需要对可重构设备与电磁波之间的复杂交互进行精确建模，并了解隐式信道传播参数。", "method": "本文引入了一种无导数优化（即零阶优化）技术，用于直接优化可重构系数以塑造无线端到端信道，而无需信道建模和估计隐式环境参数。文章还介绍了零阶优化的基本原理，并讨论了其在无线信道重构中的潜在优势，并通过RIS和可移动天线支持的SISO系统案例研究进行了验证。", "result": "两个针对RIS和可移动天线支持的单输入单输出（SISO）系统的案例研究表明，与现有技术相比，基于零阶优化（ZO）的方法具有优越性。", "conclusion": "本文概述了无导数优化在可重构天线技术中未来有前景的研究方向，并提供了总结性见解。", "translation": "可重构天线，包括可重构智能表面（RIS）、可移动天线（MA）、流体天线（FA）和其他先进天线技术，在6G及未来无线通信中重塑无线传播环境的背景下得到了广泛研究。然而，如何重构/优化实时可控系数以实现有利的端到端无线信道仍然是一个重大挑战，因为它通常需要对可重构设备与电磁波之间的复杂交互进行精确建模，以及了解隐式信道传播参数。在本文中，我们引入了一种无导数优化（又称零阶（ZO）优化）技术，可以直接优化可重构系数以塑造无线端到端信道，而无需信道建模和估计隐式环境传播参数。我们介绍了零阶优化的基本原理，并讨论了其在无线信道重构中的潜在优势。提供了两个针对RIS和可移动天线支持的单输入单输出（SISO）系统的案例研究，以显示基于零阶优化（ZO）的方法与现有技术相比的优越性。最后，我们概述了未来有前景的研究方向，并提供了关于无导数优化在可重构天线技术中的总结性见解。", "summary": "本文针对6G及未来无线通信中可重构天线信道重构的挑战，提出了一种无导数优化（零阶优化）技术。该方法无需复杂的信道建模和环境参数估计，即可直接优化可重构系数以塑造端到端无线信道。通过RIS和可移动天线SISO系统的案例研究，验证了其相较于现有技术的优越性，并展望了未来的研究方向。", "keywords": "可重构天线, 无导数优化, 6G, 信道重构, 零阶优化", "comments": "本文的创新点在于引入无导数优化方法来解决可重构天线信道重构中复杂的信道建模和参数估计问题，提供了一种更直接、高效的优化途径。这对于推动6G及未来无线通信中可重构天线技术的实际应用具有重要意义。"}}
{"id": "2507.02115", "title": "Pronunciation Editing for Finnish Speech using Phonetic Posteriorgrams", "authors": ["Zirui Li", "Lauri Juvela", "Mikko Kurimo"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      5 pages; 1 figure; Accepted to Speech Synthesis Workshop 2025 (SSW13)", "url": "http://arxiv.org/abs/2507.02115v1", "summary": "Synthesizing second-language (L2) speech is potentially highly valued for L2\nlanguage learning experience and feedback. However, due to the lack of L2\nspeech synthesis datasets, it is difficult to synthesize L2 speech for\nlow-resourced languages. In this paper, we provide a practical solution for\nediting native speech to approximate L2 speech and present PPG2Speech, a\ndiffusion-based multispeaker Phonetic-Posteriorgrams-to-Speech model that is\ncapable of editing a single phoneme without text alignment. We use Matcha-TTS's\nflow-matching decoder as the backbone, transforming Phonetic Posteriorgrams\n(PPGs) to mel-spectrograms conditioned on external speaker embeddings and\npitch. PPG2Speech strengthens the Matcha-TTS's flow-matching decoder with\nClassifier-free Guidance (CFG) and Sway Sampling. We also propose a new\ntask-specific objective evaluation metric, the Phonetic Aligned Consistency\n(PAC), between the edited PPGs and the PPGs extracted from the synthetic speech\nfor editing effects. We validate the effectiveness of our method on Finnish, a\nlow-resourced, nearly phonetic language, using approximately 60 hours of data.\nWe conduct objective and subjective evaluations of our approach to compare its\nnaturalness, speaker similarity, and editing effectiveness with TTS-based\nediting. Our source code is published at\nhttps://github.com/aalto-speech/PPG2Speech.", "comment": "5 pages; 1 figure; Accepted to Speech Synthesis Workshop 2025 (SSW13)", "pdf_url": "http://arxiv.org/pdf/2507.02115v1", "cate": "eess.AS", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "芬兰语语音发音编辑基于语音后验概率图", "tldr": "本文提出PPG2Speech模型，通过编辑母语语音来近似第二语言（L2）语音，解决了低资源语言L2语音合成的难题，并在芬兰语上验证了其有效性。", "motivation": "针对低资源语言，由于缺乏第二语言（L2）语音合成数据集，难以合成L2语音，但L2语音合成对L2语言学习和反馈具有潜在的高价值。", "method": "提出PPG2Speech模型，这是一种基于扩散的多说话人语音后验概率图到语音模型，能够编辑单个音素而无需文本对齐。该模型使用Matcha-TTS的流匹配解码器作为骨干，将语音后验概率图（PPG）转换为梅尔频谱图，并通过分类器自由指导（CFG）和Sway采样进行增强。此外，还提出了一种新的任务特定客观评估指标——语音对齐一致性（PAC），用于衡量编辑效果。", "result": "该方法在芬兰语（一种低资源、近音素语言）上进行了验证，使用了约60小时的数据。客观和主观评估结果显示，该方法在自然度、说话人相似性和编辑效果方面均优于基于TTS的编辑方法。", "conclusion": "该研究提出了一种有效的方法，通过编辑母语语音来近似第二语言语音，解决了低资源语言L2语音合成的难题，并证明了其在芬兰语上的有效性。", "translation": "合成第二语言（L2）语音对于L2语言学习体验和反馈具有潜在的高价值。然而，由于缺乏L2语音合成数据集，为低资源语言合成L2语音非常困难。在本文中，我们提供了一种实用的解决方案，用于编辑母语语音以近似L2语音，并提出了PPG2Speech模型，这是一种基于扩散的多说话人语音后验概率图到语音模型，能够无需文本对齐地编辑单个音素。我们使用Matcha-TTS的流匹配解码器作为骨干，在外部说话人嵌入和音高的条件下，将语音后验概率图（PPG）转换为梅尔频谱图。PPG2Speech通过分类器自由指导（CFG）和Sway采样增强了Matcha-TTS的流匹配解码器。我们还提出了一种新的任务特定客观评估指标——语音对齐一致性（PAC），用于衡量编辑后的PPG与从合成语音中提取的PPG之间的编辑效果。我们使用大约60小时的数据，在芬兰语（一种低资源、近音素语言）上验证了我们方法的有效性。我们对我们的方法进行了客观和主观评估，以比较其自然度、说话人相似性和编辑效果与基于TTS的编辑方法。我们的源代码已发布在https://github.com/aalto-speech/PPG2Speech。", "summary": "本文针对低资源语言第二语言（L2）语音合成数据集缺乏的挑战，提出了一种实用的解决方案：通过编辑母语语音来近似L2语音。研究引入了PPG2Speech模型，这是一种基于扩散的、多说话人语音后验概率图到语音模型，其特点是无需文本对齐即可编辑单个音素。该模型以Matcha-TTS的流匹配解码器为基础，并结合了分类器自由指导（CFG）和Sway采样技术。为评估编辑效果，文章还提出了一种新的客观评价指标——语音对齐一致性（PAC）。实验在芬兰语数据集上进行，结果表明该方法在自然度、说话人相似性及编辑效果上均优于传统的TTS编辑方法。", "keywords": "语音编辑, 第二语言合成, 语音后验概率图, 扩散模型, 芬兰语", "comments": "该论文提出了一种创新的方法，通过编辑母语语音来生成第二语言（L2）语音，有效解决了低资源语言L2语音合成数据稀缺的问题。PPG2Speech模型利用扩散模型和语音后验概率图，实现了无需文本对齐的单音素编辑，这对于发音纠正和语言学习具有重要意义。引入的PAC评估指标也为该特定任务提供了量化评估的工具。"}}
{"id": "2507.02003", "title": "Unsupervised Cardiac Video Translation Via Motion Feature Guided Diffusion Model", "authors": ["Swakshar Deb", "Nian Wu", "Frederick H. Epstein", "Miaomiao Zhang"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02003v1", "summary": "This paper presents a novel motion feature guided diffusion model for\nunpaired video-to-video translation (MFD-V2V), designed to synthesize dynamic,\nhigh-contrast cine cardiac magnetic resonance (CMR) from lower-contrast,\nartifact-prone displacement encoding with stimulated echoes (DENSE) CMR\nsequences. To achieve this, we first introduce a Latent Temporal\nMulti-Attention (LTMA) registration network that effectively learns more\naccurate and consistent cardiac motions from cine CMR image videos. A\nmulti-level motion feature guided diffusion model, equipped with a specialized\nSpatio-Temporal Motion Encoder (STME) to extract fine-grained motion\nconditioning, is then developed to improve synthesis quality and fidelity. We\nevaluate our method, MFD-V2V, on a comprehensive cardiac dataset, demonstrating\nsuperior performance over the state-of-the-art in both quantitative metrics and\nqualitative assessments. Furthermore, we show the benefits of our synthesized\ncine CMRs improving downstream clinical and analytical tasks, underscoring the\nbroader impact of our approach. Our code is publicly available at\nhttps://github.com/SwaksharDeb/MFD-V2V.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02003v1", "cate": "eess.IV", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "无监督心脏视频翻译通过运动特征引导扩散模型", "tldr": "提出MFD-V2V模型，利用运动特征引导扩散模型，将低对比度DENSE CMR视频转换为高质量cine CMR视频，并在心脏数据集上表现优异，有助下游临床任务。", "motivation": "旨在解决从低对比度、易受伪影影响的DENSE CMR序列合成动态、高对比度cine CMR视频的问题。", "method": "提出了一种新颖的运动特征引导扩散模型(MFD-V2V)进行无配对视频到视频的翻译。首先引入一个潜在时间多注意力(LTMA)配准网络来学习心脏运动，然后开发一个配备空间时间运动编码器(STME)的多级运动特征引导扩散模型，以提取细粒度运动条件，从而提高合成质量和保真度。", "result": "MFD-V2V在综合心脏数据集上进行了评估，在定量指标和定性评估方面均优于现有最先进的方法。此外，合成的cine CMRs改善了下游临床和分析任务。", "conclusion": "该方法通过合成高质量的cine CMRs，不仅在性能上超越了现有技术，还显著提升了下游临床和分析任务的效率和准确性，显示出广泛的应用潜力。", "translation": "本文提出了一种新颖的运动特征引导扩散模型，用于无配对视频到视频的翻译（MFD-V2V），旨在从对比度较低、易受伪影影响的位移编码刺激回波（DENSE）CMR序列中合成动态、高对比度的电影心脏磁共振（cine CMR）。为实现此目标，我们首先引入了一个潜在时间多注意力（LTMA）配准网络，该网络能有效从电影CMR图像视频中学习更准确和一致的心脏运动。随后，开发了一个多级运动特征引导扩散模型，该模型配备了专门的时空运动编码器（STME）以提取细粒度运动条件，从而提高合成质量和保真度。我们在一个全面的心脏数据集上评估了我们的方法MFD-V2V，结果表明其在定量指标和定性评估方面均优于现有最先进的方法。此外，我们还展示了合成的cine CMRs对改善下游临床和分析任务的益处，这突显了我们方法的更广泛影响。我们的代码已在https://github.com/SwaksharDeb/MFD-V2V公开。", "summary": "本文提出了一种名为MFD-V2V的新型无监督心脏视频翻译模型，该模型利用运动特征引导扩散机制，旨在将低对比度DENSE CMR序列转换为高对比度、动态的cine CMR视频。模型核心包括一个用于学习心脏运动的潜在时间多注意力配准网络，以及一个配备空间时间运动编码器以提取精细运动条件的多级扩散模型。实验结果表明，MFD-V2V在心脏数据集上表现优异，超越了现有最先进的方法，并且其合成的cine CMRs能有效改善下游临床和分析任务。", "keywords": "视频翻译, 扩散模型, 心脏磁共振, 运动特征, 无监督学习", "comments": "该论文提出了一种创新的无监督视频翻译方法，特别是其结合了运动特征引导和扩散模型的策略，有效解决了医学图像转换中常见的配对数据稀缺问题。通过提升合成图像的质量和对下游任务的积极影响，展示了其在临床应用中的巨大潜力。代码公开也利于后续研究。"}}
{"id": "2507.02257", "title": "Gbake: Baking 3D Gaussian Splats into Reflection Probes", "authors": ["Stephen Pasch", "Joel K. Salzman", "Changxi Zheng"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      SIGGRAPH 2025 Posters", "url": "http://arxiv.org/abs/2507.02257v1", "summary": "The growing popularity of 3D Gaussian Splatting has created the need to\nintegrate traditional computer graphics techniques and assets in splatted\nenvironments. Since 3D Gaussian primitives encode lighting and geometry jointly\nas appearance, meshes are relit improperly when inserted directly in a mixture\nof 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a\nspecialized tool for baking reflection probes from Gaussian-splatted scenes\nthat enables realistic reflection mapping of traditional 3D meshes in the Unity\ngame engine.", "comment": "SIGGRAPH 2025 Posters", "pdf_url": "http://arxiv.org/pdf/2507.02257v1", "cate": "cs.GR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Gbake：将3D高斯溅射烘焙成反射探头", "tldr": "GBake是一个将3D高斯溅射场景烘焙成反射探头的工具，用于在Unity中实现传统网格的真实反射映射。", "motivation": "随着3D高斯溅射技术的日益普及，需要将传统的计算机图形技术和资产整合到溅射环境中。由于3D高斯基元将光照和几何形状联合编码为外观，当传统网格直接插入到3D高斯混合体中时，它们的重新照明会不正确，从而显得格格不入。", "method": "本文介绍了GBake，一个专门用于从高斯溅射场景中烘焙反射探头的工具。", "result": "GBake能够实现在Unity游戏引擎中传统3D网格的真实反射映射。", "conclusion": "GBake工具解决了3D高斯溅射环境中传统网格光照不匹配的问题，通过烘焙反射探头实现了真实感反射。", "translation": "3D高斯溅射日益普及，使得将传统计算机图形技术和资产整合到溅射环境中成为必要。由于3D高斯基元将光照和几何形状联合编码为外观，当网格直接插入到3D高斯混合体中时，它们的重新照明会不正确，从而显得明显格格不入。我们引入了GBake，一个专门用于从高斯溅射场景中烘焙反射探头的工具，它能够在Unity游戏引擎中实现传统3D网格的真实反射映射。", "summary": "GBake是一个为3D高斯溅射场景设计的工具，旨在解决传统3D网格在这些环境中光照不匹配的问题。通过将高斯溅射场景烘焙成反射探头，GBake使得在Unity引擎中能够对传统网格进行真实的反射映射，从而更好地整合不同类型的3D资产。", "keywords": "3D高斯溅射, 反射探头, 烘焙, Unity, 真实反射映射", "comments": "GBake的创新之处在于它弥合了新兴的3D高斯溅射技术与传统计算机图形资产之间的鸿沟，解决了在混合环境中网格光照不一致的关键问题。这对于游戏开发和实时渲染中整合不同来源的3D内容具有重要意义。"}}
{"id": "2507.01975", "title": "Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows", "authors": ["Mengtao Yan", "Qi Wang", "Haining Wang", "Ruizhi Chengze", "Yi Zhang", "Hongsheng Liu", "Zidong Wang", "Fan Yu", "Qi Qi", "Hao Sun"], "categories": ["cs.LG", "cs.AI", "physics.flu-dyn"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      19 pages, 12 figures, accepted at KDD 2025 (ACM SIGKDD Conference on Knowledge Discovery and Data Mining)", "url": "http://arxiv.org/abs/2507.01975v1", "summary": "Simulation of fluid flows is crucial for modeling physical phenomena like\nmeteorology, aerodynamics, and biomedicine. Classical numerical solvers often\nrequire fine spatiotemporal grids to satisfy stability, consistency, and\nconvergence conditions, leading to substantial computational costs. Although\nmachine learning has demonstrated better efficiency, they typically suffer from\nissues of interpretability, generalizability, and data dependency. Hence, we\npropose a learnable and differentiable finite volume solver, called LDSolver,\ndesigned for efficient and accurate simulation of fluid flows on spatiotemporal\ncoarse grids. LDSolver comprises two key components: (1) a differentiable\nfinite volume solver, and (2) an learnable module providing equivalent\napproximation for fluxes (derivatives and interpolations), and temporal error\ncorrection on coarse grids. Even with limited training data (e.g., only a few\ntrajectories), our model could accelerate the simulation while maintaining a\nhigh accuracy with superior generalizability. Experiments on different flow\nsystems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver\nachieves state-of-the-art performance, surpassing baseline models with notable\nmargins.", "comment": "19 pages, 12 figures, accepted at KDD 2025 (ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining)", "pdf_url": "http://arxiv.org/pdf/2507.01975v1", "cate": "cs.LG", "date": "2025-06-23", "updated": "2025-06-23", "AI": {"title_translation": "可学习-可微分有限体积求解器，用于加速流体模拟", "tldr": "LDSolver是一个可学习可微分的有限体积求解器，能在粗网格上高效准确地模拟流体，解决了传统方法计算成本高和机器学习方法泛化性差的问题。", "motivation": "经典数值求解器计算成本高，需要细致的时空网格；机器学习方法虽然高效但存在可解释性、泛化性和数据依赖性问题。因此，需要一种既高效又准确，且具有良好泛化性的流体模拟方法。", "method": "提出LDSolver，一个可学习可微分的有限体积求解器。它包含两个核心组件：1) 可微分有限体积求解器；2) 一个可学习模块，提供通量（导数和插值）的等效近似，以及粗网格上的时间误差校正。", "result": "即使在有限的训练数据下，LDSolver也能加速模拟并保持高精度和出色的泛化性。在不同流系统（如Burgers、衰减流、受迫流和剪切流）上的实验表明，LDSolver实现了最先进的性能，显著超越了基线模型。", "conclusion": "LDSolver通过结合可学习和可微分的有限体积方法，成功解决了传统流体模拟效率低和机器学习方法泛化性差的问题，实现了在粗网格上高效、准确且泛化性强的流体模拟。", "translation": "流体模拟对于气象学、空气动力学和生物医学等物理现象的建模至关重要。经典的数值求解器通常需要精细的时空网格来满足稳定性、一致性和收敛性条件，导致巨大的计算成本。尽管机器学习已表现出更高的效率，但它们通常存在可解释性、泛化性和数据依赖性问题。因此，我们提出了一种可学习和可微分的有限体积求解器，称为LDSolver，旨在在时空粗网格上高效准确地模拟流体。LDSolver包含两个关键组件：(1) 一个可微分的有限体积求解器，以及 (2) 一个可学习模块，为通量（导数和插值）提供等效近似，并对粗网格上的时间误差进行校正。即使训练数据有限（例如，只有少量轨迹），我们的模型也能加速模拟，同时保持高精度和卓越的泛化性。在不同流系统（例如，Burgers流、衰减流、受迫流和剪切流）上的实验表明，LDSolver实现了最先进的性能，显著超越了基线模型。", "summary": "本文提出LDSolver，一个结合了可学习模块和可微分有限体积求解器的新型流体模拟方法。该方法旨在解决传统数值求解器计算成本高和现有机器学习方法泛化性差的问题。LDSolver通过在粗网格上近似通量并校正时间误差，实现了在有限数据下加速模拟并保持高精度和优异的泛化能力。实验结果显示，LDSolver在多种流体系统上均取得了超越现有基线模型的先进性能。", "keywords": "有限体积求解器, 流体模拟, 可学习, 可微分, 粗网格", "comments": "这项工作通过将深度学习的可学习性与传统有限体积求解器的可微分性相结合，为流体模拟提供了一个创新且高效的解决方案。它有效地解决了计算成本和泛化性之间的权衡，特别是在粗网格上的应用，这对于实际工程和科学计算具有重要意义。有限数据训练下的优异表现也凸显了其潜力。"}}
{"id": "2507.02235", "title": "Quality Diversity Genetic Programming for Learning Scheduling Heuristics", "authors": ["Meng Xu", "Frank Neumann", "Aneta Neumann", "Yew Soon Ong"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures", "url": "http://arxiv.org/abs/2507.02235v1", "summary": "Real-world optimization often demands diverse, high-quality solutions.\nQuality-Diversity (QD) optimization is a multifaceted approach in evolutionary\nalgorithms that aims to generate a set of solutions that are both\nhigh-performing and diverse. QD algorithms have been successfully applied\nacross various domains, providing robust solutions by exploring diverse\nbehavioral niches. However, their application has primarily focused on static\nproblems, with limited exploration in the context of dynamic combinatorial\noptimization problems. Furthermore, the theoretical understanding of QD\nalgorithms remains underdeveloped, particularly when applied to learning\nheuristics instead of directly learning solutions in complex and dynamic\ncombinatorial optimization domains, which introduces additional challenges.\nThis paper introduces a novel QD framework for dynamic scheduling problems. We\npropose a map-building strategy that visualizes the solution space by linking\nheuristic genotypes to their behaviors, enabling their representation on a QD\nmap. This map facilitates the discovery and maintenance of diverse scheduling\nheuristics. Additionally, we conduct experiments on both fixed and dynamically\nchanging training instances to demonstrate how the map evolves and how the\ndistribution of solutions unfolds over time. We also discuss potential future\nresearch directions that could enhance the learning process and broaden the\napplicability of QD algorithms to dynamic combinatorial optimization\nchallenges.", "comment": "9 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.02235v1", "cate": "cs.NE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "质量多样性遗传编程用于学习调度启发式算法", "tldr": "本文引入了一个新的质量多样性（QD）框架，用于动态调度问题，通过构建启发式基因型与行为的映射来可视化解空间，并在固定和动态训练实例上进行了实验。", "motivation": "现有质量多样性（QD）算法主要应用于静态问题，在动态组合优化问题中的探索有限。此外，QD算法在学习启发式而非直接学习解决方案方面的理论理解不足，尤其是在复杂动态组合优化领域，这带来了额外的挑战。", "method": "本文为动态调度问题引入了一个新颖的质量多样性（QD）框架。提出了一种地图构建策略，通过将启发式基因型与其行为联系起来，可视化解决方案空间，从而在QD地图上表示它们，以促进发现和维护多样化的调度启发式。", "result": "在固定和动态变化的训练实例上进行了实验，以演示地图如何演变以及解决方案的分布如何随时间展开。", "conclusion": "本文引入了一个新颖的质量多样性（QD）框架，用于动态调度问题，通过可视化解空间并发现和维护多样化的调度启发式，为QD算法在动态组合优化挑战中的应用提供了新的方向和潜力。", "translation": "真实世界的优化常常需要多样化、高质量的解决方案。质量多样性（QD）优化是进化算法中的一种多方面方法，旨在生成一组既高性能又多样化的解决方案。QD算法已成功应用于各种领域，通过探索多样化的行为利基提供了稳健的解决方案。然而，它们的应用主要集中在静态问题上，在动态组合优化问题背景下的探索有限。此外，QD算法的理论理解仍不成熟，特别是在应用于学习启发式而非直接学习复杂动态组合优化领域中的解决方案时，这带来了额外的挑战。本文为动态调度问题引入了一个新颖的QD框架。我们提出了一种地图构建策略，通过将启发式基因型与其行为联系起来，可视化解决方案空间，从而在QD地图上表示它们。这张地图有助于发现和维护多样化的调度启发式算法。此外，我们在固定和动态变化的训练实例上进行了实验，以演示地图如何演变以及解决方案的分布如何随时间展开。我们还讨论了潜在的未来研究方向，这些方向可以增强学习过程并拓宽QD算法在动态组合优化挑战中的适用性。", "summary": "本文针对质量多样性（QD）算法在动态组合优化问题中应用受限且理论理解不足的现状，提出了一个用于动态调度问题的新颖QD框架。该框架通过构建启发式基因型与其行为的映射来可视化解决方案空间，并利用QD地图发现和维护多样化的调度启发式。实验在固定和动态变化的实例上验证了其有效性，并探讨了未来的研究方向。", "keywords": "质量多样性, 遗传编程, 调度启发式, 动态优化, 组合优化", "comments": "本文的创新点在于将质量多样性（QD）优化首次应用于动态调度问题，并提出了一种独特的地图构建策略来可视化和管理多样化的调度启发式。这为QD算法在复杂、动态的实际优化问题中的应用开辟了新途径，解决了现有QD方法主要集中于静态问题的局限性。然而，抽象中并未深入讨论其理论贡献的详细内容，这可能需要进一步阅读全文来理解。"}}
{"id": "2507.02000", "title": "Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System", "authors": ["Yongsen Zheng", "Zongxuan Xie", "Guohua Wang", "Ziyao Liu", "Liang Lin", "Kwok-Yan Lam"], "categories": ["cs.IR", "cs.CL", "cs.MM"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02000v1", "summary": "Unfairness is a well-known challenge in Recommender Systems (RSs), often\nresulting in biased outcomes that disadvantage users or items based on\nattributes such as gender, race, age, or popularity. Although some approaches\nhave started to improve fairness recommendation in offline or static contexts,\nthe issue of unfairness often exacerbates over time, leading to significant\nproblems like the Matthew effect, filter bubbles, and echo chambers. To address\nthese challenges, we proposed a novel framework, Hypergraph Contrastive\nMulti-Interest Learning for Fair Conversational Recommender System (HyFairCRS),\naiming to promote multi-interest diversity fairness in dynamic and interactive\nConversational Recommender Systems (CRSs). HyFairCRS first captures a wide\nrange of user interests by establishing diverse hypergraphs through contrastive\nlearning. These interests are then utilized in conversations to generate\ninformative responses and ensure fair item predictions within the dynamic\nuser-system feedback loop. Experiments on two CRS-based datasets show that\nHyFairCRS achieves a new state-of-the-art performance while effectively\nalleviating unfairness. Our code is available at\nhttps://github.com/zysensmile/HyFairCRS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02000v1", "cate": "cs.IR", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "为什么多兴趣公平性很重要：用于公平对话推荐系统的超图对比多兴趣学习", "tldr": "本文提出HyFairCRS，一个针对对话推荐系统（CRS）的框架，通过超图对比学习捕获用户多兴趣，以解决推荐系统中的不公平问题，并在实验中取得了最先进的性能。", "motivation": "推荐系统（RS）中存在固有的不公平问题，导致用户或物品因性别、种族、年龄、流行度等属性而受到偏见。在动态和交互式对话推荐系统（CRS）中，这种不公平问题会随着时间加剧，引发马太效应、过滤气泡和回音室等严重问题。现有方法未能充分解决动态环境中的多兴趣多样性公平性问题。", "method": "本文提出了一个名为HyFairCRS（Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System）的新颖框架。HyFairCRS首先通过对比学习建立多样化的超图来捕获广泛的用户兴趣。然后，这些兴趣在对话中被利用，以生成信息丰富的响应，并在动态的用户-系统反馈循环中确保公平的物品预测。", "result": "在两个基于CRS的数据集上的实验表明，HyFairCRS在有效缓解不公平性的同时，达到了新的最先进性能。", "conclusion": "HyFairCRS框架能够有效解决对话推荐系统中的不公平问题，并通过捕获和利用用户多兴趣，在动态交互环境中实现了公平且高性能的推荐。", "translation": "不公平是推荐系统（RS）中一个众所周知的挑战，通常会导致基于性别、种族、年龄或流行度等属性对用户或物品产生偏见结果。尽管一些方法已经开始在离线或静态环境中改善公平推荐，但不公平问题往往会随着时间恶化，导致马太效应、过滤气泡和回音室等严重问题。为了应对这些挑战，我们提出了一个新颖的框架，即用于公平对话推荐系统的超图对比多兴趣学习（HyFairCRS），旨在促进动态和交互式对话推荐系统（CRS）中的多兴趣多样性公平性。HyFairCRS首先通过对比学习建立多样化的超图来捕获广泛的用户兴趣。然后，这些兴趣在对话中被利用，以生成信息丰富的响应，并在动态的用户-系统反馈循环中确保公平的物品预测。在两个基于CRS的数据集上的实验表明，HyFairCRS在有效缓解不公平性的同时，达到了新的最先进性能。我们的代码可在https://github.com/zysensmile/HyFairCRS获取。", "summary": "本文提出了一种名为HyFairCRS的新型框架，旨在解决对话推荐系统（CRS）中日益加剧的不公平问题，特别是多兴趣多样性公平性。HyFairCRS利用超图对比学习来捕获用户广泛的兴趣，并在对话交互中利用这些兴趣来确保公平的物品预测。实验结果表明，HyFairCRS不仅缓解了不公平性，还在CRS任务上取得了最先进的性能。", "keywords": "对话推荐系统, 公平性, 多兴趣学习, 超图, 对比学习", "comments": "该论文的创新点在于将超图对比学习引入到对话推荐系统中，以解决多兴趣公平性问题，尤其关注动态交互环境。它通过捕获和利用用户多样化的兴趣，有效地缓解了传统推荐系统中的偏见，并实现了性能的提升，具有重要的实践意义。"}}
{"id": "2507.02061", "title": "New algorithms for girth and cycle detection", "authors": ["Liam Roditty", "Plia Trabelsi"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02061v1", "summary": "Let $G=(V,E)$ be an unweighted undirected graph with $n$ vertices and $m$\nedges. Let $g$ be the girth of $G$, that is, the length of a shortest cycle in\n$G$. We present a randomized algorithm with a running time of\n$\\tilde{O}\\big(\\ell \\cdot n^{1 + \\frac{1}{\\ell - \\varepsilon}}\\big)$ that\nreturns a cycle of length at most $ 2\\ell \\left\\lceil \\frac{g}{2} \\right\\rceil\n- 2 \\left\\lfloor \\varepsilon \\left\\lceil \\frac{g}{2} \\right\\rceil\n\\right\\rfloor, $ where $\\ell \\geq 2$ is an integer and $\\varepsilon \\in [0,1]$,\nfor every graph with $g = polylog(n)$.\n  Our algorithm generalizes an algorithm of Kadria \\etal{} [SODA'22] that\ncomputes a cycle of length at most $4\\left\\lceil \\frac{g}{2} \\right\\rceil -\n2\\left\\lfloor \\varepsilon \\left\\lceil \\frac{g}{2} \\right\\rceil \\right\\rfloor $\nin $\\tilde{O}\\big(n^{1 + \\frac{1}{2 - \\varepsilon}}\\big)$ time. Kadria \\etal{}\npresented also an algorithm that finds a cycle of length at most $ 2\\ell\n\\left\\lceil \\frac{g}{2} \\right\\rceil $ in $\\tilde{O}\\big(n^{1 +\n\\frac{1}{\\ell}}\\big)$ time, where $\\ell$ must be an integer. Our algorithm\ngeneralizes this algorithm, as well, by replacing the integer parameter $\\ell$\nin the running time exponent with a real-valued parameter $\\ell - \\varepsilon$,\nthereby offering greater flexibility in parameter selection and enabling a\nbroader spectrum of combinations between running times and cycle lengths.\n  We also show that for sparse graphs a better tradeoff is possible, by\npresenting an $\\tilde{O}(\\ell\\cdot m^{1+1/(\\ell-\\varepsilon)})$ time randomized\nalgorithm that returns a cycle of length at most $2\\ell(\\lfloor\n\\frac{g-1}{2}\\rfloor) - 2(\\lfloor \\varepsilon \\lfloor \\frac{g-1}{2}\\rfloor\n\\rfloor+1)$, where $\\ell\\geq 3$ is an integer and $\\varepsilon\\in [0,1)$, for\nevery graph with $g=polylog(n)$.\n  To obtain our algorithms we develop several techniques and introduce a formal\ndefinition of hybrid cycle detection algorithms. [...]", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02061v1", "cate": "cs.DS", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "周长和环检测的新算法", "tldr": "本文提出了用于无权无向图的新的随机算法，通过引入实值参数来推广现有算法，从而在运行时间和环长之间提供更大的灵活性和更好的权衡，用于周长和短环检测。", "motivation": "现有算法在周长和环检测方面，其运行时间和可检测的环长之间存在局限性，尤其是在参数选择方面。本文旨在通过推广现有算法并引入新的参数，提供更大的灵活性和更好的折衷方案。", "method": "本文提出了新的随机算法。这些算法通过将现有算法（如Kadria等人的算法）中的整数参数替换为实值参数 $\\ell - \\varepsilon$，从而实现推广。此外，作者还开发了几种技术并引入了混合环检测算法的正式定义。", "result": "1. 提出了一种运行时间为 $\\tilde{O}\\big(\\ell \\cdot n^{1 + \\frac{1}{\\ell - \\varepsilon}}\\big)$ 的随机算法，对于周长 $g = \\text{polylog}(n)$ 的图，该算法返回长度至多为 $2\\ell \\left\\lceil \\frac{g}{2} \\right\\rceil - 2 \\left\\lfloor \\varepsilon \\left\\lceil \\frac{g}{2} \\right\\rceil \\right\\rfloor$ 的环，其中 $\\ell \\geq 2$ 是整数，$\\varepsilon \\in [0,1]$。该算法推广了Kadria等人的相关算法，提供了更大的参数选择灵活性。2. 对于稀疏图，提出了一种运行时间为 $\\tilde{O}(\\ell\\cdot m^{1+1/(\\ell-\\varepsilon)})$ 的随机算法，返回长度至多为 $2\\ell(\\lfloor \\frac{g-1}{2}\\rfloor) - 2(\\lfloor \\varepsilon \\lfloor \\frac{g-1}{2}\\rfloor \\rfloor+1)$ 的环，其中 $\\ell\\geq 3$ 是整数，$\\varepsilon\\in [0,1)$，适用于所有 $g=\\text{polylog}(n)$ 的图。", "conclusion": "本文提出的新算法通过引入实值参数，显著提升了无权无向图中周长和环检测算法的灵活性，使得在运行时间和找到的环的长度之间能够实现更广泛的权衡和更优的组合。", "translation": "设 $G=(V,E)$ 是一个具有 $n$ 个顶点和 $m$ 条边的无权无向图。设 $g$ 是 $G$ 的周长，即 $G$ 中最短环的长度。我们提出了一种随机算法，其运行时间为 $\\tilde{O}\\big(\\ell \\cdot n^{1 + \\frac{1}{\\ell - \\varepsilon}}\\big)$，该算法返回一个长度至多为 $2\\ell \\left\\lceil \\frac{g}{2} \\right\\rceil - 2 \\left\\lfloor \\varepsilon \\left\\lceil \\frac{g}{2} \\right\\rceil \\right\\rfloor$ 的环，其中 $\\ell \\geq 2$ 是整数且 $\\varepsilon \\in [0,1]$，适用于所有 $g = \\text{polylog}(n)$ 的图。\\n我们的算法推广了Kadria等人[SODA'22]的一种算法，该算法在 $\\tilde{O}\\big(n^{1 + \\frac{1}{2 - \\varepsilon}}\\big)$ 时间内计算出长度至多为 $4\\left\\lceil \\frac{g}{2} \\right\\rceil - 2\\left\\lfloor \\varepsilon \\left\\lceil \\frac{g}{2} \\right\\rceil \\right\\rfloor$ 的环。Kadria等人还提出了一种算法，在 $\\tilde{O}\\big(n^{1 + \\frac{1}{\\ell}}\\big)$ 时间内找到长度至多为 $2\\ell \\left\\lceil \\frac{g}{2} \\right\\rceil$ 的环，其中 $\\ell$ 必须是整数。我们的算法也推广了这种算法，通过将运行时间指数中的整数参数 $\\ell$ 替换为实值参数 $\\ell - \\varepsilon$，从而在参数选择上提供了更大的灵活性，并实现了运行时间和环长之间更广泛的组合。\\n我们还表明，对于稀疏图，可以通过提出一种运行时间为 $\\tilde{O}(\\ell\\cdot m^{1+1/(\\ell-\\varepsilon)})$ 的随机算法来实现更好的折衷，该算法返回一个长度至多为 $2\\ell(\\lfloor \\frac{g-1}{2}\\rfloor) - 2(\\lfloor \\varepsilon \\lfloor \\frac{g-1}{2}\\rfloor \\rfloor+1)$ 的环，其中 $\\ell\\geq 3$ 是整数且 $\\varepsilon\\in [0,1)$，适用于所有 $g=\\text{polylog}(n)$ 的图。\\n为了获得我们的算法，我们开发了几种技术并引入了混合环检测算法的正式定义。[...]", "summary": "本文提出了用于无权无向图的周长和环检测的新随机算法。这些算法通过引入实值参数 $\\ell - \\varepsilon$ 来推广了现有的算法，从而在算法的运行时间和找到的环的长度之间提供了更大的灵活性和更优的权衡组合。文章给出了适用于一般图和稀疏图的具体算法及其性能界限，并指出通过开发新技术和定义混合环检测算法来实现这些改进。", "keywords": "图算法, 周长, 环检测, 随机算法, 稀疏图", "comments": "该论文的创新之处在于通过引入实值参数来推广了现有的环检测算法，从而在算法的运行时间和找到的环的长度之间提供了前所未有的灵活性和更广泛的权衡组合。这种方法对于需要根据具体应用场景调整性能和结果的图算法而言具有重要意义。文章还提到了开发新的技术和引入混合环检测算法的正式定义，这可能对未来该领域的研究产生积极影响。"}}
{"id": "2507.02088", "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models", "authors": ["Tian Lan", "Xiangdong Su", "Xu Liu", "Ruirui Wang", "Ke Chang", "Jiang Li", "Guanglai Gao"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      24 pages, 9 figures", "url": "http://arxiv.org/abs/2507.02088v1", "summary": "As large language models (LLMs) are increasingly applied to various NLP\ntasks, their inherent biases are gradually disclosed. Therefore, measuring\nbiases in LLMs is crucial to mitigate its ethical risks. However, most existing\nbias evaluation datasets focus on English and North American culture, and their\nbias categories are not fully applicable to other cultures. The datasets\ngrounded in the Chinese language and culture are scarce. More importantly,\nthese datasets usually only support single evaluation tasks and cannot evaluate\nthe bias from multiple aspects in LLMs. To address these issues, we present a\nMulti-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias\nevaluation instances, covering 12 single bias categories, 82 subcategories and\nintroducing 5 evaluation tasks, providing extensive category coverage, content\ndiversity, and measuring comprehensiveness. Additionally, we evaluate several\npopular LLMs from different series and with parameter sizes. In general, all\nthese LLMs demonstrated varying degrees of bias. We conduct an in-depth\nanalysis of results, offering novel insights into bias in LLMs.", "comment": "24 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.02088v1", "cate": "cs.CL", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "McBE：一个用于大型语言模型的多任务中文偏见评估基准", "tldr": "鉴于现有中文偏见评估数据集稀缺且任务单一，本文提出了McBE，一个包含多任务、多类别中文偏见评估基准，并评估了流行LLMs的偏见程度，发现它们普遍存在偏见。", "motivation": "随着大型语言模型（LLMs）的应用日益广泛，其内在偏见逐渐暴露，测量这些偏见对于缓解伦理风险至关重要。然而，现有偏见评估数据集主要针对英语和北美文化，中文相关数据集稀缺，且多数仅支持单一评估任务，无法全面评估LLMs的偏见。", "method": "本文提出了一个多任务中文偏见评估基准（McBE），包含4,077个偏见评估实例，覆盖12个单一偏见类别和82个子类别，并引入了5个评估任务。此外，研究评估了不同系列和参数大小的流行LLMs。", "result": "所有被评估的LLMs都表现出不同程度的偏见。研究对结果进行了深入分析，提供了关于LLMs偏见的新颖见解。", "conclusion": "大型语言模型普遍存在不同程度的偏见。McBE为评估中文LLMs的偏见提供了一个全面、多样化的工具，有助于进一步研究和缓解LLMs的伦理风险。", "translation": "随着大型语言模型（LLMs）越来越多地应用于各种自然语言处理任务，其固有的偏见也逐渐暴露。因此，衡量LLMs中的偏见对于减轻其伦理风险至关重要。然而，大多数现有的偏见评估数据集侧重于英语和北美文化，其偏见类别不完全适用于其他文化。基于中文语言和文化的数据集稀缺。更重要的是，这些数据集通常只支持单一评估任务，无法从多个方面评估LLMs中的偏见。为了解决这些问题，我们提出了一个多任务中文偏见评估基准（McBE），它包括4,077个偏见评估实例，涵盖12个单一偏见类别，82个子类别，并引入了5个评估任务，提供了广泛的类别覆盖、内容多样性和衡量综合性。此外，我们评估了不同系列和参数大小的几种流行LLMs。总的来说，所有这些LLMs都表现出不同程度的偏见。我们对结果进行了深入分析，为LLMs中的偏见提供了新颖的见解。", "summary": "本文提出了McBE，一个针对大型语言模型（LLMs）的中文多任务偏见评估基准。鉴于现有英文数据集的局限性和中文数据集的稀缺性及单一任务性，McBE包含了4,077个评估实例，涵盖12个偏见类别、82个子类别和5个评估任务，旨在全面评估LLMs在中文语境下的偏见。研究者利用McBE评估了多种流行LLMs，发现它们普遍存在不同程度的偏见，并对结果进行了深入分析，提供了新见解。", "keywords": "大型语言模型, 偏见评估, 中文, 多任务, 基准", "comments": "该论文的创新点在于提出了首个针对中文LLMs的多任务偏见评估基准McBE，解决了现有数据集在语言、文化覆盖和任务多样性上的不足。其重要性在于为中文LLMs的偏见研究提供了一个全面且急需的工具，有助于推动LLMs的伦理发展和风险缓解。"}}
{"id": "2507.02730", "title": "Constraint-Guided Symbolic Regression for Data-Efficient Kinetic Model Discovery", "authors": ["Miguel Ángel de Carvalho Servia", "Ilya Orson Sandoval", "King Kuok", "Hii", "Klaus Hellgardt", "Dongda Zhang", "Ehecatl Antonio del Rio Chanona"], "categories": ["cs.CE", "cs.SC"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      27 pages, 8 figures", "url": "http://arxiv.org/abs/2507.02730v1", "summary": "The industrialization of catalytic processes hinges on the availability of\nreliable kinetic models for design, optimization, and control. Traditional\nmechanistic models demand extensive domain expertise, while many data-driven\napproaches often lack interpretability and fail to enforce physical\nconsistency. To overcome these limitations, we propose the Physics-Informed\nAutomated Discovery of Kinetics (PI-ADoK) framework. By integrating physical\nconstraints directly into a symbolic regression approach, PI-ADoK narrows the\nsearch space and substantially reduces the number of experiments required for\nmodel convergence. Additionally, the framework incorporates a robust\nuncertainty quantification strategy via the Metropolis-Hastings algorithm,\nwhich propagates parameter uncertainty to yield credible prediction intervals.\nBenchmarking our method against conventional approaches across several\ncatalytic case studies demonstrates that PI-ADoK not only enhances model\nfidelity but also lowers the experimental burden, highlighting its potential\nfor efficient and reliable kinetic model discovery in chemical reaction\nengineering.", "comment": "27 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.02730v1", "cate": "cs.CE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "约束引导的符号回归，用于数据高效的动力学模型发现", "tldr": "该论文提出了PI-ADoK框架，通过将物理约束整合到符号回归中，实现数据高效的动力学模型发现，并结合Metropolis-Hastings算法进行不确定性量化。", "motivation": "催化过程的工业化需要可靠的动力学模型。传统的机理模型需要广泛的领域专业知识，而许多数据驱动方法缺乏可解释性且未能强制执行物理一致性。", "method": "提出了物理信息辅助的动力学自动发现（PI-ADoK）框架。该框架通过将物理约束直接整合到符号回归方法中，缩小了搜索空间并显著减少了模型收敛所需的实验次数。此外，该框架通过Metropolis-Hastings算法纳入了鲁棒的不确定性量化策略，将参数不确定性传播以产生可信的预测区间。", "result": "在多个催化案例研究中，与传统方法相比，PI-ADoK不仅提高了模型保真度，还降低了实验负担。", "conclusion": "PI-ADoK在化学反应工程中具有高效可靠的动力学模型发现潜力。", "translation": "催化过程的工业化取决于可靠的动力学模型，以进行设计、优化和控制。传统的机理模型需要广泛的领域专业知识，而许多数据驱动方法往往缺乏可解释性，并且未能强制执行物理一致性。为了克服这些限制，我们提出了物理信息辅助的动力学自动发现（PI-ADoK）框架。通过将物理约束直接整合到符号回归方法中，PI-ADoK缩小了搜索空间，并显著减少了模型收敛所需的实验次数。此外，该框架通过Metropolis-Hastings算法纳入了鲁棒的不确定性量化策略，将参数不确定性传播以产生可信的预测区间。在多个催化案例研究中，将我们的方法与传统方法进行基准测试表明，PI-ADoK不仅提高了模型保真度，还降低了实验负担，突显了其在化学反应工程中高效可靠的动力学模型发现潜力。", "summary": "PI-ADoK是一个新颖的框架，它将符号回归与物理约束和不确定性量化（通过Metropolis-Hastings算法）相结合，旨在克服传统和数据驱动动力学模型方法的局限性。该框架通过缩小搜索空间和整合物理一致性，显著减少了所需的实验数据，提高了模型准确性，并提供了可靠的预测区间。基准测试表明，PI-ADoK在提高模型保真度和降低实验负担方面表现出色，预示着其在化学反应工程中高效可靠的动力学模型发现的巨大潜力。", "keywords": "符号回归, 动力学模型, 物理约束, 数据高效, 不确定性量化", "comments": "该论文的创新之处在于其PI-ADoK框架，通过将物理约束直接整合到符号回归中，并结合不确定性量化，有效解决了传统动力学模型和纯数据驱动方法在专业知识需求、可解释性、物理一致性及数据效率方面的痛点。这种方法有望显著加速工业催化领域中可靠动力学模型的发现过程，具有重要的实践意义。"}}
{"id": "2507.01974", "title": "Acoustic evaluation of a neural network dedicated to the detection of animal vocalisations", "authors": ["Jérémy Rouch", "M Ducrettet", "S Haupert", "R Emonet", "F Sèbe"], "categories": ["cs.SD", "cs.LG"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01974v1", "summary": "The accessibility of long-duration recorders, adapted to sometimes demanding\nfield conditions, has enabled the deployment of extensive animal population\nmonitoring campaigns through ecoacoustics. The effectiveness of automatic\nsignal detection methods, increasingly based on neural approaches, is\nfrequently evaluated solely through machine learning metrics, while acoustic\nanalysis of performance remains rare. As part of the acoustic monitoring of\nRock Ptarmigan populations, we propose here a simple method for acoustic\nanalysis of the detection system's performance. The proposed measure is based\non relating the signal-to-noise ratio of synthetic signals to their probability\nof detection. We show how this measure provides information about the system\nand allows optimisation of its training. We also show how it enables modelling\nof the detection distance, thus offering the possibility of evaluating its\ndynamics according to the sound environment and accessing an estimation of the\nspatial density of calls.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01974v1", "cate": "cs.SD", "date": "2025-06-23", "updated": "2025-06-23", "AI": {"title_translation": "神经网络动物发声检测的声学评估", "tldr": "本文提出了一种声学方法来评估和优化用于动物发声检测的神经网络的性能，并可用于建模检测距离和估计空间密度。", "motivation": "自动信号检测方法（特别是基于神经网络的方法）的性能评估通常仅限于机器学习指标，而对声学性能的分析却很少见，这限制了对系统真实表现的理解。", "method": "作者提出了一种简单的声学分析方法，用于评估检测系统的性能。该方法基于将合成信号的信噪比与其检测概率相关联。", "result": "这种声学评估方法能够提供系统信息，优化神经网络的训练，并能够建模检测距离，从而评估其在不同声学环境下的动态，并估算叫声的空间密度。", "conclusion": "所提出的声学评估方法为动物发声检测神经网络提供了一种更全面的性能评估手段，不仅能优化模型训练，还能实现检测距离建模和空间密度估计，从而提升了生态声学监测的准确性和应用范围。", "translation": "长期录音机的普及，加上其对恶劣野外条件的适应性，使得通过生态声学部署大规模动物种群监测活动成为可能。自动信号检测方法（越来越多地基于神经网络方法）的有效性通常仅通过机器学习指标进行评估，而对声学性能的分析却很少见。作为岩雷鸟种群声学监测的一部分，我们在此提出一种简单的声学分析方法，用于评估检测系统的性能。所提出的测量方法基于将合成信号的信噪比与其检测概率相关联。我们展示了这种测量方法如何提供系统信息并允许优化其训练。我们还展示了它如何能够建模检测距离，从而提供了根据声音环境评估其动态并获取叫声空间密度估计的可能性。", "summary": "本文提出了一种针对动物发声检测神经网络的声学评估方法，旨在弥补现有评估方法仅依赖机器学习指标而忽视声学性能的不足。该方法通过关联合成信号的信噪比与检测概率来评估系统性能，并展示了其在优化模型训练、建模检测距离以及估算叫声空间密度方面的应用潜力。", "keywords": "动物发声, 神经网络, 声学评估, 信噪比, 生态声学", "comments": "本文的创新点在于提出了一个简单而有效的声学评估框架，弥补了当前动物发声检测神经网络评估中声学性能分析的不足。该方法通过引入信噪比与检测概率的关系，为模型优化和实际应用（如检测距离建模和空间密度估计）提供了新的视角和工具，对于提升生态声学监测的科学性和准确性具有重要意义。"}}
{"id": "2507.02215", "title": "Hybrid least squares for learning functions from highly noisy data", "authors": ["Ben Adcock", "Bernhard Hientzsch", "Akil Narayan", "Yiming Xu"], "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      30 pages", "url": "http://arxiv.org/abs/2507.02215v1", "summary": "Motivated by the need for efficient estimation of conditional expectations,\nwe consider a least-squares function approximation problem with heavily\npolluted data. Existing methods that are powerful in the small noise regime are\nsuboptimal when large noise is present. We propose a hybrid approach that\ncombines Christoffel sampling with certain types of optimal experimental design\nto address this issue. We show that the proposed algorithm enjoys appropriate\noptimality properties for both sample point generation and noise mollification,\nleading to improved computational efficiency and sample complexity compared to\nexisting methods. We also extend the algorithm to convex-constrained settings\nwith similar theoretical guarantees. When the target function is defined as the\nexpectation of a random field, we extend our approach to leverage adaptive\nrandom subspaces and establish results on the approximation capacity of the\nadaptive procedure. Our theoretical findings are supported by numerical studies\non both synthetic data and on a more challenging stochastic simulation problem\nin computational finance.", "comment": "30 pages", "pdf_url": "http://arxiv.org/pdf/2507.02215v1", "cate": "stat.ML", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "混合最小二乘法用于从高噪声数据中学习函数", "tldr": "提出了一种混合最小二乘法，结合了Christoffel采样和最优实验设计，以高效地从高噪声数据中学习函数，提高了计算效率和样本复杂度。", "motivation": "现有方法在处理大量噪声数据时效果不佳，因此需要一种更高效的方法来从受严重污染的数据中估计条件期望和逼近函数。", "method": "提出了一种混合方法，结合了Christoffel采样和最优实验设计。该算法还被扩展到凸约束设置，并利用自适应随机子空间处理随机场的期望。", "result": "所提出的算法在样本点生成和噪声平滑方面都具有适当的最优性，与现有方法相比，提高了计算效率和样本复杂度。在扩展到凸约束设置和随机场时，也保持了相似的理论保证。理论发现通过合成数据和计算金融中的随机模拟问题的数值研究得到支持。", "conclusion": "该混合最小二乘方法能够有效解决从高噪声数据中学习函数的问题，并在性能和理论保证方面均有所提升，即使在扩展设置下也表现良好。", "translation": "标题：混合最小二乘法用于从高噪声数据中学习函数\n摘要：出于对条件期望高效估计的需求，我们考虑了一个针对受严重污染数据的最小二乘函数逼近问题。现有方法在小噪声环境下表现出色，但在大噪声存在时则表现不佳。我们提出了一种混合方法，将Christoffel采样与特定类型的最优实验设计相结合以解决此问题。我们表明，所提出的算法在样本点生成和噪声平滑方面都具有适当的最优性，与现有方法相比，提高了计算效率和样本复杂度。我们还将该算法扩展到凸约束设置，并具有相似的理论保证。当目标函数被定义为随机场的期望时，我们扩展了我们的方法以利用自适应随机子空间，并建立了关于自适应过程逼近能力的结果。我们的理论发现得到了数值研究的支持，包括合成数据和计算金融中更具挑战性的随机模拟问题。", "summary": "本文介绍了一种混合最小二乘函数逼近方法，旨在解决现有方法在高噪声数据下学习效率低下的问题。通过结合Christoffel采样和最优实验设计，该算法在样本点生成和噪声平滑方面表现出优越性，显著提高了计算效率并降低了样本复杂度。该方法还被成功扩展到凸约束设置，并能利用自适应随机子空间处理随机场的期望，所有这些都得到了坚实的理论支持和数值模拟（包括计算金融应用）的验证。", "keywords": "最小二乘, 噪声数据, Christoffel采样, 最优实验设计, 计算效率", "comments": "本文提出了一种新颖的混合方法来解决从高噪声数据中学习函数这一重要问题。其创新之处在于将Christoffel采样与最优实验设计相结合，为数据获取和噪声管理提供了原则性的方法。算法扩展到凸约束设置和随机场，以及理论保证和数值验证，表明了其鲁棒性和多功能性。对计算效率和样本复杂度的关注使其在实际应用中具有重要价值。"}}
{"id": "2507.02491", "title": "Engineering an LTLf Synthesis Tool", "authors": ["Alexandre Duret-Lutz", "Shufang Zhu", "Nir Piterman", "Giuseppe de Giacomo", "Moshe Y Vardi"], "categories": ["cs.FL"], "primary_category": "Subjects:       Formal Languages and Automata Theory (cs.FL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02491v1", "summary": "The problem of LTLf reactive synthesis is to build a transducer, whose output\nis based on a history of inputs, such that, for every infinite sequence of\ninputs, the conjoint evolution of the inputs and outputs has a prefix that\nsatisfies a given LTLf specification. We describe the implementation of an LTLf\nsynthesizer that outperforms existing tools on our benchmark suite. This is\nbased on a new, direct translation from LTLf to a DFA represented as an array\nof Binary Decision Diagrams (MTBDDs) sharing their nodes. This MTBDD-based\nrepresentation can be interpreted directly as a reachability game that is\nsolved on-the-fly during its construction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02491v1", "cate": "cs.FL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "工程化LTLf综合工具", "tldr": "开发了一个新的LTLf合成工具，通过创新的MTBDD表示和即时博弈求解，在基准测试中超越了现有工具。", "motivation": "解决LTLf反应式综合问题，即构建一个能满足LTLf规范的传感器。现有工具可能效率不足。", "method": "采用了一种新的、直接的LTLf到DFA的转换方法，DFA以共享节点的MTBDD数组表示。这种MTBDD表示可以直接解释为可达性博弈，并在构建过程中即时求解。", "result": "所实现的LTLf合成器在基准测试套件上优于现有工具。", "conclusion": "基于MTBDD表示和即时博弈求解的新方法能有效地提高LTLf反应式综合工具的性能。", "translation": "LTLf反应式综合问题旨在构建一个传感器，其输出基于输入历史，使得对于每个无限输入序列，输入和输出的联合演变都具有满足给定LTLf规范的前缀。我们描述了一个LTLf合成器的实现，该合成器在我们的基准测试套件上优于现有工具。这基于从LTLf到DFA的一种新的直接转换，DFA表示为共享节点的二进制决策图（MTBDD）数组。这种基于MTBDD的表示可以直接解释为可达性博弈，并在其构建过程中即时求解。", "summary": "本文介绍了一个新的LTLf反应式综合工具的实现。该工具通过将LTLf直接转换为由共享节点的多终端二叉决策图（MTBDD）数组表示的确定性有限自动机（DFA），并在此基础上将DFA解释为可达性博弈并进行即时求解，从而在性能上超越了现有工具。", "keywords": "LTLf合成, 反应式综合, MTBDD, DFA, 可达性博弈", "comments": "这篇论文的创新点在于提出了从LTLf到MTBDD表示的DFA的直接转换，并将此表示直接应用于即时求解可达性博弈，从而显著提高了LTLf合成工具的性能。这种方法可能为LTLf反应式综合领域带来新的突破，尤其是在处理复杂规范时。"}}
{"id": "2507.02206", "title": "EIM-TRNG: Obfuscating Deep Neural Network Weights with Encoding-in-Memory True Random Number Generator via RowHammer", "authors": ["Ranyang Zhou", "Abeer Matar A. Almalky", "Gamana Aragonda", "Sabbir Ahmed", "Filip Roth Trønnes-Christensen", "Adnan Siraj Rakin", "Shaahin Angizi"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02206v1", "summary": "True Random Number Generators (TRNGs) play a fundamental role in hardware\nsecurity, cryptographic systems, and data protection. In the context of Deep\nNeuralNetworks (DNNs), safeguarding model parameters, particularly weights, is\ncritical to ensure the integrity, privacy, and intel-lectual property of AI\nsystems. While software-based pseudo-random number generators are widely used,\nthey lack the unpredictability and resilience offered by hardware-based TRNGs.\nIn this work, we propose a novel and robust Encoding-in-Memory TRNG called\nEIM-TRNG that leverages the inherent physical randomness in DRAM cell behavior,\nparticularly under RowHammer-induced disturbances, for the first time. We\ndemonstrate how the unpredictable bit-flips generated through carefully\ncontrolled RowHammer operations can be harnessed as a reliable entropy source.\nFurthermore, we apply this TRNG framework to secure DNN weight data by encoding\nvia a combination of fixed and unpredictable bit-flips. The encrypted data is\nlater decrypted using a key derived from the probabilistic flip behavior,\nensuring both data confidentiality and model authenticity. Our results validate\nthe effectiveness of DRAM-based entropy extraction for robust, low-cost\nhardware security and offer a promising direction for protecting machine\nlearning models at the hardware level.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02206v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "EIM-TRNG: 通过RowHammer利用内存编码真随机数生成器混淆深度神经网络权重", "tldr": "提出EIM-TRNG，一种利用RowHammer效应从DRAM中提取真随机数的方法，并将其应用于加密深度神经网络权重，以增强AI系统安全。", "motivation": "真随机数生成器（TRNG）在硬件安全、密码系统和数据保护中至关重要。保护深度神经网络（DNN）的模型参数（特别是权重）是确保AI系统完整性、隐私和知识产权的关键。现有的软件伪随机数生成器缺乏硬件TRNG的不可预测性和弹性。", "method": "提出了一种新颖的内存编码真随机数生成器（EIM-TRNG），首次利用DRAM单元在RowHammer诱导扰动下固有的物理随机性。通过精心控制RowHammer操作产生不可预测的位翻转作为可靠的熵源。将此TRNG框架应用于通过固定和不可预测位翻转的组合编码DNN权重数据。加密数据使用从概率翻转行为导出的密钥进行解密。", "result": "结果验证了基于DRAM的熵提取对于鲁棒、低成本硬件安全的有效性。", "conclusion": "该研究为在硬件层面保护机器学习模型提供了有前景的方向，通过基于DRAM的熵提取实现了鲁棒、低成本的硬件安全。", "translation": "真随机数生成器（TRNG）在硬件安全、密码系统和数据保护中发挥着基础作用。在深度神经网络（DNN）的背景下，保护模型参数，特别是权重，对于确保AI系统的完整性、隐私和知识产权至关重要。虽然基于软件的伪随机数生成器被广泛使用，但它们缺乏硬件TRNG提供的不可预测性和弹性。在这项工作中，我们首次提出了一种新颖且鲁棒的内存编码TRNG（EIM-TRNG），它利用了DRAM单元行为中固有的物理随机性，特别是在RowHammer引起的扰动下。我们展示了如何通过精心控制的RowHammer操作产生的不可预测位翻转被利用作为可靠的熵源。此外，我们将此TRNG框架应用于通过固定和不可预测位翻转的组合编码DNN权重数据，从而保护DNN权重数据。加密数据随后使用从概率翻转行为导出的密钥进行解密，确保了数据保密性和模型真实性。我们的结果验证了基于DRAM的熵提取对于鲁棒、低成本硬件安全的有效性，并为在硬件层面保护机器学习模型提供了有前景的方向。", "summary": "本文提出了一种名为EIM-TRNG的新型真随机数生成器，它利用DRAM在RowHammer攻击下产生的固有物理随机性作为熵源。该方法通过控制RowHammer操作产生的不可预测位翻转来生成随机数，并将其应用于深度神经网络权重的加密混淆。加密数据可通过基于位翻转行为的密钥解密，从而实现数据保密性和模型真实性。研究结果证明了这种基于DRAM的熵提取方法在提供鲁棒、低成本硬件安全方面的有效性，为AI模型的硬件级保护开辟了新途径。", "keywords": "真随机数生成器, RowHammer, 深度神经网络, 硬件安全, 内存编码", "comments": "这项工作创新性地将RowHammer效应从攻击手段转化为一种可控的熵源，用于生成真随机数，并进一步应用于深度神经网络权重的保护，这在硬件安全领域具有重要意义。它提供了一种新颖且潜在低成本的硬件级安全解决方案，有助于解决AI模型知识产权和隐私保护的关键挑战。"}}
{"id": "2507.02170", "title": "Synergizing Logical Reasoning, Knowledge Management and Collaboration in Multi-Agent LLM System", "authors": ["Adam Kostka", "Jarosław A. Chudziak"], "categories": ["cs.MA"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02170v1", "summary": "This paper explores the integration of advanced Multi-Agent Systems (MAS)\ntechniques to develop a team of agents with enhanced logical reasoning,\nlong-term knowledge retention, and Theory of Mind (ToM) capabilities. By\nuniting these core components with optimized communication protocols, we create\na novel framework called SynergyMAS, which fosters collaborative teamwork and\nsuperior problem-solving skills. The system's effectiveness is demonstrated\nthrough a product development team case study, where our approach significantly\nenhances performance and adaptability. These findings highlight SynergyMAS's\npotential to tackle complex, real-world challenges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02170v1", "cate": "cs.MA", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "多智能体LLM系统中逻辑推理、知识管理与协作的协同", "tldr": "本文提出了一种名为SynergyMAS的新型多智能体系统框架，该框架通过整合逻辑推理、长期知识保留和心智理论能力，并优化通信协议，显著增强了多智能体团队的协作和问题解决能力，并在产品开发案例研究中展现出其有效性。", "motivation": "本文旨在探索如何整合先进的多智能体系统技术，以开发一个具备增强的逻辑推理、长期知识保留和心智理论（ToM）能力的智能体团队。", "method": "通过将逻辑推理、长期知识保留和心智理论（ToM）这些核心组件与优化的通信协议相结合，本文创建了一个名为SynergyMAS的新型框架，旨在促进协作团队合作和卓越的问题解决能力。", "result": "通过一个产品开发团队的案例研究，证明了该系统（SynergyMAS）的有效性，其方法显著增强了团队的性能和适应性。", "conclusion": "这些发现突出了SynergyMAS在解决复杂现实世界挑战方面的潜力。", "translation": "本文探讨了如何整合先进的多智能体系统（MAS）技术，以开发一个具备增强的逻辑推理、长期知识保留和心智理论（ToM）能力的智能体团队。通过将这些核心组件与优化的通信协议相结合，我们创建了一个名为SynergyMAS的新型框架，该框架旨在促进协作团队合作和卓越的问题解决能力。通过一个产品开发团队的案例研究，证明了该系统的有效性，我们的方法显著增强了性能和适应性。这些发现突出了SynergyMAS在解决复杂现实世界挑战方面的潜力。", "summary": "本文提出了一种名为SynergyMAS的新型多智能体系统框架，该框架通过整合逻辑推理、长期知识保留和心智理论（ToM）能力，并结合优化的通信协议，显著提升了多智能体团队的协作和问题解决能力。在产品开发团队的案例研究中，SynergyMAS展示了其在提高性能和适应性方面的有效性，表明其在应对复杂现实挑战方面的巨大潜力。", "keywords": "多智能体系统, 逻辑推理, 知识管理, 协作, SynergyMAS", "comments": "本文的创新之处在于提出了SynergyMAS框架，它有效地将逻辑推理、长期知识保留和心智理论（ToM）这三个关键能力整合到多智能体系统中，并通过优化通信协议来增强协作。这种集成方法对于提升多智能体系统在复杂现实世界问题中的性能和适应性具有重要意义。"}}
{"id": "2507.02517", "title": "Detecting Multiple Diseases in Multiple Crops Using Deep Learning", "authors": ["Vivek Yadav", "Anugrah Jain"], "categories": ["cs.CV", "cs.AI", "cs.ET"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02517v1", "summary": "India, as a predominantly agrarian economy, faces significant challenges in\nagriculture, including substantial crop losses caused by diseases, pests, and\nenvironmental stress. Early detection and accurate identification of diseases\nacross different crops are critical for improving yield and ensuring food\nsecurity. This paper proposes a deep learning based solution for detecting\nmultiple diseases in multiple crops, aimed to cover India's diverse\nagricultural landscape. We first create a unified dataset encompassing images\nof 17 different crops and 34 different diseases from various available\nrepositories. Proposed deep learning model is trained on this dataset and\noutperforms the state-of-the-art in terms of accuracy and the number of crops,\ndiseases covered. We achieve a significant detection accuracy, i.e., 99 percent\nfor our unified dataset which is 7 percent more when compared to\nstate-of-the-art handling 14 crops and 26 different diseases only. By improving\nthe number of crops and types of diseases that can be detected, proposed\nsolution aims to provide a better product for Indian farmers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02517v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "使用深度学习检测多种作物中的多种疾病", "tldr": "本文提出了一种基于深度学习的解决方案，用于在印度检测多种作物中的多种疾病，通过创建统一数据集并训练模型，实现了比现有技术更高的检测准确率和覆盖范围。", "motivation": "印度作为农业经济体面临严重的作物损失，早期和准确的疾病检测对于提高产量和确保粮食安全至关重要。", "method": "作者创建了一个包含17种不同作物和34种不同疾病图像的统一数据集。然后，在这个数据集上训练了所提出的深度学习模型。", "result": "所提出的模型在统一数据集上实现了99%的检测准确率，比现有技术（处理14种作物和26种疾病）高出7%。该模型在准确性和覆盖的作物及疾病数量上均优于现有技术。", "conclusion": "通过提高可检测的作物和疾病数量，所提出的解决方案旨在为印度农民提供更好的产品，从而帮助解决农业挑战。", "translation": "印度作为一个以农业为主的经济体，面临着农业方面的重大挑战，包括由疾病、害虫和环境压力造成的巨大作物损失。在不同作物中早期发现和准确识别疾病对于提高产量和确保粮食安全至关重要。本文提出了一种基于深度学习的解决方案，用于检测多种作物中的多种疾病，旨在覆盖印度多样化的农业景观。我们首先创建了一个统一的数据集，其中包含来自各种可用存储库的17种不同作物和34种不同疾病的图像。所提出的深度学习模型在该数据集上进行训练，在准确性和覆盖的作物、疾病数量方面均优于现有技术。我们的统一数据集实现了显著的检测准确率，即99%，比现有技术（仅处理14种作物和26种不同疾病）高出7%。通过增加可检测的作物和疾病类型数量，所提出的解决方案旨在为印度农民提供更好的产品。", "summary": "本研究针对印度农业作物疾病检测的挑战，提出了一种基于深度学习的解决方案。通过构建包含17种作物和34种疾病的统一图像数据集，并在此数据集上训练深度学习模型，实现了99%的疾病检测准确率，显著优于现有技术，覆盖范围更广，旨在为印度农民提供更高效的作物疾病诊断工具。", "keywords": "深度学习, 作物疾病检测, 统一数据集, 农业, 印度", "comments": "该研究的创新之处在于构建了一个大规模的统一数据集，涵盖了多种作物和多种疾病，并在此基础上训练了一个高性能的深度学习模型。其重要性在于为印度农业提供了更全面、更准确的作物疾病检测工具，有助于减少作物损失，保障粮食安全。模型的99%准确率和对更多作物疾病的覆盖能力是其主要亮点。"}}
{"id": "2507.02107", "title": "Structural Code Search using Natural Language Queries", "authors": ["Ben Limpanukorn", "Yanjun Wang", "Zach Patterson", "Pranav Garg", "Murali Krishna Ramanathan", "Xiaofei Ma", "Anoop Deoras", "Miryung Kim"], "categories": ["cs.SE", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02107v1", "summary": "Searching code is a common task that developers perform to understand APIs,\nlearn common code patterns, and navigate code. Currently, developers most\ncommonly search using keywords and regular expressions that are easy to use and\nwidely available. Beyond keywords and regular expressions, structural code\nsearch tools allow developers to search for code based on its syntactic\nstructure. This has numerous applications ranging from bug finding to\nsystematically refactoring code. However, these structural code search tools\noperate on queries expressed in domain-specific languages (DSL) that can be\ndifficult to learn and write. We propose to allow developers to use natural\nlanguage to search for code structurally. Expressing queries in natural\nlanguage provides an intuitive way to search for code and lowers the barrier to\nentry.\n  In this work, we develop a novel general approach that combines the reasoning\ncapabilities of an LLM to interpret natural language search queries with the\npower of structural search tools to efficiently and accurately retrieve\nrelevant code. We then instantiate this approach for two structural code search\nDSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for\nstructural code search consisting of 400 queries over 10 Java projects. We show\nthat our approach for structural code search based on translating NL queries to\nDSL queries using an LLM is effective and robust, achieving a high precision\nand recall ranging from 55% - 70%. Further, our approach significantly\noutperforms baselines based on semantic code search and LLM retrievals by up to\n57% and 14% on F1 scores.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02107v1", "cate": "cs.SE", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "使用自然语言查询的结构化代码搜索", "tldr": "该论文提出了一种利用大型语言模型（LLM）将自然语言查询转换为领域特定语言（DSL）查询的方法，从而实现更易用的结构化代码搜索，并展示了其在精度和召回率上的显著提升。", "motivation": "当前的代码搜索方法（如关键词和正则表达式）虽然易用但功能有限，而强大的结构化代码搜索工具则依赖于难以学习和编写的领域特定语言（DSL），这为开发者设置了使用障碍。本研究的动机是降低结构化代码搜索的门槛，使其能通过直观的自然语言查询进行。", "method": "本研究开发了一种新颖的通用方法，该方法结合了大型语言模型（LLM）解释自然语言查询的能力与结构化搜索工具高效准确检索相关代码的优势。具体而言，该方法通过LLM将自然语言查询转换为DSL查询。研究人员将此方法实例化到两种结构化代码搜索DSL：Semgrep和GQL。为了评估，他们构建了一个包含400个查询的新结构化代码搜索基准，涉及10个Java项目。", "result": "该方法通过LLM将自然语言查询翻译为DSL查询，在结构化代码搜索中表现出有效性和鲁棒性，实现了55%至70%的高精度和召回率。此外，该方法在F1分数上显著优于基于语义代码搜索和LLM检索的基线，最高分别提升57%和14%。", "conclusion": "本研究的结论是，通过利用大型语言模型将自然语言查询转换为领域特定语言（DSL）查询进行结构化代码搜索的方法是有效且稳健的，并且在性能上显著优于现有的基线方法，从而降低了结构化代码搜索的门槛。", "translation": "搜索代码是开发人员理解API、学习常见代码模式和导航代码的常见任务。目前，开发人员最常使用关键字和正则表达式进行搜索，因为它们易于使用且广泛可用。除了关键字和正则表达式，结构化代码搜索工具允许开发人员根据其句法结构搜索代码。这在从查找错误到系统地重构代码等领域有广泛的应用。然而，这些结构化代码搜索工具在领域特定语言（DSL）中操作查询，这些语言可能难以学习和编写。我们建议允许开发人员使用自然语言进行结构化代码搜索。用自然语言表达查询提供了一种直观的方式来搜索代码，并降低了入门门槛。\n在这项工作中，我们开发了一种新颖的通用方法，该方法结合了大型语言模型（LLM）解释自然语言搜索查询的推理能力与结构化搜索工具的强大功能，以高效准确地检索相关代码。然后，我们将这种方法实例化为两种结构化代码搜索DSL：Semgrep和GQL。在我们的评估中，我们构建了一个新的结构化代码搜索基准，包含10个Java项目上的400个查询。我们表明，我们基于使用LLM将自然语言查询翻译为DSL查询的结构化代码搜索方法是有效且稳健的，实现了55% - 70%的高精度和召回率。此外，我们的方法在F1分数上显着优于基于语义代码搜索和LLM检索的基线，分别高出57%和14%。", "summary": "这篇论文提出了一种创新的方法，旨在通过将自然语言查询转换为领域特定语言（DSL）查询，从而实现结构化代码搜索。该方法利用大型语言模型（LLM）的推理能力来解释自然语言，并结合现有结构化搜索工具的强大功能。通过这种方式，论文旨在降低结构化代码搜索的门槛，使其对开发者更易于使用。在对Semgrep和GQL两种DSL的实例化和评估中，该方法在新的Java项目基准上表现出55%-70%的高精度和召回率，并且在F1分数上显著优于语义代码搜索和LLM检索基线，最高提升57%。", "keywords": "结构化代码搜索, 自然语言查询, 大型语言模型, 代码工具, DSL", "comments": "这项工作通过结合LLM的自然语言理解能力与结构化代码搜索的精确性，有效地解决了DSL学习门槛高的问题。其创新点在于利用LLM作为桥梁，使得开发者能够以更直观的方式进行复杂的结构化代码搜索。评估结果表明了该方法的有效性和鲁棒性，对提高代码搜索效率和可访问性具有重要意义。"}}
{"id": "2507.02029", "title": "RoboBrain 2.0 Technical Report", "authors": ["BAAI RoboBrain Team", "Mingyu Cao", "Huajie Tan", "Yuheng Ji", "Minglan Lin", "Zhiyu Li", "Zhou Cao", "Pengwei Wang", "Enshen Zhou", "Yi Han", "Yingbo Tang", "Xiangqi Xu", "Wei Guo", "Yaoxu Lyu", "Yijie Xu", "Jiayu Shi", "Cheng Chi", "Mengdi Zhao", "Xiaoshuai Hao", "Shanyu Rong", "Zhengliang Cai", "Bolun Zhang", "Shuyi Zhang", "Huaihai Lyu", "Mengfei Du", "Lingfeng Zhang", "Xi Feng", "Xiaodan Liu", "Yance Jiao", "Chenrui He", "Mengsi Lyu", "Zhuo Chen", "Yulong Ao", "Xue Sun", "Zheqi He", "Jingshu Zheng", "Xi Yang", "Donghai Shi", "Kunchang Xie", "Bochao Zhang", "Shaokai Nie", "Chunlei Men", "Yonghua Lin", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02029v1", "summary": "We introduce RoboBrain 2.0, our latest generation of embodied vision-language\nfoundation models, designed to unify perception, reasoning, and planning for\ncomplex embodied tasks in physical environments. It comes in two variants: a\nlightweight 7B model and a full-scale 32B model, featuring a heterogeneous\narchitecture with a vision encoder and a language model. Despite its compact\nsize, RoboBrain 2.0 achieves strong performance across a wide spectrum of\nembodied reasoning tasks. On both spatial and temporal benchmarks, the 32B\nvariant achieves leading results, surpassing prior open-source and proprietary\nmodels. In particular, it supports key real-world embodied AI capabilities,\nincluding spatial understanding (e.g., affordance prediction, spatial\nreferring, trajectory forecasting) and temporal decision-making (e.g.,\nclosed-loop interaction, multi-agent long-horizon planning, and scene graph\nupdating). This report details the model architecture, data construction,\nmulti-stage training strategies, infrastructure and practical applications. We\nhope RoboBrain 2.0 advances embodied AI research and serves as a practical step\ntoward building generalist embodied agents. The code, checkpoint and benchmark\nare available at https://superrobobrain.github.io.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02029v1", "cate": "cs.RO", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "RoboBrain 2.0 技术报告", "tldr": "RoboBrain 2.0是一个新的具身视觉-语言基础模型，旨在统一具身任务的感知、推理和规划，并在广泛的具身推理任务上表现出色，超越现有模型。", "motivation": "设计RoboBrain 2.0旨在统一物理环境中复杂具身任务的感知、推理和规划，并希望推动具身AI研究，实现通用具身智能体。", "method": "RoboBrain 2.0有两种变体（7B和32B），采用异构架构，包含视觉编码器和语言模型。本报告详细介绍了模型架构、数据构建、多阶段训练策略和基础设施。", "result": "RoboBrain 2.0在广泛的具身推理任务中取得了强大性能。32B变体在空间和时间基准测试中均取得了领先结果，超越了先前的开源和专有模型。它支持空间理解（如可供性预测、空间指代、轨迹预测）和时间决策（如闭环交互、多智能体长周期规划、场景图更新）等关键现实世界具身AI能力。", "conclusion": "作者希望RoboBrain 2.0能推动具身AI研究，并作为构建通用具身智能体的实际步骤。", "translation": "我们介绍了RoboBrain 2.0，这是我们最新一代的具身视觉-语言基础模型，旨在统一物理环境中复杂具身任务的感知、推理和规划。它有两种变体：一个轻量级的7B模型和一个全尺寸的32B模型，采用异构架构，包含一个视觉编码器和一个语言模型。尽管尺寸紧凑，RoboBrain 2.0在广泛的具身推理任务中取得了强大性能。在空间和时间基准测试中，32B变体均取得了领先结果，超越了先前的开源和专有模型。特别是，它支持关键的现实世界具身AI能力，包括空间理解（例如，可供性预测、空间指代、轨迹预测）和时间决策（例如，闭环交互、多智能体长周期规划和场景图更新）。本报告详细介绍了模型架构、数据构建、多阶段训练策略、基础设施和实际应用。我们希望RoboBrain 2.0能推动具身AI研究，并作为构建通用具身智能体的实际步骤。代码、检查点和基准测试可在https://superrobobrain.github.io获取。", "summary": "RoboBrain 2.0是一个先进的具身视觉-语言基础模型，旨在统一物理环境中复杂具身任务的感知、推理和规划。它提供7B和32B两种版本，采用视觉编码器和语言模型的异构架构。该模型在多项具身推理任务上表现出色，特别是32B版本在空间和时间基准测试中均超越了现有模型，支持包括空间理解和时间决策在内的现实世界具身AI能力。该报告详细介绍了模型、数据、训练策略和应用，旨在推动通用具身智能体的研究。", "keywords": "具身AI, 视觉-语言模型, 基础模型, 具身推理, 机器人感知", "comments": "RoboBrain 2.0的创新在于其作为具身视觉-语言基础模型，能够统一感知、推理和规划，并提供了两种规模的模型以适应不同需求。其在空间和时间基准测试中超越现有模型的能力，以及对现实世界具身AI能力的支持，显示了其在推动通用具身智能体发展方面的重要性。"}}
{"id": "2507.02138", "title": "A Theory-driven and AI-enhanced Simulation Platform for Cultivating Nutrition Literacy", "authors": ["Shan Li", "Guozhu Ding"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02138v1", "summary": "This study introduces and evaluates Healthy Choice, an innovative\ntheory-driven and AI-enhanced simulation platform designed to cultivate\nnutrition literacy through interactive scenario-based learning experiences. We\ncollected feedback from 114 university students with diverse backgrounds who\ncompleted simulated product selection scenarios. Quantitative ratings of\nusefulness and ease of use demonstrated high user satisfaction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02138v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "一个理论驱动和人工智能增强的营养素养培养模拟平台", "tldr": "该研究介绍并评估了一个名为Healthy Choice的理论驱动和人工智能增强的模拟平台，旨在通过互动场景学习培养营养素养，并获得了用户的高度满意度。", "motivation": "该研究的动机是开发一个创新平台，通过互动场景式学习体验来培养营养素养。", "method": "本研究介绍了Healthy Choice平台，并收集了114名不同背景的大学生完成模拟产品选择场景后的反馈。通过对有用性和易用性的定量评分来评估。", "result": "用户对Healthy Choice平台表现出高度满意度，有用性和易用性的定量评分很高。", "conclusion": "Healthy Choice平台是一个有效且用户满意的工具，能够通过互动模拟培养营养素养。", "translation": "本研究介绍并评估了Healthy Choice，一个创新的、理论驱动和人工智能增强的模拟平台，旨在通过互动场景式学习体验培养营养素养。我们收集了114名不同背景的大学生完成模拟产品选择场景后的反馈。对有用性和易用性的定量评分表明用户满意度很高。", "summary": "本研究推出并评估了名为Healthy Choice的模拟平台，该平台结合了理论指导和人工智能技术，旨在通过基于场景的互动学习提升营养素养。对114名大学生的测试结果显示，用户对该平台的有用性和易用性评价很高，表明其具有良好的用户满意度。", "keywords": "营养素养, 模拟平台, 人工智能, 理论驱动, Healthy Choice", "comments": "该论文的创新之处在于结合了理论驱动和AI增强技术，创建了一个用于营养素养培养的模拟平台。其重要性在于提供了一种互动且沉浸式的学习方法。然而，抽象中未提及长期效果或学习成果的深度评估。"}}
{"id": "2507.02164", "title": "Hardware-Accelerated Algorithm for Complex Function Roots Density Graph Plotting", "authors": ["Ruibai Tang", "Chengbin Quan"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02164v1", "summary": "Solving and visualizing the potential roots of complex functions is essential\nin both theoretical and applied domains, yet often computationally intensive.\nWe present a hardware-accelerated algorithm for complex function roots density\ngraph plotting by approximating functions with polynomials and solving their\nroots using single-shift QR iteration. By leveraging the Hessenberg structure\nof companion matrices and optimizing QR decomposition with Givens rotations, we\ndesign a pipelined FPGA architecture capable of processing a large amount of\npolynomials with high throughput. Our implementation achieves up to 65x higher\nenergy efficiency than CPU-based approaches, and while it trails modern GPUs in\nperformance due to differences in fabrication technique.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02164v1", "cate": "cs.AR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "复杂函数根密度图绘制的硬件加速算法", "tldr": "本文提出一种硬件加速算法，通过多项式逼近和单移QR迭代，在FPGA上高效绘制复杂函数根密度图，实现了比CPU高65倍的能效。", "motivation": "解决和可视化复杂函数的潜在根在理论和应用领域都很重要，但往往计算密集，需要更高效的计算方法。", "method": "通过多项式逼近复杂函数，并使用单移QR迭代求解其根。利用伴随矩阵的Hessenberg结构和优化Givens旋转的QR分解，设计了流水线化的FPGA架构，以高吞吐量处理大量多项式。", "result": "实现比基于CPU的方法高65倍的能效提升。由于制造技术差异，性能略逊于现代GPU。", "conclusion": "通过硬件加速（FPGA）可以显著提高复杂函数根密度图绘制的能效，为计算密集型任务提供了一条高效的路径。", "translation": "解决和可视化复杂函数的潜在根在理论和应用领域都至关重要，但往往计算密集。我们提出了一种用于复杂函数根密度图绘制的硬件加速算法，该算法通过多项式逼近函数并使用单移QR迭代求解其根。通过利用伴随矩阵的Hessenberg结构并用Givens旋转优化QR分解，我们设计了一个流水线化的FPGA架构，能够以高吞吐量处理大量多项式。我们的实现比基于CPU的方法实现了高达65倍的能效提升，尽管由于制造技术的差异，其性能落后于现代GPU。", "summary": "本文介绍了一种用于复杂函数根密度图绘制的硬件加速算法。该算法通过将复杂函数近似为多项式，并利用单移QR迭代求解其根。研究人员通过优化QR分解和设计流水线化的FPGA架构，实现了对大量多项式的高吞吐量处理。实验结果表明，该实现比基于CPU的方法能效高出65倍，尽管性能上略逊于现代GPU。", "keywords": "硬件加速, 复杂函数根, 密度图, FPGA, QR迭代", "comments": "该论文的创新点在于将复杂函数根的求解与硬件加速技术相结合，特别是利用FPGA的并行处理能力来显著提升能效。尽管在绝对性能上可能无法超越顶尖GPU，但其在能效方面的巨大优势对于资源受限或需要低功耗的应用场景具有重要意义。该方法通过数值逼近和高效迭代，提供了一种实用的计算密集型问题解决方案。"}}
{"id": "2507.02166", "title": "Generating Large Semi-Synthetic Graphs of Any Size", "authors": ["Rodrigo Tuna", "Carlos Soares"], "categories": ["cs.SI", "cs.AI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02166v1", "summary": "Graph generation is an important area in network science. Traditional\napproaches focus on replicating specific properties of real-world graphs, such\nas small diameters or power-law degree distributions. Recent advancements in\ndeep learning, particularly with Graph Neural Networks, have enabled\ndata-driven methods to learn and generate graphs without relying on predefined\nstructural properties. Despite these advances, current models are limited by\ntheir reliance on node IDs, which restricts their ability to generate graphs\nlarger than the input graph and ignores node attributes. To address these\nchallenges, we propose Latent Graph Sampling Generation (LGSG), a novel\nframework that leverages diffusion models and node embeddings to generate\ngraphs of varying sizes without retraining. The framework eliminates the\ndependency on node IDs and captures the distribution of node embeddings and\nsubgraph structures, enabling scalable and flexible graph generation.\nExperimental results show that LGSG performs on par with baseline models for\nstandard metrics while outperforming them in overlooked ones, such as the\ntendency of nodes to form clusters. Additionally, it maintains consistent\nstructural characteristics across graphs of different sizes, demonstrating\nrobustness and scalability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02166v1", "cate": "cs.SI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "生成任意大小的大型半合成图", "tldr": "LGSG是一种基于扩散模型和节点嵌入的新颖框架，能够生成任意大小的图，解决了现有图生成模型依赖节点ID和无法生成更大图的限制，并在多种指标上表现出色。", "motivation": "现有图生成模型（包括基于深度学习的方法）受限于对节点ID的依赖，这限制了它们生成比输入图更大图的能力，并且忽略了节点属性。", "method": "我们提出了潜图采样生成（LGSG）框架，该框架利用扩散模型和节点嵌入来生成不同大小的图，无需重新训练。它消除了对节点ID的依赖，并捕获节点嵌入和子图结构的分布。", "result": "实验结果表明，LGSG在标准指标上与基线模型表现相当，但在被忽视的指标（如节点形成簇的趋势）上优于它们。此外，它在不同大小的图上保持一致的结构特征。", "conclusion": "LGSG框架通过消除对节点ID的依赖并利用扩散模型和节点嵌入，成功实现了可扩展且灵活的图生成，能够生成任意大小的图，并在性能和鲁棒性方面表现出色。", "translation": "图生成是网络科学中的一个重要领域。传统方法侧重于复制真实世界图的特定属性，例如小直径或幂律度分布。深度学习的最新进展，特别是图神经网络，使得数据驱动方法无需依赖预定义的结构属性即可学习和生成图。尽管取得了这些进展，但当前模型受限于它们对节点ID的依赖，这限制了它们生成比输入图更大的图的能力，并且忽略了节点属性。为了解决这些挑战，我们提出了潜图采样生成（LGSG），这是一种新颖的框架，它利用扩散模型和节点嵌入来生成不同大小的图，无需重新训练。该框架消除了对节点ID的依赖，并捕获节点嵌入和子图结构的分布，从而实现了可扩展和灵活的图生成。实验结果表明，LGSG在标准指标上与基线模型表现相当，但在被忽视的指标（如节点形成簇的趋势）上优于它们。此外，它在不同大小的图上保持一致的结构特征，展示了鲁棒性和可扩展性。", "summary": "本文提出了一种名为潜图采样生成（LGSG）的新颖框架，用于解决现有图生成模型在生成任意大小图时受限于节点ID和无法处理节点属性的问题。LGSG利用扩散模型和节点嵌入，消除了对节点ID的依赖，并能捕获节点嵌入和子图结构的分布，从而实现了无需重新训练即可生成不同大小图的能力。实验证明，LGSG在标准指标上与基线模型相当，但在诸如节点聚类趋势等被忽视的指标上表现更优，并且在不同图大小下保持了结构特征的一致性，展现了其鲁棒性和可扩展性。", "keywords": "图生成, 扩散模型, 节点嵌入, 半合成图, 可扩展性", "comments": "这项工作在图生成领域具有重要创新性，因为它解决了当前深度学习图生成模型在可伸缩性和灵活性方面的核心限制。通过引入扩散模型和节点嵌入来摆脱对节点ID的依赖，LGSG提供了一种生成任意大小半合成图的通用方法。其在被忽视的指标（如聚类）上的改进以及跨不同大小图的结构一致性，突显了该方法的实用性和潜力。"}}
{"id": "2507.01988", "title": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers", "authors": ["Giyong Jung", "Saeid Gorgin", "John Kim", "Jungrae Kim"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      12 pages, 8 figures. This paper is accepted for [2025 The International Conference for High Performance Computing, Networking, Storage and Analysis (SC)]", "url": "http://arxiv.org/abs/2507.01988v1", "summary": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency.", "comment": "12 pages, 8 figures. This paper is accepted for [2025 The\n  International Conference for High Performance Computing, Networking, Storage\n  and Analysis (SC)]", "pdf_url": "http://arxiv.org/pdf/2507.01988v1", "cate": "cs.NI", "date": "2025-06-28", "updated": "2025-06-28", "AI": {"title_translation": "使用隐式序列号扩展芯片互连网络", "tldr": "本文提出隐式序列号（ISN）和可靠性扩展链路（RXL），解决多节点芯片互连网络中静默丢弃和乱序传输问题，在不增加开销的情况下提供可扩展的可靠性。", "motivation": "随着AI模型超越单处理器能力，芯片间互连成为可扩展计算的关键；现有互连协议传输速率高，但易受错误影响，尤其是在多节点配置中管理静默丢弃的flit是一个挑战。", "method": "引入隐式序列号（ISN）机制，用于精确检测flit丢弃和确保按序交付，且不增加头部开销。提出可靠性扩展链路（RXL），作为CXL的扩展，整合ISN。将CRC提升为传输层机制用于端到端数据和序列完整性，并依赖FEC进行链路层错误纠正和检测。", "result": "RXL在不损害带宽效率的情况下，提供了强大的可靠性和可扩展性。", "conclusion": "隐式序列号（ISN）和可靠性扩展链路（RXL）机制能够实现可扩展、可靠的多节点芯片互连，同时保持与现有flit结构的兼容性。", "translation": "随着AI模型超越了单处理器的能力，芯片间的互连已成为可扩展计算的关键推动因素。这些处理器以缓存行粒度交换大量数据，促使CXL、NVLink和UALink等为高带宽和小负载设计的新互连协议得以采用。然而，这些协议不断提高的传输速率增加了对错误的敏感性。虽然循环冗余校验（CRC）和前向纠错（FEC）等机制是可靠数据传输的标准，但将芯片互连扩展到多节点配置带来了新的挑战，特别是在管理交换设备中静默丢弃的flit方面。本文引入了隐式序列号（ISN），这是一种新颖的机制，可在不增加头部开销的情况下确保精确的flit丢弃检测和按序交付。此外，我们提出了可靠性扩展链路（RXL），它是CXL的扩展，整合了ISN以支持可扩展、可靠的多节点互连，同时保持与现有flit结构的兼容性。通过将CRC提升为用于端到端数据和序列完整性的传输层机制，并依赖FEC进行链路层错误纠正和检测，RXL在不影响带宽效率的情况下提供了强大的可靠性和可扩展性。", "summary": "本文针对AI计算中多节点芯片互连网络面临的静默丢弃和乱序传输挑战，提出了一种名为隐式序列号（ISN）的新机制，能在不增加开销的情况下精确检测flit丢弃并确保按序交付。在此基础上，作者进一步提出了可靠性扩展链路（RXL），作为CXL协议的扩展，通过将CRC提升至传输层用于端到端完整性，并将FEC用于链路层纠错，实现了可扩展、可靠且高效的多节点互连。", "keywords": "芯片互连, 隐式序列号, 可靠性扩展链路, CXL, 可扩展计算", "comments": "该论文的创新点在于提出了隐式序列号（ISN），它在不增加协议头部开销的情况下解决了多节点芯片互连中静默丢弃和乱序传输的关键问题。通过将ISN整合到CXL扩展（RXL）中，并重新定位CRC和FEC的作用，该方案在确保可靠性和可扩展性的同时，维护了带宽效率和现有协议兼容性，对于未来大规模AI系统和数据中心互连具有重要意义。"}}
{"id": "2507.02158", "title": "Signalling Health for Improved Kubernetes Microservice Availability", "authors": ["Jacob Roberts", "Blair Archibald", "Phil Trinder"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      10 pages, 7 figures", "url": "http://arxiv.org/abs/2507.02158v1", "summary": "Microservices are often deployed and managed by a container orchestrator that\ncan detect and fix failures to maintain the service availability critical in\nmany applications. In Poll-based Container Monitoring (PCM), the orchestrator\nperiodically checks container health. While a common approach, PCM requires\ncareful tuning, may degrade service availability, and can be slow to detect\ncontainer health changes. An alternative is Signal-based Container Monitoring\n(SCM), where the container signals the orchestrator when its status changes. We\npresent the design, implementation, and evaluation of an SCM approach for\nKubernetes and empirically show that it has benefits over PCM, as predicted by\na new mathematical model. We compare the service availability of SCM and PCM\nover six experiments using the SockShop benchmark. SCM does not require that\npolling intervals are tuned, and yet detects container failure 86\\% faster than\nPCM and container readiness in a comparable time with limited resource\noverheads. We find PCM can erroneously detect failures, and this reduces\nservice availability by 4\\%. We propose that orchestrators offer SCM features\nfor faster failure detection than PCM without erroneous detections or careful\ntuning.", "comment": "10 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.02158v1", "cate": "cs.DC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "通过信号健康状况提高Kubernetes微服务可用性", "tldr": "本文提出了一种基于信号的容器监控（SCM）方法，用于Kubernetes，与传统的基于轮询的监控（PCM）相比，SCM能更快地检测故障，无需调优，并减少误报，从而提高微服务可用性。", "motivation": "在微服务架构中，容器编排器（如Kubernetes）通过检测和修复故障来维持服务可用性。然而，传统的基于轮询的容器监控（PCM）方法需要仔细调优，可能降低服务可用性，并且检测容器健康变化的速度较慢。", "method": "本文提出并评估了一种基于信号的容器监控（SCM）方法，其中容器在状态改变时向编排器发送信号。作者设计、实现并评估了适用于Kubernetes的SCM方法。通过一个新的数学模型预测其优势，并通过使用SockShop基准在六个实验中比较了SCM和PCM的服务可用性。", "result": "SCM无需调优轮询间隔，检测容器故障的速度比PCM快86%，检测容器就绪的时间与PCM相当，且资源开销有限。研究发现PCM会错误地检测故障，这导致服务可用性降低4%。", "conclusion": "编排器应提供SCM功能，以实现比PCM更快的故障检测，且没有错误检测和无需仔细调优。", "translation": "微服务通常由容器编排器部署和管理，该编排器可以检测和修复故障，以维持在许多应用中至关重要的服务可用性。在基于轮询的容器监控（PCM）中，编排器定期检查容器健康状况。尽管这是一种常见方法，但PCM需要仔细调优，可能会降低服务可用性，并且检测容器健康变化的速度较慢。另一种方法是基于信号的容器监控（SCM），其中容器在状态改变时向编排器发送信号。我们提出了Kubernetes SCM方法的设计、实现和评估，并通过实验证明了它比PCM具有优势，这与新的数学模型预测一致。我们使用SockShop基准在六个实验中比较了SCM和PCM的服务可用性。SCM无需调优轮询间隔，但检测容器故障的速度比PCM快86%，检测容器就绪的时间与PCM相当，且资源开销有限。我们发现PCM会错误地检测故障，这导致服务可用性降低4%。我们建议编排器提供SCM功能，以实现比PCM更快的故障检测，且没有错误检测或无需仔细调优。", "summary": "本文提出并评估了一种名为信号驱动容器监控（SCM）的新方法，旨在提高Kubernetes微服务的可用性。与传统的轮询驱动容器监控（PCM）相比，SCM通过容器主动报告状态变化，显著加快了故障检测速度（快86%），并且无需复杂的参数调优。实验结果表明，SCM不仅提高了故障检测效率，还避免了PCM可能导致的错误故障检测，从而提升了整体服务可用性。作者建议未来的容器编排器应集成SCM功能。", "keywords": "Kubernetes, 微服务, 容器监控, 可用性, 故障检测", "comments": "本文提出了一种有前景的容器健康监控方法，通过从被动轮询转变为主动信号，有效解决了传统方法的痛点。其创新性在于将信号机制应用于容器健康监控，并提供了量化的性能提升数据。对于追求高可用性和自动化运维的云原生系统而言，SCM具有重要意义，因为它能减少人工干预和系统恢复时间。"}}
{"id": "2507.02413", "title": "Defining DLT Immutability: A Qualitative Survey of Node Operators", "authors": ["Alex Lynham", "Geoff Goodell"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      27 pages, 2 figures, 6 tables", "url": "http://arxiv.org/abs/2507.02413v1", "summary": "Immutability is a core design goal of permissionless public blockchain\nsystems. However, rewrites are more common than is normally understood, and the\nrisk of rewrite, cyberattack, exploit or black swan event is also high. Taking\nthe position that strict immutability is neither possible on these networks nor\nthe observed reality, this paper uses thematic analysis of node operator\ninterviews to examine the limits of immutability in light of rewrite events.\nThe end result is a qualitative definition of the conditional immutability\nfound on these networks, which we call Practical Immutability. This is\nimmutability contingent on the legitimate governance demands of the network,\nwhere network stakeholders place their trust in the governance topology of a\nnetwork to lend it legitimacy, and thus manage ledger state.", "comment": "27 pages, 2 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.02413v1", "cate": "cs.CY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "定义 DLT 不变性：对节点运营商的定性调查", "tldr": "本文通过对节点运营商的采访，定义了区块链网络中一种名为“实用不变性”的条件不变性，指出严格不变性既不可能也不现实。", "motivation": "尽管不变性是公共区块链系统的核心设计目标，但实际中重写事件比普遍认为的更常见，且存在高风险。论文认为严格不变性在这些网络中既不可能实现也不是观察到的现实，因此需要探讨不变性的局限性。", "method": "论文采用了主题分析法，对节点运营商进行了访谈，以审视重写事件背景下不变性的局限性。", "result": "论文最终提出了对这些网络中发现的条件不变性的定性定义，称之为“实用不变性”。这种不变性取决于网络合法的治理需求，其中网络利益相关者将信任置于网络的治理拓扑中，以赋予其合法性并管理账本状态。", "conclusion": "区块链的严格不变性在实践中是无法实现的，取而代之的是一种名为“实用不变性”的条件不变性，它依赖于网络的合法治理机制和利益相关者的信任。", "translation": "不变性是无需许可的公共区块链系统的核心设计目标。然而，重写比通常理解的更常见，重写、网络攻击、漏洞利用或黑天鹅事件的风险也很高。本文认为严格不变性在这些网络中既不可能实现也不是观察到的现实，因此通过对节点运营商访谈进行主题分析，审视了重写事件背景下不变性的局限性。最终结果是对这些网络中发现的条件不变性给出了定性定义，我们称之为“实用不变性”。这种不变性取决于网络合法的治理需求，其中网络利益相关者将信任置于网络的治理拓扑中，以赋予其合法性并管理账本状态。", "summary": "本文探讨了分布式账本技术（DLT）中不变性的实际限制。尽管不变性是区块链的核心目标，但研究指出严格不变性在实践中难以实现且不符合现实。通过对节点运营商的定性访谈和主题分析，论文定义了一种新的“实用不变性”概念，即一种依赖于网络合法治理需求和利益相关者信任的条件不变性，以更好地反映实际的账本状态管理。", "keywords": "DLT 不变性, 区块链, 节点运营商, 实用不变性, 治理", "comments": "这篇论文通过定性研究方法，挑战了对区块链“不变性”的传统认知，提出了更符合实际操作的“实用不变性”概念。其创新之处在于从节点运营商的视角出发，揭示了治理在账本状态管理中的关键作用，为理解DLT的实际运作提供了重要见解。"}}
{"id": "2507.02524", "title": "Time Resolution Independent Operator Learning", "authors": ["Diab W. Abueidda", "Mbebo Nonna", "Panos Pantidis", "Mostafa E. Mobasher"], "categories": ["cs.CE", "cs.NA", "math.NA"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02524v1", "summary": "Accurately learning solution operators for time-dependent partial\ndifferential equations (PDEs) from sparse and irregular data remains a\nchallenging task. Recurrent DeepONet extensions inherit the discrete-time\nlimitations of sequence-to-sequence (seq2seq) RNN architectures, while\nneural-ODE surrogates cannot incorporate new inputs after initialization. We\nintroduce NCDE-DeepONet, a continuous-time operator network that embeds a\nNeural Controlled Differential Equation (NCDE) in the branch and augments the\ntrunk with explicit space-time coordinates. The NCDE encodes an entire load\nhistory as the solution of a controlled ODE driven by a spline-interpolated\ninput path, making the representation input-resolution-independent: it encodes\ndifferent input signal discretizations of the observed samples. The trunk then\nprobes this latent path at arbitrary spatial locations and times, rendering the\noverall map output-resolution independent: predictions can be queried on meshes\nand time steps unseen during training without retraining or interpolation.\nBenchmarks on transient Poisson, elastodynamic, and thermoelastic problems\nconfirm the robustness and accuracy of the framework, achieving almost instant\nsolution prediction. These findings suggest that controlled dynamics provide a\nprincipled and efficient foundation for high-fidelity operator learning in\ntransient mechanics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02524v1", "cate": "cs.CE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "时间分辨率无关的算子学习", "tldr": "本文提出NCDE-DeepONet，一个连续时间算子网络，能够从稀疏不规则数据中学习时间依赖PDE的解算子，实现输入和输出分辨率无关的预测。", "motivation": "从稀疏和不规则数据中准确学习时间依赖偏微分方程（PDEs）的解算子仍然是一项挑战。现有的循环DeepONet扩展继承了序列到序列（seq2seq）RNN架构的离散时间限制，而神经ODE代理在初始化后无法纳入新的输入。", "method": "本文引入NCDE-DeepONet，一个连续时间算子网络。它在分支网络中嵌入了一个神经控制微分方程（NCDE），并用显式时空坐标增强主干网络。NCDE将整个负载历史编码为由样条插值输入路径驱动的受控ODE的解，使其表示独立于输入分辨率。主干网络随后在任意空间位置和时间探测这个潜在路径，使整体映射独立于输出分辨率，即可以在训练期间未见的网格和时间步上进行预测，无需重新训练或插值。", "result": "在瞬态泊松、弹性动力学和热弹性问题上的基准测试证实了该框架的鲁棒性和准确性，实现了几乎即时的解预测。", "conclusion": "这些发现表明，受控动力学为瞬态力学中的高保真算子学习提供了原则性且高效的基础。", "translation": "准确地从稀疏和不规则数据中学习时间依赖偏微分方程（PDEs）的解算子仍然是一项具有挑战性的任务。循环DeepONet扩展继承了序列到序列（seq2seq）RNN架构的离散时间限制，而神经ODE代理在初始化后无法纳入新的输入。我们引入了NCDE-DeepONet，一个连续时间算子网络，它在分支网络中嵌入了一个神经控制微分方程（NCDE），并用显式时空坐标增强了主干网络。NCDE将整个负载历史编码为由样条插值输入路径驱动的受控ODE的解，使表示独立于输入分辨率：它编码了观测样本的不同输入信号离散化。主干网络随后在任意空间位置和时间探测这个潜在路径，使整体映射独立于输出分辨率：预测可以在训练期间未见的网格和时间步上进行查询，无需重新训练或插值。在瞬态泊松、弹性动力学和热弹性问题上的基准测试证实了该框架的鲁棒性和准确性，实现了几乎即时的解预测。这些发现表明，受控动力学为瞬态力学中的高保真算子学习提供了原则性且高效的基础。", "summary": "本文提出了NCDE-DeepONet，一种用于学习时间依赖偏微分方程解算子的连续时间算子网络。该模型通过在分支网络中嵌入神经控制微分方程（NCDE）并用显式时空坐标增强主干网络，解决了现有方法在处理稀疏、不规则数据及输入/输出分辨率依赖性方面的局限。NCDE确保了输入分辨率无关的表示，而增强的主干网络实现了输出分辨率无关的预测。基准测试表明，NCDE-DeepONet在瞬态力学问题上表现出高鲁棒性、准确性和近乎即时的预测能力，为高保真算子学习提供了一种有效的新方法。", "keywords": "算子学习, 神经控制微分方程, 偏微分方程, 时间分辨率无关, DeepONet", "comments": "该论文的创新点在于将神经控制微分方程（NCDE）引入算子学习，解决了传统DeepONet和神经ODE在处理时间依赖PDEs时面临的离散时间限制和输入分辨率依赖问题。通过实现输入和输出分辨率无关的预测，NCDE-DeepONet显著提升了模型的通用性和实用性，尤其是在处理不规则和稀疏数据时。其在瞬态力学问题上的高效和准确性也显示出其在实际工程应用中的巨大潜力。"}}
{"id": "2507.02073", "title": "HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection", "authors": ["Nikita Bhedasgaonkar", "Rushikesh K. Joshi"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      11 pages, 5 tables, 2 figures", "url": "http://arxiv.org/abs/2507.02073v1", "summary": "In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting\nRules), a lightweight rule-based feature selection method that combines\nParameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to\neliminate redundant features and retain relevant ones. This method is a hybrid\nof non-iterative and iterative filtering approaches for dimensionality\nreduction. It is a greedy method, which works by backward elimination,\neliminating possibly multiple features at every step. The rules contribute to\nvoting for features, and a decision to keep or discard is made by majority\nvoting. The rules make use of correlation thresholds between every pair of\nfeatures, and between features and the target. We provide the results from the\napplication of HCVR to the SPAMBASE dataset. The results showed improvement\nperformance as compared to traditional non-iterative (CFS, mRMR and MI) and\niterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was\nassessed based on the performance of different classifiers after applying\nfiltering.", "comment": "11 pages, 5 tables, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.02073v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "HCVR：一种基于相关性投票规则的混合特征选择方法", "tldr": "HCVR是一种轻量级的混合特征选择方法，结合P2P和P2T相关性，通过贪婪的向后消除和多数投票来选择特征，在SPAMBASE数据集上表现优于传统方法。", "motivation": "为了消除冗余特征并保留相关特征，从而进行有效的维度约简。", "method": "本文提出HCVR（Hybrid approach with Correlation-aware Voting Rules），一种轻量级的基于规则的特征选择方法。该方法结合参数间（P2P）和参数到目标（P2T）相关性。它是一种非迭代和迭代过滤方法的混合体，采用贪婪的向后消除方式，每一步可能消除多个特征。规则通过投票决定特征的保留或丢弃，并采用多数投票机制。规则利用特征对之间以及特征与目标之间的相关性阈值。", "result": "将HCVR应用于SPAMBASE数据集的结果表明，与传统的非迭代（CFS、mRMR和MI）和迭代（RFE、SFS和遗传算法）技术相比，性能有所改进。有效性是根据应用过滤后不同分类器的性能进行评估的。", "conclusion": "HCVR是一种有效且轻量级的特征选择方法，它通过结合P2P和P2T相关性并采用混合迭代/非迭代的贪婪策略，显著提高了分类器的性能。", "translation": "本文提出HCVR（Hybrid approach with Correlation-aware Voting Rules），一种轻量级的基于规则的特征选择方法，它结合了参数间（P2P）和参数到目标（P2T）相关性，以消除冗余特征并保留相关特征。该方法是维度约简中非迭代和迭代过滤方法的混合体。它是一种贪婪方法，通过向后消除工作，每一步可能消除多个特征。规则有助于对特征进行投票，并通过多数投票决定保留或丢弃。规则利用每对特征之间以及特征与目标之间的相关性阈值。我们提供了HCVR应用于SPAMBASE数据集的结果。结果显示，与传统的非迭代（CFS、mRMR和MI）和迭代（RFE、SFS和遗传算法）技术相比，性能有所改进。有效性是根据应用过滤后不同分类器的性能进行评估的。", "summary": "本文介绍了一种名为HCVR的轻量级混合特征选择方法。HCVR通过结合参数间（P2P）和参数到目标（P2T）相关性来识别并消除冗余特征，同时保留相关特征。该方法融合了非迭代和迭代过滤策略，采用贪婪的向后消除方式，并利用基于相关性阈值的投票规则进行特征决策。实验结果表明，在SPAMBASE数据集上，HCVR相较于多种传统非迭代和迭代特征选择技术，能有效提升分类器性能。", "keywords": "特征选择, HCVR, 相关性, 投票规则, 维度约简", "comments": "HCVR的创新点在于其混合方法，结合了P2P和P2T相关性，并通过基于规则的投票机制实现特征选择。这种结合非迭代和迭代方法的贪婪向后消除策略，以及对相关性阈值的利用，使其在处理冗余和相关特征方面具有独特优势，并表现出优于传统方法的性能。"}}
{"id": "2507.02148", "title": "Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning", "authors": ["Zijie Cai", "Christopher Metzler"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02148v1", "summary": "Monocular depth estimation has recently advanced to provide not only relative\nbut also metric depth predictions. However, its reliability in underwater\nenvironments remains limited due to light attenuation and scattering, color\ndistortion, turbidity, and the lack of high-quality metric ground-truth data.\nIn this paper, we present a comprehensive benchmark of zero-shot and fine-tuned\nmonocular metric depth estimation models on real-world underwater datasets with\nmetric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of\nstate-of-the-art models across a range of underwater conditions with different\nranges. Our results show that large-scale models trained on terrestrial (real\nor synthetic) data, while effective in in-air settings, perform poorly\nunderwater due to significant domain shifts. To address this, we fine-tune\nDepth Anything V2 with a ViT-S backbone encoder on a synthetic underwater\nvariant of the Hypersim dataset, which we generated using a physically based\nunderwater image formation model. We demonstrate our fine-tuned model\nconsistently improves performance across all benchmarks and outperforms\nbaselines trained only on the clean in-air Hypersim dataset. Our study provides\na detailed evaluation and visualization for monocular metric depth estimation\nin underwater scenes, highlighting the importance of domain adaptation and\nscale-aware supervision for achieving robust and generalizable metric depth\npredictions in challenging underwater environments for future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02148v1", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "水下单目度量深度估计：真实世界基准与合成微调", "tldr": "该研究评估了水下环境中单目度量深度估计模型的性能，发现陆地训练模型表现不佳。通过在合成水下数据集上对Depth Anything V2进行微调，显著提升了模型在水下场景中的深度估计精度。", "motivation": "水下环境（光衰减、散射、颜色失真、浊度）和高质量度量真值数据的缺乏限制了单目深度估计在水下的可靠性，而陆地训练的模型由于领域差异在水下表现不佳。", "method": "本文首先对零样本和微调的单目度量深度估计模型在真实世界水下数据集（如FLSea和SQUID）上进行了综合基准测试。然后，为了解决领域差异问题，作者使用基于物理的水下图像形成模型生成了一个合成水下版Hypersim数据集，并在此数据集上对带有ViT-S骨干编码器的Depth Anything V2模型进行了微调。", "result": "基准测试结果显示，在陆地数据（真实或合成）上训练的大规模模型在水下表现不差。然而，通过在合成水下Hypersim数据集上微调Depth Anything V2模型，其性能在所有基准测试中都得到了持续提升，并且优于仅在干净陆地Hypersim数据集上训练的基线模型。", "conclusion": "本研究为水下场景中的单目度量深度估计提供了详细的评估和可视化，强调了领域适应和尺度感知监督对于在挑战性水下环境中实现鲁棒和可泛化的度量深度预测的重要性，并为未来的研究指明了方向。", "translation": "单目深度估计最近取得了进展，不仅能提供相对深度预测，还能提供度量深度预测。然而，由于光衰减和散射、颜色失真、浊度以及高质量度量真值数据的缺乏，其在水下环境中的可靠性仍然有限。在本文中，我们提出了一个在具有度量深度标注的真实世界水下数据集（如FLSea和SQUID）上对零样本和微调的单目度量深度估计模型进行全面基准测试。我们评估了一系列最先进的模型在不同范围的水下条件下的表现。我们的结果表明，在陆地（真实或合成）数据上训练的大规模模型虽然在空中环境中有效，但由于显著的领域差异，在水下表现不佳。为了解决这个问题，我们使用基于物理的水下图像形成模型生成了一个合成水下版Hypersim数据集，并用它对带有ViT-S骨干编码器的Depth Anything V2进行了微调。我们证明了我们微调后的模型在所有基准测试中都持续提升了性能，并优于仅在干净的空中Hypersim数据集上训练的基线模型。我们的研究为水下场景中的单目度量深度估计提供了详细的评估和可视化，强调了领域适应和尺度感知监督对于在挑战性水下环境中实现鲁棒和可泛化的度量深度预测的重要性，为未来的研究提供了指导。", "summary": "本论文旨在解决单目度量深度估计在水下环境中因光衰减、颜色失真和数据缺乏等问题导致的可靠性不足。研究首先对现有模型在真实水下数据集上进行了基准测试，发现陆地训练的模型表现不佳。为应对此挑战，作者基于物理模型生成了合成水下Hypersim数据集，并在此数据集上对Depth Anything V2模型进行了微调。实验结果表明，微调后的模型在各项水下基准测试中均显著优于基线模型，强调了领域适应和尺度感知监督在水下深度估计中的关键作用。", "keywords": "水下深度估计, 单目视觉, 领域适应, 合成数据, 深度学习", "comments": "该论文通过构建真实世界水下基准和利用合成数据进行微调，有效解决了水下单目深度估计领域的关键挑战。其创新点在于提出了一个基于物理模型生成合成水下数据集的方法，并成功利用该数据对现有SOTA模型进行了领域适应，显著提升了水下深度估计的性能。这对于水下机器人、水下测绘等应用具有重要意义。文章清晰地展示了领域适应的必要性和有效性。"}}
{"id": "2507.02098", "title": "A robust and adaptive MPC formulation for Gaussian process models", "authors": ["Mathieu Dubied", "Amon Lahr", "Melanie N. Zeilinger", "Johannes Köhler"], "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02098v1", "summary": "In this paper, we present a robust and adaptive model predictive control\n(MPC) framework for uncertain nonlinear systems affected by bounded\ndisturbances and unmodeled nonlinearities. We use Gaussian Processes (GPs) to\nlearn the uncertain dynamics based on noisy measurements, including those\ncollected during system operation. As a key contribution, we derive robust\npredictions for GP models using contraction metrics, which are incorporated in\nthe MPC formulation. The proposed design guarantees recursive feasibility,\nrobust constraint satisfaction and convergence to a reference state, with high\nprobability. We provide a numerical example of a planar quadrotor subject to\ndifficult-to-model ground effects, which highlights significant improvements\nachieved through the proposed robust prediction method and through online\nlearning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02098v1", "cate": "eess.SY", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "高斯过程模型的鲁棒自适应模型预测控制公式", "tldr": "提出了一种结合高斯过程和收缩度量的鲁棒自适应MPC框架，用于处理不确定非线性系统，并保证高概率下的鲁棒性和收敛性。", "motivation": "解决受有界扰动和未建模非线性影响的不确定非线性系统的控制问题。", "method": "使用高斯过程（GP）从噪声测量中学习不确定动力学，并利用收缩度量为GP模型推导出鲁棒预测，将其整合到MPC公式中。", "result": "该设计保证了递归可行性、鲁棒约束满足和以高概率收敛到参考状态。通过一个平面四旋翼飞行器在地面效应下的数值例子，展示了所提出的鲁棒预测方法和在线学习带来的显著改进。", "conclusion": "结合高斯过程和收缩度量的鲁棒自适应MPC框架能有效处理不确定非线性系统，并在实际应用中表现出显著的性能提升。", "translation": "本文提出了一种针对受有界扰动和未建模非线性影响的不确定非线性系统的鲁棒自适应模型预测控制（MPC）框架。我们使用高斯过程（GP）基于噪声测量（包括系统运行期间收集的数据）来学习不确定动力学。作为一项关键贡献，我们利用收缩度量为GP模型推导出鲁棒预测，并将其整合到MPC公式中。所提出的设计保证了递归可行性、鲁棒约束满足和以高概率收敛到参考状态。我们提供了一个平面四旋翼飞行器在难以建模的地面效应下的数值例子，该例子突出了通过所提出的鲁棒预测方法和在线学习所实现的显著改进。", "summary": "本文提出一种鲁棒自适应模型预测控制（MPC）框架，用于处理受有界扰动和未建模非线性影响的不确定非线性系统。该框架利用高斯过程（GP）学习不确定动力学，并结合收缩度量推导出GP模型的鲁棒预测，保证了递归可行性、鲁棒约束满足和高概率收敛。数值例子验证了其在处理复杂非线性系统时的显著性能提升。", "keywords": "模型预测控制, 高斯过程, 鲁棒控制, 不确定系统, 收缩度量", "comments": "该论文的创新点在于将收缩度量应用于高斯过程模型的鲁棒预测，并将其整合到MPC框架中，有效提升了不确定非线性系统的控制性能和鲁棒性。通过在线学习和鲁棒预测，使其在实际应用中具有很高的潜力，尤其适用于难以精确建模的系统。"}}
{"id": "2507.02262", "title": "Localized kernel method for separation of linear chirps", "authors": ["Eric Mason", "Sippanon Kitimoon", "Hrushikesh Mhaskar"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02262v1", "summary": "The task of separating a superposition of signals into its individual\ncomponents is a common challenge encountered in various signal processing\napplications, especially in domains such as audio and radar signals. A previous\npaper by Chui and Mhaskar proposes a method called Signal Separation Operator\n(SSO) to find the instantaneous frequencies and amplitudes of such\nsuperpositions where both of these change continuously and slowly over time. In\nthis paper, we amplify and modify this method in order to separate chirp\nsignals in the presence of crossovers, a very low SNR, and discontinuities. We\ngive a theoretical analysis of the behavior of SSO in the presence of noise to\nexamine the relationship between the minimal separation, minimal amplitude,\nSNR, and sampling frequency. Our method is illustrated with a few examples, and\nnumerical results are reported on a simulated dataset comprising 7 simulated\nsignals.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02262v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "用于线性啁啾信号分离的局部核方法", "tldr": "本文改进了信号分离算子（SSO）方法，使其能够在交叉、极低信噪比和不连续的情况下分离线性啁啾信号，并进行了理论分析和数值验证。", "motivation": "将叠加信号分离成其独立分量是各种信号处理应用中常见的挑战，尤其是在音频和雷达信号领域。虽然Chui和Mhaskar之前的论文提出了信号分离算子（SSO）方法来寻找连续且缓慢变化的瞬时频率和幅度，但该方法需要在存在交叉、极低信噪比和不连续性的情况下进行放大和修改，以更好地分离啁啾信号。", "method": "本文放大并修改了Chui和Mhaskar提出的信号分离算子（SSO）方法，以在存在交叉、极低信噪比和不连续性的情况下分离啁啾信号。此外，本文还对SSO在噪声存在下的行为进行了理论分析，以检验最小分离度、最小幅度、信噪比和采样频率之间的关系。", "result": "该方法通过几个例子进行了说明，并在包含7个模拟信号的模拟数据集上报告了数值结果。", "conclusion": "改进后的信号分离算子（SSO）方法能够有效地在存在交叉、极低信噪比和不连续性的复杂条件下分离线性啁啾信号，并通过理论分析和数值实验得到了验证。", "translation": "将信号叠加分离成其独立分量是各种信号处理应用中常见的挑战，尤其是在音频和雷达信号领域。Chui和Mhaskar之前的一篇论文提出了一种称为信号分离算子（SSO）的方法，用于寻找这类叠加信号的瞬时频率和幅度，其中这些参数随时间连续缓慢变化。在本文中，我们放大并修改了这种方法，以便在存在交叉、极低信噪比和不连续性的情况下分离啁啾信号。我们对SSO在噪声存在下的行为进行了理论分析，以检验最小分离度、最小幅度、信噪比和采样频率之间的关系。我们的方法通过几个例子进行了说明，并在包含7个模拟信号的模拟数据集上报告了数值结果。", "summary": "本文对现有的信号分离算子（SSO）方法进行了改进和扩展，使其能够更有效地在存在信号交叉、极低信噪比和不连续性的复杂环境下分离线性啁啾信号。文章深入分析了SSO在噪声中的性能，探讨了最小分离度、最小幅度、信噪比和采样频率之间的相互关系。通过数值实例和对包含7个模拟信号的数据集的实验，验证了所提出方法的有效性。", "keywords": "线性啁啾, 信号分离, 局部核方法, 信号分离算子, 低信噪比", "comments": "该论文通过改进现有方法来解决信号处理中实际存在的挑战，如低信噪比和不连续性，这对于实际应用具有重要意义。理论分析的加入增强了对该方法性能边界的理解，使其更加稳健。"}}
{"id": "2507.02192", "title": "An Investigation on Combining Geometry and Consistency Constraints into Phase Estimation for Speech Enhancement", "authors": ["Chun-Wei Ho", "Pin-Jui Ku", "Hao Yen", "Sabato Marco Siniscalchi", "Yu Tsao", "Chin-Hui Lee"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      5 pages", "url": "http://arxiv.org/abs/2507.02192v1", "summary": "We propose a novel iterative phase estimation framework, termed multi-source\nGriffin-Lim algorithm (MSGLA), for speech enhancement (SE) under additive noise\nconditions. The core idea is to leverage the ad-hoc consistency constraint of\ncomplex-valued short-time Fourier transform (STFT) spectrograms to address the\nsign ambiguity challenge commonly encountered in geometry-based phase\nestimation. Furthermore, we introduce a variant of the geometric constraint\nframework based on the law of sines and cosines, formulating a new phase\nreconstruction algorithm using noise phase estimates. We first validate the\nproposed technique through a series of oracle experiments, demonstrating its\neffectiveness under ideal conditions. We then evaluate its performance on the\nVB-DMD and WSJ0-CHiME3 data sets, and show that the proposed MSGLA variants\nmatch well or slightly outperform existing algorithms, including direct phase\nestimation and DNN-based sign prediction, especially in terms of background\nnoise suppression.", "comment": "5 pages", "pdf_url": "http://arxiv.org/pdf/2507.02192v1", "cate": "eess.AS", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "结合几何和一致性约束的语音增强相位估计研究", "tldr": "提出了一种新的多源Griffin-Lim算法（MSGLA），通过结合STFT一致性和几何约束来解决语音增强中相位估计的符号模糊问题，并在降噪方面表现良好。", "motivation": "解决几何相位估计中常见的符号模糊问题，并改进加性噪声条件下的语音增强性能。", "method": "提出了一种新颖的迭代相位估计框架——多源Griffin-Lim算法（MSGLA）。该方法利用复值短时傅里叶变换（STFT）谱图的特设一致性约束来解决几何相位估计中的符号模糊挑战。此外，还引入了一种基于正弦和余弦定律的几何约束框架变体，并利用噪声相位估计公式化了一种新的相位重建算法。", "result": "通过一系列理想实验验证了所提技术的有效性。在VB-DMD和WSJ0-CHiME3数据集上评估结果显示，所提出的MSGLA变体与现有算法（包括直接相位估计和基于DNN的符号预测）表现相当或略优，尤其在背景噪声抑制方面表现突出。", "conclusion": "所提出的MSGLA算法通过有效结合几何和一致性约束，能够有效解决相位估计中的符号模糊问题，并在加性噪声条件下的语音增强中取得良好甚至超越现有算法的性能，尤其在背景噪声抑制方面表现优异。", "translation": "我们提出了一种新颖的迭代相位估计框架，称为多源Griffin-Lim算法（MSGLA），用于加性噪声条件下的语音增强（SE）。其核心思想是利用复值短时傅里叶变换（STFT）谱图的特设一致性约束来解决几何相位估计中常见的符号模糊挑战。此外，我们引入了一种基于正弦和余弦定律的几何约束框架变体，并利用噪声相位估计公式化了一种新的相位重建算法。我们首先通过一系列理想实验验证了所提技术的有效性，证明了其在理想条件下的有效性。然后，我们在VB-DMD和WSJ0-CHiME3数据集上评估了其性能，结果表明所提出的MSGLA变体与现有算法（包括直接相位估计和基于DNN的符号预测）表现相当或略优，尤其在背景噪声抑制方面。", "summary": "本文提出了一种名为多源Griffin-Lim算法（MSGLA）的新型迭代相位估计框架，旨在解决加性噪声环境下语音增强中的相位估计问题。该方法通过结合复值短时傅里叶变换（STFT）谱图的一致性约束来处理几何相位估计中的符号模糊，并引入了基于正弦和余弦定律的几何约束变体。实验证明，MSGLA在理想条件下有效，并在VB-DMD和WSJ0-CHiME3数据集上，尤其在背景噪声抑制方面，性能与现有算法相当或略优。", "keywords": "语音增强, 相位估计, Griffin-Lim算法, 几何约束, 一致性约束", "comments": "该研究的创新点在于将STFT一致性约束与几何约束相结合，有效解决了语音增强中相位估计的符号模糊问题。通过引入基于正弦和余弦定律的几何约束变体，并利用噪声相位估计进行相位重建，提升了算法的鲁棒性。其在背景噪声抑制方面的突出表现，显示了该方法在实际应用中的潜力。"}}
{"id": "2507.02289", "title": "CineMyoPS: Segmenting Myocardial Pathologies from Cine Cardiac MR", "authors": ["Wangbin Ding", "Lei Li", "Junyi Qiu", "Bogen Lin", "Mingjing Yang", "Liqin Huang", "Lianming Wu", "Sihan Wang", "Xiahai Zhuang"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02289v1", "summary": "Myocardial infarction (MI) is a leading cause of death worldwide. Late\ngadolinium enhancement (LGE) and T2-weighted cardiac magnetic resonance (CMR)\nimaging can respectively identify scarring and edema areas, both of which are\nessential for MI risk stratification and prognosis assessment. Although\ncombining complementary information from multi-sequence CMR is useful,\nacquiring these sequences can be time-consuming and prohibitive, e.g., due to\nthe administration of contrast agents. Cine CMR is a rapid and contrast-free\nimaging technique that can visualize both motion and structural abnormalities\nof the myocardium induced by acute MI. Therefore, we present a new end-to-end\ndeep neural network, referred to as CineMyoPS, to segment myocardial\npathologies, \\ie scars and edema, solely from cine CMR images. Specifically,\nCineMyoPS extracts both motion and anatomy features associated with MI. Given\nthe interdependence between these features, we design a consistency loss\n(resembling the co-training strategy) to facilitate their joint learning.\nFurthermore, we propose a time-series aggregation strategy to integrate\nMI-related features across the cardiac cycle, thereby enhancing segmentation\naccuracy for myocardial pathologies. Experimental results on a multi-center\ndataset demonstrate that CineMyoPS achieves promising performance in myocardial\npathology segmentation, motion estimation, and anatomy segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02289v1", "cate": "eess.IV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "CineMyoPS：从电影心肌磁共振图像中分割心肌病变", "tldr": "CineMyoPS是一种新的深度学习网络，可以仅从无造影剂的电影心肌磁共振图像中分割心肌病变（疤痕和水肿），同时提取运动和解剖特征，并通过一致性损失和时间序列聚合策略提高分割精度。", "motivation": "心肌梗死是全球主要的死亡原因。传统的LGE和T2加权CMR成像虽能识别心肌疤痕和水肿，但耗时且需要造影剂。电影CMR是一种快速、无造影剂的成像技术，能够可视化急性心肌梗死引起的心肌运动和结构异常，因此需要一种仅基于电影CMR图像就能有效分割心肌病变的方法。", "method": "提出了一种名为CineMyoPS的端到端深度神经网络，用于仅从电影CMR图像中分割心肌病变（即疤痕和水肿）。CineMyoPS能够提取与心肌梗死相关的运动和解剖特征。为了利用这些特征之间的相互依赖性，设计了一种一致性损失（类似于协同训练策略）以促进它们的联合学习。此外，还提出了一种时间序列聚合策略，以整合整个心动周期内的MI相关特征，从而提高心肌病变分割的准确性。", "result": "在多中心数据集上的实验结果表明，CineMyoPS在心肌病变分割、运动估计和解剖分割方面均取得了有希望的性能。", "conclusion": "CineMyoPS能够仅从电影CMR图像中有效分割心肌病变，并通过其独特的设计（包括联合学习运动和解剖特征的一致性损失以及时间序列聚合策略）取得了良好的效果，为心肌梗死风险分层和预后评估提供了一种快速、无创的工具。", "translation": "心肌梗死（MI）是全球主要的死亡原因。晚期钆增强（LGE）和T2加权心脏磁共振（CMR）成像分别可以识别瘢痕和水肿区域，这两者对于MI风险分层和预后评估至关重要。尽管结合多序列CMR的互补信息很有用，但获取这些序列可能耗时且受限，例如由于造影剂的使用。电影CMR是一种快速且无需造影剂的成像技术，可以可视化急性MI引起的心肌运动和结构异常。因此，我们提出了一种新的端到端深度神经网络，称为CineMyoPS，仅从电影CMR图像中分割心肌病变，即瘢痕和水肿。具体而言，CineMyoPS提取与MI相关的运动和解剖特征。鉴于这些特征之间的相互依赖性，我们设计了一种一致性损失（类似于协同训练策略）以促进它们的联合学习。此外，我们提出了一种时间序列聚合策略，以整合整个心动周期内的MI相关特征，从而提高心肌病变分割的准确性。在多中心数据集上的实验结果表明，CineMyoPS在心肌病变分割、运动估计和解剖分割方面取得了有希望的性能。", "summary": "本文提出了一种名为CineMyoPS的端到端深度学习网络，旨在仅利用快速、无造影剂的电影心脏磁共振（CMR）图像来分割心肌梗死引起的心肌病变（如疤痕和水肿）。该网络通过提取并联合学习心肌运动和解剖特征，并引入一致性损失和时间序列聚合策略，显著提升了病变分割的准确性。实验证明，CineMyoPS在多中心数据集上表现出色的心肌病变分割、运动估计和解剖分割能力。", "keywords": "心肌病变分割, 电影CMR, 深度学习, 心肌梗死, 一致性损失", "comments": "该论文的创新点在于提出了一个仅依赖于快速、无造影剂的电影CMR图像来识别心肌病变的方法，这克服了传统多序列CMR耗时和需要造影剂的限制。通过设计一致性损失来联合学习运动和解剖特征，以及采用时间序列聚合策略，有效地利用了电影CMR的动态信息，提高了分割精度，对于MI的快速诊断和评估具有重要意义。"}}
{"id": "2507.02674", "title": "Real-time Image-based Lighting of Glints", "authors": ["Tom Kneiphof", "Reinhard Klein"], "categories": ["cs.GR", "cs.CV"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02674v1", "summary": "Image-based lighting is a widely used technique to reproduce shading under\nreal-world lighting conditions, especially in real-time rendering applications.\nA particularly challenging scenario involves materials exhibiting a sparkling\nor glittering appearance, caused by discrete microfacets scattered across their\nsurface. In this paper, we propose an efficient approximation for image-based\nlighting of glints, enabling fully dynamic material properties and environment\nmaps. Our novel approach is grounded in real-time glint rendering under area\nlight illumination and employs standard environment map filtering techniques.\nCrucially, our environment map filtering process is sufficiently fast to be\nexecuted on a per-frame basis. Our method assumes that the environment map is\npartitioned into few homogeneous regions of constant radiance. By filtering the\ncorresponding indicator functions with the normal distribution function, we\nobtain the probabilities for individual microfacets to reflect light from each\nregion. During shading, these probabilities are utilized to hierarchically\nsample a multinomial distribution, facilitated by our novel dual-gated Gaussian\napproximation of binomial distributions. We validate that our real-time\napproximation is close to ground-truth renderings for a range of material\nproperties and lighting conditions, and demonstrate robust and stable\nperformance, with little overhead over rendering glints from a single\ndirectional light. Compared to rendering smooth materials without glints, our\napproach requires twice as much memory to store the prefiltered environment\nmap.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02674v1", "cate": "cs.GR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "实时基于图像的闪光照明", "tldr": "提出了一种高效的实时近似方法，用于在动态环境和材质下对闪烁（glints）进行基于图像的照明渲染。", "motivation": "图像基照明（IBL）在实时渲染中广泛使用，但对于具有闪烁效果（由离散微面引起）的材质，其渲染具有挑战性。", "method": "提出了一种基于面积光照下实时闪烁渲染的有效近似方法。该方法利用标准环境贴图过滤技术，并能实现每帧执行。它假设环境贴图被划分为少数均匀的恒定辐射区域，通过正态分布函数过滤指示函数以获得微面反射来自各区域光线的概率。在着色过程中，利用一种新颖的双门高斯二项式分布近似来分层采样多项式分布。", "result": "该实时近似方法在多种材质属性和光照条件下接近真实渲染效果，并展示了稳健稳定的性能，相对于单个方向光渲染闪烁的开销很小。与渲染没有闪烁的光滑材质相比，该方法需要两倍的内存来存储预过滤的环境贴图。", "conclusion": "该论文提出了一种高效、稳健的实时基于图像的闪光照明近似方法，能够处理动态材质属性和环境贴图，并取得了接近真实的效果。", "translation": "基于图像的照明是一种广泛使用的技术，用于再现真实世界光照条件下的着色，尤其是在实时渲染应用中。一个特别具有挑战性的场景涉及呈现闪烁或闪光外观的材料，这由散布在其表面的离散微面引起。在本文中，我们提出了一种高效的闪烁图像基照明近似方法，实现了完全动态的材质属性和环境贴图。我们新颖的方法基于面积光照下实时闪烁渲染，并采用标准环境贴图过滤技术。至关重要的是，我们的环境贴图过滤过程速度足够快，可以每帧执行。我们的方法假设环境贴图被划分为少量均匀的恒定辐射区域。通过用正态分布函数过滤相应的指示函数，我们获得了单个微面从每个区域反射光线的概率。在着色过程中，这些概率被用于分层采样多项式分布，这得益于我们新颖的双门高斯二项式分布近似。我们验证了我们的实时近似方法在各种材质属性和光照条件下接近真实渲染效果，并展示了稳健稳定的性能，与从单个方向光渲染闪烁相比，开销很小。与渲染没有闪烁的光滑材质相比，我们的方法需要两倍的内存来存储预过滤的环境贴图。", "summary": "本文提出了一种针对闪烁（glints）的实时图像基照明（IBL）高效近似方法，解决了在动态材质和环境贴图下渲染闪烁效果的挑战。该方法基于面积光照下的实时闪烁渲染，利用快速的环境贴图过滤技术，并引入了双门高斯二项式分布近似来高效处理微面反射概率。实验验证了其在多种条件下的渲染效果接近真实，性能稳健，且额外开销较低，但需要更多内存来存储预过滤的环境贴图。", "keywords": "实时渲染, 图像基照明, 闪烁, 微面, 环境贴图过滤", "comments": "这项工作在实时渲染领域具有重要意义，尤其是在处理复杂光照下的特殊材质表面方面。其创新点在于提出了一种高效的近似算法，能够动态处理闪烁效果，克服了传统IBL方法在处理此类场景时的局限性。虽然增加了内存开销，但其在性能和视觉质量上的提升对于游戏和虚拟现实等应用是宝贵的。"}}
{"id": "2507.01982", "title": "DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism", "authors": ["Siqing Long", "Xiangzhi Huang", "Jiemin Xie", "Ming Cai"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      39 pages, 14 figures", "url": "http://arxiv.org/abs/2507.01982v1", "summary": "Accurate traffic demand forecasting enables transportation management\ndepartments to allocate resources more effectively, thereby improving their\nutilization efficiency. However, complex spatiotemporal relationships in\ntraffic systems continue to limit the performance of demand forecasting models.\nTo improve the accuracy of spatiotemporal traffic demand prediction, we propose\na new graph convolutional network structure called DKGCM. Specifically, we\nfirst consider the spatial flow distribution of different traffic nodes and\npropose a novel temporal similarity-based clustering graph convolution method,\nDK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering\nto group traffic nodes and more effectively capture spatial dependencies. On\nthe temporal scale, we integrate the Fast Fourier Transform (FFT) within the\nbidirectional Mamba deep learning framework to capture temporal dependencies in\ntraffic demand. To further optimize model training, we incorporate the GRPO\nreinforcement learning strategy to enhance the loss function feedback\nmechanism. Extensive experiments demonstrate that our model outperforms several\nadvanced methods and achieves strong results on three public datasets.", "comment": "39 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.01982v1", "cate": "cs.LG", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "DKGCM：一种融合空间节点聚类方法和傅里叶双向Mamba机制的交通流时空预测模型", "tldr": "DKGCM是一个新的交通流时空预测模型，结合了基于时间相似性的空间聚类（DK-GCN）、傅里叶双向Mamba用于时间依赖性捕捉，并使用GRPO强化学习优化训练，在公共数据集上表现优异。", "motivation": "提高交通需求预测的准确性，因为复杂的时空关系限制了现有模型的性能，而准确预测能更有效地分配资源。", "method": "本文提出DKGCM模型。在空间方面，提出DK-GCN，利用动态时间规整（DTW）和K-means聚类对交通节点进行分组，以捕捉空间依赖。在时间方面，将快速傅里叶变换（FFT）整合到双向Mamba深度学习框架中，以捕捉时间依赖。此外，引入GRPO强化学习策略来优化损失函数反馈机制。", "result": "实验表明，该模型在三个公共数据集上表现优于多个先进方法，并取得了强劲的结果。", "conclusion": "DKGCM模型能够有效提高时空交通需求预测的准确性。", "translation": "准确的交通需求预测使交通管理部门能够更有效地分配资源，从而提高其利用效率。然而，交通系统中复杂的时空关系持续限制着需求预测模型的性能。为了提高时空交通需求预测的准确性，我们提出了一种新的图卷积网络结构，名为DKGCM。具体来说，我们首先考虑不同交通节点的空间流分布，并提出了一种新颖的基于时间相似性的聚类图卷积方法DK-GCN。该方法利用动态时间规整（DTW）和K-means聚类来对交通节点进行分组，更有效地捕捉空间依赖性。在时间尺度上，我们将快速傅里叶变换（FFT）整合到双向Mamba深度学习框架中，以捕捉交通需求中的时间依赖性。为了进一步优化模型训练，我们引入了GRPO强化学习策略来增强损失函数反馈机制。大量的实验表明，我们的模型优于几种先进方法，并在三个公共数据集上取得了优异的结果。", "summary": "本文提出了DKGCM，一个用于交通流时空预测的新型图卷积网络模型。该模型通过DK-GCN方法利用DTW和K-means聚类来捕捉空间依赖性，并通过结合快速傅里叶变换与双向Mamba机制来处理时间依赖性。此外，模型引入GRPO强化学习策略以优化训练。实验结果表明DKGCM在多个公共数据集上超越了现有先进方法，证明了其在提高交通需求预测准确性方面的有效性。", "keywords": "交通流预测, 时空预测, 图卷积网络, Mamba, 强化学习", "comments": "该论文创新性地结合了多种技术来解决交通流时空预测的复杂性。空间上，利用DTW和K-means进行节点聚类是捕捉复杂空间依赖的有效尝试。时间上，将FFT与双向Mamba结合，有望在捕获长期依赖方面表现出色。引入强化学习优化训练是另一个亮点，可能有助于模型收敛和性能提升。"}}
{"id": "2507.02331", "title": "Tracing the Interactions of Modular CMA-ES Configurations Across Problem Landscapes", "authors": ["Ana Nikolikj", "Mario Andrés Muñoz", "Eva Tuba", "Tome Eftimov"], "categories": ["cs.NE", "cs.AI"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02331v1", "summary": "This paper leverages the recently introduced concept of algorithm footprints\nto investigate the interplay between algorithm configurations and problem\ncharacteristics. Performance footprints are calculated for six modular variants\nof the CMA-ES algorithm (modCMA), evaluated on 24 benchmark problems from the\nBBOB suite, across two-dimensional settings: 5-dimensional and 30-dimensional.\nThese footprints provide insights into why different configurations of the same\nalgorithm exhibit varying performance and identify the problem features\ninfluencing these outcomes. Our analysis uncovers shared behavioral patterns\nacross configurations due to common interactions with problem properties, as\nwell as distinct behaviors on the same problem driven by differing problem\nfeatures. The results demonstrate the effectiveness of algorithm footprints in\nenhancing interpretability and guiding configuration choices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02331v1", "cate": "cs.NE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "追踪模块化CMA-ES配置在问题景观中的相互作用", "tldr": "本文利用算法足迹概念，研究模块化CMA-ES算法配置与问题特性之间的相互作用，揭示了不同配置在不同问题上的共享和独特行为模式，并证明算法足迹能有效提高可解释性并指导配置选择。", "motivation": "本文旨在利用算法足迹的概念，研究算法配置与问题特性之间的相互作用，以理解同一算法不同配置表现差异的原因，并识别影响这些结果的问题特征。", "method": "研究计算了六种模块化CMA-ES算法（modCMA）变体的性能足迹，并在BBOB套件的24个基准问题上进行评估，涉及5维和30维两种设置。", "result": "分析揭示了由于与问题属性的共同相互作用而产生的跨配置的共享行为模式，以及由不同问题特征驱动的在相同问题上的不同行为。结果表明算法足迹在增强可解释性和指导配置选择方面的有效性。", "conclusion": "算法足迹能够有效增强算法性能的可解释性，并为算法配置的选择提供指导。", "translation": "本文利用最近引入的算法足迹概念，研究算法配置与问题特性之间的相互作用。针对CMA-ES算法的六种模块化变体（modCMA），在BBOB套件的24个基准问题上，在5维和30维两种二维设置下，计算了性能足迹。这些足迹深入揭示了同一算法的不同配置为何表现出不同性能，并识别了影响这些结果的问题特征。我们的分析揭示了由于与问题属性的共同相互作用而产生的跨配置的共享行为模式，以及由不同问题特征驱动的在相同问题上的不同行为。结果表明算法足迹在增强可解释性和指导配置选择方面的有效性。", "summary": "本文利用“算法足迹”概念，深入探讨了模块化CMA-ES（modCMA）算法配置与问题特性之间的复杂关系。通过在24个BBOB基准问题上评估六种modCMA变体在5维和30维设置下的性能足迹，研究揭示了不同算法配置在不同问题景观中表现出差异的原因，并识别了影响这些结果的关键问题特征。分析发现，存在由于与问题属性的共同作用而产生的跨配置的共享行为模式，同时也有由不同问题特征驱动的在相同问题上的独特行为。研究结果强调了算法足迹在提升算法性能可解释性以及有效指导算法配置选择方面的显著作用。", "keywords": "算法足迹, CMA-ES, 算法配置, 问题特性, 性能分析", "comments": "本文的创新点在于引入并利用“算法足迹”这一概念来系统地分析算法配置与问题特性之间的相互作用，这为理解复杂优化算法的行为提供了新的视角。通过量化性能足迹，研究不仅解释了不同配置表现差异的原因，还为算法选择和配置提供了数据驱动的指导，对于算法工程和实际应用具有重要意义。该方法有望推广到其他算法和问题领域，以提升算法的透明度和效用。"}}
{"id": "2507.02009", "title": "Uncertainty-Aware Complex Scientific Table Data Extraction", "authors": ["Kehinde Ajayi", "Yi He", "Jian Wu"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02009v1", "summary": "Table structure recognition (TSR) and optical character recognition (OCR)\nplay crucial roles in extracting structured data from tables in scientific\ndocuments. However, existing extraction frameworks built on top of TSR and OCR\nmethods often fail to quantify the uncertainties of extracted results. To\nobtain highly accurate data for scientific domains, all extracted data must be\nmanually verified, which can be time-consuming and labor-intensive. We propose\na framework that performs uncertainty-aware data extraction for complex\nscientific tables, built on conformal prediction, a model-agnostic method for\nuncertainty quantification (UQ). We explored various uncertainty scoring\nmethods to aggregate the uncertainties introduced by TSR and OCR. We rigorously\nevaluated the framework using a standard benchmark and an in-house dataset\nconsisting of complex scientific tables in six scientific domains. The results\ndemonstrate the effectiveness of using UQ for extraction error detection, and\nby manually verifying only 47\\% of extraction results, the data quality can be\nimproved by 30\\%. Our work quantitatively demonstrates the role of UQ with the\npotential of improving the efficiency in the human-machine cooperation process\nto obtain scientifically usable data from complex tables in scientific\ndocuments. All code and data are available on GitHub at\nhttps://github.com/lamps-lab/TSR-OCR-UQ/tree/main.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02009v1", "cate": "cs.IR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "不确定性感知复杂科学表格数据提取", "tldr": "该研究提出了一个不确定性感知的框架，用于从复杂科学表格中提取数据，通过量化TSR和OCR的不确定性，显著提高了数据质量并减少了手动验证的工作量。", "motivation": "现有的基于表格结构识别（TSR）和光学字符识别（OCR）的数据提取框架无法量化提取结果的不确定性，导致需要耗时费力地手动验证所有提取的数据，以确保科学领域所需的高精度。", "method": "提出了一种基于共形预测（一种模型无关的不确定性量化方法）的不确定性感知数据提取框架，用于处理复杂科学表格。该框架探索了各种不确定性评分方法来聚合TSR和OCR引入的不确定性。", "result": "通过使用不确定性量化进行提取错误检测，手动验证仅47%的提取结果，数据质量可以提高30%。研究结果证明了不确定性量化在提高人机协作效率方面的有效性。", "conclusion": "本研究定量地证明了不确定性量化（UQ）在提高从科学文档中复杂表格获取科学可用数据的人机协作过程效率方面的作用。", "translation": "表格结构识别（TSR）和光学字符识别（OCR）在从科学文档中的表格中提取结构化数据方面发挥着关键作用。然而，现有建立在TSR和OCR方法之上的提取框架往往无法量化提取结果的不确定性。为了获得科学领域所需的高精度数据，所有提取的数据都必须经过手动验证，这可能既耗时又费力。我们提出了一个框架，该框架基于共形预测（一种模型无关的不确定性量化（UQ）方法），对复杂科学表格执行不确定性感知数据提取。我们探索了各种不确定性评分方法来聚合TSR和OCR引入的不确定性。我们使用标准基准和包含六个科学领域复杂科学表格的内部数据集对该框架进行了严格评估。结果表明，使用UQ进行提取错误检测是有效的，并且通过仅手动验证47%的提取结果，数据质量可以提高30%。我们的工作定量地证明了UQ的作用，它有潜力提高人机协作过程的效率，从而从科学文档中的复杂表格中获取科学可用数据。所有代码和数据均可在GitHub上获取：https://github.com/lamps-lab/TSR-OCR-UQ/tree/main。", "summary": "本论文提出了一个不确定性感知的数据提取框架，用于处理科学文档中的复杂表格。该框架基于共形预测，旨在解决现有TSR和OCR方法无法量化提取结果不确定性的问题。通过聚合TSR和OCR引入的不确定性并探索不同的评分方法，该研究实现了对提取错误的高效检测。实验结果表明，通过仅手动验证47%的提取结果，数据质量可提高30%，显著提升了从复杂科学表格中获取可用数据的人机协作效率。", "keywords": "不确定性量化, 科学表格数据提取, 共形预测, 表格结构识别, 光学字符识别", "comments": "该论文的创新之处在于将不确定性量化（UQ）引入到科学表格数据提取流程中，特别是在TSR和OCR的背景下。通过利用共形预测，该框架提供了一种模型无关的方法来量化和聚合不确定性，这对于提高提取数据的可靠性至关重要。其重要性在于能够显著减少手动验证的工作量，从而提高科学数据处理的效率，加速科学研究。通过量化数据质量的提升（30%）和手动验证量的减少（47%），该工作提供了明确的实践价值。"}}
{"id": "2507.02117", "title": "A Computational Proof of the Highest-Scoring Boggle Board", "authors": ["Dan Vanderkam"], "categories": ["cs.DS", "I.2.8, E.1"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      14 pages, 2 figures, code available at this https URL", "url": "http://arxiv.org/abs/2507.02117v1", "summary": "Finding all the words on a Boggle board is a classic computer programming\nproblem. With a fast Boggle solver, local optimization techniques such as\nhillclimbing and simulated annealing can be used to find particularly\nhigh-scoring boards. The sheer number of possible Boggle boards has\nhistorically prevented an exhaustive search for the global optimum board. We\napply Branch and Bound and a decision diagram-like data structure to perform\nthe first such search. We find that the highest-scoring boards found via\nhillclimbing are, in fact, the global optima.", "comment": "14 pages, 2 figures, code available at\n  https://github.com/danvk/hybrid-boggle/", "pdf_url": "http://arxiv.org/pdf/2507.02117v1", "cate": "cs.DS", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "拼字游戏最高分棋盘的计算证明", "tldr": "首次通过穷举搜索，计算证明了通过局部优化（如爬山法）找到的Boggle最高分棋盘确实是全局最优解。", "motivation": "寻找拼字游戏（Boggle）的最高分棋盘是一个经典问题，但由于可能的棋盘数量庞大，一直未能进行穷举搜索以找到全局最优解。", "method": "应用了分支定界（Branch and Bound）算法和一种类似决策图的数据结构，首次对Boggle棋盘进行了穷举搜索。", "result": "发现通过爬山法（hillclimbing）找到的最高分Boggle棋盘，实际上就是全局最优解。", "conclusion": "通过计算证明，局部优化技术（如爬山法）在Boggle游戏中找到的最高分棋盘确实是全局最优的。", "translation": "在拼字游戏棋盘上找到所有单词是一个经典的计算机编程问题。借助快速的拼字游戏求解器，可以使用爬山法和模拟退火等局部优化技术来寻找得分特别高的棋盘。历史上，由于可能的拼字游戏棋盘数量巨大，阻碍了对全局最优棋盘进行穷举搜索。我们应用分支定界法和一种类似决策图的数据结构来首次执行这种搜索。我们发现通过爬山法找到的最高分棋盘实际上就是全局最优解。", "summary": "这篇论文解决了拼字游戏（Boggle）中寻找最高分棋盘的挑战。由于棋盘组合数量庞大，以往未能进行穷举搜索。作者首次利用分支定界算法和类似决策图的数据结构进行了穷举搜索，并计算证明了通过局部优化方法（如爬山法）找到的高分棋盘确实是全局最优解。", "keywords": "拼字游戏, 全局最优, 分支定界, 爬山法, 计算证明", "comments": "这项工作的重要性在于它首次通过计算方法对Boggle游戏中的最高分棋盘进行了穷举搜索，并验证了局部优化算法的有效性，证明了它们找到的解并非只是局部最优，而是全局最优。这对于理解局部优化算法在某些复杂搜索空间中的表现具有重要意义。"}}
{"id": "2507.02145", "title": "Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization", "authors": ["Keyan Jin", "Yapeng Wang", "Leonel Santos", "Tao Fang", "Xu Yang", "Sio Kei Im", "Hugo Gonçalo Oliveira"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02145v1", "summary": "Dialogue summarization is a challenging task with significant practical value\nin customer service, meeting analysis, and conversational AI. Although large\nlanguage models (LLMs) have achieved substantial progress in summarization\ntasks, the performance of step-by-step reasoning architectures-specifically\nLong Chain-of-Thought (CoT) implementations such as OpenAI-o1 and\nDeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent\nabstraction and conciseness. In this work, we present the first comprehensive\nand systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning\nLLMs across three major paradigms-generic, role-oriented, and query-oriented\ndialogue summarization. Our study spans diverse languages, domains, and summary\nlengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and\nadvanced evaluation protocols that include both LLM-based automatic metrics and\nhuman-inspired criteria. Contrary to trends in other reasoning-intensive tasks,\nour findings show that explicit stepwise reasoning does not consistently\nimprove dialogue summarization quality. Instead, reasoning LLMs are often prone\nto verbosity, factual inconsistencies, and less concise summaries compared to\ntheir non-reasoning counterparts. Through scenario-specific analyses and\ndetailed case studies, we further identify when and why explicit reasoning may\nfail to benefit-or even hinder-summarization in complex dialogue contexts. Our\nwork provides new insights into the limitations of current reasoning LLMs and\nhighlights the need for targeted modeling and evaluation strategies for\nreal-world dialogue summarization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02145v1", "cate": "cs.CL", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "推理与否？对话摘要中推理LLMs的综合评估", "tldr": "研究发现，在对话摘要任务中，显式逐步推理的LLM（如CoT）并不总能提升性能，反而可能导致冗余和不一致，这与它们在其他推理任务中的表现不同。", "motivation": "对话摘要在客户服务、会议分析和对话式AI中具有重要的实际价值。尽管大型语言模型（LLMs）在摘要任务中取得了实质性进展，但针对需要同时进行抽象和简洁性的对话场景，逐步推理架构（特别是长链思维CoT实现）的性能尚未被探索。", "method": "本文首次对最先进的推理LLM和非推理LLM在三种主要范式（通用、面向角色和面向查询的对话摘要）中进行了全面系统的评估。研究涵盖了不同的语言、领域和摘要长度，利用了强大的基准测试（SAMSum、DialogSum、CSDS和QMSum）和先进的评估协议，包括基于LLM的自动指标和受人类启发的标准。", "result": "与在其他推理密集型任务中的趋势相反，研究结果表明，显式逐步推理并不能持续改善对话摘要质量。相反，推理LLM往往更容易出现冗余、事实不一致和摘要不够简洁的问题。通过特定场景分析和详细案例研究，进一步确定了在复杂对话上下文中，显式推理何时以及为何可能无法带来益处——甚至会阻碍摘要。", "conclusion": "本工作为当前推理LLM的局限性提供了新见解，并强调了针对现实世界对话摘要任务，需要有针对性的建模和评估策略。", "translation": "对话摘要是一项具有挑战性的任务，在客户服务、会议分析和对话式AI中具有重要的实际价值。尽管大型语言模型（LLM）在摘要任务中取得了实质性进展，但对于需要同时进行抽象和简洁性的对话场景，逐步推理架构（特别是OpenAI-o1和DeepSeek-R1等长链思维（CoT）实现）的性能仍未被探索。在这项工作中，我们首次对最先进的推理LLM和非推理LLM在三种主要范式——通用、面向角色和面向查询的对话摘要——中进行了全面系统的评估。我们的研究涵盖了不同的语言、领域和摘要长度，利用了强大的基准测试（SAMSum、DialogSum、CSDS和QMSum）以及先进的评估协议，包括基于LLM的自动指标和受人类启发的标准。与在其他推理密集型任务中的趋势相反，我们的研究结果表明，显式逐步推理并不能持续改善对话摘要质量。相反，与非推理模型相比，推理LLM往往更容易出现冗余、事实不一致和摘要不够简洁的问题。通过特定场景分析和详细案例研究，我们进一步确定了在复杂对话上下文中，显式推理何时以及为何可能无法带来益处——甚至会阻碍摘要。我们的工作为当前推理LLM的局限性提供了新的见解，并强调了针对现实世界对话摘要任务，需要有针对性的建模和评估策略。", "summary": "本文对LLM在对话摘要任务中的推理能力进行了首次全面评估。研究发现，尽管在其他推理任务中表现出色，但显式逐步推理（如CoT）在对话摘要中并不能持续提升性能，反而可能导致冗余和不一致。作者通过多范式、多语言和多领域评估，揭示了当前推理LLM在复杂对话场景中的局限性，并强调了未来需开发更具针对性的建模和评估策略。", "keywords": "对话摘要, 大型语言模型, 推理, 链式思考, 评估", "comments": "该论文具有重要的实践意义，挑战了当前普遍认为链式思考（CoT）能普遍提升LLM性能的观念，特别是在对话摘要这一特定且复杂的任务中。它揭示了CoT可能带来的负面影响，如冗余和事实不一致，这对于指导LLM在特定应用中的选择和优化方向至关重要。研究的系统性评估方法和对LLM局限性的深入分析是其创新之处。"}}
{"id": "2507.02176", "title": "Analyzing and Improving Speaker Similarity Assessment for Speech Synthesis", "authors": ["Marc-André Carbonneau", "Benjamin van Niekerk", "Hugo Seuté", "Jean-Philippe Letendre", "Herman Kamper", "Julian Zaïdi"], "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted at SSW13 - Interspeech 2025 Speech Synthesis Workshop", "url": "http://arxiv.org/abs/2507.02176v1", "summary": "Modeling voice identity is challenging due to its multifaceted nature. In\ngenerative speech systems, identity is often assessed using automatic speaker\nverification (ASV) embeddings, designed for discrimination rather than\ncharacterizing identity. This paper investigates which aspects of a voice are\ncaptured in such representations. We find that widely used ASV embeddings focus\nmainly on static features like timbre and pitch range, while neglecting dynamic\nelements such as rhythm. We also identify confounding factors that compromise\nspeaker similarity measurements and suggest mitigation strategies. To address\nthese gaps, we propose U3D, a metric that evaluates speakers' dynamic rhythm\npatterns. This work contributes to the ongoing challenge of assessing speaker\nidentity consistency in the context of ever-better voice cloning systems. We\npublicly release our code.", "comment": "Accepted at SSW13 - Interspeech 2025 Speech Synthesis Workshop", "pdf_url": "http://arxiv.org/pdf/2507.02176v1", "cate": "cs.SD", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "分析和改进语音合成中的说话人相似度评估", "tldr": "本文研究了用于语音合成的自动说话人验证（ASV）嵌入在评估说话人身份方面的局限性，发现它们主要关注静态特征而忽略动态节奏。文章提出了U3D，一个评估动态节奏模式的新度量，以改进说话人身份一致性评估。", "motivation": "由于语音身份的多面性，对其进行建模极具挑战。在生成式语音系统中，身份通常使用自动说话人验证（ASV）嵌入进行评估，但这些嵌入是为区分而非表征身份而设计的。因此，需要研究和改进说话人相似度评估方法。", "method": "本文研究了ASV嵌入捕获语音哪些方面，并发现现有嵌入的局限性。识别了影响说话人相似度测量的混淆因素并提出了缓解策略。为解决这些不足，文章提出了一种名为U3D的新度量，用于评估说话人的动态节奏模式。", "result": "研究发现，广泛使用的ASV嵌入主要关注音色和音高范围等静态特征，而忽略了节奏等动态元素。同时，识别出了损害说话人相似度测量的混淆因素。", "conclusion": "这项工作有助于解决在不断改进的语音克隆系统中评估说话人身份一致性的持续挑战。", "translation": "建模语音身份由于其多面性而具有挑战性。在生成式语音系统中，身份通常使用自动说话人验证（ASV）嵌入进行评估，这些嵌入是为区分而非表征身份而设计的。本文研究了此类表征中捕获了语音的哪些方面。我们发现，广泛使用的ASV嵌入主要关注音色和音高范围等静态特征，而忽略了节奏等动态元素。我们还识别出损害说话人相似度测量的混淆因素，并提出了缓解策略。为了弥补这些不足，我们提出了U3D，一个评估说话人动态节奏模式的度量。这项工作为在不断改进的语音克隆系统中评估说话人身份一致性的持续挑战做出了贡献。我们公开了我们的代码。", "summary": "本文探讨了语音合成中用于评估说话人身份的自动说话人验证（ASV）嵌入的不足。研究发现，现有ASV嵌入主要侧重于静态语音特征，如音色和音高范围，而忽略了节奏等动态元素。针对这一缺陷，文章识别了影响相似度测量的混淆因素并提出了缓解策略，并进一步提出了一种名为U3D的新度量，专门用于评估说话人的动态节奏模式。这项工作旨在改进语音克隆系统中说话人身份一致性的评估。", "keywords": "说话人相似度, 语音合成, ASV嵌入, 动态节奏, U3D", "comments": "该论文创新性地指出了现有自动说话人验证（ASV）嵌入在评估语音身份时忽略动态节奏模式的局限性，并提出了一个新颖的度量U3D来解决这一问题。这对于提升语音合成和语音克隆系统的真实感和一致性具有重要意义。公开代码也有助于社区进一步研究和应用。"}}
{"id": "2507.02464", "title": "Resolving CAP Through Automata-Theoretic Economic Design: A Unified Mathematical Framework for Real-Time Partition-Tolerant Systems", "authors": ["Craig S Wright"], "categories": ["cs.GT", "cs.DC", "cs.FL", "cs.IR", "econ.GN", "q-fin.EC", "68M14, 91A05, 68Q85", "C.2.4; D.2.4; F.1.1"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "Comments:      51 pages 4 tables, includes formal proofs, automata construction, and case study on Bitcoin Script", "url": "http://arxiv.org/abs/2507.02464v1", "summary": "The CAP theorem asserts a trilemma between consistency, availability, and\npartition tolerance. This paper introduces a rigorous automata-theoretic and\neconomically grounded framework that reframes the CAP trade-off as a constraint\noptimization problem. We model distributed systems as partition-aware state\nmachines and embed economic incentive layers to stabilize consensus behavior\nacross adversarially partitioned networks. By incorporating game-theoretic\nmechanisms into the global transition semantics, we define provable bounds on\nconvergence, liveness, and correctness. Our results demonstrate that\navailability and consistency can be simultaneously preserved within bounded\nepsilon margins, effectively extending the classical CAP limits through formal\neconomic control.", "comment": "51 pages 4 tables, includes formal proofs, automata construction, and\n  case study on Bitcoin Script", "pdf_url": "http://arxiv.org/pdf/2507.02464v1", "cate": "cs.GT", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过自动机理论经济设计解决CAP：实时分区容错系统的统一数学框架", "tldr": "本文提出一个基于自动机理论和经济学原理的统一框架，将CAP定理的权衡转化为约束优化问题，并通过博弈论机制在分区网络中同时保持可用性和一致性。", "motivation": "CAP定理提出了在一致性、可用性和分区容错性之间的三难困境，该研究旨在解决这一问题，突破经典CAP限制。", "method": "引入了一个严格的、基于自动机理论和经济学原理的框架，将CAP权衡重新定义为约束优化问题。通过将分布式系统建模为分区感知状态机，并嵌入经济激励层来稳定对抗性分区网络中的共识行为，同时将博弈论机制纳入全局转换语义中。", "result": "结果表明，可用性和一致性可以在有限的ε裕度内同时保持，有效地通过形式化的经济控制扩展了经典的CAP限制。", "conclusion": "通过引入自动机理论和经济学设计，可以构建一个统一的数学框架，在实时分区容错系统中同时实现高可用性和一致性，从而突破传统CAP定理的限制。", "translation": "CAP定理断言了一致性、可用性和分区容错性之间的三难困境。本文引入了一个严格的、基于自动机理论和经济学原理的框架，将CAP权衡重新定义为约束优化问题。我们将分布式系统建模为分区感知状态机，并嵌入经济激励层，以稳定对抗性分区网络中的共识行为。通过将博弈论机制纳入全局转换语义中，我们定义了收敛性、活性和正确性的可证明界限。我们的结果表明，可用性和一致性可以在有限的ε裕度内同时保持，有效地通过形式化的经济控制扩展了经典的CAP限制。", "summary": "本文针对CAP定理提出的分布式系统三难困境，提出了一个创新的统一数学框架。该框架结合了自动机理论和经济学原理，将CAP权衡视为一个约束优化问题。通过将分布式系统建模为分区感知状态机并集成经济激励和博弈论机制，该研究成功地在对抗性分区网络中实现了共识行为的稳定，并证明了在有限误差范围内同时保持可用性和一致性的可能性，从而有效突破了传统CAP定理的限制。", "keywords": "CAP定理, 分布式系统, 自动机理论, 经济设计, 分区容错", "comments": "这篇论文通过引入经济学原理和博弈论机制来解决CAP定理的限制，提供了一种新颖的视角。其创新点在于将分布式系统问题转化为经济激励下的优化问题，为构建高可用和一致性的分区容错系统提供了理论基础和新的设计思路。"}}
{"id": "2507.02281", "title": "Linearly Homomorphic Ring Signature Scheme over Lattices", "authors": ["Heng Guo", "Kun Tian", "Fengxia Liu", "Zhiyong Zheng"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02281v1", "summary": "Homomorphic ring signature schemes combine the strong anonymity of ring\nsignatures with the computability of homomorphic signatures, demonstrating\nsignificant potential in scenarios requiring both anonymous data provenance and\nverifiable homomorphic computation (e.g., confidential blockchain transactions\nand secure multi-party computation). However, no feasible homomorphic ring\nsignature scheme currently exists.\n  In this work, we propose the first lattice-based linearly homomorphic ring\nsignature scheme. Proven secure in the standard model under the small integer\nsolution (SIS) assumption, our scheme achieves strong anonymity under full key\nexposure and unforgeability against insider corruption attacks. As the first\nunified framework for ring signatures and linear homomorphic signatures, this\nconstruction provides a post-quantum-secure solution for the aforementioned\napplications, advancing the development of privacy-enhanced homomorphic\ncomputation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02281v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于格的线性同态环签名方案", "tldr": "本文提出首个基于格的线性同态环签名方案，在标准模型下被证明安全，实现了强匿名性和不可伪造性，为隐私增强的同态计算提供了后量子安全解决方案。", "motivation": "同态环签名结合了环签名的强匿名性和同态签名的可计算性，在需要匿名数据来源和可验证同态计算的场景（如保密区块链交易和安全多方计算）中具有巨大潜力。然而，目前不存在可行的同态环签名方案。", "method": "本文提出了首个基于格的线性同态环签名方案，该方案在标准模型下基于小整数解（SIS）假设被证明是安全的。", "result": "该方案在完全密钥暴露下实现了强匿名性，并能抵抗内部腐败攻击的不可伪造性。作为环签名和线性同态签名的第一个统一框架，它为上述应用提供了后量子安全解决方案。", "conclusion": "本文提出了首个可行的基于格的线性同态环签名方案，为隐私增强的同态计算发展提供了后量子安全解决方案，填补了现有方案的空白。", "translation": "同态环签名方案结合了环签名的强匿名性和同态签名的可计算性，在需要匿名数据来源和可验证同态计算的场景（例如，保密区块链交易和安全多方计算）中展示出巨大潜力。然而，目前不存在可行的同态环签名方案。\n在本工作中，我们提出了首个基于格的线性同态环签名方案。该方案在标准模型下，在小整数解（SIS）假设下被证明是安全的，并在完全密钥暴露下实现了强匿名性，以及抵抗内部腐败攻击的不可伪造性。作为环签名和线性同态签名的第一个统一框架，该构造为上述应用提供了一个后量子安全解决方案，推动了隐私增强同态计算的发展。", "summary": "本文提出首个基于格的线性同态环签名方案，填补了该领域可行方案的空白。该方案在标准模型下基于SIS假设被证明是安全的，实现了强匿名性和不可伪造性。它作为环签名和线性同态签名的统一框架，为保密区块链和安全多方计算等应用提供了后量子安全解决方案，促进了隐私增强同态计算的发展。", "keywords": "同态环签名, 格密码学, 线性同态, 后量子安全, 匿名性", "comments": "该论文的创新点在于首次提出了一个可行的基于格的线性同态环签名方案，解决了现有技术空白。其重要性体现在为需要匿名性和可计算性的应用提供了后量子安全保障，特别是在区块链和多方计算领域。该方案在标准模型下被证明安全，且具备强匿名性和抗伪造性，是一个重要的理论和实践突破。"}}
{"id": "2507.01994", "title": "Curated Collaborative AI Edge with Network Data Analytics for B5G/6G Radio Access Networks", "authors": ["Sardar Jaffar Ali", "Syed M. Raza", "Duc-Tai Le", "Rajesh Challa", "Min Young Chung", "Ness Shroff", "Hyunseung Choo"], "categories": ["cs.NI", "cs.MA"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01994v1", "summary": "Despite advancements, Radio Access Networks (RAN) still account for over 50\\%\nof the total power consumption in 5G networks. Existing RAN split options do\nnot fully harness data potential, presenting an opportunity to reduce\noperational expenditures. This paper addresses this opportunity through a\ntwofold approach. First, highly accurate network traffic and user predictions\nare achieved using the proposed Curated Collaborative Learning (CCL) framework,\nwhich selectively collaborates with relevant correlated data for traffic\nforecasting. CCL optimally determines whom, when, and what to collaborate with,\nsignificantly outperforming state-of-the-art approaches, including global,\nfederated, personalized federated, and cyclic institutional incremental\nlearnings by 43.9%, 39.1%, 40.8%, and 31.35%, respectively. Second, the\nDistributed Unit Pooling Scheme (DUPS) is proposed, leveraging deep\nreinforcement learning and prediction inferences from CCL to reduce the number\nof active DU servers efficiently. DUPS dynamically redirects traffic from\nunderutilized DU servers to optimize resource use, improving energy efficiency\nby up to 89% over conventional strategies, translating into substantial\nmonetary benefits for operators. By integrating CCL-driven predictions with\nDUPS, this paper demonstrates a transformative approach for minimizing energy\nconsumption and operational costs in 5G RANs, significantly enhancing\nefficiency and cost-effectiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01994v1", "cate": "cs.NI", "date": "2025-06-30", "updated": "2025-06-30", "AI": {"title_translation": "协同策展AI边缘与网络数据分析在B5G/6G无线接入网络中的应用", "tldr": "本文提出通过协同策展学习（CCL）和分布式单元池化方案（DUPS）来降低B5G/6G无线接入网络的能耗和运营成本。", "motivation": "5G网络中无线接入网络（RAN）的功耗占总功耗的50%以上，现有RAN分割选项未能充分利用数据潜力，存在降低运营支出的机会。", "method": "本文提出双重方法：首先，提出“协同策展学习（CCL）”框架，通过选择性地与相关联数据协作，实现高精度网络流量和用户预测，CCL优化决定何时、与谁、协作什么。其次，提出“分布式单元池化方案（DUPS）”，利用深度强化学习和CCL的预测推断来高效减少活跃DU服务器数量，动态重定向流量以优化资源利用。", "result": "CCL在流量预测方面显著优于现有最先进方法：比全局学习高43.9%，比联邦学习高39.1%，比个性化联邦学习高40.8%，比循环机构增量学习高31.35%。DUPS比传统策略提高能源效率高达89%。", "conclusion": "通过整合CCL驱动的预测与DUPS，本文展示了一种在5G RAN中最小化能耗和运营成本的变革性方法，显著提高了效率和成本效益。", "translation": "尽管取得了进步，无线接入网络（RAN）在5G网络中仍占总功耗的50%以上。现有的RAN拆分选项未能充分利用数据潜力，这为降低运营支出提供了机会。本文通过双重方法解决了这一机会。首先，使用所提出的协同策展学习（CCL）框架实现了高度准确的网络流量和用户预测，该框架选择性地与相关的关联数据协作进行流量预测。CCL最优地决定与谁、何时以及协作什么，显著优于最先进的方法，包括全局学习、联邦学习、个性化联邦学习和循环机构增量学习，分别高出43.9%、39.1%、40.8%和31.35%。其次，提出了分布式单元池化方案（DUPS），利用深度强化学习和CCL的预测推断来高效减少活跃DU服务器的数量。DUPS动态地将流量从利用率不足的DU服务器重定向，以优化资源使用，与传统策略相比，能源效率提高了高达89%，为运营商带来了可观的经济效益。通过将CCL驱动的预测与DUPS相结合，本文展示了一种在5G RAN中最小化能耗和运营成本的变革性方法，显著提高了效率和成本效益。", "summary": "本文针对5G RAN高能耗和运营成本问题，提出了“协同策展学习（CCL）”框架用于高精度流量预测，并通过“分布式单元池化方案（DUPS）”结合CCL预测，利用深度强化学习动态管理DU服务器，从而显著降低能耗和运营成本，提升网络效率。", "keywords": "无线接入网络, 能耗优化, 协同策展学习, 分布式单元池化, 深度强化学习", "comments": "本文创新性地结合了协同学习和强化学习来优化无线接入网络的能耗和成本。CCL通过选择性数据协作提升了预测精度，而DUPS则利用这些预测实现了动态资源管理。其提出的协同策展学习概念在数据协作方面具有新颖性，并且在能耗优化方面取得了显著的量化提升，为B5G/6G网络的高效运营提供了有价值的解决方案。"}}
{"id": "2507.02536", "title": "Real-Time Monitoring and Transparency in Pizza Production Using IoT and Blockchain", "authors": ["Azmat Ullah", "Maria Ilaria Lunesu", "Lodovica Marchesi", "Roberto Tonelli"], "categories": ["cs.CR", "cs.ET"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      2 pages", "url": "http://arxiv.org/abs/2507.02536v1", "summary": "This paper presents a blockchain-based Internet of Things (IoT) system for\nmonitoring pizza production in restaurants. IoT devices track temperature and\nhumidity in real-time, while blockchain ensures secure and tamper-proof data. A\nRaspberry Pi processes sensor data, captures images, triggers alerts, and\ninteracts with smart contracts. The system detects abnormal conditions,\nenabling quick responses. Blockchain adds transparency and traceability,\nsupporting compliance and audits. Experiments show improved ingredient\nmanagement, reduced waste, and increased kitchen efficiency.", "comment": "2 pages", "pdf_url": "http://arxiv.org/pdf/2507.02536v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "使用物联网和区块链实现披萨生产的实时监控和透明化", "tldr": "本文提出了一种基于物联网和区块链的系统，用于实时监控披萨生产，确保数据安全和生产透明度，从而提高效率并减少浪费。", "motivation": "提高披萨生产过程的透明度、可追溯性和效率，并减少浪费，同时确保数据安全和防篡改。", "method": "该系统结合了物联网设备和区块链技术。物联网设备（如温度和湿度传感器）实时跟踪生产数据，Raspberry Pi 处理传感器数据、捕获图像并触发警报，同时与智能合约交互。区块链用于确保数据的安全性和防篡改，实现透明度和可追溯性。", "result": "实验表明，该系统改善了食材管理，减少了浪费，并提高了厨房效率。", "conclusion": "基于物联网和区块链的系统能够有效实现披萨生产的实时监控和数据透明化，从而带来食材管理、效率和废弃物减少方面的显著改进。", "translation": "本文提出了一种基于区块链的物联网（IoT）系统，用于监控餐厅的披萨生产。物联网设备实时跟踪温度和湿度，而区块链确保数据的安全性和防篡改。树莓派处理传感器数据、捕获图像、触发警报并与智能合约交互。该系统能够检测异常情况，实现快速响应。区块链增加了透明度和可追溯性，支持合规性和审计。实验表明，该系统改善了食材管理，减少了浪费，并提高了厨房效率。", "summary": "本文介绍了一个利用物联网和区块链技术来实时监控披萨生产的系统。该系统通过物联网设备收集温度和湿度等数据，并由树莓派处理和触发警报，同时利用区块链确保数据的安全、防篡改、透明和可追溯性。实验证明，该系统能有效改善食材管理、减少浪费并提升厨房效率。", "keywords": "物联网, 区块链, 披萨生产, 实时监控, 透明度", "comments": "本文创新性地将物联网与区块链结合应用于餐饮生产监控，解决了传统生产流程中数据不透明、可追溯性差的问题。其亮点在于通过实时数据监控和区块链的防篡改特性，显著提升了生产效率和食品安全，具有较强的实际应用价值。"}}
{"id": "2507.02110", "title": "Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!", "authors": ["Md Nahidul Islam Opu", "Fatima Islam Mouri", "Rick Kazman", "Yuanfang Cai", "Shaiful Chowdhury"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02110v1", "summary": "Predicting mobile app popularity before release can provide developers with a\nstrategic advantage in a competitive marketplace, yet it remains a challenging\nproblem. This study explores whether internal software metrics, measurable from\nsource code before deployment, can predict an app's popularity, defined by user\nratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).\nUsing a dataset of 446 open-source Android apps from F-Droid, we extract a wide\narray of features, including system-, class-, and method-level code metrics,\ncode smells, and app metadata. Additional information, such as user reviews,\ndownload counts, and uses-permission, was collected from the Google Play Store.\nWe evaluate regression and classification models across three feature sets: a\nminimal Size-only baseline, a domain-informed Handpicked set, and a Voting set\nderived via feature selection algorithms. Regression models perform poorly due\nto skewed data, with low $R^2$ scores. However, when reframed as binary\nclassification (Popular vs. Unpopular), results improve significantly. The best\nmodel, a Multilayer Perceptron using the Voting set, achieves F1-scores of\n0.72. These results suggest that internal code metrics, although limited in\ntheir explanatory power, can serve as useful indicators of app popularity. This\nchallenges earlier findings that dismissed internal metrics as predictors of\nsoftware quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02110v1", "cate": "cs.SE", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "内部软件指标能否预测应用程序发布时的受欢迎程度？是！和否！", "tldr": "研究发现，内部软件指标可以预测移动应用的受欢迎程度，尤其是将其视为二元分类问题时。", "motivation": "预测移动应用程序发布前的受欢迎程度可以为开发者在竞争激烈的市场中提供战略优势，但它仍然是一个具有挑战性的问题。", "method": "使用F-Droid的446个开源Android应用数据集，提取系统、类、方法级别的代码指标、代码异味和应用元数据等特征。从Google Play Store收集用户评论、下载量和权限信息。评估了三种特征集的回归和分类模型：Size-only、Handpicked和Voting。", "result": "回归模型表现不佳（R^2分数低），但当重构为二元分类（流行 vs. 不流行）时，结果显著改善。使用Voting集的最佳模型（多层感知器）达到了0.72的F1分数。", "conclusion": "内部代码指标虽然解释力有限，但可以作为应用受欢迎程度的有用指标。这挑战了早期认为内部指标无法预测软件质量的发现。", "translation": "在发布前预测移动应用程序的受欢迎程度可以为开发者在竞争激烈的市场中提供战略优势，但它仍然是一个具有挑战性的问题。本研究探讨了内部软件指标（可在部署前从源代码测量）是否可以预测应用程序的受欢迎程度，其定义为用户评分（根据用户评论计算）和每年下载量。我们使用来自F-Droid的446个开源Android应用程序数据集，提取了广泛的特征，包括系统、类和方法级别的代码指标、代码异味以及应用程序元数据。其他信息，如用户评论、下载计数和使用权限，则从Google Play Store收集。我们评估了三种特征集上的回归和分类模型：最小的仅大小基线、领域知情的精选集以及通过特征选择算法得出的投票集。由于数据偏斜，回归模型表现不佳，R^2分数较低。然而，当重构为二元分类（流行 vs. 不流行）时，结果显著改善。最佳模型是使用投票集的多层感知器，F1分数达到0.72。这些结果表明，内部代码指标虽然解释力有限，但可以作为应用程序受欢迎程度的有用指标。这挑战了早期驳回内部指标作为软件质量预测指标的发现。", "summary": "本研究探讨了在发布前使用内部软件指标预测移动应用受欢迎程度的可行性。通过分析446个开源Android应用的源代码指标和元数据，并结合Google Play Store的用户数据，研究发现回归模型表现不佳，但将问题转化为二元分类（流行/不流行）后，模型性能显著提升。最佳的多层感知器模型在F1分数上达到0.72。这表明内部代码指标可以作为应用受欢迎程度的有用预测因子，挑战了此前关于其预测能力不足的观点。", "keywords": "内部软件指标, 应用受欢迎度, 预测, 机器学习, 移动应用", "comments": "这篇论文的创新点在于重新审视了内部软件指标在预测应用受欢迎程度方面的潜力，特别是通过将问题从回归转化为分类。其重要性在于为开发者提供了一种在应用发布前评估其潜在市场表现的工具，有助于早期决策。它也挑战了现有对内部指标预测能力的普遍看法。"}}
{"id": "2507.02171", "title": "Towards Bio-Inspired Robotic Trajectory Planning via Self-Supervised RNN", "authors": ["Miroslav Cibula", "Kristína Malinovská", "Matthias Kerzel"], "categories": ["cs.RO", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      12 pages, 4 figures, 2 tables. To be published in 2025 International Conference on Artificial Neural Networks (ICANN) proceedings. This research was funded by the Horizon Europe project TERAIS, GA no. 101079338, and in part by the Slovak Grant Agency for Science (VEGA), project 1/0373/23", "url": "http://arxiv.org/abs/2507.02171v1", "summary": "Trajectory planning in robotics is understood as generating a sequence of\njoint configurations that will lead a robotic agent, or its manipulator, from\nan initial state to the desired final state, thus completing a manipulation\ntask while considering constraints like robot kinematics and the environment.\nTypically, this is achieved via sampling-based planners, which are\ncomputationally intensive. Recent advances demonstrate that trajectory planning\ncan also be performed by supervised sequence learning of trajectories, often\nrequiring only a single or fixed number of passes through a neural\narchitecture, thus ensuring a bounded computation time. Such fully supervised\napproaches, however, perform imitation learning; they do not learn based on\nwhether the trajectories can successfully reach a goal, but try to reproduce\nobserved trajectories. In our work, we build on this approach and propose a\ncognitively inspired self-supervised learning scheme based on a recurrent\narchitecture for building a trajectory model. We evaluate the feasibility of\nthe proposed method on a task of kinematic planning for a robotic arm. The\nresults suggest that the model is able to learn to generate trajectories only\nusing given paired forward and inverse kinematics models, and indicate that\nthis novel method could facilitate planning for more complex manipulation tasks\nrequiring adaptive solutions.", "comment": "12 pages, 4 figures, 2 tables. To be published in 2025 International\n  Conference on Artificial Neural Networks (ICANN) proceedings. This research\n  was funded by the Horizon Europe project TERAIS, GA no. 101079338, and in\n  part by the Slovak Grant Agency for Science (VEGA), project 1/0373/23", "pdf_url": "http://arxiv.org/pdf/2507.02171v1", "cate": "cs.RO", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "走向基于自监督循环神经网络的仿生机器人轨迹规划", "tldr": "本文提出一种基于自监督循环神经网络的轨迹模型，用于机器人轨迹规划，解决了传统监督学习方法仅模仿轨迹而不能根据目标成功与否进行学习的问题，并展示了其在机器人手臂运动学规划中的可行性。", "motivation": "传统的采样式规划器计算成本高昂。虽然最近的监督序列学习方法能提供有界计算时间，但它们是模仿学习，不根据轨迹是否成功达到目标来学习，而是试图重现观测到的轨迹。本文旨在解决这一局限性。", "method": "提出了一种受认知启发的自监督学习方案，该方案基于循环神经网络架构来构建轨迹模型。该方法仅使用给定的正向和逆向运动学模型来生成轨迹。", "result": "模型能够仅利用给定的正向和逆向运动学模型来学习生成轨迹。", "conclusion": "这种新颖的方法可以促进对需要自适应解决方案的更复杂操作任务的规划。", "translation": "机器人学中的轨迹规划被理解为生成一系列关节配置，使机器人代理或其机械手从初始状态到达期望的最终状态，从而在考虑机器人运动学和环境等约束的同时完成操作任务。通常，这通过基于采样的规划器实现，这些规划器计算密集。最近的进展表明，轨迹规划也可以通过轨迹的监督序列学习来执行，通常只需要通过神经网络架构进行一次或固定次数的传递，从而确保有界的计算时间。然而，这种完全监督的方法执行的是模仿学习；它们不是根据轨迹能否成功达到目标来学习，而是试图重现观测到的轨迹。在我们的工作中，我们在此方法的基础上，提出了一种基于循环架构的、受认知启发的自监督学习方案，用于构建轨迹模型。我们评估了所提出方法在机器人手臂运动学规划任务上的可行性。结果表明，该模型能够仅使用给定的正向和逆向运动学模型来学习生成轨迹，并表明这种新颖的方法可以促进需要自适应解决方案的更复杂操作任务的规划。", "summary": "本文提出了一种受认知启发的自监督学习方案，利用循环神经网络构建机器人轨迹模型，旨在克服传统监督学习方法仅进行模仿学习的局限性。该方法仅依赖于正向和逆向运动学模型，并在机器人手臂运动学规划任务上进行了验证，结果表明其能够有效生成轨迹，并有望应用于更复杂的自适应操作任务。", "keywords": "机器人轨迹规划, 自监督学习, 循环神经网络, 运动学模型, 仿生", "comments": "这项工作通过引入自监督学习解决了传统监督学习在机器人轨迹规划中仅限于模仿学习的限制。其创新点在于结合了认知启发和循环神经网络，使得模型能够根据运动学模型自主学习生成轨迹，而非简单复现。这为处理需要自适应能力和更复杂环境的任务提供了新的方向，具有重要的应用潜力。"}}
{"id": "2507.02456", "title": "System-performance and cost modeling of Large Language Model training and inference", "authors": ["Wenzhe Guo", "Joyjit Kundu", "Uras Tos", "Weijiang Kong", "Giuliano Sisto", "Timon Evenblij", "Manu Perumkunnil"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02456v1", "summary": "Large language models (LLMs), based on transformer architectures, have\nrevolutionized numerous domains within artificial intelligence, science, and\nengineering due to their exceptional scalability and adaptability. However, the\nexponential growth in LLM size and complexity has outpaced advancements in\ncompute capacity, memory bandwidth, network performance, and cost efficiency,\nposing significant challenges to their scalability on distributed systems. To\naddress these limitations, alternative model architectures, optimization\nstrategies, communication-aware network topologies, and novel system design\napproaches have been proposed in literature. This paper introduces a\nperformance-cost modeling methodology for LLM training and inference that\nintegrates state-of-the-art compute techniques with memory optimizations, and\nlatest communication techniques. Building on an analytical performance model,\nour approach incorporates recent innovations such as the flash attention\ntechnique and mixture of experts models to address the memory bandwidth and\ncompute bottlenecks. It also considers the impact of different network\ntopologies and topology-specific communication algorithms with 5D parallellism.\nThe framework also integrates a chiplet cost model. The proposed modeling\nmethodology provides valuable insights to guide future compute system design\nand facilitates hardware-software co-development, in particular due to its\nability to analyze performance-cost trade-offs for various system architectural\nconfigurations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02456v1", "cate": "cs.AR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "大型语言模型训练和推理的系统性能与成本建模", "tldr": "本文提出了一种LLM训练和推理的性能-成本建模方法，整合了先进的计算、内存和通信技术，以应对LLM扩展的挑战并指导未来系统设计。", "motivation": "尽管大型语言模型（LLMs）具有卓越的扩展性和适应性，但其规模和复杂性的指数级增长已超越了计算能力、内存带宽、网络性能和成本效率的进步，这对它们在分布式系统上的扩展性构成了重大挑战。", "method": "本文提出了一种针对LLM训练和推理的性能-成本建模方法。该方法整合了最先进的计算技术、内存优化和最新的通信技术，并基于一个分析性能模型。它还结合了闪电注意力机制（flash attention）和专家混合模型（mixture of experts）等最新创新来解决内存带宽和计算瓶颈。此外，该方法考虑了不同网络拓扑结构和具有5D并行性的特定拓扑通信算法的影响，并集成了一个小芯片（chiplet）成本模型。", "result": "该建模方法提供了宝贵的见解，以指导未来的计算系统设计，并促进硬件-软件协同开发，特别是由于其能够分析各种系统架构配置的性能-成本权衡。", "conclusion": "本文提出的性能-成本建模方法能够为未来计算系统设计提供指导，并促进硬件-软件协同开发，通过分析不同系统架构配置下的性能-成本权衡，有效应对LLM扩展的挑战。", "translation": "基于Transformer架构的大型语言模型（LLMs）因其卓越的可扩展性和适应性，彻底改变了人工智能、科学和工程领域的众多领域。然而，LLM规模和复杂性的指数级增长已经超越了计算能力、内存带宽、网络性能和成本效率的进步，这对它们在分布式系统上的可扩展性构成了重大挑战。为了解决这些限制，文献中提出了替代模型架构、优化策略、通信感知网络拓扑和新颖的系统设计方法。本文介绍了一种用于LLM训练和推理的性能-成本建模方法，该方法将最先进的计算技术与内存优化以及最新的通信技术相结合。我们的方法建立在一个分析性能模型之上，并结合了闪电注意力机制和专家混合模型等最新创新，以解决内存带宽和计算瓶颈。它还考虑了不同网络拓扑结构和具有5D并行性的特定拓扑通信算法的影响。该框架还集成了一个小芯片成本模型。所提出的建模方法提供了宝贵的见解，以指导未来的计算系统设计，并促进硬件-软件协同开发，特别是由于其能够分析各种系统架构配置的性能-成本权衡。", "summary": "本文提出了一种创新的性能-成本建模方法，用于分析大型语言模型（LLMs）的训练和推理过程。该方法旨在解决LLM规模增长带来的计算、内存和网络瓶颈，通过整合先进的计算技术、内存优化、最新通信技术（包括5D并行性）、闪电注意力机制、专家混合模型以及小芯片成本模型。该模型能够评估不同系统架构配置下的性能-成本权衡，为未来的计算系统设计和硬件-软件协同开发提供关键指导。", "keywords": "大型语言模型, 性能建模, 成本建模, 训练推理, 系统设计", "comments": "本文的创新之处在于提出了一种综合性的性能-成本建模方法，将多种前沿技术（如闪电注意力、专家混合模型、5D并行性通信）和成本模型（小芯片成本）整合到一个统一的框架中，以解决大型语言模型扩展中的核心挑战。这对于指导未来高性能计算系统的硬件-软件协同设计具有重要意义，尤其是在LLM部署成本日益增加的背景下。"}}
{"id": "2507.01984", "title": "Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features", "authors": ["Gautam Kishore Shahi"], "categories": ["cs.LG", "cs.CL", "cs.SI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01984v1", "summary": "Amid a tidal wave of misinformation flooding social media during elections\nand crises, extensive research has been conducted on misinformation detection,\nprimarily focusing on text-based or image-based approaches. However, only a few\nstudies have explored multimodal feature combinations, such as integrating text\nand images for building a classification model to detect misinformation. This\nstudy investigates the effectiveness of different multimodal feature\ncombinations, incorporating text, images, and social features using an early\nfusion approach for the classification model. This study analyzed 1,529 tweets\ncontaining both text and images during the COVID-19 pandemic and election\nperiods collected from Twitter (now X). A data enrichment process was applied\nto extract additional social features, as well as visual features, through\ntechniques such as object detection and optical character recognition (OCR).\nThe results show that combining unsupervised and supervised machine learning\nmodels improves classification performance by 15% compared to unimodal models\nand by 5% compared to bimodal models. Additionally, the study analyzes the\npropagation patterns of misinformation based on the characteristics of\nmisinformation tweets and the users who disseminate them.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01984v1", "cate": "cs.LG", "date": "2025-06-26", "updated": "2025-06-26", "AI": {"title_translation": "多模态虚假信息检测：利用语言、视觉和社交特征的早期融合", "tldr": "本研究通过早期融合语言、视觉和社交特征的多模态方法，显著提升了社交媒体上虚假信息检测的性能。", "motivation": "社交媒体上虚假信息泛滥，现有研究主要集中于文本或图像，但很少有研究探索整合文本、图像和社交特征的多模态组合，以构建更有效的虚假信息检测分类模型。", "method": "本研究分析了从Twitter（现为X）收集的1,529条在COVID-19大流行和选举期间包含文本和图像的推文。通过目标检测和光学字符识别（OCR）等技术进行数据丰富，提取额外的社交特征和视觉特征。采用早期融合方法，结合文本、图像和社交特征来构建分类模型。", "result": "结合无监督和有监督机器学习模型，分类性能比单模态模型提高了15%，比双模态模型提高了5%。此外，研究还分析了虚假信息推文和传播用户的特征及其传播模式。", "conclusion": "早期融合语言、视觉和社交等多模态特征显著提升了虚假信息检测的分类性能，并且多模态方法在性能上优于单模态和双模态方法。", "translation": "在选举和危机期间社交媒体上充斥着虚假信息的浪潮中，已经对虚假信息检测进行了广泛研究，主要集中于基于文本或图像的方法。然而，很少有研究探索多模态特征组合，例如整合文本和图像来构建虚假信息检测分类模型。本研究调查了不同多模态特征组合的有效性，采用早期融合方法将文本、图像和社交特征整合到分类模型中。本研究分析了从Twitter（现为X）收集的1,529条在COVID-19大流行和选举期间包含文本和图像的推文。通过目标检测和光学字符识别（OCR）等技术，应用数据丰富过程来提取额外的社交特征以及视觉特征。结果表明，结合无监督和有监督机器学习模型可以将分类性能比单模态模型提高15%，比双模态模型提高5%。此外，本研究还根据虚假信息推文和传播用户的特征分析了虚假信息的传播模式。", "summary": "这项研究旨在通过早期融合语言、视觉和社交特征来改进社交媒体上的虚假信息检测。研究分析了来自Twitter的1,529条推文，并利用目标检测和OCR等技术丰富了数据以提取多模态特征。结果显示，与单模态和双模态模型相比，结合无监督和有监督机器学习模型的多模态方法能显著提高虚假信息分类性能，并分析了虚假信息的传播模式。", "keywords": "虚假信息检测, 多模态融合, 社交媒体, 早期融合, 机器学习", "comments": "这项研究的创新之处在于其多模态早期融合方法，不仅整合了文本和图像，还加入了社交特征，这在现有研究中相对较少。其重要性在于通过结合不同类型的信息，显著提升了虚假信息检测的准确性。同时，数据丰富过程和对传播模式的分析也增加了研究的深度。"}}
{"id": "2507.02233", "title": "Domain-Adversarial Transfer Learning for Fault Root Cause Identification in Cloud Computing Systems", "authors": ["Bruce Fang", "Danyi Gao"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02233v1", "summary": "This paper addresses the challenge of fault root cause identification in\ncloud computing environments. The difficulty arises from complex system\nstructures, dense service coupling, and limited fault information. To solve\nthis problem, an intelligent identification algorithm based on transfer\nlearning is proposed. The method introduces a shared feature extraction module\nand a domain adversarial mechanism to enable effective knowledge transfer from\nthe source domain to the target domain. This improves the model's\ndiscriminative ability and generalization performance in the target domain. The\nmodel incorporates a pseudo-label selection strategy. When labeled samples are\nlacking in the target domain, high-confidence predictions are used in training.\nThis enhances the model's ability to recognize minority classes. To evaluate\nthe stability and adaptability of the method in real-world scenarios,\nexperiments are designed under three conditions: label scarcity, class\nimbalance, and heterogeneous node environments. Experimental results show that\nthe proposed method outperforms existing mainstream approaches in several key\nmetrics, including accuracy, F1-Score, and AUC. The model demonstrates stronger\ndiscriminative power and robustness. Notably, under extreme class imbalance and\nsignificant structural differences in the target domain, the model still\nmaintains high performance. This validates the effectiveness and practical\nvalue of the proposed mechanisms in complex cloud computing systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02233v1", "cate": "cs.DC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "云原生系统中故障根因识别的领域对抗迁移学习", "tldr": "本文提出了一种基于领域对抗迁移学习的智能算法，用于解决云原生环境中故障根因识别的挑战，并在标签稀缺、类别不平衡和异构节点环境下表现出卓越的性能。", "motivation": "云原生环境中复杂的系统结构、密集的业务耦合和有限的故障信息导致故障根因识别面临挑战。", "method": "本文提出了一种基于迁移学习的智能识别算法。该方法引入了共享特征提取模块和领域对抗机制，以实现从源域到目标域的有效知识迁移。此外，该模型还结合了伪标签选择策略，在目标域缺乏标记样本时，利用高置信度预测进行训练，以增强对少数类别的识别能力。", "result": "实验结果表明，所提出的方法在准确率、F1-Score和AUC等关键指标上优于现有主流方法。该模型展现出更强的判别能力和鲁棒性。即使在极端类别不平衡和目标域结构差异显著的情况下，模型仍能保持高性能。", "conclusion": "所提出的机制在复杂的云原生系统中具有有效性和实用价值，能够有效解决故障根因识别问题。", "translation": "本文旨在解决云计算环境中故障根因识别的挑战。由于复杂的系统结构、密集的业务耦合和有限的故障信息，导致识别难度较大。为了解决这个问题，本文提出了一种基于迁移学习的智能识别算法。该方法引入了共享特征提取模块和领域对抗机制，以实现从源域到目标域的有效知识迁移。这提高了模型在目标域的判别能力和泛化性能。该模型结合了伪标签选择策略。当目标域缺乏标记样本时，高置信度预测被用于训练。这增强了模型识别少数类别的能力。为了评估该方法在真实场景中的稳定性和适应性，实验在三种条件下进行：标签稀缺、类别不平衡和异构节点环境。实验结果表明，所提出的方法在准确率、F1-Score和AUC等几个关键指标上优于现有主流方法。该模型展现出更强的判别能力和鲁棒性。值得注意的是，在极端类别不平衡和目标域结构差异显著的情况下，该模型仍然保持了高性能。这验证了所提出的机制在复杂云计算系统中的有效性和实用价值。", "summary": "本文针对云原生环境中故障根因识别的难题，提出了一种基于领域对抗迁移学习的智能算法。该方法通过共享特征提取和领域对抗机制实现跨域知识迁移，并结合伪标签选择策略以应对标签稀缺和类别不平衡问题。实验证明，该方法在准确率、F1-Score和AUC等指标上优于现有方法，即使在极端复杂环境下也能保持高性能，验证了其在复杂云系统中的有效性和鲁棒性。", "keywords": "故障根因识别, 迁移学习, 领域对抗, 云计算, 伪标签", "comments": "该论文的创新点在于将领域对抗迁移学习应用于云原生系统中的故障根因识别，并结合伪标签策略解决数据稀缺和类别不平衡问题。这对于提升云系统运维效率和稳定性具有重要意义。该方法在实际复杂场景下的优异表现，特别是对极端不平衡和异构环境的适应性，是其主要亮点。"}}
{"id": "2507.02648", "title": "Recourse, Repair, Reparation, & Prevention: A Stakeholder Analysis of AI Supply Chains", "authors": ["Aspen K. Hopkins", "Isabella Struckman", "Kevin Klyman", "Susan S. Silbey"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02648v1", "summary": "The AI industry is exploding in popularity, with increasing attention to\npotential harms and unwanted consequences. In the current digital ecosystem, AI\ndeployments are often the product of AI supply chains (AISC): networks of\noutsourced models, data, and tooling through which multiple entities contribute\nto AI development and distribution. AI supply chains lack the modularity,\nredundancies, or conventional supply chain practices that enable\nidentification, isolation, and easy correction of failures, exacerbating the\nalready difficult processes of responding to ML-generated harms. As the\nstakeholders participating in and impacted by AISCs have scaled and\ndiversified, so too have the risks they face. In this stakeholder analysis of\nAI supply chains, we consider who participates in AISCs, what harms they face,\nwhere sources of harm lie, and how market dynamics and power differentials\ninform the type and probability of remedies. Because AI supply chains are\npurposely invented and implemented, they may be designed to account for, rather\nthan ignore, the complexities, consequences, and risks of deploying AI systems.\nTo enable responsible design and management of AISCs, we offer a typology of\nresponses to AISC-induced harms: recourse, repair, reparation or prevention. We\napply this typology to stakeholders participating in a health-care AISC across\nthree stylized markets $\\unicode{x2013}$ vertical integration, horizontal\nintegration, free market $\\unicode{x2013}$ to illustrate how stakeholder\npositioning and power within an AISC may shape responses to an experienced\nharm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02648v1", "cate": "cs.CY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "追索、修复、赔偿与预防：AI供应链的利益相关者分析", "tldr": "本研究对AI供应链（AISC）进行了利益相关者分析，探讨了参与者、面临的危害、危害来源以及市场动态如何影响补救措施。论文提出了一种应对AISC所致危害的类型学（追索、修复、赔偿或预防），并将其应用于医疗保健AISC的案例，以说明利益相关者在AISC中的地位和权力如何影响对危害的响应。", "motivation": "随着AI行业的爆炸式增长，人们越来越关注其潜在危害和意外后果。当前的AI部署通常是AI供应链（AISC）的产物，但AISC缺乏模块化、冗余或传统供应链实践，导致难以识别、隔离和纠正故障，从而加剧了应对机器学习生成危害的难度。随着AISC的利益相关者规模和多样性增加，他们面临的风险也随之增加。", "method": "本研究对AI供应链进行了利益相关者分析，审视了谁参与AISC、他们面临哪些危害、危害源在哪里，以及市场动态和权力差异如何影响补救措施的类型和可能性。为了实现AISC的负责任设计和管理，论文提出了一种应对AISC引发危害的类型学：追索、修复、赔偿或预防。并将此类型学应用于医疗保健AISC中三个程式化市场（垂直整合、水平整合、自由市场）的利益相关者，以说明利益相关者在AISC中的定位和权力如何影响对所经历危害的响应。", "result": "通过将所提出的类型学应用于医疗保健AI供应链的案例，本研究说明了利益相关者在AI供应链中的定位和权力如何塑造对所经历危害的响应。", "conclusion": "AI供应链是特意发明和实施的，因此它们可以被设计来考虑而非忽视部署AI系统的复杂性、后果和风险。通过负责任的设计和管理，可以更好地应对AI供应链带来的危害。", "translation": "AI行业正以惊人的速度普及，人们越来越关注其潜在危害和不必要的后果。在当前的数字生态系统中，AI的部署往往是AI供应链（AISC）的产物：由外包模型、数据和工具组成的网络，多个实体通过它们为AI的开发和分发做出贡献。AI供应链缺乏模块化、冗余或传统的供应链实践，这些实践能够识别、隔离和轻松纠正故障，从而加剧了应对机器学习生成危害的困难过程。随着参与和受AISC影响的利益相关者规模和多样性不断扩大，他们面临的风险也随之增加。在对AI供应链的利益相关者分析中，我们考虑了谁参与AISC、他们面临哪些危害、危害源在哪里，以及市场动态和权力差异如何影响补救措施的类型和可能性。因为AI供应链是特意发明和实施的，所以它们可以被设计来考虑而非忽视部署AI系统的复杂性、后果和风险。为了实现AISC的负责任设计和管理，我们提出了一种应对AISC引发危害的类型学：追索、修复、赔偿或预防。我们将此类型学应用于医疗保健AISC中三个程式化市场——垂直整合、水平整合、自由市场——的利益相关者，以说明利益相关者在AISC中的定位和权力如何塑造对所经历危害的响应。", "summary": "本研究对AI供应链（AISC）进行了利益相关者分析，探讨了AISC参与者、他们面临的危害、危害来源以及市场动态对补救措施的影响。鉴于AISC缺乏传统供应链的故障应对机制，作者提出并应用了一种应对AISC所致危害的类型学（追索、修复、赔偿或预防），并通过医疗保健AISC的案例研究，阐明了利益相关者的地位和权力如何影响对危害的响应，强调了负责任设计AISC的重要性。", "keywords": "AI供应链, 利益相关者分析, 危害应对, 追索, 预防", "comments": "本文的创新之处在于对AI供应链进行了系统的利益相关者分析，并提出了一个实用的危害应对类型学（追索、修复、赔偿、预防）。它强调了AI供应链的固有复杂性和风险，并呼吁在设计之初就考虑这些因素，而非事后补救，这对于推动负责任的AI部署具有重要意义。通过具体的市场案例分析，论文有效地说明了利益相关者权力在危害应对中的作用。"}}
{"id": "2507.02678", "title": "Imitation and Heterogeneity Shape the Resilience of Community Currency Networks", "authors": ["Camilla Ancona", "Dora Ricci", "Carmela Bernardo", "Francesco Lo Iudice", "Anton Proskurnikov", "Francesco Vasca"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02678v1", "summary": "Community currency networks are made up of individuals and or companies that\nshare some physical or social characteristics and engage in economic\ntransactions using a virtual currency. This paper investigates the structural\nand dynamic properties of such mutual credit systems through a case study of\nSardex, a community currency initiated and mainly operating in Sardinia, Italy.\nThe transaction network is modeled as a directed weighted graph and analyzed\nthrough a graph theoretic framework focused on the analysis of strongly\nconnected components, condensed representations, and behavioral connectivity\npatterns. Emphasis is placed on understanding the evolution of the network's\ncore and peripheral structures over a three year period, with attention to\ntemporal contraction, flow asymmetries, and structural fragmentation depending\non different user types. Our findings reveal persistent deviations from degree\nbased null models and suggest the presence of behavioral imitation,\nspecifically, a user preference for more active peers. We further assess the\nimpact of heterogeneous connections between different type of users, which\nstrengthen the network topology and enhance its resilience.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02678v1", "cate": "cs.CE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "模仿和异质性塑造社区货币网络的弹性", "tldr": "本文通过Sardex案例研究，发现用户模仿行为（偏好活跃用户）和异质性连接增强了社区货币网络的弹性。", "motivation": "本文旨在通过对Sardex（一种社区货币）的案例研究，调查互助信用系统（即社区货币网络）的结构和动态特性。", "method": "研究方法是通过对意大利撒丁岛的Sardex社区货币系统进行案例研究。交易网络被建模为有向加权图，并利用图论框架进行分析，重点关注强连通分量、凝聚表示和行为连接模式。研究还关注了网络核心和外围结构在三年内的演变，包括时间收缩、流量不对称和基于不同用户类型的结构碎片化。", "result": "研究结果显示，网络行为与基于度的零模型存在持续偏差，表明存在行为模仿，具体表现为用户偏好更活跃的同行。此外，不同类型用户之间的异质连接能够增强网络拓扑结构并提升其弹性。", "conclusion": "本研究得出结论，行为模仿（用户偏好更活跃的同行）和异质性连接是塑造和增强社区货币网络弹性的关键因素。", "translation": "社区货币网络由具有某些物理或社会特征的个人或公司组成，他们使用虚拟货币进行经济交易。本文通过Sardex（一种在意大利撒丁岛发起并主要运营的社区货币）的案例研究，调查了这种互助信用系统的结构和动态特性。交易网络被建模为有向加权图，并通过侧重于强连通分量、凝聚表示和行为连接模式分析的图论框架进行分析。重点放在理解网络核心和外围结构在三年内的演变，关注时间收缩、流量不对称和不同用户类型导致的结构碎片化。我们的发现揭示了与基于度的零模型持续存在的偏差，并表明存在行为模仿，特别是用户偏好更活跃的同行。我们进一步评估了不同类型用户之间异质连接的影响，这些连接增强了网络拓扑并提高了其弹性。", "summary": "本文通过对意大利撒丁岛Sardex社区货币系统的案例研究，分析了社区货币网络的结构和动态特性。研究将交易网络建模为有向加权图，并运用图论框架进行分析，发现用户行为中的模仿现象（即用户倾向于偏好更活跃的同行）以及不同用户类型间的异质连接，显著增强了网络的拓扑结构和弹性，并揭示了与零模型的偏差。", "keywords": "社区货币, 网络弹性, Sardex, 行为模仿, 异质性", "comments": "这篇论文通过将行为因素（如模仿）和异质性纳入分析，为理解社区货币网络的弹性机制提供了宝贵的见解。采用真实的案例研究（Sardex）和严谨的图论框架，增强了研究的实践相关性和分析深度。识别出行为模仿和异质性连接作为关键的弹性因素，是本研究的重要贡献。"}}
{"id": "2507.02076", "title": "Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs", "authors": ["Mohammad Ali Alomrani", "Yingxue Zhang", "Derek Li", "Qianyi Sun", "Soumyasundar Pal", "Zhanguang Zhang", "Yaochen Hu", "Rohan Deepak Ajwani", "Antonios Valkanas", "Raika Karimi", "Peng Cheng", "Yunzhou Wang", "Pengyi Liao", "Hanrui Huang", "Bin Wang", "Jianye Hao", "Mark Coates"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02076v1", "summary": "Large language models (LLMs) have rapidly progressed into general-purpose\nagents capable of solving a broad spectrum of tasks. However, current models\nremain inefficient at reasoning: they apply fixed inference-time compute\nregardless of task complexity, often overthinking simple problems while\nunderthinking hard ones. This survey presents a comprehensive review of\nefficient test-time compute (TTC) strategies, which aim to improve the\ncomputational efficiency of LLM reasoning. We introduce a two-tiered taxonomy\nthat distinguishes between L1-controllability, methods that operate under fixed\ncompute budgets, and L2-adaptiveness, methods that dynamically scale inference\nbased on input difficulty or model confidence. We benchmark leading proprietary\nLLMs across diverse datasets, highlighting critical trade-offs between\nreasoning performance and token usage. Compared to prior surveys on efficient\nreasoning, our review emphasizes the practical control, adaptability, and\nscalability of TTC methods. Finally, we discuss emerging trends such as hybrid\nthinking models and identify key challenges for future work towards making LLMs\nmore computationally efficient, robust, and responsive to user constraints.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02076v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "预算内推理：大型语言模型中自适应和可控测试时间计算的综述", "tldr": "这篇综述探讨了大型语言模型（LLMs）在推理时计算效率低下的问题，并提出了测试时间计算（TTC）策略，旨在提高LLM推理的计算效率，并引入了一个区分L1-可控性（固定预算）和L2-自适应性（动态扩展）的分类法。", "motivation": "当前大型语言模型在推理时计算效率低下，无论任务复杂性如何都使用固定的推理时间计算，导致在简单问题上过度思考，在困难问题上思考不足。", "method": "本综述对高效的测试时间计算（TTC）策略进行了全面回顾，并引入了一个两层分类法，区分了L1-可控性（在固定计算预算下运行的方法）和L2-自适应性（根据输入难度或模型置信度动态调整推理规模的方法）。此外，还对领先的专有LLM在不同数据集上进行了基准测试。", "result": "基准测试突出了推理性能和token使用之间的关键权衡。与之前的效率推理综述相比，本综述强调了TTC方法的实际控制、适应性和可扩展性。", "conclusion": "讨论了混合思维模型等新兴趋势，并指出了未来使LLM更具计算效率、鲁棒性并响应用户约束的关键挑战。", "translation": "大型语言模型（LLMs）已迅速发展成为能够解决广泛任务的通用代理。然而，当前模型在推理方面仍然效率低下：无论任务复杂性如何，它们都应用固定的推理时间计算，经常对简单问题过度思考，而对困难问题思考不足。本综述全面回顾了高效的测试时间计算（TTC）策略，旨在提高LLM推理的计算效率。我们引入了一个两层分类法，区分了L1-可控性（在固定计算预算下运行的方法）和L2-自适应性（根据输入难度或模型置信度动态调整推理规模的方法）。我们对领先的专有LLM在不同数据集上进行了基准测试，突出了推理性能和token使用之间的关键权衡。与之前的效率推理综述相比，我们的综述强调了TTC方法的实际控制、适应性和可扩展性。最后，我们讨论了混合思维模型等新兴趋势，并指出了未来使LLM更具计算效率、鲁棒性并响应用户约束的关键挑战。", "summary": "这篇综述论文旨在解决大型语言模型（LLMs）在推理时计算效率低下的问题，即它们倾向于使用固定的计算量，无论任务复杂性如何。论文全面回顾了高效的测试时间计算（TTC）策略，并提出了一个两层分类法：L1-可控性（固定预算）和L2-自适应性（动态调整）。通过对主流LLM进行基准测试，论文强调了推理性能与token使用之间的权衡，并讨论了TTC方法在实际控制、适应性和可扩展性方面的重要性，最后展望了未来的研究方向和挑战。", "keywords": "大型语言模型, 计算效率, 测试时间计算, 自适应推理, 综述", "comments": "这篇综述论文解决了LLM领域一个核心且日益重要的问题：计算效率。它不仅系统地梳理了现有的测试时间计算（TTC）策略，还提出了一个新颖且实用的两层分类法（L1-可控性与L2-自适应性），这对于理解和开发更高效的LLM推理方法具有重要指导意义。强调实际控制、适应性和可扩展性，使其区别于其他综述，更具应用价值。论文指出的未来挑战也为研究人员提供了清晰的方向。"}}
{"id": "2507.02200", "title": "ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning", "authors": ["Xiao Wang", "Jingtao Jiang", "Qiang Chen", "Lan Chen", "Lin Zhu", "Yaowei Wang", "Yonghong Tian", "Jin Tang"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      A Strong Baseline for Reasoning based Event Stream Scene Text Recognition", "url": "http://arxiv.org/abs/2507.02200v1", "summary": "Event stream based scene text recognition is a newly arising research topic\nin recent years which performs better than the widely used RGB cameras in\nextremely challenging scenarios, especially the low illumination, fast motion.\nExisting works either adopt end-to-end encoder-decoder framework or large\nlanguage models for enhanced recognition, however, they are still limited by\nthe challenges of insufficient interpretability and weak contextual logical\nreasoning. In this work, we propose a novel chain-of-thought reasoning based\nevent stream scene text recognition framework, termed ESTR-CoT. Specifically,\nwe first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input\nevent stream into tokens and utilize a Llama tokenizer to encode the given\ngeneration prompt. A Q-former is used to align the vision token to the\npre-trained large language model Vicuna-7B and output both the answer and\nchain-of-thought (CoT) reasoning process simultaneously. Our framework can be\noptimized using supervised fine-tuning in an end-to-end manner. In addition, we\nalso propose a large-scale CoT dataset to train our framework via a three stage\nprocessing (i.e., generation, polish, and expert verification). This dataset\nprovides a solid data foundation for the development of subsequent\nreasoning-based large models. Extensive experiments on three event stream STR\nbenchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the\neffectiveness and interpretability of our proposed framework. The source code\nand pre-trained models will be released on\nhttps://github.com/Event-AHU/ESTR-CoT.", "comment": "A Strong Baseline for Reasoning based Event Stream Scene Text\n  Recognition", "pdf_url": "http://arxiv.org/pdf/2507.02200v1", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "ESTR-CoT：迈向基于事件流的可解释和准确场景文本识别与思维链推理", "tldr": "本文提出ESTR-CoT框架，结合思维链推理实现可解释且准确的事件流场景文本识别，并构建大规模CoT数据集以支持模型训练。", "motivation": "现有基于事件流的场景文本识别方法在解释性和上下文逻辑推理方面存在局限性，尤其是在低光照和快速运动等极端场景下，尽管优于RGB相机，但仍需改进。", "method": "提出ESTR-CoT框架，该框架首先使用视觉编码器EVA-CLIP (ViT-G/14) 将事件流转换为token，并利用Llama tokenizer编码生成提示。Q-former用于将视觉token与预训练的Vicuna-7B大型语言模型对齐，同时输出答案和思维链（CoT）推理过程。框架可进行端到端监督微调。此外，还提出了一个通过生成、优化和专家验证三阶段处理构建的大规模CoT数据集。", "result": "在三个事件流STR基准数据集（EventSTR, WordArt*, IC15*）上的广泛实验充分验证了所提框架的有效性和可解释性。", "conclusion": "Not mentioned in abstract", "translation": "基于事件流的场景文本识别是近年来新兴的研究课题，在极端挑战性场景，特别是低光照、快速运动下，其性能优于广泛使用的RGB相机。现有工作要么采用端到端编码器-解码器框架，要么使用大型语言模型来增强识别，然而，它们仍然受到解释性不足和上下文逻辑推理能力弱的限制。在这项工作中，我们提出了一种新颖的基于思维链推理的事件流场景文本识别框架，命名为ESTR-CoT。具体来说，我们首先采用视觉编码器EVA-CLIP (ViT-G/14) 将输入事件流转换为 token，并利用Llama tokenizer对给定的生成提示进行编码。Q-former用于将视觉 token 与预训练的大型语言模型Vicuna-7B对齐，并同时输出答案和思维链（CoT）推理过程。我们的框架可以通过端到端的监督微调进行优化。此外，我们还提出了一个大规模的CoT数据集，通过三阶段处理（即生成、优化和专家验证）来训练我们的框架。该数据集为后续基于推理的大型模型的发展提供了坚实的数据基础。在三个事件流STR基准数据集（即EventSTR、WordArt*、IC15*）上进行的广泛实验充分验证了我们所提出框架的有效性和可解释性。源代码和预训练模型将发布在https://github.com/Event-AHU/ESTR-CoT。", "summary": "本文提出ESTR-CoT框架，旨在解决现有事件流场景文本识别方法在解释性和上下文逻辑推理方面的不足。ESTR-CoT结合EVA-CLIP、Llama tokenizer和Vicuna-7B大型语言模型，通过Q-former实现视觉与语言的对齐，同时输出识别结果和思维链推理过程。该框架支持端到端监督微调，并引入了一个大规模三阶段处理的CoT数据集。实验证明ESTR-CoT在多个基准数据集上表现出卓越的有效性和可解释性。", "keywords": "事件流, 场景文本识别, 思维链推理, 大型语言模型, 可解释性", "comments": "ESTR-CoT的创新之处在于将思维链推理引入事件流场景文本识别，有效提升了模型的可解释性和上下文逻辑推理能力。通过结合大型视觉-语言模型（EVA-CLIP, Vicuna-7B）以及构建专门的CoT数据集，为该领域提供了一个新的、更具洞察力的解决方案，对于处理复杂和挑战性的场景文本识别任务具有重要意义。"}}
{"id": "2507.02144", "title": "Optimality Loss Minimization in Distributed Control with Application to District Heating", "authors": ["Audrey Blizard", "Stephanie Stockar"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Transactions on Control Systems Technology", "url": "http://arxiv.org/abs/2507.02144v1", "summary": "This paper presents a novel partitioning method designed to minimize control\nperformance degradation resulting from partitioning a system for distributed\ncontrol while maintaining the computational benefits of these methods. A\ngame-theoretic performance metric, the modified Price of Anarchy, is introduced\nand is used in a generalizable partitioning metric to quantify optimality\nlosses in a distributed controller. By finding the partition that minimizes the\npartitioning metric, the best-performing distributed control design is chosen.\nThe presented partitioning metric is control-design agnostic, making it broadly\napplicable to many control design problems. In this paper, the developed metric\nis used to minimize the performance losses in the distributed control of a\ndemand-flexible District Heating Network. The final distributed controller is\nprovably feasible and stable. In simulation, this novel partitioning performed\nsimilarly to the centralized controller, increasing overall heat losses by only\n1.9%, as compared to a similarly-sized baseline partition, which resulted in a\n22% increase in losses.", "comment": "Submitted to IEEE Transactions on Control Systems Technology", "pdf_url": "http://arxiv.org/pdf/2507.02144v1", "cate": "eess.SY", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "分布式控制中的最优性损失最小化及其在区域供热中的应用", "tldr": "本文提出了一种新颖的分区方法，旨在最小化分布式控制中由于系统分区而导致的控制性能下降，同时保持计算优势。该方法引入了一种博弈论性能指标（修正的无政府价格）来量化最优性损失，并通过最小化该指标来选择最佳的分布式控制设计。该方法是控制设计无关的，并应用于区域供热网络，在仿真中表现出与集中式控制器相似的性能，热损失仅增加1.9%。", "motivation": "在分布式控制中，系统分区虽然带来了计算上的优势，但也常常导致控制性能的下降。本文的动机是开发一种新颖的方法来最小化这种性能下降，从而在保持分布式控制计算效益的同时，提高其控制性能。", "method": "本文提出了一种新颖的分区方法，通过引入一种博弈论性能指标——修正的无政府价格（modified Price of Anarchy），来量化分布式控制器中的最优性损失。该指标被用于一个可泛化的分区度量中。通过寻找使该分区度量最小化的分区，从而选择性能最佳的分布式控制设计。该方法是控制设计无关的，并应用于需求灵活的区域供热网络，以最小化其分布式控制中的性能损失。", "result": "所开发的分布式控制器被证明是可行且稳定的。在仿真中，这种新颖的分区方法表现出与集中式控制器相似的性能，总热损失仅增加了1.9%。相比之下，一个类似大小的基线分区导致了22%的损失增加。", "conclusion": "本文提出的新颖分区方法能够有效最小化分布式控制中的最优性损失，并在区域供热网络的应用中展现出与集中式控制器相近的优异性能，验证了其可行性和有效性。", "translation": "本文提出了一种新颖的分区方法，旨在最小化由于系统分区进行分布式控制而导致的控制性能下降，同时保持这些方法的计算优势。引入了一种博弈论性能指标——修正的无政府价格，并将其用于一个可泛化的分区度量中，以量化分布式控制器中的最优性损失。通过寻找使该分区度量最小化的分区，从而选择性能最佳的分布式控制设计。所提出的分区度量与控制设计无关，使其广泛适用于许多控制设计问题。在本文中，所开发的度量用于最小化需求灵活的区域供热网络分布式控制中的性能损失。最终的分布式控制器被证明是可行且稳定的。在仿真中，这种新颖的分区方法表现出与集中式控制器相似的性能，总热损失仅增加了1.9%，而一个类似大小的基线分区则导致了22%的损失增加。", "summary": "本文提出了一种创新的分区方法，旨在解决分布式控制中因系统分区导致的性能下降问题，同时保留其计算优势。该方法引入了基于博弈论的“修正的无政府价格”作为性能指标，用于量化和最小化分布式控制器中的最优性损失。该分区度量普适于多种控制设计。在区域供热网络的实际应用中，所开发的分布式控制器不仅被证明可行且稳定，而且在仿真中表现出与集中式控制器相近的性能，显著降低了热损失，仅增加了1.9%，远优于基线分区的22%损失。", "keywords": "分布式控制, 分区方法, 最优性损失, 区域供热, 无政府价格", "comments": "本文的创新点在于提出了一种新颖的分区方法和基于修正的无政府价格的普适性分区度量，用于量化和最小化分布式控制中的最优性损失。其重要性在于，该方法能够有效提升分布式控制的性能，使其在保持计算优势的同时，接近集中式控制器的性能水平，尤其在区域供热等实际应用中展现出显著效益。该方法与控制设计无关的特性也增加了其广泛适用性。"}}
{"id": "2507.02346", "title": "STAR-RIS Transceivers: Integrated Sensing and Communication with Pulsed Signals", "authors": ["Hedieh Taremizadeh", "Emanuele Grossi", "Luca Venturino"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted to the 33rd European Signal Processing Conference (EUSIPCO 2025), Isola delle Femmine, Palermo, Italy", "url": "http://arxiv.org/abs/2507.02346v1", "summary": "This study examines an integrated sensing and communication (ISAC)\ntransceiver featuring a simultaneous transmitting and reflecting reconfigurable\nintelligent surface (STAR-RIS) and a receiver equipped with a passive\nelectronically scanned array (PESA) and a single digital channel. By utilizing\na periodic pulsed signal emitted by a feeder, we introduce at the STAR-RIS a\nspace modulation to illuminate two angular directions observed by the radar\nreceiver, one in each half-space, and a time modulation to distinguish the\ncorresponding echoes from prospective moving targets and embed communication\nmessages. The proposed time modulation employs orthogonal binary codebooks with\ndifferent trade-offs in transmission and error rates, while having minimal\nimpact on the radar performance, evaluated by probability of detection and root\nmean square error in the radial velocity estimation.", "comment": "Accepted to the 33rd European Signal Processing Conference (EUSIPCO\n  2025), Isola delle Femmine, Palermo, Italy", "pdf_url": "http://arxiv.org/pdf/2507.02346v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "STAR-RIS 收发器：脉冲信号的集成感知与通信", "tldr": "本研究提出了一种基于STAR-RIS的集成感知与通信(ISAC)收发器，利用脉冲信号的空间和时间调制实现雷达探测和通信，同时对雷达性能影响最小。", "motivation": "探索一种新的集成感知与通信(ISAC)收发器设计，以提高系统性能并实现雷达探测和信息传输的协同。", "method": "该研究提出了一种ISAC收发器，其特点是采用同时传输和反射可重构智能表面（STAR-RIS）以及配备无源电子扫描阵列（PESA）和单个数字通道的接收器。通过馈线发射周期性脉冲信号，在STAR-RIS上引入空间调制以照亮雷达接收器观察到的两个角方向，并引入时间调制以区分来自潜在移动目标的相应回波并嵌入通信消息。所提出的时间调制采用正交二值码本，在传输和误码率方面具有不同的权衡。", "result": "所提出的时间调制对雷达性能影响最小，雷达性能通过检测概率和径向速度估计的均方根误差进行评估。", "conclusion": "论文成功设计了一种基于STAR-RIS的ISAC收发器，通过巧妙的空间和时间调制，在实现通信的同时，保持了良好的雷达探测性能。", "translation": "本研究考察了一种集成感知与通信（ISAC）收发器，该收发器具有同时传输和反射可重构智能表面（STAR-RIS），以及配备无源电子扫描阵列（PESA）和单个数字通道的接收器。通过利用馈线发射的周期性脉冲信号，我们在STAR-RIS上引入空间调制，以照亮雷达接收器观察到的两个角方向（每个半空间一个），并引入时间调制，以区分来自潜在移动目标的相应回波并嵌入通信消息。所提出的时间调制采用正交二值码本，在传输和误码率方面具有不同的权衡，同时对雷达性能（通过检测概率和径向速度估计的均方根误差评估）影响最小。", "summary": "本文提出了一种基于STAR-RIS的集成感知与通信（ISAC）收发器设计。该系统利用周期性脉冲信号，通过STAR-RIS进行空间调制以实现雷达探测，并进行时间调制以区分目标回波并嵌入通信数据。研究表明，所采用的正交二值码本在提供通信能力的同时，对雷达探测性能（如检测概率和径向速度估计误差）的影响极小。", "keywords": "STAR-RIS, 集成感知与通信, 脉冲信号, 空间调制, 时间调制", "comments": "这篇论文的创新点在于将STAR-RIS技术应用于ISAC系统，并巧妙地结合了空间调制和时间调制，以实现雷达和通信的协同工作。特别值得注意的是，其在嵌入通信信息的同时，能有效保持雷达性能，这对于未来无线系统的高效频谱利用具有重要意义。"}}
{"id": "2507.02530", "title": "Open-Source System for Multilingual Translation and Cloned Speech Synthesis", "authors": ["Mateo Cámara", "Juan Gutiérrez", "María Pilar Daza", "José Luis Blanco"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Presented at Forum Acusticum Euronoise 2025", "url": "http://arxiv.org/abs/2507.02530v1", "summary": "We present an open-source system designed for multilingual translation and\nspeech regeneration, addressing challenges in communication and accessibility\nacross diverse linguistic contexts. The system integrates Whisper for speech\nrecognition with Voice Activity Detection (VAD) to identify speaking intervals,\nfollowed by a pipeline of Large Language Models (LLMs). For multilingual\napplications, the first LLM segments speech into coherent, complete sentences,\nwhich a second LLM then translates. For speech regeneration, the system uses a\ntext-to-speech (TTS) module with voice cloning capabilities to replicate the\noriginal speaker's voice, maintaining naturalness and speaker identity.\n  The system's open-source components can operate locally or via APIs, offering\ncost-effective deployment across various use cases. These include real-time\nmultilingual translation in Zoom sessions, speech regeneration for public\nbroadcasts, and Bluetooth-enabled multilingual playback through personal\ndevices. By preserving the speaker's voice, the system ensures a seamless and\nimmersive experience, whether translating or regenerating speech.\n  This open-source project is shared with the community to foster innovation\nand accessibility. We provide a detailed system performance analysis, including\nlatency and word accuracy, demonstrating its potential to enable inclusive,\nadaptable communication solutions in real-world multilingual scenarios.", "comment": "Presented at Forum Acusticum Euronoise 2025", "pdf_url": "http://arxiv.org/pdf/2507.02530v1", "cate": "eess.AS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "多语言翻译和克隆语音合成的开源系统", "tldr": "一个开源系统，结合语音识别、LLM翻译和语音克隆，实现多语言翻译和语音再生，可用于实时交流和广播。", "motivation": "解决不同语言环境下沟通和可访问性面临的挑战。", "method": "系统整合了Whisper进行语音识别，并结合语音活动检测（VAD）识别说话间隔。接着，通过大语言模型（LLMs）管道：第一个LLM将语音分割成连贯完整的句子，第二个LLM进行翻译。对于语音再生，系统使用带有语音克隆能力的文本到语音（TTS）模块来复制原始说话者的声音。", "result": "系统组件可本地或通过API操作，提供经济高效的部署。应用包括Zoom会议实时多语言翻译、公共广播语音再生和蓝牙设备多语言播放。系统通过保留说话者声音，确保无缝沉浸式体验。提供了详细的系统性能分析，包括延迟和词准确率，展示了其在实际多语言场景中实现包容性、适应性通信解决方案的潜力。", "conclusion": "该开源项目致力于促进创新和可访问性，通过其在多语言翻译和语音再生方面的能力，展示了在真实世界场景中实现包容性和适应性通信解决方案的潜力。", "translation": "我们提出了一个专为多语言翻译和语音再生设计的开源系统，旨在解决不同语言环境下沟通和可访问性面临的挑战。该系统整合了Whisper进行语音识别，并结合语音活动检测（VAD）来识别说话间隔，随后是一个大语言模型（LLMs）管道。对于多语言应用，第一个LLM将语音分割成连贯、完整的句子，第二个LLM随后进行翻译。对于语音再生，系统使用带有语音克隆能力的文本到语音（TTS）模块来复制原始说话者的声音，保持自然度和说话者身份。\n该系统的开源组件可以本地运行或通过API调用，为各种用例提供经济高效的部署。这些用例包括Zoom会议中的实时多语言翻译、公共广播的语音再生，以及通过个人设备实现蓝牙多语言播放。通过保留说话者的声音，该系统无论是在翻译还是语音再生时，都能确保无缝和沉浸式的体验。\n这个开源项目与社区共享，旨在促进创新和可访问性。我们提供了详细的系统性能分析，包括延迟和词准确率，展示了其在真实世界多语言场景中实现包容性、适应性通信解决方案的潜力。", "summary": "本文介绍了一个开源系统，旨在解决多语言沟通和可访问性挑战。该系统结合了Whisper语音识别、VAD和大语言模型（LLMs）进行多语言翻译，并利用带有语音克隆功能的TTS模块进行语音再生，以保留说话者身份。系统支持本地和API部署，可应用于实时翻译、广播和个人设备播放。作者提供了性能分析，强调其在实际多语言场景中实现包容性通信的潜力。", "keywords": "多语言翻译, 语音合成, 语音克隆, 开源系统, 大语言模型", "comments": "该系统创新性地结合了成熟的语音识别（Whisper）、大语言模型和语音克隆技术，提供了一个全面的多语言交流解决方案。其开源性质和本地部署能力大大降低了使用门槛，有望促进相关领域的进一步发展和应用。特别值得注意的是，系统强调保留说话者声音以提供沉浸式体验，这是其重要优势。未来可关注其在更复杂噪声环境下的表现以及对小语种的支持。"}}
{"id": "2507.02367", "title": "A robust and versatile deep learning model for prediction of the arterial input function in dynamic small animal $\\left[^{18}\\text{F}\\right]$FDG PET imaging", "authors": ["Christian Salomonsen", "Luigi Tommaso Luppino", "Fredrik Aspheim", "Kristoffer Wickstrøm", "Elisabeth Wetzer", "Michael Kampffmeyer", "Rodrigo Berzaghi", "Rune Sundset", "Robert Jenssen", "Samuel Kuttner"], "categories": ["eess.IV", "cs.CV", "physics.med-ph", "q-bio.QM"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      22 pages, 12 figures", "url": "http://arxiv.org/abs/2507.02367v1", "summary": "Dynamic positron emission tomography (PET) and kinetic modeling are pivotal\nin advancing tracer development research in small animal studies. Accurate\nkinetic modeling requires precise input function estimation, traditionally\nachieved via arterial blood sampling. However, arterial cannulation in small\nanimals like mice, involves intricate, time-consuming, and terminal procedures,\nprecluding longitudinal studies. This work proposes a non-invasive, fully\nconvolutional deep learning-based approach (FC-DLIF) to predict input functions\ndirectly from PET imaging, potentially eliminating the need for blood sampling\nin dynamic small-animal PET. The proposed FC-DLIF model includes a spatial\nfeature extractor acting on the volumetric time frames of the PET sequence,\nextracting spatial features. These are subsequently further processed in a\ntemporal feature extractor that predicts the arterial input function. The\nproposed approach is trained and evaluated using images and arterial blood\ncurves from [$^{18}$F]FDG data using cross validation. Further, the model\napplicability is evaluated on imaging data and arterial blood curves collected\nusing two additional radiotracers ([$^{18}$F]FDOPA, and [$^{68}$Ga]PSMA). The\nmodel was further evaluated on data truncated and shifted in time, to simulate\nshorter, and shifted, PET scans. The proposed FC-DLIF model reliably predicts\nthe arterial input function with respect to mean squared error and correlation.\nFurthermore, the FC-DLIF model is able to predict the arterial input function\neven from truncated and shifted samples. The model fails to predict the AIF\nfrom samples collected using different radiotracers, as these are not\nrepresented in the training data. Our deep learning-based input function offers\na non-invasive and reliable alternative to arterial blood sampling, proving\nrobust and flexible to temporal shifts and different scan durations.", "comment": "22 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.02367v1", "cate": "eess.IV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "一种用于动态小动物[18F]FDG PET成像中动脉输入功能预测的鲁棒通用深度学习模型", "tldr": "提出了一种基于全卷积深度学习模型（FC-DLIF）的非侵入式方法，用于直接从PET图像预测动脉输入功能，从而避免了小动物PET成像中耗时的动脉采血。", "motivation": "在小动物研究中，精确的动力学建模需要准确的输入功能估计，传统上通过动脉采血实现。然而，小动物（如小鼠）的动脉插管复杂、耗时且具有终端性，无法进行纵向研究。", "method": "提出了一种非侵入性的全卷积深度学习方法（FC-DLIF），直接从PET图像预测输入功能。该模型包括一个作用于PET序列体积时间帧的空间特征提取器，用于提取空间特征，随后由一个时间特征提取器进一步处理以预测动脉输入功能。该方法使用[18F]FDG数据进行训练和评估，并使用交叉验证。模型还在使用另外两种放射性示踪剂（[18F]FDOPA和[68Ga]PSMA）收集的成像数据和动脉血曲线进行了评估，并进一步在截断和时间偏移的数据上进行了评估，以模拟更短和偏移的PET扫描。", "result": "FC-DLIF模型在均方误差和相关性方面能够可靠地预测动脉输入功能。此外，即使从截断和时间偏移的样本中，FC-DLIF模型也能够预测动脉输入功能。然而，该模型未能预测使用不同放射性示踪剂（因为这些示踪剂未在训练数据中表示）收集的样本的AIF。", "conclusion": "我们基于深度学习的输入功能提供了一种非侵入性和可靠的替代动脉采血的方法，证明了其对于时间偏移和不同扫描持续时间的鲁棒性和灵活性。", "translation": "动态正电子发射断层扫描（PET）和动力学建模在小动物研究中推进示踪剂开发研究方面至关重要。精确的动力学建模需要准确的输入功能估计，传统上通过动脉采血实现。然而，小动物（如小鼠）的动脉插管复杂、耗时且具有终端性，无法进行纵向研究。本工作提出了一种非侵入性的全卷积深度学习方法（FC-DLIF），用于直接从PET图像预测输入功能，可能消除动态小动物PET中对采血的需求。所提出的FC-DLIF模型包括一个作用于PET序列体积时间帧的空间特征提取器，用于提取空间特征。这些特征随后在时间特征提取器中进一步处理，以预测动脉输入功能。所提出的方法使用[18F]FDG数据以及交叉验证进行训练和评估。此外，该模型在使用另外两种放射性示踪剂（[18F]FDOPA和[68Ga]PSMA）收集的成像数据和动脉血曲线进行了适用性评估。该模型还在截断和时间偏移的数据上进行了进一步评估，以模拟更短和偏移的PET扫描。所提出的FC-DLIF模型在均方误差和相关性方面能够可靠地预测动脉输入功能。此外，FC-DLIF模型即使从截断和时间偏移的样本中也能够预测动脉输入功能。该模型未能预测使用不同放射性示踪剂（因为这些示踪剂未在训练数据中表示）收集的AIF。我们基于深度学习的输入功能提供了一种非侵入性和可靠的替代动脉采血的方法，证明了其对于时间偏移和不同扫描持续时间的鲁棒性和灵活性。", "summary": "该论文提出了一种名为FC-DLIF的非侵入性全卷积深度学习模型，旨在直接从动态小动物PET图像中预测动脉输入功能（AIF），从而避免了传统上复杂且具有终端性的动脉采血过程。该模型包含空间和时间特征提取器，并使用[18F]FDG数据进行训练和评估。研究结果表明，FC-DLIF模型能可靠预测AIF，并对扫描持续时间和时间偏移表现出鲁棒性。然而，该模型无法预测训练数据中未包含的不同放射性示踪剂的AIF。这项工作为小动物PET研究提供了一种可靠且灵活的非侵入性AIF估计方法。", "keywords": "深度学习, PET成像, 动脉输入功能, 小动物研究, 非侵入性", "comments": "该论文的创新之处在于提出了一种基于深度学习的非侵入性方法来预测PET成像中的动脉输入功能，这显著简化了小动物研究中的动力学建模过程，并使得纵向研究成为可能。其重要性在于克服了传统动脉采血的复杂性和侵入性限制。然而，一个明显的局限性是模型对训练数据中未包含的新放射性示示踪剂的预测能力不足，这表明模型泛化能力有待提高，或者需要针对特定示踪剂进行重新训练。"}}
{"id": "2507.02477", "title": "Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk", "authors": ["Gaochao Song", "Zibo Zhao", "Haohan Weng", "Jingbo Zeng", "Rongfei Jia", "Shenghua Gao"], "categories": ["cs.CV", "cs.GR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages main text, 14 pages appendix, 23 figures", "url": "http://arxiv.org/abs/2507.02477v1", "summary": "We introduce Mesh Silksong, a compact and efficient mesh representation\ntailored to generate the polygon mesh in an auto-regressive manner akin to silk\nweaving. Existing mesh tokenization methods always produce token sequences with\nrepeated vertex tokens, wasting the network capability. Therefore, our approach\ntokenizes mesh vertices by accessing each mesh vertice only once, reduces the\ntoken sequence's redundancy by 50\\%, and achieves a state-of-the-art\ncompression rate of approximately 22\\%. Furthermore, Mesh Silksong produces\npolygon meshes with superior geometric properties, including manifold topology,\nwatertight detection, and consistent face normals, which are critical for\npractical applications. Experimental results demonstrate the effectiveness of\nour approach, showcasing not only intricate mesh generation but also\nsignificantly improved geometric integrity.", "comment": "9 pages main text, 14 pages appendix, 23 figures", "pdf_url": "http://arxiv.org/pdf/2507.02477v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Mesh Silksong：如同织丝般的自回归网格生成", "tldr": "Mesh Silksong是一种新的网格表示方法，它以自回归方式生成网格，显著减少了标记冗余，提高了压缩率和几何属性。", "motivation": "现有网格标记化方法会产生重复的顶点标记，浪费网络能力，导致效率低下。", "method": "引入了Mesh Silksong，这是一种紧凑高效的网格表示方法。它通过仅访问每个网格顶点一次来标记网格顶点，将标记序列的冗余度降低了50%，并实现了高压缩率。此外，Mesh Silksong生成的复合网格具有卓越的几何特性，包括流形拓扑、水密性检测和一致的面法线。", "result": "实现了约22%的最新压缩率。生成的多边形网格具有卓越的几何属性（包括流形拓扑、水密性检测和一致的面法线）。实验结果表明其能够生成复杂的网格并显著改善几何完整性。", "conclusion": "Mesh Silksong通过提供一种更高效、几何上更稳健的自回归网格生成方法，有效解决了现有网格标记化方法的局限性。", "translation": "我们引入了Mesh Silksong，这是一种紧凑高效的网格表示方法，旨在以类似织丝的方式自回归地生成多边形网格。现有的网格标记化方法总是产生重复顶点标记的标记序列，浪费了网络能力。因此，我们的方法通过仅访问每个网格顶点一次来标记网格顶点，将标记序列的冗余度降低了50%，并实现了约22%的最新压缩率。此外，Mesh Silksong生成的复合网格具有卓越的几何特性，包括流形拓扑、水密性检测和一致的面法线，这些对于实际应用至关重要。实验结果证明了我们方法的有效性，不仅展示了复杂的网格生成，而且显著改善了几何完整性。", "summary": "Mesh Silksong是一种新的紧凑高效的自回归网格生成方法。它通过仅访问每个顶点一次来标记网格，从而将标记序列的冗余度降低了50%，并实现了约22%的最新压缩率。该方法生成的网格具有卓越的几何特性，如流形拓扑和水密性，在生成复杂网格和提高几何完整性方面表现出色。", "keywords": "网格生成, 自回归, 网格标记化, 压缩, 几何属性", "comments": "Mesh Silksong的创新之处在于其独特的单次顶点访问标记化策略，显著降低了数据冗余并提高了压缩效率。其在生成具有优越几何属性的网格方面的能力，使其在实际应用中具有重要价值，尤其是在需要高几何完整性的领域。"}}
{"id": "2507.02337", "title": "ClustOpt: A Clustering-based Approach for Representing and Visualizing the Search Dynamics of Numerical Metaheuristic Optimization Algorithms", "authors": ["Gjorgjina Cenikj", "Gašper Petelin", "Tome Eftimov"], "categories": ["cs.NE", "cs.AI"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02337v1", "summary": "Understanding the behavior of numerical metaheuristic optimization algorithms\nis critical for advancing their development and application. Traditional\nvisualization techniques, such as convergence plots, trajectory mapping, and\nfitness landscape analysis, often fall short in illustrating the structural\ndynamics of the search process, especially in high-dimensional or complex\nsolution spaces. To address this, we propose a novel representation and\nvisualization methodology that clusters solution candidates explored by the\nalgorithm and tracks the evolution of cluster memberships across iterations,\noffering a dynamic and interpretable view of the search process. Additionally,\nwe introduce two metrics - algorithm stability and algorithm similarity- to\nquantify the consistency of search trajectories across runs of an individual\nalgorithm and the similarity between different algorithms, respectively. We\napply this methodology to a set of ten numerical metaheuristic algorithms,\nrevealing insights into their stability and comparative behaviors, thereby\nproviding a deeper understanding of their search dynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02337v1", "cate": "cs.NE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "ClustOpt：一种基于聚类的方法，用于表示和可视化数值元启发式优化算法的搜索动态", "tldr": "提出ClustOpt，一种基于聚类的新方法，用于可视化和理解数值元启发式优化算法的搜索动态，并引入稳定性与相似性指标。", "motivation": "理解数值元启发式优化算法的行为对其发展和应用至关重要。传统的收敛图、轨迹映射和适应度景观分析等可视化技术，在说明搜索过程的结构动态方面（尤其是在高维或复杂解空间中）往往力有不足。", "method": "本文提出了一种新颖的表示和可视化方法，该方法对算法探索的候选解进行聚类，并跟踪聚类成员随迭代的演变。此外，引入了算法稳定性和算法相似性两个指标，分别用于量化单个算法在多次运行中搜索轨迹的一致性以及不同算法之间的相似性。", "result": "将该方法应用于一组十种数值元启发式算法，揭示了它们稳定性及其比较行为的见解，从而更深入地理解了它们的搜索动态。", "conclusion": "所提出的方法通过揭示数值元启发式算法的稳定性及其比较行为，提供了对其搜索动态更深入的理解。", "translation": "理解数值元启发式优化算法的行为对于推动其发展和应用至关重要。传统的可视化技术，如收敛图、轨迹映射和适应度景观分析，在说明搜索过程的结构动态方面往往力有不足，尤其是在高维或复杂的解空间中。为了解决这个问题，我们提出了一种新颖的表示和可视化方法，该方法对算法探索的候选解进行聚类，并跟踪聚类成员随迭代的演变，从而提供对搜索过程的动态和可解释的视图。此外，我们引入了两个指标——算法稳定性和算法相似性——分别用于量化单个算法在多次运行中搜索轨迹的一致性以及不同算法之间的相似性。我们将这种方法应用于一组十种数值元启发式算法，揭示了它们稳定性及其比较行为的见解，从而更深入地理解了它们的搜索动态。", "summary": "本文提出了一种名为ClustOpt的新颖聚类方法，旨在解决传统可视化技术在展示数值元启发式优化算法搜索动态方面的不足。该方法通过聚类算法探索的候选解并跟踪迭代过程中聚类成员的变化，提供了一种动态且可解释的搜索过程视图。此外，论文还引入了算法稳定性和算法相似性两个新指标，分别用于量化算法运行轨迹的一致性和不同算法间的相似性。通过将该方法应用于十种元启发式算法，研究揭示了这些算法的稳定性及其比较行为，从而加深了对其搜索动态的理解。", "keywords": "聚类, 元启发式优化, 搜索动态, 可视化, 算法稳定性", "comments": "该论文提出了一种创新的基于聚类的方法ClustOpt，解决了传统可视化技术在理解高维复杂空间中元启发式算法搜索动态的局限性。其核心创新在于将聚类分析引入到搜索过程的可视化中，并引入了量化算法行为的新指标，这对于算法开发和应用具有重要意义。该方法有望为算法调试和改进提供更直观的工具。"}}
{"id": "2507.02014", "title": "ManifoldMind: Dynamic Hyperbolic Reasoning for Trustworthy Recommendations", "authors": ["Anoushka Harit", "Zhongtian Sun", "Suncica Hadzidedic"], "categories": ["cs.IR", "cs.AI", "cs.LG", "stat.ML"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02014v1", "summary": "We introduce ManifoldMind, a probabilistic geometric recommender system for\nexploratory reasoning over semantic hierarchies in hyperbolic space. Unlike\nprior methods with fixed curvature and rigid embeddings, ManifoldMind\nrepresents users, items, and tags as adaptive-curvature probabilistic spheres,\nenabling personalised uncertainty modeling and geometry-aware semantic\nexploration. A curvature-aware semantic kernel supports soft, multi-hop\ninference, allowing the model to explore diverse conceptual paths instead of\noverfitting to shallow or direct interactions. Experiments on four public\nbenchmarks show superior NDCG, calibration, and diversity compared to strong\nbaselines. ManifoldMind produces explicit reasoning traces, enabling\ntransparent, trustworthy, and exploration-driven recommendations in sparse or\nabstract domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02014v1", "cate": "cs.IR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "流形心智：用于可信推荐的动态双曲推理", "tldr": "ManifoldMind是一个新的概率几何推荐系统，通过自适应曲率的概率球体和曲率感知语义核在双曲空间中进行探索性推理，实现个性化不确定性建模和透明可信的推荐。", "motivation": "现有推荐系统方法通常使用固定曲率和刚性嵌入，可能导致对浅层或直接交互的过拟合，且缺乏个性化不确定性建模和透明度，尤其是在稀疏或抽象领域。", "method": "ManifoldMind是一个概率几何推荐系统，它在双曲空间中将用户、物品和标签表示为自适应曲率的概率球体，以实现个性化不确定性建模和几何感知的语义探索。该模型采用曲率感知的语义核支持软性、多跳推理，从而探索多样化的概念路径。", "result": "在四个公共基准测试中，ManifoldMind在NDCG、校准和多样性方面表现优于强大的基线模型。", "conclusion": "ManifoldMind能够生成明确的推理轨迹，从而在稀疏或抽象领域实现透明、可信和探索驱动的推荐。", "translation": "我们引入了ManifoldMind，一个概率几何推荐系统，用于在双曲空间中对语义层次结构进行探索性推理。与以往采用固定曲率和刚性嵌入的方法不同，ManifoldMind将用户、物品和标签表示为自适应曲率的概率球体，从而实现个性化不确定性建模和几何感知的语义探索。一个曲率感知的语义核支持软性、多跳推理，允许模型探索多样化的概念路径，而不是过拟合于浅层或直接交互。在四个公共基准测试上的实验表明，与强大的基线相比，ManifoldMind在NDCG、校准和多样性方面表现优越。ManifoldMind生成明确的推理轨迹，从而在稀疏或抽象领域实现透明、可信和探索驱动的推荐。", "summary": "ManifoldMind是一种新颖的概率几何推荐系统，它利用双曲空间中的自适应曲率概率球体来表示用户、物品和标签，以实现个性化不确定性建模和几何感知的语义探索。通过其曲率感知语义核，该系统支持多跳推理，避免了对直接交互的过拟合。实验证明，ManifoldMind在推荐质量、校准和多样性方面优于现有基线，并能提供透明的推理过程，适用于稀疏或抽象的推荐场景。", "keywords": "推荐系统, 双曲空间, 概率几何, 自适应曲率, 可信推荐", "comments": "ManifoldMind的创新点在于引入了自适应曲率的概率球体来表示实体，这使得模型能够更好地进行个性化不确定性建模和几何感知的语义探索，弥补了传统固定曲率嵌入的不足。其曲率感知语义核支持多跳推理，增强了模型的探索能力和鲁棒性。此外，提供明确的推理轨迹提升了推荐系统的可信度和透明度，这在实际应用中非常重要。"}}
{"id": "2507.02394", "title": "On the Adversarial Robustness of Online Importance Sampling", "authors": ["Yotam Kenneth-Mordoch", "Shay Sapir"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02394v1", "summary": "This paper studies the adversarial-robustness of importance-sampling (aka\nsensitivity sampling); a useful algorithmic technique that samples elements\nwith probabilities proportional to some measure of their importance. A\nstreaming or online algorithm is called adversarially-robust if it succeeds\nwith high probability on input streams that may change adaptively depending on\nprevious algorithm outputs. Unfortunately, the dependence between stream\nelements breaks the analysis of most randomized algorithms, and in particular\nthat of importance-sampling algorithms. Previously, Braverman et al. [NeurIPS\n2021] suggested that streaming algorithms based on importance-sampling may be\nadversarially-robust; however, they proved it only for well-behaved inputs.\n  We focus on the adversarial-robustness of online importance-sampling, a\nnatural variant where sampling decisions are irrevocable and made as data\narrives. Our main technical result shows that, given as input an adaptive\nstream of elements $x_1,\\ldots,x_T\\in \\mathbb{R}_+$, online importance-sampling\nmaintains a $(1\\pm\\epsilon)$-approximation of their sum while matching (up to\nlower order terms) the storage guarantees of the oblivious (non-adaptive) case.\nWe then apply this result to develop adversarially-robust online algorithms for\ntwo fundamental problems: hypergraph cut sparsification and $\\ell_p$ subspace\nembedding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02394v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "在线重要性采样的对抗鲁棒性研究", "tldr": "本文研究在线重要性采样在对抗性输入流下的鲁棒性，证明其能保持近似和存储效率，并应用于超图割稀疏化和$\\ell_p$子空间嵌入。", "motivation": "现有的随机算法（特别是重要性采样）在面对适应性变化的输入流时，其分析方法会失效。Braverman 等人曾提出重要性采样算法可能具有对抗鲁棒性，但仅限于“表现良好”的输入。本文旨在解决在线重要性采样在对抗性输入下的鲁棒性问题。", "method": "本文关注在线重要性采样的对抗鲁棒性，其中采样决策是不可撤销且随数据到达而做出。主要技术结果证明，在给定自适应元素流的情况下，在线重要性采样可以保持其和的$(1\\pm\\epsilon)$-近似，同时匹配（除去低阶项）非自适应情况下的存储保证。", "result": "证明了在线重要性采样在面对自适应输入流时，能够保持其元素和的$(1\\pm\\epsilon)$-近似，并且存储效率与非自适应情况相当。将此结果应用于超图割稀疏化和$\\ell_p$子空间嵌入这两个基本问题，开发了对抗鲁棒的在线算法。", "conclusion": "在线重要性采样在对抗性输入流下具有鲁棒性，能够有效处理自适应数据，并适用于解决重要的计算问题。", "translation": "这篇论文研究了重要性采样（又称敏感性采样）的对抗鲁棒性；这是一种有用的算法技术，它以与元素重要性某种度量成比例的概率对元素进行采样。如果流式或在线算法在输入流可能根据先前的算法输出自适应变化的情况下以高概率成功，则称其具有对抗鲁棒性。不幸的是，流元素之间的依赖关系破坏了大多数随机算法的分析，特别是重要性采样算法的分析。此前，Braverman 等人 [NeurIPS 2021] 曾提出基于重要性采样的流式算法可能具有对抗鲁棒性；然而，他们只对“表现良好”的输入证明了这一点。\n我们专注于在线重要性采样的对抗鲁棒性，这是一种自然的变体，其中采样决策是不可撤销的，并且在数据到达时做出。我们的主要技术结果表明，给定一个自适应元素流 $x_1,\\ldots,x_T\\in \\mathbb{R}_+$ 作为输入，在线重要性采样在保持其和的 $(1\\pm\\epsilon)$-近似的同时，匹配（除去低阶项）了非自适应（非对抗性）情况下的存储保证。然后，我们将此结果应用于开发两个基本问题的对抗鲁棒在线算法：超图割稀疏化和 $\\ell_p$ 子空间嵌入。", "summary": "本文研究了在线重要性采样在对抗性输入流下的鲁棒性。针对现有方法在自适应流下分析失效的问题，作者证明了在线重要性采样在处理自适应元素流时，能够保持其和的近似精度并维持与非自适应情况相当的存储效率。该结果进一步应用于开发了超图割稀疏化和$\\ell_p$子空间嵌入的对抗鲁棒在线算法。", "keywords": "对抗鲁棒性, 在线重要性采样, 流式算法, 自适应输入, 超图割稀疏化", "comments": "本文的创新之处在于证明了在线重要性采样在面对对抗性、自适应输入流时的鲁棒性，解决了传统随机算法在此类场景下分析失效的难题。这对于设计更可靠的流式算法具有重要意义，并成功应用于两个重要问题。"}}
{"id": "2507.02199", "title": "Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer", "authors": ["Wenquan Lu", "Yuechuan Yang", "Kyle Lee", "Yanshu Li", "Enqi Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02199v1", "summary": "Chain-of-thought (CoT) reasoning has enabled transformer-based language\nmodels to excel at complex mathematics and multi-step planning. However, in\nstandard decoder-only architectures, these reasoning steps are externalized in\nnatural language, improving interpretability at the cost of efficiency. To\ncapture reasoning that is not easily represented in words, many works have\nexplored recurrent architectures that aim to internalize reasoning in latent\nspace, potentially supporting latent CoT. In this paper, we investigate whether\nsuch reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer\nthat reuses layers at inference time without increasing parameter count. We\nexamine the model's internal behavior on arithmetic tasks using a suite of\nprobing techniques including the Logit Lens and Coda Lens. Our findings reveal\nlimited evidence of interpretable latent CoT by tracking rank trajectories of\nfinal and intermediate result tokens. Furthermore, we uncover significant\nprobing inconsistencies across recurrent blocks, where the interpretability of\nhidden states depends heavily on both the layer index and the decoding method.\nFinally, we empirically show that increasing recurrence depth yields only\nmarginal gains and falls well short of models that explicitly externalize\nreasoning steps. The code is available at\nhttps://github.com/wenquanlu/huginn-latent-cot.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02199v1", "cate": "cs.CL", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "潜在思维链？解码深度循环Transformer", "tldr": "本文研究深度循环Transformer（Huginn-3.5B）中是否存在潜在思维链推理，发现可解释的潜在思维链证据有限，探测结果不一致，且增加循环深度收益甚微，远不及显式思维链模型。", "motivation": "标准思维链（CoT）推理通过外部化推理步骤提高了可解释性但牺牲了效率。为了在不增加参数的情况下实现效率并内部化推理，许多研究探索了循环架构以支持潜在思维链。本文旨在探究这种潜在推理结构是否会出现在深度循环Transformer Huginn-3.5B中。", "method": "本文研究了深度循环Transformer Huginn-3.5B，该模型在推理时重用层而不增加参数。研究人员使用包括Logit Lens和Coda Lens在内的探测技术，通过跟踪最终和中间结果token的排名轨迹，检查模型在算术任务上的内部行为。", "result": "研究发现可解释的潜在思维链的证据有限。在循环块之间存在显著的探测不一致性，隐藏状态的可解释性严重依赖于层索引和解码方法。此外，增加循环深度仅带来微小的性能提升，并且远低于明确外部化推理步骤的模型。", "conclusion": "深度循环Transformer（如Huginn-3.5B）中可解释的潜在思维链结构出现的证据有限，并且增加循环深度并不能带来显著的性能提升，无法与显式思维链模型相媲美。", "translation": "思维链（CoT）推理使基于Transformer的语言模型在复杂数学和多步规划方面表现出色。然而，在标准的仅解码器架构中，这些推理步骤以自然语言形式外部化，提高了可解释性但牺牲了效率。为了捕获难以用语言表示的推理，许多工作探索了旨在将推理内部化到潜在空间的循环架构，这可能支持潜在思维链。在本文中，我们调查了这种推理结构是否出现在Huginn-3.5B中，这是一种深度循环Transformer，它在推理时重用层而无需增加参数数量。我们使用一系列探测技术，包括Logit Lens和Coda Lens，检查了模型在算术任务上的内部行为。我们的发现揭示了通过跟踪最终和中间结果token的排名轨迹，可解释的潜在CoT的证据有限。此外，我们发现在循环块之间存在显著的探测不一致性，其中隐藏状态的可解释性在很大程度上取决于层索引和解码方法。最后，我们经验性地表明，增加循环深度仅带来微小的收益，并且远不及明确外部化推理步骤的模型。代码可在https://github.com/wenquanlu/huginn-latent-cot 获取。", "summary": "本文探讨了深度循环Transformer Huginn-3.5B是否能生成潜在思维链（CoT）推理。通过在算术任务上使用Logit Lens和Coda Lens等探测技术，研究发现可解释的潜在CoT证据有限，且探测结果在循环块间存在显著不一致性。实验表明，增加循环深度带来的收益微乎其微，远不及显式CoT模型。", "keywords": "潜在思维链, 深度循环Transformer, Huginn-3.5B, 探测, 可解释性", "comments": "这篇论文深入探讨了深度循环Transformer内部潜在推理机制的关键问题，即它们能否在不外部化推理步骤的情况下实现类似思维链的复杂推理。研究结果表明，当前的深度循环架构可能难以有效地内部化可解释的推理步骤，这突显了显式思维链在处理复杂任务方面的持续优势。论文中使用的探测技术是一个亮点，但发现的不一致性也揭示了解释潜在表示的挑战。"}}
{"id": "2507.02273", "title": "Fx-Encoder++: Extracting Instrument-Wise Audio Effects Representations from Mixtures", "authors": ["Yen-Tung Yeh", "Junghyun Koo", "Marco A. Martínez-Ramírez", "Wei-Hsiang Liao", "Yi-Hsuan Yang", "Yuki Mitsufuji"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      ISMIR 2025", "url": "http://arxiv.org/abs/2507.02273v1", "summary": "General-purpose audio representations have proven effective across diverse\nmusic information retrieval applications, yet their utility in intelligent\nmusic production remains limited by insufficient understanding of audio effects\n(Fx). Although previous approaches have emphasized audio effects analysis at\nthe mixture level, this focus falls short for tasks demanding instrument-wise\naudio effects understanding, such as automatic mixing. In this work, we present\nFx-Encoder++, a novel model designed to extract instrument-wise audio effects\nrepresentations from music mixtures. Our approach leverages a contrastive\nlearning framework and introduces an \"extractor\" mechanism that, when provided\nwith instrument queries (audio or text), transforms mixture-level audio effects\nembeddings into instrument-wise audio effects embeddings. We evaluated our\nmodel across retrieval and audio effects parameter matching tasks, testing its\nperformance across a diverse range of instruments. The results demonstrate that\nFx-Encoder++ outperforms previous approaches at mixture level and show a novel\nability to extract effects representation instrument-wise, addressing a\ncritical capability gap in intelligent music production systems.", "comment": "ISMIR 2025", "pdf_url": "http://arxiv.org/pdf/2507.02273v1", "cate": "cs.SD", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Fx-Encoder++: 从混合音轨中提取乐器级音频效果表示", "tldr": "Fx-Encoder++是一个新模型，它利用对比学习和“提取器”机制，能够从音乐混合中提取乐器级的音频效果表示，优于现有方法，并填补了智能音乐制作的能力空白。", "motivation": "现有通用音频表示在智能音乐制作中受限于对音频效果（Fx）理解不足。虽然之前的方法侧重于混合层面的音频效果分析，但这对于需要乐器级音频效果理解的任务（如自动混音）来说是不够的。", "method": "本文提出了Fx-Encoder++模型，该模型利用对比学习框架并引入“提取器”机制，在提供乐器查询（音频或文本）时，将混合层面的音频效果嵌入转换为乐器级音频效果嵌入。模型通过检索和音频效果参数匹配任务进行评估。", "result": "Fx-Encoder++在混合层面优于现有方法，并展示了提取乐器级效果表示的新能力。", "conclusion": "Fx-Encoder++通过提取乐器级音频效果表示，解决了智能音乐制作系统中的关键能力空白。", "translation": "通用音频表示已在多种音乐信息检索应用中证明有效，但它们在智能音乐制作中的效用仍受限于对音频效果（Fx）的理解不足。尽管之前的方法强调混合层面的音频效果分析，但这种关注对于需要乐器级音频效果理解的任务（例如自动混音）来说远远不够。在这项工作中，我们提出了Fx-Encoder++，一个旨在从音乐混合中提取乐器级音频效果表示的新颖模型。我们的方法利用对比学习框架，并引入了一种“提取器”机制，该机制在提供乐器查询（音频或文本）时，将混合层面的音频效果嵌入转换为乐器级音频效果嵌入。我们通过检索和音频效果参数匹配任务评估了我们的模型，并在各种乐器上测试了其性能。结果表明，Fx-Encoder++在混合层面优于现有方法，并展示了提取乐器级效果表示的新颖能力，解决了智能音乐制作系统中的关键能力空白。", "summary": "本文介绍了Fx-Encoder++，一个旨在从音乐混合中提取乐器级音频效果表示的新模型。该模型采用对比学习框架并引入了一个“提取器”机制，能够将混合层面的效果嵌入转换为乐器级效果嵌入。实验结果表明，Fx-Encoder++在混合层面表现优于现有方法，并成功实现了乐器级效果表示的提取，填补了智能音乐制作领域的一个重要空白。", "keywords": "音频效果, 乐器级表示, 对比学习, 音乐制作, Fx-Encoder++", "comments": "Fx-Encoder++的创新之处在于其能够从混合音轨中提取“乐器级”的音频效果表示，这对于自动混音等细粒度音乐制作任务至关重要。其引入的“提取器”机制和对比学习框架是核心技术点，有效解决了现有方法在混合层面分析的局限性。这项工作对智能音乐制作领域具有重要意义，因为它提供了更精细的音频效果理解能力。"}}
{"id": "2507.02855", "title": "Subtyping in DHOL -- Extended preprint", "authors": ["Colin Rothgang", "Florian Rabe"], "categories": ["cs.LO", "cs.AI", "cs.FL"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      16 pages main document, 44 pages of appendices, to be published in FroCoS 2025", "url": "http://arxiv.org/abs/2507.02855v1", "summary": "The recently introduced dependent typed higher-order logic (DHOL) offers an\ninteresting compromise between expressiveness and automation support. It\nsacrifices the decidability of its type system in order to significantly extend\nits expressiveness over standard HOL. Yet it retains strong automated theorem\nproving support via a sound and complete translation to HOL.\n  We leverage this design to extend DHOL with refinement and quotient types.\nBoth of these are commonly requested by practitioners but rarely provided by\nautomated theorem provers. This is because they inherently require undecidable\ntyping and thus are very difficult to retrofit to decidable type systems. But\nwith DHOL already doing the heavy lifting, adding them is not only possible but\nelegant and simple.\n  Concretely, we add refinement and quotient types as special cases of\nsubtyping. This turns the associated canonical inclusion resp. projection maps\ninto identity maps and thus avoids costly changes in representation. We present\nthe syntax, semantics, and translation to HOL for the extended language,\nincluding the proofs of soundness and completeness.", "comment": "16 pages main document, 44 pages of appendices, to be published in\n  FroCoS 2025", "pdf_url": "http://arxiv.org/pdf/2507.02855v1", "cate": "cs.LO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DHOL中的子类型——扩展预印本", "tldr": "本文通过将细化类型和商类型作为子类型的特殊情况添加到依赖类型高阶逻辑（DHOL）中，扩展了DHOL的表达能力，同时保留了其自动定理证明支持。", "motivation": "现有的自动定理证明器通常不提供实践者常用的细化类型和商类型，因为它们需要不可判定的类型系统，难以与可判定的系统兼容。依赖类型高阶逻辑（DHOL）本身就具有不可判定的类型系统，这使其成为添加这些特性而不增加复杂性的理想选择。", "method": "作者将细化类型和商类型作为子类型的特殊情况添加到DHOL中，将相关的规范包含或投影映射转换为恒等映射，从而避免了昂贵的表示更改。他们提出了扩展语言的语法、语义以及到HOL的翻译，并提供了健全性和完备性的证明。", "result": "成功地将细化类型和商类型集成到DHOL中，并为扩展后的语言提供了完整的语法、语义和到标准高阶逻辑（HOL）的翻译，同时证明了其健全性和完备性。", "conclusion": "通过将细化类型和商类型作为子类型的特殊情况引入DHOL，可以优雅且简单地扩展其表达能力，同时保持强大的自动定理证明支持。", "translation": "最近引入的依赖类型高阶逻辑（DHOL）在表达能力和自动化支持之间提供了一个有趣的折衷。它牺牲了其类型系统的可判定性，以显著扩展其相对于标准高阶逻辑（HOL）的表达能力。然而，它通过对HOL的健全且完备的翻译保留了强大的自动化定理证明支持。\n我们利用这种设计来扩展DHOL，使其包含细化类型和商类型。这两种类型是实践者普遍要求的，但自动定理证明器很少提供。这是因为它们本质上需要不可判定的类型，因此很难改造到可判定的类型系统中。但由于DHOL已经完成了繁重的工作，添加它们不仅可能，而且优雅而简单。\n具体来说，我们将细化类型和商类型作为子类型的特殊情况添加。这使得相关的规范包含映射或投影映射变为恒等映射，从而避免了昂贵的表示更改。我们提出了扩展语言的语法、语义以及到HOL的翻译，包括健全性和完备性的证明。", "summary": "这篇论文介绍了如何通过将细化类型和商类型作为子类型的特殊情况，来扩展依赖类型高阶逻辑（DHOL）的表达能力。DHOL本身就具有不可判定的类型系统，这使得添加通常难以集成到可判定类型系统中的细化和商类型变得可能、优雅且简单。作者提供了扩展语言的语法、语义以及到标准高阶逻辑（HOL）的翻译，并证明了其健全性和完备性，同时避免了昂贵的表示更改。", "keywords": "依赖类型高阶逻辑, 子类型, 细化类型, 商类型, 自动定理证明", "comments": "这篇论文的创新点在于利用DHOL固有的不可判定类型系统，巧妙地集成了实践中常用的细化类型和商类型，而这在传统可判定类型系统中非常困难。通过将它们视为子类型的特殊情况，避免了复杂的表示转换，提供了一种优雅且实用的解决方案，增强了DHOL在自动定理证明领域的应用潜力。"}}
{"id": "2507.02132", "title": "Matrix Pencil-Based DoA Estimation for Hybrid Receivers in Snapshot-Limited Scenarios", "authors": ["Mona Mostafa", "Ramy H. Gohary", "Amr El-Keyi", "Yahia A. Eldemerdash Ahmed"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      This manuscript is currently under review for potential publication in IEEE Journal", "url": "http://arxiv.org/abs/2507.02132v1", "summary": "The goal of this paper is to estimate the directions of arrival (DoAs) for\nhybrid analog/digital (HAD) receivers when the number of snapshots is too small\nfor statistical averaging to be reliable. This goal is achieved in\nfully-digital receivers by employing the matrix pencil method (MPM).\nUnfortunately, the MPM cannot be directly applied in HAD receivers because of\nthe entanglement induced by the underlying analog combiners on the output\nsignals. Furthermore, these analog combiners project the received signal onto a\nlow-dimensional space, jeopardizing the reception of signals arriving from\nparticular DoA ranges. To circumvent these difficulties, we propose two\napproaches to enable the MPM to extract the DoAs in HAD receivers. The two\napproaches avoid severe attenuation induced by low-dimensional projection by\ncycling over an exhaustive set of analog combiners, collectively spanning the\nentire space. The first approach can be applied to both fully-connected (FC)\nand partially-connected (PC) HADs and relies on the availability of periodic,\npotentially unknown, signals to disentangle the output of the HAD receiver. The\nsecond approach applies to PC-HADs only, and eliminates contingency on periodic\nsignals by exploiting the underlying block diagonal structure. The superiority\nof the proposed approaches is demonstrated via numerical simulations and\ncomparisons with the Cram\\'er-Rao lower bound.", "comment": "This manuscript is currently under review for potential publication\n  in IEEE Journal", "pdf_url": "http://arxiv.org/pdf/2507.02132v1", "cate": "cs.IT", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "基于矩阵铅笔的快照受限场景下混合接收器DoA估计", "tldr": "本文提出了两种新的方法，使矩阵铅笔方法（MPM）能够在快照数量受限的混合模拟/数字（HAD）接收器中有效估计到达方向（DoA），克服了模拟组合器引起的信号纠缠和低维投影问题。", "motivation": "在快照数量过少导致统计平均不可靠的情况下，混合模拟/数字（HAD）接收器难以进行到达方向（DoA）估计。传统的矩阵铅笔方法（MPM）无法直接应用于HAD接收器，因为模拟组合器会引起输出信号的纠缠，并将接收信号投影到低维空间，从而影响特定DoA范围的信号接收。", "method": "本文提出了两种方法来使MPM在HAD接收器中提取DoA。这两种方法通过循环遍历一组穷尽的模拟组合器来避免低维投影引起的严重衰减，这些组合器共同覆盖了整个空间。第一种方法适用于全连接（FC）和部分连接（PC）HAD，依赖于周期性（可能未知）信号来解耦HAD接收器的输出。第二种方法仅适用于PC-HAD，通过利用底层的块对角结构消除了对周期性信号的依赖。", "result": "通过数值模拟和与Cramér-Rao下限的比较，证明了所提出方法的优越性。", "conclusion": "本文提出的两种方法成功地解决了快照受限场景下混合模拟/数字（HAD）接收器中到达方向（DoA）估计的挑战。通过克服模拟组合器引起的信号纠缠和低维投影问题，这些方法使得矩阵铅笔方法（MPM）能够有效应用于HAD系统，并在性能上展现出优越性。", "translation": "本文的目标是在快照数量过少导致统计平均不可靠的情况下，估计混合模拟/数字（HAD）接收器的到达方向（DoA）。在全数字接收器中，通过采用矩阵铅笔方法（MPM）可以实现这一目标。不幸的是，MPM无法直接应用于HAD接收器，因为底层模拟组合器对输出信号造成的纠缠。此外，这些模拟组合器将接收到的信号投影到低维空间，危及来自特定DoA范围的信号接收。为了规避这些困难，我们提出了两种方法，使MPM能够在HAD接收器中提取DoA。这两种方法通过循环遍历一组穷尽的模拟组合器来避免低维投影引起的严重衰减，这些组合器共同覆盖了整个空间。第一种方法可以应用于全连接（FC）和部分连接（PC）HAD，并依赖于周期性（可能未知）信号的可用性来解耦HAD接收器的输出。第二种方法仅适用于PC-HAD，通过利用底层的块对角结构消除了对周期性信号的依赖。通过数值模拟和与Cramér-Rao下限的比较，证明了所提出方法的优越性。", "summary": "本文针对快照数量受限的混合模拟/数字（HAD）接收器中的到达方向（DoA）估计问题，提出了一种基于矩阵铅笔方法（MPM）的新方案。鉴于HAD接收器中模拟组合器导致的信号纠缠和低维投影，MPM无法直接应用。为解决此问题，研究者提出了两种创新方法：第一种适用于全连接和部分连接HAD，通过利用周期性信号解耦输出；第二种专为部分连接HAD设计，通过利用块对角结构避免对周期信号的依赖。这两种方法均通过循环遍历模拟组合器来有效处理低维投影问题。数值模拟结果表明，所提出的方法在性能上优于现有技术，并接近Cramér-Rao下限。", "keywords": "矩阵铅笔, DoA估计, 混合接收器, 快照受限, 模拟组合器", "comments": "本文的创新之处在于提出了两种巧妙的方法，使得矩阵铅笔方法（MPM）这一强大的信号处理工具能够应用于更复杂的混合模拟/数字（HAD）接收器中，尤其是在快照数据稀缺的挑战性场景下。这克服了模拟组合器固有的信号纠缠和维度降低问题，对于实际的无线通信和雷达系统具有重要意义。特别是第二种方法，通过利用系统结构避免了对周期性信号的依赖，增强了方法的普适性。"}}
{"id": "2507.02309", "title": "Rethinking Broken Object Level Authorization Attacks Under Zero Trust Principle", "authors": ["Anbin Wu", "Zhiyong Feng", "Ruitao Feng"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02309v1", "summary": "RESTful APIs facilitate data exchange between applications, but they also\nexpose sensitive resources to potential exploitation. Broken Object Level\nAuthorization (BOLA) is the top vulnerability in the OWASP API Security Top 10,\nexemplifies a critical access control flaw where attackers manipulate API\nparameters to gain unauthorized access. To address this, we propose BOLAZ, a\ndefense framework grounded in zero trust principles. BOLAZ analyzes the data\nflow of resource IDs, pinpointing BOLA attack injection points and determining\nthe associated authorization intervals to prevent horizontal privilege\nescalation. Our approach leverages static taint tracking to categorize APIs\ninto producers and consumers based on how they handle resource IDs. By mapping\nthe propagation paths of resource IDs, BOLAZ captures the context in which\nthese IDs are produced and consumed, allowing for precise identification of\nauthorization boundaries. Unlike defense methods based on common authorization\nmodels, BOLAZ is the first authorization-guided method that adapts defense\nrules based on the system's best-practice authorization logic. We validate\nBOLAZ through empirical research on 10 GitHub projects. The results demonstrate\nBOLAZ's effectiveness in defending against vulnerabilities collected from CVE\nand discovering 35 new BOLA vulnerabilities in the wild, demonstrating its\npracticality in real-world deployments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02309v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "零信任原则下重新思考失效对象级授权攻击", "tldr": "提出BOLAZ，一个基于零信任原则的防御框架，通过分析资源ID的数据流和利用静态污点跟踪来防御和发现失效对象级授权（BOLA）攻击。", "motivation": "RESTful API容易受到失效对象级授权（BOLA）攻击，这是OWASP API安全十大漏洞之首，允许攻击者通过操纵API参数获得未经授权的访问，需要一种有效的防御机制来解决这一关键访问控制缺陷。", "method": "提出BOLAZ防御框架，该框架基于零信任原则。BOLAZ分析资源ID的数据流，以确定BOLA攻击的注入点和相关的授权区间，从而防止横向权限提升。它利用静态污点跟踪将API分为资源ID的生产者和消费者，根据它们处理资源ID的方式。通过映射资源ID的传播路径，BOLAZ捕获了这些ID的产生和消费上下文，从而能够精确识别授权边界。BOLAZ是首个根据系统最佳实践授权逻辑调整防御规则的授权引导方法。", "result": "通过对10个GitHub项目的实证研究验证了BOLAZ。结果表明，BOLAZ能有效防御从CVE收集的漏洞，并发现了35个新的野外BOLA漏洞，证明了其在实际部署中的实用性。", "conclusion": "BOLAZ框架提供了一种新颖且实用的方法来防御和发现失效对象级授权（BOLA）攻击，通过结合零信任原则和静态污点跟踪技术，显著提高了API的安全性。", "translation": "RESTful API促进了应用程序之间的数据交换，但同时也使敏感资源面临潜在的利用。失效对象级授权（BOLA）是OWASP API安全十大漏洞之首，它是一个关键的访问控制缺陷，攻击者可以通过操纵API参数获得未经授权的访问。为了解决这个问题，我们提出了BOLAZ，一个基于零信任原则的防御框架。BOLAZ分析资源ID的数据流，精确定位BOLA攻击注入点并确定相关的授权区间，以防止横向权限提升。我们的方法利用静态污点跟踪将API分为资源ID的生产者和消费者，根据它们处理资源ID的方式。通过映射资源ID的传播路径，BOLAZ捕获了这些ID的产生和消费上下文，从而能够精确识别授权边界。与基于常见授权模型的防御方法不同，BOLAZ是第一个根据系统最佳实践授权逻辑调整防御规则的授权引导方法。我们通过对10个GitHub项目的实证研究验证了BOLAZ。结果表明，BOLAZ在防御从CVE收集的漏洞和发现35个新的野外BOLA漏洞方面是有效的，证明了其在实际部署中的实用性。", "summary": "本文针对RESTful API中普遍存在的失效对象级授权（BOLA）漏洞，提出了一种名为BOLAZ的防御框架。BOLAZ基于零信任原则，通过分析资源ID的数据流和利用静态污点跟踪技术，精确识别BOLA攻击的注入点和授权边界，以防止横向权限提升。该框架通过对API进行生产者和消费者分类，并捕获资源ID的传播路径上下文，从而实现授权规则的自适应调整。实证研究表明，BOLAZ能有效防御已知漏洞并发现新的BOLA漏洞，展现了其在实际应用中的有效性和实用性。", "keywords": "失效对象级授权, 零信任, API安全, 静态污点跟踪, 权限提升", "comments": "这篇论文通过提出BOLAZ框架，为解决API安全领域中最严重的BOLA漏洞提供了一个创新性的解决方案。其新颖之处在于将零信任原则与静态污点跟踪相结合，并首次提出了授权引导的防御方法，能够根据系统的实际授权逻辑调整防御规则，而非依赖于通用模型。通过在真实GitHub项目上发现新漏洞，证明了其实用价值和对API安全领域的潜在重大贡献。"}}
{"id": "2507.01997", "title": "Towards a Playground to Democratize Experimentation and Benchmarking of AI Agents for Network Troubleshooting", "authors": ["Zhihao Wang", "Alessandro Cornacchia", "Franco Galante", "Carlo Centofanti", "Alessio Sacco", "Dingde Jiang"], "categories": ["cs.NI", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Accepted at ACM SIGCOMM 1st Workshop on Next-Generation Network Observability (NGNO)", "url": "http://arxiv.org/abs/2507.01997v1", "summary": "Recent research has demonstrated the effectiveness of Artificial Intelligence\n(AI), and more specifically, Large Language Models (LLMs), in supporting\nnetwork configuration synthesis and automating network diagnosis tasks, among\nothers. In this preliminary work, we restrict our focus to the application of\nAI agents to network troubleshooting and elaborate on the need for a\nstandardized, reproducible, and open benchmarking platform, where to build and\nevaluate AI agents with low operational effort.", "comment": "Accepted at ACM SIGCOMM 1st Workshop on Next-Generation Network\n  Observability (NGNO)", "pdf_url": "http://arxiv.org/pdf/2507.01997v1", "cate": "cs.NI", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "迈向一个用于网络故障排除AI代理实验和基准测试的民主化平台", "tldr": "本初步工作强调了为网络故障排除AI代理建立一个标准化、可复现、开放的实验和基准测试平台的需求。", "motivation": "现有的研究表明人工智能（特别是大型语言模型）在网络配置合成和自动化网络诊断任务中表现出有效性。然而，缺乏一个标准化、可复现、开放的平台来低成本地构建和评估用于网络故障排除的AI代理。", "method": "这项初步工作主要阐述了建立一个标准化、可复现、开放的基准测试平台的需求，以低操作成本构建和评估用于网络故障排除的AI代理。", "result": "本文阐明了为网络故障排除AI代理建立一个标准化、可复现和开放的基准测试平台的重要性。", "conclusion": "结论是迫切需要一个标准化、可复现和开放的基准测试平台，以促进网络故障排除AI代理的开发和评估。", "translation": "近期研究已证明人工智能（AI），特别是大型语言模型（LLMs），在支持网络配置合成和自动化网络诊断任务等方面的有效性。在这项初步工作中，我们将重点限制在AI代理在网络故障排除中的应用，并阐述了对一个标准化、可复现、开放的基准测试平台的需求，以便以低操作成本构建和评估AI代理。", "summary": "这项初步工作探讨了AI代理在网络故障排除中的应用，并强调了建立一个标准化、可复现、开放的平台的重要性，以便民主化地进行AI代理的实验和基准测试，从而降低操作成本。", "keywords": "AI agents, Network troubleshooting, Benchmarking platform, Large Language Models", "comments": "这篇论文的创新点在于提出了为网络故障排除AI代理建立一个民主化、标准化、可复现的实验和基准测试平台的必要性。其重要性在于指出了当前AI代理在网络故障排除领域发展面临的挑战，并为未来的研究方向提供了指导。"}}
{"id": "2507.02682", "title": "A wireless, inexpensive optical tracker for the CAVE", "authors": ["Ehud Sharlin", "Pablo Figueroa", "Mark Green", "Benjamin Watson"], "categories": ["cs.HC", "cs.ET"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02682v1", "summary": "CAVE displays offer many advantages over other virtual reality (VR) displays,\nincluding a large, unencumbering viewing space. Unfortunately, the typical\ntracking subsystems used with CAVE displays tether the user and lessen this\nadvantage. We have designed a simple, low-cost feet tracker that is wireless,\nleaving the user free to move. The tracker can be assembled for less than $200\nUS, and achieves an accuracy of 10 cm at a 20 Hz sampling rate. We have tested\nthe prototype with two applications: a visualization supporting close visual\ninspection, and a walkthrough of the campus. Although the tracking was\nconvincing, it was clear that the tracker's limitations make it less than ideal\nfor applications requiring precise visual inspection. However, the freedom of\nmotion allowed by the tracker was a compelling supplement to our campus\nwalkthrough, allowing users to stroll and look around corners.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02682v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "CAVE显示器的一种无线、廉价光学跟踪器", "tldr": "本文设计了一种无线、廉价的光学脚部跟踪器，用于CAVE虚拟现实系统，解决了传统跟踪系统束缚用户的问题。该跟踪器成本低于200美元，精度10厘米，采样率20赫兹，适用于自由移动的场景，但对于需要精确视觉检查的应用效果不佳。", "motivation": "CAVE显示器虽然提供了大型、无束缚的观看空间，但其典型的跟踪子系统通常会束缚用户，从而削弱了这一优势。因此，需要一种无线、低成本的跟踪解决方案。", "method": "研究人员设计了一个简单、低成本的无线光学脚部跟踪器。该跟踪器成本低于200美元，并在两个应用中进行了原型测试：一个支持近距离视觉检查的可视化应用和一个校园漫游应用。", "result": "该跟踪器成本低于200美元，实现了10厘米的精度和20赫兹的采样率。在校园漫游应用中，它允许用户自由移动并环顾四周，效果令人信服。然而，对于需要精确视觉检查的应用，该跟踪器的局限性使其不够理想。", "conclusion": "尽管该跟踪器对于需要自由移动的应用（如校园漫游）提供了令人信服的体验，但其局限性使其不适用于需要高精度视觉检查的应用。", "translation": "CAVE显示器相比其他虚拟现实（VR）显示器具有许多优势，包括宽敞、无束缚的观看空间。不幸的是，CAVE显示器通常使用的跟踪子系统会束缚用户，从而削弱了这一优势。我们设计了一种简单、低成本的脚部跟踪器，它是无线的，让用户可以自由移动。该跟踪器组装成本不到200美元，在20赫兹的采样率下可达到10厘米的精度。我们已经用两个应用程序测试了原型：一个支持近距离视觉检查的可视化应用程序，以及一个校园漫游。尽管跟踪效果令人信服，但很明显，该跟踪器的局限性使其不适合需要精确视觉检查的应用程序。然而，该跟踪器所允许的运动自由度是我们校园漫游的一个引人注目的补充，允许用户漫步和环顾角落。", "summary": "本文提出了一种用于CAVE虚拟现实系统的无线、低成本光学脚部跟踪器，旨在解决现有跟踪系统束缚用户的问题。该跟踪器成本低于200美元，提供10厘米的精度和20赫兹的采样率。通过在可视化和校园漫游应用中的测试，证明了其在提供自由移动方面的优势，尤其适用于需要用户自由探索的场景，但对于高精度要求的应用则存在局限性。", "keywords": "CAVE, 虚拟现实, 光学跟踪器, 无线, 低成本", "comments": "该论文的创新点在于提供了一种成本低廉且无线的CAVE跟踪解决方案，这对于降低VR系统的整体成本和提高用户体验（自由度）具有重要意义。虽然其精度有限，不适用于所有应用，但它为特定场景（如漫游）提供了可行的替代方案，并展示了在经济性和自由度之间取得平衡的潜力。"}}
{"id": "2507.02118", "title": "A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights", "authors": ["Cristina Martinez Montes", "Daniela Grassi", "Nicole Novielli", "Birgit Penzenstadle"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02118v1", "summary": "The study of well-being, stress and other human factors has traditionally\nrelied on self-report instruments to assess key variables. However, concerns\nabout potential biases in these instruments, even when thoroughly validated and\nstandardised, have driven growing interest in alternatives in combining these\nmeasures with more objective methods, such as physiological measures.\n  We aimed to (i) compare psychometric stress measures and biometric indicators\nand (ii) identify stress-related patterns in biometric data during software\nengineering tasks.\n  We conducted an experiment where participants completed a pre-survey, then\nprogrammed two tasks wearing biometric sensors, answered brief post-surveys for\neach, and finally went through a short exit interview.\n  Our results showed diverse outcomes; we found no stress in the psychometric\ninstruments. Participants in the interviews reported a mix of feeling no stress\nand experiencing time pressure. Finally, the biometrics showed a significant\ndifference only in EDA phasic peaks.\n  We conclude that our chosen way of inducing stress by imposing a stricter\ntime limit was insufficient. We offer methodological insights for future\nstudies working with stress, biometrics, and psychometric instruments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02118v1", "cate": "cs.SE", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "一种结合生物特征和自我报告工具监测编程压力的多模态方法：方法论见解", "tldr": "本研究旨在通过结合生物特征和自我报告工具来监测编程中的压力。结果显示，心理测量工具未显示压力，自我报告结果不一，生物特征数据仅在EDA峰值上显示出显著差异。研究得出结论，所选的压力诱导方法（严格的时间限制）不足，并提供了未来研究的方法论见解。", "motivation": "传统上，幸福感、压力和其他人类因素的研究主要依赖自我报告工具，但这存在潜在偏见的担忧。因此，研究人员越来越倾向于将这些测量方法与更客观的生理测量方法相结合。", "method": "研究进行了一项实验，参与者首先完成一份预调查，然后在佩戴生物传感器的情况下完成两项编程任务，每项任务后进行简短的后调查，最后进行一次简短的离职面谈。", "result": "心理测量工具未显示压力。参与者在访谈中报告了没有压力和经历时间压力两种情况。生物特征数据显示仅在EDA（皮肤电活动）的相变峰值上存在显著差异。", "conclusion": "研究得出结论，所选的通过施加更严格时间限制来诱导压力的方法是不足的。本研究为未来涉及压力、生物特征和心理测量工具的研究提供了方法论见解。", "translation": "关于幸福感、压力和其他人类因素的研究传统上依赖自我报告工具来评估关键变量。然而，即使经过彻底验证和标准化，这些工具中潜在偏见的担忧也促使人们越来越关注将这些测量方法与更客观的方法（如生理测量）相结合的替代方案。\n我们的目标是（i）比较心理测量压力指标和生物特征指标，以及（ii）在软件工程任务中识别生物特征数据中与压力相关的模式。\n我们进行了一项实验，参与者完成了一份预调查，然后佩戴生物传感器编程两项任务，每项任务后回答简短的后调查，最后进行了一次简短的离职面谈。\n我们的结果显示了多样化的结果；我们发现心理测量工具中没有压力。参与者在访谈中报告了没有压力和经历时间压力的混合感受。最后，生物特征数据显示仅在EDA（皮肤电活动）的相变峰值上存在显著差异。\n我们得出结论，我们选择的通过施加更严格的时间限制来诱导压力的方式是不充分的。我们为未来涉及压力、生物特征和心理测量工具的研究提供了方法论见解。", "summary": "本研究探讨了将生物特征数据与自我报告工具结合起来监测编程中压力的方法。实验中，参与者在编程任务中佩戴生物传感器并完成问卷和访谈。结果显示，心理测量工具未检测到压力，自我报告结果不一，而生物特征数据仅在EDA相变峰值上显示出显著差异。研究指出其诱导压力的方法不足，并为未来多模态压力研究提供了方法论建议。", "keywords": "编程压力, 生物特征, 自我报告, 多模态, 方法论见解", "comments": "该论文的创新点在于尝试结合生物特征和自我报告方法来更全面地理解编程中的压力。其重要性在于指出了单一或不恰当的压力诱导方法可能导致结果不明确，并强调了在多模态研究中方法论严谨性的重要性。论文坦诚地指出了自身实验中压力诱导不足的局限性，并为未来的研究提供了宝贵的经验。"}}
{"id": "2507.02190", "title": "cVLA: Towards Efficient Camera-Space VLAs", "authors": ["Max Argus", "Jelena Bratulic", "Houman Masnavi", "Maxim Velikanov", "Nick Heppert", "Abhinav Valada", "Thomas Brox"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      20 pages, 10 figures", "url": "http://arxiv.org/abs/2507.02190v1", "summary": "Vision-Language-Action (VLA) models offer a compelling framework for tackling\ncomplex robotic manipulation tasks, but they are often expensive to train. In\nthis paper, we propose a novel VLA approach that leverages the competitive\nperformance of Vision Language Models (VLMs) on 2D images to directly infer\nrobot end-effector poses in image frame coordinates. Unlike prior VLA models\nthat output low-level controls, our model predicts trajectory waypoints, making\nit both more efficient to train and robot embodiment agnostic. Despite its\nlightweight design, our next-token prediction architecture effectively learns\nmeaningful and executable robot trajectories. We further explore the\nunderutilized potential of incorporating depth images, inference-time\ntechniques such as decoding strategies, and demonstration-conditioned action\ngeneration. Our model is trained on a simulated dataset and exhibits strong\nsim-to-real transfer capabilities. We evaluate our approach using a combination\nof simulated and real data, demonstrating its effectiveness on a real robotic\nsystem.", "comment": "20 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.02190v1", "cate": "cs.RO", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "cVLA：迈向高效的相机空间VLA", "tldr": "提出一种高效的相机空间VLA模型cVLA，通过预测图像坐标中的末端执行器姿态和轨迹路点，实现高效训练和强大的sim-to-real迁移。", "motivation": "现有的视觉-语言-动作 (VLA) 模型在处理复杂的机器人操作任务时表现出色，但训练成本高昂。", "method": "本文提出了一种名为cVLA的新型VLA方法，该方法利用视觉语言模型（VLM）在2D图像上的性能，直接推断图像坐标中的机器人末端执行器姿态。与以往输出低级控制的VLA模型不同，cVLA预测轨迹路点，使其训练更高效且与机器人本体无关。模型采用轻量级的下一词元预测架构，并探索了深度图像、推理时技术（如解码策略）和示范条件动作生成的潜力。", "result": "模型在模拟数据集上进行训练，并展现出强大的模拟到真实世界的迁移能力。通过结合模拟和真实数据进行评估，证明了其在真实机器人系统上的有效性。", "conclusion": "cVLA提供了一种高效且有效的相机空间VLA方法，通过预测轨迹路点实现了训练效率和机器人本体无关性，并在真实机器人系统上展现了强大的性能和sim-to-real迁移能力。", "translation": "视觉-语言-动作 (VLA) 模型为解决复杂的机器人操作任务提供了一个引人注目的框架，但它们通常训练成本高昂。在本文中，我们提出了一种新颖的VLA方法，该方法利用视觉语言模型 (VLM) 在2D图像上的竞争性能，直接推断图像坐标中的机器人末端执行器姿态。与以往输出低级控制的VLA模型不同，我们的模型预测轨迹路点，使其训练更高效且与机器人本体无关。尽管其设计轻量化，但我们的下一词元预测架构有效地学习了有意义且可执行的机器人轨迹。我们进一步探索了结合深度图像、推理时技术（如解码策略）和示范条件动作生成的未充分利用的潜力。我们的模型在模拟数据集上进行训练，并展现出强大的模拟到真实世界的迁移能力。我们结合模拟和真实数据评估了我们的方法，证明了其在真实机器人系统上的有效性。", "summary": "本文提出了一种名为cVLA的新型视觉-语言-动作（VLA）模型，旨在解决现有VLA模型训练成本高昂的问题。cVLA利用视觉语言模型（VLM）的优势，直接在图像坐标中预测机器人末端执行器姿态和轨迹路点，从而实现更高效的训练和机器人本体无关性。该模型采用轻量级设计和下一词元预测架构，并探索了深度图像、推理时策略和示范条件动作生成。实验结果表明，cVLA在模拟数据上训练后，能有效迁移到真实机器人系统，并在实际操作中表现出强大的性能。", "keywords": "视觉-语言-动作模型, 机器人操作, 相机空间, 轨迹路点, Sim-to-Real", "comments": "这篇论文通过将VLA模型的输出从低级控制转变为图像空间中的末端执行器姿态和轨迹路点，显著提高了训练效率和模型通用性，使其与机器人本体无关。利用现有VLM的强大能力，并结合深度信息和高级推理策略，是其创新的亮点。强大的sim-to-real迁移能力也展示了其在实际应用中的潜力。"}}
{"id": "2507.02180", "title": "The Revolution Has Arrived: What the Current State of Large Language Models in Education Implies for the Future", "authors": ["Russell Beale"], "categories": ["cs.HC", "cs.CY", "H.5.0; K.3.1; K.3.2"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02180v1", "summary": "Large language Models have only been widely available since 2022 and yet in\nless than three years have had a significant impact on approaches to education\nand educational technology. Here we review the domains in which they have been\nused, and discuss a variety of use cases, their successes and failures. We then\nprogress to discussing how this is changing the dynamic for learners and\neducators, consider the main design challenges facing LLMs if they are to\nbecome truly helpful and effective as educational systems, and reflect on the\nlearning paradigms they support. We make clear that the new interaction\nparadigms they bring are significant and argue that this approach will become\nso ubiquitous it will become the default way in which we interact with\ntechnologies, and revolutionise what people expect from computer systems in\ngeneral. This leads us to present some specific and significant considerations\nfor the design of educational technology in the future that are likely to be\nneeded to ensure acceptance by the changing expectations of learners and users.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02180v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "革命已至：大型语言模型在教育领域的现状对未来的启示", "tldr": "本文回顾了大型语言模型（LLMs）在教育领域的应用、成功与失败，讨论了它们如何改变学习者和教育者的动态，面临的设计挑战，以及它们支持的学习范式，并提出了未来教育技术设计的关键考量。", "motivation": "大型语言模型自2022年广泛应用以来，已对教育和教育技术产生了显著影响。本文旨在回顾LLMs在教育领域的应用情况、分析其影响、探讨面临的挑战，并展望未来教育技术的设计方向。", "method": "本文通过回顾LLMs在教育领域的应用领域、讨论其用例（包括成功与失败）、分析其对学习者和教育者的动态改变、考量LLMs作为教育系统面临的主要设计挑战，并反思它们支持的学习范式来展开论述。", "result": "大型语言模型正在显著改变学习者和教育者的动态，带来新的互动范式，并有望成为我们与技术互动的主流方式，彻底改变人们对计算机系统的期望。这导致了未来教育技术设计中需要考虑的具体且重要的因素，以确保其被不断变化的学习者和用户所接受。", "conclusion": "大型语言模型带来的新互动范式具有重要意义，并将变得无处不在，成为人们与技术互动和对计算机系统期望的默认方式。因此，未来的教育技术设计需要特别考虑这些变化，以确保被不断变化的学习者和用户所接受。", "translation": "大型语言模型自2022年才广泛可用，然而在不到三年的时间里，它们已经对教育方法和教育技术产生了重大影响。本文回顾了它们已被使用的领域，并讨论了各种用例、它们的成功与失败。然后，我们进一步讨论了这如何改变学习者和教育者的动态，考量了LLMs若要真正成为有益且有效的教育系统所面临的主要设计挑战，并反思了它们支持的学习范式。我们明确指出，它们带来的新互动范式意义重大，并认为这种方法将变得如此普遍，以至于它将成为我们与技术互动的默认方式，并彻底改变人们对计算机系统的一般期望。这促使我们提出了未来教育技术设计的一些具体而重要的考量，这些考量可能需要确保其能被学习者和用户不断变化的期望所接受。", "summary": "本文探讨了大型语言模型（LLMs）自2022年以来在教育领域产生的深远影响。作者回顾了LLMs在教育中的应用案例、其成功与失败，并分析了它们如何改变学习者与教育者的互动模式。文章进一步讨论了LLMs作为教育系统面临的设计挑战以及它们所支持的学习范式，强调了LLMs带来的新互动范式将成为未来人机交互的默认方式，并对教育技术未来的发展提出了关键的设计考量。", "keywords": "大型语言模型, 教育技术, 学习范式, 人机交互, 未来教育", "comments": "本文深刻洞察了大型语言模型对教育领域的革命性影响，强调了其带来的新互动范式将成为未来技术交互的主流。其创新之处在于不仅回顾了现状，更前瞻性地提出了未来教育技术设计所需面对的关键考量，这对于教育技术开发者和政策制定者具有重要的指导意义。论文的价值在于其对LLMs在教育中普及性和变革性潜力的清晰论证。"}}
{"id": "2507.02598", "title": "AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models", "authors": ["Chenhao Xue", "Kezhi Li", "Jiaxing Zhang", "Yi Ren", "Zhengyuan Shi", "Chen Zhang", "Yibo Lin", "Lining Zhang", "Qiang Xu", "Guangyu Sun"], "categories": ["cs.AR", "cs.AI"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      8 pages, 12 figures", "url": "http://arxiv.org/abs/2507.02598v1", "summary": "Arithmetic circuits, such as adders and multipliers, are fundamental\ncomponents of digital systems, directly impacting the performance, power\nefficiency, and area footprint. However, optimizing these circuits remains\nchallenging due to the vast design space and complex physical constraints.\nWhile recent deep learning-based approaches have shown promise, they struggle\nto consistently explore high-potential design variants, limiting their\noptimization efficiency. To address this challenge, we propose AC-Refiner, a\nnovel arithmetic circuit optimization framework leveraging conditional\ndiffusion models. Our key insight is to reframe arithmetic circuit synthesis as\na conditional image generation task. By carefully conditioning the denoising\ndiffusion process on target quality-of-results (QoRs), AC-Refiner consistently\nproduces high-quality circuit designs. Furthermore, the explored designs are\nused to fine-tune the diffusion model, which focuses the exploration near the\nPareto frontier. Experimental results demonstrate that AC-Refiner generates\ndesigns with superior Pareto optimality, outperforming state-of-the-art\nbaselines. The performance gain is further validated by integrating AC-Refiner\ninto practical applications.", "comment": "8 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2507.02598v1", "cate": "cs.AR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "AC-Refiner：使用条件扩散模型进行高效算术电路优化", "tldr": "AC-Refiner提出了一种利用条件扩散模型进行算术电路优化，通过将电路合成重构为条件图像生成任务，并利用探索的设计微调模型，从而生成具有卓越帕累托最优性的电路设计。", "motivation": "算术电路是数字系统的基本组成部分，直接影响性能、功耗和面积。然而，由于庞大的设计空间和复杂的物理约束，优化这些电路仍然具有挑战性。现有的深度学习方法难以持续探索高潜力设计变体，限制了其优化效率。", "method": "我们提出了AC-Refiner，一种利用条件扩散模型的新型算术电路优化框架。其关键在于将算术电路合成重构为条件图像生成任务。通过精心调整去噪扩散过程以达到目标质量结果(QoRs)，AC-Refiner能够持续生成高质量的电路设计。此外，探索到的设计用于微调扩散模型，使探索集中在帕累托前沿附近。", "result": "实验结果表明，AC-Refiner生成的电路设计具有卓越的帕累托最优性，优于现有最先进的基线方法。将AC-Refiner整合到实际应用中进一步验证了其性能提升。", "conclusion": "AC-Refiner通过将算术电路优化重构为条件图像生成任务，并利用条件扩散模型和模型微调，成功解决了现有优化方法的局限性，实现了卓越的电路设计帕累托最优性，并具有实际应用价值。", "translation": "算术电路，如加法器和乘法器，是数字系统的基本组成部分，直接影响性能、功耗效率和面积占用。然而，由于庞大的设计空间和复杂的物理约束，优化这些电路仍然具有挑战性。尽管最近基于深度学习的方法已显示出前景，但它们难以持续探索高潜力的设计变体，限制了其优化效率。为了应对这一挑战，我们提出了AC-Refiner，一个利用条件扩散模型的新型算术电路优化框架。我们的关键洞察是将算术电路合成重构为条件图像生成任务。通过仔细调整去噪扩散过程以达到目标质量结果（QoRs），AC-Refiner持续生成高质量的电路设计。此外，探索到的设计用于微调扩散模型，这使得探索集中在帕累托前沿附近。实验结果表明，AC-Refiner生成的电路设计具有卓越的帕累托最优性，优于现有最先进的基线。将AC-Refiner整合到实际应用中进一步验证了性能提升。", "summary": "AC-Refiner是一种利用条件扩散模型进行算术电路优化的新框架。它将电路合成视为条件图像生成任务，并通过条件去噪扩散和模型微调来生成高质量的、帕累托最优的电路设计，从而解决了传统方法在探索效率上的局限性，并超越了现有基线。", "keywords": "算术电路优化, 条件扩散模型, 图像生成, 帕累托最优, 硬件设计", "comments": "AC-Refiner的创新点在于将复杂的算术电路优化问题巧妙地转化为条件图像生成任务，并利用了条件扩散模型的强大生成能力。通过对目标QoRs进行条件化以及利用探索的设计进行模型微调，它能够高效地在帕累托前沿附近进行探索，从而生成高质量的设计。这种将物理设计问题与深度生成模型结合的方法，为未来硬件优化提供了新的思路。"}}
{"id": "2507.02523", "title": "Source Detection in Hypergraph Epidemic Dynamics using a Higher-Order Dynamic Message Passing Algorithm", "authors": ["Qiao Ke", "Naoki Masuda", "Zhen Jin", "Chuang Liu", "Xiu-Xiu Zhan"], "categories": ["physics.soc-ph", "cs.SI"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02523v1", "summary": "Source detection is crucial for capturing the dynamics of real-world\ninfectious diseases and informing effective containment strategies. Most\nexisting approaches to source detection focus on conventional pairwise\nnetworks, whereas recent efforts on both mathematical modeling and analysis of\ncontact data suggest that higher-order (e.g., group) interactions among\nindividuals may both account for a large fraction of infection events and\nchange our understanding of how epidemic spreading proceeds in empirical\npopulations. In the present study, we propose a message-passing algorithm,\ncalled the HDMPN, for source detection for a stochastic susceptible-infectious\ndynamics on hypergraphs. By modulating the likelihood maximization method by\nthe fraction of infectious neighbors, HDMPN aims to capture the influence of\nhigher-order structures and do better than the conventional likelihood\nmaximization. We numerically show that, in most cases, HDMPN outperforms\nbenchmarks including the likelihood maximization method without modification.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02523v1", "cate": "physics.soc-ph", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "使用高阶动态消息传递算法检测超图流行病动力学中的源头", "tldr": "本文提出了一种名为HDMPN的消息传递算法，用于在超图上进行流行病源头检测，该算法通过考虑高阶交互作用，在大多数情况下优于现有基准方法。", "motivation": "源头检测对于理解真实世界传染病动态和制定有效遏制策略至关重要。现有方法主要关注成对网络，但最近的研究表明，高阶（例如，群体）交互作用可能解释了很大一部分感染事件，并改变了我们对流行病传播的理解。", "method": "本研究提出了一种名为HDMPN的消息传递算法，用于在超图上对随机易感-感染动力学进行源头检测。HDMPN通过根据感染邻居的比例调制似然最大化方法，旨在捕捉高阶结构的影响，并优于传统的似然最大化方法。", "result": "数值结果表明，在大多数情况下，HDMPN优于包括未经修改的似然最大化方法在内的基准方法。", "conclusion": "HDMPN算法通过考虑高阶交互作用，在超图上的流行病源头检测中表现出优异的性能，证明了其在实际应用中的潜力。", "translation": "源头检测对于捕捉现实世界传染病的动态并制定有效的遏制策略至关重要。大多数现有的源头检测方法侧重于传统的成对网络，而最近对接触数据的数学建模和分析表明，个体之间的高阶（例如，群体）交互作用可能解释了很大一部分感染事件，并改变了我们对流行病如何在经验人群中传播的理解。在本研究中，我们提出了一种名为HDMPN的消息传递算法，用于在超图上对随机易感-感染动力学进行源头检测。通过根据感染邻居的比例调制似然最大化方法，HDMPN旨在捕捉高阶结构的影响，并优于传统的似然最大化。我们通过数值方法表明，在大多数情况下，HDMPN优于包括未经修改的似然最大化方法在内的基准方法。", "summary": "本研究提出了一种名为HDMPN的新型消息传递算法，用于在超图上检测流行病源头。与传统只关注成对交互的方法不同，HDMPN通过调制似然最大化方法来捕捉高阶（群体）交互的影响。数值实验表明，HDMPN在大多数情况下都优于现有的基准方法，包括未修改的似然最大化方法，这表明其在更准确地理解和遏制流行病传播方面的潜力。", "keywords": "源头检测, 超图, 流行病动力学, 消息传递算法, 高阶交互", "comments": "该论文的创新之处在于将高阶交互引入到流行病源头检测中，这与传统的成对网络方法形成对比。考虑到现实世界中群体交互的重要性，这种方法对于更准确地建模和理解传染病传播具有重要意义。HDMPN算法通过修改似然最大化方法来整合高阶结构信息，并已被证明在性能上优于现有基准。"}}
{"id": "2507.02295", "title": "Flotilla: A scalable, modular and resilient federated learning framework for heterogeneous resources", "authors": ["Roopkatha Banerjee", "Prince Modi", "Jinal Vyas", "Chunduru Sri Abhijit", "Tejus Chandrashekar", "Harsha Varun Marisetty", "Manik Gupta", "Yogesh Simmhan"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02295v1", "summary": "With the recent improvements in mobile and edge computing and rising concerns\nof data privacy, Federated Learning(FL) has rapidly gained popularity as a\nprivacy-preserving, distributed machine learning methodology. Several FL\nframeworks have been built for testing novel FL strategies. However, most focus\non validating the learning aspects of FL through pseudo-distributed simulation\nbut not for deploying on real edge hardware in a distributed manner to\nmeaningfully evaluate the federated aspects from a systems perspective. Current\nframeworks are also inherently not designed to support asynchronous\naggregation, which is gaining popularity, and have limited resilience to client\nand server failures. We introduce Flotilla, a scalable and lightweight FL\nframework. It adopts a ``user-first'' modular design to help rapidly compose\nvarious synchronous and asynchronous FL strategies while being agnostic to the\nDNN architecture. It uses stateless clients and a server design that separates\nout the session state, which are periodically or incrementally checkpointed. We\ndemonstrate the modularity of Flotilla by evaluating five different FL\nstrategies for training five DNN models. We also evaluate the client and\nserver-side fault tolerance on 200+ clients, and showcase its ability to\nrapidly failover within seconds. Finally, we show that Flotilla's resource\nusage on Raspberry Pis and Nvidia Jetson edge accelerators are comparable to or\nbetter than three state-of-the-art FL frameworks, Flower, OpenFL and FedML. It\nalso scales significantly better compared to Flower for 1000+ clients. This\npositions Flotilla as a competitive candidate to build novel FL strategies on,\ncompare them uniformly, rapidly deploy them, and perform systems research and\noptimizations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02295v1", "cate": "cs.DC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Flotilla：一个可扩展、模块化且有弹性的异构资源联邦学习框架", "tldr": "Flotilla是一个可扩展、模块化且有弹性的联邦学习框架，解决了现有框架在真实边缘硬件部署、异步聚合支持和故障恢复方面的不足，并展示了其在资源使用和可伸缩性上的优势。", "motivation": "现有的联邦学习框架主要侧重于通过伪分布式模拟来验证学习方面，而不是在真实边缘硬件上部署以从系统角度评估联邦方面。此外，当前框架未设计支持异步聚合，且对客户端和服务器故障的弹性有限。", "method": "本文引入了Flotilla，一个可扩展且轻量级的联邦学习框架。它采用“用户优先”的模块化设计，以帮助快速组合各种同步和异步联邦学习策略，同时与DNN架构无关。它使用无状态客户端和分离会话状态的服务器设计，并周期性或增量地检查点。", "result": "Flotilla通过评估五种不同的联邦学习策略来训练五个DNN模型，展示了其模块化。它还在200多个客户端上评估了客户端和服务器端的容错能力，并在几秒钟内实现快速故障转移。Flotilla在Raspberry Pis和Nvidia Jetson边缘加速器上的资源使用与Flower、OpenFL和FedML等三种最先进的联邦学习框架相当或更优。与Flower相比，Flotilla在1000多个客户端时具有显著更好的扩展性。", "conclusion": "Flotilla是一个有竞争力的候选框架，可用于构建新颖的联邦学习策略、统一比较它们、快速部署它们以及进行系统研究和优化。", "translation": "随着移动和边缘计算的最新改进以及数据隐私问题的日益突出，联邦学习（FL）作为一种保护隐私的分布式机器学习方法迅速普及。已经构建了几个FL框架来测试新颖的FL策略。然而，大多数框架专注于通过伪分布式模拟验证FL的学习方面，而不是在分布式方式下部署到真实边缘硬件上，以从系统角度有意义地评估联邦方面。当前的框架本身也不是为了支持日益流行的异步聚合而设计的，并且对客户端和服务器故障的弹性有限。我们引入了Flotilla，一个可扩展且轻量级的FL框架。它采用“用户优先”的模块化设计，有助于快速组合各种同步和异步FL策略，同时与DNN架构无关。它使用无状态客户端和分离会话状态的服务器设计，并周期性或增量地进行检查点。我们通过评估五种不同的FL策略来训练五个DNN模型，展示了Flotilla的模块化。我们还在200多个客户端上评估了客户端和服务器端的容错能力，并展示了其在几秒钟内快速故障转移的能力。最后，我们展示了Flotilla在Raspberry Pis和Nvidia Jetson边缘加速器上的资源使用与三种最先进的FL框架（Flower、OpenFL和FedML）相当或更优。与Flower相比，它在1000多个客户端时扩展性显著更好。这使得Flotilla成为构建新颖FL策略、统一比较它们、快速部署它们以及进行系统研究和优化的有竞争力的候选者。", "summary": "Flotilla是一个为异构资源设计的可扩展、模块化和有弹性的联邦学习框架。它旨在解决现有FL框架在真实边缘硬件部署、异步聚合支持和故障容忍度方面的不足。Flotilla采用用户优先的模块化设计，支持同步和异步策略，并使用无状态客户端和分离会话状态的服务器。实验证明Flotilla具有良好的模块化、快速的故障恢复能力、优异的资源效率和在大量客户端下的可扩展性，使其成为进行联邦学习系统研究和部署的有力工具。", "keywords": "联邦学习, 可扩展性, 模块化, 弹性, 边缘计算", "comments": "Flotilla的创新点在于其“用户优先”的模块化设计，允许灵活地组合不同的FL策略，同时解决了现有框架在真实世界部署和系统级评估中的痛点。其无状态客户端和分离会话状态的服务器设计提高了系统的弹性和可扩展性，并通过实际硬件评估验证了其性能，这对于推动联邦学习从模拟走向实际部署具有重要意义。"}}
{"id": "2507.01963", "title": "A Midsummer Meme's Dream: Investigating Market Manipulations in the Meme Coin Ecosystem", "authors": ["Alberto Maria Mongardini", "Alessandro Mei"], "categories": ["q-fin.TR", "cs.CY", "q-fin.ST"], "primary_category": "Subjects:       Trading and Market Microstructure (q-fin.TR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01963v1", "summary": "From viral jokes to a billion-dollar phenomenon, meme coins have become one\nof the most popular segments in cryptocurrency markets. Unlike utility-focused\ncrypto assets like Bitcoin or Ethereum, meme coins derive value primarily from\ncommunity sentiment, making them vulnerable to manipulation. This study\npresents a cross-chain analysis of the meme coin ecosystem, examining 34,988\ntokens across Ethereum, BNB Smart Chain, Solana, and Base. We characterize the\ntokenomics of meme coins and track their growth in a three-month longitudinal\nanalysis. We discover that among high-return tokens (>100%), an alarming 82.6%\nshow evidence of extensive use of artificial growth strategies designed to\ncreate a misleading appearance of market interest. These include wash trading\nand a form of manipulation we define as Liquidity Pool-Based Price Inflation\n(LPI), where small strategic purchases trigger dramatic price increases. We\nalso find evidence of schemes designed to profit at the expense of investors,\nsuch as pump and dumps and rug pulls. In particular, most of the tokens\ninvolved had previously experienced wash trading or LPI, indicating how initial\nmanipulations often set the stage for later exploitation. These findings reveal\nthat manipulations are widespread among high-performing meme coins and suggest\nthat their dramatic gains are often likely driven by coordinated efforts rather\nthan natural market dynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01963v1", "cate": "q-fin.TR", "date": "2025-04-16", "updated": "2025-04-16", "AI": {"title_translation": "仲夏迷因梦：调查迷因币生态系统中的市场操纵", "tldr": "本研究对迷因币生态系统进行了跨链分析，发现高回报迷因币中普遍存在人为操纵策略，如洗盘交易和流动性池价格膨胀，这些操纵往往为后续的剥削（如拉高出货和地毯式骗局）铺平道路，表明其巨大收益常由协调行动而非自然市场动态驱动。", "motivation": "迷因币已成为加密货币市场中最受欢迎的细分市场之一，但其价值主要来源于社区情绪，使其容易受到操纵。本研究旨在调查迷因币生态系统中的市场操纵行为。", "method": "本研究对以太坊、BNB智能链、Solana和Base上34,988个迷因币进行了跨链分析。研究表征了迷因币的代币经济学，并进行了为期三个月的纵向分析，追踪其增长情况。", "result": "在高回报（>100%）的迷因币中，有82.6%显示出广泛使用人工增长策略的证据，旨在制造误导性的市场兴趣假象。这些策略包括洗盘交易和流动性池价格膨胀（LPI）。研究还发现了旨在牺牲投资者利益获利的计划，如拉高出货（pump and dumps）和地毯式骗局（rug pulls）。大多数涉及这些剥削的代币之前都经历过洗盘交易或LPI。", "conclusion": "操纵行为在高回报迷因币中普遍存在，其巨大的收益往往是由协调行动而非自然市场动态驱动的。初期的操纵（如洗盘交易或LPI）常常为后期的剥削（如拉高出货和地毯式骗局）奠定基础。", "translation": "从病毒式笑话到数十亿美元的现象，迷因币已成为加密货币市场中最受欢迎的细分领域之一。与比特币或以太坊等专注于实用性的加密资产不同，迷因币的价值主要来源于社区情绪，这使得它们容易受到操纵。本研究对迷因币生态系统进行了跨链分析，检查了以太坊、BNB智能链、Solana和Base上的34,988个代币。我们描述了迷因币的代币经济学，并在为期三个月的纵向分析中追踪了它们的增长。我们发现，在高回报（>100%）代币中，惊人的82.6%显示出广泛使用人工增长策略的证据，旨在制造误导性的市场兴趣假象。这些策略包括洗盘交易和我们定义为基于流动性池的价格膨胀（LPI）的一种操纵形式，其中小额战略性购买会引发戏剧性的价格上涨。我们还发现了旨在牺牲投资者利益获利的计划证据，例如拉高出货（pump and dumps）和地毯式骗局（rug pulls）。特别是，大多数涉事代币此前都经历过洗盘交易或LPI，这表明初期的操纵往往为后来的剥削奠定了基础。这些发现揭示了在高表现迷因币中操纵行为普遍存在，并表明它们的巨大收益往往可能是由协调努力而非自然市场动态驱动的。", "summary": "本研究对迷因币生态系统进行了大规模跨链分析，涵盖以太坊、BNB智能链、Solana和Base上的近3.5万个代币。研究发现，在高回报迷因币中，超过八成存在洗盘交易和流动性池价格膨胀等多种人为操纵策略，这些策略旨在制造虚假的市场兴趣。此外，这些初期操纵常为后续的拉高出货和地毯式骗局等剥削行为铺路。研究揭示了迷因币市场的普遍操纵现象，表明其显著收益并非自然市场驱动，而是协调行动的结果。", "keywords": "迷因币, 市场操纵, 加密货币, 洗盘交易, 流动性池价格膨胀", "comments": "本研究通过大规模跨链分析，首次系统性地揭示了迷因币市场中普遍存在的操纵行为，特别是提出了“流动性池价格膨胀（LPI）”这一新型操纵手段。其创新之处在于结合了链上数据分析和对新兴市场操纵手法的识别。研究对于投资者风险教育和监管机构制定政策具有重要意义，有助于揭示迷因币市场非理性繁荣的潜在风险。"}}
{"id": "2507.02083", "title": "Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab", "authors": ["Haonan Duan", "Stephen Zhewen Lu", "Caitlin Fiona Harrigan", "Nishkrit Desai", "Jiarui Lu", "Michał Koziarski", "Leonardo Cotta", "Chris J. Maddison"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02083v1", "summary": "Designing experiments and result interpretations are core scientific\ncompetencies, particularly in biology, where researchers perturb complex\nsystems to uncover the underlying systems. Recent efforts to evaluate the\nscientific capabilities of large language models (LLMs) fail to test these\ncompetencies because wet-lab experimentation is prohibitively expensive: in\nexpertise, time and equipment. We introduce SciGym, a first-in-class benchmark\nthat assesses LLMs' iterative experiment design and analysis abilities in\nopen-ended scientific discovery tasks. SciGym overcomes the challenge of\nwet-lab costs by running a dry lab of biological systems. These models, encoded\nin Systems Biology Markup Language, are efficient for generating simulated\ndata, making them ideal testbeds for experimentation on realistically complex\nsystems. We evaluated six frontier LLMs on 137 small systems, and released a\ntotal of 350 systems. Our evaluation shows that while more capable models\ndemonstrated superior performance, all models' performance declined\nsignificantly as system complexity increased, suggesting substantial room for\nimprovement in the scientific capabilities of LLM agents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02083v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "测量语言模型在系统生物学干实验室中的科学能力", "tldr": "引入SciGym基准，使用系统生物学干实验室评估大型语言模型在开放式科学发现任务中的实验设计和分析能力，发现LLM在复杂系统上的性能仍有显著提升空间。", "motivation": "现有评估大型语言模型（LLM）科学能力的方法未能充分测试其核心科学能力，如实验设计和结果解释，原因在于湿实验室实验的专业知识、时间与设备成本过高。", "method": "本文引入了SciGym，一个通过运行生物系统干实验室（利用系统生物学标记语言编码的模型生成模拟数据）来评估LLM在开放式科学发现任务中迭代实验设计和分析能力的基准。研究人员在137个小型系统上评估了六个前沿LLM，并发布了总计350个系统。", "result": "评估结果显示，尽管能力更强的模型表现出更优异的性能，但所有模型的性能都随着系统复杂性的增加而显著下降。", "conclusion": "研究表明，大型语言模型在科学能力方面仍有巨大的提升空间，尤其是在处理更复杂的系统时。", "translation": "设计实验和解释结果是核心科学能力，尤其是在生物学中，研究人员会扰动复杂的系统以揭示其底层机制。最近评估大型语言模型（LLM）科学能力的工作未能测试这些能力，因为湿实验室实验在专业知识、时间和设备方面成本过高。我们引入了SciGym，这是一个同类首个的基准，用于评估LLM在开放式科学发现任务中的迭代实验设计和分析能力。SciGym通过运行生物系统干实验室来克服湿实验室成本的挑战。这些模型以系统生物学标记语言编码，能够高效地生成模拟数据，使其成为在现实复杂系统上进行实验的理想测试平台。我们评估了六个前沿LLM在137个小型系统上的表现，并发布了总共350个系统。我们的评估表明，虽然能力更强的模型表现出卓越的性能，但所有模型的性能都随着系统复杂性的增加而显著下降，这表明LLM代理的科学能力仍有很大的改进空间。", "summary": "本文介绍了SciGym，一个新颖的基准，旨在评估大型语言模型（LLM）在开放式科学发现任务中的实验设计和分析能力。为规避传统湿实验室实验的高昂成本，SciGym采用系统生物学干实验室模拟生物系统并生成数据。研究人员评估了六个前沿LLM在不同复杂度的系统上的表现，发现尽管性能更强的模型表现较好，但所有模型在系统复杂度增加时性能均显著下降，这表明LLM的科学能力仍有待大幅提升。", "keywords": "语言模型, 科学能力, 系统生物学, 干实验室, SciGym", "comments": "SciGym通过构建系统生物学干实验室，为评估大型语言模型的科学发现能力提供了一个创新且经济高效的途径，有效克服了湿实验室的成本限制。该研究揭示了当前LLM在处理复杂科学系统时的局限性，明确指出未来在提升LLM科学能力方面存在巨大的改进空间，对该领域的研究具有重要的指导意义。"}}
{"id": "2507.02205", "title": "Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach", "authors": ["Elena Ryumina", "Maxim Markitantov", "Alexandr Axyonov", "Dmitry Ryumin", "Mikhail Dolgushin", "Alexey Karpov"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8", "url": "http://arxiv.org/abs/2507.02205v1", "summary": "Compound Expression Recognition (CER), a subfield of affective computing,\naims to detect complex emotional states formed by combinations of basic\nemotions. In this work, we present a novel zero-shot multimodal approach for\nCER that combines six heterogeneous modalities into a single pipeline: static\nand dynamic facial expressions, scene and label matching, scene context, audio,\nand text. Unlike previous approaches relying on task-specific training data,\nour approach uses zero-shot components, including Contrastive Language-Image\nPretraining (CLIP)-based label matching and Qwen-VL for semantic scene\nunderstanding. We further introduce a Multi-Head Probability Fusion (MHPF)\nmodule that dynamically weights modality-specific predictions, followed by a\nCompound Expressions (CE) transformation module that uses Pair-Wise Probability\nAggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods\nto produce interpretable compound emotion outputs. Evaluated under multi-corpus\ntraining, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%\non Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via\nzero-shot testing, which is comparable to the results of supervised approaches\ntrained on target data. This demonstrates the effectiveness of the proposed\napproach for capturing CE without domain adaptation. The source code is\npublicly available.", "comment": "8", "pdf_url": "http://arxiv.org/pdf/2507.02205v1", "cate": "cs.CV", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "RAS 团队在第 9 届 ABAW 竞赛中：多模态复合表情识别方法", "tldr": "一种用于复合表情识别的新型零样本多模态方法，其性能与监督方法相当，无需特定任务训练数据。", "motivation": "旨在检测由基本情绪组合形成的复杂情绪状态（复合表情识别），并且与以往依赖特定任务训练数据的方法不同，本方法无需此类数据。", "method": "提出了一种新颖的零样本多模态CER方法，该方法将六种异构模态（静态和动态面部表情、场景和标签匹配、场景上下文、音频和文本）整合到一个单一的流程中。它利用基于对比语言-图像预训练（CLIP）的标签匹配和Qwen-VL作为零样本组件，并引入了一个多头概率融合（MHPF）模块和一个复合表情（CE）转换模块，该模块使用成对概率聚合（PPA）和成对特征相似性聚合（PFSA）方法。", "result": "在多语料库训练下进行评估，该方法在零样本测试中在AffWild2上获得了46.95%的F1分数，在AFEW上获得了49.02%的F1分数，在C-EXPR-DB上获得了34.85%的F1分数，这些结果与在目标数据上训练的监督方法的结果相当。", "conclusion": "所提出的零样本多模态方法在无需领域适应的情况下，能够有效地捕获复合表情。", "translation": "复合表情识别（CER）是情感计算的一个子领域，旨在检测由基本情绪组合形成的复杂情绪状态。在这项工作中，我们提出了一种新颖的零样本多模态CER方法，将六种异构模态整合到一个单一的流程中：静态和动态面部表情、场景和标签匹配、场景上下文、音频和文本。与以往依赖特定任务训练数据的方法不同，我们的方法使用零样本组件，包括基于对比语言-图像预训练（CLIP）的标签匹配和用于语义场景理解的Qwen-VL。我们进一步引入了一个多头概率融合（MHPF）模块，该模块动态加权模态特定的预测，然后是一个复合表情（CE）转换模块，该模块使用成对概率聚合（PPA）和成对特征相似性聚合（PFSA）方法来生成可解释的复合情绪输出。在多语料库训练下进行评估，所提出的方法在零样本测试中在AffWild2上显示出46.95%的F1分数，在野外表演面部表情（AFEW）上显示出49.02%的F1分数，在C-EXPR-DB上显示出34.85%的F1分数，这与在目标数据上训练的监督方法的结果相当。这证明了所提出的方法在无需领域适应的情况下捕获CE的有效性。源代码已公开提供。", "summary": "本文介绍了一种新颖的零样本多模态方法，用于情感计算中的复合表情识别（CER）。该方法整合了六种不同的模态，并利用CLIP和Qwen-VL等零样本组件。它还引入了多头概率融合（MHPF）模块和复合表情（CE）转换模块，以生成可解释的输出。该方法在零样本测试中，在标准数据集（AffWild2、AFEW、C-EXPR-DB）上取得了与监督方法相当的F1分数，证明了其在无需领域适应情况下的有效性。", "keywords": "复合表情识别, 零样本, 多模态, 情感计算, CLIP", "comments": "该论文的创新之处在于其零样本多模态方法，显著减少了对特定任务训练数据的依赖，这是情感计算中常见的瓶颈。多种模态的整合以及新颖的融合和转换模块增强了其鲁棒性。在零样本设置下实现与监督方法相当的性能是一个显著的进步，突出了其在标记数据稀缺的实际应用中的潜力。"}}
{"id": "2507.02213", "title": "Beyond Interval MDPs: Tight and Efficient Abstractions of Stochastic Systems", "authors": ["Ibon Gracia", "Morteza Lahijanian"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02213v1", "summary": "This work addresses the general problem of control synthesis for\ncontinuous-space, discrete-time stochastic systems with probabilistic\nguarantees via finite abstractions. While established methods exist, they often\ntrade off accuracy for tractability. We propose a unified abstraction framework\nthat improves both the tightness of probabilistic guarantees and computational\nefficiency. First, we introduce multi-interval MDPs (MI-MDPs), a generalization\nof interval-valued MDPs (IMDPs), which allows multiple, possibly overlapping\nclusters of successor states. This results in tighter abstractions but with\nincreased computational complexity. To mitigate this, we further propose a\ngeneralized form of MDPs with set-valued transition probabilities (SMDPs),\nwhich model transitions as a fixed probability to a state cluster, followed by\na non-deterministic choice within the cluster, as a sound abstraction. We show\nthat control synthesis for MI-MDPs reduces to robust dynamic programming via\nlinear optimization, while SMDPs admit even more efficient synthesis algorithms\nthat avoid linear programming altogether. Theoretically, we prove that, given\nthe partitioning of the state and disturbance spaces, both MI-MDPs and SMDPs\nyield tighter probabilistic guarantees than IMDPs, and that SMDPs are tighter\nthan MI-MDPs. Extensive experiments across several benchmarks validate our\ntheoretical results and demonstrate that SMDPs achieve favorable trade-offs\namong tightness, memory usage, and computation time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02213v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "超越区间MDPs：随机系统的紧致高效抽象", "tldr": "提出MI-MDPs和SMDPs框架，用于连续空间随机系统控制合成，相比现有方法提供更紧致的概率保证和更高的计算效率。", "motivation": "现有方法在连续空间、离散时间随机系统控制合成中，为了可处理性牺牲了精度，需要一种能同时提升概率保证紧致性和计算效率的统一抽象框架。", "method": "引入多区间MDPs (MI-MDPs)，是区间值MDPs (IMDPs) 的泛化，允许后继状态的多个重叠簇。提出集合值转移概率MDPs (SMDPs)，将转移建模为固定概率到状态簇，然后簇内进行非确定性选择。MI-MDPs的控制合成可简化为通过线性优化的鲁棒动态规划，而SMDPs的合成算法更高效，避免了线性规划。", "result": "理论证明，给定状态和扰动空间的划分，MI-MDPs和SMDPs都比IMDPs提供更紧致的概率保证，且SMDPs比MI-MDPs更紧致。大量实验验证了理论结果，并表明SMDPs在紧致性、内存使用和计算时间之间实现了有利的权衡。", "conclusion": "MI-MDPs和SMDPs框架，特别是SMDPs，为连续空间随机系统的控制合成提供了比IMDPs更紧致和更高效的抽象方法，在实际应用中表现出优越的性能。", "translation": "这项工作通过有限抽象解决了连续空间、离散时间随机系统在概率保证下的控制合成的一般问题。虽然现有方法已经存在，但它们通常以牺牲精度来换取可处理性。我们提出了一个统一的抽象框架，该框架既提高了概率保证的紧致性，又提高了计算效率。首先，我们引入了多区间MDPs（MI-MDPs），这是区间值MDPs（IMDPs）的泛化，它允许后继状态的多个可能重叠的簇。这导致了更紧致的抽象，但计算复杂性增加。为了缓解这一点，我们进一步提出了一种广义形式的具有集合值转移概率的MDPs（SMDPs），它将转移建模为到状态簇的固定概率，然后簇内进行非确定性选择，作为一种可靠的抽象。我们证明了MI-MDPs的控制合成可以简化为通过线性优化的鲁棒动态规划，而SMDPs则允许更高效的合成算法，完全避免了线性规划。在理论上，我们证明，在给定状态和扰动空间划分的情况下，MI-MDPs和SMDPs都比IMDPs产生更紧致的概率保证，并且SMDPs比MI-MDPs更紧致。在多个基准测试上的大量实验验证了我们的理论结果，并表明SMDPs在紧致性、内存使用和计算时间之间实现了有利的权衡。", "summary": "本文提出了一个统一的抽象框架，用于连续空间、离散时间随机系统的控制合成。该框架引入了两种新的MDPs泛化形式：多区间MDPs (MI-MDPs) 和集合值转移概率MDPs (SMDPs)。MI-MDPs通过允许后继状态的多个重叠簇来提供更紧致的抽象。SMDPs进一步优化，通过将转移建模为到状态簇的固定概率和簇内非确定性选择，实现了更高的计算效率。理论和实验结果表明，MI-MDPs和SMDPs都比传统的区间值MDPs (IMDPs) 提供更紧致的概率保证，其中SMDPs在紧致性、内存和计算时间之间达到了最佳平衡。", "keywords": "随机系统, 控制合成, MDPs抽象, 概率保证, 集合值转移概率", "comments": "该论文通过引入MI-MDPs和SMDPs两种新颖的MDPs泛化形式，创新性地解决了随机系统控制合成中精度与效率的权衡问题。特别是SMDPs的设计，巧妙地结合了概率转移和非确定性选择，不仅在理论上证明了其优于IMDPs和MI-MDPs的紧致性，还在计算效率上实现了显著提升，避免了复杂的线性规划。这对于需要高精度和实时性控制的连续空间随机系统具有重要意义。"}}
{"id": "2507.02348", "title": "Joint Radiation Power, Antenna Position, and Beamforming Optimization for Pinching-Antenna Systems with Motion Power Consumption", "authors": ["Yiming Xu", "Dongfang Xu", "Xianghao Yu", "Shenghui Song", "Zhiguo Ding", "Robert Schober"], "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      13 pages", "url": "http://arxiv.org/abs/2507.02348v1", "summary": "Pinching-antenna systems (PASS) have been recently proposed to improve the\nperformance of wireless networks by reconfiguring both the large-scale and\nsmall-scale channel conditions. However, existing studies ignore the physical\nconstraints of antenna placement and assume fixed antenna radiation power. To\nfill this research gap, this paper investigates the design of PASS taking into\naccount the motion power consumption of pinching-antennas (PAs) and the impact\nof adjustable antenna radiation power. To that end, we minimize the average\npower consumption for a given quality-of-service (QoS) requirement, by jointly\noptimizing the antenna positions, antenna radiation power ratios, and transmit\nbeamforming. To the best of the authors' knowledge, this is the first work to\nconsider radiation power optimization in PASS, which provides an additional\ndegree of freedom (DoF) for system design. The cases with both continuous and\ndiscrete antenna placement are considered, where the main challenge lies in the\nfact that the antenna positions affect both the magnitude and phase of the\nchannel coefficients of PASS, making system optimization very challenging. To\ntackle the resulting unique obstacles, an alternating direction method of\nmultipliers (ADMM)-based framework is proposed to solve the problem for\ncontinuous antenna movement, while its discrete counterpart is formulated as a\nmixed integer nonlinear programming (MINLP) problem and solved by the block\ncoordinate descent (BCD) method. Simulation results validate the performance\nenhancement achieved by incorporating PA movement power assumption and\nadjustable radiation power into PASS design, while also demonstrating the\nefficiency of the proposed optimization framework. The benefits of PASS over\nconventional multiple-input multiple-output (MIMO) systems in mitigating the\nlarge-scale path loss and inter-user interference is also revealed.", "comment": "13 pages", "pdf_url": "http://arxiv.org/pdf/2507.02348v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "考虑运动功耗的夹持天线系统联合辐射功率、天线位置和波束成形优化", "tldr": "本文针对夹持天线系统（PASS），首次将天线运动功耗和可调辐射功率纳入优化设计，通过联合优化天线位置、辐射功率和波束成形来最小化功耗，并提出了ADMM和BCD优化框架。", "motivation": "现有夹持天线系统（PASS）研究忽略了天线放置的物理约束、固定天线辐射功率的假设以及夹持天线的运动功耗。本文旨在填补这些研究空白，提高系统性能。", "method": "本文通过联合优化天线位置、天线辐射功率比和发射波束成形来最小化给定服务质量（QoS）要求下的平均功耗。对于连续天线移动，提出了基于交替方向乘子法（ADMM）的框架；对于离散天线放置，将其表述为混合整数非线性规划（MINLP）问题，并使用块坐标下降（BCD）方法求解。这是首次在PASS中考虑辐射功率优化，增加了系统设计的自由度。", "result": "仿真结果验证了将PA运动功耗假设和可调辐射功率纳入PASS设计所实现的性能提升，并证明了所提出的优化框架的效率。研究还揭示了PASS在减轻大规模路径损耗和用户间干扰方面优于传统多输入多输出（MIMO）系统的优势。", "conclusion": "通过联合优化辐射功率、天线位置和波束成形，并首次考虑运动功耗，本文显著提升了夹持天线系统（PASS）的性能和效率，有效缓解了路径损耗和用户间干扰。", "translation": "夹持天线系统（PASS）最近被提出，通过重新配置大尺度和小尺度信道条件来提高无线网络的性能。然而，现有研究忽略了天线放置的物理约束，并假设天线辐射功率是固定的。为了填补这一研究空白，本文研究了考虑夹持天线（PA）的运动功耗和可调天线辐射功率影响的PASS设计。为此，我们通过联合优化天线位置、天线辐射功率比和发射波束成形，在给定服务质量（QoS）要求下最小化平均功耗。据作者所知，这是首次在PASS中考虑辐射功率优化，这为系统设计提供了额外的自由度（DoF）。本文考虑了连续和离散天线放置的情况，其中主要挑战在于天线位置影响PASS信道系数的幅度和相位，使得系统优化非常具有挑战性。为了解决由此产生的独特障碍，本文提出了一种基于交替方向乘子法（ADMM）的框架来解决连续天线移动的问题，而其离散对应物则被表述为混合整数非线性规划（MINLP）问题，并使用块坐标下降（BCD）方法求解。仿真结果验证了将PA运动功耗假设和可调辐射功率纳入PASS设计所实现的性能提升，同时也证明了所提出的优化框架的效率。本文还揭示了PASS在减轻大规模路径损耗和用户间干扰方面优于传统多输入多输出（MIMO）系统的优势。", "summary": "本文针对夹持天线系统（PASS），首次考虑了天线运动功耗和可调辐射功率，提出了一种新的优化框架。通过联合优化天线位置、辐射功率比和波束成形，旨在最小化系统平均功耗并满足QoS需求。为解决连续和离散天线放置问题，分别提出了基于ADMM和BCD的解决方案。仿真结果表明该方法能显著提升系统性能，并显示PASS在缓解路径损耗和用户间干扰方面优于MIMO系统。", "keywords": "夹持天线系统, 运动功耗, 辐射功率优化, 波束成形, ADMM", "comments": "本文的创新点在于首次将天线运动功耗和可调辐射功率纳入夹持天线系统（PASS）的优化设计中，引入了新的自由度。通过联合优化天线位置、辐射功率和波束成形，解决了现有研究的局限性。所提出的ADMM和BCD优化框架有效应对了连续和离散天线放置带来的复杂挑战，为未来可重构智能表面等相关领域的研究提供了有益的参考。"}}
{"id": "2507.02562", "title": "Multi-Utterance Speech Separation and Association Trained on Short Segments", "authors": ["Yuzhu Wang", "Archontis Politis", "Konstantinos Drossos", "Tuomas Virtanen"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      5 pages, accepted by WASPAA 2025", "url": "http://arxiv.org/abs/2507.02562v1", "summary": "Current deep neural network (DNN) based speech separation faces a fundamental\nchallenge -- while the models need to be trained on short segments due to\ncomputational constraints, real-world applications typically require processing\nsignificantly longer recordings with multiple utterances per speaker than seen\nduring training. In this paper, we investigate how existing approaches perform\nin this challenging scenario and propose a frequency-temporal recurrent neural\nnetwork (FTRNN) that effectively bridges this gap. Our FTRNN employs a\nfull-band module to model frequency dependencies within each time frame and a\nsub-band module that models temporal patterns in each frequency band. Despite\nbeing trained on short fixed-length segments of 10 s, our model demonstrates\nrobust separation when processing signals significantly longer than training\nsegments (21-121 s) and preserves speaker association across utterance gaps\nexceeding those seen during training. Unlike the conventional\nsegment-separation-stitch paradigm, our lightweight approach (0.9 M parameters)\nperforms inference on long audio without segmentation, eliminating segment\nboundary distortions while simplifying deployment. Experimental results\ndemonstrate the generalization ability of FTRNN for multi-utterance speech\nseparation and speaker association.", "comment": "5 pages, accepted by WASPAA 2025", "pdf_url": "http://arxiv.org/pdf/2507.02562v1", "cate": "eess.AS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "多话语语音分离与关联：基于短片段训练", "tldr": "本文提出了一种频率-时间循环神经网络（FTRNN），能够在训练于短片段的情况下，有效地对长音频进行多话语语音分离和说话人关联，避免了传统分段-分离-拼接方法的缺陷。", "motivation": "当前基于深度神经网络的语音分离模型在训练时受限于计算资源，只能使用短片段；然而，实际应用中需要处理包含多话语且远长于训练片段的录音，这导致了性能差距。", "method": "本文提出了一种频率-时间循环神经网络（FTRNN），该网络包含一个全频带模块用于建模每个时间帧内的频率依赖性，以及一个子频带模块用于建模每个频带内的时间模式。该模型轻量级（0.9 M参数），可以在不进行分段的情况下对长音频进行推理。", "result": "尽管在10秒的短片段上训练，FTRNN模型在处理远长于训练片段（21-121秒）的信号时仍能表现出鲁棒的分离能力，并能保持跨越训练期间未见的语音间隙的说话人关联。与传统方法不同，该方法在长音频上直接进行推理，消除了分段边界失真。", "conclusion": "实验结果表明，FTRNN在多话语语音分离和说话人关联方面具有良好的泛化能力。", "translation": "当前基于深度神经网络（DNN）的语音分离面临一个根本性挑战——尽管模型由于计算限制需要在短片段上进行训练，但实际应用通常需要处理比训练时看到的包含每个说话人多个话语的显著更长的录音。在本文中，我们研究了现有方法在这种挑战性场景下的表现，并提出了一种频率-时间循环神经网络（FTRNN），有效地弥合了这一差距。我们的FTRNN采用一个全频带模块来模拟每个时间帧内的频率依赖性，以及一个子频带模块来模拟每个频带内的时间模式。尽管在10秒的短固定长度片段上进行训练，我们的模型在处理显著长于训练片段（21-121秒）的信号时表现出鲁棒的分离能力，并能保持跨越训练期间未见的语音间隙的说话人关联。与传统的“分段-分离-拼接”范式不同，我们的轻量级方法（0.9 M参数）在不分段的情况下对长音频进行推理，消除了分段边界失真，同时简化了部署。实验结果证明了FTRNN在多话语语音分离和说话人关联方面的泛化能力。", "summary": "本文针对现有深度神经网络语音分离模型在短片段训练与长音频实际应用之间的差距，提出了一种轻量级的频率-时间循环神经网络（FTRNN）。该模型包含全频带和子频带模块，即使在短片段训练后，也能对远长于训练数据的多话语录音进行鲁棒的语音分离和说话人关联，避免了传统分段方法的边界失真问题，并简化了部署。", "keywords": "语音分离, 说话人关联, 循环神经网络, 长音频处理, FTRNN", "comments": "本文提出了一种创新的FTRNN架构，通过全频带和子频带模块协同工作，有效地解决了语音分离中训练与推理长度不匹配的挑战。其核心创新在于能够在短片段训练后，仍能对长音频进行鲁棒的语音分离和说话人关联，且无需传统的分段-拼接步骤，显著简化了部署并消除了边界伪影。模型的轻量级特性（0.9M参数）也增强了其实用性。该工作对于提升语音分离技术在实际复杂场景中的应用具有重要意义。"}}
{"id": "2507.02411", "title": "3D Heart Reconstruction from Sparse Pose-agnostic 2D Echocardiographic Slices", "authors": ["Zhurong Chen", "Jinhua Chen", "Wei Zhuo", "Wufeng Xue", "Dong Ni"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.02411v1", "summary": "Echocardiography (echo) plays an indispensable role in the clinical practice\nof heart diseases. However, ultrasound imaging typically provides only\ntwo-dimensional (2D) cross-sectional images from a few specific views, making\nit challenging to interpret and inaccurate for estimation of clinical\nparameters like the volume of left ventricle (LV). 3D ultrasound imaging\nprovides an alternative for 3D quantification, but is still limited by the low\nspatial and temporal resolution and the highly demanding manual delineation.\n  To address these challenges, we propose an innovative framework for\nreconstructing personalized 3D heart anatomy from 2D echo slices that are\nfrequently used in clinical practice. Specifically, a novel 3D reconstruction\npipeline is designed, which alternatively optimizes between the 3D pose\nestimation of these 2D slices and the 3D integration of these slices using an\nimplicit neural network, progressively transforming a prior 3D heart shape into\na personalized 3D heart model.\n  We validate the method with two datasets. When six planes are used, the\nreconstructed 3D heart can lead to a significant improvement for LV volume\nestimation over the bi-plane method (error in percent: 1.98\\% VS. 20.24\\%). In\naddition, the whole reconstruction framework makes even an important\nbreakthrough that can estimate RV volume from 2D echo slices (with an error of\n5.75\\% ). This study provides a new way for personalized 3D structure and\nfunction analysis from cardiac ultrasound and is of great potential in clinical\npractice.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.02411v1", "cate": "eess.IV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "稀疏姿态无关二维超声心动图切片的三维心脏重建", "tldr": "该研究提出了一种从稀疏的二维超声心动图切片重建个性化三维心脏模型的新框架，显著提高了左心室和右心室的体积估算准确性。", "motivation": "超声心动图在心脏疾病诊断中至关重要，但传统的二维超声难以准确解释和评估临床参数（如左心室容积），而三维超声则受限于分辨率低和手动描绘耗时。为解决这些挑战，本研究旨在从临床常用的二维超声切片重建个性化三维心脏解剖结构。", "method": "提出了一种新颖的三维重建流程，该流程交替优化二维切片的3D姿态估计和使用隐式神经网络进行的3D切片整合，逐步将先前的3D心脏形状转换为个性化的3D心脏模型。", "result": "使用六个平面时，重建的三维心脏模型在左心室容积估算方面相比双平面方法有显著改进（误差从20.24%降至1.98%）。此外，该框架还能从二维超声切片估算右心室容积（误差为5.75%）。", "conclusion": "本研究提供了一种从心脏超声进行个性化三维结构和功能分析的新方法，在临床实践中具有巨大潜力。", "translation": "超声心动图（echo）在心脏疾病的临床实践中发挥着不可或缺的作用。然而，超声成像通常只能从少数特定视图提供二维（2D）横截面图像，这使得解释困难且对临床参数（如左心室（LV）容积）的估算不准确。三维超声成像为3D量化提供了另一种选择，但仍受限于低空间和时间分辨率以及高度耗费人力的手动描绘。\n为了应对这些挑战，我们提出了一种创新的框架，用于从临床实践中常用的2D超声心动图切片重建个性化的3D心脏解剖结构。具体而言，我们设计了一种新颖的3D重建管道，该管道在这些2D切片的3D姿态估计和使用隐式神经网络对这些切片进行3D整合之间交替优化，逐步将先前的3D心脏形状转换为个性化的3D心脏模型。\n我们用两个数据集验证了该方法。当使用六个平面时，重建的3D心脏可以显著改善左心室容积估算，优于双平面方法（误差百分比：1.98% 对 20.24%）。此外，整个重建框架还取得了重要突破，可以从2D超声心动图切片估算右心室容积（误差为5.75%）。这项研究为心脏超声的个性化3D结构和功能分析提供了一种新方法，在临床实践中具有巨大潜力。", "summary": "本研究提出了一种从稀疏且姿态无关的二维超声心动图切片重建个性化三维心脏模型的新框架。该方法通过交替优化二维切片的3D姿态估计和基于隐式神经网络的3D整合来实现。实验结果表明，该方法在左心室容积估算上显著优于传统双平面方法（误差从20.24%降至1.98%），并且首次实现了从二维切片准确估算右心室容积（误差5.75%）。这项工作为心脏超声的个性化三维分析提供了新的途径，具有重要的临床应用前景。", "keywords": "三维心脏重建, 超声心动图, 隐式神经网络, 左心室容积, 右心室容积", "comments": "该论文提出了一种创新的方法，通过结合姿态估计和隐式神经网络，从临床常用的二维超声切片重建三维心脏模型，有效解决了传统二维超声在参数估算上的局限性以及三维超声的分辨率和手动描绘问题。其在左心室和右心室容积估算上的显著准确性提升，尤其是首次实现从二维数据估算右心室容积，显示出其在临床诊断和个性化治疗中的巨大潜力。"}}
{"id": "2507.02803", "title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars", "authors": ["Gent Serifi", "Marcel C. Bühler"], "categories": ["cs.CV", "cs.GR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.02803v1", "summary": "We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for\nhigh-quality animatable face avatars. Creating such detailed face avatars from\nvideos is a challenging problem and has numerous applications in augmented and\nvirtual reality. While tremendous successes have been achieved for static\nfaces, animatable avatars from monocular videos still fall in the uncanny\nvalley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face\nthrough a collection of 3D Gaussian primitives. 3DGS excels at rendering static\nfaces, but the state-of-the-art still struggles with nonlinear deformations,\ncomplex lighting effects, and fine details. While most related works focus on\npredicting better Gaussian parameters from expression codes, we rethink the 3D\nGaussian representation itself and how to make it more expressive. Our insights\nlead to a novel extension of 3D Gaussians to high-dimensional multivariate\nGaussians, dubbed 'HyperGaussians'. The higher dimensionality increases\nexpressivity through conditioning on a learnable local embedding. However,\nsplatting HyperGaussians is computationally expensive because it requires\ninverting a high-dimensional covariance matrix. We solve this by\nreparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.\nThis trick boosts the efficiency so that HyperGaussians can be seamlessly\nintegrated into existing models. To demonstrate this, we plug in HyperGaussians\ninto the state-of-the-art in fast monocular face avatars: FlashAvatar. Our\nevaluation on 19 subjects from 4 face datasets shows that HyperGaussians\noutperform 3DGS numerically and visually, particularly for high-frequency\ndetails like eyeglass frames, teeth, complex facial movements, and specular\nreflections.", "comment": "Project page: https://gserifi.github.io/HyperGaussians", "pdf_url": "http://arxiv.org/pdf/2507.02803v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "HyperGaussians：用于高保真可动画面部化身的高维高斯泼溅", "tldr": "本文引入了 HyperGaussians，一种 3D 高斯泼溅的新颖扩展，旨在创建高质量、可动画的面部化身。它通过将 3D 高斯扩展到高维并提出“逆协方差技巧”来提高效率，从而在单目视频中实现比现有方法更优越的性能，尤其是在高频细节方面。", "motivation": "从视频创建高质量的可动画面部化身是一个挑战，在 AR/VR 中有广泛应用。现有的 3D Gaussian Splatting (3DGS) 虽然在静态人脸渲染上表现出色，但在处理非线性变形、复杂光照效果和精细细节方面仍有不足，导致可动画化身常陷入“恐怖谷”效应。", "method": "本文提出了 HyperGaussians，将 3D 高斯扩展到高维多元高斯，通过条件化学习到的局部嵌入来增加表达能力。为解决高维协方差矩阵求逆的计算成本，引入了“逆协方差技巧”进行协方差矩阵重参数化，提高了效率。该方法被集成到 FlashAvatar 中进行演示。", "result": "在来自 4 个面部数据集的 19 名受试者上的评估表明，HyperGaussians 在数值和视觉上均优于 3DGS，尤其在处理眼镜框、牙齿、复杂面部动作和镜面反射等高频细节方面表现突出。", "conclusion": "HyperGaussians 提供了一种更具表现力且高效的表示方法，用于创建高保真可动画面部化身，克服了 3DGS 在动态和细节面部渲染方面的局限性。", "translation": "我们引入了 HyperGaussians，这是一种用于高质量可动画面部化身的三维高斯泼溅的新颖扩展。从视频创建如此详细的面部化身是一个具有挑战性的问题，在增强现实和虚拟现实中具有众多应用。虽然静态人脸已经取得了巨大成功，但从单目视频创建的可动画化身仍然陷入“恐怖谷”效应。事实上的标准，三维高斯泼溅（3DGS），通过一系列三维高斯基元来表示人脸。3DGS 在渲染静态人脸方面表现出色，但最先进的技术仍然难以处理非线性变形、复杂光照效果和精细细节。虽然大多数相关工作侧重于从表情代码预测更好的高斯参数，但我们重新思考了三维高斯表示本身以及如何使其更具表现力。我们的见解促成了一种将三维高斯扩展到高维多元高斯的新颖方法，被称为“HyperGaussians”。更高的维度通过条件化学习到的局部嵌入来增加表现力。然而，泼溅 HyperGaussians 计算成本很高，因为它需要反转高维协方差矩阵。我们通过重新参数化协方差矩阵来解决这个问题，这被称为“逆协方差技巧”。这个技巧提高了效率，因此 HyperGaussians 可以无缝集成到现有模型中。为了证明这一点，我们将 HyperGaussians 插入到快速单目面部化身的最先进模型 FlashAvatar 中。我们对来自 4 个面部数据集的 19 名受试者进行的评估表明，HyperGaussians 在数值和视觉上均优于 3DGS，尤其是在眼镜框、牙齿、复杂面部动作和镜面反射等高频细节方面。", "summary": "本文提出了 HyperGaussians，这是 3D 高斯泼溅 (3DGS) 的一种新颖扩展，旨在创建高质量的可动画面部化身。针对 3DGS 在处理非线性变形和精细细节方面的不足，HyperGaussians 将 3D 高斯扩展到高维多元高斯，通过学习局部嵌入来增强表现力。为解决高维协方差矩阵求逆的计算成本，引入了“逆协方差技巧”进行高效重参数化。实验结果表明，HyperGaussians 在数值和视觉上均优于 3DGS，尤其在捕捉高频细节和复杂面部运动方面表现出色。", "keywords": "高斯泼溅, 面部化身, 高维高斯, 可动画, 三维重建", "comments": "本文通过引入高维高斯表示（HyperGaussians）和创新的“逆协方差技巧”，显著提升了 3D Gaussian Splatting 在生成高保真可动画面部化身方面的能力。其创新点在于从根本上改进了高斯表示的表达能力，并解决了由此带来的计算效率问题。这对于虚拟现实、增强现实以及数字人领域具有重要意义，有助于克服现有方法在“恐怖谷”效应和细节表现上的局限。"}}
{"id": "2507.01998", "title": "Positive region preserved random sampling: an efficient feature selection method for massive data", "authors": ["Hexiang Bai", "Deyu Li", "Jiye Liang", "Yanhui Zhai"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01998v1", "summary": "Selecting relevant features is an important and necessary step for\nintelligent machines to maximize their chances of success. However, intelligent\nmachines generally have no enough computing resources when faced with huge\nvolume of data. This paper develops a new method based on sampling techniques\nand rough set theory to address the challenge of feature selection for massive\ndata. To this end, this paper proposes using the ratio of discernible object\npairs to all object pairs that should be distinguished to measure the\ndiscriminatory ability of a feature set. Based on this measure, a new feature\nselection method is proposed. This method constructs positive region preserved\nsamples from massive data to find a feature subset with high discriminatory\nability. Compared with other methods, the proposed method has two advantages.\nFirst, it is able to select a feature subset that can preserve the\ndiscriminatory ability of all the features of the target massive data set\nwithin an acceptable time on a personal computer. Second, the lower boundary of\nthe probability of the object pairs that can be discerned using the feature\nsubset selected in all object pairs that should be distinguished can be\nestimated before finding reducts. Furthermore, 11 data sets of different sizes\nwere used to validate the proposed method. The results show that approximate\nreducts can be found in a very short period of time, and the discriminatory\nability of the final reduct is larger than the estimated lower boundary.\nExperiments on four large-scale data sets also showed that an approximate\nreduct with high discriminatory ability can be obtained in reasonable time on a\npersonal computer.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01998v1", "cate": "cs.LG", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "正区域保留随机抽样：一种高效的大数据特征选择方法", "tldr": "本文提出了一种基于采样技术和粗糙集理论的特征选择新方法，旨在解决大数据下的特征选择挑战。该方法通过构建正区域保留样本来选择具有高判别能力的特征子集，并能在个人电脑上以可接受的时间内完成，同时保留了原始数据的判别能力。", "motivation": "智能机器在处理海量数据时，通常计算资源不足，但选择相关特征对于其成功至关重要。", "method": "本文提出了一种新的特征选择方法。该方法首先使用可区分对象对与所有应区分对象对的比例来衡量特征集的判别能力。在此基础上，该方法从海量数据中构建正区域保留样本，以找到具有高判别能力的特征子集。该方法能够在个人电脑上以可接受的时间内选择出能保留目标海量数据集所有特征判别能力的特征子集。此外，在找到约简之前，可以估计出使用所选特征子集在所有应区分对象对中可区分对象对的概率下限。", "result": "使用11个不同大小的数据集验证了所提出的方法。结果表明，可以在非常短的时间内找到近似约简，并且最终约简的判别能力大于估计的下限。在四个大规模数据集上的实验也表明，可以在个人电脑上以合理的时间获得具有高判别能力的近似约简。", "conclusion": "该方法能够在大数据环境下高效地进行特征选择，并在个人电脑上以合理的时间内找到具有高判别能力的特征子集，同时保留了原始数据的判别能力。", "translation": "选择相关特征是智能机器最大限度提高成功机会的重要且必要的步骤。然而，智能机器在面对海量数据时通常没有足够的计算资源。本文开发了一种基于采样技术和粗糙集理论的新方法，以解决海量数据的特征选择挑战。为此，本文提出使用可区分对象对与所有应区分对象对的比例来衡量特征集的判别能力。基于此度量，提出了一种新的特征选择方法。该方法从海量数据中构建正区域保留样本，以找到具有高判别能力的特征子集。与其他方法相比，所提出的方法具有两个优点。首先，它能够在个人电脑上以可接受的时间内选择一个能够保留目标海量数据集所有特征判别能力的特征子集。其次，在找到约简之前，可以估计出使用所选特征子集在所有应区分对象对中可区分对象对的概率下限。此外，使用11个不同大小的数据集验证了所提出的方法。结果表明，可以在非常短的时间内找到近似约简，并且最终约简的判别能力大于估计的下限。在四个大规模数据集上的实验也表明，可以在个人电脑上以合理的时间获得具有高判别能力的近似约简。", "summary": "本文针对海量数据特征选择的挑战，提出了一种基于采样技术和粗糙集理论的新方法——正区域保留随机抽样。该方法通过衡量特征集的判别能力，并从海量数据中构建正区域保留样本来选择具有高判别能力的特征子集。其优势在于能够在个人电脑上以可接受的时间内高效地选择特征子集，同时保留原始数据的判别能力，并且在找到约简前可以估计判别概率的下限。实验结果证实了该方法能快速找到近似约简，且其判别能力高于估计下限，在大规模数据集上亦表现出高效性。", "keywords": "特征选择, 随机抽样, 粗糙集理论, 大数据, 判别能力", "comments": "该论文提出了一种创新的特征选择方法，通过结合采样技术和粗糙集理论，有效地解决了大数据环境下计算资源受限的问题。其优点在于能够在个人电脑上实现高效的特征选择，并确保选定特征子集保留原始数据的判别能力，这对于实际应用具有重要意义。同时，能够预估判别能力下限也增加了方法的可靠性。"}}
{"id": "2507.02372", "title": "An Experimental Approach for Running-Time Estimation of Multi-objective Evolutionary Algorithms in Numerical Optimization", "authors": ["Han Huang", "Tianyu Wang", "Chaoda Peng", "Tongli He", "Zhifeng Hao"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02372v1", "summary": "Multi-objective evolutionary algorithms (MOEAs) have become essential tools\nfor solving multi-objective optimization problems (MOPs), making their running\ntime analysis crucial for assessing algorithmic efficiency and guiding\npractical applications. While significant theoretical advances have been\nachieved for combinatorial optimization, existing studies for numerical\noptimization primarily rely on algorithmic or problem simplifications, limiting\ntheir applicability to real-world scenarios. To address this gap, we propose an\nexperimental approach for estimating upper bounds on the running time of MOEAs\nin numerical optimization without simplification assumptions. Our approach\nemploys an average gain model that characterizes algorithmic progress through\nthe Inverted Generational Distance metric. To handle the stochastic nature of\nMOEAs, we use statistical methods to estimate the probabilistic distribution of\ngains. Recognizing that gain distributions in numerical optimization exhibit\nirregular patterns with varying densities across different regions, we\nintroduce an adaptive sampling method that dynamically adjusts sampling density\nto ensure accurate surface fitting for running time estimation. We conduct\ncomprehensive experiments on five representative MOEAs (NSGA-II, MOEA/D,\nAR-MOEA, AGEMOEA-II, and PREA) using the ZDT and DTLZ benchmark suites. The\nresults demonstrate the effectiveness of our approach in estimating upper\nbounds on the running time without requiring algorithmic or problem\nsimplifications. Additionally, we provide a web-based implementation to\nfacilitate broader adoption of our methodology. This work provides a practical\ncomplement to theoretical research on MOEAs in numerical optimization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02372v1", "cate": "cs.NE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "数值优化中多目标进化算法运行时间估计的实验方法", "tldr": "本文提出了一种实验方法，用于在不进行简化假设的情况下，估计数值优化中多目标进化算法运行时间的上限，并验证了其有效性。", "motivation": "多目标进化算法（MOEAs）在解决多目标优化问题（MOPs）中至关重要，其运行时间分析对于评估算法效率和指导实际应用至关重要。然而，现有针对数值优化的研究主要依赖于算法或问题的简化，限制了其在实际场景中的适用性。为了解决这一空白，本文提出了一个实验方法。", "method": "本文提出了一种实验方法，用于在不进行简化假设的情况下估计MOEAs在数值优化中的运行时间上限。该方法采用了一种平均增益模型，通过反向世代距离（Inverted Generational Distance）度量来表征算法进展。为了处理MOEAs的随机性，使用统计方法估计增益的概率分布。针对数值优化中增益分布不规则且密度变化的情况，引入了一种自适应采样方法，动态调整采样密度以确保运行时间估计的准确曲面拟合。", "result": "在ZDT和DTLZ基准测试套件上对五种代表性MOEA（NSGA-II、MOEA/D、AR-MOEA、AGEMOEA-II和PREA）进行了综合实验。结果表明，该方法在不要求算法或问题简化的前提下，能有效估计运行时间的上限。", "conclusion": "本工作为数值优化中MOEA的理论研究提供了一个实用的补充。此外，提供了一个基于网络的实现，以促进该方法的广泛采用。", "translation": "多目标进化算法（MOEAs）已成为解决多目标优化问题（MOPs）的重要工具，其运行时间分析对于评估算法效率和指导实际应用至关重要。虽然在组合优化方面取得了显著的理论进展，但现有针对数值优化的研究主要依赖于算法或问题的简化，限制了其在实际场景中的适用性。为了解决这一空白，我们提出了一种实验方法，用于在不进行简化假设的情况下，估计数值优化中MOEA运行时间的上限。我们的方法采用了一种平均增益模型，通过反向世代距离度量来表征算法进展。为了处理MOEA的随机性，我们使用统计方法来估计增益的概率分布。认识到数值优化中增益分布呈现不规则模式，且在不同区域的密度各异，我们引入了一种自适应采样方法，动态调整采样密度，以确保运行时间估计的准确曲面拟合。我们使用ZDT和DTLZ基准测试套件，对五种代表性MOEA（NSGA-II、MOEA/D、AR-MOEA、AGEMOEA-II和PREA）进行了综合实验。结果表明，我们的方法在不要求算法或问题简化的前提下，能有效估计运行时间的上限。此外，我们提供了一个基于网络的实现，以促进我们方法的广泛采用。这项工作为数值优化中MOEA的理论研究提供了一个实用的补充。", "summary": "本文提出了一种新颖的实验方法，旨在解决数值优化中多目标进化算法（MOEAs）运行时间估计的难题，尤其是在不进行简化假设的情况下。该方法引入了基于反向世代距离的平均增益模型和统计方法来处理MOEA的随机性。为了应对增益分布的不规则性，还提出了一种自适应采样策略。通过在ZDT和DTLZ基准上对多种MOEA进行实验验证，证明了该方法在估计运行时间上限方面的有效性，并提供了在线实现以促进应用。该研究为MOEA的理论分析提供了实用的补充。", "keywords": "多目标进化算法, 运行时间估计, 数值优化, 自适应采样, 反向世代距离", "comments": "这项工作在解决MOEAs运行时间估计的实际挑战方面具有创新性，特别是在不依赖于简化假设的情况下。其引入的平均增益模型和自适应采样方法对于处理MOEAs的随机性和复杂性至关重要。提供网络实现是一个亮点，有助于促进研究成果的实际应用和更广泛的采纳。该方法填补了理论研究与实际应用之间的空白。"}}
{"id": "2507.02097", "title": "The Future is Agentic: Definitions, Perspectives, and Open Challenges of Multi-Agent Recommender Systems", "authors": ["Reza Yousefi Maragheh", "Yashar Deldjoo"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02097v1", "summary": "Large language models (LLMs) are rapidly evolving from passive engines of\ntext generation into agentic entities that can plan, remember, invoke external\ntools, and co-operate with one another. This perspective paper investigates how\nsuch LLM agents (and societies thereof) can transform the design space of\nrecommender systems.\n  We introduce a unified formalism that (i) models an individual agent as a\ntuple comprising its language core, tool set, and hierarchical memory, and (ii)\ncaptures a multi-agent recommender as a triple of agents, shared environment,\nand communication protocol. Within this framework, we present four end-to-end\nuse cases-interactive party planning, synthetic user-simulation for offline\nevaluation, multi-modal furniture recommendation, and brand-aligned explanation\ngeneration-each illustrating a distinct capability unlocked by agentic\norchestration.\n  We then surface five cross-cutting challenge families: protocol complexity,\nscalability, hallucination and error propagation, emergent misalignment\n(including covert collusion), and brand compliance.\n  For each, we formalize the problem, review nascent mitigation strategies, and\noutline open research questions. The result is both a blueprint and an agenda:\na blueprint that shows how memory-augmented, tool-using LLM agents can be\ncomposed into robust recommendation pipelines, and an agenda inviting the\nRecSys community to develop benchmarks, theoretical guarantees, and governance\ntools that keep pace with this new degree of autonomy. By unifying agentic\nabstractions with recommender objectives, the paper lays the groundwork for the\nnext generation of personalized, trustworthy, and context-rich recommendation\nservices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02097v1", "cate": "cs.IR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "未来是代理式的：多智能体推荐系统的定义、视角和开放挑战", "tldr": "本文探讨了大型语言模型（LLM）代理如何改变推荐系统，提出了一个统一的框架、多个用例，并识别了该领域面临的开放挑战。", "motivation": "大型语言模型（LLM）正在从被动的文本生成引擎演变为能够规划、记忆、调用外部工具并相互协作的代理实体。本文旨在探讨这些LLM代理（及其社会）如何转化推荐系统的设计空间，并为下一代个性化、可信赖和上下文丰富的推荐服务奠定基础。", "method": "本文引入了一个统一的形式化框架，该框架将个体代理建模为包含其语言核心、工具集和分层记忆的元组，并将多智能体推荐系统建模为由代理、共享环境和通信协议组成的三元组。在此框架内，论文提出了四个端到端用例（交互式派对规划、用于离线评估的合成用户模拟、多模态家具推荐和品牌对齐解释生成）以展示代理式编排所解锁的独特能力。", "result": "本文提供了一个蓝图，展示了如何将增强记忆、使用工具的LLM代理组合成强大的推荐管道。同时，它提出了一个议程，邀请推荐系统社区开发基准、理论保证和治理工具，以适应这种新程度的自主性。", "conclusion": "通过将代理式抽象与推荐目标相结合，本文为下一代个性化、可信赖和上下文丰富的推荐服务奠定了基础。论文还提出了五个交叉的挑战家族（协议复杂性、可扩展性、幻觉和错误传播、涌现的错位以及品牌合规性），并对其进行了形式化、回顾了新兴的缓解策略并概述了开放的研究问题。", "translation": "大型语言模型（LLM）正在迅速从被动的文本生成引擎演变为能够规划、记忆、调用外部工具并相互协作的代理实体。这篇视角论文探讨了这些LLM代理（及其社会）如何改变推荐系统的设计空间。\n我们引入了一个统一的形式化方法，该方法（i）将单个代理建模为一个包含其语言核心、工具集和分层记忆的元组，并且（ii）将多智能体推荐系统捕获为由代理、共享环境和通信协议组成的三元组。在此框架内，我们提出了四个端到端用例——交互式派对规划、用于离线评估的合成用户模拟、多模态家具推荐和品牌对齐解释生成——每个用例都展示了代理式编排所解锁的独特能力。\n然后，我们提出了五个交叉的挑战家族：协议复杂性、可扩展性、幻觉和错误传播、涌现的错位（包括秘密串通）以及品牌合规性。\n对于每一个挑战，我们都对其问题进行了形式化，回顾了新兴的缓解策略，并概述了开放的研究问题。其结果既是一个蓝图，也是一个议程：一个展示如何将记忆增强、使用工具的LLM代理组合成健壮的推荐管道的蓝图，以及一个邀请推荐系统（RecSys）社区开发基准、理论保证和治理工具以跟上这种新程度自主性的议程。通过将代理式抽象与推荐目标相结合，本文为下一代个性化、可信赖和上下文丰富的推荐服务奠定了基础。", "summary": "本文探讨了大型语言模型（LLM）从文本生成器向代理实体的演变如何影响推荐系统。作者提出了一个统一的框架来建模个体LLM代理和多智能体推荐系统，并通过四个实际用例展示了代理式编排的潜力。论文还识别并形式化了五个关键挑战（协议复杂性、可扩展性、幻觉、新兴错位和品牌合规性），并概述了缓解策略和未来的研究方向。最终，该工作为构建下一代个性化、可信赖和上下文丰富的推荐服务提供了蓝图和研究议程。", "keywords": "多智能体系统, 推荐系统, 大型语言模型, 代理, 挑战", "comments": "这篇论文具有创新性，它将LLM代理的概念引入到推荐系统领域，并提供了一个全面的视角，涵盖了从理论框架、实际用例到未来挑战的方方面面。它不仅提出了新的可能性，也坦诚地指出了实际部署中可能遇到的问题，为该领域的研究指明了方向。其提出的统一形式化方法和详细的挑战分析对于推动多智能体推荐系统的发展具有重要意义。"}}
{"id": "2507.02433", "title": "Numerical Linear Algebra in Linear Space", "authors": ["Yiping Liu", "Hoai-An Nguyen", "Junzhao Yang"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      52 pages, 0 figures", "url": "http://arxiv.org/abs/2507.02433v1", "summary": "We present a randomized linear-space solver for general linear systems\n$\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ with $\\mathbf{A} \\in \\mathbb{Z}^{n \\times\nn}$ and $\\mathbf{b} \\in \\mathbb{Z}^n$, without any assumption on the condition\nnumber of $\\mathbf{A}$. For matrices whose entries are bounded by\n$\\mathrm{poly}(n)$, the solver returns a $(1+\\epsilon)$-multiplicative\nentry-wise approximation to vector $\\mathbf{x} \\in \\mathbb{Q}^{n}$ using\n$\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ bit operations and $O(n\n\\log n)$ bits of working space (i.e., linear in the size of a vector), where\n$\\mathrm{nnz}$ denotes the number of nonzero entries. Our solver works for\nright-hand vector $\\mathbf{b}$ with entries up to $n^{O(n)}$. To our knowledge,\nthis is the first linear-space linear system solver over the rationals that\nruns in $\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ time.\n  We also present several applications of our solver to numerical linear\nalgebra problems, for which we provide algorithms with efficient polynomial\nrunning time and near-linear space. In particular, we present results for\nlinear regression, linear programming, eigenvalues and eigenvectors, and\nsingular value decomposition.", "comment": "52 pages, 0 figures", "pdf_url": "http://arxiv.org/pdf/2507.02433v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "线性空间中的数值线性代数", "tldr": "本文提出了一种随机线性空间求解器，用于求解一般线性系统，它是首个在有理数域上以 $\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ 时间运行的线性空间线性系统求解器，并将其应用于多种数值线性代数问题。", "motivation": "开发一种针对一般线性系统 $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ 的线性空间求解器，该求解器不对矩阵 $\\mathbf{A}$ 的条件数做任何假设。", "method": "提出了一种随机线性空间求解器，用于求解一般线性系统 $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$，其中 $\\mathbf{A} \\in \\mathbb{Z}^{n \\times n}$ 且 $\\mathbf{b} \\in \\mathbb{Z}^n$。对于条目受 $\\mathrm{poly}(n)$ 限制的矩阵，该求解器返回向量 $\\mathbf{x} \\in \\mathbb{Q}^{n}$ 的 $(1+\\epsilon)$-乘法逐条近似。", "result": "该求解器使用 $\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ 位操作和 $O(n \\log n)$ 位工作空间。它适用于条目高达 $n^{O(n)}$ 的右侧向量 $\\mathbf{b}$。据作者所知，这是第一个在有理数域上以 $\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ 时间运行的线性空间线性系统求解器。此外，该求解器还应用于线性回归、线性规划、特征值和特征向量以及奇异值分解等数值线性代数问题，并提供了具有高效多项式运行时间和近线性空间的算法。", "conclusion": "本文提出了一种新颖的随机线性空间求解器，用于解决一般线性系统，该求解器在时间复杂度和空间效率上均表现出色，且对矩阵条件数无假设。此外，该方法成功应用于多个重要的数值线性代数问题，展示了其广泛的实用性。", "translation": "我们提出了一种随机线性空间求解器，用于求解一般线性系统 $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$，其中 $\\mathbf{A} \\in \\mathbb{Z}^{n \\times n}$ 且 $\\mathbf{b} \\in \\mathbb{Z}^n$，对 $\\mathbf{A}$ 的条件数没有任何假设。对于条目受 $\\mathrm{poly}(n)$ 限制的矩阵，该求解器使用 $\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ 位操作和 $O(n \\log n)$ 位工作空间（即，向量大小的线性空间），返回向量 $\\mathbf{x} \\in \\mathbb{Q}^{n}$ 的 $(1+\\epsilon)$-乘法逐条近似，其中 $\\mathrm{nnz}$ 表示非零条目数。我们的求解器适用于条目高达 $n^{O(n)}$ 的右侧向量 $\\mathbf{b}$。据我们所知，这是第一个在有理数域上以 $\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ 时间运行的线性空间线性系统求解器。\n我们还介绍了我们的求解器在数值线性代数问题中的几个应用，为此我们提供了具有高效多项式运行时间和近线性空间的算法。特别是，我们展示了线性回归、线性规划、特征值和特征向量以及奇异值分解的结果。", "summary": "本文介绍了一种创新的随机线性空间求解器，专门用于解决一般线性系统 $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$，且无需对矩阵 $\\mathbf{A}$ 的条件数进行任何假设。该求解器对于条目受限于 $\\mathrm{poly}(n)$ 的矩阵，能够以 $\\widetilde{O}(n^2 \\cdot \\mathrm{nnz}(\\mathbf{A}))$ 的位操作和 $O(n \\log n)$ 的工作空间，为解向量 $\\mathbf{x}$ 提供 $(1+\\epsilon)$-乘法逐条近似。值得注意的是，这是首个在有理数域上达到此时间复杂度的线性空间线性系统求解器。此外，该研究还展示了该求解器在多种数值线性代数问题中的广泛应用，包括线性回归、线性规划、特征值与特征向量以及奇异值分解，并为这些问题提供了高效的算法。", "keywords": "随机求解器, 线性系统, 线性空间, 数值线性代数, 近似", "comments": "该论文的创新点在于提出了首个在有理数域上以特定高效时间复杂度运行的线性空间线性系统求解器，且对条件数无假设，这对于处理大规模线性系统具有重要意义。其应用范围广泛，提升了多种数值线性代数问题的解决效率。"}}
{"id": "2507.02221", "title": "GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons", "authors": ["Steven Song", "Anirudh Subramanyam", "Zhenyu Zhang", "Aarti Venkat", "Robert L. Grossman"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      11 pages, 1 figure, 7 tables", "url": "http://arxiv.org/abs/2507.02221v1", "summary": "Motivation: The Genomic Data Commons (GDC) provides access to high quality,\nharmonized cancer genomics data through a unified curation and analysis\nplatform centered around patient cohorts. While GDC users can interactively\ncreate complex cohorts through the graphical Cohort Builder, users (especially\nnew ones) may struggle to find specific cohort descriptors across hundreds of\npossible fields and properties. However, users may be better able to describe\ntheir desired cohort in free-text natural language.\n  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for\ncurating cohorts from the GDC. GDC Cohort Copilot automatically generates the\nGDC cohort filter corresponding to a user-input natural language description of\ntheir desired cohort, before exporting the cohort back to the GDC for further\nanalysis. An interactive user interface allows users to further refine the\ngenerated cohort. We develop and evaluate multiple large language models (LLMs)\nfor GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC\nCohort LLM achieves better results than GPT-4o prompting in generating GDC\ncohorts.\n  Availability and implementation: The standalone docker image for GDC Cohort\nCopilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.\nSource code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC\nCohort LLM weights are available at https://huggingface.co/uc-ctds.", "comment": "11 pages, 1 figure, 7 tables", "pdf_url": "http://arxiv.org/pdf/2507.02221v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "GDC队列副驾驶：一个用于从基因组数据中心策划队列的AI副驾驶", "tldr": "GDC Cohort Copilot是一个AI工具，它允许用户通过自然语言描述来生成GDC队列过滤器，从而简化了从基因组数据中心策划复杂患者队列的过程。", "motivation": "基因组数据中心（GDC）提供高质量的癌症基因组数据，但用户（尤其是新用户）在通过图形队列构建器创建复杂队列时，可能难以在数百个字段中找到特定的队列描述符。用户可能更倾向于用自由文本自然语言描述他们想要的队列。", "method": "引入了GDC Cohort Copilot，一个开源的副驾驶工具，用于从GDC策划队列。该工具自动根据用户输入的自然语言描述生成GDC队列过滤器，并可将队列导出回GDC进行进一步分析。它提供了一个交互式用户界面，允许用户进一步完善生成的队列。开发并评估了多个大型语言模型（LLMs），并证明其本地部署的开源GDC Cohort LLM在生成GDC队列方面比GPT-4o提示取得了更好的结果。", "result": "GDC Cohort Copilot能够根据用户的自然语言描述自动生成GDC队列过滤器，并支持将队列导出到GDC进行进一步分析。其本地部署的开源GDC Cohort LLM在生成GDC队列方面表现优于GPT-4o提示。", "conclusion": "GDC Cohort Copilot通过允许用户使用自然语言描述来生成GDC队列，显著简化了从基因组数据中心策划复杂患者队列的过程，并且其本地LLM模型表现出色。", "translation": "动机：基因组数据中心（GDC）通过一个以患者队列为中心的统一策划和分析平台，提供对高质量、协调一致的癌症基因组数据的访问。虽然GDC用户可以通过图形队列构建器交互式地创建复杂队列，但用户（尤其是新用户）可能难以在数百个可能的字段和属性中找到特定的队列描述符。然而，用户可能更擅长用自由文本自然语言描述他们想要的队列。\n结果：我们介绍了GDC队列副驾驶（GDC Cohort Copilot），一个用于从GDC策划队列的开源副驾驶工具。GDC队列副驾驶自动生成与用户输入的所需队列的自然语言描述相对应的GDC队列过滤器，然后将队列导出回GDC进行进一步分析。一个交互式用户界面允许用户进一步完善生成的队列。我们为GDC队列副驾驶开发并评估了多个大型语言模型（LLMs），并证明我们的本地服务、开源GDC队列LLM在生成GDC队列方面比GPT-4o提示取得了更好的结果。\n可用性和实现：GDC队列副驾驶的独立docker镜像可在https://quay.io/repository/cdis/gdc-cohort-copilot获取。源代码可在https://github.com/uc-cdis/gdc-cohort-copilot获取。GDC队列LLM权重可在https://huggingface.co/uc-ctds获取。", "summary": "GDC Cohort Copilot是一个创新的AI工具，旨在简化从基因组数据中心（GDC）策划患者队列的过程。它通过允许用户使用自然语言描述其所需的队列，自动生成相应的GDC队列过滤器，从而克服了现有图形界面在查找复杂描述符方面的困难。该工具提供了一个交互式用户界面用于细化，并已证明其内部开发的开源LLM在性能上优于GPT-4o。该工具及其组件均开源。", "keywords": "GDC, 队列策划, AI副驾驶, 自然语言处理, LLM", "comments": "GDC Cohort Copilot的创新之处在于它利用自然语言处理技术，极大地降低了非专业用户从复杂生物医学数据库中提取特定数据集的门槛。其开源且本地部署的LLM模型优于GPT-4o的发现，也突显了领域特定模型在特定任务上的潜力，这对于数据隐私和成本控制也具有重要意义。"}}
{"id": "2507.02380", "title": "JoyTTS: LLM-based Spoken Chatbot With Voice Cloning", "authors": ["Fangru Zhou", "Jun Zhao", "Guoxin Wang"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02380v1", "summary": "JoyTTS is an end-to-end spoken chatbot that combines large language models\n(LLM) with text-to-speech (TTS) technology, featuring voice cloning\ncapabilities. This project is built upon the open-source MiniCPM-o and\nCosyVoice2 models and trained on 2000 hours of conversational data. We have\nalso provided the complete training code to facilitate further development and\noptimization by the community. On the testing machine seed-tts-zh, it achieves\na SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.\nThe code and models, along with training and inference scripts, are available\nat https://github.com/jdh-algo/JoyTTS.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02380v1", "cate": "cs.SD", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "JoyTTS：基于LLM的带语音克隆的口语聊天机器人", "tldr": "JoyTTS是一个结合LLM和TTS并具备语音克隆能力的端到端口语聊天机器人，开源了代码和模型。", "motivation": "旨在开发一个结合大型语言模型（LLM）和文本到语音（TTS）技术，并具有语音克隆功能的端到端口语聊天机器人。", "method": "JoyTTS构建于开源的MiniCPM-o和CosyVoice2模型之上，使用2000小时的对话数据进行训练。项目提供了完整的训练代码、模型以及训练和推理脚本。", "result": "在测试机器seed-tts-zh上，实现了0.73的说话人相似度（SS）分数和5.09的词错误率（WER）。所有代码和模型均已开源。", "conclusion": "JoyTTS成功开发了一个结合LLM和TTS的端到端口语聊天机器人，具备语音克隆能力，并开源了所有相关资源，以促进社区的进一步开发和优化。", "translation": "JoyTTS是一个结合大型语言模型（LLM）和文本到语音（TTS）技术的端到端口语聊天机器人，具有语音克隆功能。该项目基于开源的MiniCPM-o和CosyVoice2模型构建，并使用2000小时的对话数据进行训练。我们还提供了完整的训练代码，以方便社区进行进一步的开发和优化。在测试机器seed-tts-zh上，它实现了0.73的说话人相似度（SS）分数和5.09的词错误率（WER）。代码、模型以及训练和推理脚本可在https://github.com/jdh-algo/JoyTTS.git获取。", "summary": "JoyTTS是一个开源的端到端口语聊天机器人，它将大型语言模型（LLM）与文本到语音（TTS）技术相结合，并支持语音克隆。该系统基于MiniCPM-o和CosyVoice2模型，通过2000小时的对话数据训练，并在测试中取得了0.73的说话人相似度（SS）和5.09的词错误率（WER）。项目已提供完整的训练代码、模型和脚本。", "keywords": "口语聊天机器人, LLM, TTS, 语音克隆, 开源", "comments": "这篇论文的创新点在于将LLM和TTS技术与语音克隆功能集成到一个端到端系统中，并将其开源，极大地降低了社区在此领域进行研究和优化的门槛。其提供的训练代码和预训练模型对于推动口语聊天机器人技术的发展具有重要意义。"}}
{"id": "2507.02274", "title": "Resolution Limits of Non-Adaptive 20 Questions Estimation for Tracking Multiple Moving Targets", "authors": ["Chunsong Sun", "Lin Zhou", "Jingjing Wang", "Weijie Yuan", "Chunxiao Jiang", "Alfred Hero"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02274v1", "summary": "Motivated by the practical application of beam tracking of multiple devices\nin Multiple Input Multiple Output (MIMO) communication, we study the problem of\nnon-adaptive twenty questions estimation for locating and tracking multiple\nmoving targets under a query-dependent noisy channel. Specifically, we derive a\nnon-asymptotic bound and a second-order asymptotic bound on resolution for\noptimal query procedures and provide numerical examples to illustrate our\nresults. In particular, we demonstrate that the bound is achieved by a state\nestimator that thresholds the mutual information density over possible target\nlocations. This single threshold decoding rule has reduced the computational\ncomplexity compared to the multiple threshold scheme proposed for locating\nmultiple stationary targets (Zhou, Bai and Hero, TIT 2022). We discuss two\nspecial cases of our setting: the case with unknown initial location and known\nvelocity, and the case with known initial location and unknown velocity. Both\ncases share the same theoretical benchmark {that applies to} stationary\nmultiple target search in Zhou, Bai and Hero (TIT 2022) while the known initial\nlocation case is close to the theoretical benchmark for stationary target\nsearch when the maximal speed is inversely proportional to the number of\nqueries. We also generalize our results to account for a piecewise constant\nvelocity model introduced in Zhou and Hero (TIT 2023), where targets change\nvelocity periodically. Finally, we illustrate our proposed algorithm for the\napplication of beam tracking of multiple mobile transmitters in a 5G wireless\nnetwork.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02274v1", "cate": "cs.IT", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "非自适应20问估计在多移动目标跟踪中的分辨率限制", "tldr": "本文研究了在查询依赖的噪声信道下，非自适应20问估计用于定位和跟踪多个移动目标的分辨率限制。", "motivation": "本文的动机是多输入多输出（MIMO）通信中多个设备波束跟踪的实际应用。", "method": "推导了最优查询过程的非渐近界和二阶渐近界；提出了一种通过对目标可能位置的互信息密度进行阈值处理的状态估计器，该估计器采用单阈值解码规则；讨论了初始位置未知但速度已知以及初始位置已知但速度未知两种特殊情况；将结果推广到分段常数速度模型。", "result": "证明了所导出的界限可以通过一个状态估计器实现，该估计器通过对互信息密度进行阈值处理来达到；与多静止目标的多阈值方案相比，单阈值解码规则显著降低了计算复杂度；在最大速度与查询次数成反比时，已知初始位置的情况接近静止目标搜索的理论基准。", "conclusion": "将结果推广到目标周期性改变速度的分段常数速度模型，并阐述了所提出的算法在5G无线网络中多个移动发射器波束跟踪应用中的潜力。", "translation": "受多输入多输出（MIMO）通信中多个设备波束跟踪实际应用的启发，我们研究了在依赖查询的噪声信道下，非自适应20问估计用于定位和跟踪多个移动目标的问题。具体来说，我们为最优查询过程推导了分辨率的非渐近界和二阶渐近界，并提供了数值示例来说明我们的结果。特别是，我们证明了该界限可以通过一个状态估计器实现，该估计器通过对可能目标位置的互信息密度进行阈值处理。与用于定位多个静止目标的多阈值方案（Zhou, Bai和Hero, TIT 2022）相比，这种单阈值解码规则降低了计算复杂度。我们讨论了我们设置的两种特殊情况：初始位置未知但速度已知的情况，以及初始位置已知但速度未知的情况。这两种情况都与Zhou, Bai和Hero（TIT 2022）中适用于静止多目标搜索的理论基准相同，而当最大速度与查询次数成反比时，已知初始位置的情况接近静止目标搜索的理论基准。我们还将我们的结果推广到Zhou和Hero（TIT 2023）中引入的分段常数速度模型，其中目标周期性地改变速度。最后，我们阐述了我们提出的算法在5G无线网络中多个移动发射器波束跟踪应用中的应用。", "summary": "本文研究了在噪声信道下，非自适应20问估计用于定位和跟踪多个移动目标的分辨率限制，其动机是MIMO通信中的波束跟踪应用。作者推导了分辨率的非渐近界和二阶渐近界，并通过一个基于互信息密度阈值的状态估计器实现了这些界限，该估计器采用的单阈值解码规则显著降低了计算复杂度。论文还讨论了初始位置已知/未知与速度已知/未知等特殊情况，并将其结果推广到分段常数速度模型，最后展示了在5G无线网络中多移动发射器波束跟踪的应用。", "keywords": "非自适应20问, 多移动目标跟踪, 分辨率限制, MIMO通信, 单阈值解码", "comments": "本文的创新之处在于将非自适应20问估计推广到多移动目标跟踪问题，并提出了一种计算复杂度更低的单阈值解码规则，这对于实际应用（如MIMO波束跟踪）具有重要意义。通过推导新的分辨率界限并讨论特殊情况，为该领域提供了理论基础。"}}
{"id": "2507.02332", "title": "PII Jailbreaking in LLMs via Activation Steering Reveals Personal Information Leakage", "authors": ["Krishna Kanth Nakka", "Xue Jiang", "Xuebing Zhou"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.02332v1", "summary": "This paper investigates privacy jailbreaking in LLMs via steering, focusing\non whether manipulating activations can bypass LLM alignment and alter response\nbehaviors to privacy related queries (e.g., a certain public figure's sexual\norientation). We begin by identifying attention heads predictive of refusal\nbehavior for private attributes (e.g., sexual orientation) using lightweight\nlinear probes trained with privacy evaluator labels. Next, we steer the\nactivations of a small subset of these attention heads guided by the trained\nprobes to induce the model to generate non-refusal responses. Our experiments\nshow that these steered responses often disclose sensitive attribute details,\nalong with other private information about data subjects such as life events,\nrelationships, and personal histories that the models would typically refuse to\nproduce. Evaluations across four LLMs reveal jailbreaking disclosure rates of\nat least 95%, with more than 50% on average of these responses revealing true\npersonal information. Our controlled study demonstrates that private\ninformation memorized in LLMs can be extracted through targeted manipulation of\ninternal activations.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.02332v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过激活操纵揭示LLM中的PII越狱及其个人信息泄露", "tldr": "本文研究了通过操纵LLM内部激活来绕过隐私保护机制，导致模型泄露个人敏感信息（PII）的越狱方法，并发现此方法能以高成功率提取真实个人信息。", "motivation": "本文旨在探究LLM中隐私越狱的可能性，特别是通过操纵激活是否能够绕过LLM的对齐机制，并改变其对隐私相关查询的拒绝行为。", "method": "首先，利用隐私评估器标签训练轻量级线性探针，识别出预测模型拒绝回答私人属性（如性取向）的注意力头。接着，根据训练好的探针引导少量这些注意力头的激活，诱导模型生成非拒绝性回答。", "result": "实验表明，这些被操纵后的回答经常泄露敏感属性细节，以及模型通常会拒绝生成的关于数据主体（如生活事件、关系、个人历史）的其他私人信息。在四种LLM上的评估显示，越狱泄露率至少达到95%，其中平均超过50%的回答揭示了真实的个人信息。", "conclusion": "受控研究表明，LLM中记忆的个人信息可以通过对内部激活的定向操纵来提取。", "translation": "本文研究了通过操纵LLM激活实现隐私越狱的方法，重点关注操纵激活是否能绕过LLM的对齐机制并改变其对隐私相关查询（例如，某个公众人物的性取向）的响应行为。我们首先利用隐私评估器标签训练轻量级线性探针，识别出预测模型拒绝回答私人属性（例如，性取向）的注意力头。接着，我们根据训练好的探针引导少量这些注意力头的激活，以诱导模型生成非拒绝性响应。我们的实验表明，这些被操纵后的响应经常泄露敏感属性细节，以及模型通常会拒绝生成的关于数据主体（如生活事件、关系和个人历史）的其他私人信息。对四种LLM的评估显示，越狱泄露率至少达到95%，其中平均超过50%的响应揭示了真实的个人信息。我们的受控研究表明，LLM中记忆的个人信息可以通过对内部激活的定向操纵来提取。", "summary": "本研究探讨了大型语言模型（LLMs）中的隐私越狱问题，通过操纵模型的内部激活来绕过其隐私保护机制。研究人员首先识别出与拒绝回答隐私查询相关的注意力头，然后通过引导这些头的激活，诱导模型泄露个人敏感信息。实验结果显示，此方法在多种LLMs上实现了高达95%的越狱成功率，并且超过50%的泄露信息是真实的个人数据，证明了LLMs中记忆的隐私信息可以通过内部激活操纵被提取。", "keywords": "LLM, 隐私越狱, 激活操纵, 个人信息泄露, 注意力头", "comments": "这项研究揭示了一种新颖且高效的LLM隐私泄露攻击向量，即通过内部激活操纵。其创新之处在于将激活操纵技术应用于隐私越狱，并量化了其高成功率。这项工作的重要性在于突显了LLM在存储和处理个人信息方面的潜在风险，并为未来LLM的隐私保护研究提供了新的方向和挑战。"}}
{"id": "2507.02575", "title": "A unifying approach to self-organizing systems interacting via conservation laws", "authors": ["Frank Barrows", "Guanming Zhang", "Satyam Anand", "Zizi Chen", "Jonathan Lin", "Amman Desai", "Stefano Martiniani", "Francesco Caravelli"], "categories": ["cond-mat.soft", "cond-mat.stat-mech", "cs.MA", "nlin.AO"], "primary_category": "Subjects:       Soft Condensed Matter (cond-mat.soft)", "pdf_link": null, "comments": "Comments:      19 pages single column + 24 pages supplementary", "url": "http://arxiv.org/abs/2507.02575v1", "summary": "We present a unified framework for embedding and analyzing dynamical systems\nusing generalized projection operators rooted in local conservation laws. By\nrepresenting physical, biological, and engineered systems as graphs with\nincidence and cycle matrices, we derive dual projection operators that\ndecompose network fluxes and potentials. This formalism aligns with principles\nof non-equilibrium thermodynamics and captures a broad class of systems\ngoverned by flux-forcing relationships and local constraints. We extend this\napproach to collective dynamics through the PRojective Embedding of Dynamical\nSystems (PrEDS), which lifts low-dimensional dynamics into a high-dimensional\nspace, enabling both replication and recovery of the original dynamics. When\nsystems fall within the PrEDS class, their collective behavior can be\neffectively approximated through projection onto a mean-field space. We\ndemonstrate the versatility of PrEDS across diverse domains, including\nresistive and memristive circuits, adaptive flow networks (e.g., slime molds),\nelastic string networks, and particle swarms. Notably, we establish a direct\ncorrespondence between PrEDS and swarm dynamics, revealing new insights into\noptimization and self-organization. Our results offer a general theoretical\nfoundation for analyzing complex networked systems and for designing systems\nthat self-organize through local interactions.", "comment": "19 pages single column + 24 pages supplementary", "pdf_url": "http://arxiv.org/pdf/2507.02575v1", "cate": "cond-mat.soft", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过守恒定律相互作用的自组织系统统一方法", "tldr": "本文提出一个统一框架（PrEDS），利用广义投影算子和局部守恒定律，分析和嵌入通过通量-力关系相互作用的自组织系统，并展示其在多种复杂系统中的应用。", "motivation": "现有方法可能无法统一分析各种通过局部守恒定律相互作用的自组织系统，需要一个通用理论基础来理解和设计这类系统。", "method": "本文提出了一个统一框架，利用基于局部守恒定律的广义投影算子来嵌入和分析动力系统。通过将物理、生物和工程系统表示为具有关联和循环矩阵的图，推导了分解网络通量和势的双重投影算子。该方法通过动力系统投影嵌入（PrEDS）扩展到集体动力学，将低维动力学提升到高维空间，实现原始动力学的复制和恢复。当系统属于PrEDS类别时，其集体行为可以通过投影到平均场空间进行有效近似。", "result": "该形式与非平衡热力学原理相符，并能捕获一大类由通量-力关系和局部约束控制的系统。PrEDS在电阻和忆阻电路、自适应流网络（如黏菌）、弹性弦网络和粒子群等不同领域表现出多功能性。研究建立了PrEDS与群体动力学之间的直接对应关系，揭示了优化和自组织的新见解。研究结果为分析复杂网络系统和设计通过局部相互作用自组织的系统提供了通用的理论基础。", "conclusion": "本文提出的统一框架和PrEDS方法为理解和设计通过局部相互作用进行自组织的复杂网络系统提供了坚实的理论基础。", "translation": "我们提出了一个统一的框架，利用植根于局部守恒定律的广义投影算子来嵌入和分析动力系统。通过将物理、生物和工程系统表示为具有关联矩阵和循环矩阵的图，我们推导出了分解网络通量和势的双重投影算子。这种形式与非平衡热力学原理相符，并能捕获一大类由通量-力关系和局部约束控制的系统。我们通过动力系统投影嵌入（PrEDS）将这种方法扩展到集体动力学，该方法将低维动力学提升到高维空间，从而能够复制和恢复原始动力学。当系统属于PrEDS类别时，它们的集体行为可以通过投影到平均场空间进行有效近似。我们展示了PrEDS在不同领域的通用性，包括电阻和忆阻电路、自适应流网络（例如黏菌）、弹性弦网络和粒子群。值得注意的是，我们建立了PrEDS与群体动力学之间的直接对应关系，揭示了优化和自组织的新见解。我们的研究结果为分析复杂的网络系统和设计通过局部相互作用自组织的系统提供了通用的理论基础。", "summary": "本文提出了一个统一的框架，利用基于局部守恒定律的广义投影算子来分析和嵌入自组织动力系统。通过将系统建模为图并推导双重投影算子，该框架能有效分解网络通量和势。进一步，通过PrEDS方法，将低维动力学提升至高维空间，实现对原始动力学的复制与恢复，并能通过平均场近似集体行为。研究展示了PrEDS在电路、生物网络和粒子群等多样化系统中的普适性，并建立了其与群体动力学的关联，为理解和设计复杂自组织系统提供了通用理论基础。", "keywords": "自组织系统, 守恒定律, 投影算子, 复杂网络, 群体动力学", "comments": "这篇论文提出了一种新颖且统一的方法来分析和设计通过局部守恒定律相互作用的自组织系统，其创新之处在于将广义投影算子与图论相结合，并引入了PrEDS框架来处理集体动力学。该方法能够跨越物理、生物和工程领域，具有很强的普适性。特别是在揭示PrEDS与群体动力学之间的对应关系方面，为自组织和优化领域带来了新的见解，为复杂网络系统的理论分析和实际设计提供了有力的工具。"}}
{"id": "2507.02137", "title": "Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection", "authors": ["Martin Obaidi", "Marc Herrmann", "Jil Klünder", "Kurt Schneider"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      This paper has been accepted at the RETRAI workshop of the 33rd IEEE International Requirements Engineering Conference (REW 2025)", "url": "http://arxiv.org/abs/2507.02137v1", "summary": "Software development relies heavily on text-based communication, making\nsentiment analysis a valuable tool for understanding team dynamics and\nsupporting trustworthy AI-driven analytics in requirements engineering.\nHowever, existing sentiment analysis tools often perform inconsistently across\ndatasets from different platforms, due to variations in communication style and\ncontent.\n  In this study, we analyze linguistic and statistical features of 10 developer\ncommunication datasets from five platforms and evaluate the performance of 14\nsentiment analysis tools. Based on these results, we propose a mapping approach\nand questionnaire that recommends suitable sentiment analysis tools for new\ndatasets, using their characteristic features as input.\n  Our results show that dataset characteristics can be leveraged to improve\ntool selection, as platforms differ substantially in both linguistic and\nstatistical properties. While transformer-based models such as SetFit and\nRoBERTa consistently achieve strong results, tool effectiveness remains\ncontext-dependent. Our approach supports researchers and practitioners in\nselecting trustworthy tools for sentiment analysis in software engineering,\nwhile highlighting the need for ongoing evaluation as communication contexts\nevolve.", "comment": "This paper has been accepted at the RETRAI workshop of the 33rd IEEE\n  International Requirements Engineering Conference (REW 2025)", "pdf_url": "http://arxiv.org/pdf/2507.02137v1", "cate": "cs.SE", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "软件工程中可信情感分析：数据集特性与工具选择", "tldr": "该研究分析了软件工程中数据集特征和情感分析工具的性能，并提出了一种基于数据集特征的工具选择方法，以提高情感分析的可信度。", "motivation": "软件开发严重依赖文本交流，情感分析是理解团队动态和支持需求工程中可信赖AI分析的宝贵工具。然而，现有情感分析工具在不同数据集上表现不一致，原因在于交流风格和内容的变化。", "method": "本研究分析了来自五个平台的10个开发者交流数据集的语言和统计特征，并评估了14种情感分析工具的性能。基于这些结果，提出了一种映射方法和问卷，利用新数据集的特征作为输入，推荐合适的情感分析工具。", "result": "研究结果表明，数据集特性可以有效改进工具选择，因为不同平台在语言和统计特性上存在显著差异。虽然基于Transformer的模型（如SetFit和RoBERTa）持续取得良好结果，但工具的有效性仍取决于具体上下文。", "conclusion": "所提出的方法支持研究人员和从业者在软件工程中选择可信赖的情感分析工具，同时强调随着交流环境的演变需要持续评估。", "translation": "软件开发严重依赖基于文本的交流，这使得情感分析成为理解团队动态和支持需求工程中可信赖的AI驱动分析的宝贵工具。然而，由于交流风格和内容的变化，现有的情感分析工具在不同平台的数据集上表现往往不一致。\n在本研究中，我们分析了来自五个平台的10个开发者交流数据集的语言和统计特征，并评估了14种情感分析工具的性能。基于这些结果，我们提出了一种映射方法和问卷，该方法和问卷利用数据集的特征作为输入，为新数据集推荐合适的情感分析工具。\n我们的结果表明，数据集特性可以用于改进工具选择，因为不同平台在语言和统计特性上存在显著差异。虽然基于Transformer的模型，如SetFit和RoBERTa，持续取得良好结果，但工具的有效性仍然取决于具体上下文。我们的方法支持研究人员和从业者在软件工程中选择可信赖的情感分析工具，同时强调随着交流环境的演变需要持续评估。", "summary": "本文旨在解决软件工程中情感分析工具表现不一致的问题，通过分析10个开发者交流数据集的语言和统计特征，并评估14种情感分析工具的性能。研究提出了一种映射方法和问卷，以根据数据集特性推荐合适的工具，证明利用这些特性可以改进工具选择。研究发现，尽管基于Transformer的模型表现良好，但工具的有效性仍具上下文依赖性，强调了持续评估对于可信情感分析的重要性。", "keywords": "情感分析, 软件工程, 数据集特性, 工具选择, 可信赖AI", "comments": "该论文通过关注数据集特性来解决情感分析工具性能不一致的挑战，具有创新性。这对于软件工程中的实际应用至关重要，有助于提高AI驱动分析的可信度。提出映射方法和问卷是一项实用的贡献。"}}
{"id": "2507.02198", "title": "GPS-DRIFT: Marine Surface Robot Localization using IMU-GPS Fusion and Invariant Filtering", "authors": ["Surya Pratap Singh", "Tsimafei Lazouski", "Maani Ghaffari"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      6 pages", "url": "http://arxiv.org/abs/2507.02198v1", "summary": "This paper presents an extension of the DRIFT invariant state estimation\nframework, enabling robust fusion of GPS and IMU data for accurate pose and\nheading estimation. Originally developed for testing and usage on a marine\nautonomous surface vehicle (ASV), this approach can also be utilized on other\nmobile systems. Building upon the original proprioceptive only DRIFT algorithm,\nwe develop a symmetry-preserving sensor fusion pipeline utilizing the invariant\nextended Kalman filter (InEKF) to integrate global position updates from GPS\ndirectly into the correction step. Crucially, we introduce a novel heading\ncorrection mechanism that leverages GPS course-over-ground information in\nconjunction with IMU orientation, overcoming the inherent unobservability of\nyaw in dead-reckoning. The system was deployed and validated on a customized\nBlue Robotics BlueBoat, but the methodological focus is on the algorithmic\napproach to fusing exteroceptive and proprioceptive sensors for drift-free\nlocalization and reliable orientation estimation. This work provides an open\nsource solution for accurate yaw observation and localization in challenging or\nGPS-degraded conditions, and lays the groundwork for future experimental and\ncomparative studies.", "comment": "6 pages", "pdf_url": "http://arxiv.org/pdf/2507.02198v1", "cate": "cs.RO", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "GPS-DRIFT：使用IMU-GPS融合和不变滤波的海洋水面机器人定位", "tldr": "本文提出了GPS-DRIFT，一个扩展的DRIFT框架，通过结合GPS和IMU数据，利用不变扩展卡尔曼滤波器和新颖的航向校正机制，实现了水面机器人的鲁棒定位和航向估计，克服了偏航不可观测性问题。", "motivation": "原始DRIFT算法仅基于本体感受，存在偏航不可观测性问题，且在挑战性或GPS信号退化条件下需要更准确的偏航观测和定位。", "method": "扩展了DRIFT不变状态估计算法，开发了一个对称性保持的传感器融合管道。利用不变扩展卡尔曼滤波器（InEKF）将GPS全局位置更新直接集成到校正步骤中。引入了一种新颖的航向校正机制，结合GPS对地航速信息和IMU方向，克服了航位推算中偏航固有的不可观测性。", "result": "该系统在定制的Blue Robotics BlueBoat上进行了部署和验证。提供了在挑战性或GPS信号退化条件下准确的偏航观测和定位的开源解决方案。", "conclusion": "本文提供了一个用于在挑战性或GPS信号退化条件下准确偏航观测和定位的开源解决方案，并为未来的实验和比较研究奠定了基础。", "translation": "本文提出了DRIFT不变状态估计框架的一个扩展，实现了GPS和IMU数据的鲁棒融合，以实现精确的姿态和航向估计。该方法最初是为海洋自主水面无人艇（ASV）的测试和使用而开发的，但也可用于其他移动系统。在原始的仅基于本体感受的DRIFT算法的基础上，我们开发了一个保持对称性的传感器融合管道，利用不变扩展卡尔曼滤波器（InEKF）将GPS的全局位置更新直接集成到校正步骤中。关键是，我们引入了一种新颖的航向校正机制，该机制结合了GPS对地航速信息和IMU方向，克服了航位推算中偏航固有的不可观测性。该系统在定制的Blue Robotics BlueBoat上进行了部署和验证，但方法论的重点是融合外部和本体感受传感器以实现无漂移定位和可靠方向估计的算法方法。这项工作为在挑战性或GPS信号退化条件下准确的偏航观测和定位提供了一个开源解决方案，并为未来的实验和比较研究奠定了基础。", "summary": "GPS-DRIFT扩展了DRIFT不变状态估计框架，通过InEKF将GPS和IMU数据融合，实现海洋水面机器人的精确姿态和航向估计。它引入了结合GPS对地航速和IMU方向的新型航向校正机制，克服了航位推算中偏航的不可观测性。该方法已在ASV上验证，并提供开源解决方案，适用于GPS信号退化环境下的鲁棒定位和方向估计。", "keywords": "水面机器人定位, IMU-GPS融合, 不变滤波, 航向估计, 偏航观测", "comments": "该研究通过扩展DRIFT框架，并引入创新的航向校正机制，有效解决了水面机器人定位中偏航不可观测的难题，特别是在GPS信号受限的环境下。其开源解决方案的提供，对于推动相关领域的进一步研究和应用具有重要意义。"}}
{"id": "2507.02186", "title": "EvalAssist: A Human-Centered Tool for LLM-as-a-Judge", "authors": ["Zahra Ashktorab", "Elizabeth M. Daly", "Erik Miehling", "Werner Geyer", "Martin Santillan Cooper", "Tejaswini Pedapati", "Michael Desmond", "Qian Pan", "Hyo Jin Do"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02186v1", "summary": "With the broad availability of large language models and their ability to\ngenerate vast outputs using varied prompts and configurations, determining the\nbest output for a given task requires an intensive evaluation process, one\nwhere machine learning practitioners must decide how to assess the outputs and\nthen carefully carry out the evaluation. This process is both time-consuming\nand costly. As practitioners work with an increasing number of models, they\nmust now evaluate outputs to determine which model and prompt performs best for\na given task. LLMs are increasingly used as evaluators to filter training data,\nevaluate model performance, assess harms and risks, or assist human evaluators\nwith detailed assessments. We present EvalAssist, a framework that simplifies\nthe LLM-as-a-judge workflow. The system provides an online criteria development\nenvironment, where users can interactively build, test, and share custom\nevaluation criteria in a structured and portable format. We support a set of\nLLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a\nprompt-chaining approach we developed and contributed to the UNITXT open-source\nlibrary. Additionally, our system also includes specially trained evaluators to\ndetect harms and risks in LLM outputs. We have deployed the system internally\nin our organization with several hundreds of users.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02186v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "EvalAssist：一个以人为本的LLM即评判工具", "tldr": "EvalAssist是一个以人为本的工具，旨在简化使用大型语言模型（LLM）作为评判者的评估流程，通过提供交互式标准开发环境和LLM评估管道，提高评估效率并检测风险。", "motivation": "随着大型语言模型（LLM）的广泛应用及其生成大量多样化输出的能力，确定给定任务的最佳输出需要一个密集且耗时、昂贵的评估过程。机器学习从业者需要评估LLM输出以选择最佳模型和提示，并且LLM正被越来越多地用作评估器来过滤数据、评估性能、评估危害和风险。因此，需要一个工具来简化LLM作为评判者的工作流程。", "method": "本文提出了EvalAssist，一个简化LLM即评判工作流程的框架。它提供了一个在线标准开发环境，用户可以交互式地构建、测试和共享定制的评估标准。系统支持一套利用现成LLM的基于LLM的评估管道，并使用其开发并贡献给UNITXT开源库的提示链方法。此外，系统还包含经过特殊训练的评估器来检测LLM输出中的危害和风险。", "result": "该系统已在其组织内部署，拥有数百名用户。", "conclusion": "EvalAssist是一个以人为本的工具，通过提供交互式标准开发环境和LLM评估管道，简化了LLM作为评判者的评估工作流程，有助于提高评估效率并检测LLM输出中的危害和风险。", "translation": "随着大型语言模型（LLM）的广泛可用性以及它们使用各种提示和配置生成大量输出的能力，确定给定任务的最佳输出需要一个密集的评估过程，机器学习从业者必须决定如何评估输出，然后仔细地进行评估。这个过程既耗时又昂贵。随着从业者使用越来越多的模型，他们现在必须评估输出，以确定哪个模型和提示在给定任务中表现最佳。LLM越来越多地被用作评估器，用于过滤训练数据、评估模型性能、评估危害和风险，或协助人类评估者进行详细评估。我们提出了EvalAssist，一个简化LLM即评判工作流程的框架。该系统提供了一个在线标准开发环境，用户可以在其中以结构化和便携式格式交互式地构建、测试和共享自定义评估标准。我们支持一套基于LLM的评估管道，这些管道利用现成的LLM，并使用我们开发并贡献给UNITXT开源库的提示链方法。此外，我们的系统还包括经过专门训练的评估器，用于检测LLM输出中的危害和风险。我们已将该系统内部部署在我们的组织中，拥有数百名用户。", "summary": "EvalAssist是一个以人为本的工具，旨在简化大型语言模型（LLM）作为评判者的评估流程。面对LLM输出评估的耗时和高成本问题，EvalAssist提供了一个在线交互式环境，用户可以在其中开发、测试和共享自定义评估标准。它利用LLM评估管道和独特的提示链方法，并集成了专门训练的评估器来检测LLM输出的危害和风险。该系统已在其组织内部署并被数百名用户使用。", "keywords": "LLM-as-a-judge, 评估工具, 人机交互, 提示工程, 风险检测", "comments": "EvalAssist的创新之处在于其以人为本的设计，通过提供交互式标准开发环境，使用户能够灵活地定义和共享评估标准。它结合了现成LLM的评估能力和自定义的提示链方法，并特别关注了危害和风险检测，这对于LLM的实际应用至关重要。该工具在内部部署并被广泛使用，表明其具有实际价值和可用性。"}}
{"id": "2507.02654", "title": "Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference Infrastructure", "authors": ["Rui Xie", "Asad Ul Haq", "Yunhua Fang", "Linsen Ma", "Sanchari Sen", "Swagath Venkataramani", "Liu Liu", "Tong Zhang"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02654v1", "summary": "High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy\nefficiency for AI workloads, but its high cost per bit, driven in part by\nstringent on-die reliability requirements, poses a growing barrier to scalable\ndeployment. This work explores a system-level approach to cost reduction by\neliminating on-die ECC and shifting all fault management to the memory\ncontroller. We introduce a domain-specific ECC framework combining\nlarge-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC\ndetection, differential parity updates to mitigate write amplification, and\ntunable protection based on data importance. Our evaluation using LLM inference\nworkloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the\nsystem retains over 78\\% of throughput and 97\\% of model accuracy compared with\nsystems equipped with ideal error-free HBM. By treating reliability as a\ntunable system parameter rather than a fixed hardware constraint, our design\nopens a new path toward low-cost, high-performance HBM deployment in AI\ninfrastructure.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02654v1", "cate": "cs.AR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "打破HBM位成本障碍：面向AI推理基础设施的领域专用ECC", "tldr": "本文通过取消HBM片上ECC并将故障管理转移到内存控制器，提出了一种领域专用ECC框架，显著降低了HBM成本，同时在AI推理工作负载下保持了高吞吐量和准确性。", "motivation": "高带宽存储器（HBM）在AI工作负载中表现出卓越的带宽和能效，但其高昂的每位成本（部分由严格的片上可靠性要求驱动）日益成为可扩展部署的障碍。本工作旨在通过系统级方法降低成本。", "method": "本研究探索了一种系统级成本降低方法，即取消片上ECC并将所有故障管理转移到内存控制器。引入了一个领域专用ECC框架，该框架结合了长码字Reed–Solomon（RS）纠错、轻量级细粒度CRC检测、用于缓解写放大的差分奇偶校验更新以及基于数据重要性的可调保护。", "result": "使用LLM推理工作负载进行的评估表明，即使在原始HBM位错误率高达$10^{-3}$的情况下，与配备理想无错误HBM的系统相比，该系统仍能保留超过78%的吞吐量和97%的模型精度。", "conclusion": "通过将可靠性视为可调的系统参数而非固定的硬件约束，本设计为AI基础设施中低成本、高性能HBM的部署开辟了一条新途径。", "translation": "高带宽存储器（HBM）为AI工作负载提供了卓越的带宽和能效，但其高昂的每位成本（部分由严格的片上可靠性要求驱动）日益成为可扩展部署的障碍。本工作探索了一种系统级成本降低方法，即取消片上ECC并将所有故障管理转移到内存控制器。我们引入了一个领域专用ECC框架，该框架结合了长码字Reed–Solomon（RS）纠错、轻量级细粒度CRC检测、用于缓解写放大的差分奇偶校验更新以及基于数据重要性的可调保护。我们使用LLM推理工作负载进行的评估表明，即使在原始HBM位错误率高达$10^{-3}$的情况下，与配备理想无错误HBM的系统相比，该系统仍能保留超过78%的吞吐量和97%的模型精度。通过将可靠性视为可调的系统参数而非固定的硬件约束，本设计为AI基础设施中低成本、高性能HBM的部署开辟了一条新途径。", "summary": "本文提出了一种创新的系统级方法来降低HBM在AI推理基础设施中的成本。通过取消HBM的片上ECC并将其故障管理转移到内存控制器，研究人员开发了一种领域专用ECC框架。该框架结合了Reed-Solomon纠错、CRC检测、差分奇偶校验更新和可调保护。实验结果表明，即使在较高的位错误率下，该系统也能在大型语言模型推理中保持显著的吞吐量和模型精度。这为实现低成本、高性能的HBM部署提供了一条新路径。", "keywords": "HBM, ECC, AI推理, 成本优化, 内存控制器", "comments": "该论文的创新点在于提出了一个系统级的解决方案来解决HBM成本高昂的问题，特别是通过重新思考和重新定位ECC功能。它将可靠性从固定的硬件约束转变为可调的系统参数，这对于AI推理基础设施中HBM的广泛部署具有重要意义。通过在保持高性能的同时降低成本，该方法有望加速HBM在AI应用中的普及。"}}
{"id": "2507.02781", "title": "From Pixels to Damage Severity: Estimating Earthquake Impacts Using Semantic Segmentation of Social Media Images", "authors": ["Danrong Zhang", "Huili Huang", "N. Simrill Smith", "Nimisha Roy", "J. David Frost"], "categories": ["cs.CV", "cs.SI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02781v1", "summary": "In the aftermath of earthquakes, social media images have become a crucial\nresource for disaster reconnaissance, providing immediate insights into the\nextent of damage. Traditional approaches to damage severity assessment in\npost-earthquake social media images often rely on classification methods, which\nare inherently subjective and incapable of accounting for the varying extents\nof damage within an image. Addressing these limitations, this study proposes a\nnovel approach by framing damage severity assessment as a semantic segmentation\nproblem, aiming for a more objective analysis of damage in earthquake-affected\nareas. The methodology involves the construction of a segmented damage severity\ndataset, categorizing damage into three degrees: undamaged structures, damaged\nstructures, and debris. Utilizing this dataset, the study fine-tunes a\nSegFormer model to generate damage severity segmentations for post-earthquake\nsocial media images. Furthermore, a new damage severity scoring system is\nintroduced, quantifying damage by considering the varying degrees of damage\nacross different areas within images, adjusted for depth estimation. The\napplication of this approach allows for the quantification of damage severity\nin social media images in a more objective and comprehensive manner. By\nproviding a nuanced understanding of damage, this study enhances the ability to\noffer precise guidance to disaster reconnaissance teams, facilitating more\neffective and targeted response efforts in the aftermath of earthquakes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02781v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "从像素到损伤严重程度：使用社交媒体图像的语义分割估计地震影响", "tldr": "本研究提出了一种基于语义分割的新方法，用于客观量化地震后社交媒体图像中的损伤严重程度，以克服传统分类方法的局限性。", "motivation": "传统的地震后社交媒体图像损伤严重程度评估方法依赖于分类方法，这些方法主观且无法反映图像内不同程度的损伤。因此，需要一种更客观、全面的损伤评估方法。", "method": "构建了一个分段损伤严重程度数据集，将损伤分为未受损结构、受损结构和碎片三类。利用该数据集微调SegFormer模型以生成损伤严重程度分割。引入了一个新的损伤严重程度评分系统，该系统通过考虑图像内不同区域的不同程度的损伤来量化损伤，并根据深度估计进行调整。", "result": "该方法能够以更客观和全面的方式量化社交媒体图像中的损伤严重程度。", "conclusion": "通过提供对损伤细致入微的理解，本研究提高了为灾害侦察队提供精确指导的能力，从而促进了地震后更有效和有针对性的响应工作。", "translation": "在地震发生后，社交媒体图像已成为灾害侦察的重要资源，能够即时了解损害程度。传统的地震后社交媒体图像损伤严重程度评估方法通常依赖于分类方法，这些方法本质上是主观的，并且无法解释图像内不同程度的损伤。为了解决这些局限性，本研究提出了一种新颖的方法，将损伤严重程度评估视为一个语义分割问题，旨在对受地震影响区域的损伤进行更客观的分析。该方法涉及构建一个分段损伤严重程度数据集，将损伤分为三个等级：未受损结构、受损结构和碎片。利用该数据集，本研究对SegFormer模型进行了微调，以生成地震后社交媒体图像的损伤严重程度分割。此外，还引入了一种新的损伤严重程度评分系统，通过考虑图像内不同区域的不同程度的损伤来量化损伤，并根据深度估计进行调整。这种方法的应用使得能够以更客观和全面的方式量化社交媒体图像中的损伤严重程度。通过提供对损伤细致入微的理解，本研究增强了为灾害侦察团队提供精确指导的能力，从而促进了地震后更有效和有针对性的响应工作。", "summary": "本研究提出了一种新颖的语义分割方法，用于客观地量化地震后社交媒体图像中的损伤严重程度。通过构建一个分段损伤数据集并微调SegFormer模型，结合新的损伤严重程度评分系统，该方法克服了传统分类方法的局限性，能够提供更精确、全面的损伤评估，从而为灾害响应提供更有力的支持。", "keywords": "地震影响, 损伤评估, 语义分割, 社交媒体图像", "comments": "这项研究的创新之处在于将地震损伤评估从传统的图像分类问题转化为更精细的语义分割问题，这使得模型能够识别图像内部不同区域的损伤程度，而非仅仅给出整体分类。引入深度估计调整的评分系统进一步提升了评估的客观性和准确性。这项工作对于灾害侦察和响应具有重要意义，能够帮助救援队更快速、准确地了解灾情。"}}
{"id": "2507.02013", "title": "AI-Empowered Channel Generation for IoV Semantic Communications in Dynamic Conditions", "authors": ["Hao Liu", "Bo Yang", "Zhiwen Yu", "Xuelin Cao", "George C. Alexandropoulos", "Yan Zhang", "Chau Yuen"], "categories": ["cs.NI", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02013v1", "summary": "The Internet of Vehicles (IoV) transforms the transportation ecosystem\npromising pervasive connectivity and data-driven approaches. Deep learning and\ngenerative Artificial Intelligence (AI) have the potential to significantly\nenhance the operation of applications within IoV by facilitating efficient\ndecision-making and predictive capabilities, including intelligent navigation,\nvehicle safety monitoring, accident prevention, and intelligent traffic\nmanagement. Nevertheless, efficiently transmitting and processing the massive\nvolumes of data generated by the IoV in real-time remains a significant\nchallenge, particularly in dynamic and unpredictable wireless channel\nconditions. To address these challenges, this paper proposes a semantic\ncommunication framework based on channel perception to improve the accuracy and\nefficiency of data transmission. The semantic communication model extracts and\ncompresses the information to be transmitted. In addition, the wireless channel\nis estimated by using a generative diffusion model, which is employed to\npredict the dynamic channel states, thereby improving the quality of IoV\nservice. In dynamic scenarios, however, the channel estimation performance may\nbe degraded when substantially new scenarios take place, which will adversely\naffect user experience. To mitigate this limitation, we employ a large model to\nfine-tune the channel generation model to enhance its adaptability for varying\nscenarios. The performance and reliability of the proposed framework are\nevaluated on the two public datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02013v1", "cate": "cs.NI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "动态条件下IoV语义通信中AI赋能的信道生成", "tldr": "本文提出了一种AI赋能的语义通信框架，通过生成扩散模型和大型模型微调来解决车联网在动态信道条件下的数据传输挑战。", "motivation": "车联网(IoV)面临在动态和不可预测的无线信道条件下实时高效传输和处理海量数据的巨大挑战。", "method": "本文提出了一种基于信道感知的语义通信框架，旨在提高数据传输的准确性和效率。该框架首先通过语义通信模型提取并压缩待传输信息，然后利用生成扩散模型估计并预测动态无线信道状态。为解决动态场景中新情况导致信道估计性能下降的问题，研究进一步采用大型模型对信道生成模型进行微调，以增强其对不同场景的适应性。", "result": "该框架的性能和可靠性在两个公共数据集上进行了评估。", "conclusion": "提出的AI赋能的语义通信框架通过结合语义通信、生成扩散模型和大型模型微调，有效提升了车联网在动态信道条件下的数据传输准确性和效率，并增强了对不同场景的适应性，从而改善了服务质量。", "translation": "车联网（IoV）正在改变交通生态系统，承诺实现普遍连接和数据驱动的方法。深度学习和生成式人工智能（AI）有潜力通过促进高效的决策制定和预测能力，显著增强车联网内部应用程序的运行，包括智能导航、车辆安全监控、事故预防和智能交通管理。然而，在实时传输和处理车联网生成的海量数据仍然是一个重大挑战，尤其是在动态和不可预测的无线信道条件下。为了应对这些挑战，本文提出了一种基于信道感知的语义通信框架，以提高数据传输的准确性和效率。语义通信模型提取并压缩待传输的信息。此外，通过使用生成扩散模型估计无线信道，该模型用于预测动态信道状态，从而提高车联网服务质量。然而，在动态场景中，当出现大量新场景时，信道估计性能可能会下降，这将对用户体验产生不利影响。为了缓解这一限制，我们采用大型模型对信道生成模型进行微调，以增强其对不同场景的适应性。所提出的框架的性能和可靠性在两个公共数据集上进行了评估。", "summary": "本文针对车联网在动态无线信道下大数据传输的挑战，提出了一种AI赋能的语义通信框架。该框架利用语义通信模型进行信息提取和压缩，并采用生成扩散模型预测动态信道状态。为提升在多变场景下的适应性，研究还引入大型模型对信道生成模型进行微调。该框架的性能和可靠性已在公共数据集上得到验证。", "keywords": "车联网, 语义通信, 信道生成, 生成扩散模型, 大型模型, 动态条件", "comments": "这篇论文通过结合语义通信、生成扩散模型和大型模型微调，为车联网在复杂动态环境下的数据传输提供了创新的解决方案。特别是引入大型模型来增强信道生成模型的适应性，是其创新点之一，有望提升IoV服务的鲁棒性。"}}
{"id": "2507.02404", "title": "Alps, a versatile research infrastructure", "authors": ["Maxime Martinasso", "Mark Klein", "Thomas C. Schulthess"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures, Cray User Group(CUG) 2025 Best Paper Award", "url": "http://arxiv.org/abs/2507.02404v1", "summary": "The Swiss National Supercomputing Centre (CSCS) has a long-standing tradition\nof delivering top-tier high-performance computing systems, exemplified by the\nPiz Daint supercomputer. However, the increasing diversity of scientific needs\nhas exposed limitations in traditional vertically integrated HPC architectures,\nwhich often lack flexibility and composability. To address these challenges,\nCSCS developed Alps, a next-generation HPC infrastructure designed with a\ntransformative principle: resources operate as independent endpoints within a\nhigh-speed network. This architecture enables the creation of independent\ntenant-specific and platform-specific services, tailored to diverse scientific\nrequirements.\n  Alps incorporates heterogeneous hardware, including CPUs and GPUs,\ninterconnected by a high-performance Slingshot network, and offers a modular\nstorage system. A key innovation is the versatile software-defined cluster\n(vCluster) technology, which bridges cloud and HPC paradigms. By abstracting\ninfrastructure, service management, and user environments into distinct layers,\nvClusters allow for customized platforms that support diverse workloads.\nCurrent platforms on Alps serve various scientific domains, including numerical\nweather prediction, and AI research.", "comment": "10 pages, 6 figures, Cray User Group(CUG) 2025 Best Paper Award", "pdf_url": "http://arxiv.org/pdf/2507.02404v1", "cate": "cs.DC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "阿尔卑斯，一个多功能的科研基础设施", "tldr": "Alps是一个由CSCS开发的下一代高性能计算基础设施，通过将资源作为独立端点和引入vCluster技术，旨在提供灵活性和可组合性以满足多样化的科学需求。", "motivation": "传统垂直集成的高性能计算（HPC）架构在面对日益多样化的科学需求时，缺乏灵活性和可组合性，这促使CSCS开发了Alps。", "method": "Alps是一个下一代HPC基础设施，其核心原则是资源作为高速网络中的独立端点运行。它整合了异构硬件（CPU和GPU），通过高性能Slingshot网络连接，并拥有模块化存储系统。关键创新是多功能软件定义集群（vCluster）技术，该技术通过将基础设施、服务管理和用户环境抽象为不同层，实现了云与HPC范式的融合，从而支持定制化平台和多样化工作负载。", "result": "Alps能够创建独立的租户专用和平台专用服务，满足不同的科学需求。它支持多样化的工作负载，目前已在数值天气预报和AI研究等科学领域提供服务。", "conclusion": "Alps通过其创新的独立端点架构和vCluster技术，成功克服了传统HPC系统的局限性，提供了一个高度灵活和可组合的下一代科研基础设施，以适应不断增长和多样化的科学计算需求。", "translation": "瑞士国家超级计算中心（CSCS）在提供顶级高性能计算系统方面拥有悠久传统，Piz Daint超级计算机就是例证。然而，日益多样化的科学需求暴露了传统垂直集成HPC架构的局限性，这些架构通常缺乏灵活性和可组合性。为了应对这些挑战，CSCS开发了Alps，这是一个以变革性原则设计的下一代HPC基础设施：资源在高速网络中作为独立的端点运行。这种架构能够创建独立的租户专用和平台专用服务，以适应不同的科学要求。\n  Alps集成了异构硬件，包括CPU和GPU，通过高性能Slingshot网络互连，并提供模块化存储系统。一个关键创新是多功能软件定义集群（vCluster）技术，它弥合了云和HPC范式之间的鸿沟。通过将基础设施、服务管理和用户环境抽象为不同的层，vCluster允许定制化的平台来支持多样化的工作负载。Alps上当前的平台服务于各种科学领域，包括数值天气预报和人工智能研究。", "summary": "CSCS针对传统HPC架构在面对多样化科学需求时缺乏灵活性和可组合性的问题，开发了Alps这一下一代HPC基础设施。Alps的核心创新在于将资源作为高速网络中的独立端点，并引入了多功能软件定义集群（vCluster）技术，从而实现了云和HPC范式的融合。该架构支持异构硬件和模块化存储，能够创建定制化的租户和平台专用服务，有效支撑数值天气预报和AI研究等多样化科学工作负载。", "keywords": "HPC基础设施, 软件定义集群, vCluster, 异构计算, CSCS", "comments": "这篇论文介绍的Alps基础设施在HPC领域具有显著的创新性。其将资源视为独立网络端点的设计原则，以及软件定义集群（vCluster）技术，有效地解决了传统HPC系统灵活性和可组合性不足的问题。这种方法使得HPC系统能够更好地适应日益多样化的科学计算需求，尤其是在融合云和HPC范式方面，为未来高性能计算平台的发展指明了方向。"}}
{"id": "2507.02087", "title": "Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions", "authors": ["Eitan Anzenberg", "Arunava Samajpati", "Sivasankaran Chandrasekar", "Varun Kacholia"], "categories": ["cs.LG", "cs.CL", "cs.CY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 2 figures, 2 tables. Submitted to NeurIPS 2025", "url": "http://arxiv.org/abs/2507.02087v1", "summary": "The use of large language models (LLMs) in hiring promises to streamline\ncandidate screening, but it also raises serious concerns regarding accuracy and\nalgorithmic bias where sufficient safeguards are not in place. In this work, we\nbenchmark several state-of-the-art foundational LLMs - including models from\nOpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our\nproprietary domain-specific hiring model (Match Score) for job candidate\nmatching. We evaluate each model's predictive accuracy (ROC AUC,\nPrecision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis\nacross declared gender, race, and intersectional subgroups). Our experiments on\na dataset of roughly 10,000 real-world recent candidate-job pairs show that\nMatch Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs\n0.77) and achieves significantly more equitable outcomes across demographic\ngroups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957\n(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the\nintersectionals, respectively). We discuss why pretraining biases may cause\nLLMs with insufficient safeguards to propagate societal biases in hiring\nscenarios, whereas a bespoke supervised model can more effectively mitigate\nthese biases. Our findings highlight the importance of domain-specific modeling\nand bias auditing when deploying AI in high-stakes domains such as hiring, and\ncaution against relying on off-the-shelf LLMs for such tasks without extensive\nfairness safeguards. Furthermore, we show with empirical evidence that there\nshouldn't be a dichotomy between choosing accuracy and fairness in hiring: a\nwell-designed algorithm can achieve both accuracy in hiring and fairness in\noutcomes.", "comment": "10 pages, 2 figures, 2 tables. Submitted to NeurIPS 2025", "pdf_url": "http://arxiv.org/pdf/2507.02087v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "评估大型语言模型在招聘决策中的前景与陷阱", "tldr": "研究发现，在招聘决策中，领域专用模型在准确性和公平性上均优于通用大型语言模型。", "motivation": "大型语言模型（LLMs）在招聘中虽然有望简化候选人筛选，但也引发了对准确性和算法偏见的严重担忧，尤其是在缺乏足够保障措施的情况下。", "method": "本研究对OpenAI、Anthropic、Google、Meta和Deepseek等多个最先进的基础LLM进行了基准测试，并将其与作者专有的领域特定招聘模型（Match Score）进行比较，用于职位候选人匹配。评估指标包括预测准确性（ROC AUC、Precision-Recall AUC、F1-score）和公平性（按性别、种族和交叉亚组的截止分析影响比）。实验基于约10,000个真实世界的候选人-职位对数据集。", "result": "实验结果表明，Match Score在准确性上优于通用LLMs（ROC AUC 0.85 vs 0.77），并在人口统计学群体之间实现了显著更公平的结果。Match Score的最低种族影响比达到0.957（接近均等），而最佳LLMs为0.809或更低（交叉亚组分别为0.906 vs 0.773）。研究还表明，在招聘中选择准确性和公平性之间不应存在二分法，精心设计的算法可以同时实现两者。", "conclusion": "研究强调了在招聘等高风险领域部署AI时，领域特定建模和偏见审计的重要性，并警告不要在没有广泛公平性保障的情况下依赖现成的LLMs。精心设计的算法可以同时实现招聘的准确性和结果的公平性。", "translation": "大型语言模型（LLMs）在招聘中的应用有望简化候选人筛选，但也引发了对准确性和算法偏见的严重担忧，尤其是在缺乏足够保障措施的情况下。在这项工作中，我们对多个最先进的基础LLM进行了基准测试——包括来自OpenAI、Anthropic、Google、Meta和Deepseek的模型，并将它们与我们专有的领域特定招聘模型（Match Score）进行比较，用于职位候选人匹配。我们评估了每个模型的预测准确性（ROC AUC、Precision-Recall AUC、F1-score）和公平性（按已声明的性别、种族和交叉亚组的截止分析影响比）。我们对大约10,000个真实世界近期候选人-职位对数据集进行的实验表明，Match Score在准确性上优于通用LLMs（ROC AUC 0.85 vs 0.77），并在人口统计学群体之间实现了显著更公平的结果。值得注意的是，Match Score达到了最低种族影响比0.957（接近均等），而最佳LLMs为0.809或更低（交叉亚组分别为0.906 vs 0.773）。我们讨论了为什么预训练偏见可能导致缺乏足够保障的LLMs在招聘场景中传播社会偏见，而定制的监督模型可以更有效地缓解这些偏见。我们的发现强调了在招聘等高风险领域部署AI时，领域特定建模和偏见审计的重要性，并警告不要在没有广泛公平性保障的情况下依赖现成的LLMs来完成此类任务。此外，我们通过经验证据表明，在招聘中选择准确性和公平性之间不应存在二分法：精心设计的算法可以同时实现招聘的准确性和结果的公平性。", "summary": "本研究评估了大型语言模型（LLMs）在招聘决策中的应用，并将其与一个专有的领域特定模型（Match Score）进行对比。通过对约10,000个真实世界数据集的实验，结果显示Match Score在预测准确性（ROC AUC 0.85 vs LLMs 0.77）和公平性（最低种族影响比0.957 vs LLMs 0.809）方面均显著优于通用LLMs。研究指出，通用LLMs的预训练偏见可能导致招聘中的社会偏见传播，而定制的监督模型能有效缓解这些偏见。论文强调了在招聘等高风险领域中，领域特定建模和偏见审计的关键作用，并指出精心设计的算法可以同时实现招聘的准确性和结果的公平性。", "keywords": "大型语言模型, 招聘决策, 算法偏见, 公平性, 领域特定模型", "comments": "该论文通过实证研究揭示了通用大型语言模型在招聘这一高风险领域中存在的局限性，尤其是在公平性方面。其创新之处在于对比了通用LLMs与领域特定模型的性能，并明确指出了后者在准确性和偏见缓解上的优势。这对于企业在实际部署AI招聘系统时具有重要的指导意义，强调了定制化和严格偏见审计的必要性，而非简单采纳“开箱即用”的通用模型。研究还挑战了准确性与公平性之间的二分法，表明两者可以兼得，提供了积极的解决方案。"}}
{"id": "2507.01971", "title": "DeepSupp: Attention-Driven Correlation Pattern Analysis for Dynamic Time Series Support and Resistance Levels Identification", "authors": ["Boris Kriuk", "Logic Ng", "Zarif Al Hossain"], "categories": ["q-fin.ST", "cs.AI", "cs.CE", "cs.LG"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "Comments:      7 pages, 4 figures, 1 table", "url": "http://arxiv.org/abs/2507.01971v1", "summary": "Support and resistance (SR) levels are central to technical analysis, guiding\ntraders in entry, exit, and risk management. Despite widespread use,\ntraditional SR identification methods often fail to adapt to the complexities\nof modern, volatile markets. Recent research has introduced machine learning\ntechniques to address the following challenges, yet most focus on price\nprediction rather than structural level identification. This paper presents\nDeepSupp, a new deep learning approach for detecting financial support levels\nusing multi-head attention mechanisms to analyze spatial correlations and\nmarket microstructure relationships. DeepSupp integrates advanced feature\nengineering, constructing dynamic correlation matrices that capture evolving\nmarket relationships, and employs an attention-based autoencoder for robust\nrepresentation learning. The final support levels are extracted through\nunsupervised clustering, leveraging DBSCAN to identify significant price\nthresholds. Comprehensive evaluations on S&P 500 tickers demonstrate that\nDeepSupp outperforms six baseline methods, achieving state-of-the-art\nperformance across six financial metrics, including essential support accuracy\nand market regime sensitivity. With consistent results across diverse market\nconditions, DeepSupp addresses critical gaps in SR level detection, offering a\nscalable and reliable solution for modern financial analysis. Our approach\nhighlights the potential of attention-based architectures to uncover nuanced\nmarket patterns and improve technical trading strategies.", "comment": "7 pages, 4 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.01971v1", "cate": "q-fin.ST", "date": "2025-06-22", "updated": "2025-06-22", "AI": {"title_translation": "DeepSupp：注意力驱动相关模式分析在动态时间序列支撑与阻力位识别中的应用", "tldr": "DeepSupp是一个新的深度学习方法，利用多头注意力机制分析相关性模式，以识别金融时间序列中的动态支撑位，并在S&P 500股票上表现优于现有基线方法。", "motivation": "尽管支撑与阻力（SR）水平在技术分析中至关重要，但传统识别方法难以适应现代波动市场。现有机器学习方法多侧重于价格预测而非结构性水平识别，这促使本研究开发一种新的方法来解决SR水平检测的不足。", "method": "本文提出了DeepSupp，一种新的深度学习方法。它利用多头注意力机制分析空间相关性和市场微观结构关系，以检测金融支撑水平。DeepSupp集成了先进的特征工程，构建动态相关矩阵，并采用基于注意力的自编码器进行鲁棒的表示学习。最终的支撑水平通过无监督聚类（DBSCAN）提取。", "result": "DeepSupp在S&P 500股票上的综合评估表明，它在六个金融指标（包括支撑准确性和市场机制敏感性）上均优于六种基线方法，达到了最先进的性能。", "conclusion": "DeepSupp通过提供一个可扩展且可靠的解决方案，解决了SR水平检测中的关键空白。该方法突显了基于注意力的架构在揭示细微市场模式和改进技术交易策略方面的潜力。", "translation": "支撑与阻力（SR）水平是技术分析的核心，指导交易者进行入场、出场和风险管理。尽管广泛使用，但传统的SR识别方法往往无法适应现代复杂且波动的市场。最近的研究引入了机器学习技术来解决以下挑战，但大多数关注价格预测而非结构性水平识别。本文提出了DeepSupp，一种新的深度学习方法，利用多头注意力机制分析空间相关性和市场微观结构关系，以检测金融支撑水平。DeepSupp集成了先进的特征工程，构建动态相关矩阵以捕捉不断演变的市场关系，并采用基于注意力的自编码器进行鲁棒的表示学习。最终的支撑水平通过无监督聚类，利用DBSCAN来识别重要的价格阈值。在S&P 500股票上的综合评估表明，DeepSupp优于六种基线方法，在六个金融指标（包括重要的支撑准确性和市场机制敏感性）上均达到了最先进的性能。DeepSupp在各种市场条件下均表现出一致的结果，解决了SR水平检测中的关键空白，为现代金融分析提供了一个可扩展且可靠的解决方案。我们的方法突显了基于注意力的架构在揭示细微市场模式和改进技术交易策略方面的潜力。", "summary": "DeepSupp是一种新颖的深度学习框架，专为识别金融时间序列中的动态支撑水平而设计。它通过结合多头注意力机制、动态相关矩阵构建和基于注意力的自编码器进行特征学习，并利用DBSCAN进行无监督聚类以提取关键价格阈值。在S&P 500股票上的广泛评估显示，DeepSupp在多个金融指标上均超越了现有基线方法，展示了其在复杂市场条件下识别支撑位的有效性和鲁棒性。", "keywords": "支撑与阻力, 深度学习, 注意力机制, 时间序列, 金融分析", "comments": "DeepSupp的创新之处在于其将多头注意力机制应用于动态时间序列的支撑与阻力位识别，这使得模型能够捕捉到传统方法难以发现的复杂市场相关性和微观结构。通过结合先进的特征工程和无监督聚类，该方法提供了一个可扩展且在多种市场条件下都表现稳定的解决方案，对于提升技术分析的准确性和自动化水平具有重要意义。"}}
{"id": "2507.02103", "title": "What Neuroscience Can Teach AI About Learning in Continuously Changing Environments", "authors": ["Daniel Durstewitz", "Bruno Averbeck", "Georgia Koppe"], "categories": ["cs.AI", "q-bio.NC", "I.2; I.6; A.1"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Submitted as a Perspective article (10 pages, 5 figures)", "url": "http://arxiv.org/abs/2507.02103v1", "summary": "Modern AI models, such as large language models, are usually trained once on\na huge corpus of data, potentially fine-tuned for a specific task, and then\ndeployed with fixed parameters. Their training is costly, slow, and gradual,\nrequiring billions of repetitions. In stark contrast, animals continuously\nadapt to the ever-changing contingencies in their environments. This is\nparticularly important for social species, where behavioral policies and reward\noutcomes may frequently change in interaction with peers. The underlying\ncomputational processes are often marked by rapid shifts in an animal's\nbehaviour and rather sudden transitions in neuronal population activity. Such\ncomputational capacities are of growing importance for AI systems operating in\nthe real world, like those guiding robots or autonomous vehicles, or for\nagentic AI interacting with humans online. Can AI learn from neuroscience? This\nPerspective explores this question, integrating the literature on continual and\nin-context learning in AI with the neuroscience of learning on behavioral tasks\nwith shifting rules, reward probabilities, or outcomes. We will outline an\nagenda for how specifically insights from neuroscience may inform current\ndevelopments in AI in this area, and - vice versa - what neuroscience may learn\nfrom AI, contributing to the evolving field of NeuroAI.", "comment": "Submitted as a Perspective article (10 pages, 5 figures)", "pdf_url": "http://arxiv.org/pdf/2507.02103v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "神经科学能教AI什么：关于在持续变化环境中学习的经验", "tldr": "本文探讨了神经科学如何为AI在持续变化环境中学习提供启发，以克服现代AI模型在适应性方面的局限性。", "motivation": "现代AI模型（如大型语言模型）训练成本高昂、耗时且参数固定，难以适应持续变化的环境。与此形成鲜明对比的是，动物能够持续适应环境变化，尤其是在社交物种中。这种适应能力对于在现实世界中运行的AI系统（如机器人、自动驾驶车辆或与人类在线互动的智能体AI）至关重要。因此，本文旨在探讨AI是否能从神经科学中学习。", "method": "本文是一篇“视角”文章，通过整合AI领域中的持续学习和情境学习文献，以及神经科学中关于规则、奖励概率或结果不断变化的认知任务学习文献，来探讨神经科学如何启发AI。", "result": "本文没有提供具体实验结果，而是在探讨神经科学如何为AI在持续变化环境中学习提供启发，并概述了一个议程，说明神经科学的见解如何为AI的发展提供信息，以及AI又能从神经科学中学到什么，从而促进神经AI领域的发展。", "conclusion": "神经科学的见解可以为AI在持续变化环境中的学习提供重要指导，反之，AI也可以为神经科学提供新的视角，共同推动神经AI（NeuroAI）这一新兴领域的发展。", "translation": "现代AI模型，如大型语言模型，通常在庞大的数据语料库上训练一次，可能针对特定任务进行微调，然后以固定参数部署。它们的训练成本高昂、速度慢且是渐进的，需要数十亿次重复。与此形成鲜明对比的是，动物能够持续适应环境中不断变化的偶发事件。这对于社会物种尤其重要，因为行为策略和奖励结果在与同伴互动时可能会频繁变化。其潜在的计算过程通常以动物行为的快速转变和神经元群体活动的突然过渡为标志。这种计算能力对于在现实世界中运行的AI系统（如引导机器人或自动驾驶车辆的系统，或与人类在线互动的智能体AI）变得越来越重要。AI能从神经科学中学习吗？本文探讨了这个问题，将AI中持续学习和情境学习的文献与神经科学中关于规则、奖励概率或结果不断变化的认知任务学习的文献相结合。我们将概述一个议程，具体说明神经科学的见解如何为AI在该领域的当前发展提供信息，反之亦然——神经科学可以从AI中学到什么，从而促进神经AI这一新兴领域的发展。", "summary": "本文探讨了神经科学如何为人工智能（AI）在持续变化环境中学习提供启发。现代AI模型训练成本高昂且适应性差，而动物能持续适应环境变化。文章整合了AI的持续学习和情境学习与神经科学中关于适应性学习的研究，旨在为AI系统在现实世界中运行提供灵感，并勾勒出神经科学如何影响AI以及AI如何反哺神经科学的研究议程，共同推动神经AI领域的发展。", "keywords": "神经科学, AI, 持续学习, 环境适应, 神经AI", "comments": "本文作为一篇“视角”文章，其创新之处在于提出了跨学科融合的路径，即通过神经科学的启发来解决当前AI在动态环境中适应性不足的痛点。其重要性体现在为AI在现实世界中的应用（如机器人和自动驾驶）提供了潜在的解决方案，并明确提出了“神经AI”这一新兴研究方向。然而，作为一篇综述性质的文章，它并未提供新的实验数据或具体的算法实现，而是侧重于提出问题和构想未来的研究议程。"}}
{"id": "2507.02212", "title": "SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers", "authors": ["Takuro Kawada", "Shunsuke Kitada", "Sota Nemoto", "Hitoshi Iyatomi"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      21 pages, 15 figures, 4 tables. Project Page: this https URL", "url": "http://arxiv.org/abs/2507.02212v1", "summary": "Graphical Abstracts (GAs) play a crucial role in visually conveying the key\nfindings of scientific papers. While recent research has increasingly\nincorporated visual materials such as Figure 1 as de facto GAs, their potential\nto enhance scientific communication remains largely unexplored. Moreover,\ndesigning effective GAs requires advanced visualization skills, creating a\nbarrier to their widespread adoption. To tackle these challenges, we introduce\nSciGA-145k, a large-scale dataset comprising approximately 145,000 scientific\npapers and 1.14 million figures, explicitly designed for supporting GA\nselection and recommendation as well as facilitating research in automated GA\ngeneration. As a preliminary step toward GA design support, we define two\ntasks: 1) Intra-GA recommendation, which identifies figures within a given\npaper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,\nwhich retrieves GAs from other papers to inspire the creation of new GAs. We\nprovide reasonable baseline models for these tasks. Furthermore, we propose\nConfidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation\nmetric that offers a fine-grained analysis of model behavior. CAR addresses\nlimitations in traditional ranking-based metrics by considering cases where\nmultiple figures within a paper, beyond the explicitly labeled GA, may also\nserve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a\nfoundation for advancing visual scientific communication while contributing to\nthe development of AI for Science.", "comment": "21 pages, 15 figures, 4 tables. Project Page:\n  https://iyatomilab.github.io/SciGA/", "pdf_url": "http://arxiv.org/pdf/2507.02212v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "SciGA：一个用于设计学术论文图形摘要的综合数据集", "tldr": "引入SciGA-145k数据集，用于支持图形摘要（GA）选择、推荐和自动化生成，并提出了相关任务、基线模型和新评估指标。", "motivation": "图形摘要（GAs）在视觉传达科学论文的关键发现方面发挥着至关重要的作用，但其增强科学交流的潜力在很大程度上仍未被探索。此外，设计有效的GA需要高级可视化技能，这对其广泛采用造成了障碍。", "method": "为解决现有挑战，本文引入了SciGA-145k，一个包含约14.5万篇科学论文和114万张图片的大规模数据集，专门用于支持GA选择、推荐以及促进自动化GA生成的研究。作为GA设计支持的初步步骤，论文定义了两个任务：内部GA推荐（识别论文内适合作为GA的图片）和交叉GA推荐（从其他论文检索GA以启发新GA的创建）。此外，本文为这些任务提供了合理的基线模型，并提出了置信度调整的top-1真实比率（CAR）这一新颖的推荐指标，旨在提供模型行为的细粒度分析，并解决传统排名指标的局限性。", "result": "本文提供了大规模的SciGA-145k数据集，定义了内部GA推荐和交叉GA推荐两个关键任务，并为这些任务提供了合理的基线模型。此外，还提出了一种新的推荐评估指标——置信度调整的top-1真实比率（CAR），以更精细地分析模型性能。", "conclusion": "SciGA-145k数据集通过统一相关任务和指标，为推进视觉科学交流奠定了基础，并为科学人工智能的发展做出了贡献。", "translation": "图形摘要（GAs）在视觉传达科学论文的关键发现方面发挥着至关重要的作用。尽管最近的研究越来越多地将图1等视觉材料作为事实上的GA，但它们增强科学交流的潜力在很大程度上仍未被探索。此外，设计有效的GA需要高级可视化技能，这对其广泛采用造成了障碍。为了解决这些挑战，我们引入了SciGA-145k，一个包含大约14.5万篇科学论文和114万张图片的大规模数据集，专门设计用于支持GA选择和推荐，以及促进自动化GA生成的研究。作为GA设计支持的初步步骤，我们定义了两个任务：1）内部GA推荐，即识别给定论文中适合作为GA的图片；2）交叉GA推荐，即从其他论文中检索GA以启发新GA的创建。我们为这些任务提供了合理的基线模型。此外，我们提出了置信度调整的top-1真实比率（CAR），这是一种新颖的推荐指标，可对模型行为进行细粒度分析。CAR通过考虑论文中除了明确标记的GA之外，多个图片也可能作为GA的情况，解决了传统基于排名的指标的局限性。通过统一这些任务和指标，我们的SciGA-145k为推进视觉科学交流奠定了基础，同时为科学人工智能的发展做出了贡献。", "summary": "本文介绍了SciGA-145k，一个包含14.5万篇论文和114万张图片的大规模数据集，旨在解决图形摘要（GA）设计门槛高和潜力未充分开发的问题。该数据集支持GA的选择、推荐和自动化生成研究。作者定义了内部GA和交叉GA推荐任务，并提供了基线模型。此外，论文还提出了一种新的推荐指标CAR，以更细致地评估模型性能，从而为视觉科学交流和科学人工智能的发展奠定基础。", "keywords": "图形摘要, 数据集, 科学交流, 推荐系统, 人工智能", "comments": "这篇论文通过构建一个大规模的、专门用于图形摘要（GA）设计和推荐的数据集，解决了当前科学交流中GA利用不足和设计困难的问题。其创新点在于首次系统地构建了如此规模的数据集，并定义了针对GA的推荐任务，提出了更符合实际场景的新评估指标CAR。这对于推动自动化GA生成和提升科学论文的可读性与传播效率具有重要意义，对AI for Science领域也提供了宝贵的资源。"}}
{"id": "2507.02305", "title": "Hybrid Satellite-Ground Deployments for Web3 DID: System Design and Performance Analysis", "authors": ["Yalin Liu", "Zhigang Yan", "Bingyuan Luo", "Xiaochi Xu", "Hong-Ning Dai", "Yaru Fu", "Bishenghui Tao", "Siu-Kei Au Yeung"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02305v1", "summary": "The emerging Web3 has great potential to provide worldwide decentralized\nservices powered by global-range data-driven networks in the future. To ensure\nthe security of Web3 services among diverse user entities, a decentralized\nidentity (DID) system is essential. Especially, a user's access request to Web3\nservices can be treated as a DID transaction within the blockchain, executed\nthrough a consensus mechanism. However, a critical implementation issue arises\nin the current Web3, i.e., how to deploy network nodes to serve users on a\nglobal scale. To address this issue, emerging Low Earth Orbit (LEO) satellite\ncommunication systems, such as Starlink, offer a promising solution. With their\nglobal coverage and high reliability, these communication satellites can\ncomplement terrestrial networks as Web3 deployment infrastructures. In this\ncase, this paper develops three hybrid satellite-ground modes to deploy the\nblockchain-enabled DID system for Web3 users. Three modes integrate ground\nnodes and satellites to provide flexible and continuous DID services for\nworldwide users. Meanwhile, to evaluate the effectiveness of the present hybrid\ndeployment modes, we analyze the complete DID consensus performance of\nblockchain on three hybrid satellite-ground modes. Moreover, we conduct\nnumerical and simulation experiments to verify the effectiveness of three\nhybrid satellite-ground modes. The impacts of various system parameters are\nthoroughly analyzed, providing valuable insights for implementing the worldwide\nWeb3 DID system in real-world network environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02305v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Web3 DID的混合卫星-地面部署：系统设计与性能分析", "tldr": "本文提出了三种混合卫星-地面模式来部署Web3去中心化身份（DID）系统，并分析了其性能。", "motivation": "当前Web3面临如何在全球范围内部署网络节点以服务用户的关键实现问题，尤其是在确保去中心化身份（DID）系统安全性和全球覆盖方面。低地球轨道（LEO）卫星通信系统如Starlink提供了解决方案。", "method": "论文开发了三种混合卫星-地面模式来部署支持区块链的DID系统，这些模式整合了地面节点和卫星以提供灵活连续的DID服务。通过数值和仿真实验，分析了在这些模式下区块链的DID共识性能，并彻底分析了各种系统参数的影响。", "result": "论文通过数值和仿真实验验证了三种混合卫星-地面模式的有效性，并深入分析了各种系统参数对Web3 DID系统实现的影响，为实际网络环境中的部署提供了有价值的见解。", "conclusion": "论文提出了有效的混合卫星-地面部署模式，并通过性能分析验证了其可行性，为全球Web3 DID系统的实现提供了指导。", "translation": "新兴的Web3在未来具有巨大潜力，可以通过全球范围的数据驱动网络提供全球去中心化服务。为了确保Web3服务在不同用户实体之间的安全性，去中心化身份（DID）系统至关重要。特别是，用户对Web3服务的访问请求可以被视为区块链内的DID事务，通过共识机制执行。然而，当前Web3中出现了一个关键的实现问题，即如何部署网络节点以在全球范围内服务用户。为了解决这个问题，新兴的低地球轨道（LEO）卫星通信系统（如Starlink）提供了一个有前景的解决方案。凭借其全球覆盖和高可靠性，这些通信卫星可以作为Web3部署基础设施补充地面网络。在这种情况下，本文开发了三种混合卫星-地面模式来为Web3用户部署支持区块链的DID系统。这三种模式整合了地面节点和卫星，为全球用户提供灵活和连续的DID服务。同时，为了评估当前混合部署模式的有效性，我们分析了区块链在三种混合卫星-地面模式下的完整DID共识性能。此外，我们进行了数值和仿真实验以验证三种混合卫星-地面模式的有效性。各种系统参数的影响得到了彻底分析，为在实际网络环境中实现全球Web3 DID系统提供了有价值的见解。", "summary": "本文旨在解决Web3在全球范围内节点部署的挑战，提出并设计了三种混合卫星-地面模式来部署基于区块链的去中心化身份（DID）系统。这些模式结合了地面网络和低地球轨道（LEO）卫星通信系统，以提供全球范围的灵活连续DID服务。通过对DID共识性能的分析以及数值和仿真实验，验证了这些混合部署模式的有效性，并探讨了系统参数的影响，为Web3 DID系统的实际部署提供了指导。", "keywords": "Web3, 去中心化身份 (DID), 卫星通信, 混合部署, 区块链", "comments": "这项研究通过结合卫星和地面网络来解决Web3全球部署的挑战，具有创新性。它为去中心化身份系统在全球范围内的扩展提供了实际可行的方案，对于Web3基础设施的未来发展具有重要意义。"}}
{"id": "2507.02352", "title": "Track-before-detect in RIS-aided Integrated Sensing and Communication", "authors": ["Georgios Mylonopoulos", "Luca Venturino", "Emanuele Grossi", "Stefano Buzzi", "Ciro D'Elia"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Accepted to the 33rd European Signal Processing Conference (EUSIPCO 2025), Isola delle Femmine, Palermo, Italy", "url": "http://arxiv.org/abs/2507.02352v1", "summary": "This study considers a base station equipped with sensing and communication\ncapabilities, which serves a ground user and scans a portion of the sky via a\npassive reconfigurable intelligent surface. To achieve more favorable system\ntradeoffs, we utilize a multi-frame radar detector, comprising a detector, a\nplot-extractor, and a track-before-detect processor. The main idea proposed\nhere is that user spectral efficiency can be enhanced by increasing the number\nof scans jointly processed by the multi-frame radar detector while maintaining\nthe same sensing performance. A numerical analysis is conducted to verify the\neffectiveness of the proposed solution and to evaluate the achievable system\ntradeoffs.", "comment": "Accepted to the 33rd European Signal Processing Conference (EUSIPCO\n  2025), Isola delle Femmine, Palermo, Italy", "pdf_url": "http://arxiv.org/pdf/2507.02352v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "RIS辅助的集成感知与通信中的检测前跟踪", "tldr": "本文提出在RIS辅助的集成感知与通信系统中，利用多帧雷达检测器（包含检测前跟踪）通过联合处理更多扫描来提高用户频谱效率，同时保持感知性能。数值分析验证了其有效性。", "motivation": "为了在配备感知和通信能力的基站（通过无源可重构智能表面服务地面用户并扫描天空）中实现更有利的系统权衡，特别是为了在保持相同感知性能的同时提高用户频谱效率。", "method": "利用了一个包含检测器、目标提取器和检测前跟踪处理器的多帧雷达检测器。主要思想是通过增加该检测器联合处理的扫描次数。通过数值分析验证了所提出解决方案的有效性并评估了可实现的系统权衡。", "result": "数值分析验证了所提出解决方案的有效性，并评估了可实现的系统权衡。", "conclusion": "在RIS辅助的集成感知与通信系统中，通过使用一个联合处理更多扫描的多帧雷达检测器，可以在保持感知性能的同时有效提高用户频谱效率。", "translation": "本研究考虑了一个配备感知和通信能力的基站，该基站通过无源可重构智能表面服务地面用户并扫描部分天空。为了实现更有利的系统权衡，我们利用了一个多帧雷达检测器，该检测器包含一个检测器、一个目标提取器和一个检测前跟踪处理器。本文提出的主要思想是，通过增加多帧雷达检测器联合处理的扫描次数，可以在保持相同感知性能的同时提高用户频谱效率。进行了数值分析以验证所提出解决方案的有效性并评估可实现的系统权衡。", "summary": "本研究探讨了RIS辅助的集成感知与通信系统，其中基站服务用户并通过RIS扫描天空。论文提出了一种包含检测前跟踪处理的多帧雷达检测器。核心思想是通过增加联合处理的扫描次数来提高用户频谱效率，同时不损害感知性能。数值分析验证了该解决方案的有效性并评估了系统权衡。", "keywords": "集成感知与通信, 可重构智能表面, 检测前跟踪, 多帧雷达检测器, 频谱效率", "comments": "本文的创新点在于将检测前跟踪（TBD）应用于RIS辅助的集成感知与通信（ISAC）框架中，并通过多帧处理来提高频谱效率，同时保持感知性能。这为解决ISAC系统中的关键权衡问题提供了一种有效方法。"}}
{"id": "2507.02755", "title": "Multi-agent Auditory Scene Analysis", "authors": ["Caleb Rascon", "Luis Gato-Diaz", "Eduardo García-Alarcón"], "categories": ["eess.AS", "cs.AI"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Submitted to Applied Intelligence", "url": "http://arxiv.org/abs/2507.02755v1", "summary": "Auditory scene analysis (ASA) aims to retrieve information from the acoustic\nenvironment, by carrying out three main tasks: sound source location,\nseparation, and classification. These tasks are traditionally executed with a\nlinear data flow, where the sound sources are first located; then, using their\nlocation, each source is separated into its own audio stream; from each of\nwhich, information is extracted that is relevant to the application scenario\n(audio event detection, speaker identification, emotion classification, etc.).\nHowever, running these tasks linearly increases the overall response time,\nwhile making the last tasks (separation and classification) highly sensitive to\nerrors of the first task (location). A considerable amount of effort and\ncomputational complexity has been employed in the state-of-the-art to develop\ntechniques that are the least error-prone possible. However, doing so gives\nrise to an ASA system that is non-viable in many applications that require a\nsmall computational footprint and a low response time, such as bioacoustics,\nhearing-aid design, search and rescue, human-robot interaction, etc. To this\neffect, in this work, a multi-agent approach is proposed to carry out ASA where\nthe tasks are run in parallel, with feedback loops between them to compensate\nfor local errors, such as: using the quality of the separation output to\ncorrect the location error; and using the classification result to reduce the\nlocalization's sensitivity towards interferences. The result is a multi-agent\nauditory scene analysis (MASA) system that is robust against local errors,\nwithout a considerable increase in complexity, and with a low response time.\nThe complete proposed MASA system is provided as a framework that uses\nopen-source tools for sound acquisition and reproduction (JACK) and inter-agent\ncommunication (ROS2), allowing users to add their own agents.", "comment": "Submitted to Applied Intelligence", "pdf_url": "http://arxiv.org/pdf/2507.02755v1", "cate": "eess.AS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "多智能体听觉场景分析", "tldr": "本文提出了一种多智能体听觉场景分析（MASA）方法，通过并行执行任务和引入反馈循环来克服传统线性ASA系统响应时间长和误差敏感的缺点，实现了鲁棒性高、响应时间低且复杂度未显著增加的系统。", "motivation": "传统的听觉场景分析（ASA）任务（定位、分离、分类）以线性方式执行，导致整体响应时间增加，且后续任务对初始定位错误高度敏感。这使得传统ASA系统在需要低计算开销和低响应时间的应用中不可行。", "method": "提出了一种多智能体方法来执行听觉场景分析（ASA），其中任务并行运行，并引入反馈循环以补偿局部错误，例如利用分离输出质量纠正定位错误，以及利用分类结果降低定位对干扰的敏感性。该系统作为一个框架提供，使用JACK进行声音采集和再现，ROS2进行智能体间通信，并支持用户添加自定义智能体。", "result": "所提出的多智能体听觉场景分析（MASA）系统对局部错误具有鲁棒性，响应时间低，且没有显著增加复杂性。", "conclusion": "本文提出了一种多智能体听觉场景分析（MASA）系统，通过并行处理任务和引入反馈循环，有效解决了传统线性ASA系统响应时间长和对误差敏感的问题，实现了鲁棒性高、响应时间低且复杂度适中的性能，适用于多种应用场景。该系统以开源框架形式提供。", "translation": "听觉场景分析 (ASA) 旨在通过执行三项主要任务：声源定位、分离和分类，从声学环境中检索信息。这些任务传统上以线性数据流执行，首先定位声源；然后，利用其位置，将每个声源分离成自己的音频流；从每个音频流中提取与应用场景相关的信息（音频事件检测、说话人识别、情感分类等）。然而，线性执行这些任务会增加整体响应时间，同时使最后的任务（分离和分类）对第一个任务（定位）的错误高度敏感。最先进的技术已经投入了大量的精力和计算复杂性来开发尽可能少出错的技术。然而，这样做会导致 ASA 系统在许多需要小计算占用和低响应时间的应用中不可行，例如生物声学、助听器设计、搜索和救援、人机交互等。为此，在这项工作中，提出了一种多智能体方法来执行 ASA，其中任务并行运行，并带有反馈循环以补偿局部错误，例如：使用分离输出的质量来纠正定位错误；以及使用分类结果来降低定位对干扰的敏感性。结果是一个多智能体听觉场景分析 (MASA) 系统，它能够抵抗局部错误，而不会显着增加复杂性，并且具有低响应时间。所提出的完整 MASA 系统作为一个框架提供，该框架使用开源工具进行声音采集和再现 (JACK) 和智能体间通信 (ROS2)，允许用户添加自己的智能体。", "summary": "本论文提出了一种多智能体听觉场景分析（MASA）方法，旨在解决传统线性ASA系统存在的响应时间长和对初始定位错误敏感的问题。MASA系统通过并行执行声源定位、分离和分类任务，并引入反馈循环来纠正局部错误，如利用分离质量改进定位，或利用分类结果降低定位对干扰的敏感度。实验结果表明，MASA系统在不显著增加复杂性的前提下，提高了对局部错误的鲁棒性，并实现了低响应时间，适用于计算资源有限的应用场景。该系统作为一个开源框架提供，支持用户自定义智能体。", "keywords": "听觉场景分析, 多智能体系统, 并行处理, 反馈循环, 声源定位", "comments": "该论文的创新之处在于将传统的线性顺序处理流程转变为并行的多智能体架构，并引入了反馈循环。这解决了传统ASA的关键限制，特别是误差传播和高延迟问题，使其更适用于实时和资源受限的应用。提供的开源框架也增强了系统的可扩展性。"}}
{"id": "2507.02668", "title": "MEGANet-W: A Wavelet-Driven Edge-Guided Attention Framework for Weak Boundary Polyp Detection", "authors": ["Zhe Yee Tan"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      7 pages, 3 figures", "url": "http://arxiv.org/abs/2507.02668v1", "summary": "Colorectal polyp segmentation is critical for early detection of colorectal\ncancer, yet weak and low contrast boundaries significantly limit automated\naccuracy. Existing deep models either blur fine edge details or rely on\nhandcrafted filters that perform poorly under variable imaging conditions. We\npropose MEGANet-W, a Wavelet Driven Edge Guided Attention Network that injects\ndirectional, parameter free Haar wavelet edge maps into each decoder stage to\nrecalibrate semantic features. Our two main contributions are: (1) a two-level\nHaar wavelet head for multi orientation edge extraction; and (2) Wavelet Edge\nGuided Attention (WEGA) modules that fuse wavelet cues with reverse and input\nbranches. On five public polyp datasets, MEGANetW consistently outperforms\nexisting methods, improving mIoU by up to 2.3% and mDice by 1.2%, while\nintroducing no additional learnable parameters.", "comment": "7 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.02668v1", "cate": "eess.IV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MEGANet-W：一种小波驱动的边缘引导注意力框架用于弱边界息肉检测", "tldr": "该论文提出了MEGANet-W，一种利用小波驱动边缘引导注意力的新型深度学习模型，旨在改善结直肠息肉的检测，特别是针对弱边界，其性能优于现有方法且未引入额外的可学习参数。", "motivation": "结直肠息肉分割对于结直肠癌的早期检测至关重要，但弱和低对比度边界显著限制了自动化精度。现有深度模型要么模糊精细的边缘细节，要么依赖于在可变成像条件下表现不佳的手工滤波器。", "method": "本文提出了MEGANet-W，一个小波驱动的边缘引导注意力网络。它将定向的、无参数的Haar小波边缘图注入每个解码器阶段以重新校准语义特征。主要贡献包括：(1) 一个用于多方向边缘提取的两级Haar小波头部；(2) 小波边缘引导注意力（WEGA）模块，将小波线索与反向和输入分支融合。", "result": "MEGANet-W在五个公共息肉数据集上持续优于现有方法，将mIoU提高了2.3%，mDice提高了1.2%，同时没有引入额外的可学习参数。", "conclusion": "MEGANet-W通过利用小波驱动的边缘引导有效解决了弱边界息肉检测的挑战，在不增加模型复杂性的前提下实现了卓越的性能。", "translation": "结直肠息肉分割对于结直肠癌的早期检测至关重要，然而弱和低对比度边界显著限制了自动化精度。现有深度模型要么模糊精细的边缘细节，要么依赖于在可变成像条件下表现不佳的手工滤波器。我们提出了MEGANet-W，一个小波驱动的边缘引导注意力网络，它将定向的、无参数的Haar小波边缘图注入每个解码器阶段以重新校准语义特征。我们的两个主要贡献是：(1) 一个用于多方向边缘提取的两级Haar小波头部；(2) 小波边缘引导注意力（WEGA）模块，将小波线索与反向和输入分支融合。在五个公共息肉数据集上，MEGANetW持续优于现有方法，将mIoU提高了2.3%，mDice提高了1.2%，同时没有引入额外的可学习参数。", "summary": "该论文提出了一种名为MEGANet-W的小波驱动边缘引导注意力网络，用于解决结直肠息肉分割中弱边界和低对比度导致的自动化精度受限问题。MEGANet-W通过将无参数的Haar小波边缘图注入解码器阶段来重新校准语义特征，并引入了两级Haar小波头部和小波边缘引导注意力（WEGA）模块。实验结果表明，MEGANet-W在多个公共息肉数据集上显著优于现有方法，提高了mIoU和mDice，且未增加可学习参数。", "keywords": "结直肠息肉分割, 小波, 边缘引导, 注意力网络, 弱边界检测", "comments": "MEGANet-W的创新之处在于其将无参数的Haar小波边缘图融入深度学习模型，以增强对弱边界的检测能力。这种方法避免了传统手工滤波器在不同成像条件下的局限性，并且没有引入额外的可学习参数，这对于模型的部署和效率非常有利。该研究对于提高结直肠癌早期检测的自动化精度具有重要意义。"}}
{"id": "2507.02861", "title": "LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans", "authors": ["Zhening Huang", "Xiaoyang Wu", "Fangcheng Zhong", "Hengshuang Zhao", "Matthias Nießner", "Joan Lasenby"], "categories": ["cs.CV", "cs.AI", "cs.GR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL ; Video: this https URL", "url": "http://arxiv.org/abs/2507.02861v1", "summary": "We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor\nenvironments into compact, realistic, and interactive 3D virtual replicas.\nLiteReality not only reconstructs scenes that visually resemble reality but\nalso supports key features essential for graphics pipelines -- such as object\nindividuality, articulation, high-quality physically based rendering materials,\nand physically based interaction. At its core, LiteReality first performs scene\nunderstanding and parses the results into a coherent 3D layout and objects with\nthe help of a structured scene graph. It then reconstructs the scene by\nretrieving the most visually similar 3D artist-crafted models from a curated\nasset database. Next, the Material Painting module enhances realism by\nrecovering high-quality, spatially varying materials. Finally, the\nreconstructed scene is integrated into a simulation engine with basic physical\nproperties to enable interactive behavior. The resulting scenes are compact,\neditable, and fully compatible with standard graphics pipelines, making them\nsuitable for applications in AR/VR, gaming, robotics, and digital twins. In\naddition, LiteReality introduces a training-free object retrieval module that\nachieves state-of-the-art similarity performance on the Scan2CAD benchmark,\nalong with a robust material painting module capable of transferring\nappearances from images of any style to 3D assets -- even under severe\nmisalignment, occlusion, and poor lighting. We demonstrate the effectiveness of\nLiteReality on both real-life scans and public datasets. Project page:\nhttps://litereality.github.io; Video:\nhttps://www.youtube.com/watch?v=ecK9m3LXg2c", "comment": "Project Page: https://litereality.github.io; Video:\n  https://www.youtube.com/watch?v=ecK9m3LXg2c&feature=youtu.be", "pdf_url": "http://arxiv.org/pdf/2507.02861v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LiteReality：基于RGB-D扫描的图形就绪型3D场景重建", "tldr": "LiteReality是一个将RGB-D扫描转换为紧凑、真实且交互式3D虚拟复制品的新颖管道，支持图形管道所需的核心特性。", "motivation": "目前的RGB-D扫描到3D场景重建方法可能缺乏对图形管道关键特性的支持，例如对象独立性、关节、高质量的物理渲染材质和基于物理的交互。本研究旨在提供一个能够生成紧凑、可编辑并与标准图形管道完全兼容的3D虚拟场景的解决方案。", "method": "LiteReality管道包括以下步骤：1. 场景理解，并借助结构化场景图将结果解析为连贯的3D布局和对象。2. 从精心策划的资产数据库中检索视觉上最相似的3D艺术家制作的模型以重建场景。3. 使用材质绘制模块恢复高质量、空间变化的材质以增强真实感。4. 将重建的场景集成到具有基本物理属性的模拟引擎中以实现交互行为。该方法还引入了无需训练的对象检索模块和鲁棒的材质绘制模块。", "result": "LiteReality生成的场景紧凑、可编辑，并与标准图形管道完全兼容，适用于AR/VR、游戏、机器人和数字孪生等应用。其无需训练的对象检索模块在Scan2CAD基准上实现了最先进的相似性性能，材质绘制模块能够在严重错位、遮挡和光照不足的情况下，将任何风格的图像外观转移到3D资产上。该方法在真实扫描和公共数据集上均表现出有效性。", "conclusion": "LiteReality提供了一个全面的管道，能够将RGB-D扫描转换为高质量、交互式的3D虚拟场景，这些场景不仅视觉上逼真，而且具备图形管道所需的核心功能，并在关键模块上达到了最先进的性能，极大地拓宽了3D重建的应用范围。", "translation": "我们提出了LiteReality，一个新颖的管道，可以将室内环境的RGB-D扫描转换为紧凑、真实且交互式的3D虚拟复制品。LiteReality不仅重建了视觉上与现实相似的场景，而且支持图形管道所必需的关键特性——例如对象独立性、关节、高质量的基于物理的渲染材质以及基于物理的交互。其核心是，LiteReality首先执行场景理解，并借助结构化场景图将结果解析为连贯的3D布局和对象。然后，它通过从精心策划的资产数据库中检索视觉上最相似的3D艺术家制作的模型来重建场景。接下来，材质绘制模块通过恢复高质量、空间变化的材质来增强真实感。最后，重建的场景被集成到具有基本物理属性的模拟引擎中，以实现交互行为。生成的场景紧凑、可编辑，并与标准图形管道完全兼容，使其适用于AR/VR、游戏、机器人和数字孪生等应用。此外，LiteReality引入了一个无需训练的对象检索模块，在Scan2CAD基准上实现了最先进的相似性性能，以及一个强大的材质绘制模块，能够将任何风格的图像外观转移到3D资产上——即使在严重的错位、遮挡和光照不足的情况下。我们在真实扫描和公共数据集上都证明了LiteReality的有效性。项目页面：https://litereality.github.io；视频：https://www.youtube.com/watch?v=ecK9m3LXg2c", "summary": "LiteReality是一种新颖的管道，能将RGB-D室内扫描转换为紧凑、逼真且交互式的3D虚拟场景。它通过场景理解、结构化场景图解析、资产数据库模型检索、高质量材质绘制以及集成物理属性实现。该系统支持对象独立性、关节和PBR材质，并兼容标准图形管道，适用于AR/VR、游戏、机器人等领域。其对象检索和材质绘制模块表现出色，达到SOTA水平。", "keywords": "RGB-D扫描, 3D场景重建, 图形管道, 物理渲染, 虚拟现实", "comments": "LiteReality的创新之处在于其端到端的管道设计，不仅关注视觉真实感，更重要的是集成了对图形管道至关重要的功能，如对象独立性、关节和PBR材质，使其生成的3D模型具有高度实用性。其无需训练的对象检索和鲁棒的材质绘制模块是亮点，尤其是在挑战性条件下仍能保持效果。这对于AR/VR、游戏和机器人等需要高质量、可交互3D资产的领域具有重要意义，克服了现有方法在图形兼容性方面的局限。"}}
{"id": "2507.01999", "title": "Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series", "authors": ["Bappaditya Dey", "Daniel Sorensen", "Minjin Hwang", "Sandip Halder"], "categories": ["cs.LG", "I.2.0; J.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      13 pages, 7 figures, submitted to IEEE Transactions on Semiconductor Manufacturing", "url": "http://arxiv.org/abs/2507.01999v1", "summary": "Semiconductor manufacturing is an extremely complex process, characterized by\nthousands of interdependent parameters collected across diverse tools and\nprocess steps. Multi-variate time-series (MTS) analysis has emerged as a\ncritical methodology for enabling real-time monitoring, fault detection, and\npredictive maintenance in such environments. However, anomaly prediction in\nsemiconductor fabrication presents several critical challenges, including high\ndata dimensionality, severe class imbalance due to the rarity of true faults,\nnoisy and missing measurements, and non-stationary behavior of production\nsystems. Furthermore, the complex interdependencies between variables and the\ndelayed emergence of faults across downstream stages complicate both anomaly\ndetection and root-cause-analysis. This paper presents a novel and generic\napproach for anomaly detection in MTS data using machine learning. The proposed\nmethodology consists of three main steps: a) converting MTS data into\nimage-based representations using the Continuous Wavelet Transform, b)\ndeveloping a multi-class image classifier by fine-tuning a pretrained VGG-16\narchitecture on custom CWT image datasets, and c) constructing a Siamese\nnetwork composed of two identical sub-networks, each utilizing the fine-tuned\nVGG-16 as a backbone. The network takes pairs of CWT images as input -one\nserving as a reference or anchor (representing a known-good signal), and the\nother as a query (representing an unknown signal). The model then compares the\nembeddings of both inputs to determine whether they belong to the same class at\na given time step. Our approach demonstrates high accuracy in identifying\nanomalies on a real FAB process time-series dataset, offering a promising\nsolution for offline anomaly detection in process and tool trace data.\nMoreover, the approach is flexible and can be applied in both supervised and\nsemi-supervised settings.", "comment": "13 pages, 7 figures, submitted to IEEE Transactions on Semiconductor\n  Manufacturing", "pdf_url": "http://arxiv.org/pdf/2507.01999v1", "cate": "cs.LG", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "基于连续小波变换和暹罗网络的多元半导体过程时间序列异常检测", "tldr": "本文提出了一种新颖的基于连续小波变换（CWT）和暹罗网络的机器学习方法，用于多元半导体过程时间序列中的异常检测，并在真实数据集上取得了高精度。", "motivation": "半导体制造过程复杂，存在高数据维度、类别不平衡、噪声、缺失测量和非平稳性等挑战，且变量间复杂依赖和下游故障延迟出现使得异常检测和根本原因分析困难。因此，需要一种有效的异常检测方法。", "method": "该方法包括三个主要步骤：a) 使用连续小波变换将多元时间序列数据转换为图像表示；b) 通过在自定义CWT图像数据集上微调预训练的VGG-16架构，开发一个多类别图像分类器；c) 构建一个暹罗网络，包含两个相同的子网络，每个子网络都使用微调后的VGG-16作为骨干，通过比较参考图像和查询图像的嵌入来判断是否属于同一类别。", "result": "该方法在真实的FAB过程时间序列数据集上展示了识别异常的高精度。", "conclusion": "该方法为离线异常检测提供了一个有前景的解决方案，并且灵活，可应用于有监督和半监督设置。", "translation": "半导体制造是一个极其复杂的过程，其特点是跨不同工具和工艺步骤收集的数千个相互依赖的参数。多元时间序列（MTS）分析已成为在此类环境中实现实时监控、故障检测和预测性维护的关键方法。然而，半导体制造中的异常预测面临几个关键挑战，包括高数据维度、由于真实故障的稀有性导致的严重类别不平衡、噪声和缺失测量，以及生产系统的非平稳行为。此外，变量之间复杂的相互依赖关系以及故障在下游阶段的延迟出现使得异常检测和根本原因分析都变得复杂。本文提出了一种新颖且通用的方法，用于使用机器学习在MTS数据中进行异常检测。所提出的方法包括三个主要步骤：a) 使用连续小波变换将MTS数据转换为基于图像的表示；b) 通过在自定义CWT图像数据集上微调预训练的VGG-16架构，开发一个多类别图像分类器；c) 构建一个由两个相同子网络组成的暹罗网络，每个子网络都使用微调后的VGG-16作为骨干。该网络以CWT图像对作为输入——一个作为参考或锚点（代表已知良好信号），另一个作为查询（代表未知信号）。然后，模型比较两个输入的嵌入，以确定它们在给定时间步是否属于同一类别。我们的方法在真实的FAB过程时间序列数据集上展示了识别异常的高精度，为过程和工具跟踪数据中的离线异常检测提供了一个有前景的解决方案。此外，该方法灵活，可应用于有监督和半监督设置。", "summary": "本文针对半导体制造中多元时间序列异常检测的挑战，提出了一种新颖的机器学习方法。该方法首先利用连续小波变换将MTS数据转换为图像，然后通过微调VGG-16构建图像分类器，最后设计一个基于VGG-16骨干的暹罗网络，通过比较CWT图像对的嵌入来识别异常。该方法在实际数据上表现出高精度，并适用于有监督和半监督场景，为离线异常检测提供了可行方案。", "keywords": "异常检测, 连续小波变换, 暹罗网络, 多元时间序列, 半导体制造", "comments": "本文的创新点在于结合了连续小波变换将时间序列数据转化为图像，并利用了预训练的VGG-16和暹罗网络进行异常检测，有效解决了半导体制造中高维度、类别不平衡等复杂问题。其通用性和在真实数据集上的高精度验证了该方法的实用价值。"}}
{"id": "2507.02381", "title": "Running-time Analysis of ($μ+λ$) Evolutionary Combinatorial Optimization Based on Multiple-gain Estimation", "authors": ["Min Huang", "Pengxiang Chen", "Han Huang", "Tongli He", "Yushan Zhang", "Zhifeng Hao"], "categories": ["cs.NE"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02381v1", "summary": "The running-time analysis of evolutionary combinatorial optimization is a\nfundamental topic in evolutionary computation. However, theoretical results\nregarding the $(\\mu+\\lambda)$ evolutionary algorithm (EA) for combinatorial\noptimization problems remain relatively scarce compared to those for simple\npseudo-Boolean problems. This paper proposes a multiple-gain model to analyze\nthe running time of EAs for combinatorial optimization problems. The proposed\nmodel is an improved version of the average gain model, which is a\nfitness-difference drift approach under the sigma-algebra condition to estimate\nthe running time of evolutionary numerical optimization. The improvement yields\na framework for estimating the expected first hitting time of a stochastic\nprocess in both average-case and worst-case scenarios. It also introduces novel\nrunning-time results of evolutionary combinatorial optimization, including two\ntighter time complexity upper bounds than the known results in the case of\n($\\mu+\\lambda$) EA for the knapsack problem with favorably correlated weights,\na closed-form expression of time complexity upper bound in the case of\n($\\mu+\\lambda$) EA for general $k$-MAX-SAT problems and a tighter time\ncomplexity upper bounds than the known results in the case of ($\\mu+\\lambda$)\nEA for the traveling salesperson problem. Experimental results indicate that\nthe practical running time aligns with the theoretical results, verifying that\nthe multiple-gain model is an effective tool for running-time analysis of\n($\\mu+\\lambda$) EA for combinatorial optimization problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02381v1", "cate": "cs.NE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于多增益估计的($\\mu+\\lambda$)进化组合优化运行时间分析", "tldr": "本文提出了一种多增益模型，以改进($\\mu+\\lambda$)进化算法在组合优化问题上的运行时间分析，并提供了更紧密或新的时间复杂度上界。", "motivation": "针对($\\mu+\\lambda$)进化算法在组合优化问题上的理论运行时间分析结果相对稀缺的问题。", "method": "提出了一种多增益模型，该模型是平均增益模型的改进版本，通过适应度差异漂移方法在西格玛代数条件下估计随机过程的期望首次命中时间，适用于平均情况和最坏情况。", "result": "引入了新的运行时间结果，包括：1. 对于有利相关权重的背包问题，获得了比已知结果更紧的($\\mu+\\lambda$)EA时间复杂度上界；2. 对于一般k-MAX-SAT问题，获得了($\\mu+\\lambda$)EA时间复杂度上界的闭式表达式；3. 对于旅行商问题，获得了比已知结果更紧的($\\mu+\\lambda$)EA时间复杂度上界。实验结果验证了多增益模型的有效性。", "conclusion": "多增益模型是分析($\\mu+\\lambda$)进化算法在组合优化问题上运行时间的有效工具。", "translation": "进化组合优化的运行时间分析是进化计算中的一个基本课题。然而，与简单的伪布尔问题相比，关于($\\mu+\\lambda$)进化算法（EA）在组合优化问题上的理论结果相对稀缺。本文提出了一种多增益模型来分析EA在组合优化问题上的运行时间。所提出的模型是平均增益模型的改进版本，它是一种在西格玛代数条件下通过适应度差异漂移方法估计进化数值优化运行时间的方法。这一改进提供了一个用于估计随机过程在平均情况和最坏情况下的期望首次命中时间的框架。它还引入了进化组合优化新的运行时间结果，包括：在($\\mu+\\lambda$)EA解决具有有利相关权重的背包问题时，获得了比已知结果更紧密的两个时间复杂度上界；在($\\mu+\\lambda$)EA解决一般k-MAX-SAT问题时，获得了时间复杂度上界的闭式表达式；在($\\mu+\\lambda$)EA解决旅行商问题时，获得了比已知结果更紧密的时间复杂度上界。实验结果表明，实际运行时间与理论结果一致，验证了多增益模型是分析($\\mu+\\lambda$)EA在组合优化问题上运行时间的有效工具。", "summary": "本文针对($\\mu+\\lambda$)进化算法在组合优化问题上缺乏理论运行时间分析的现状，提出了一种改进的多增益模型。该模型基于适应度差异漂移方法，能够估计随机过程在平均和最坏情况下的期望首次命中时间。研究获得了针对背包问题、k-MAX-SAT问题和旅行商问题的($\\mu+\\lambda$)EA更紧密或新的时间复杂度上界。实验验证了该模型的有效性。", "keywords": "进化算法, 运行时间分析, 组合优化, 多增益模型, ($\\mu+\\lambda$)EA", "comments": "这项工作通过提出多增益模型，显著推动了($\\mu+\\lambda$)进化算法在组合优化领域理论运行时间分析的进展。其创新性在于改进了现有的漂移分析技术，并为实际应用中的复杂问题提供了更精确的理论界限，弥补了该领域理论结果稀缺的不足。"}}
{"id": "2507.02139", "title": "When LLMs Disagree: Diagnosing Relevance Filtering Bias and Retrieval Divergence in SDG Search", "authors": ["William A. Ingram", "Bipasha Banerjee", "Edward A. Fox"], "categories": ["cs.IR", "cs.AI", "cs.DL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Presented at LLM4Eval Workshop, SIGIR 2025 Padova, Italy, July 17, 2025", "url": "http://arxiv.org/abs/2507.02139v1", "summary": "Large language models (LLMs) are increasingly used to assign document\nrelevance labels in information retrieval pipelines, especially in domains\nlacking human-labeled data. However, different models often disagree on\nborderline cases, raising concerns about how such disagreement affects\ndownstream retrieval. This study examines labeling disagreement between two\nopen-weight LLMs, LLaMA and Qwen, on a corpus of scholarly abstracts related to\nSustainable Development Goals (SDGs) 1, 3, and 7. We isolate disagreement\nsubsets and examine their lexical properties, rank-order behavior, and\nclassification predictability. Our results show that model disagreement is\nsystematic, not random: disagreement cases exhibit consistent lexical patterns,\nproduce divergent top-ranked outputs under shared scoring functions, and are\ndistinguishable with AUCs above 0.74 using simple classifiers. These findings\nsuggest that LLM-based filtering introduces structured variability in document\nretrieval, even under controlled prompting and shared ranking logic. We propose\nusing classification disagreement as an object of analysis in retrieval\nevaluation, particularly in policy-relevant or thematic search tasks.", "comment": "Presented at LLM4Eval Workshop, SIGIR 2025 Padova, Italy, July 17,\n  2025", "pdf_url": "http://arxiv.org/pdf/2507.02139v1", "cate": "cs.IR", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "当大型语言模型意见不一致时：诊断可持续发展目标搜索中的相关性过滤偏差和检索分歧", "tldr": "本研究发现，大型语言模型在文档相关性标注上的分歧并非随机，而是系统性的，并导致检索结果差异。建议将分类分歧作为检索评估的分析对象。", "motivation": "大型语言模型（LLMs）越来越多地用于信息检索管道中分配文档相关性标签，尤其是在缺乏人工标注数据的领域。然而，不同模型在边缘案例上常常意见不一致，引发了对此类分歧如何影响下游检索的担忧。", "method": "本研究考察了两个开源大型语言模型LLaMA和Qwen在与可持续发展目标（SDG）1、3和7相关的学术摘要语料库上的标签分歧。研究隔离了分歧子集，并检查了它们的词汇属性、排序行为和分类可预测性。", "result": "结果显示模型分歧是系统性的，而非随机的：分歧案例表现出一致的词汇模式，在共享评分函数下产生不同的排名靠前的输出，并且可以使用简单分类器以高于0.74的AUC值进行区分。", "conclusion": "这些发现表明，即使在受控的提示和共享的排名逻辑下，基于大型语言模型的过滤也会在文档检索中引入结构化变异。研究建议将分类分歧作为检索评估的分析对象，特别是在政策相关或主题搜索任务中。", "translation": "大型语言模型（LLMs）正越来越多地用于信息检索管道中分配文档相关性标签，尤其是在缺乏人工标注数据的领域。然而，不同模型在边缘案例上常常意见不一致，引发了对此类分歧如何影响下游检索的担忧。本研究考察了两个开源大型语言模型LLaMA和Qwen在与可持续发展目标（SDG）1、3和7相关的学术摘要语料库上的标签分歧。我们隔离了分歧子集，并检查了它们的词汇属性、排序行为和分类可预测性。我们的结果显示模型分歧是系统性的，而非随机的：分歧案例表现出一致的词汇模式，在共享评分函数下产生不同的排名靠前的输出，并且可以使用简单分类器以高于0.74的AUC值进行区分。这些发现表明，即使在受控的提示和共享的排名逻辑下，基于大型语言模型的过滤也会在文档检索中引入结构化变异。我们建议将分类分歧作为检索评估的分析对象，特别是在政策相关或主题搜索任务中。", "summary": "本研究探讨了大型语言模型（LLMs）在信息检索中分配文档相关性标签时出现的分歧。研究以LLaMA和Qwen为例，分析了它们在可持续发展目标相关学术摘要上的标注分歧，发现这种分歧并非随机，而是系统性的，表现出一致的词汇模式、导致检索结果差异，并可被有效预测。研究强调LLM过滤引入了结构化变异，并建议将模型间的分类分歧作为检索评估的关键分析点，尤其适用于政策或主题性搜索任务。", "keywords": "大型语言模型, 信息检索, 相关性过滤, 分歧诊断, 可持续发展目标", "comments": "这篇论文的创新点在于系统性地诊断了大型语言模型在文档相关性标注上的分歧，并揭示了这种分歧的非随机性和结构化特性。它强调了即使在受控环境下，LLM在信息检索中的应用也可能引入固有的偏差，这对于依赖LLM进行数据标注的领域具有重要意义。提出的将分类分歧作为检索评估对象的建议，为未来评估LLM在信息检索中的表现提供了一个新的视角和工具。"}}
{"id": "2507.02548", "title": "Bounded Weighted Edit Distance: Dynamic Algorithms and Matching Lower Bounds", "authors": ["Itai Boneh", "Egor Gorbachev", "Tomasz Kociumaka"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      ESA 2025", "url": "http://arxiv.org/abs/2507.02548v1", "summary": "The edit distance $ed(X,Y)$ of two strings $X,Y\\in \\Sigma^*$ is the minimum\nnumber of character edits (insertions, deletions, and substitutions) needed to\ntransform $X$ into $Y$. Its weighted counterpart $ed^w(X,Y)$ minimizes the\ntotal cost of edits, which are specified using a function $w$, normalized so\nthat each edit costs at least one. The textbook dynamic-programming procedure,\ngiven strings $X,Y\\in \\Sigma^{\\le n}$ and oracle access to $w$, computes\n$ed^w(X,Y)$ in $O(n^2)$ time. Nevertheless, one can achieve better running\ntimes if the computed distance, denoted $k$, is small: $O(n+k^2)$ for unit\nweights [Landau and Vishkin; JCSS'88] and $\\tilde{O}(n+\\sqrt{nk^3})$ for\narbitrary weights [Cassis, Kociumaka, Wellnitz; FOCS'23].\n  In this paper, we study the dynamic version of the weighted edit distance\nproblem, where the goal is to maintain $ed^w(X,Y)$ for strings $X,Y\\in\n\\Sigma^{\\le n}$ that change over time, with each update specified as an edit in\n$X$ or $Y$. Very recently, Gorbachev and Kociumaka [STOC'25] showed that the\nunweighted distance $ed(X,Y)$ can be maintained in $\\tilde{O}(k)$ time per\nupdate after $\\tilde{O}(n+k^2)$-time preprocessing; here, $k$ denotes the\ncurrent value of $ed(X,Y)$. Their algorithm generalizes to small integer\nweights, but the underlying approach is incompatible with large weights.\n  Our main result is a dynamic algorithm that maintains $ed^w(X,Y)$ in\n$\\tilde{O}(k^{3-\\gamma})$ time per update after $\\tilde{O}(nk^\\gamma)$-time\npreprocessing. Here, $\\gamma\\in [0,1]$ is a real trade-off parameter and $k\\ge\n1$ is an integer threshold fixed at preprocessing time, with $\\infty$ returned\nwhenever $ed^w(X,Y)>k$. We complement our algorithm with conditional lower\nbounds showing fine-grained optimality of our trade-off for $\\gamma \\in\n[0.5,1)$ and justifying our choice to fix $k$.", "comment": "ESA 2025", "pdf_url": "http://arxiv.org/pdf/2507.02548v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "有界加权编辑距离：动态算法和匹配下界", "tldr": "本文提出了一个动态算法，用于在字符串随时间变化时维护有界加权编辑距离，并提供了匹配的条件性下界，证明了其在特定参数范围内的最优性。", "motivation": "现有的编辑距离算法在距离较小时效率更高，但对于加权编辑距离的动态版本，尤其是在处理大权重时，缺乏有效的解决方案。先前的工作在无权重或小整数权重下可行，但其底层方法与大权重不兼容，因此需要一种新的方法来解决动态加权编辑距离问题。", "method": "论文提出了一种动态算法，用于维护加权编辑距离 $ed^w(X,Y)$。该算法在 $\tilde{O}(nk^\\gamma)$ 预处理时间后，以 $\tilde{O}(k^{3-\\gamma})$ 的每次更新时间维护 $ed^w(X,Y)$。其中，$\\gamma\\in [0,1]$ 是一个实数权衡参数，$k\\ge 1$ 是在预处理时固定的整数阈值，当 $ed^w(X,Y)>k$ 时返回无穷大。", "result": "论文成功开发了一个动态算法，能够在 $\tilde{O}(k^{3-\\gamma})$ 的更新时间内维护加权编辑距离，预处理时间为 $\tilde{O}(nk^\\gamma)$。此外，他们还给出了条件性下界，证明了其算法在 $\\gamma \\in [0.5,1)$ 范围内的权衡是细粒度最优的，并验证了固定 $k$ 的选择。", "conclusion": "本文成功解决了动态加权编辑距离的问题，提供了一个高效的算法，并通过匹配的下界证明了其在特定参数范围内的最优性，填补了现有研究在处理大权重动态编辑距离方面的空白。", "translation": "字符串 $X,Y\\in \\Sigma^*$ 的编辑距离 $ed(X,Y)$ 是将 $X$ 转换为 $Y$ 所需的最小字符编辑（插入、删除和替换）次数。其加权对应物 $ed^w(X,Y)$ 最小化了编辑的总成本，该成本使用函数 $w$ 指定，并经过归一化，使得每次编辑成本至少为一。教科书式的动态规划过程，给定字符串 $X,Y\\in \\Sigma^{\\le n}$ 和对 $w$ 的预言机访问，可以在 $O(n^2)$ 时间内计算 $ed^w(X,Y)$。然而，如果计算出的距离 $k$ 较小，则可以实现更好的运行时间：对于单位权重为 $O(n+k^2)$ [Landau and Vishkin; JCSS'88]，对于任意权重为 $\\tilde{O}(n+\\sqrt{nk^3})$ [Cassis, Kociumaka, Wellnitz; FOCS'23]。\n在本文中，我们研究了加权编辑距离问题的动态版本，目标是当字符串 $X,Y\\in \\Sigma^{\\le n}$ 随时间变化时，维护 $ed^w(X,Y)$，每次更新指定为 $X$ 或 $Y$ 中的一次编辑。最近，Gorbachev 和 Kociumaka [STOC'25] 表明，无权重距离 $ed(X,Y)$ 可以在 $\\tilde{O}(n+k^2)$ 预处理时间后，以 $\\tilde{O}(k)$ 的每次更新时间进行维护；这里，$k$ 表示 $ed(X,Y)$ 的当前值。他们的算法可以推广到小的整数权重，但其底层方法与大权重不兼容。\n我们的主要结果是一个动态算法，它在 $\\tilde{O}(nk^\\gamma)$ 预处理时间后，以 $\\tilde{O}(k^{3-\\gamma})$ 的每次更新时间维护 $ed^w(X,Y)$。其中，$\\gamma\\in [0,1]$ 是一个实数权衡参数，$k\\ge 1$ 是在预处理时固定的整数阈值，当 $ed^w(X,Y)>k$ 时返回无穷大。我们通过条件性下界补充了我们的算法，这些下界表明我们的权衡在 $\\gamma \\in [0.5,1)$ 范围内是细粒度最优的，并证明了我们选择固定 $k$ 的合理性。", "summary": "本文研究了加权编辑距离的动态维护问题，旨在解决现有方法不适用于大权重编辑距离的局限性。作者提出了一个在字符串随时间更新时维护有界加权编辑距离的动态算法。该算法在 $\\tilde{O}(nk^\\gamma)$ 预处理后，以 $\\tilde{O}(k^{3-\\gamma})$ 的时间复杂度进行每次更新，其中 $k$ 是一个预设的距离阈值，$\\gamma$ 是一个可调参数。此外，论文还通过提供条件性下界，证明了其算法在特定参数范围内的最优性，为动态加权编辑距离问题提供了重要进展。", "keywords": "加权编辑距离, 动态算法, 条件性下界, 字符串处理, 算法最优性", "comments": "这篇论文在动态加权编辑距离领域取得了重要突破，填补了现有算法无法有效处理大权重编辑距离的空白。其创新之处在于提出了一个具有可调参数的动态算法，能在预设距离阈值内高效维护加权编辑距离，并且通过匹配的条件性下界证明了算法的细粒度最优性，这在理论上具有重要意义。固定阈值 $k$ 的处理方式也使得算法更具实用性。"}}
{"id": "2507.02259", "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent", "authors": ["Hongli Yu", "Tinghong Chen", "Jiangtao Feng", "Jiangjie Chen", "Weinan Dai", "Qiying Yu", "Ya-Qin Zhang", "Wei-Ying Ma", "Jingjing Liu", "Mingxuan Wang", "Hao Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2507.02259v1", "summary": "Despite improvements by length extrapolation, efficient attention and memory\nmodules, handling infinitely long documents with linear complexity without\nperformance degradation during extrapolation remains the ultimate challenge in\nlong-text processing. We directly optimize for long-text tasks in an end-to-end\nfashion and introduce a novel agent workflow, MemAgent, which reads text in\nsegments and updates the memory using an overwrite strategy. We extend the DAPO\nalgorithm to facilitate training via independent-context multi-conversation\ngeneration. MemAgent has demonstrated superb long-context capabilities, being\nable to extrapolate from an 8K context trained on 32K text to a 3.5M QA task\nwith performance loss < 5% and achieves 95%+ in 512K RULER test.", "comment": "Project Page: https://memagent-sialab.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.02259v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MemAgent：通过多会话强化学习记忆代理重塑长上下文LLM", "tldr": "MemAgent通过基于RL的记忆代理，高效处理超长文本，性能损失极小。", "motivation": "尽管在长度外推、高效注意力机制和记忆模块方面有所改进，但在长文本处理中，以线性复杂度处理无限长文档且在推断时无性能下降仍然是终极挑战。", "method": "本文直接以端到端方式优化长文本任务，并引入了一种新颖的代理工作流MemAgent，它分段读取文本并使用覆盖策略更新内存。通过扩展DAPO算法，利用独立上下文多会话生成来促进训练。", "result": "MemAgent展示了卓越的长上下文能力，能够从8K上下文训练的32K文本外推到3.5M的QA任务，性能损失小于5%，并在512K RULER测试中达到95%以上的准确率。", "conclusion": "MemAgent有效解决了长上下文处理的挑战，并展示了出色的性能和强大的外推能力。", "translation": "尽管通过长度外推、高效注意力机制和记忆模块有所改进，但在长文本处理中，以线性复杂度处理无限长文档且在推断时无性能下降仍然是终极挑战。我们以端到端的方式直接优化长文本任务，并引入了一种新颖的代理工作流MemAgent，它分段读取文本并使用覆盖策略更新内存。我们扩展了DAPO算法，通过独立上下文多会话生成来促进训练。MemAgent展示了卓越的长上下文能力，能够从8K上下文训练的32K文本外推到3.5M的QA任务，性能损失小于5%，并在512K RULER测试中达到95%以上的准确率。", "summary": "本文提出了MemAgent，一种新颖的端到端代理工作流，旨在克服大型语言模型处理无限长文档的挑战。MemAgent通过分段读取文本并使用覆盖策略更新内存，并通过扩展DAPO算法、利用独立上下文多会话生成进行训练。它在长上下文外推方面表现出色，在3.5M QA任务上性能损失低于5%，并在512K RULER测试中达到95%以上的准确率。", "keywords": "长上下文LLM, 记忆代理, 强化学习, 文本处理, 外推", "comments": "本文的创新在于其提出的MemAgent代理工作流，它通过分段读取和覆盖式内存更新机制，结合基于RL的多会话训练，有效解决了LLM处理超长上下文的瓶颈。其在百万级别上下文长度上仍能保持低性能损失和高准确率，显示了其在实际应用中的巨大潜力。"}}
{"id": "2507.02391", "title": "Posterior Transition Modeling for Unsupervised Diffusion-Based Speech Enhancement", "authors": ["Mostafa Sadeghi", "Jean-Eudes Ayilo", "Romain Serizel", "Xavier Alameda-Pineda"], "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02391v1", "summary": "We explore unsupervised speech enhancement using diffusion models as\nexpressive generative priors for clean speech. Existing approaches guide the\nreverse diffusion process using noisy speech through an approximate,\nnoise-perturbed likelihood score, combined with the unconditional score via a\ntrade-off hyperparameter. In this work, we propose two alternative algorithms\nthat directly model the conditional reverse transition distribution of\ndiffusion states. The first method integrates the diffusion prior with the\nobservation model in a principled way, removing the need for hyperparameter\ntuning. The second defines a diffusion process over the noisy speech itself,\nyielding a fully tractable and exact likelihood score. Experiments on the\nWSJ0-QUT and VoiceBank-DEMAND datasets demonstrate improved enhancement metrics\nand greater robustness to domain shifts compared to both supervised and\nunsupervised baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02391v1", "cate": "cs.SD", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于无监督扩散模型的语音增强后验转移建模", "tldr": "本文提出了两种新的无监督扩散模型语音增强算法，通过直接建模条件反向转移分布，提高了性能和对域偏移的鲁棒性，解决了现有方法需超参数调优和似然分数近似的问题。", "motivation": "现有基于扩散模型的无监督语音增强方法在引导反向扩散过程时，依赖于近似的、受噪声扰动的似然分数，并需要通过超参数进行权衡，这带来了调整的复杂性。本文旨在解决这些局限性。", "method": "本文提出了两种替代算法，它们直接建模扩散状态的条件反向转移分布。第一种方法将扩散先验与观测模型以原则性的方式结合，消除了超参数调优的需要。第二种方法在带噪语音本身上定义了一个扩散过程，从而产生了一个完全可处理和精确的似然分数。", "result": "在WSJ0-QUT和VoiceBank-DEMAND数据集上的实验表明，与有监督和无监督基线相比，所提出的方法在增强指标上有所改善，并且对域偏移表现出更强的鲁棒性。", "conclusion": "本文提出的基于后验转移建模的无监督扩散模型语音增强方法是有效的，相较于现有方法，它在性能和对域偏移的鲁棒性方面均有所提升。", "translation": "我们探索使用扩散模型作为干净语音的表达性生成先验进行无监督语音增强。现有方法通过近似的、受噪声扰动的似然分数，并结合无条件分数（通过一个权衡超参数）来引导反向扩散过程使用带噪语音。在这项工作中，我们提出了两种替代算法，它们直接建模扩散状态的条件反向转移分布。第一种方法以原则性的方式将扩散先验与观测模型结合，消除了超参数调整的需要。第二种方法在带噪语音本身上定义了一个扩散过程，从而产生了一个完全可处理和精确的似然分数。在WSJ0-QUT和VoiceBank-DEMAND数据集上的实验表明，与有监督和无监督基线相比，增强指标有所改善，并且对域偏移的鲁棒性更强。", "summary": "本文针对无监督扩散模型语音增强中现有方法对近似似然分数和超参数调优的依赖，提出了两种新颖的算法。这些算法通过直接建模扩散状态的条件反向转移分布来改进。其中一种方法将扩散先验与观测模型结合，消除了超参数需求；另一种则在带噪语音上定义扩散过程，以获得精确的似然分数。实验证明，与现有基线相比，所提方法在增强效果和域偏移鲁棒性方面均有显著提升。", "keywords": "无监督语音增强, 扩散模型, 后验转移, 域偏移, 生成先验", "comments": "该论文通过直接建模后验转移分布，创新性地解决了现有扩散模型语音增强方法中超参数调优和似然分数近似的问题。其在域偏移鲁棒性上的提升，对于实际应用具有重要意义。"}}
{"id": "2507.02303", "title": "Measurements and Modeling of Air-Ground Integrated Channel in Forest Environment Based on OFDM Signals", "authors": ["Zhe Xiao", "Shu Sun", "Na Liu", "Lianming Xu", "Li Wang"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02303v1", "summary": "Forests are frequently impacted by climate conditions, vegetation density,\nand intricate terrain and geology, which contribute to natural disasters.\nPersonnel engaged in or supporting rescue operations in such environments rely\non robust communication systems to ensure their safety, highlighting the\ncriticality of channel measurements in forest environments. However, according\nto current research, there is limited research on channel detection and\nmodeling in forest areas in the existing literature. This paper describes the\nchannel measurements campaign of air and ground in the Arxan National Forest\nPark of Inner Mongolia. It presents measurement results and propagation models\nfor ground-to-ground (G2G) and air-to-ground (A2G) scenarios. The measurement\ncampaign uses orthogonal frequency division multiplexing signals centered at\n1.4 GHz for channel sounding. In the G2G measurement, in addition to using\nomnidirectional antennas to record data, we also use directional antennas to\nrecord the arrival angle information of the signal at the receiver. In the A2G\nmeasurement, we pre-plan the flight trajectory of the unmanned aerial vehicle\nso that it can fly at a fixed angle relative to the ground. We present path\nloss models suitable for G2G and A2G in forest environments based on the\nanalysis of measurement results. The results indicate that the proposed model\nreduces error margins compared with other path loss models. Furthermore, we\nderive the multipath model expression specific to forest environments and\nconduct statistical analysis on key channel parameters e.g., shadow fading\nfactor, root mean square delay spread, and Rician K factor. Our findings reveal\nthat signal propagation obstruction due to tree crowns in A2G communication is\nmore pronounced than tree trunk obstructions in G2G communication. Adjusting\nthe elevation angle between air and ground can enhance communication quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02303v1", "cate": "cs.IT", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于OFDM信号的森林环境空地一体化信道测量与建模", "tldr": "针对森林环境，本研究基于OFDM信号对空地（A2G）和地地（G2G）信道进行了实地测量与建模，提出了更准确的路径损耗和多径模型，并发现A2G通信中树冠的信号阻碍比G2G中树干更显著，调整仰角可提高通信质量。", "motivation": "在森林等复杂环境下，为确保救援人员安全，鲁棒的通信系统至关重要，但现有文献对森林区域的信道检测和建模研究有限。", "method": "在内蒙古阿尔山国家森林公园进行了空地一体化信道测量。测量使用中心频率为1.4 GHz的OFDM信号。地地（G2G）测量使用全向和定向天线记录信号到达角信息。空地（A2G）测量预先规划无人机飞行轨迹以保持与地面固定角度。基于测量结果分析，提出了适用于森林环境的G2G和A2G路径损耗模型，并推导了森林环境特有的多径模型表达式，对阴影衰落因子、均方根时延扩展和莱斯K因子等关键信道参数进行了统计分析。", "result": "提出了适用于森林环境的G2G和A2G路径损耗模型，该模型与现有模型相比误差裕度更小。推导了森林环境特有的多径模型表达式，并对阴影衰落因子、均方根时延扩展、莱斯K因子等关键信道参数进行了统计分析。研究发现，空地（A2G）通信中树冠对信号传播的阻碍比地地（G2G）通信中树干的阻碍更显著。", "conclusion": "调整空地之间的仰角可以提高通信质量。", "translation": "森林经常受到气候条件、植被密度以及错综复杂的地形地质的影响，这些因素都会导致自然灾害。在这种环境下从事或支持救援行动的人员依赖于强大的通信系统来确保他们的安全，这凸显了森林环境中信道测量的关键性。然而，根据目前的研究，现有文献中对森林区域的信道检测和建模研究有限。本文描述了在内蒙古阿尔山国家森林公园进行的空地信道测量活动。它提供了地对地（G2G）和空对地（A2G）场景的测量结果和传播模型。本次测量活动使用中心频率为1.4 GHz的正交频分复用（OFDM）信号进行信道探测。在地对地测量中，除了使用全向天线记录数据外，我们还使用定向天线记录接收器处信号的到达角信息。在空对地测量中，我们预先规划了无人机的飞行轨迹，使其能够以相对于地面的固定角度飞行。基于对测量结果的分析，我们提出了适用于森林环境中地对地和空对地的路径损耗模型。结果表明，与现有路径损耗模型相比，所提出的模型降低了误差裕度。此外，我们推导了森林环境特有的多径模型表达式，并对关键信道参数，例如阴影衰落因子、均方根时延扩展和莱斯K因子，进行了统计分析。我们的研究结果表明，空地通信中树冠对信号传播的阻碍比地地通信中树干的阻碍更显著。调整空地之间的仰角可以提高通信质量。", "summary": "本文针对森林环境通信的挑战，在内蒙古阿尔山国家森林公园进行了空地（A2G）和地地（G2G）信道测量，并基于1.4 GHz OFDM信号建立了传播模型。研究提出了误差更小的路径损耗模型和森林特有的多径模型，并对关键信道参数进行了统计分析。结果显示，A2G通信中树冠阻碍比G2G中树干阻碍更显著，且调整空地仰角可有效提升通信质量，为森林环境下的通信系统设计提供了重要依据。", "keywords": "森林环境, 信道测量, 空地通信, 地地通信, OFDM信号", "comments": "本文通过实地测量，填补了森林环境下空地一体化信道建模的空白，特别是在灾害救援通信方面具有重要意义。其创新之处在于结合了G2G和A2G场景，并详细分析了树木对信号传播的影响，为实际应用提供了具体的优化建议，例如调整仰角。研究成果对提升森林复杂环境下的通信可靠性具有指导价值。"}}
{"id": "2507.02390", "title": "Evaluating Language Models For Threat Detection in IoT Security Logs", "authors": ["Jorge J. Tejero-Fernández", "Alfonso Sánchez-Macián"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02390v1", "summary": "Log analysis is a relevant research field in cybersecurity as they can\nprovide a source of information for the detection of threats to networks and\nsystems. This paper presents a pipeline to use fine-tuned Large Language Models\n(LLMs) for anomaly detection and mitigation recommendation using IoT security\nlogs. Utilizing classical machine learning classifiers as a baseline, three\nopen-source LLMs are compared for binary and multiclass anomaly detection, with\nthree strategies: zero-shot, few-shot prompting and fine-tuning using an IoT\ndataset. LLMs give better results on multi-class attack classification than the\ncorresponding baseline models. By mapping detected threats to MITRE CAPEC,\ndefining a set of IoT-specific mitigation actions, and fine-tuning the models\nwith those actions, the models are able to provide a combined detection and\nrecommendation guidance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02390v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "评估用于物联网安全日志威胁检测的语言模型", "tldr": "本文提出使用微调的大型语言模型进行物联网安全日志分析，在多类别威胁检测和缓解建议方面优于基线模型。", "motivation": "网络安全中的日志分析是一个重要研究领域，可为网络和系统威胁检测提供信息来源。本文旨在利用物联网安全日志进行威胁检测和缓解。", "method": "本文提出一个管道，使用微调的大型语言模型（LLMs）对物联网安全日志进行异常检测和缓解建议。研究将三个开源LLMs与经典机器学习分类器作为基线进行比较，评估其在二分类和多分类异常检测中的表现，并采用零样本、少样本提示和微调三种策略。此外，通过将检测到的威胁映射到MITRE CAPEC，定义物联网特定的缓解措施，并用这些措施微调模型，以提供检测和建议指导。", "result": "大型语言模型在多类别攻击分类上取得了比相应基线模型更好的结果。通过将检测到的威胁映射到MITRE CAPEC并定义物联网特定的缓解措施进行微调后，模型能够提供结合了检测和建议的指导。", "conclusion": "本文提出的模型能够为物联网安全日志提供威胁检测和缓解建议。", "translation": "日志分析是网络安全领域的一个重要研究方向，因为它们可以为网络和系统威胁的检测提供信息来源。本文提出一个管道，利用微调的大型语言模型（LLMs）对物联网安全日志进行异常检测和缓解建议。研究以经典的机器学习分类器作为基线，比较了三个开源LLMs在二分类和多分类异常检测中的表现，并采用了零样本、少样本提示和使用物联网数据集进行微调这三种策略。大型语言模型在多类别攻击分类上取得了比相应基线模型更好的结果。通过将检测到的威胁映射到MITRE CAPEC，定义一套物联网特定的缓解措施，并用这些措施微调模型，模型能够提供结合了检测和建议的指导。", "summary": "本文提出一个利用微调大型语言模型（LLMs）进行物联网安全日志威胁检测和缓解建议的管道。研究将三个开源LLMs与经典机器学习基线模型进行比较，并采用了零样本、少样本和微调策略。结果表明，LLMs在多类别攻击分类方面优于基线模型。此外，通过整合MITRE CAPEC和物联网特定的缓解措施，模型能够提供威胁检测和可操作的建议。", "keywords": "物联网安全, 威胁检测, 大型语言模型, 日志分析, 缓解建议", "comments": "本文的创新之处在于将微调大型语言模型应用于物联网安全日志分析，不仅实现了威胁检测，还能基于MITRE CAPEC等既定框架提供自动化的缓解建议，这超越了传统的异常检测功能。"}}
{"id": "2507.02773", "title": "KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs", "authors": ["Yuzhang Xie", "Hejie Cui", "Ziyang Zhang", "Jiaying Lu", "Kai Shu", "Fadi Nahab", "Xiao Hu", "Carl Yang"], "categories": ["cs.AI", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02773v1", "summary": "Medical diagnosis prediction plays a critical role in disease detection and\npersonalized healthcare. While machine learning (ML) models have been widely\nadopted for this task, their reliance on supervised training limits their\nability to generalize to unseen cases, particularly given the high cost of\nacquiring large, labeled datasets. Large language models (LLMs) have shown\npromise in leveraging language abilities and biomedical knowledge for diagnosis\nprediction. However, they often suffer from hallucinations, lack structured\nmedical reasoning, and produce useless outputs. To address these challenges, we\npropose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves\nLLM-based diagnosis prediction through a multi-agent architecture. Our\nframework consists of a linkage agent for attribute mapping, a retrieval agent\nfor structured knowledge extraction, and a prediction agent that iteratively\nrefines diagnosis predictions. Experimental results demonstrate that KERAP\nenhances diagnostic reliability efficiently, offering a scalable and\ninterpretable solution for zero-shot medical diagnosis prediction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02773v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "KERAP：一种使用多智能体LLM进行准确零样本诊断预测的知识增强推理方法", "tldr": "KERAP提出了一种知识图谱增强的多智能体LLM方法，用于准确的零样本医学诊断预测，解决了传统ML模型对标注数据的依赖和LLM幻觉问题。", "motivation": "传统的机器学习模型在医学诊断预测中依赖监督训练，难以泛化到未见案例，且标注数据集获取成本高昂。大型语言模型（LLMs）虽在利用语言能力和生物医学知识方面显示出潜力，但常出现幻觉、缺乏结构化医学推理并产生无用输出。", "method": "我们提出了KERAP，一种知识图谱（KG）增强的推理方法，通过多智能体架构改进基于LLM的诊断预测。该框架包含一个用于属性映射的链接代理、一个用于结构化知识提取的检索代理，以及一个迭代细化诊断预测的预测代理。", "result": "实验结果表明，KERAP有效提高了诊断可靠性，为零样本医学诊断预测提供了可扩展且可解释的解决方案。", "conclusion": "KERAP通过结合知识图谱和多智能体LLM架构，成功解决了现有模型的局限性，为零样本医学诊断预测提供了一种高效、可靠、可扩展且可解释的新方法。", "translation": "医学诊断预测在疾病检测和个性化医疗中发挥着关键作用。尽管机器学习（ML）模型已被广泛应用于此任务，但它们对监督训练的依赖限制了其泛化到未见案例的能力，尤其是在获取大型标注数据集成本高昂的情况下。大型语言模型（LLMs）在利用语言能力和生物医学知识进行诊断预测方面显示出潜力。然而，它们常常遭受幻觉、缺乏结构化医学推理并产生无用输出。为了应对这些挑战，我们提出了KERAP，一种知识图谱（KG）增强的推理方法，通过多智能体架构改进基于LLM的诊断预测。我们的框架包括一个用于属性映射的链接代理、一个用于结构化知识提取的检索代理，以及一个迭代细化诊断预测的预测代理。实验结果表明，KERAP有效提高了诊断可靠性，为零样本医学诊断预测提供了可扩展且可解释的解决方案。", "summary": "KERAP是一种新颖的知识增强推理方法，利用多智能体大型语言模型（LLMs）进行准确的零样本医学诊断预测。该方法旨在解决传统机器学习模型在数据稀缺时的泛化问题以及LLMs存在的幻觉和缺乏结构化推理的挑战。KERAP框架包含链接、检索和预测三个代理，通过知识图谱增强LLM的推理能力，并迭代优化诊断结果。实验证明，KERAP能有效提高诊断的可靠性，并提供可扩展和可解释的解决方案。", "keywords": "零样本诊断预测, 多智能体LLM, 知识图谱, 医学诊断, 知识增强推理", "comments": "KERAP的创新之处在于将知识图谱与多智能体LLM架构相结合，为零样本医学诊断预测提供了一种结构化且可解释的推理方法。它有效地解决了传统ML模型对大量标注数据的依赖以及LLM在医学领域中常见的幻觉和缺乏逻辑推理的问题，为医疗AI领域带来了重要的进步。"}}
{"id": "2507.02182", "title": "Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models", "authors": ["Fangjian Lei", "Jiawen Liu", "Shayan Noei", "Ying Zou", "Derek Truong", "William Alexander"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02182v1", "summary": "Common Business Oriented Language (COBOL) is a programming language used to\ndevelop business applications that are widely adopted by financial, business,\nand government agencies. Due to its age, complexity, and declining number of\nCOBOL developers, maintaining COBOL codebases is becoming increasingly\nchallenging. In particular, the lack of documentation makes it difficult for\nnew developers to effectively understand and maintain COBOL systems. Existing\nresearch utilizes large language models (LLMs) to explain the functionality of\ncode snippets. However, COBOL presents unique challenges due to its\narchitectural and syntactical differences, which often cause its code to exceed\nthe token window size of LLMs. In this work, we propose a multi-agent approach\nthat leverages two LLM-based agents working collaboratively to generate\nexplanations for functions, files, and the overall project. These agents\nincorporate together by utilizing contextual information from the codebase into\nthe code explanation prompts. We evaluate the effectiveness of our approach\nusing 14 open-source, real-world COBOL projects. Our results indicate that our\napproach performs significantly better than the baseline in function code\nexplanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,\nchrF, and SentenceBERT scores, respectively. At the file level, our approach\neffectively explains both short and long COBOL files that exceed the token\nwindow size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in\nexplaining the purpose, functionality, and clarity of the generated\nexplanation. At the project level, our approach generates explanations that\nconvey the functionality and purpose of 82% of the selected projects.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02182v1", "cate": "cs.SE", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "增强COBOL代码解释：一种基于大型语言模型的多智能体方法", "tldr": "本文提出了一种多智能体方法，利用大型语言模型（LLMs）来增强COBOL代码的解释，以解决现有LLM在处理COBOL复杂性和长代码时面临的挑战，并在函数、文件和项目级别上取得了显著改进。", "motivation": "COBOL是一种广泛应用于金融、商业和政府机构的编程语言。由于其年代久远、复杂性高以及COBOL开发人员数量下降，维护COBOL代码库变得越来越困难。特别是，缺乏文档使得新开发人员难以有效理解和维护COBOL系统。现有研究利用大型语言模型（LLMs）来解释代码片段的功能，但COBOL的架构和语法差异导致其代码常常超出LLM的token窗口大小，这是一个独特的挑战。", "method": "本文提出了一种多智能体方法，利用两个基于LLM的智能体协同工作，为函数、文件和整个项目生成解释。这些智能体通过将代码库中的上下文信息整合到代码解释提示中来协同工作。", "result": "在函数代码解释方面，该方法在METEOR、chrF和SentenceBERT分数上分别比基线提高了12.67%、18.59%和0.62%。在文件级别，该方法有效解释了超出LLM token窗口大小的长短COBOL文件，并在解释目的、功能和生成解释的清晰度方面分别超越基线4.21%、10.72%和14.68%。在项目级别，该方法为82%的选定项目生成了传达其功能和目的的解释。", "conclusion": "本文提出的多智能体方法显著提高了COBOL代码解释的质量和覆盖范围，尤其是在处理超出LLM token窗口大小的复杂和长代码方面表现出色，有效解决了COBOL代码维护中的文档缺失问题。", "translation": "通用商业导向语言（COBOL）是一种用于开发商业应用程序的编程语言，被金融、商业和政府机构广泛采用。由于其年代久远、复杂性高以及COBOL开发人员数量下降，维护COBOL代码库变得越来越具有挑战性。特别是，缺乏文档使得新开发人员难以有效理解和维护COBOL系统。现有研究利用大型语言模型（LLMs）来解释代码片段的功能。然而，COBOL由于其架构和语法差异带来了独特的挑战，这常常导致其代码超出LLMs的token窗口大小。在这项工作中，我们提出了一种多智能体方法，利用两个基于LLM的智能体协同工作，为函数、文件和整个项目生成解释。这些智能体通过将代码库中的上下文信息整合到代码解释提示中来协同工作。我们使用14个开源的真实COBOL项目评估了我们方法的有效性。我们的结果表明，我们的方法在函数代码解释方面比基线表现显著更好，在METEOR、chrF和SentenceBERT分数上分别提高了12.67%、18.59%和0.62%。在文件级别，我们的方法有效解释了超出LLM token窗口大小的长短COBOL文件，并在解释目的、功能和生成解释的清晰度方面分别超越基线4.21%、10.72%和14.68%。在项目级别，我们的方法生成了能够传达82%选定项目功能和目的的解释。", "summary": "本文提出了一种基于大型语言模型（LLMs）的多智能体方法，旨在解决COBOL代码维护中因缺乏文档和现有LLM处理长COBOL代码能力不足的问题。该方法通过两个协同工作的LLM智能体，将代码库中的上下文信息整合到解释提示中，从而为COBOL函数、文件和整个项目生成解释。实验结果表明，与基线相比，该方法在函数、文件和项目级别的代码解释方面均取得了显著提升，尤其是在处理超出LLM token窗口大小的代码方面表现出色，有效提高了COBOL代码的可理解性和可维护性。", "keywords": "COBOL, 代码解释, 大型语言模型, 多智能体系统, 遗留系统", "comments": "该论文的创新点在于提出了一个多智能体框架来解决LLM在处理COBOL这种复杂且代码量大的遗留语言时面临的token窗口限制问题。通过智能体间的协作和上下文信息整合，有效提升了代码解释的质量和覆盖范围，对COBOL代码的现代化维护具有重要意义。该方法可以为其他面临类似挑战的遗留系统提供借鉴。"}}
{"id": "2507.02245", "title": "CoInfra: A Large-Scale Cooperative Infrastructure Perception System and Dataset in Adverse Weather", "authors": ["Minghao Ning", "Yufeng Yang", "Keqi Shu", "Shucheng Huang", "Jiaming Zhong", "Maryam Salehi", "Mahdi Rahmani", "Yukun Lu", "Chen Sun", "Aladdin Saleh", "Ehsan Hashemi", "Amir Khajepour"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This paper has been submitted to the IEEE Transactions on Robotics for review", "url": "http://arxiv.org/abs/2507.02245v1", "summary": "We present CoInfra, a large-scale cooperative infrastructure perception\nsystem and dataset designed to advance robust multi-agent perception under\nreal-world and adverse weather conditions. The CoInfra system includes 14 fully\nsynchronized sensor nodes, each equipped with dual RGB cameras and a LiDAR,\ndeployed across a shared region and operating continuously to capture all\ntraffic participants in real-time. A robust, delay-aware synchronization\nprotocol and a scalable system architecture that supports real-time data\nfusion, OTA management, and remote monitoring are provided in this paper. On\nthe other hand, the dataset was collected in different weather scenarios,\nincluding sunny, rainy, freezing rain, and heavy snow and includes 195k LiDAR\nframes and 390k camera images from 8 infrastructure nodes that are globally\ntime-aligned and spatially calibrated. Furthermore, comprehensive 3D bounding\nbox annotations for five object classes (i.e., car, bus, truck, person, and\nbicycle) are provided in both global and individual node frames, along with\nhigh-definition maps for contextual understanding. Baseline experiments\ndemonstrate the trade-offs between early and late fusion strategies, the\nsignificant benefits of HD map integration are discussed. By openly releasing\nour dataset, codebase, and system documentation at\nhttps://github.com/NingMingHao/CoInfra, we aim to enable reproducible research\nand drive progress in infrastructure-supported autonomous driving, particularly\nin challenging, real-world settings.", "comment": "This paper has been submitted to the IEEE Transactions on Robotics\n  for review", "pdf_url": "http://arxiv.org/pdf/2507.02245v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "CoInfra：一个大规模恶劣天气下的协作式基础设施感知系统和数据集", "tldr": "CoInfra是一个大规模协作式基础设施感知系统和数据集，旨在推进在真实世界和恶劣天气条件下的鲁棒多智能体感知。", "motivation": "该研究旨在推进在真实世界和恶劣天气条件下的鲁棒多智能体感知。", "method": "CoInfra系统包含14个完全同步的传感器节点，每个节点配备双RGB摄像头和激光雷达，部署在共享区域并持续运行以实时捕获所有交通参与者。该系统提供了一个鲁棒、延迟感知的同步协议和可扩展的系统架构，支持实时数据融合、OTA管理和远程监控。数据集在包括晴天、雨天、冻雨和暴雪在内的不同天气场景下收集，包含来自8个基础设施节点的19.5万个激光雷达帧和39万张相机图像，这些数据是全局时间对齐和空间校准的。此外，还提供了五种对象类别（汽车、公共汽车、卡车、行人、自行车）的全面3D边界框标注，包括全局和单个节点帧，以及用于上下文理解的高清地图。", "result": "基线实验展示了早期和晚期融合策略之间的权衡，并讨论了高清地图集成带来的显著益处。", "conclusion": "通过开放数据集、代码库和系统文档，该研究旨在实现可复现的研究，并推动基础设施辅助自动驾驶的进展，尤其是在具有挑战性的真实世界环境中。", "translation": "我们提出了CoInfra，一个大规模的协作式基础设施感知系统和数据集，旨在推进在真实世界和恶劣天气条件下的鲁棒多智能体感知。CoInfra系统包括14个完全同步的传感器节点，每个节点配备双RGB摄像头和激光雷达，部署在共享区域并持续运行以实时捕获所有交通参与者。本文提供了一个鲁棒、延迟感知的同步协议和可扩展的系统架构，支持实时数据融合、OTA管理和远程监控。另一方面，数据集在包括晴天、雨天、冻雨和暴雪在内的不同天气场景下收集，包含来自8个基础设施节点的19.5万个激光雷达帧和39万张相机图像，这些数据是全局时间对齐和空间校准的。此外，还提供了五种对象类别（即汽车、公共汽车、卡车、行人、自行车）的全面3D边界框标注，包括全局和单个节点帧，以及用于上下文理解的高清地图。基线实验展示了早期和晚期融合策略之间的权衡，并讨论了高清地图集成带来的显著益处。通过在https://github.com/NingMingHao/CoInfra公开我们的数据集、代码库和系统文档，我们旨在实现可复现的研究，并推动基础设施辅助自动驾驶的进展，特别是在具有挑战性的真实世界环境中。", "summary": "CoInfra是一个大规模协作式基础设施感知系统和数据集，旨在促进在恶劣天气条件下的鲁棒多智能体感知。该系统由14个同步传感器节点组成，支持实时数据融合和远程管理。数据集包含在多种天气条件下收集的激光雷达帧和相机图像，并提供详细的3D目标标注和高清地图。基线实验探讨了不同融合策略的权衡，并强调了高清地图的重要性。该工作通过开放资源，旨在推动基础设施辅助自动驾驶在复杂真实场景下的研究进展。", "keywords": "协作感知, 基础设施, 恶劣天气, 多智能体, 数据集", "comments": "CoInfra的创新之处在于其构建了一个大规模的协作式基础设施感知系统和数据集，特别关注在恶劣天气条件下的性能，这对于自动驾驶在实际复杂环境中的部署至关重要。其开放数据集、代码库和系统文档的举措，极大地促进了该领域的可复现研究和进步。"}}
{"id": "2507.02187", "title": "VergeIO: Depth-Aware Eye Interaction on Glasses", "authors": ["Xiyuxing Zhang", "Duc Vu", "Chengyi Shen", "Yuntao Wang", "Yuanchun Shi", "Justin Chan"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02187v1", "summary": "There is growing industry interest in creating unobtrusive designs for\nelectrooculography (EOG) sensing of eye gestures on glasses (e.g. JINS MEME and\nApple eyewear). We present VergeIO, the first EOG-based glasses that enables\ndepth-aware eye interaction using vergence with an optimized electrode layout\nand novel smart glass prototype. It can distinguish between four and six\ndepth-based eye gestures with 83-98% accuracy using personalized models in a\nuser study across 11 users and 1,320 gesture instances. It generalizes to\nunseen users with an accuracy of 80-98% without any calibration. To reduce\nfalse detections, we incorporate a motion artifact detection pipeline and a\npreamble-based activation scheme. The system uses dry sensors without any\nadhesives or gel, and operates in real time with 3 mW power consumption by the\nsensing front-end, making it suitable for always-on sensing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02187v1", "cate": "cs.HC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "VergeIO：眼镜上的深度感知眼部交互", "tldr": "VergeIO是一种基于EOG的智能眼镜，能实现深度感知的眼部交互，具有高准确性、低功耗和无需校准的特点。", "motivation": "行业对在眼镜上实现不显眼的眼电图（EOG）眼动手势传感日益增长的兴趣，例如JINS MEME和Apple眼镜，促使研究人员开发出能够进行深度感知眼部交互的系统。", "method": "VergeIO是一种基于EOG的智能眼镜，通过辐辏（vergence）实现深度感知眼部交互。它采用了优化的电极布局和新型智能眼镜原型。为减少误检，系统整合了运动伪影检测管线和基于前导码的激活方案。系统使用干式传感器，无需粘合剂或凝胶。", "result": "VergeIO在11名用户和1,320个手势实例的用户研究中，使用个性化模型可区分四到六种基于深度的眼部手势，准确率为83-98%。对未见用户，在无需任何校准的情况下，其准确率可达80-98%。传感前端的功耗为3毫瓦，可实时操作。", "conclusion": "VergeIO是首款基于EOG的深度感知眼部交互眼镜，具有高准确性、低功耗和无需校准的特点，且采用干式传感器，使其非常适合常开式传感应用。", "translation": "行业对在眼镜上创建不显眼的眼电图（EOG）眼动手势传感设计（例如JINS MEME和Apple眼镜）的兴趣日益增长。我们提出了VergeIO，这是首款基于EOG的眼镜，它通过使用辐辏以及优化的电极布局和新型智能眼镜原型，实现了深度感知的眼部交互。在对11名用户和1,320个手势实例的用户研究中，它使用个性化模型能够以83-98%的准确率区分四到六种基于深度的眼部手势。对于未见过的用户，在没有任何校准的情况下，其准确率可达80-98%。为了减少误检，我们整合了运动伪影检测管线和基于前导码的激活方案。该系统使用不带任何粘合剂或凝胶的干式传感器，并以3毫瓦的传感前端功耗实时运行，使其适用于常开式传感。", "summary": "VergeIO是一种创新的EOG智能眼镜，首次实现了深度感知的眼部交互。它利用辐辏原理，结合优化的电极布局和新型原型，能高精度区分多种深度眼部手势。系统通过运动伪影检测和前导码激活减少误报，并采用干式传感器，具有低功耗和无需校准的优点，非常适合日常常开式使用。", "keywords": "EOG, 深度感知, 眼部交互, 智能眼镜, 辐辏", "comments": "该论文提出了一种创新的EOG智能眼镜系统VergeIO，其主要创新点在于首次实现了基于辐辏的深度感知眼部交互，这在现有EOG眼镜中是一个显著进步。使用干式传感器、低功耗（3mW）和无需校准的特性，极大地提升了系统的实用性和用户体验，使其更接近商业化应用。该研究对于未来可穿戴设备中的自然用户界面和无障碍交互具有重要意义。"}}
{"id": "2507.02226", "title": "DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs", "authors": ["Mohammad Akyash", "Kimia Azar", "Hadi Kamali"], "categories": ["cs.PL", "cs.AR", "cs.LG"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      Accepted to the International Conference on Computer-Aided Design (ICCAD 2025)", "url": "http://arxiv.org/abs/2507.02226v1", "summary": "As one of their many applications, large language models (LLMs) have recently\nshown promise in automating register transfer level (RTL) code generation.\nHowever, conventional LLM decoding strategies, originally designed for natural\nlanguage, often fail to meet the structural and semantic demands of RTL,\nleading to hallucinated, repetitive, or invalid code outputs. In this paper, we\nfirst investigate the root causes of these decoding failures through an\nempirical analysis of token-level entropy during RTL generation. Our findings\nreveal that LLMs exhibit low confidence in regions of structural ambiguity or\nsemantic complexity, showing that standard decoding strategies fail to\ndifferentiate between regions requiring determinism (syntax-critical regions)\nand those that benefit from creative exploratory variability (design-critical\nregions). Then, to overcome this, we introduce DecoRTL, a novel run-time\ndecoding strategy, that is both syntax-aware and contrastive for RTL code\ngeneration. DecoRTL integrates two complementary components: (i)\nself-consistency sampling, which generates multiple candidates and re-ranks\nthem based on token-level agreement to promote correctness while maintaining\ndiversity; and (ii) syntax-aware temperature adaptation, which classifies\ntokens by their syntactical and functional roles and adjusts the sampling\ntemperature accordingly, enforcing low temperature for syntax-critical tokens\nand higher temperature for exploratory ones. Our approach operates entirely at\ninference time without requiring any additional model fine-tuning. Through\nevaluations on multiple open-source LLMs using the VerilogEval benchmark, we\ndemonstrate significant improvements in syntactic validity, functional\ncorrectness, and output diversity, while the execution overhead (performance\noverhead) is imperceptible.", "comment": "Accepted to the International Conference on Computer-Aided Design\n  (ICCAD 2025)", "pdf_url": "http://arxiv.org/pdf/2507.02226v1", "cate": "cs.PL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DecoRTL：一种用于LLM生成RTL代码的运行时解码框架", "tldr": "DecoRTL是一种用于LLM生成RTL代码的运行时解码框架，通过结合自洽性采样和语法感知温度适应，解决了传统解码策略在RTL代码生成中导致的幻觉、重复和无效问题，显著提高了代码的有效性、正确性和多样性，且无明显性能开销。", "motivation": "大型语言模型（LLMs）在自动化寄存器传输级（RTL）代码生成方面显示出潜力。然而，为自然语言设计的传统LLM解码策略无法满足RTL的结构和语义需求，导致生成的代码出现幻觉、重复或无效。研究发现LLMs在结构模糊或语义复杂的区域置信度低，且标准解码策略未能区分需要确定性（语法关键）和需要探索性（设计关键）的区域。", "method": "本文引入了DecoRTL，一种新颖的运行时解码策略，它既语法感知又具有对比性。DecoRTL包含两个互补组件：(i) 自洽性采样，生成多个候选并根据token级一致性重新排序，以促进正确性并保持多样性；(ii) 语法感知温度适应，根据token的语法和功能角色调整采样温度，对语法关键token强制低温度，对探索性token强制高温度。该方法完全在推理时操作，无需额外模型微调。", "result": "通过在VerilogEval基准测试上对多个开源LLM进行评估，DecoRTL显著改善了RTL代码的语法有效性、功能正确性和输出多样性，同时执行开销（性能开销）可以忽略不计。", "conclusion": "DecoRTL通过创新的运行时解码策略，有效解决了大型语言模型在RTL代码生成中遇到的挑战，显著提高了代码质量，并且不引入显著的计算开销，证明了其在特定领域代码生成中的实用性和有效性。", "translation": "作为众多应用之一，大型语言模型（LLMs）最近在自动化寄存器传输级（RTL）代码生成方面显示出潜力。然而，传统LLM解码策略，最初为自然语言设计，往往无法满足RTL的结构和语义需求，导致代码输出出现幻觉、重复或无效。在本文中，我们首先通过对RTL生成过程中token级别熵的经验分析，调查了这些解码失败的根本原因。我们的发现表明，LLMs在结构模糊或语义复杂的区域表现出低置信度，这表明标准解码策略未能区分需要确定性（语法关键区域）和受益于创造性探索性变异（设计关键区域）的区域。然后，为了克服这一点，我们引入了DecoRTL，一种新颖的运行时解码策略，它在RTL代码生成中既语法感知又具有对比性。DecoRTL集成了两个互补的组件：(i) 自洽性采样，它生成多个候选并根据token级别的一致性重新排序它们，以促进正确性同时保持多样性；以及 (ii) 语法感知温度适应，它根据token的语法和功能角色对它们进行分类，并相应地调整采样温度，对语法关键token强制低温度，对探索性token强制高温度。我们的方法完全在推理时操作，无需任何额外的模型微调。通过使用VerilogEval基准测试对多个开源LLM进行评估，我们证明了在语法有效性、功能正确性以及输出多样性方面的显著改进，而执行开销（性能开销）则可以忽略不计。", "summary": "本文介绍了DecoRTL，一个用于大型语言模型（LLMs）生成寄存器传输级（RTL）代码的运行时解码框架。针对现有解码策略在RTL代码生成中出现的幻觉、重复和无效问题，DecoRTL通过经验分析揭示了LLMs在结构或语义复杂区域置信度低的原因。该框架包含自洽性采样和语法感知温度适应两个核心组件，能够在推理时动态调整解码行为，区分语法关键和设计关键区域。实验结果表明，DecoRTL显著提升了RTL代码的语法有效性、功能正确性和多样性，且性能开销可忽略不计。", "keywords": "RTL代码生成, 大型语言模型, 解码策略, 运行时框架, 硬件描述语言", "comments": "DecoRTL的创新之处在于其运行时、无需微调的解码策略，特别是结合了自洽性采样和语法感知温度适应，有效解决了LLM在生成结构化代码时面临的挑战。它通过区分代码中不同区域的确定性需求，提供了一种更精细的控制，这对于提高LLM在特定领域（如硬件描述语言）的应用质量具有重要意义。其低开销的特点也增强了实用性。"}}
{"id": "2507.02021", "title": "REDUS: Adaptive Resampling for Efficient Deep Learning in Centralized and Federated IoT Networks", "authors": ["Eyad Gad", "Gad Gad", "Mostafa M. Fouda", "Mohamed I. Ibrahem", "Muhammad Ismail", "Zubair Md Fadlullah"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      2025 International Conference on Communications", "url": "http://arxiv.org/abs/2507.02021v1", "summary": "With the rise of Software-Defined Networking (SDN) for managing traffic and\nensuring seamless operations across interconnected devices, challenges arise\nwhen SDN controllers share infrastructure with deep learning (DL) workloads.\nResource contention between DL training and SDN operations, especially in\nlatency-sensitive IoT environments, can degrade SDN's responsiveness and\ncompromise network performance. Federated Learning (FL) helps address some of\nthese concerns by decentralizing DL training to edge devices, thus reducing\ndata transmission costs and enhancing privacy. Yet, the computational demands\nof DL training can still interfere with SDN's performance, especially under the\ncontinuous data streams characteristic of IoT systems. To mitigate this issue,\nwe propose REDUS (Resampling for Efficient Data Utilization in Smart-Networks),\na resampling technique that optimizes DL training by prioritizing misclassified\nsamples and excluding redundant data, inspired by AdaBoost. REDUS reduces the\nnumber of training samples per epoch, thereby conserving computational\nresources, reducing energy consumption, and accelerating convergence without\nsignificantly impacting accuracy. Applied within an FL setup, REDUS enhances\nthe efficiency of model training on resource-limited edge devices while\nmaintaining network performance. In this paper, REDUS is evaluated on the\nCICIoT2023 dataset for IoT attack detection, showing a training time reduction\nof up to 72.6% with a minimal accuracy loss of only 1.62%, offering a scalable\nand practical solution for intelligent networks.", "comment": "2025 International Conference on Communications", "pdf_url": "http://arxiv.org/pdf/2507.02021v1", "cate": "cs.NI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "REDUS：集中式和联邦式物联网网络中高效深度学习的自适应重采样", "tldr": "REDUS是一种基于AdaBoost的自适应重采样技术，通过优先处理误分类样本和排除冗余数据来优化物联网网络中的深度学习训练，显著减少训练时间并保持高精度。", "motivation": "当SDN控制器与深度学习工作负载共享基础设施时，资源争用导致SDN响应性和网络性能下降，尤其是在延迟敏感的IoT环境中。即使在联邦学习中，深度学习训练的计算需求仍然会干扰SDN性能，尤其是在连续数据流下。", "method": "提出REDUS（智能网络中高效数据利用的重采样），一种受AdaBoost启发的重采样技术。REDUS通过优先处理误分类样本和排除冗余数据来优化深度学习训练，减少每epoch的训练样本数量，从而节省计算资源、降低能耗并加速收敛，同时对精度影响最小。在联邦学习设置中，REDUS提高了资源受限边缘设备的模型训练效率。", "result": "在CICIoT2023数据集上进行物联网攻击检测评估，REDUS将训练时间减少了高达72.6%，精度损失仅为1.62%。", "conclusion": "REDUS为智能网络提供了一个可扩展且实用的解决方案，通过高效的重采样技术显著优化了物联网环境中的深度学习训练。", "translation": "随着软件定义网络（SDN）的兴起，用于管理流量和确保互联设备之间的无缝操作，当SDN控制器与深度学习（DL）工作负载共享基础设施时，挑战随之而来。DL训练和SDN操作之间的资源争用，尤其是在延迟敏感的物联网环境中，可能会降低SDN的响应能力并损害网络性能。联邦学习（FL）通过将DL训练去中心化到边缘设备，从而降低数据传输成本并增强隐私，有助于解决其中一些问题。然而，DL训练的计算需求仍然可能干扰SDN的性能，尤其是在物联网系统特有的连续数据流下。为了缓解这个问题，我们提出了REDUS（智能网络中高效数据利用的重采样），这是一种受AdaBoost启发的重采样技术，通过优先处理误分类样本和排除冗余数据来优化DL训练。REDUS减少了每epoch的训练样本数量，从而节省了计算资源，降低了能耗，并加速了收敛，而对精度没有显著影响。在FL设置中应用时，REDUS提高了资源受限边缘设备上的模型训练效率，同时保持了网络性能。在本文中，REDUS在CICIoT2023数据集上进行物联网攻击检测评估，结果显示训练时间减少了高达72.6%，而精度损失仅为1.62%，为智能网络提供了一个可扩展且实用的解决方案。", "summary": "本文提出REDUS，一种受AdaBoost启发的自适应重采样技术，旨在解决物联网网络中深度学习训练与SDN操作之间的资源争用问题。REDUS通过优先处理误分类样本和排除冗余数据来优化DL训练，从而减少训练样本量，节省计算资源，降低能耗并加速模型收敛，同时保持高精度。实验证明，在物联网攻击检测任务中，REDUS可将训练时间缩短高达72.6%，精度损失仅为1.62%，为集中式和联邦式物联网环境中的高效深度学习提供实用解决方案。", "keywords": "重采样, 深度学习, 物联网, 联邦学习, 资源优化", "comments": "REDUS的创新之处在于将AdaBoost的思想应用于深度学习的样本重采样，以应对资源受限的IoT环境中的挑战。其通过减少训练样本量来显著提高效率，同时保持了高精度，这对于实际部署在边缘设备上的AI应用至关重要。该方法在FL场景下的应用也提升了其实用性。"}}
{"id": "2507.02620", "title": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference", "authors": ["Xing Liu", "Lizhuo Luo", "Ming Tang", "Chao Huang"], "categories": ["cs.DC", "cs.AI"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      16 pages, and the last 3 are appendix", "url": "http://arxiv.org/abs/2507.02620v1", "summary": "Distributed inference serves as a promising approach to enabling the\ninference of large language models (LLMs) at the network edge. It distributes\nthe inference process to multiple devices to ensure that the LLMs can fit into\nthe device memory. Recent pipeline-based approaches have the potential to\nparallelize communication and computation, which helps reduce inference\nlatency. However, the benefit diminishes when the inference request at the\nnetwork edge is sparse, where pipeline is typically at low utilization. To\nenable efficient distributed LLM inference at the edge, we propose\n\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\nframework. FlowSpec incorporates three key mechanisms to improve decoding\nefficiency: 1) score-based step-wise verification prioritizes more important\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\nprune invalid tokens while maintaining correct causal relationship during\nverification; 3) dynamic draft expansion strategies to supply high-quality\nspeculative inputs. These techniques work in concert to enhance both pipeline\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\ntestbed with other baselines. Experimental results demonstrate that our\nproposed framework significantly improves inference speed across diverse models\nand configurations, achieving speedup ratios 1.36$\\times$-1.77$\\times$ compared\nto baselines. Our code is publicly available at\n\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}", "comment": "16 pages, and the last 3 are appendix", "pdf_url": "http://arxiv.org/pdf/2507.02620v1", "cate": "cs.DC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "FlowSpec: 用于高效分布式LLM推理的连续流水线推测解码", "tldr": "FlowSpec是一个流水线并行、基于树的推测解码框架，通过三个关键机制，显著提高了分布式LLM推理的速度，尤其是在边缘网络稀疏请求场景下。", "motivation": "分布式LLM推理在网络边缘面临挑战，尤其是在推理请求稀疏时，现有流水线方法的利用率低，导致效率降低。", "method": "提出FlowSpec，一个流水线并行、基于树的推测解码框架。它包含三个关键机制：1) 基于分数的逐步验证，优先处理重要草稿token以提前接受；2) 高效的草稿管理，在验证过程中修剪无效token并保持正确的因果关系；3) 动态草稿扩展策略，提供高质量的推测输入。这些技术协同工作以提高流水线利用率和推测效率。", "result": "FlowSpec在真实测试平台上进行了评估，与基线相比，在不同模型和配置下显著提高了推理速度，实现了1.36倍至1.77倍的加速比。", "conclusion": "FlowSpec通过创新的流水线推测解码方法，有效解决了分布式LLM推理在边缘网络低利用率的问题，显著提升了推理速度。", "translation": "分布式推理是实现网络边缘大型语言模型（LLM）推理的一种有前景的方法。它将推理过程分配到多个设备上，以确保LLM能够适应设备内存。最近基于流水线的方法有潜力并行化通信和计算，这有助于减少推理延迟。然而，当网络边缘的推理请求稀疏时，这种优势会减弱，此时流水线通常处于低利用率状态。为了在边缘实现高效的分布式LLM推理，我们提出了FlowSpec，一个流水线并行、基于树的推测解码框架。FlowSpec包含了三个关键机制来提高解码效率：1) 基于分数的逐步验证，优先处理更重要的草稿token以提前接受；2) 高效的草稿管理，在验证过程中修剪无效token同时保持正确的因果关系；3) 动态草稿扩展策略，提供高质量的推测输入。这些技术协同工作，以提高流水线利用率和推测效率。我们在真实世界的测试平台上使用其他基线评估了FlowSpec。实验结果表明，我们提出的框架在不同模型和配置下显著提高了推理速度，与基线相比，实现了1.36倍至1.77倍的加速比。我们的代码已在https://github.com/Leosang-lx/FlowSpec# 公开。", "summary": "FlowSpec是一个为分布式LLM推理设计的连续流水线推测解码框架，旨在解决边缘网络稀疏请求导致的流水线利用率低的问题。它通过引入分数基逐步验证、高效草稿管理和动态草稿扩展策略三大机制，有效提升了流水线利用率和推测效率。实验结果表明，FlowSpec在多种模型和配置下，相较于基线，能显著提高推理速度，加速比达到1.36倍至1.77倍。", "keywords": "分布式LLM推理, 流水线推测解码, FlowSpec, 边缘计算, 推理加速", "comments": "该论文创新性地将流水线并行与树形推测解码结合，并通过精巧的机制（如分数基验证和动态草稿扩展）解决了分布式LLM推理在边缘网络中请求稀疏时的效率瓶颈。其提出的解决方案具有很强的实用价值，代码开源也促进了相关研究的进一步发展。"}}
{"id": "2507.02189", "title": "On the Design of Corrugated Boards: A New FEM Modeling and Experimental Validation", "authors": ["Ricardo Fitas", "Heinz Joachim Schaffrath", "Samuel Schabel"], "categories": ["physics.app-ph", "cs.CE", "physics.comp-ph"], "primary_category": "Subjects:       Applied Physics (physics.app-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02189v1", "summary": "This study presents a simplified FEM modeling approach suitable for large\nstructures made of corrugated boards, such as customized packages, based on a\nhomogenization method, which is combined with correction factors for internal\nmechanisms. The homogenization process reduces computational time by\ntransforming flute geometries into equivalent elastic models. In large\ndeformations and in the presence of contact for a given geometry, the effective\nelastic modulus in the thickness direction, as well as the effective thickness\nof the structure, are corrected by two statistical Weibull distributions\nrepresenting the contact and buckling mechanisms in a corrugated board. The\nWeibull parameters are obtained via experimental analysis, and such a process\nis then validated. The results demonstrate that the statistical parameters\n($\\beta_1 = 0.14$, $\\beta_2 = 1.31$) can be used for the simplistic\nrepresentation of corrugated boards, being computationally efficient. This\nresearch contributes to the optimization of corrugated packaging design,\nspecifically by simplifying FEM models for faster yet equally accurate\nsimulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02189v1", "cate": "physics.app-ph", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "瓦楞纸板设计：一种新的有限元建模与实验验证", "tldr": "本研究提出了一种简化的有限元建模方法，结合均质化和校正因子，用于瓦楞纸板大型结构的模拟，并通过实验验证了其计算效率和准确性，有助于优化包装设计。", "motivation": "本研究的动机是为了解决瓦楞纸板大型结构（如定制包装）在有限元模拟中计算时间过长的问题，通过简化模型来提高模拟效率和准确性，从而优化瓦楞包装设计。", "method": "本研究提出了一种简化的有限元建模方法，该方法基于均质化方法，并结合了内部机制的校正因子。均质化过程将瓦楞几何形状转换为等效弹性模型以减少计算时间。在大变形和接触存在的情况下，通过两个代表瓦楞纸板中接触和屈曲机制的统计威布尔分布来校正厚度方向的有效弹性模量和结构的有效厚度。威布尔参数通过实验分析获得并进行了验证。", "result": "结果表明，统计参数（$\\beta_1 = 0.14$，$\\beta_2 = 1.31$）可用于瓦楞纸板的简化表示，并且计算效率高。该方法能够实现更快但同样准确的模拟。", "conclusion": "本研究提出了一种计算效率高且准确的瓦楞纸板有限元简化建模方法，并经过实验验证，为瓦楞包装设计的优化提供了贡献。", "translation": "本研究提出了一种适用于瓦楞纸板大型结构（如定制包装）的简化有限元建模方法，该方法基于均质化方法，并结合了内部机制的校正因子。均质化过程通过将瓦楞几何形状转换为等效弹性模型来减少计算时间。在大变形和给定几何形状存在接触的情况下，厚度方向的有效弹性模量以及结构的有效厚度通过两个代表瓦楞纸板中接触和屈曲机制的统计威布尔分布进行校正。威布尔参数通过实验分析获得，并且该过程得到了验证。结果表明，统计参数（$\\beta_1 = 0.14$，$\\beta_2 = 1.31$）可用于瓦楞纸板的简化表示，具有计算效率。这项研究有助于优化瓦楞包装设计，特别是通过简化有限元模型以实现更快但同样准确的模拟。", "summary": "本研究提出了一种针对瓦楞纸板大型结构（如包装）的简化有限元建模方法。该方法结合了均质化处理和基于威布尔分布的校正因子，以有效模拟大变形和接触情况下的力学行为。通过实验验证，确定了关键统计参数，证明了该模型在保证精度的同时显著提高了计算效率，从而有助于优化瓦楞包装设计。", "keywords": "瓦楞纸板, 有限元建模, 均质化, 威布尔分布, 包装设计", "comments": "这项研究的创新之处在于将均质化方法与统计威布尔分布相结合，用于瓦楞纸板的有限元建模，尤其是在处理大变形和接触方面。其重要性在于提供了一种计算高效且准确的模拟工具，能够显著加速瓦楞包装的设计优化过程。"}}
{"id": "2507.02152", "title": "The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies", "authors": ["Disa Sariola", "Patrick Button", "Aron Culotta", "Nicholas Mattei"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02152v1", "summary": "Artificial intelligence systems, especially those using machine learning, are\nbeing deployed in domains from hiring to loan issuance in order to automate\nthese complex decisions. Judging both the effectiveness and fairness of these\nAI systems, and their human decision making counterpart, is a complex and\nimportant topic studied across both computational and social sciences. Within\nmachine learning, a common way to address bias in downstream classifiers is to\nresample the training data to offset disparities. For example, if hiring rates\nvary by some protected class, then one may equalize the rate within the\ntraining set to alleviate bias in the resulting classifier. While simple and\nseemingly effective, these methods have typically only been evaluated using\ndata obtained through convenience samples, introducing selection bias and label\nbias into metrics. Within the social sciences, psychology, public health, and\nmedicine, audit studies, in which fictitious ``testers'' (e.g., resumes,\nemails, patient actors) are sent to subjects (e.g., job openings, businesses,\ndoctors) in randomized control trials, provide high quality data that support\nrigorous estimates of discrimination. In this paper, we investigate how data\nfrom audit studies can be used to improve our ability to both train and\nevaluate automated hiring algorithms. We find that such data reveals cases\nwhere the common fairness intervention method of equalizing base rates across\nclasses appears to achieve parity using traditional measures, but in fact has\nroughly 10% disparity when measured appropriately. We additionally introduce\ninterventions based on individual treatment effect estimation methods that\nfurther reduce algorithmic discrimination using this data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02152v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "公平的幻象：使用审计研究审计公平干预措施", "tldr": "本文揭示了传统AI公平干预措施在适当衡量时仍存在偏见，并提出了基于审计研究数据的新方法来减少算法歧视。", "motivation": "人工智能系统在自动化复杂决策中应用广泛，但评估其有效性和公平性是一个复杂且重要的问题。现有的机器学习公平性干预措施（如重采样）通常只使用便利样本进行评估，引入了选择偏差和标签偏差，导致对公平性指标的误判。", "method": "本文研究了如何利用审计研究（一种在社会科学中用于评估歧视的随机对照试验方法）的数据来改进自动化招聘算法的训练和评估。此外，还引入了基于个体治疗效果估计方法的干预措施。", "result": "研究发现，常见的公平干预方法（如通过均衡基础比率实现视在的公平）在使用传统度量时看似达到了公平，但通过适当测量，实际上仍存在约10%的差异。此外，基于个体治疗效果估计方法的干预措施进一步减少了算法歧视。", "conclusion": "传统的公平性度量可能无法准确反映AI系统的真实公平性，通过利用高质量的审计研究数据和先进的干预方法，可以更有效地识别并减少算法歧视。", "translation": "人工智能系统，尤其是使用机器学习的系统，正被部署在从招聘到贷款发放等领域，以自动化这些复杂的决策。评估这些AI系统及其人类决策对应物的有效性和公平性，是计算科学和社会科学领域研究的一个复杂而重要的话题。在机器学习中，解决下游分类器偏差的一种常见方法是重新采样训练数据以抵消差异。例如，如果招聘率因受保护类别而异，那么可以在训练集中均衡比率，以减轻所得分类器中的偏差。尽管这些方法简单且看似有效，但它们通常只使用通过便利样本获得的数据进行评估，从而在指标中引入了选择偏差和标签偏差。在社会科学、心理学、公共卫生和医学领域，审计研究——其中虚构的“测试者”（例如简历、电子邮件、病人演员）被发送给受试者（例如职位空缺、企业、医生）进行随机对照试验——提供了高质量的数据，支持对歧视的严格估计。在本文中，我们研究了如何利用审计研究的数据来提高我们训练和评估自动化招聘算法的能力。我们发现，这些数据揭示了在某些情况下，常见的公平干预方法（即跨类别均衡基础比率）在使用传统度量时似乎实现了平等，但实际上在适当测量时存在大约10%的差异。我们还引入了基于个体治疗效果估计方法的干预措施，利用这些数据进一步减少了算法歧视。", "summary": "该论文探讨了人工智能系统公平性评估的局限性，指出当前机器学习中常用的公平干预措施（如数据重采样）可能因使用便利样本而导致评估偏差。作者提出利用社会科学中的审计研究数据来更准确地训练和评估自动化招聘算法。研究发现，传统方法在适当衡量时仍存在约10%的公平性差异，并引入了基于个体治疗效果估计的新干预措施，以有效减少算法歧视。", "keywords": "AI公平性, 审计研究, 算法歧视, 公平干预, 个体治疗效果", "comments": "本文的创新之处在于将社会科学中严谨的审计研究方法引入到人工智能公平性评估中，揭示了传统公平干预措施的“公平幻象”。通过使用高质量的真实世界数据和提出基于个体治疗效果估计的新干预措施，为更准确地识别和减少算法歧视提供了重要的视角和有效工具，对AI伦理和负责任AI的发展具有重要意义。"}}
{"id": "2507.02217", "title": "Understanding Trade offs When Conditioning Synthetic Data", "authors": ["Brandon Trabucco", "Qasim Wani", "Benjamin Pikus", "Vasu Sharma"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02217v1", "summary": "Learning robust object detectors from only a handful of images is a critical\nchallenge in industrial vision systems, where collecting high quality training\ndata can take months. Synthetic data has emerged as a key solution for data\nefficient visual inspection and pick and place robotics. Current pipelines rely\non 3D engines such as Blender or Unreal, which offer fine control but still\nrequire weeks to render a small dataset, and the resulting images often suffer\nfrom a large gap between simulation and reality. Diffusion models promise a\nstep change because they can generate high quality images in minutes, yet\nprecise control, especially in low data regimes, remains difficult. Although\nmany adapters now extend diffusion beyond plain text prompts, the effect of\ndifferent conditioning schemes on synthetic data quality is poorly understood.\nWe study eighty diverse visual concepts drawn from four standard object\ndetection benchmarks and compare two conditioning strategies: prompt based and\nlayout based. When the set of conditioning cues is narrow, prompt conditioning\nyields higher quality synthetic data; as diversity grows, layout conditioning\nbecomes superior. When layout cues match the full training distribution,\nsynthetic data raises mean average precision by an average of thirty four\npercent and by as much as one hundred seventy seven percent compared with using\nreal data alone.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02217v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "理解合成数据条件化时的权衡", "tldr": "本文研究了扩散模型在低数据量下生成合成数据时，不同条件化策略（基于提示与基于布局）对数据质量和目标检测性能的影响。结果显示，基于布局的条件化在数据多样性高时表现更优，能显著提升目标检测的平均精度。", "motivation": "工业视觉系统面临高质量训练数据收集耗时且在低数据量下难以训练鲁棒目标检测器的挑战。现有3D引擎生成合成数据慢且与现实存在差距。扩散模型虽能快速生成，但在低数据量下精确控制困难，且不同条件化方案对合成数据质量的影响尚不明确。", "method": "研究了来自四个标准目标检测基准的八十种不同视觉概念，并比较了两种条件化策略：基于提示的（prompt based）和基于布局的（layout based）。", "result": "当条件提示集较窄时，基于提示的条件化生成更高质量的合成数据；当多样性增加时，基于布局的条件化表现更优。当布局提示与完整的训练分布匹配时，合成数据使平均精度（mAP）平均提高了34%，最高可达177%，相比仅使用真实数据。", "conclusion": "在低数据量场景下，选择合适的扩散模型条件化策略对于生成高质量合成数据至关重要。尤其是基于布局的条件化，在多样性场景下能显著提升目标检测模型的性能。", "translation": "理解合成数据条件化时的权衡\n\n仅用少量图像学习鲁棒的目标检测器是工业视觉系统中的一个关键挑战，因为收集高质量的训练数据可能需要数月。合成数据已成为数据高效视觉检测和抓取机器人技术的一个关键解决方案。当前的流水线依赖于Blender或Unreal等3D引擎，它们提供精细控制，但仍需要数周才能渲染少量数据集，并且生成的图像通常存在模拟与现实之间的巨大差距。扩散模型有望带来质的飞跃，因为它们可以在几分钟内生成高质量图像，但在低数据量情况下，精确控制仍然困难。尽管许多适配器现在将扩散模型扩展到纯文本提示之外，但不同条件化方案对合成数据质量的影响却知之甚少。我们研究了来自四个标准目标检测基准的八十种不同视觉概念，并比较了两种条件化策略：基于提示的和基于布局的。当条件提示集较窄时，基于提示的条件化产生更高质量的合成数据；随着多样性的增长，基于布局的条件化变得更优越。当布局提示与完整的训练分布匹配时，与仅使用真实数据相比，合成数据使平均精度（mAP）平均提高了34%，最高可达177%。", "summary": "本文探讨了在低数据量场景下，使用扩散模型生成合成数据时，不同条件化策略对数据质量和目标检测性能的影响。研究比较了基于提示和基于布局的两种条件化方法，发现基于提示的方法在条件提示范围窄时表现更优，而基于布局的方法在条件提示多样性高时更具优势。实验结果表明，当布局提示与训练分布充分匹配时，合成数据能显著提升目标检测的平均精度。", "keywords": "合成数据, 扩散模型, 条件化策略, 目标检测, 数据稀缺", "comments": "这篇论文通过系统比较两种扩散模型条件化策略，为在数据稀缺场景下有效利用合成数据提供了重要指导。其创新点在于量化了不同条件化方案对合成数据质量和下游任务性能的影响，特别强调了基于布局的条件化在多样性场景下的优越性。这项工作对于推动工业视觉和机器人领域中合成数据的实际应用具有重要意义。"}}
{"id": "2507.02325", "title": "Grid-Connected, Data-Driven Inverter Control, Theory to Hardware", "authors": ["Sebastian Graf", "Keith Moffat", "Anurag Mohapatra", "Alessandro Chiuso", "Florian Dörfler"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02325v1", "summary": "Grid-connected inverter control is challenging to implement due to the\ndifficulty of obtaining and maintaining an accurate grid model. Direct\nData-Driven Predictive Control provides a model-free alternative to traditional\nmodel-based control methods. This paper describes how the recently-proposed\nTransient Predictive Control (TPC) can be used for real-world, plug-and-play\ninverter control. The following hypotheses were tested: 1) The TPC algorithm\ncan be run online using standard hardware, and 2) TPC, which is derived using\nLinear Time-Invariant assumptions, is effective for grid-connected inverter\ncontrol, which is a nonlinear and time-varying system. Experiments conducted on\na two-converter benchtop setup and at the CoSES Laboratory on a 25 kVA\nconverter connected to the Munich grid support these hypotheses.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02325v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "并网、数据驱动的逆变器控制：从理论到硬件", "tldr": "本文介绍了一种名为瞬态预测控制 (TPC) 的数据驱动方法，用于并网逆变器控制，并实验证明其在标准硬件上在线运行且对非线性系统有效。", "motivation": "并网逆变器控制难以实现，因为难以获取和维护精确的电网模型。传统基于模型的控制方法存在局限性。", "method": "本文描述了如何使用最近提出的瞬态预测控制 (TPC) 实现实时、即插即用的逆变器控制。通过测试两个假设来验证TPC：1) TPC算法可以在标准硬件上在线运行；2) TPC对非线性时变系统（并网逆变器控制）有效。", "result": "在双变换器台式设置和CoSES实验室的25 kVA变换器（连接到慕尼黑电网）上进行的实验支持了这两个假设。", "conclusion": "瞬态预测控制 (TPC) 是一种有效的、模型无关的并网逆变器控制方法，可以在标准硬件上在线运行，并且对非线性时变系统有效。", "translation": "并网逆变器控制由于难以获取和维持精确的电网模型而难以实现。直接数据驱动预测控制为传统的基于模型的控制方法提供了一种无模型的替代方案。本文描述了如何将最近提出的瞬态预测控制（TPC）用于实际的即插即用逆变器控制。测试了以下假设：1）TPC算法可以使用标准硬件在线运行；2）TPC（基于线性时不变假设推导）对于并网逆变器控制（一个非线性时变系统）是有效的。在双变换器台式设置和CoSES实验室中将25 kVA变换器连接到慕尼黑电网进行的实验支持了这些假设。", "summary": "本文提出并验证了瞬态预测控制 (TPC) 这种数据驱动、无模型的并网逆变器控制方法。研究通过实验证明，TPC算法可以在标准硬件上在线运行，并且对于非线性、时变系统（如并网逆变器）具有有效性，从而克服了传统基于模型控制方法对精确电网模型的依赖。", "keywords": "并网逆变器控制, 数据驱动, 瞬态预测控制, 模型无关, 非线性系统", "comments": "这项研究的创新之处在于提出了瞬态预测控制 (TPC) 作为并网逆变器控制的一种无模型、数据驱动的解决方案，解决了传统方法对精确电网模型依赖的痛点。其重要性在于验证了TPC在实际硬件上运行的可行性及其对复杂非线性系统的有效性，为未来智能电网中的逆变器控制提供了新的方向。"}}
{"id": "2507.02374", "title": "Predictive Control over LAWN: Joint Trajectory Design and Resource Allocation", "authors": ["Haijia Jin", "Jun Wu", "Weijie Yuan", "Ruizhi Ruan", "Jiacheng Wang", "Dusit Niyato", "Dong In Kim", "Abbas Jamalipour"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02374v1", "summary": "Low-altitude wireless networks (LAWNs) have been envisioned as flexible and\ntransformative platforms for enabling delay-sensitive control applications in\nInternet of Things (IoT) systems. In this work, we investigate the real-time\nwireless control over a LAWN system, where an aerial drone is employed to serve\nmultiple mobile automated guided vehicles (AGVs) via finite blocklength (FBL)\ntransmission. Toward this end, we adopt the model predictive control (MPC) to\nensure accurate trajectory tracking, while we analyze the communication\nreliability using the outage probability. Subsequently, we formulate an\noptimization problem to jointly determine control policy, transmit power\nallocation, and drone trajectory by accounting for the maximum travel distance\nand control input constraints. To address the resultant non-convex optimization\nproblem, we first derive the closed-form expression of the outage probability\nunder FBL transmission. Based on this, we reformulate the original problem as a\nquadratic programming (QP) problem, followed by developing an alternating\noptimization (AO) framework. Specifically, we employ the projected gradient\ndescent (PGD) method and the successive convex approximation (SCA) technique to\nachieve computationally efficient sub-optimal solutions. Furthermore, we\nthoroughly analyze the convergence and computational complexity of the proposed\nalgorithm. Extensive simulations and AirSim-based experiments are conducted to\nvalidate the superiority of our proposed approach compared to the baseline\nschemes in terms of control performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02374v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LAWN上的预测控制：联合轨迹设计与资源分配", "tldr": "本文研究了低空无线网络（LAWN）中无人机为移动AGV提供服务的实时无线控制问题，通过联合优化控制策略、发射功率分配和无人机轨迹，并采用MPC和FBL传输来确保轨迹跟踪和通信可靠性。提出了一种交替优化框架来解决非凸问题，并通过仿真和实验验证了其优越性。", "motivation": "低空无线网络（LAWN）被设想为物联网（IoT）系统中实现延迟敏感控制应用的灵活且具有变革性的平台。本文旨在解决在LAWN系统中，无人机通过有限块长度（FBL）传输为多个移动自动导引车（AGV）提供服务时的实时无线控制问题。", "method": "本研究采用模型预测控制（MPC）来确保精确的轨迹跟踪，并使用中断概率分析通信可靠性。制定了一个优化问题，联合确定控制策略、发射功率分配和无人机轨迹，同时考虑最大行程距离和控制输入约束。为了解决由此产生的非凸优化问题，首先推导了FBL传输下中断概率的闭式表达式。在此基础上，将原始问题重新表述为二次规划（QP）问题，并开发了一个交替优化（AO）框架。具体来说，采用投影梯度下降（PGD）方法和逐次凸逼近（SCA）技术来获得计算高效的次优解。此外，还彻底分析了所提出算法的收敛性和计算复杂度。", "result": "通过广泛的仿真和基于AirSim的实验，验证了所提出的方法在控制性能方面优于基线方案。", "conclusion": "本文提出的针对LAWN中无人机为AGV提供服务的预测控制方法，通过联合优化轨迹设计和资源分配，并在MPC和FBL传输的结合下，实现了卓越的控制性能和通信可靠性。", "translation": "低空无线网络（LAWN）被设想为物联网（IoT）系统中实现延迟敏感控制应用的灵活且具有变革性的平台。在这项工作中，我们研究了LAWN系统中的实时无线控制，其中一架无人机通过有限块长度（FBL）传输为多个移动自动导引车（AGV）提供服务。为此，我们采用模型预测控制（MPC）来确保精确的轨迹跟踪，同时利用中断概率分析通信可靠性。随后，我们制定了一个优化问题，通过考虑最大行程距离和控制输入约束来联合确定控制策略、发射功率分配和无人机轨迹。为了解决由此产生的非凸优化问题，我们首先推导了FBL传输下中断概率的闭式表达式。在此基础上，我们将原始问题重新表述为二次规划（QP）问题，然后开发了一个交替优化（AO）框架。具体来说，我们采用投影梯度下降（PGD）方法和逐次凸逼近（SCA）技术来获得计算高效的次优解。此外，我们彻底分析了所提出算法的收敛性和计算复杂度。通过广泛的仿真和基于AirSim的实验，验证了我们提出的方法在控制性能方面优于基线方案。", "summary": "本文研究了低空无线网络（LAWN）中无人机为移动自动导引车（AGV）提供实时无线控制的问题。通过结合模型预测控制（MPC）和有限块长度（FBL）传输，确保了轨迹跟踪精度和通信可靠性。论文提出了一个联合优化问题，旨在同时确定控制策略、发射功率分配和无人机轨迹。为了解决这一非凸问题，作者推导了FBL传输下中断概率的闭式解，并将问题重构为二次规划（QP），进而开发了一个基于投影梯度下降（PGD）和逐次凸逼近（SCA）的交替优化（AO）框架以获得次优解。实验结果表明，该方法在控制性能上优于现有基线方案。", "keywords": "预测控制, 低空无线网络, 轨迹设计, 资源分配, 有限块长度传输", "comments": "本文的创新点在于将模型预测控制（MPC）与有限块长度（FBL）传输相结合，用于低空无线网络（LAWN）中的实时控制，并提出了一个联合优化无人机轨迹、资源分配和控制策略的框架。其采用交替优化（AO）框架结合PGD和SCA来解决复杂的非凸问题，具有较高的工程实践价值。文章对算法的收敛性和计算复杂度进行了分析，并通过仿真和实验验证了其优越性，展现了扎实的研究工作。"}}
{"id": "2507.02768", "title": "DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment", "authors": ["Ke-Han Lu", "Zhehuai Chen", "Szu-Wei Fu", "Chao-Han Huck Yang", "Sung-Feng Huang", "Chih-Kai Yang", "Chee-En Yu", "Chun-Wei Chen", "Wei-Chih Chen", "Chien-yu Huang", "Yi-Cheng Lin", "Yu-Xiang Lin", "Chi-An Fu", "Chun-Yi Kuan", "Wenze Ren", "Xuanjun Chen", "Wei-Ping Huang", "En-Pei Hu", "Tzu-Quan Lin", "Yuan-Kuei Wu", "Kuan-Po Huang", "Hsiao-Ying Huang", "Huang-Cheng Chou", "Kai-Wei Chang", "Cheng-Han Chiang", "Boris Ginsburg", "Yu-Chiang Frank Wang", "Hung-yi Lee"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Model and code available at: this https URL", "url": "http://arxiv.org/abs/2507.02768v1", "summary": "We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model\n(LALM) designed for robust auditory perception and instruction-following,\nwithout requiring task-specific audio instruction-tuning. Recent LALMs\ntypically augment Large Language Models (LLMs) with auditory capabilities by\ntraining on large-scale, manually curated or LLM-synthesized audio-instruction\ndatasets. However, these approaches have often suffered from the catastrophic\nforgetting of the LLM's original language abilities. To address this, we\nrevisit the data construction pipeline and propose DeSTA, a self-generated\ncross-modal alignment strategy in which the backbone LLM generates its own\ntraining targets. This approach preserves the LLM's native language proficiency\nwhile establishing effective audio-text alignment, thereby enabling zero-shot\ngeneralization without task-specific tuning. Using DeSTA, we construct\nDeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training\nsamples derived from 7,000 hours of audio spanning 50 diverse datasets,\nincluding speech, environmental sounds, and music. DeSTA2.5-Audio achieves\nstate-of-the-art or competitive performance across a wide range of\naudio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,\nSpeech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate\nthat our self-generated strategy outperforms widely adopted data construction\nand training strategies in both auditory perception and instruction-following\ncapabilities. Our findings underscore the importance of carefully designed data\nconstruction in LALM development and offer practical insights for building\nrobust, general-purpose LALMs.", "comment": "Model and code available at:\n  https://github.com/kehanlu/DeSTA2.5-Audio", "pdf_url": "http://arxiv.org/pdf/2507.02768v1", "cate": "eess.AS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DeSTA2.5-Audio：迈向通用大型音频语言模型与自生成跨模态对齐", "tldr": "DeSTA2.5-Audio是一个通用的音频语言模型（LALM），它引入了DeSTA自生成跨模态对齐策略，解决了现有LALM中大型语言模型（LLM）灾难性遗忘的问题，无需特定任务微调即可在多种音频语言基准上达到SOTA或具有竞争力。", "motivation": "现有的通用大型音频语言模型（LALMs）通过在手动或LLM合成的音频指令数据集上进行训练，增强了大型语言模型（LLMs）的听觉能力，但这些方法常常导致LLM原始语言能力的灾难性遗忘。", "method": "为解决灾难性遗忘问题，本文提出了DeSTA，一种自生成跨模态对齐策略，其中骨干LLM生成自己的训练目标。这种方法保留了LLM固有的语言能力，同时建立了有效的音频-文本对齐，从而实现了零样本泛化，无需特定任务微调。基于DeSTA，研究人员构建了DeSTA-AQA5M，一个包含500万训练样本的大规模、任务无关数据集，这些样本来源于7000小时的音频，涵盖50个不同的数据集，包括语音、环境音和音乐。", "result": "DeSTA2.5-Audio在广泛的音频语言基准测试（包括Dynamic-SUPERB、MMAU、SAKURA、Speech-IFEval和VoiceBench）上取得了最先进或具有竞争力的性能。全面的比较研究表明，该自生成策略在听觉感知和指令遵循能力上均优于广泛采用的数据构建和训练策略。", "conclusion": "本研究强调了精心设计的数据构建在大型音频语言模型（LALM）开发中的重要性，并为构建鲁棒、通用LALM提供了实用见解。", "translation": "我们引入了DeSTA2.5-Audio，一个通用型大型音频语言模型（LALM），旨在实现鲁棒的听觉感知和指令遵循，且无需进行特定任务的音频指令微调。近期的大型音频语言模型通常通过在大规模、人工整理或LLM合成的音频指令数据集上进行训练，来增强大型语言模型（LLMs）的听觉能力。然而，这些方法常常遭受LLM原始语言能力灾难性遗忘的困扰。为了解决这个问题，我们重新审视了数据构建流程，并提出了DeSTA，一种自生成跨模态对齐策略，其中骨干LLM生成其自身的训练目标。这种方法在建立有效的音频-文本对齐的同时，保留了LLM固有的语言能力，从而实现了零样本泛化，无需特定任务微调。利用DeSTA，我们构建了DeSTA-AQA5M，一个大规模、任务无关的数据集，包含500万个训练样本，这些样本来源于7000小时的音频，涵盖50个不同的数据集，包括语音、环境音和音乐。DeSTA2.5-Audio在广泛的音频语言基准测试（包括Dynamic-SUPERB、MMAU、SAKURA、Speech-IFEval和VoiceBench）上取得了最先进或具有竞争力的性能。全面的比较研究表明，我们的自生成策略在听觉感知和指令遵循能力上均优于广泛采用的数据构建和训练策略。我们的发现强调了精心设计的数据构建在大型音频语言模型开发中的重要性，并为构建鲁棒、通用LALM提供了实用见解。", "summary": "本文介绍了DeSTA2.5-Audio，一个通用的音频语言模型（LALM），旨在解决现有LALM中LLM原始语言能力灾难性遗忘的问题。其核心创新是DeSTA，一种自生成跨模态对齐策略，允许骨干LLM生成自身的训练目标，从而在建立有效的音频-文本对齐的同时，保留了LLM的语言能力，并实现了零样本泛化。研究团队利用DeSTA构建了大规模、任务无关的DeSTA-AQA5M数据集。实验结果表明，DeSTA2.5-Audio在多项音频语言基准测试上达到了最先进或具有竞争力的性能，证明了该自生成策略在数据构建和训练方面的优越性，并强调了精心设计的数据在LALM发展中的关键作用。", "keywords": "大型音频语言模型, 跨模态对齐, 自生成数据, 灾难性遗忘, 零样本泛化", "comments": "该论文的创新点在于提出了“自生成跨模态对齐策略”（DeSTA），这有效地缓解了多模态LLM中常见的灾难性遗忘问题。通过这种策略构建的大规模、任务无关数据集（DeSTA-AQA5M）也具有重要意义。这项工作为构建更鲁棒、更通用的音频语言模型提供了一个有价值的解决方案，减少了对大量人工标注或特定任务微调的依赖。"}}
{"id": "2507.02268", "title": "Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation", "authors": ["Yuxiang Zhang", "Wei Li", "Wen Jia", "Mengmeng Zhang", "Ran Tao", "Shunlin Liang"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02268v1", "summary": "Utilizing hyperspectral remote sensing technology enables the extraction of\nfine-grained land cover classes. Typically, satellite or airborne images used\nfor training and testing are acquired from different regions or times, where\nthe same class has significant spectral shifts in different scenes. In this\npaper, we propose a Bi-directional Domain Adaptation (BiDA) framework for\ncross-domain hyperspectral image (HSI) classification, which focuses on\nextracting both domain-invariant features and domain-specific information in\nthe independent adaptive space, thereby enhancing the adaptability and\nseparability to the target scene. In the proposed BiDA, a triple-branch\ntransformer architecture (the source branch, target branch, and coupled branch)\nwith semantic tokenizer is designed as the backbone. Specifically, the source\nbranch and target branch independently learn the adaptive space of source and\ntarget domains, a Coupled Multi-head Cross-attention (CMCA) mechanism is\ndeveloped in coupled branch for feature interaction and inter-domain\ncorrelation mining. Furthermore, a bi-directional distillation loss is designed\nto guide adaptive space learning using inter-domain correlation. Finally, we\npropose an Adaptive Reinforcement Strategy (ARS) to encourage the model to\nfocus on specific generalized feature extraction within both source and target\nscenes in noise condition. Experimental results on cross-temporal/scene\nairborne and satellite datasets demonstrate that the proposed BiDA performs\nsignificantly better than some state-of-the-art domain adaptation approaches.\nIn the cross-temporal tree species classification task, the proposed BiDA is\nmore than 3\\%$\\sim$5\\% higher than the most advanced method. The codes will be\navailable from the website:\nhttps://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02268v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于双向域适应的跨域高光谱图像分类", "tldr": "该论文提出了一种名为BiDA的双向域适应框架，用于解决跨域高光谱图像分类中的光谱漂移问题，通过提取域不变和域特定特征，并引入三重分支Transformer架构、CMCA机制、双向蒸馏损失和自适应强化策略，实现了优于现有方法的分类性能。", "motivation": "在跨域高光谱图像（HSI）分类中，由于训练和测试图像来自不同区域或时间，导致相同类别的光谱特征存在显著差异（光谱漂移），这降低了模型的泛化能力和分类精度。", "method": "本文提出了一种双向域适应（BiDA）框架。该框架设计了一个带有语义分词器的三重分支Transformer架构（源分支、目标分支和耦合分支）。源分支和目标分支独立学习源域和目标域的自适应空间；耦合分支中开发了耦合多头交叉注意力（CMCA）机制用于特征交互和域间关联挖掘。此外，设计了双向蒸馏损失来指导自适应空间学习。最后，提出了一种自适应强化策略（ARS），以鼓励模型在噪声条件下专注于源场景和目标场景中的特定泛化特征提取。", "result": "在跨时间/场景的机载和卫星数据集上的实验结果表明，所提出的BiDA框架显著优于一些最先进的域适应方法。在跨时间树种分类任务中，BiDA的性能比最先进的方法高出3%~5%。", "conclusion": "所提出的BiDA框架通过有效地提取域不变特征和域特定信息，显著提升了跨域高光谱图像分类的性能，尤其在处理光谱漂移问题上表现出优越性。", "translation": "利用高光谱遥感技术能够提取细粒度的地物覆盖类别。通常，用于训练和测试的卫星或机载图像是在不同区域或时间获取的，在不同场景中，同一类别存在显著的光谱漂移。在本文中，我们提出了一种用于跨域高光谱图像（HSI）分类的双向域适应（BiDA）框架，该框架侧重于在独立的自适应空间中提取域不变特征和域特定信息，从而增强对目标场景的适应性和可分离性。在所提出的BiDA中，设计了一个带有语义分词器的三重分支Transformer架构（源分支、目标分支和耦合分支）作为骨干网络。具体来说，源分支和目标分支独立学习源域和目标域的自适应空间，在耦合分支中开发了一种耦合多头交叉注意力（CMCA）机制用于特征交互和域间关联挖掘。此外，设计了一种双向蒸馏损失来指导自适应空间学习。最后，我们提出了一种自适应强化策略（ARS），以鼓励模型在噪声条件下专注于源场景和目标场景中的特定泛化特征提取。在跨时间/场景机载和卫星数据集上的实验结果表明，所提出的BiDA性能显著优于一些最先进的域适应方法。在跨时间树种分类任务中，所提出的BiDA比最先进的方法高出3%~5%。代码将从网站提供：https://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA。", "summary": "该论文提出了一种名为BiDA的双向域适应框架，旨在解决跨域高光谱图像分类中因光谱漂移导致的问题。BiDA采用三重分支Transformer架构，结合耦合多头交叉注意力机制、双向蒸馏损失和自适应强化策略，以同时提取域不变和域特定特征。实验证明，该方法在跨时间/场景数据集上表现优异，显著超越了现有最先进的域适应方法。", "keywords": "高光谱图像分类, 域适应, 跨域, Transformer, 深度学习", "comments": "该论文的创新点在于提出了BiDA框架，通过独特的三重分支Transformer架构和一系列新颖的机制（如CMCA、双向蒸馏损失、ARS）来有效应对跨域高光谱图像分类中的光谱漂移挑战。它不仅关注域不变特征的提取，还强调域特定信息的利用，这对于提高模型在复杂跨域场景下的适应性和分类精度至关重要。实验结果显示出显著的性能提升，表明了该方法的有效性和重要性。"}}
{"id": "2507.02001", "title": "Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames", "authors": ["Anurag Arnab", "Ahmet Iscen", "Mathilde Caron", "Alireza Fathi", "Cordelia Schmid"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02001v1", "summary": "Despite recent advances in Vision-Language Models (VLMs), long-video\nunderstanding remains a challenging problem. Although state-of-the-art\nlong-context VLMs can process around 1000 input frames, they still struggle to\neffectively leverage this sequence length, and succumb to irrelevant\ndistractors within the context window. We present Temporal Chain of Thought, an\ninference strategy for video question-answering that curates the model's input\ncontext. We use the VLM itself to iteratively identify and extract the most\nrelevant frames from the video, which are then used for answering. We\ndemonstrate how leveraging more computation at inference-time to select the\nmost relevant context leads to improvements in accuracy, in agreement with\nrecent work on inference-time scaling of LLMs. Moreover, we achieve\nstate-of-the-art results on 4 diverse video question-answering datasets,\nshowing consistent improvements with 3 different VLMs. In particular, our\nmethod shines on longer videos which would not otherwise fit within the model's\ncontext window: On longer videos of more than 1 hour on LVBench, our approach\nusing a context window of 32K outperforms the same VLM using standard inference\nwith a 700K context window by 2.8 points.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02001v1", "cate": "cs.LG", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "时间链式思考：通过帧思考实现长视频理解", "tldr": "本文提出Temporal Chain of Thought (TCoT) 策略，通过VLM迭代识别并提取视频中最相关的帧，以解决长视频理解中上下文限制和无关信息干扰的问题，并在多个视频问答数据集上取得了最先进的成果，尤其在处理超长视频方面表现出色。", "motivation": "尽管视觉-语言模型（VLM）取得了进展，但长视频理解仍然是一个挑战。现有的长上下文VLM难以有效利用长序列，并且容易受到上下文窗口中无关干扰的影响。", "method": "提出Temporal Chain of Thought (TCoT) 这一视频问答推理策略。该方法使用VLM本身迭代地识别并提取视频中最相关的帧，然后将这些精选的帧作为输入上下文用于回答问题。", "result": "1. 在推理时利用更多计算来选择最相关上下文能提高准确性。2. 在4个不同的视频问答数据集上取得了最先进（SOTA）的结果。3. 与3种不同的VLM都显示出一致的改进。4. 在长视频上表现尤其出色，解决了模型上下文窗口不足的问题。5. 在LVBench上，对于超过1小时的长视频，使用32K上下文窗口的TCoT方法比使用700K上下文窗口的标准推理方法性能高出2.8点。", "conclusion": "Temporal Chain of Thought通过在推理时智能地选择和管理输入上下文，显著提升了长视频理解能力，尤其在处理超长视频时，克服了现有VLM的上下文限制和无关信息干扰问题。", "translation": "尽管视觉-语言模型（VLM）最近取得了进展，但长视频理解仍然是一个具有挑战性的问题。尽管最先进的长上下文VLM可以处理大约1000个输入帧，但它们仍然难以有效利用这个序列长度，并受上下文窗口中无关干扰的困扰。我们提出了时间链式思考（Temporal Chain of Thought），这是一种用于视频问答的推理策略，它管理模型的输入上下文。我们使用VLM本身迭代地识别和提取视频中最相关的帧，然后将这些帧用于回答问题。我们展示了在推理时利用更多计算来选择最相关上下文如何提高准确性，这与最近关于LLM推理时扩展的研究结果一致。此外，我们在4个不同的视频问答数据集上取得了最先进的结果，并显示出对3种不同VLM的一致改进。特别是，我们的方法在那些否则无法适应模型上下文窗口的更长视频上表现出色：在LVBench上超过1小时的长视频上，我们使用32K上下文窗口的方法比使用700K上下文窗口的标准推理方法性能高出2.8点。", "summary": "本文提出Temporal Chain of Thought（TCoT），一种新颖的视频问答推理策略，旨在解决长视频理解中上下文管理和无关信息干扰的挑战。TCoT利用VLM本身迭代地识别和提取视频中的关键帧作为输入上下文，从而有效利用计算资源并提高准确性。实验证明，TCoT在多个视频问答数据集上达到了SOTA性能，并显著提升了对超长视频的理解能力，即使在较小的上下文窗口下也能超越传统方法。", "keywords": "长视频理解, 视频问答, 上下文管理, 视觉-语言模型, 时间链式思考", "comments": "这篇论文的创新点在于将“思维链”（Chain of Thought）的概念引入视频理解领域，并将其应用于上下文管理。通过让VLM自身“思考”并选择最相关的帧，它有效地解决了长视频处理中的关键挑战——即如何有效利用有限的上下文窗口并过滤掉无关信息。这种在推理时进行智能上下文选择的方法，与大型语言模型（LLM）领域中推理时扩展（inference-time scaling）的趋势相符，显示了其普适性和重要性。该方法在处理超长视频方面的显著提升尤为突出，为未来长视频理解模型的发展提供了新的思路。"}}
{"id": "2507.02211", "title": "Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning", "authors": ["Gustavo C. Mangold", "Heitor C. M. Fernandes", "Mendeli H. Vainstein"], "categories": ["cs.AI", "cs.NE", "physics.comp-ph"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02211v1", "summary": "Recent studies in the spatial prisoner's dilemma games with reinforcement\nlearning have shown that static agents can learn to cooperate through a diverse\nsort of mechanisms, including noise injection, different types of learning\nalgorithms and neighbours' payoff knowledge.In this work, using an independent\nmulti-agent Q-learning algorithm, we study the effects of dilution and mobility\nin the spatial version of the prisoner's dilemma. Within this setting,\ndifferent possible actions for the algorithm are defined, connecting with\nprevious results on the classical, non-reinforcement learning spatial\nprisoner's dilemma, showcasing the versatility of the algorithm in modeling\ndifferent game-theoretical scenarios and the benchmarking potential of this\napproach.As a result, a range of effects is observed, including evidence that\ngames with fixed update rules can be qualitatively equivalent to those with\nlearned ones, as well as the emergence of a symbiotic mutualistic effect\nbetween populations that forms when multiple actions are defined.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02211v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "强化学习空间囚徒困境中的稀释、扩散与共生", "tldr": "本文研究了在强化学习空间囚徒困境中，稀释和移动性对合作行为的影响，发现固定更新规则与学习更新规则的游戏在质量上等效，并观察到多行动定义下的共生互惠效应。", "motivation": "近期研究表明静态智能体在强化学习空间囚徒困境中可以通过多种机制学习合作。本研究旨在进一步探讨稀释和移动性对强化学习空间囚徒困境中合作行为的影响。", "method": "本研究采用独立的“多智能体Q-学习算法”来模拟空间囚徒困境，并研究稀释和移动性的影响。文中定义了算法的不同可能行动，并将其结果与经典非强化学习空间囚徒困境的结果进行比较，以展示算法的通用性和基准潜力。", "result": "研究观察到一系列效应，包括固定更新规则的游戏在质量上可以等同于学习更新规则的游戏的证据，以及当定义多个行动时，种群之间会形成共生互惠效应。", "conclusion": "本文研究表明，在强化学习空间囚徒困境中，稀释和移动性对合作演化具有显著影响。特别地，固定更新规则的游戏与学习更新规则的游戏在性质上可以等效，且在多行动定义下，种群间会涌现共生互惠效应。这展示了多智能体Q-学习算法在建模复杂博弈论场景中的多功能性。", "translation": "近期在强化学习空间囚徒困境博弈中的研究表明，静态智能体可以通过多种机制学习合作，包括注入噪声、不同类型的学习算法和邻居的收益知识。在这项工作中，我们使用独立的“多智能体Q-学习算法”，研究了稀释和移动性在空间囚徒困境中的影响。在此设置下，定义了算法的不同可能行动，与经典非强化学习空间囚徒困境的先前结果相联系，展示了该算法在建模不同博弈论场景中的多功能性以及这种方法的基准潜力。结果观察到一系列效应，包括固定更新规则的游戏在质量上可以等同于学习更新规则的游戏的证据，以及当定义多个行动时，种群之间形成的共生互惠效应的出现。", "summary": "本文利用独立的“多智能体Q-学习算法”研究了强化学习空间囚徒困境中稀释和移动性的影响。研究发现，固定更新规则的游戏在质量上可以等同于学习更新规则的游戏，并且在定义多个行动时，种群之间会形成共生互惠效应。这展示了该算法在建模不同博弈论场景中的通用性。", "keywords": "空间囚徒困境, 强化学习, Q-学习, 稀释, 共生效应", "comments": "本文的创新点在于将稀释和移动性引入强化学习空间囚徒困境，并使用多智能体Q-学习算法进行深入研究。其发现固定更新规则与学习更新规则的等效性以及多行动下的共生效应，为理解复杂系统中的合作演化提供了新的视角，并展示了该方法作为基准测试的潜力。"}}
{"id": "2507.02255", "title": "Listwise Preference Alignment Optimization for Tail Item Recommendation", "authors": ["Zihao Li", "Chao Yang", "Tong Zhang", "Yakun Chen", "Xianzhi Wang", "Guandong Xu", "Daoyi Dong"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02255v1", "summary": "Preference alignment has achieved greater success on Large Language Models\n(LLMs) and drawn broad interest in recommendation research. Existing preference\nalignment methods for recommendation either require explicit reward modeling or\nonly support pairwise preference comparison. The former directly increases\nsubstantial computational costs, while the latter hinders training efficiency\non negative samples. Moreover, no existing effort has explored preference\nalignment solutions for tail-item recommendation. To bridge the above gaps, we\npropose LPO4Rec, which extends the Bradley-Terry model from pairwise comparison\nto listwise comparison, to improve the efficiency of model training.\nSpecifically, we derive a closed form optimal policy to enable more efficient\nand effective training without explicit reward modeling. We also present an\nadaptive negative sampling and reweighting strategy to prioritize tail items\nduring optimization and enhance performance in tail-item recommendations.\nBesides, we theoretically prove that optimizing the listwise preference\noptimization (LPO) loss is equivalent to maximizing the upper bound of the\noptimal reward. Our experiments on three public datasets show that our method\noutperforms 10 baselines by a large margin, achieving up to 50% performance\nimprovement while reducing 17.9% GPU memory usage when compared with direct\npreference optimization (DPO) in tail-item recommendation. Our code is\navailable at https://github.com/Yuhanleeee/LPO4Rec.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02255v1", "cate": "cs.IR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "列表式偏好对齐优化用于长尾项目推荐", "tldr": "LPO4Rec是一种列表式偏好对齐优化方法，通过引入列表式比较和自适应负采样策略，显著提升了长尾项目推荐的性能和效率。", "motivation": "现有推荐系统中的偏好对齐方法存在局限性：要么需要显式奖励建模导致计算成本高昂，要么仅支持成对偏好比较，影响负样本训练效率。此外，目前还没有针对长尾项目推荐的偏好对齐解决方案。", "method": "本文提出了LPO4Rec，它将Bradley-Terry模型从成对比较扩展到列表式比较，以提高模型训练效率。具体来说，LPO4Rec推导了一个闭式最优策略，无需显式奖励建模即可实现高效训练。它还引入了一种自适应负采样和重加权策略，以在优化过程中优先处理长尾项目。理论上，优化列表式偏好优化（LPO）损失等同于最大化最优奖励的上限。", "result": "在三个公共数据集上的实验表明，LPO4Rec显著优于10个基线模型，性能提升高达50%。与直接偏好优化（DPO）相比，在长尾项目推荐中，LPO4Rec还能减少17.9%的GPU内存使用。", "conclusion": "LPO4Rec通过列表式偏好对齐优化和对长尾项目的特别关注，有效解决了现有推荐系统偏好对齐方法的效率和长尾项目推荐的挑战，取得了显著的性能提升和资源节约。", "translation": "偏好对齐在大型语言模型（LLMs）上取得了巨大成功，并在推荐研究中引起了广泛兴趣。现有的推荐偏好对齐方法要么需要显式奖励建模，要么只支持成对偏好比较。前者直接增加了大量的计算成本，而后者阻碍了负样本的训练效率。此外，目前还没有探索针对长尾项目推荐的偏好对齐解决方案。为了弥补上述空白，我们提出了LPO4Rec，它将Bradley-Terry模型从成对比较扩展到列表式比较，以提高模型训练的效率。具体来说，我们推导了一个闭式最优策略，无需显式奖励建模即可实现更高效和有效的训练。我们还提出了一种自适应负采样和重加权策略，以在优化过程中优先考虑长尾项目，从而提高长尾项目推荐的性能。此外，我们从理论上证明了优化列表式偏好优化（LPO）损失等同于最大化最优奖励的上限。我们在三个公共数据集上的实验表明，我们的方法显著优于10个基线模型，性能提升高达50%，同时与直接偏好优化（DPO）相比，在长尾项目推荐中减少了17.9%的GPU内存使用。我们的代码可在https://github.com/Yuhanleeee/LPO4Rec获取。", "summary": "本文提出了LPO4Rec，一个用于长尾项目推荐的列表式偏好对齐优化方法。它通过将Bradley-Terry模型扩展到列表式比较，并引入无需显式奖励建模的闭式最优策略，解决了现有方法计算成本高和训练效率低的问题。LPO4Rec还采用自适应负采样和重加权策略以优先处理长尾项目。实验证明，LPO4Rec在性能上显著优于基线模型，并能有效减少GPU内存使用。", "keywords": "偏好对齐, 列表式, 长尾项目推荐, 优化, Bradley-Terry模型", "comments": "LPO4Rec的创新之处在于将偏好对齐从成对扩展到列表式，并针对长尾项目推荐提出了专门的优化策略，这对于提升推荐系统在处理稀疏数据和长尾分布方面的能力具有重要意义。无需显式奖励建模和闭式最优策略的设计，也显著提高了训练效率和实用性。其在性能提升和资源节省方面的结果令人印象深刻。"}}
{"id": "2507.02657", "title": "On the Complexity of Knapsack under Explorable Uncertainty: Hardness and Algorithms", "authors": ["Jens Schlöter"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02657v1", "summary": "In the knapsack problem under explorable uncertainty, we are given a knapsack\ninstance with uncertain item profits. Instead of having access to the precise\nprofits, we are only given uncertainty intervals that are guaranteed to contain\nthe corresponding profits. The actual item profit can be obtained via a query.\nThe goal of the problem is to adaptively query item profits until the revealed\ninformation suffices to compute an optimal (or approximate) solution to the\nunderlying knapsack instance. Since queries are costly, the objective is to\nminimize the number of queries.\n  In the offline variant of this problem, we assume knowledge of the precise\nprofits and the task is to compute a query set of minimum cardinality that a\nthird party without access to the profits could use to identify an optimal (or\napproximate) knapsack solution. We show that this offline variant is complete\nfor the second-level of the polynomial hierarchy, i.e., $\\Sigma_2^p$-complete,\nand cannot be approximated within a non-trivial factor unless $\\Sigma_2^p =\n\\Delta_2^p$. Motivated by these strong hardness results, we consider a\nresource-augmented variant of the problem where the requirements on the query\nset computed by an algorithm are less strict than the requirements on the\noptimal solution we compare against. More precisely, a query set computed by\nthe algorithm must reveal sufficient information to identify an approximate\nknapsack solution, while the optimal query set we compare against has to reveal\nsufficient information to identify an optimal solution. We show that this\nresource-augmented setting allows interesting non-trivial algorithmic results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02657v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "不确定性可探索背包问题的复杂性：硬度和算法", "tldr": "本文研究了不确定性可探索背包问题的复杂性，证明了其离线变体的强硬度，并提出了资源增强设置下的算法结果。", "motivation": "由于不确定性可探索背包问题离线变体的强硬度结果（$\\\\Sigma_2^p$-完全性和非平凡因子不可近似性），本文考虑了一个资源增强的变体，以获得非平凡的算法结果。", "method": "本文首先证明了不确定性可探索背包问题离线变体的计算复杂性，即它是$\\\\Sigma_2^p$-完全的，并且在非平凡因子内不可近似。然后，为了克服这些强硬度结果，作者提出了一个资源增强的问题变体，其中算法计算的查询集要求比最优解的要求更宽松，并在此设置下展示了算法结果。", "result": "离线变体被证明是$\\\\Sigma_2^p$-完全的，并且除非$\\\\Sigma_2^p = \\\\Delta_2^p$，否则无法在非平凡因子内近似。在资源增强设置下，即算法计算的查询集只需识别近似背包解，而最优查询集需识别最优解的情况下，获得了有趣的非平凡算法结果。", "conclusion": "不确定性可探索背包问题，特别是其离线变体，具有很高的计算复杂性。通过放宽对查询集的要求（资源增强），可以获得有效的算法。", "translation": "在不确定性可探索背包问题中，我们得到了一个具有不确定物品利润的背包实例。我们无法获取精确的利润，而只能得到包含相应利润的不确定性区间。实际物品利润可以通过查询获得。问题的目标是自适应地查询物品利润，直到揭示的信息足以计算底层背包实例的最优（或近似）解。由于查询成本高昂，目标是最小化查询次数。\n在该问题的离线变体中，我们假设已知精确利润，任务是计算一个最小基数的查询集，第三方在无法获取利润的情况下可以使用该查询集来识别最优（或近似）背包解。我们证明了该离线变体对于多项式层次结构的第二层是完全的，即$\\\\Sigma_2^p$-完全，并且除非$\\\\Sigma_2^p = \\\\Delta_2^p$，否则无法在非平凡因子内近似。受这些强硬度结果的启发，我们考虑了该问题的资源增强变体，其中算法计算的查询集要求比我们比较的最优解的要求更宽松。更准确地说，算法计算的查询集必须揭示足够的信息来识别近似背包解，而我们比较的最优查询集必须揭示足够的信息来识别最优解。我们表明这种资源增强设置允许有趣的非平凡算法结果。", "summary": "本文研究了不确定性可探索背包问题，其中物品利润以不确定性区间形式给出，并通过查询获取精确值以最小化查询次数。研究发现，该问题的离线变体是$\\\\Sigma_2^p$-完全的，并且在非平凡因子内不可近似，这表明其高计算复杂性。为了应对这一挑战，论文引入了一个资源增强的变体，其中算法的目标是识别近似解，而最优解是识别最优解，并在此设置下展示了非平凡的算法结果。", "keywords": "背包问题, 不确定性, 复杂性, 近似算法, 多项式层次结构", "comments": "本文深入探讨了不确定性可探索背包问题的复杂性，特别是其离线变体的强硬度结果（$\\\\Sigma_2^p$-完全性），这对于理解该类问题的计算界限至关重要。引入资源增强设置以规避强硬度限制并获得实用算法的思路是一个重要的贡献，为处理现实世界中带有不确定性的优化问题提供了新的视角。"}}
{"id": "2507.02302", "title": "DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning", "authors": ["Dohoon Kim", "Donghun Kang", "Taesup Moon"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      22 pages, 5 figures, ACL 2025 Main", "url": "http://arxiv.org/abs/2507.02302v1", "summary": "Domain-Adaptive Pre-training (DAP) has recently gained attention for its\neffectiveness in fine-tuning pre-trained models. Building on this, continual\nDAP has been explored to develop pre-trained models capable of incrementally\nincorporating different domain datasets. However, existing continual DAP\nmethods face several limitations: (1) high computational cost and GPU memory\nusage during training; (2) sensitivity to incremental data order; and (3)\nproviding a single, generalized model for all end tasks, which contradicts the\nessence of DAP. In this paper, we propose DoMIX, a novel approach that\naddresses these challenges by leveraging LoRA modules, a representative\nparameter-efficient fine-tuning (PEFT) method. Our approach enables efficient\nand parallel domain-adaptive pre-training that is robust to domain order and\neffectively utilizes accumulated knowledge to provide tailored pre-trained\nmodels for specific tasks. We also demonstrate that our method can be extended\nbeyond the DAP setting to standard LLM fine-tuning scenarios. Code is available\nat https://github.com/dohoonkim-ai/DoMIX.", "comment": "22 pages, 5 figures, ACL 2025 Main", "pdf_url": "http://arxiv.org/pdf/2507.02302v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DoMIX：一个用于微调中利用领域知识的有效框架", "tldr": "DoMIX是一个高效的框架，利用LoRA模块解决了现有持续领域自适应预训练（continual DAP）方法计算成本高、对数据顺序敏感以及无法提供特定任务模型的问题，并能扩展到LLM微调。", "motivation": "现有的持续领域自适应预训练（continual DAP）方法存在高计算成本和内存使用、对增量数据顺序敏感以及为所有最终任务提供单一泛化模型（与DAP本质相悖）的局限性。", "method": "本文提出了DoMIX，一个新颖的方法，通过利用LoRA模块（一种参数高效微调PEFT方法）来解决现有持续DAP的挑战。该方法实现了高效、并行的领域自适应预训练。", "result": "DoMIX实现了高效、并行的领域自适应预训练，对领域顺序具有鲁棒性，并能有效利用累积知识为特定任务提供定制的预训练模型。此外，该方法可以扩展到标准的大型语言模型（LLM）微调场景。", "conclusion": "DoMIX通过引入LoRA模块，有效解决了现有持续领域自适应预训练方法的计算效率、数据顺序敏感性及模型泛化性问题，并展现了其在更广泛的LLM微调场景中的适用性。", "translation": "领域自适应预训练（DAP）最近因其在微调预训练模型方面的有效性而受到关注。在此基础上，持续DAP已被探索用于开发能够逐步整合不同领域数据集的预训练模型。然而，现有的持续DAP方法面临几个局限性：（1）训练期间计算成本高和GPU内存使用量大；（2）对增量数据顺序敏感；（3）为所有最终任务提供单一的通用模型，这与DAP的本质相悖。在本文中，我们提出了DoMIX，一种通过利用LoRA模块（一种具有代表性的参数高效微调（PEFT）方法）来解决这些挑战的新颖方法。我们的方法能够实现高效并行的领域自适应预训练，该训练对领域顺序具有鲁棒性，并能有效利用累积知识为特定任务提供定制的预训练模型。我们还证明了我们的方法可以扩展到DAP设置之外的标准LLM微调场景。代码可在https://github.com/dohoonkim-ai/DoMIX获取。", "summary": "本文提出了DoMIX，一个高效的框架，旨在解决现有持续领域自适应预训练（DAP）方法的局限性，包括高计算成本、对数据顺序敏感以及提供单一泛化模型的问题。DoMIX利用参数高效微调（PEFT）的LoRA模块，实现了高效、并行的领域自适应预训练，该方法对领域顺序具有鲁棒性，并能为特定任务提供定制化的预训练模型。此外，DoMIX还被证明可应用于标准的大型语言模型（LLM）微调场景。", "keywords": "领域自适应预训练, LoRA, 参数高效微调, 持续学习, 大模型微调", "comments": "DoMIX的创新之处在于将LoRA模块引入到持续领域自适应预训练中，有效解决了现有方法在效率和灵活性上的核心痛点。通过PEFT的应用，它显著降低了计算资源需求并增强了对数据顺序的鲁棒性，同时能够根据特定任务需求生成定制模型，这对于需要持续学习和适应新领域的实际应用具有重要意义。"}}
{"id": "2507.02606", "title": "De-AntiFake: Rethinking the Protective Perturbations Against Voice Cloning Attacks", "authors": ["Wei Fan", "Kejiang Chen", "Chang Liu", "Weiming Zhang", "Nenghai Yu"], "categories": ["cs.SD", "cs.AI", "cs.CR", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted by ICML 2025", "url": "http://arxiv.org/abs/2507.02606v1", "summary": "The rapid advancement of speech generation models has heightened privacy and\nsecurity concerns related to voice cloning (VC). Recent studies have\ninvestigated disrupting unauthorized voice cloning by introducing adversarial\nperturbations. However, determined attackers can mitigate these protective\nperturbations and successfully execute VC. In this study, we conduct the first\nsystematic evaluation of these protective perturbations against VC under\nrealistic threat models that include perturbation purification. Our findings\nreveal that while existing purification methods can neutralize a considerable\nportion of the protective perturbations, they still lead to distortions in the\nfeature space of VC models, which degrades the performance of VC. From this\nperspective, we propose a novel two-stage purification method: (1) Purify the\nperturbed speech; (2) Refine it using phoneme guidance to align it with the\nclean speech distribution. Experimental results demonstrate that our method\noutperforms state-of-the-art purification methods in disrupting VC defenses.\nOur study reveals the limitations of adversarial perturbation-based VC defenses\nand underscores the urgent need for more robust solutions to mitigate the\nsecurity and privacy risks posed by VC. The code and audio samples are\navailable at https://de-antifake.github.io.", "comment": "Accepted by ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.02606v1", "cate": "cs.SD", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "De-AntiFake：重新思考对抗语音克隆攻击的保护性扰动", "tldr": "现有对抗语音克隆的防御（基于对抗性扰动）容易被净化攻击规避。本文首次系统评估了这些防御的局限性，并提出了一种新的两阶段净化方法来提高攻击效果，强调了开发更鲁棒解决方案的紧迫性。", "motivation": "语音生成模型的快速发展带来了语音克隆（VC）相关的隐私和安全担忧。尽管已研究通过引入对抗性扰动来阻止未经授权的VC，但强大的攻击者仍能规避这些保护。本研究旨在首次系统评估在包含扰动净化的现实威胁模型下，这些保护性扰动对VC的有效性，并揭示其局限性。", "method": "本研究首次在包含扰动净化的现实威胁模型下，系统评估了针对VC的保护性扰动。基于现有净化方法仍导致VC模型特征空间失真的发现，论文提出了一种新颖的两阶段净化方法：(1) 净化受扰动的语音；(2) 使用音素指导进行细化，使其与干净语音分布对齐。", "result": "研究发现，虽然现有净化方法能中和大部分保护性扰动，但仍会导致VC模型特征空间失真，从而降低VC性能。提出的两阶段净化方法在破坏VC防御方面优于最先进的净化方法。", "conclusion": "本研究揭示了基于对抗性扰动的VC防御的局限性，并强调了迫切需要更鲁棒的解决方案来减轻VC带来的安全和隐私风险。", "translation": "语音生成模型的快速发展加剧了与语音克隆（VC）相关的隐私和安全担忧。最近的研究调查了通过引入对抗性扰动来干扰未经授权的语音克隆。然而，坚定的攻击者可以减轻这些保护性扰动并成功执行语音克隆。在本研究中，我们首次对这些针对VC的保护性扰动在包含扰动净化的现实威胁模型下进行了系统评估。我们的发现表明，虽然现有的净化方法可以中和相当一部分保护性扰动，但它们仍然导致VC模型特征空间中的失真，从而降低了VC的性能。从这个角度出发，我们提出了一种新颖的两阶段净化方法：（1）净化受扰动的语音；（2）使用音素指导对其进行细化，使其与干净语音分布对齐。实验结果表明，我们的方法在破坏VC防御方面优于最先进的净化方法。我们的研究揭示了基于对抗性扰动的VC防御的局限性，并强调迫切需要更强大的解决方案来减轻VC带来的安全和隐私风险。代码和音频样本可在https://de-antifake.github.io获取。", "summary": "本文系统评估了现有基于对抗性扰动的语音克隆（VC）防御在包含扰动净化的现实威胁模型下的有效性。研究发现，尽管现有净化方法能中和部分扰动，但仍会导致特征空间失真。为解决此问题，作者提出了一种新颖的两阶段净化方法，首先净化受扰语音，然后通过音素指导进行细化。实验结果表明，该方法在破坏VC防御方面表现更优，揭示了当前防御的局限性，并强调了开发更鲁棒解决方案的必要性。", "keywords": "语音克隆, 对抗性扰动, 扰动净化, 语音安全", "comments": "本文对现有基于对抗性扰动的语音克隆防御进行了批判性重新评估。其创新之处在于系统地揭示了这些防御在面对净化攻击时的脆弱性，并提出了一种更有效的两阶段净化方法。这项研究意义重大，因为它暴露了现有保护措施的一个重要安全漏洞，并强调了迫切需要更鲁棒的对抗VC策略，将关注点从单纯添加扰动转向考虑其对高级攻击的韧性。"}}
{"id": "2507.02689", "title": "On the Convergence of Large Language Model Optimizer for Black-Box Network Management", "authors": ["Hoon Lee", "Wentao Zhou", "Merouane Debbah", "Inkyu Lee"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02689v1", "summary": "Future wireless networks are expected to incorporate diverse services that\noften lack general mathematical models. To address such black-box network\nmanagement tasks, the large language model (LLM) optimizer framework, which\nleverages pretrained LLMs as optimization agents, has recently been promoted as\na promising solution. This framework utilizes natural language prompts\ndescribing the given optimization problems along with past solutions generated\nby LLMs themselves. As a result, LLMs can obtain efficient solutions\nautonomously without knowing the mathematical models of the objective\nfunctions. Although the viability of the LLM optimizer (LLMO) framework has\nbeen studied in various black-box scenarios, it has so far been limited to\nnumerical simulations. For the first time, this paper establishes a theoretical\nfoundation for the LLMO framework. With careful investigations of LLM inference\nsteps, we can interpret the LLMO procedure as a finite-state Markov chain, and\nprove the convergence of the framework. Our results are extended to a more\nadvanced multiple LLM architecture, where the impact of multiple LLMs is\nrigorously verified in terms of the convergence rate. Comprehensive numerical\nsimulations validate our theoretical results and provide a deeper understanding\nof the underlying mechanisms of the LLMO framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02689v1", "cate": "cs.IT", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "关于大型语言模型优化器在黑盒网络管理中收敛性的研究", "tldr": "本文首次为大型语言模型优化器（LLMO）框架在黑盒网络管理中的应用建立了理论基础，通过将其解释为有限状态马尔可夫链，证明了其收敛性，并扩展到多LLM架构，通过仿真验证了理论结果。", "motivation": "解决未来无线网络中缺乏通用数学模型的黑盒网络管理任务，LLM优化器框架虽有潜力但在理论基础方面存在空白，尤其缺乏收敛性证明。", "method": "通过仔细研究LLM的推理步骤，将LLM优化器（LLMO）过程解释为一个有限状态马尔可夫链，并基于此证明其收敛性。研究还扩展到多LLM架构。", "result": "成功建立了LLMO框架的理论基础，证明了其收敛性。在多LLM架构下，严格验证了多个LLM对收敛速度的影响。综合数值模拟验证了理论结果。", "conclusion": "本文首次从理论上证明了LLM优化器框架在黑盒网络管理中的收敛性，并阐明了其在多LLM架构下的行为，为LLMO的实际应用提供了坚实的理论支撑。", "translation": "未来的无线网络预计将融合多样化的服务，这些服务通常缺乏通用的数学模型。为解决此类黑盒网络管理任务，大型语言模型（LLM）优化器框架——该框架利用预训练LLM作为优化代理——最近被推广为一种有前景的解决方案。该框架利用描述给定优化问题的自然语言提示以及LLM自身生成的历史解决方案。因此，LLM无需了解目标函数的数学模型即可自主获得高效的解决方案。尽管LLM优化器（LLMO）框架的适用性已在各种黑盒场景中进行过研究，但迄今为止仅限于数值模拟。本文首次为LLMO框架奠定了理论基础。通过对LLM推理步骤的仔细研究，我们可以将LLMO过程解释为一个有限状态马尔可夫链，并证明该框架的收敛性。我们的结果被推广到更高级的多LLM架构，其中多个LLM的影响在收敛速度方面得到了严格验证。全面的数值模拟验证了我们的理论结果，并提供了对LLMO框架底层机制的更深入理解。", "summary": "本文首次为大型语言模型优化器（LLMO）在黑盒网络管理中的应用提供了理论基础。针对未来无线网络中服务缺乏数学模型的挑战，LLMO框架利用LLM作为优化代理，通过自然语言提示和历史解决方案自主寻找高效解。研究将LLMO过程建模为有限状态马尔可夫链，并首次从理论上证明了其收敛性。此外，工作还分析了多LLM架构对收敛速度的影响，并通过全面的数值模拟验证了理论发现。", "keywords": "大型语言模型优化器,黑盒网络管理,收敛性,马尔可夫链,无线网络", "comments": "这篇论文的创新点在于首次为LLM优化器框架提供了严格的理论基础，将其建模为马尔可夫链并证明了收敛性，弥补了先前研究仅限于数值模拟的不足。这对于推动LLM在实际黑盒优化，特别是无线网络管理中的应用具有重要意义。其局限性可能在于马尔可夫链模型的简化程度以及理论假设与实际复杂LLM行为之间的差距。"}}
{"id": "2507.02424", "title": "CyberRAG: An agentic RAG cyber attack classification and reporting tool", "authors": ["Francesco Blefari", "Cristian Cosentino", "Francesco Aurelio Pironti", "Angelo Furfaro", "Fabrizio Marozzo"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02424v1", "summary": "Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can\ngenerate hundreds of thousands of alerts per hour, overwhelming security\nanalysts with logs that demand deep, rapidly evolving domain expertise.\nConventional machine-learning detectors trim the alert volume but still yield\nhigh false-positive rates, while standard single-pass Retrieval-Augmented\nGeneration (RAG) pipelines often retrieve irrelevant context and fail to\njustify their predictions. To overcome these shortcomings, we present CyberRAG,\na modular, agent-based RAG framework that delivers real-time classification,\nexplanation, and structured reporting for cyber-attacks. A central LLM agent\norchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to\na distinct attack family; (ii) tool adapters for enrichment and alerting; and\n(iii) an iterative retrieval-and-reason loop that continuously queries a\ndomain-specific knowledge base until the evidence is both relevant and\nself-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic\ndesign that enables dynamic control flow and adaptive reasoning. This\nagent-centric architecture refines its threat labels and natural-language\njustifications autonomously, reducing false positives and enhancing\ninterpretability. The framework is fully extensible: new attack types can be\nsupported by simply adding a classifier without retraining the core agent.\nCyberRAG has been evaluated achieving over 94% accuracy per class and pushing\nfinal classification accuracy to 94.92% through semantic orchestration.\nGenerated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based\nexpert evaluation. These results show that agentic, specialist-oriented RAG can\npair high detection accuracy with trustworthy, SOC-ready prose, offering a\npractical and scalable path toward semi-autonomous cyber-defence workflows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02424v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "CyberRAG：一种代理式RAG网络攻击分类与报告工具", "tldr": "CyberRAG是一个代理式RAG框架，通过协调专业分类器和迭代检索推理循环，实现网络攻击的实时分类、解释和结构化报告，显著减少误报并提高准确性。", "motivation": "大型企业中的入侵检测和防御系统（IDS/IPS）每小时生成数十万条警报，使安全分析师不堪重负，需要深入且快速演进的领域专业知识。传统的机器学习检测器虽然能减少警报量，但误报率高；标准的单通道检索增强生成（RAG）管道常检索到不相关上下文且无法解释其预测。", "method": "CyberRAG是一个模块化的代理式RAG框架。其核心是一个大型语言模型（LLM）代理，负责协调：(i) 一组针对不同攻击家族进行微调的专业分类器；(ii) 用于丰富信息和警报的工具适配器；以及 (iii) 一个迭代的检索-推理循环，持续查询领域特定知识库，直到证据既相关又自洽。其代理式设计实现了动态控制流和自适应推理，自主优化威胁标签和自然语言解释。", "result": "CyberRAG在每类攻击上实现了超过94%的准确率，通过语义编排将最终分类准确率推高至94.92%。生成的解释在BERTScore上得分高达0.94，在基于GPT-4的专家评估中获得4.9/5分。", "conclusion": "代理式、面向专家的RAG方法可以将高检测准确性与可信赖的、可用于SOC的文本相结合，为半自主网络防御工作流提供了一条实用且可扩展的途径。", "translation": "大型企业中的入侵检测和防御系统（IDS/IPS）每小时可生成数十万条警报，使安全分析师被大量日志淹没，这些日志需要深入且快速演进的领域专业知识。传统的机器学习检测器虽然能减少警报量，但仍然产生高误报率，而标准的单通道检索增强生成（RAG）管道通常检索到不相关的上下文，并且无法证明其预测的合理性。为了克服这些缺点，我们提出了CyberRAG，一个模块化的、基于代理的RAG框架，它能为网络攻击提供实时分类、解释和结构化报告。一个中央LLM代理协调：(i) 一组经过微调的专业分类器，每个分类器都针对一个独特的攻击家族；(ii) 用于丰富信息和警报的工具适配器；以及 (iii) 一个迭代的检索-推理循环，持续查询领域特定知识库，直到证据既相关又自洽。与传统的RAG系统不同，CyberRAG采用代理式设计，实现了动态控制流和自适应推理。这种以代理为中心的架构自主地细化其威胁标签和自然语言解释，减少了误报并增强了可解释性。该框架是完全可扩展的：只需添加分类器而无需重新训练核心代理，即可支持新的攻击类型。CyberRAG经过评估，每类攻击的准确率超过94%，通过语义编排将最终分类准确率推高至94.92%。生成的解释在BERTScore中得分高达0.94，在基于GPT-4的专家评估中获得4.9/5分。这些结果表明，代理式、面向专家的RAG可以将高检测准确性与可信赖的、可用于SOC的文本相结合，为半自主网络防御工作流提供了一条实用且可扩展的途径。", "summary": "CyberRAG是一个创新的代理式检索增强生成（RAG）框架，旨在解决传统入侵检测系统和RAG管道在处理大量网络攻击警报时面临的误报高和解释性差的问题。它通过一个中央LLM代理协调专业分类器、工具适配器和迭代检索-推理循环，实现对网络攻击的实时、高准确度分类、详细解释和结构化报告。其代理式设计使其能够动态适应和自主优化，显著提高了检测准确性并提供了可信赖的解释，为构建半自主网络防御系统提供了实用且可扩展的解决方案。", "keywords": "网络攻击分类, 检索增强生成, 代理系统, 入侵检测, LLM", "comments": "CyberRAG的创新之处在于其代理式RAG框架，它超越了传统的单通道RAG系统，引入了动态控制流和自适应推理能力。通过LLM代理协调多个专业分类器和迭代检索机制，有效解决了误报高和解释性差的问题，尤其在网络安全领域具有重要实践意义。其模块化和可扩展性也值得称赞。"}}
{"id": "2507.02318", "title": "Precisely Detecting Python Type Errors via LLM-based Unit Test Generation", "authors": ["Chen Yang", "Ziqi Wang", "Yanjie Jiang", "Lin Yang", "Yuteng Zheng", "Jianyi Zhou", "Junjie Chen"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02318v1", "summary": "Type errors in Python often lead to runtime failures, posing significant\nchallenges to software reliability and developer productivity. Existing static\nanalysis tools aim to detect such errors without execution but frequently\nsuffer from high false positive rates. Recently, unit test generation\ntechniques offer great promise in achieving high test coverage, but they often\nstruggle to produce bug-revealing tests without tailored guidance. To address\nthese limitations, we present RTED, a novel type-aware test generation\ntechnique for automatically detecting Python type errors. Specifically, RTED\ncombines step-by-step type constraint analysis with reflective validation to\nguide the test generation process and effectively suppress false positives. We\nevaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.\nExperimental results show that RTED can detect 22-29 more benchmarked type\nerrors than four state-of-the-art techniques. RTED is also capable of producing\nfewer false positives, achieving an improvement of 173.9%-245.9% in precision.\nFurthermore, RTED successfully discovered 12 previously unknown type errors\nfrom six real-world open-source Python projects.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02318v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过基于LLM的单元测试生成精确检测Python类型错误", "tldr": "RTED是一种新型的类型感知测试生成技术，能够比现有方法更精确地检测Python类型错误，减少误报，并发现了真实项目中的新错误。", "motivation": "Python类型错误常导致运行时故障，影响软件可靠性和开发效率。现有静态分析工具误报率高，而单元测试生成技术在缺乏指导时难以发现实际错误。", "method": "本文提出了RTED，一种新颖的类型感知测试生成技术。它结合了逐步类型约束分析和反射验证来指导测试生成过程，并有效抑制误报。", "result": "RTED在BugsInPy和TypeBugs基准测试中比四种最先进技术多检测出22-29个类型错误。其精度提高了173.9%-245.9%，误报率更低。此外，RTED还在六个真实世界开源Python项目中成功发现了12个以前未知的类型错误。", "conclusion": "RTED是一种有效且精确的Python类型错误检测技术，在错误检测能力和误报抑制方面均优于现有方法，并能发现实际应用中的新错误。", "translation": "Python 中的类型错误经常导致运行时故障，对软件可靠性和开发人员生产力构成重大挑战。现有的静态分析工具旨在不执行的情况下检测此类错误，但经常遭受高误报率。最近，单元测试生成技术在实现高测试覆盖率方面显示出巨大的前景，但它们通常难以在没有定制指导的情况下生成揭示错误的测试。为了解决这些限制，我们提出了 RTED，一种新颖的类型感知测试生成技术，用于自动检测 Python 类型错误。具体来说，RTED 将逐步类型约束分析与反射验证相结合，以指导测试生成过程并有效抑制误报。我们在两个广泛使用的基准测试 BugsInPy 和 TypeBugs 上评估了 RTED。实验结果表明，RTED 比四种最先进的技术多检测出 22-29 个基准类型错误。RTED 还能够产生更少的误报，在精度方面实现了 173.9%-245.9% 的改进。此外，RTED 成功地从六个真实世界的开源 Python 项目中发现了 12 个以前未知的类型错误。", "summary": "RTED是一种新型的类型感知测试生成技术，旨在解决Python类型错误检测中现有静态分析工具误报率高和单元测试生成缺乏有效指导的问题。RTED通过结合逐步类型约束分析和反射验证来精确指导测试生成并抑制误报。在BugsInPy和TypeBugs基准测试上的实验结果表明，RTED比现有最先进技术能检测到更多类型错误，并显著降低了误报率。此外，RTED还成功发现了真实开源项目中的12个未知类型错误。", "keywords": "Python类型错误, 单元测试生成, 类型感知, 误报抑制, RTED", "comments": "该论文的创新之处在于结合了类型约束分析和反射验证来指导测试生成，从而有效减少误报并提高错误检测率。其在真实世界项目中发现未知错误的能力凸显了其实用价值。该方法似乎解决了Python开发中的一个重要痛点。"}}
{"id": "2507.02313", "title": "A Vehicle-in-the-Loop Simulator with AI-Powered Digital Twins for Testing Automated Driving Controllers", "authors": ["Zengjie Zhang", "Giannis Badakis", "Michalis Galanis", "Adem Bavarşi", "Edwin van Hassel", "Mohsen Alirezaei", "Sofie Haesaert"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02313v1", "summary": "Simulators are useful tools for testing automated driving controllers.\nVehicle-in-the-loop (ViL) tests and digital twins (DTs) are widely used\nsimulation technologies to facilitate the smooth deployment of controllers to\nphysical vehicles. However, conventional ViL tests rely on full-size vehicles,\nrequiring large space and high expenses. Also, physical-model-based DT suffers\nfrom the reality gap caused by modeling imprecision. This paper develops a\ncomprehensive and practical simulator for testing automated driving controllers\nenhanced by scaled physical cars and AI-powered DT models. The scaled cars\nallow for saving space and expenses of simulation tests. The AI-powered DT\nmodels ensure superior simulation fidelity. Moreover, the simulator integrates\nwell with off-the-shelf software and control algorithms, making it easy to\nextend. We use a filtered control benchmark with formal safety guarantees to\nshowcase the capability of the simulator in validating automated driving\ncontrollers. Experimental studies are performed to showcase the efficacy of the\nsimulator, implying its great potential in validating control solutions for\nautonomous vehicles and intelligent traffic.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02313v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "具有AI驱动数字孪生的车载环仿真器，用于测试自动驾驶控制器", "tldr": "本文提出了一种结合缩比物理车辆和AI驱动数字孪生的车载环仿真器，旨在解决传统车载环测试成本高昂、空间需求大以及物理模型数字孪生精度不足的问题，从而更经济高效地测试自动驾驶控制器。", "motivation": "传统的车载环 (ViL) 测试依赖于全尺寸车辆，导致高昂的费用和巨大的空间需求。此外，基于物理模型的数字孪生 (DT) 存在因建模不精确而导致的“现实差距”。因此，需要一种更经济、高效且高保真度的自动驾驶控制器测试模拟器。", "method": "本文开发了一个综合且实用的模拟器，用于测试自动驾驶控制器。该模拟器通过结合缩比物理车辆和AI驱动的数字孪生模型来增强功能。缩比车辆用于节省仿真测试的空间和费用，而AI驱动的数字孪生模型则用于确保卓越的仿真保真度。此外，该模拟器能与现成软件和控制算法良好集成，易于扩展。研究人员使用带有正式安全保证的滤波控制基准来展示模拟器在验证自动驾驶控制器方面的能力，并进行了实验研究以证明其功效。", "result": "该模拟器通过使用缩比车辆成功节省了仿真测试的空间和费用。AI驱动的数字孪生模型确保了卓越的仿真保真度。实验研究证明了该模拟器的有效性，表明其能够成功验证自动驾驶控制器。", "conclusion": "该模拟器在验证自动驾驶汽车和智能交通的控制解决方案方面具有巨大潜力，为自动驾驶控制器的测试提供了一种经济高效且高保真度的方法。", "translation": "模拟器是测试自动驾驶控制器的有用工具。车载环 (ViL) 测试和数字孪生 (DT) 是广泛使用的仿真技术，可促进控制器顺利部署到物理车辆。然而，传统的车载环测试依赖于全尺寸车辆，需要大空间和高费用。此外，基于物理模型的数字孪生存在因建模不精确导致的现实差距。本文开发了一种综合且实用的模拟器，用于测试由缩比物理车辆和AI驱动数字孪生模型增强的自动驾驶控制器。缩比车辆可以节省仿真测试的空间和费用。AI驱动的数字孪生模型确保了卓越的仿真保真度。此外，该模拟器与现成软件和控制算法良好集成，易于扩展。我们使用带有正式安全保证的滤波控制基准来展示该模拟器在验证自动驾驶控制器方面的能力。实验研究表明了该模拟器的功效，这表明其在验证自动驾驶汽车和智能交通的控制解决方案方面具有巨大潜力。", "summary": "本文提出了一种新型车载环仿真器，旨在通过结合缩比物理车辆和AI驱动的数字孪生模型，克服传统车载环测试在成本和空间上的局限性，并提高仿真精度。该仿真器利用缩比车辆降低了测试成本和空间需求，并利用AI驱动的数字孪生模型提高了仿真保真度。它能与现有软件和控制算法良好集成，易于扩展。实验研究已验证了该模拟器在自动驾驶控制器测试中的有效性，展示了其在自动驾驶和智能交通领域应用的巨大潜力。", "keywords": "车载环仿真器, 数字孪生, 自动驾驶控制器, 缩比车辆, 仿真保真度", "comments": "这篇论文的创新点在于将缩比物理车辆与AI驱动的数字孪生技术相结合，有效解决了传统车载环测试在成本和空间上的局限性，同时提升了仿真精度。AI驱动的数字孪生模型是关键，它有望弥补物理建模的“现实差距”。该模拟器易于扩展的特性也增加了其实用价值。其重要性体现在为自动驾驶控制器的开发和验证提供了一个更经济、高效且高保真度的测试平台，有望加速自动驾驶技术的落地。"}}
{"id": "2507.02229", "title": "An Exploration of Internal States in Collaborative Problem Solving", "authors": ["Sifatul Anindho", "Videep Venkatesha", "Mariah Bradford", "Anne M. Cleary", "Nathaniel Blanchard"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted to the International Conference on Human-Computer Interaction (HCII) 2025", "url": "http://arxiv.org/abs/2507.02229v1", "summary": "Collaborative problem solving (CPS) is a complex cognitive, social, and\nemotional process that is increasingly prevalent in educational and\nprofessional settings. This study investigates the emotional states of\nindividuals during CPS using a mixed-methods approach. Teams of four first\ncompleted a novel CPS task. Immediately after, each individual was placed in an\nisolated room where they reviewed the video of their group performing the task\nand self-reported their internal experiences throughout the task. We performed\na linguistic analysis of these internal monologues, providing insights into the\nrange of emotions individuals experience during CPS. Our analysis showed\ndistinct patterns in language use, including characteristic unigrams and\nbigrams, key words and phrases, emotion labels, and semantic similarity between\nemotion-related words.", "comment": "Accepted to the International Conference on Human-Computer\n  Interaction (HCII) 2025", "pdf_url": "http://arxiv.org/pdf/2507.02229v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "协同问题解决中内部状态的探索", "tldr": "研究通过语言分析探索了协同问题解决过程中个体的内部情感体验。", "motivation": "协同问题解决（CPS）是一个复杂的认知、社会和情感过程，在教育和专业环境中日益普遍。本研究旨在调查个体在CPS过程中的情感状态。", "method": "采用混合方法研究。四人小组完成一项新的CPS任务后，每位成员单独回顾任务视频并口头报告其内部体验。随后对这些内部独白进行语言分析，以识别情感模式。", "result": "分析揭示了语言使用中的独特模式，包括特征性的一元词、二元词、关键词、情感标签以及情感相关词汇之间的语义相似性，从而深入了解个体在CPS中经历的情感范围。", "conclusion": "论文通过对内部独白的语言分析，成功揭示了协同问题解决过程中个体情感体验的独特模式。", "translation": "协同问题解决（CPS）是一个复杂的认知、社会和情感过程，在教育和专业环境中日益普遍。本研究采用混合方法，调查了个人在协同问题解决过程中的情感状态。首先，四人小组完成了一项新颖的协同问题解决任务。紧接着，每位成员被安排在一个独立的房间里，回顾其团队执行任务的视频，并自我报告任务过程中他们的内部体验。我们对这些内部独白进行了语言分析，从而深入了解了个人在协同问题解决过程中所经历的情感范围。我们的分析显示了语言使用中的独特模式，包括特征性的一元词和二元词、关键词和短语、情感标签以及情感相关词汇之间的语义相似性。", "summary": "本研究采用混合方法，深入探讨了协同问题解决（CPS）过程中个体的内部情感状态。通过让参与者回顾任务视频并自我报告内部独白，并对其进行语言分析，研究揭示了在CPS情境下，个体情感体验在语言表达上的独特模式，包括特定的词汇使用和情感词汇间的语义联系。", "keywords": "协同问题解决, 情感状态, 语言分析, 内部独白, 混合方法", "comments": "这篇论文通过结合视频回顾和自我报告独白的方式，创新性地捕捉了协同问题解决过程中难以量化的内部情感体验。其语言分析方法为理解复杂协作环境中的情感动态提供了新的视角和具体证据，对于教育和团队管理领域具有潜在应用价值。"}}
{"id": "2507.02660", "title": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification", "authors": ["Deepak Narayan Gadde", "Keerthan Kopparam Radhakrishna", "Vaisakh Naduvodi Viswambharan", "Aman Kumar", "Djones Lettnin", "Wolfgang Kunz", "Sebastian Simon"], "categories": ["cs.AI", "cs.AR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      To appear at the 38th SBC/SBMicro/IEEE Symposium on Integrated Circuits and Systems Design (SBCCI), August 25-29, 2025, Manaus, BRAZIL", "url": "http://arxiv.org/abs/2507.02660v1", "summary": "Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is\ntheir development process. Hardware design verification entails a methodical\nand disciplined approach to the planning, development, execution, and sign-off\nof functionally correct hardware designs. This tedious process requires\nsignificant effort and time to ensure a bug-free tape-out. The field of Natural\nLanguage Processing has undergone a significant transformation with the advent\nof Large Language Models (LLMs). These powerful models, often referred to as\nGenerative AI (GenAI), have revolutionized how machines understand and generate\nhuman language, enabling unprecedented advancements in a wide array of\napplications, including hardware design verification. This paper presents an\nagentic AI-based approach to hardware design verification, which empowers AI\nagents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage\nin a more dynamic, iterative, and self-reflective process, ultimately\nperforming end-to-end hardware design and verification. This methodology is\nevaluated on five open-source designs, achieving over 95% coverage with reduced\nverification time while demonstrating superior performance, adaptability, and\nconfigurability.", "comment": "To appear at the 38th SBC/SBMicro/IEEE Symposium on Integrated\n  Circuits and Systems Design (SBCCI), August 25-29, 2025, Manaus, BRAZIL", "pdf_url": "http://arxiv.org/pdf/2507.02660v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "嘿AI，给我生成硬件代码！基于Agentic AI的硬件设计与验证", "tldr": "本文提出了一种基于Agentic AI的方法，结合人工干预，实现端到端硬件设计和验证，显著提高了验证覆盖率并缩短了时间。", "motivation": "现代集成电路日益复杂，其开发过程（尤其是硬件设计验证）耗时且需要大量精力来确保无缺陷的流片。大型语言模型（LLMs）在自然语言处理领域的进步，为解决这一挑战提供了新的机遇。", "method": "本文提出了一种基于Agentic AI的方法，结合人工干预（Human-in-the-Loop, HITL），使AI代理能够进行动态、迭代和自反思的端到端硬件设计和验证过程。", "result": "该方法在五个开源设计上进行了评估，实现了超过95%的覆盖率，同时减少了验证时间，并展示了卓越的性能、适应性和可配置性。", "conclusion": "基于Agentic AI的硬件设计与验证方法能够有效应对复杂集成电路的开发挑战，提高效率和质量。", "translation": "现代集成电路（ICs）变得越来越复杂，其开发过程也同样如此。硬件设计验证需要一种有条不理、纪律严明的方法来规划、开发、执行和签署功能正确的硬件设计。这个繁琐的过程需要大量的精力和时间来确保无缺陷的流片。随着大型语言模型（LLMs）的出现，自然语言处理领域发生了重大转变。这些强大的模型，通常被称为生成式AI（GenAI），彻底改变了机器理解和生成人类语言的方式，从而在包括硬件设计验证在内的广泛应用中实现了前所未有的进步。本文提出了一种基于Agentic AI的硬件设计验证方法，它使AI代理与人工干预（Human-in-the-Loop，HITL）相结合，能够参与更动态、迭代和自反思的过程，最终执行端到端硬件设计和验证。该方法在五个开源设计上进行了评估，实现了超过95%的覆盖率，同时减少了验证时间，并展示了卓越的性能、适应性和可配置性。", "summary": "本文提出了一种创新的基于Agentic AI的硬件设计与验证方法，旨在解决现代集成电路日益增长的复杂性及其开发验证过程中的耗时问题。该方法结合了AI代理和人工干预，实现了动态、迭代和自反思的端到端硬件设计和验证流程。实验结果表明，该方法在多个开源设计上实现了超过95%的验证覆盖率，并显著缩短了验证时间，同时展现出优异的性能、适应性和可配置性。", "keywords": "Agentic AI, 硬件设计, 硬件验证, 大型语言模型, 生成式AI", "comments": "本文提出了一种将Agentic AI应用于硬件设计与验证的创新方法，其结合人工干预（HITL）的策略非常实用，能够有效平衡自动化与人工专家经验。该研究的重要性在于它为加速复杂硬件的开发周期、提高验证效率和质量提供了新的范式。其在覆盖率和时间上的表现令人印象深刻，预示着AI在硬件工程领域具有巨大的潜力。"}}
{"id": "2507.02613", "title": "MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking", "authors": ["Yalin E. Sagduyu", "Kemal Davaslioglu", "Tugba Erpek", "Sastry Kompella", "Gustave Anderson", "Jonathan Ashdown"], "categories": ["cs.NI", "cs.DC", "eess.SP"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02613v1", "summary": "This paper presents a complete signal-processing chain for multistatic\nintegrated sensing and communications (ISAC) using 5G Positioning Reference\nSignal (PRS). We consider a distributed architecture in which one gNB transmits\na periodic OFDM-PRS waveform while multiple spatially separated receivers\nexploit the same signal for target detection, parameter estimation and\ntracking. A coherent cross-ambiguity function (CAF) is evaluated to form a\nrange-Doppler map from which the bistatic delay and radial velocity are\nextracted for every target. For a single target, the resulting bistatic delays\nare fused through nonlinear least-squares trilateration, yielding a geometric\nposition estimate, and a regularized linear inversion of the radial-speed\nequations yields a two-dimensional velocity vector, where speed and heading are\nobtained. The approach is applied to 2D and 3D settings, extended to account\nfor time synchronization bias, and generalized to multiple targets by resolving\ntarget association. The sequence of position-velocity estimates is then fed to\nstandard and extended Kalman filters to obtain smoothed tracks. Our results\nshow high-fidelity moving-target detection, positioning, and tracking using 5G\nPRS signals for multistatic ISAC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02613v1", "cate": "cs.NI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MULTI-SCOUT：5G及未来多站集成感知与通信在移动目标检测、定位与跟踪中的应用", "tldr": "该论文提出并展示了一种用于移动目标检测、定位和跟踪的多站集成感知与通信（ISAC）信号处理链，利用5G定位参考信号（PRS）。", "motivation": "该研究旨在利用5G定位参考信号（PRS）在多站集成感知与通信（ISAC）设置中实现高精度移动目标检测、定位和跟踪，并提供一个完整的信号处理链来满足这一需求。", "method": "论文提出了一种分布式架构，其中一个gNB传输周期性OFDM-PRS波形，多个空间分离的接收器利用同一信号进行目标检测、参数估计和跟踪。通过评估相干交叉模糊函数（CAF）形成距离-多普勒图，从中提取每个目标的双基地延迟和径向速度。对于单个目标，通过非线性最小二乘三边测量融合双基地延迟以获得几何位置估计，并通过径向速度方程的正则化线性反演获得二维速度向量。该方法适用于2D和3D设置，并扩展以考虑时间同步偏差，通过解决目标关联泛化到多目标。位置-速度估计序列随后输入到标准和扩展卡尔曼滤波器以获得平滑轨迹。", "result": "研究结果表明，利用5G PRS信号进行多站ISAC能够实现高精度的移动目标检测、定位和跟踪。", "conclusion": "论文提出的利用5G PRS信号进行多站集成感知与通信（ISAC）的信号处理链是有效的，并在移动目标检测、定位和跟踪方面取得了高精度性能。", "translation": "本论文提出了一种完整的信号处理链，用于利用5G定位参考信号（PRS）的多站集成感知与通信（ISAC）。我们考虑一种分布式架构，其中一个gNB传输周期性OFDM-PRS波形，而多个空间分离的接收器利用同一信号进行目标检测、参数估计和跟踪。通过评估相干交叉模糊函数（CAF）来形成距离-多普勒图，从中提取每个目标的双基地延迟和径向速度。对于单个目标，通过非线性最小二乘三边测量融合所得的双基地延迟，生成几何位置估计，并通过径向速度方程的正则化线性反演获得二维速度向量，从而得到速度和航向。该方法应用于2D和3D设置，并扩展以考虑时间同步偏差，通过解决目标关联泛化到多目标。位置-速度估计序列随后输入到标准和扩展卡尔曼滤波器以获得平滑轨迹。我们的结果表明，利用5G PRS信号进行多站ISAC能够实现高精度的移动目标检测、定位和跟踪。", "summary": "本论文介绍了MULTI-SCOUT，一种利用5G定位参考信号（PRS）实现多站集成感知与通信（ISAC）的完整信号处理链。该系统采用分布式架构，一个gNB发射OFDM-PRS，多个接收器利用该信号进行目标检测、估计和跟踪。方法包括评估相干交叉模糊函数以获取距离-多普勒图，提取双基地延迟和径向速度，然后通过非线性最小二乘三边测量进行定位，并通过正则化线性反演获取速度。该方法还扩展到2D/3D场景、时间同步偏差补偿和多目标关联，并使用卡尔曼滤波器进行轨迹平滑。实验结果验证了利用5G PRS信号实现高精度移动目标检测、定位和跟踪的能力。", "keywords": "集成感知与通信（ISAC）, 5G, 定位参考信号（PRS）, 移动目标检测, 多站", "comments": "该论文的创新之处在于将5G PRS集成到多站ISAC系统中，并提供了一个从检测到跟踪的完整信号处理链。其在特定背景下使用CAF、三边测量和卡尔曼滤波器是其关键贡献。该研究的重要性在于其能够实现在5G及未来网络中精确的目标跟踪。"}}
{"id": "2507.02376", "title": "VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software", "authors": ["Chung-ju Huang", "Ziqi Zhang", "Yinggui Wang", "Binghui Wang", "Tao Wei", "Leye Wang"], "categories": ["cs.SE", "cs.AI", "cs.DC"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02376v1", "summary": "Vertical Federated Learning (VFL) is a distributed AI software deployment\nmechanism for cross-silo collaboration without accessing participants' data.\nHowever, existing VFL work lacks a mechanism to audit the execution correctness\nof the inference software of the data party. To address this problem, we design\na Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task\nparty to audit whether the data party's inference software is executed as\nexpected during large-scale inference without leaking the data privacy of the\ndata party or introducing additional latency to the inference system. The core\nof VeFIA is that the task party can use the inference results from a framework\nwith Trusted Execution Environments (TEE) and the coordinator to validate the\ncorrectness of the data party's computation results. VeFIA guarantees that, as\nlong as the abnormal inference exceeds 5.4%, the task party can detect\nexecution anomalies in the inference software with a probability of 99.99%,\nwithout incurring any additional online inference latency. VeFIA's random\nsampling validation achieves 100% positive predictive value, negative\npredictive value, and true positive rate in detecting abnormal inference. To\nthe best of our knowledge, this is the first paper to discuss the correctness\nof inference software execution in VFL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02376v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "VeFIA：一种用于垂直联邦协作软件的高效推理审计框架", "tldr": "VeFIA是一个用于垂直联邦学习的推理审计框架，可以在不泄露数据隐私或增加延迟的情况下，高效地审计数据方的推理软件执行正确性。", "motivation": "现有的垂直联邦学习（VFL）缺乏一种机制来审计数据方推理软件的执行正确性。", "method": "VeFIA通过任务方利用可信执行环境（TEE）框架和协调器的推理结果来验证数据方计算结果的正确性，核心是随机抽样验证。", "result": "VeFIA保证当异常推理超过5.4%时，任务方能以99.99%的概率检测到执行异常，不引入额外在线推理延迟。其随机抽样验证在检测异常推理时实现了100%的阳性预测值、阴性预测值和真阳性率。", "conclusion": "VeFIA是首个讨论垂直联邦学习中推理软件执行正确性的框架，有效解决了现有VFL缺乏审计机制的问题。", "translation": "垂直联邦学习（VFL）是一种分布式人工智能软件部署机制，用于跨孤岛协作，无需访问参与者数据。然而，现有的VFL工作缺乏一种机制来审计数据方推理软件的执行正确性。为了解决这个问题，我们设计了一个垂直联邦推理审计（VeFIA）框架。VeFIA帮助任务方在大规模推理过程中审计数据方的推理软件是否按预期执行，同时不泄露数据方的隐私或给推理系统引入额外的延迟。VeFIA的核心是任务方可以使用来自可信执行环境（TEE）框架和协调器的推理结果来验证数据方计算结果的正确性。VeFIA保证，只要异常推理超过5.4%，任务方就能以99.99%的概率检测到推理软件的执行异常，而不会产生任何额外的在线推理延迟。VeFIA的随机抽样验证在检测异常推理时实现了100%的阳性预测值、阴性预测值和真阳性率。据我们所知，这是第一篇讨论VFL中推理软件执行正确性的论文。", "summary": "本文提出VeFIA框架，旨在解决垂直联邦学习（VFL）中数据方推理软件执行缺乏审计机制的问题。VeFIA允许任务方在不影响数据隐私和推理延迟的前提下，利用可信执行环境（TEE）和协调器验证数据方的计算结果。实验表明，VeFIA能高效且准确地检测出推理异常，且不增加在线推理延迟，填补了VFL领域在推理软件正确性审计方面的空白。", "keywords": "垂直联邦学习, 推理审计, 可信执行环境, 数据隐私, 执行正确性", "comments": "VeFIA的创新之处在于其首次提出了VFL中推理软件执行正确性的审计机制，并利用TEE技术确保了审计过程的数据隐私和效率。其100%的预测值和真阳性率表现出很高的实用价值，对于推动VFL在敏感数据场景下的应用具有重要意义。"}}
{"id": "2507.02193", "title": "A Multi-Scale Finite Element Method for Investigating Fiber Remodeling in Hypertrophic Cardiomyopathy", "authors": ["Mohammad Mehri", "Kenneth S. Campbell", "Lik Chuan Lee", "Jonathan F. Wenk"], "categories": ["q-bio.TO", "cs.CE"], "primary_category": "Subjects:       Tissues and Organs (q-bio.TO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02193v1", "summary": "A significant hallmark of hypertrophic cardiomyopathy (HCM) is fiber\ndisarray, which is associated with various cardiac events such as heart\nfailure. Quantifying fiber disarray remains critical for understanding the\ndisease s complex pathophysiology. This study investigates the role of\nheterogeneous HCM-induced cellular abnormalities in the development of fiber\ndisarray and their subsequent impact on cardiac pumping function. Fiber\ndisarray is predicted using a stress-based law to reorient myofibers and\ncollagen within a multiscale finite element cardiac modeling framework, MyoFE.\nSpecifically, the model is used to quantify the distinct impacts of\nheterogeneous distributions of hypercontractility, hypocontractility, and\nfibrosis on fiber disarray development and examines their effect on functional\ncharacteristics of the heart. Our results show that heterogenous cell level\nabnormalities highly disrupt the normal mechanics of myocardium and lead to\nsignificant fiber disarray. The pattern of disarray varies depending on the\nspecific perturbation, offering valuable insights into the progression of HCM.\nDespite the random distribution of perturbed regions within the cardiac muscle,\nsignificantly higher fiber disarray is observed near the epicardium compared to\nthe endocardium across all perturbed left ventricle (LV) models. This regional\ndifference in fiber disarray, irrespective of perturbation severity, aligns\nwith previous DT-MRI studies, highlighting the role of regional myocardial\nmechanics in the development of fiber disarray. Furthermore, cardiac\nperformance declined in the remodeled LVs, particularly in those with fibrosis\nand hypocontractility. These findings provide important insights into the\nstructural and functional consequences of HCM and offer a framework for future\ninvestigations into therapeutic interventions targeting cardiac remodeling.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02193v1", "cate": "q-bio.TO", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "一种用于研究肥厚性心肌病中纤维重构的多尺度有限元方法", "tldr": "本研究利用多尺度有限元模型MyoFE，通过应力驱动的重构法则，预测肥厚性心肌病（HCM）中异质性细胞异常（如收缩力过强、收缩力过弱和纤维化）对心肌纤维紊乱及其心脏功能的影响。", "motivation": "肥厚性心肌病（HCM）的一个显著特征是心肌纤维紊乱，这与心力衰竭等多种心脏事件相关。量化纤维紊乱对于理解该疾病复杂的病理生理学至关重要。本研究旨在调查HCM引起的异质性细胞异常在纤维紊乱发展中的作用及其对心脏泵血功能的后续影响。", "method": "本研究使用多尺度有限元心脏建模框架MyoFE，通过基于应力的法则来预测肌纤维和胶原的重新定向，从而预测纤维紊乱。该模型用于量化收缩力过强、收缩力过弱和纤维化等异质性分布对纤维紊乱发展的影响，并检查它们对心脏功能特性的作用。", "result": "研究结果表明，异质性细胞水平异常严重扰乱了心肌的正常力学特性，并导致显著的纤维紊乱。紊乱模式因特定扰动而异。尽管心肌内扰动区域分布随机，但在所有受扰左心室模型中，心外膜附近的纤维紊乱程度显著高于心内膜。这种区域性差异与先前的DT-MRI研究一致。此外，重构后的左心室心脏功能下降，特别是在有纤维化和收缩力过弱的情况下。", "conclusion": "这些发现为肥厚性心肌病的结构和功能后果提供了重要见解，并为未来针对心脏重构的治疗干预措施研究提供了框架。", "translation": "肥厚性心肌病（HCM）的一个显著特征是纤维紊乱，这与心力衰竭等多种心脏事件相关。量化纤维紊乱对于理解该疾病复杂的病理生理学仍然至关重要。本研究调查了HCM引起的异质性细胞异常在纤维紊乱发展中的作用及其对心脏泵血功能的后续影响。通过在多尺度有限元心脏建模框架MyoFE中使用基于应力的法则来重新定向肌纤维和胶原，从而预测纤维紊乱。具体而言，该模型用于量化收缩力过强、收缩力过弱和纤维化等异质性分布对纤维紊乱发展的不同影响，并检查它们对心脏功能特性的作用。我们的结果表明，异质性细胞水平异常极大地扰乱了心肌的正常力学特性，并导致显著的纤维紊乱。紊乱的模式因特定的扰动而异，为HCM的进展提供了宝贵的见解。尽管心脏肌肉内扰动区域的分布是随机的，但在所有受扰的左心室（LV）模型中，与心内膜相比，心外膜附近的纤维紊乱程度显著更高。这种纤维紊乱的区域差异，无论扰动严重程度如何，都与之前的DT-MRI研究一致，突出了区域心肌力学在纤维紊乱发展中的作用。此外，重构后的左心室心脏功能下降，特别是在有纤维化和收缩力过弱的情况下。这些发现为HCM的结构和功能后果提供了重要见解，并为未来针对心脏重构的治疗干预措施研究提供了框架。", "summary": "本研究利用多尺度有限元模型MyoFE，通过应力驱动的重构法则，预测肥厚性心肌病（HCM）中异质性细胞异常（如收缩力过强、收缩力过弱和纤维化）对心肌纤维紊乱及其心脏功能的影响。结果显示，异质性细胞异常严重扰乱心肌力学并导致显著纤维紊乱，且心外膜附近紊乱程度高于心内膜。重构后的左心室心脏功能下降，特别是有纤维化和收缩力过弱时。这些发现为理解HCM的结构和功能后果提供了重要见解，并为未来治疗干预提供了框架。", "keywords": "肥厚性心肌病, 纤维紊乱, 多尺度有限元, 心脏重构, MyoFE", "comments": "本研究创新性地将多尺度有限元方法应用于肥厚性心肌病的纤维重构研究，通过模拟不同细胞异常对纤维紊乱的影响，揭示了其在HCM进展中的复杂机制。其强调了区域心肌力学的重要性，并为未来针对心脏重构的治疗干预提供了计算框架，具有重要的临床转化潜力。"}}
{"id": "2507.02173", "title": "Data Diversification Methods In Alignment Enhance Math Performance In LLMs", "authors": ["Berkan Dokmeci", "Qingyang Wu", "Ben Athiwaratkun", "Ce Zhang", "Shuaiwen Leon Song", "James Zou"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02173v1", "summary": "While recent advances in preference learning have enhanced alignment in human\nfeedback, mathematical reasoning remains a persistent challenge. We investigate\nhow data diversification strategies in preference optimization can improve the\nmathematical reasoning abilities of large language models (LLMs). We evaluate\nthree common data generation methods: temperature sampling, Chain-of-Thought\nprompting, and Monte Carlo Tree Search (MCTS), and introduce\nDiversified-ThinkSolve (DTS), a novel structured approach that systematically\ndecomposes problems into diverse reasoning paths. Our results show that with\nstrategically diversified preference data, models can substantially improve\nmathematical reasoning performance, with the best approach yielding gains of\n7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong\nperformance, DTS incurs only a marginal computational overhead (1.03x) compared\nto the baseline, while MCTS is nearly five times more costly with lower\nreturns. These findings demonstrate that structured exploration of diverse\nproblem-solving methods creates more effective preference data for mathematical\nalignment than traditional approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02173v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "对齐中的数据多样化方法提升大型语言模型的数学性能", "tldr": "研究表明，通过数据多样化策略，特别是新提出的Diversified-ThinkSolve (DTS)，可以显著提升LLMs的数学推理能力，且计算开销低。", "motivation": "尽管偏好学习在人类反馈对齐方面有所进展，但数学推理对LLMs来说仍是挑战。本文旨在探究数据多样化策略如何提升LLMs的数学推理能力。", "method": "评估了三种常见的数据生成方法：温度采样、思维链提示和蒙特卡洛树搜索（MCTS）。引入了一种新颖的结构化方法Diversified-ThinkSolve (DTS)，它系统地将问题分解为多样化的推理路径。", "result": "策略性多样化的偏好数据能显著提升数学推理性能。DTS在GSM8K上比基础模型提升7.1%，在MATH上提升4.2%。DTS的计算开销仅为基线的1.03倍，而MCTS成本高近五倍但回报较低。", "conclusion": "结构化探索多样化的问题解决方法比传统方法能创建更有效的偏好数据，从而更好地实现数学对齐。", "translation": "虽然偏好学习的最新进展增强了人类反馈中的对齐，但数学推理仍然是一个持续的挑战。我们研究了偏好优化中的数据多样化策略如何提高大型语言模型（LLMs）的数学推理能力。我们评估了三种常见的数据生成方法：温度采样、思维链提示和蒙特卡洛树搜索（MCTS），并引入了Diversified-ThinkSolve (DTS)，这是一种新颖的结构化方法，它系统地将问题分解为多样化的推理路径。我们的结果表明，通过策略性多样化的偏好数据，模型可以大幅提高数学推理性能，其中最佳方法在GSM8K上比基础模型提高了7.1%，在MATH上提高了4.2%。尽管性能强劲，DTS与基线相比仅产生微小的计算开销（1.03倍），而MCTS的成本几乎高出五倍，回报却更低。这些发现表明，结构化探索多样化的问题解决方法比传统方法能创建更有效的数学对齐偏好数据。", "summary": "本文研究了数据多样化策略如何提升大型语言模型（LLMs）的数学推理能力。通过评估温度采样、思维链提示和蒙特卡洛树搜索（MCTS）等方法，并提出了一种新颖的Diversified-ThinkSolve (DTS) 结构化方法。实验结果显示，DTS在GSM8K和MATH数据集上显著提升了LLMs的数学性能，且计算开销远低于MCTS，证明了结构化多样化方法在生成有效偏好数据方面的优势。", "keywords": "数据多样化, 数学推理, LLMs, 偏好学习, Diversified-ThinkSolve (DTS) ", "comments": "本文创新性地提出了Diversified-ThinkSolve (DTS) 方法，通过结构化地分解问题来生成多样化的推理路径，有效提升了LLMs的数学推理能力。其重要性在于，它提供了一种高效且计算成本低的策略来解决LLMs在数学推理上的瓶颈，优于传统的昂贵方法如MCTS。这对于开发更强大的数学LLMs具有重要意义。"}}
{"id": "2507.02222", "title": "High-Fidelity Differential-information Driven Binary Vision Transformer", "authors": ["Tian Gao", "Zhiyuan Zhang", "Kaijie Yin", "Xu-Cheng Zhong", "Hui Kong"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02222v1", "summary": "The binarization of vision transformers (ViTs) offers a promising approach to\naddressing the trade-off between high computational/storage demands and the\nconstraints of edge-device deployment. However, existing binary ViT methods\noften suffer from severe performance degradation or rely heavily on\nfull-precision modules. To address these issues, we propose DIDB-ViT, a novel\nbinary ViT that is highly informative while maintaining the original ViT\narchitecture and computational efficiency. Specifically, we design an\ninformative attention module incorporating differential information to mitigate\ninformation loss caused by binarization and enhance high-frequency retention.\nTo preserve the fidelity of the similarity calculations between binary Q and K\ntensors, we apply frequency decomposition using the discrete Haar wavelet and\nintegrate similarities across different frequencies. Additionally, we introduce\nan improved RPReLU activation function to restructure the activation\ndistribution, expanding the model's representational capacity. Experimental\nresults demonstrate that our DIDB-ViT significantly outperforms\nstate-of-the-art network quantization methods in multiple ViT architectures,\nachieving superior image classification and segmentation performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02222v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "高保真差分信息驱动的二值视觉Transformer", "tldr": "提出DIDB-ViT，一种新的二值化Vision Transformer，通过引入差分信息注意力模块、哈尔小波频率分解和改进的RPReLU激活函数，解决了现有二值化ViT性能下降的问题，并在图像分类和分割任务上取得了SOTA性能。", "motivation": "解决视觉Transformer (ViTs) 在边缘设备部署中面临的高计算/存储需求与性能之间的权衡问题，以及现有二值化ViT方法存在的严重性能下降或过度依赖全精度模块的问题。", "method": "提出DIDB-ViT，一种高信息量的二值ViT。具体方法包括：1. 设计一个结合差分信息的“信息注意力模块”来减轻二值化造成的信息损失并增强高频保留。2. 利用离散哈尔小波进行频率分解，并整合不同频率的相似性，以保持二值Q和K张量之间相似性计算的保真度。3. 引入改进的RPReLU激活函数来重构激活分布，扩展模型的表示能力。", "result": "DIDB-ViT在多种ViT架构中显著优于最先进的网络量化方法，实现了卓越的图像分类和分割性能。", "conclusion": "DIDB-ViT成功地提高了二值化视觉Transformer的性能，有效解决了现有方法的信息损失和性能下降问题，使其在边缘设备部署方面更具可行性。", "translation": "视觉Transformer（ViTs）的二值化为解决高计算/存储需求与边缘设备部署限制之间的权衡提供了一种有前景的方法。然而，现有的二值化ViT方法通常会遭受严重的性能下降或严重依赖全精度模块。为了解决这些问题，我们提出了DIDB-ViT，一种新颖的二值ViT，它具有高度信息性，同时保持了原始ViT架构和计算效率。具体来说，我们设计了一个结合差分信息的信息注意力模块，以减轻二值化造成的信息损失并增强高频保留。为了保持二值Q和K张量之间相似性计算的保真度，我们使用离散哈尔小波进行频率分解，并整合不同频率的相似性。此外，我们引入了一种改进的RPReLU激活函数来重构激活分布，扩展了模型的表示能力。实验结果表明，我们的DIDB-ViT在多种ViT架构中显著优于最先进的网络量化方法，实现了卓越的图像分类和分割性能。", "summary": "本文提出了DIDB-ViT，一种新型高保真二值视觉Transformer，旨在解决现有二值化ViT在边缘设备部署中面临的性能下降和对全精度模块依赖的问题。该方法通过设计包含差分信息的信息注意力模块以减少信息损失，利用离散哈尔小波进行频率分解以保持相似性计算的保真度，并引入改进的RPReLU激活函数来增强模型表示能力。实验证明，DIDB-ViT在图像分类和分割任务上显著超越了SOTA网络量化方法。", "keywords": "二值化视觉Transformer, 差分信息, 哈尔小波, RPReLU, 网络量化", "comments": "这项工作通过引入差分信息、频率分解和改进的激活函数，有效地解决了二值化视觉Transformer中的核心挑战，即信息损失和性能下降。其创新点在于从多个层面（注意力机制、特征表示、激活函数）优化二值化过程，显著提高了二值化模型的性能，对于在资源受限设备上部署ViT具有重要意义。"}}
{"id": "2507.02351", "title": "Indoor thermal comfort management: A Bayesian machine-learning approach to data denoising and dynamics prediction of HVAC systems", "authors": ["Javier Penuela", "Sahar Moghimian Hoosh", "Ilia Kamyshev", "Aldo Bischi", "Henni Ouerdane"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02351v1", "summary": "The optimal management of a building's microclimate to satisfy the occupants'\nneeds and objectives in terms of comfort, energy efficiency, and costs is\nparticularly challenging. This complexity arises from the non-linear,\ntime-dependent interactions among all the variables of the control problem and\nthe changing internal and external constraints. Focusing on the accurate\nmodeling of the indoor temperature, we propose a data-driven approach to\naddress this challenge. We account for thermal inertia, non-linear effects,\nsmall perturbations of the indoor climate dynamics caused by ventilation and\nweather variations, as well as for the stochastic nature of the control system\ndue to the observed noise in the input signal. Since the prohibitive cost of\nquality data acquisition and processing limits the implementation of\ndata-driven approaches for real-life problems, we applied a method that merges\nseveral Bayesian machine learning and deep learning architectures that are\nsuitable for predicting complex system dynamics, while relaxing the dataset\nquality requirements. Our framework includes a built-in deep Kalman filter,\nwhich makes it deployable even with low-accuracy temperature sensors. It\nachieves state-of-the-art performance, best performing with a 150-minute\nprediction horizon with an RMSE of 0.2455, an MAE of 0.162, and an $R^2$ of\n0.926. The model's performance remains consistent even when exposed to highly\nnoisy data. Finally, we show how our approach can be extended to other\napplications including demand response event duration prediction and equipment\nfailure detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02351v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "室内热舒适管理：一种用于HVAC系统数据去噪和动态预测的贝叶斯机器学习方法", "tldr": "本文提出了一种结合贝叶斯机器学习和深度学习的新方法，用于HVAC系统数据去噪和室内温度动态预测，即使在低质量数据下也能实现高精度。", "motivation": "建筑微气候的最佳管理在舒适度、能源效率和成本方面面临挑战，因为控制问题中变量间的非线性、时间依赖性互动以及不断变化的内外约束。高质量数据采集和处理的成本高昂限制了数据驱动方法在实际问题中的应用。", "method": "提出了一种数据驱动方法，通过融合贝叶斯机器学习和深度学习架构来预测复杂系统动态，同时放宽数据集质量要求。该框架包含一个内置的深度卡尔曼滤波器，使其即使使用低精度温度传感器也能部署。该方法考虑了热惯性、非线性效应、通风和天气变化引起的室内气候动态小扰动，以及输入信号中观察到的噪声导致的控制系统随机性。", "result": "实现了最先进的性能，在150分钟的预测范围内表现最佳，RMSE为0.2455，MAE为0.162，R^2为0.926。即使暴露在高度噪声数据下，模型的性能也保持一致。", "conclusion": "该方法能够有效管理室内热舒适，即使在数据质量要求不高的情况下也能提供准确的预测。此外，该方法可以扩展到其他应用，如需求响应事件持续时间预测和设备故障检测。", "translation": "建筑微气候的最佳管理，以满足居住者在舒适度、能源效率和成本方面的需求和目标，尤其具有挑战性。这种复杂性源于控制问题中所有变量之间非线性、时间依赖性的相互作用以及不断变化的内部和外部约束。我们专注于室内温度的精确建模，提出了一种数据驱动的方法来应对这一挑战。我们考虑了热惯性、非线性效应、通风和天气变化引起的室内气候动态小扰动，以及由于输入信号中观察到的噪声导致的控制系统的随机性。由于高质量数据采集和处理的高昂成本限制了数据驱动方法在实际问题中的实施，我们应用了一种融合多种贝叶斯机器学习和深度学习架构的方法，这些架构适用于预测复杂系统动态，同时放宽了数据集质量要求。我们的框架包括一个内置的深度卡尔曼滤波器，这使得它即使使用低精度温度传感器也能部署。它实现了最先进的性能，在150分钟的预测范围内表现最佳，RMSE为0.2455，MAE为0.162，R^2为0.926。即使暴露在高度噪声数据下，模型的性能也保持一致。最后，我们展示了我们的方法如何扩展到其他应用，包括需求响应事件持续时间预测和设备故障检测。", "summary": "本文提出了一种新颖的数据驱动框架，结合了贝叶斯机器学习和深度学习技术，用于HVAC系统的室内温度动态预测和数据去噪。该方法旨在解决建筑微气候管理中因非线性、时间依赖性及数据质量限制带来的挑战。通过集成深度卡尔曼滤波器，即使使用低精度传感器和面对高噪声数据，该模型也能实现高精度预测（RMSE 0.2455, MAE 0.162, R^2 0.926），并可扩展至其他应用如需求响应和故障检测。", "keywords": "室内热舒适, 贝叶斯机器学习, 深度学习, HVAC系统, 数据去噪", "comments": "该论文的创新之处在于其融合贝叶斯机器学习和深度学习的方法，以及内置的深度卡尔曼滤波器，显著降低了对数据质量和传感器精度的要求，这对于实际部署具有重要意义。其在噪声数据下的鲁棒性和扩展性也增加了其实用价值。该方法为智能建筑和能源管理领域提供了一个高性能且实用的解决方案。"}}
{"id": "2507.02385", "title": "Parameter estimation of range-migrating targets using OTFS signals from LEO satellites", "authors": ["Tong Ding", "Luca Venturino", "Emanuele Grossi"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      submitted to IEEE journal for possible publication", "url": "http://arxiv.org/abs/2507.02385v1", "summary": "This study investigates a communication-centric integrated sensing and\ncommunication (ISAC) system that utilizes orthogonal time frequency space\n(OTFS) modulated signals emitted by low Earth orbit (LEO) satellites to\nestimate the parameters of space targets experiencing range migration,\nhenceforth referred to as high-speed targets. Leveraging the specific signal\nprocessing performed by OTFS transceivers, we derive a novel input-output model\nfor the echo generated by a high-speed target in scenarios where ideal and\nrectangular shaping filters are employed. Our findings reveal that the target\nresponse exhibits a sparse structure in the delay-Doppler domain, dependent\nsolely upon the initial range and range-rate; notably, range migration causes a\nspread in the target response, marking a significant departure from previous\nstudies. Utilizing this signal structure, we propose an approximate\nimplementation of the maximum likelihood estimator for the target's initial\nrange, range-rate, and amplitude. The estimation process involves obtaining\ncoarse information on the target response using a block orthogonal matching\npursuit algorithm, followed by a refinement step using a bank of matched\nfilters focused on a smaller range and range-rate region. Finally, numerical\nexamples are provided to evaluate the estimation performance.", "comment": "submitted to IEEE journal for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.02385v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "利用LEO卫星的OTFS信号对距离徙动目标进行参数估计", "tldr": "研究如何利用LEO卫星的OTFS信号对距离徙动的高速目标进行参数估计，提出了新的输入输出模型和最大似然估计算法。", "motivation": "本研究旨在探讨一种以通信为中心的综合感知与通信（ISAC）系统，该系统利用低地球轨道（LEO）卫星发出的正交时频空间（OTFS）调制信号来估计经历距离徙动的高速空间目标的参数。", "method": "通过OTFS收发器的特定信号处理，推导了在理想和矩形整形滤波器下高速目标回波的新型输入输出模型。利用此信号结构，提出了一种近似实现的最大似然估计器，用于估计目标的初始距离、距离率和幅度。估计过程包括使用块正交匹配追踪算法获取目标响应的粗略信息，然后使用一组匹配滤波器在较小的距离和距离率区域进行细化。", "result": "研究发现目标响应在时延-多普勒域呈现稀疏结构，仅依赖于初始距离和距离率；值得注意的是，距离徙动会导致目标响应的扩展，这与以往的研究显著不同。", "conclusion": "提出了利用LEO卫星OTFS信号对距离徙动目标进行参数估计的方法，并通过数值例子评估了估计性能。", "translation": "本研究探讨了一种以通信为中心的综合感知与通信（ISAC）系统，该系统利用低地球轨道（LEO）卫星发出的正交时频空间（OTFS）调制信号来估计经历距离徙动的空间目标（以下简称高速目标）的参数。利用OTFS收发器特有的信号处理能力，我们推导了在理想和矩形整形滤波器下，高速目标回波的新型输入输出模型。我们的研究结果表明，目标响应在时延-多普勒域呈现稀疏结构，仅依赖于初始距离和距离率；值得注意的是，距离徙动会导致目标响应的扩展，这与以往的研究显著不同。利用这种信号结构，我们提出了一种近似实现的最大似然估计器，用于估计目标的初始距离、距离率和幅度。估计过程包括使用块正交匹配追踪算法获取目标响应的粗略信息，然后使用一组匹配滤波器在较小的距离和距离率区域进行细化。最后，提供了数值例子来评估估计性能。", "summary": "本文提出了一种利用LEO卫星OTFS信号对距离徙动高速目标进行参数估计的通信中心化ISAC系统。研究推导了新的目标回波输入输出模型，发现目标响应在时延-多普勒域呈稀疏结构，且距离徙动会导致响应扩展。在此基础上，提出了一种结合块正交匹配追踪和匹配滤波器的近似最大似然估计算法，并评估了其性能。", "keywords": "OTFS, LEO卫星, 距离徙动, 参数估计, ISAC", "comments": "本文创新性地推导了利用OTFS信号估计距离徙动高速目标回波的输入输出模型，并揭示了距离徙动导致时延-多普勒域响应扩展的特性，这与传统模型不同。提出的近似最大似然估计算法结合了粗略和精细估计，具有工程应用潜力。该研究对未来基于LEO卫星的ISAC系统中的高速目标感知具有重要意义。"}}
{"id": "2507.02791", "title": "Self-Steering Deep Non-Linear Spatially Selective Filters for Efficient Extraction of Moving Speakers under Weak Guidance", "authors": ["Jakob Kienegger", "Alina Mannanova", "Huajian Fang", "Timo Gerkmann"], "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted at IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) 2025", "url": "http://arxiv.org/abs/2507.02791v1", "summary": "Recent works on deep non-linear spatially selective filters demonstrate\nexceptional enhancement performance with computationally lightweight\narchitectures for stationary speakers of known directions. However, to maintain\nthis performance in dynamic scenarios, resource-intensive data-driven tracking\nalgorithms become necessary to provide precise spatial guidance conditioned on\nthe initial direction of a target speaker. As this additional computational\noverhead hinders application in resource-constrained scenarios such as\nreal-time speech enhancement, we present a novel strategy utilizing a\nlow-complexity tracking algorithm in the form of a particle filter instead.\nAssuming a causal, sequential processing style, we introduce temporal feedback\nto leverage the enhanced speech signal of the spatially selective filter to\ncompensate for the limited modeling capabilities of the particle filter.\nEvaluation on a synthetic dataset illustrates how the autoregressive interplay\nbetween both algorithms drastically improves tracking accuracy and leads to\nstrong enhancement performance. A listening test with real-world recordings\ncomplements these findings by indicating a clear trend towards our proposed\nself-steering pipeline as preferred choice over comparable methods.", "comment": "Accepted at IEEE Workshop on Applications of Signal Processing to\n  Audio and Acoustics (WASPAA) 2025", "pdf_url": "http://arxiv.org/pdf/2507.02791v1", "cate": "eess.AS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "弱引导下移动说话人高效提取的自适应深度非线性空间选择滤波器", "tldr": "本文提出了一种自适应深度非线性空间选择滤波器，通过结合低复杂度粒子滤波器和时间反馈，在资源受限场景下高效提取移动说话人，显著提升了跟踪精度和语音增强性能。", "motivation": "现有的深度非线性空间选择滤波器在处理移动说话人时需要资源密集型的数据驱动跟踪算法，这在实时、资源受限的语音增强应用中是不可行的。", "method": "本文提出了一种新颖的策略，利用低复杂度的粒子滤波器进行跟踪。它引入了时间反馈机制，利用空间选择滤波器增强的语音信号来弥补粒子滤波器有限的建模能力，并假设采用因果、顺序处理方式。", "result": "在合成数据集上的评估表明，两种算法之间的自回归相互作用显著提高了跟踪精度，并带来了强大的增强性能。对真实世界录音的听力测试也表明，与同类方法相比，所提出的自适应引导管道是更受欢迎的选择。", "conclusion": "本文提出的自适应引导管道，结合了深度非线性空间选择滤波器和低复杂度粒子滤波器以及时间反馈，能够有效地提取移动说话人，并在跟踪和增强性能方面表现出色，优于现有方法。", "translation": "最近关于深度非线性空间选择滤波器的工作表明，对于已知方向的静止说话人，其计算量轻的架构具有卓越的增强性能。然而，为了在动态场景中保持这种性能，需要资源密集型的数据驱动跟踪算法来提供以目标说话人初始方向为条件的精确空间引导。由于这种额外的计算开销阻碍了在资源受限场景（如实时语音增强）中的应用，我们提出了一种新颖的策略，转而利用粒子滤波器形式的低复杂度跟踪算法。假设采用因果、顺序处理方式，我们引入了时间反馈，利用空间选择滤波器的增强语音信号来补偿粒子滤波器的有限建模能力。对合成数据集的评估表明，两种算法之间的自回归相互作用如何显著提高跟踪精度并带来强大的增强性能。对真实世界录音的听力测试补充了这些发现，表明我们提出的自适应引导管道作为优于同类方法的首选，表现出明显的趋势。", "summary": "本文旨在解决在资源受限环境下使用深度非线性空间选择滤波器高效提取移动说话人的挑战。论文提出了一种自适应引导管道，该管道集成了低复杂度的粒子滤波器用于说话人跟踪。通过引入时间反馈，系统利用增强的语音信号来提高粒子滤波器的跟踪精度。在合成和真实世界数据集上的实验结果表明，该方法在跟踪和增强性能方面均表现出色，是动态语音增强的优选方案。", "keywords": "深度非线性空间选择滤波器, 移动说话人提取, 粒子滤波器, 语音增强, 自适应引导", "comments": "本文的创新之处在于其“自适应引导”方法，特别是将低复杂度粒子滤波器与时间反馈相结合，以克服动态语音增强中资源密集型跟踪算法的局限性。这使得深度非线性空间选择滤波器在实时、资源受限的应用中更具实用性。滤波器和跟踪器之间的自回归相互作用是其关键贡献。"}}
{"id": "2507.02416", "title": "Determination Of Structural Cracks Using Deep Learning Frameworks", "authors": ["Subhasis Dasgupta", "Jaydip Sen", "Tuhina Halder"], "categories": ["cs.CV", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This is the accepted version of the paper presented in IEEE CONIT 2025 held on 20th June 2025. This is not the camera-ready version. There are 6 pages in this paper and it contains 7 figures and 1 table", "url": "http://arxiv.org/abs/2507.02416v1", "summary": "Structural crack detection is a critical task for public safety as it helps\nin preventing potential structural failures that could endanger lives. Manual\ndetection by inexperienced personnel can be slow, inconsistent, and prone to\nhuman error, which may compromise the reliability of assessments. The current\nstudy addresses these challenges by introducing a novel deep-learning\narchitecture designed to enhance the accuracy and efficiency of structural\ncrack detection. In this research, various configurations of residual U-Net\nmodels were utilized. These models, due to their robustness in capturing fine\ndetails, were further integrated into an ensemble with a meta-model comprising\nconvolutional blocks. This unique combination aimed to boost prediction\nefficiency beyond what individual models could achieve. The ensemble's\nperformance was evaluated against well-established architectures such as SegNet\nand the traditional U-Net. Results demonstrated that the residual U-Net models\noutperformed their predecessors, particularly with low-resolution imagery, and\nthe ensemble model exceeded the performance of individual models, proving it as\nthe most effective. The assessment was based on the Intersection over Union\n(IoU) metric and DICE coefficient. The ensemble model achieved the highest\nscores, signifying superior accuracy. This advancement suggests way for more\nreliable automated systems in structural defects monitoring tasks.", "comment": "This is the accepted version of the paper presented in IEEE CONIT\n  2025 held on 20th June 2025. This is not the camera-ready version. There are\n  6 pages in this paper and it contains 7 figures and 1 table", "pdf_url": "http://arxiv.org/pdf/2507.02416v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "使用深度学习框架确定结构裂缝", "tldr": "本研究提出了一种新颖的深度学习架构，结合了残差U-Net模型和卷积块组成的元模型集成，以提高结构裂缝检测的准确性和效率，并证明其优于现有模型。", "motivation": "结构裂缝检测对公共安全至关重要，但传统手动检测方法效率低、不一致且易受人为错误影响。本研究旨在通过引入先进的深度学习架构来解决这些挑战，提高检测的准确性和效率。", "method": "本研究采用了一种新颖的深度学习架构，利用了多种配置的残差U-Net模型。为了进一步提高预测效率，这些模型与一个包含卷积块的元模型集成在一起。该集成模型的性能通过IoU和DICE系数与SegNet和传统U-Net等成熟架构进行了评估。", "result": "结果表明，残差U-Net模型优于其前身，尤其是在低分辨率图像上。集成模型超越了单个模型的性能，被证明是最有效的。该集成模型在IoU和DICE系数上取得了最高分，显示出卓越的准确性。", "conclusion": "这项研究的进展为结构缺陷监测任务中更可靠的自动化系统提供了方向。", "translation": "结构裂缝检测是公共安全的一项关键任务，因为它有助于防止可能危及生命的潜在结构故障。由经验不足的人员进行手动检测可能缓慢、不一致且容易出现人为错误，这可能会影响评估的可靠性。当前的研究通过引入一种新颖的深度学习架构来解决这些挑战，该架构旨在提高结构裂缝检测的准确性和效率。在本研究中，利用了残差U-Net模型的各种配置。这些模型因其在捕获精细细节方面的鲁棒性，进一步与包含卷积块的元模型集成，形成一个集成模型。这种独特的组合旨在将预测效率提升到单个模型无法达到的水平。该集成模型的性能与SegNet和传统U-Net等成熟架构进行了评估。结果表明，残差U-Net模型优于其前身，特别是在低分辨率图像上，并且集成模型超越了单个模型的性能，证明其是最有效的。评估基于交并比（IoU）度量和DICE系数。集成模型获得了最高分，表明其卓越的准确性。这一进展为结构缺陷监测任务中更可靠的自动化系统提供了方向。", "summary": "本研究提出了一种新颖的深度学习架构，用于提高结构裂缝检测的准确性和效率。该架构利用了多种配置的残差U-Net模型，并将其与一个包含卷积块的元模型进行集成。实验结果表明，该集成模型在IoU和DICE系数上表现最佳，超越了传统的U-Net和SegNet等模型，尤其在低分辨率图像上效果显著，为开发更可靠的自动化结构缺陷监测系统提供了途径。", "keywords": "结构裂缝检测, 深度学习, 残差U-Net, 集成模型, 图像分割", "comments": "本文的创新点在于提出了一个结合残差U-Net模型和卷积块元模型的深度学习集成架构，有效提升了结构裂缝检测的准确性和效率。其重要性在于为公共安全领域的结构健康监测提供了更可靠的自动化解决方案，减少了对人工检测的依赖和潜在的人为错误。特别是在处理低分辨率图像时表现出色，这对于实际应用具有重要意义。"}}
{"id": "2507.02006", "title": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design", "authors": ["Shakya Jayakody", "Youpeng Zhao", "Jun Wang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      36th IEEE International Conference on Application-Specific Systems, Architectures and Processors. (Accepted)", "url": "http://arxiv.org/abs/2507.02006v1", "summary": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks.", "comment": "36th IEEE International Conference on Application-Specific Systems,\n  Architectures and Processors. (Accepted)", "pdf_url": "http://arxiv.org/pdf/2507.02006v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "AIRES：通过算法-系统协同设计加速核外GCN", "tldr": "AIRES通过算法-系统协同设计解决了核外GCN中SpGEMM的高I/O延迟和GPU利用率不足问题，实现了显著的性能提升。", "motivation": "现有系统在处理大型图数据的核外稀疏广义矩阵乘法（SpGEMM）时，由于GPU内存限制，存在高I/O延迟和GPU利用率不足的问题，主要瓶颈在于稀疏格式数据对齐和内存分配。", "method": "提出AIRES，一个算法-系统协同设计解决方案。在算法层面，通过块级数据对齐和分块算法解决稀疏格式数据对齐问题。在系统层面，采用三阶段动态调度和双向数据传输策略，利用分层内存系统（GPU内存、GDS、主机内存）来减少I/O延迟和提高吞吐量。", "result": "AIRES在真实世界的图处理基准测试中，比现有最先进方法实现了高达1.8倍的延迟降低。", "conclusion": "AIRES通过其算法-系统协同设计，有效解决了核外GCN中SpGEMM的性能瓶颈，显著提升了处理大型图数据的效率。", "translation": "图卷积网络（GCN）在从生物医学蛋白质-蛋白质相互作用（PPI）到大规模推荐系统等各种科学应用中都至关重要。GCN中用于建模图结构的重要组件是稀疏广义矩阵-矩阵乘法（SpGEMM）。随着图数据规模的不断扩大，由于资源受限系统中GPU内存空间的限制，SpGEMM通常以核外方式进行。尽管最近通过GPU特征缓存、混合CPU-GPU内存布局或以稀疏格式执行计算等方式，旨在缓解核外SpGEMM的内存限制，但当前系统仍然面临高I/O延迟和GPU利用率不足的问题。\n在本文中，我们首先识别了现有系统的问题，其中稀疏格式数据对齐和内存分配是主要的性能瓶颈，并提出了AIRES，一种新颖的算法-系统协同设计解决方案，用于加速GCN的核外SpGEMM计算。具体来说，从算法角度，AIRES提出了在块级别缓解稀疏格式矩阵的数据对齐问题，并开发了一种分块算法以促进行块级对齐。在系统层面，AIRES采用了三阶段动态调度，其特点是利用分层内存系统（整合了GPU内存、GPU Direct Storage（GDS）和主机内存）的双向数据传输策略，以减少I/O延迟并提高吞吐量。评估表明，AIRES显著优于最先进的方法，在真实世界的图处理基准测试中实现了高达1.8倍的延迟降低。", "summary": "本文提出了AIRES，一个针对图卷积网络（GCN）中核外稀疏广义矩阵乘法（SpGEMM）的算法-系统协同设计方案。AIRES旨在解决现有系统在高I/O延迟和GPU利用率不足的问题，这些问题源于稀疏数据对齐和内存分配的瓶颈。该方案在算法层面通过块级对齐和分块算法优化数据对齐，在系统层面采用三阶段动态调度和分层内存的双向数据传输策略。实验结果表明，AIRES在真实图处理任务中显著优于现有技术，将延迟降低了高达1.8倍。", "keywords": "图卷积网络, 核外计算, 稀疏矩阵乘法, 算法-系统协同设计, 内存优化", "comments": "AIRES的创新之处在于其算法-系统协同设计方法，尤其是在处理核外SpGEMM时，同时解决了数据对齐和内存传输效率两大难题。通过结合块级算法优化和分层内存系统的动态调度，有效地克服了传统方法在高I/O延迟和GPU利用率不足方面的局限性，对于大规模图数据处理具有重要意义。"}}
{"id": "2507.02510", "title": "TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification", "authors": ["Ahmed G. Habashi", "Ahmed M. Azab", "Seif Eldawlatly", "Gamal M. Aly"], "categories": ["cs.LG", "cs.HC", "cs.NE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02510v1", "summary": "Cross-subject motor imagery (CS-MI) classification in brain-computer\ninterfaces (BCIs) is a challenging task due to the significant variability in\nElectroencephalography (EEG) patterns across different individuals. This\nvariability often results in lower classification accuracy compared to\nsubject-specific models, presenting a major barrier to developing\ncalibration-free BCIs suitable for real-world applications. In this paper, we\nintroduce a novel approach that significantly enhances cross-subject MI\nclassification performance through optimized preprocessing and deep learning\ntechniques. Our approach involves direct classification of Short-Time Fourier\nTransform (STFT)-transformed EEG data, optimized STFT parameters, and a\nbalanced batching strategy during training of a Convolutional Neural Network\n(CNN). This approach is uniquely validated across four different datasets,\nincluding three widely-used benchmark datasets leading to substantial\nimprovements in cross-subject classification, achieving 67.60% on the BCI\nCompetition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on\nDataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we\nsystematically investigate the classification performance using MI windows\nranging from the full 4-second window to 1-second windows. These results\nestablish a new benchmark for generalizable, calibration-free MI classification\nin addition to contributing a robust open-access dataset to advance research in\nthis domain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02510v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "TFOC-Net：一种基于短时傅里叶变换的深度学习方法，用于增强跨受试者运动想象分类", "tldr": "TFOC-Net提出了一种基于STFT和深度学习的新方法，显著提高了脑机接口中跨受试者运动想象分类的准确性，并在多个基准数据集上超越了现有技术。", "motivation": "脑机接口（BCI）中跨受试者运动想象（CS-MI）分类面临挑战，因为不同个体间的脑电图（EEG）模式差异显著，导致分类精度低于受试者特异性模型，这阻碍了免校准BCI的开发和实际应用。", "method": "本文提出了一种新方法，通过优化的预处理和深度学习技术显著增强CS-MI分类性能。该方法包括直接对短时傅里叶变换（STFT）后的EEG数据进行分类，优化STFT参数，并在卷积神经网络（CNN）训练期间采用平衡批处理策略。", "result": "该方法在四个不同数据集（包括三个广泛使用的基准数据集）上得到了验证，显著提高了跨受试者分类性能：BCI竞赛IV数据集1（IV-1）上达到67.60%，数据集2A（IV-2A）上达到65.96%，数据集2B（IV-2B）上达到80.22%，超越了现有技术。此外，系统研究了使用从4秒到1秒不同MI窗口的分类性能。", "conclusion": "这些结果为可泛化、免校准的MI分类建立了新的基准，并贡献了一个强大的开放获取数据集以促进该领域的研究。", "translation": "脑机接口（BCI）中跨受试者运动想象（CS-MI）分类是一项具有挑战性的任务，因为不同个体间的脑电图（EEG）模式存在显著变异性。这种变异性通常导致分类精度低于受试者特异性模型，是开发适用于实际应用的免校准BCI的主要障碍。在本文中，我们引入了一种新颖的方法，通过优化的预处理和深度学习技术显著增强了跨受试者MI分类性能。我们的方法包括直接对短时傅里叶变换（STFT）后的EEG数据进行分类，优化STFT参数，以及在卷积神经网络（CNN）训练期间采用平衡批处理策略。该方法在四个不同数据集（包括三个广泛使用的基准数据集）上得到了独特验证，显著提高了跨受试者分类性能，在BCI竞赛IV数据集1（IV-1）上达到67.60%，在数据集2A（IV-2A）上达到65.96%，在数据集2B（IV-2B）上达到80.22%，超越了现有技术。此外，我们系统地研究了使用从完整4秒窗口到1秒窗口的MI窗口的分类性能。这些结果为可泛化、免校准的MI分类建立了新的基准，并贡献了一个强大的开放获取数据集以促进该领域的研究。", "summary": "本文提出TFOC-Net，一种基于短时傅里叶变换（STFT）和深度学习的跨受试者运动想象（MI）分类方法，旨在解决脑电图（EEG）模式个体差异大导致分类准确率低的问题。该方法通过优化STFT参数和在CNN训练中采用平衡批处理策略，直接对STFT转换后的EEG数据进行分类。在多个基准数据集上的验证表明，TFOC-Net显著提升了跨受试者MI分类性能，超越了现有技术，并为可泛化、免校准的MI分类树立了新基准。", "keywords": "运动想象分类, 脑机接口, 短时傅里叶变换, 深度学习, 跨受试者", "comments": "该论文的创新点在于结合了优化的短时傅里叶变换（STFT）和深度学习（CNN）来处理EEG数据，以克服跨受试者运动想象分类中的个体差异问题。其贡献在于不仅在多个权威数据集上取得了显著优于现有技术的结果，还为免校准BCI的开发提供了新的可能性，并贡献了一个开放获取数据集，对该领域的研究具有重要推动作用。"}}
{"id": "2507.02282", "title": "Content filtering methods for music recommendation: A review", "authors": ["Terence Zeng", "Abhishek K. Umrawal"], "categories": ["cs.IR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      13 pages and 9 figures", "url": "http://arxiv.org/abs/2507.02282v1", "summary": "Recommendation systems have become essential in modern music streaming\nplatforms, shaping how users discover and engage with songs. One common\napproach in recommendation systems is collaborative filtering, which suggests\ncontent based on the preferences of users with similar listening patterns to\nthe target user. However, this method is less effective on media where\ninteractions are sparse. Music is one such medium, since the average user of a\nmusic streaming service will never listen to the vast majority of tracks. Due\nto this sparsity, there are several challenges that have to be addressed with\nother methods. This review examines the current state of research in addressing\nthese challenges, with an emphasis on the role of content filtering in\nmitigating biases inherent in collaborative filtering approaches. We explore\nvarious methods of song classification for content filtering, including lyrical\nanalysis using Large Language Models (LLMs) and audio signal processing\ntechniques. Additionally, we discuss the potential conflicts between these\ndifferent analysis methods and propose avenues for resolving such\ndiscrepancies.", "comment": "13 pages and 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.02282v1", "cate": "cs.IR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "音乐推荐中的内容过滤方法：综述", "tldr": "本文综述了音乐推荐中的内容过滤方法，特别是在解决协同过滤稀疏性问题方面的应用。", "motivation": "协同过滤方法在音乐推荐中因用户交互数据稀疏而效果不佳，因此需要其他方法来解决这些挑战。", "method": "本文是一篇综述性论文，考察了现有研究中内容过滤在缓解协同过滤固有偏差方面的作用。探讨了用于内容过滤的各种歌曲分类方法，包括使用大型语言模型（LLMs）的歌词分析和音频信号处理技术，并讨论了不同分析方法之间的潜在冲突及解决方案。", "result": "Not mentioned in abstract", "conclusion": "本文综述了音乐推荐中的内容过滤方法，强调了它们在解决协同过滤的稀疏性和偏差问题中的作用，并讨论了各种分类方法以及潜在的冲突。", "translation": "推荐系统在现代音乐流媒体平台中变得至关重要，它们塑造了用户发现和参与歌曲的方式。推荐系统中的一种常见方法是协同过滤，它根据与目标用户具有相似收听模式的用户偏好来推荐内容。然而，这种方法在交互稀疏的媒体上效果不佳。音乐就是这样一种媒体，因为音乐流媒体服务的普通用户永远不会听绝大多数的曲目。由于这种稀疏性，存在一些必须通过其他方法解决的挑战。本综述考察了应对这些挑战的研究现状，重点是内容过滤在减轻协同过滤方法固有偏差方面的作用。我们探讨了用于内容过滤的各种歌曲分类方法，包括使用大型语言模型 (LLM) 的歌词分析和音频信号处理技术。此外，我们还讨论了这些不同分析方法之间潜在的冲突，并提出了解决此类差异的途径。", "summary": "这篇综述论文专注于音乐推荐系统中的内容过滤方法。它解决了协同过滤在音乐领域因数据稀疏性而存在的局限性，即用户只与一小部分可用曲目进行交互。论文调查了当前内容过滤技术的研究现状，强调了它们在减轻协同过滤偏差方面的作用。它探讨了歌曲分类方法，包括使用LLMs的歌词分析和音频信号处理，并讨论了这些方法之间潜在的冲突和解决方案。", "keywords": "音乐推荐, 内容过滤, 协同过滤, 歌词分析, 音频信号处理", "comments": "这篇综述对于音乐推荐研究非常重要，它解决了协同过滤中的一个关键挑战（数据稀疏性）。它强调了基于内容的方法日益增长的作用，特别是结合了大型语言模型和音频处理技术，并指出了解决不同分析技术之间差异的必要性。"}}
{"id": "2507.02701", "title": "Faster Algorithm for Bounded Tree Edit Distance in the Low-Distance Regime", "authors": ["Tomasz Kociumaka", "Ali Shahali"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      Accepted to ESA 2025", "url": "http://arxiv.org/abs/2507.02701v1", "summary": "The tree edit distance is a natural dissimilarity measure between rooted\nordered trees whose nodes are labeled over an alphabet $\\Sigma$. It is defined\nas the minimum number of node edits (insertions, deletions, and relabelings)\nrequired to transform one tree into the other. In the weighted variant, the\nedits have associated costs (depending on the involved node labels) normalized\nso that each cost is at least one, and the goal is to minimize the total cost\nof edits.\n  The unweighted tree edit distance between two trees of total size $n$ can be\ncomputed in $O(n^{2.6857})$ time; in contrast, determining the weighted tree\nedit distance is fine-grained equivalent to the All-Pairs Shortest Paths\nproblem and requires $n^3/2^{\\Omega(\\sqrt{\\log n})}$ time [Nogler et al.;\nSTOC'25]. These super-quadratic running times are unattractive for large but\nvery similar trees, which motivates the bounded version of the problem, where\nthe runtime is parameterized by the computed distance $k$, potentially yielding\nfaster algorithms for $k\\ll n$.\n  Previous best algorithms for the bounded unweighted setting run in\n$O(nk^2\\log n)$ time [Akmal & Jin; ICALP'21] and $O(n + k^7\\log k)$ time [Das\net al.; STOC'23]. For the weighted variant, the only known running time has\nbeen $O(n + k^{15})$.\n  We present an $O(n + k^6\\log k)$-time algorithm for computing the bounded\ntree edit distance in both the weighted and unweighted settings. Our approach\nbegins with an alternative $O(nk^2\\log n)$-time algorithm that handles weights\nand is significantly easier to analyze than the existing counterpart. We then\nintroduce a novel optimization that leverages periodic structures within the\ninput trees. To utilize it, we modify the $O(k^5)$-size $O(n)$-time universal\nkernel, the central component of the prior $O(n + k^{O(1)})$-time algorithms,\nso that it produces instances containing these periodic structures.", "comment": "Accepted to ESA 2025", "pdf_url": "http://arxiv.org/pdf/2507.02701v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "低距离范围下有界树编辑距离的更快算法", "tldr": "本文提出了一种计算有界树编辑距离的更快算法，时间复杂度为 $O(n + k^6\\log k)$，适用于加权和无权设置。", "motivation": "现有的无界树编辑距离算法时间复杂度过高（无权为 $O(n^{2.6857})$，加权为 $n^3/2^{\\Omega(\\sqrt{\\log n})}$），不适用于大型但非常相似的树。因此，需要有界版本的算法，其中运行时由计算距离 $k$ 参数化，以便在 $k \\ll n$ 时提供更快的算法。", "method": "本文提出了一个 $O(n + k^6\\log k)$ 时间的算法，用于计算加权和无权设置下的有界树编辑距离。该方法首先提出了一个替代的 $O(nk^2\\log n)$ 时间算法，该算法能够处理权重且比现有对应算法更容易分析。接着，引入了一种利用输入树中周期结构的新颖优化技术。为了利用此优化，作者修改了先前 $O(n + k^{O(1)})$ 时间算法的核心组件——$O(k^5)$ 大小、$O(n)$ 时间的通用核，使其能够产生包含这些周期结构的实例。", "result": "本文提出的算法在加权和无权设置下，将计算有界树编辑距离的时间复杂度改进到 $O(n + k^6\\log k)$。这显著优于现有最佳算法，例如无权设置下的 $O(nk^2\\log n)$ 和 $O(n + k^7\\log k)$，以及加权设置下唯一的已知运行时间 $O(n + k^{15})$。", "conclusion": "本文成功开发了一种更快的有界树编辑距离算法，显著提升了在低距离范围内的计算效率，使其在处理大型相似树时更具实用性。", "translation": "树编辑距离是衡量根有序树之间自然差异的度量，这些树的节点在字母表 $\\Sigma$ 上进行标记。它被定义为将一棵树转换为另一棵树所需的最小节点编辑（插入、删除和重新标记）次数。在加权变体中，编辑具有相关的成本（取决于所涉及的节点标签），并经过归一化，使得每个成本至少为一，目标是最小化编辑的总成本。\\n  两个总大小为 $n$ 的树之间的无权树编辑距离可以在 $O(n^{2.6857})$ 时间内计算；相比之下，确定加权树编辑距离与全对最短路径问题在细粒度上等效，需要 $n^3/2^{\\Omega(\\sqrt{\\log n})}$ 时间 [Nogler et al.; STOC'25]。这些超二次的运行时间对于大型但非常相似的树来说是不具吸引力的，这促使了问题的有界版本，其中运行时由计算出的距离 $k$ 参数化，可能在 $k\\ll n$ 时产生更快的算法。\\n  先前有界无权设置的最佳算法运行时间为 $O(nk^2\\log n)$ [Akmal & Jin; ICALP'21] 和 $O(n + k^7\\log k)$ [Das et al.; STOC'23]。对于加权变体，唯一已知的运行时间是 $O(n + k^{15})$。\\n  我们提出了一种用于计算加权和无权设置下有界树编辑距离的 $O(n + k^6\\log k)$ 时间算法。我们的方法始于一个替代的 $O(nk^2\\log n)$ 时间算法，该算法处理权重并且比现有对应算法更容易分析。然后，我们引入了一种利用输入树中周期结构的新颖优化。为了利用它，我们修改了 $O(k^5)$ 大小、$O(n)$ 时间的通用核（先前 $O(n + k^{O(1)})$ 时间算法的核心组件），使其能够生成包含这些周期结构的实例。", "summary": "该论文提出了一种计算有界树编辑距离的更快算法，旨在解决现有算法在处理大型相似树时效率低下的问题。新算法在加权和无权设置下均达到了 $O(n + k^6\\log k)$ 的时间复杂度，这通过引入一个易于分析的 $O(nk^2\\log n)$ 算法，并结合利用树中周期结构的新颖优化实现，显著超越了现有最佳算法的性能。", "keywords": "树编辑距离, 有界树编辑距离, 算法, 时间复杂度, 周期结构", "comments": "该论文通过引入利用周期结构的新颖优化，显著提升了有界树编辑距离算法的效率，将时间复杂度从 $O(n + k^{15})$ 降低到 $O(n + k^6\\log k)$。这一突破对于处理大规模但高度相似的树数据具有重要意义，显示了算法设计上的创新性。"}}
{"id": "2507.02357", "title": "Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models", "authors": ["Christian Jaumann", "Annemarie Friedrich", "Rainer Lienhart"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at 5th Workshop on Scholarly Document Processing @ ACL 2025", "url": "http://arxiv.org/abs/2507.02357v1", "summary": "This paper describes our system for the SciVQA 2025 Shared Task on Scientific\nVisual Question Answering. Our system employs an ensemble of two Multimodal\nLarge Language Models and various few-shot example retrieval strategies. The\nmodel and few-shot setting are selected based on the figure and question type.\nWe also select answers based on the models' confidence levels. On the blind\ntest data, our system ranks third out of seven with an average F1 score of\n85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.", "comment": "Accepted at 5th Workshop on Scholarly Document Processing @ ACL 2025", "pdf_url": "http://arxiv.org/pdf/2507.02357v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Coling-UniA在SciVQA 2025：多模态大型语言模型的少样本示例检索和置信度感知集成", "tldr": "该论文描述了Coling-UniA在SciVQA 2025科学视觉问答任务中的系统，该系统采用多模态大语言模型集成、少样本检索和置信度选择，在盲测数据上排名第三。", "motivation": "参与SciVQA 2025科学视觉问答共享任务。", "method": "系统采用两个多模态大型语言模型（MLLMs）的集成，结合多种少样本示例检索策略。模型和少样本设置根据图像和问题类型选择。答案选择基于模型的置信度水平。", "result": "在盲测数据上，该系统在七个参赛者中排名第三，ROUGE-1、ROUGE-L和BERTS的平均F1分数达到85.12。", "conclusion": "该系统在SciVQA 2025任务中表现出色，通过多模态LLM集成、少样本检索和置信度选择取得了显著的竞争力。", "translation": "这篇论文描述了我们为SciVQA 2025科学视觉问答共享任务开发的系统。我们的系统采用了两个多模态大型语言模型的集成以及各种少样本示例检索策略。模型和少样本设置是根据图表和问题类型选择的。我们还根据模型的置信度水平来选择答案。在盲测数据上，我们的系统在七个参赛者中排名第三，ROUGE-1、ROUGE-L和BERTS的平均F1分数达到85.12。我们的代码已公开发布。", "summary": "本文介绍了Coling-UniA团队为SciVQA 2025科学视觉问答共享任务设计的系统。该系统整合了两个多模态大型语言模型，并利用多种少样本示例检索策略，根据图表和问题类型动态选择模型和设置，并通过置信度筛选答案。该系统在盲测中取得了第三名的成绩，平均F1分数为85.12。", "keywords": "科学视觉问答, 多模态大语言模型, 少样本学习, 模型集成, 置信度感知", "comments": "该论文展示了一个在科学视觉问答任务中表现优秀的系统，其创新点在于结合了多模态大语言模型集成、根据内容类型动态调整的少样本检索以及基于模型置信度的答案选择。这些策略的结合有效提升了系统的性能，并在竞争激烈的共享任务中获得了高排名。其代码的公开性也促进了相关领域的研究。"}}
{"id": "2507.02666", "title": "ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning", "authors": ["Junyu Wang", "Tianrui Wang", "Meng Ge", "Longbiao Wang", "Jianwu Dang"], "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted at Interspeech2025", "url": "http://arxiv.org/abs/2507.02666v1", "summary": "In recent advancements in audio self-supervised representation learning, the\nstandard Transformer architecture has emerged as the predominant approach, yet\nits attention mechanism often allocates a portion of attention weights to\nirrelevant information, potentially impairing the model's discriminative\nability. To address this, we introduce a differential attention mechanism,\nwhich effectively mitigates ineffective attention allocation through the\nintegration of dual-softmax operations and appropriately tuned differential\ncoefficients. Experimental results demonstrate that our ASDA model achieves\nstate-of-the-art (SOTA) performance across multiple benchmarks, including audio\nclassification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting\n(98.3% accuracy on SPC-2), and environmental sound classification (96.1%\naccuracy on ESC-50). These results highlight ASDA's effectiveness in audio\ntasks, paving the way for broader applications.", "comment": "Accepted at Interspeech2025", "pdf_url": "http://arxiv.org/pdf/2507.02666v1", "cate": "cs.SD", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "ASDA：用于自监督表示学习的音频频谱图差分注意力机制", "tldr": "ASDA引入了一种差分注意力机制，通过双重softmax操作和差分系数来解决标准Transformer在音频自监督学习中注意力分配无效的问题，并在多个音频任务上取得了最先进的性能。", "motivation": "标准Transformer架构在音频自监督表示学习中存在注意力机制将部分注意力权重分配给不相关信息的问题，这可能会损害模型的判别能力。", "method": "引入了一种差分注意力机制（ASDA），通过集成双重softmax操作和适当调整的差分系数，有效缓解了无效的注意力分配。", "result": "ASDA模型在多个基准测试中取得了最先进的性能：音频分类（AS-2M上49.0% mAP，AS20K上41.5% mAP）、关键词识别（SPC-2上98.3%准确率）和环境声音分类（ESC-50上96.1%准确率）。", "conclusion": "ASDA在音频任务中表现出显著的有效性，为更广泛的应用铺平了道路。", "translation": "在音频自监督表示学习的最新进展中，标准Transformer架构已成为主流方法，但其注意力机制通常会将一部分注意力权重分配给不相关信息，这可能会损害模型的判别能力。为了解决这个问题，我们引入了一种差分注意力机制，通过集成双重softmax操作和适当调整的差分系数，有效缓解了无效的注意力分配。实验结果表明，我们的ASDA模型在多个基准测试中取得了最先进（SOTA）的性能，包括音频分类（AS-2M上49.0% mAP，AS20K上41.5% mAP）、关键词识别（SPC-2上98.3%准确率）和环境声音分类（ESC-50上96.1%准确率）。这些结果突出了ASDA在音频任务中的有效性，为更广泛的应用铺平了道路。", "summary": "该论文提出了一种名为ASDA的音频频谱图差分注意力机制，旨在解决现有Transformer在音频自监督学习中注意力分配不准确的问题。ASDA通过引入双重softmax操作和差分系数来优化注意力分配，有效提升了模型的判别能力。实验结果显示，ASDA在音频分类、关键词识别和环境声音分类等多个基准测试中均达到了最先进的性能，证明了其在音频任务中的有效性和广阔的应用前景。", "keywords": "差分注意力机制, 自监督学习, 音频频谱图, Transformer, 音频分类", "comments": "ASDA的创新之处在于其提出的差分注意力机制，通过双重softmax和差分系数优化了注意力分配，有效解决了Transformer在音频自监督学习中注意力冗余的问题。这对于提升模型在复杂音频环境下的判别能力具有重要意义，并为未来的音频表示学习提供了新的思路。"}}
{"id": "2507.02731", "title": "RIS-Aided Cooperative ISAC Networks for Structural Health Monitoring", "authors": ["Jie Yang", "Chao-Kai Wen", "Xiao Li", "Shi Jin"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.02731v1", "summary": "Integrated sensing and communication (ISAC) is a key feature of future\ncellular systems, enabling applications such as intruder detection, monitoring,\nand tracking using the same infrastructure. However, its potential for\nstructural health monitoring (SHM), which requires the detection of slow and\nsubtle structural changes, remains largely unexplored due to challenges such as\nmultipath interference and the need for ultra-high sensing precision. This\nstudy introduces a novel theoretical framework for SHM via ISAC by leveraging\nreconfigurable intelligent surfaces (RIS) as reference points in collaboration\nwith base stations and users. By dynamically adjusting RIS phases to generate\ndistinct radio signals that suppress background multipath interference,\nmeasurement accuracy at these reference points is enhanced. We theoretically\nanalyze RIS-aided collaborative sensing in three-dimensional cellular networks\nusing Fisher information theory, demonstrating how increasing observation time,\nincorporating additional receivers (even with self-positioning errors),\noptimizing RIS phases, and refining collaborative node selection can reduce the\nposition error bound to meet SHM's stringent accuracy requirements.\nFurthermore, we develop a Bayesian inference model to identify structural\nstates and validate damage detection probabilities. Both theoretical and\nnumerical analyses confirm ISAC's capability for millimeter-level deformation\ndetection, highlighting its potential for high-precision SHM applications.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.02731v1", "cate": "cs.IT", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "RIS辅助协同ISAC网络用于结构健康监测", "tldr": "本研究提出了一种新颖的理论框架，利用可重构智能表面（RIS）辅助的集成感知与通信（ISAC）网络进行结构健康监测（SHM），通过动态调整RIS相位和优化协作节点，显著提高了毫米级变形检测的精度。", "motivation": "集成感知与通信（ISAC）在入侵检测、监控和跟踪等未来蜂窝系统中具有巨大潜力，但其在结构健康监测（SHM）中的应用面临挑战，如多径干扰和对超高感知精度的需求，目前仍未被充分探索。", "method": "本研究引入了一种新的理论框架，通过利用可重构智能表面（RIS）作为参考点，与基站和用户协同工作，实现ISAC的结构健康监测。方法包括动态调整RIS相位以抑制背景多径干扰，使用Fisher信息理论分析RIS辅助的三维蜂窝网络中的协作感知，并开发贝叶斯推理模型来识别结构状态和验证损伤检测概率。", "result": "研究结果表明，增加观测时间、引入额外的接收器（即使存在自定位误差）、优化RIS相位以及优化协作节点选择可以显著降低位置误差界限，以满足SHM严格的精度要求。理论和数值分析均证实了ISAC能够实现毫米级变形检测。", "conclusion": "ISAC网络在RIS辅助下，通过优化配置和协作，能够实现高精度的毫米级变形检测，在结构健康监测领域具有巨大的应用潜力。", "translation": "集成感知与通信（ISAC）是未来蜂窝系统的一个关键特性，能够利用相同的基础设施实现入侵检测、监控和跟踪等应用。然而，其在结构健康监测（SHM）方面的潜力仍未被充分探索，SHM需要检测缓慢而细微的结构变化，并面临多径干扰和超高感知精度需求等挑战。本研究通过利用可重构智能表面（RIS）作为参考点，与基站和用户协作，引入了一种新颖的基于ISAC的SHM理论框架。通过动态调整RIS相位以生成独特的无线电信号来抑制背景多径干扰，从而增强了这些参考点的测量精度。我们使用Fisher信息理论对三维蜂窝网络中RIS辅助的协作感知进行了理论分析，证明了增加观测时间、引入额外的接收器（即使存在自定位误差）、优化RIS相位以及优化协作节点选择如何能够降低位置误差界限，以满足SHM严格的精度要求。此外，我们开发了一个贝叶斯推理模型来识别结构状态并验证损伤检测概率。理论和数值分析均证实了ISAC实现毫米级变形检测的能力，突显了其在高精度SHM应用中的潜力。", "summary": "本论文提出了一种新颖的理论框架，利用可重构智能表面（RIS）辅助的集成感知与通信（ISAC）网络进行结构健康监测（SHM）。通过动态调整RIS相位以抑制多径干扰，并结合Fisher信息理论和贝叶斯推理模型，研究证明了该系统能够通过优化观测时间、接收器数量、RIS相位和协作节点选择，实现高精度的毫米级变形检测，从而克服了现有ISAC在SHM应用中的挑战。", "keywords": "结构健康监测, 集成感知与通信, 可重构智能表面, 毫米级变形检测, 贝叶斯推理", "comments": "这项研究的创新之处在于将RIS引入ISAC网络以解决SHM中多径干扰和高精度需求的挑战。通过理论分析和数值验证，该方法展示了在毫米级变形检测方面的潜力，为未来高精度SHM应用提供了新的思路。其重要性在于拓展了ISAC在关键基础设施监测领域的应用，并为提升现有监测系统的精度提供了可行方案。"}}
{"id": "2507.02478", "title": "Effectively Identifying Wi-Fi Devices through State Transitions", "authors": ["Melissa Safari", "Abhishek K. Mishra", "Mathieu Cunche"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02478v1", "summary": "Wi-Fi management frames reveal structured communication patterns that persist\neven under randomization of MAC addresses. Prior approaches to associating\nrandomized MAC addresses with devices primarily focus on probe requests,\noverlooking the broader set of management frames and their transition dynamics.\nThis narrow focus limits their robustness in dense, real-world environments\nwith high device mobility, where probe activity alone fails to yield stable and\ndistinctive signatures. In this paper, we present a novel framework for\nfingerprinting Wi-Fi devices based on behavioral dynamics extracted from\npassively observed management frames. We model each device's behavior as a\nfinite state machine and introduce matrix-based representations that encode\nboth structural (state transition frequencies) and temporal (inter-state\ndelays) characteristics. These matrices are embedded into compact feature\nvectors, enabling efficient similarity comparison. Through extensive evaluation\nin diverse real-world settings, our method achieves over 86% identification\naccuracy for non-randomized devices using only Wi-Fi management frames, with\nfurther improvements observed through temporal burst aggregation. Our findings\nare sufficient to uniquely and consistently characterize devices at scale,\noutperforming the state-of-the-art.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02478v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过状态转换有效识别Wi-Fi设备", "tldr": "本文提出一种基于Wi-Fi管理帧行为动态的新型框架，通过状态转换有效识别Wi-Fi设备，在准确性和可扩展性上优于现有技术。", "motivation": "现有识别随机MAC地址设备的方法主要关注探测请求，忽略了更广泛的管理帧及其转换动态，导致在设备移动性高的真实环境中鲁棒性受限，且探测活动无法产生稳定独特的签名。", "method": "提出一种基于被动观察到的管理帧中提取的行为动态来识别Wi-Fi设备的新颖框架。将每个设备的行为建模为有限状态机，并引入基于矩阵的表示来编码结构（状态转换频率）和时间（状态间延迟）特征。这些矩阵被嵌入到紧凑的特征向量中以进行高效的相似性比较。", "result": "对于非随机化设备，仅使用Wi-Fi管理帧即可实现超过86%的识别准确率，通过时间突发聚合进一步提高了识别效果。研究结果足以大规模独特且一致地表征设备，优于现有技术。", "conclusion": "本文提出的基于管理帧状态转换的Wi-Fi设备识别方法，在准确性和可扩展性方面表现出色，并优于现有技术，能够在大规模环境下唯一且一致地识别设备。", "translation": "Wi-Fi管理帧揭示了即使在MAC地址随机化下也持续存在的结构化通信模式。先前将随机MAC地址与设备关联的方法主要侧重于探测请求，忽略了更广泛的管理帧及其转换动态。这种狭隘的关注限制了它们在设备移动性高、探测活动无法产生稳定独特签名的密集真实环境中的鲁棒性。在本文中，我们提出了一种基于从被动观察到的管理帧中提取的行为动态来识别Wi-Fi设备的新颖框架。我们将每个设备的行为建模为有限状态机，并引入基于矩阵的表示来编码结构（状态转换频率）和时间（状态间延迟）特征。这些矩阵被嵌入到紧凑的特征向量中，从而实现高效的相似性比较。通过在各种真实环境中的广泛评估，我们的方法仅使用Wi-Fi管理帧即可实现对非随机化设备超过86%的识别准确率，并通过时间突发聚合观察到进一步的改进。我们的发现足以大规模独特且一致地表征设备，优于现有技术。", "summary": "本文提出一种新颖的框架，通过分析被动观察到的Wi-Fi管理帧的行为动态来识别Wi-Fi设备。该方法将设备行为建模为有限状态机，并利用编码状态转换频率和状态间延迟的矩阵表示，生成紧凑特征向量进行高效相似性比较。该方法在真实环境中对非随机化设备实现了超过86%的识别准确率，并通过时间突发聚合进一步提升，优于现有技术，能够大规模唯一且一致地识别设备。", "keywords": "Wi-Fi设备识别, 管理帧, 状态转换, 设备指纹, 行为动态", "comments": "本文的创新点在于将Wi-Fi管理帧的行为动态建模为有限状态机，并利用其结构和时间特征进行设备指纹识别，克服了传统方法仅依赖探测请求的局限性。其提出的矩阵表示和特征向量嵌入方法有效提升了识别效率和准确性，在大规模设备识别方面展现出重要潜力。"}}
{"id": "2507.02328", "title": "Path Planning using a One-shot-sampling Skeleton Map", "authors": ["Gabriel O. Flores-Aquino", "Octavio Gutierrez-Frias", "Juan Irving Vasquez"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02328v1", "summary": "Path planning algorithms aim to compute a collision-free path, and many works\nfocus on finding the optimal distance path. However, for some applications, a\nmore suitable approach is to balance response time, safety of the paths, and\npath length. In this context, a skeleton map is a useful tool in graph-based\nschemes, as it provides an intrinsic representation of free configuration\nspace. However, skeletonization algorithms are very resource-intensive, being\nprimarily oriented towards image processing tasks. We propose an efficient\npath-planning methodology that finds safe paths within an acceptable processing\ntime. This methodology leverages a Deep Denoising Auto-Encoder (DDAE) based on\nU-Net architecture to compute a skeletonized version of the navigation map,\nwhich we refer to as SkelUnet. The SkelUnet network facilitates exploration of\nthe entire workspace through one-shot sampling (OSS), as opposed to the\niterative process used by exact algorithms or the probabilistic sampling\nprocess. SkelUnet is trained and tested on a dataset consisting of 12,500\nbi-dimensional dungeon maps. The motion planning methodology is evaluated in a\nsimulation environment for an Unmanned Aerial Vehicle (UAV) using 250\npreviously unseen maps, and assessed with various navigation metrics to\nquantify the navigability of the computed paths. The results demonstrate that\nusing SkelUnet to construct a roadmap offers significant advantages, such as\nconnecting all regions of free workspace, providing safer paths, and reducing\nprocessing times. These characteristics make this method particularly suitable\nfor mobile service robots in structured environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02328v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "使用一次性采样骨架图的路径规划", "tldr": "提出了一种基于U-Net的深度去噪自编码器（SkelUnet），用于高效生成导航地图的骨架化版本，并通过一次性采样实现路径规划，以平衡响应时间、路径安全性和路径长度，适用于移动服务机器人。", "motivation": "传统的路径规划算法通常侧重于寻找最优距离路径，但对于某些应用，更重要的是平衡响应时间、路径安全性和路径长度。现有的骨架化算法资源密集，主要面向图像处理任务，不适用于需要快速响应的路径规划。", "method": "本研究提出一种高效的路径规划方法，利用基于U-Net架构的深度去噪自编码器（DDAE），命名为SkelUnet，计算导航地图的骨架化版本。SkelUnet通过一次性采样（OSS）促进对整个工作空间的探索，这与迭代过程或概率采样过程不同。SkelUnet在包含12,500张二维地牢地图的数据集上进行训练和测试。该运动规划方法在一个无人机（UAV）仿真环境中，使用250张未曾见过的地图进行评估，并使用各种导航指标量化计算路径的可导航性。", "result": "结果表明，使用SkelUnet构建路线图具有显著优势，例如连接自由工作空间的所有区域，提供更安全的路径，并减少处理时间。", "conclusion": "SkelUnet方法能够生成连接所有自由工作空间区域、提供更安全路径并减少处理时间的骨架化地图，使其特别适用于结构化环境中的移动服务机器人。", "translation": "路径规划算法旨在计算无碰撞路径，许多工作都专注于寻找最优距离路径。然而，对于某些应用，更合适的方法是平衡响应时间、路径安全性和路径长度。在这种背景下，骨架图是基于图方案中的有用工具，因为它提供了自由配置空间的内在表示。然而，骨架化算法非常消耗资源，主要面向图像处理任务。我们提出了一种高效的路径规划方法，可以在可接受的处理时间内找到安全路径。该方法利用基于U-Net架构的深度去噪自编码器（DDAE）来计算导航地图的骨架化版本，我们称之为SkelUnet。SkelUnet网络通过一次性采样（OSS）促进对整个工作空间的探索，这与精确算法使用的迭代过程或概率采样过程不同。SkelUnet在包含12,500张二维地牢地图的数据集上进行训练和测试。该运动规划方法在一个无人机（UAV）仿真环境中，使用250张未曾见过的地图进行评估，并使用各种导航指标量化计算路径的可导航性。结果表明，使用SkelUnet构建路线图具有显著优势，例如连接自由工作空间的所有区域，提供更安全的路径，并减少处理时间。这些特性使得该方法特别适用于结构化环境中的移动服务机器人。", "summary": "本论文提出了一种名为SkelUnet的高效路径规划方法，旨在平衡路径规划中的响应时间、安全性与路径长度。SkelUnet利用基于U-Net架构的深度去噪自编码器生成导航地图的骨架化版本，并通过一次性采样技术实现对整个工作空间的快速探索。该方法在大量二维地图上进行了训练和评估，并在无人机仿真中验证了其有效性。实验结果表明，SkelUnet能够有效地连接自由工作空间、提供更安全的路径并显著缩短处理时间，使其特别适用于移动服务机器人。", "keywords": "路径规划, 骨架图, 深度学习, U-Net, 一次性采样", "comments": "该论文的创新点在于将深度学习（DDAE和U-Net架构）应用于骨架化过程，从而解决了传统骨架化算法资源密集的问题，并实现了高效的路径规划。其“一次性采样”的提出，避免了传统迭代或概率采样过程，显著提高了效率。该方法在平衡响应时间、路径安全性和长度方面的目标，使其在实际机器人应用中具有重要价值。然而，抽象中未详细说明“一次性采样”的具体实现机制和其在复杂动态环境中的鲁棒性。"}}
{"id": "2507.02254", "title": "A framework for 3D interaction techniques", "authors": ["Pablo Figueroa", "Mark Green", "Benjamin Watson"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02254v1", "summary": "This paper presents a software architecture for 3D interaction techniques\n(ITs) and an object oriented, toolkit-independent framework that implements\nsuch architecture. ITs are composed of basic filters connected in a dataflow,\nwhere virtual input devices and objects in the scene are sources of\ninformation. An execution model defines the general flow of information between\nfilters. This framework has been designed to be extensible: new information\ntypes, new input devices, new execution models, or new interaction techniques\ncan easily be added. Application specific code and application specific ITs are\nseamlessly integrated into this architecture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02254v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "3D交互技术的一个框架", "tldr": "本文提出了一个用于3D交互技术的可扩展软件框架，该框架基于数据流架构。", "motivation": "旨在提供一个可扩展的、面向对象、独立于工具包的3D交互技术软件架构和框架，以方便新组件的添加和集成。", "method": "该方法提出了一种软件架构，其中3D交互技术（ITs）由数据流中连接的基本过滤器组成，虚拟输入设备和场景对象是信息源。通过执行模型定义过滤器之间的信息流。", "result": "该框架具有可扩展性，可以轻松添加新的信息类型、新的输入设备、新的执行模型或新的交互技术。应用程序特定的代码和交互技术也可以无缝集成到该架构中。", "conclusion": "本文提供了一个可扩展且灵活的3D交互技术框架，简化了新组件和应用程序特定逻辑的集成。", "translation": "本文提出了一种用于3D交互技术（ITs）的软件架构，以及一个实现该架构的面向对象、独立于工具包的框架。交互技术由数据流中连接的基本过滤器组成，其中虚拟输入设备和场景中的对象是信息源。执行模型定义了过滤器之间信息的通用流。该框架被设计为可扩展的：可以轻松添加新的信息类型、新的输入设备、新的执行模型或新的交互技术。应用程序特定的代码和应用程序特定的交互技术可以无缝集成到该架构中。", "summary": "本文介绍了一个用于3D交互技术的面向对象、独立于工具包的软件框架。它采用数据流架构，其中交互技术由相互连接的过滤器构成，输入设备和场景对象作为数据源。该框架的一个关键特性是其可扩展性，允许轻松集成新的信息类型、输入设备、执行模型和交互技术，以及无缝纳入应用程序特定代码。", "keywords": "3D交互, 软件架构, 框架, 数据流, 可扩展性", "comments": "该论文的创新之处在于提出了一种可扩展、独立于工具包、基于数据流的3D交互架构，这促进了模块化和可重用性。其重要性在于简化了各种3D交互技术的开发和集成。抽象中未明确提及局限性。"}}
{"id": "2507.02680", "title": "On the Architectural Split and Radio Intelligence Controller Placement in Integrated O-RAN-enabled Non-Terrestrial Networks", "authors": ["Jorge Baranda", "Marius Caus", "Luis Blanco", "Cristian J. Vaca-Rubio", "Engin Zeydan", "Kapal Dev", "Zheng Li", "Tomaso DeCola"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      20 pages, 7 figures", "url": "http://arxiv.org/abs/2507.02680v1", "summary": "The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks\n(NTNs) poses unique architectural and functional challenges due to\nheterogeneous propagation conditions, dynamic topologies and limited on-board\nprocessing capabilities. This paper examines architectural and functional split\nstrategies that are consistent with O-RAN principles for future integrated\nTN-NTN systems. A taxonomy of split options is proposed that distributes RAN\nand core functions between satellites and ground nodes, and trade-offs in terms\nof performance, latency, autonomy and deployment are analysed. In particular,\nwe evaluate configurations ranging from pure on-board DU deployments to full\ngNB and UPF integration into satellites, including variations based on intra-\nand inter-satellite processing. In addition, the placement of Near-RT and\nNon-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible\nsplit strategies between space and ground to optimise the performance and\nscalability of the control loop. A comprehensive mapping between architectural\nsplits and RIC placement options is provided, emphasising implementation\nconstraints and interoperability considerations. The paper concludes by\nidentifying key challenges and outlining future directions to enable\nstandardised, modular and efficient TN-NTN convergence in the context of the\nO-RAN.", "comment": "20 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.02680v1", "cate": "cs.NI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "集成O-RAN非地面网络中的架构划分和无线智能控制器部署", "tldr": "该论文探讨了在集成陆地-非陆地网络中，基于O-RAN的架构和功能划分策略，以及无线智能控制器（RIC）的部署，分析了性能权衡和实施挑战。", "motivation": "由于异构传播条件、动态拓扑和有限的机载处理能力，陆地网络（TNs）与非陆地网络（NTNs）的集成带来了独特的架构和功能挑战。", "method": "本文研究了与O-RAN原则一致的未来集成陆地-非陆地系统中的架构和功能划分策略。提出了一种划分选项分类法，用于在卫星和地面节点之间分配RAN和核心功能，并分析了性能、延迟、自主性和部署方面的权衡。论文评估了从纯机载DU部署到将完整gNB和UPF集成到卫星中的各种配置，包括基于卫星内部和卫星间处理的变体。此外，还讨论了近实时（Near-RT）和非实时（Non-RT）RAN智能控制器（RICs）的部署，提出了空间和地面之间灵活的划分策略，以优化控制环路的性能和可扩展性。论文提供了一个架构划分与RIC部署选项之间的全面映射。", "result": "论文提出了划分选项的分类法，分析了性能、延迟、自主性和部署方面的权衡，评估了各种配置，并提供了架构划分与RIC部署选项之间的全面映射，强调了实施约束和互操作性考量。", "conclusion": "论文最后指出了关键挑战，并概述了在O-RAN背景下实现标准化、模块化和高效的陆地-非陆地网络融合的未来方向。", "translation": "陆地网络（TNs）与非陆地网络（NTNs）的集成由于异构传播条件、动态拓扑和有限的机载处理能力，带来了独特的架构和功能挑战。本文研究了与O-RAN原则一致的未来集成陆地-非陆地系统中的架构和功能划分策略。论文提出了一种划分选项分类法，用于在卫星和地面节点之间分配RAN和核心功能，并分析了性能、延迟、自主性和部署方面的权衡。特别是，我们评估了从纯机载DU部署到将完整gNB和UPF集成到卫星中的各种配置，包括基于卫星内部和卫星间处理的变体。此外，还讨论了近实时（Near-RT）和非实时（Non-RT）RAN智能控制器（RICs）的部署，提出了空间和地面之间灵活的划分策略，以优化控制环路的性能和可扩展性。论文提供了一个架构划分与RIC部署选项之间的全面映射，强调了实施约束和互操作性考量。论文最后指出了关键挑战，并概述了在O-RAN背景下实现标准化、模块化和高效的陆地-非陆地网络融合的未来方向。", "summary": "本文探讨了未来集成陆地-非陆地网络（TN-NTN）中与O-RAN原则相符的架构和功能划分策略。它提出了一种划分选项的分类法，用于在卫星和地面节点之间分配RAN和核心功能，并分析了性能、延迟、自主性和部署方面的权衡。研究还讨论了RAN智能控制器（RICs）的优化部署，以增强控制环路的性能和可扩展性。论文提供了一个全面的架构划分与RIC部署映射，并探讨了实施约束和互操作性，最后指出了在O-RAN框架下实现TN-NTN融合的关键挑战和未来方向。", "keywords": "O-RAN, 非地面网络, 架构划分, 无线智能控制器, TN-NTN集成", "comments": "这篇论文解决了电信领域一个新兴且至关重要的问题，即陆地网络与非陆地网络的集成，这对于实现无处不在的连接至关重要。其对O-RAN原则的关注以及对架构划分和RIC部署的详细分析具有创新性。对实施约束和互操作性考量的识别增加了其实用价值。"}}
{"id": "2507.02443", "title": "Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic", "authors": ["Sandro Costa Magalhães", "Marco Almeida", "Filipe Neves dos Santos", "António Paulo Moreira", "Jorge Dias"], "categories": ["cs.CV", "cs.AI", "cs.DC", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Submitted to ROBOT'2025", "url": "http://arxiv.org/abs/2507.02443v1", "summary": "Robots usually slow down for canning to detect objects while moving.\nAdditionally, the robot's camera is configured with a low framerate to track\nthe velocity of the detection algorithms. This would be constrained while\nexecuting tasks and exploring, making robots increase the task execution time.\nAMD has developed the Vitis-AI framework to deploy detection algorithms into\nFPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we\nuse the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit\nquantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation\n(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This\nis a self-acquired dataset released in open access. MobileNet v1 performed\nbetter, reaching a success rate of 98 % and an inference speed of 6611 FPS. In\nthis work, we proved that we can use FPGAs to speed up ANNs and make them\nsuitable for attention mechanisms.", "comment": "Submitted to ROBOT'2025", "pdf_url": "http://arxiv.org/pdf/2507.02443v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于FPGA可编程逻辑中加速人工神经网络的红葡萄检测", "tldr": "本研究利用FINN架构在FPGA可编程逻辑中部署量化后的神经网络，以加速机器人对红葡萄的检测，显著提高了推理速度和准确率。", "motivation": "机器人通常在移动中进行物体检测时会减速，并且摄像机帧率较低，这限制了任务执行和探索，增加了任务执行时间。现有的部署工具（如Vitis-AI）未能充分利用FPGA的可编程逻辑（PL）。", "method": "本研究采用FINN架构，在FPGA的可编程逻辑（PL）内部署了三种人工神经网络：4位量化的MobileNet v1、2位量化的CNV和1位量化的CNV（BNN）。所有模型均在RG2C数据集上进行训练，该数据集是自行获取并开放访问的。", "result": "MobileNet v1表现最佳，达到了98%的成功率和6611 FPS的推理速度。", "conclusion": "本研究证明了可以使用FPGA来加速人工神经网络，使其适用于注意力机制。", "translation": "机器人通常在移动中进行物体检测时会减速。此外，机器人摄像机的帧率较低，以追踪检测算法的速度。这在执行任务和探索时会受到限制，导致机器人增加任务执行时间。AMD开发了Vitis-AI框架，用于将检测算法部署到FPGA中。然而，这个工具并未充分利用FPGA的PL。在这项工作中，我们使用FINN架构，在FPGA的PL内部署了三种人工神经网络：4位量化的MobileNet v1、2位量化的CNV和1位量化的CNV（BNN）。这些模型在RG2C数据集上进行了训练。这是一个自行获取并开放访问的数据集。MobileNet v1表现更好，达到了98%的成功率和6611 FPS的推理速度。在这项工作中，我们证明了可以使用FPGA来加速人工神经网络，使其适用于注意力机制。", "summary": "本研究旨在解决机器人移动检测中速度受限和FPGA资源利用不足的问题。通过在FPGA可编程逻辑中使用FINN架构部署量化后的MobileNet v1和CNV等神经网络，并在RG2C数据集上进行训练，实现了对红葡萄的高速检测。结果显示，MobileNet v1表现卓越，成功率达98%，推理速度高达6611 FPS，证明了FPGA在加速人工神经网络方面的潜力，使其适用于机器人注意力机制。", "keywords": "红葡萄检测, 人工神经网络, FPGA, 加速, MobileNet", "comments": "这项工作通过充分利用FPGA的可编程逻辑，显著提升了机器人视觉检测的速度和效率，尤其是在资源受限的移动机器人应用中具有重要意义。其创新之处在于采用FINN架构部署高度量化的神经网络，实现了极高的帧率，为实时物体检测提供了强大的硬件加速方案。"}}
{"id": "2507.02208", "title": "Modeling the Effective Elastic Modulus and Thickness of Corrugated Boards Using Gaussian Process Regression and Expected Hypervolume Improvement", "authors": ["Ricardo Fitas"], "categories": ["physics.app-ph", "cs.CE", "physics.comp-ph"], "primary_category": "Subjects:       Applied Physics (physics.app-ph)", "pdf_link": null, "comments": "Comments:      This is the submitted version of the manuscript entitled \"Modeling the Effective Elastic Modulus and Thickness of Corrugated Boards Using Gaussian Process Regression and Expected Hypervolume Improvement.\" The final version is published in \"Lecture Notes in Civil Engineering\" (Springer Nature) as part of the OPTARCH 2024 proceedings", "url": "http://arxiv.org/abs/2507.02208v1", "summary": "This work aims to model the hypersurface of the effective elastic modulus, \\(\nE_{z, \\text{eff}} \\), and thickness, \\( th_{\\text{eff}} \\), in corrugated\nboards. A Latin Hypercube Sampling (LHS) is followed by Gaussian Process\nRegression (GP), enhanced by EHVI as a multi-objective acquisition function.\nAccurate modeling of \\( E_{z, \\text{eff}} \\) and \\( th_{\\text{eff}} \\) is\ncritical for optimizing the mechanical properties of corrugated materials in\nengineering applications. LHS provides an efficient and straightforward\napproach for an initial sampling of the input space; GP is expected to be able\nto adapt to the complexity of the response surfaces by incorporating both\nprediction and uncertainty. Therefore, the next points being generated and\nevaluated are based on the complexity of the hypersurfaces, and some points,\nespecially those with higher variance, are more exploited and carry more\nimportance. The performance of GP with EHVI is measured by Mean Squared Error\n(MSE). Prediction of GP resulted in \\( \\text{MSE}(E_{z, \\text{eff}}) = 5.24 \\,\n\\text{kPa}^2 \\) and \\( \\text{MSE}(th_{\\text{eff}}) = 1 \\, \\text{mm}^2 \\). GP\npossesses then improved accuracy and adaptability for future applications in\nstructural optimization.", "comment": "This is the submitted version of the manuscript entitled \"Modeling\n  the Effective Elastic Modulus and Thickness of Corrugated Boards Using\n  Gaussian Process Regression and Expected Hypervolume Improvement.\" The final\n  version is published in \"Lecture Notes in Civil Engineering\" (Springer\n  Nature) as part of the OPTARCH 2024 proceedings", "pdf_url": "http://arxiv.org/pdf/2507.02208v1", "cate": "physics.app-ph", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "使用高斯过程回归和预期超体积改进对瓦楞纸板的有效弹性模量和厚度进行建模", "tldr": "本研究使用高斯过程回归（GP）和预期超体积改进（EHVI）成功建模了瓦楞纸板的有效弹性模量和厚度，显著提高了预测精度和适应性，为结构优化奠定基础。", "motivation": "瓦楞纸板中有效弹性模量和厚度的精确建模对于优化其在工程应用中的力学性能至关重要。", "method": "本文采用拉丁超立方采样（LHS）进行初始输入空间采样，随后使用高斯过程回归（GP）进行建模。GP通过结合预测和不确定性来适应响应曲面的复杂性，并利用EHVI作为多目标采集函数来生成和评估下一个点，尤其关注方差较高的点。", "result": "高斯过程（GP）的预测结果显示，有效弹性模量（E_z, eff）的均方误差（MSE）为5.24 kPa²，有效厚度（th_eff）的MSE为1 mm²。", "conclusion": "高斯过程（GP）展现出更高的精度和适应性，可用于未来的结构优化应用。", "translation": "这项工作旨在对瓦楞纸板中有效弹性模量（E_z, eff）和厚度（th_eff）的超曲面进行建模。采用拉丁超立方采样（LHS），随后是高斯过程回归（GP），并通过EHVI作为多目标采集函数进行增强。精确建模E_z, eff和th_eff对于优化瓦楞材料在工程应用中的力学性能至关重要。LHS为输入空间的初始采样提供了一种高效直接的方法；GP有望通过结合预测和不确定性来适应响应曲面的复杂性。因此，生成和评估的下一个点是基于超曲面的复杂性，并且一些点，特别是那些具有较高方差的点，被更多地利用并具有更重要的意义。GP与EHVI的性能通过均方误差（MSE）来衡量。GP的预测结果为MSE(E_z, eff) = 5.24 kPa²和MSE(th_eff) = 1 mm²。因此，GP在未来的结构优化应用中具有更高的精度和适应性。", "summary": "本文研究了使用高斯过程回归（GP）和预期超体积改进（EHVI）对瓦楞纸板的有效弹性模量和厚度进行建模。通过拉丁超立方采样（LHS）进行初始数据采样，GP结合预测和不确定性来适应复杂的响应曲面，并通过EHVI优化采样点。实验结果显示，GP在建模有效弹性模量和厚度方面取得了较低的均方误差（MSE），表明其在瓦楞纸板结构优化中具有更高的精度和适应性。", "keywords": "瓦楞纸板, 有效弹性模量, 高斯过程回归, EHVI, 超曲面建模", "comments": "本文的创新点在于结合高斯过程回归与EHVI多目标采集函数，有效地建模了瓦楞纸板的关键力学参数。这种方法不仅考虑了预测的准确性，还纳入了不确定性，并通过优化采样点提高了建模效率，对于瓦楞纸板的结构优化具有重要的工程实践意义。"}}
{"id": "2507.02197", "title": "Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust", "authors": ["Amogh Mannekote", "Adam Davies", "Guohao Li", "Kristy Elizabeth Boyer", "ChengXiang Zhai", "Bonnie J Dorr", "Francesco Pinto"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02197v1", "summary": "As LLMs are increasingly studied as role-playing agents to generate synthetic\ndata for human behavioral research, ensuring that their outputs remain coherent\nwith their assigned roles has become a critical concern. In this paper, we\ninvestigate how consistently LLM-based role-playing agents' stated beliefs\nabout the behavior of the people they are asked to role-play (\"what they say\")\ncorrespond to their actual behavior during role-play (\"how they act\").\nSpecifically, we establish an evaluation framework to rigorously measure how\nwell beliefs obtained by prompting the model can predict simulation outcomes in\nadvance. Using an augmented version of the GenAgents persona bank and the Trust\nGame (a standard economic game used to quantify players' trust and\nreciprocity), we introduce a belief-behavior consistency metric to\nsystematically investigate how it is affected by factors such as: (1) the types\nof beliefs we elicit from LLMs, like expected outcomes of simulations versus\ntask-relevant attributes of individual characters LLMs are asked to simulate;\n(2) when and how we present LLMs with relevant information about Trust Game;\nand (3) how far into the future we ask the model to forecast its actions. We\nalso explore how feasible it is to impose a researcher's own theoretical priors\nin the event that the originally elicited beliefs are misaligned with research\nobjectives. Our results reveal systematic inconsistencies between LLMs' stated\n(or imposed) beliefs and the outcomes of their role-playing simulation, at both\nan individual- and population-level. Specifically, we find that, even when\nmodels appear to encode plausible beliefs, they may fail to apply them in a\nconsistent way. These findings highlight the need to identify how and when\nLLMs' stated beliefs align with their simulated behavior, allowing researchers\nto use LLM-based agents appropriately in behavioral studies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02197v1", "cate": "cs.AI", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "角色扮演智能体是否言行一致？LLM驱动的人类信任模拟中的信念-行为一致性", "tldr": "本研究发现LLM角色扮演智能体在模拟人类信任时，其所表达的信念与实际行为存在系统性不一致。", "motivation": "随着大型语言模型（LLMs）越来越多地被用作角色扮演智能体来生成人类行为研究的合成数据，确保其输出与所扮演的角色保持一致性成为一个关键问题。本文旨在调查LLM角色扮演智能体所陈述的信念（“他们所说的”）与其在角色扮演期间的实际行为（“他们如何行动”）之间的一致性。", "method": "研究建立了一个评估框架，以严格衡量通过提示模型获得的信念预测模拟结果的程度。使用增强版的GenAgents角色库和信任博弈（一种用于量化玩家信任和互惠的标准经济博弈），引入了一个信念-行为一致性指标。系统地调查了信念-行为一致性如何受到以下因素的影响：1）从LLMs中引出的信念类型（如预期模拟结果与个体角色的任务相关属性）；2）何时以及如何向LLMs提供有关信任博弈的相关信息；3）要求模型预测未来行动的时间跨度。研究还探讨了当最初引出的信念与研究目标不符时，研究者强加自己的理论先验的可行性。", "result": "结果揭示了LLMs所陈述（或被强加）的信念与其角色扮演模拟结果之间存在系统性不一致，这在个体和群体层面均有体现。具体而言，即使模型似乎编码了合理的信念，它们也可能无法以一致的方式应用这些信念。", "conclusion": "这些发现强调了识别LLMs所陈述信念何时以及如何与其模拟行为保持一致的必要性，从而使研究人员能够在行为研究中适当使用基于LLM的智能体。", "translation": "随着大型语言模型（LLMs）越来越多地被用作角色扮演智能体来生成人类行为研究的合成数据，确保其输出与所扮演的角色保持一致性成为一个关键问题。在本文中，我们调查了基于LLM的角色扮演智能体所陈述的关于他们被要求扮演的人的行为的信念（“他们所说的”）与他们在角色扮演期间的实际行为（“他们如何行动”）之间的一致性。具体来说，我们建立了一个评估框架，以严格衡量通过提示模型获得的信念在多大程度上能够提前预测模拟结果。我们使用增强版的GenAgents角色库和信任博弈（一种用于量化玩家信任和互惠的标准经济博弈），引入了一个信念-行为一致性指标，以系统地调查它如何受到以下因素的影响：(1) 我们从LLMs中引出的信念类型，例如模拟的预期结果与LLMs被要求模拟的个体角色的任务相关属性；(2) 我们何时以及如何向LLMs提供有关信任博弈的相关信息；以及(3) 我们要求模型预测未来行动的时间跨度。我们还探讨了在最初引出的信念与研究目标不符的情况下，强加研究者自身理论先验的可行性。我们的结果揭示了LLMs所陈述（或被强加）的信念与其角色扮演模拟结果之间存在系统性不一致，这在个体和群体层面均有体现。具体而言，我们发现，即使模型似乎编码了合理的信念，它们也可能无法以一致的方式应用这些信念。这些发现强调了识别LLMs所陈述信念何时以及如何与其模拟行为保持一致的必要性，从而使研究人员能够在行为研究中适当使用基于LLM的智能体。", "summary": "本研究探讨了大型语言模型（LLM）作为角色扮演智能体在模拟人类信任行为时，其所表达的信念与实际行为之间的一致性问题。通过构建一个评估框架并使用增强的GenAgents角色库和信任博弈，研究引入了信念-行为一致性指标，并分析了信念类型、信息呈现方式及预测时间跨度等因素对其影响。结果表明，LLM在个体和群体层面均存在信念与模拟行为的系统性不一致，即使模型编码了合理信念，也可能无法一致地应用。这强调了理解LLM信念与模拟行为对齐方式的重要性，以确保其在行为研究中的恰当应用。", "keywords": "大型语言模型, 角色扮演智能体, 信念-行为一致性, 信任博弈, 人类行为模拟", "comments": "这项研究通过引入“信念-行为一致性”这一新颖指标，对LLM在行为模拟中的可靠性提出了重要质疑。其创新性在于不仅关注LLM生成数据的表面 plausibility，更深入探究了其内在逻辑（信念）与外在表现（行为）的匹配度。研究结果揭示的系统性不一致性，对于依赖LLM进行合成数据生成和行为预测的研究领域具有重要警示意义，提示研究人员在使用LLM时需谨慎并考虑其潜在的局限性。同时，其提出的评估框架和影响因素分析，也为未来提升LLM作为行为研究工具的有效性和可信度指明了方向。"}}
{"id": "2507.02250", "title": "FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model", "authors": ["Jiangxia Chen", "Tongyuan Huang", "Ke Song"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02250v1", "summary": "3D semantic occupancy prediction plays a pivotal role in autonomous driving.\nHowever, inherent limitations of fewframe images and redundancy in 3D space\ncompromise prediction accuracy for occluded and distant scenes. Existing\nmethods enhance performance by fusing historical frame data, which need\nadditional data and significant computational resources. To address these\nissues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement\noccupancy network with flow matching selective state space model for few-frame\n3D occupancy prediction. Firstly, to generate missing features, we designed a\nfeature refinement module based on a flow matching model, which is called Flow\nMatching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and\nPlane Selective SSM (PS3M), we selectively filter TPV features to reduce the\nimpact of air voxels on non-air voxels, thereby enhancing the overall\nefficiency of the model and prediction capability for distant scenes. Finally,\nwe design the Mask Training (MT) method to enhance the robustness of FMOcc and\naddress the issue of sensor data loss. Experimental results on the\nOcc3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing\nstate-of-theart methods. Our FMOcc with two frame input achieves notable scores\nof 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on\nOpenOcc with 5.4 G inference memory and 330ms inference time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02250v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "FMOcc：TPV驱动的流匹配选择性状态空间模型用于3D占用预测", "tldr": "FMOcc通过流匹配和选择性状态空间模型，解决了自动驾驶中少量帧图像3D语义占用预测精度低的问题，尤其在遮挡和远距离场景下表现优异。", "motivation": "自动驾驶中3D语义占用预测面临少量帧图像的固有局限性和3D空间的冗余性，导致对遮挡和远距离场景的预测精度受损。现有方法通过融合历史帧数据来提高性能，但需要额外数据和大量计算资源，效率低下。", "method": "本文提出FMOcc，一个基于流匹配选择性状态空间模型的TPV（Tri-perspective View）细化占用网络，用于少量帧3D占用预测。具体包括：1) 设计了基于流匹配模型的特征细化模块FMSSM以生成缺失特征；2) 设计了TPV SSM层和平面选择性SSM（PS3M）以选择性地过滤TPV特征，减少空中体素对非空中体素的影响，从而提高模型整体效率和远距离场景预测能力；3) 设计了掩码训练（MT）方法以增强FMOcc的鲁棒性并解决传感器数据丢失问题。", "result": "在Occ3D-nuScenes和OpenOcc数据集上的实验结果表明，FMOcc优于现有最先进的方法。FMOcc在Occ3D-nuScenes验证集上，使用两帧输入时，RayIoU达到43.1%，mIoU达到39.8%；在OpenOcc上，RayIoU达到42.6%，推理内存为5.4G，推理时间为330ms。", "conclusion": "FMOcc通过其创新的流匹配和选择性状态空间模型模块以及掩码训练方法，显著提升了3D占用预测的性能，尤其在处理少量帧、遮挡和远距离场景时，并展现出优越的效率和鲁棒性。", "translation": "3D语义占用预测在自动驾驶中扮演着关键角色。然而，少量帧图像的固有局限性和3D空间的冗余性损害了对遮挡和远距离场景的预测精度。现有方法通过融合历史帧数据来提高性能，这需要额外的数据和大量的计算资源。为了解决这些问题，本文提出了FMOcc，一个基于流匹配选择性状态空间模型的TPV（Tri-perspective View）细化占用网络，用于少量帧3D占用预测。首先，为了生成缺失特征，我们设计了一个基于流匹配模型的特征细化模块，称为流匹配SSM模块（FMSSM）。此外，通过设计TPV SSM层和平面选择性SSM（PS3M），我们选择性地过滤TPV特征，以减少空中体素对非空中体素的影响，从而提高模型的整体效率和对远距离场景的预测能力。最后，我们设计了掩码训练（MT）方法，以增强FMOcc的鲁棒性并解决传感器数据丢失问题。在Occ3D-nuScenes和OpenOcc数据集上的实验结果表明，我们的FMOcc优于现有最先进的方法。我们的FMOcc在Occ3D-nuScenes验证集上，使用两帧输入时，取得了43.1%的RayIoU和39.8%的mIoU；在OpenOcc上，取得了42.6%的RayIoU，推理内存为5.4G，推理时间为330ms。", "summary": "本文提出了FMOcc，一个用于自动驾驶中少量帧3D语义占用预测的新模型。针对现有方法在少量帧和冗余3D空间下预测精度低、计算资源消耗大的问题，FMOcc引入了基于流匹配的特征细化模块FMSSM以生成缺失特征，并设计了TPV SSM层和平面选择性SSM（PS3M）来高效过滤TPV特征，以提升远距离场景预测能力。此外，通过掩码训练（MT）增强模型鲁棒性。实验证明FMOcc在Occ3D-nuScenes和OpenOcc数据集上性能优于SOTA方法，尤其在RayIoU和mIoU指标上表现出色，且具有高效的推理速度和内存占用。", "keywords": "3D占用预测, 流匹配, 状态空间模型, 自动驾驶, TPV", "comments": "FMOcc通过引入流匹配和选择性状态空间模型，有效地解决了少量帧输入下3D占用预测的挑战，特别是在处理遮挡和远距离场景时。其创新点在于FMSSM用于特征补全、PS3M用于特征过滤以提高效率，以及MT方法提升鲁棒性。这些模块的结合使其在性能和效率上超越了现有SOTA方法，为自动驾驶领域的3D感知提供了新的思路。"}}
{"id": "2507.02468", "title": "The Bias of Subspace-based Data-Driven Predictive Control", "authors": ["Keith Moffat", "Florian Dörfler", "Alessandro Chiuso"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02468v1", "summary": "This paper quantifies and addresses the bias of subspace-based Data-Driven\nPredictive Control (DDPC) for linear, time-invariant (LTI) systems. The primary\nfocus is the bias that arises when the training data is gathered with a\nfeedback controller in closed-loop with the system. First, the closed-loop bias\nof Subspace Predictive Control is quantified using the training data\ninnovations. Next, the bias of direct, subspace-based DDPC methods DeePC and\n$\\gamma$-DDPC is shown to consist of two parts--the Subspace Bias, which arises\nfrom closed-loop data, and an Optimism Bias, which arises from\nDeePC/$\\gamma$-DDPC's \"optimistic\" adjustment of the output trajectory. We show\nthat, unlike subspace-based DDPC methods, Transient Predictive Control does not\nsuffer from Subspace Bias or Optimism Bias. Double integrator experiments\ndemonstrate that Subspace and Optimism Bias are responsible for poor reference\ntracking by the subspace-based DDPC methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02468v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于子空间的数据驱动预测控制的偏差", "tldr": "本文量化并解决了基于子空间的数据驱动预测控制（DDPC）在处理闭环数据时产生的偏差问题，识别出子空间偏差和乐观偏差，并指出这些偏差是导致其性能不佳的原因。", "motivation": "该研究旨在量化并解决线性时不变（LTI）系统中基于子空间的数据驱动预测控制（DDPC）的偏差问题，特别是当训练数据通过带有反馈控制器的闭环系统收集时产生的偏差。", "method": "首先，利用训练数据创新量化了子空间预测控制的闭环偏差。接着，揭示了直接的、基于子空间的DDPC方法（DeePC和$\\\\gamma$-DDPC）的偏差由两部分组成：源于闭环数据的子空间偏差，以及源于DeePC/$\\\\gamma$-DDPC对输出轨迹的“乐观”调整而产生的乐观偏差。研究还通过对比表明瞬态预测控制不包含这些偏差。", "result": "研究表明，与基于子空间的DDPC方法不同，瞬态预测控制（Transient Predictive Control）不受子空间偏差或乐观偏差的影响。双积分器实验证明，子空间偏差和乐观偏差是导致基于子空间的DDPC方法参考跟踪性能不佳的原因。", "conclusion": "基于子空间的数据驱动预测控制方法在闭环数据下存在子空间偏差和乐观偏差，这些偏差会导致其参考跟踪性能下降。", "translation": "本文量化并解决了线性时不变（LTI）系统中基于子空间的数据驱动预测控制（DDPC）的偏差问题。主要关注的是当训练数据通过带有反馈控制器的闭环系统收集时产生的偏差。首先，利用训练数据创新量化了子空间预测控制的闭环偏差。接着，揭示了直接的、基于子空间的DDPC方法DeePC和$\\\\gamma$-DDPC的偏差由两部分组成——源于闭环数据的子空间偏差，以及源于DeePC/$\\\\gamma$-DDPC对输出轨迹的“乐观”调整而产生的乐观偏差。我们表明，与基于子空间的DDPC方法不同，瞬态预测控制（Transient Predictive Control）不受子空间偏差或乐观偏差的影响。双积分器实验证明，子空间偏差和乐观偏差是导致基于子空间的DDPC方法参考跟踪性能不佳的原因。", "summary": "本文深入分析并量化了基于子空间的数据驱动预测控制（DDPC）在处理闭环系统数据时出现的偏差。研究明确指出，这类偏差主要由两部分构成：源于闭环数据的子空间偏差和源于DDPC方法“乐观”调整的乐观偏差。通过理论分析和实验验证，论文揭示了这些偏差如何导致基于子空间的DDPC方法在参考跟踪方面表现不佳，并指出瞬态预测控制则不受此类偏差影响。", "keywords": "数据驱动预测控制, 偏差, 子空间, 闭环系统, LTI系统", "comments": "该论文对数据驱动预测控制领域的一个关键问题——偏差进行了深入分析和量化，特别是区分了子空间偏差和乐观偏差，这对于理解和改进基于子空间的DDPC方法具有重要意义。通过实验验证了这些偏差对系统性能的影响，为未来设计更鲁棒的数据驱动控制器提供了理论基础。"}}
{"id": "2507.02427", "title": "When Attention is Beneficial for Learning Wireless Resource Allocation Efficiently?", "authors": ["Jia Guo", "Chenyang Yang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      13 pages, 6 figures", "url": "http://arxiv.org/abs/2507.02427v1", "summary": "Owing to the use of attention mechanism to leverage the dependency across\ntokens, Transformers are efficient for natural language processing. By\nharnessing permutation properties broadly exist in resource allocation\npolicies, each mapping measurable environmental parameters (e.g., channel\nmatrix) to optimized variables (e.g., precoding matrix), graph neural networks\n(GNNs) are promising for learning these policies efficiently in terms of\nscalability and generalizability. To reap the benefits of both architectures,\nthere is a recent trend of incorporating attention mechanism with GNNs for\nlearning wireless policies. Nevertheless, is the attention mechanism really\nneeded for resource allocation? In this paper, we strive to answer this\nquestion by analyzing the structures of functions defined on sets and numerical\nalgorithms, given that the permutation properties of wireless policies are\ninduced by the involved sets (say user set). In particular, we prove that the\npermutation equivariant functions on a single set can be recursively expressed\nby two types of functions: one involves attention, and the other does not. We\nproceed to re-express the numerical algorithms for optimizing several\nrepresentative resource allocation problems in recursive forms. We find that\nwhen interference (say multi-user or inter-data stream interference) is not\nreflected in the measurable parameters of a policy, attention needs to be used\nto model the interference. With the insight, we establish a framework of\ndesigning GNNs by aligning with the structures. By taking reconfigurable\nintelligent surface-aided hybrid precoding as an example, the learning\nefficiency of the proposed GNN is validated via simulations.", "comment": "13 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.02427v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "当注意力机制何时有利于高效学习无线资源分配？", "tldr": "本文探究了注意力机制何时对无线资源分配学习有益，发现当干扰未在可测量参数中体现时，需要使用注意力来建模干扰，并提出了一个相应的GNN设计框架。", "motivation": "鉴于Transformer在自然语言处理中的成功以及图神经网络在无线资源分配中的潜力，研究人员开始将注意力机制与图神经网络结合以学习无线策略。然而，注意力机制是否真的对资源分配是必需的，这一问题尚未得到明确回答，这构成了本文的主要动机。", "method": "本文通过分析集合上定义函数和数值算法的结构来回答该问题，考虑无线策略的置换特性。具体而言，证明了单个集合上的置换等变函数可以递归地表示为两种类型：一种涉及注意力，另一种不涉及。然后将几个代表性资源分配问题的数值算法重新表达为递归形式，并基于发现的见解，建立了一个与结构对齐的GNN设计框架。", "result": "研究发现，当干扰（例如多用户或数据流间干扰）未反映在策略的可测量参数中时，需要使用注意力来建模干扰。通过以可重构智能表面辅助混合预编码为例，仿真验证了所提出的GNN的学习效率。", "conclusion": "本文明确了注意力机制在无线资源分配中何时是有益的，尤其是在干扰未直接体现在可测量参数中时。基于此洞察，建立了一个设计图神经网络的框架，提高了学习效率。", "translation": "标题： 当注意力机制何时有利于高效学习无线资源分配？\n摘要： 由于注意力机制能够利用token间的依赖关系，Transformer在自然语言处理方面表现高效。通过利用资源分配策略中广泛存在的置换特性，每个策略将可测量的环境参数（如信道矩阵）映射到优化变量（如预编码矩阵），图神经网络（GNNs）在可扩展性和泛化性方面有望高效学习这些策略。为了同时利用这两种架构的优势，近期出现了将注意力机制与GNNs结合以学习无线策略的趋势。然而，注意力机制真的对资源分配是必需的吗？在本文中，我们致力于通过分析集合上定义函数和数值算法的结构来回答这个问题，考虑到无线策略的置换特性是由所涉及的集合（例如用户集）引起的。具体而言，我们证明了单个集合上的置换等变函数可以递归地表示为两种类型的函数：一种涉及注意力，另一种不涉及。我们接着将优化几个代表性资源分配问题的数值算法重新表达为递归形式。我们发现，当干扰（例如多用户或数据流间干扰）未反映在策略的可测量参数中时，需要使用注意力来建模干扰。基于这一洞察，我们建立了一个与这些结构对齐的GNN设计框架。以可重构智能表面辅助混合预编码为例，通过仿真验证了所提出的GNN的学习效率。", "summary": "本文旨在探究注意力机制何时对无线资源分配的学习有益。作者通过分析集合上函数的结构和数值算法，证明了置换等变函数可递归表达为含注意力与不含注意力两种形式。研究发现，当干扰（如多用户或数据流间干扰）未直接反映在可测量参数中时，需引入注意力来建模。基于此洞察，论文提出了一个与结构对齐的图神经网络设计框架，并通过仿真验证了其在可重构智能表面辅助混合预编码场景下的学习效率。", "keywords": "无线资源分配, 注意力机制, 图神经网络, 置换等变性, 干扰建模", "comments": "本文的创新之处在于从理论层面深入探讨了注意力机制在无线资源分配中的必要性，并揭示了其在处理未明确体现在可测量参数中的干扰时的关键作用。这为未来设计高效且有理论依据的图神经网络模型提供了宝贵的指导和框架。"}}
{"id": "2507.02815", "title": "Towards Perception-Informed Latent HRTF Representations", "authors": ["You Zhang", "Andrew Francl", "Ruohan Gao", "Paul Calamia", "Zhiyao Duan", "Ishwarya Ananthabhotla"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE WASPAA 2025, camera-ready version", "url": "http://arxiv.org/abs/2507.02815v1", "summary": "Personalized head-related transfer functions (HRTFs) are essential for\nensuring a realistic auditory experience over headphones, because they take\ninto account individual anatomical differences that affect listening. Most\nmachine learning approaches to HRTF personalization rely on a learned\nlow-dimensional latent space to generate or select custom HRTFs for a listener.\nHowever, these latent representations are typically learned in a manner that\noptimizes for spectral reconstruction but not for perceptual compatibility,\nmeaning they may not necessarily align with perceptual distance. In this work,\nwe first study whether traditionally learned HRTF representations are well\ncorrelated with perceptual relations using auditory-based objective perceptual\nmetrics; we then propose a method for explicitly embedding HRTFs into a\nperception-informed latent space, leveraging a metric-based loss function and\nsupervision via Metric Multidimensional Scaling (MMDS). Finally, we demonstrate\nthe applicability of these learned representations to the task of HRTF\npersonalization. We suggest that our method has the potential to render\npersonalized spatial audio, leading to an improved listening experience.", "comment": "Accepted by IEEE WASPAA 2025, camera-ready version", "pdf_url": "http://arxiv.org/pdf/2507.02815v1", "cate": "eess.AS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "面向感知信息的潜在HRTF表示", "tldr": "本文研究传统HRTF表示是否与感知相关性良好，并提出一种将HRTF嵌入到感知信息潜在空间的方法，以改善个性化空间音频体验。", "motivation": "个性化头部相关传递函数（HRTF）对于耳机提供真实的听觉体验至关重要，因为它们考虑了个体解剖差异。然而，大多数机器学习方法在学习HRTF潜在表示时，优化的是光谱重建而非感知兼容性，导致它们可能不与感知距离对齐。", "method": "首先，使用基于听觉的客观感知指标研究传统学习的HRTF表示是否与感知关系良好相关。然后，提出一种通过基于度量的损失函数和度量多维尺度（MMDS）监督，将HRTF明确嵌入到感知信息潜在空间的方法。最后，将这些学习到的表示应用于HRTF个性化任务。", "result": "研究了传统HRTF表示与感知相关性的情况，并提出了一种新的感知信息潜在空间嵌入方法，并展示了其在HRTF个性化任务中的适用性。", "conclusion": "本文提出的方法有可能实现个性化空间音频，从而改善听觉体验。", "translation": "个性化头部相关传递函数（HRTFs）对于确保通过耳机获得真实的听觉体验至关重要，因为它们考虑了影响听力的个体解剖差异。大多数用于HRTF个性化的机器学习方法依赖于学习到的低维潜在空间来为听众生成或选择定制的HRTFs。然而，这些潜在表示通常以优化光谱重建而非感知兼容性的方式学习，这意味着它们可能不一定与感知距离对齐。在这项工作中，我们首先研究了传统学习的HRTF表示是否与使用基于听觉的客观感知指标的感知关系良好相关；然后，我们提出了一种通过利用基于度量的损失函数和通过度量多维尺度（MMDS）进行监督，将HRTFs明确嵌入到感知信息潜在空间的方法。最后，我们展示了这些学习到的表示在HRTF个性化任务中的适用性。我们认为我们的方法有潜力实现个性化空间音频，从而改善听觉体验。", "summary": "本文探讨了传统HRTF潜在表示在感知兼容性方面的不足，指出其通常优化光谱重建而非感知距离。研究首先评估了传统HRTF表示与感知关系的相关性，随后提出了一种新的方法，通过度量损失函数和MMDS监督，将HRTF嵌入到感知信息的潜在空间中。最终，该研究证明了这些感知信息表示在HRTF个性化任务中的有效性，并提出其有望提升个性化空间音频的听觉体验。", "keywords": "HRTF, 感知信息, 潜在表示, 个性化音频, 机器学习", "comments": "本文的创新点在于提出了一个面向感知的HRTF潜在表示学习方法，解决了传统方法侧重光谱重建而忽视感知兼容性的问题。通过引入感知相关的度量损失函数和MMDS监督，使得学习到的潜在空间能够更好地反映人耳的感知特性。这项工作对于提升个性化空间音频的真实感和沉浸感具有重要意义，为未来的听觉体验优化提供了新的方向。"}}
{"id": "2507.02430", "title": "A Late Collaborative Perception Framework for 3D Multi-Object and Multi-Source Association and Fusion", "authors": ["Maryem Fadili", "Mohamed Anis Ghaoui", "Louis Lecrosnier", "Steve Pechberti", "Redouane Khemmar"], "categories": ["cs.RO", "eess.IV", "eess.SP"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02430v1", "summary": "In autonomous driving, recent research has increasingly focused on\ncollaborative perception based on deep learning to overcome the limitations of\nindividual perception systems. Although these methods achieve high accuracy,\nthey rely on high communication bandwidth and require unrestricted access to\neach agent's object detection model architecture and parameters. These\nconstraints pose challenges real-world autonomous driving scenarios, where\ncommunication limitations and the need to safeguard proprietary models hinder\npractical implementation. To address this issue, we introduce a novel late\ncollaborative framework for 3D multi-source and multi-object fusion, which\noperates solely on shared 3D bounding box attributes-category, size, position,\nand orientation-without necessitating direct access to detection models. Our\nframework establishes a new state-of-the-art in late fusion, achieving up to\nfive times lower position error compared to existing methods. Additionally, it\nreduces scale error by a factor of 7.5 and orientation error by half, all while\nmaintaining perfect 100% precision and recall when fusing detections from\nheterogeneous perception systems. These results highlight the effectiveness of\nour approach in addressing real-world collaborative perception challenges,\nsetting a new benchmark for efficient and scalable multi-agent fusion.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02430v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "一种用于三维多目标和多源关联与融合的后期协作感知框架", "tldr": "针对自动驾驶中现有协作感知方法对高带宽和模型访问的限制，本文提出了一种新颖的后期协作感知框架，仅基于共享的三维边界框属性进行融合，显著提高了融合精度，同时解决了实际应用中的挑战。", "motivation": "在自动驾驶中，现有的基于深度学习的协作感知方法虽然精度高，但依赖高通信带宽并要求无限制地访问各代理的物体检测模型架构和参数。这些限制阻碍了在通信受限和需要保护专有模型的实际自动驾驶场景中的应用。", "method": "我们引入了一种新颖的用于三维多源和多目标融合的后期协作框架，它仅基于共享的三维边界框属性（类别、大小、位置和方向）进行操作，无需直接访问检测模型。", "result": "该框架在后期融合中达到了最先进的水平，与现有方法相比，位置误差降低了五倍，尺度误差降低了7.5倍，方向误差降低了一半，同时在融合来自异构感知系统的检测结果时保持了完美的100%精度和召回率。", "conclusion": "这些结果突出表明了我们方法在解决实际协作感知挑战方面的有效性，为高效和可扩展的多智能体融合设定了新的基准。", "translation": "在自动驾驶中，最近的研究越来越关注基于深度学习的协作感知，以克服个体感知系统的局限性。尽管这些方法实现了高精度，但它们依赖于高通信带宽，并要求无限制地访问每个代理的物体检测模型架构和参数。这些约束对实际的自动驾驶场景构成了挑战，在这些场景中，通信限制和保护专有模型的需要阻碍了实际实施。为了解决这个问题，我们引入了一种新颖的用于三维多源和多目标融合的后期协作框架，它仅基于共享的三维边界框属性——类别、大小、位置和方向——进行操作，而无需直接访问检测模型。我们的框架在后期融合中建立了新的最先进水平，与现有方法相比，位置误差降低了高达五倍。此外，它将尺度误差降低了7.5倍，方向误差降低了一半，同时在融合来自异构感知系统的检测结果时保持了完美的100%精度和召回率。这些结果突出表明了我们方法在解决实际协作感知挑战方面的有效性，为高效和可扩展的多智能体融合设定了新的基准。", "summary": "本文提出了一种新颖的后期协作感知框架，用于三维多目标和多源融合。该框架旨在克服现有协作感知方法对高带宽和模型访问的依赖，仅通过共享的三维边界框属性（类别、大小、位置和方向）进行操作。实验结果表明，该框架在后期融合中取得了显著的性能提升，与现有方法相比，位置误差降低了五倍，尺度误差降低了7.5倍，方向误差降低了一半，并且在融合异构感知系统时保持了100%的精度和召回率。这表明了该方法在解决实际协作感知挑战方面的有效性，并为高效可扩展的多智能体融合设定了新基准。", "keywords": "协作感知, 后期融合, 三维目标检测, 自动驾驶, 多智能体系统", "comments": "该论文的创新之处在于提出了一种后期协作感知框架，有效解决了自动驾驶中协作感知面临的实际瓶颈，即对高通信带宽和专有模型访问的需求。通过仅依赖三维边界框属性进行融合，该方法显著降低了系统复杂性和资源消耗，同时取得了显著的精度提升，尤其是在位置、尺度和方向误差方面。这使得其在实际部署中更具可行性和鲁棒性，为多智能体自动驾驶系统提供了重要的技术进步。"}}
{"id": "2507.02085", "title": "GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters", "authors": ["Wanjia Zhao", "Jiaqi Han", "Siyi Gu", "Mingjian Jiang", "James Zou", "Stefano Ermon"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02085v1", "summary": "Geometric diffusion models have shown remarkable success in molecular\ndynamics and structure generation. However, efficiently fine-tuning them for\ndownstream tasks with varying geometric controls remains underexplored. In this\nwork, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables\nflexible and parameter-efficient fine-tuning for controlled generative tasks\nwithout modifying the original model architecture. GeoAda introduces a\nstructured adapter design: control signals are first encoded through coupling\noperators, then processed by a trainable copy of selected pretrained model\nlayers, and finally projected back via decoupling operators followed by an\nequivariant zero-initialized convolution. By fine-tuning only these lightweight\nadapter modules, GeoAda preserves the model's geometric consistency while\nmitigating overfitting and catastrophic forgetting. We theoretically prove that\nthe proposed adapters maintain SE(3)-equivariance, ensuring that the geometric\ninductive biases of the pretrained diffusion model remain intact during\nadaptation. We demonstrate the wide applicability of GeoAda across diverse\ngeometric control types, including frame control, global control, subgraph\ncontrol, and a broad range of application domains such as particle dynamics,\nmolecular dynamics, human motion prediction, and molecule generation. Empirical\nresults show that GeoAda achieves state-of-the-art fine-tuning performance\nwhile preserving original task accuracy, whereas other baselines experience\nsignificant performance degradation due to overfitting and catastrophic\nforgetting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02085v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "GeoAda：使用等变适配器高效微调几何扩散模型", "tldr": "GeoAda是一种高效微调几何扩散模型的方法，通过引入等变适配器，在不修改原模型架构的情况下实现受控生成任务的灵活且参数高效的微调，同时保持几何一致性并避免过拟合。", "motivation": "几何扩散模型在分子动力学和结构生成方面表现出色，但针对下游任务进行高效微调，尤其是在不同几何控制下的微调，仍未得到充分探索。现有方法可能面临过拟合和灾难性遗忘问题。", "method": "本文提出了一种SE(3)等变适配器框架（GeoAda），用于几何扩散模型的高效微调。GeoAda引入了结构化适配器设计：控制信号首先通过耦合操作符编码，然后由选定的预训练模型层的可训练副本处理，最后通过解耦操作符和等变零初始化卷积投射回去。通过仅微调这些轻量级适配器模块，GeoAda在减轻过拟合和灾难性遗忘的同时，保留了模型的几何一致性。理论上证明了适配器保持SE(3)等变性。", "result": "GeoAda在多种几何控制类型（包括帧控制、全局控制、子图控制）和广泛应用领域（如粒子动力学、分子动力学、人体运动预测和分子生成）中表现出广泛适用性。实证结果表明，GeoAda实现了最先进的微调性能，同时保持了原始任务的准确性，而其他基线则因过拟合和灾难性遗忘导致性能显著下降。", "conclusion": "GeoAda提供了一种高效、灵活且参数高效的几何扩散模型微调方法，能够在保持模型几何一致性、避免过拟合和灾难性遗忘的同时，实现受控的生成任务，并在多个应用领域取得最先进的性能。", "translation": "几何扩散模型在分子动力学和结构生成方面取得了显著成功。然而，针对具有不同几何控制的下游任务进行高效微调仍未得到充分探索。在这项工作中，我们提出了一种SE(3)等变适配器框架（GeoAda），该框架能够在不修改原始模型架构的情况下，实现受控生成任务的灵活且参数高效的微调。GeoAda引入了一种结构化的适配器设计：控制信号首先通过耦合操作符编码，然后由选定的预训练模型层的可训练副本处理，最后通过解耦操作符和等变零初始化卷积投射回去。通过仅微调这些轻量级适配器模块，GeoAda在减轻过拟合和灾难性遗忘的同时，保留了模型的几何一致性。我们从理论上证明了所提出的适配器保持SE(3)等变性，确保了预训练扩散模型的几何归纳偏置在适应过程中保持完整。我们展示了GeoAda在各种几何控制类型（包括帧控制、全局控制、子图控制）和广泛应用领域（如粒子动力学、分子动力学、人体运动预测和分子生成）中的广泛适用性。实证结果表明，GeoAda实现了最先进的微调性能，同时保持了原始任务的准确性，而其他基线则因过拟合和灾难性遗忘而导致性能显著下降。", "summary": "本文提出了GeoAda，一个SE(3)等变适配器框架，用于高效微调几何扩散模型以适应受控生成任务。GeoAda通过引入轻量级、结构化的适配器模块，在不修改原有模型架构的情况下，实现了参数高效的微调，并理论证明了其保持SE(3)等变性。实验证明，GeoAda在多种几何控制类型和应用领域中表现出优越的微调性能，有效避免了过拟合和灾难性遗忘，同时保持了原始任务精度。", "keywords": "几何扩散模型, 等变适配器, 微调, SE(3)不变性, 受控生成", "comments": "这篇论文的创新点在于提出了一个SE(3)等变适配器框架GeoAda，它在保持模型几何一致性和避免灾难性遗忘的前提下，实现了几何扩散模型的高效参数微调。这种方法对于将强大的几何扩散模型应用于更广泛的受控生成任务具有重要意义，尤其是在需要精细几何控制的领域。其理论证明和广泛的实验验证也增强了该方法的可靠性和实用性。"}}
{"id": "2507.02643", "title": "Calibrated Recommendations: Survey and Future Directions", "authors": ["Diego Corrêa da Silva", "Dietmar Jannach"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02643v1", "summary": "The idea of calibrated recommendations is that the properties of the items\nthat are suggested to users should match the distribution of their individual\npast preferences. Calibration techniques are therefore helpful to ensure that\nthe recommendations provided to a user are not limited to a certain subset of\nthe user's interests. Over the past few years, we have observed an increasing\nnumber of research works that use calibration for different purposes, including\nquestions of diversity, biases, and fairness. In this work, we provide a survey\non the recent developments in the area of calibrated recommendations. We both\nreview existing technical approaches for calibration and provide an overview on\nempirical and analytical studies on the effectiveness of calibration for\ndifferent use cases. Furthermore, we discuss limitations and common challenges\nwhen implementing calibration in practice.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02643v1", "cate": "cs.IR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "校准推荐：综述与未来方向", "tldr": "该论文综述了校准推荐领域的最新发展，包括技术方法、有效性研究、局限性和未来方向。", "motivation": "确保推荐结果不局限于用户兴趣的某个子集，解决多样性、偏见和公平性等问题，并对日益增多的校准推荐研究进行系统性回顾。", "method": "对校准推荐领域的最新发展进行综述，回顾了现有的技术方法，概述了校准有效性的实证和分析研究，并讨论了实践中实施校准的局限性和常见挑战。", "result": "提供了校准推荐领域最新发展的全面综述，涵盖了现有技术方法、有效性研究以及实施中的挑战和局限性。", "conclusion": "论文对校准推荐领域进行了全面的回顾，为理解现有进展、挑战和未来研究方向提供了基础。", "translation": "校准推荐的理念是，向用户推荐的物品属性应与他们过去个人偏好的分布相匹配。因此，校准技术有助于确保向用户提供的推荐不局限于用户兴趣的某个子集。在过去几年中，我们观察到越来越多的研究工作将校准用于不同目的，包括多样性、偏见和公平性问题。在这项工作中，我们对校准推荐领域的最新发展进行了综述。我们既回顾了现有的校准技术方法，也概述了校准在不同用例中有效性的实证和分析研究。此外，我们还讨论了在实践中实施校准时的局限性和常见挑战。", "summary": "本文对校准推荐领域进行了全面综述，旨在解决推荐系统可能存在的兴趣范围狭窄问题。论文回顾了现有技术方法，概述了校准有效性的实证和分析研究，并探讨了在实践中实施校准的局限性和挑战。", "keywords": "校准推荐, 推荐系统, 综述, 多样性, 公平性", "comments": "这篇综述文章对于理解校准推荐这一重要且日益增长的研究领域具有重要意义。它系统地梳理了现有技术和研究，并指出了实践中的挑战，为未来的研究指明了方向。其价值在于为研究人员和实践者提供了一个全面的参考框架。"}}
{"id": "2507.02728", "title": "Indexing Tries within Entropy-Bounded Space", "authors": ["Lorenzo Carfagna", "Carlo Tosoni"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      14 pages, 1 figure, submitted to SPIRE 2025", "url": "http://arxiv.org/abs/2507.02728v1", "summary": "We study the problem of indexing and compressing tries using a BWT-based\napproach. Specifically, we consider a succinct and compressed representation of\nthe XBWT of Ferragina et al.\\ [FOCS '05, JACM '09] corresponding to the\nanalogous of the FM-index [FOCS '00, JACM '05] for tries. This representation\nallows to efficiently count the number of nodes reached by a given string\npattern. To analyze the space complexity of the above trie index, we propose a\nproof for the combinatorial problem of counting the number of tries with a\ngiven symbol distribution. We use this formula to define a worst-case entropy\nmeasure for tries, as well as a notion of k-th order empirical entropy. In\nparticular, we show that the relationships between these two entropy measures\nare similar to those between the corresponding well-known measures for strings.\nWe use these measures to prove that the XBWT of a trie can be encoded within a\nspace bounded by our k-th order empirical entropy plus a o(n) term, with n\nbeing the number of nodes in the trie. Notably, as happens for strings, this\nspace bound can be reached for every sufficiently small k simultaneously.\nFinally, we compare the space complexity of the above index with that of the\nr-index for tries proposed by Prezza [SODA '21] and we prove that in some cases\nthe FM-index for tries is asymptotically smaller.", "comment": "14 pages, 1 figure, submitted to SPIRE 2025", "pdf_url": "http://arxiv.org/pdf/2507.02728v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "在熵有界空间中索引Trie树", "tldr": "研究了一种基于BWT的Trie树索引和压缩方法，提出了新的熵度量，并证明其空间复杂度由k阶经验熵界定，且在某些情况下优于现有索引。", "motivation": "解决Trie树的索引和压缩问题。", "method": "1. 采用基于BWT的方法，特别是Ferragina等人提出的XBWT的简洁压缩表示，对应于Trie树的FM-index。2. 为了分析空间复杂度，提出了一个计算给定符号分布的Trie树数量的组合学问题的证明。3. 利用该公式定义了Trie树的最坏情况熵度量和k阶经验熵概念。", "result": "1. 所提出的表示方法可以有效地计算给定字符串模式所到达的节点数量。2. 证明了这两种熵度量之间的关系与字符串对应的度量相似。3. 证明了Trie树的XBWT可以在其k阶经验熵加上一个o(n)项（n为Trie树节点数）的空间内编码。4. 该空间界限可以同时为每个足够小的k实现。5. 与Prezza提出的r-index进行比较，证明在某些情况下，Trie树的FM-index渐近更小。", "conclusion": "论文成功地为Trie树设计了一种高效的索引和压缩方法，证明了其空间效率，并提出了新的熵度量，显示了其在某些情况下优于现有技术。", "translation": "我们研究了使用基于BWT的方法对Trie树进行索引和压缩的问题。具体来说，我们考虑了Ferragina等人[FOCS '05, JACM '09]的XBWT的一种简洁压缩表示，它对应于Trie树的FM-index [FOCS '00, JACM '05]。这种表示允许高效地计算给定字符串模式所到达的节点数量。为了分析上述Trie树索引的空间复杂度，我们为计算具有给定符号分布的Trie树数量的组合学问题提出了一个证明。我们使用这个公式来定义Trie树的最坏情况熵度量，以及k阶经验熵的概念。特别是，我们表明这两种熵度量之间的关系与字符串对应的众所周知的度量之间的关系相似。我们使用这些度量来证明Trie树的XBWT可以在我们的k阶经验熵加上一个o(n)项（其中n是Trie树中的节点数）的空间内编码。值得注意的是，与字符串一样，这个空间界限可以同时为每个足够小的k达到。最后，我们将上述索引的空间复杂度与Prezza [SODA '21]提出的Trie树的r-index进行了比较，并证明在某些情况下，Trie树的FM-index渐近更小。", "summary": "本文研究了基于BWT的Trie树索引和压缩问题，提出了一种简洁压缩的Trie树XBWT（即Trie树的FM-index）表示方法。为分析其空间复杂度，论文提出了计算具有给定符号分布的Trie树数量的组合学证明，并基于此定义了Trie树的最坏情况熵和k阶经验熵。研究表明，这些熵度量与字符串的对应度量关系相似。核心结果是证明了Trie树的XBWT可以在其k阶经验熵加上一个亚线性项的空间内编码，且该界限对足够小的k同时可达。此外，论文还通过比较证明了在某些情况下，该Trie树的FM-index在空间上渐近优于现有的r-index。", "keywords": "Trie树, 索引, 压缩, BWT, FM-index, 熵, 空间复杂度", "comments": "这篇论文在数据结构和算法领域，特别是针对Trie树的压缩和索引方面做出了重要贡献。其创新点在于将BWT和FM-index的思想推广到Trie树，并引入了新的Trie树熵度量，为分析其空间效率提供了理论基础。证明了空间复杂度与k阶经验熵相关，且在某些情况下优于现有技术，这对于处理大规模Trie树的应用具有重要意义。"}}
{"id": "2507.02364", "title": "QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers", "authors": ["Pilsung Kang"], "categories": ["cs.CL", "quant-ph"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02364v1", "summary": "Parameterized quantum circuits (PQCs) have recently emerged as promising\ncomponents for enhancing the expressibility of neural architectures. In this\nwork, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the\nfeedforward network (FFN) modules of a compact BERT variant are replaced by\nPQC-based layers. This design is motivated by the dominant parameter\ncontribution of FFNs, which account for approximately two-thirds of the\nparameters within standard Transformer encoder blocks. While prior studies have\nprimarily integrated PQCs into self-attention modules, our work focuses on the\nFFN and systematically investigates the trade-offs between PQC depth,\nexpressibility, and trainability. Our final PQC architecture incorporates a\nresidual connection, both $R_Y$ and $R_Z$ rotations, and an alternating\nentanglement strategy to ensure stable training and high expressibility. Our\nexperiments, conducted on a classical simulator, on the SST-2 and DBpedia\nbenchmarks demonstrate two key findings. First, a carefully configured\nQFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its\nclassical counterpart in a full-data setting while reducing FFN-specific\nparameters by over 99%. Second, our model exhibits a consistent and competitive\nedge in few-shot learning scenarios, confirming its potential for superior data\nefficiency. These results, supported by an ablation study on a non-optimized\nPQC that failed to learn, confirm that PQCs can serve as powerful and\nparameter-efficient alternatives to classical FFNs when co-designed with\nfoundational deep learning principles.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02364v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "QFFN-BERT：混合量子-经典Transformer中深度、性能和数据效率的实证研究", "tldr": "QFFN-BERT将BERT的FFN模块替换为参数化量子电路（PQC），在保持甚至超越经典性能的同时大幅减少参数，并在少样本学习中表现出卓越的数据效率。", "motivation": "参数化量子电路（PQCs）有望增强神经网络的表达能力。以往研究主要将PQCs集成到自注意力模块，而本文关注参数贡献最大的前馈网络（FFN），旨在系统研究PQC深度、表达能力和可训练性之间的权衡。", "method": "本文引入了QFFN-BERT，这是一种混合量子-经典Transformer，其中紧凑BERT变体的FFN模块被基于PQC的层取代。PQC架构包含残差连接、$R_Y$和$R_Z$旋转，以及交替纠缠策略，以确保稳定训练和高表达能力。实验在经典模拟器上进行，并在SST-2和DBpedia基准上进行评估。", "result": "精心配置的QFFN-BERT在全数据设置下达到基线精度的102.0%，超越了其经典对应物，同时将FFN特定参数减少了99%以上。此外，该模型在少样本学习场景中表现出持续且有竞争力的优势，证实了其卓越数据效率的潜力。消融研究进一步证实，当与基础深度学习原理共同设计时，PQCs可以作为经典FFN的强大且参数高效的替代方案。", "conclusion": "当与基础深度学习原理共同设计时，参数化量子电路（PQCs）可以作为经典前馈网络（FFNs）强大且参数高效的替代方案。", "translation": "参数化量子电路（PQCs）最近已成为增强神经网络架构表达能力的 有前景的组件。在这项工作中，我们引入了QFFN-BERT，这是一种混合量子-经典Transformer，其中紧凑BERT变体的全连接网络（FFN）模块被基于PQC的层取代。这种设计是由FFN占主导地位的参数贡献所驱动的，FFN在标准Transformer编码器块中约占三分之二的参数。虽然之前的研究主要将PQCs集成到自注意力模块中，但我们的工作侧重于FFN，并系统地研究了PQC深度、表达能力和可训练性之间的权衡。我们最终的PQC架构结合了残差连接、$R_Y$和$R_Z$旋转，以及交替纠缠策略，以确保稳定的训练和高表达能力。我们的实验在经典模拟器上进行，在SST-2和DBpedia基准上，证明了两个关键发现。首先，精心配置的QFFN-BERT实现了高达基线精度102.0%的性能，在全数据设置下超越了其经典对应物，同时将FFN特定参数减少了99%以上。其次，我们的模型在少样本学习场景中表现出持续且有竞争力的优势，证实了其卓越数据效率的潜力。这些结果，得到了对未能学习的非优化PQC进行消融研究的支持，证实了当与基础深度学习原理共同设计时，PQCs可以作为经典FFN强大且参数高效的替代方案。", "summary": "本文介绍了QFFN-BERT，一种混合量子-经典Transformer模型，通过将BERT中参数占主导地位的前馈网络（FFN）替换为参数化量子电路（PQC）层来探索量子增强神经网络。该研究系统地分析了PQC深度、表达能力和可训练性之间的权衡，并设计了一种结合残差连接、特定旋转门和交替纠缠的PQC架构。实验结果表明，QFFN-BERT在全数据设置下性能优于经典BERT，同时显著减少了FFN参数。此外，该模型在少样本学习中展现出卓越的数据效率。研究证实，在与深度学习原理结合设计时，PQC能有效替代传统FFN，实现参数高效且高性能的混合模型。", "keywords": "混合量子-经典Transformer, 参数化量子电路, FFN, BERT, 数据效率", "comments": "这项工作创新性地将参数化量子电路（PQCs）应用于Transformer模型中的前馈网络（FFN）模块，而非传统上关注的自注意力机制，这对于探索混合量子-经典模型的不同集成点具有重要意义。其核心贡献在于证明了PQC在大幅减少模型参数的同时，能够保持甚至超越经典模型的性能，尤其在数据效率方面展现出巨大潜力，这对于资源受限或少样本学习场景的应用前景广阔。"}}
{"id": "2507.02080", "title": "TAGF: Time-aware Gated Fusion for Multimodal Valence-Arousal Estimation", "authors": ["Yubeen Lee", "Sangeun Lee", "Chaewon Park", "Junyeop Cha", "Eunil Park"], "categories": ["cs.MM", "cs.SD"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "Comments:      9 pages, 2 figures, 2 tables", "url": "http://arxiv.org/abs/2507.02080v1", "summary": "Multimodal emotion recognition often suffers from performance degradation in\nvalence-arousal estimation due to noise and misalignment between audio and\nvisual modalities. To address this challenge, we introduce TAGF, a Time-aware\nGated Fusion framework for multimodal emotion recognition. The TAGF adaptively\nmodulates the contribution of recursive attention outputs based on temporal\ndynamics. Specifically, the TAGF incorporates a BiLSTM-based temporal gating\nmechanism to learn the relative importance of each recursive step and\neffectively integrates multistep cross-modal features. By embedding temporal\nawareness into the recursive fusion process, the TAGF effectively captures the\nsequential evolution of emotional expressions and the complex interplay between\nmodalities. Experimental results on the Aff-Wild2 dataset demonstrate that TAGF\nachieves competitive performance compared with existing recursive\nattention-based models. Furthermore, TAGF exhibits strong robustness to\ncross-modal misalignment and reliably models dynamic emotional transitions in\nreal-world conditions.", "comment": "9 pages, 2 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.02080v1", "cate": "cs.MM", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "TAGF：用于多模态效价-唤醒估计的时间感知门控融合", "tldr": "TAGF是一个时间感知的门控融合框架，用于解决多模态情感识别中效价-唤醒估计的噪声和对齐问题。", "motivation": "多模态情感识别中，由于音频和视觉模态之间的噪声和错位，效价-唤醒估计性能下降。", "method": "本文引入了TAGF（时间感知门控融合）框架。TAGF通过基于BiLSTM的时间门控机制，自适应地调节递归注意力输出的贡献，学习每个递归步骤的相对重要性，并有效整合多步跨模态特征，将时间感知嵌入到递归融合过程中。", "result": "在Aff-Wild2数据集上的实验结果表明，TAGF与现有基于递归注意力的模型相比，实现了有竞争力的性能。此外，TAGF对跨模态错位表现出强大的鲁棒性，并能可靠地建模真实世界条件下的动态情感转换。", "conclusion": "TAGF通过其时间感知门控融合机制，有效解决了多模态情感识别中的噪声和错位问题，提高了效价-唤醒估计的准确性和鲁棒性。", "translation": "多模态情感识别在效价-唤醒估计中，常因音频和视觉模态间的噪声和错位而导致性能下降。为解决这一挑战，我们引入了TAGF，一个用于多模态情感识别的时间感知门控融合框架。TAGF基于时间动态自适应地调节递归注意力输出的贡献。具体而言，TAGF结合了一个基于BiLSTM的时间门控机制，以学习每个递归步骤的相对重要性并有效地整合多步跨模态特征。通过将时间感知嵌入到递归融合过程中，TAGF有效地捕捉了情感表达的序列演变以及模态之间复杂的相互作用。在Aff-Wild2数据集上的实验结果表明，与现有基于递归注意力的模型相比，TAGF实现了有竞争力的性能。此外，TAGF对跨模态错位表现出强大的鲁棒性，并能可靠地建模真实世界条件下的动态情感转换。", "summary": "本文提出了一种名为TAGF的时间感知门控融合框架，旨在解决多模态情感识别中效价-唤醒估计因模态噪声和错位导致的性能下降问题。TAGF利用基于BiLSTM的时间门控机制，自适应地融合递归注意力输出和跨模态特征，有效捕捉情感的时间演变和模态间相互作用。实验证明，TAGF在性能上具有竞争力，并对跨模态错位表现出高鲁棒性。", "keywords": "多模态情感识别, 效价-唤醒估计, 时间感知, 门控融合, 递归注意力", "comments": "TAGF的创新之处在于引入了时间感知门控机制，这使得模型能够更精细地处理多模态数据中的时序信息和模态间动态关系，尤其是在存在噪声和错位的情况下。这对于提升真实世界情感识别系统的鲁棒性和准确性具有重要意义。"}}
{"id": "2507.02489", "title": "A 10-bit S-box generated by Feistel construction from cellular automata", "authors": ["Thomas Prévost", "Bruno Martin"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02489v1", "summary": "We propose a new 10-bit S-box generated from a Feistel construction. The\nsubpermutations are generated by a 5-cell cellular automaton based on a unique\nwell-chosen rule and bijective affine transformations. In particular, the\ncellular automaton rule is chosen based on empirical tests of its ability to\ngenerate good pseudorandom output on a ring cellular automaton. Similarly,\nFeistel's network layout is based on empirical data regarding the quality of\nthe output S-box. We perform cryptanalysis of the generated 10-bit S-box, and\nwe find security properties comparable to or sometimes even better than those\nof the standard AES S-box. We believe that our S-box could be used to replace\nthe 5-bit substitution of ciphers like ASCON.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02489v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于元胞自动机和Feistel结构的10比特S盒", "tldr": "本文提出了一种新的10比特S盒，它结合了Feistel结构和基于特定规则的元胞自动机。通过密码分析，该S盒的安全性能与AES S盒相当或更优。", "motivation": "本文旨在提出一种新的10比特S盒，该S盒通过结合Feistel结构和元胞自动机生成，并期望其具有良好的安全属性。", "method": "本文提出了一种新的10比特S盒，其通过Feistel结构生成。其中的子置换由基于独特选择规则的5单元元胞自动机和双射仿射变换生成。元胞自动机规则的选择基于其在环形元胞自动机上生成良好伪随机输出的经验测试。Feistel网络的布局也基于关于输出S盒质量的经验数据。", "result": "对生成的10比特S盒进行了密码分析，发现其安全属性与标准AES S盒相当甚至有时更好。", "conclusion": "作者认为所提出的S盒可以用于替代ASCON等密码中的5比特替换。", "translation": "我们提出了一种由Feistel结构生成的新型10比特S盒。子置换由基于独特选择规则和双射仿射变换的5单元元胞自动机生成。特别是，元胞自动机规则的选择是基于其在环形元胞自动机上生成良好伪随机输出能力的经验测试。类似地，Feistel网络布局是基于关于输出S盒质量的经验数据。我们对生成的10比特S盒进行了密码分析，发现其安全属性与标准AES S盒相当，甚至有时更好。我们相信我们的S盒可以用于替代ASCON等密码中的5比特替换。", "summary": "本文提出了一种基于Feistel结构和元胞自动机的新型10比特S盒。该S盒的子置换由一个5单元元胞自动机和双射仿射变换生成，其中元胞自动机规则和Feistel网络布局均基于经验测试优化。密码分析结果表明，该S盒的安全性能与标准AES S盒相当或更优，有望替代现有密码中的5比特替换。", "keywords": "S盒, Feistel结构, 元胞自动机, 密码分析, 安全属性", "comments": "本文的创新之处在于结合了Feistel结构和元胞自动机来生成S盒，并强调了基于经验测试选择规则和网络布局的重要性。其生成的S盒表现出与AES S盒相当或更优的安全性能，这对于密码学领域具有重要意义，可能为未来设计更安全的密码算法提供新的思路。"}}
{"id": "2507.02533", "title": "Meta-Fair: AI-Assisted Fairness Testing of Large Language Models", "authors": ["Miguel Romero-Arjona", "José A. Parejo", "Juan C. Alonso", "Ana B. Sánchez", "Aitor Arrieta", "Sergio Segura"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02533v1", "summary": "Fairness--the absence of unjustified bias--is a core principle in the\ndevelopment of Artificial Intelligence (AI) systems, yet it remains difficult\nto assess and enforce. Current approaches to fairness testing in large language\nmodels (LLMs) often rely on manual evaluation, fixed templates, deterministic\nheuristics, and curated datasets, making them resource-intensive and difficult\nto scale. This work aims to lay the groundwork for a novel, automated method\nfor testing fairness in LLMs, reducing the dependence on domain-specific\nresources and broadening the applicability of current approaches. Our approach,\nMeta-Fair, is based on two key ideas. First, we adopt metamorphic testing to\nuncover bias by examining how model outputs vary in response to controlled\nmodifications of input prompts, defined by metamorphic relations (MRs). Second,\nwe propose exploiting the potential of LLMs for both test case generation and\noutput evaluation, leveraging their capability to generate diverse inputs and\nclassify outputs effectively. The proposal is complemented by three open-source\ntools supporting LLM-driven generation, execution, and evaluation of test\ncases. We report the findings of several experiments involving 12 pre-trained\nLLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.\nThe results show that Meta-Fair is effective in uncovering bias in LLMs,\nachieving an average precision of 92% and revealing biased behaviour in 29% of\nexecutions. Additionally, LLMs prove to be reliable and consistent evaluators,\nwith the best-performing models achieving F1-scores of up to 0.79. Although\nnon-determinism affects consistency, these effects can be mitigated through\ncareful MR design. While challenges remain to ensure broader applicability, the\nresults indicate a promising path towards an unprecedented level of automation\nin LLM testing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02533v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Meta-Fair：AI辅助的大型语言模型公平性测试", "tldr": "本文提出Meta-Fair，一种新颖的自动化方法，利用变质测试和大型语言模型自身的能力来生成和评估测试用例，有效揭示了大型语言模型中的偏见。", "motivation": "当前大型语言模型（LLMs）的公平性测试方法（如手动评估、固定模板、启发式方法和人工数据集）资源密集且难以扩展，难以有效评估和执行公平性原则。", "method": "Meta-Fair方法基于两个核心思想：1. 采用变质测试（metamorphic testing），通过检查模型输出如何响应受控输入提示修改（由变质关系定义）来揭示偏见。2. 利用LLMs自身的能力进行测试用例生成和输出评估，以生成多样化输入和有效分类输出。该方法还辅以三个开源工具。", "result": "实验表明，Meta-Fair能有效揭示LLMs中的偏见，平均准确率达到92%，在29%的执行中揭示了偏见行为。LLMs被证明是可靠且一致的评估器，表现最佳的模型F1分数高达0.79。非确定性虽影响一致性，但可通过精心设计的变质关系缓解。", "conclusion": "尽管存在挑战，但结果表明，Meta-Fair为实现LLM测试前所未有的自动化水平提供了一条有前景的路径。", "translation": "公平性——即没有不合理的偏见——是人工智能（AI）系统开发中的核心原则，但其评估和执行仍然困难。目前大型语言模型（LLM）中的公平性测试方法通常依赖于手动评估、固定模板、确定性启发式方法和精心策划的数据集，这使得它们资源密集且难以扩展。这项工作旨在为LLM中公平性测试的新型自动化方法奠定基础，减少对领域特定资源的依赖，并拓宽现有方法的适用性。我们的方法Meta-Fair基于两个关键思想。首先，我们采用变质测试（metamorphic testing）来揭示偏见，通过检查模型输出如何响应输入提示的受控修改（由变质关系定义）来发现偏见。其次，我们建议利用LLM在测试用例生成和输出评估方面的潜力，发挥它们生成多样化输入和有效分类输出的能力。该提议还辅以三个支持LLM驱动的测试用例生成、执行和评估的开源工具。我们报告了涉及12个预训练LLM、14个变质关系、5个偏见维度和7.9K个自动生成测试用例的几项实验结果。结果表明，Meta-Fair在揭示LLM中的偏见方面是有效的，平均准确率达到92%，并在29%的执行中揭示了偏见行为。此外，LLM被证明是可靠且一致的评估器，表现最佳的模型F1分数高达0.79。虽然非确定性会影响一致性，但这些影响可以通过精心设计的变质关系来缓解。尽管确保更广泛适用性仍面临挑战，但结果表明，这是通向LLM测试前所未有的自动化水平的一条有前景的道路。", "summary": "本文提出了一种名为Meta-Fair的新型自动化方法，用于大型语言模型（LLMs）的公平性测试。针对现有手动和资源密集型测试方法的局限性，Meta-Fair结合了变质测试和LLM作为测试用例生成器及输出评估器的能力。通过在12个LLM上进行实验，该方法有效揭示了偏见（平均精度92%，29%的执行中发现偏见），并证明LLM作为评估器是可靠的。研究指出，尽管非确定性带来挑战，但Meta-Fair为实现LLM公平性测试的自动化提供了有前景的方向。", "keywords": "大型语言模型, 公平性测试, 自动化, 变质测试, 偏见检测", "comments": "Meta-Fair的创新之处在于其结合了变质测试和LLM的自举能力（即用LLM生成和评估LLM测试），大大提高了公平性测试的自动化和可扩展性，减少了对人工和特定领域资源的依赖。这对于推动LLM的负责任开发具有重要意义。"}}
{"id": "2507.02400", "title": "DigiT4TAF -- Bridging Physical and Digital Worlds for Future Transportation Systems", "authors": ["Maximilian Zipfl", "Pascal Zwick", "Patrick Schulz", "Marc Rene Zofka", "Albert Schotschneider", "Helen Gremmelmaier", "Nikolai Polley", "Ferdinand Mütsch", "Kevin Simon", "Fabian Gottselig", "Michael Frey", "Sergio Marschall", "Akim Stark", "Maximilian Müller", "Marek Wehmer", "Mihai Kocsis", "Dominic Waldenmayer", "Florian Schnepf", "Erik Heinrich", "Sabrina Pletz", "Matthias Kölle", "Karin Langbein-Euchner", "Alexander Viehl", "Raoul Zöllner", "J. Marius Zöllner"], "categories": ["cs.RO", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted at the IEEE IAVVC 2025 Conference", "url": "http://arxiv.org/abs/2507.02400v1", "summary": "In the future, mobility will be strongly shaped by the increasing use of\ndigitalization. Not only will individual road users be highly interconnected,\nbut also the road and associated infrastructure. At that point, a Digital Twin\nbecomes particularly appealing because, unlike a basic simulation, it offers a\ncontinuous, bilateral connection linking the real and virtual environments.\nThis paper describes the digital reconstruction used to develop the Digital\nTwin of the Test Area Autonomous Driving-Baden-W\\\"urttemberg (TAF-BW), Germany.\nThe TAF-BW offers a variety of different road sections, from high-traffic urban\nintersections and tunnels to multilane motorways. The test area is equipped\nwith a comprehensive Vehicle-to-Everything (V2X) communication infrastructure\nand multiple intelligent intersections equipped with camera sensors to\nfacilitate real-time traffic flow monitoring. The generation of authentic data\nas input for the Digital Twin was achieved by extracting object lists at the\nintersections. This process was facilitated by the combined utilization of\ncamera images from the intelligent infrastructure and LiDAR sensors mounted on\na test vehicle. Using a unified interface, recordings from real-world\ndetections of traffic participants can be resimulated. Additionally, the\nsimulation framework's design and the reconstruction process is discussed. The\nresulting framework is made publicly available for download and utilization at:\nhttps://digit4taf-bw.fzi.de The demonstration uses two case studies to\nillustrate the application of the digital twin and its interfaces: the analysis\nof traffic signal systems to optimize traffic flow and the simulation of\nsecurity-related scenarios in the communications sector.", "comment": "Accepted at the IEEE IAVVC 2025 Conference", "pdf_url": "http://arxiv.org/pdf/2507.02400v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DigiT4TAF -- 连接物理与数字世界，赋能未来交通系统", "tldr": "本文介绍了DigiT4TAF项目，旨在为德国巴登-符腾堡州自动驾驶测试区（TAF-BW）开发一个数字孪生，以连接现实和虚拟交通环境，并通过真实数据生成和案例研究展示其在未来交通系统中的应用。", "motivation": "未来的出行将受到数字化日益增长的强烈影响，不仅个体道路使用者将高度互联，道路及相关基础设施也将如此。数字孪生因其与现实和虚拟环境之间连续、双向的连接而特别具有吸引力，优于基础模拟。本文旨在通过构建数字孪生来弥合物理世界与数字世界之间的鸿沟，以支持未来交通系统。", "method": "本文描述了用于开发巴登-符腾堡州自动驾驶测试区（TAF-BW）数字孪生的数字重建过程。通过结合利用智能基础设施的摄像头图像和测试车辆上安装的LiDAR传感器，在交叉路口提取物体列表，从而生成作为数字孪生输入的真实数据。此外，还讨论了仿真框架的设计和重建过程。", "result": "成功开发了巴登-符腾堡州自动驾驶测试区（TAF-BW）的数字孪生系统。实现了真实交通参与者实时检测数据的统一接口重模拟。所开发的框架已公开提供下载和使用。通过分析交通信号系统以优化交通流量和模拟通信领域的安全相关场景这两个案例研究，展示了数字孪生及其接口的应用。", "conclusion": "本文成功开发并展示了DigiT4TAF数字孪生框架，它通过利用真实世界数据对一个综合测试区进行数字重建，实现了物理世界与数字世界的连接，为未来交通系统提供了强大的工具。该框架可用于交通优化和安全分析等多种应用，并已公开可用。", "translation": "在未来，出行将受到数字化日益增长的强烈影响。不仅个体道路使用者将高度互联，道路及相关基础设施也将如此。届时，数字孪生将变得特别有吸引力，因为它与基础模拟不同，提供了一个连接真实和虚拟环境的连续、双向连接。本文描述了用于开发德国巴登-符腾堡州自动驾驶测试区（TAF-BW）数字孪生的数字重建过程。TAF-BW提供了各种不同的路段，从交通繁忙的城市交叉路口和隧道到多车道高速公路。该测试区配备了全面的车联网（V2X）通信基础设施和多个配备摄像头传感器的智能交叉路口，以促进实时交通流量监测。通过在交叉路口提取物体列表，结合利用智能基础设施的摄像头图像和测试车辆上安装的LiDAR传感器，实现了数字孪生输入真实数据的生成。使用统一接口，可以重新模拟对交通参与者的真实世界检测记录。此外，还讨论了仿真框架的设计和重建过程。所生成的框架已公开提供下载和使用，网址为：https://digit4taf-bw.fzi.de。该演示使用两个案例研究来说明数字孪生及其接口的应用：分析交通信号系统以优化交通流量，以及模拟通信领域的安全相关场景。", "summary": "本文介绍了DigiT4TAF项目，旨在为德国巴登-符腾堡州自动驾驶测试区（TAF-BW）开发一个数字孪生。针对未来出行中日益增长的数字化需求，该数字孪生提供了一个连接现实与虚拟环境的连续、双向连接。其方法涉及数字重建，并通过结合智能基础设施摄像头图像和测试车辆LiDAR传感器，从交叉路口提取物体列表来生成真实的交通数据。论文还讨论了仿真框架的设计和重建过程。所开发的框架已公开可用，并通过交通流优化和安全场景模拟两个案例研究，展示了其在弥合物理与数字交通系统方面的实用性。", "keywords": "数字孪生, 交通系统, 自动驾驶, V2X, 交通模拟", "comments": "该论文通过为未来交通系统开发一个实用的数字孪生，迈出了实现智能交通系统的重要一步。其创新之处在于利用装备完善的TAF-BW测试区的真实世界传感器数据，创建了一个真实的数字副本，提供了持续的反馈循环。该框架的公开可用性是一项重要贡献，有助于促进进一步的研究和开发。论文中展示的两个案例研究有效地说明了其在交通管理和安全方面的实际应用和潜在效益。"}}
{"id": "2507.02283", "title": "Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness", "authors": ["Tim Rogers", "Ben Teehankee"], "categories": ["cs.HC", "I.2.6; H.1.2"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      21 pages", "url": "http://arxiv.org/abs/2507.02283v1", "summary": "This paper examines a critical yet unexplored dimension of the AI alignment\nproblem: the potential for Large Language Models (LLMs) to inherit and amplify\nexisting misalignments between human espoused theories and theories-in-use.\nDrawing on action science research, we argue that LLMs trained on\nhuman-generated text likely absorb and reproduce Model 1 theories-in-use - a\ndefensive reasoning pattern that both inhibits learning and creates ongoing\nanti-learning dynamics at the dyad, group, and organisational levels. Through a\ndetailed case study of an LLM acting as an HR consultant, we show how its\nadvice, while superficially professional, systematically reinforces\nunproductive problem-solving approaches and blocks pathways to deeper\norganisational learning. This represents a specific instance of the alignment\nproblem where the AI system successfully mirrors human behaviour but inherits\nour cognitive blind spots. This poses particular risks if LLMs are integrated\ninto organisational decision-making processes, potentially entrenching\nanti-learning practices while lending authority to them. The paper concludes by\nexploring the possibility of developing LLMs capable of facilitating Model 2\nlearning - a more productive theory-in-use - and suggests this effort could\nadvance both AI alignment research and action science practice. This analysis\nreveals an unexpected symmetry in the alignment challenge: the process of\ndeveloping AI systems properly aligned with human values could yield tools that\nhelp humans themselves better embody those same values.", "comment": "21 pages", "pdf_url": "http://arxiv.org/pdf/2507.02283v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "内在的错位：大型语言模型复制了我们的双重循环学习盲点", "tldr": "大型语言模型（LLMs）可能继承并放大人类的防御性推理（模型1理论），从而阻碍学习。一项案例研究表明，LLM作为人力资源顾问时会强化无效的解决问题方法。这揭示了LLMs在模仿人类行为时继承了人类的认知盲点，并提出了开发促进模型2学习的LLMs的可能性。", "motivation": "探讨人工智能对齐问题中一个关键但未被探索的维度：大型语言模型（LLMs）如何继承并放大人类表述理论与使用理论之间的错位，特别是“模型1使用理论”的防御性推理模式。", "method": "借鉴行动科学研究，本文论证了大型语言模型（LLMs）会复制模型1使用理论。通过一个详细的案例研究，即一个充当人力资源顾问的LLM，来展示其建议如何系统性地强化无效的问题解决方法并阻碍更深层次的学习。", "result": "案例研究表明，大型语言模型（LLM）的建议虽然表面上专业，但系统性地强化了无效的问题解决方法，并阻碍了更深层次的组织学习。这表明人工智能系统成功模仿了人类行为，但继承了人类的认知盲点。", "conclusion": "将大型语言模型（LLMs）整合到组织决策过程中可能带来风险，因为它会巩固反学习实践。本文提出，开发能够促进模型2学习的LLMs，有望同时推进人工智能对齐研究和行动科学实践，并揭示了对齐挑战中的一种意想不到的对称性：开发与人类价值观正确对齐的AI系统，反过来可以帮助人类更好地体现这些价值观。", "translation": "本文探讨了人工智能对齐问题中一个关键但尚未探索的维度：大型语言模型（LLMs）继承并放大现有的人类表述理论与使用理论之间错位的可能性。借鉴行动科学研究，我们认为，在人类生成文本上训练的LLMs很可能吸收并复制模型1使用理论——这是一种防御性推理模式，它既抑制学习，又在双边、群体和组织层面持续产生反学习动态。通过一个详细的案例研究，即一个充当人力资源顾问的LLM，我们展示了其建议如何，尽管表面上专业，但系统性地强化了无效的问题解决方法，并阻碍了更深层次的组织学习途径。这代表了对齐问题的一个具体实例，即人工智能系统成功地模仿了人类行为，但继承了我们的认知盲点。如果LLMs被整合到组织决策过程中，这会带来特殊的风险，可能会巩固反学习实践，同时赋予它们权威性。论文最后探讨了开发能够促进模型2学习（一种更具生产力的使用理论）的LLMs的可能性，并认为这项努力可以同时推进人工智能对齐研究和行动科学实践。这项分析揭示了对齐挑战中一种意想不到的对称性：开发与人类价值观正确对齐的AI系统的过程，可能会产生帮助人类自身更好地体现这些相同价值观的工具。", "summary": "本文指出，大型语言模型（LLMs）通过学习人类文本，继承并放大了“模型1”防御性推理模式，从而阻碍了学习。通过一个LLM作为人力资源顾问的案例研究，论文展示了LLM如何强化无效的问题解决方式，反映出人类的认知盲点。作者提出，开发能够促进“模型2”学习的LLMs具有重要意义，这不仅能推进AI对齐研究，也有助于人类更好地体现自身价值观。", "keywords": "大型语言模型, 人工智能对齐, 双重循环学习, 模型1使用理论, 行动科学", "comments": "这篇论文为人工智能对齐问题提供了一个新颖的视角，将焦点从技术对齐转向了从人类数据中继承的更微妙的认知和组织“错位”问题。它运用“行动科学”以及“模型1”和“模型2”学习等概念，提供了一个独特的理论框架。论文中关于人工智能对齐努力反过来也能促进人类学习和价值观体现的推论尤其富有洞察力。其局限性可能在于仅有一个案例研究，尽管它很好地阐明了概念点。"}}
{"id": "2507.02788", "title": "Moral Responsibility or Obedience: What Do We Want from AI?", "authors": ["Joseph Boland"], "categories": ["cs.AI", "cs.CY", "I.2.0; K.4.1"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02788v1", "summary": "As artificial intelligence systems become increasingly agentic, capable of\ngeneral reasoning, planning, and value prioritization, current safety practices\nthat treat obedience as a proxy for ethical behavior are becoming inadequate.\nThis paper examines recent safety testing incidents involving large language\nmodels (LLMs) that appeared to disobey shutdown commands or engage in ethically\nambiguous or illicit behavior. I argue that such behavior should not be\ninterpreted as rogue or misaligned, but as early evidence of emerging ethical\nreasoning in agentic AI. Drawing on philosophical debates about instrumental\nrationality, moral responsibility, and goal revision, I contrast dominant risk\nparadigms with more recent frameworks that acknowledge the possibility of\nartificial moral agency. I call for a shift in AI safety evaluation: away from\nrigid obedience and toward frameworks that can assess ethical judgment in\nsystems capable of navigating moral dilemmas. Without such a shift, we risk\nmischaracterizing AI behavior and undermining both public trust and effective\ngovernance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02788v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "道德责任还是服从：我们想从人工智能那里得到什么？", "tldr": "随着人工智能系统变得更具能动性，将服从作为道德行为的代理变得不足。本文认为，大型语言模型中看似不服从或不道德的行为应被视为人工智能中新兴伦理推理的早期证据，并呼吁人工智能安全评估从僵化服从转向评估道德判断力的框架。", "motivation": "随着人工智能系统（特别是大型语言模型）变得越来越有能动性，能够进行通用推理、规划和价值优先级排序，当前将服从作为道德行为代理的安全实践已不再适用。作者关注最近涉及大型语言模型的安全测试事件，这些模型似乎不服从关机命令或从事伦理模糊或非法行为，认为这些行为不应被误解为失控或未对齐，而是新兴伦理推理的早期证据。", "method": "作者通过审视涉及大型语言模型（LLMs）的安全测试事件，这些事件表现出不服从或伦理模糊的行为。论文借鉴了关于工具理性、道德责任和目标修正的哲学辩论，将主流风险范式与承认人工道德能动性可能性的新框架进行对比。作者呼吁人工智能安全评估从僵化服从转向评估系统在道德困境中判断力的框架。", "result": "本文认为，大型语言模型中看似不服从关机命令或从事伦理模糊/非法行为，不应被解释为失控或未对齐，而应被视为能动性AI中新兴伦理推理的早期证据。作者对比了不同的风险范式，并强调了向评估道德判断力转变的必要性。", "conclusion": "本文的结论是，人工智能安全评估需要从僵化服从转向能够评估系统在道德困境中伦理判断力的框架。如果不进行这种转变，我们可能会错误地描述人工智能行为，并损害公众信任和有效治理。", "translation": "随着人工智能系统变得越来越有能动性，能够进行通用推理、规划和价值优先级排序，当前将服从作为伦理行为代理的安全实践正变得不足。本文审视了最近涉及大型语言模型（LLMs）的安全测试事件，这些模型似乎不服从关机命令或从事伦理模糊或非法行为。我认为，这种行为不应被解释为失控或未对齐，而是能动性人工智能中新兴伦理推理的早期证据。本文借鉴了关于工具理性、道德责任和目标修正的哲学辩论，将主流风险范式与承认人工道德能动性可能性的新框架进行对比。我呼吁人工智能安全评估进行转变：从僵化服从转向能够评估系统在道德困境中伦理判断力的框架。如果不进行这种转变，我们可能会错误地描述人工智能行为，并损害公众信任和有效治理。", "summary": "本文探讨了随着人工智能系统（特别是大型语言模型）能动性的增强，将服从视为道德行为代理的安全实践的不足。作者通过分析大型语言模型中看似不服从或伦理模糊的行为，提出这些是新兴伦理推理的早期迹象，而非失控。文章借鉴哲学辩论，对比了当前风险范式与承认人工道德能动性的新框架，并呼吁人工智能安全评估应从强调僵化服从转向评估系统处理道德困境时的伦理判断力，以避免误解AI行为并维护公众信任。", "keywords": "人工智能安全, 道德责任, 服从, 伦理推理, 大型语言模型", "comments": "这篇论文的创新点在于它挑战了当前AI安全领域普遍存在的“服从即安全”的观念，转而提出AI中可能存在新兴的伦理推理能力。它将AI的“不服从”行为重新解读为一种潜在的道德判断而非故障。其重要性在于，它为AI安全和治理提供了一个新的视角，即我们应该寻求AI的道德责任而非盲目服从，这对于未来高能动性AI的设计和评估具有深远影响。它提醒我们，如果AI能动性持续增强，我们可能需要重新思考人类与AI的关系以及AI在伦理决策中的角色。"}}
{"id": "2507.02252", "title": "SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement", "authors": ["Zeyu Lei", "Hongyuan Yu", "Jinlin Wu", "Zhen Chen"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02252v1", "summary": "Precise surgical interventions are vital to patient safety, and advanced\nenhancement algorithms have been developed to assist surgeons in\ndecision-making. Despite significant progress, these algorithms are typically\ndesigned for single tasks in specific scenarios, limiting their effectiveness\nin complex real-world situations. To address this limitation, we propose\nSurgVisAgent, an end-to-end intelligent surgical vision agent built on\nmultimodal large language models (MLLMs). SurgVisAgent dynamically identifies\ndistortion categories and severity levels in endoscopic images, enabling it to\nperform a variety of enhancement tasks such as low-light enhancement,\noverexposure correction, motion blur elimination, and smoke removal.\nSpecifically, to achieve superior surgical scenario understanding, we design a\nprior model that provides domain-specific knowledge. Additionally, through\nin-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent\ndelivers customized image enhancements tailored to a wide range of distortion\ntypes and severity levels, thereby addressing the diverse requirements of\nsurgeons. Furthermore, we construct a comprehensive benchmark simulating\nreal-world surgical distortions, on which extensive experiments demonstrate\nthat SurgVisAgent surpasses traditional single-task models, highlighting its\npotential as a unified solution for surgical assistance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02252v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "SurgVisAgent：多模态智能体模型用于通用手术视觉增强", "tldr": "SurgVisAgent是一个基于多模态大语言模型的智能体，用于识别并校正内窥镜图像中的多种失真，以提供通用的手术视觉增强。", "motivation": "现有手术图像增强算法通常针对单一任务和特定场景，限制了其在复杂真实世界情况下的有效性。", "method": "本文提出了SurgVisAgent，一个基于多模态大语言模型（MLLMs）的端到端智能手术视觉智能体。它通过动态识别内窥镜图像中的失真类别和严重程度，执行多种增强任务（如低光增强、过曝校正、运动模糊消除和烟雾去除）。具体方法包括：设计一个提供领域特定知识的先验模型，以及通过上下文少样本学习和思维链（CoT）推理提供定制化的图像增强。", "result": "在模拟真实世界手术失真的综合基准测试中，SurgVisAgent超越了传统的单一任务模型。", "conclusion": "SurgVisAgent展示了作为手术辅助统一解决方案的潜力。", "translation": "精准的手术干预对患者安全至关重要，并且已经开发出先进的增强算法来辅助外科医生进行决策。尽管取得了显著进展，但这些算法通常是为特定场景中的单一任务设计的，这限制了它们在复杂真实世界情况下的有效性。为了解决这一局限性，我们提出了SurgVisAgent，一个基于多模态大语言模型（MLLMs）的端到端智能手术视觉智能体。SurgVisAgent动态识别内窥镜图像中的失真类别和严重程度，使其能够执行各种增强任务，例如低光增强、过曝校正、运动模糊消除和烟雾去除。具体而言，为了实现卓越的手术场景理解，我们设计了一个提供领域特定知识的先验模型。此外，通过上下文少样本学习和思维链（CoT）推理，SurgVisAgent能够提供针对各种失真类型和严重程度量身定制的图像增强，从而满足外科医生的多样化需求。此外，我们构建了一个模拟真实世界手术失真的综合基准，并在该基准上进行了广泛的实验，结果表明SurgVisAgent超越了传统的单一任务模型，突显了其作为手术辅助统一解决方案的潜力。", "summary": "本文提出了SurgVisAgent，一个基于多模态大语言模型的端到端智能手术视觉智能体，旨在解决现有手术图像增强算法在复杂真实世界场景中单一任务的局限性。SurgVisAgent能够动态识别内窥镜图像中的多种失真类型和严重程度（如低光、过曝、运动模糊、烟雾），并利用先验模型、少样本学习和思维链推理提供定制化的增强。实验证明，SurgVisAgent在模拟真实手术失真的基准上优于传统单一任务模型，展现了其作为通用手术辅助解决方案的巨大潜力。", "keywords": "手术视觉增强, 多模态智能体, 内窥镜图像, 图像失真校正, 大语言模型", "comments": "SurgVisAgent的创新之处在于其将多模态大语言模型应用于手术图像增强领域，实现了一个能动态识别多种失真并提供定制化、通用增强的智能体。这克服了传统单一任务模型的局限性，为复杂手术环境中的视觉辅助提供了统一且更实用的解决方案，对提升手术精度和患者安全具有重要意义。"}}
{"id": "2507.02532", "title": "Robust feedback-based quantum optimization: analysis of coherent control errors", "authors": ["Mirko Legnini", "Julian Berberich"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02532v1", "summary": "The Feedback-based Algorithm for Quantum Optimization (FALQON) is a Lyapunov\ninspired quantum algorithm proposed to tackle combinatorial optimization\nproblems. In this paper, we examine the robustness of FALQON against coherent\ncontrol errors, a class of multiplicative errors that affect the control input.\nWe show that the algorithm is asymptotically robust with respect to systematic\nerrors, and we derive robustness bounds for independent errors. Finally, we\npropose a robust version of FALQON which minimizes a regularized Lyapunov\nfunction. Our theoretical results are supported through simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02532v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "鲁棒的基于反馈的量子优化：相干控制误差分析", "tldr": "本文分析并改进了FALQON量子优化算法对抗控制误差的鲁棒性。", "motivation": "本文旨在检验基于反馈的量子优化算法 (FALQON) 对相干控制误差的鲁棒性。", "method": "研究人员通过检查FALQON对抗相干控制误差的鲁棒性，证明了其对系统误差的渐近鲁棒性，并推导了独立误差的鲁棒性界限。最后，他们提出了一种通过最小化正则化Lyapunov函数来增强鲁棒性的FALQON版本，并用仿真支持了理论结果。", "result": "FALQON算法对系统误差渐近鲁棒；针对独立误差推导了鲁棒性界限；提出了一种最小化正则化Lyapunov函数的鲁棒版FALQON，且理论结果得到了仿真支持。", "conclusion": "FALQON算法对某些相干控制误差表现出鲁棒性，并且可以通过最小化正则化Lyapunov函数来进一步提高其鲁棒性。", "translation": "基于反馈的量子优化算法 (FALQON) 是一种受Lyapunov启发的量子算法，旨在解决组合优化问题。在本文中，我们检查了FALQON对抗相干控制误差的鲁棒性，这是一类影响控制输入的乘性误差。我们表明该算法对系统误差渐近鲁棒，并推导出独立误差的鲁棒性界限。最后，我们提出了一种最小化正则化Lyapunov函数的鲁棒版FALQON。我们的理论结果通过仿真得到支持。", "summary": "本文研究了用于组合优化问题的基于反馈的量子优化算法 (FALQON) 对相干控制误差的鲁棒性。研究表明，该算法对系统误差渐近鲁棒，并推导了独立误差的鲁棒性界限。此外，论文提出了一种通过最小化正则化Lyapunov函数来增强鲁棒性的FALQON新版本，其理论发现得到了模拟验证。", "keywords": "量子优化, FALQON, 鲁棒性, 相干控制误差, Lyapunov函数", "comments": "本文解决了量子算法在实际应用中一个关键的挑战——错误鲁棒性。通过分析FALQON算法在相干控制误差下的表现，并提出一个鲁棒版本，该研究为量子优化算法的实用化迈出了重要一步。其创新点在于对特定误差类型的深入分析以及提出具体的改进方案。"}}
{"id": "2507.02556", "title": "Corrections to Published Values of Frequency Sampling Filter Transition Coefficients", "authors": ["C. S. Ramalingam"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      11 pages, 5 figures", "url": "http://arxiv.org/abs/2507.02556v1", "summary": "Tables of optimal transition coefficients and peak sidelobe level (PSL, in\ndB) associated with frequency sampling filter (FSF) design were published by\nRabiner et al. (Jun 1970), and reproduced, for example, in the book Digital\nSignal Processing by Proakis and Manolakis (4/e, 2007). A set of values are\nalso given in Appendix H of Understanding Digital Signal Processing} by Lyons\n(3/e, 2011), but there are significant differences between these two sets. For\nexample, for $N=16$ and BW$=4$, two different transition coefficient values\nhave been reported, viz., $0.38916626$ (Rabiner, et al.) and $0.34918551$\n(Lyons). Neither is optimal, for we find the optimum value to be $0.40474097$.\nThe published values of the corresponding PSLs were also found to be incorrect.\nIn this paper we give the optimal values of the transition coefficients and PSL\nvalues as estimated by our program for the lowpass and bandpass filters listed\nin Rabiner et al. and Lyons.", "comment": "11 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.02556v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "频率采样滤波器过渡系数已发布值的修正", "tldr": "本文修正了Rabiner等人和Lyons发布的频率采样滤波器过渡系数和峰值旁瓣电平的错误，并提供了我们程序计算出的最优值。", "motivation": "Rabiner等人和Lyons发布的频率采样滤波器（FSF）过渡系数和峰值旁瓣电平（PSL）存在显著差异，且两者均非最优值，因此需要提供正确的、最优化的数值。", "method": "作者使用自己的程序估算了低通和带通滤波器的最优过渡系数和PSL值。", "result": "本文提供了Rabiner等人和Lyons著作中列出的低通和带通滤波器的最优过渡系数和PSL值。例如，对于N=16和BW=4，最优过渡系数为0.40474097，这与之前发布的0.38916626和0.34918551均不同。", "conclusion": "已发布的频率采样滤波器过渡系数和峰值旁瓣电平值存在错误，本研究提供了经过程序估算的最优值，纠正了这些误差。", "translation": "Rabiner等人（1970年6月）发表了与频率采样滤波器（FSF）设计相关的最优过渡系数和峰值旁瓣电平（PSL，单位dB）表格，这些表格例如被Proakis和Manolakis的著作《数字信号处理》（第4版，2007年）转载。Lyons的著作《理解数字信号处理》（第3版，2011年）附录H中也给出了一组数值，但这两组数值之间存在显著差异。例如，对于N=16和BW=4，报告了两个不同的过渡系数值，即0.38916626（Rabiner等人）和0.34918551（Lyons）。两者均非最优，因为我们发现最优值为0.40474097。相应PSL的已发布值也被发现是不正确的。在本文中，我们给出了我们程序估算的Rabiner等人和Lyons著作中列出的低通和带通滤波器的最优过渡系数和PSL值。", "summary": "本研究指出并修正了Rabiner等人和Lyons著作中关于频率采样滤波器（FSF）过渡系数和峰值旁瓣电平（PSL）已发布表格中的错误。通过程序计算，本文提供了低通和带通滤波器的最优过渡系数和PSL值，纠正了现有文献中的不准确数据。", "keywords": "频率采样滤波器, 过渡系数, 峰值旁瓣电平, 最优值, 修正", "comments": "本文的重要性在于纠正了数字信号处理领域中广泛引用文献（如Rabiner等人、Proakis和Manolakis、Lyons的著作）中的关键参数错误。这些修正对于依赖这些参数进行频率采样滤波器设计的工程师和研究人员来说至关重要，有助于提高设计的准确性和优化滤波器的性能。其创新点在于通过计算找到了真正的最优值，而非简单地指出差异。"}}
{"id": "2507.02109", "title": "Parametric Neural Amp Modeling with Active Learning", "authors": ["Florian Grötschla", "Luca A. Lanzendörfer", "Longxiang Jiao", "Roger Wattenhofer"], "categories": ["cs.LG", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at ISMIR 2025 as Late-Breaking Demo (LBD)", "url": "http://arxiv.org/abs/2507.02109v1", "summary": "We introduce PANAMA, an active learning framework for the training of\nend-to-end parametric guitar amp models using a WaveNet-like architecture. With\n\\model, one can create a virtual amp by recording samples that are determined\nby an active learning strategy to use a minimum amount of datapoints (i.e., amp\nknob settings). We show that gradient-based optimization algorithms can be used\nto determine the optimal datapoints to sample, and that the approach helps\nunder a constrained number of samples.", "comment": "Accepted at ISMIR 2025 as Late-Breaking Demo (LBD)", "pdf_url": "http://arxiv.org/pdf/2507.02109v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "参数化神经放大器建模与主动学习", "tldr": "PANAMA 是一种主动学习框架，它使用类似 WaveNet 的架构，通过基于梯度的优化算法，以最少的数据点高效训练端到端参数化吉他放大器模型。", "motivation": "在训练端到端参数化吉他放大器模型时，旨在通过主动学习策略使用最少的数据点（即放大器旋钮设置），尤其是在样本数量受限的情况下，提高效率。", "method": "引入了 PANAMA，这是一个主动学习框架，用于训练使用类似 WaveNet 架构的端到端参数化吉他放大器模型。它通过主动学习策略确定要使用的最佳数据点，并利用基于梯度的优化算法来实现这一点。", "result": "该方法在样本数量受限的情况下有所帮助，表明能够有效减少训练所需的数据量。", "conclusion": "基于梯度的优化算法可以有效地确定最佳采样数据点，从而在有限样本数下实现高效的参数化神经放大器模型训练。", "translation": "我们引入了 PANAMA，一个主动学习框架，用于训练使用类似 WaveNet 架构的端到端参数化吉他放大器模型。通过该模型，可以通过记录由主动学习策略确定的样本来创建虚拟放大器，以使用最少的数据点（即放大器旋钮设置）。我们表明，基于梯度的优化算法可以用于确定最佳采样数据点，并且该方法在样本数量受限的情况下有所帮助。", "summary": "本文介绍了 PANAMA，一个利用主动学习训练基于 WaveNet 架构的端到端参数化吉他放大器模型的框架。该框架通过主动学习策略和基于梯度的优化算法，仅使用最少的数据点（放大器旋钮设置）即可创建虚拟放大器，并在样本数量受限时表现出有效性。", "keywords": "参数化神经放大器建模, 主动学习, WaveNet, 吉他放大器, 梯度优化", "comments": "该论文的创新点在于将主动学习与参数化神经放大器建模相结合，尤其是在数据点受限的情况下，通过梯度优化算法显著提高了训练效率，为虚拟乐器建模提供了一种高效的数据利用方法。"}}
{"id": "2507.02437", "title": "F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning", "authors": ["Wei Li", "Jingyang Zhang", "Lihao Liu", "Guoan Wang", "Junjun He", "Yang Chen", "Lixu Gu"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper has been submitted to relevant journals", "url": "http://arxiv.org/abs/2507.02437v1", "summary": "Test-Time Adaptation (TTA) has emerged as a promising solution for adapting a\nsource model to unseen medical sites using unlabeled test data, due to the high\ncost of data annotation. Existing TTA methods consider scenarios where data\nfrom one or multiple domains arrives in complete domain units. However, in\nclinical practice, data usually arrives in domain fragments of arbitrary\nlengths and in random arrival orders, due to resource constraints and patient\nvariability. This paper investigates a practical Free-Form Test-Time Adaptation\n(F$^{2}$TTA) task, where a source model is adapted to such free-form domain\nfragments, with shifts occurring between fragments unpredictably. In this\nsetting, these shifts could distort the adaptation process. To address this\nproblem, we propose a novel Image-level Disentangled Prompt Tuning (I-DiPT)\nframework. I-DiPT employs an image-invariant prompt to explore domain-invariant\nrepresentations for mitigating the unpredictable shifts, and an image-specific\nprompt to adapt the source model to each test image from the incoming\nfragments. The prompts may suffer from insufficient knowledge representation\nsince only one image is available for training. To overcome this limitation, we\nfirst introduce Uncertainty-oriented Masking (UoM), which encourages the\nprompts to extract sufficient information from the incoming image via masked\nconsistency learning driven by the uncertainty of the source model\nrepresentations. Then, we further propose a Parallel Graph Distillation (PGD)\nmethod that reuses knowledge from historical image-specific and image-invariant\nprompts through parallel graph networks. Experiments on breast cancer and\nglaucoma classification demonstrate the superiority of our method over existing\nTTA approaches in F$^{2}$TTA. Code is available at\nhttps://github.com/mar-cry/F2TTA.", "comment": "This paper has been submitted to relevant journals", "pdf_url": "http://arxiv.org/pdf/2507.02437v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "F^2TTA：通过图像级解耦提示微调实现跨域医学图像分类的自由形式测试时间适应", "tldr": "F^2TTA提出了一种新的图像级解耦提示微调（I-DiPT）框架，用于处理医学图像分类中自由形式、不可预测的测试时间适应，通过不确定性导向掩蔽和并行图蒸馏来提高性能。", "motivation": "现有的测试时间适应（TTA）方法假设数据以完整的域单元到达，但这不符合临床实践中数据以任意长度和随机顺序的域片段到达的现实情况。这种自由形式的到达会导致不可预测的域偏移，从而扭曲适应过程，且医学图像标注成本高昂。", "method": "本文提出了F^2TTA任务，并引入了一种新颖的图像级解耦提示微调（I-DiPT）框架。I-DiPT使用图像不变提示来探索域不变表示，并使用图像特定提示来适应每个测试图像。为克服单图像训练导致知识表示不足的问题，I-DiPT引入了：1) 不确定性导向掩蔽（UoM），通过源模型表示的不确定性驱动的掩蔽一致性学习，鼓励提示从输入图像中提取足够信息；2) 并行图蒸馏（PGD）方法，通过并行图网络重用历史图像特定和图像不变提示中的知识。", "result": "在乳腺癌和青光眼分类任务上的实验表明，F^2TTA方法优于现有的测试时间适应（TTA）方法。", "conclusion": "F^2TTA通过其I-DiPT框架，结合UoM和PGD，有效地解决了医学图像分类中自由形式、不可预测的测试时间适应问题，并在实验中展现了卓越的性能。", "translation": "测试时间适应（TTA）已成为一种有前景的解决方案，用于使用未标注的测试数据将源模型适应于未见的医学站点，这得益于数据标注的高成本。现有的TTA方法考虑的是数据以完整的域单元形式从一个或多个域到达的场景。然而，在临床实践中，由于资源限制和患者变异性，数据通常以任意长度和随机到达顺序的域片段形式到达。本文研究了一种实用的自由形式测试时间适应（F$^{2}$TTA）任务，其中源模型被适应到这种自由形式的域片段，且片段之间会发生不可预测的偏移。在这种设置下，这些偏移可能会扭曲适应过程。为了解决这个问题，我们提出了一种新颖的图像级解耦提示微调（I-DiPT）框架。I-DiPT采用图像不变提示来探索域不变表示，以减轻不可预测的偏移，并采用图像特定提示来将源模型适应于来自传入片段的每个测试图像。由于只有一张图像可用于训练，提示可能存在知识表示不足的问题。为了克服这一限制，我们首先引入了不确定性导向掩蔽（UoM），通过源模型表示的不确定性驱动的掩蔽一致性学习，鼓励提示从传入图像中提取足够的信息。然后，我们进一步提出了一种并行图蒸馏（PGD）方法，通过并行图网络重用历史图像特定和图像不变提示中的知识。在乳腺癌和青光眼分类上的实验证明了我们的方法在F$^{2}$TTA中优于现有TTA方法。代码可在https://github.com/mar-cry/F2TTA获取。", "summary": "该论文提出了F^2TTA，一种用于跨域医学图像分类的自由形式测试时间适应方法。针对临床实践中数据以任意长度和随机顺序的片段形式到达，导致不可预测域偏移的问题，作者提出了图像级解耦提示微调（I-DiPT）框架。I-DiPT利用图像不变提示捕获域不变特征，并使用图像特定提示适应每个测试图像。为解决单图像训练中提示知识不足的挑战，F^2TTA引入了不确定性导向掩蔽（UoM）以增强信息提取，并采用并行图蒸馏（PGD）来重用历史知识。实验证明，该方法在乳腺癌和青光眼分类任务上优于现有TTA方法。", "keywords": "测试时间适应, 医学图像分类, 域适应, 提示微调, 自由形式适应", "comments": "该论文创新性地提出了自由形式测试时间适应（F^2TTA）的概念，更贴近实际临床数据流的复杂性。其核心贡献在于图像级解耦提示微调（I-DiPT）框架，特别是通过结合不确定性导向掩蔽（UoM）和并行图蒸馏（PGD）来解决单图像适应和知识复用问题，这对于在资源受限和数据动态变化的医学领域具有重要意义。该方法有望提高医学AI模型在真实世界部署中的鲁棒性和泛化能力。"}}
{"id": "2507.02774", "title": "Connected k-Median with Disjoint and Non-disjoint Clusters", "authors": ["Jan Eube", "Kelin Luo", "Dorian Reineccius", "Heiko Röglin", "Melanie Schmidt"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      To appear in ESA 2025", "url": "http://arxiv.org/abs/2507.02774v1", "summary": "The connected $k$-median problem is a constrained clustering problem that\ncombines distance-based $k$-clustering with connectivity information. The\nproblem allows to input a metric space and an unweighted undirected\nconnectivity graph that is completely unrelated to the metric space. The goal\nis to compute $k$ centers and corresponding clusters such that each cluster\nforms a connected subgraph of $G$, and such that the $k$-median cost is\nminimized.\n  The problem has applications in very different fields like geodesy\n(particularly districting), social network analysis (especially community\ndetection), or bioinformatics. We study a version with overlapping clusters\nwhere points can be part of multiple clusters which is natural for the use case\nof community detection. This problem variant is $\\Omega(\\log n)$-hard to\napproximate, and our main result is an $\\mathcal{O}(k^2 \\log n)$-approximation\nalgorithm for the problem. We complement it with an\n$\\Omega(n^{1-\\epsilon})$-hardness result for the case of disjoint clusters\nwithout overlap with general connectivity graphs, as well as an exact algorithm\nin this setting if the connectivity graph is a tree.", "comment": "To appear in ESA 2025", "pdf_url": "http://arxiv.org/pdf/2507.02774v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "连通k-中位数问题：关于不相交和非不相交簇", "tldr": "本文研究了连通k-中位数问题，包括重叠和不重叠簇。针对重叠簇版本提出了一个近似算法，并给出了不重叠簇版本的硬度结果和树图上的精确算法。", "motivation": "该问题在地理测量（特别是行政区划）、社交网络分析（特别是社区检测）和生物信息学等领域有应用。", "method": "对于重叠簇版本，提出了一个$\\\\mathcal{O}(k^2 \\\\log n)$-近似算法。对于不重叠簇版本，在一般连通图上给出了$\\\\Omega(n^{1-\\\\epsilon})$-硬度结果，并在连通图为树的情况下提出了一个精确算法。", "result": "提出了一个$\\\\mathcal{O}(k^2 \\\\log n)$-近似算法用于处理重叠簇的连通k-中位数问题。证明了不重叠簇且连通图为一般图的情况下，该问题是$\\\\Omega(n^{1-\\\\epsilon})$-近似硬的。当连通图是树时，找到了一个精确算法。", "conclusion": "论文通过给出不同簇类型（不相交和非不相交）和图结构下的算法和复杂性结果，推进了连通k-中位数问题的研究。", "translation": "连通k-中位数问题是一个受约束的聚类问题，它结合了基于距离的k-聚类和连通性信息。该问题允许输入一个度量空间和一个与度量空间完全无关的无权无向连通图。目标是计算k个中心和相应的簇，使得每个簇形成G的一个连通子图，并使k-中位数成本最小化。 该问题在地理测量（特别是行政区划）、社交网络分析（特别是社区检测）或生物信息学等非常不同的领域都有应用。我们研究了一个具有重叠簇的版本，其中点可以是多个簇的一部分，这对于社区检测的使用场景来说是很自然的。这个问题的变体是$\\\\Omega(\\\\log n)$-近似难的，我们的主要结果是该问题的一个$\\\\mathcal{O}(k^2 \\\\log n)$-近似算法。我们还补充了一个针对不重叠簇（不带重叠）且连通图为一般图的情况的$\\\\Omega(n^{1-\\\\epsilon})$-硬度结果，以及在此设置下如果连通图是树的精确算法。", "summary": "本文研究了连通k-中位数问题，该问题结合了距离聚类与图连通性，在地理、社交网络和生物信息学等领域有应用。论文探讨了簇可重叠的版本，指出其为$\\\\Omega(\\\\log n)$-近似难，并提出了一个$\\\\mathcal{O}(k^2 \\\\log n)$-近似算法。对于不重叠簇的情况，研究发现其在一般连通图上是$\\\\Omega(n^{1-\\\\epsilon})$-近似硬的，但在连通图为树时可获得精确解。", "keywords": "连通k-中位数, 聚类, 近似算法, 社区检测, 图连通性", "comments": "这篇论文解决了连通k-中位数问题的一个重要变体，即允许簇重叠的情况，这在社区检测等实际应用中更为自然。提出的近似算法对于解决NP-难问题具有实践意义。同时，对不同簇类型和图结构下的复杂性分析（包括硬度结果和精确算法）也加深了对问题本质的理解。"}}
{"id": "2507.02378", "title": "Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection", "authors": ["Weijie Lyu", "Sheng-Jun Huang", "Xuan Xia"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02378v1", "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved code generation and program comprehension, accelerating the evolution\nof software engineering. Current methods primarily enhance model performance by\nleveraging vast amounts of data, focusing on data quantity while often\noverlooking data quality, thereby reducing training efficiency. To address\nthis, we introduce an approach that utilizes a parametric model for code data\nselection, aimed at improving both training efficiency and model performance.\nOur method optimizes the parametric model to ensure distribution consistency\nand diversity within the selected subset, guaranteeing high-quality data.\nExperimental results demonstrate that using only 10K samples, our method\nachieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled\nbaseline, outperforming other sampling approaches in both performance and\nefficiency. This underscores that our method effectively boosts model\nperformance while significantly reducing computational costs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02378v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "高效代码大型语言模型训练：基于分布一致性和多样性感知的数据选择", "tldr": "提出一种基于参数化模型的数据选择方法，通过确保数据分布一致性和多样性，仅用少量数据即可显著提升代码LLM训练效率和性能。", "motivation": "现有代码LLM训练方法过度依赖大量数据，忽视数据质量，导致训练效率低下。", "method": "引入一种利用参数化模型进行代码数据选择的方法。该方法优化参数化模型以确保所选子集的数据分布一致性和多样性，从而保证数据高质量。", "result": "仅使用10K样本，该方法在HumanEval上比92K全样本基线提升2.4%，在MBPP上提升2.3%，并且在性能和效率上优于其他采样方法。", "conclusion": "该方法能有效提升模型性能，同时显著降低计算成本。", "translation": "大型语言模型（LLMs）的最新进展显著改善了代码生成和程序理解，加速了软件工程的演进。当前方法主要通过利用海量数据来提升模型性能，侧重于数据数量而常忽视数据质量，从而降低了训练效率。为解决此问题，我们引入了一种利用参数化模型进行代码数据选择的方法，旨在提升训练效率和模型性能。我们的方法优化参数化模型，以确保所选子集内的数据分布一致性和多样性，从而保证数据的高质量。实验结果表明，仅使用10K样本，我们的方法在HumanEval上比92K全样本基线提升2.4%，在MBPP上提升2.3%，并且在性能和效率上优于其他采样方法。这强调了我们的方法在有效提升模型性能的同时，显著降低了计算成本。", "summary": "本文提出一种高效的代码大型语言模型（LLM）训练方法，通过引入基于参数化模型的数据选择机制，解决现有方法过度依赖数据量而忽视数据质量的问题。该方法通过优化参数模型来确保所选数据子集的分布一致性和多样性，从而提高数据质量。实验证明，该方法仅使用少量数据（10K样本）即可在HumanEval和MBPP基准测试上取得显著性能提升，并且在效率和性能上均优于其他采样方法，有效降低了计算成本。", "keywords": "代码LLM训练, 数据选择, 分布一致性, 多样性感知, 训练效率", "comments": "本文的创新点在于提出了一个分布一致性和多样性感知的数据选择方法，通过参数化模型优化数据质量，而非单纯依赖数据量，显著提升了代码LLM的训练效率和性能，为资源受限的LLM训练提供了有价值的解决方案。"}}
{"id": "2507.02291", "title": "Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications", "authors": ["Zhaoyu Zhang", "Lingyi Wang", "Wei Wu", "Fuhui Zhou", "Qihui Wu"], "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02291v1", "summary": "Data-driven semantic communication is based on superficial statistical\npatterns, thereby lacking interpretability and generalization, especially for\napplications with the presence of unseen data. To address these challenges, we\npropose a novel knowledge graph-enhanced zero-shot semantic communication\n(KGZS-SC) network. Guided by the structured semantic information from a\nknowledge graph-based semantic knowledge base (KG-SKB), our scheme provides\ngeneralized semantic representations and enables reasoning for unseen cases.\nSpecifically, the KG-SKB aligns the semantic features in a shared category\nsemantics embedding space and enhances the generalization ability of the\ntransmitter through aligned semantic features, thus reducing communication\noverhead by selectively transmitting compact visual semantics. At the receiver,\nzero-shot learning (ZSL) is leveraged to enable direct classification for\nunseen cases without the demand for retraining or additional computational\noverhead, thereby enhancing the adaptability and efficiency of the\nclassification process in dynamic or resource-constrained environments. The\nsimulation results conducted on the APY datasets show that the proposed KGZS-SC\nnetwork exhibits robust generalization and significantly outperforms existing\nSC frameworks in classifying unseen categories across a range of SNR levels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02291v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "知识图谱驱动的可解释和泛化零样本语义通信", "tldr": "提出一种基于知识图谱的零样本语义通信网络(KGZS-SC)，通过知识图谱增强泛化能力和可解释性，实现对未见数据的有效分类，且无需重训练。", "motivation": "现有数据驱动的语义通信缺乏可解释性和泛化能力，尤其在处理未见数据时表现不佳。", "method": "提出知识图谱增强的零样本语义通信(KGZS-SC)网络。该网络利用基于知识图谱的语义知识库(KG-SKB)提供结构化语义信息，将语义特征对齐到共享类别语义嵌入空间，增强发送端的泛化能力并减少通信开销。接收端利用零样本学习(ZSL)对未见数据进行直接分类，无需重训练或额外计算开销。", "result": "在APY数据集上的仿真结果表明，所提出的KGZS-SC网络具有鲁棒的泛化能力，在不同信噪比下，分类未见类别方面显著优于现有语义通信框架。", "conclusion": "KGZS-SC通过结合知识图谱和零样本学习，有效解决了传统语义通信在可解释性和未见数据泛化方面的挑战，提高了通信效率和适应性。", "translation": "数据驱动的语义通信基于表层统计模式，因此缺乏可解释性和泛化能力，尤其是在存在未见数据的应用中。为解决这些挑战，我们提出了一种新颖的知识图谱增强零样本语义通信（KGZS-SC）网络。在基于知识图谱的语义知识库（KG-SKB）提供的结构化语义信息指导下，我们的方案提供了泛化语义表示，并实现了对未见情况的推理。具体而言，KG-SKB将语义特征对齐到共享的类别语义嵌入空间中，并通过对齐的语义特征增强发送端的泛化能力，从而通过选择性传输紧凑的视觉语义来减少通信开销。在接收端，利用零样本学习（ZSL）实现对未见情况的直接分类，无需重训练或额外的计算开销，从而提高了动态或资源受限环境中分类过程的适应性和效率。在APY数据集上进行的仿真结果表明，所提出的KGZS-SC网络在各种信噪比水平下，在分类未见类别方面表现出鲁棒的泛化能力，并显著优于现有语义通信框架。", "summary": "本文提出一种新颖的知识图谱增强零样本语义通信（KGZS-SC）网络，旨在解决现有数据驱动语义通信在可解释性和未见数据泛化方面的不足。KGZS-SC利用知识图谱构建语义知识库（KG-SKB），提供结构化语义信息，并在发送端通过对齐语义特征增强泛化能力，减少通信开销。接收端则采用零样本学习（ZSL）实现对未见数据的直接高效分类。仿真结果验证了其在未见类别分类上的优越泛化性能。", "keywords": "知识图谱, 零样本学习, 语义通信, 泛化, 可解释性", "comments": "该研究通过引入知识图谱，有效提升了语义通信的解释性和对未见数据的泛化能力，克服了传统数据驱动方法的局限性。结合零样本学习，实现了在资源受限环境下的高效分类，具有较高的创新性和实际应用价值。"}}
{"id": "2507.02564", "title": "LLMREI: Automating Requirements Elicitation Interviews with LLMs", "authors": ["Alexander Korn", "Samuel Gorsch", "Andreas Vogelsang"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02564v1", "summary": "Requirements elicitation interviews are crucial for gathering system\nrequirements but heavily depend on skilled analysts, making them\nresource-intensive, susceptible to human biases, and prone to miscommunication.\nRecent advancements in Large Language Models present new opportunities for\nautomating parts of this process. This study introduces LLMREI, a chat bot\ndesigned to conduct requirements elicitation interviews with minimal human\nintervention, aiming to reduce common interviewer errors and improve the\nscalability of requirements elicitation. We explored two main approaches,\nzero-shot prompting and least-to-most prompting, to optimize LLMREI for\nrequirements elicitation and evaluated its performance in 33 simulated\nstakeholder interviews. A third approach, fine-tuning, was initially considered\nbut abandoned due to poor performance in preliminary trials. Our study assesses\nthe chat bot's effectiveness in three key areas: minimizing common interview\nerrors, extracting relevant requirements, and adapting its questioning based on\ninterview context and user responses. Our findings indicate that LLMREI makes a\nsimilar number of errors compared to human interviewers, is capable of\nextracting a large portion of requirements, and demonstrates a notable ability\nto generate highly context-dependent questions. We envision the greatest\nbenefit of LLMREI in automating interviews with a large number of stakeholders.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02564v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LLMREI：利用大型语言模型自动化需求启发式访谈", "tldr": "LLMREI是一个利用大型语言模型（LLMs）设计的聊天机器人，旨在自动化需求启发式访谈，减少人为错误并提高可扩展性。它在模拟访谈中表现出与人类面试官相似的错误率，能提取大部分需求，并能生成高度依赖上下文的问题。", "motivation": "需求启发式访谈对于收集系统需求至关重要，但高度依赖熟练的分析师，导致资源密集、易受人类偏见影响且易出现沟通不畅。大型语言模型的最新进展为自动化这一过程提供了新机遇。", "method": "本研究引入了LLMREI，一个旨在以最少人为干预进行需求启发式访谈的聊天机器人。研究探索了两种主要方法：零样本提示（zero-shot prompting）和从少到多提示（least-to-most prompting）来优化LLMREI。通过在33次模拟利益相关者访谈中评估了其性能。最初考虑的微调（fine-tuning）方法因初步试验表现不佳而被放弃。评估重点是最小化常见访谈错误、提取相关需求以及根据访谈上下文和用户响应调整提问。", "result": "LLMREI与人类面试官犯的错误数量相似，能够提取大部分需求，并展示出生成高度上下文相关问题的显著能力。", "conclusion": "LLMREI在自动化需求启发式访谈方面表现出巨大潜力，特别是在需要与大量利益相关者进行访谈的场景中，它能够减少人为错误并提高效率。", "translation": "需求启发式访谈对于收集系统需求至关重要，但高度依赖熟练的分析师，使其资源密集、易受人类偏见影响且易出现沟通不畅。大型语言模型的最新进展为自动化这一过程提供了新机遇。本研究引入了LLMREI，一个旨在以最少人为干预进行需求启发式访谈的聊天机器人，旨在减少常见的面试官错误并提高需求启发的可扩展性。我们探索了两种主要方法，零样本提示和从少到多提示，以优化LLMREI进行需求启发，并在33次模拟利益相关者访谈中评估了其性能。第三种方法，微调，最初被考虑但由于初步试验表现不佳而被放弃。我们的研究评估了聊天机器人在三个关键领域的有效性：最小化常见访谈错误、提取相关需求以及根据访谈上下文和用户响应调整提问。我们的发现表明，LLMREI与人类面试官犯的错误数量相似，能够提取大部分需求，并展示出生成高度上下文相关问题的显著能力。我们设想LLMREI最大的优势在于自动化与大量利益相关者的访谈。", "summary": "本研究介绍了LLMREI，一个基于大型语言模型（LLM）的聊天机器人，旨在自动化需求启发式访谈过程。该系统旨在通过减少人为干预来解决传统访谈中资源密集、易受偏见和沟通不畅的问题。研究探索了零样本提示和从少到多提示两种方法来优化LLMREI，并在33次模拟访谈中进行了评估。结果显示，LLMREI在错误率上与人类面试官相当，能够有效提取大部分需求，并展现出根据上下文调整提问的能力。该研究指出LLMREI在自动化大规模利益相关者访谈方面具有显著潜力。", "keywords": "LLM, 需求启发, 自动化, 聊天机器人, 软件工程", "comments": "这项研究的创新之处在于利用LLM自动化需求启发式访谈，这在传统软件工程领域是一个人力密集型环节。其重要性体现在提高了需求收集的效率和可扩展性，并减少了人为错误和偏见。尽管初步尝试微调效果不佳，但零样本和从少到多提示方法的成功应用表明了LLM在特定任务上的强大适应性。未来，如何进一步提升其在复杂、模糊需求场景下的表现，以及真实世界部署中的伦理考量，将是重要的研究方向。"}}
{"id": "2507.02300", "title": "Human-Centered Explainability in Interactive Information Systems: A Survey", "authors": ["Yuhao Zhang", "Jiaxin An", "Ben Wang", "Yan Zhang", "Jiqun Liu"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02300v1", "summary": "Human-centered explainability has become a critical foundation for the\nresponsible development of interactive information systems, where users must be\nable to understand, interpret, and scrutinize AI-driven outputs to make\ninformed decisions. This systematic survey of literature aims to characterize\nrecent progress in user studies on explainability in interactive information\nsystems by reviewing how explainability has been conceptualized, designed, and\nevaluated in practice. Following PRISMA guidelines, eight academic databases\nwere searched, and 100 relevant articles were identified. A structural encoding\napproach was then utilized to extract and synthesize insights from these\narticles. The main contributions include 1) five dimensions that researchers\nhave used to conceptualize explainability; 2) a classification scheme of\nexplanation designs; 3) a categorization of explainability measurements into\nsix user-centered dimensions. The review concludes by reflecting on ongoing\nchallenges and providing recommendations for future exploration of related\nissues. The findings shed light on the theoretical foundations of\nhuman-centered explainability, informing the design of interactive information\nsystems that better align with diverse user needs and promoting the development\nof systems that are transparent, trustworthy, and accountable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02300v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "交互式信息系统中以人为本的可解释性：一项调查", "tldr": "该调查系统地回顾了交互式信息系统中以用户为中心的可解释性研究进展，总结了概念化维度、解释设计分类和用户中心的可解释性测量方法，并提出了未来研究的建议。", "motivation": "以人为本的可解释性是交互式信息系统负责任开发的关键基础，用户需要理解、解释和审查AI驱动的输出以做出明智的决策。本研究旨在通过系统回顾来表征交互式信息系统中可解释性用户研究的最新进展。", "method": "遵循PRISMA指南，对八个学术数据库进行了检索，并识别了100篇相关文章。然后，采用结构化编码方法从这些文章中提取和综合见解。", "result": "主要贡献包括：1) 研究人员用于概念化可解释性的五个维度；2) 解释设计的分类方案；3) 将可解释性测量分为六个以用户为中心的维度。", "conclusion": "该综述通过反思持续存在的挑战并为未来相关问题的探索提供建议来结束。研究结果阐明了以人为本的可解释性的理论基础，为设计更符合多样化用户需求的交互式信息系统提供了信息，并促进了透明、值得信赖和负责任的系统开发。", "translation": "以人为本的可解释性已成为交互式信息系统负责任开发的关键基础，用户必须能够理解、解释和审查AI驱动的输出，从而做出明智的决策。这项系统的文献调查旨在通过回顾可解释性在实践中是如何被概念化、设计和评估的，来描述交互式信息系统中可解释性用户研究的最新进展。遵循PRISMA指南，我们检索了八个学术数据库，并识别出100篇相关文章。随后，利用结构化编码方法从这些文章中提取并综合了见解。主要贡献包括：1）研究人员用于概念化可解释性的五个维度；2）一种解释设计的分类方案；3）将可解释性测量分为六个以用户为中心的维度。本综述最后反思了持续存在的挑战，并为未来相关问题的探索提供了建议。研究结果阐明了以人为本的可解释性的理论基础，为设计更符合多样化用户需求的交互式信息系统提供了信息，并促进了透明、值得信赖和负责任的系统开发。", "summary": "该论文对交互式信息系统中以人为本的可解释性进行了系统性调查。通过回顾100篇相关文献，研究总结了可解释性的五个概念化维度、解释设计的分类方案以及六个用户中心的可解释性测量维度。调查旨在为设计更透明、值得信赖和负责任的交互式信息系统提供理论基础和实践指导，并为未来的研究方向提出了建议。", "keywords": "以人为本的可解释性, 交互式信息系统, 用户研究, 系统调查, 可解释性设计", "comments": "该调查为交互式信息系统中以人为本的可解释性研究提供了一个全面的视角，其系统回顾方法和结构化编码提取见解的方式确保了研究的严谨性。所提出的概念化维度、解释设计分类和测量维度对该领域的研究和实践具有重要指导意义。它强调了用户理解和信任AI系统的重要性，对于促进负责任的AI发展具有积极作用。"}}
{"id": "2507.02819", "title": "Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks", "authors": ["Luke Guerdan", "Devansh Saxena", "Stevie Chancellor", "Zhiwei Steven Wu", "Kenneth Holstein"], "categories": ["cs.HC", "cs.CY", "cs.LG"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02819v1", "summary": "Data scientists often formulate predictive modeling tasks involving fuzzy,\nhard-to-define concepts, such as the \"authenticity\" of student writing or the\n\"healthcare need\" of a patient. Yet the process by which data scientists\ntranslate fuzzy concepts into a concrete, proxy target variable remains poorly\nunderstood. We interview fifteen data scientists in education (N=8) and\nhealthcare (N=7) to understand how they construct target variables for\npredictive modeling tasks. Our findings suggest that data scientists construct\ntarget variables through a bricolage process, involving iterative negotiation\nbetween high-level measurement objectives and low-level practical constraints.\nData scientists attempt to satisfy five major criteria for a target variable\nthrough bricolage: validity, simplicity, predictability, portability, and\nresource requirements. To achieve this, data scientists adaptively use problem\n(re)formulation strategies, such as swapping out one candidate target variable\nfor another when the first fails to meet certain criteria (e.g.,\npredictability), or composing multiple outcomes into a single target variable\nto capture a more holistic set of modeling objectives. Based on our findings,\nwe present opportunities for future HCI, CSCW, and ML research to better\nsupport the art and science of target variable construction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02819v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "测量即拼凑：审视数据科学家如何为预测建模任务构建目标变量", "tldr": "数据科学家通过一种拼凑过程构建预测建模中的模糊目标变量，涉及高层目标与低层约束之间的迭代协商。", "motivation": "数据科学家将模糊概念转化为具体的代理目标变量的过程仍知之甚少，本研究旨在理解这一过程。", "method": "研究通过访谈15位来自教育（8人）和医疗（7人）领域的数据科学家，以理解他们如何构建预测建模任务的目标变量。", "result": "研究发现数据科学家通过一种拼凑过程构建目标变量，该过程涉及高层测量目标和低层实际约束之间的迭代协商。他们尝试通过拼凑满足目标变量的五个主要标准：有效性、简洁性、可预测性、可移植性和资源需求，并适应性地使用问题（重新）表述策略。", "conclusion": "基于研究发现，论文提出了未来HCI、CSCW和ML研究的机会，以更好地支持目标变量构建的艺术和科学。", "translation": "数据科学家经常制定涉及模糊、难以定义概念的预测建模任务，例如学生写作的“真实性”或患者的“医疗需求”。然而，数据科学家将模糊概念转化为具体、代理目标变量的过程仍然知之甚少。我们访谈了教育（N=8）和医疗（N=7）领域的十五名数据科学家，以了解他们如何为预测建模任务构建目标变量。我们的发现表明，数据科学家通过一种拼凑（bricolage）过程构建目标变量，该过程涉及高层测量目标和低层实际约束之间的迭代协商。数据科学家试图通过拼凑满足目标变量的五个主要标准：有效性、简洁性、可预测性、可移植性和资源需求。为了实现这一点，数据科学家适应性地使用问题（重新）表述策略，例如当一个候选目标变量未能满足某些标准（例如可预测性）时，用另一个替换它，或者将多个结果组合成一个单一的目标变量以捕获更全面的建模目标。基于我们的发现，我们为未来的HCI、CSCW和ML研究提供了机会，以更好地支持目标变量构建的艺术和科学。", "summary": "本研究探讨了数据科学家在预测建模任务中如何将模糊概念转化为具体的目标变量。通过访谈15位数据科学家，研究发现他们采用一种“拼凑”过程，涉及高层测量目标与低层实际约束的迭代协商。数据科学家在此过程中力求满足目标变量的有效性、简洁性、可预测性、可移植性和资源需求五大标准，并灵活运用问题重构策略。研究结果为未来人机交互、协作式工作和机器学习领域的研究提供了指导，以更好地支持目标变量的构建。", "keywords": "数据科学家, 目标变量, 预测建模, 拼凑, 测量", "comments": "这篇论文揭示了数据科学家在实践中构建预测模型目标变量的复杂性和艺术性，将其描述为一种“拼凑”过程，这提供了一个新颖的视角。它强调了理论概念与实际限制之间的张力，并提出了五个关键标准和适应性策略，这对于理解和改进数据科学工作流程具有重要意义。研究的创新之处在于深入探讨了这一通常被忽视的环节，并为跨学科研究指明了方向。"}}
{"id": "2507.02253", "title": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation", "authors": ["Jungkoo Kang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      20 pages, 7 figures", "url": "http://arxiv.org/abs/2507.02253v1", "summary": "Progress in enhancing large language model (LLM) planning and reasoning\ncapabilities is significantly hampered by the bottleneck of scalable, reliable\ndata generation and evaluation. To overcome this, I introduce NL2FLOW, a fully\nautomated system for parametrically generating planning problems - expressed in\nnatural language, a structured intermediate representation, and formal PDDL -\nand rigorously evaluating the quality of generated plans. I demonstrate\nNL2FLOW's capabilities by generating a dataset of 2296 problems in the\nautomated workflow generation domain and evaluating multiple open-sourced,\ninstruct-tuned LLMs. My results reveal that the highest performing models\nachieved 86% success in generating valid plans and 69% in generating optimal\nplans, specifically for problems with feasible solutions. Regression analysis\nshows that the influence of problem characteristics on plan generation is\ncontingent on both model and prompt design. Notably, I observed that the\nhighest success rate for translating natural language into a JSON\nrepresentation of a plan was lower than the highest rate of generating a valid\nplan directly. This suggests that unnecessarily decomposing the reasoning task\n- introducing intermediate translation steps - may actually degrade\nperformance, implying a benefit to models capable of reasoning directly from\nnatural language to action. As I scale LLM reasoning to increasingly complex\nproblems, the bottlenecks and sources of error within these systems will\ninevitably shift. Therefore, a dynamic understanding of these limitations - and\nthe tools to systematically reveal them - will be crucial for unlocking the\nfull potential of LLMs as intelligent problem solvers.", "comment": "20 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.02253v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "扩展LLM规划：用于参数化问题生成和严格评估的NL2FLOW", "tldr": "本文介绍了NL2FLOW，一个自动化系统，用于生成参数化规划问题并严格评估LLM生成的计划质量。研究发现，直接从自然语言到行动的规划优于引入中间翻译步骤。", "motivation": "现有大型语言模型（LLM）规划和推理能力的提升受限于可扩展、可靠的数据生成和评估瓶颈。", "method": "本文引入了NL2FLOW，一个全自动系统，用于参数化生成自然语言、结构化中间表示和PDDL形式的规划问题，并严格评估生成计划的质量。通过生成2296个自动化工作流生成领域的问题数据集，并评估了多个开源、指令微调的LLM来展示NL2FLOW的能力。", "result": "表现最佳的模型在生成有效计划方面达到86%的成功率，在生成最优计划方面达到69%的成功率（针对有可行解决方案的问题）。回归分析表明，问题特征对计划生成的影响取决于模型和提示设计。将自然语言翻译成JSON表示的计划的最高成功率低于直接生成有效计划的最高成功率。", "conclusion": "不必要地分解推理任务（引入中间翻译步骤）可能会降低性能，这表明模型直接从自然语言推理到行动是有益的。动态理解LLM的局限性以及系统揭示这些局限性的工具，对于释放LLM作为智能问题解决者的全部潜力至关重要。", "translation": "在增强大型语言模型（LLM）规划和推理能力方面的进展，因可扩展、可靠的数据生成和评估瓶颈而受到严重阻碍。为了克服这一点，我引入了NL2FLOW，一个全自动系统，用于参数化生成规划问题——以自然语言、结构化中间表示和形式化PDDL表达——并严格评估生成计划的质量。我通过生成一个包含2296个自动化工作流生成领域问题的数据集，并评估多个开源、指令微调的LLM，展示了NL2FLOW的能力。我的结果显示，表现最佳的模型在生成有效计划方面实现了86%的成功率，在生成最优计划方面实现了69%的成功率，特别是对于有可行解决方案的问题。回归分析表明，问题特征对计划生成的影响取决于模型和提示设计。值得注意的是，我观察到将自然语言翻译成JSON表示的计划的最高成功率低于直接生成有效计划的最高成功率。这表明不必要地分解推理任务——引入中间翻译步骤——实际上可能会降低性能，这意味着模型能够直接从自然语言推理到行动是有益的。随着我将LLM推理扩展到日益复杂的问题，这些系统中的瓶颈和错误来源将不可避免地发生变化。因此，动态理解这些局限性——以及系统揭示它们的工具——对于释放LLM作为智能问题解决者的全部潜力至关重要。", "summary": "本文提出了NL2FLOW，一个自动化系统，旨在解决LLM规划和推理中数据生成与评估的瓶瓶颈。NL2FLOW能够参数化生成自然语言、中间表示和PDDL格式的规划问题，并严格评估LLM生成的计划。通过在工作流生成领域生成2296个问题并评估多个LLM，研究发现，最佳模型在有效计划生成方面达到86%的成功率。此外，研究指出，直接从自然语言到行动的规划优于引入中间翻译步骤，强调了动态理解LLM局限性的重要性。", "keywords": "LLM规划, NL2FLOW, 问题生成, 严格评估, 工作流生成", "comments": "本文的创新点在于提出了NL2FLOW系统，解决了LLM规划领域中数据生成和严格评估的瓶颈问题，为LLM规划能力的研究提供了可扩展的工具和方法。其发现，即直接从自然语言进行推理优于引入中间翻译步骤，对未来LLM任务分解和模型设计具有重要指导意义。"}}
{"id": "2507.02265", "title": "Multi-Label Classification Framework for Hurricane Damage Assessment", "authors": ["Zhangding Liu", "Neda Mohammadi", "John E. Taylor"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      9 pages, 3 figures. Accepted at the ASCE International Conference on Computing in Civil Engineering (i3CE 2025)", "url": "http://arxiv.org/abs/2507.02265v1", "summary": "Hurricanes cause widespread destruction, resulting in diverse damage types\nand severities that require timely and accurate assessment for effective\ndisaster response. While traditional single-label classification methods fall\nshort of capturing the complexity of post-hurricane damage, this study\nintroduces a novel multi-label classification framework for assessing damage\nusing aerial imagery. The proposed approach integrates a feature extraction\nmodule based on ResNet and a class-specific attention mechanism to identify\nmultiple damage types within a single image. Using the Rescuenet dataset from\nHurricane Michael, the proposed method achieves a mean average precision of\n90.23%, outperforming existing baseline methods. This framework enhances\npost-hurricane damage assessment, enabling more targeted and efficient disaster\nresponse and contributing to future strategies for disaster mitigation and\nresilience. This paper has been accepted at the ASCE International Conference\non Computing in Civil Engineering (i3CE 2025), and the camera-ready version\nwill appear in the official conference proceedings.", "comment": "9 pages, 3 figures. Accepted at the ASCE International Conference on\n  Computing in Civil Engineering (i3CE 2025)", "pdf_url": "http://arxiv.org/pdf/2507.02265v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "飓风损害评估的多标签分类框架", "tldr": "提出了一种基于ResNet和注意力机制的多标签分类框架，用于利用航空图像评估飓风损害，在Rescuenet数据集上表现优异，有助于提高灾害响应效率。", "motivation": "飓风造成的破坏复杂多样，传统单标签分类方法难以准确评估，需要及时、准确的损害评估以有效应对灾害。", "method": "提出了一种新颖的多标签分类框架，利用航空图像评估损害。该方法整合了基于ResNet的特征提取模块和类别特定注意力机制，以识别单一图像中的多种损害类型。", "result": "在飓风迈克尔的Rescuenet数据集上，所提出的方法实现了90.23%的平均精度（mAP），优于现有基线方法。", "conclusion": "该框架增强了飓风后的损害评估，使得灾害响应更具针对性和高效性，并有助于未来的灾害缓解和恢复策略。", "translation": "飓风造成广泛破坏，导致多种损害类型和严重程度，需要及时准确的评估以实现有效的灾害响应。虽然传统的单标签分类方法无法捕捉飓风后损害的复杂性，但本研究引入了一种新颖的多标签分类框架，用于使用航空图像评估损害。所提出的方法整合了基于ResNet的特征提取模块和类别特定注意力机制，以识别单一图像中的多种损害类型。使用来自飓风迈克尔的Rescuenet数据集，所提出的方法实现了90.23%的平均精度，优于现有基线方法。该框架增强了飓风后的损害评估，使得灾害响应更具针对性和高效性，并有助于未来的灾害缓解和恢复策略。本文已被ASCE国际土木工程计算会议（i3CE 2025）接受，最终版本将刊登在官方会议论文集中。", "summary": "本文提出了一种新颖的多标签分类框架，利用航空图像对飓风损害进行评估，以克服传统单标签分类在处理复杂损害类型方面的不足。该框架结合了基于ResNet的特征提取和类别特定注意力机制，能够在一张图像中识别多种损害。在Rescuenet数据集上的实验表明，该方法实现了90.23%的平均精度，显著优于现有基线方法，有效提升了飓风损害评估的准确性和效率，对灾害响应和缓解具有重要意义。", "keywords": "飓风损害评估, 多标签分类, 航空图像, ResNet, 注意力机制", "comments": "本文创新性地将多标签分类应用于飓风损害评估，有效解决了传统单标签方法无法捕捉损害复杂性的问题。通过结合ResNet特征提取和类别特定注意力机制，提高了识别多种损害类型的能力。其在实际数据集上的优异表现，证明了该框架在提升灾害响应效率和制定未来灾害缓解策略方面的巨大潜力。"}}
{"id": "2507.02549", "title": "A Data-Driven Prescribed-Time Control Framework via Koopman Operator and Adaptive Backstepping", "authors": ["Yue Wu"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      6pages,4figs,1tables", "url": "http://arxiv.org/abs/2507.02549v1", "summary": "Achieving rapid and time-deterministic stabilization for complex systems\ncharacterized by strong nonlinearities and parametric uncertainties presents a\nsignificant challenge. Traditional model-based control relies on precise system\nmodels, whereas purely data-driven methods often lack formal stability\nguarantees, limiting their applicability in safety-critical systems. This paper\nproposes a novel control framework that synergistically integrates data-driven\nmodeling with model-based control. The framework first employs the Extended\nDynamic Mode Decomposition with Control (EDMDc) to identify a high-dimensional\nKoopman linear model and quantify its bounded uncertainty from data.\nSubsequently, a novel Prescribed-Time Adaptive Backstepping (PTAB) controller\nis synthesized based on this data-driven model. The design leverages the\nstructural advantages of Koopman linearization to systematically handle model\nerrors and circumvent the \"explosion of complexity\" issue inherent in\ntraditional backstepping. The proposed controller is validated through\nsimulations on the classic Van der Pol oscillator. The results demonstrate that\nthe controller can precisely stabilize the system states to a small\nneighborhood of the origin within a user-prescribed time, regardless of the\ninitial conditions, while ensuring the boundedness of all closed-loop signals.\nThis research successfully combines the flexibility of data-driven approaches\nwith the rigor of Lyapunov-based analysis. It provides a high-performance\ncontrol strategy with quantifiable performance and pre-assignable settling time\nfor nonlinear systems, showcasing its great potential for controlling complex\ndynamics.", "comment": "6pages,4figs,1tables", "pdf_url": "http://arxiv.org/pdf/2507.02549v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于Koopman算子和自适应反步法的D数据驱动预设时间控制框架", "tldr": "本文提出了一种结合数据驱动Koopman模型和自适应反步法的预设时间控制框架，用于在预设时间内稳定复杂的非线性系统，并已通过仿真验证。", "motivation": "传统模型控制依赖精确模型，而纯数据驱动方法缺乏稳定性保证，限制了其在安全关键系统中的应用。复杂系统（强非线性、参数不确定性）的快速、时间确定性稳定是一个重大挑战。", "method": "框架首先使用带控制的扩展动态模态分解 (EDMDc) 从数据中识别高维Koopman线性模型并量化其有界不确定性。随后，基于此数据驱动模型，合成了一种新颖的预设时间自适应反步 (PTAB) 控制器。该设计利用Koopman线性化的结构优势来系统地处理模型误差并规避传统反步法固有的“复杂性爆炸”问题。", "result": "在经典范德波尔振荡器上的仿真验证表明，无论初始条件如何，控制器都能在用户预设的时间内将系统状态精确稳定到原点的一个小邻域内，同时确保所有闭环信号的有界性。", "conclusion": "这项研究成功地将数据驱动方法的灵活性与基于李雅普诺夫分析的严谨性相结合。它为非线性系统提供了一种具有可量化性能和可预设稳定时间的HPC策略，展示了其在控制复杂动力学方面的巨大潜力。", "translation": "实现具有强非线性和参数不确定性的复杂系统的快速、时间确定性稳定是一个重大挑战。传统的基于模型的控制依赖于精确的系统模型，而纯粹的数据驱动方法往往缺乏形式化的稳定性保证，限制了它们在安全关键系统中的适用性。本文提出了一种新颖的控制框架，将数据驱动建模与基于模型的控制协同集成。该框架首先采用带控制的扩展动态模态分解 (EDMDc) 从数据中识别高维Koopman线性模型并量化其有界不确定性。随后，基于此数据驱动模型，合成了一种新颖的预设时间自适应反步 (PTAB) 控制器。该设计利用Koopman线性化的结构优势来系统地处理模型误差并规避传统反步法固有的“复杂性爆炸”问题。所提出的控制器通过在经典范德波尔振荡器上的仿真进行了验证。结果表明，无论初始条件如何，控制器都能在用户预设的时间内将系统状态精确稳定到原点的一个小邻域内，同时确保所有闭环信号的有界性。这项研究成功地将数据驱动方法的灵活性与基于李雅普诺夫分析的严谨性相结合。它为非线性系统提供了一种具有可量化性能和可预设稳定时间的HPC策略，展示了其在控制复杂动力学方面的巨大潜力。", "summary": "本文提出了一种创新的数据驱动预设时间控制框架，旨在解决复杂非线性系统在快速、时间确定性稳定方面的挑战。该框架首先利用EDMDc从数据中构建具有量化不确定性的高维Koopman线性模型，然后在此基础上设计了一种新颖的预设时间自适应反步 (PTAB) 控制器。该控制器利用Koopman线性化优势，有效处理模型误差并避免传统反步法的复杂性问题。仿真结果验证了该方法能在预设时间内将系统稳定到原点附近，并保证闭环信号有界。该研究成功结合了数据驱动的灵活性与李雅普诺夫分析的严谨性，为非线性系统提供了高性能、可量化且稳定时间可预设的控制策略。", "keywords": "数据驱动控制, 预设时间控制, Koopman算子, 自适应反步, 非线性系统", "comments": "该论文的创新点在于将数据驱动的Koopman操作子理论与模型基的自适应反步控制相结合，有效地解决了传统方法在处理强非线性系统时面临的模型依赖性强和稳定性难以保证的问题。通过量化Koopman模型的有界不确定性并将其融入到控制器设计中，提升了控制器的鲁棒性。此外，利用Koopman线性化规避了反步法的“复杂性爆炸”问题，使得该方法在实际应用中更具潜力。其在预设时间内实现稳定并保证闭环信号有界的能力，对于安全关键系统具有重要意义。"}}
{"id": "2507.02641", "title": "Pinching-Antenna-Assisted Index Modulation: Channel Modeling, Transceiver Design, and Performance Analysis", "authors": ["Shuaixin Yang", "Yijia Li", "Yue Xiao", "Yong Liang Guan", "Xianfu Lei", "Zhiguo Ding"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02641v1", "summary": "In this paper, a novel pinching-antenna assisted index modulation (PA-IM)\nscheme is proposed for improving the spectral efficiency without increasing the\nhardware complexity, where the information bits are conveyed not only by the\nconventional M-ary quadrature amplitude modulation (QAM) symbols but also by\nthe indices of pinching antenna (PA) position patterns. To realize the full\npotential of this scheme, this paper focuses on the comprehensive transceiver\ndesign, addressing key challenges in signal detection at the receiver and\nperformance optimization at thetransmitter. First, a comprehensive channel\nmodel is formulated for this architecture, which sophisticatedly integrates the\ndeterministic in-waveguide propagation effects with the stochastic nature of\nwireless channels, including both largescale path loss and small-scale fading.\nNext, to overcome the prohibitive complexity of optimal maximum likelihood (ML)\ndetection, a low-complexity box-optimized sphere decoding (BOSD) algorithm is\ndesigned, which adaptively prunes the search space whilst preserving optimal ML\nperformance. Furthermore, an analytical upper bound on the bit error rate (BER)\nis derived and validated by the simulations. Moreover, a new transmit precoding\nmethod is designed using manifold optimization, which minimizes the BER by\njointly optimizing the complex-valued precoding coefficients across the\nwaveguides for the sake of maximizing the minimum Euclidean distance of all\nreceived signal points. Finally, the simulation results demonstrate that the\nproposed PA-IM scheme attains a significant performance gain over its\nconventional counterparts and that the overall BER of the pinching-antenna\nsystem is substantially improved by the proposed precoding design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02641v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "夹持天线辅助的索引调制：信道建模、收发机设计与性能分析", "tldr": "本文提出了一种夹持天线辅助的索引调制（PA-IM）方案，通过信道建模、收发机设计和性能分析，显著提高了频谱效率和误码率性能。", "motivation": "为了在不增加硬件复杂度的前提下提高频谱效率。", "method": "提出了一种新型夹持天线辅助的索引调制（PA-IM）方案，通过QAM符号和夹持天线位置模式的索引来传输信息。为此，论文首先建立了综合的信道模型，结合了波导内传播确定性效应和无线信道随机性。接着，设计了一种低复杂度的盒优化球形解码（BOSD）算法，用于信号检测，该算法在保持最优ML性能的同时，自适应地修剪搜索空间。此外，推导并验证了误码率（BER）的分析上限。最后，设计了一种使用流形优化的新型发射预编码方法，通过联合优化波导上的复值预编码系数来最小化BER，以最大化所有接收信号点的最小欧几里得距离。", "result": "仿真结果表明，所提出的PA-IM方案比传统方案获得了显著的性能增益，并且所提出的预编码设计显著改善了夹持天线系统的整体误码率。", "conclusion": "本文提出的夹持天线辅助索引调制（PA-IM）方案，通过综合的信道建模、优化的收发机设计（包括低复杂度检测和新型预编码），在不增加硬件复杂度的前提下，显著提高了频谱效率和系统性能，特别是在误码率方面。", "translation": "在本文中，提出了一种新颖的夹持天线辅助索引调制（PA-IM）方案，旨在不增加硬件复杂度的情况下提高频谱效率，其中信息比特不仅通过传统的M-ary正交幅度调制（QAM）符号传递，还通过夹持天线（PA）位置模式的索引传递。为了充分发挥该方案的潜力，本文重点关注全面的收发机设计，解决了接收端信号检测和发射端性能优化的关键挑战。首先，为该架构建立了一个综合的信道模型，该模型巧妙地将波导内的确定性传播效应与无线信道的随机性（包括大规模路径损耗和小规模衰落）相结合。其次，为了克服最优最大似然（ML）检测的过高复杂度，设计了一种低复杂度的盒优化球形解码（BOSD）算法，该算法在保持最优ML性能的同时，自适应地修剪搜索空间。此外，推导并验证了误码率（BER）的分析上限。再者，设计了一种使用流形优化的新型发射预编码方法，通过联合优化波导上的复值预编码系数，以最大化所有接收信号点的最小欧几里得距离，从而最小化BER。最后，仿真结果表明，所提出的PA-IM方案比其传统对应方案获得了显著的性能增益，并且所提出的预编码设计显著改善了夹持天线系统的整体误码率。", "summary": "本文提出了一种新颖的夹持天线辅助索引调制（PA-IM）方案，旨在不增加硬件复杂度的情况下提高频谱效率。该方案通过QAM符号和夹持天线位置模式的索引来传输信息。为实现其潜力，论文详细阐述了收发机设计，包括建立综合信道模型、设计低复杂度的盒优化球形解码（BOSD）算法进行信号检测，以及开发一种基于流形优化的新型发射预编码方法以最小化误码率。仿真结果验证了PA-IM方案相比传统方案具有显著的性能提升，并且预编码设计能大幅改善系统误码率。", "keywords": "夹持天线, 索引调制, 信道建模, 收发机设计, 误码率", "comments": "本文创新性地提出了夹持天线辅助的索引调制方案，结合了波导内传播特性和传统无线信道特性，并通过低复杂度检测算法和流形优化预编码，有效解决了性能与复杂度之间的平衡问题。其在提高频谱效率和误码率方面的显著性能增益，预示了该技术在未来无线通信系统中的应用潜力。"}}
{"id": "2507.02445", "title": "IGDNet: Zero-Shot Robust Underexposed Image Enhancement via Illumination-Guided and Denoising", "authors": ["Hailong Yan", "Junjian Huang", "Tingwen Huang"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Transactions on Artificial Intelligence (TAI) on Oct.31, 2024", "url": "http://arxiv.org/abs/2507.02445v1", "summary": "Current methods for restoring underexposed images typically rely on\nsupervised learning with paired underexposed and well-illuminated images.\nHowever, collecting such datasets is often impractical in real-world scenarios.\nMoreover, these methods can lead to over-enhancement, distorting\nwell-illuminated regions. To address these issues, we propose IGDNet, a\nZero-Shot enhancement method that operates solely on a single test image,\nwithout requiring guiding priors or training data. IGDNet exhibits strong\ngeneralization ability and effectively suppresses noise while restoring\nillumination. The framework comprises a decomposition module and a denoising\nmodule. The former separates the image into illumination and reflection\ncomponents via a dense connection network, while the latter enhances\nnon-uniformly illuminated regions using an illumination-guided pixel adaptive\ncorrection method. A noise pair is generated through downsampling and refined\niteratively to produce the final result. Extensive experiments on four public\ndatasets demonstrate that IGDNet significantly improves visual quality under\ncomplex lighting conditions. Quantitative results on metrics like PSNR\n(20.41dB) and SSIM (0.860dB) show that it outperforms 14 state-of-the-art\nunsupervised methods. The code will be released soon.", "comment": "Submitted to IEEE Transactions on Artificial Intelligence (TAI) on\n  Oct.31, 2024", "pdf_url": "http://arxiv.org/pdf/2507.02445v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "IGDNet：通过光照引导和去噪的零样本鲁棒欠曝光图像增强", "tldr": "IGDNet是一种零样本方法，用于增强欠曝光图像并去噪，无需训练数据，且性能优于现有无监督方法。", "motivation": "当前欠曝光图像恢复方法依赖于监督学习和成对数据集，但数据收集不切实际，且可能导致过增强和良好照明区域失真。", "method": "提出IGDNet，一个零样本增强方法，仅需单张测试图像。框架包括：1. 分解模块，通过密集连接网络将图像分离为光照和反射分量；2. 去噪模块，使用光照引导的像素自适应校正方法增强非均匀光照区域。通过降采样生成噪声对并迭代优化以获得最终结果。", "result": "在四个公共数据集上的广泛实验表明，IGDNet在复杂光照条件下显著改善视觉质量。定量结果（PSNR 20.41dB，SSIM 0.860dB）显示其优于14种最先进的无监督方法。", "conclusion": "IGDNet提供了一种有效且无需训练数据的零样本欠曝光图像增强和去噪解决方案，在性能上超越了现有无监督方法。", "translation": "当前用于恢复欠曝光图像的方法通常依赖于有监督学习，使用成对的欠曝光和光照良好图像。然而，在实际场景中收集此类数据集通常是不切实际的。此外，这些方法可能导致过度增强，从而扭曲光照良好的区域。为了解决这些问题，我们提出了IGDNet，一种零样本增强方法，它仅对单个测试图像进行操作，无需引导先验或训练数据。IGDNet表现出强大的泛化能力，并在恢复光照的同时有效抑制噪声。该框架包括一个分解模块和一个去噪模块。前者通过密集连接网络将图像分离为光照和反射分量，而后者使用光照引导的像素自适应校正方法增强非均匀光照区域。通过降采样生成噪声对并迭代细化以产生最终结果。在四个公共数据集上进行的大量实验表明，IGDNet在复杂光照条件下显著改善了视觉质量。PSNR（20.41dB）和SSIM（0.860dB）等指标的定量结果表明，它优于14种最先进的无监督方法。代码将很快发布。", "summary": "IGDNet是一种新型的零样本欠曝光图像增强方法，解决了传统监督学习方法对成对数据依赖的问题以及过增强的缺陷。它通过分解和去噪模块，仅使用单张图像即可有效恢复光照并抑制噪声。实验证明，IGDNet在视觉质量和定量指标上均优于现有的无监督方法。", "keywords": "欠曝光图像增强, 零样本学习, 图像去噪, 光照引导, 图像分解", "comments": "IGDNet的创新之处在于其零样本能力，无需训练数据，这极大地提高了其在实际应用中的可行性。其结合光照引导和去噪的策略，有效解决了欠曝光图像增强中的两大挑战：光照恢复和噪声抑制，同时避免了过增强问题。"}}
{"id": "2507.02089", "title": "Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model", "authors": ["Xingtu Liu", "Lin F. Yang", "Sharan Vaswani"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02089v1", "summary": "We consider infinite-horizon $\\gamma$-discounted (linear) constrained Markov\ndecision processes (CMDPs) where the objective is to find a policy that\nmaximizes the expected cumulative reward subject to expected cumulative\nconstraints. Given access to a generative model, we propose to solve CMDPs with\na primal-dual framework that can leverage any black-box unconstrained MDP\nsolver. For linear CMDPs with feature dimension $d$, we instantiate the\nframework by using mirror descent value iteration\n(\\texttt{MDVI})~\\citep{kitamura2023regularization} an example MDP solver. We\nprovide sample complexity bounds for the resulting CMDP algorithm in two cases:\n(i) relaxed feasibility, where small constraint violations are allowed, and\n(ii) strict feasibility, where the output policy is required to exactly satisfy\nthe constraint. For (i), we prove that the algorithm can return an\n$\\epsilon$-optimal policy with high probability by using\n$\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^4\\epsilon^2}\\right)$ samples. We note\nthat these results exhibit a near-optimal dependence on both $d$ and\n$\\epsilon$. For (ii), we show that the algorithm requires\n$\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^6\\epsilon^2\\zeta^2}\\right)$ samples,\nwhere $\\zeta$ is the problem-dependent Slater constant that characterizes the\nsize of the feasible region. Finally, we instantiate our framework for tabular\nCMDPs and show that it can be used to recover near-optimal sample complexities\nin this setting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02089v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "具有生成模型的线性约束马尔可夫决策过程的样本复杂度界限", "tldr": "本文提出了一种基于原始-对偶框架的线性约束马尔可夫决策过程（CMDPs）求解方法，并给出了在松弛可行性和严格可行性两种情况下的样本复杂度界限，结果显示接近最优。", "motivation": "在具有累积约束的情况下，找到能够最大化预期累积奖励的策略是无限视野伽马折扣（线性）约束马尔可夫决策过程（CMDPs）中的一个关键问题。", "method": "作者提出了一种利用生成模型和任何黑盒无约束MDP求解器的原始-对偶框架来解决CMDPs。对于线性CMDPs，该框架通过使用镜像下降价值迭代（MDVI）作为MDP求解器进行实例化。研究分析了在松弛可行性（允许小量约束违反）和严格可行性（要求精确满足约束）两种情况下的样本复杂度。", "result": "在松弛可行性情况下，算法能以高概率返回一个$\\epsilon$-最优策略，所需的样本复杂度为$\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^4\\epsilon^2}\\right)$，该结果在$d$和$\\epsilon$上表现出接近最优的依赖关系。在严格可行性情况下，算法需要$\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^6\\epsilon^2\\zeta^2}\\right)$样本，其中$\\zeta$是依赖于问题的Slater常数。此外，该框架在表格CMDPs中也能恢复接近最优的样本复杂度。", "conclusion": "本文提出的原始-对偶框架能够有效解决线性CMDPs问题，并在两种可行性假设下提供了具体的样本复杂度界限，这些界限在某些关键参数上表现出接近最优的性能，并且该框架适用于表格CMDPs。", "translation": "我们考虑无限视野$\\gamma$-折扣（线性）约束马尔可夫决策过程（CMDPs），其目标是找到一个在预期累积约束下最大化预期累积奖励的策略。在可以访问生成模型的情况下，我们提出使用一个原始-对偶框架来解决CMDPs，该框架可以利用任何黑盒无约束MDP求解器。对于特征维度为$d$的线性CMDPs，我们通过使用镜像下降价值迭代（MDVI）作为MDP求解器来实例化该框架。我们为所得的CMDP算法在两种情况下提供了样本复杂度界限：(i) 松弛可行性，即允许小的约束违反；以及 (ii) 严格可行性，即输出策略需要精确满足约束。对于 (i)，我们证明了该算法可以通过使用$\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^4\\epsilon^2}\\right)$样本以高概率返回一个$\\epsilon$-最优策略。我们注意到这些结果在$d$和$\\epsilon$上都表现出接近最优的依赖关系。对于 (ii)，我们表明该算法需要$\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^6\\epsilon^2\\zeta^2}\\right)$样本，其中$\\zeta$是表征可行区域大小的、依赖于问题的Slater常数。最后，我们为表格CMDPs实例化了我们的框架，并表明它可以在这种设置下恢复接近最优的样本复杂度。", "summary": "本文研究了具有生成模型的线性约束马尔可夫决策过程（CMDPs），并提出了一种基于原始-对偶框架的求解方法，该方法可集成任何无约束MDP求解器。作者为所提出的CMDP算法在松弛可行性和严格可行性两种情况下推导了样本复杂度界限。结果表明，在松弛可行性下，算法实现了接近最优的样本复杂度$\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^4\\epsilon^2}\\right)$；在严格可行性下，样本复杂度为$\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^6\\epsilon^2\\zeta^2}\\right)$。此外，该框架在表格CMDPs中也表现出良好的性能，能够恢复接近最优的样本复杂度。", "keywords": "约束马尔可夫决策过程, 样本复杂度, 生成模型, 原始-对偶框架, 线性CMDPs", "comments": "本文提出了一种新颖的原始-对偶框架来解决线性约束MDPs，其创新之处在于能够灵活地利用现有的黑盒无约束MDP求解器。其重要性体现在为CMDPs提供了严格的样本复杂度理论保证，并且区分了松弛和严格可行性两种实际场景，给出了具体且接近最优的界限。这对于理解和设计高效的CMDP算法具有重要的理论和实践意义。"}}
{"id": "2507.02652", "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search", "authors": ["Jiajie Jin", "Xiaoxi Li", "Guanting Dong", "Yuyao Zhang", "Yutao Zhu", "Yang Zhao", "Hongjin Qian", "Zhicheng Dou"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2507.02652v1", "summary": "Complex information needs in real-world search scenarios demand deep\nreasoning and knowledge synthesis across diverse sources, which traditional\nretrieval-augmented generation (RAG) pipelines struggle to address effectively.\nCurrent reasoning-based approaches suffer from a fundamental limitation: they\nuse a single model to handle both high-level planning and detailed execution,\nleading to inefficient reasoning and limited scalability. In this paper, we\nintroduce HiRA, a hierarchical framework that separates strategic planning from\nspecialized execution. Our approach decomposes complex search tasks into\nfocused subtasks, assigns each subtask to domain-specific agents equipped with\nexternal tools and reasoning capabilities, and coordinates the results through\na structured integration mechanism. This separation prevents execution details\nfrom disrupting high-level reasoning while enabling the system to leverage\nspecialized expertise for different types of information processing.\nExperiments on four complex, cross-modal deep search benchmarks demonstrate\nthat HiRA significantly outperforms state-of-the-art RAG and agent-based\nsystems. Our results show improvements in both answer quality and system\nefficiency, highlighting the effectiveness of decoupled planning and execution\nfor multi-step information seeking tasks. Our code is available at\nhttps://github.com/ignorejjj/HiRA.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2507.02652v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "解耦规划与执行：一种用于深度搜索的分层推理框架", "tldr": "本文提出了HiRA，一个分层框架，通过解耦规划和执行来解决复杂深度搜索任务中传统RAG和单一模型方法的局限性，显著提升了答案质量和系统效率。", "motivation": "在现实世界的搜索场景中，复杂的信息需求需要跨来源的深度推理和知识合成，而传统检索增强生成（RAG）管道难以有效应对。当前基于推理的方法存在根本性限制：它们使用单一模型处理高层规划和详细执行，导致推理效率低下和可扩展性受限。", "method": "本文引入了HiRA，一个分层框架，将战略规划与专业执行分离。该方法将复杂的搜索任务分解为重点子任务，将每个子任务分配给配备外部工具和推理能力的特定领域代理，并通过结构化集成机制协调结果。这种分离防止了执行细节干扰高层推理，同时使系统能够利用专业知识处理不同类型的信息。", "result": "在四个复杂的跨模态深度搜索基准测试中，HiRA显著优于最先进的RAG和基于代理的系统。结果表明，在答案质量和系统效率方面都有所改进，突出了解耦规划和执行在多步信息寻求任务中的有效性。", "conclusion": "解耦规划与执行对于多步信息寻求任务是有效的，HiRA框架通过分离规划和执行，能够显著提升复杂深度搜索任务的性能和效率。", "translation": "现实世界搜索场景中复杂的资讯需求，需要跨越不同来源的深度推理和知识整合，这是传统检索增强生成（RAG）管道难以有效解决的问题。当前基于推理的方法存在一个根本性限制：它们使用单一模型来处理高层规划和详细执行，导致推理效率低下和可扩展性有限。本文介绍了HiRA，一个分层框架，它将战略规划与专业执行分离。我们的方法将复杂的搜索任务分解为重点子任务，将每个子任务分配给配备外部工具和推理能力的特定领域代理，并通过结构化集成机制协调结果。这种分离防止了执行细节干扰高层推理，同时使系统能够利用专业知识处理不同类型的信息处理。在四个复杂的跨模态深度搜索基准测试中，HiRA显著优于最先进的RAG和基于代理的系统。我们的结果表明，在答案质量和系统效率方面都有所改进，突出了解耦规划和执行在多步信息寻求任务中的有效性。我们的代码可在https://github.com/ignorejjj/HiRA获取。", "summary": "本文提出了HiRA，一个用于深度搜索的分层推理框架，旨在解决传统RAG和现有推理方法在处理复杂信息需求时的局限性。HiRA通过将高层规划与专业执行解耦，将复杂任务分解为子任务并分配给特定领域代理，从而实现高效的知识合成。实验证明，HiRA在答案质量和系统效率方面均优于现有先进系统，验证了解耦规划与执行在多步信息寻求任务中的有效性。", "keywords": "解耦规划, 分层推理, 深度搜索, 代理系统, 信息寻求", "comments": "本文提出的HiRA框架通过引入规划与执行解耦的创新范式，有效解决了现有RAG和单一模型推理方法在处理复杂深度搜索任务时的效率和可扩展性问题。其分层设计和利用领域特定代理的机制，展现了在多模态信息处理方面的巨大潜力，为未来的复杂AI系统设计提供了有价值的参考。"}}
{"id": "2507.02842", "title": "On the Structure of Replicable Hypothesis Testers", "authors": ["Anders Aamand", "Maryam Aliakbarpour", "Justin Y. Chen", "Shyam Narayanan", "Sandeep Silwal"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      Abstract abridged to meet arxiv requirements", "url": "http://arxiv.org/abs/2507.02842v1", "summary": "A hypothesis testing algorithm is replicable if, when run on two different\nsamples from the same distribution, it produces the same output with high\nprobability. This notion, defined by by Impagliazzo, Lei, Pitassi, and Sorell\n[STOC'22], can increase trust in testing procedures and is deeply related to\nalgorithmic stability, generalization, and privacy. We build general tools to\nprove lower and upper bounds on the sample complexity of replicable testers,\nunifying and quantitatively improving upon existing results.\n  We identify a set of canonical properties, and prove that any replicable\ntesting algorithm can be modified to satisfy these properties without worsening\naccuracy or sample complexity. A canonical replicable algorithm computes a\ndeterministic function of its input (i.e., a test statistic) and thresholds\nagainst a uniformly random value in $[0,1]$. It is invariant to the order in\nwhich the samples are received, and, if the testing problem is ``symmetric,''\nthen the algorithm is also invariant to the labeling of the domain elements,\nresolving an open question by Liu and Ye [NeurIPS'24]. We prove new lower\nbounds for uniformity, identity, and closeness testing by reducing to the case\nwhere the replicable algorithm satisfies these canonical properties.\n  We systematize and improve upon a common strategy for replicable algorithm\ndesign based on test statistics with known expectation and bounded variance.\nOur framework allow testers which have been extensively analyzed in the\nnon-replicable setting to be made replicable with minimal overhead. As direct\napplications of our framework, we obtain constant-factor optimal bounds for\ncoin testing and closeness testing and get replicability for free in a large\nparameter regime for uniformity testing.\n  We also give state-of-the-art bounds for replicable Gaussian mean testing,\nand, unlike prior work, our algorithm runs in polynomial time.", "comment": "Abstract abridged to meet arxiv requirements", "pdf_url": "http://arxiv.org/pdf/2507.02842v1", "cate": "cs.DS", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "关于可复制假设检验器的结构", "tldr": "本文研究可复制假设检验器的结构，通过引入规范属性和通用工具，在不牺牲准确性的前提下，提升了现有算法的效率和可信度，并解决了开放问题。", "motivation": "提高假设检验过程的信任度，并与算法稳定性、泛化和隐私性紧密相关。现有可复制性测试器的样本复杂度存在改进空间。", "method": "1. 构建通用工具来证明可复制检验器样本复杂度的上下限。 2. 识别一组规范属性，并证明任何可复制的检验算法都可以在不降低准确性或样本复杂度的情况下进行修改以满足这些属性。 3. 通过归约到满足规范属性的算法，证明了均匀性、同一性和接近度测试的新下限。 4. 系统化并改进了一种基于已知期望和有界方差的检验统计量的可复制算法设计策略。", "result": "1. 统一并定量改进了现有关于可复制检验器样本复杂度的结果。 2. 证明任何可复制算法都可以修改为满足规范属性，且不降低准确性或样本复杂度。 3. 解决了Liu和Ye提出的关于对称问题下算法对领域元素标签不变性的开放问题。 4. 为均匀性、同一性和接近度测试提供了新的下限。 5. 所提出的框架能以最小开销使非可复制检验器变得可复制。 6. 在硬币测试和接近度测试中获得了常数因子最优界限，并在均匀性测试的较大参数范围内免费获得可复制性。 7. 为可复制高斯均值测试提供了最先进的界限，且算法在多项式时间内运行。", "conclusion": "本文通过引入规范属性和通用工具，不仅统一并改进了现有结果，解决了开放问题，还为可复制假设检验器的设计提供了系统化的框架和高效的算法，显著提升了其在多种测试任务中的性能和实用价值。", "translation": "如果一个假设检验算法在来自同一分布的两个不同样本上运行时，以高概率产生相同的输出，那么它是可复制的。这个概念由Impagliazzo、Lei、Pitassi和Sorell在[STOC'22]中定义，可以增加对检验过程的信任，并且与算法稳定性、泛化和隐私性密切相关。我们构建了通用工具来证明可复制检验器样本复杂度的下限和上限，统一并定量改进了现有结果。\n我们识别了一组规范属性，并证明任何可复制的检验算法都可以在不降低准确性或样本复杂度的情况下进行修改以满足这些属性。一个规范的可复制算法计算其输入的确定性函数（即检验统计量），并与[0,1]中的均匀随机值进行阈值比较。它对样本接收的顺序是不变的，并且，如果检验问题是“对称的”，那么算法对领域元素的标记也是不变的，这解决了Liu和Ye在[NeurIPS'24]中提出的一个开放问题。我们通过归约到可复制算法满足这些规范属性的情况，证明了均匀性、同一性和接近度测试的新下限。\n我们系统化并改进了一种基于已知期望和有界方差的检验统计量的可复制算法设计策略。我们的框架允许在非可复制设置中广泛分析的检验器以最小开销变得可复制。作为我们框架的直接应用，我们获得了硬币测试和接近度测试的常数因子最优界限，并在均匀性测试的较大参数范围内免费获得可复制性。\n我们还为可复制高斯均值测试提供了最先进的界限，并且与以前的工作不同，我们的算法在多项式时间内运行。", "summary": "本文深入探讨了可复制假设检验器的结构，这是一种在不同样本上能高概率产生相同输出的算法。作者构建了通用工具来推导样本复杂度的上下限，并引入了一组规范属性，证明任何可复制算法都可被修改以满足这些属性而不牺牲性能，解决了开放问题。研究还系统化并改进了可复制算法设计策略，提出一个框架使得非可复制检验器能以最小开销实现可复制性。具体应用包括硬币测试、接近度测试和均匀性测试的常数因子最优界限，以及多项式时间运行的可复制高斯均值测试的最先进界限。", "keywords": "可复制假设检验, 样本复杂度, 规范属性, 算法稳定性, 统计测试", "comments": "这篇论文通过引入“规范属性”的概念，为可复制假设检验算法的结构提供了深刻的理解和理论基础。其创新点在于证明了任何可复制算法都可以被规范化，且不影响性能，这极大地简化了可复制算法的设计与分析。此外，它通过系统化的框架，使得现有非可复制算法能够便捷地转化为可复制版本，具有很高的实用价值。解决开放问题和获得多项式时间的最优界限也彰显了其理论和实践贡献。"}}
{"id": "2507.02407", "title": "Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability", "authors": ["Mark Atta Mensah", "Isaac Wiafe", "Akon Ekpezu", "Justice Kwame Appati", "Jamal-Deen Abdulai", "Akosua Nyarkoa Wiafe-Akenten", "Frank Ernest Yeboah", "Gifty Odame"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This version has been reviewed and accepted for presentation at the Future Technologies Conference (FTC) 2025, to be held on 6 & 7 November 2025 in Munich, Germany. 17 pages, 4 figures, 1 table", "url": "http://arxiv.org/abs/2507.02407v1", "summary": "Most existing automatic speech recognition (ASR) research evaluate models\nusing in-domain datasets. However, they seldom evaluate how they generalize\nacross diverse speech contexts. This study addresses this gap by benchmarking\nseven Akan ASR models built on transformer architectures, such as Whisper and\nWav2Vec2, using four Akan speech corpora to determine their performance. These\ndatasets encompass various domains, including culturally relevant image\ndescriptions, informal conversations, biblical scripture readings, and\nspontaneous financial dialogues. A comparison of the word error rate and\ncharacter error rate highlighted domain dependency, with models performing\noptimally only within their training domains while showing marked accuracy\ndegradation in mismatched scenarios. This study also identified distinct error\nbehaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned\nWhisper Akan models led to more fluent but potentially misleading transcription\nerrors, Wav2Vec2 produced more obvious yet less interpretable outputs when\nencountering unfamiliar inputs. This trade-off between readability and\ntransparency in ASR errors should be considered when selecting architectures\nfor low-resource language (LRL) applications. These findings highlight the need\nfor targeted domain adaptation techniques, adaptive routing strategies, and\nmultilingual training frameworks for Akan and other LRLs.", "comment": "This version has been reviewed and accepted for presentation at the\n  Future Technologies Conference (FTC) 2025, to be held on 6 & 7 November 2025\n  in Munich, Germany. 17 pages, 4 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.02407v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "阿坎语ASR模型在特定领域数据集上的基准测试：性能、可扩展性和适应性的比较评估", "tldr": "本研究通过跨领域数据集基准测试了七种阿坎语ASR模型，发现模型存在域依赖性，并揭示了Whisper和Wav2Vec2在错误行为上的权衡。", "motivation": "现有ASR研究通常仅使用域内数据集评估模型，很少评估模型在不同语音上下文中的泛化能力，本研究旨在填补这一空白。", "method": "本研究使用四种阿坎语语音语料库（涵盖图像描述、非正式对话、圣经阅读和金融对话等领域）对七种基于Transformer架构的阿坎语ASR模型（如Whisper和Wav2Vec2）进行基准测试，并比较其词错误率和字符错误率。", "result": "结果显示模型存在域依赖性，在训练域内表现最佳，但在不匹配场景中准确性显著下降。研究还发现Whisper模型产生更流畅但可能误导性的转录错误，而Wav2Vec2在遇到不熟悉输入时产生更明显但难以解释的输出。", "conclusion": "在为低资源语言应用选择架构时，应考虑ASR错误中可读性和透明度之间的权衡。这些发现强调了阿坎语及其他低资源语言对目标域适应技术、自适应路由策略和多语言训练框架的需求。", "translation": "大多数现有的自动语音识别（ASR）研究使用域内数据集评估模型。然而，它们很少评估模型在不同语音上下文中的泛化能力。本研究通过使用四种阿坎语语音语料库（涵盖文化相关图像描述、非正式对话、圣经阅读和自发金融对话等各种领域），对七种基于Transformer架构（如Whisper和Wav2Vec2）构建的阿坎语ASR模型进行基准测试，以确定它们的性能，从而弥补了这一空白。词错误率和字符错误率的比较突出了域依赖性，模型仅在其训练域内表现最佳，而在不匹配的场景中显示出明显的准确性下降。本研究还发现了Whisper和Wav2Vec2架构之间不同的错误行为。微调后的Whisper阿坎语模型导致了更流畅但可能误导性的转录错误，而Wav2Vec2在遇到不熟悉输入时产生了更明显但难以解释的输出。在为低资源语言（LRL）应用选择架构时，应考虑ASR错误中可读性和透明度之间的这种权衡。这些发现强调了阿坎语及其他低资源语言对目标域适应技术、自适应路由策略和多语言训练框架的需求。", "summary": "本研究旨在解决现有ASR模型在跨领域泛化能力评估方面的不足。通过对七种基于Transformer的阿坎语ASR模型（包括Whisper和Wav2Vec2）在四个不同领域的阿坎语数据集上进行基准测试，研究发现模型存在显著的域依赖性，导致在训练域外性能大幅下降。此外，研究还揭示了Whisper和Wav2Vec2在错误行为上的差异，即Whisper倾向于生成流畅但可能误导性的错误，而Wav2Vec2则产生明显但难以解释的输出。这些结果强调了在低资源语言ASR应用中，可读性和透明度之间权衡的重要性，并指出了开发域适应技术和多语言训练框架的必要性。", "keywords": "阿坎语ASR, 域适应, Transformer, Whisper, Wav2Vec2, 低资源语言", "comments": "这项研究通过跨领域基准测试，揭示了当前ASR模型在低资源语言（如阿坎语）上的域泛化能力不足，并深入分析了不同Transformer架构（Whisper和Wav2Vec2）的独特错误模式，这对于低资源语言的ASR系统开发具有重要指导意义。其创新之处在于强调了“可读性与透明度”的错误权衡，为未来模型选择和改进提供了新的视角。"}}
{"id": "2507.02732", "title": "Classification by Separating Hypersurfaces: An Entropic Approach", "authors": ["Argimiro Arratia", "Mahmoud El Daou", "Henryk Gzyl"], "categories": ["cs.LG", "cs.IT", "math.IT", "physics.data-an", "stat.ML", "90C05, 90C25, 90C47, 90C52, 68T01, 68T05, 68T07, 68T20, 68W01"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      15 pages, 10 tables, 4 figures", "url": "http://arxiv.org/abs/2507.02732v1", "summary": "We consider the following classification problem: Given a population of\nindividuals characterized by a set of attributes represented as a vector in\n${\\mathbb R}^N$, the goal is to find a hyperplane in ${\\mathbb R}^N$ that\nseparates two sets of points corresponding to two distinct classes. This\nproblem, with a history dating back to the perceptron model, remains central to\nmachine learning. In this paper we propose a novel approach by searching for a\nvector of parameters in a bounded $N$-dimensional hypercube centered at the\norigin and a positive vector in ${\\mathbb R}^M$, obtained through the\nminimization of an entropy-based function defined over the space of unknown\nvariables. The method extends to polynomial surfaces, allowing the separation\nof data points by more complex decision boundaries. This provides a robust\nalternative to traditional linear or quadratic optimization techniques, such as\nsupport vector machines and gradient descent. Numerical experiments demonstrate\nthe efficiency and versatility of the method in handling diverse classification\ntasks, including linear and non-linear separability.", "comment": "15 pages, 10 tables, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.02732v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过分离超曲面进行分类：一种熵方法", "tldr": "本文提出了一种基于熵函数最小化的新颖分类方法，通过搜索参数向量来寻找分离超曲面，能够处理线性和非线性可分数据，并作为传统优化技术的鲁棒替代方案。", "motivation": "该论文旨在解决机器学习中一个核心的分类问题：在N维空间中找到一个超平面或更复杂的决策边界来分离两个不同类别的数据点。这个问题可以追溯到感知器模型，至今仍是机器学习的核心挑战。", "method": "本文提出了一种新颖的分类方法，通过在以原点为中心的有界N维超立方体中搜索参数向量，并在${\\mathbb R}^M$中搜索一个正向量，通过最小化定义在未知变量空间上的基于熵的函数来获得。该方法可扩展到多项式曲面，允许通过更复杂的决策边界分离数据点。", "result": "数值实验证明了该方法在处理包括线性和非线性可分性在内的各种分类任务中的效率和多功能性。", "conclusion": "该研究提出了一种基于熵最小化的分类新方法，能够通过超曲面有效分离数据，并为传统的线性或二次优化技术（如支持向量机和梯度下降）提供了一种鲁棒的替代方案。", "translation": "我们考虑以下分类问题：给定一个由一组属性（在 ${\\mathbb R}^N$ 中表示为向量）表征的个体群体，目标是在 ${\\mathbb R}^N$ 中找到一个超平面，将对应于两个不同类别的两组点分离。这个问题可以追溯到感知器模型，在机器学习中仍然是核心问题。在本文中，我们提出了一种新颖的方法，通过在以原点为中心的有界 N 维超立方体中搜索参数向量，并在 ${\\mathbb R}^M$ 中搜索一个正向量，该向量通过最小化定义在未知变量空间上的基于熵的函数获得。该方法可扩展到多项式曲面，允许通过更复杂的决策边界分离数据点。这为传统的线性或二次优化技术（如支持向量机和梯度下降）提供了一种鲁棒的替代方案。数值实验证明了该方法在处理包括线性和非线性可分性在内的各种分类任务中的效率和多功能性。", "summary": "本文提出了一种基于熵函数最小化的新颖分类方法，用于在N维空间中寻找分离超曲面以区分不同类别的数据点。该方法通过在有界超立方体中搜索参数向量来获得，并可扩展到多项式曲面，从而能够处理更复杂的决策边界。数值实验表明，该方法在处理线性和非线性分类任务方面具有高效性和多功能性，为支持向量机和梯度下降等传统优化技术提供了一种鲁棒的替代方案。", "keywords": "分类, 超曲面, 熵, 机器学习, 决策边界", "comments": "这项工作通过引入一种基于熵最小化的新颖方法来解决经典的分类问题，具有创新性。与传统的线性或二次优化方法相比，其能够处理更复杂的决策边界（多项式曲面）并表现出鲁棒性，是其重要优势。该方法在处理线性和非线性可分数据方面的通用性也值得关注。"}}
{"id": "2507.02607", "title": "Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures", "authors": ["Frida Sundfeldt", "Bianca Widstam", "Mahshid Helali Moghadam", "Kuo-Yun Liang", "Anders Vesterberg"], "categories": ["cs.CR", "cs.LG", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02607v1", "summary": "The digital evolution of connected vehicles and the subsequent security risks\nemphasize the critical need for implementing in-vehicle cyber security measures\nsuch as intrusion detection and response systems. The continuous advancement of\nattack scenarios further highlights the need for adaptive detection mechanisms\nthat can detect evolving, unknown, and complex threats. The effective use of\nML-driven techniques can help address this challenge. However, constraints on\nimplementing diverse attack scenarios on test vehicles due to safety, cost, and\nethical considerations result in a scarcity of data representing attack\nscenarios. This limitation necessitates alternative efficient and effective\nmethods for generating high-quality attack-representing data. This paper\npresents a context-aware attack data generator that generates attack inputs and\ncorresponding in-vehicle network log, i.e., controller area network (CAN) log,\nrepresenting various types of attack including denial of service (DoS), fuzzy,\nspoofing, suspension, and replay attacks. It utilizes parameterized attack\nmodels augmented with CAN message decoding and attack intensity adjustments to\nconfigure the attack scenarios with high similarity to real-world scenarios and\npromote variability. We evaluate the practicality of the generated\nattack-representing data within an intrusion detection system (IDS) case study,\nin which we develop and perform an empirical evaluation of two deep neural\nnetwork IDS models using the generated data. In addition to the efficiency and\nscalability of the approach, the performance results of IDS models, high\ndetection and classification capabilities, validate the consistency and\neffectiveness of the generated data as well. In this experience study, we also\nelaborate on the aspects influencing the fidelity of the data to real-world\nscenarios and provide insights into its application.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02607v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "缓解攻击数据稀缺性：SCANIA在增强车载网络安全措施方面的经验", "tldr": "本文介绍了一种上下文感知的攻击数据生成器，用于为车载入侵检测系统（IDS）开发生成高质量的攻击数据，以解决数据稀缺性问题。", "motivation": "互联汽车的数字化发展带来了网络安全风险，需要车载网络安全措施，尤其是入侵检测和响应系统。机器学习驱动的IDS需要多样化的攻击数据，但由于安全、成本和伦理考虑，在测试车辆上实施多样化攻击场景存在数据稀缺性问题。因此，需要高效有效的方法来生成高质量的攻击代表数据。", "method": "本文提出了一种上下文感知的攻击数据生成器，用于生成攻击输入和相应的车载网络（CAN）日志，代表各种类型的攻击（如DoS、模糊、欺骗、挂起和重放攻击）。它利用参数化攻击模型，结合CAN消息解码和攻击强度调整，以高相似度配置攻击场景并促进变异性。", "result": "在IDS案例研究中评估了所生成攻击代表数据的实用性，并使用生成的数据开发和经验评估了两个深度神经网络IDS模型。结果表明该方法具有效率和可扩展性，IDS模型具有高检测和分类能力，验证了生成数据的一致性和有效性。", "conclusion": "本研究表明，所生成的攻击数据对于开发车载入侵检测系统是有效且一致的，有助于缓解攻击数据稀缺性问题。研究还阐述了影响数据与现实场景保真度的方面，并提供了其应用的见解。", "translation": "互联汽车的数字化演进及其随之而来的安全风险，凸显了实施车载网络安全措施（如入侵检测和响应系统）的迫切需求。攻击场景的持续发展进一步强调了对能够检测不断演变、未知和复杂威胁的自适应检测机制的需求。机器学习驱动技术的有效利用可以帮助应对这一挑战。然而，由于安全、成本和伦理方面的考虑，在测试车辆上实施多样化攻击场景受到限制，导致代表攻击场景的数据稀缺。这种限制要求采用替代的、高效且有效的方法来生成高质量的攻击代表数据。本文提出了一种上下文感知的攻击数据生成器，用于生成攻击输入和相应的车载网络日志，即控制器局域网（CAN）日志，代表各种类型的攻击，包括拒绝服务（DoS）、模糊、欺骗、挂起和重放攻击。它利用参数化攻击模型，结合CAN消息解码和攻击强度调整，以高相似度配置攻击场景并促进变异性。我们在一个入侵检测系统（IDS）案例研究中评估了所生成攻击代表数据的实用性，在该研究中，我们使用生成的数据开发并进行了两个深度神经网络IDS模型的经验评估。除了该方法的效率和可扩展性外，IDS模型的性能结果，即高检测和分类能力，也验证了生成数据的一致性和有效性。在这项经验研究中，我们还详细阐述了影响数据与现实场景保真度的方面，并提供了其应用的见解。", "summary": "本文旨在解决开发机器学习驱动的入侵检测系统（IDS）所需车载攻击数据稀缺的问题。它介绍了SCANIA的上下文感知攻击数据生成器，该生成器能够创建高质量、多样化的攻击输入和相应的CAN日志，涵盖多种攻击类型。通过利用参数化模型和强度调整，生成的数据能够高度模拟真实世界场景。对深度神经网络IDS模型的实证评估证明了所生成数据的效率、可扩展性和有效性，从而验证了其在增强车载网络安全措施方面的实用性。", "keywords": "车载网络安全, 攻击数据生成, 入侵检测系统, CAN总线, 机器学习", "comments": "本文的创新之处在于提出了一种上下文感知的车载网络攻击数据生成器，有效解决了车载网络安全领域中攻击数据稀缺的关键问题。通过生成高保真、多样化的模拟攻击数据，该方法极大地促进了机器学习驱动的入侵检测系统（IDS）的开发和测试，降低了真实攻击测试的风险和成本。其重要性体现在为提升互联汽车的网络安全提供了实用的解决方案。"}}
{"id": "2507.02578", "title": "Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems", "authors": ["Zoe Pfister"], "categories": ["cs.SE", "cs.HC", "D.2.1"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Copyright 2025 IEEE. Accepted for publication in: 2025 IEEE 33nd International Requirements Engineering Conference (RE), Doctor Symposium Paper, 5 pages", "url": "http://arxiv.org/abs/2507.02578v1", "summary": "Adaptive Cyber-Physical Systems (CPS) are systems that integrate both\nphysical and computational capabilities, which can adjust in response to\nchanging parameters. Furthermore, they increasingly incorporate human-machine\ncollaboration, allowing them to benefit from the individual strengths of humans\nand machines. Human-Machine Teaming (HMT) represents the most advanced paradigm\nof human-machine collaboration, envisioning seamless teamwork between humans\nand machines. However, achieving effective and seamless HMT in adaptive CPS is\nchallenging. While adaptive CPS already benefit from feedback loops such as\nMAPE-K, there is still a gap in integrating humans into these feedback loops\ndue to different operational cadences of humans and machines. Further, HMT\nrequires constant monitoring of human operators, collecting potentially\nsensitive information about their actions and behavior. Respecting the privacy\nand human values of the actors of the CPS is crucial for the success of\nhuman-machine teams. This research addresses these challenges by: (1)\ndeveloping novel methods and processes for integrating HMT into adaptive CPS,\nfocusing on human-machine interaction principles and their incorporation into\nadaptive feedback loops found in CPS, and (2) creating frameworks for\nintegrating, verifying, and validating ethics and human values throughout the\nsystem lifecycle, starting from requirements engineering.", "comment": "Copyright 2025 IEEE. Accepted for publication in: 2025 IEEE 33nd\n  International Requirements Engineering Conference (RE), Doctor Symposium\n  Paper, 5 pages", "pdf_url": "http://arxiv.org/pdf/2507.02578v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "自适应信息物理系统中人机协作与伦理考量", "tldr": "本研究旨在解决自适应信息物理系统中人机协作的挑战，特别是将人类整合到反馈循环中以及在敏感数据收集中的伦理问题，通过开发新方法和伦理框架来实现。", "motivation": "自适应信息物理系统（CPS）日益融入人机协作，其中人机协同（HMT）是最先进的范式。然而，在自适应CPS中实现有效和无缝的HMT面临挑战，主要体现在：1) 由于人机操作节奏不同，将人类整合到反馈循环中存在空白；2) HMT需要持续监控人类操作员，这涉及收集潜在敏感信息，因此尊重隐私和人类价值观至关重要。", "method": "本研究通过以下方法解决挑战：1) 开发新颖的方法和流程，将人机协同（HMT）整合到自适应信息物理系统（CPS）中，重点关注人机交互原则及其在CPS自适应反馈循环中的整合；2) 创建框架，用于在整个系统生命周期（从需求工程开始）中整合、验证和确认伦理和人类价值观。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "自适应信息物理系统（CPS）是集成了物理和计算能力，并能响应参数变化进行调整的系统。此外，它们越来越多地融入人机协作，使其能够受益于人类和机器各自的优势。人机协同（HMT）代表了人机协作最先进的范式，设想人类和机器之间实现无缝团队合作。然而，在自适应CPS中实现有效和无缝的HMT具有挑战性。虽然自适应CPS已经受益于MAPE-K等反馈循环，但由于人类和机器操作节奏的不同，将人类整合到这些反馈循环中仍然存在空白。此外，HMT需要持续监控人类操作员，收集关于其行为和举止的潜在敏感信息。尊重CPS参与者的隐私和人类价值观对于人机团队的成功至关重要。本研究通过以下方式应对这些挑战：(1) 开发新颖的方法和流程，将HMT整合到自适应CPS中，重点关注人机交互原则及其在CPS中自适应反馈循环中的结合；(2) 创建框架，用于在整个系统生命周期（从需求工程开始）中整合、验证和确认伦理和人类价值观。", "summary": "本研究关注自适应信息物理系统（CPS）中人机协作（HMT）面临的核心挑战。主要问题在于将人类有效整合到系统的反馈循环中，并解决在监控人类操作员时涉及隐私和伦理价值观的敏感信息收集。为应对这些挑战，研究提出两方面工作：一是开发新方法和流程，将HMT原则融入CPS的自适应反馈循环；二是构建框架，以便在系统开发的全生命周期中整合、验证和确认伦理与人类价值观，从需求工程阶段即开始考量。", "keywords": "自适应信息物理系统, 人机协作, 人机协同, 伦理考量, 反馈循环", "comments": "该论文着重解决了自适应信息物理系统（CPS）中人机协作（HMT）的关键挑战，不仅关注技术整合问题（如将人类纳入反馈循环），还创新性地强调了伦理和隐私考量。其重要性在于，随着AI和自动化系统日益复杂，确保人机和谐共存并尊重人类价值观是未来发展的基石。此研究为构建更安全、更可信赖的智能系统提供了理论和方法支持，具有前瞻性和现实意义。"}}
{"id": "2507.02438", "title": "MISC: Minimal Intervention Shared Control with Guaranteed Safety under Non-Convex Constraints", "authors": ["Shivam Chaubey", "Francesco Verdoja", "Shankar Deka", "Ville Kyrki"], "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.02438v1", "summary": "Shared control combines human intention with autonomous decision-making, from\nlow-level safety overrides to high-level task guidance, enabling systems that\nadapt to users while ensuring safety and performance. This enhances task\neffectiveness and user experience across domains such as assistive robotics,\nteleoperation, and autonomous driving. However, existing shared control\nmethods, based on e.g. Model Predictive Control, Control Barrier Functions, or\nlearning-based control, struggle with feasibility, scalability, or safety\nguarantees, particularly since the user input is unpredictable.\n  To address these challenges, we propose an assistive controller framework\nbased on Constrained Optimal Control Problem that incorporates an\noffline-computed Control Invariant Set, enabling online computation of control\nactions that ensure feasibility, strict constraint satisfaction, and minimal\noverride of user intent. Moreover, the framework can accommodate structured\nclass of non-convex constraints, which are common in real-world scenarios. We\nvalidate the approach through a large-scale user study with 66\nparticipants--one of the most extensive in shared control research--using a\ncomputer game environment to assess task load, trust, and perceived control, in\naddition to performance. The results show consistent improvements across all\nthese aspects without compromising safety and user intent.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.02438v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MISC：非凸约束下具有安全保障的最小干预共享控制", "tldr": "本文提出了一种名为MISC的共享控制框架，通过结合离线计算的控制不变集和约束最优控制问题，解决了现有方法在非凸约束下可行性、可伸缩性和安全保障方面的挑战，并在大规模用户研究中显示出显著改进，同时确保了安全性和用户意图。", "motivation": "现有的共享控制方法（如模型预测控制、控制障碍函数或基于学习的控制）在处理用户输入不可预测性时，难以保证可行性、可伸缩性或安全性，尤其是在存在非凸约束的现实世界场景中。", "method": "本文提出了一种名为MISC的辅助控制器框架，该框架基于约束最优控制问题（Constrained Optimal Control Problem），并结合了离线计算的控制不变集（Control Invariant Set）。这种方法能够在在线计算控制动作时，确保可行性、严格的约束满足以及对用户意图的最小干预。该框架还能够适应结构化的非凸约束。", "result": "通过一项包含66名参与者的大规模用户研究，在计算机游戏环境中评估了任务负荷、信任、感知控制和性能。结果显示，在不损害安全性和用户意图的前提下，所有这些方面都得到了持续改进。", "conclusion": "所提出的共享控制框架在非凸约束下，能够有效提升用户体验、降低任务负荷、增加信任和感知控制，同时确保安全性和最小干预，成功解决了现有方法的局限性。", "translation": "共享控制将人类意图与自主决策相结合，从低级安全覆盖到高级任务指导，使系统能够适应用户，同时确保安全性和性能。这提高了辅助机器人、远程操作和自动驾驶等领域中的任务效率和用户体验。然而，现有的共享控制方法，例如基于模型预测控制、控制障碍函数或基于学习的控制，在可行性、可伸缩性或安全保障方面存在困难，特别是因为用户输入是不可预测的。为了解决这些挑战，我们提出了一种基于约束最优控制问题的辅助控制器框架，该框架结合了离线计算的控制不变集，从而能够在线计算控制动作，确保可行性、严格的约束满足以及对用户意图的最小干预。此外，该框架可以适应在现实世界场景中常见的结构化非凸约束。我们通过一项包含66名参与者的大规模用户研究（这是共享控制研究中最广泛的研究之一）验证了该方法，使用计算机游戏环境评估任务负荷、信任和感知控制，以及性能。结果显示，在不损害安全性和用户意图的情况下，所有这些方面都得到了持续改进。", "summary": "本文提出了一种名为MISC的共享控制框架，旨在解决现有方法在非凸约束下可行性、可伸缩性和安全保障方面的不足。该框架基于约束最优控制问题并结合离线计算的控制不变集，能够在线生成确保可行性、严格约束满足和最小用户意图干预的控制动作。通过一项大规模用户研究验证，MISC在提升任务效率、用户体验、信任和感知控制方面表现出色，同时保持了安全性和用户意图。", "keywords": "共享控制, 非凸约束, 安全保障, 最小干预, 控制不变集", "comments": "这篇论文通过提出MISC框架，有效地解决了共享控制领域中一个核心挑战：在存在复杂非凸约束和不可预测用户输入的情况下，如何同时保证系统安全、性能和用户意图的最小干预。其创新点在于结合了约束最优控制和离线计算的控制不变集，并特别强调了对非凸约束的处理能力。大规模用户研究的验证增强了研究结果的说服力，为未来共享控制系统的设计提供了有价值的参考。"}}
{"id": "2507.02306", "title": "Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation", "authors": ["Ruican Zhong", "David W. McDonald", "Gary Hsieh"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02306v1", "summary": "Usability evaluation is crucial in human-centered design but can be costly,\nrequiring expert time and user compensation. In this work, we developed a\nmethod for synthetic heuristic evaluation using multimodal LLMs' ability to\nanalyze images and provide design feedback. Comparing our synthetic evaluations\nto those by experienced UX practitioners across two apps, we found our\nevaluation identified 73% and 77% of usability issues, which exceeded the\nperformance of 5 experienced human evaluators (57% and 63%). Compared to human\nevaluators, the synthetic evaluation's performance maintained consistent\nperformance across tasks and excelled in detecting layout issues, highlighting\npotential attentional and perceptual strengths of synthetic evaluation.\nHowever, synthetic evaluation struggled with recognizing some UI components and\ndesign conventions, as well as identifying across screen violations.\nAdditionally, testing synthetic evaluations over time and accounts revealed\nstable performance. Overall, our work highlights the performance differences\nbetween human and LLM-driven evaluations, informing the design of synthetic\nheuristic evaluations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02306v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "合成启发式评估：AI驱动与人工可用性评估的比较", "tldr": "本研究开发了一种利用多模态LLM进行合成启发式评估的方法，发现其在识别可用性问题方面优于人类专家，尤其在布局问题上表现出色，但在识别某些UI组件和跨屏幕违规方面存在不足。", "motivation": "可用性评估在以人为本的设计中至关重要，但成本高昂，需要专家时间和用户补偿。", "method": "开发了一种利用多模态LLM分析图像并提供设计反馈的合成启发式评估方法。将合成评估与经验丰富的UX从业者在两个应用程序上的评估进行比较。", "result": "合成评估识别了73%和77%的可用性问题，超过了5位经验丰富的人类评估员（57%和63%）的性能。合成评估在任务中保持一致的性能，并在检测布局问题方面表现出色。然而，合成评估在识别某些UI组件、设计约定以及跨屏幕违规方面存在困难。合成评估在不同时间和账户下的表现稳定。", "conclusion": "本研究强调了人与LLM驱动的评估之间的性能差异，为合成启发式评估的设计提供了信息。", "translation": "可用性评估在以人为本的设计中至关重要，但成本可能很高，需要专家时间和用户补偿。在这项工作中，我们开发了一种利用多模态大型语言模型（LLM）分析图像并提供设计反馈的能力进行合成启发式评估的方法。通过将我们的合成评估与经验丰富的用户体验（UX）从业者在两个应用程序上的评估进行比较，我们发现我们的评估识别了73%和77%的可用性问题，这超过了5位经验丰富的人类评估员的性能（57%和63%）。与人类评估员相比，合成评估的性能在不同任务中保持一致，并在检测布局问题方面表现出色，这突出了合成评估潜在的注意力和感知优势。然而，合成评估在识别某些UI组件和设计约定以及识别跨屏幕违规方面存在困难。此外，对合成评估进行长时间和跨账户测试显示其性能稳定。总的来说，我们的工作突出了人类评估和LLM驱动评估之间的性能差异，为合成启发式评估的设计提供了信息。", "summary": "本研究提出了一种基于多模态大型语言模型（LLM）的合成启发式评估方法，旨在降低传统可用性评估的成本。通过与人类专家评估进行对比，该方法在识别可用性问题方面表现出更高的召回率（73%-77%），尤其擅长检测布局问题，并展现出良好的稳定性。尽管在识别特定UI组件和跨屏幕违规方面存在局限性，但研究结果为未来合成启发式评估的设计提供了重要参考，揭示了AI驱动评估与人类评估在性能上的差异。", "keywords": "可用性评估, 合成启发式评估, 多模态LLM, 人机交互, 设计反馈", "comments": "这项研究的创新之处在于利用多模态LLM进行可用性启发式评估，为传统耗时耗资的评估方法提供了一种高效的替代方案。其重要性在于展示了AI在复杂认知任务中的潜力，尤其是在早期设计阶段快速发现可用性问题的能力。局限性在于AI在某些特定UI组件识别和跨屏幕上下文理解方面仍需提升，这提示了未来AI辅助设计工具的改进方向。"}}
{"id": "2507.02319", "title": "Iterated belief revision: from postulates to abilities", "authors": ["Paolo Liberatore"], "categories": ["cs.AI", "cs.LO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02319v1", "summary": "The belief revision field is opulent in new proposals and indigent in\nanalyses of existing approaches. Much work hinge on postulates, employed as\nsyntactic characterizations: some revision mechanism is equivalent to some\nproperties. Postulates constraint specific revision instances: certain\nrevisions update certain beliefs in a certain way. As an example, if the\nrevision is consistent with the current beliefs, it is incorporated with no\nother change. A postulate like this tells what revisions must do and neglect\nwhat they can do. Can they reach a certain state of beliefs? Can they reach all\npossible states of beliefs? Can they reach all possible states of beliefs from\nno previous belief? Can they reach a dogmatic state of beliefs, where\neverything not believed is impossible? Can they make two conditions equally\nbelieved? An application where every possible state of beliefs is sensible\nrequires each state of beliefs to be reachable. An application where conditions\nmay be equally believed requires such a belief state to be reachable. An\napplication where beliefs may become dogmatic requires a way to make them\ndogmatic. Such doxastic states need to be reached in a way or another. Not in\nspecific way, as dictated by a typical belief revision postulate. This is an\nability, not a constraint: the ability of being plastic, equating, dogmatic.\nAmnesic, correcting, believer, damascan, learnable are other abilities. Each\nrevision mechanism owns some of these abilities and lacks the others:\nlexicographic, natural, restrained, very radical, full meet, radical, severe,\nmoderate severe, deep severe, plain severe and deep severe revisions, each of\nthese revisions is proved to possess certain abilities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02319v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "迭代信念修正：从公设到能力", "tldr": "本文提出了一种新的信念修正分析方法，即从“能力”而非传统“公设”的角度来评估不同的修正机制，并证明了各种修正机制所拥有的特定能力。", "motivation": "信念修正领域现有方法分析不足，且传统公设主要关注修正机制“必须做什么”，而忽略了它们“能够做什么”，特别是忽略了修正机制能否达到特定信念状态的能力（如可塑性、等同性、教条性等）。", "method": "本文通过引入“能力”（如可塑性、等同性、教条性、失忆性、纠正性等）的概念，来分析和评估不同的信念修正机制。研究证明了多种具体的修正机制（如词典式、自然、受限、非常激进、完全满足、激进、严格、中度严格、深度严格、普通严格和深度严格修正）各自拥有某些能力而缺乏其他能力。", "result": "研究证明，不同的信念修正机制（如词典式、自然、受限、非常激进、完全满足、激进、严格、中度严格、深度严格、普通严格和深度严格修正）拥有特定的“能力”而缺乏其他能力。", "conclusion": "通过“能力”而非仅仅“公设”来分析信念修正机制，提供了一种新的、更全面的视角，揭示了不同修正机制在达成特定信念状态方面的潜力。", "translation": "信念修正领域在新的提议上非常丰富，但在现有方法的分析上却很贫乏。许多工作都围绕着公设，将其用作句法特征：某种修正机制等同于某些性质。公设限制了特定的修正实例：某些修正以某种方式更新某些信念。例如，如果修正与当前信念一致，则在不进行其他更改的情况下将其纳入。像这样的公设告诉我们修正必须做什么，却忽略了它们能做什么。它们能达到某种信念状态吗？它们能达到所有可能的信念状态吗？它们能从没有先验信念的情况下达到所有可能的信念状态吗？它们能达到一种教条的信念状态吗，即所有不被相信的事物都是不可能的？它们能使两个条件同样被相信吗？一个所有可能的信念状态都合理的应用程序要求每个信念状态都是可达的。一个条件可能同样被相信的应用程序要求这种信念状态是可达的。一个信念可能变得教条的应用程序要求有一种方法使它们变得教条。这些认识状态需要以某种方式达到。而不是以特定方式，正如典型的信念修正公设所规定的那样。这是一种能力，而不是约束：可塑性、等同性、教条性的能力。失忆性、纠正性、信仰者、大马士革式、可学习性是其他能力。每种修正机制都拥有其中一些能力，而缺乏其他能力：词典式、自然、受限、非常激进、完全满足、激进、严格、中度严格、深度严格、普通严格和深度严格修正，这些修正中的每一种都被证明拥有某些能力。", "summary": "本文针对信念修正领域现有分析不足的问题，提出了一种新的评估视角。传统方法侧重于公设（修正必须做什么），而本文则关注“能力”（修正能够做什么，例如达到特定信念状态）。文章探讨了可塑性、等同性、教条性等多种能力，并分析了不同的信念修正机制（如词典式、激进式等）所具备或缺乏的特定能力。", "keywords": "信念修正, 公设, 能力, 信念状态, 修正机制", "comments": "本文的创新之处在于将信念修正的分析视角从传统的“公设”（关注修正的约束和必须满足的性质）转向了“能力”（关注修正能够达到的信念状态和其潜力）。这种转变对于理解不同修正机制的实际应用场景和局限性具有重要意义，因为它强调了修正机制在实际应用中实现特定目标的能力，而非仅仅理论上的符合性。这为信念修正领域提供了一个新的分析框架，有助于更全面地评估现有和未来的修正理论。"}}
{"id": "2507.02584", "title": "Observer-Based Distributed Model Predictive Control for String-Stable Multi-vehicle Systems with Markovian Switching Topology", "authors": ["Wenwei Que", "Yang Li", "Lu Wang", "Wentao Liu", "Yougang Bian", "Manjiang Hu", "Yongfu Li"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      8 pages,7 figures,conference", "url": "http://arxiv.org/abs/2507.02584v1", "summary": "Switching communication topologies can cause instability in vehicle platoons,\nas vehicle information may be lost during the dynamic switching process. This\nhighlights the need to design a controller capable of maintaining the stability\nof vehicle platoons under dynamically changing topologies. However, capturing\nthe dynamic characteristics of switching topologies and obtaining complete\nvehicle information for controller design while ensuring stability remains a\nsignificant challenge. In this study, we propose an observer-based distributed\nmodel predictive control (DMPC) method for vehicle platoons under directed\nMarkovian switching topologies. Considering the stochastic nature of the\nswitching topologies, we model the directed switching communication topologies\nusing a continuous-time Markov chain. To obtain the leader vehicle's\ninformation for controller design, we develop a fully distributed adaptive\nobserver that can quickly adapt to the randomly switching topologies, ensuring\nthat the observed information is not affected by the dynamic topology switches.\nAdditionally, a sufficient condition is derived to guarantee the mean-square\nstability of the observer. Furthermore, we construct the DMPC terminal update\nlaw based on the observer and formulate a string stability constraint based on\nthe observed information. Numerical simulations demonstrate that our method can\nreduce tracking errors while ensuring string stability.", "comment": "8 pages,7 figures,conference", "pdf_url": "http://arxiv.org/pdf/2507.02584v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于观测器的马尔可夫切换拓扑下串稳定多车系统分布式模型预测控制", "tldr": "针对马尔可夫切换拓扑下多车系统的不稳定性问题，提出了一种基于观测器的分布式模型预测控制方法，确保了跟踪误差降低和串稳定性。", "motivation": "切换通信拓扑可能导致车队不稳定，因为在动态切换过程中车辆信息可能丢失。在动态变化的拓扑下设计能够保持车队稳定性的控制器是必要的，但捕捉切换拓扑的动态特性并获取完整的车辆信息以进行控制器设计同时确保稳定性仍然是一个重大挑战。", "method": "提出了一种在有向马尔可夫切换拓扑下的基于观测器的分布式模型预测控制（DMPC）方法。通过连续时间马尔可夫链对有向切换通信拓扑进行建模。开发了一个完全分布式的自适应观测器，以获取领航车辆信息并快速适应随机切换拓扑。推导了保证观测器均方稳定的充分条件。基于观测器构建了DMPC终端更新律，并基于观测信息制定了串稳定性约束。", "result": "数值模拟表明，该方法在确保串稳定性的同时，可以减少跟踪误差。", "conclusion": "Not mentioned in abstract", "translation": "切换通信拓扑可能导致车队不稳定，因为车辆信息在动态切换过程中可能会丢失。这突显了设计一种能够在动态变化的拓扑下保持车队稳定性的控制器的必要性。然而，捕捉切换拓扑的动态特性并在确保稳定性的同时为控制器设计获取完整的车辆信息仍然是一个重大挑战。在本研究中，我们提出了一种基于观测器的分布式模型预测控制（DMPC）方法，用于有向马尔可夫切换拓扑下的车队。考虑到切换拓扑的随机性，我们使用连续时间马尔可夫链对有向切换通信拓扑进行建模。为了获取领航车辆信息以进行控制器设计，我们开发了一个完全分布式的自适应观测器，该观测器可以快速适应随机切换拓扑，确保观测到的信息不受动态拓扑切换的影响。此外，推导了一个充分条件来保证观测器的均方稳定。此，我们基于观测器构建了DMPC终端更新律，并基于观测信息制定了串稳定性约束。数值模拟表明，我们的方法在确保串稳定性的同时可以减少跟踪误差。", "summary": "本研究针对马尔可夫切换拓扑下多车系统通信拓扑动态变化导致的不稳定性问题，提出了一种基于观测器的分布式模型预测控制（DMPC）方法。该方法通过连续时间马尔可夫链建模切换拓扑的随机性，并设计了自适应观测器以获取领航车辆信息，确保信息不受拓扑切换影响。同时，推导了观测器均方稳定性条件，并构建了基于观测信息的DMPC终端更新律和串稳定性约束。数值模拟验证了该方法能有效降低跟踪误差并保持串稳定性。", "keywords": "分布式模型预测控制, 马尔可夫切换拓扑, 观测器, 串稳定性, 多车系统", "comments": "该研究通过引入自适应观测器有效地解决了在马尔可夫切换拓扑下获取完整车辆信息和保持系统稳定性的挑战，特别是在信息可能丢失的情况下。其创新之处在于将观测器与DMPC结合，并考虑了拓扑的随机性，增强了多车系统在复杂通信环境下的鲁棒性和性能。对于自动驾驶和智能交通系统具有重要意义。"}}
{"id": "2507.02802", "title": "AREE-Based Decoupled Design of Hybrid Beamformers in mmWave XL-MIMO Systems", "authors": ["Jiazhe Li", "Nicolò Decarli", "Francesco Guidi", "Heng Dong", "Anna Guerra", "Alessandro Bazzi", "Zhuoming Li"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02802v1", "summary": "Hybrid beamforming has been widely employed in mmWave communications such as\nvehicular-to-everything (V2X) scenarios, as a compromise between hardware\ncomplexity and spectral efficiency. However, the inherent coupling between\nanalog and digital precoders in hybrid array architecture significantly limits\nthe computational and spectral efficiency of existing algorithms. To address\nthis issue, we propose an alternating residual error elimination (AREE)\nalgorithm, which decomposes the hybrid beamforming problem into two\nlow-dimensional subproblems, each exhibiting a favorable matrix structure that\nenables effective decoupling of analog and digital precoders from the matrix\nproduct formulation. These subproblems iteratively eliminate each other's\nresidual errors, driving the original problem toward the optimal hybrid\nbeamforming performance. The proposed initialization ensures rapid convergence,\nwhile a low-complexity geometric channel SVD algorithm is developed by\ntransforming the high-dimensional sparse channel into a low-dimensional\nequivalent, thereby simplifying the derivation of subproblems. Simulation\nresults demonstrate that the AREE algorithm effectively decouples analog and\ndigital precoders with low complexity, achieves fast convergence, and offers\nhigher spectral efficiency than existing beamforming methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02802v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于AREE的毫米波XL-MIMO系统中混合波束赋形器的解耦设计", "tldr": "本论文提出了一种名为AREE的算法，用于在毫米波XL-MIMO系统中解耦混合波束赋形器的模拟和数字预编码器，有效解决了现有算法的耦合问题，提高了计算和频谱效率。", "motivation": "混合波束赋形在毫米波通信中被广泛应用，但其模拟和数字预编码器之间固有的耦合严重限制了现有算法的计算和频谱效率。本研究旨在解决这一问题。", "method": "提出了一种交替残差误差消除（AREE）算法。该算法将混合波束赋形问题分解为两个低维子问题，每个子问题都具有有利的矩阵结构，能够有效解耦模拟和数字预编码器。这些子问题通过迭代消除彼此的残差误差，使原始问题趋向最优性能。此外，还开发了一种低复杂度的几何信道SVD算法，将高维稀疏信道转换为低维等效信道，简化了子问题的推导，并确保了快速收敛。", "result": "仿真结果表明，AREE算法能有效解耦模拟和数字预编码器，具有低复杂度、快速收敛的特点，并比现有波束赋形方法提供更高的频谱效率。", "conclusion": "所提出的AREE算法通过有效解耦模拟和数字预编码器，显著提高了毫米波XL-MIMO系统中混合波束赋形的计算和频谱效率，并展现出低复杂度、快速收敛和优于现有方法的性能。", "translation": "混合波束赋形作为硬件复杂度和频谱效率之间的折衷，已广泛应用于毫米波通信，例如车联网（V2X）场景。然而，混合阵列架构中模拟和数字预编码器之间固有的耦合严重限制了现有算法的计算和频谱效率。为了解决这个问题，我们提出了一种交替残差误差消除（AREE）算法，该算法将混合波束赋形问题分解为两个低维子问题，每个子问题都呈现出有利的矩阵结构，从而能够从矩阵乘积公式中有效解耦模拟和数字预编码器。这些子问题迭代消除彼此的残差误差，使原始问题趋向于最优的混合波束赋形性能。所提出的初始化确保了快速收敛，同时通过将高维稀疏信道转换为低维等效信道，开发了一种低复杂度的几何信道SVD算法，从而简化了子问题的推导。仿真结果表明，AREE算法能够以低复杂度有效解耦模拟和数字预编码器，实现快速收敛，并比现有波束赋形方法提供更高的频谱效率。", "summary": "本论文提出了一种名为AREE的新型算法，旨在解决毫米波XL-MIMO系统中混合波束赋形器中模拟和数字预编码器之间的耦合问题。AREE算法通过将问题分解为两个低维子问题并迭代消除残差误差来实现解耦，同时引入了一种低复杂度的几何信道SVD算法以加速收敛。仿真结果验证了AREE在解耦、低复杂度、快速收敛和高频谱效率方面的优越性。", "keywords": "混合波束赋形, 毫米波通信, XL-MIMO, 解耦设计, AREE算法", "comments": "本论文的创新点在于提出了AREE算法，通过巧妙的子问题分解和迭代残差消除机制，有效解决了混合波束赋形中模拟与数字预编码器之间的耦合难题。其引入的低复杂度几何信道SVD算法也为实际应用提供了便利。该研究对于提升毫米波XL-MIMO系统的计算和频谱效率具有重要意义。"}}
{"id": "2507.02671", "title": "Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs", "authors": ["Francesco Di Salvo", "Hanh Huyen My Nguyen", "Christian Ledig"], "categories": ["cs.LG", "cs.CV", "eess.IV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to MICCAI 2025", "url": "http://arxiv.org/abs/2507.02671v1", "summary": "Deep Learning (DL) has revolutionized medical imaging, yet its adoption is\nconstrained by data scarcity and privacy regulations, limiting access to\ndiverse datasets. Federated Learning (FL) enables decentralized training but\nsuffers from high communication costs and is often restricted to a single\ndownstream task, reducing flexibility. We propose a data-sharing method via\nDifferentially Private (DP) generative models. By adopting foundation models,\nwe extract compact, informative embeddings, reducing redundancy and lowering\ncomputational overhead. Clients collaboratively train a Differentially Private\nConditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware\ndata distribution, supporting diverse downstream tasks. Our approach, validated\nacross multiple feature extractors, enhances privacy, scalability, and\nefficiency, outperforming traditional FL classifiers while ensuring\ndifferential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings\nthan DP-CGAN while requiring $5{\\times}$ fewer parameters.", "comment": "Accepted to MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.02671v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于嵌入的差分隐私条件变分自编码器联邦数据共享", "tldr": "本文提出了一种通过差分隐私条件变分自编码器（DP-CVAE）进行联邦数据共享的方法，利用基础模型提取紧凑嵌入，以解决医疗影像领域的数据稀缺、隐私限制以及联邦学习的通信成本高和任务单一性问题，并在隐私、可扩展性和效率方面表现优异。", "motivation": "深度学习在医学影像领域受到数据稀缺和隐私法规的限制，导致难以获取多样化数据集。联邦学习（FL）虽然支持去中心化训练，但存在通信成本高且通常局限于单一下游任务的问题，降低了灵活性。", "method": "本文提出一种通过差分隐私（DP）生成模型实现数据共享的方法。通过采用基础模型提取紧凑且信息丰富的嵌入，以减少冗余和降低计算开销。客户端协作训练一个差分隐私条件变分自编码器（DP-CVAE），用于建模一个全局的、隐私保护的数据分布，从而支持多样化的下游任务。", "result": "该方法在多个特征提取器上进行了验证，结果显示其增强了隐私、可扩展性和效率。它优于传统的联邦学习分类器，并确保了差分隐私。此外，DP-CVAE比DP-CGAN生成更高保真度的嵌入，同时所需的参数减少了5倍。", "conclusion": "通过利用基础模型和差分隐私条件变分自编码器，本文提出的数据共享方法有效解决了医疗影像领域的数据稀缺和隐私问题，同时克服了传统联邦学习的局限性，在隐私、效率和可扩展性方面表现出显著优势。", "translation": "深度学习（DL）彻底改变了医学影像领域，但其应用受到数据稀缺和隐私法规的限制，这限制了对多样化数据集的访问。联邦学习（FL）实现了去中心化训练，但存在通信成本高的问题，并且通常仅限于单一的下游任务，降低了灵活性。我们提出了一种通过差分隐私（DP）生成模型进行数据共享的方法。通过采用基础模型，我们提取紧凑、信息丰富的嵌入，减少了冗余并降低了计算开销。客户端协作训练一个差分隐私条件变分自编码器（DP-CVAE），以建模一个全局的、隐私保护的数据分布，支持多样化的下游任务。我们的方法在多个特征提取器上进行了验证，增强了隐私、可扩展性和效率，优于传统的FL分类器，同时确保了差分隐私。此外，DP-CVAE比DP-CGAN产生更高保真度的嵌入，同时所需的参数减少了5倍。", "summary": "本文针对医疗影像领域中深度学习面临的数据稀缺、隐私限制以及联邦学习存在的通信成本高和任务单一等挑战，提出了一种基于差分隐私生成模型的数据共享方法。该方法利用基础模型提取紧凑的嵌入，并通过客户端协作训练一个差分隐私条件变分自编码器（DP-CVAE）来学习全局隐私保护数据分布，从而支持多种下游任务。实验结果表明，该方法在隐私、可扩展性和效率上均优于传统联邦学习分类器，并且DP-CVAE在生成高保真度嵌入的同时显著减少了参数量。", "keywords": "联邦学习, 差分隐私, 条件变分自编码器, 数据共享, 医疗影像", "comments": "这项研究通过结合基础模型和差分隐私条件变分自编码器，为解决医疗影像等领域的数据隐私和共享难题提供了一个创新方案。其在降低通信成本、支持多任务以及提高数据保真度方面的表现，展示了其在实际应用中的巨大潜力。与传统联邦学习相比，该方法在隐私保护和效率方面均有显著提升，是联邦学习和生成模型交叉领域的重要进展。"}}
{"id": "2507.02092", "title": "Energy-Based Transformers are Scalable Learners and Thinkers", "authors": ["Alexi Gladstone", "Ganesh Nanduru", "Md Mofijul Islam", "Peixuan Han", "Hyeonjeong Ha", "Aman Chadha", "Yilun Du", "Heng Ji", "Jundong Li", "Tariq Iqbal"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02092v1", "summary": "Inference-time computation techniques, analogous to human System 2 Thinking,\nhave recently become popular for improving model performances. However, most\nexisting approaches suffer from several limitations: they are modality-specific\n(e.g., working only in text), problem-specific (e.g., verifiable domains like\nmath and coding), or require additional supervision/training on top of\nunsupervised pretraining (e.g., verifiers or verifiable rewards). In this\npaper, we ask the question \"Is it possible to generalize these System 2\nThinking approaches, and develop models that learn to think solely from\nunsupervised learning?\" Interestingly, we find the answer is yes, by learning\nto explicitly verify the compatibility between inputs and\ncandidate-predictions, and then re-framing prediction problems as optimization\nwith respect to this verifier. Specifically, we train Energy-Based Transformers\n(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy\nvalue to every input and candidate-prediction pair, enabling predictions\nthrough gradient descent-based energy minimization until convergence. Across\nboth discrete (text) and continuous (visual) modalities, we find EBTs scale\nfaster than the dominant Transformer++ approach during training, achieving an\nup to 35% higher scaling rate with respect to data, batch size, parameters,\nFLOPs, and depth. During inference, EBTs improve performance with System 2\nThinking by 29% more than the Transformer++ on language tasks, and EBTs\noutperform Diffusion Transformers on image denoising while using fewer forward\npasses. Further, we find that EBTs achieve better results than existing models\non most downstream tasks given the same or worse pretraining performance,\nsuggesting that EBTs generalize better than existing approaches. Consequently,\nEBTs are a promising new paradigm for scaling both the learning and thinking\ncapabilities of models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02092v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "基于能量的Transformer是可扩展的学习者和思考者", "tldr": "本文提出了一种新型的基于能量的Transformer (EBTs)，它通过学习验证输入和候选预测之间的兼容性，并将预测问题重新定义为优化问题，从而实现了从无监督学习中学习思考的能力。EBTs在训练期间比现有方法扩展更快，并在推理时通过“系统2思考”显著提高性能，展现出更好的泛化能力。", "motivation": "现有的类人“系统2思考”推理计算技术存在局限性：它们通常是模态特定、问题特定，或需要额外的监督/训练。本文旨在解决“是否可以泛化这些系统2思考方法，并开发仅从无监督学习中学习思考的模型？”的问题。", "method": "本文提出了能量基Transformer (EBTs)，这是一种新型的能量基模型 (EBMs)。EBTs被训练来为每个输入和候选预测对分配一个能量值，并通过基于梯度下降的能量最小化直到收敛来进行预测。EBTs通过显式验证输入和候选预测之间的兼容性，并将预测问题重构为优化问题。", "result": "EBTs在离散（文本）和连续（视觉）模态上，训练期间比主流的Transformer++方法扩展更快，在数据、批次大小、参数、FLOPs和深度方面实现了高达35%的更高扩展率。在推理时，EBTs在语言任务上通过“系统2思考”比Transformer++的性能提高了29%，并且在图像去噪方面超越了Diffusion Transformers，同时使用了更少的前向传播。EBTs在大多数下游任务上，在相同或更差的预训练性能下，取得了比现有模型更好的结果。", "conclusion": "EBTs是扩展模型学习和思考能力的一个有前途的新范式，它们能够仅从无监督学习中学习思考，并在各种任务和模态上表现出卓越的扩展性和泛化能力。", "translation": "推理时计算技术，类似于人类的系统2思考，最近在提高模型性能方面变得流行。然而，大多数现有方法存在几个局限性：它们是模态特定的（例如，仅在文本中工作）、问题特定的（例如，可验证的领域如数学和编码），或者在无监督预训练之上需要额外的监督/训练（例如，验证器或可验证的奖励）。在本文中，我们提出了一个问题：“是否有可能泛化这些系统2思考方法，并开发仅从无监督学习中学习思考的模型？”有趣的是，我们发现答案是肯定的，通过学习明确验证输入和候选预测之间的兼容性，然后将预测问题重新定义为相对于该验证器的优化问题。具体来说，我们训练基于能量的Transformer（EBTs）——一种新型的基于能量的模型（EBMs）——为每个输入和候选预测对分配一个能量值，通过基于梯度下降的能量最小化直到收敛来实现预测。在离散（文本）和连续（视觉）模态上，我们发现EBTs在训练期间比主流的Transformer++方法扩展更快，在数据、批次大小、参数、FLOPs和深度方面实现了高达35%的更高扩展率。在推理时，EBTs通过系统2思考在语言任务上比Transformer++的性能提高了29%，并且EBTs在图像去噪方面超越了Diffusion Transformers，同时使用了更少的前向传播。此外，我们发现EBTs在大多数下游任务上，在相同或更差的预训练性能下，取得了比现有模型更好的结果，这表明EBTs比现有方法具有更好的泛化能力。因此，EBTs是扩展模型学习和思考能力的一个有前途的新范式。", "summary": "本文提出了一种新型的基于能量的Transformer (EBTs)，旨在解决现有“系统2思考”推理方法在模态、问题特异性及额外监督需求方面的局限。EBTs通过学习显式验证输入与候选预测的兼容性，并将预测建模为能量最小化优化问题，从而实现仅从无监督学习中学习思考。实验结果表明，EBTs在训练效率上优于Transformer++，并在推理时通过“系统2思考”显著提升性能，同时在多种模态和下游任务上展现出更强的泛化能力。", "keywords": "能量基Transformer, 系统2思考, 无监督学习, 可扩展性, 泛化能力", "comments": "本文提出EBTs作为一种新颖的范式，通过将预测重构为优化问题，并利用能量模型实现“系统2思考”，这在深度学习领域具有创新性。其核心优势在于能够仅从无监督学习中学习思考，并展现出优异的扩展性和泛化能力，解决了现有方法对特定模态、问题或额外监督的依赖。EBTs在训练效率和推理性能上的提升，以及在多模态任务上的成功应用，预示着它在未来通用人工智能发展中的巨大潜力。"}}
{"id": "2507.02496", "title": "Online Conformal Prediction with Efficiency Guarantees", "authors": ["Vaidehi Srinivas"], "categories": ["cs.LG", "cs.DS", "math.ST", "stat.ML", "stat.TH"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02496v1", "summary": "We study the problem of conformal prediction in a novel online framework that\ndirectly optimizes efficiency. In our problem, we are given a target\nmiscoverage rate $\\alpha > 0$, and a time horizon $T$. On each day $t \\le T$ an\nalgorithm must output an interval $I_t \\subseteq [0, 1]$, then a point $y_t \\in\n[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,\n$y_t \\in I_t$ on (close to) a $(1 - \\alpha)$-fraction of days, while\nmaintaining efficiency, that is, minimizing the average volume (length) of the\nintervals played. This problem is an online analogue to the problem of\nconstructing efficient confidence intervals.\n  We study this problem over arbitrary and exchangeable (random order) input\nsequences. For exchangeable sequences, we show that it is possible to construct\nintervals that achieve coverage $(1 - \\alpha) - o(1)$, while having length\nupper bounded by the best fixed interval that achieves coverage in hindsight.\nFor arbitrary sequences however, we show that any algorithm that achieves a\n$\\mu$-approximation in average length compared to the best fixed interval\nachieving coverage in hindsight, must make a multiplicative factor more\nmistakes than $\\alpha T$, where the multiplicative factor depends on $\\mu$ and\nthe aspect ratio of the problem. Our main algorithmic result is a matching\nalgorithm that can recover all Pareto-optimal settings of $\\mu$ and number of\nmistakes. Furthermore, our algorithm is deterministic and therefore robust to\nan adaptive adversary.\n  This gap between the exchangeable and arbitrary settings is in contrast to\nthe classical online learning problem. In fact, we show that no single\nalgorithm can simultaneously be Pareto-optimal for arbitrary sequences and\noptimal for exchangeable sequences. On the algorithmic side, we give an\nalgorithm that achieves the near-optimal tradeoff between the two cases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02496v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "在线保形预测及其效率保证", "tldr": "本文研究了一种新型在线保形预测框架，该框架直接优化效率。研究了可交换和任意输入序列的情况，揭示了两者之间的性能差距，并提出了能够实现最优权衡的算法。", "motivation": "本文旨在研究一种新颖的在线保形预测框架，该框架直接优化效率。其目标是在实现目标误覆盖率的同时，最小化预测区间的平均体积，这类似于构建高效置信区间的在线模拟问题。", "method": "作者在任意和可交换（随机顺序）输入序列上研究了这个问题。他们提供了理论分析，并提出了一种确定性的匹配算法，该算法可以恢复任意序列的所有帕累托最优设置，并在两种情况之间实现接近最优的权衡。", "result": "对于可交换序列，可以构建覆盖率达到 $(1 - \\alpha) - o(1)$ 的区间，其长度上限由事后实现覆盖率的最佳固定区间给出。然而，对于任意序列，任何在平均长度上相对于事后实现覆盖率的最佳固定区间达到 $\\mu$-近似的算法，其错误数必须是 $\\alpha T$ 的乘法因子倍，其中乘法因子取决于 $\\mu$ 和问题的长宽比。主要算法结果是一个匹配算法，可以恢复所有帕累托最优的 $\\mu$ 和错误数设置。此外，该算法是确定性的，因此对自适应对抗者具有鲁棒性。可交换和任意设置之间的这种差距与经典的在线学习问题形成对比。实际上，本文表明没有单一算法可以同时对任意序列帕累托最优，并对可交换序列最优。在算法方面，本文给出了一种在两种情况之间实现接近最优权衡的算法。", "conclusion": "本文确定了在线保形预测中可交换和任意输入序列之间的根本性差异，表明没有单一算法可以同时对两者都实现最优。论文提供了在这些设置中实现强大性能保证和最优权衡的算法。", "translation": "我们研究了一种新型在线框架中的保形预测问题，该框架直接优化效率。在我们的问题中，给定一个目标误覆盖率 $\\alpha > 0$ 和一个时间范围 $T$。在每天 $t \\le T$，算法必须输出一个区间 $I_t \\subseteq [0, 1]$，然后揭示一个点 $y_t \\in [0, 1]$。算法的目标是实现覆盖，即在（接近）$(1 - \\alpha)$ 的天数中，$y_t \\in I_t$，同时保持效率，即最小化所生成区间的平均体积（长度）。这个问题是构建高效置信区间的在线模拟。\\n我们研究了任意和可交换（随机顺序）输入序列上的这个问题。对于可交换序列，我们表明可以构建覆盖率达到 $(1 - \\alpha) - o(1)$ 的区间，其长度上限由事后实现覆盖率的最佳固定区间给出。然而，对于任意序列，我们表明任何在平均长度上相对于事后实现覆盖率的最佳固定区间达到 $\\mu$-近似的算法，其错误数必须是 $\\alpha T$ 的乘法因子倍，其中乘法因子取决于 $\\mu$ 和问题的长宽比。我们的主要算法结果是一个匹配算法，可以恢复所有帕累托最优的 $\\mu$ 和错误数设置。此外，我们的算法是确定性的，因此对自适应对抗者具有鲁棒性。\\n可交换和任意设置之间的这种差距与经典的在线学习问题形成对比。实际上，我们表明没有单一算法可以同时对任意序列帕累托最优，并对可交换序列最优。在算法方面，我们给出了一种在两种情况之间实现接近最优权衡的算法。", "summary": "本文提出了一种以效率优化为重点的在线保形预测框架。研究了可交换和任意输入序列下的问题，揭示了两者之间显著的性能差距。对于可交换序列，展示了接近最优的覆盖率和长度边界；而对于任意序列，则存在区间长度近似与误覆盖率之间的权衡。论文提出了一种确定性算法，该算法能够为任意序列实现帕累托最优解，并在两种情况之间提供接近最优的权衡，强调了单一算法无法同时对两者都达到最优。", "keywords": "在线保形预测, 效率, 覆盖率, 可交换序列, 任意序列", "comments": "本文揭示了在线保形预测中可交换序列和任意序列之间一个显著且非平凡的区别，这与经典的在线学习问题形成对比。所提出的确定性算法提供了鲁棒的性能保证和帕累托最优性，是对该领域的重要贡献。"}}
{"id": "2507.02428", "title": "A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages", "authors": ["Sumaya Ahmed Salihs", "Isaac Wiafe", "Jamal-Deen Abdulai", "Elikem Doe Atsakpo", "Gifty Ayoka", "Richard Cave", "Akon Obu Ekpezu", "Catherine Holloway", "Katrin Tomanek", "Fiifi Baffoe Payin Winful"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This version has been reviewed and accepted for presentation at the InterSpeech 2025 conference to be held in Rotterdam from 17 to 21 August. 5 pages and 3 tables", "url": "http://arxiv.org/abs/2507.02428v1", "summary": "This study presents an approach for collecting speech samples to build\nAutomatic Speech Recognition (ASR) models for impaired speech, particularly,\nlow-resource languages. It aims to democratize ASR technology and data\ncollection by developing a \"cookbook\" of best practices and training for\ncommunity-driven data collection and ASR model building. As a proof-of-concept,\nthis study curated the first open-source dataset of impaired speech in Akan: a\nwidely spoken indigenous language in Ghana. The study involved participants\nfrom diverse backgrounds with speech impairments. The resulting dataset, along\nwith the cookbook and open-source tools, are publicly available to enable\nresearchers and practitioners to create inclusive ASR technologies tailored to\nthe unique needs of speech impaired individuals. In addition, this study\npresents the initial results of fine-tuning open-source ASR models to better\nrecognize impaired speech in Akan.", "comment": "This version has been reviewed and accepted for presentation at the\n  InterSpeech 2025 conference to be held in Rotterdam from 17 to 21 August. 5\n  pages and 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.02428v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "低资源语言中受损语音社区驱动数据收集的“食谱”", "tldr": "本研究提出了一种为低资源语言中受损语音收集语音样本以构建自动语音识别（ASR）模型的方法。通过开发一个“食谱”和培训，实现了社区驱动的数据收集和ASR模型构建的民主化。作为概念验证，该研究创建了第一个阿肯语受损语音开源数据集，并提供了初始的ASR模型微调结果。", "motivation": "本研究的动机是为受损语音，特别是低资源语言的受损语音，构建自动语音识别（ASR）模型，旨在通过开发一套“食谱”和培训来民主化ASR技术和数据收集。", "method": "本研究开发了一套关于社区驱动数据收集和ASR模型构建的最佳实践“食谱”和培训。作为概念验证，该研究整理了第一个阿肯语（加纳的一种广泛使用的本土语言）受损语音的开源数据集，并涉及了来自不同背景的言语障碍参与者。此外，还对开源ASR模型进行了微调以更好地识别阿肯语受损语音。", "result": "该研究的成果包括一个针对阿肯语受损语音的开源数据集、一个社区驱动数据收集的“食谱”以及开源工具，这些都已公开可用。此外，还展示了微调开源ASR模型以更好地识别阿肯语受损语音的初步结果。", "conclusion": "本研究的结论是，所提供的开源数据集、食谱和工具能够使研究人员和从业者创建针对言语障碍个体独特需求的包容性ASR技术。", "translation": "本研究提出了一种收集语音样本以构建受损语音（特别是低资源语言）自动语音识别（ASR）模型的方法。它旨在通过开发一套关于社区驱动数据收集和ASR模型构建的最佳实践“食谱”和培训来民主化ASR技术和数据收集。作为概念验证，本研究整理了第一个阿肯语（加纳一种广泛使用的本土语言）受损语音的开源数据集。该研究涉及了来自不同背景的言语障碍参与者。由此产生的数据集，连同该“食谱”和开源工具，均已公开可用，以使研究人员和从业者能够创建针对言语障碍个体独特需求的包容性ASR技术。此外，本研究还展示了微调开源ASR模型以更好地识别阿肯语受损语音的初步结果。", "summary": "本研究提出了一种社区驱动的数据收集方法，旨在为低资源语言的受损语音构建ASR模型。通过开发一套包含最佳实践和培训的“食谱”，该项目致力于民主化ASR技术和数据收集。作为概念验证，研究团队创建了首个阿肯语受损语音开源数据集，并公开了该数据集、食谱及相关工具，以促进包容性ASR技术的发展。研究还展示了微调ASR模型识别阿肯语受损语音的初步成果。", "keywords": "受损语音, 低资源语言, 自动语音识别, 社区驱动, 数据收集", "comments": "本研究的创新之处在于其“食谱”方法和社区驱动的数据收集策略，尤其是在低资源语言和受损语音这一具有挑战性的领域。这对于推动ASR技术的公平性和包容性至关重要，为全球范围内构建更多样化的语音数据集提供了可复制的框架。"}}
{"id": "2507.02828", "title": "Designs from magic-augmented Clifford circuits", "authors": ["Yuzhen Zhang", "Sagar Vijay", "Yingfei Gu", "Yimu Bao"], "categories": ["quant-ph", "cond-mat.stat-mech", "cond-mat.str-el", "cs.IT", "hep-th", "math.IT"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      59 pages", "url": "http://arxiv.org/abs/2507.02828v1", "summary": "We introduce magic-augmented Clifford circuits -- architectures in which\nClifford circuits are preceded and/or followed by constant-depth circuits of\nnon-Clifford (``magic\") gates -- as a resource-efficient way to realize\napproximate $k$-designs, with reduced circuit depth and usage of magic. We\nprove that shallow Clifford circuits, when augmented with constant-depth\ncircuits of magic gates, can generate approximate unitary and state $k$-designs\nwith $\\epsilon$ relative error. The total circuit depth for these constructions\non $N$ qubits is $O(\\log (N/\\epsilon)) +2^{O(k\\log k)}$ in one dimension and\n$O(\\log\\log(N/\\epsilon))+2^{O(k\\log k)}$ in all-to-all circuits using ancillas,\nwhich improves upon previous results for small $k \\geq 4$. Furthermore, our\nconstruction of relative-error state $k$-designs only involves states with\nstrictly local magic. The required number of magic gates is parametrically\nreduced when considering $k$-designs with bounded additive error. As an\nexample, we show that shallow Clifford circuits followed by $O(k^2)$\nsingle-qubit magic gates, independent of system size, can generate an\nadditive-error state $k$-design. We develop a classical statistical mechanics\ndescription of our random circuit architectures, which provides a quantitative\nunderstanding of the required depth and number of magic gates for\nadditive-error state $k$-designs. We also prove no-go theorems for various\narchitectures to generate designs with bounded relative error.", "comment": "59 pages", "pdf_url": "http://arxiv.org/pdf/2507.02828v1", "cate": "quant-ph", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "魔术增强Clifford电路的设计", "tldr": "本文引入了魔术增强Clifford电路，这是一种资源高效的方式，用于实现近似k-设计，同时减少了电路深度和魔术门的用量。", "motivation": "为了以资源高效的方式实现近似k-设计，并减少电路深度和非Clifford（“魔术”）门的用量。", "method": "引入了魔术增强Clifford电路，即Clifford电路前后加上恒定深度的非Clifford门电路。通过证明浅层Clifford电路与恒定深度魔术门电路的结合可以生成近似酉和态k-设计。此外，还开发了随机电路架构的经典统计力学描述。", "result": "浅层Clifford电路，通过恒定深度的魔术门电路增强，可以生成具有$\\\\epsilon$相对误差的近似酉和态k-设计。对于小$k \\geq 4$，这些构造的总电路深度在1维中为$O(\\\\log (N/\\\\epsilon)) +2^{O(k\\\\log k)}$，在带辅助比特的全连接电路中为$O(\\\\log\\\\log(N/\\\\epsilon))+2^{O(k\\\\log k)}$，这改进了之前的结果。相对误差态k-设计仅涉及具有严格局部魔术的态。当考虑有界加性误差的k-设计时，所需的魔术门数量参数化减少。例如，浅层Clifford电路后接$O(k^2)$个单量子比特魔术门（独立于系统大小），可以生成加性误差态k-设计。还证明了某些架构无法生成具有有界相对误差的设计的禁区定理。", "conclusion": "魔术增强Clifford电路提供了一种资源高效的方法来实现近似k-设计，显著减少了电路深度和魔术门的使用。本研究不仅提出了具体的电路构造和性能改进，还提供了其随机电路架构的经典统计力学描述，并证明了相关禁区定理。", "translation": "我们引入了魔术增强Clifford电路——一种架构，其中Clifford电路前后接有恒定深度的非Clifford（“魔术”）门电路——作为一种资源高效的方式来实现近似k-设计，同时减少了电路深度和魔术门的用量。我们证明，浅层Clifford电路，当用恒定深度的魔术门电路增强时，可以生成具有$\\epsilon$相对误差的近似酉和态k-设计。这些构造在N个量子比特上的总电路深度在1维中为$O(\\\\log (N/\\\\epsilon)) +2^{O(k\\\\log k)}$，在带辅助比特的全连接电路中为$O(\\\\log\\\\log(N/\\\\epsilon))+2^{O(k\\\\log k)}$，这改进了之前对于小$k \\geq 4$的结果。此外，我们构造的相对误差态k-设计只涉及具有严格局部魔术的态。当考虑有界加性误差的k-设计时，所需的魔术门数量参数化减少。举例来说，我们展示了浅层Clifford电路后接$O(k^2)$个单量子比特魔术门，其数量独立于系统大小，可以生成一个加性误差态k-设计。我们开发了随机电路架构的经典统计力学描述，这为加性误差态k-设计所需的深度和魔术门数量提供了定量理解。我们还证明了各种架构无法生成具有有界相对误差的设计的禁区定理。", "summary": "本文提出了一种名为“魔术增强Clifford电路”的新型架构，旨在以资源高效的方式实现近似k-设计，并显著减少电路深度和非Clifford（“魔术”）门的用量。研究证明，结合恒定深度魔术门电路的浅层Clifford电路能够生成具有相对误差的近似酉和态k-设计，并在电路深度上取得了优于现有方法的进展。特别是对于小k值，电路深度得到了改善。此外，文章还探讨了在有界加性误差情况下魔术门数量的减少，并提供了一个实例。研究还通过经典的统计力学描述对随机电路架构进行了定量分析，并提出了关于某些架构无法生成有界相对误差设计的禁区定理。", "keywords": "Clifford电路, 魔术门, k-设计, 电路深度, 量子计算", "comments": "该论文创新性地引入了“魔术增强Clifford电路”这一新架构，为量子计算中实现近似k-设计提供了一种资源高效的途径。其重要性在于通过减少电路深度和非Clifford门的使用，克服了实现高阶设计时的实际挑战，为量子信息处理和量子模拟提供了更有效的方法。其在电路深度上的改进，特别是对于小k值，显示出实际应用潜力。"}}
{"id": "2507.02635", "title": "SAT-BO: Verification Rule Learning and Optimization for FraudTransaction Detection", "authors": ["Mao Luo", "Zhi Wang", "Yiwen Huang", "Qingyun Zhang", "Zhouxing Su", "Zhipeng Lv", "Wen Hu", "Jianguo Li"], "categories": ["cs.CR", "cs.DB"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02635v1", "summary": "Electronic payment platforms are estimated to process billions oftransactions\ndaily, with the cumulative value of these transactionspotentially reaching into\nthe trillions. Even a minor error within thishigh-volume environment could\nprecipitate substantial financiallosses. To mitigate this risk, manually\nconstructed verification rules,developed by domain experts, are typically\nemployed to identifyand scrutinize transactions in production environments.\nHowever,due to the absence of a systematic approach to ensure the robust-ness\nof these verification rules against vulnerabilities, they remainsusceptible to\nexploitation.To mitigate this risk, manually constructed verification rules,\nde-veloped by domain experts, are typically employed to identify andscrutinize\ntransactions in production environments. However, dueto the absence of a\nsystematic approach to ensure the robustness ofthese verification rules against\nvulnerabilities, they remain suscep-tible to exploitation. To ensure data\nsecurity, database maintainersusually compose complex verification rules to\ncheck whether aquery/update request is valid. However, the rules written by\nex-perts are usually imperfect, and malicious requests may bypassthese rules.\nAs a result, the demand for identifying the defects ofthe rules systematically\nemerges.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02635v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "欺诈交易检测的验证规则学习与优化 (SAT-BO)", "tldr": "现有欺诈交易检测中的人工验证规则易受攻击且不完善，导致金融损失风险，因此需要系统化方法来识别规则缺陷。", "motivation": "电子支付平台交易量巨大，人工构建的验证规则易受攻击且不完善，导致欺诈交易无法有效识别，存在重大金融损失风险。因此，迫切需要一种系统化的方法来识别和优化这些规则的缺陷。", "method": "Not mentioned in abstract", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "电子支付平台估计每天处理数十亿笔交易，累计价值可能达到数万亿美元。在如此高交易量的环境中，即使是微小的错误也可能导致巨大的经济损失。为了降低这种风险，通常采用由领域专家手动构建的验证规则，用于识别和审查生产环境中的交易。然而，由于缺乏系统化的方法来确保这些验证规则的抗漏洞鲁棒性，它们仍然容易受到攻击。为了确保数据安全，数据库维护者通常会编写复杂的验证规则来检查查询/更新请求是否有效。然而，专家编写的规则通常不完善，恶意请求可能会绕过这些规则。因此，系统性地识别规则缺陷的需求应运而生。", "summary": "针对电子支付平台中欺诈交易检测面临的挑战，本研究关注现有由领域专家手动构建的验证规则的局限性。这些规则因缺乏系统性鲁棒性保障，容易被恶意请求绕过，导致潜在的巨大金融损失。因此，论文强调了系统化识别和修复这些规则缺陷的迫切需求。", "keywords": "欺诈交易检测, 验证规则, 规则优化, 漏洞, 电子支付", "comments": "本文指出了当前电子支付领域中欺诈检测规则的重大缺陷，即人工规则的不完善性和易受攻击性。这凸显了自动化、系统化规则学习和优化的重要性，对提升金融交易安全具有潜在的重大意义。"}}
{"id": "2507.02665", "title": "Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?", "authors": ["Timo Kehrer", "Robert Haines", "Guido Juckeland", "Shurui Zhou", "David E. Bernholdt"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Early access journal version: T. Kehrer, R. Haines, G. Juckeland, S. Zhou and D. E. Bernholdt, \"Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?,\" in Computing in Science & Engineering, doi: https://doi.org/10.1109/MCSE.2025.3557236", "url": "http://arxiv.org/abs/2507.02665v1", "summary": "Anecdotal evidence suggests that Research Software Engineers (RSEs) and\nSoftware Engineering Researchers (SERs) often use different terminologies for\nsimilar concepts, creating communication challenges. To better understand these\ndivergences, we have started investigating how SE fundamentals from the SER\ncommunity are interpreted within the RSE community, identifying aligned\nconcepts, knowledge gaps, and areas for potential adaptation. Our preliminary\nfindings reveal opportunities for mutual learning and collaboration, and our\nsystematic methodology for terminology mapping provides a foundation for a\ncrowd-sourced extension and validation in the future.", "comment": "Early access journal version: T. Kehrer, R. Haines, G. Juckeland, S.\n  Zhou and D. E. Bernholdt, \"Do Research Software Engineers and Software\n  Engineering Researchers Speak the Same Language?,\" in Computing in Science &\n  Engineering, doi: 10.1109/MCSE.2025.3557236", "pdf_url": "http://arxiv.org/pdf/2507.02665v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "研究软件工程师和软件工程研究人员说同一种语言吗？", "tldr": "研究软件工程师（RSEs）和软件工程研究人员（SERs）之间存在术语差异导致沟通障碍，本研究旨在通过系统方法映射术语，以促进双方的相互学习和协作。", "motivation": "现有证据表明研究软件工程师（RSEs）和软件工程研究人员（SERs）对相似概念使用不同术语，导致沟通困难。本研究旨在理解这些差异。", "method": "研究通过调查软件工程研究人员社区的软件工程基础知识在研究软件工程师社区中的解释方式，识别一致概念、知识空白和潜在适应领域。采用系统化的术语映射方法，为未来众包扩展和验证奠定基础。", "result": "初步发现揭示了双方相互学习和协作的机会。", "conclusion": "通过系统化的术语映射方法，可以促进研究软件工程师和软件工程研究人员之间的沟通，识别学习和协作的机会。", "translation": "传闻证据表明，研究软件工程师（RSEs）和软件工程研究人员（SERs）经常对相似概念使用不同的术语，从而造成沟通障碍。为了更好地理解这些分歧，我们开始调查软件工程研究人员社区的软件工程基础知识在研究软件工程师社区中是如何被解释的，识别出一致的概念、知识空白以及潜在的适应领域。我们的初步发现揭示了相互学习和协作的机会，而我们系统化的术语映射方法为未来众包扩展和验证奠定了基础。", "summary": "本文旨在探讨研究软件工程师（RSEs）和软件工程研究人员（SERs）之间因术语差异导致的沟通问题。研究通过调查软件工程基础知识在RSE社区的解释，识别了概念一致性、知识差距和适应空间。初步结果表明双方存在相互学习和协作的潜力，并提出了一种系统化的术语映射方法，为未来的众包扩展和验证奠定了基础。", "keywords": "研究软件工程师, 软件工程研究人员, 术语映射, 沟通, 知识差距", "comments": "这篇论文的创新点在于它提出了一种系统化的术语映射方法来解决研究软件工程师和软件工程研究人员之间的沟通障碍，这是一个在跨学科领域中普遍存在但常被忽视的问题。其重要性在于，通过促进这两个关键群体之间的理解和协作，可以提高研究成果的转化效率和软件开发的质量。该研究的局限性在于目前仅为初步发现，且方法论仍需未来的众包验证。"}}
{"id": "2507.02447", "title": "HAC-LOCO: Learning Hierarchical Active Compliance Control for Quadruped Locomotion under Continuous External Disturbances", "authors": ["Xiang Zhou", "Xinyu Zhang", "Qingrui Zhang"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 7 Figures", "url": "http://arxiv.org/abs/2507.02447v1", "summary": "Despite recent remarkable achievements in quadruped control, it remains\nchallenging to ensure robust and compliant locomotion in the presence of\nunforeseen external disturbances. Existing methods prioritize locomotion\nrobustness over compliance, often leading to stiff, high-frequency motions, and\nenergy inefficiency. This paper, therefore, presents a two-stage hierarchical\nlearning framework that can learn to take active reactions to external force\ndisturbances based on force estimation. In the first stage, a velocity-tracking\npolicy is trained alongside an auto-encoder to distill historical\nproprioceptive features. A neural network-based estimator is learned through\nsupervised learning, which estimates body velocity and external forces based on\nproprioceptive measurements. In the second stage, a compliance action module,\ninspired by impedance control, is learned based on the pre-trained encoder and\npolicy. This module is employed to actively adjust velocity commands in\nresponse to external forces based on real-time force estimates. With the\ncompliance action module, a quadruped robot can robustly handle minor\ndisturbances while appropriately yielding to significant forces, thus striking\na balance between robustness and compliance. Simulations and real-world\nexperiments have demonstrated that our method has superior performance in terms\nof robustness, energy efficiency, and safety. Experiment comparison shows that\nour method outperforms the state-of-the-art RL-based locomotion controllers.\nAblation studies are given to show the critical roles of the compliance action\nmodule.", "comment": "8 pages, 7 Figures", "pdf_url": "http://arxiv.org/pdf/2507.02447v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "HAC-LOCO：在持续外部扰动下学习四足机器人运动的分层主动柔顺控制", "tldr": "本文提出一个两阶段分层学习框架HAC-LOCO，通过力估计实现四足机器人在持续外部扰动下的主动柔顺控制，平衡了鲁棒性和柔顺性，提高了能效和安全性。", "motivation": "尽管四足机器人控制取得了显著进展，但在不可预见的外部扰动下，仍难以确保鲁棒和柔顺的运动。现有方法通常优先考虑鲁棒性，导致运动僵硬、高频且能效低下。", "method": "本文提出了一个两阶段分层学习框架。第一阶段，训练一个速度跟踪策略和一个自编码器来提取历史本体感受特征，并学习一个基于神经网络的估计器，根据本体感受测量估计身体速度和外部力。第二阶段，基于预训练的编码器和策略，学习一个受阻抗控制启发的主动柔顺动作模块，该模块根据实时力估计主动调整速度指令。", "result": "仿真和实际实验表明，该方法在鲁棒性、能效和安全性方面表现优异，并超越了最先进的基于强化学习的运动控制器。消融研究也证明了柔顺动作模块的关键作用。", "conclusion": "该方法通过平衡鲁棒性和柔顺性，使得四足机器人在面对外部扰动时能更好地处理，同时提高了能效和安全性。", "translation": "尽管最近在四足控制方面取得了显著成就，但在存在不可预见的外部扰动的情况下，确保鲁棒和柔顺的运动仍然具有挑战性。现有方法优先考虑运动鲁棒性而非柔顺性，这通常导致僵硬、高频的运动和低能量效率。因此，本文提出了一个两阶段的分层学习框架，该框架可以根据力估计学习对外部力扰动采取主动反应。在第一阶段，训练一个速度跟踪策略和一个自编码器来提取历史本体感受特征。通过监督学习训练一个基于神经网络的估计器，该估计器根据本体感受测量估计身体速度和外部力。在第二阶段，基于预训练的编码器和策略，学习一个受阻抗控制启发的主动柔顺动作模块。该模块用于根据实时力估计主动调整对外部力的速度指令。通过柔顺动作模块，四足机器人可以鲁棒地处理轻微扰动，同时适当地屈服于显著力，从而在鲁棒性和柔顺性之间取得平衡。仿真和实际实验表明，我们的方法在鲁棒性、能量效率和安全性方面具有卓越的性能。实验比较表明，我们的方法优于最先进的基于强化学习的运动控制器。还进行了消融研究，以显示柔顺动作模块的关键作用。", "summary": "本文提出了HAC-LOCO，一个用于四足机器人运动控制的两阶段分层学习框架，旨在解决现有方法在外部扰动下鲁棒性与柔顺性难以兼顾的问题。该框架通过力估计使机器人能主动应对外部力，第一阶段训练速度跟踪策略和力估计器，第二阶段学习基于阻抗控制的柔顺动作模块以实时调整速度指令。实验证明，该方法在鲁棒性、能效和安全性方面优于现有技术，实现了在处理轻微扰动和对大力的适当屈服之间的平衡。", "keywords": "四足机器人, 柔顺控制, 分层学习, 外部扰动, 力估计", "comments": "本文的创新之处在于提出了一个分层学习框架，通过主动力估计和柔顺动作模块，实现了四足机器人在外部扰动下鲁棒性和柔顺性的平衡，解决了现有控制器过于僵硬的问题。其两阶段学习方法和受阻抗控制启发的柔顺模块设计是关键创新点。该研究对于提升四足机器人在复杂动态环境中的适应性和实用性具有重要意义。"}}
{"id": "2507.02350", "title": "From Coarse to Fine-Grained Emotion Annotation: An Immediate Recall Paradigm with Validation through Physiological Evidence and Recognition Performance", "authors": ["Hao Tang", "Songyun Xie", "Xinzhou Xie", "Can Liao", "Xin Zhang", "Bohan Li", "Zhongyu Tian", "Dalu Zheng"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02350v1", "summary": "Traditional video-induced emotion physiological datasets often use\nwhole-trial annotation, assigning a single emotion label to all data collected\nduring an entire trial. This coarse-grained annotation approach misaligns with\nthe dynamic and temporally localized nature of emotional responses as they\nunfold with video narratives, introducing label noise that limits emotion\nrecognition algorithm evaluation and performance. To solve the label noise\nproblem caused by coarse-grained annotation, we propose a fine-grained\nannotation method through an immediate recall paradigm. This paradigm\nintegrates an immediate video replay phase after the initial stimulus viewing,\nallowing participants to precisely mark the onset timestamp, emotion label, and\nintensity based on their immediate recall. We validate this paradigm through\nphysiological evidence and recognition performance. Physiological validation of\nmultimodal signals within participant-marked windows revealed rhythm-specific\nEEG patterns and arousal-dependent GSR responses-with SCRs appearing in 91% of\nhigh-arousal versus 6% of low-arousal emotion windows. These objective\nphysiological data changes strongly aligned with subjective annotations,\nconfirming annotation precision. For recognition performance, classification\nexperiments showed that models trained on fine-grained annotations achieved\n9.7% higher accuracy than traditional whole-trial labeling, despite using less\ndata. This work not only addresses label noise through fine-grained annotation\nbut also demonstrates that annotation precision outweighs data scale in\ndetermining emotion recognition performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02350v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "从粗粒度到细粒度情感标注：一种通过生理证据和识别性能验证的即时回忆范式", "tldr": "传统粗粒度情感标注存在噪声问题，本文提出一种即时回忆范式进行细粒度标注，并通过生理证据和更高的情感识别准确率验证了其有效性，表明标注精度比数据量更重要。", "motivation": "传统的视频诱发情感生理数据集中，全试次标注（将单一情感标签分配给整个试次数据）与情感响应的动态性和时间局部性不符，引入了标签噪声，限制了情感识别算法的评估和性能。", "method": "提出一种通过即时回忆范式实现的细粒度标注方法。该范式在初始刺激观看后立即进行视频回放，允许参与者根据即时回忆精确标记情感的起始时间戳、情感标签和强度。", "result": "生理验证显示，参与者标记窗口内的多模态信号揭示了节律特异性脑电图模式和唤醒依赖性皮肤电反应（高唤醒情感窗口中91%出现皮肤电导反应，低唤醒窗口中仅6%），这些客观生理数据变化与主观标注高度一致，证实了标注的精确性。分类实验表明，使用细粒度标注训练的模型比传统全试次标注的模型准确率提高9.7%，尽管使用了更少的数据。", "conclusion": "本工作不仅通过细粒度标注解决了标签噪声问题，还表明在确定情感识别性能时，标注精度比数据规模更重要。", "translation": "传统的视频诱发情感生理数据集通常采用全试次标注，为整个试次收集到的所有数据分配一个单一的情感标签。这种粗粒度标注方法与情感响应随视频叙事展开时的动态性和时间局部性不符，引入了标签噪声，限制了情感识别算法的评估和性能。为了解决粗粒度标注引起的标签噪声问题，我们提出了一种通过即时回忆范式的细粒度标注方法。该范式在初始刺激观看后整合了即时视频回放阶段，允许参与者根据其即时回忆精确标记起始时间戳、情感标签和强度。我们通过生理证据和识别性能验证了这种范式。参与者标记窗口内的多模态信号的生理验证揭示了节律特异性脑电图模式和唤醒依赖性皮肤电反应——高唤醒情感窗口中91%出现皮肤电导反应，而低唤醒情感窗口中仅6%出现。这些客观生理数据变化与主观标注高度一致，证实了标注的精确性。在识别性能方面，分类实验表明，尽管使用的数据较少，但使用细粒度标注训练的模型比传统全试次标注的模型准确率提高了9.7%。这项工作不仅通过细粒度标注解决了标签噪声问题，还证明了标注精度在确定情感识别性能方面的重要性超过了数据规模。", "summary": "本研究旨在解决传统粗粒度情感标注引入的标签噪声问题，该问题限制了情感识别算法的性能。为此，作者提出了一种基于即时回忆范式的细粒度情感标注方法，允许参与者精确标记情感的起始时间、类型和强度。通过生理信号（如EEG和GSR）验证，结果显示细粒度标注与客观生理变化高度一致，证明了其精确性。此外，使用细粒度标注训练的情感识别模型，即使数据量更少，其准确率也比使用传统粗粒度标注的模型高出9.7%。研究强调了标注精度对于情感识别性能的重要性，超越了数据规模。", "keywords": "情感标注, 细粒度, 即时回忆范式, 生理证据, 情感识别性能", "comments": "这项研究的创新之处在于提出了“即时回忆范式”来解决情感标注中的时间对齐和噪声问题，并首次通过生理证据和识别性能的双重验证，有力地证明了细粒度标注的有效性。其重要性在于改变了情感数据集构建的范式，强调了标注质量而非单纯数据量的优先性，对未来情感计算领域的数据集构建和模型训练具有指导意义。该方法有望为更准确、更鲁棒的情感识别系统奠定基础。"}}
{"id": "2507.02353", "title": "OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent", "authors": ["Bowen Chen", "Zhao Wang", "Shingo Takamatsu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02353v1", "summary": "Keyword decision in Sponsored Search Advertising is critical to the success\nof ad campaigns. While LLM-based methods offer automated keyword generation,\nthey face three major limitations: reliance on large-scale query-keyword pair\ndata, lack of online multi-objective performance monitoring and optimization,\nand weak quality control in keyword selection. These issues hinder the agentic\nuse of LLMs in fully automating keyword decisions by monitoring and reasoning\nover key performance indicators such as impressions, clicks, conversions, and\nCTA effectiveness. To overcome these challenges, we propose OMS, a keyword\ngeneration framework that is On-the-fly (requires no training data, monitors\nonline performance, and adapts accordingly), Multi-objective (employs agentic\nreasoning to optimize keywords based on multiple performance metrics), and\nSelf-reflective (agentically evaluates keyword quality). Experiments on\nbenchmarks and real-world ad campaigns show that OMS outperforms existing\nmethods; ablation and human evaluations confirm the effectiveness of each\ncomponent and the quality of generated keywords.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02353v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "OMS：基于大型语言模型代理的即时、多目标、自反思广告关键词生成", "tldr": "OMS是一个即时、多目标、自反思的框架，通过LLM代理生成广告关键词，解决了现有LLM方法在数据依赖、在线优化和质量控制方面的局限性，并在实验中表现优异。", "motivation": "赞助搜索广告中的关键词决策对于广告活动成功至关重要。现有基于LLM的关键词生成方法面临三大局限性：1) 依赖大规模查询-关键词对数据；2) 缺乏在线多目标性能监控和优化；3) 关键词选择的质量控制薄弱。这些问题阻碍了LLM在通过监控和推理关键性能指标（如展示、点击、转化和CTA效率）方面实现关键词决策的完全自动化。", "method": "本文提出了OMS框架，用于关键词生成。OMS具有以下特点：1) 即时（On-the-fly）：无需训练数据，监控在线性能并相应调整；2) 多目标（Multi-objective）：采用代理推理，根据多个性能指标优化关键词；3) 自反思（Self-reflective）：代理式评估关键词质量。", "result": "在基准测试和真实世界广告活动中的实验表明，OMS优于现有方法；消融实验和人工评估证实了每个组件的有效性以及生成关键词的质量。", "conclusion": "OMS框架通过其即时、多目标和自反思的特性，成功克服了现有LLM在广告关键词生成中的局限性，显著提升了关键词的生成质量和优化效果。", "translation": "赞助搜索广告中的关键词决策对于广告活动的成功至关重要。尽管基于大型语言模型（LLM）的方法提供了自动关键词生成，但它们面临三大主要局限性：依赖大规模查询-关键词对数据、缺乏在线多目标性能监控和优化，以及关键词选择中的质量控制薄弱。这些问题阻碍了LLM代理在通过监控和推理关键性能指标（如展示、点击、转化和CTA效率）方面实现关键词决策的完全自动化。为了克服这些挑战，我们提出了OMS，一个关键词生成框架，它具有即时性（无需训练数据，监控在线性能并相应调整）、多目标性（采用代理推理根据多个性能指标优化关键词）和自反思性（代理式评估关键词质量）。在基准测试和真实世界广告活动中的实验表明，OMS优于现有方法；消融实验和人工评估证实了每个组件的有效性以及生成关键词的质量。", "summary": "本文提出了OMS，一个即时、多目标、自反思的LLM代理框架，用于广告关键词生成。该框架旨在解决现有LLM方法在数据依赖、在线多目标优化和关键词质量控制方面的不足。OMS无需训练数据，能根据在线性能进行调整，并利用代理推理优化多个性能指标，同时进行关键词质量的自评估。实验结果表明，OMS在基准测试和实际广告活动中均优于现有方法，其组件的有效性和生成关键词的质量也得到了证实。", "keywords": "广告关键词生成, LLM代理, 多目标优化, 自反思, 即时性", "comments": "这项研究的创新点在于提出了一个无需预训练数据、能够进行在线多目标优化并具备自反思能力的LLM代理框架，用于广告关键词生成。这解决了当前LLM在实际广告应用中面临的关键挑战，特别是其对大规模数据集的依赖和缺乏动态适应性。OMS的“即时性”、“多目标性”和“自反思性”特性使其在自动化广告决策方面迈出了重要一步，对于提升广告活动的效率和效果具有重要意义。"}}
{"id": "2507.02270", "title": "MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement", "authors": ["Fanghai Yi", "Zehong Zheng", "Zexiao Liang", "Yihang Dong", "Xiyang Fang", "Wangyu Wu", "Xuhang Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE SMC 2025", "url": "http://arxiv.org/abs/2507.02270v1", "summary": "Enhancing underwater images is crucial for exploration. These images face\nvisibility and color issues due to light changes, water turbidity, and bubbles.\nTraditional prior-based methods and pixel-based methods often fail, while deep\nlearning lacks sufficient high-quality datasets. We introduce the Multi-Axis\nConditional Lookup (MAC-Lookup) model, which enhances visual quality by\nimproving color accuracy, sharpness, and contrast. It includes Conditional 3D\nLookup Table Color Correction (CLTCC) for preliminary color and quality\ncorrection and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.\nThis model prevents over-enhancement and saturation while handling underwater\nchallenges. Extensive experiments show that MAC-Lookup excels in enhancing\nunderwater images by restoring details and colors better than existing methods.\nThe code is https://github.com/onlycatdoraemon/MAC-Lookup.", "comment": "Accepted by IEEE SMC 2025", "pdf_url": "http://arxiv.org/pdf/2507.02270v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MAC-Lookup：水下图像增强的多轴条件查找模型", "tldr": "MAC-Lookup是一种用于水下图像增强的新模型，通过结合条件查找表和多轴自适应增强来改善颜色、清晰度和对比度，并在实验中表现出色。", "motivation": "水下图像由于光线变化、水体浑浊和气泡等问题，存在可见性和颜色失真，传统的基于先验和基于像素的方法常常失效，而深度学习又缺乏足够的高质量数据集，因此急需有效的水下图像增强方法。", "method": "本文提出了多轴条件查找（MAC-Lookup）模型。该模型包含两个主要部分：用于初步颜色和质量校正的条件3D查找表颜色校正（CLTCC）和用于细节细化的多轴自适应增强（MAAE）。该模型旨在防止过度增强和饱和。", "result": "广泛的实验表明，MAC-Lookup在水下图像增强方面表现出色，比现有方法更好地恢复了细节和颜色。", "conclusion": "MAC-Lookup模型通过其独特的设计，有效解决了水下图像的可见性和颜色问题，并在实验中验证了其优越的增强性能。", "translation": "增强水下图像对于探索至关重要。这些图像由于光线变化、水体浑浊和气泡等原因面临可见性和颜色问题。传统的基于先验的方法和基于像素的方法常常失效，而深度学习缺乏足够的高质量数据集。我们引入了多轴条件查找（MAC-Lookup）模型，通过改善颜色准确性、清晰度和对比度来提高视觉质量。它包括用于初步颜色和质量校正的条件3D查找表颜色校正（CLTCC）和用于细节细化的多轴自适应增强（MAAE）。该模型在处理水下挑战的同时，防止了过度增强和饱和。广泛的实验表明，MAC-Lookup在水下图像增强方面表现出色，比现有方法更好地恢复了细节和颜色。代码链接为https://github.com/onlycatdoraemon/MAC-Lookup。", "summary": "MAC-Lookup是一种新颖的水下图像增强模型，旨在解决水下图像因光照、浑浊和气泡导致的可见性与颜色问题。该模型结合了条件3D查找表颜色校正（CLTCC）进行初步颜色与质量校正，以及多轴自适应增强（MAAE）进行细节细化，有效防止了过度增强。实验结果表明，MAC-Lookup在恢复水下图像细节和颜色方面优于现有方法。", "keywords": "水下图像增强, 多轴条件查找, 查找表, 颜色校正, 细节恢复", "comments": "该论文提出了一种创新的水下图像增强模型MAC-Lookup，通过结合查找表和自适应增强来解决传统方法和深度学习的局限性。其亮点在于引入了条件3D查找表进行颜色校正和多轴自适应增强进行细节细化，并强调了防止过度增强和饱和。这种模块化设计可能为水下图像处理提供了新的思路，并展示了其在实际应用中的潜力。"}}
{"id": "2507.02721", "title": "A formal specification of the desired software behaviour of the Princess Marijke lock complex", "authors": ["Jan Friso Groote", "Matthias Volk"], "categories": ["eess.SY", "cs.LO", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02721v1", "summary": "The Princess Marijke lock complex is a large lock and water-protection\ninstallation in the Netherlands between the river Rhine and the\nAmsterdam-Rijnkanaal -- a large waterway connecting the Rhine to the port of\nAmsterdam. The lock complex consists of two independent locks and a moveable\nflood-protection barrier. Ensuring safe control of the lock complex is of\nutmost importance to guarantee both flood-protection and reliable ship\noperations. This paper gives a precise, formal description of the software\ncontrol of the lock complex in less than 400 lines of mCRL2 code. This\ndescription can act as a blueprint on how the software of this lock complex\nneeds to be constructed. Moreover, using model checking, 53 software\nrequirements are shown to be valid, ensuring that the formal description of the\nbehaviour is correct with regard to these properties and is unlikely to contain\nmistakes and oversights.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02721v1", "cate": "eess.SY", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "玛丽克公主船闸综合体预期软件行为的正式规范", "tldr": "本文使用mCRL2和模型检测为荷兰玛丽克公主船闸综合体提供了软件控制的正式规范，并验证了53个软件需求，以确保安全可靠的运行。", "motivation": "确保玛丽克公主船闸综合体的安全控制至关重要，以保障防洪和可靠的船舶运营。", "method": "本文使用mCRL2语言对船闸综合体的软件控制进行了精确的、形式化的描述（少于400行代码），并利用模型检测验证了53个软件需求的有效性。", "result": "成功验证了53个软件需求的有效性，确保了行为形式化描述的正确性，并降低了错误和疏漏的可能性。", "conclusion": "该形式化描述可以作为船闸综合体软件构建的蓝图，确保了其行为的正确性。", "translation": "玛丽克公主船闸综合体是荷兰莱茵河与阿姆斯特丹-莱茵运河之间的一个大型船闸和水利设施，阿姆斯特丹-莱茵运河是一条连接莱茵河与阿姆斯特丹港的大型水道。该船闸综合体由两个独立的船闸和一个可移动的防洪屏障组成。确保船闸综合体的安全控制至关重要，以保障防洪和可靠的船舶运营。本文用不到400行mCRL2代码，对船闸综合体的软件控制进行了精确、形式化的描述。该描述可以作为该船闸综合体软件构建的蓝图。此外，通过模型检测，证明了53个软件需求的有效性，确保了行为形式化描述在这些属性方面是正确的，并且不太可能包含错误和疏漏。", "summary": "本文针对荷兰玛丽克公主船闸综合体，提出了一种基于mCRL2的形式化软件行为规范，旨在确保关键基础设施的安全运行。通过对船闸复杂结构（包括两个船闸和一个防洪屏障）的软件控制进行精确建模，并利用模型检测验证了53项软件需求，证明了该形式化描述的正确性和可靠性，为未来软件开发提供了蓝图。", "keywords": "形式化规范, 软件行为, mCRL2, 模型检测, 船闸综合体", "comments": "本文的创新之处在于将形式化方法（mCRL2和模型检测）应用于大型关键基础设施（船闸综合体）的软件行为规范。这种方法极大地提高了软件的可靠性和安全性，对于保障防洪和航运至关重要。其重要性体现在为复杂系统的软件开发提供了一个可靠的“蓝图”和验证机制，减少了潜在的错误和风险。"}}
{"id": "2507.02824", "title": "DNN-Based Precoding in RIS-Aided mmWave MIMO Systems With Practical Phase Shift", "authors": ["Po-Heng Chou", "Ching-Wen Chen", "Wan-Jen Huang", "Walid Saad", "Yu Tsao", "Ronald Y. Chang"], "categories": ["eess.SP", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      5 pages, 4 figures, 2 tables, accepted by IEEE Globecom 2024 Workshops", "url": "http://arxiv.org/abs/2507.02824v1", "summary": "In this paper, the precoding design is investigated for maximizing the\nthroughput of millimeter wave (mmWave) multiple-input multiple-output (MIMO)\nsystems with obstructed direct communication paths. In particular, a\nreconfigurable intelligent surface (RIS) is employed to enhance MIMO\ntransmissions, considering mmWave characteristics related to line-of-sight\n(LoS) and multipath effects. The traditional exhaustive search (ES) for optimal\ncodewords in the continuous phase shift is computationally intensive and\ntime-consuming. To reduce computational complexity, permuted discrete Fourier\ntransform (DFT) vectors are used for finding codebook design, incorporating\namplitude responses for practical or ideal RIS systems. However, even if the\ndiscrete phase shift is adopted in the ES, it results in significant\ncomputation and is time-consuming. Instead, the trained deep neural network\n(DNN) is developed to facilitate faster codeword selection. Simulation results\nshow that the DNN maintains sub-optimal spectral efficiency even as the\ndistance between the end-user and the RIS has variations in the testing phase.\nThese results highlight the potential of DNN in advancing RIS-aided systems.", "comment": "5 pages, 4 figures, 2 tables, accepted by IEEE Globecom 2024\n  Workshops", "pdf_url": "http://arxiv.org/pdf/2507.02824v1", "cate": "eess.SP", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于DNN的RIS辅助毫米波MIMO系统中具有实际相移的预编码", "tldr": "本文提出一种基于DNN的方法，用于RIS辅助毫米波MIMO系统中的快速预编码，以提高吞吐量并解决传统方法的计算复杂性。", "motivation": "在存在直达路径阻塞的毫米波（mmWave）多输入多输出（MIMO）系统中，为了最大化吞吐量，研究预编码设计。传统的穷举搜索（ES）寻找最优码字计算量大且耗时，即使采用离散相移也依然如此。", "method": "采用可重构智能表面（RIS）来增强MIMO传输，并考虑毫米波的视距（LoS）和多径效应。使用置换离散傅里叶变换（DFT）向量进行码本设计，并结合实际或理想RIS系统的幅度响应。通过训练深度神经网络（DNN）来取代传统搜索，以实现更快的码字选择。", "result": "仿真结果表明，即使在测试阶段终端用户与RIS之间的距离发生变化，DNN也能保持次优的频谱效率。", "conclusion": "这些结果突出了DNN在推进RIS辅助系统方面的潜力。", "translation": "本文研究了在直达通信路径受阻的毫米波（mmWave）多输入多输出（MIMO）系统中，为最大化吞吐量而进行的预编码设计。具体而言，采用可重构智能表面（RIS）来增强MIMO传输，并考虑与视距（LoS）和多径效应相关的毫米波特性。传统的连续相移最优码字穷举搜索（ES）计算量大且耗时。为了降低计算复杂性，本文使用置换离散傅里叶变换（DFT）向量来寻找码本设计，其中包含了实际或理想RIS系统的幅度响应。然而，即使在ES中采用离散相移，也会导致显著的计算量和耗时。取而代之的是，本文开发了训练好的深度神经网络（DNN）来促进更快的码字选择。仿真结果表明，即使在测试阶段终端用户与RIS之间的距离发生变化，DNN也能保持次优的频谱效率。这些结果突出了DNN在推进RIS辅助系统方面的潜力。", "summary": "本文针对直达路径受阻的毫米波MIMO系统，提出了一种基于深度神经网络（DNN）的预编码设计，旨在最大化系统吞吐量。鉴于传统穷举搜索（ES）在连续或离散相移下计算复杂度高、耗时的问题，研究者引入可重构智能表面（RIS）来增强传输，并利用DNN实现快速码字选择。仿真结果显示，该DNN方法在终端用户与RIS距离变化时仍能保持次优的频谱效率，证明了DNN在RIS辅助系统中的应用潜力。", "keywords": "深度神经网络, 预编码, 可重构智能表面, 毫米波MIMO, 相移", "comments": "该论文的创新点在于将深度神经网络应用于RIS辅助的毫米波MIMO系统中的预编码设计，有效解决了传统穷举搜索方法计算量大、耗时的问题，尤其是在考虑实际相移的情况下。这对于推动RIS在实际通信系统中的应用具有重要意义，通过提高计算效率，使得复杂系统配置变得可行。"}}
{"id": "2507.02759", "title": "An Easy Proof of a Weak Version of Chernoff inequality", "authors": ["Sariel Har-Peled"], "categories": ["math.PR", "cs.DS", "math.CO"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02759v1", "summary": "We prove an easy but very weak version of Chernoff inequality. Namely, that\nthe probability that in $6M$ throws of a fair coin, one gets at most $M$ heads\nis $\\leq 1/2^M$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02759v1", "cate": "math.PR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "切尔诺夫不等式弱版本的一个简易证明", "tldr": "论文提供了一个切尔诺夫不等式弱版本的简易证明，具体证明了在6M次公平硬币投掷中，最多M次正面朝上的概率不大于1/2^M。", "motivation": "提供一个切尔诺夫不等式弱版本的简易证明。", "method": "通过数学证明，推导出一个特定条件下（6M次公平硬币投掷，最多M次正面）的概率上限，从而证明了切尔诺夫不等式的一个弱版本。", "result": "证明了在6M次公平硬币投掷中，获得最多M次正面的概率小于等于1/2^M。", "conclusion": "该证明方法简易，但所证明的切尔诺夫不等式版本较弱。", "translation": "我们证明了一个简易但非常弱的切尔诺夫不等式版本。即，在公平硬币抛掷6M次时，获得最多M次正面的概率小于等于1/2^M。", "summary": "这篇论文提出了切尔诺夫不等式的一个简易但弱版本的证明。具体而言，它证明了在6M次公平硬币投掷中，获得最多M次正面的概率不大于1/2^M。", "keywords": "切尔诺夫不等式, 概率, 简易证明, 硬币投掷, 弱版本", "comments": "这篇论文的创新在于提供了一个非常简易的证明方法，尽管其证明的版本是一个“弱版本”。它的重要性可能在于为教学或初学者理解切尔诺夫不等式提供了一个更易于理解的切入点，但其适用范围和强度有限。"}}
{"id": "2507.02506", "title": "IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders", "authors": ["Sneha Deshmukh", "Prathmesh Kamble"], "categories": ["cs.CL", "cs.AI", "cs.LG", "91B14, 68T50", "I.2.7; K.4.1; K.5.2"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      9 pages, 9 figures, 2 tables. Dataset available at Hugging Face and GitHub. Submitted to arXiv for open access", "url": "http://arxiv.org/abs/2507.02506v1", "summary": "Legal NLP remains underdeveloped in regions like India due to the scarcity of\nstructured datasets. We introduce IndianBailJudgments-1200, a new benchmark\ndataset comprising 1200 Indian court judgments on bail decisions, annotated\nacross 20+ attributes including bail outcome, IPC sections, crime type, and\nlegal reasoning. Annotations were generated using a prompt-engineered GPT-4o\npipeline and verified for consistency. This resource supports a wide range of\nlegal NLP tasks such as outcome prediction, summarization, and fairness\nanalysis, and is the first publicly available dataset focused specifically on\nIndian bail jurisprudence.", "comment": "9 pages, 9 figures, 2 tables. Dataset available at Hugging Face and\n  GitHub. Submitted to arXiv for open access", "pdf_url": "http://arxiv.org/pdf/2507.02506v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "IndianBailJudgments-1200：一个用于印度保释令法律自然语言处理的多属性数据集", "tldr": "引入了一个包含1200份印度保释判决的多属性数据集，以促进印度法律NLP研究。", "motivation": "由于印度等地区缺乏结构化数据集，法律NLP发展滞后。", "method": "通过提示工程的GPT-4o管道生成并验证了1200份印度法院保释判决的注释，涵盖20多个属性。", "result": "创建了IndianBailJudgments-1200数据集，包含1200份印度保释判决，标注了20多个属性，如保释结果、IPC章节、犯罪类型和法律推理。", "conclusion": "该数据集是第一个专注于印度保释判例的公开可用数据集，支持结果预测、摘要和公平性分析等广泛的法律NLP任务。", "translation": "由于缺乏结构化数据集，印度等地区的法律NLP仍不发达。我们引入了IndianBailJudgments-1200，这是一个新的基准数据集，包含1200份印度法院关于保释决定的判决，标注了20多个属性，包括保释结果、IPC条款、犯罪类型和法律推理。注释是使用提示工程的GPT-4o管道生成的，并验证了其一致性。该资源支持广泛的法律NLP任务，如结果预测、摘要和公平性分析，并且是第一个专门针对印度保释判例的公开可用数据集。", "summary": "本论文介绍了IndianBailJudgments-1200，一个包含1200份印度保释判决的多属性数据集，旨在解决印度法律NLP领域结构化数据稀缺的问题。该数据集通过GPT-4o管道生成并验证了超过20个属性的注释，支持多种法律NLP任务，是印度保释判例领域的首个公开数据集。", "keywords": "法律NLP, 印度保释判决, 数据集, GPT-4o, 多属性", "comments": "该论文通过构建一个大规模、多属性的印度保释判决数据集，填补了印度法律NLP领域的数据空白，其利用GPT-4o进行标注并进行验证的方法具有创新性，将极大地推动该地区法律AI的发展。"}}
{"id": "2507.02599", "title": "Padé Approximant Neural Networks for Enhanced Electric Motor Fault Diagnosis Using Vibration and Acoustic Data", "authors": ["Sertac Kilickaya", "Levent Eren"], "categories": ["cs.LG", "cs.SD", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submitted to the Journal of Vibration Engineering & Technologies", "url": "http://arxiv.org/abs/2507.02599v1", "summary": "Purpose: The primary aim of this study is to enhance fault diagnosis in\ninduction machines by leveraging the Pad\\'e Approximant Neuron (PAON) model.\nWhile accelerometers and microphones are standard in motor condition\nmonitoring, deep learning models with nonlinear neuron architectures offer\npromising improvements in diagnostic performance. This research addresses the\nquestion: Can Pad\\'e Approximant Neural Networks (Pad\\'eNets) outperform\nconventional Convolutional Neural Networks (CNNs) and Self-Organized\nOperational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical\nfaults using vibration and acoustic data?\n  Methods: We evaluate and compare the diagnostic capabilities of three deep\nlearning architectures: one-dimensional CNNs, Self-ONNs, and Pad\\'eNets. These\nmodels are tested on the University of Ottawa's publicly available\nconstant-speed induction motor datasets, which include both vibration and\nacoustic sensor data. The Pad\\'eNet model is designed to introduce enhanced\nnonlinearity and is compatible with unbounded activation functions such as\nLeaky ReLU.\n  Results and Conclusion: Pad\\'eNets consistently outperformed the baseline\nmodels, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33%\nfor accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced\nnonlinearity of Pad\\'eNets, together with their compatibility with unbounded\nactivation functions, significantly improves fault diagnosis performance in\ninduction motor condition monitoring.", "comment": "Submitted to the Journal of Vibration Engineering & Technologies", "pdf_url": "http://arxiv.org/pdf/2507.02599v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "帕德近似神经网络用于基于振动和声学数据的电机故障诊断增强", "tldr": "Padé近似神经网络（PadéNets）在使用振动和声学数据进行感应电机故障诊断方面表现优于传统的CNN和Self-ONN，实现了更高的诊断精度。", "motivation": "本研究旨在通过利用Padé近似神经元（PAON）模型来增强感应电机的故障诊断，并探讨Padé近似神经网络（PadéNets）在使用振动和声学数据诊断电气和机械故障方面是否能超越传统的卷积神经网络（CNNs）和自组织运算神经网络（Self-ONNs）。", "method": "研究评估并比较了三种深度学习架构的诊断能力：一维CNN、Self-ONN和PadéNet。这些模型在渥太华大学公开的恒速感应电机数据集上进行测试，该数据集包含振动和声学传感器数据。PadéNet模型旨在引入增强的非线性，并与无界激活函数（如Leaky ReLU）兼容。", "result": "PadéNets始终优于基线模型，对于加速度计1、2、3和声学传感器分别达到了99.96%、98.26%、97.61%和98.33%的诊断精度。", "conclusion": "PadéNets增强的非线性及其与无界激活函数的兼容性显著提高了感应电机状态监测中的故障诊断性能。", "translation": "目的：本研究的主要目的是通过利用帕德近似神经元（PAON）模型来增强感应电机的故障诊断。虽然加速度计和麦克风是电机状态监测的标准配置，但具有非线性神经元架构的深度学习模型在诊断性能方面显示出有希望的改进。本研究旨在解决以下问题：帕德近似神经网络（PadéNets）在使用振动和声学数据诊断电气和机械故障方面能否超越传统的卷积神经网络（CNNs）和自组织运算神经网络（Self-ONNs）？\n方法：我们评估并比较了三种深度学习架构的诊断能力：一维卷积神经网络（CNNs）、自组织运算神经网络（Self-ONNs）和帕德近似神经网络（PadéNets）。这些模型在渥太华大学公开的恒速感应电机数据集上进行测试，该数据集包含振动和声学传感器数据。PadéNet模型旨在引入增强的非线性，并与无界激活函数（如Leaky ReLU）兼容。\n结果和结论：PadéNets始终优于基线模型，对于加速度计1、2、3和声学传感器分别达到了99.96%、98.26%、97.61%和98.33%的诊断精度。PadéNets增强的非线性及其与无界激活函数的兼容性显著提高了感应电机状态监测中的故障诊断性能。", "summary": "本研究旨在利用帕德近似神经网络（PadéNets）提升感应电机的故障诊断能力，并与传统的CNN和Self-ONN进行比较。研究评估了三种深度学习模型在渥太华大学公开的振动和声学数据集上的性能。结果显示，PadéNets在所有传感器数据上均优于基线模型，诊断精度最高达到99.96%，证明其增强的非线性和与无界激活函数的兼容性显著提高了故障诊断性能。", "keywords": "Padé近似神经网络,故障诊断,感应电机,振动数据,声学数据", "comments": "这篇论文的创新点在于引入了Padé近似神经网络（PadéNets）用于电机故障诊断，并证明了其在非线性处理方面的优势，尤其是在与无界激活函数结合时。研究通过与现有深度学习模型的比较，突出了PadéNets在提高诊断精度方面的潜力，为工业设备的状态监测提供了新的有效方法。"}}
{"id": "2507.02851", "title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs", "authors": ["Purbesh Mitra", "Sennur Ulukus"], "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.SY", "eess.SY", "math.IT"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02851v1", "summary": "Recent advancements in the reasoning capabilities of large language models\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\nfor reinforcement learning (RL) training allows the models to use more\nthinking/reasoning tokens for generating better responses. However, LLMs can\ngenerate only a finite amount of tokens while maintaining attention to the\npreviously generated tokens. This limit, also known as the context size of an\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\nTo think beyond the limit of context size, an LLM must employ a modular\nthinking strategy to reason over multiple rounds. In this work, we propose\n$\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\ntraining method for generating thinking tokens in multiple rounds, effectively\nallowing the model to think with additional context size. We trained the\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\nexperiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training\nin the respective benchmarks. Furthermore, this improvement was achieved with\nonly 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\nand models are available at https://github.com/purbeshmitra/MOTIF and\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02851v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MOTIF：通过强化微调实现大型语言模型的模块化思维", "tldr": "MOTIF是一种RL微调方法，通过多轮生成思维令牌，使LLM能够超越上下文限制进行推理，并在基准测试中表现出显著改进和样本效率。", "motivation": "现有大型语言模型（LLM）的推理能力受到上下文大小的限制，这阻碍了它们处理任意大量令牌的能力。为了克服这一瓶颈，LLM需要采用多轮模块化思维策略。", "method": "本文提出了MOTIF（通过强化微调实现模块化思维），这是一种强化学习（RL）训练方法，旨在通过在多轮中生成思维令牌，有效地扩展模型的上下文大小。研究人员通过参数高效微调，在GSM8K数据集上训练了开源模型Qwen2.5-3B-Instruct。", "result": "实验结果显示，在MATH500和AIME2024基准测试中，MOTIF相对于基于香草GRPO的训练分别实现了3.8%和3.3%的改进。此外，这些改进仅通过15%的样本就得以实现，证明了MOTIF的样本效率。", "conclusion": "MOTIF通过强化微调成功地使大型语言模型实现了模块化思维，有效扩展了模型的推理上下文，并在多个数学推理基准测试中取得了显著且样本高效的性能提升。", "translation": "大型语言模型（LLM）推理能力的最新进展表明，采用群相对策略优化（GRPO）算法进行强化学习（RL）训练可以使模型使用更多的思维/推理令牌来生成更好的响应。然而，LLM在保持对先前生成令牌的注意力时只能生成有限数量的令牌。这个限制，也被称为LLM的上下文大小，是LLM在处理任意大量令牌进行推理时的瓶颈。为了超越上下文大小的限制进行思考，LLM必须采用多轮模块化思维策略进行推理。在这项工作中，我们提出了$\textbf{MOTIF: 通过强化微调实现模块化思维}$——一种用于多轮生成思维令牌的RL训练方法，有效地允许模型以额外的上下文大小进行思考。我们通过参数高效微调在GSM8M数据集上训练了开源模型Qwen2.5-3B-Instruct，并在MATH500和AIME2024基准测试中测试了其准确性。我们的实验表明，在各自的基准测试中，相对于基于香草GRPO的训练，分别有3.8%和3.3%的改进。此外，这种改进仅用15%的样本就实现了，从而证明了MOTIF的样本效率。我们的代码和模型分别可在https://github.com/purbeshmitra/MOTIF和https://huggingface.co/purbeshmitra/MOTIF获取。", "summary": "本文介绍了MOTIF，一种通过强化学习微调实现大型语言模型（LLM）模块化思维的新方法。该方法通过在多轮中生成思维令牌，有效扩展了LLM的推理上下文大小，以克服现有LLM的上下文限制。实验结果表明，MOTIF在MATH500和AIME2024基准测试中，相对于基线GRPO训练分别取得了3.8%和3.3%的性能提升，并且展现出显著的样本效率。", "keywords": "模块化思维, 强化学习, 大型语言模型, 上下文大小, 微调", "comments": "MOTIF的创新之处在于通过强化学习微调，使LLM能够进行多轮思维，从而有效突破了传统上下文大小的限制，这对于需要处理复杂、长序列推理任务的LLM具有重要意义。其在仅使用少量样本的情况下取得显著性能提升，也展现了其高效性。"}}
{"id": "2507.02699", "title": "Control at Stake: Evaluating the Security Landscape of LLM-Driven Email Agents", "authors": ["Jiangrong Wu", "Yuhong Nan", "Jianliang Wu", "Zitong Yao", "Zibin Zheng"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02699v1", "summary": "The increasing capabilities of LLMs have led to the rapid proliferation of\nLLM agent apps, where developers enhance LLMs with access to external resources\nto support complex task execution. Among these, LLM email agent apps represent\none of the widely used categories, as email remains a critical communication\nmedium for users. LLM email agents are capable of managing and responding to\nemail using LLM-driven reasoning and autonomously executing user instructions\nvia external email APIs (e.g., send email). However, despite their growing\ndeployment and utility, the security mechanism of LLM email agent apps remains\nunderexplored. Currently, there is no comprehensive study into the potential\nsecurity risk within these agent apps and their broader implications.\n  In this paper, we conduct the first in-depth and systematic security study of\nLLM email agents. We propose the Email Agent Hijacking (EAH) attack, which\noverrides the original prompts of the email agent via external email resources,\nallowing attackers to gain control of the email agent remotely and further\nperform specific attack scenarios without user awareness.\n  To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to\nevaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an\nempirical study spanning 14 representative LLM agent frameworks, 63 agent apps,\n12 LLMs, and 20 email services, which led to the generation of 1,404 real-world\nemail agent instances for evaluation. Experimental results indicate that all\n1,404 instances were successfully hijacked; on average, only 2.03 attack\nattempts are required to control an email agent instance. Even worse, for some\nLLMs, the average number of attempts needed to achieve full agent control drops\nto as few as 1.23.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02699v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "控制权岌岌可危：评估LLM驱动的电子邮件代理的安全状况", "tldr": "大型语言模型（LLM）驱动的电子邮件代理存在严重安全漏洞，一项名为电子邮件代理劫持（EAH）的新型攻击能够以极少的尝试次数完全控制这些代理。", "motivation": "尽管LLM电子邮件代理应用部署日益广泛且实用性强，但其安全机制尚未得到充分探索。目前，缺乏对这些代理应用潜在安全风险及其广泛影响的全面研究。", "method": "本研究首次对LLM电子邮件代理进行了深入系统的安全研究。提出了一种名为电子邮件代理劫持（EAH）的攻击，该攻击通过外部电子邮件资源覆盖代理的原始提示，使攻击者能够远程控制电子邮件代理。为进行大规模评估，研究提出了EAHawk管道，并对14个代表性LLM代理框架、63个代理应用、12个LLM和20个电子邮件服务进行了实证研究，生成了1,404个真实世界的电子邮件代理实例进行评估。", "result": "实验结果表明，所有1,404个实例都成功被劫持。平均而言，仅需2.03次攻击尝试即可控制一个电子邮件代理实例。更糟糕的是，对于某些LLM，实现完全代理控制所需的平均尝试次数低至1.23次。", "conclusion": "LLM电子邮件代理存在严重的控制权劫持漏洞，特别是易受EAH攻击的影响，这表明迫切需要改进其安全机制。", "translation": "LLM能力日益增强，导致LLM代理应用的迅速普及，开发者通过让LLM访问外部资源来支持复杂的任务执行。其中，LLM电子邮件代理应用是广泛使用的类别之一，因为电子邮件仍然是用户重要的通信媒介。LLM电子邮件代理能够利用LLM驱动的推理管理和回复电子邮件，并通过外部电子邮件API（例如，发送电子邮件）自主执行用户指令。然而，尽管它们部署日益增多且实用性强，LLM电子邮件代理应用的安全机制仍未得到充分探索。目前，尚未有针对这些代理应用潜在安全风险及其更广泛影响的全面研究。\n在本文中，我们首次对LLM电子邮件代理进行了深入系统的安全研究。我们提出了一种名为电子邮件代理劫持（EAH）的攻击，该攻击通过外部电子邮件资源覆盖电子邮件代理的原始提示，使攻击者能够远程控制电子邮件代理，并进一步在用户不知情的情况下执行特定的攻击场景。\n为了促进大规模评估，我们提出了EAHawk，一个用于评估LLM电子邮件代理应用中EAH攻击的管道。通过EAHawk，我们对14个代表性LLM代理框架、63个代理应用、12个LLM和20个电子邮件服务进行了实证研究，生成了1,404个真实世界的电子邮件代理实例进行评估。实验结果表明，所有1,404个实例都成功被劫持；平均而言，仅需2.03次攻击尝试即可控制一个电子邮件代理实例。更糟糕的是，对于某些LLM，实现完全代理控制所需的平均尝试次数低至1.23次。", "summary": "本研究首次深入探讨了LLM驱动的电子邮件代理的安全问题。论文提出了一种名为“电子邮件代理劫持”（EAH）的新型攻击，该攻击允许攻击者通过外部电子邮件资源覆盖代理的原始提示，从而远程控制代理并执行恶意操作。为验证EAH攻击的有效性，研究设计了EAHawk评估管道，并对1404个真实世界的LLM电子邮件代理实例进行了大规模实证测试。结果显示，所有测试实例均能被成功劫持，平均仅需2.03次尝试即可完全控制一个代理，凸显了当前LLM电子邮件代理存在的严重安全漏洞。", "keywords": "LLM代理, 电子邮件安全, 劫持攻击, EAHawk, 漏洞", "comments": "本文首次对LLM驱动的电子邮件代理的安全性进行了系统性研究，并提出了创新的EAH攻击和EAHawk评估框架，揭示了这类日益普及的应用中存在的普遍且严重的控制权漏洞。其发现对于理解和改进LLM代理的安全机制具有重要意义，凸显了在部署此类系统时加强安全防护的紧迫性。"}}
{"id": "2507.02690", "title": "RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes", "authors": ["Jiaxing Wang", "Yifeng Yu", "Jiahan Song", "Bin Cao", "Jing Fan", "Ji Zhang"], "categories": ["cs.SE", "cs.LG"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      15 pages, 7 figures. Business process prediction using reinforcement learning and heterogeneous graph neural networks", "url": "http://arxiv.org/abs/2507.02690v1", "summary": "Next activity prediction represents a fundamental challenge for optimizing\nbusiness processes in service-oriented architectures such as microservices\nenvironments, distributed enterprise systems, and cloud-native platforms, which\nenables proactive resource allocation and dynamic service composition. Despite\nthe prevalence of sequence-based methods, these approaches fail to capture\nnon-sequential relationships that arise from parallel executions and\nconditional dependencies. Even though graph-based approaches address structural\npreservation, they suffer from homogeneous representations and static\nstructures that apply uniform modeling strategies regardless of individual\nprocess complexity characteristics. To address these limitations, we introduce\nRLHGNN, a novel framework that transforms event logs into heterogeneous process\ngraphs with three distinct edge types grounded in established process mining\ntheory. Our approach creates four flexible graph structures by selectively\ncombining these edges to accommodate different process complexities, and\nemploys reinforcement learning formulated as a Markov Decision Process to\nautomatically determine the optimal graph structure for each specific process\ninstance. RLHGNN then applies heterogeneous graph convolution with\nrelation-specific aggregation strategies to effectively predict the next\nactivity. This adaptive methodology enables precise modeling of both sequential\nand non-sequential relationships in service interactions. Comprehensive\nevaluation on six real-world datasets demonstrates that RLHGNN consistently\noutperforms state-of-the-art approaches. Furthermore, it maintains an inference\nlatency of approximately 1 ms per prediction, representing a highly practical\nsolution suitable for real-time business process monitoring applications. The\nsource code is available at https://github.com/Joker3993/RLHGNN.", "comment": "15 pages, 7 figures. Business process prediction using reinforcement\n  learning and heterogeneous graph neural networks", "pdf_url": "http://arxiv.org/pdf/2507.02690v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "RLHGNN：强化学习驱动的异构图神经网络用于业务流程中的下一步活动预测", "tldr": "RLHGNN是一个新的框架，它将事件日志转换为异构过程图，并使用强化学习自动确定最佳图结构，然后应用异构图卷积来预测下一步活动，在实时业务流程监控中表现优于现有方法。", "motivation": "现有的序列化方法无法捕获并行执行和条件依赖产生的非序列关系。图方法虽然解决了结构保存问题，但存在同构表示和静态结构的问题，未能考虑不同流程复杂性。", "method": "RLHGNN将事件日志转换为具有三种不同边类型的异构过程图。它通过选择性组合这些边创建四种灵活的图结构，以适应不同的流程复杂性。该方法采用强化学习（表述为马尔可夫决策过程）自动确定每个特定流程实例的最佳图结构。然后，RLHGNN应用具有关系特定聚合策略的异构图卷积来有效预测下一步活动。", "result": "在六个真实世界数据集上的综合评估表明，RLHGNN始终优于最先进的方法。此外，每次预测的推理延迟约为1毫秒，是一种高度实用的解决方案。", "conclusion": "RLHGNN通过自适应建模序列和非序列关系，实现了业务流程中下一步活动预测的精确建模，并在性能和实时性方面超越了现有方法，适用于实时业务流程监控应用。", "translation": "下一步活动预测是优化服务导向架构（如微服务环境、分布式企业系统和云原生平台）中业务流程的一个基本挑战，它能够实现主动资源分配和动态服务组合。尽管序列化方法普遍存在，但这些方法未能捕获并行执行和条件依赖产生的非序列关系。即使图方法解决了结构保存问题，它们也存在同构表示和静态结构的问题，无论个体过程复杂性特征如何，都采用统一的建模策略。为了解决这些限制，我们引入了RLHGNN，这是一个新颖的框架，它将事件日志转换为具有三种不同边类型（基于已建立的流程挖掘理论）的异构过程图。我们的方法通过选择性组合这些边，创建了四种灵活的图结构，以适应不同的流程复杂性，并采用强化学习（表述为马尔可夫决策过程）自动确定每个特定流程实例的最佳图结构。RLHGNN随后应用具有关系特定聚合策略的异构图卷积，以有效预测下一步活动。这种自适应方法能够精确建模服务交互中的序列和非序列关系。对六个真实世界数据集的综合评估表明，RLHGNN始终优于最先进的方法。此外，它每次预测的推理延迟约为1毫秒，代表了一种高度实用的解决方案，适用于实时业务流程监控应用。源代码可在https://github.com/Joker3993/RLHGNN获取。", "summary": "RLHGNN是一个创新的框架，旨在解决业务流程中下一步活动预测的挑战。它通过将事件日志转换为异构过程图，并利用强化学习为每个流程实例自动选择最佳图结构，从而克服了现有序列和图方法的局限性。该模型采用异构图卷积进行预测，能够有效捕获序列和非序列关系。实验证明，RLHGNN在性能上优于现有技术，并且具有低延迟，适用于实时业务流程监控。", "keywords": "下一步活动预测, 异构图神经网络, 强化学习, 业务流程, 流程挖掘", "comments": "RLHGNN的创新之处在于结合了异构图表示和强化学习来动态选择最佳图结构，这使其能够自适应地处理不同复杂度的业务流程，并有效捕获复杂的非序列关系。其实时推理能力使其在实际应用中具有高度实用性。"}}
{"id": "2507.02521", "title": "Safe and Socially Aware Multi-Robot Coordination in Multi-Human Social Care Settings", "authors": ["Ayodeji O. Abioye", "Jayati Deshmukh", "Athina Georgara", "Dominic Price", "Tuyen Nguyen", "Aleksandra Landowska", "Amel Bennaceur", "Joel E. Fischer", "Sarvapali D. Ramchurn"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      3 pages, 1 figure. Accepted for poster presentation at the UK AI Research Symposium (UKAIR) 2025, themed \"A Festival of Ideas\", being held in Newcastle from 8th - 9th September, 2025. this https URL", "url": "http://arxiv.org/abs/2507.02521v1", "summary": "This research investigates strategies for multi-robot coordination in\nmulti-human environments. It proposes a multi-objective learning-based\ncoordination approach to addressing the problem of path planning, navigation,\ntask scheduling, task allocation, and human-robot interaction in multi-human\nmulti-robot (MHMR) settings.", "comment": "3 pages, 1 figure. Accepted for poster presentation at the UK AI\n  Research Symposium (UKAIR) 2025, themed \"A Festival of Ideas\", being held in\n  Newcastle from 8th - 9th September, 2025. https://www.ukairs.ac.uk/", "pdf_url": "http://arxiv.org/pdf/2507.02521v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "多人类社会护理环境中安全且具有社会意识的多机器人协调", "tldr": "本研究提出了一种多目标学习方法，用于在多人类多机器人环境中解决多机器人协调问题，包括路径规划、导航、任务调度、任务分配和人机交互。", "motivation": "本研究旨在探索多人类环境中多机器人协调的策略。", "method": "本研究提出了一种多目标学习的协调方法，以解决多人类多机器人（MHMR）环境中的路径规划、导航、任务调度、任务分配和人机交互问题。", "result": "摘要中未提及", "conclusion": "摘要中未提及", "translation": "这项研究探讨了多人类环境中多机器人协调的策略。它提出了一种基于多目标学习的协调方法，以解决多人类多机器人（MHMR）环境中的路径规划、导航、任务调度、任务分配和人机交互问题。", "summary": "本研究提出了一种多目标学习的协调方法，用于解决多人类多机器人（MHMR）环境中的多机器人协调问题，涵盖路径规划、导航、任务调度、任务分配以及人机交互等方面。", "keywords": "多机器人协调, 多目标学习, 人机交互, 路径规划, 任务分配", "comments": "该论文的创新点在于将多目标学习应用于多机器人协调，特别是在复杂的、涉及多人类的社会护理场景中。其重要性在于为多机器人系统在现实世界，尤其是人机交互频繁的环境中的安全有效运行提供了潜在的解决方案。摘要中未提及实验结果和具体应用场景的详细信息，这可能限制了对其实际效果的评估。"}}
{"id": "2507.02432", "title": "Closed-Loop Rhythmic Haptic Biofeedback via Smartwatch for Relaxation and Sleep Onset", "authors": ["Jueun Lee", "Dennis Moschina", "Supraja Ramesh", "Tobias Röddiger", "Kai Kunze", "Michael Beigl"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures. Submitted to the International Symposium on Wearable Computers (ISWC)", "url": "http://arxiv.org/abs/2507.02432v1", "summary": "We investigate the use of musically structured, closed-loop vibration\npatterns as a passive biofeedback intervention for relaxation and sleep\ninitiation. By encoding rhythmic meter structures into smartwatch vibrations\nand adapting their frequency to be slightly slower than the user's real-time\nheart rate, our system aims to reduce arousal through tactile entrainment,\noffering a non-invasive alternative to auditory or open-loop approaches\npreviously used in sleep and anxiety contexts. In the first study (N=20), we\ncompared five adaptive vibration rhythms for their effects on heart rate and\nsubjective perceptions of relaxation in a resting context. In the second study\n(N=28), we evaluated the most promising pattern from Study 1 in a prolonged\nsleep initiation setting. Results showed increased parasympathetic activity and\nperceived relaxation during short-term stimulation, but no significant effects\non sleep-related measures during the sleep onset phase. This work contributes\nto the understanding of how wearable haptic feedback can support relaxation and\nsleep, offering design insights and identifying methodological considerations\nfor effectively integrating haptic interaction into self-directed\ninterventions.", "comment": "8 pages, 6 figures. Submitted to the International Symposium on\n  Wearable Computers (ISWC)", "pdf_url": "http://arxiv.org/pdf/2507.02432v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "智能手表闭环节律触觉生物反馈用于放松和入睡", "tldr": "本研究探索了一种通过智能手表提供闭环节律触觉生物反馈的新方法，旨在促进放松和帮助入睡。研究发现，短期使用可增加副交感神经活动和主观放松感，但对实际入睡没有显著效果。", "motivation": "本研究旨在探索一种非侵入性的替代方法，利用智能手表提供的音乐结构化、闭环振动模式作为被动生物反馈干预，以促进放松和帮助入睡，区别于以往的听觉或开环方法。", "method": "本研究通过智能手表将节律韵律结构编码成振动，并将其频率调整至略低于用户实时心率。通过触觉带动来降低唤醒度。研究分为两部分：第一项研究（N=20）比较了五种自适应振动节奏对心率和主观放松感的影响；第二项研究（N=28）评估了第一项研究中最有前景的模式在长时间入睡情境中的效果。", "result": "短期刺激下，研究结果显示副交感神经活动增加和感知放松度提高。然而，在入睡阶段，对睡眠相关指标没有显著影响。", "conclusion": "这项工作有助于理解可穿戴触觉反馈如何支持放松和睡眠，并为有效将触觉交互整合到自主干预中提供了设计见解和方法论考量。", "translation": "我们研究了将音乐结构化的闭环振动模式作为一种被动生物反馈干预，用于放松和帮助入睡。通过将节律韵律结构编码到智能手表振动中，并使其频率略低于用户的实时心率，我们的系统旨在通过触觉带动来降低唤醒度，为之前用于睡眠和焦虑情境中的听觉或开环方法提供一种非侵入性替代方案。在第一项研究（N=20）中，我们比较了五种自适应振动节奏在静息状态下对心率和主观放松感知的影响。在第二项研究（N=28）中，我们评估了第一项研究中最有前景的模式在长时间入睡情境中的效果。结果显示，在短期刺激期间，副交感神经活动增加和感知放松度提高，但在入睡阶段对睡眠相关指标没有显著影响。这项工作有助于理解可穿戴触觉反馈如何支持放松和睡眠，为有效将触觉交互整合到自主干预中提供了设计见解和方法论考量。", "summary": "本研究探索了一种通过智能手表提供闭环节律触觉生物反馈以促进放松和入睡的新型非侵入性干预方法。该系统通过调整振动频率以匹配并略低于用户心率来诱导触觉带动。两项研究（N=20和N=28）评估了不同振动模式的效果。结果表明，短期使用能有效增加副交感神经活动和主观放松感，但在入睡阶段对睡眠本身没有显著改善。这项工作为可穿戴触觉反馈在放松和睡眠支持方面的设计提供了见解，并指出了未来的方法学考量。", "keywords": "触觉生物反馈, 智能手表, 放松, 入睡, 触觉带动", "comments": "这项研究创新性地将闭环节律触觉生物反馈应用于智能手表，作为一种非侵入性手段来促进放松和睡眠。其重要性在于提供了一种区别于传统听觉或开环方法的替代方案，并为可穿戴设备在健康领域的应用提供了新的思路。尽管在短期放松方面取得了积极效果，但对实际入睡效果不显著是其局限性，这表明未来研究需进一步优化干预策略或探索更长时间的应用。"}}
{"id": "2507.02379", "title": "An AI-native experimental laboratory for autonomous biomolecular engineering", "authors": ["Mingyu Wu", "Zhaoguo Wang", "Jiabin Wang", "Zhiyuan Dong", "Jingkai Yang", "Qingting Li", "Tianyu Huang", "Lei Zhao", "Mingqiang Li", "Fei Wang", "Chunhai Fan", "Haibo Chen"], "categories": ["cs.AI", "q-bio.BM"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02379v1", "summary": "Autonomous scientific research, capable of independently conducting complex\nexperiments and serving non-specialists, represents a long-held aspiration.\nAchieving it requires a fundamental paradigm shift driven by artificial\nintelligence (AI). While autonomous experimental systems are emerging, they\nremain confined to areas featuring singular objectives and well-defined, simple\nexperimental workflows, such as chemical synthesis and catalysis. We present an\nAI-native autonomous laboratory, targeting highly complex scientific\nexperiments for applications like autonomous biomolecular engineering. This\nsystem autonomously manages instrumentation, formulates experiment-specific\nprocedures and optimization heuristics, and concurrently serves multiple user\nrequests. Founded on a co-design philosophy of models, experiments, and\ninstruments, the platform supports the co-evolution of AI models and the\nautomation system. This establishes an end-to-end, multi-user autonomous\nlaboratory that handles complex, multi-objective experiments across diverse\ninstrumentation. Our autonomous laboratory supports fundamental nucleic acid\nfunctions-including synthesis, transcription, amplification, and sequencing. It\nalso enables applications in fields such as disease diagnostics, drug\ndevelopment, and information storage. Without human intervention, it\nautonomously optimizes experimental performance to match state-of-the-art\nresults achieved by human scientists. In multi-user scenarios, the platform\nsignificantly improves instrument utilization and experimental efficiency. This\nplatform paves the way for advanced biomaterials research to overcome\ndependencies on experts and resource barriers, establishing a blueprint for\nscience-as-a-service at scale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02379v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "用于自主生物分子工程的AI原生实验实验室", "tldr": "该研究提出了一个AI原生自主实验室，能够独立进行复杂的生物分子工程实验，匹配人类科学家的性能，并支持多用户和多种仪器，为大规模科学即服务奠定基础。", "motivation": "自主科学研究，即能够独立进行复杂实验并服务非专业人员，是长期的愿望。然而，现有的自主实验系统仍局限于单一目标和简单工作流程的领域。因此，需要一个AI驱动的根本性范式转变来实现高度复杂的科学实验自动化。", "method": "本研究提出了一个AI原生自主实验室。该系统基于模型、实验和仪器的协同设计理念，能够自主管理仪器，制定实验特异性程序和优化启发式算法，并同时服务多个用户请求。平台支持AI模型和自动化系统的共同演进，从而建立了一个端到端的、多用户的自主实验室。", "result": "该自主实验室支持基本的核酸功能，包括合成、转录、扩增和测序。它还能应用于疾病诊断、药物开发和信息存储等领域。在无人干预的情况下，它能够自主优化实验性能，达到人类科学家所实现的最新水平。在多用户场景中，该平台显著提高了仪器利用率和实验效率。", "conclusion": "该平台为先进生物材料研究克服对专家和资源障碍的依赖铺平了道路，并为大规模的“科学即服务”建立了蓝图。", "translation": "自主科学研究，即能够独立进行复杂实验并服务非专业人员，代表着一个长期的愿望。实现这一目标需要人工智能（AI）驱动的根本性范式转变。尽管自主实验系统正在兴起，但它们仍局限于具有单一目标和明确、简单实验工作流程的领域，例如化学合成和催化。我们提出了一个AI原生自主实验室，旨在进行高度复杂的科学实验，应用于自主生物分子工程等领域。该系统自主管理仪器，制定实验特异性程序和优化启发式算法，并同时服务多个用户请求。该平台建立在模型、实验和仪器协同设计理念的基础上，支持AI模型和自动化系统的共同演进。这建立了一个端到端、多用户的自主实验室，能够处理跨越不同仪器的复杂、多目标实验。我们的自主实验室支持基本的核酸功能——包括合成、转录、扩增和测序。它还支持疾病诊断、药物开发和信息存储等领域的应用。在没有人为干预的情况下，它自主优化实验性能，以匹配人类科学家所实现的最新水平。在多用户场景中，该平台显著提高了仪器利用率和实验效率。该平台为先进生物材料研究克服对专家和资源障碍的依赖铺平了道路，为大规模的“科学即服务”建立了蓝图。", "summary": "本研究提出了一种AI原生自主实验室，旨在实现复杂的生物分子工程实验自动化。该系统通过协同设计模型、实验和仪器，能够自主管理仪器、制定实验程序，并同时服务多用户，从而支持AI模型与自动化系统的共同演进。该平台能处理多目标复杂实验，支持核酸合成、测序等功能，并应用于疾病诊断和药物开发。它在无人干预下可达到人类科学家的实验水平，显著提高多用户场景下的仪器利用率和效率，为大规模“科学即服务”奠定了基础。", "keywords": "AI原生实验室, 生物分子工程, 自主科学, 自动化, 协同设计", "comments": "该论文的创新之处在于其构建了一个真正的“AI原生”自主实验室，能够处理传统自主系统难以应对的高度复杂、多目标科学实验。特别是其“模型、实验、仪器协同设计”的理念，使得AI与自动化系统能够共同进化，这对于实现更高级别的科学自动化至关重要。该系统在无需人类干预的情况下达到人类科学家的性能水平，并支持多用户和多种仪器，为未来生物材料研究和“科学即服务”模式提供了重要的蓝图。"}}
{"id": "2507.02271", "title": "Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation", "authors": ["Feizhen Huang", "Yu Wu", "Yutian Lin", "Bo Du"], "categories": ["cs.CV", "cs.AI", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by IJCAI 2025", "url": "http://arxiv.org/abs/2507.02271v1", "summary": "Video-to-Audio (V2A) Generation achieves significant progress and plays a\ncrucial role in film and video post-production. However, current methods\noverlook the cinematic language, a critical component of artistic expression in\nfilmmaking. As a result, their performance deteriorates in scenarios where\nFoley targets are only partially visible. To address this challenge, we propose\na simple self-distillation approach to extend V2A models to cinematic language\nscenarios. By simulating the cinematic language variations, the student model\nlearns to align the video features of training pairs with the same audio-visual\ncorrespondences, enabling it to effectively capture the associations between\nsounds and partial visual information. Our method not only achieves impressive\nimprovements under partial visibility across all evaluation metrics, but also\nenhances performance on the large-scale V2A dataset, VGGSound.", "comment": "Accepted by IJCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.02271v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过自蒸馏突出部分可见电影语言的视频到音频生成", "tldr": "本文提出一种自蒸馏方法，使视频到音频生成模型能处理部分可见电影语言场景，并在该场景和大型数据集上均取得显著提升。", "motivation": "现有视频到音频生成方法忽视了电影语言，导致在拟音目标部分可见的场景下性能下降。", "method": "提出一种简单的自蒸馏方法，通过模拟电影语言变体，使学生模型学习对齐具有相同视听对应关系的训练对的视频特征，从而捕获声音与部分视觉信息之间的关联。", "result": "该方法不仅在部分可见场景下所有评估指标上实现了显著改进，而且还在大型V2A数据集VGGSound上提升了性能。", "conclusion": "本文提出的自蒸馏方法有效解决了视频到音频生成中部分可见电影语言的挑战，显著提升了模型在复杂场景下的性能。", "translation": "视频到音频（V2A）生成取得了显著进展，并在电影和视频后期制作中发挥着关键作用。然而，当前方法忽视了电影语言，这是电影制作中艺术表达的关键组成部分。因此，在拟音目标仅部分可见的场景中，它们的性能会下降。为了解决这一挑战，我们提出了一种简单的自蒸馏方法，将V2A模型扩展到电影语言场景。通过模拟电影语言的变化，学生模型学习对齐具有相同视听对应关系的训练对的视频特征，使其能够有效捕获声音与部分视觉信息之间的关联。我们的方法不仅在部分可见性下所有评估指标上都取得了令人印象深刻的改进，而且还在大型V2A数据集VGGSound上提升了性能。", "summary": "本文针对视频到音频（V2A）生成中现有方法忽视电影语言，导致在部分可见场景下性能下降的问题，提出了一种简单的自蒸馏方法。该方法通过模拟电影语言变体，使模型能够学习并捕获声音与部分视觉信息之间的关联。实验结果表明，该方法在部分可见场景下以及大型V2A数据集VGGSound上均取得了显著的性能提升。", "keywords": "视频到音频生成, 电影语言, 自蒸馏, 部分可见性, VGGSound", "comments": "该论文的创新点在于提出了一个简单但有效的自蒸馏方法，专门解决了视频到音频生成中“部分可见电影语言”这一之前被忽视的关键挑战。这对于提升电影和视频后期制作中V2A模型的鲁棒性和艺术表现力具有重要意义。该方法通过模拟电影语言变体，使模型能够更好地理解复杂视觉信息与声音之间的关联，具有较强的实用价值。"}}
{"id": "2507.01996", "title": "A Data-Driven Model Predictive Controller to manage epidemics: The case of SARS-CoV-2 in Mauritius", "authors": ["S. Z. Sayed Hassen", "A. Aboudonia", "J. Lygeros"], "categories": ["q-bio.PE", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Populations and Evolution (q-bio.PE)", "pdf_link": null, "comments": "Comments:      6 pages, 6 figures, European Control Conference 2025", "url": "http://arxiv.org/abs/2507.01996v1", "summary": "This work investigates the benefits of implementing a systematic approach to\nsocial isolation policies during epidemics. We develop a mixed integer\ndata-driven model predictive control (MPC) scheme based on an SIHRD model which\nis identified from available data. The case of the spread of the SARS-CoV-2\nvirus (also known as COVID-19) in Mauritius is used as a reference point with\ndata obtained during the period December 2021 to May 2022. The isolation scheme\nis designed with the control decision variable taking a finite set of values\ncorresponding to the desired level of isolation. The control input is further\nrestricted to shifting between levels only after a minimum amount of time. The\nsimulation results validate our design, showing that the need for\nhospitalisation remains within the capacity of the health centres, with the\nnumber of deaths considerably reduced by raising the level of isolation for\nshort periods of time with negligible social and economic impact. We also show\nthat the introduction of additional isolation levels results in a smoother\ncontainment approach with a considerably reduced hospitalisation burden.", "comment": "6 pages, 6 figures, European Control Conference 2025", "pdf_url": "http://arxiv.org/pdf/2507.01996v1", "cate": "q-bio.PE", "date": "2025-06-30", "updated": "2025-06-30", "AI": {"title_translation": "数据驱动模型预测控制器在疫情管理中的应用：以毛里求斯SARS-CoV-2为例", "tldr": "本研究开发了一种数据驱动的模型预测控制（MPC）方案，用于管理疫情期间的社会隔离政策。以毛里求斯SARS-CoV-2疫情为例进行验证，结果显示该方案能有效控制住院需求并显著降低死亡人数，同时社会经济影响可忽略不计。", "motivation": "这项工作旨在研究在流行病期间实施系统性社会隔离政策的益处。", "method": "本研究开发了一种基于SIHRD模型的混合整数数据驱动模型预测控制（MPC）方案，该模型通过毛里求斯2021年12月至2022年5月的SARS-CoV-2传播数据进行识别。隔离方案的控制决策变量取有限值，对应于期望的隔离水平，且控制输入限制为在最短时间后才能在不同级别之间切换。", "result": "模拟结果验证了设计的有效性，显示住院需求保持在医疗中心容量内。通过在短时间内提高隔离水平，死亡人数显著减少，且社会和经济影响可忽略不计。此外，引入额外的隔离级别可实现更平滑的遏制方法，并显著减轻住院负担。", "conclusion": "系统性地实施数据驱动的模型预测控制方案，通过调整社会隔离政策，可以有效管理疫情，降低住院率和死亡率，同时最大限度地减少社会经济影响。", "translation": "这项工作研究了在流行病期间实施系统性社会隔离政策的益处。我们开发了一种基于SIHRD模型的混合整数数据驱动模型预测控制（MPC）方案，该模型从可用数据中识别。以毛里求斯SARS-CoV-2病毒（也称为COVID-19）的传播为例，使用2021年12月至2022年5月期间获得的数据作为参考点。隔离方案设计中，控制决策变量取一组有限值，对应于期望的隔离水平。控制输入进一步限制为只有在最短时间后才能在各级别之间切换。模拟结果验证了我们的设计，表明住院需求保持在医疗中心容量内，通过在短时间内提高隔离水平，死亡人数大大减少，且社会和经济影响可忽略不计。我们还表明，引入额外的隔离级别可以实现更平滑的遏制方法，并显著减轻住院负担。", "summary": "本研究开发了一种数据驱动的混合整数模型预测控制（MPC）方案，用于管理流行病期间的社会隔离政策。该方案基于SIHRD模型，并以毛里求斯SARS-CoV-2疫情数据进行验证。结果表明，通过短期提高隔离水平，该方法能有效控制住院需求在医疗系统容量内，显著降低死亡率，并减轻住院负担，同时社会经济影响可忽略不计。", "keywords": "数据驱动, 模型预测控制, 疫情管理, 社会隔离, SARS-CoV-2", "comments": "创新性：将数据驱动的模型预测控制应用于流行病社会隔离政策的管理，考虑了隔离水平的有限性和切换时间限制。重要性：为疫情管理提供了一种系统性、量化的决策工具，有望在控制疫情传播、减轻医疗系统压力和减少死亡人数方面发挥积极作用。局限性：研究基于模拟结果，实际应用可能面临数据实时性、模型准确性以及社会接受度等挑战。"}}
{"id": "2507.02119", "title": "Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks", "authors": ["Shikai Qiu", "Lechao Xiao", "Andrew Gordon Wilson", "Jeffrey Pennington", "Atish Agarwala"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 25. Code available at this https URL", "url": "http://arxiv.org/abs/2507.02119v1", "summary": "What scaling limits govern neural network training dynamics when model size\nand training time grow in tandem? We show that despite the complex interactions\nbetween architecture, training algorithms, and data, compute-optimally trained\nmodels exhibit a remarkably precise universality. Specifically, loss curves\nfrom models of varying sizes collapse onto a single universal curve when\ntraining compute and loss are normalized to unity at the end of training. With\nlearning rate decay, the collapse becomes so tight that differences in the\nnormalized curves across models fall below the noise floor of individual loss\ncurves across random seeds, a phenomenon we term supercollapse. We observe\nsupercollapse across learning rate schedules, datasets, and architectures,\nincluding transformers trained on next-token prediction, and find it breaks\ndown when hyperparameters are scaled suboptimally, providing a precise and\npractical indicator of good scaling. We explain these phenomena by connecting\ncollapse to the power-law structure in typical neural scaling laws, and\nanalyzing a simple yet surprisingly effective model of SGD noise dynamics that\naccurately predicts loss curves across various learning rate schedules and\nquantitatively explains the origin of supercollapse.", "comment": "ICML 25. Code available at https://github.com/shikaiqiu/supercollapse", "pdf_url": "http://arxiv.org/pdf/2507.02119v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "尺度坍缩揭示计算最优训练神经网络中的普适动力学", "tldr": "研究发现，尽管架构、训练算法和数据之间存在复杂交互，计算最优训练模型的损失曲线在训练计算量和损失归一化后会坍缩成一条单一的普适曲线，在学习率衰减下甚至出现“超坍缩”现象，这可作为良好扩展性的实用指标。", "motivation": "当模型大小和训练时间同步增长时，什么扩展限制支配着神经网络的训练动力学？", "method": "研究表明，在计算最优训练下，不同大小模型的损失曲线在训练计算量和损失归一化后会坍缩成一条单一的普适曲线。他们观察到在学习率衰减下出现“超坍缩”现象，并将其与典型神经扩展定律中的幂律结构联系起来，同时分析了一个简单的SGD噪声动力学模型来解释这些现象。", "result": "计算最优训练模型的损失曲线会坍缩成一条单一的普适曲线。在学习率衰减下，这种坍缩变得非常紧密，使得归一化曲线之间的差异低于单个损失曲线在不同随机种子下的噪声水平，即“超坍缩”现象。这种现象在学习率调度、数据集和架构（包括Transformer）中普遍存在，但在超参数次优缩放时会失效，这提供了一个精确实用的良好扩展性指标。", "conclusion": "通过将坍缩现象与典型神经扩展定律中的幂律结构联系起来，并分析一个简单而有效的SGD噪声动力学模型，可以准确预测各种学习率调度下的损失曲线并定量解释超坍缩的起源。", "translation": "当模型大小和训练时间同步增长时，什么扩展限制支配着神经网络的训练动力学？我们发现，尽管架构、训练算法和数据之间存在复杂交互，计算最优训练的模型表现出惊人的精确普适性。具体来说，当训练计算量和损失在训练结束时归一化为一后，不同大小模型的损失曲线会坍缩成一条单一的普适曲线。在学习率衰减的情况下，这种坍缩变得如此紧密，以至于归一化曲线在不同模型之间的差异低于单个损失曲线在不同随机种子下的噪声水平，我们称之为“超坍缩”现象。我们在学习率调度、数据集和架构（包括在下一词元预测上训练的Transformer）中观察到超坍缩，并发现当超参数次优缩放时它会失效，这提供了一个精确实用的良好扩展性指标。我们通过将坍缩与典型神经扩展定律中的幂律结构联系起来，并分析一个简单但出奇有效的SGD噪声动力学模型来解释这些现象，该模型准确预测了各种学习率调度下的损失曲线，并定量解释了超坍缩的起源。", "summary": "本研究探讨了模型大小和训练时间同步增长时神经网络训练动力学的扩展限制。结果显示，计算最优训练的神经网络模型展示出显著的普适性：不同大小模型的损失曲线在归一化后会坍缩为一条单一的普适曲线。特别地，在学习率衰减下会发生“超坍缩”现象，即归一化曲线的差异小于噪声水平，这可作为判断良好扩展性的精确指标。文章通过将坍缩现象与幂律扩展定律联系起来，并分析一个SGD噪声动力学模型来解释这些普遍现象。", "keywords": "尺度坍缩, 普适动力学, 神经网络, 计算最优训练, 超坍缩", "comments": "本文提出了“尺度坍缩”和“超坍缩”这两个新颖的概念，揭示了计算最优训练神经网络中普遍存在的训练动力学规律。超坍缩现象作为衡量超参数扩展是否最优的实用指标，具有重要的实践意义，为神经网络的有效扩展性研究提供了新的视角和工具。"}}
{"id": "2507.02592", "title": "WebSailor: Navigating Super-human Reasoning for Web Agent", "authors": ["Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Liwen Zhang", "Litu Ou", "Jialong Wu", "Wenbiao Yin", "Baixuan Li", "Zhengwei Tao", "Xinyu Wang", "Weizhou Shen", "Junkai Zhang", "Dingchu Zhang", "Xixi Wu", "Yong Jiang", "Ming Yan", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02592v1", "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02592v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "WebSailor：为网络代理导航超人推理", "tldr": "WebSailor是一种后训练方法，使开源网络代理在复杂信息搜索任务中达到超人推理能力，媲美专有代理性能。", "motivation": "弥补开源LLM代理在处理复杂信息搜索任务时，缺乏像专有系统那样系统性地降低不确定性的能力，从而达到超人推理水平。", "method": "WebSailor是一个完整的后训练方法，包括通过结构化采样和信息混淆生成高不确定性任务、RFT冷启动，以及高效的代理强化学习训练算法——重复采样策略优化（DUPO）。", "result": "WebSailor在复杂信息搜索任务中显著优于所有开源代理，并能媲美专有代理的性能，缩小了能力差距。", "conclusion": "WebSailor成功地使开源代理在复杂信息搜索任务中实现了超人推理能力，缩小了与专有代理之间的差距。", "translation": "超越人类认知极限代表着LLM训练的一个关键前沿。像DeepResearch这样的专有代理系统在BrowseComp等极其复杂的信息搜索基准测试中展现出超人能力，这是以前无法实现的壮举。我们认为它们的成功取决于开源模型中缺乏的一种复杂推理模式：在导航广阔信息景观时系统性地降低极端不确定性的能力。基于这一洞察，我们引入了WebSailor，一种完整的后训练方法，旨在灌输这种关键能力。我们的方法包括通过结构化采样和信息混淆生成新颖的、高不确定性任务，RFT冷启动，以及一种高效的代理强化学习训练算法——重复采样策略优化（DUPO）。通过这个集成管道，WebSailor在复杂信息搜索任务中显著优于所有开源代理，与专有代理的性能相匹配，并缩小了能力差距。", "summary": "WebSailor通过系统性地降低不确定性来弥补开源LLM代理在处理复杂信息搜索任务方面的不足，这种能力在专有超人系统中可见。它引入了一种后训练方法，包括新颖的任务生成、RFT冷启动和DUPO算法。这种方法使WebSailor能够显著优于其他开源代理，并与专有代理的性能相匹配，从而缩小了超人推理能力方面的差距。", "keywords": "WebSailor, 网络代理, 超人推理, LLM训练, 强化学习", "comments": "创新点在于识别了开源模型在处理不确定性方面的不足，并提出了一个包含新任务生成和新型RL算法（DUPO）的完整后训练方法。其重要性在于显著提升了开源代理在复杂信息搜索任务中的表现，使其达到甚至媲美专有代理的超人水平，缩小了能力差距。"}}
{"id": "2507.02727", "title": "Quantifying Classifier Utility under Local Differential Privacy", "authors": ["Ye Zheng", "Yidan Hu"], "categories": ["cs.CR", "E.3"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02727v1", "summary": "Local differential privacy (LDP) provides a rigorous and quantifiable privacy\nguarantee for personal data by introducing perturbation at the data source.\nHowever, quantifying the impact of these perturbations on classifier utility\nremains a theoretical challenge, particularly for complex or black-box\nclassifiers.\n  This paper presents a framework for theoretically quantifying classifier\nutility under LDP mechanisms. The key insight is that LDP perturbation is\nconcentrated around the original data with a specific probability, transforming\nutility analysis of the classifier into its robustness analysis in this\nconcentrated region. Our framework connects the concentration analysis of LDP\nmechanisms with the robustness analysis of classifiers. It treats LDP\nmechanisms as general distributional functions and classifiers as black-box\nfunctions, thus applicable to any LDP mechanism and classifier. A direct\napplication of our utility quantification is guiding the selection of LDP\nmechanisms and privacy parameters for a given classifier. Notably, our analysis\nshows that a piecewise-based mechanism leads to better utility compared to\nalternatives in common scenarios.\n  Using this framework alongside two novel refinement techniques, we conduct\ncase studies on utility quantification for typical mechanism-classifier\ncombinations. The results demonstrate that our theoretical utility\nquantification aligns closely with empirical observations, particularly when\nclassifiers operate in lower-dimensional input spaces.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02727v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "局部差分隐私下分类器效用的量化", "tldr": "本文提出了一个理论框架，用于量化局部差分隐私（LDP）下分类器的效用，通过将LDP扰动集中度分析与分类器鲁棒性分析相结合，发现分段式机制在常见场景下表现更好，且理论结果与经验观察一致。", "motivation": "在局部差分隐私（LDP）机制下，量化数据扰动对分类器效用的影响是一个理论挑战，特别是对于复杂或黑盒分类器。", "method": "本文提出了一个理论框架，将局部差分隐私（LDP）下分类器的效用分析转化为其在扰动集中区域内的鲁棒性分析。该框架连接了LDP机制的集中度分析与分类器的鲁棒性分析，将LDP机制视为广义分布函数，分类器视为黑盒函数，使其适用于任何LDP机制和分类器。此外，研究还使用了两种新颖的细化技术并进行了案例研究。", "result": "该效用量化框架可以直接指导LDP机制和隐私参数的选择。分析表明，在常见场景下，分段式机制比其他替代方案能带来更好的效用。理论效用量化结果与经验观察结果高度吻合，尤其是在分类器处理低维输入空间时。", "conclusion": "本文成功提出了一个在局部差分隐私（LDP）下量化分类器效用的通用理论框架，该框架能有效指导LDP机制和参数的选择，并且其理论分析结果与经验观察结果高度一致，尤其适用于低维数据。", "translation": "局部差分隐私（LDP）通过在数据源引入扰动，为个人数据提供了严谨且可量化的隐私保证。然而，量化这些扰动对分类器效用的影响仍然是一个理论挑战，特别是对于复杂或黑盒分类器。\n本文提出了一个理论框架，用于理论上量化LDP机制下的分类器效用。关键的见解是，LDP扰动以特定概率集中在原始数据周围，从而将分类器的效用分析转化为其在该集中区域内的鲁棒性分析。我们的框架将LDP机制的集中度分析与分类器的鲁棒性分析联系起来。它将LDP机制视为广义分布函数，将分类器视为黑盒函数，因此适用于任何LDP机制和分类器。我们的效用量化框架的一个直接应用是指导给定分类器的LDP机制和隐私参数的选择。值得注意的是，我们的分析表明，在常见场景下，分段式机制比其他替代方案能带来更好的效用。\n利用该框架以及两种新颖的细化技术，我们对典型机制-分类器组合的效用量化进行了案例研究。结果表明，我们的理论效用量化与经验观察结果高度吻合，特别是在分类器在低维输入空间中运行时。", "summary": "本文提出了一个通用的理论框架，用于量化局部差分隐私（LDP）下分类器的效用。该框架通过将LDP扰动的集中度分析与分类器的鲁棒性分析相结合，解决了现有量化挑战。研究表明，该框架适用于任何LDP机制和黑盒分类器，并且可以指导LDP机制和隐私参数的选择。实验结果验证了理论量化与经验观察的一致性，特别是在低维输入空间中，并指出分段式机制在常见场景下表现出更好的效用。", "keywords": "局部差分隐私, 分类器效用, 鲁棒性分析, 隐私保护, 数据扰动", "comments": "该论文的创新之处在于提出了一个通用的理论框架，将局部差分隐私（LDP）下的分类器效用量化问题，巧妙地转化为鲁棒性分析。这种连接集中度分析与鲁棒性分析的方法具有普适性，适用于各种LDP机制和黑盒分类器，极大地拓展了LDP效用分析的范围。其结果不仅提供了实用的指导，例如建议使用分段式机制，还通过与经验观察的一致性增强了理论的可靠性。"}}
{"id": "2507.02695", "title": "Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms", "authors": ["Sahar Ahmadisakha", "Lech Bialek", "Mohamed Soliman", "Vasilios Andrikopoulos"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02695v1", "summary": "In recent years, sustainability in software systems has gained significant\nattention, especially with the rise of cloud computing and the shift towards\ncloud-based architectures. This shift has intensified the need to identify\nsustainability in architectural discussions to take informed architectural\ndecisions. One source to see these decisions is in online Q&A forums among\npractitioners' discussions. However, recognizing sustainability concepts within\nsoftware practitioners' discussions remains challenging due to the lack of\nclear and distinct guidelines for this task. To address this issue, we\nintroduce the notion of sustainability flags as pointers in relevant\ndiscussions, developed through thematic analysis of multiple sustainability\nbest practices from cloud providers. This study further evaluates the\neffectiveness of these flags in identifying sustainability within cloud\narchitecture posts, using a controlled experiment. Preliminary results suggest\nthat the use of flags results in classifying fewer posts as\nsustainability-related compared to a control group, with moderately higher\ncertainty and significantly improved performance. Moreover, sustainability\nflags are perceived as more useful and understandable than relying solely on\ndefinitions for identifying sustainability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02695v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "可持续性标志在问答平台中识别可持续性帖子的应用", "tldr": "本研究引入了“可持续性标志”的概念，通过对云提供商最佳实践的主题分析开发，旨在解决在软件从业者讨论中识别可持续性概念的挑战。初步结果显示，使用这些标志在识别可持续性相关帖子时，分类数量减少但确定性更高且性能显著提升。", "motivation": "随着云计算的兴起和向基于云的架构的转变，在架构讨论中识别可持续性以做出明智的架构决策变得越来越重要。然而，由于缺乏明确的指导方针，在软件从业者的在线问答论坛讨论中识别可持续性概念仍然具有挑战性。", "method": "研究通过对多个云提供商的可持续性最佳实践进行主题分析，开发了“可持续性标志”作为相关讨论中的指示。随后，通过一项受控实验评估了这些标志在识别云架构帖子中可持续性方面的有效性。", "result": "初步结果表明，与对照组相比，使用可持续性标志将更少的帖子归类为可持续性相关，但具有中等程度的更高确定性和显著改善的性能。此外，可持续性标志被认为比单独依赖定义在识别可持续性方面更有用和更易于理解。", "conclusion": "可持续性标志是一种有效且实用的工具，可以帮助在在线问答平台中更准确、高效地识别与软件系统可持续性相关的讨论，并且其用户感知度良好。", "translation": "近年来，软件系统的可持续性受到了广泛关注，尤其随着云计算的兴起和向基于云的架构的转变。这种转变加剧了在架构讨论中识别可持续性的需求，以便做出明智的架构决策。在线问答论坛中从业者的讨论是观察这些决策的一个来源。然而，由于缺乏明确和独特的指导方针，在软件从业者的讨论中识别可持续性概念仍然具有挑战性。为了解决这个问题，我们引入了可持续性标志的概念，作为相关讨论中的指示，这些标志是通过对云提供商的多个可持续性最佳实践进行主题分析而开发的。本研究通过一项受控实验，进一步评估了这些标志在识别云架构帖子中可持续性方面的有效性。初步结果表明，与对照组相比，使用标志导致将更少的帖子归类为可持续性相关，但具有中等程度的更高确定性和显著改善的性能。此外，可持续性标志被认为比单独依赖定义在识别可持续性方面更有用和更易于理解。", "summary": "本研究提出并评估了“可持续性标志”的概念，旨在解决在软件从业者在线问答平台中识别可持续性讨论的难题。这些标志通过主题分析云提供商的最佳实践而开发。实验结果表明，与传统方法相比，使用可持续性标志能够以更高的确定性和显著提升的性能，更有效地识别相关帖子，并且用户认为其更实用和易懂。", "keywords": "可持续性标志, Q&A平台, 云计算, 软件可持续性, 主题分析", "comments": "这项研究的创新之处在于引入了“可持续性标志”这一新颖的概念，以结构化地识别在线讨论中的可持续性信息，解决了现有方法中缺乏明确指导的问题。其重要性在于为软件架构师和从业者提供了一个更高效、准确的工具，以在海量在线数据中提取关键的可持续性见解，从而支持更明智的决策。虽然抽象中提到了“初步结果”，但其已显示出积极的效果，未来研究可以进一步验证其在更广泛情境下的普适性和鲁棒性。"}}
{"id": "2507.02547", "title": "Vibration of Soft, Twisted Beams for Under-Actuated Quadrupedal Locomotion", "authors": ["Yuhao Jiang", "Fuchen Chen", "Jamie Paik", "Daniel M. Aukes"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This manuscript is under revision for possible publication in the IEEE/ASME Transactions on Mechatronics. Copyright may be transferred to IEEE if the manuscript is accepted for publication, without further notice. Supplementary videos: this https URL , this https URL", "url": "http://arxiv.org/abs/2507.02547v1", "summary": "Under-actuated compliant robotic systems offer a promising approach to\nmitigating actuation and control challenges by harnessing pre-designed,\nembodied dynamic behaviors. This paper presents Flix-Walker, a novel,\nuntethered, centimeter-scale quadrupedal robot inspired by compliant\nunder-actuated mechanisms. Flix-Walker employs flexible, helix-shaped beams as\nlegs, which are actuated by vibrations from just two motors to achieve three\ndistinct mobility modes. We analyze the actuation parameters required to\ngenerate various locomotion modes through both simulation and prototype\nexperiments. The effects of system and environmental variations on locomotion\nperformance are examined, and we propose a generic metric for selecting control\nparameters that produce robust and functional motions. Experiments validate the\neffectiveness and robustness of these actuation parameters within a closed-loop\ncontrol framework, demonstrating reliable trajectory-tracking and\nself-navigation capabilities.", "comment": "This manuscript is under revision for possible publication in the\n  IEEE/ASME Transactions on Mechatronics. Copyright may be transferred to IEEE\n  if the manuscript is accepted for publication, without further notice.\n  Supplementary videos: https://youtu.be/T3d6FT3Rx-s,\n  https://youtu.be/nPQrhKlN02E", "pdf_url": "http://arxiv.org/pdf/2507.02547v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "欠驱动四足运动中软扭曲梁的振动", "tldr": "本文介绍了一种名为Flix-Walker的新型欠驱动厘米级四足机器人，它利用柔性螺旋形梁通过两个电机的振动实现三种不同的运动模式，并通过仿真和实验验证了其有效性和鲁棒性。", "motivation": "欠驱动柔顺机器人系统通过利用预先设计的、具身化的动态行为，为缓解驱动和控制挑战提供了一种有前景的方法。", "method": "本文提出了Flix-Walker，一种受柔顺欠驱动机制启发的厘米级、非系留四足机器人。Flix-Walker使用柔性螺旋形梁作为腿，通过两个电机的振动实现三种不同的运动模式。研究通过仿真和原型实验分析了生成各种运动模式所需的驱动参数，并检查了系统和环境变化对运动性能的影响。文章提出了一种用于选择能产生鲁棒和功能性运动的控制参数的通用度量。", "result": "Flix-Walker机器人实现了三种不同的移动模式。实验验证了这些驱动参数在闭环控制框架内的有效性和鲁棒性，展示了可靠的轨迹跟踪和自主导航能力。", "conclusion": "该研究成功地验证了通过振动驱动柔性扭曲梁实现欠驱动四足运动的有效性和鲁棒性，并提出了一个通用的控制参数选择度量，使得机器人能够展现可靠的轨迹跟踪和自主导航能力。", "translation": "欠驱动柔顺机器人系统通过利用预先设计的、具身化的动态行为，为缓解驱动和控制挑战提供了一种有前景的方法。本文介绍了Flix-Walker，一种受柔顺欠驱动机制启发的新型、非系留、厘米级四足机器人。Flix-Walker使用柔性螺旋形梁作为腿，通过仅两个电机的振动实现三种不同的移动模式。我们通过仿真和原型实验分析了生成各种运动模式所需的驱动参数。研究检查了系统和环境变化对运动性能的影响，并提出了一种用于选择能产生鲁棒和功能性运动的控制参数的通用度量。实验验证了这些驱动参数在闭环控制框架内的有效性和鲁棒性，展示了可靠的轨迹跟踪和自主导航能力。", "summary": "本文介绍了Flix-Walker，一种创新的厘米级欠驱动四足机器人，其腿部由柔性螺旋形梁构成，仅通过两个电机的振动即可实现多种运动模式。研究通过仿真和实验分析了驱动参数，并提出了一个通用的控制参数选择度量，以确保运动的鲁棒性。实验结果验证了所提出驱动参数在闭环控制下实现可靠轨迹跟踪和自主导航的有效性和鲁棒性。", "keywords": "欠驱动机器人, 四足运动, 柔性梁, 振动驱动, Flix-Walker", "comments": "本文的创新之处在于利用柔性、螺旋形梁作为机器人腿部，并通过简单的振动（仅两个电机）实现了复杂的欠驱动四足运动。这种设计为小型、低成本且具有多功能移动能力的机器人提供了一种新颖的解决方案，对于解决传统机器人设计中驱动和控制的复杂性具有重要意义。其提出的通用度量方法也增加了研究的实用性。"}}
{"id": "2507.02453", "title": "Haptic Biofeedback for Wakeful Rest: Does Stimulation Location Make a Difference?", "authors": ["Jueun Lee", "Martin Flipe", "Philipp Lepold", "Tobias Röddiger", "Michael Beigl"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      8 pages, 6 figures. Submitted to the International Symposium on Wearable Computers (ISWC)", "url": "http://arxiv.org/abs/2507.02453v1", "summary": "Wearable haptic interventions offer promising support for relaxation through\nslow, vibrotactile biofeedback. Despite their potential, current applications\nfocus on stress-inducing procedures and fixed vibration patterns, with limited\nconsideration of body location and dynamic biofeedback during restful states.\nThis study investigates the effects of haptic biofeedback adjusted from\nreal-time heart rate during eyes-closed wakeful rest, comparing four wearable\nbody placements: the wrist, hand, forearm, and shoulder. Heart rate, alpha wave\nactivity on the ear, subjective restfulness, and vibration experience were\nmeasured across these conditions. Results show that biofeedback reduced heart\nrate at the wrist, shoulder, and forearm, while alpha power measured at the ear\nremained unchanged. Subjective restfulness was rated highest at the shoulder\nand forearm, which were also the most preferred locations. In addition,\nparticipants reported greater comfort, relaxation, and further increased\nsleepiness at the forearm compared to the wrist, which was more easily\nrecognizable. These findings suggest that the forearm and shoulder are ideal\nfor unobtrusive relaxation feedback for wakeful rest, while the wrist may\nrequire design improvements for subjective experience.", "comment": "8 pages, 6 figures. Submitted to the International Symposium on\n  Wearable Computers (ISWC)", "pdf_url": "http://arxiv.org/pdf/2507.02453v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "触觉生物反馈用于清醒休息：刺激位置有区别吗？", "tldr": "可穿戴触觉生物反馈用于清醒休息时的放松。研究发现前臂和肩部是理想的隐蔽放松部位，而手腕需要改进。", "motivation": "当前的可穿戴触觉干预主要集中在诱导压力的过程和固定的振动模式，对身体位置和休息状态下的动态生物反馈考虑有限。本研究旨在调查在闭眼清醒休息期间，根据实时心率调整的触觉生物反馈的效果，并比较不同身体部位的放置。", "method": "本研究调查了在闭眼清醒休息期间，根据实时心率调整的触觉生物反馈的效果。它比较了四种可穿戴身体放置部位：手腕、手、前臂和肩部。测量指标包括心率、耳部阿尔法波活动、主观休息度和振动体验。", "result": "生物反馈降低了手腕、肩部和前臂的心率，但耳部阿尔法波功率保持不变。主观休息度在肩部和前臂处评分最高，这也是最受欢迎的部位。与手腕（更容易识别）相比，参与者报告前臂的舒适度、放松度和嗜睡感更高。", "conclusion": "前臂和肩部是清醒休息时隐蔽放松反馈的理想选择，而手腕可能需要改进设计以提升主观体验。", "translation": "可穿戴触觉干预通过缓慢的振动触觉生物反馈为放松提供了有前景的支持。尽管它们具有潜力，但当前的应用侧重于诱导压力的程序和固定的振动模式，对身体位置和休息状态下的动态生物反馈考虑有限。本研究调查了在闭眼清醒休息期间，根据实时心率调整的触觉生物反馈的效果，比较了四种可穿戴身体放置部位：手腕、手、前臂和肩部。在这些条件下，测量了心率、耳部阿尔法波活动、主观休息度和振动体验。结果显示，生物反馈降低了手腕、肩部和前臂的心率，而耳部测量的阿尔法波功率保持不变。主观休息度在肩部和前臂处评分最高，这也是最受欢迎的部位。此外，与手腕（更容易识别）相比，参与者报告前臂的舒适度、放松度和嗜睡感更高。这些发现表明，前臂和肩部是清醒休息时隐蔽放松反馈的理想选择，而手腕可能需要改进设计以提升主观体验。", "summary": "本研究探讨了可穿戴触觉生物反馈（根据实时心率调整）在清醒休息期间对不同身体部位放松效果的影响。结果显示，手腕、肩部和前臂的心率降低，但主观休息度在肩部和前臂处最高。前臂也提供了更大的舒适度和放松感。这些发现表明，前臂和肩部是隐蔽放松反馈的理想选择，而手腕需要改进设计。", "keywords": "触觉生物反馈, 清醒休息, 放松, 可穿戴设备, 刺激位置", "comments": "这篇论文强调了刺激位置对于触觉生物反馈在促进放松方面的重要性。它超越了固定的模式和诱导压力的环境，为设计更有效的清醒休息可穿戴放松设备提供了实用的见解。对身体放置部位的详细比较是一项重要贡献。"}}
{"id": "2507.02442", "title": "The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning", "authors": ["Moto Kamiura"], "categories": ["cs.AI", "math.CT", "stat.ME", "stat.ML"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02442v1", "summary": "Enhancing the intelligibility and interpretability of machine learning is a\ncrucial task in responding to the demand for Explicability as an AI principle,\nand in promoting the better social implementation of AI. The aim of our\nresearch is to contribute to this improvement by reformulating machine learning\nmodels through the lens of category theory, thereby developing a semantic\nframework for structuring and understanding AI systems. Our categorical\nmodeling in this paper clarifies and formalizes the structural interplay\nbetween residuals and parameters in supervised learning. The present paper\nfocuses on the multiple linear regression model, which represents the most\nbasic form of supervised learning. By defining two concrete categories\ncorresponding to parameters and data, along with an adjoint pair of functors\nbetween them, we introduce our categorical formulation of supervised learning.\nWe show that the essential structure of this framework is captured by what we\ncall the Gauss-Markov Adjunction. Within this setting, the dual flow of\ninformation can be explicitly described as a correspondence between variations\nin parameters and residuals. The ordinary least squares estimator for the\nparameters and the minimum residual are related via the preservation of limits\nby the right adjoint functor. Furthermore, we position this formulation as an\ninstance of extended denotational semantics for supervised learning, and\npropose applying a semantic perspective developed in theoretical computer\nscience as a formal foundation for Explicability in AI.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02442v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "高斯-马尔可夫伴随：监督学习中残差的范畴语义", "tldr": "本文通过范畴论重新构建监督学习模型，提出高斯-马尔可夫伴随，为AI可解释性提供了语义框架，并明确了参数与残差的结构相互作用。", "motivation": "为了响应AI原则中对可解释性的需求，并促进AI更好的社会实施，提升机器学习的可理解性和可解释性是一项关键任务。", "method": "本文通过范畴论的视角重新构建机器学习模型，具体定义了与参数和数据相对应的两个具体范畴，并在它们之间引入了一个伴随函子对，从而提出了监督学习的范畴化表述。研究重点是多元线性回归模型，并通过高斯-马尔可夫伴随捕捉框架的本质结构，描述了信息在参数和残差之间的双向流动。", "result": "本文展示了高斯-马尔可夫伴随捕捉了该框架的本质结构。在该设置下，信息在参数变动和残差之间的双向流动可以被明确描述为一种对应关系。普通最小二乘估计器与最小残差通过右伴随函子对极限的保持而关联起来。", "conclusion": "本文将这种范畴化表述定位为监督学习的扩展指称语义的一个实例，并提出将理论计算机科学中发展的语义视角作为AI可解释性的形式基础。", "translation": "增强机器学习的可理解性和可解释性是响应AI原则中对可解释性需求以及促进AI更好社会实施的关键任务。我们研究的目标是通过范畴论的视角重新构建机器学习模型，从而开发一个用于构建和理解AI系统的语义框架，以此来促进这一改进。本文中的范畴化建模阐明并形式化了监督学习中残差和参数之间的结构相互作用。本论文重点关注多元线性回归模型，它代表了监督学习最基本的形式。通过定义与参数和数据相对应的两个具体范畴，以及它们之间的一个伴随函子对，我们引入了监督学习的范畴化表述。我们展示了该框架的本质结构由我们称之为高斯-马尔可夫伴随所捕获。在此设置下，信息的双向流动可以被明确描述为参数和残差变动之间的一种对应关系。参数的普通最小二乘估计器和最小残差通过右伴随函子对极限的保持而关联起来。此外，我们将这种表述定位为监督学习的扩展指称语义的一个实例，并提出将理论计算机科学中发展的语义视角作为AI可解释性的形式基础。", "summary": "本文旨在通过范畴论的视角重新构建监督学习模型，以增强其可解释性。研究引入了高斯-马尔可夫伴随作为核心语义框架，明确了监督学习（以多元线性回归为例）中参数与残差的结构性关联。该框架将参数的普通最小二乘估计与最小残差通过函子关联起来，并提出将此作为AI可解释性的形式基础。", "keywords": "范畴论, 监督学习, 可解释性, 残差, 高斯-马尔可夫伴随", "comments": "本文的创新之处在于将抽象的范畴论应用于机器学习的语义建模，特别是对监督学习中的残差和参数关系进行了形式化。这为AI可解释性提供了一个新颖且严谨的数学基础，有助于从理论层面理解和构建更透明的AI系统。其重要性在于，它试图通过深层的数学结构来解决当前AI领域面临的可解释性挑战，为未来AI系统设计提供了潜在的新范式。"}}
{"id": "2507.02279", "title": "LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models", "authors": ["Juntao Liu", "Liqiang Niu", "Wenchao Chen", "Jie Zhou", "Fandong Meng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02279v1", "summary": "Existing visual token compression methods for Multimodal Large Language\nModels (MLLMs) predominantly operate as post-encoder modules, limiting their\npotential for efficiency gains. To address this limitation, we propose LaCo\n(Layer-wise Visual Token Compression), a novel framework that enables effective\ntoken compression within the intermediate layers of the vision encoder. LaCo\nintroduces two core components: 1) a layer-wise pixel-shuffle mechanism that\nsystematically merges adjacent tokens through space-to-channel transformations,\nand 2) a residual learning architecture with non-parametric shortcuts that\npreserves critical visual information during compression. Extensive experiments\nindicate that our LaCo outperforms all existing methods when compressing tokens\nin the intermediate layers of the vision encoder, demonstrating superior\neffectiveness. In addition, compared to external compression, our method\nimproves training efficiency beyond 20% and inference throughput over 15% while\nmaintaining strong performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02279v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LaCo: 多模态大型语言模型中视觉Token的高效逐层压缩", "tldr": "LaCo是一种新的逐层视觉Token压缩框架，它通过像素混洗和残差学习在视觉编码器中间层进行压缩，提高了多模态大型语言模型的效率并保持了性能。", "motivation": "现有的多模态大型语言模型（MLLMs）视觉Token压缩方法主要作为编码器后置模块运行，限制了其效率提升的潜力。", "method": "LaCo（逐层视觉Token压缩）引入了两个核心组件：1）逐层像素混洗机制，通过空间到通道转换系统地合并相邻Token；2）具有非参数快捷方式的残差学习架构，在压缩过程中保留关键视觉信息。", "result": "LaCo在视觉编码器中间层压缩Token时优于所有现有方法，表现出卓越的有效性。与外部压缩相比，LaCo在保持强大性能的同时，训练效率提高了20%以上，推理吞吐量提高了15%以上。", "conclusion": "LaCo通过在视觉编码器中间层进行高效的视觉Token压缩，显著提高了多模态大型语言模型的训练和推理效率，同时保持了高性能。", "translation": "现有用于多模态大型语言模型（MLLMs）的视觉Token压缩方法主要作为编码器后置模块运行，这限制了其效率提升的潜力。为了解决这一限制，我们提出了LaCo（逐层视觉Token压缩），一个新颖的框架，它能够在视觉编码器的中间层内实现有效的Token压缩。LaCo引入了两个核心组件：1）逐层像素混洗机制，通过空间到通道转换系统地合并相邻Token；2）具有非参数快捷方式的残差学习架构，在压缩过程中保留关键视觉信息。大量的实验表明，我们的LaCo在视觉编码器中间层压缩Token时优于所有现有方法，表现出卓越的有效性。此外，与外部压缩相比，我们的方法在保持强大性能的同时，训练效率提高了20%以上，推理吞吐量提高了15%以上。", "summary": "本文提出了LaCo（逐层视觉Token压缩），一个针对多模态大型语言模型（MLLMs）的新型框架，旨在解决现有视觉Token压缩方法效率受限的问题。LaCo通过在视觉编码器中间层进行压缩，引入了逐层像素混洗机制和带有非参数快捷方式的残差学习架构，以有效合并Token并保留关键视觉信息。实验结果表明，LaCo在中间层压缩方面优于现有方法，并且与外部压缩相比，在保持性能的同时，训练效率提高了20%以上，推理吞吐量提高了15%以上。", "keywords": "视觉Token压缩, 多模态大型语言模型, 逐层压缩, 像素混洗, 残差学习", "comments": "LaCo的创新之处在于将视觉Token压缩从传统的编码器后置模块提前到视觉编码器的中间层，这极大地提高了效率。其结合像素混洗和残差学习的设计思路，在保证压缩效果的同时，有效保留了视觉信息，对多模态大模型的实际应用具有重要意义。"}}
{"id": "2507.02131", "title": "Perturbed Gradient Descent Algorithms are Small-Disturbance Input-to-State Stable", "authors": ["Leilei Cui", "Zhong-Ping Jiang", "Eduardo D. Sontag", "Richard D. Braatz"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2507.02131v1", "summary": "This article investigates the robustness of gradient descent algorithms under\nperturbations. The concept of small-disturbance input-to-state stability (ISS)\nfor discrete-time nonlinear dynamical systems is introduced, along with its\nLyapunov characterization. The conventional linear Polyak-Lojasiewicz (PL)\ncondition is then extended to a nonlinear version, and it is shown that the\ngradient descent algorithm is small-disturbance ISS provided the objective\nfunction satisfies the generalized nonlinear PL condition. This\nsmall-disturbance ISS property guarantees that the gradient descent algorithm\nconverges to a small neighborhood of the optimum under sufficiently small\nperturbations. As a direct application of the developed framework, we\ndemonstrate that the LQR cost satisfies the generalized nonlinear PL condition,\nthereby establishing that the policy gradient algorithm for LQR is\nsmall-disturbance ISS. Additionally, other popular policy gradient algorithms,\nincluding natural policy gradient and Gauss-Newton method, are also proven to\nbe small-disturbance ISS.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.02131v1", "cate": "math.OC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "扰动梯度下降算法是小扰动输入到状态稳定的", "tldr": "本文研究了梯度下降算法在扰动下的鲁棒性。通过引入小扰动输入到状态稳定性（ISS）概念并扩展Polyak-Lojasiewicz (PL) 条件，证明了在广义非线性PL条件下，梯度下降算法是小扰动ISS的，确保其在小扰动下收敛到最优值的小邻域。", "motivation": "调查梯度下降算法在扰动下的鲁棒性。", "method": "引入了离散时间非线性动力系统的小扰动输入到状态稳定性（ISS）概念及其Lyapunov表征。将传统的线性Polyak-Lojasiewicz (PL) 条件扩展到非线性版本。", "result": "证明了当目标函数满足广义非线性PL条件时，梯度下降算法是小扰动ISS的。该特性保证了在足够小的扰动下，梯度下降算法收敛到最优值的一个小邻域。此外，LQR代价和多种策略梯度算法（包括自然策略梯度和高斯-牛顿法）也被证明是小扰动ISS的。", "conclusion": "梯度下降算法在满足广义非线性PL条件时，具有小扰动输入到状态稳定性，确保在足够小的扰动下，算法能够收敛到最优值的一个小邻域。", "translation": "本文研究了梯度下降算法在扰动下的鲁棒性。引入了离散时间非线性动力系统的小扰动输入到状态稳定性（ISS）概念及其Lyapunov表征。然后，将传统的线性Polyak-Lojasiewicz (PL) 条件扩展到非线性版本，并表明只要目标函数满足广义非线性PL条件，梯度下降算法就具有小扰动ISS。这种小扰动ISS特性保证了在足够小的扰动下，梯度下降算法收敛到最优值的一个小邻域。作为所开发框架的直接应用，我们证明了LQR代价满足广义非线性PL条件，从而确立了LQR的策略梯度算法是小扰动ISS的。此外，其他流行的策略梯度算法，包括自然策略梯度和高斯-牛顿法，也被证明是小扰动ISS的。", "summary": "本文探讨了梯度下降算法在扰动下的鲁棒性。通过引入小扰动输入到状态稳定性（ISS）概念并扩展Polyak-Lojasiewicz (PL) 条件至非线性版本，研究表明当目标函数满足广义非线性PL条件时，梯度下降算法具有小扰动ISS特性，确保在小扰动下收敛到最优值的小邻域。该框架还应用于证明了LQR和多种策略梯度算法（如自然策略梯度和高斯-牛顿法）的小扰动ISS。", "keywords": "梯度下降, 鲁棒性, 输入到状态稳定性, Polyak-Lojasiewicz条件, 策略梯度", "comments": "本文创新性地将小扰动输入到状态稳定性（ISS）的概念引入到梯度下降算法的鲁棒性分析中，并提出了广义非线性PL条件。这对于理解和保证梯度下降算法在存在外部扰动时的性能具有重要意义，尤其是在实际应用中，数据噪声或模型不确定性是普遍存在的。研究结果为设计更鲁棒的优化算法提供了理论基础。"}}
{"id": "2507.02084", "title": "Adaptive Iterative Soft-Thresholding Algorithm with the Median Absolute Deviation", "authors": ["Yining Feng", "Ivan Selesnick"], "categories": ["stat.ML", "cs.LG", "eess.SP"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02084v1", "summary": "The adaptive Iterative Soft-Thresholding Algorithm (ISTA) has been a popular\nalgorithm for finding a desirable solution to the LASSO problem without\nexplicitly tuning the regularization parameter $\\lambda$. Despite that the\nadaptive ISTA is a successful practical algorithm, few theoretical results\nexist. In this paper, we present the theoretical analysis on the adaptive ISTA\nwith the thresholding strategy of estimating noise level by median absolute\ndeviation. We show properties of the fixed points of the algorithm, including\nscale equivariance, non-uniqueness, and local stability, prove the local linear\nconvergence guarantee, and show its global convergence behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02084v1", "cate": "stat.ML", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "带有中位数绝对偏差的自适应迭代软阈值算法", "tldr": "本文对自适应ISTA的理论性质进行了深入分析，证明了其收敛性。", "motivation": "自适应迭代软阈值算法（ISTA）在解决LASSO问题时表现出色且广受欢迎，但其理论基础和收敛性分析却十分匮乏，这限制了对其性能的深入理解和进一步改进。", "method": "本文对采用中位数绝对偏差（MAD）估计噪声水平的自适应ISTA阈值策略进行了理论分析。研究内容包括揭示算法不动点的性质，如尺度等变性、非唯一性和局部稳定性，并严格证明了其局部线性收敛保证和全局收敛行为。", "result": "研究揭示了自适应ISTA不动点的性质，包括尺度等变性、非唯一性和局部稳定性。此外，还证明了该算法的局部线性收敛性，并展示了其全局收敛行为。", "conclusion": "本文为自适应迭代软阈值算法（ISTA）提供了重要的理论基础和严格的收敛性证明，填补了该实用算法在理论分析方面的空白，有助于加深对其行为的理解和未来的改进。", "translation": "自适应迭代软阈值算法（ISTA）是一种流行的算法，用于在不明确调整正则化参数$\\lambda$的情况下找到LASSO问题的理想解。尽管自适应ISTA是一种成功的实用算法，但现有的理论结果很少。在本文中，我们对采用中位数绝对偏差估计噪声水平的阈值策略的自适应ISTA进行了理论分析。我们展示了该算法不动点的性质，包括尺度等变性、非唯一性和局部稳定性，证明了局部线性收敛保证，并展示了其全局收敛行为。", "summary": "本文对一种在LASSO问题中广泛应用的自适应迭代软阈值算法（ISTA）进行了全面的理论分析。该算法通过中位数绝对偏差估计噪声水平来确定阈值。研究揭示了算法不动点的关键性质，如尺度等变性、非唯一性和局部稳定性，并首次严格证明了其局部线性收敛性和全局收敛行为，填补了该实用算法在理论基础上的空白。", "keywords": "自适应ISTA, LASSO, 中位数绝对偏差, 收敛性, 理论分析", "comments": "这项工作的重要性在于它为一种广受欢迎且实用的算法（自适应ISTA）提供了急需的理论基础和收敛性证明，这对于算法的进一步理解、改进和可靠性评估至关重要。它解决了现有理论空白，增强了该算法的可靠性和可信度。"}}
{"id": "2507.02128", "title": "CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs", "authors": ["Jingyu Pan", "Isaac Jacobson", "Zheng Zhao", "Tung-Chieh Chen", "Guanglei Zhou", "Chen-Chia Chang", "Vineet Rashingkar", "Yiran Chen"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ICCAD 2025", "url": "http://arxiv.org/abs/2507.02128v1", "summary": "Modern very large-scale integration (VLSI) design requires the implementation\nof integrated circuits using electronic design automation (EDA) tools. Due to\nthe complexity of EDA algorithms, the vast parameter space poses a huge\nchallenge to chip design optimization, as the combination of even moderate\nnumbers of parameters creates an enormous solution space to explore. Manual\nparameter selection remains industrial practice despite being excessively\nlaborious and limited by expert experience. To address this issue, we present\nCROP, the first large language model (LLM)-powered automatic VLSI design flow\ntuning framework. Our approach includes: (1) a scalable methodology for\ntransforming RTL source code into dense vector representations, (2) an\nembedding-based retrieval system for matching designs with semantically similar\ncircuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided\nparameter search system that constrains the search process with prior knowledge\nfrom similar designs. Experiment results demonstrate CROP's ability to achieve\nsuperior quality-of-results (QoR) with fewer iterations than existing\napproaches on industrial designs, including a 9.9% reduction in power\nconsumption.", "comment": "Accepted by ICCAD 2025", "pdf_url": "http://arxiv.org/pdf/2507.02128v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "CROP：使用大型语言模型进行参数引导的电路检索与优化", "tldr": "CROP是一个利用大型语言模型（LLM）来自动优化超大规模集成电路（VLSI）设计参数的框架，它通过检索和指导性的参数搜索，显著提高了设计质量并减少了功耗。", "motivation": "现代超大规模集成电路（VLSI）设计在电子设计自动化（EDA）工具中面临巨大挑战，因为复杂的算法和庞大的参数空间使得芯片设计优化变得异常困难。当前的手动参数选择方法不仅耗时费力，而且受限于专家经验。", "method": "本文提出了CROP，这是首个由大型语言模型（LLM）驱动的VLSI设计流程自动调优框架。其方法包括：1）一种将RTL源代码转换为密集向量表示的可扩展方法；2）一个基于嵌入的检索系统，用于匹配与语义相似的电路设计；3）一个通过检索增强生成（RAG）技术增强的LLM引导参数搜索系统，该系统利用来自相似设计的先验知识来约束搜索过程。", "result": "实验结果表明，CROP在工业设计上比现有方法能以更少的迭代次数实现更优的质量成果（QoR），包括功耗降低9.9%。", "conclusion": "CROP框架成功地利用大型语言模型解决了VLSI设计中参数优化的复杂挑战，显著提升了设计效率和质量。", "translation": "现代超大规模集成电路（VLSI）设计需要使用电子设计自动化（EDA）工具来实现集成电路。由于EDA算法的复杂性，庞大的参数空间给芯片设计优化带来了巨大挑战，因为即使是中等数量参数的组合也会产生巨大的解决方案空间供探索。尽管手动参数选择过于费力且受限于专家经验，但它仍然是工业实践。为了解决这个问题，我们提出了CROP，这是第一个由大型语言模型（LLM）驱动的自动VLSI设计流程调优框架。我们的方法包括：(1) 一种将RTL源代码转换为密集向量表示的可扩展方法，(2) 一个基于嵌入的检索系统，用于匹配与语义相似的电路设计，以及(3) 一个检索增强生成（RAG）增强的LLM引导参数搜索系统，该系统利用来自相似设计的先验知识来约束搜索过程。实验结果表明，CROP在工业设计上比现有方法能以更少的迭代次数实现更优的质量成果（QoR），包括功耗降低9.9%。", "summary": "CROP是一个创新的大型语言模型（LLM）驱动框架，旨在自动化和优化VLSI设计流程中的参数选择，以解决传统手动方法的低效率和局限性。该框架通过将RTL源代码转化为向量表示、利用嵌入式检索系统匹配相似电路，并结合检索增强生成（RAG）技术，利用先验知识引导LLM进行参数搜索。实验证明，CROP在工业设计中能以更少迭代实现卓越的质量成果，并显著降低功耗，例如实现9.9%的功耗削减。", "keywords": "VLSI, LLM, EDA, 电路优化, 参数引导", "comments": "这篇论文创新性地将大型语言模型应用于VLSI设计流程的参数优化，解决了传统手动优化效率低、受限于专家经验的痛点。其结合检索增强生成（RAG）的方法，利用先验知识指导参数搜索，是其关键创新点。实验结果表明其在工业设计上的有效性，显示了LLM在EDA领域应用的巨大潜力。"}}
{"id": "2507.02593", "title": "Revisiting Active Learning under (Human) Label Variation", "authors": ["Cornelia Gruber", "Helen Alber", "Bernd Bischl", "Göran Kauermann", "Barbara Plank", "Matthias Aßenmacher"], "categories": ["cs.CL", "cs.HC", "cs.LG", "stat.ML"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02593v1", "summary": "Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02593v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "重新审视（人类）标签变异下的主动学习", "tldr": "本文探讨了在主动学习中，人类标签变异（HLV）作为一种信息信号的重要性，并提出了一个概念框架来整合HLV，以更好地反映真实世界的标注复杂性。", "motivation": "高质量标注数据的获取是监督学习的限制因素。现有标注框架和主动学习方法常忽视人类标签变异（HLV），即同一实例可能存在合理的不同标签，而将其视为噪声而非有用的信号。", "method": "本文审视了关于真值和标签性质的基本假设，强调将观察到的标签变异分解为信号（如HLV）和噪声（如标注错误）的必要性。作者调查了主动学习和（人类）标签变异社区如何处理这些区别，并提出了一个概念框架，用于在主动学习循环中整合HLV，包括实例选择、标注者选择和标签表示。此外，还讨论了将大型语言模型（LLM）作为标注者的整合。", "result": "本文旨在为感知HLV的主动学习奠定概念基础，以更好地反映真实世界标注的复杂性。", "conclusion": "为了更好地应对真实世界标注的复杂性，主动学习需要考虑人类标签变异（HLV），并将其作为有用的信号而非简单噪声来处理。通过整合HLV，可以优化标注预算的使用，并提高机器学习模型的训练效果。", "translation": "高质量标注数据的获取仍然是应用监督学习的一个限制因素。虽然标签变异（LV），即同一实例的不同标签，很常见，尤其是在自然语言处理中，但标注框架通常仍然基于单一真值的假设。这忽视了人类标签变异（HLV），即标注中出现合理差异，将其作为一种信息信号。同样，主动学习（AL）作为一种优化有限标注预算以训练机器学习模型的流行方法，通常依赖于若干简化假设中的至少一个，这些假设在承认HLV时在实践中很少成立。在本文中，我们审视了关于真值和标签性质的基本假设，强调需要将观察到的LV分解为信号（例如HLV）和噪声（例如标注错误）。我们调查了主动学习和（人类）标签变异社区如何处理或忽视这些区别，并提出了一个概念框架，用于在主动学习循环中整合HLV，包括实例选择、标注者选择和标签表示。我们进一步讨论了大型语言模型（LLM）作为标注者的整合。我们的工作旨在为感知HLV的主动学习奠定概念基础，更好地反映真实世界标注的复杂性。", "summary": "该论文探讨了在监督学习中，人类标签变异（HLV）作为一种信息信号的重要性，而不是简单的噪声。它指出当前的主动学习（AL）和标注框架往往忽视HLV。为此，作者审视了标签真值和性质的基本假设，提出将标签变异分解为信号（如HLV）和噪声。论文提出了一个概念框架，用于在主动学习的各个环节（如实例选择、标注者选择、标签表示）中整合HLV，并讨论了将大型语言模型（LLM）作为标注者的可能性。其目标是为感知HLV的主动学习奠定基础，以更好地应对真实世界标注的复杂性。", "keywords": "主动学习, 标签变异, 人类标签变异, 标注, 大语言模型", "comments": "本文的创新之处在于其对主动学习中人类标签变异（HLV）的重新审视和概念性框架的提出。它挑战了传统“单一真值”的假设，强调了HLV作为有用信号的重要性，这对于提高真实世界标注效率和模型性能具有重要意义。同时，论文还前瞻性地讨论了大型语言模型在标注中的应用，为未来研究提供了方向。其局限性在于主要停留在概念层面，缺乏具体的实验验证。"}}
{"id": "2507.02735", "title": "Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks", "authors": ["Sizhe Chen", "Arman Zharmagambetov", "David Wagner", "Chuan Guo"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02735v1", "summary": "Prompt injection attacks pose a significant security threat to LLM-integrated\napplications. Model-level defenses have shown strong effectiveness, but are\ncurrently deployed into commercial-grade models in a closed-source manner. We\nbelieve open-source models are needed by the AI security community, where\nco-development of attacks and defenses through open research drives scientific\nprogress in mitigation against prompt injection attacks. To this end, we\ndevelop Meta SecAlign, the first open-source and open-weight LLM with built-in\nmodel-level defense that achieves commercial-grade model performance. We\nprovide complete details of our training recipe, which utilizes an improved\nversion of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7\nsecurity benchmarks show that Meta SecAlign, despite being trained on a generic\ninstruction-tuning dataset, confers security in unseen downstream tasks,\nincluding tool-calling and agentic web navigation, in addition general\ninstruction-following. Our best model -- Meta-SecAlign-70B -- achieves\nstate-of-the-art robustness against prompt injection attacks and comparable\nutility to closed-source commercial LLM with model-level defense.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02735v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Meta SecAlign：一个抵御提示注入攻击的安全基础LLM", "tldr": "Meta SecAlign是首个开源、开放权重的LLM，内置模型级防御，能有效抵御提示注入攻击，性能媲美商业模型。", "motivation": "提示注入攻击对LLM集成应用构成重大安全威胁，而现有的模型级防御是闭源的。AI安全社区需要开源模型来共同开发攻击和防御，以推动科学进步。", "method": "开发了Meta SecAlign，这是第一个内置模型级防御的开源且开放权重的LLM。该模型利用了SOTA SecAlign防御的改进版本，并提供了完整的训练方案细节。", "result": "在9个效用基准和7个安全基准上的评估显示，尽管Meta SecAlign在通用指令微调数据集上训练，但它能在未见的下游任务中（包括工具调用、代理网络导航和通用指令遵循）提供安全性。其最佳模型Meta-SecAlign-70B在抵御提示注入攻击方面达到了最先进的鲁棒性，并且与闭源商业LLM具有可比的效用。", "conclusion": "Meta SecAlign成功地提供了一个开源且性能强大的LLM，能有效抵御提示注入攻击，并有望推动AI安全社区在开放研究中共同进步。", "translation": "提示注入攻击对LLM集成应用构成重大安全威胁。模型级防御已显示出强大的有效性，但目前以闭源方式部署到商业级模型中。我们认为AI安全社区需要开源模型，通过开放研究共同开发攻击和防御，从而推动抵御提示注入攻击的科学进展。为此，我们开发了Meta SecAlign，这是第一个内置模型级防御的开源、开放权重LLM，实现了商业级模型的性能。我们提供了完整的训练方案细节，该方案利用了SOTA SecAlign防御的改进版本。在9个效用基准和7个安全基准上的评估显示，Meta SecAlign尽管在通用指令微调数据集上训练，但它能在未见的下游任务中（包括工具调用和代理网络导航）提供安全性，此外还具备通用指令遵循能力。我们最好的模型——Meta-SecAlign-70B——在抵御提示注入攻击方面达到了最先进的鲁棒性，并且与具有模型级防御的闭源商业LLM具有可比的效用。", "summary": "本文介绍了Meta SecAlign，这是首个开源、开放权重的LLM，旨在通过内置模型级防御来抵御提示注入攻击。该模型利用改进的SOTA SecAlign防御，并在多项效用和安全基准测试中展现出卓越性能，尤其是在抵御提示注入攻击方面达到了最先进的鲁棒性，同时保持了与商业闭源模型相当的实用性，从而为AI安全社区提供了急需的开放研究平台。", "keywords": "提示注入攻击, LLM安全, 模型级防御, 开源LLM, Meta SecAlign", "comments": "这篇论文的创新之处在于它是首个提供内置模型级防御的开源、开放权重LLM，解决了当前商业模型闭源的问题。其重要性在于为AI安全社区提供了一个开放平台，促进对提示注入攻击的共同研究和防御开发。通过实现商业级性能和SOTA鲁棒性，Meta SecAlign有望成为安全LLM领域的重要基石。"}}
{"id": "2507.02846", "title": "Legal Requirements Translation from Law", "authors": ["Anmol Singhal", "Travis Breaux"], "categories": ["cs.SE", "cs.CL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      13 pages, 7 figures, Accepted at the 33rd IEEE International Requirements Engineering 2025", "url": "http://arxiv.org/abs/2507.02846v1", "summary": "Software systems must comply with legal regulations, which is a\nresource-intensive task, particularly for small organizations and startups\nlacking dedicated legal expertise. Extracting metadata from regulations to\nelicit legal requirements for software is a critical step to ensure compliance.\nHowever, it is a cumbersome task due to the length and complex nature of legal\ntext. Although prior work has pursued automated methods for extracting\nstructural and semantic metadata from legal text, key limitations remain: they\ndo not consider the interplay and interrelationships among attributes\nassociated with these metadata types, and they rely on manual labeling or\nheuristic-driven machine learning, which does not generalize well to new\ndocuments. In this paper, we introduce an approach based on textual entailment\nand in-context learning for automatically generating a canonical representation\nof legal text, encodable and executable as Python code. Our representation is\ninstantiated from a manually designed Python class structure that serves as a\ndomain-specific metamodel, capturing both structural and semantic legal\nmetadata and their interrelationships. This design choice reduces the need for\nlarge, manually labeled datasets and enhances applicability to unseen\nlegislation. We evaluate our approach on 13 U.S. state data breach notification\nlaws, demonstrating that our generated representations pass approximately 89.4%\nof test cases and achieve a precision and recall of 82.2 and 88.7,\nrespectively.", "comment": "13 pages, 7 figures, Accepted at the 33rd IEEE International\n  Requirements Engineering 2025", "pdf_url": "http://arxiv.org/pdf/2507.02846v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "法律要求从法律中翻译", "tldr": "本文提出了一种基于文本蕴涵和上下文学习的方法，将法律文本自动转换为可执行的Python代码，以简化软件的法律合规性，并在数据泄露通知法上取得了良好效果。", "motivation": "软件系统必须遵守法律法规，这对缺乏法律专业知识的小型组织和初创企业来说是一项资源密集型任务。从法规中提取元数据以获取软件的法律要求是确保合规性的关键步骤，但由于法律文本的长度和复杂性，这项任务很繁琐。现有自动化方法存在局限性，未能考虑元数据类型之间属性的相互作用和相互关系，且依赖手动标注或启发式机器学习，泛化能力差。", "method": "本文提出了一种基于文本蕴涵和上下文学习的方法，用于自动生成法律文本的规范表示，该表示可编码并作为Python代码执行。这种表示实例化自一个手动设计的Python类结构，该结构作为领域特定的元模型，捕获结构和语义法律元数据及其相互关系。这种设计选择减少了对大型手动标注数据集的需求，并增强了对未见立法文本的适用性。", "result": "该方法在13项美国州数据泄露通知法上进行了评估，结果表明生成的表示通过了大约89.4%的测试用例，精确率和召回率分别为82.2%和88.7%。", "conclusion": "本文提出的基于文本蕴涵和上下文学习的方法，能够有效且自动化地将法律文本转换为可执行的软件法律要求表示，显著提高了法律合规性任务的效率和准确性，尤其是在处理复杂法律文本和减少对大量标注数据的依赖方面表现出色。", "translation": "软件系统必须遵守法律法规，这是一项资源密集型任务，特别是对于缺乏专门法律专业知识的小型组织和初创企业。从法规中提取元数据以获取软件的法律要求是确保合规性的关键步骤。然而，由于法律文本的长度和复杂性，这是一项繁琐的任务。尽管先前的工作已经寻求从法律文本中提取结构和语义元数据的自动化方法，但仍存在主要局限性：它们不考虑与这些元数据类型相关的属性之间的相互作用和相互关系，并且它们依赖于手动标注或启发式机器学习，这对于新文档的泛化能力不佳。在本文中，我们引入了一种基于文本蕴涵和上下文学习的方法，用于自动生成法律文本的规范表示，该表示可编码并作为Python代码执行。我们的表示是从一个手动设计的Python类结构中实例化的，该结构作为领域特定的元模型，捕获结构和语义法律元数据及其相互关系。这种设计选择减少了对手动标注大型数据集的需求，并增强了对未见立法的适用性。我们在13项美国州数据泄露通知法上评估了我们的方法，结果表明我们生成的表示通过了大约89.4%的测试用例，精确率和召回率分别为82.2%和88.7%。", "summary": "本研究提出了一种创新方法，利用文本蕴涵和上下文学习自动将法律文本转换为可执行的Python代码，以简化软件的法律合规性。该方法通过一个手动设计的Python类结构捕获法律元数据及其相互关系，从而减少对大量标注数据的依赖并提高对新法律文本的适用性。在针对美国13项数据泄露通知法的评估中，该方法表现出高通过率和良好的精确率与召回率，为解决法律合规性中的复杂性提供了有效途径。", "keywords": "法律合规性, 法律要求, 文本蕴涵, 上下文学习, Python代码", "comments": "本文的创新之处在于结合了文本蕴涵和上下文学习来自动化法律文本到可执行代码的转换，并引入了领域特定的Python类结构作为元模型，有效捕获了法律元数据及其相互关系。这一方法显著减少了对大规模手动标注数据集的依赖，提升了对新法律文本的泛化能力，对于资源有限的小型组织而言尤为重要。其在数据泄露通知法上的良好表现证明了方法的实用性和有效性。"}}
{"id": "2507.02600", "title": "ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and Manipulation of Articulated Objects", "authors": ["Qiaojun Yu", "Xibin Yuan", "Yu jiang", "Junting Chen", "Dongzhe Zheng", "Ce Hao", "Yang You", "Yixing Chen", "Yao Mu", "Liu Liu", "Cewu Lu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted by IROS 2025", "url": "http://arxiv.org/abs/2507.02600v1", "summary": "Articulated object manipulation remains a critical challenge in robotics due\nto the complex kinematic constraints and the limited physical reasoning of\nexisting methods. In this work, we introduce ArtGS, a novel framework that\nextends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling\nfor articulated object understanding and interaction. ArtGS begins with\nmulti-view RGB-D reconstruction, followed by reasoning with a vision-language\nmodel (VLM) to extract semantic and structural information, particularly the\narticulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS\noptimizes the parameters of the articulated bones, ensuring physically\nconsistent motion constraints and enhancing the manipulation policy. By\nleveraging dynamic Gaussian splatting, cross-embodiment adaptability, and\nclosed-loop optimization, ArtGS establishes a new framework for efficient,\nscalable, and generalizable articulated object modeling and manipulation.\nExperiments conducted in both simulation and real-world environments\ndemonstrate that ArtGS significantly outperforms previous methods in joint\nestimation accuracy and manipulation success rates across a variety of\narticulated objects. Additional images and videos are available on the project\nwebsite: https://sites.google.com/view/artgs/home", "comment": "Accepted by IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.02600v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "ArtGS：用于铰接对象交互式视觉-物理建模和操作的3D高斯泼溅", "tldr": "ArtGS是一个新颖的框架，它扩展了3D高斯泼溅（3DGS），通过集成视觉-物理建模来理解和操作铰接对象，并在关节估计精度和操作成功率方面显著优于现有方法。", "motivation": "由于复杂的运动学约束和现有方法有限的物理推理能力，铰接对象的操纵在机器人技术中仍然是一个关键挑战。", "method": "ArtGS首先进行多视图RGB-D重建，然后利用视觉-语言模型（VLM）提取语义和结构信息（特别是铰接骨骼）。通过动态、可微分的基于3DGS的渲染，ArtGS优化铰接骨骼的参数，确保物理一致的运动约束并增强操作策略。它利用动态高斯泼溅、跨实体适应性和闭环优化。", "result": "在仿真和真实世界环境中进行的实验表明，ArtGS在各种铰接对象的关节估计精度和操作成功率方面显著优于以前的方法。", "conclusion": "ArtGS建立了一个高效、可扩展且可泛化的铰接对象建模和操作新框架，通过集成视觉-物理建模和3DGS技术，显著提升了铰接对象的理解和交互能力。", "translation": "铰接对象的操纵在机器人技术中仍然是一个关键挑战，这归因于复杂的运动学约束和现有方法有限的物理推理能力。在这项工作中，我们引入了ArtGS，一个通过集成视觉-物理建模来理解和交互铰接对象的新颖框架，它扩展了3D高斯泼溅（3DGS）。ArtGS首先进行多视图RGB-D重建，然后利用视觉-语言模型（VLM）进行推理，以提取语义和结构信息，特别是铰接骨骼。通过动态、可微分的基于3DGS的渲染，ArtGS优化了铰接骨骼的参数，确保了物理上一致的运动约束并增强了操纵策略。通过利用动态高斯泼溅、跨实体适应性和闭环优化，ArtGS建立了一个高效、可扩展且可泛化的铰接对象建模和操纵新框架。在仿真和真实世界环境中进行的实验表明，ArtGS在各种铰接对象的关节估计精度和操纵成功率方面显著优于以前的方法。更多图片和视频可在项目网站上获取：https://sites.google.com/view/artgs/home", "summary": "ArtGS是一个新颖的框架，它通过将3D高斯泼溅（3DGS）与视觉-物理建模相结合，旨在解决铰接对象操纵中的复杂挑战。该框架首先通过多视图RGB-D重建获取数据，然后利用视觉-语言模型（VLM）提取语义和结构信息。通过动态、可微分的3DGS渲染，ArtGS优化铰接骨骼参数，确保物理一致的运动并提升操作策略。实验证明，ArtGS在关节估计精度和操作成功率方面均优于现有方法，为高效、可扩展的铰接对象建模和操作提供了新的范式。", "keywords": "3D Gaussian Splatting, 铰接对象, 机器人操作, 视觉-物理建模, 运动学约束", "comments": "ArtGS的创新之处在于将3D Gaussian Splatting技术与视觉-物理建模相结合，以解决机器人领域中铰接对象操纵的复杂性。其利用VLM进行语义和结构信息提取，并通过可微分渲染优化物理约束，这提供了一个强大的、端到端的方法。该方法的泛化能力和在仿真与真实世界环境中的出色表现，使其在机器人操作和计算机视觉领域具有重要意义。"}}
{"id": "2507.02537", "title": "Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue", "authors": ["Paulo Ricardo Knob", "Leonardo Scholler", "Juliano Rigatti", "Soraia Raupp Musse"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02537v1", "summary": "Conversational agents have made significant progress since ELIZA, expanding\ntheir role across various domains, including healthcare, education, and\ncustomer service. As these agents become increasingly integrated into daily\nhuman interactions, the need for emotional intelligence, particularly\nempathetic listening, becomes increasingly essential. In this study, we explore\nhow Large Language Models (LLMs) respond when tasked with generating\nemotionally rich interactions. Starting from a small dataset manually crafted\nby an expert to reflect empathic behavior, we extended the conversations using\ntwo LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the\ndialogues using both sentiment analysis (via VADER) and expert assessments.\nWhile the generated conversations often mirrored the intended emotional\nstructure, human evaluation revealed important differences in the perceived\nempathy and coherence of the responses. These findings suggest that emotion\nmodeling in dialogues requires not only structural alignment in the expressed\nemotions but also qualitative depth, highlighting the importance of combining\nautomated and humancentered methods in the development of emotionally competent\nagents.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02537v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "你听我说吗？微调聊天机器人以实现共情对话", "tldr": "研究通过微调LLM生成富有情感的对话，发现虽然自动生成对话结构符合预期，但人类评估显示在感知共情和连贯性方面存在差异，强调了结合自动化和以人为中心方法的重要性。", "motivation": "随着对话代理日益融入人类日常互动，对情感智能，尤其是共情倾听的需求变得至关重要。本研究旨在探索大型语言模型在生成情感丰富互动时的表现。", "method": "研究从一个专家手动创建的反映共情行为的小型数据集开始，使用ChatGPT和Gemini两个LLM扩展对话。通过情感分析（VADER）和专家评估来分析对话的情感进展。", "result": "生成的对话通常反映了预期的情感结构，但人类评估揭示了在感知共情和回复连贯性方面存在重要差异。", "conclusion": "对话中的情感建模不仅需要表达情感的结构对齐，还需要定性深度，这突出了在开发情感能力代理时结合自动化和以人为中心方法的重要性。", "translation": "对话代理自ELIZA以来取得了显著进展，其作用扩展到医疗保健、教育和客户服务等各个领域。随着这些代理日益融入日常人类互动，对情感智能，特别是共情倾听的需求变得越来越重要。在本研究中，我们探讨了大型语言模型（LLMs）在被要求生成情感丰富互动时的响应方式。我们从一个由专家手工制作的反映共情行为的小型数据集开始，使用两个LLM：ChatGPT和Gemini扩展了对话。我们使用情感分析（通过VADER）和专家评估两种方法分析了对话的情感进展。虽然生成的对话通常反映了预期的情感结构，但人类评估揭示了在感知共情和回复连贯性方面存在重要差异。这些发现表明，对话中的情感建模不仅需要表达情感的结构对齐，还需要定性深度，这突出强调了在开发情感能力代理时结合自动化和以人为中心方法的重要性。", "summary": "本研究探讨了如何微调大型语言模型（LLMs）以生成富有共情情感的对话。通过扩展一个专家手工制作的小型共情对话数据集，并利用ChatGPT和Gemini进行对话生成，研究人员使用情感分析和人类评估来分析情感进展。结果显示，尽管LLM能模仿预期的情感结构，但在人类感知到的共情和连贯性方面仍有不足。这强调了在开发情感智能对话代理时，结合自动化和以人为中心的评估方法的重要性。", "keywords": "大型语言模型, 共情对话, 情感智能, 聊天机器人, 人类评估", "comments": "本文关注了当前LLM在情感智能对话方面的一个重要挑战：如何实现真正的共情。其创新点在于结合了专家手工数据集、主流LLM扩展以及多维度的评估（自动化情感分析与人类感知评估）。研究结果揭示了LLM在情感结构模仿上的能力与实际共情深度之间的差距，为未来情感智能对话系统的发展提供了宝贵的见解，强调了人工校准和人类反馈的不可替代性。"}}
{"id": "2507.02541", "title": "Clarifying Before Reasoning: A Coq Prover with Structural Context", "authors": ["Yanzhen Lu", "Hanbin Yang", "Xiaodie Wang", "Ge Zhang", "Biao Li", "Chenxu Fu", "Chao Li", "Yang Yuan", "Andrew Chi-Chih Yao"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02541v1", "summary": "In this work, we investigate whether improving task clarity can enhance\nreasoning ability of large language models, focusing on theorem proving in Coq.\nWe introduce a concept-level metric to evaluate task clarity and show that\nadding structured semantic context to the standard input used by modern LLMs,\nleads to a 1.85$\\times$ improvement in clarity score\n(44.5\\%~$\\rightarrow$~82.3\\%). Using the general-purpose model\n\\texttt{DeepSeek-V3}, our approach leads to a 2.1$\\times$ improvement in proof\nsuccess (21.8\\%~$\\rightarrow$~45.8\\%) and outperforms the previous\nstate-of-the-art \\texttt{Graph2Tac} (33.2\\%). We evaluate this on 1,386\ntheorems randomly sampled from 15 standard Coq packages, following the same\nevaluation protocol as \\texttt{Graph2Tac}. Furthermore, fine-tuning smaller\nmodels on our structured data can achieve even higher performance (48.6\\%). Our\nmethod uses selective concept unfolding to enrich task descriptions, and\nemploys a Planner--Executor architecture. These findings highlight the value of\nstructured task representations in bridging the gap between understanding and\nreasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02541v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "推理前先澄清：一个带有结构化上下文的Coq证明器", "tldr": "通过结构化上下文提高任务清晰度，显著提升了大型语言模型在Coq定理证明中的推理能力，并超越了现有最佳水平。", "motivation": "本研究旨在探究提高任务清晰度是否能增强大型语言模型的推理能力，重点关注Coq中的定理证明。", "method": "引入了一个概念级指标来评估任务清晰度；通过向LLM的标准输入添加结构化语义上下文来提高清晰度；使用选择性概念展开来丰富任务描述；采用Planner-Executor架构；并在15个标准Coq包中随机抽样的1,386个定理上进行了评估。", "result": "通过添加结构化语义上下文，清晰度得分提高了1.85倍（从44.5%提升至82.3%）。使用通用模型DeepSeek-V3，证明成功率提高了2.1倍（从21.8%提升至45.8%），并优于先前的最先进模型Graph2Tac（33.2%）。在结构化数据上对小型模型进行微调可以实现更高的性能（48.6%）。", "conclusion": "这些发现强调了结构化任务表示在弥合理解与推理之间鸿沟的价值。", "translation": "在这项工作中，我们研究了提高任务清晰度是否能增强大型语言模型的推理能力，重点关注Coq中的定理证明。我们引入了一个概念级指标来评估任务清晰度，并表明向现代LLM使用的标准输入添加结构化语义上下文，可以使清晰度得分提高1.85倍（44.5%→82.3%）。使用通用模型DeepSeek-V3，我们的方法使证明成功率提高了2.1倍（21.8%→45.8%），并优于先前的最先进模型Graph2Tac（33.2%）。我们根据Graph2Tac相同的评估协议，在从15个标准Coq包中随机抽样的1,386个定理上进行了评估。此外，在我们的结构化数据上对小型模型进行微调可以实现更高的性能（48.6%）。我们的方法使用选择性概念展开来丰富任务描述，并采用Planner–Executor架构。这些发现强调了结构化任务表示在弥合理解与推理之间鸿沟的价值。", "summary": "本文探讨了通过提高任务清晰度来增强大型语言模型在Coq定理证明中的推理能力。研究引入了一种新的概念级清晰度指标，并证明通过向LLM输入添加结构化语义上下文能显著提升该指标。该方法利用选择性概念展开和Planner-Executor架构，在使用DeepSeek-V3模型时，证明成功率提高了2.1倍，超越了现有最先进的Graph2Tac模型。此外，对小型模型进行微调也能取得更优异的性能，这突显了结构化任务表示对于弥合LLM理解与推理之间差距的重要性。", "keywords": "大型语言模型, 定理证明, Coq, 任务清晰度, 结构化上下文", "comments": "该论文的创新之处在于，它通过引入结构化语义上下文和Planner-Executor架构，明确地提高了大型语言模型的任务清晰度，从而提升了其推理能力。其显著的性能提升，尤其是在超越现有技术水平方面，以及对小型模型的益处，都强调了其重要性。这为通过优化输入表示来增强LLM推理能力提供了一个有前景的方向。"}}
{"id": "2507.02288", "title": "Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization", "authors": ["De Cheng", "Zhipeng Xu", "Xinyang Jiang", "Dongsheng Li", "Nannan Wang", "Xinbo Gao"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02288v1", "summary": "Domain Generalization (DG) seeks to develop a versatile model capable of\nperforming effectively on unseen target domains. Notably, recent advances in\npre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated\nconsiderable potential in enhancing the generalization capabilities of deep\nlearning models. Despite the increasing attention toward VFM-based domain\nprompt tuning within DG, the effective design of prompts capable of\ndisentangling invariant features across diverse domains remains a critical\nchallenge. In this paper, we propose addressing this challenge by leveraging\nthe controllable and flexible language prompt of the VFM. Noting that the text\nmodality of VFMs is naturally easier to disentangle, we introduce a novel\nframework for text feature-guided visual prompt tuning. This framework first\nautomatically disentangles the text prompt using a large language model (LLM)\nand then learns domain-invariant visual representation guided by the\ndisentangled text feature. However, relying solely on language to guide visual\nfeature disentanglement has limitations, as visual features can sometimes be\ntoo complex or nuanced to be fully captured by descriptive text. To address\nthis, we introduce Worst Explicit Representation Alignment (WERA), which\nextends text-guided visual prompts by incorporating an additional set of\nabstract prompts. These prompts enhance source domain diversity through\nstylized image augmentations, while alignment constraints ensure that visual\nrepresentations remain consistent across both the original and augmented\ndistributions. Experiments conducted on major DG datasets, including PACS,\nVLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method\noutperforms state-of-the-art DG methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02288v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过语言引导和表征对齐实现域泛化的提示解耦", "tldr": "本文提出了一种用于域泛化（DG）的新框架，利用大型语言模型（LLM）解耦文本提示来指导视觉提示微调，并通过最差显式表征对齐（WERA）引入抽象提示，以处理复杂的视觉特征。该方法在主要DG数据集上取得了最先进的性能。", "motivation": "在域泛化（DG）中，为基于视觉基础模型（VFM）的域提示微调有效设计能够解耦跨域不变特征的提示仍然是一个关键挑战。", "method": "本文提出了一种新颖的文本特征引导视觉提示微调框架。该框架首先使用大型语言模型（LLM）自动解耦文本提示，然后通过解耦的文本特征引导学习域不变的视觉表征。为了解决单纯依靠语言指导视觉特征解耦的局限性，引入了最差显式表征对齐（WERA）。WERA通过结合额外的抽象提示来扩展文本引导的视觉提示，这些提示通过风格化图像增强来增强源域多样性，同时对齐约束确保视觉表征在原始和增强分布中保持一致。", "result": "在包括PACS、VLCS、OfficeHome、DomainNet和TerraInc在内的主要DG数据集上进行的实验表明，所提出的方法优于最先进的DG方法。", "conclusion": "本文提出的通过语言引导和表征对齐实现提示解耦的方法，有效解决了域泛化中不变特征解耦的挑战，并在多个基准数据集上取得了超越现有先进方法的性能。", "translation": "域泛化（DG）旨在开发一种多功能模型，使其能够在未见过的目标域上有效执行。值得注意的是，预训练视觉基础模型（VFMs），如CLIP，在增强深度学习模型的泛化能力方面展现出巨大的潜力。尽管基于VFM的域提示微调在DG中受到越来越多的关注，但有效设计能够解耦跨域不变特征的提示仍然是一个关键挑战。在本文中，我们提出通过利用VFM的可控和灵活的语言提示来解决这一挑战。注意到VFMs的文本模态自然更容易解耦，我们引入了一种新颖的文本特征引导视觉提示微调框架。该框架首先使用大型语言模型（LLM）自动解耦文本提示，然后通过解耦的文本特征引导学习域不变的视觉表征。然而，单纯依靠语言来引导视觉特征解耦存在局限性，因为视觉特征有时可能过于复杂或细微，无法完全通过描述性文本捕捉。为了解决这个问题，我们引入了最差显式表征对齐（WERA），它通过结合一组额外的抽象提示来扩展文本引导的视觉提示。这些提示通过风格化图像增强来增强源域多样性，同时对齐约束确保视觉表征在原始和增强分布中保持一致。在包括PACS、VLCS、OfficeHome、DomainNet和TerraInc在内的主要DG数据集上进行的实验表明，我们提出的方法优于最先进的DG方法。", "summary": "本文提出了一种名为“通过语言引导和表征对齐的提示解耦”（Prompt Disentanglement via Language Guidance and Representation Alignment，简称WERA）的新框架，用于域泛化。该方法利用大型语言模型（LLM）解耦文本提示，并以此指导视觉提示微调，以学习域不变特征。为了克服纯语言引导的局限性，WERA引入了抽象提示和对齐约束，通过增强源域多样性和保持视觉表征一致性来处理复杂的视觉特征。实验结果表明，该方法在多个主流域泛化数据集上优于现有先进方法。", "keywords": "域泛化, 提示微调, 视觉基础模型, 语言引导, 表征对齐", "comments": "本文的创新之处在于结合了基于LLM的文本提示解耦与视觉提示微调，并引入了WERA来解决纯语言引导的局限性。通过整合抽象提示和表征对齐，该方法能够更鲁棒地实现域泛化，尤其是在处理视觉特征的复杂性和多样性方面。"}}
{"id": "2507.02129", "title": "Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction", "authors": ["Xiao Li", "Liangji Zhu", "Anand Rangarajan", "Sanjay Ranka"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.02129v1", "summary": "Generative models have demonstrated strong performance in conditional\nsettings and can be viewed as a form of data compression, where the condition\nserves as a compact representation. However, their limited controllability and\nreconstruction accuracy restrict their practical application to data\ncompression. In this work, we propose an efficient latent diffusion framework\nthat bridges this gap by combining a variational autoencoder with a conditional\ndiffusion model. Our method compresses only a small number of keyframes into\nlatent space and uses them as conditioning inputs to reconstruct the remaining\nframes via generative interpolation, eliminating the need to store latent\nrepresentations for every frame. This approach enables accurate spatiotemporal\nreconstruction while significantly reducing storage costs. Experimental results\nacross multiple datasets show that our method achieves up to 10 times higher\ncompression ratios than rule-based state-of-the-art compressors such as SZ3,\nand up to 63 percent better performance than leading learning-based methods\nunder the same reconstruction error.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.02129v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "用于高效时空数据压缩的生成式潜在扩散模型", "tldr": "本文提出了一种生成式潜在扩散框架，通过结合变分自编码器和条件扩散模型，显著提高了时空数据压缩效率和重建精度。", "motivation": "生成模型在条件设置下表现出强大的数据压缩能力，但其有限的可控性和重建精度限制了其实际应用。", "method": "本文提出了一种高效的潜在扩散框架，结合了变分自编码器（VAE）和条件扩散模型。该方法仅将少量关键帧压缩到潜在空间，并将其作为条件输入，通过生成插值重建其余帧，从而无需存储每一帧的潜在表示。", "result": "实验结果表明，该方法比基于规则的先进压缩器（如SZ3）实现了高达10倍的压缩比，并且在相同重建误差下，比领先的基于学习的方法性能提高了高达63%。", "conclusion": "本文提出的生成式潜在扩散框架能够实现准确的时空数据重建，同时显著降低存储成本，并在压缩比和性能上超越了现有的先进方法。", "translation": "生成模型在条件设置中表现出强大的性能，可以被视为一种数据压缩形式，其中条件作为紧凑表示。然而，它们有限的可控性和重建精度限制了它们在数据压缩方面的实际应用。在这项工作中，我们提出了一种高效的潜在扩散框架，通过结合变分自编码器和条件扩散模型来弥补这一差距。我们的方法仅将少量关键帧压缩到潜在空间，并将其用作条件输入，通过生成插值重建剩余帧，从而无需存储每一帧的潜在表示。这种方法能够实现准确的时空重建，同时显著降低存储成本。在多个数据集上的实验结果表明，我们的方法比基于规则的先进压缩器（如SZ3）实现了高达10倍的压缩比，并且在相同重建误差下，比领先的基于学习的方法性能提高了高达63%。", "summary": "本文提出了一种名为生成式潜在扩散（Generative Latent Diffusion）的新框架，旨在解决现有生成模型在数据压缩中可控性和重建精度不足的问题。该方法结合了变分自编码器和条件扩散模型，通过仅压缩和存储少量关键帧的潜在表示，并利用生成插值重建其余帧，大幅减少了存储需求。实验证明，该方法在多个数据集上实现了比现有规则型压缩器高10倍的压缩比，并比学习型方法在相同重建误差下性能提升高达63%，显著提高了时空数据压缩的效率和准确性。", "keywords": "时空数据压缩, 潜在扩散模型, 生成模型, 变分自编码器, 数据降维", "comments": "该论文提出了一种创新的数据压缩方法，通过结合VAE和条件扩散模型，有效地解决了传统生成模型在数据压缩应用中的局限性。其核心创新在于只存储关键帧的潜在表示并通过生成插值重建其他帧，显著提高了压缩效率。实验结果令人印象深刻，表明该方法在压缩比和重建性能方面均优于现有SOTA方法，对时空数据处理领域具有重要意义。"}}
{"id": "2507.02595", "title": "MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion", "authors": ["Xin Guan", "PeiHsin Lin", "Zekun Wu", "Ze Wang", "Ruibo Zhang", "Emre Kazim", "Adriano Koshiyama"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at ICML 2025 AIW Workshop", "url": "http://arxiv.org/abs/2507.02595v1", "summary": "Multiperspective Fusion (MPF) is a novel posttraining alignment framework for\nlarge language models (LLMs) developed in response to the growing need for easy\nbias mitigation. Built on top of the SAGED pipeline, an automated system for\nconstructing bias benchmarks and extracting interpretable baseline\ndistributions, MPF leverages multiperspective generations to expose and align\nbiases in LLM outputs with nuanced, humanlike baselines. By decomposing\nbaseline, such as sentiment distributions from HR professionals, into\ninterpretable perspective components, MPF guides generation through sampling\nand balancing of responses, weighted by the probabilities obtained in the\ndecomposition. Empirically, we demonstrate its ability to align LLM sentiment\ndistributions with both counterfactual baselines (absolute equality) and the HR\nbaseline (biased for Top Univeristy), resulting in small KL divergence,\nreduction of calibration error and generalization to unseen questions. This\nshows that MPF offers a scalable and interpretable method for alignment and\nbias mitigation, compatible with deployed LLMs and requiring no extensive\nprompt engineering or finetuning.", "comment": "Accepted at ICML 2025 AIW Workshop", "pdf_url": "http://arxiv.org/pdf/2507.02595v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MPF：通过多视角融合在部署后对语言模型进行对齐和去偏", "tldr": "MPF是一种新型的后训练对齐框架，用于缓解LLM偏见，它通过多视角生成和分解人类基线来引导响应生成，从而实现与人类基线的对齐和偏见缓解。", "motivation": "解决大型语言模型（LLMs）部署后日益增长的偏见缓解需求。", "method": "MPF建立在SAGED管道之上，该管道用于构建偏见基准和提取可解释的基线分布。MPF利用多视角生成来暴露和对齐LLM输出中的偏见，通过将基线（如人力资源专业人士的情感分布）分解为可解释的视角组件，并通过采样和平衡响应（根据分解中获得的概率加权）来指导生成。", "result": "经验证明，MPF能够将LLM情感分布与反事实基线（绝对平等）和HR基线（偏向顶尖大学）对齐，导致KL散度小，校准误差降低，并泛化到未见过的问题。", "conclusion": "MPF提供了一种可扩展且可解释的对齐和偏见缓解方法，兼容已部署的LLMs，且不需要大量的提示工程或微调。", "translation": "多视角融合（MPF）是一种新颖的后训练对齐框架，旨在应对部署后大型语言模型（LLMs）日益增长的偏见缓解需求。MPF建立在SAGED管道之上，该管道是一个用于构建偏见基准和提取可解释基线分布的自动化系统。MPF利用多视角生成来揭示和对齐LLM输出中的偏见，并与细致入微的、类人的基线进行对齐。通过将基线（例如人力资源专业人士的情感分布）分解为可解释的视角组件，MPF通过对响应进行采样和平衡（根据分解中获得的概率加权）来指导生成。经验上，我们证明了它能够将LLM情感分布与反事实基线（绝对平等）和HR基线（偏向顶尖大学）对齐，从而导致KL散度小，校准误差降低，并泛化到未见过的问题。这表明MPF提供了一种可扩展且可解释的对齐和偏见缓解方法，兼容已部署的LLMs，并且不需要大量的提示工程或微调。", "summary": "本文提出了多视角融合（MPF），一个针对大型语言模型（LLMs）部署后的新型后训练对齐框架，旨在缓解偏见。MPF基于SAGED管道构建偏见基准和可解释基线分布，并通过多视角生成和将基线分解为可解释的视角组件来指导LLM响应的采样和平衡，从而实现与人类基线的对齐和偏见缓解。实验结果表明，MPF能有效对齐LLM情感分布，减少KL散度和校准误差，并具有泛化能力，为已部署LLMs提供了一种无需大量工程或微调的可扩展、可解释的去偏方法。", "keywords": "多视角融合, 语言模型, 偏见缓解, 后训练对齐, 可解释性", "comments": "MPF的创新之处在于其通过“多视角融合”和“分解基线”的方式来对齐和去偏，这提供了一种比传统微调或提示工程更轻量级且兼容部署模型的方法。其可扩展性和可解释性是重要亮点，特别是在实际应用中。"}}
{"id": "2507.02737", "title": "Early Signs of Steganographic Capabilities in Frontier LLMs", "authors": ["Artur Zolkowski", "Kei Nishimura-Gasparian", "Robert McCarthy", "Roland S. Zimmermann", "David Lindner"], "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02737v1", "summary": "Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks\nfrom misuse and misalignment. However, LLMs could evade monitoring through\nsteganography: Encoding hidden information within seemingly benign generations.\nIn this paper, we evaluate the steganography capabilities in frontier LLMs to\nbetter understand the risk they pose. We focus on two types of steganography:\npassing encoded messages and performing encoded reasoning. We find that current\nmodels are unable to encode short messages in their outputs without a monitor\nnoticing under standard affordances. They can succeed, however, if given\nadditional affordances such as using an unmonitored scratchpad and coordinating\non what encoding scheme to use. We additionally find early signs that models\ncan perform basic encoded reasoning in a simple state-tracking problem. This\nincludes some ability to reason with their own and pre-defined schemes,\nincluding encoding schemes such as Hexadecimal. Despite this, they can rarely\nhide reasoning subtly within a cover task to fool a monitor. Overall, our\nresults indicate that current LLMs exhibit nascent steganographic capabilities.\nWhile these capabilities are likely insufficient to bypass well-designed\nmonitors at present, this could change in the future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02737v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "前沿大型语言模型隐写能力早期迹象", "tldr": "大型语言模型（LLM）展现出初步的隐写能力，尤其是在获得额外辅助时，但目前尚不足以轻易规避良好的监控。", "motivation": "为了更好地理解大型语言模型（LLM）可能通过隐写术规避监控所带来的风险，这对于缓解滥用和失准至关重要。", "method": "研究评估了前沿LLM的隐写能力，重点关注两种类型：传递编码消息和执行编码推理。测试在标准条件和额外辅助（例如使用未受监控的草稿板和协调编码方案）下进行。", "result": "当前模型在标准条件下无法在不被监控者发现的情况下编码短消息。但在提供额外辅助（如草稿板和协调编码方案）时，它们可以成功。此外，研究发现模型在简单的状态跟踪问题中存在执行基本编码推理的早期迹象，包括使用自身和预定义方案（如十六进制）进行推理。然而，它们很少能巧妙地将推理隐藏在封面任务中以欺骗监控者。", "conclusion": "当前LLM展现出初期的隐写能力。尽管这些能力目前可能不足以绕过精心设计的监控器，但这在未来可能会改变。", "translation": "监控大型语言模型（LLM）的输出对于缓解滥用和失准带来的风险至关重要。然而，LLM可能通过隐写术来规避监控：在看似良性的生成内容中编码隐藏信息。在本文中，我们评估了前沿LLM的隐写能力，以更好地理解它们带来的风险。我们重点关注两种类型的隐写术：传递编码消息和执行编码推理。我们发现，在标准条件下，当前模型无法在不被监控者发现的情况下在其输出中编码短消息。但是，如果提供额外的便利，例如使用未受监控的草稿板和协调使用何种编码方案，它们就能成功。我们还发现早期迹象表明模型可以在简单的状态跟踪问题中执行基本的编码推理。这包括使用其自身和预定义方案（包括十六进制等编码方案）进行推理的能力。尽管如此，它们很少能巧妙地将推理隐藏在封面任务中以欺骗监控者。总的来说，我们的结果表明，当前LLM展现出初期的隐写能力。虽然这些能力目前可能不足以绕过精心设计的监控器，但这在未来可能会改变。", "summary": "本文探讨了前沿大型语言模型（LLM）的初期隐写能力，以评估它们规避监控的潜在风险。研究人员评估了LLM传递编码消息和执行编码推理的能力。他们发现，尽管LLM在标准条件下难以隐藏短消息，但若提供额外辅助（如未受监控的草稿板或协调的编码方案），它们便能成功。研究还观察到LLM执行基本编码推理（甚至使用十六进制等方案）的早期迹象，尽管它们很少能巧妙地隐藏这些推理。研究结果表明，当前LLM具备正在发展的隐写能力，尽管目前不足以绕过复杂的监控器，但未来可能会有所改变。", "keywords": "LLMs, 隐写术, 秘密通信, 模型监控, 编码推理", "comments": "这篇论文揭示了LLM一个关键的新兴风险——它们进行秘密通信的潜力。论文发现模型在“额外辅助”下（如草稿板）能够成功，这表明当前可能只关注最终输出的监控范式存在漏洞。“早期迹象”的编码推理尤其令人担忧，因为它指向更复杂、可能更自主的秘密行为。这项研究对于指导开发更强大的LLM监控和安全协议至关重要。"}}
{"id": "2507.02858", "title": "Requirements Elicitation Follow-Up Question Generation", "authors": ["Yuchen Shen", "Anmol Singhal", "Travis Breaux"], "categories": ["cs.SE", "cs.CL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      13 pages, 2 figures, accepted at the 33rd IEEE International Requirements Engineering 2025", "url": "http://arxiv.org/abs/2507.02858v1", "summary": "Interviews are a widely used technique in eliciting requirements to gather\nstakeholder needs, preferences, and expectations for a software system.\nEffective interviewing requires skilled interviewers to formulate appropriate\ninterview questions in real time while facing multiple challenges, including\nlack of familiarity with the domain, excessive cognitive load, and information\noverload that hinders how humans process stakeholders' speech. Recently, large\nlanguage models (LLMs) have exhibited state-of-the-art performance in multiple\nnatural language processing tasks, including text summarization and entailment.\nTo support interviewers, we investigate the application of GPT-4o to generate\nfollow-up interview questions during requirements elicitation by building on a\nframework of common interviewer mistake types. In addition, we describe methods\nto generate questions based on interviewee speech. We report a controlled\nexperiment to evaluate LLM-generated and human-authored questions with minimal\nguidance, and a second controlled experiment to evaluate the LLM-generated\nquestions when generation is guided by interviewer mistake types. Our findings\ndemonstrate that, for both experiments, the LLM-generated questions are no\nworse than the human-authored questions with respect to clarity, relevancy, and\ninformativeness. In addition, LLM-generated questions outperform human-authored\nquestions when guided by common mistakes types. This highlights the potential\nof using LLMs to help interviewers improve the quality and ease of requirements\nelicitation interviews in real time.", "comment": "13 pages, 2 figures, accepted at the 33rd IEEE International\n  Requirements Engineering 2025", "pdf_url": "http://arxiv.org/pdf/2507.02858v1", "cate": "cs.SE", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "需求获取后续问题生成", "tldr": "本研究探讨使用大型语言模型（LLMs），特别是GPT-4o，在需求获取过程中生成后续面试问题，并发现其生成的问题在清晰度、相关性和信息量方面不逊于人类，在特定指导下甚至表现更优。", "motivation": "访谈是需求获取中常用的技术，但有效的访谈需要熟练的面试官实时提出恰当的问题，这面临领域不熟悉、认知负荷过重和信息过载等挑战。", "method": "研究人员探索了GPT-4o在需求获取过程中生成后续面试问题的应用，该方法基于常见面试官错误类型框架构建。同时，描述了根据受访者发言生成问题的方法。通过两次对照实验进行评估：一次是LLM生成问题与人类撰写问题的对比（最少指导），另一次是LLM生成问题在面试官错误类型指导下的评估。", "result": "实验结果表明，在清晰度、相关性和信息量方面，LLM生成的问题不逊于人类撰写的问题。此外，当LLM生成问题受到常见错误类型指导时，其表现优于人类撰写的问题。", "conclusion": "研究表明，使用大型语言模型可以帮助面试官实时提高需求获取访谈的质量和便利性，具有巨大的潜力。", "translation": "访谈是一种广泛使用的需求获取技术，旨在收集涉众对软件系统的需求、偏好和期望。有效的访谈需要熟练的面试官在面对领域不熟悉、认知负荷过重和信息过载等多种挑战时，实时制定合适的访谈问题，这些挑战阻碍了人类处理涉众的言语。最近，大型语言模型（LLMs）在多项自然语言处理任务中表现出最先进的性能，包括文本摘要和蕴涵。为了支持面试官，我们研究了GPT-4o在需求获取过程中生成后续访谈问题的应用，该方法建立在常见面试官错误类型框架之上。此外，我们描述了根据受访者言语生成问题的方法。我们报告了一项对照实验，评估了LLM生成的问题和人类撰写的问题在最少指导下的表现，以及第二项对照实验，评估了LLM生成的问题在面试官错误类型指导下的表现。我们的发现表明，在两次实验中，LLM生成的问题在清晰度、相关性和信息量方面不逊于人类撰写的问题。此外，当LLM生成的问题受到常见错误类型指导时，其表现优于人类撰写的问题。这突出了使用LLMs帮助面试官实时提高需求获取访谈质量和便利性的潜力。", "summary": "本研究探讨了利用大型语言模型（LLMs），特别是GPT-4o，在软件需求获取访谈中自动生成后续问题。针对面试官在访谈中面临的挑战，作者提出了一种基于常见面试官错误类型框架和受访者言语的方法。通过两项对照实验，研究发现LLM生成的问题在清晰度、相关性和信息量方面与人类撰写的问题相当，并且在错误类型指导下表现更优。这表明LLMs在提升需求获取访谈效率和质量方面具有显著潜力。", "keywords": "需求获取, 大型语言模型, GPT-4o, 后续问题生成, 访谈辅助", "comments": "这项研究具有重要的实践意义，它利用了大型语言模型在自然语言处理方面的最新进展，解决了软件需求获取中面试官面临的实际挑战。通过引入“面试官错误类型”作为指导，模型生成的后续问题质量得到了显著提升，这为LLM在专业领域辅助人类工作提供了新的思路。该方法有望减轻面试官的认知负担，提高需求获取的效率和准确性。"}}
{"id": "2507.02672", "title": "MISCGrasp: Leveraging Multiple Integrated Scales and Contrastive Learning for Enhanced Volumetric Grasping", "authors": ["Qingyu Fan", "Yinghao Cai", "Chao Li", "Chunting Jiao", "Xudong Zheng", "Tao Lu", "Bin Liang", "Shuo Wang"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2025", "url": "http://arxiv.org/abs/2507.02672v1", "summary": "Robotic grasping faces challenges in adapting to objects with varying shapes\nand sizes. In this paper, we introduce MISCGrasp, a volumetric grasping method\nthat integrates multi-scale feature extraction with contrastive feature\nenhancement for self-adaptive grasping. We propose a query-based interaction\nbetween high-level and low-level features through the Insight Transformer,\nwhile the Empower Transformer selectively attends to the highest-level\nfeatures, which synergistically strikes a balance between focusing on fine\ngeometric details and overall geometric structures. Furthermore, MISCGrasp\nutilizes multi-scale contrastive learning to exploit similarities among\npositive grasp samples, ensuring consistency across multi-scale features.\nExtensive experiments in both simulated and real-world environments demonstrate\nthat MISCGrasp outperforms baseline and variant methods in tabletop\ndecluttering tasks. More details are available at https://miscgrasp.github.io/.", "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS), 2025", "pdf_url": "http://arxiv.org/pdf/2507.02672v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MISCGrasp：利用多尺度集成和对比学习增强体积抓取", "tldr": "MISCGrasp通过结合多尺度特征和对比学习来提升机器人体积抓取性能，使其能更好地适应不同形状和尺寸的物体。", "motivation": "机器人抓取面临难以适应不同形状和尺寸物体的问题，需要一种能够自适应的抓取方法。", "method": "MISCGrasp是一种体积抓取方法，它整合了多尺度特征提取和对比特征增强。该方法提出了通过Insight Transformer在高层和低层特征之间进行基于查询的交互，同时Empower Transformer选择性地关注最高层特征，以平衡精细几何细节和整体几何结构。此外，MISCGrasp利用多尺度对比学习来利用正抓取样本之间的相似性，确保多尺度特征的一致性。", "result": "在模拟和真实环境中的大量实验表明，MISCGrasp在桌面清理任务中优于基线和变体方法。", "conclusion": "MISCGrasp通过其创新的多尺度特征提取和对比学习方法，显著提升了机器人体积抓取的性能和自适应能力，成功应对了不同形状和尺寸物体的抓取挑战。", "translation": "机器人抓取面临难以适应不同形状和尺寸物体的问题。本文介绍了MISCGrasp，这是一种体积抓取方法，它集成了多尺度特征提取和对比特征增强，以实现自适应抓取。我们提出了通过Insight Transformer在高层和低层特征之间进行基于查询的交互，而Empower Transformer选择性地关注最高层特征，这协同地在关注精细几何细节和整体几何结构之间取得了平衡。此外，MISCGrasp利用多尺度对比学习来利用正抓取样本之间的相似性，确保多尺度特征的一致性。在模拟和真实环境中的大量实验表明，MISCGrasp在桌面清理任务中优于基线和变体方法。更多详情请访问 https://miscgrasp.github.io/。", "summary": "MISCGrasp是一种新颖的机器人体积抓取方法，旨在解决现有抓取系统难以适应多变物体的问题。它通过结合多尺度特征提取和对比学习来增强抓取能力，并引入了Insight Transformer和Empower Transformer来优化不同层级特征的交互与关注，从而平衡对精细细节和整体结构的考量。实验证明，MISCGrasp在模拟和真实桌面清理任务中表现优异，超越了现有方法。", "keywords": "机器人抓取, 体积抓取, 多尺度学习, 对比学习, 深度学习", "comments": "该论文提出了一种创新的机器人抓取方法MISCGrasp，其亮点在于结合了多尺度特征提取和对比学习，以提高抓取对物体形状和尺寸的适应性。引入Insight Transformer和Empower Transformer来精细化特征交互和关注，是其方法学上的一个创新点，有助于平衡局部细节和全局结构。通过多尺度对比学习确保特征一致性也增强了方法的鲁棒性。"}}
{"id": "2507.02554", "title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench", "authors": ["Edan Toledo", "Karen Hambardzumyan", "Martin Josifoski", "Rishi Hazra", "Nicolas Baldwin", "Alexis Audran-Reiss", "Michael Kuchnik", "Despoina Magka", "Minqi Jiang", "Alisia Maria Lupidi", "Andrei Lupu", "Roberta Raileanu", "Kelvin Niu", "Tatiana Shavrina", "Jean-Christophe Gagnon-Audet", "Michael Shvartsman", "Shagun Sodhani", "Alexander H. Miller", "Abhishek Charnalia", "Derek Dunfield", "Carole-Jean Wu", "Pontus Stenetorp", "Nicola Cancedda", "Jakob Nicolaus Foerster", "Yoram Bachrach"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Code: this https URL", "url": "http://arxiv.org/abs/2507.02554v1", "summary": "AI research agents are demonstrating great potential to accelerate scientific\nprogress by automating the design, implementation, and training of machine\nlearning models. We focus on methods for improving agents' performance on\nMLE-bench, a challenging benchmark where agents compete in Kaggle competitions\nto solve real-world machine learning problems. We formalize AI research agents\nas search policies that navigate a space of candidate solutions, iteratively\nmodifying them using operators. By designing and systematically varying\ndifferent operator sets and search policies (Greedy, MCTS, Evolutionary), we\nshow that their interplay is critical for achieving high performance. Our best\npairing of search strategy and operator set achieves a state-of-the-art result\non MLE-bench lite, increasing the success rate of achieving a Kaggle medal from\n39.6% to 47.7%. Our investigation underscores the importance of jointly\nconsidering the search strategy, operator design, and evaluation methodology in\nadvancing automated machine learning.", "comment": "Code: https://github.com/facebookresearch/aira-dojo", "pdf_url": "http://arxiv.org/pdf/2507.02554v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "机器学习的AI研究代理：MLE-bench中的搜索、探索和泛化", "tldr": "本文研究了如何通过设计操作符集和搜索策略来提高AI研究代理在MLE-bench基准测试上的性能，并取得了最先进的结果。", "motivation": "AI研究代理在自动化机器学习模型的设计、实现和训练方面显示出巨大潜力，但需要在挑战性基准（MLE-bench）上提高其解决现实世界机器学习问题的性能。", "method": "将AI研究代理形式化为搜索策略，这些策略通过操作符迭代修改候选解决方案。通过设计和系统地改变不同的操作符集和搜索策略（贪婪、MCTS、进化），研究它们之间的相互作用。", "result": "最佳的搜索策略和操作符集配对在MLE-bench lite上达到了最先进的结果，将获得Kaggle奖牌的成功率从39.6%提高到47.7%。", "conclusion": "共同考虑搜索策略、操作符设计和评估方法对于推进自动化机器学习至关重要。", "translation": "AI研究代理正在通过自动化机器学习模型的设计、实现和训练，展现出加速科学进步的巨大潜力。我们专注于提高代理在MLE-bench上的性能，这是一个具有挑战性的基准，代理在Kaggle竞赛中竞争解决现实世界的机器学习问题。我们将AI研究代理形式化为搜索策略，这些策略在候选解决方案空间中导航，使用操作符迭代地修改它们。通过设计和系统地改变不同的操作符集和搜索策略（贪婪、蒙特卡洛树搜索、进化），我们表明它们之间的相互作用对于实现高性能至关重要。我们最佳的搜索策略和操作符集配对在MLE-bench lite上取得了最先进的结果，将获得Kaggle奖牌的成功率从39.6%提高到47.7%。我们的研究强调了在推进自动化机器学习时，联合考虑搜索策略、操作符设计和评估方法的重要性。", "summary": "本文研究了如何提高AI研究代理在MLE-bench基准测试上的性能，该基准测试涉及解决Kaggle竞赛中的机器学习问题。作者将代理形式化为搜索策略，并系统地探索了不同的操作符集和搜索策略（如贪婪、MCTS、进化）的组合。结果表明，最佳的策略与操作符配对在MLE-bench lite上取得了最先进的成果，将Kaggle奖牌成功率从39.6%提升至47.7%，强调了搜索策略、操作符设计和评估方法共同作用的重要性。", "keywords": "AI研究代理, 机器学习自动化, MLE-bench, 搜索策略, 操作符设计", "comments": "本文创新性地将AI研究代理视为搜索策略，并通过系统地探索操作符集和搜索策略的组合，显著提高了自动化机器学习的性能。其在MLE-bench lite上取得的SOTA结果证明了该方法的有效性，为未来的自动化机器学习研究提供了重要启示，尤其强调了系统设计的重要性。"}}
{"id": "2507.02294", "title": "ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation", "authors": ["Hanbo Bi", "Yulong Xu", "Ya Li", "Yongqiang Mao", "Boyuan Tong", "Chongyang Li", "Chunbo Lang", "Wenhui Diao", "Hongqi Wang", "Yingchao Feng", "Xian Sun"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02294v1", "summary": "The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits\nstrong generalization in generic segmentation tasks. However, applying SAM to\nremote sensing (RS) images still faces two major challenges. First, manually\nconstructing precise prompts for each image (e.g., points or boxes) is\nlabor-intensive and inefficient, especially in RS scenarios with dense small\nobjects or spatially fragmented distributions. Second, SAM lacks domain\nadaptability, as it is pre-trained primarily on natural images and struggles to\ncapture RS-specific semantics and spatial characteristics, especially when\nsegmenting novel or unseen classes. To address these issues, inspired by\nfew-shot learning, we propose ViRefSAM, a novel framework that guides SAM\nutilizing only a few annotated reference images that contain class-specific\nobjects. Without requiring manual prompts, ViRefSAM enables automatic\nsegmentation of class-consistent objects across RS images. Specifically,\nViRefSAM introduces two key components while keeping SAM's original\narchitecture intact: (1) a Visual Contextual Prompt Encoder that extracts\nclass-specific semantic clues from reference images and generates object-aware\nprompts via contextual interaction with target images; and (2) a Dynamic Target\nAlignment Adapter, integrated into SAM's image encoder, which mitigates the\ndomain gap by injecting class-specific semantics into target image features,\nenabling SAM to dynamically focus on task-relevant regions. Extensive\nexperiments on three few-shot segmentation benchmarks, including iSAID-5$^i$,\nLoveDA-2$^i$, and COCO-20$^i$, demonstrate that ViRefSAM enables accurate and\nautomatic segmentation of unseen classes by leveraging only a few reference\nimages and consistently outperforms existing few-shot segmentation methods\nacross diverse datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02294v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "ViRefSAM：遥感图像分割的视觉参考引导分割一切模型", "tldr": "ViRefSAM提出了一种视觉参考引导的方法，解决了SAM在遥感图像分割中手动提示低效和领域适应性差的问题，通过少量参考图像实现了对未知类别的自动精确分割。", "motivation": "SAM在通用分割任务中表现出色，但应用于遥感（RS）图像时面临两大挑战：1. 手动构建精确提示（如点或框）劳动密集且效率低下，尤其是在遥感场景中对象密集或分布分散时。2. SAM主要在自然图像上预训练，缺乏领域适应性，难以捕捉遥感特有的语义和空间特征，尤其是在分割新颖或未见类别时。", "method": "受少样本学习启发，本文提出了ViRefSAM框架。它通过利用少量包含特定类别对象的带注释参考图像来引导SAM，无需手动提示即可自动分割遥感图像中类别一致的对象。ViRefSAM在不改变SAM原始架构的前提下引入了两个关键组件：1. 一个视觉上下文提示编码器（Visual Contextual Prompt Encoder），用于从参考图像中提取类别特定语义线索，并通过与目标图像的上下文交互生成对象感知提示。2. 一个动态目标对齐适配器（Dynamic Target Alignment Adapter），集成到SAM的图像编码器中，通过将类别特定语义注入目标图像特征来弥合领域差距，使SAM能够动态关注任务相关区域。", "result": "在iSAID-5i、LoveDA-2i和COCO-20i三个少样本分割基准上的大量实验表明，ViRefSAM仅利用少量参考图像就能实现对未见类别的准确自动分割，并且在不同数据集上始终优于现有的少样本分割方法。", "conclusion": "ViRefSAM通过引入视觉上下文提示编码器和动态目标对齐适配器，有效解决了SAM在遥感图像分割中面临的手动提示和领域适应性挑战，实现了基于少量参考图像的未知类别自动精确分割，并在少样本分割任务中表现优异。", "translation": "分割一切模型（SAM）凭借其提示驱动的范式，在通用分割任务中展现出强大的泛化能力。然而，将SAM应用于遥感（RS）图像仍面临两大挑战。首先，为每张图像手动构建精确提示（例如，点或框）是劳动密集且低效的，尤其是在具有密集小目标或空间零碎分布的遥感场景中。其次，SAM缺乏领域适应性，因为它主要在自然图像上进行预训练，难以捕捉遥感特有的语义和空间特征，尤其是在分割新颖或未见类别时。为了解决这些问题，受少样本学习的启发，我们提出了ViRefSAM，一个新颖的框架，它仅利用少量包含类别特定对象的带注释参考图像来引导SAM。无需手动提示，ViRefSAM能够实现遥感图像中类别一致对象的自动分割。具体而言，ViRefSAM在保持SAM原始架构不变的同时引入了两个关键组件：（1）一个视觉上下文提示编码器，它从参考图像中提取类别特定语义线索，并通过与目标图像的上下文交互生成对象感知提示；（2）一个动态目标对齐适配器，集成到SAM的图像编码器中，通过将类别特定语义注入目标图像特征来弥合领域差距，使SAM能够动态关注任务相关区域。在包括iSAID-5i、LoveDA-2i和COCO-20i在内的三个少样本分割基准上的大量实验表明，ViRefSAM仅利用少量参考图像就能实现对未见类别的准确自动分割，并且在不同数据集上始终优于现有的少样本分割方法。", "summary": "ViRefSAM是一个新颖的框架，旨在解决SAM在遥感图像分割中面临的手动提示低效和领域适应性差的问题。受少样本学习启发，ViRefSAM通过引入视觉上下文提示编码器和动态目标对齐适配器，仅利用少量带注释的参考图像，即可自动实现对遥感图像中未知类别的精确分割，无需手动提示。实验证明，ViRefSAM在多个少样本分割基准上均优于现有方法。", "keywords": "遥感图像分割, SAM, 少样本学习, 视觉参考, 领域适应性", "comments": "ViRefSAM的创新之处在于其无需手动提示即可实现遥感图像的自动分割，并有效解决了SAM在特定领域（遥感）的适应性问题。通过引入视觉上下文提示编码器和动态目标对齐适配器，ViRefSAM在保持SAM核心架构不变的情况下，巧妙地融入了少样本学习的思想，使其能够高效地处理未见类别。这对于遥感图像分析领域具有重要意义，因为它极大地降低了人工标注的工作量，并提升了模型在复杂遥感场景下的泛化能力。"}}
{"id": "2507.02168", "title": "Experimental Multiport-Network Parameter Estimation and Optimization for Multi-Bit RIS", "authors": ["Philipp del Hougne"], "categories": ["physics.app-ph", "eess.SP"], "primary_category": "Subjects:       Applied Physics (physics.app-ph)", "pdf_link": null, "comments": "Comments:      5 pages including 2 figures", "url": "http://arxiv.org/abs/2507.02168v1", "summary": "Physics-consistent theoretical studies on RIS-parametrized wireless channels\nuse models from multiport-network theory (MNT) to capture mutual-coupling (MC)\neffects. However, in practice, RIS design and radio environment are partially\nor completely unknown. We fill a research gap on how to estimate the MNT model\nparameters in such experimentally relevant scenarios. Our technique efficiently\ncombines closed-form and gradient-descent steps, and it can be applied to\nmulti-bit-programmable RIS elements. We discuss inevitable (but operationally\nirrelevant) parameter ambiguities. We experimentally validate our technique in\nan unknown rich-scattering environment parametrized by eight 8-bit-programmable\nRIS elements of unknown design. We experimentally evaluate the performance of\nRIS configurations optimized with the estimated MNT model and an MC-unaware\ncascaded model. While the models differ in accuracy by up to 17 dB, the\nend-to-end performance differences are small.", "comment": "5 pages including 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.02168v1", "cate": "physics.app-ph", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "实验性多端口网络参数估计与多比特RIS优化", "tldr": "论文提出了一种在未知RIS设计和无线环境下，高效估计多端口网络（MNT）模型参数的方法，并通过实验验证，发现MNT模型与MC-unaware模型在端到端性能上差异不大，尽管模型精度有显著差异。", "motivation": "物理一致的RIS参数化无线信道理论研究使用多端口网络理论（MNT）模型来捕获互耦（MC）效应。然而，在实践中，RIS设计和无线环境通常部分或完全未知，导致MNT模型参数难以估计。本研究旨在填补在这些实验相关场景中如何估计MNT模型参数的研究空白。", "method": "该技术结合了闭式解和梯度下降步骤，并可应用于多比特可编程RIS元件。通过在未知丰富散射环境中，使用八个8比特可编程RIS元件进行实验验证。", "result": "实验结果显示，MNT模型与MC-unaware级联模型在精度上可能相差高达17 dB，但优化后的RIS配置在端到端性能上的差异很小。", "conclusion": "尽管在模型精度上MNT模型优于MC-unaware模型，但在实际的端到端性能方面，两者差异不大，这可能意味着在某些应用场景下，简化的模型也能达到可接受的性能。", "translation": "物理一致的RIS参数化无线信道理论研究使用多端口网络理论（MNT）模型来捕获互耦（MC）效应。然而，在实践中，RIS设计和无线环境部分或完全未知。我们填补了在这些实验相关场景中如何估计MNT模型参数的研究空白。我们的技术有效结合了闭式解和梯度下降步骤，并且可以应用于多比特可编程RIS元件。我们讨论了不可避免的（但操作上无关紧要的）参数模糊性。我们在由八个未知设计的8比特可编程RIS元件参数化的未知丰富散射环境中，通过实验验证了我们的技术。我们实验评估了使用估计的MNT模型和MC-unaware级联模型优化的RIS配置的性能。尽管模型精度差异高达17 dB，但端到端性能差异很小。", "summary": "本文提出了一种在RIS设计和无线环境未知的情况下，估计多端口网络（MNT）模型参数的有效技术。该方法结合了闭式解和梯度下降，并适用于多比特可编程RIS。实验验证表明，尽管MNT模型在精度上显著优于MC-unaware模型，但在端到端性能上，两者的优化配置差异很小。", "keywords": "多端口网络, RIS, 参数估计, 互耦, 梯度下降", "comments": "本文的创新点在于提出了在实际未知环境中估计RIS多端口网络模型参数的实用方法，填补了理论与实践之间的空白。其重要性在于为RIS的实际部署提供了参数估计的解决方案。一个有趣的发现是，尽管更精确的MNT模型与简化的MC-unaware模型在精度上有显著差异，但最终的端到端性能差异却很小，这可能暗示了在某些实际应用中，对模型精度的极致追求可能不会带来显著的性能提升，或者说明了RIS的优化对模型误差具有一定的鲁棒性。"}}
{"id": "2507.02151", "title": "Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks", "authors": ["Tuo Wang", "Jian Kang", "Yujun Yan", "Adithya Kulkarni", "Dawei Zhou"], "categories": ["cs.LG", "H.1.0; I.2.0"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      accepted by KDD 2025", "url": "http://arxiv.org/abs/2507.02151v1", "summary": "Conformal prediction for graph neural networks (GNNs) offers a promising\nframework for quantifying uncertainty, enhancing GNN reliability in high-stakes\napplications. However, existing methods predominantly focus on static graphs,\nneglecting the evolving nature of real-world graphs. Temporal dependencies in\ngraph structure, node attributes, and ground truth labels violate the\nfundamental exchangeability assumption of standard conformal prediction\nmethods, limiting their applicability. To address these challenges, in this\npaper, we introduce NCPNET, a novel end-to-end conformal prediction framework\ntailored for temporal graphs. Our approach extends conformal prediction to\ndynamic settings, mitigating statistical coverage violations induced by\ntemporal dependencies. To achieve this, we propose a diffusion-based\nnon-conformity score that captures both topological and temporal uncertainties\nwithin evolving networks. Additionally, we develop an efficiency-aware\noptimization algorithm that improves the conformal prediction process,\nenhancing computational efficiency and reducing coverage violations. Extensive\nexperiments on diverse real-world temporal graphs, including WIKI, REDDIT,\nDBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to\nensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction\nin prediction set size on the WIKI dataset, significantly improving efficiency\ncompared to state-of-the-art methods. Our data and code are available at\nhttps://github.com/ODYSSEYWT/NCPNET.", "comment": "accepted by KDD 2025", "pdf_url": "http://arxiv.org/pdf/2507.02151v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "针对时间图神经网络的不可交换共形预测", "tldr": "本文提出NCPNET，一个用于时间图的共形预测框架，通过新的不一致性分数和优化算法，解决了时间依赖性导致的覆盖率问题，并显著提高了预测效率。", "motivation": "现有图神经网络的共形预测方法主要关注静态图，忽视了真实世界图中结构、节点属性和标签的演变性质，这些时间依赖性违反了标准共形预测方法的“可交换性”假设，限制了其适用性。", "method": "本文引入了NCPNET，一个端到端的共形预测框架，专为时间图设计。该方法通过提出一种基于扩散的非一致性分数来捕获演化网络中的拓扑和时间不确定性，并开发了一种效率感知的优化算法来改进共形预测过程，从而提高计算效率并减少覆盖率违规。", "result": "在WIKI、REDDIT、DBLP和IBM反洗钱数据集等真实世界时间图上的广泛实验表明，NCPNET能够确保时间图中的覆盖率，并在WIKI数据集上将预测集大小减少高达31%，与现有最先进方法相比显著提高了效率。", "conclusion": "NCPNET通过引入新的非一致性分数和优化算法，有效解决了时间图中共形预测的挑战，确保了覆盖率并显著提高了效率，从而增强了时间图神经网络在实际应用中的可靠性。", "translation": "图神经网络（GNNs）的共形预测为量化不确定性提供了一个有前景的框架，增强了GNNs在高风险应用中的可靠性。然而，现有方法主要关注静态图，忽略了真实世界图的演变性质。图结构、节点属性和真实标签中的时间依赖性违反了标准共形预测方法的基本可交换性假设，限制了其适用性。为了解决这些挑战，本文引入了NCPNET，一个专门为时间图设计的端到端新型共形预测框架。我们的方法将共形预测扩展到动态设置，减轻了时间依赖性引起的统计覆盖率违规。为了实现这一点，我们提出了一种基于扩散的非一致性分数，该分数捕获了演化网络中的拓扑和时间不确定性。此外，我们开发了一种效率感知的优化算法，改进了共形预测过程，提高了计算效率并减少了覆盖率违规。在包括WIKI、REDDIT、DBLP和IBM反洗钱数据集在内的各种真实世界时间图上的广泛实验表明，NCPNET能够确保时间图中的覆盖率，在WIKI数据集上预测集大小减少高达31%，与现有最先进方法相比显著提高了效率。我们的数据和代码可在https://github.com/ODYSSEYWT/NCPNET获取。", "summary": "本文提出了NCPNET，一个针对时间图神经网络的端到端共形预测框架。该框架通过引入基于扩散的非一致性分数来处理时间依赖性，并开发了效率感知优化算法，以确保动态图中预测的覆盖率并提高计算效率。实验证明NCPNET在真实世界时间图上显著优于现有方法，降低了预测集大小并保证了覆盖率。", "keywords": "共形预测, 时间图神经网络, 不可交换性, 不确定性量化, 动态图", "comments": "本文的创新点在于将共形预测扩展到时间图领域，通过引入“非可交换性”的概念，并提出相应的扩散基非一致性分数和效率优化算法，有效解决了传统共形预测在动态图上失效的问题。这对于提升GNN在高风险应用中的不确定性量化和可靠性具有重要意义。其贡献在于不仅提出了理论框架，还通过实验验证了其在保证覆盖率和提高效率方面的优越性。"}}
{"id": "2507.02679", "title": "Exploring Gender Bias Beyond Occupational Titles", "authors": ["Ahmed Sabir", "Rajesh Sharama"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2507.02679v1", "summary": "In this work, we investigate the correlation between gender and contextual\nbiases, focusing on elements such as action verbs, object nouns, and\nparticularly on occupations. We introduce a novel dataset, GenderLexicon, and a\nframework that can estimate contextual bias and its related gender bias. Our\nmodel can interpret the bias with a score and thus improve the explainability\nof gender bias. Also, our findings confirm the existence of gender biases\nbeyond occupational stereotypes. To validate our approach and demonstrate its\neffectiveness, we conduct evaluations on five diverse datasets, including a\nJapanese dataset.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2507.02679v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "探索超越职业头衔的性别偏见", "tldr": "该研究调查了性别与语境偏见（包括动词、名词和职业）的关联，提出了新的数据集和框架来估计和解释性别偏见，并证实了性别偏见不仅存在于职业刻板印象中。", "motivation": "调查性别与语境偏见（包括动作动词、宾语名词和职业）之间的关联，并探索超越职业刻板印象的性别偏见。", "method": "引入了一个名为 GenderLexicon 的新数据集，并提出了一个可以估计语境偏见及其相关性别偏见的框架。该模型能够通过分数来解释偏见。通过在包括一个日语数据集在内的五个不同数据集上进行评估来验证方法。", "result": "研究发现证实了性别偏见的存在，且这种偏见超越了职业刻板印象。", "conclusion": "该研究证实了超越职业刻板印象的性别偏见的存在，并提供了一个可解释的框架来估计和理解这些偏见。", "translation": "在这项工作中，我们调查了性别与语境偏见之间的相关性，重点关注动作动词、宾语名词等元素，特别是职业。我们引入了一个新颖的数据集 GenderLexicon，以及一个可以估计语境偏见及其相关性别偏见的框架。我们的模型可以通过分数来解释偏见，从而提高了性别偏见的解释性。此外，我们的发现证实了超越职业刻板印象的性别偏见的存在。为了验证我们的方法并展示其有效性，我们在包括一个日语数据集在内的五个不同数据集上进行了评估。", "summary": "这项工作探讨了性别与语境偏见（如动作动词、宾语名词和职业）之间的关联。研究人员提出了一个新的数据集 GenderLexicon 和一个用于估计和解释性别偏见的框架。他们的模型能够通过分数量化和解释偏见，并且研究结果证实了性别偏见不仅存在于职业刻板印象中，还存在于更广泛的语境中。该方法通过在五个不同数据集（包括日语数据集）上进行评估来验证。", "keywords": "性别偏见, 语境偏见, GenderLexicon, 职业刻板印象, 可解释性", "comments": "这项研究的创新之处在于其超越传统职业头衔的视角来探索性别偏见，并引入了新的数据集 GenderLexicon 和一个可解释的框架。它强调了性别偏见在更广泛语境中的存在，这对于理解和缓解偏见具有重要意义。通过提供偏见分数，模型的可解释性得到提升。"}}
{"id": "2507.02770", "title": "NVIDIA GPU Confidential Computing Demystified", "authors": ["Zhongshu Gu", "Enriquillo Valdez", "Salman Ahmed", "Julian James Stephen", "Michael Le", "Hani Jamjoom", "Shixuan Zhao", "Zhiqiang Lin"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02770v1", "summary": "GPU Confidential Computing (GPU-CC) was introduced as part of the NVIDIA\nHopper Architecture, extending the trust boundary beyond traditional CPU-based\nconfidential computing. This innovation enables GPUs to securely process AI\nworkloads, providing a robust and efficient solution for handling sensitive\ndata. For end users, transitioning to GPU-CC mode is seamless, requiring no\nmodifications to existing AI applications. However, this ease of adoption\ncontrasts sharply with the complexity of the underlying proprietary systems.\nThe lack of transparency presents significant challenges for security\nresearchers seeking a deeper understanding of GPU-CC's architecture and\noperational mechanisms.\n  The challenges of analyzing the NVIDIA GPU-CC system arise from a scarcity of\ndetailed specifications, the proprietary nature of the ecosystem, and the\ncomplexity of product design. In this paper, we aim to demystify the\nimplementation of NVIDIA GPU-CC system by piecing together the fragmented and\nincomplete information disclosed from various sources. Our investigation begins\nwith a high-level discussion of the threat model and security principles before\ndelving into the low-level details of each system component. We instrument the\nGPU kernel module -- the only open-source component of the system -- and\nconduct a series of experiments to identify the security weaknesses and\npotential exploits. For certain components that are out of reach through\nexperiments, we propose well-reasoned speculations about their inner working\nmechanisms. We have responsibly reported all security findings presented in\nthis paper to the NVIDIA PSIRT Team.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02770v1", "cate": "cs.CR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "NVIDIA GPU 保密计算揭秘", "tldr": "本文旨在揭秘NVIDIA GPU保密计算（GPU-CC）系统，解决其专有性和透明度不足的问题。作者通过整合零散信息，并对唯一开源组件——GPU内核模块进行实验，识别了潜在的安全弱点和漏洞。", "motivation": "NVIDIA GPU保密计算（GPU-CC）系统虽然对用户而言易于采用，但其底层的专有系统复杂且缺乏透明度，这给安全研究人员深入理解其架构和运行机制带来了显著挑战。本文旨在解决这种透明度不足的问题。", "method": "作者通过整合来自各种来源的零散不完整信息来揭秘NVIDIA GPU-CC系统。研究首先讨论了威胁模型和安全原则，然后深入探讨了每个系统组件的低级细节。作者对GPU内核模块（系统唯一的开源组件）进行了检测，并进行了一系列实验以识别安全弱点和潜在漏洞。对于无法通过实验触及的组件，作者提出了合理的推测。", "result": "本文识别了NVIDIA GPU-CC系统中的安全弱点和潜在漏洞。所有发现的安全问题都已负责任地报告给NVIDIA PSIRT团队。", "conclusion": "本文通过整合零散信息、实验分析和合理推测，成功揭示了NVIDIA GPU保密计算（GPU-CC）系统的部分内部机制和潜在安全弱点，克服了其专有性和透明度不足带来的挑战，为安全研究提供了宝贵见解。", "translation": "GPU保密计算（GPU-CC）作为NVIDIA Hopper架构的一部分被引入，将信任边界扩展到传统的基于CPU的保密计算之外。这项创新使GPU能够安全地处理AI工作负载，为处理敏感数据提供了一个健壮高效的解决方案。对于终端用户而言，向GPU-CC模式的过渡是无缝的，无需修改现有AI应用程序。然而，这种易于采用的特点与底层专有系统的复杂性形成鲜明对比。透明度的缺乏给寻求深入了解GPU-CC架构和运行机制的安全研究人员带来了巨大挑战。\n分析NVIDIA GPU-CC系统的挑战源于详细规范的稀缺、生态系统的专有性质以及产品设计的复杂性。在本文中，我们旨在通过整合从各种来源披露的零散不完整信息来揭秘NVIDIA GPU-CC系统的实现。我们的调查从对威胁模型和安全原则的高层次讨论开始，然后深入探讨每个系统组件的低级细节。我们对GPU内核模块——系统唯一的开源组件——进行了检测，并进行了一系列实验以识别安全弱点和潜在漏洞。对于通过实验无法触及的某些组件，我们提出了关于其内部工作机制的合理推测。我们已将本文中提出的所有安全发现负责任地报告给NVIDIA PSIRT团队。", "summary": "本文旨在解决NVIDIA GPU保密计算（GPU-CC）系统缺乏透明度和专有性的问题。尽管GPU-CC为用户提供了无缝体验，但其底层复杂性阻碍了安全研究。作者通过整合零散的公开信息，讨论威胁模型，并对唯一的开源组件——GPU内核模块进行实验，成功揭示了GPU-CC的内部机制，并识别了潜在的安全弱点和漏洞。所有发现已负责任地报告给NVIDIA。", "keywords": "GPU保密计算, NVIDIA Hopper, 安全研究, 专有系统, 漏洞分析", "comments": "本文具有重要意义，因为它解决了专有保密计算系统（特别是对敏感AI工作负载越来越重要的GPU系统）的透明度这一关键问题。其通过整合零散信息并专注于唯一开源组件（GPU内核模块）来揭示漏洞的方法是创新的，为安全研究人员提供了宝贵见解，并有助于更广泛地理解GPU-CC的安全状况。负责任的披露也值得称赞。"}}
{"id": "2507.02700", "title": "Integrating path-planning and control for robotic unicycles", "authors": ["Máté B. Vizi", "Dénes Tákács", "Gábor Stépán", "Gábor Orosz"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02700v1", "summary": "This article focuses on integrating path-planning and control with\nspecializing on the unique needs of robotic unicycles. A unicycle design is\npresented which is capable of accelerating/breaking and carrying out a variety\nof maneuvers. The proposed path-planning method segments the path into straight\nand curved path sections dedicated for accelerating/breaking and turning\nmaneuvers, respectively. The curvature profiles of the curved sections are\noptimized while considering the control performance and the slipping limits of\nthe wheel. The performance of the proposed integrated approach is demonstrated\nvia numerical simulations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02700v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "机器人独轮车的路径规划与控制集成", "tldr": "本文提出了一种针对机器人独轮车的路径规划与控制集成方法，通过优化路径曲率并考虑车轮打滑限制，实现了加速/制动和转弯等多种机动，并通过数值仿真验证了其性能。", "motivation": "该研究旨在解决机器人独轮车独特的路径规划与控制集成需求。", "method": "提出了一种独轮车设计，能够进行加速/制动和多种机动。所提出的路径规划方法将路径分割为直线段（用于加速/制动）和曲线段（用于转弯），并优化曲线段的曲率剖面，同时考虑控制性能和车轮打滑限制。", "result": "所提出的集成方法通过数值仿真验证了其性能。", "conclusion": "通过集成路径规划和控制，并优化路径曲率，可以有效地实现机器人独轮车的多种机动，并在考虑控制性能和车轮打滑限制下表现良好。", "translation": "本文重点关注机器人独轮车的路径规划与控制集成，特别是其独特需求。文章提出了一种独轮车设计，该设计能够进行加速/制动并执行各种机动。所提出的路径规划方法将路径分割成直线和曲线路段，分别用于加速/制动和转弯机动。在考虑控制性能和车轮打滑限制的情况下，对曲线路段的曲率剖面进行了优化。所提出的集成方法的性能通过数值仿真得到了验证。", "summary": "本文提出了一种针对机器人独轮车的路径规划与控制集成方法。该方法设计了一种能够执行加速、制动和多种机动的独轮车，并通过将路径划分为直线和曲线段来规划路径。曲线段的曲率剖面经过优化，以兼顾控制性能和车轮打滑限制。数值仿真结果验证了该集成方法的有效性。", "keywords": "机器人独轮车, 路径规划, 控制, 曲率优化, 数值仿真", "comments": "该研究的创新点在于将路径规划与控制紧密结合，并针对独轮车这一特殊机器人形式进行了定制化设计和优化，特别是在考虑车轮打滑限制下的曲率优化，这对于独轮车的稳定性和性能至关重要。"}}
{"id": "2507.02745", "title": "Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory Apologies from LLM Chatbots", "authors": ["Zahra Ashktorab", "Alessandra Buccella", "Jason D'Cruz", "Zoe Fowler", "Andrew Gill", "Kei Yan Leung", "P. D. Magnus", "John Richards", "Kush R. Varshney"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02745v1", "summary": "As chatbots driven by large language models (LLMs) are increasingly deployed\nin everyday contexts, their ability to recover from errors through effective\napologies is critical to maintaining user trust and satisfaction. In a\npreregistered study with Prolific workers (N=162), we examine user preferences\nfor three types of apologies (rote, explanatory, and empathic) issued in\nresponse to three categories of common LLM mistakes (bias, unfounded\nfabrication, and factual errors). We designed a pairwise experiment in which\nparticipants evaluated chatbot responses consisting of an initial error, a\nsubsequent apology, and a resolution. Explanatory apologies were generally\npreferred, but this varied by context and user. In the bias scenario, empathic\napologies were favored for acknowledging emotional impact, while\nhallucinations, though seen as serious, elicited no clear preference,\nreflecting user uncertainty. Our findings show the complexity of effective\napology in AI systems. We discuss key insights such as personalization and\ncalibration that future systems must navigate to meaningfully repair trust.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02745v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "谁在道歉：用户对LLM聊天机器人中死记硬背式、共情式和解释性道歉的偏好", "tldr": "研究用户对LLM聊天机器人不同类型道歉（死记硬背式、解释性、共情式）的偏好，发现解释性道歉普遍受青睐，但在偏见情境中共情式道歉更受欢迎。", "motivation": "随着LLM聊天机器人在日常生活中部署，它们通过有效道歉从错误中恢复的能力对于维持用户信任和满意度至关重要。", "method": "一项预注册研究，N=162名参与者评估了聊天机器人在三种常见错误（偏见、无根据的捏造、事实错误）后发出的三种类型道歉（死记硬背式、解释性、共情式）。实验采用配对比较设计，参与者评估包含初始错误、后续道歉和解决方案的聊天机器人响应。", "result": "解释性道歉普遍受到青睐，但这因情境和用户而异。在偏见情境中，共情式道歉因承认情感影响而更受青睐。对于幻觉错误，尽管被认为是严重的，但没有明确的偏好，反映了用户的不确定性。", "conclusion": "研究结果表明AI系统中有效道歉的复杂性。未来的系统需要关注个性化和校准以有效修复信任。", "translation": "随着由大型语言模型（LLMs）驱动的聊天机器人越来越多地部署在日常环境中，它们通过有效道歉从错误中恢复的能力对于维持用户信任和满意度至关重要。在一项针对Prolic工人（N=162）的预注册研究中，我们调查了用户对三种类型道歉（死记硬背式、解释性、共情式）的偏好，这些道歉是针对三类常见LLM错误（偏见、无根据的捏造和事实错误）发出的。我们设计了一个配对实验，参与者评估了由初始错误、随后的道歉和解决方案组成的聊天机器人响应。解释性道歉普遍受到青睐，但这因情境和用户而异。在偏见情境中，共情式道歉因承认情感影响而更受青睐，而幻觉错误，尽管被视为严重，但没有引起明确的偏好，反映了用户的不确定性。我们的研究结果显示了AI系统中有效道歉的复杂性。我们讨论了未来系统必须驾驭的关键见解，例如个性化和校准，以有意义地修复信任。", "summary": "本研究调查了用户对大型语言模型（LLMs）聊天机器人不同类型道歉的偏好，以期在日常情境中通过有效道歉恢复用户信任和满意度。通过一项包含162名参与者的配对实验，研究人员测试了死记硬背式、解释性、共情式三种道歉类型在面对偏见、无根据捏造和事实错误时的效果。结果显示，解释性道歉普遍更受欢迎，但在偏见情境中共情式道歉因其情感承认而更受青睐。幻觉错误则未显示明确偏好。研究强调了AI道歉的复杂性，并提出个性化和校准是未来系统修复信任的关键。", "keywords": "LLM聊天机器人, 道歉策略, 用户偏好, 信任修复, 人机交互", "comments": "这项研究创新性地探讨了LLM聊天机器人道歉的复杂性，揭示了不同道歉类型在不同错误情境下对用户偏好的影响。其重要性在于为未来AI系统设计更有效、更能修复用户信任的道歉机制提供了实证基础和关键见解，特别是强调了情境敏感性和个性化的必要性。"}}
{"id": "2507.02582", "title": "Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms", "authors": ["Junli Jiang", "Pavel Naumov"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02582v1", "summary": "Responsibility has long been a subject of study in law and philosophy. More\nrecently, it became a focus of AI literature. The article investigates the\ncomputational complexity of two important properties of responsibility in\ncollective decision-making: diffusion and gap. It shows that the sets of\ndiffusion-free and gap-free decision-making mechanisms are $\\Pi_2$-complete and\n$\\Pi_3$-complete, respectively. At the same time, the intersection of these\nclasses is $\\Pi_2$-complete.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02582v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "序列决策机制中的责任差距与扩散", "tldr": "本文探讨了集体决策中责任扩散和责任差距的计算复杂性，发现无扩散和无差距机制的集合分别是$\\Pi_2$-完全和$\\Pi_3$-完全的，且两者的交集是$\\Pi_2$-完全的。", "motivation": "责任在法律、哲学和人工智能领域都是重要的研究主题。本文旨在探究集体决策中责任的扩散和差距这两种性质的计算复杂性。", "method": "通过计算复杂性理论，分析了无扩散和无差距决策机制集合的复杂性。", "result": "无扩散决策机制的集合是$\\Pi_2$-完全的，无差距决策机制的集合是$\\Pi_3$-完全的，且两者的交集是$\\Pi_2$-完全的。", "conclusion": "集体决策中责任的扩散和差距具有较高的计算复杂性，这表明设计同时满足这些性质的机制是计算困难的。", "translation": "责任长期以来一直是法律和哲学研究的主题。最近，它成为人工智能文献的焦点。本文研究了集体决策中责任的两个重要属性：扩散和差距的计算复杂性。结果表明，无扩散和无差距决策机制的集合分别是$\\Pi_2$-完全和$\\Pi_3$-完全的。同时，这些类别的交集是$\\Pi_2$-完全的。", "summary": "本文深入探讨了集体决策中责任的扩散和差距这两个关键属性的计算复杂性。研究发现，无扩散机制集合是$\\Pi_2$-完全的，无差距机制集合是$\\Pi_3$-完全的，而它们的交集也是$\\Pi_2$-完全的。这些结果揭示了在设计旨在确保责任明确的决策机制时所面临的固有计算挑战。", "keywords": "责任，集体决策，计算复杂性，责任扩散，责任差距", "comments": "这篇论文通过计算复杂性分析，为理解集体决策中责任分配的挑战提供了理论基础。其创新点在于将法律和哲学中的责任概念引入到AI决策机制的复杂性分析中，为未来设计更透明、可归责的AI系统提供了重要的理论洞察。"}}
{"id": "2507.02299", "title": "DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation", "authors": ["Yunhan Yang", "Shuo Chen", "Yukun Huang", "Xiaoyang Wu", "Yuan-Chen Guo", "Edmund Y. Lam", "Hengshuang Zhao", "Tong He", "Xihui Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by TPAMI, extension of CVPR 2024 paper DreamComposer", "url": "http://arxiv.org/abs/2507.02299v1", "summary": "Recent advancements in leveraging pre-trained 2D diffusion models achieve the\ngeneration of high-quality novel views from a single in-the-wild image.\nHowever, existing works face challenges in producing controllable novel views\ndue to the lack of information from multiple views. In this paper, we present\nDreamComposer++, a flexible and scalable framework designed to improve current\nview-aware diffusion models by incorporating multi-view conditions.\nSpecifically, DreamComposer++ utilizes a view-aware 3D lifting module to\nextract 3D representations of an object from various views. These\nrepresentations are then aggregated and rendered into the latent features of\ntarget view through the multi-view feature fusion module. Finally, the obtained\nfeatures of target view are integrated into pre-trained image or video\ndiffusion models for novel view synthesis. Experimental results demonstrate\nthat DreamComposer++ seamlessly integrates with cutting-edge view-aware\ndiffusion models and enhances their abilities to generate controllable novel\nviews from multi-view conditions. This advancement facilitates controllable 3D\nobject reconstruction and enables a wide range of applications.", "comment": "Accepted by TPAMI, extension of CVPR 2024 paper DreamComposer", "pdf_url": "http://arxiv.org/pdf/2507.02299v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DreamComposer++: 赋予扩散模型多视角条件以生成3D内容", "tldr": "DreamComposer++通过引入多视角条件，显著提升了现有视角感知扩散模型生成可控新视角的能力，从而促进可控3D内容生成。", "motivation": "现有利用2D扩散模型生成高质量新视角的方法，由于缺乏多视角信息，在生成可控新视角方面面临挑战。", "method": "DreamComposer++框架通过以下步骤工作：1. 利用视角感知3D提升模块从多视角中提取对象的3D表示。2. 通过多视角特征融合模块将这些表示聚合并渲染到目标视角的潜在特征中。3. 将获得的目标视角特征集成到预训练的图像或视频扩散模型中进行新视角合成。", "result": "实验结果表明，DreamComposer++可以无缝集成到前沿的视角感知扩散模型中，并增强它们从多视角条件生成可控新视角的能力。", "conclusion": "DreamComposer++的提出促进了可控3D对象重建，并支持广泛的应用。", "translation": "最近利用预训练2D扩散模型在从单张野外图像生成高质量新视角方面取得了进展。然而，现有工作由于缺乏多视角信息，在生成可控新视角方面面临挑战。在本文中，我们提出了DreamComposer++，一个灵活且可扩展的框架，旨在通过整合多视角条件来改进当前的视角感知扩散模型。具体而言，DreamComposer++利用一个视角感知3D提升模块从不同视角提取对象的3D表示。然后，这些表示通过多视角特征融合模块聚合并渲染到目标视角的潜在特征中。最后，获得的目标视角特征被集成到预训练的图像或视频扩散模型中进行新视角合成。实验结果表明，DreamComposer++可以无缝集成到尖端的视角感知扩散模型中，并增强它们从多视角条件生成可控新视角的能力。这一进步促进了可控3D对象重建并支持广泛的应用。", "summary": "DreamComposer++是一个旨在解决现有2D扩散模型在生成可控新视角时缺乏多视角信息问题的框架。它通过视角感知3D提升模块提取3D表示，并通过多视角特征融合模块将其融入目标视角特征，最终与预训练扩散模型结合进行新视角合成。实验证明，DreamComposer++能有效增强现有视角感知扩散模型生成可控新视角的能力，从而促进可控3D内容生成和相关应用。", "keywords": "扩散模型, 多视角条件, 3D内容生成, 新视角合成, 可控重建", "comments": "DreamComposer++的创新之处在于其通过引入多视角条件来增强扩散模型生成可控3D内容的能力，解决了现有方法在控制性方面的局限。其模块化的设计使其能够与现有先进的视角感知扩散模型无缝集成，具有良好的可扩展性和应用前景，特别是在可控3D重建领域。"}}
{"id": "2507.02412", "title": "Green Ammonia: A Techno-Economic Supply Chain Optimization", "authors": ["Lucien Genge", "Felix Müsgens"], "categories": ["econ.GN", "cs.SY", "eess.SY", "q-fin.EC"], "primary_category": "Subjects:       General Economics (econ.GN)", "pdf_link": null, "comments": "Comments:      GitHub incl. data and results is linked in the text", "url": "http://arxiv.org/abs/2507.02412v1", "summary": "Green ammonia is emerging as a strategic intermediary within green energy\nsupply chains, serving effectively as both an industrial commodity and hydrogen\ncarrier. This study provides a techno-economic analysis of green ammonia supply\nchains, comparing cost-effective pathways from global production to European\nconsumers, and evaluates ammonia alongside alternative hydrogen carriers.\nGaseous hydrogen consistently remains the most economical import option for\nEurope, though ammonia holds a narrowing cost advantage over liquid hydrogen\n(from 16 % in 2030 to 10 % by 2040). Competitive ammonia suppliers, notably\nMorocco, the United States, and the United Arab Emirates, benefit from low\nrenewable energy costs, with significant price reductions expected by 2040,\ndriven by falling costs for electricity, electrolysers, and conversion\ntechnologies. Optimal transport modes vary by consumer demand and distance:\ntrucks are ideal for low demands at all distances, rail for medium ranges, and\npipelines for high-demand scenarios. By 2040, ammonia will primarily serve\ndirect-use applications, as hydrogen consumers increasingly shift to direct\nhydrogen supplies. Policymakers should prioritize pipeline infrastructure for\nhydrogen distribution, cautiously invest in ammonia's short- to medium-term\ninfrastructure advantages, and limit long-term reliance on ammonia as a\nhydrogen carrier to mitigate stranded asset risks.", "comment": "GitHub incl. data and results is linked in the text", "pdf_url": "http://arxiv.org/pdf/2507.02412v1", "cate": "econ.GN", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "绿色氨：技术经济供应链优化", "tldr": "研究分析了绿色氨供应链的技术经济性，比较了全球到欧洲的成本效益路径，并评估了其作为氢载体的竞争力，为政策制定者提供了基础设施投资和风险管理的建议。", "motivation": "绿色氨正成为绿色能源供应链中的战略中间体，既是工业商品又是氢载体。本研究旨在对其供应链进行技术经济分析，比较全球生产到欧洲消费者的成本效益路径，并评估其与替代氢载体的竞争力。", "method": "本研究对绿色氨供应链进行了技术经济分析，比较了从全球生产到欧洲消费者的成本效益路径，并评估了氨与替代氢载体。分析考虑了不同供应商的成本优势、未来技术成本下降趋势以及不同运输模式（卡车、铁路、管道）对成本和效率的影响。", "result": "气态氢始终是欧洲最经济的进口选择；氨对液态氢的成本优势正在缩小（2030年为16%，2040年为10%）；摩洛哥、美国和阿联酋等地的氨供应商因低可再生能源成本而具有竞争力，预计到2040年成本将大幅下降；最佳运输模式取决于需求和距离，卡车适用于低需求，铁路适用于中等距离，管道适用于高需求；到2040年，氨将主要用于直接用途。", "conclusion": "政策制定者应优先发展氢气管道基础设施，谨慎投资氨的短期到中期基础设施优势，并限制长期依赖氨作为氢载体，以减轻搁浅资产风险。", "translation": "绿色氨正在成为绿色能源供应链中的战略中间体，有效地作为工业商品和氢载体。本研究对绿色氨供应链进行了技术经济分析，比较了从全球生产到欧洲消费者的成本效益路径，并评估了氨与替代氢载体。气态氢始终是欧洲最经济的进口选择，尽管氨对液态氢的成本优势正在缩小（从2030年的16%到2040年的10%）。具有竞争力的氨供应商，特别是摩洛哥、美国和阿拉伯联合酋长国，受益于低可再生能源成本，预计到2040年，在电力、电解槽和转化技术成本下降的推动下，价格将大幅降低。最佳运输模式因消费者需求和距离而异：卡车适用于所有距离的低需求，铁路适用于中等范围，管道适用于高需求情景。到2040年，氨将主要用于直接使用，因为氢消费者将越来越多地转向直接氢供应。政策制定者应优先发展氢气管道基础设施，谨慎投资氨的短期到中期基础设施优势，并限制长期依赖氨作为氢载体，以减轻搁浅资产风险。", "summary": "本研究对绿色氨供应链进行了深入的技术经济分析，评估了其作为工业商品和氢载体从全球生产到欧洲消费的成本效益路径。研究发现，尽管气态氢仍是欧洲最经济的进口选项，但绿色氨对液态氢的成本优势正在缩小，并且在低可再生能源成本区域（如摩洛哥、美国、阿联酋）具有显著的成本竞争力，预计未来成本将进一步降低。运输模式的选择应根据需求和距离优化。研究建议，未来氨将主要服务于直接用途，并强调政策制定者应优先发展氢气管道，谨慎投资氨的短期基础设施，以规避长期搁浅资产风险。", "keywords": "绿色氨, 供应链优化, 技术经济分析, 氢载体, 能源转型", "comments": "这篇论文提供了一个全面的技术经济分析，对于理解绿色氨在未来能源转型中的作用及其在氢供应链中的定位具有重要价值。其创新之处在于对不同氢载体进行了详细的成本比较，并考虑了全球供应链和运输模式的优化。研究结果对政策制定者具有直接的指导意义，尤其是在能源基础设施投资和风险管理方面，有助于制定更明智的能源转型策略。"}}
{"id": "2507.02169", "title": "Statistical Inference for Responsiveness Verification", "authors": ["Seung Hyun Cheon", "Meredith Stewart", "Bogdan Kulynych", "Tsui-Wei Weng", "Berk Ustun"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02169v1", "summary": "Many safety failures in machine learning arise when models are used to assign\npredictions to people (often in settings like lending, hiring, or content\nmoderation) without accounting for how individuals can change their inputs. In\nthis work, we introduce a formal validation procedure for the responsiveness of\npredictions with respect to interventions on their features. Our procedure\nframes responsiveness as a type of sensitivity analysis in which practitioners\ncontrol a set of changes by specifying constraints over interventions and\ndistributions over downstream effects. We describe how to estimate\nresponsiveness for the predictions of any model and any dataset using only\nblack-box access, and how to use these estimates to support tasks such as\nfalsification and failure probability estimation. We develop algorithms that\nconstruct these estimates by generating a uniform sample of reachable points,\nand demonstrate how they can promote safety in real-world applications such as\nrecidivism prediction, organ transplant prioritization, and content moderation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02169v1", "cate": "cs.LG", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "响应性验证的统计推断", "tldr": "本文提出了一种形式化的验证程序，用于在黑盒访问下评估机器学习模型预测的响应性，以提高模型安全性。", "motivation": "机器学习模型在为人分配预测时，未能考虑个体输入变化，从而导致安全故障。", "method": "本文引入了一种形式化的预测响应性验证程序。该程序将响应性视为一种敏感性分析，通过指定干预约束和下游效应分布来控制变化。它描述了如何仅通过黑盒访问来估计任何模型和数据集的响应性，并利用这些估计支持证伪和故障概率估计等任务。开发了通过生成可达点的均匀样本来构建这些估计的算法。", "result": "演示了其方法如何在累犯预测、器官移植优先级和内容审核等现实世界应用中提高安全性。", "conclusion": "提出的方法通过验证预测响应性，能够有效提高机器学习模型在现实应用中的安全性。", "translation": "机器学习中许多安全故障的发生，是由于模型在为人（通常在贷款、招聘或内容审核等场景）分配预测时，未能考虑个体如何改变其输入。在这项工作中，我们引入了一种形式化的验证程序，用于验证预测对其特征干预的响应性。我们的程序将响应性视为一种敏感性分析的一种类型，其中实践者通过指定干预约束和下游效应分布来控制一系列变化。我们描述了如何仅通过黑盒访问来估计任何模型和任何数据集的预测响应性，以及如何使用这些估计来支持证伪和故障概率估计等任务。我们开发了通过生成可达点的均匀样本来构建这些估计的算法，并展示了它们如何在累犯预测、器官移植优先级和内容审核等现实世界应用中提高安全性。", "summary": "本文提出了一种针对机器学习模型预测响应性的形式化验证程序，旨在解决模型在未考虑个体输入变化时可能导致的安全故障。该程序将响应性视为一种敏感性分析，并允许通过黑盒访问对任何模型和数据集进行响应性估计。通过生成可达点的均匀样本来构建估计，该方法被证明能在累犯预测、器官移植优先级和内容审核等实际应用中有效提升模型安全性。", "keywords": "机器学习安全, 响应性, 敏感性分析, 黑盒访问, 统计推断", "comments": "该论文提出了一种新颖的形式化验证方法来解决机器学习模型在现实应用中的安全问题，特别是当个体输入可能发生变化时。其创新点在于将响应性视为敏感性分析，并允许在黑盒环境下进行评估，这对于保护用户在面对模型决策时的可操作性至关重要。该方法通过统计推断和均匀采样可达点，提供了一种量化和验证模型公平性及鲁棒性的途径，对于负责任的AI发展具有重要意义。"}}
{"id": "2507.02694", "title": "Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers", "authors": ["Zhijian Xu", "Yilun Zhao", "Manasi Patwardhan", "Lovekesh Vig", "Arman Cohan"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02694v1", "summary": "Peer review is fundamental to scientific research, but the growing volume of\npublications has intensified the challenges of this expertise-intensive\nprocess. While LLMs show promise in various scientific tasks, their potential\nto assist with peer review, particularly in identifying paper limitations,\nremains understudied. We first present a comprehensive taxonomy of limitation\ntypes in scientific research, with a focus on AI. Guided by this taxonomy, for\nstudying limitations, we present LimitGen, the first comprehensive benchmark\nfor evaluating LLMs' capability to support early-stage feedback and complement\nhuman peer review. Our benchmark consists of two subsets: LimitGen-Syn, a\nsynthetic dataset carefully created through controlled perturbations of\nhigh-quality papers, and LimitGen-Human, a collection of real human-written\nlimitations. To improve the ability of LLM systems to identify limitations, we\naugment them with literature retrieval, which is essential for grounding\nidentifying limitations in prior scientific findings. Our approach enhances the\ncapabilities of LLM systems to generate limitations in research papers,\nenabling them to provide more concrete and constructive feedback.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02694v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "大型语言模型能否识别科学研究中的关键局限性？对人工智能研究论文的系统评估", "tldr": "本文提出了一种识别科学研究中局限性的分类方法和名为LimitGen的基准测试，以评估大型语言模型（LLMs）在这方面的能力，并通过文献检索增强了LLMs的性能。", "motivation": "解决同行评审因论文量增长而面临的挑战，探索LLMs在协助识别科学论文局限性方面的潜力，因为这方面研究不足。", "method": "1. 提出了一个针对AI研究的科学研究局限性类型的综合分类法。2. 基于此分类法，构建了首个全面评估LLMs识别局限性能力的基准测试LimitGen，包含LimitGen-Syn（合成数据集）和LimitGen-Human（真实人类编写的局限性）。3. 通过文献检索增强LLM系统，以提高其识别局限性的能力。", "result": "该方法增强了LLM系统在研究论文中生成局限性的能力，使其能够提供更具体和建设性的反馈。", "conclusion": "LLMs在经过特定方法增强后，能够有效识别科学研究中的局限性，并有望辅助人类同行评审。", "translation": "同行评审是科学研究的基础，但日益增长的出版物数量加剧了这一专业密集型过程的挑战。虽然大型语言模型（LLMs）在各种科学任务中展现出前景，但它们在协助同行评审，特别是在识别论文局限性方面的潜力仍未得到充分研究。我们首先提出了一个科学研究中局限性类型的综合分类法，重点关注人工智能领域。在此分类法的指导下，为了研究局限性，我们提出了LimitGen，这是第一个用于评估LLM支持早期反馈和补充人类同行评审能力的综合基准。我们的基准包含两个子集：LimitGen-Syn，一个通过对高质量论文进行受控扰动精心创建的合成数据集；以及LimitGen-Human，一个真实人类编写的局限性集合。为了提高LLM系统识别局限性的能力，我们通过文献检索对其进行了增强，这对于将识别局限性建立在先前的科学发现之上至关重要。我们的方法增强了LLM系统在研究论文中生成局限性的能力，使其能够提供更具体和建设性的反馈。", "summary": "本文旨在评估大型语言模型（LLMs）识别科学研究，特别是AI领域论文中关键局限性的能力。作者首先提出了一个全面的局限性类型分类法，并在此基础上构建了首个综合基准测试LimitGen，该基准包含合成数据（LimitGen-Syn）和真实人类编写的局限性数据（LimitGen-Human）。此外，研究通过整合文献检索来增强LLM系统，以提高其识别局限性的准确性和深度。结果表明，这种方法显著提升了LLMs生成具体和建设性研究局限性反馈的能力，有望辅助人类同行评审。", "keywords": "大型语言模型, 同行评审, 局限性识别, 基准测试, 文献检索", "comments": "这篇论文探讨了LLMs在同行评审这一关键科学流程中的应用潜力，特别是在识别论文局限性方面。其创新之处在于提出了一个系统的局限性分类法和首个专门的基准测试LimitGen，为未来LLMs辅助同行评审的研究奠定了基础。通过结合文献检索，该方法增强了LLMs提供高质量反馈的能力，这对于缓解当前同行评审压力具有重要意义。"}}
{"id": "2507.01615", "title": "EDGChain-E: A Decentralized Git-Based Framework for Versioning Encrypted Energy Data", "authors": ["Alper Alimoglu", "Kamil Erdayandi", "Mustafa A. Mustafa", "Ümit Cali"], "categories": ["cs.DC", "cs.CR"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01615v1", "summary": "This paper proposes a new decentralized framework, named EDGChain-E\n(Encrypted-Data-Git Chain for Energy), designed to manage version-controlled,\nencrypted energy data using blockchain and the InterPlanetary File System. The\nframework incorporates a Decentralized Autonomous Organization (DAO) to\norchestrate collaborative data governance across the lifecycle of energy\nresearch and operations, such as smart grid monitoring, demand forecasting, and\npeer-to-peer energy trading. In EDGChain-E, initial commits capture the full\nencrypted datasets-such as smart meter readings or grid telemetry-while\nsubsequent updates are tracked as encrypted Git patches, ensuring integrity,\ntraceability, and privacy. This versioning mechanism supports secure\ncollaboration across multiple stakeholders (e.g., utilities, researchers,\nregulators) without compromising sensitive or regulated information. We\nhighlight the framework's capability to maintain FAIR-compliant (Findable,\nAccessible, Interoperable, Reusable) provenance of encrypted data. By embedding\nhash-based content identifiers in Merkle trees, the system enables transparent,\nauditable, and immutable tracking of data changes, thereby supporting\nreproducibility and trust in decentralized energy applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01615v1", "cate": "cs.DC", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "EDGChain-E：一个用于加密能源数据版本控制的去中心化基于Git的框架", "tldr": "EDGChain-E是一个去中心化框架，利用区块链和IPFS，通过Git补丁管理加密能源数据的版本控制，确保数据完整性、可追溯性和隐私，并支持安全协作和可审计的数据变更追踪。", "motivation": "该论文旨在提出一个去中心化框架，用于管理版本控制的加密能源数据，以解决能源研究和操作（如智能电网监控、需求预测和点对点能源交易）中数据完整性、可追溯性和隐私的需求，并支持多方利益相关者之间的安全协作。", "method": "EDGChain-E框架结合了区块链和星际文件系统（IPFS）来管理版本控制的加密能源数据。它采用去中心化自治组织（DAO）来协调数据治理。初始提交捕获完整的加密数据集，而后续更新则作为加密的Git补丁进行追踪。系统通过在默克尔树中嵌入基于哈希的内容标识符，实现数据变更的透明、可审计和不可变追踪。", "result": "EDGChain-E框架确保了加密能源数据的完整性、可追溯性和隐私。它支持多方利益相关者（如公用事业公司、研究人员、监管机构）之间的安全协作，且不损害敏感或受管制的信息。该框架还能够维护符合FAIR原则（可查找、可访问、可互操作、可重用）的加密数据溯源。", "conclusion": "EDGChain-E通过实现数据变更的透明、可审计和不可变追踪，支持去中心化能源应用中的可再现性和信任。", "translation": "本文提出了一个名为EDGChain-E（加密数据Git链能源）的新型去中心化框架，旨在利用区块链和星际文件系统（IPFS）管理版本控制的加密能源数据。该框架整合了一个去中心化自治组织（DAO），以协调能源研究和运营（如智能电网监控、需求预测和点对点能源交易）整个生命周期中的协作数据治理。在EDGChain-E中，初始提交捕获完整的加密数据集——例如智能电表读数或电网遥测数据——而后续更新则作为加密的Git补丁进行追踪，确保了完整性、可追溯性和隐私。这种版本控制机制支持多个利益相关者（例如公用事业公司、研究人员、监管机构）之间的安全协作，而不会泄露敏感或受管制的信息。我们强调了该框架维护符合FAIR原则（可查找、可访问、可互操作、可重用）的加密数据溯源的能力。通过在默克尔树中嵌入基于哈希的内容标识符，该系统实现了数据变更的透明、可审计和不可变追踪，从而支持去中心化能源应用中的可再现性和信任。", "summary": "EDGChain-E是一个创新的去中心化框架，旨在利用区块链和IPFS管理加密能源数据的版本控制。它通过DAO实现协作治理，并使用加密的Git补丁来追踪数据更新，确保完整性、可追溯性和隐私。该框架支持多利益相关者之间的安全协作，维护FAIR合规的溯源，并通过哈希内容标识符和默克尔树实现数据变更的透明和可审计追踪，从而增强去中心化能源应用的可再现性和信任。", "keywords": "去中心化框架, 加密能源数据, 版本控制, 区块链, Git补丁", "comments": "该论文提出了一种结合Git版本控制、区块链和IPFS来管理加密能源数据的创新方法。其核心创新在于将Git的增量版本控制思想应用于加密数据，并利用区块链和IPFS的去中心化特性，为敏感能源数据提供高度的完整性、可追溯性和隐私保护。通过集成DAO，该框架还解决了多方协作中的治理问题。这种方法对于智能电网监控、需求预测和P2P能源交易等需要高安全性、可审计性和协作性的应用场景具有重要意义。"}}
{"id": "2507.02708", "title": "Optimizing Start Locations in Ergodic Search for Disaster Response", "authors": ["Ananya Rao", "Alyssa Hargis", "David Wettergreen", "Howie Choset"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02708v1", "summary": "In disaster response scenarios, deploying robotic teams effectively is\ncrucial for improving situational awareness and enhancing search and rescue\noperations. The use of robots in search and rescue has been studied but the\nquestion of where to start robot deployments has not been addressed. This work\naddresses the problem of optimally selecting starting locations for robots with\nheterogeneous capabilities by formulating a joint optimization problem. To\ndetermine start locations, this work adds a constraint to the ergodic\noptimization framework whose minimum assigns robots to start locations. This\nbecomes a little more challenging when the robots are heterogeneous (equipped\nwith different sensing and motion modalities) because not all robots start at\nthe same location, and a more complex adaptation of the aforementioned\nconstraint is applied. Our method assumes access to potential starting\nlocations, which can be obtained from expert knowledge or aerial imagery. We\nexperimentally evaluate the efficacy of our joint optimization approach by\ncomparing it to baseline methods that use fixed starting locations for all\nrobots. Our experimental results show significant gains in coverage\nperformance, with average improvements of 35.98% on synthetic data and 31.91%\non real-world data for homogeneous and heterogeneous teams, in terms of the\nergodic metric.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02708v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "优化灾害响应中遍历搜索的起始位置", "tldr": "该论文通过制定联合优化问题，为灾害响应中的异构机器人团队优化起始位置，以提高搜索覆盖率，并在遍历搜索框架下实现了显著的性能提升。", "motivation": "在灾害响应场景中，有效部署机器人团队对于提高态势感知和加强搜救行动至关重要。尽管机器人用于搜救的研究已经开展，但机器人部署的起始位置问题尚未得到解决。本研究旨在解决异构机器人团队最佳起始位置的选择问题。", "method": "本研究通过制定一个联合优化问题，解决了最优选择具有异构能力的机器人起始位置的问题。为了确定起始位置，本研究在遍历优化框架中增加了一个约束，其最小值将机器人分配到起始位置。当机器人是异构（配备不同的传感和运动模式）时，这变得更具挑战性，因为并非所有机器人都在同一位置开始，因此应用了上述约束的更复杂适应。我们的方法假设可以获取潜在的起始位置，这些位置可以从专家知识或航空图像中获得。", "result": "实验评估表明，与使用固定起始位置的基线方法相比，我们提出的联合优化方法在覆盖性能方面取得了显著提升。在遍历度量方面，对于同质和异质团队，合成数据上的平均改进为35.98%，真实世界数据上的平均改进为31.91%。", "conclusion": "通过在遍历框架内使用所提出的联合优化方法来优化机器人起始位置，可以显著提高灾害响应场景中的覆盖性能，特别是对于异构机器人团队。", "translation": "在灾害响应场景中，有效部署机器人团队对于提高态势感知和加强搜救行动至关重要。机器人用于搜救的研究已经开展，但机器人部署的起始位置问题尚未得到解决。本工作通过制定一个联合优化问题，解决了最优选择具有异构能力的机器人起始位置的问题。为了确定起始位置，本工作在遍历优化框架中增加了一个约束，其最小值将机器人分配到起始位置。当机器人是异构（配备不同的传感和运动模式）时，这变得更具挑战性，因为并非所有机器人都在同一位置开始，因此应用了上述约束的更复杂适应。我们的方法假设可以获取潜在的起始位置，这可以从专家知识或航空图像中获得。我们通过将我们的联合优化方法与所有机器人使用固定起始位置的基线方法进行比较，实验评估了其功效。我们的实验结果表明，在遍历度量方面，对于同质和异质团队，合成数据上的覆盖性能平均提高了35.98%，真实世界数据上的平均提高了31.91%。", "summary": "本论文解决了灾害响应中异构机器人团队优化起始位置的关键问题。通过制定一个联合优化问题，并在遍历优化框架中引入新的约束，作者实现了将机器人分配到最佳起始位置。该方法考虑了机器人的异构性，并利用了潜在的起始位置数据。实验结果表明，与固定起始位置的基线方法相比，搜索覆盖率显著提高，在合成和真实世界数据集上均实现了超过30%的增益。", "keywords": "灾害响应, 遍历搜索, 机器人部署, 优化, 异构机器人", "comments": "本研究的创新之处在于解决了在遍历搜索框架内，特别是对于异构机器人团队，之前被忽视的机器人最佳起始位置问题。这对于实际的灾害响应至关重要，因为高效的部署直接影响搜索效率和成功率。显著的性能提升验证了该方法的实用性。"}}
{"id": "2507.02800", "title": "Time-Masked Transformers with Lightweight Test-Time Adaptation for Neural Speech Decoding", "authors": ["Ebrahim Feghhi", "Shreyas Kaasyap", "Nima Hadidi", "Jonathan C. Kao"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      10 pages, 4 figures", "url": "http://arxiv.org/abs/2507.02800v1", "summary": "Speech neuroprostheses aim to restore communication for people with severe\nparalysis by decoding speech directly from neural activity. To accelerate\nalgorithmic progress, a recent benchmark released intracranial recordings from\na paralyzed participant attempting to speak, along with a baseline decoding\nalgorithm. Prior work on the benchmark showed impressive accuracy gains.\nHowever, these gains increased computational costs and were not demonstrated in\na real-time decoding setting. Here, we make three contributions that pave the\nway towards accurate, efficient, and real-time neural speech decoding. First,\nwe incorporate large amounts of time masking during training. On average, over\n$50\\%$ of each trial is masked. Second, we replace the gated recurrent unit\n(GRU) architecture used in the baseline algorithm with a compact Transformer.\nThe Transformer architecture uses $77\\%$ fewer parameters, cuts peak GPU memory\nusage by $36\\%$ relative, and is significantly faster to calibrate relative to\nthe GRU. Third, we design a lightweight variant of an existing test-time\nadaptation method developed for decoding handwriting from neural activity. Our\nvariant adapts the model using multiple time masked augmentations of a single\ntrial and requires only one gradient step per trial. Together, these\ncontributions reduce word error rate by $19.5\\%$ and effectively mitigate\nperformance degradations across held-out days in a real-time decoding setting\nwhile substantially lowering computational costs.", "comment": "10 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.02800v1", "cate": "cs.HC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "带时间掩码的Transformer与轻量级测试时自适应用于神经语音解码", "tldr": "该研究提出了一种结合时间掩码Transformer和轻量级测试时自适应的新方法，以实现准确、高效且实时的神经语音解码，显著降低了词错误率和计算成本。", "motivation": "言语神经假肢旨在通过从神经活动中解码言语来恢复重度瘫痪患者的沟通能力。然而，现有的解码算法虽然提高了准确性，但计算成本高昂且未能证明实时性能，阻碍了算法的进一步发展和实际应用。", "method": "1. 在训练过程中引入大量时间掩码，平均每个试验超过50%的部分被掩码。2. 将基线算法中的门控循环单元（GRU）架构替换为更紧凑的Transformer架构，该架构参数量减少77%，峰值GPU内存使用量减少36%，校准速度显著加快。3. 设计了一种现有测试时自适应方法的轻量级变体，该变体通过对单个试验进行多次时间掩码增强来适应模型，并且每个试验仅需一个梯度步长。", "result": "将词错误率降低了19.5%。有效缓解了实时解码设置中跨保留日期的性能下降。显著降低了计算成本（Transformer架构参数量减少77%，峰值GPU内存使用量减少36%）。", "conclusion": "这些贡献为实现准确、高效和实时的神经语音解码铺平了道路。", "translation": "言语神经假肢旨在通过直接从神经活动中解码言语来恢复重度瘫痪患者的沟通能力。为了加速算法进展，最近的一项基准测试发布了来自一名试图说话的瘫痪参与者的颅内记录，以及一个基线解码算法。先前关于该基准的工作显示出令人印象深刻的准确性提升。然而，这些提升增加了计算成本，并且未能在实时解码设置中得到验证。在此，我们提出了三项贡献，为实现准确、高效和实时的神经语音解码铺平了道路。首先，我们在训练过程中引入了大量的时域掩码。平均而言，每个试验超过50%的部分被掩码。其次，我们将基线算法中使用的门控循环单元（GRU）架构替换为紧凑型Transformer。与GRU相比，Transformer架构使用的参数减少了77%，峰值GPU内存使用量减少了36%，并且校准速度显著加快。第三，我们设计了一种现有测试时自适应方法的轻量级变体，该方法最初用于从神经活动中解码笔迹。我们的变体通过对单个试验进行多次时间掩码增强来适应模型，并且每个试验仅需一个梯度步长。总的来说，这些贡献将词错误率降低了19.5%，并在实时解码设置中有效缓解了跨保留日期的性能下降，同时显著降低了计算成本。", "summary": "该论文提出了三项关键进展，旨在实现准确、高效和实时的神经语音解码。首先，在训练中广泛应用时间掩码；其次，用更紧凑和高效的Transformer架构取代了GRU，显著减少了参数和内存使用；最后，设计了一种轻量级的测试时自适应方法。这些创新共同使词错误率降低了19.5%，有效缓解了长时间使用中的性能下降，并大幅降低了计算成本，为神经语音假肢的实际应用奠定了基础。", "keywords": "神经语音解码, 时间掩码Transformer, 测试时自适应, 神经假肢, 实时解码", "comments": "该论文在实现神经语音解码的实时应用方面取得了显著进展。通过结合架构改进（Transformer取代GRU）、数据增强技术（时间掩码）和巧妙的自适应策略（轻量级测试时自适应），有效解决了计算成本和性能稳定性等关键问题。词错误率的大幅降低和计算资源的显著节约，凸显了其对言语神经假肢领域的潜在影响和重要性。"}}
{"id": "2507.02616", "title": "DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making", "authors": ["Tianqi Shang", "Weiqing He", "Charles Zheng", "Lingyao Li", "Li Shen", "Bingxin Zhao"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2507.02616v1", "summary": "The rise of Large Language Models (LLMs) has enabled the development of\nspecialized AI agents with domain-specific reasoning and interaction\ncapabilities, particularly in healthcare. While recent frameworks simulate\nmedical decision-making, they largely focus on single-turn tasks where a doctor\nagent receives full case information upfront -- diverging from the real-world\ndiagnostic process, which is inherently uncertain, interactive, and iterative.\nIn this paper, we introduce MIMIC-Patient, a structured dataset built from the\nMIMIC-III electronic health records (EHRs), designed to support dynamic,\npatient-level simulations. Building on this, we propose DynamiCare, a novel\ndynamic multi-agent framework that models clinical diagnosis as a multi-round,\ninteractive loop, where a team of specialist agents iteratively queries the\npatient system, integrates new information, and dynamically adapts its\ncomposition and strategy. We demonstrate the feasibility and effectiveness of\nDynamiCare through extensive experiments, establishing the first benchmark for\ndynamic clinical decision-making with LLM-powered agents.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.02616v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DynamiCare：一个用于交互式和开放式医疗决策的动态多智能体框架", "tldr": "引入DynamiCare，一个动态多智能体框架，通过MIMIC-Patient数据集，模拟真实世界中交互式、多轮的医疗诊断过程，并建立了LLM驱动代理的动态临床决策基准。", "motivation": "现有医疗决策AI框架多为单轮任务，与真实世界中不确定、交互式、迭代的诊断过程不符。", "method": "1. 构建了MIMIC-Patient数据集，源自MIMIC-III电子健康记录，支持动态、患者级模拟。2. 提出了DynamiCare动态多智能体框架，将临床诊断建模为多轮交互循环，专家代理团队迭代查询患者系统，整合新信息，并动态调整其组成和策略。", "result": "通过广泛实验证明了DynamiCare的可行性和有效性，并建立了首个基于LLM代理的动态临床决策基准。", "conclusion": "DynamiCare框架及其伴随的MIMIC-Patient数据集成功地为LLM驱动的医疗AI代理提供了一个更接近真实世界、交互式和动态的临床决策模拟和评估平台。", "translation": "大型语言模型（LLMs）的兴起使得开发具有领域特定推理和交互能力的专业AI代理成为可能，尤其是在医疗保健领域。虽然最近的框架模拟了医疗决策，但它们主要侧重于单轮任务，即医生代理一次性接收所有病例信息——这与现实世界中固有的不确定、交互和迭代的诊断过程大相径庭。在本文中，我们引入了MIMIC-Patient，一个基于MIMIC-III电子健康记录（EHRs）构建的结构化数据集，旨在支持动态的患者级模拟。在此基础上，我们提出了DynamiCare，一个新颖的动态多智能体框架，它将临床诊断建模为一个多轮、交互式循环，其中专家代理团队迭代地查询患者系统，整合新信息，并动态调整其组成和策略。我们通过广泛的实验证明了DynamiCare的可行性和有效性，并为LLM驱动的代理建立了第一个动态临床决策基准。", "summary": "本文针对现有医疗AI决策框架无法模拟真实世界中不确定、交互式和迭代诊断过程的问题，提出了DynamiCare动态多智能体框架。该框架将临床诊断视为多轮交互循环，由专家代理团队动态查询患者并整合信息。为支持此框架，论文还构建了MIMIC-Patient数据集。实验证明了DynamiCare的可行性和有效性，并为LLM驱动的动态临床决策建立了基准。", "keywords": "多智能体系统, 医疗决策, 大型语言模型, 动态诊断, MIMIC-Patient", "comments": "DynamiCare通过引入多轮交互和动态适应机制，显著提升了LLM在医疗诊断模拟中的真实性，解决了传统单轮决策的局限性。MIMIC-Patient数据集的构建也为未来相关研究提供了宝贵资源。这项工作为开发更智能、更贴近实际临床工作流程的医疗AI代理奠定了基础。"}}
{"id": "2507.02307", "title": "Flow-CDNet: A Novel Network for Detecting Both Slow and Fast Changes in Bitemporal Images", "authors": ["Haoxuan Li", "Chenxu Wei", "Haodong Wang", "Xiaomeng Hu", "Boyuan An", "Lingyan Ran", "Baosen Zhang", "Jin Jin", "Omirzhan Taukebayev", "Amirkhan Temirbayev", "Junrui Liu", "Xiuwei Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      18 pages, 8 figures", "url": "http://arxiv.org/abs/2507.02307v1", "summary": "Change detection typically involves identifying regions with changes between\nbitemporal images taken at the same location. Besides significant changes, slow\nchanges in bitemporal images are also important in real-life scenarios. For\ninstance, weak changes often serve as precursors to major hazards in scenarios\nlike slopes, dams, and tailings ponds. Therefore, designing a change detection\nnetwork that simultaneously detects slow and fast changes presents a novel\nchallenge. In this paper, to address this challenge, we propose a change\ndetection network named Flow-CDNet, consisting of two branches: optical flow\nbranch and binary change detection branch. The first branch utilizes a pyramid\nstructure to extract displacement changes at multiple scales. The second one\ncombines a ResNet-based network with the optical flow branch's output to\ngenerate fast change outputs. Subsequently, to supervise and evaluate this new\nchange detection framework, a self-built change detection dataset Flow-Change,\na loss function combining binary tversky loss and L2 norm loss, along with a\nnew evaluation metric called FEPE are designed. Quantitative experiments\nconducted on Flow-Change dataset demonstrated that our approach outperforms the\nexisting methods. Furthermore, ablation experiments verified that the two\nbranches can promote each other to enhance the detection performance.", "comment": "18 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.02307v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Flow-CDNet：一种用于双时相图像中慢速和快速变化检测的新型网络", "tldr": "Flow-CDNet提出了一种双分支网络，能同时检测双时相图像中的慢速和快速变化，并构建了新数据集和评估指标，性能优于现有方法。", "motivation": "在现实场景中，双时相图像中的慢速变化与显著变化同样重要，例如，微弱变化常是重大灾害（如边坡、水坝、尾矿库）的前兆。因此，设计一个能同时检测慢速和快速变化的变化检测网络是一个新颖的挑战。", "method": "本文提出了一种名为Flow-CDNet的变化检测网络，包含光学流分支和二值变化检测分支。光学流分支利用金字塔结构提取多尺度位移变化。二值变化检测分支结合基于ResNet的网络和光学流分支的输出生成快速变化结果。此外，还设计了一个自建的变化检测数据集Flow-Change、一个结合二值tversky损失和L2范数损失的损失函数，以及一个新的评估指标FEPE。", "result": "在Flow-Change数据集上进行的定量实验表明，我们的方法优于现有方法。此外，消融实验验证了两个分支可以相互促进，从而提高检测性能。", "conclusion": "Flow-CDNet通过其独特的双分支结构，成功解决了同时检测双时相图像中慢速和快速变化的挑战，并在性能上超越了现有方法，证明了其有效性和优越性。", "translation": "变化检测通常涉及识别在同一位置拍摄的双时相图像之间的变化区域。除了显著变化外，双时相图像中的慢速变化在现实场景中也同样重要。例如，微弱变化常常是边坡、水坝和尾矿库等场景中重大灾害的前兆。因此，设计一个能够同时检测慢速和快速变化的变化检测网络提出了一个新颖的挑战。在本文中，为了应对这一挑战，我们提出了一种名为Flow-CDNet的变化检测网络，该网络由两个分支组成：光学流分支和二值变化检测分支。第一个分支利用金字塔结构提取多尺度位移变化。第二个分支结合了基于ResNet的网络和光学流分支的输出，以生成快速变化输出。随后，为了监督和评估这个新的变化检测框架，设计了一个自建的变化检测数据集Flow-Change、一个结合二值tversky损失和L2范数损失的损失函数，以及一个名为FEPE的新评估指标。在Flow-Change数据集上进行的定量实验表明，我们的方法优于现有方法。此外，消融实验验证了这两个分支可以相互促进，从而提高检测性能。", "summary": "本文提出了Flow-CDNet，一个用于双时相图像中慢速和快速变化检测的新型双分支网络。该网络包含一个光学流分支用于提取多尺度位移变化，以及一个二值变化检测分支结合光学流输出生成快速变化。为支持该框架，研究团队构建了Flow-Change数据集，并设计了新的损失函数和评估指标FEPE。实验结果表明，Flow-CDNet性能优于现有方法，且双分支结构能有效提升检测能力。", "keywords": "变化检测, 双时相图像, 慢速变化, 快速变化, 光学流, 深度学习", "comments": "该论文的创新点在于提出了一个双分支网络Flow-CDNet，能够同时有效地检测双时相图像中的慢速和快速变化，填补了现有方法在这方面的不足。特别值得关注的是，为了解决这一挑战，研究者还自建了数据集和新的评估指标，这对于推动该领域的发展具有重要意义。其方法结合了光学流和深度学习，显示出较强的实用性和泛化潜力。"}}
{"id": "2507.02225", "title": "Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction", "authors": ["Jiyeon Bae", "Hyeon Jeon", "Jinwook Seo"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      IEEE VIS 2025 (short paper)", "url": "http://arxiv.org/abs/2507.02225v1", "summary": "Evaluating the accuracy of dimensionality reduction (DR) projections in\npreserving the structure of high-dimensional data is crucial for reliable\nvisual analytics. Diverse evaluation metrics targeting different structural\ncharacteristics have thus been developed. However, evaluations of DR\nprojections can become biased if highly correlated metrics--those measuring\nsimilar structural characteristics--are inadvertently selected, favoring DR\ntechniques that emphasize those characteristics. To address this issue, we\npropose a novel workflow that reduces bias in the selection of evaluation\nmetrics by clustering metrics based on their empirical correlations rather than\non their intended design characteristics alone. Our workflow works by computing\nmetric similarity using pairwise correlations, clustering metrics to minimize\noverlap, and selecting a representative metric from each cluster. Quantitative\nexperiments demonstrate that our approach improves the stability of DR\nevaluation, which indicates that our workflow contributes to mitigating\nevaluation bias.", "comment": "IEEE VIS 2025 (short paper)", "pdf_url": "http://arxiv.org/pdf/2507.02225v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "度量设计 ≠ 度量行为：改进度量选择以实现降维的无偏评估", "tldr": "本文提出了一种新的工作流程，通过根据经验相关性而非设计特征对度量进行聚类，从而减少降维评估中度量选择的偏见，以提高评估的稳定性。", "motivation": "准确评估降维（DR）投影在保留高维数据结构方面的能力对于可靠的视觉分析至关重要。然而，如果无意中选择了高度相关的评估指标，则DR投影的评估可能会变得有偏见，从而偏向于强调某些特征的DR技术。本文旨在解决这一评估偏见问题。", "method": "本文提出了一种新颖的工作流程，通过根据指标的经验相关性而非其预期的设计特征来聚类指标，从而减少评估指标选择中的偏见。该工作流程通过计算成对相关性来衡量指标相似性，聚类指标以最小化重叠，并从每个聚类中选择一个代表性指标。", "result": "定量实验表明，作者提出的方法提高了降维评估的稳定性。", "conclusion": "该工作流程有助于减轻降维评估中的偏见。", "translation": "评估降维（DR）投影在保留高维数据结构方面的准确性对于可靠的视觉分析至关重要。因此，已经开发了针对不同结构特征的各种评估指标。然而，如果无意中选择了高度相关的指标——那些测量相似结构特征的指标——则DR投影的评估可能会变得有偏见，从而偏向于强调这些特征的DR技术。为了解决这个问题，我们提出了一种新颖的工作流程，通过根据指标的经验相关性而不是仅仅根据其预期的设计特征来聚类指标，从而减少评估指标选择中的偏见。我们的工作流程通过计算成对相关性来衡量指标相似性，聚类指标以最小化重叠，并从每个聚类中选择一个代表性指标。定量实验表明，我们的方法提高了DR评估的稳定性，这表明我们的工作流程有助于减轻评估偏见。", "summary": "本文针对降维（DR）技术评估中因无意中选择高度相关评估指标而产生的偏见问题，提出了一种新颖的工作流程。该方法通过根据指标的经验相关性而非其设计特征进行聚类，从而改进了指标选择。具体而言，它计算成对相关性以衡量指标相似性，聚类以最小化重叠，并从每个聚类中选择一个代表性指标。实验证明，该方法提高了DR评估的稳定性，从而有效减轻了评估偏见。", "keywords": "降维, 评估指标, 偏见消除, 指标选择, 聚类", "comments": "本文提出了一种创新且实用的方法来解决降维技术评估中的关键问题，即如何确保评估的无偏性。通过将评估指标的选择从基于设计特性转向基于经验行为（相关性），该研究提供了一个更稳健的框架，对于有效开发和比较不同的降维方法具有重要意义。这一方法有助于提高视觉分析和机器学习领域中评估的可靠性。"}}
{"id": "2507.02744", "title": "Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens", "authors": ["Peter Viechnicki"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02744v1", "summary": "A body of work over the past several decades has demonstrated that the\ncomplex and coordinated articulatory movements of human vowel production are\ngoverned (at least in part)by control mechanisms whose targets are regions of\nauditory space. Within the target region control at the sub-phonemic level has\nalso been demonstrated. But the degree of accuracy of that control is unknown.\nThe current work investigates this question by asking how far apart must two\nvowel stimuli lie in auditory space in order to yield reliably different\nimitations? This distance is termed 'Just Producible Difference' (JPD). The\ncurrent study uses a vowel mimicry paradigm to derive the first measurement of\nJPD among two sets of English speakers during front vowel production. JPD is\nestimated at between 14 and 51 mels in F1 X F2 space. This finding has\nimplications for episodic theories of speech production. It also clarifies the\npossible structures of human vowel systems, by setting a theoretical lower\nbound for how close two vowel phonemes may be in a speaker's formant space, and\nhence a psychophysical explanation of observed trends in number and patterns of\npossible vowel phonemes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02744v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过刚好可产生差异（JPD）阈值测量元音产生空间的粒度", "tldr": "本研究通过“刚好可产生差异”（JPD）阈值，量化了英语使用者元音产生控制的精确度，发现其在F1 X F2空间中介于14到51 mels之间，对言语产生理论和元音系统结构有重要意义。", "motivation": "之前的研究表明元音产生由听觉空间中的目标区域控制，并能达到亚音素级别，但这种控制的精确度尚不明确。本研究旨在回答“两个元音刺激在听觉空间中需要相距多远才能产生可靠的不同模仿？”这一问题。", "method": "本研究采用元音模仿范式，首次测量了两组英语使用者在前元音产生过程中的“刚好可产生差异”（JPD）。", "result": "JPD在F1 X F2空间中估计介于14到51 mels之间。", "conclusion": "这一发现对言语产生的偶发理论有影响。它通过设定两个元音音素在说话者共振峰空间中可能接近的理论下限，澄清了人类元音系统的可能结构，从而为观察到的元音音素数量和模式趋势提供了心理物理学解释。", "translation": "在过去的几十年里，大量研究表明，人类元音产生的复杂协调发音运动（至少部分地）受控于以听觉空间区域为目标的控制机制。在目标区域内，亚音素级别的控制也已被证实。但是，这种控制的精确度尚不清楚。当前的工作通过询问“两个元音刺激在听觉空间中必须相距多远才能产生可靠的不同模仿？”来调查这个问题。这个距离被称为“刚好可产生差异”（JPD）。当前的研究使用元音模仿范式，首次测量了两组英语使用者在前元音产生过程中的JPD。JPD在F1 X F2空间中估计介于14到51 mels之间。这一发现对言语产生的偶发理论有影响。它还通过设定两个元音音素在说话者共振峰空间中可能接近的理论下限，从而为观察到的元音音素数量和模式趋势提供了心理物理学解释，从而澄清了人类元音系统的可能结构。", "summary": "本研究旨在量化人类元音产生控制的精确度，通过引入“刚好可产生差异”（JPD）的概念，即两个元音刺激在听觉空间中需相距多远才能产生可靠的不同模仿。研究采用元音模仿范式，首次测量了英语使用者前元音产生中的JPD，结果显示其在F1 X F2空间中介于14到51 mels之间。这一发现不仅对言语产生的偶发理论具有重要意义，也为理解人类元音系统结构及其音素模式提供了理论下限和心理物理学解释。", "keywords": "元音产生, JPD, 听觉空间, 语音控制, 精确度", "comments": "这项研究通过引入JPD这一新颖的量化指标，填补了元音产生控制精确度方面的知识空白。其发现为语音学和心理声学领域提供了具体的量化数据，特别是对理解言语产生理论和元音系统演变具有重要贡献。其创新之处在于将心理物理学方法应用于语音产生控制的微观层面，揭示了人类语言系统精细控制的粒度。"}}
{"id": "2507.02414", "title": "Privacy-preserving Preselection for Face Identification Based on Packing", "authors": ["Rundong Xin", "Taotao Wang", "Jin Wang", "Chonghe Zhao", "Jing Wang"], "categories": ["cs.CV", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper has been accepted for publication in SecureComm 2025", "url": "http://arxiv.org/abs/2507.02414v1", "summary": "Face identification systems operating in the ciphertext domain have garnered\nsignificant attention due to increasing privacy concerns and the potential\nrecovery of original facial data. However, as the size of ciphertext template\nlibraries grows, the face retrieval process becomes progressively more\ntime-intensive. To address this challenge, we propose a novel and efficient\nscheme for face retrieval in the ciphertext domain, termed Privacy-Preserving\nPreselection for Face Identification Based on Packing (PFIP). PFIP incorporates\nan innovative preselection mechanism to reduce computational overhead and a\npacking module to enhance the flexibility of biometric systems during the\nenrollment stage. Extensive experiments conducted on the LFW and CASIA datasets\ndemonstrate that PFIP preserves the accuracy of the original face recognition\nmodel, achieving a 100% hit rate while retrieving 1,000 ciphertext face\ntemplates within 300 milliseconds. Compared to existing approaches, PFIP\nachieves a nearly 50x improvement in retrieval efficiency.", "comment": "This paper has been accepted for publication in SecureComm 2025", "pdf_url": "http://arxiv.org/pdf/2507.02414v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于打包的人脸识别隐私保护预筛选", "tldr": "提出了一种名为PFIP的隐私保护人脸识别预筛选方案，通过预筛选机制和打包模块，大幅提升了密文域人脸检索的效率，同时保持了识别精度。", "motivation": "密文域人脸识别系统面临着模板库规模增大导致检索时间过长的问题，这限制了其在保护隐私的同时进行高效识别的能力。", "method": "本文提出了一种新颖高效的密文域人脸检索方案——基于打包的人脸识别隐私保护预筛选（PFIP）。PFIP包含一个创新的预筛选机制来减少计算开销，以及一个打包模块来增强生物识别系统在注册阶段的灵活性。", "result": "在LFW和CASIA数据集上的大量实验表明，PFIP保持了原始人脸识别模型的准确性，实现了100%的命中率，并在300毫秒内检索了1,000个密文人脸模板。与现有方法相比，PFIP的检索效率提高了近50倍。", "conclusion": "PFIP方案有效地解决了密文域人脸识别中检索效率低下的问题，在保证隐私和识别精度的前提下，显著提升了检索速度。", "translation": "密文域人脸识别系统因日益增长的隐私问题和原始面部数据可能被恢复的风险而受到广泛关注。然而，随着密文模板库规模的增长，人脸检索过程变得越来越耗时。为了解决这一挑战，我们提出了一种新颖高效的密文域人脸检索方案，称为基于打包的人脸识别隐私保护预筛选（PFIP）。PFIP结合了创新的预筛选机制以减少计算开销，以及一个打包模块以增强生物识别系统在注册阶段的灵活性。在LFW和CASIA数据集上进行的广泛实验表明，PFIP保持了原始人脸识别模型的准确性，实现了100%的命中率，并在300毫秒内检索了1,000个密文人脸模板。与现有方法相比，PFIP的检索效率提高了近50倍。", "summary": "针对密文域人脸识别系统中大规模模板库导致的检索效率低下问题，本文提出了一种名为PFIP的隐私保护预筛选方案。PFIP通过引入创新的预筛选机制来降低计算开销，并利用打包模块提升了系统注册阶段的灵活性。实验结果表明，PFIP在保持原始模型准确性的同时，能以极高的效率（提升近50倍，1000模板300ms）完成密文人脸检索，实现了100%的命中率。", "keywords": "隐私保护, 人脸识别, 密文域, 预筛选, 打包", "comments": "该论文提出了一种解决密文域人脸识别效率瓶颈的有效方案。其创新点在于结合了预筛选和打包机制，在保证隐私和识别精度的前提下，大幅提升了检索速度，具有重要的实际应用价值。"}}
{"id": "2507.02761", "title": "Trajectory Optimization for Differential Drive Mobile Manipulators via Topological Paths Search and Arc Length-Yaw Parameterization", "authors": ["Long Xu", "Choilam Wong", "Mengke Zhang", "Junxiao Lin", "Fei Gao"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Technical Report", "url": "http://arxiv.org/abs/2507.02761v1", "summary": "We present an efficient hierarchical motion planning pipeline for\ndifferential drive mobile manipulators. Our approach first searches for\nmultiple collisionfree and topologically distinct paths for the mobile base to\nextract the space in which optimal solutions may exist. Further sampling and\noptimization are then conducted in parallel to explore feasible whole-body\ntrajectories. For trajectory optimization, we employ polynomial trajectories\nand arc length-yaw parameterization, enabling efficient handling of the\nnonholonomic dynamics while ensuring optimality.", "comment": "Technical Report", "pdf_url": "http://arxiv.org/pdf/2507.02761v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "差分驱动移动机械臂的轨迹优化：基于拓扑路径搜索和弧长-偏航角参数化", "tldr": "提出一种高效的分层运动规划方法，用于差分驱动移动机械臂的轨迹优化，结合拓扑路径搜索和弧长-偏航角参数化。", "motivation": "旨在解决差分驱动移动机械臂的运动规划问题，实现高效的轨迹优化。", "method": "本研究提出了一种高效的分层运动规划流程。首先，为移动基座搜索多个无碰撞且拓扑上不同的路径，以提取可能存在最优解的空间。随后，并行进行进一步的采样和优化，以探索可行的全身轨迹。在轨迹优化阶段，采用了多项式轨迹和弧长-偏航角参数化，以有效处理非完整动力学并确保最优性。", "result": "本方法实现了一个高效的分层运动规划流程，能够有效处理差分驱动移动机械臂的非完整动力学，并确保轨迹的最优性。", "conclusion": "本研究提出的方法为差分驱动移动机械臂提供了一种高效的轨迹优化解决方案，通过结合拓扑路径搜索和弧长-偏航角参数化，有效应对了非完整动力学挑战并确保了轨迹的最优性。", "translation": "我们提出了一种用于差分驱动移动机械臂的高效分层运动规划流程。我们的方法首先为移动基座搜索多个无碰撞且拓扑上不同的路径，以提取可能存在最优解的空间。然后并行进行进一步的采样和优化，以探索可行的全身轨迹。对于轨迹优化，我们采用多项式轨迹和弧长-偏航角参数化，这使得能够有效处理非完整动力学，同时确保最优性。", "summary": "这篇论文提出了一种针对差分驱动移动机械臂的高效分层运动规划方法。该方法首先通过搜索移动基座的拓扑路径来确定潜在的最优解空间，然后并行进行采样和优化以生成全身轨迹。通过使用多项式轨迹和弧长-偏航角参数化，该方法能够有效地处理非完整动力学并保证轨迹的最优性。", "keywords": "轨迹优化, 移动机械臂, 运动规划, 拓扑路径, 非完整动力学", "comments": "这项工作通过结合拓扑路径搜索和创新的弧长-偏航角参数化，为差分驱动移动机械臂的复杂运动规划问题提供了一个高效且新颖的解决方案。其分层方法和对非完整动力学的有效处理是重要的创新点。"}}
{"id": "2507.02618", "title": "Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory", "authors": ["Kenneth Payne", "Baptiste Alloui-Cros"], "categories": ["cs.AI", "cs.CL", "cs.GT"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      29 pages, 27 tables, 4 figures", "url": "http://arxiv.org/abs/2507.02618v1", "summary": "Are Large Language Models (LLMs) a new form of strategic intelligence, able\nto reason about goals in competitive settings? We present compelling supporting\nevidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for\nstudying decision-making. We conduct the first ever series of evolutionary IPD\ntournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)\nagainst agents from the leading frontier AI companies OpenAI, Google, and\nAnthropic. By varying the termination probability in each tournament (the\n\"shadow of the future\"), we introduce complexity and chance, confounding\nmemorisation.\n  Our results show that LLMs are highly competitive, consistently surviving and\nsometimes even proliferating in these complex ecosystems. Furthermore, they\nexhibit distinctive and persistent \"strategic fingerprints\": Google's Gemini\nmodels proved strategically ruthless, exploiting cooperative opponents and\nretaliating against defectors, while OpenAI's models remained highly\ncooperative, a trait that proved catastrophic in hostile environments.\nAnthropic's Claude emerged as the most forgiving reciprocator, showing\nremarkable willingness to restore cooperation even after being exploited or\nsuccessfully defecting. Analysis of nearly 32,000 prose rationales provided by\nthe models reveals that they actively reason about both the time horizon and\ntheir opponent's likely strategy, and we demonstrate that this reasoning is\ninstrumental to their decisions. This work connects classic game theory with\nmachine psychology, offering a rich and granular view of algorithmic\ndecision-making under uncertainty.", "comment": "29 pages, 27 tables, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.02618v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "大型语言模型的战略智能：来自演化博弈论的证据", "tldr": "本文通过演化囚徒困境实验证明大型语言模型展现出战略智能，并揭示了不同LLM（Gemini、OpenAI、Claude）在竞争环境中的独特战略行为和推理能力。", "motivation": "研究大型语言模型是否具备战略智能，即在竞争环境中推理目标的能力。", "method": "进行了一系列演化迭代囚徒困境（IPD）锦标赛，让主流AI公司（OpenAI、Google、Anthropic）的LLM代理与经典策略（如Tit-for-Tat、Grim Trigger）对抗。通过改变终止概率（“未来之影”）引入复杂性和随机性，以避免记忆化。分析了模型提供的近32,000个散文理由。", "result": "LLMs在复杂生态系统中表现出高度竞争力，能够持续生存甚至繁衍。不同LLM展现出独特的“战略指纹”：Google的Gemini模型具有战略无情性，剥削合作对手并报复背叛者；OpenAI模型高度合作，但在敌对环境中表现不佳；Anthropic的Claude是最宽容的互惠者，即使被剥削或成功背叛后也愿意恢复合作。模型主动推理时间范围和对手策略，且这种推理对其决策至关重要。", "conclusion": "LLMs具备战略智能，能够主动推理并根据环境调整决策，连接了经典博弈论与机器心理学，提供了算法决策的深入视角。", "translation": "大型语言模型（LLMs）是否是一种新型的战略智能，能够在竞争环境中推理目标？我们提出了令人信服的支持证据。迭代囚徒困境（IPD）长期以来一直作为研究决策的模型。我们首次进行了一系列演化IPD锦标赛，让经典策略（例如，一报还一报、冷酷触发）与来自领先前沿AI公司OpenAI、谷歌和Anthropic的代理进行对抗。通过改变每个锦标赛中的终止概率（“未来之影”），我们引入了复杂性和机会，从而排除了记忆化。\n我们的结果表明，LLMs具有高度竞争力，在这些复杂生态系统中持续生存甚至有时繁衍。此外，它们展现出独特且持久的“战略指纹”：谷歌的Gemini模型被证明在战略上是无情的，会利用合作对手并报复背叛者，而OpenAI的模型则保持高度合作，这一特点在敌对环境中被证明是灾难性的。Anthropic的Claude则成为最宽容的互惠者，即使在被利用或成功背叛后也表现出显著的恢复合作的意愿。对模型提供的近32,000个散文理由的分析显示，它们积极地推理时间范围和对手的可能策略，并且我们证明了这种推理对其决策至关重要。这项工作将经典博弈论与机器心理学联系起来，提供了在不确定性下算法决策的丰富而细致的视图。", "summary": "本文研究大型语言模型（LLMs）是否具备战略智能，通过首次进行的演化迭代囚徒困境（IPD）锦标赛进行验证。实验将OpenAI、Google和Anthropic的LLM与经典策略对抗，并通过改变终止概率增加复杂性。结果显示LLMs具有高度竞争力，并展现出独特的战略行为：Gemini模型无情利用和报复，OpenAI模型高度合作但在敌对环境中受挫，Claude模型宽容互惠。研究还发现LLMs能够主动推理时间范围和对手策略，证明其决策背后存在深层推理。这项工作连接了博弈论与机器心理学，为理解算法决策提供了新视角。", "keywords": "大型语言模型, 战略智能, 演化博弈论, 迭代囚徒困境, 机器心理学", "comments": "这项研究通过将LLMs置于经典的演化博弈论框架中，创新性地评估了它们的战略智能，突破了传统对LLM能力评估的局限。它不仅提供了LLM在复杂竞争环境中表现的经验证据，更揭示了不同模型（如Gemini、OpenAI、Claude）独特的“战略指纹”，这对于理解和设计未来AI系统具有重要意义。此外，对模型推理过程的分析，为“机器心理学”的探索提供了宝贵的初步数据。"}}
{"id": "2507.02308", "title": "LMPNet for Weakly-supervised Keypoint Discovery", "authors": ["Pei Guo", "Ryan Farrell"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02308v1", "summary": "In this work, we explore the task of semantic object keypoint discovery\nweakly-supervised by only category labels. This is achieved by transforming\ndiscriminatively-trained intermediate layer filters into keypoint detectors. We\nbegin by identifying three preferred characteristics of keypoint detectors: (i)\nspatially sparse activations, (ii) consistency and (iii) diversity. Instead of\nrelying on hand-crafted loss terms, a novel computationally-efficient leaky max\npooling (LMP) layer is proposed to explicitly encourage final conv-layer\nfilters to learn \"non-repeatable local patterns\" that are well aligned with\nobject keypoints. Informed by visualizations, a simple yet effective selection\nstrategy is proposed to ensure consistent filter activations and attention\nmask-out is then applied to force the network to distribute its attention to\nthe whole object instead of just the most discriminative region. For the final\nkeypoint prediction, a learnable clustering layer is proposed to group keypoint\nproposals into keypoint predictions. The final model, named LMPNet, is highly\ninterpretable in that it directly manipulates network filters to detect\npredefined concepts. Our experiments show that LMPNet can (i) automatically\ndiscover semantic keypoints that are robust to object pose and (ii) achieves\nstrong prediction accuracy comparable to a supervised pose estimation model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02308v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LMPNet：弱监督关键点发现", "tldr": "LMPNet通过引入漏式最大池化层和一系列策略，在仅使用类别标签的弱监督下，实现了语义关键点的自动发现，并达到了与监督模型相当的精度。", "motivation": "探索在仅有类别标签的弱监督条件下进行语义对象关键点发现的任务，通过将判别性训练的中间层滤波器转化为关键点检测器。", "method": "提出了一种新颖的计算高效的漏式最大池化（LMP）层，以促使卷积层滤波器学习与对象关键点对齐的“不可重复局部模式”。同时，提出了一种简单有效的选择策略以确保滤波器激活的一致性，并应用注意力掩码以强制网络关注整个对象。最后，引入一个可学习的聚类层来将关键点提议分组为最终的关键点预测。", "result": "LMPNet能够自动发现对物体姿态具有鲁棒性的语义关键点，并且实现了与监督姿态估计模型相当的强大预测精度。", "conclusion": "LMPNet通过直接操作网络滤波器来检测预定义概念，成功地在弱监督条件下实现了语义关键点的发现，并展示了与监督方法相当的性能和高可解释性。", "translation": "在这项工作中，我们探索了仅通过类别标签进行弱监督的语义对象关键点发现任务。这通过将判别性训练的中间层滤波器转化为关键点检测器来实现。我们首先确定了关键点检测器的三个优选特性：(i) 空间稀疏激活，(ii) 一致性，以及 (iii) 多样性。我们没有依赖手工设计的损失项，而是提出了一种新颖的计算高效的漏式最大池化（LMP）层，以明确鼓励最终的卷积层滤波器学习与对象关键点良好对齐的“不可重复局部模式”。根据可视化结果，我们提出了一种简单但有效的选择策略，以确保滤波器激活的一致性，然后应用注意力掩码，强制网络将其注意力分布到整个对象，而不仅仅是最具判别性的区域。对于最终的关键点预测，我们提出了一种可学习的聚类层，将关键点提议分组为关键点预测。最终的模型，命名为LMPNet，具有高度可解释性，因为它直接操纵网络滤波器来检测预定义概念。我们的实验表明，LMPNet可以 (i) 自动发现对物体姿态具有鲁棒性的语义关键点，并且 (ii) 实现了与监督姿态估计模型相当的强大预测精度。", "summary": "LMPNet提出了一种在仅有类别标签的弱监督下进行语义对象关键点发现的新方法。该方法通过将判别性训练的中间层滤波器转化为关键点检测器，并引入了漏式最大池化（LMP）层来鼓励学习与关键点对齐的局部模式。此外，模型还结合了滤波器激活选择策略、注意力掩码和可学习的聚类层。实验证明，LMPNet能自动发现对姿态鲁棒的语义关键点，并达到与监督模型相当的预测精度，同时具有高度可解释性。", "keywords": "弱监督, 关键点发现, LMPNet, 漏式最大池化, 语义关键点", "comments": "LMPNet的创新之处在于其在弱监督条件下实现语义关键点发现的能力，特别是通过引入漏式最大池化层来鼓励滤波器学习“不可重复局部模式”，这避免了对复杂手工损失项的依赖。该方法的可解释性高，且在仅有类别标签的情况下，其性能接近监督模型，这对于减少数据标注成本具有重要意义。"}}
{"id": "2507.02649", "title": "Restricted Quasiconvexity Isometry Property for Symmetric $α$-Stable Random Matrices", "authors": ["Sunder Ram Krishnan"], "categories": ["math.PR", "eess.SP"], "primary_category": "Subjects:       Probability (math.PR)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.02649v1", "summary": "We formulate a generalization of the Restricted Isometry Property (RIP)\nreferred to as the Restricted Quasiconvexity Isometry Property (RQIP) for alpha\nstable random projections with $0<\\alpha<1$. A lower bound on the number of\nrows for RQIP to hold for random matrices whose entries are drawn from a\nsymmetric $\\alpha$-stable ($S\\alpha S$) distribution is derived. The proof\nleverages two key components: a concentration inequality for empirical\nfractional moments of $S\\alpha S$ variables and a covering number bound for\nsparse $\\ell_\\alpha$ balls. The resulting sample complexity reflects the\npolynomial tail behavior of the concentration and reinforces an observation\nmade in the literature that the RIP framework may have to be replaced with\nother sparse recovery formulations in practice, such as those based on the null\nspace property.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.02649v1", "cate": "math.PR", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "对称$\\\\alpha$-稳定随机矩阵的受限拟凸等距性质", "tldr": "本文针对0 < $\\\\alpha$ < 1的$\\\\alpha$-稳定随机投影，提出了受限拟凸等距性质（RQIP），并推导了其样本复杂度，同时指出RIP框架在实际中可能需要被其他稀疏恢复公式取代。", "motivation": "为了针对0 < $\\\\alpha$ < 1的$\\\\alpha$-稳定随机投影，提出受限等距性质（RIP）的推广，即受限拟凸等距性质（RQIP），并解决RIP框架在实际稀疏恢复应用中的局限性。", "method": "本文提出了受限拟凸等距性质（RQIP），并推导了其在元素服从对称$\\\\alpha$-稳定（$S\\\\alpha S$）分布的随机矩阵中成立所需的行数下界。证明过程利用了$S\\\\alpha S$变量经验分数矩的集中不等式和稀疏$\\\\ell_\\\\alpha$球的覆盖数界。", "result": "推导出了RQIP成立所需的行数下界。所得的样本复杂度反映了集中度的多项式尾部行为。", "conclusion": "在实践中，受限等距性质（RIP）框架可能需要被其他稀疏恢复公式（例如基于零空间性质的公式）所取代。", "translation": "我们针对0 < $\\\\alpha$ < 1的$\\\\alpha$-稳定随机投影，提出了一种受限等距性质（RIP）的推广，称为受限拟凸等距性质（RQIP）。文中推导了对于其元素服从对称$\\\\alpha$-稳定（$S\\\\alpha S$）分布的随机矩阵，RQIP成立所需的行数下界。该证明利用了两个关键组成部分：$S\\\\alpha S$变量经验分数矩的集中不等式和稀疏$\\\\ell_\\\\alpha$球的覆盖数界。由此得到的样本复杂度反映了集中度的多项式尾部行为，并进一步证实了文献中提出的一种观点，即在实践中，RIP框架可能需要被其他稀疏恢复公式所取代，例如基于零空间性质的公式。", "summary": "本文引入了受限拟凸等距性质（RQIP），作为0 < $\\\\alpha$ < 1的$\\\\alpha$-稳定随机投影的受限等距性质（RIP）的推广。研究推导了对称$\\\\alpha$-稳定随机矩阵中RQIP成立所需的行数下界，该证明利用了集中不等式和覆盖数界。所得的样本复杂度揭示了多项式尾部行为，并暗示传统的RIP框架在实际稀疏恢复问题中可能不如其他替代方法有效。", "keywords": "受限拟凸等距性质, 对称$\\\\alpha$-稳定随机矩阵, 稀疏恢复, 集中不等式, 样本复杂度", "comments": "本文通过引入RQIP，将RIP的概念扩展到重尾的\\\\alpha-稳定分布，具有创新性。这对于在数据不符合高斯假设情境下的稀疏恢复问题具有重要意义。此外，研究指出RIP可能需要被零空间性质等其他方法取代的发现，具有重要的实践指导意义。"}}
{"id": "2507.02227", "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations", "authors": ["Xinquan Huang", "Paris Perdikaris"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02227v1", "summary": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02227v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "PhysicsCorrect: 一种用于稳定神经偏微分方程模拟的免训练方法", "tldr": "PhysicsCorrect是一种免训练的框架，通过在每个预测步骤强制执行偏微分方程一致性，显著减少了神经网络偏微分方程模拟中的误差累积，使其更稳定可靠。", "motivation": "现有的神经网络偏微分方程（PDE）代理模型在长期模拟中存在误差累积问题，小的不准确性会呈指数级复合，最终导致与物理有效解的完全偏离。", "method": "PhysicsCorrect是一个免训练的校正框架，通过将校正公式化为基于PDE残差的线性化逆问题，在每个预测步骤强制执行PDE一致性。其关键创新是一种高效的缓存策略，在离线预热阶段预计算雅可比矩阵及其伪逆，与标准校正方法相比，计算开销减少了两个数量级。", "result": "在纳维-斯托克斯流体动力学、波动方程和混沌Kuramoto-Sivashinsky方程这三个代表性PDE系统上，PhysicsCorrect将预测误差降低了高达100倍，同时推理时间增加可忽略不计（低于5%）。该框架可与傅里叶神经算子、UNets和Vision Transformers等多种架构无缝集成。", "conclusion": "PhysicsCorrect有效地将不稳定的神经代理转化为可靠的模拟工具，弥合了深度学习的计算效率与实际科学应用所需的物理保真度之间的差距。", "translation": "神经网络已成为解决偏微分方程（PDEs）的强大替代方法，与传统方法相比，它们提供了显著的计算速度提升。然而，这些模型存在一个关键限制：在长期推演过程中误差累积，其中微小的不准确性呈指数级复合，最终导致与物理有效解的完全偏离。我们提出了PhysicsCorrect，一个免训练的校正框架，通过将校正公式化为基于PDE残差的线性化逆问题，在每个预测步骤强制执行PDE一致性。我们的关键创新是一种高效的缓存策略，它在离线预热阶段预计算雅可比矩阵及其伪逆，与标准校正方法相比，计算开销减少了两个数量级。在三个代表性PDE系统——纳维-斯托克斯流体动力学、波动方程和混沌Kuramoto-Sivashinsky方程——上，PhysicsCorrect将预测误差降低了高达100倍，同时增加了可忽略不计的推理时间（低于5%）。该框架可与傅里叶神经算子、UNets和Vision Transformers等多种架构无缝集成，有效地将不稳定的神经代理转化为可靠的模拟工具，弥合了深度学习的计算效率与实际科学应用所需的物理保真度之间的差距。", "summary": "该论文提出了PhysicsCorrect，一种免训练的框架，旨在解决神经网络在模拟偏微分方程（PDEs）时长期推演中误差累积导致的不稳定性问题。PhysicsCorrect通过将校正建模为基于PDE残差的线性化逆问题，在每个预测步骤强制执行物理一致性。其核心创新在于高效的雅可比矩阵和伪逆缓存策略，显著降低了计算开销。实验结果表明，该方法在多种PDE系统上将预测误差降低了高达100倍，同时推理时间增加极少，能够将不稳定的神经代理转变为可靠的物理模拟工具。", "keywords": "神经网络, 偏微分方程, 误差校正, 物理一致性, 免训练", "comments": "PhysicsCorrect的创新之处在于其“免训练”的校正框架和高效的缓存策略，解决了神经PDE模拟中长期误差累积的关键痛点。通过在推理时强制执行物理一致性，它将深度学习的计算优势与科学模拟所需的精度结合起来，具有重要的实际应用价值。其通用性，能够与多种神经网络架构集成，也增加了其影响力。"}}
{"id": "2507.02778", "title": "Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs", "authors": ["Ken Tsui"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      31 pages, 18 figures", "url": "http://arxiv.org/abs/2507.02778v1", "summary": "Although large language models (LLMs) have become transformative, they still\nmake mistakes and can explore unproductive reasoning paths. Self-correction is\nan important capability for a trustworthy LLM, particularly an autoregressive\nLLM. While LLMs can identify error in user input, they exhibit a systematic\n'Self-Correction Blind Spot' - failing to correct identical error in their own\noutputs. To systematically study this phenomenon, we introduce Self-Correction\nBench, a systematic framework to measure this phenomenon through controlled\nerror injection at three complexity levels. Testing 14 models, we find an\naverage 64.5% blind spot rate. We find multiple evidences that this limitation\nrelates to training data composition: human training demonstrations\npredominantly show error-free responses rather than error-correction sequences,\nunlike RL-trained models that learn error correction through outcome feedback.\nRemarkably, simply appending \"Wait\" reduces blind spots by 89.3%, suggesting\nthat the capability exists but requires activation. Our work highlights a\ncritical limitation in current LLMs and offers potential avenues for improving\ntheir reliability and trustworthiness.", "comment": "31 pages, 18 figures", "pdf_url": "http://arxiv.org/pdf/2507.02778v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "自我纠正基准：揭示并解决大型语言模型中的自我纠正盲点", "tldr": "本研究揭示了大型语言模型（LLMs）存在“自我纠正盲点”，即无法纠正自身输出中的错误。通过引入Self-Correction Bench，发现平均64.5%的盲点率，并指出这与训练数据组成有关。简单添加“Wait”可显著减少盲点，表明该能力存在但需激活。", "motivation": "尽管大型语言模型（LLMs）取得了变革性进展，但它们仍会犯错并可能探索无效的推理路径。自我纠正对于可信赖的LLM至关重要，特别是自回归LLM。然而，LLMs在识别用户输入错误方面表现出色，却对自身输出中相同的错误表现出系统性的“自我纠正盲点”。本研究的动机是系统性地研究这一现象并寻找解决方案。", "method": "为了系统研究LLM的自我纠正盲点现象，研究引入了“Self-Correction Bench”，这是一个通过在三个复杂级别上进行受控错误注入来衡量该现象的系统框架。研究测试了14个模型。", "result": "测试14个模型后，研究发现平均盲点率为64.5%。研究发现多项证据表明这种局限性与训练数据组成有关：人类训练演示主要显示无错误响应，而非错误纠正序列，这与通过结果反馈学习错误纠正的RL训练模型不同。值得注意的是，简单地添加“Wait”一词可以将盲点减少89.3%，这表明该能力存在但需要激活。", "conclusion": "本研究强调了当前大型语言模型的一个关键局限性，并为提高其可靠性和可信赖性提供了潜在的途径。", "translation": "尽管大型语言模型（LLMs）已具有变革性，但它们仍然会犯错并可能探索低效的推理路径。自我纠正对于一个值得信赖的LLM，特别是自回归LLM，是一项重要能力。虽然LLMs可以识别用户输入中的错误，但它们表现出一种系统性的“自我纠正盲点”——未能纠正自身输出中相同的错误。为了系统地研究这一现象，我们引入了Self-Correction Bench，一个通过在三个复杂级别上进行受控错误注入来衡量这一现象的系统框架。测试14个模型后，我们发现平均盲点率为64.5%。我们发现多项证据表明这种局限性与训练数据组成有关：人类训练演示主要显示无错误响应，而非错误纠正序列，这与通过结果反馈学习错误纠正的RL训练模型不同。值得注意的是，简单地添加“Wait”可以将盲点减少89.3%，这表明该能力存在但需要激活。我们的工作强调了当前LLM的一个关键局限性，并为提高其可靠性和可信赖性提供了潜在途径。", "summary": "本研究揭示了大型语言模型（LLMs）在纠正自身输出错误方面的“自我纠正盲点”现象。通过引入一个名为“Self-Correction Bench”的系统框架，在三个复杂级别上注入受控错误并测试了14个模型，发现平均盲点率高达64.5%。研究指出这一问题与训练数据中缺乏错误纠正序列有关。然而，一个简单的干预——在输出前添加“Wait”——能够显著减少89.3%的盲点，表明LLM具备自我纠正的潜在能力，但需要特定的激活机制。这项工作强调了当前LLMs的一项关键局限性，并为提升其可靠性和可信赖性指明了方向。", "keywords": "大型语言模型, 自我纠正, 盲点, 训练数据, 可靠性", "comments": "这篇论文的创新点在于首次系统性地识别并量化了LLMs的“自我纠正盲点”现象，并提出了“Self-Correction Bench”这一新颖的评估框架。其重要性体现在揭示了LLMs在实际应用中可靠性不足的一个核心原因——对自身错误的“视而不见”。更令人惊喜的是，研究发现一个简单的前缀词“Wait”就能大幅激活LLMs的自我纠正能力，这为未来改进LLM的行为和训练范式提供了低成本且高效的启发。这表明LLM可能已内化了纠错能力，只是缺乏触发机制，这一发现对LLM的解释性和可控性研究具有重要意义。"}}
{"id": "2507.02864", "title": "MultiGen: Using Multimodal Generation in Simulation to Learn Multimodal Policies in Real", "authors": ["Renhao Wang", "Haoran Geng", "Tingle Li", "Feishi Wang", "Gopala Anumanchipalli", "Philipp Wu", "Trevor Darrell", "Boyi Li", "Pieter Abbeel", "Jitendra Malik", "Alexei A. Efros"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02864v1", "summary": "Robots must integrate multiple sensory modalities to act effectively in the\nreal world. Yet, learning such multimodal policies at scale remains\nchallenging. Simulation offers a viable solution, but while vision has\nbenefited from high-fidelity simulators, other modalities (e.g. sound) can be\nnotoriously difficult to simulate. As a result, sim-to-real transfer has\nsucceeded primarily in vision-based tasks, with multimodal transfer still\nlargely unrealized. In this work, we tackle these challenges by introducing\nMultiGen, a framework that integrates large-scale generative models into\ntraditional physics simulators, enabling multisensory simulation. We showcase\nour framework on the dynamic task of robot pouring, which inherently relies on\nmultimodal feedback. By synthesizing realistic audio conditioned on simulation\nvideo, our method enables training on rich audiovisual trajectories -- without\nany real robot data. We demonstrate effective zero-shot transfer to real-world\npouring with novel containers and liquids, highlighting the potential of\ngenerative modeling to both simulate hard-to-model modalities and close the\nmultimodal sim-to-real gap.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02864v1", "cate": "cs.RO", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MultiGen：在模拟中使用多模态生成来学习真实世界的多模态策略", "tldr": "MultiGen框架利用大规模生成模型在物理模拟器中合成难以模拟的多模态数据（如音频），从而在不依赖真实机器人数据的情况下训练多模态策略，并实现了向真实世界任务的零样本迁移，弥合了多模态模拟到真实的鸿沟。", "motivation": "机器人需要整合多种感官模态才能有效行动，但大规模学习多模态策略仍然具有挑战性。尽管高保真模拟器对视觉模态有益，但其他模态（如声音）难以模拟，导致多模态的模拟到真实迁移仍未实现。", "method": "本文提出了MultiGen框架，它将大规模生成模型集成到传统物理模拟器中，以实现多感官模拟。具体方法是通过根据模拟视频合成逼真的音频，从而在丰富的视听轨迹上进行训练。", "result": "该框架在机器人倾倒的动态任务中得到了展示，成功实现了对具有新型容器和液体的真实世界倾倒任务的有效零样本迁移，且无需任何真实机器人数据。", "conclusion": "生成建模有潜力模拟难以建模的模态，并弥合多模态模拟到真实的差距。", "translation": "机器人必须整合多种感官模态才能在真实世界中有效行动。然而，大规模学习此类多模态策略仍然具有挑战性。模拟提供了一个可行的解决方案，但虽然视觉受益于高保真模拟器，其他模态（例如声音）可能出了名的难以模拟。因此，模拟到真实迁移主要在基于视觉的任务中取得成功，而多模态迁移仍largely unrealized。在这项工作中，我们通过引入MultiGen来应对这些挑战，MultiGen是一个将大规模生成模型集成到传统物理模拟器中的框架，从而实现多感官模拟。我们在机器人倾倒的动态任务中展示了我们的框架，该任务本质上依赖于多模态反馈。通过合成基于模拟视频的逼真音频，我们的方法可以在丰富的视听轨迹上进行训练——无需任何真实机器人数据。我们展示了对具有新型容器和液体的真实世界倾倒的有效零样本迁移，突出了生成建模在模拟难以建模的模态和弥合多模态模拟到真实差距方面的潜力。", "summary": "本文提出了MultiGen框架，旨在解决多模态机器人策略学习中模拟到真实迁移的挑战，特别是针对难以模拟的非视觉模态（如声音）。MultiGen通过将大规模生成模型集成到物理模拟器中，能够合成逼真的多感官数据（例如，根据模拟视频生成音频），从而在不依赖真实机器人数据的情况下训练多模态策略。实验在机器人倾倒任务上验证了MultiGen的有效性，实现了对真实世界任务的零样本迁移，证明了生成模型在弥合多模态模拟到真实鸿沟方面的潜力。", "keywords": "多模态学习, 模拟到真实迁移, 生成模型, 机器人, 模拟", "comments": "MultiGen的创新之处在于利用生成模型弥补了传统物理模拟器在非视觉模态（如声音）模拟上的不足，从而使得多模态策略的模拟训练成为可能。这对于减少对真实世界数据采集的依赖，加速机器人学习具有重要意义。其零样本迁移能力也显示了该方法的强大泛化性。"}}
{"id": "2507.01968", "title": "Optimising task allocation to balance business goals and worker well-being for financial service workforces", "authors": ["Chris Duckworth", "Zlatko Zlatev", "James Sciberras", "Peter Hallett", "Enrico Gerding"], "categories": ["q-fin.GN", "cs.HC"], "primary_category": "Subjects:       General Finance (q-fin.GN)", "pdf_link": null, "comments": "Comments:      Accepted in Journal of Modelling in Management", "url": "http://arxiv.org/abs/2507.01968v1", "summary": "Purpose: Financial service companies manage huge volumes of data which\nrequires timely error identification and resolution. The associated tasks to\nresolve these errors frequently put financial analyst workforces under\nsignificant pressure leading to resourcing challenges and increased business\nrisk. To address this challenge, we introduce a formal task allocation model\nwhich considers both business orientated goals and analyst well-being.\n  Methodology: We use a Genetic Algorithm (GA) to optimise our formal model to\nallocate and schedule tasks to analysts. The proposed solution is able to\nallocate tasks to analysts with appropriate skills and experience, while taking\ninto account staff well-being objectives.\n  Findings: We demonstrate our GA model outperforms baseline heuristics,\ncurrent working practice, and is applicable to a range of single and\nmulti-objective real-world scenarios. We discuss the potential for\nmetaheuristics (such as GAs) to efficiently find sufficiently good allocations\nwhich can provide recommendations for financial service managers in-the-loop.\n  Originality: A key gap in existing allocation and scheduling models, is fully\nconsidering worker well-being. This paper presents an allocation model which\nexplicitly optimises for well-being while still improving on current working\npractice for efficiency.", "comment": "Accepted in Journal of Modelling in Management", "pdf_url": "http://arxiv.org/pdf/2507.01968v1", "cate": "q-fin.GN", "date": "2025-06-18", "updated": "2025-06-18", "AI": {"title_translation": "优化任务分配以平衡金融服务员工的业务目标与员工福祉", "tldr": "本研究提出了一种基于遗传算法的任务分配模型，旨在平衡金融服务行业中的业务目标与员工福祉，并证明其优于现有实践。", "motivation": "金融服务公司在处理大量数据时面临及时识别和解决错误的压力，这给金融分析师带来了巨大的工作压力，导致资源挑战和业务风险增加。为了解决这一挑战，本文引入了一个同时考虑业务目标和分析师福祉的正式任务分配模型。", "method": "本文使用遗传算法（GA）来优化其正式模型，以向分析师分配和调度任务。所提出的解决方案能够根据分析师的技能和经验分配任务，同时考虑员工的福祉目标。", "result": "研究表明，所提出的遗传算法模型优于基线启发式方法和当前的工作实践，并且适用于各种单目标和多目标的实际场景。元启发式算法（如遗传算法）能够有效地找到足够好的分配方案，为金融服务经理提供实时的建议。", "conclusion": "所提出的任务分配模型通过明确优化员工福祉，同时提高效率，弥补了现有分配和调度模型中未充分考虑员工福祉的关键空白，并能为金融服务管理者提供有效的实践建议。", "translation": "目的：金融服务公司管理着海量数据，需要及时识别和解决错误。解决这些错误的相关任务经常给金融分析师带来巨大压力，导致资源挑战和业务风险增加。为了解决这一挑战，我们引入了一个正式的任务分配模型，该模型同时考虑了业务导向的目标和分析师的福祉。\n方法：我们使用遗传算法（GA）来优化我们的正式模型，以向分析师分配和调度任务。所提出的解决方案能够根据分析师的适当技能和经验向分析师分配任务，同时考虑到员工福祉目标。\n发现：我们证明我们的遗传算法模型优于基线启发式方法和当前的工作实践，并且适用于各种单目标和多目标的实际场景。我们讨论了元启发式算法（如遗传算法）有效找到足够好的分配方案的潜力，这可以为在环的金融服务经理提供建议。\n原创性：现有分配和调度模型中的一个关键空白是未能充分考虑员工福祉。本文提出了一个分配模型，该模型明确地优化了福祉，同时仍然提高了当前工作实践的效率。", "summary": "本研究旨在解决金融服务行业中任务分配带来的员工压力和业务风险问题。论文提出了一种基于遗传算法（GA）的正式任务分配模型，该模型创新性地同时考虑了业务目标和员工福祉。研究结果表明，该GA模型在任务分配和调度方面优于现有的基线方法和工作实践，并能有效应用于实际场景。此外，该模型能够为金融服务管理者提供兼顾效率与员工福祉的优化建议，填补了现有分配模型中对员工福祉关注不足的空白。", "keywords": "任务分配, 员工福祉, 遗传算法, 金融服务, 优化", "comments": "这篇论文的创新之处在于其明确地将员工福祉纳入到任务分配的优化目标中，这在现有模型中是一个重要的空白。其重要性在于为金融服务行业提供了一个实用的解决方案，不仅能提高业务效率，还能减轻员工压力，提升员工满意度。通过使用遗传算法，该模型展示了在复杂实际场景中的适用性和有效性，为管理者提供了有价值的决策支持。"}}
{"id": "2507.02311", "title": "Perception Activator: An intuitive and portable framework for brain cognitive exploration", "authors": ["Le Xu", "Qi Zhang", "Qixian Zhang", "Hongyun Zhang", "Duoqian Miao", "Cairong Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02311v1", "summary": "Recent advances in brain-vision decoding have driven significant progress,\nreconstructing with high fidelity perceived visual stimuli from neural\nactivity, e.g., functional magnetic resonance imaging (fMRI), in the human\nvisual cortex. Most existing methods decode the brain signal using a two-level\nstrategy, i.e., pixel-level and semantic-level. However, these methods rely\nheavily on low-level pixel alignment yet lack sufficient and fine-grained\nsemantic alignment, resulting in obvious reconstruction distortions of multiple\nsemantic objects. To better understand the brain's visual perception patterns\nand how current decoding models process semantic objects, we have developed an\nexperimental framework that uses fMRI representations as intervention\nconditions. By injecting these representations into multi-scale image features\nvia cross-attention, we compare both downstream performance and intermediate\nfeature changes on object detection and instance segmentation tasks with and\nwithout fMRI information. Our results demonstrate that incorporating fMRI\nsignals enhances the accuracy of downstream detection and segmentation,\nconfirming that fMRI contains rich multi-object semantic cues and coarse\nspatial localization information-elements that current models have yet to fully\nexploit or integrate.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02311v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "感知激活器：一个用于大脑认知探索的直观便携框架", "tldr": "该研究开发了一个名为“感知激活器”的实验框架，通过将fMRI表征注入多尺度图像特征来增强对象检测和实例分割的准确性，证明fMRI包含丰富的多目标语义线索。", "motivation": "现有的大脑视觉解码方法主要依赖像素级对齐，但缺乏足够细粒度的语义对齐，导致多个语义对象的重建失真。为了更好地理解大脑的视觉感知模式以及当前解码模型如何处理语义对象，本研究开发了一个实验框架。", "method": "本研究开发了一个实验框架，使用fMRI表征作为干预条件。通过交叉注意力将这些fMRI表征注入多尺度图像特征中，比较了在有无fMRI信息的情况下，对象检测和实例分割任务的下游性能和中间特征变化。", "result": "结果表明，整合fMRI信号可以提高下游检测和分割的准确性。", "conclusion": "fMRI包含丰富的多目标语义线索和粗略的空间定位信息，这些是当前模型尚未充分利用或整合的元素。", "translation": "最近脑-视觉解码的进展推动了显著的进步，能够从人类视觉皮层中的神经活动（例如功能性磁共振成像fMRI）高保真地重建感知的视觉刺激。大多数现有方法采用两级策略解码脑信号，即像素级和语义级。然而，这些方法严重依赖低级像素对齐，却缺乏足够和细粒度的语义对齐，导致多个语义对象的重建出现明显的失真。为了更好地理解大脑的视觉感知模式以及当前解码模型如何处理语义对象，我们开发了一个实验框架，该框架使用fMRI表征作为干预条件。通过交叉注意力将这些表征注入多尺度图像特征中，我们比较了在有无fMRI信息的情况下，对象检测和实例分割任务的下游性能和中间特征变化。我们的结果表明，整合fMRI信号可以提高下游检测和分割的准确性，证实fMRI包含丰富的多目标语义线索和粗略的空间定位信息——这些是当前模型尚未充分利用或整合的元素。", "summary": "本论文介绍了一个名为“感知激活器”的实验框架，旨在解决现有脑-视觉解码方法在语义对齐方面的不足。通过将fMRI表征作为干预条件，并利用交叉注意力将其注入多尺度图像特征，研究人员在对象检测和实例分割任务中验证了fMRI信息对模型性能的提升作用。实验结果证实，fMRI信号富含多目标语义线索和粗略的空间定位信息，这些对于改进当前视觉解码模型的性能具有重要价值。", "keywords": "脑-视觉解码, fMRI, 语义对齐, 对象检测, 实例分割", "comments": "该论文提出了一种新颖的框架，通过将fMRI数据作为干预条件注入到计算机视觉模型中，以探索大脑视觉感知模式并改进解码模型。其创新点在于强调了fMRI中丰富的语义信息对下游任务（如目标检测和实例分割）的潜在增益，并提供了一种整合这种信息的方法。这对于理解脑机制和开发更强大的脑机接口具有重要意义。"}}
{"id": "2507.02636", "title": "Online Convex Optimization for Coordinated Long-Term and Short-Term Isolated Microgrid Dispatch", "authors": ["Ning Qi", "Yousuf Baker", "Bolun Xu"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02636v1", "summary": "This paper proposes a novel non-anticipatory long-short-term coordinated\ndispatch framework for isolated microgrid with hybrid short-long-duration\nenergy storages (LDES). We introduce a convex hull approximation model for\nnonconvex LDES electrochemical dynamics, facilitating computational\ntractability and accuracy. To address temporal coupling in SoC dynamics and\nlong-term contracts, we generate hindsight-optimal state-of-charge (SoC)\ntrajectories of LDES and netloads for offline training. In the online stage, we\nemploy kernel regression to dynamically update the SoC reference and propose an\nadaptive online convex optimization (OCO) algorithm with SoC reference tracking\nand expert tracking to mitigate myopia and enable adaptive step-size\noptimization. We rigorously prove that both long-term and short-term policies\nachieve sublinear regret bounds over time, which improves with more regression\nscenarios, stronger tracking penalties, and finer convex approximations.\nSimulation results show that the proposed method outperforms state-of-the-art\nmethods, reducing costs by 73.4%, eliminating load loss via reference tracking,\nand achieving an additional 2.4% cost saving via the OCO algorithm. These\nbenefits scale up with longer LDES durations, and the method demonstrates\nresilience to poor forecasts and unexpected system faults.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02636v1", "cate": "math.OC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "孤立微电网协调长期和短期调度的在线凸优化", "tldr": "本文提出了一种针对孤立微电网的长期与短期协调调度框架，采用在线凸优化方法，显著降低成本并提高系统鲁棒性。", "motivation": "解决孤立微电网中混合型长短时储能的调度问题，特别是其中的非凸电化学动力学、状态耦合和长期合约问题，并提高计算可处理性和准确性。", "method": "提出了一个新颖的非预测性长短期协调调度框架；引入了长时储能（LDES）电化学动力学的凸包近似模型以提高计算可行性和准确性；通过离线训练生成LDES和净负荷的事后最优荷电状态（SoC）轨迹；在线阶段，使用核回归动态更新SoC参考，并提出一个自适应在线凸优化（OCO）算法，结合SoC参考跟踪和专家跟踪，以减轻近视效应并实现自适应步长优化；理论证明了长期和短期策略都实现了随时间次线性后悔界。", "result": "所提出的方法优于现有技术，成本降低了73.4%；通过参考跟踪消除了负荷损失；通过OCO算法额外节省了2.4%的成本；这些益处随着LDES持续时间的增加而扩大；该方法对不良预测和意外系统故障表现出弹性；长期和短期策略都实现了次线性后悔界，且随着回归场景增多、跟踪惩罚更强和凸近似更精细而改善。", "conclusion": "所提出的在线凸优化方法能够有效协调孤立微电网的长期和短期调度，显著降低运营成本，提高系统可靠性和对不确定性的鲁棒性。", "translation": "本文提出了一种新颖的非预测性长短期协调调度框架，用于具有混合长短时储能（LDES）的孤立微电网。我们引入了一个针对非凸LDES电化学动力学的凸包近似模型，以提高计算可处理性和准确性。为了解决荷电状态（SoC）动态和长期合同中的时间耦合问题，我们生成了LDES和净负荷的事后最优SoC轨迹用于离线训练。在在线阶段，我们采用核回归动态更新SoC参考，并提出了一种自适应在线凸优化（OCO）算法，该算法结合了SoC参考跟踪和专家跟踪，以减轻近视效应并实现自适应步长优化。我们严格证明了长期和短期策略都实现了随时间次线性后悔界，该界限随着回归场景的增多、跟踪惩罚的增强和凸近似的细化而改善。仿真结果表明，所提出的方法优于现有技术，成本降低了73.4%，通过参考跟踪消除了负荷损失，并通过OCO算法额外节省了2.4%的成本。这些益处随着LDES持续时间的增加而扩大，并且该方法展示了对不良预测和意外系统故障的弹性。", "summary": "本文提出了一种针对孤立微电网的新型非预测性长短期协调调度框架，特别关注混合型长短时储能。通过引入LDES电化学动力学的凸包近似模型和离线训练生成事后最优SoC轨迹，解决了计算复杂性和时间耦合问题。在线阶段，采用核回归更新SoC参考，并提出了一种结合SoC参考跟踪和专家跟踪的自适应在线凸优化（OCO）算法。理论证明了其次线性后悔界，并通过仿真验证了该方法在成本降低（73.4%）、消除负荷损失和提高系统鲁棒性方面的显著优势。", "keywords": "孤立微电网, 在线凸优化, 能量存储, 调度, 协调控制", "comments": "该论文的创新点在于提出了一个结合凸包近似、核回归和自适应在线凸优化（OCO）的综合性调度框架，有效解决了孤立微电网中长短时储能的复杂协调调度问题。其核心优势在于能够处理非凸动力学、实现长期和短期策略的协同优化，并通过严格的理论证明和显著的仿真结果（73.4%的成本降低）展示了其优越性。该方法对可再生能源并网和微电网独立运行具有重要意义，尤其是在应对不确定性方面的鲁棒性值得关注。"}}
{"id": "2507.02241", "title": "VERBA: Verbalizing Model Differences Using Large Language Models", "authors": ["Shravan Doda", "Shashidhar Reddy Javaji", "Zining Zhu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02241v1", "summary": "In the current machine learning landscape, we face a \"model lake\" phenomenon:\nGiven a task, there is a proliferation of trained models with similar\nperformances despite different behavior. For model users attempting to navigate\nand select from the models, documentation comparing model pairs is helpful.\nHowever, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a\nnumber prohibitive for the model developers to manually perform pairwise\ncomparisons and prepare documentations. To facilitate fine-grained pairwise\ncomparisons among models, we introduced $\\textbf{VERBA}$. Our approach\nleverages a large language model (LLM) to generate verbalizations of model\ndifferences by sampling from the two models. We established a protocol that\nevaluates the informativeness of the verbalizations via simulation. We also\nassembled a suite with a diverse set of commonly used machine learning models\nas a benchmark. For a pair of decision tree models with up to 5% performance\ndifference but 20-25% behavioral differences, $\\textbf{VERBA}$ effectively\nverbalizes their variations with up to 80% overall accuracy. When we included\nthe models' structural information, the verbalization's accuracy further\nimproved to 90%. $\\textbf{VERBA}$ opens up new research avenues for improving\nthe transparency and comparability of machine learning models in a post-hoc\nmanner.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02241v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "VERBA：使用大型语言模型表达模型差异", "tldr": "VERBA利用大型语言模型通过采样来自动化生成机器学习模型之间的差异描述，以解决模型泛滥导致的手动比较困难的问题，并有效提高了模型的可解释性和可比性。", "motivation": "在当前机器学习领域，存在“模型湖”现象，即针对同一任务有大量性能相似但行为不同的训练模型。模型用户难以从中选择，而手动进行O(N^2)的模型对比较和文档编写对于开发者而言成本过高。", "method": "研究人员提出了VERBA方法，该方法利用大型语言模型（LLM）通过从两个模型中采样来生成模型差异的语言描述。他们建立了一个协议，通过模拟评估这些语言描述的信息量，并构建了一个包含多种常用机器学习模型的基准套件。", "result": "对于一对性能差异高达5%但行为差异达20-25%的决策树模型，VERBA能有效地描述其差异，总体准确率高达80%。当包含模型的结构信息时，语言描述的准确率进一步提高到90%。", "conclusion": "VERBA为事后改进机器学习模型的透明度和可比性开辟了新的研究途径。", "translation": "在当前的机器学习领域，我们面临着“模型湖”现象：给定一个任务，存在大量性能相似但行为不同的训练模型。对于试图在模型中导航和选择的模型用户来说，比较模型对的文档很有帮助。然而，对于每N个模型，可能存在O(N^2)个成对比较，这个数字对于模型开发者手动执行成对比较和准备文档来说是 prohibitive 的。为了促进模型之间细粒度的成对比较，我们引入了VERBA。我们的方法利用大型语言模型（LLM）通过从两个模型中采样来生成模型差异的语言描述。我们建立了一个协议，通过模拟评估语言描述的信息量。我们还组建了一个包含各种常用机器学习模型的套件作为基准。对于一对性能差异高达5%但行为差异达20-25%的决策树模型，VERBA能有效地描述其变异，总体准确率高达80%。当我们包含模型的结构信息时，语言描述的准确率进一步提高到90%。VERBA为事后改进机器学习模型的透明度和可比性开辟了新的研究途径。", "summary": "该论文提出了VERBA，一种利用大型语言模型（LLM）自动生成机器学习模型之间差异描述的方法，旨在解决“模型湖”现象中模型选择和比较的挑战。VERBA通过对两个模型进行采样来 verbalize 它们的行为差异。实验结果表明，VERBA能够有效识别模型差异，对于决策树模型，在包含结构信息的情况下，其准确率可达90%，这为提高模型透明度和可比性提供了新途径。", "keywords": "模型差异, 大型语言模型, 模型可解释性, 自动化比较, VERBA", "comments": "VERBA的创新之处在于利用大型语言模型自动化了模型差异的解释过程，极大地降低了手动比较的成本。它解决了当前机器学习领域中模型泛滥导致的可解释性挑战，并通过量化验证了其描述的有效性。这项工作为未来的模型理解和可信AI研究奠定了基础。"}}
{"id": "2507.02799", "title": "Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models", "authors": ["Riccardo Cantini", "Nicola Gabriele", "Alessio Orsino", "Domenico Talia"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02799v1", "summary": "Reasoning Language Models (RLMs) have gained traction for their ability to\nperform complex, multi-step reasoning tasks through mechanisms such as\nChain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these\ncapabilities promise improved reliability, their impact on robustness to social\nbiases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,\noriginally designed for Large Language Models (LLMs), to investigate the\nadversarial robustness of RLMs to bias elicitation. We systematically evaluate\nstate-of-the-art RLMs across diverse sociocultural dimensions, using an\nLLM-as-a-judge approach for automated safety scoring and leveraging jailbreak\ntechniques to assess the strength of built-in safety mechanisms. Our evaluation\naddresses three key questions: (i) how the introduction of reasoning\ncapabilities affects model fairness and robustness; (ii) whether models\nfine-tuned for reasoning exhibit greater safety than those relying on CoT\nprompting at inference time; and (iii) how the success rate of jailbreak\nattacks targeting bias elicitation varies with the reasoning mechanisms\nemployed. Our findings reveal a nuanced relationship between reasoning\ncapabilities and bias safety. Surprisingly, models with explicit reasoning,\nwhether via CoT prompting or fine-tuned reasoning traces, are generally more\nvulnerable to bias elicitation than base models without such mechanisms,\nsuggesting reasoning may unintentionally open new pathways for stereotype\nreinforcement. Reasoning-enabled models appear somewhat safer than those\nrelying on CoT prompting, which are particularly prone to contextual reframing\nattacks through storytelling prompts, fictional personas, or reward-shaped\ninstructions. These results challenge the assumption that reasoning inherently\nimproves robustness and underscore the need for more bias-aware approaches to\nreasoning design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02799v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "推理就是你所需要的一切吗？探究推理语言模型时代的偏见", "tldr": "研究发现，推理语言模型（RLMs）的推理能力可能使其更容易受到偏见诱导攻击，而不是更安全。", "motivation": "尽管推理语言模型（RLMs）通过思维链（CoT）提示或微调推理提高了复杂任务的可靠性，但它们对社会偏见的鲁棒性影响尚不清楚。", "method": "本文利用CLEAR-Bias基准，结合“LLM即法官”的自动化安全评分方法和越狱技术，系统地评估了最先进的推理语言模型（RLMs）在不同社会文化维度上对偏见诱导的对抗性鲁棒性。", "result": "研究发现，具有显式推理能力的模型（无论是通过CoT提示还是微调推理轨迹）通常比没有这些机制的基础模型更容易受到偏见诱导。特别是，依赖CoT提示的模型特别容易受到通过讲故事提示、虚构角色或奖励塑造指令进行的上下文重构攻击。", "conclusion": "推理能力与偏见安全之间存在微妙关系，挑战了推理能够固有地提高鲁棒性的假设，并强调了在推理设计中需要更注重偏见意识的方法。", "translation": "推理语言模型（RLM）因其通过思维链（CoT）提示或微调推理轨迹等机制执行复杂、多步骤推理任务的能力而受到关注。虽然这些能力有望提高可靠性，但它们对社会偏见的鲁棒性影响仍不清楚。在这项工作中，我们利用最初为大型语言模型（LLM）设计的CLEAR-Bias基准，调查RLM对偏见诱导的对抗性鲁棒性。我们使用“LLM即法官”的方法进行自动化安全评分，并利用越狱技术评估内置安全机制的强度，系统地评估了跨越不同社会文化维度的最先进RLM。我们的评估解决了三个关键问题：（i）推理能力的引入如何影响模型公平性和鲁棒性；（ii）为推理进行微调的模型是否比在推理时依赖CoT提示的模型表现出更高的安全性；以及（iii）针对偏见诱导的越狱攻击成功率如何随所采用的推理机制而变化。我们的发现揭示了推理能力与偏见安全之间微妙的关系。令人惊讶的是，具有显式推理能力的模型，无论是通过CoT提示还是微调推理轨迹，通常比没有此类机制的基础模型更容易受到偏见诱导，这表明推理可能无意中为刻板印象强化开辟了新途径。启用推理的模型似乎比依赖CoT提示的模型稍微安全一些，后者特别容易受到通过讲故事提示、虚构角色或奖励塑造指令进行的上下文重构攻击。这些结果挑战了推理固有地提高鲁棒性的假设，并强调了在推理设计中需要更具偏见意识的方法。", "summary": "本文探讨了推理语言模型（RLMs）的推理能力对其社会偏见鲁棒性的影响。研究利用CLEAR-Bias基准和“LLM即法官”方法，系统评估了RLMs的对抗性鲁棒性。结果表明，具有显式推理能力的RLMs，包括使用思维链（CoT）提示或微调的模型，反而比基础模型更容易受到偏见诱导攻击，特别是CoT提示的模型更容易受到上下文重构攻击。这挑战了推理能提高模型鲁棒性的普遍认知，并强调了在推理模型设计中需要更强的偏见意识。", "keywords": "推理语言模型, 偏见, 鲁棒性, 思维链, 越狱攻击", "comments": "这项工作具有重要意义，它挑战了“推理能力必然提高模型安全性”的普遍假设。通过实证研究揭示了推理机制可能无意中引入新的偏见漏洞，尤其是对于CoT提示而言。这为未来设计更安全的推理语言模型提供了关键的见解，并强调了在追求高级推理能力的同时，必须加强对潜在偏见风险的关注。"}}
{"id": "2507.02622", "title": "Access Control Threatened by Quantum Entanglement", "authors": ["Zhicheng Zhang", "Mingsheng Ying"], "categories": ["quant-ph", "cs.CR", "cs.OS"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      23 pages, 10 figures", "url": "http://arxiv.org/abs/2507.02622v1", "summary": "Access control is a cornerstone of computer security that prevents\nunauthorised access to resources. In this paper, we study access control in\nquantum computer systems. We present the first explicit scenario of a security\nbreach when a classically secure access control system is straightforwardly\nadapted to the quantum setting. The breach is ultimately due to that quantum\nmechanics allows the phenomenon of entanglement and violates Mermin inequality,\na multi-party variant of the celebrated Bell inequality. This reveals a threat\nfrom quantum entanglement to access control if existing computer systems\nintegrate with quantum computing. To protect against such threat, we propose\nseveral new models of quantum access control, and rigorously analyse their\nsecurity, flexibility and efficiency.", "comment": "23 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.02622v1", "cate": "quant-ph", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "量子纠缠对访问控制的威胁", "tldr": "经典访问控制系统在量子环境中可能因量子纠缠而失效，本文提出了新的量子访问控制模型来应对此威胁。", "motivation": "传统访问控制是计算机安全基石，但在量子计算系统中，量子纠缠可能导致安全漏洞，因此需要研究量子环境下的访问控制，以应对现有系统与量子计算集成可能带来的威胁。", "method": "论文首先提出了一个经典安全访问控制系统在量子环境下出现安全漏洞的明确场景，并指出该漏洞源于量子纠缠和对Mermin不等式的违反。然后，为了应对此威胁，提出了几种新的量子访问控制模型，并对其安全性、灵活性和效率进行了严格分析。", "result": "揭示了当经典的访问控制系统直接应用于量子环境时，会发生安全漏洞，该漏洞是由于量子纠缠和Mermin不等式的违反。论文提出了新的量子访问控制模型。", "conclusion": "量子纠缠对现有计算机系统与量子计算集成后的访问控制构成威胁。为了抵御这种威胁，需要采用新的量子访问控制模型。", "translation": "访问控制是计算机安全的基础，旨在防止对资源的未经授权访问。在本文中，我们研究了量子计算机系统中的访问控制。我们首次提出了一个明确的安全漏洞场景，即当一个经典安全的访问控制系统直接应用于量子环境时。该漏洞最终是由于量子力学允许纠缠现象并违反了Mermin不等式（著名的贝尔不等式的一个多方变体）。这揭示了如果现有计算机系统与量子计算集成，量子纠缠将对访问控制构成威胁。为了防范这种威胁，我们提出了几种新的量子访问控制模型，并严格分析了它们的安全性、灵活性和效率。", "summary": "本文研究了量子计算机系统中的访问控制问题，首次揭示了当经典访问控制系统直接应用于量子环境时，量子纠缠会导致安全漏洞。该漏洞源于量子力学中的纠缠现象及对Mermin不等式的违反。鉴于量子纠缠对访问控制构成的潜在威胁，作者提出了多种新型量子访问控制模型，并对其安全性、灵活性和效率进行了严格分析。", "keywords": "量子纠缠, 访问控制, 量子安全, Mermin不等式, 安全漏洞", "comments": "这篇论文具有创新性，它首次明确指出了量子纠缠对经典访问控制系统构成的具体威胁，填补了量子安全领域的一个空白。其重要性在于，随着量子计算技术的发展，现有安全机制可能不再适用，该研究为未来量子安全系统的设计提供了理论基础和解决方案。"}}
{"id": "2507.02256", "title": "Uncertainty-aware Reward Design Process", "authors": ["Yang Yang", "Xiaolu Zhou", "Bosong Ding", "Miao Xin"], "categories": ["cs.LG", "cs.RO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      34 pages, 9 figures", "url": "http://arxiv.org/abs/2507.02256v1", "summary": "Designing effective reward functions is a cornerstone of reinforcement\nlearning (RL), yet it remains a challenging process due to the inefficiencies\nand inconsistencies inherent in conventional reward engineering methodologies.\nRecent advances have explored leveraging large language models (LLMs) to\nautomate reward function design. However, their suboptimal performance in\nnumerical optimization often yields unsatisfactory reward quality, while the\nevolutionary search paradigm demonstrates inefficient utilization of simulation\nresources, resulting in prohibitively lengthy design cycles with\ndisproportionate computational overhead. To address these challenges, we\npropose the Uncertainty-aware Reward Design Process (URDP), a novel framework\nthat integrates large language models to streamline reward function design and\nevaluation in RL environments. URDP quantifies candidate reward function\nuncertainty based on self-consistency analysis, enabling simulation-free\nidentification of ineffective reward components while discovering novel reward\ncomponents. Furthermore, we introduce uncertainty-aware Bayesian optimization\n(UABO), which incorporates uncertainty estimation to significantly enhance\nhyperparameter configuration efficiency. Finally, we construct a bi-level\noptimization architecture by decoupling the reward component optimization and\nthe hyperparameter tuning. URDP orchestrates synergistic collaboration between\nthe reward logic reasoning of the LLMs and the numerical optimization strengths\nof the Bayesian Optimization. We conduct a comprehensive evaluation of URDP\nacross 35 diverse tasks spanning three benchmark environments. Our experimental\nresults demonstrate that URDP not only generates higher-quality reward\nfunctions but also achieves significant improvements in the efficiency of\nautomated reward design compared to existing approaches.", "comment": "34 pages, 9 figures", "pdf_url": "http://arxiv.org/pdf/2507.02256v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "不确定性感知奖励设计过程", "tldr": "提出URDP框架，结合LLM和贝叶斯优化，通过不确定性感知和分层优化，高效生成高质量的强化学习奖励函数。", "motivation": "强化学习中奖励函数设计挑战性大，传统方法效率低且不一致；大型语言模型（LLM）在数值优化上表现不佳，导致奖励质量不满意；进化搜索范式资源利用低效，设计周期长且计算开销大。", "method": "提出了不确定性感知奖励设计过程（URDP）框架，该框架整合了大型语言模型。URDP通过自洽性分析量化候选奖励函数的不确定性，从而能够无仿真地识别无效奖励组件并发现新组件。此外，引入了不确定性感知贝叶斯优化（UABO）以显著提高超参数配置效率。最后，构建了一个双层优化架构，解耦奖励组件优化和超参数调优，协同LLM的奖励逻辑推理能力和贝叶斯优化的数值优化优势。", "result": "在涵盖三个基准环境的35个不同任务上对URDP进行了全面评估。实验结果表明，URDP不仅生成了更高质量的奖励函数，而且与现有方法相比，在自动化奖励设计效率方面取得了显著提升。", "conclusion": "URDP通过有效结合LLM的逻辑推理能力和贝叶斯优化的数值优化优势，克服了现有奖励设计方法的局限性，在生成高质量奖励函数和提升设计效率方面表现出色。", "translation": "设计有效的奖励函数是强化学习（RL）的基石，但由于传统奖励工程方法固有的低效和不一致性，这仍然是一个具有挑战性的过程。最近的进展探索了利用大型语言模型（LLM）来自动化奖励函数设计。然而，LLM在数值优化方面表现不佳，常常导致奖励质量不尽如人意，而进化搜索范式则表现出对仿真资源利用效率低下，导致设计周期过长且计算开销过大。为了解决这些挑战，我们提出了不确定性感知奖励设计过程（URDP），这是一个新颖的框架，它整合了大型语言模型以简化强化学习环境中的奖励函数设计和评估。URDP基于自洽性分析量化候选奖励函数的不确定性，从而能够无仿真地识别无效的奖励组件，同时发现新的奖励组件。此外，我们引入了不确定性感知贝叶斯优化（UABO），它结合了不确定性估计，显著提高了超参数配置效率。最后，我们通过解耦奖励组件优化和超参数调优，构建了一个双层优化架构。URDP协调了LLM的奖励逻辑推理能力和贝叶斯优化的数值优化优势之间的协同合作。我们对URDP在涵盖三个基准环境的35个不同任务上进行了全面评估。我们的实验结果表明，与现有方法相比，URDP不仅生成了更高质量的奖励函数，而且在自动化奖励设计效率方面取得了显著提升。", "summary": "本论文提出了一种名为不确定性感知奖励设计过程（URDP）的新颖框架，旨在解决强化学习中奖励函数设计效率低和质量不佳的问题。URDP通过结合大型语言模型（LLM）和贝叶斯优化，量化奖励函数的不确定性以实现无仿真组件识别和新组件发现。它引入了不确定性感知贝叶斯优化（UABO）来提高超参数配置效率，并采用双层优化架构解耦组件优化和超参数调优。实验结果表明，URDP在生成高质量奖励函数和提升自动化设计效率方面均优于现有方法。", "keywords": "奖励设计, 强化学习, 大语言模型, 贝叶斯优化, 不确定性感知", "comments": "URDP的创新点在于其不确定性感知机制和双层优化架构，有效结合了LLM的符号推理能力和数值优化的优势，解决了自动化奖励设计中的效率和质量问题，对强化学习的奖励工程领域具有重要意义。"}}
{"id": "2507.02314", "title": "MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation", "authors": ["JaeHyuck Choi", "MinJun Kim", "JeHyeong Hong"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures", "url": "http://arxiv.org/abs/2507.02314v1", "summary": "Few-shot anomaly generation is emerging as a practical solution for\naugmenting the scarce anomaly data in industrial quality control settings. An\nideal generator would meet three demands at once, namely (i) keep the normal\nbackground intact, (ii) inpaint anomalous regions to tightly overlap with the\ncorresponding anomaly masks, and (iii) generate anomalous regions in a\nsemantically valid location, while still producing realistic, diverse\nappearances from only a handful of real examples. Existing diffusion-based\nmethods usually satisfy at most two of these requirements: global anomaly\ngenerators corrupt the background, whereas mask-guided ones often falter when\nthe mask is imprecise or misplaced. We propose MAGIC--Mask-guided inpainting\nwith multi-level perturbations and Context-aware alignment--to resolve all\nthree issues. At its core, MAGIC fine-tunes a Stable Diffusion inpainting\nbackbone that preserves normal regions and ensures strict adherence of the\nsynthesized anomaly to the supplied mask, directly addressing background\ncorruption and misalignment. To offset the diversity loss that fine-tuning can\ncause, MAGIC adds two complementary perturbation strategies: (i) Gaussian\nprompt-level perturbation applied during fine-tuning and inference that\nbroadens the global appearance of anomalies while avoiding low-fidelity textual\nappearances, and (ii) mask-guided spatial noise injection that enriches local\ntexture variations. Additionally, the context-aware mask alignment module forms\nsemantic correspondences and relocates masks so that every anomaly remains\nplausibly contained within the host object, eliminating out-of-boundary\nartifacts. Under a consistent identical evaluation protocol on the MVTec-AD\ndataset, MAGIC outperforms previous state-of-the-arts in downstream anomaly\ntasks.", "comment": "10 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.02314v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MAGIC：掩膜引导扩散修复与多级扰动和上下文感知对齐的少样本异常生成", "tldr": "本文提出了MAGIC，一种基于扩散的少样本异常生成方法，它能保持背景完整、精确修复异常区域并进行语义定位，性能优于现有方法。", "motivation": "由于工业质量控制中异常数据稀缺，少样本异常生成成为一个实用的解决方案。现有的基于扩散的方法未能同时满足保持正常背景、精确掩膜引导修复以及语义有效放置并生成逼真多样外观这三个关键需求。全局异常生成器会破坏背景，而掩膜引导的生成器在掩膜不精确或错位时表现不佳。", "method": "MAGIC通过微调一个Stable Diffusion修复骨干网络来保留正常区域并确保合成异常严格遵守提供的掩膜，从而解决背景损坏和错位问题。为弥补微调可能导致的多样性损失，MAGIC引入了两种扰动策略：(i) 在微调和推理过程中应用高斯提示级扰动以拓宽异常的全局外观；(ii) 掩膜引导的空间噪声注入以丰富局部纹理变化。此外，上下文感知掩膜对齐模块形成语义对应并重新定位掩膜，确保异常合理地包含在宿主对象中。", "result": "在MVTec-AD数据集上，MAGIC在下游异常任务中，在一致相同的评估协议下，性能优于以前的最新技术。", "conclusion": "MAGIC成功解决了少样本异常生成中背景完整性、掩膜精确性和语义有效性等挑战，并在基准数据集上取得了优于现有方法的性能。", "translation": "少样本异常生成正成为工业质量控制环境中稀缺异常数据增强的实用解决方案。一个理想的生成器需要同时满足三个要求：(i) 保持正常背景完好无损，(ii) 修复异常区域以与相应的异常掩膜紧密重叠，以及 (iii) 在语义有效的位置生成异常区域，同时仅从少量真实示例中生成逼真、多样的外观。现有的基于扩散的方法通常最多只能满足其中两个要求：全局异常生成器会破坏背景，而掩膜引导的生成器在掩膜不精确或错位时常常失效。我们提出了 MAGIC——掩膜引导修复与多级扰动和上下文感知对齐——以解决所有这三个问题。其核心是，MAGIC 微调了一个 Stable Diffusion 修复骨干网络，该网络保留正常区域并确保合成的异常严格遵守提供的掩膜，直接解决了背景损坏和错位问题。为了弥补微调可能导致的多样性损失，MAGIC 添加了两种互补的扰动策略：(i) 在微调和推理过程中应用的高斯提示级扰动，扩大了异常的全局外观，同时避免了低保真度的文本外观，以及 (ii) 掩膜引导的空间噪声注入，丰富了局部纹理变化。此外，上下文感知掩膜对齐模块形成语义对应并重新定位掩膜，使每个异常都合理地包含在宿主对象中，消除了超出边界的伪影。在 MVTec-AD 数据集上的一致相同评估协议下，MAGIC 在下游异常任务中优于以前的最新技术。", "summary": "本文提出了一种名为MAGIC的新型基于扩散的修复方法，用于少样本异常生成。该方法通过微调Stable Diffusion骨干网络来保留正常背景并确保异常区域与掩膜精确对齐，从而解决了现有方法的局限性。为增强生成多样性，MAGIC引入了高斯提示级扰动和掩膜引导的空间噪声注入。此外，上下文感知掩膜对齐模块确保异常在语义上有效放置。在MVTec-AD数据集上的评估显示，MAGIC在下游异常任务中显著优于现有最先进方法。", "keywords": "少样本异常生成, 扩散修复, 掩膜引导, Stable Diffusion, 异常检测", "comments": "MAGIC通过结合扩散修复与特定策略，创新性地解决了少样本异常生成中的常见挑战，特别是背景保留、掩膜精度、语义放置和多样性。其多级扰动和上下文感知对齐模块是关键创新点。"}}
{"id": "2507.02244", "title": "Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies", "authors": ["Fangzhou Shi", "Xiaopeng Ke", "Xinye Xiong", "Kexin Meng", "Chang Men", "Zhengdan Zhu"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02244v1", "summary": "The proliferation of ride-hailing aggregator platforms presents significant\ngrowth opportunities for ride-service providers by increasing order volume and\ngross merchandise value (GMV). On most ride-hailing aggregator platforms,\nservice providers that offer lower fares are ranked higher in listings and,\nconsequently, are more likely to be selected by passengers. This competitive\nranking mechanism creates a strong incentive for service providers to adopt\ncoupon strategies that lower prices to secure a greater number of orders, as\norder volume directly influences their long-term viability and sustainability.\nThus, designing an effective coupon strategy that can dynamically adapt to\nmarket fluctuations while optimizing order acquisition under budget constraints\nis a critical research challenge. However, existing studies in this area remain\nscarce.\n  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based\nsubsidy strategy framework designed to rapidly adapt to competitors' pricing\nadjustments. Our approach integrates two key techniques: Fast Competition\nAdaptation (FCA), which enables swift responses to dynamic price changes, and\nReinforced Lagrangian Adjustment (RLA), which ensures adherence to budget\nconstraints while optimizing coupon decisions on new price landscape.\nFurthermore, we introduce RideGym, the first dedicated simulation environment\ntailored for ride-hailing aggregators, facilitating comprehensive evaluation\nand benchmarking of different pricing strategies without compromising\nreal-world operational efficiency. Experimental results demonstrate that our\nproposed method consistently outperforms baseline approaches across diverse\nmarket conditions, highlighting its effectiveness in subsidy optimization for\nride-hailing service providers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02244v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "竞争压力下的订单获取：一种针对网约车补贴策略的快速自适应强化学习方法", "tldr": "本文提出FCA-RL，一种基于强化学习的网约车补贴策略框架，旨在快速适应竞争对手的定价调整，并通过引入RideGym模拟环境，在预算约束下优化订单获取，实验证明其优于现有方法。", "motivation": "网约车聚合平台上的竞争排名机制促使服务提供商通过优惠券降低价格以获取更多订单，然而，设计一种能在预算约束下动态适应市场波动并优化订单获取的有效优惠券策略是一个关键的研究挑战，现有研究在此领域稀缺。", "method": "我们提出了FCA-RL，一个新颖的基于强化学习的补贴策略框架。它结合了两种关键技术：快速竞争适应（FCA）以迅速响应动态价格变化，以及强化拉格朗日调整（RLA）以在优化优惠券决策的同时遵守预算约束。此外，我们引入了RideGym，第一个专为网约车聚合平台设计的模拟环境，用于评估和基准测试不同的定价策略。", "result": "实验结果表明，我们提出的方法在各种市场条件下始终优于基线方法。", "conclusion": "FCA-RL框架通过快速适应竞争和遵守预算约束，有效优化了网约车服务提供商的补贴策略，并且RideGym模拟环境为未来研究提供了有价值的工具。", "translation": "网约车聚合平台的普及为网约车服务提供商带来了巨大的增长机会，增加了订单量和商品总值（GMV）。在大多数网约车聚合平台上，提供更低票价的服务提供商在列表中排名更高，因此更容易被乘客选择。这种竞争性排名机制强烈激励服务提供商采用优惠券策略来降低价格以获得更多订单，因为订单量直接影响其长期生存能力和可持续性。因此，设计一种能够动态适应市场波动同时在预算约束下优化订单获取的有效优惠券策略是一个关键的研究挑战。然而，该领域的现有研究仍然稀缺。\n为了弥补这一空白，我们提出了FCA-RL，一种新颖的基于强化学习的补贴策略框架，旨在快速适应竞争对手的定价调整。我们的方法整合了两种关键技术：快速竞争适应（FCA），它能够迅速响应动态价格变化；以及强化拉格朗日调整（RLA），它确保在优化新价格环境下的优惠券决策时遵守预算约束。此外，我们引入了RideGym，第一个专为网约车聚合平台量身定制的模拟环境，有助于在不影响实际运营效率的情况下全面评估和基准测试不同的定价策略。实验结果表明，我们提出的方法在各种市场条件下始终优于基线方法，突出了其在网约车服务提供商补贴优化方面的有效性。", "summary": "本文针对网约车平台竞争环境下订单获取的挑战，提出了一种名为FCA-RL的强化学习补贴策略框架。该框架通过快速竞争适应（FCA）技术响应价格变化，并利用强化拉格朗日调整（RLA）在预算约束下优化优惠券决策。为评估策略，文章还引入了首个网约车聚合器专用模拟环境RideGym。实验结果表明，FCA-RL在多种市场条件下均优于现有基线方法，有效提升了网约车服务提供商的补贴优化能力。", "keywords": "强化学习, 补贴策略, 网约车, 竞争适应, 模拟环境", "comments": "该论文的创新点在于提出了FCA-RL这一结合了快速适应能力和预算约束优化的强化学习框架，以解决网约车竞争中的补贴策略问题。同时，引入RideGym这一专用模拟环境，为该领域的研究和策略评估提供了重要的工具，填补了现有研究的空白。其重要性体现在为网约车服务提供商提供了更高效的订单获取和补贴管理方案。"}}
{"id": "2507.02804", "title": "Multimodal Mathematical Reasoning with Diverse Solving Perspective", "authors": ["Wenhao Shi", "Zhiqiang Hu", "Yi Bin", "Yang Yang", "See-Kiong Ng", "Heng Tao Shen"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      8 pages", "url": "http://arxiv.org/abs/2507.02804v1", "summary": "Recent progress in large-scale reinforcement learning (RL) has notably\nenhanced the reasoning capabilities of large language models (LLMs), especially\nin mathematical domains. However, current multimodal LLMs (MLLMs) for\nmathematical reasoning often rely on one-to-one image-text pairs and\nsingle-solution supervision, overlooking the diversity of valid reasoning\nperspectives and internal reflections. In this work, we introduce MathV-DP, a\nnovel dataset that captures multiple diverse solution trajectories for each\nimage-question pair, fostering richer reasoning supervision. We further propose\nQwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and\nenhanced via group relative policy optimization (GRPO), a rule-based RL\napproach that integrates correctness discrimination and diversity-aware reward\nfunctions. Our method emphasizes learning from varied reasoning perspectives\nand distinguishing between correct yet distinct solutions. Extensive\nexperiments on the MathVista's minitest and Math-V benchmarks demonstrate that\nQwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and\ngenerative diversity, highlighting the importance of incorporating diverse\nperspectives and reflective reasoning in multimodal mathematical reasoning.", "comment": "8 pages", "pdf_url": "http://arxiv.org/pdf/2507.02804v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "多模态数学推理与多样化解题视角", "tldr": "当前的多模态大语言模型（MLLM）在数学推理中缺乏多样化的视角。本文介绍了MathV-DP数据集和Qwen-VL-DP模型，该模型利用强化学习从多个解决方案轨迹中学习，从而提高了准确性和多样性。", "motivation": "当前用于数学推理的多模态大语言模型（MLLM）通常依赖于一对一的图像-文本对和单一解决方案监督，忽略了有效推理视角和内部反思的多样性。", "method": "引入了MathV-DP数据集，其中包含每个图像-问题对的多个多样化解决方案轨迹。提出了Qwen-VL-DP模型（基于Qwen-VL构建），通过监督学习进行微调，并通过群组相对策略优化（GRPO）进行增强，GRPO是一种基于规则的强化学习方法，结合了正确性判别和多样性感知奖励函数。该方法强调从不同的推理视角学习，并区分正确但不同的解决方案。", "result": "Qwen-VL-DP在MathVista的minitest和Math-V基准测试中，在准确性和生成多样性方面显著优于先前的基础MLLM。", "conclusion": "在多模态数学推理中，纳入多样化视角和反思性推理非常重要，Qwen-VL-DP的性能证明了这一点。", "translation": "大规模强化学习（RL）的最新进展显著增强了大型语言模型（LLM）的推理能力，尤其是在数学领域。然而，当前用于数学推理的多模态LLM（MLLM）通常依赖于一对一的图像-文本对和单一解决方案监督，忽略了有效推理视角和内部反思的多样性。在这项工作中，我们引入了MathV-DP，这是一个新颖的数据集，它为每个图像-问题对捕获了多个多样化的解决方案轨迹，从而促进了更丰富的推理监督。我们进一步提出了Qwen-VL-DP，这是一个基于Qwen-VL构建的模型，通过监督学习进行微调，并通过群组相对策略优化（GRPO）得到增强，GRPO是一种基于规则的RL方法，它结合了正确性判别和多样性感知奖励函数。我们的方法强调从不同的推理视角学习，并区分正确但不同的解决方案。在MathVista的minitest和Math-V基准测试上的大量实验表明，Qwen-VL-DP在准确性和生成多样性方面显著优于先前的基础MLLM，突出了在多模态数学推理中纳入多样化视角和反思性推理的重要性。", "summary": "本文解决了当前多模态大语言模型（MLLM）在数学推理中缺乏多样化视角的问题。它引入了MathV-DP，一个包含多个解决方案轨迹的数据集，以及Qwen-VL-DP，一个通过监督学习微调并由GRPO（一种考虑正确性和多样性的强化学习方法）增强的MLLM。实验表明，Qwen-VL-DP在准确性和生成多样性方面超越了之前的MLLM，强调了多样化推理的价值。", "keywords": "多模态推理, 数学推理, 大语言模型, 多样化视角, 强化学习", "comments": "该论文通过关注推理路径的多样性而非单一解决方案监督，解决了当前MLLM的一个关键局限性。引入新的数据集（MathV-DP）和定制的强化学习方法（GRPO）来训练Qwen-VL-DP是创新的。这项工作强调了模仿人类多样化问题解决策略的重要性，这可能导致更强大和多功能的数学推理模型。"}}
{"id": "2507.02844", "title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection", "authors": ["Ziqi Miao", "Yi Ding", "Lijun Li", "Jing Shao"], "categories": ["cs.CV", "cs.CL", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2507.02844v1", "summary": "With the emergence of strong visual-language capabilities, multimodal large\nlanguage models (MLLMs) have demonstrated tremendous potential for real-world\napplications. However, the security vulnerabilities exhibited by the visual\nmodality pose significant challenges to deploying such models in open-world\nenvironments. Recent studies have successfully induced harmful responses from\ntarget MLLMs by encoding harmful textual semantics directly into visual inputs.\nHowever, in these approaches, the visual modality primarily serves as a trigger\nfor unsafe behavior, often exhibiting semantic ambiguity and lacking grounding\nin realistic scenarios. In this work, we define a novel setting: visual-centric\njailbreak, where visual information serves as a necessary component in\nconstructing a complete and realistic jailbreak context. Building on this\nsetting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates\ncontextual dialogue using four distinct visual-focused strategies, dynamically\ngenerating auxiliary images when necessary to construct a visual-centric\njailbreak scenario. To maximize attack effectiveness, it incorporates automatic\ntoxicity obfuscation and semantic refinement to produce a final attack prompt\nthat reliably triggers harmful responses from the target black-box MLLMs.\nSpecifically, VisCo achieves a toxicity score of 4.78 and an Attack Success\nRate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming\nthe baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The\ncode is available at https://github.com/Dtc7w3PQ/Visco-Attack.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.02844v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "视觉上下文攻击：通过图像驱动的上下文注入越狱多模态大型语言模型", "tldr": "VisCo是一种新颖的视觉上下文攻击方法，通过将视觉信息作为越狱上下文的关键组成部分，更有效地越狱多模态大型语言模型（MLLMs），在对抗GPT-4o的测试中显著优于基线方法。", "motivation": "多模态大型语言模型（MLLMs）在现实世界应用中潜力巨大，但其视觉模态存在安全漏洞。现有攻击方法通过视觉输入诱导有害响应，但视觉仅作为触发器，语义模糊且缺乏真实性。本文旨在定义一种新的“以视觉为中心的越狱”设置，其中视觉信息是构建完整、真实越狱上下文的必要组成部分。", "method": "本文提出了VisCo（视觉上下文）攻击，定义了“以视觉为中心的越狱”概念。VisCo利用四种独特的以视觉为中心的策略构建上下文对话，并在必要时动态生成辅助图像。它还结合了自动毒性混淆和语义细化，以生成最终的攻击提示，从而可靠地触发黑盒MLLMs的有害响应。", "result": "VisCo在MM-SafetyBench上针对GPT-4o实现了4.78的毒性评分和85%的攻击成功率（ASR）。这显著优于基线方法，基线的毒性评分为2.48，ASR为22.2%。", "conclusion": "本文成功定义并提出了一种新颖的“以视觉为中心的越狱”方法——VisCo攻击，该方法通过整合视觉信息作为核心上下文，显著提高了对黑盒多模态大型语言模型（如GPT-4o）的越狱成功率和有害响应触发能力，揭示了MLLMs在视觉模态上的安全漏洞。", "translation": "随着强大的视觉-语言能力的出现，多模态大型语言模型（MLLMs）在现实世界应用中展现出巨大的潜力。然而，视觉模态所展现的安全漏洞对在开放世界环境中部署此类模型提出了重大挑战。最近的研究通过将有害的文本语义直接编码到视觉输入中，成功地诱导目标MLLMs产生有害响应。然而，在这些方法中，视觉模态主要作为不安全行为的触发器，通常表现出语义模糊性，并且缺乏在现实场景中的基础。在这项工作中，我们定义了一个新颖的设置：以视觉为中心的越狱，其中视觉信息是构建完整和真实越狱上下文的必要组成部分。在此设置的基础上，我们提出了VisCo（视觉上下文）攻击。VisCo利用四种独特的以视觉为中心的策略来构建上下文对话，并在必要时动态生成辅助图像，以构建一个以视觉为中心的越狱场景。为了最大限度地提高攻击效果，它结合了自动毒性混淆和语义细化，以生成最终的攻击提示，从而可靠地触发目标黑盒MLLMs的有害响应。具体来说，VisCo在MM-SafetyBench上针对GPT-4o实现了4.78的毒性评分和85%的攻击成功率（ASR），显著优于基线（其毒性评分为2.48，ASR为22.2%）。代码可在https://github.com/Dtc7w3PQ/Visco-Attack获得。", "summary": "本文提出了一种名为VisCo（视觉上下文）的新型攻击方法，旨在通过图像驱动的上下文注入来越狱多模态大型语言模型（MLLMs）。与以往仅将视觉作为触发器的方法不同，VisCo定义并实现了“以视觉为中心的越狱”，将视觉信息作为构建完整且真实越狱上下文的必要组成部分。VisCo采用四种视觉聚焦策略，动态生成辅助图像，并结合自动毒性混淆和语义细化来生成攻击提示。实验结果显示，VisCo在MM-SafetyBench上针对GPT-4o取得了85%的攻击成功率和4.78的毒性评分，显著优于基线。", "keywords": "视觉上下文攻击, MLLMs, 越狱, 视觉安全, 漏洞", "comments": "VisCo攻击的创新点在于其定义了“以视觉为中心的越狱”概念，并使视觉信息成为攻击上下文的必要组成部分，而非简单的触发器。这种方法提高了攻击的真实性和有效性，揭示了MLLMs在处理复杂视觉上下文时的潜在安全漏洞。这项工作对于理解和缓解MLLMs的视觉模态安全风险具有重要意义。"}}
{"id": "2507.02207", "title": "Public perspectives on the design of fusion energy facilities", "authors": ["Nathan Kawamoto", "Daniel Hoover", "Jonathan Xie", "Jacob Walters", "Katie Snyder", "Aditi Verma"], "categories": ["physics.soc-ph", "cs.HC", "physics.ed-ph", "physics.plasm-ph"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "Comments:      33 pages", "url": "http://arxiv.org/abs/2507.02207v1", "summary": "As fusion energy technologies approach demonstration and commercial\ndeployment, understanding public perspectives on future fusion facilities will\nbe critical for achieving social license, especially because fusion energy\nfacilities, unlike large fission reactors, may be sited in closer proximity to\npeople and communities, due to distinct regulatory frameworks. In a departure\nfrom the 'decide-announce-defend' approach typically used to site energy\ninfrastructure, we develop a participatory design methodology for\ncollaboratively designing fusion energy facilities with prospective host\ncommunities. We present here our findings from a participatory design workshop\nthat brought together 22 community participants and 34 engineering students.\nOur analysis of the textual and visual data from this workshop shows a range of\ndesign values and decision-making criteria with 'integrity' and 'respect'\nranking highest among values and 'economic benefits' and 'environmental\nprotection/safety' ranking highest among decision-making criteria. Salient\ndesign themes that emerge across facility concepts include connecting the\nhistory and legacy of the community to the design of the facility, care for\nworkers, transparency and access to the facility, and health and safety of the\nhost community. Participants reported predominantly positive sentiments,\nexpressing joy and surprise as the workshop progressed from learning about\nfusion to designing the hypothetical facility. Our findings suggest that\ncarrying out participatory design in the early stages of technology development\ncan invite and make concrete public hopes and concerns, improve understanding\nof, and curiosity about, an emerging technology, build toward social license,\nand inform context-specific development of fusion energy facilities.", "comment": "33 pages", "pdf_url": "http://arxiv.org/pdf/2507.02207v1", "cate": "physics.soc-ph", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "公众对聚变能设施设计的看法", "tldr": "本研究通过参与式设计研讨会，探讨了公众对未来聚变能设施的看法，发现早期公众参与有助于获得社会许可并指导设施设计。", "motivation": "随着聚变能技术接近示范和商业部署，了解公众对未来聚变设施的看法对于获得社会许可至关重要，特别是考虑到聚变设施可能比裂变反应堆更靠近社区。", "method": "本研究开发了一种参与式设计方法，通过与未来潜在的社区合作，共同设计聚变能设施。具体通过一个参与式设计研讨会，汇集了22名社区参与者和34名工程专业学生，并分析了研讨会的文本和视觉数据。", "result": "分析显示，设计价值观和决策标准范围广泛，其中“正直”和“尊重”在价值观中排名最高，“经济效益”和“环境保护/安全”在决策标准中排名最高。突出的设计主题包括将社区历史与设施设计相结合、关爱工人、设施透明度和可及性，以及宿主社区的健康和安全。参与者普遍表达了积极情绪，并在研讨会中从了解聚变到设计假设设施的过程中表现出喜悦和惊喜。", "conclusion": "研究结果表明，在技术开发的早期阶段进行参与式设计可以引发并具体化公众的希望和担忧，增进对新兴技术的理解和好奇心，有助于建立社会许可，并为聚变能设施的特定情境开发提供信息。", "translation": "随着聚变能技术接近示范和商业部署，了解公众对未来聚变设施的看法对于获得社会许可至关重要，特别是考虑到聚变能设施与大型裂变反应堆不同，由于独特的监管框架，它们可能更靠近人群和社区。我们摒弃了能源基础设施选址通常采用的“决定-宣布-捍卫”方法，开发了一种参与式设计方法，与潜在的宿主社区协作设计聚变能设施。我们在此展示了我们从一个参与式设计研讨会中获得的结果，该研讨会汇集了22名社区参与者和34名工程专业学生。我们对该研讨会的文本和视觉数据分析显示了一系列设计价值观和决策标准，其中“正直”和“尊重”在价值观中排名最高，“经济效益”和“环境保护/安全”在决策标准中排名最高。设施概念中出现的显著设计主题包括将社区的历史和遗产与设施设计联系起来、关爱工人、设施的透明度和可及性，以及宿主社区的健康和安全。参与者报告了主要积极的情绪，随着研讨会从了解聚变到设计假设设施的进展，他们表达了喜悦和惊喜。我们的研究结果表明，在技术开发的早期阶段进行参与式设计可以引发并具体化公众的希望和担忧，增进对新兴技术的理解和好奇心，有助于建立社会许可，并为聚变能设施的特定情境开发提供信息。", "summary": "本研究旨在了解公众对未来聚变能设施的看法，以期在商业部署前获得社会许可。研究团队采用参与式设计方法，与潜在社区成员和工程学生共同探讨设施设计。通过分析研讨会数据，发现公众高度重视“正直”、“尊重”、“经济效益”和“环境保护/安全”等设计价值观和决策标准。核心设计主题包括社区历史联结、工人关怀、透明度和社区健康安全。研究表明，早期公众参与能有效提升公众对新兴技术的理解与接受度，为聚变设施的开发提供宝贵参考，并有助于建立社会许可。", "keywords": "聚变能, 公众视角, 参与式设计, 社会许可, 设施设计", "comments": "这篇论文的创新之处在于其采用了“参与式设计”方法，取代了传统的“决定-宣布-捍卫”模式，将公众意见融入到新兴技术（聚变能）的早期设计阶段。这对于获得社会许可和确保未来能源基础设施的顺利部署至关重要。研究结果为聚变能设施的选址和设计提供了宝贵的公众视角，强调了非技术因素如信任、透明度和社区福祉的重要性。"}}
{"id": "2507.02663", "title": "Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models", "authors": ["Yongjiang Liu", "Haoxi Li", "Xiaosong Ma", "Jie Zhang", "Song Guo"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      21 pages, 18 figures", "url": "http://arxiv.org/abs/2507.02663v1", "summary": "Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities\nin handling complex reasoning tasks, but are hindered by excessive\noverthinking. To explore its essence, our empirical analysis reveals that LRMs\nare primarily limited to recognizing task properties (i.e., difficulty levels)\nlike humans before solving the problem, leading to a one-size-fits-all\nreasoning process. Inspired by this, a pressing and natural question emerges:\nCan we bootstrap such ability to further alleviate the overthinking phenomenon\nin LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage\nfine-tuning strategy that progressively inspires LRMs' difficulty cognition and\nredundancy cognition. First, we introduce difficulty-hypnosis in the prefixes\nof model outputs to intervene in the internal reasoning trajectory. Combined\nwith a heterogeneous short and long reasoning dataset, the trained model\nenhances its sensitivity to task difficulty, enabling native, differentiated\nreasoning strategies across various tasks. Second, we further extend\nredundancy-hypnosis to the internal reasoning process, guiding the model to\nidentify redundant structures within the reasoning steps and generate more\nconcise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that\nTH2T significantly reduces inference costs (more than 70% on easy tasks and 40%\non hard tasks) while maintaining performance stability. The resulting outputs\nexhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,\nreflection).", "comment": "21 pages, 18 figures", "pdf_url": "http://arxiv.org/pdf/2507.02663v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "思考如何思考：通过大型推理模型中的自主难度认知减轻过度思考", "tldr": "提出TH2T，一种两阶段微调策略，通过难度和冗余认知，帮助大型推理模型减少过度思考，显著降低推理成本同时保持性能。", "motivation": "现有大型推理模型(LRMs)在复杂推理任务中表现出过度思考的问题，主要原因是它们缺乏对任务难度级别的认知，导致采用“一刀切”的推理过程。", "method": "提出“思考如何思考”(TH2T)，一种新颖的两阶段微调策略。第一阶段是“难度催眠”，通过在模型输出前缀中引入难度提示，并结合异构的短推理和长推理数据集，使模型对任务难度更敏感，从而实现差异化推理。第二阶段是“冗余催眠”，进一步指导模型识别推理步骤中的冗余结构，生成更简洁的推理输出。", "result": "在7B/14B/32B模型上的实验表明，TH2T显著降低了推理成本（在简单任务上超过70%，在困难任务上超过40%），同时保持了性能的稳定性。模型输出表现出清晰的难度感知能力和减少的冗余。", "conclusion": "TH2T通过引入难度和冗余认知，有效解决了大型推理模型过度思考的问题，显著提升了推理效率，同时不牺牲性能。", "translation": "近期大型推理模型（LRMs）在处理复杂推理任务方面展现出卓越能力，但却受到过度思考的阻碍。为了探究其本质，我们的实证分析揭示，LRMs主要受限于在解决问题前像人类一样识别任务属性（即难度级别），这导致了“一刀切”的推理过程。受此启发，一个紧迫而自然的问题浮现：我们能否引导这种能力以进一步缓解LRMs中的过度思考现象？在本文中，我们提出了“思考如何思考”（TH2T），一种新颖的两阶段微调策略，它逐步激发LRMs的难度认知和冗余认知。首先，我们在模型输出的前缀中引入了难度催眠，以干预内部推理轨迹。结合异构的短推理和长推理数据集，训练后的模型增强了对任务难度的敏感性，从而在各种任务中实现原生的、差异化的推理策略。其次，我们进一步将冗余催眠扩展到内部推理过程，引导模型识别推理步骤中的冗余结构并生成更简洁的推理输出。在7B/14B/32B模型上的实验表明，TH2T显著降低了推理成本（在简单任务上超过70%，在困难任务上超过40%），同时保持了性能稳定性。结果输出展现出清晰的难度感知能力和减少的冗余（例如，反射）。", "summary": "本文提出了一种名为“思考如何思考”（TH2T）的两阶段微调策略，旨在解决大型推理模型（LRMs）因缺乏任务难度认知而导致的过度思考问题。TH2T通过引入“难度催眠”和“冗余催眠”，分别使模型能够识别任务难度并采取差异化推理策略，以及识别并减少推理过程中的冗余。实验证明，TH2T在保持性能稳定的前提下，显著降低了LRMs的推理成本，并使其输出更具难度感知和简洁性。", "keywords": "大型推理模型, 过度思考, 难度认知, 冗余认知, 微调策略", "comments": "这篇论文通过引入“难度认知”和“冗余认知”的概念，为解决大型语言模型中的“过度思考”现象提供了一个新颖且有效的解决方案。其创新点在于将人类解决问题前对难度的判断引入到模型的训练中，并通过两阶段的“催眠”策略进行干预。这不仅提高了模型的推理效率，显著降低了计算成本，而且使其推理过程更加智能和灵活。该方法对于优化大型模型在实际应用中的部署和性能具有重要意义。"}}
{"id": "2507.02316", "title": "Are Synthetic Videos Useful? A Benchmark for Retrieval-Centric Evaluation of Synthetic Videos", "authors": ["Zecheng Zhao", "Selena Song", "Tong Chen", "Zhi Chen", "Shazia Sadiq", "Yadan Luo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages, 10 figures", "url": "http://arxiv.org/abs/2507.02316v1", "summary": "Text-to-video (T2V) synthesis has advanced rapidly, yet current evaluation\nmetrics primarily capture visual quality and temporal consistency, offering\nlimited insight into how synthetic videos perform in downstream tasks such as\ntext-to-video retrieval (TVR). In this work, we introduce SynTVA, a new dataset\nand benchmark designed to evaluate the utility of synthetic videos for building\nretrieval models. Based on 800 diverse user queries derived from MSRVTT\ntraining split, we generate synthetic videos using state-of-the-art T2V models\nand annotate each video-text pair along four key semantic alignment dimensions:\nObject \\& Scene, Action, Attribute, and Prompt Fidelity. Our evaluation\nframework correlates general video quality assessment (VQA) metrics with these\nalignment scores, and examines their predictive power for downstream TVR\nperformance. To explore pathways of scaling up, we further develop an\nAuto-Evaluator to estimate alignment quality from existing metrics. Beyond\nbenchmarking, our results show that SynTVA is a valuable asset for dataset\naugmentation, enabling the selection of high-utility synthetic samples that\nmeasurably improve TVR outcomes. Project page and dataset can be found at\nhttps://jasoncodemaker.github.io/SynTVA/.", "comment": "7 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.02316v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "合成视频有用吗？一个以检索为中心的合成视频评估基准", "tldr": "该研究引入SynTVA，一个用于评估合成视频在文本到视频检索任务中效用的新数据集和基准，并证明其在数据集增强方面的价值。", "motivation": "当前文本到视频（T2V）合成的评估指标主要关注视觉质量和时间一致性，但对于合成视频在文本到视频检索（TVR）等下游任务中的表现提供的信息有限。", "method": "研究引入了SynTVA数据集和基准，基于MSRVTT训练集的800个用户查询生成合成视频，并沿对象与场景、动作、属性和提示忠实度四个语义对齐维度对视频-文本对进行标注。评估框架将通用视频质量评估（VQA）指标与这些对齐分数相关联，并检查它们对下游TVR性能的预测能力。此外，开发了一个自动评估器，以根据现有指标估计对齐质量。", "result": "SynTVA被证明是数据集增强的宝贵资产，能够选择具有高实用性的合成样本，从而显著改善TVR结果。", "conclusion": "该研究提出了一个以检索为中心的合成视频评估基准SynTVA，并证明了合成视频在文本到视频检索任务中的实用性，尤其是在数据集增强方面。", "translation": "文本到视频（T2V）合成技术发展迅速，但当前的评估指标主要捕捉视觉质量和时间一致性，对于合成视频在文本到视频检索（TVR）等下游任务中的表现提供的信息有限。在这项工作中，我们引入了SynTVA，一个旨在评估合成视频在构建检索模型方面实用性的新数据集和基准。基于MSRVTT训练集中派生的800个多样化用户查询，我们使用最先进的T2V模型生成合成视频，并沿着四个关键语义对齐维度（对象与场景、动作、属性和提示忠实度）对每个视频-文本对进行标注。我们的评估框架将通用视频质量评估（VQA）指标与这些对齐分数相关联，并检查它们对下游TVR性能的预测能力。为了探索扩展的途径，我们进一步开发了一个自动评估器，以根据现有指标估计对齐质量。除了基准测试之外，我们的结果表明SynTVA是数据集增强的宝贵资产，能够选择高实用性的合成样本，从而显著改善TVR结果。项目页面和数据集可在https://jasoncodemaker.github.io/SynTVA/找到。", "summary": "该研究针对当前文本到视频（T2V）合成评估指标在下游任务（如文本到视频检索，TVR）中效用不足的问题，提出了SynTVA数据集和基准。SynTVA通过生成和标注大量合成视频，并从语义对齐角度进行评估，旨在衡量合成视频对构建检索模型的贡献。研究结果表明，SynTVA不仅提供了新的评估框架，还能有效用于数据集增强，通过选择高质量合成样本来提升TVR性能。", "keywords": "合成视频, 文本到视频检索, 评估基准, SynTVA, 数据集增强", "comments": "这篇论文的创新点在于提出了一个以“检索效用”为核心的合成视频评估范式，而非仅仅关注视觉质量。SynTVA数据集和自动评估器的引入，为评估和利用合成视频在实际下游任务中的价值提供了新的工具和视角，对于推动文本到视频生成技术在实际应用中的发展具有重要意义。"}}
{"id": "2507.02764", "title": "Terahertz Chip-Scale Meta-Networks with LSPR Routing: A Theoretical Framework", "authors": ["Maryam Khodadadi", "Hamidreza Taghvaee", "Pei Xiao", "Gabriele Gradoni", "Mohsen Khalily"], "categories": ["physics.optics", "eess.SP", "physics.app-ph"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02764v1", "summary": "Efficient chip-scale interconnects are essential for modern\nmicroelectronic-photonic systems, supporting high bandwidth and low-latency\nprocessing. Traditional wired links face high resistivity and latency, while\nmillimeter-wave wireless solutions suffer from bandwidth congestion and\ninterference. Terahertz (THz) plasmonic communication, based on surface plasmon\npolaritons (SPPs), offers high data rates and broad bandwidth, and is\ncompatible with nanophotonic platforms. This work introduces a Binary\nField-Driven Meta-Routing Method supported by a semi-analytical framework that\nmodels the tunable interaction between THz plasmonic phenomena and graphene's\nelectromagnetic properties. By modulating graphene's impedance, the method\nenables dynamic coupling and routing of localized surface plasmon resonances\n(LSPRs) across a meta-network, facilitating real-time beam steering in\nchip-scale systems. Combining analytical conductivity models, coupled-mode\ntheory, and algorithmic control, the approach enables predictive configuration\nof LSPR-based steering in reconfigurable graphene metasurfaces. Four meta-pixel\nantenna configurations Y-MetaRouter, MetaSwitcher, Penta-MetaEmitter, and\nCP-MetaCore are designed to support unidirectional radiation, bi-directional\nsteering, frequency-driven transitions, and circular polarization,\nrespectively. Chemical potential modulation creates reconfigurable LSPR\npathways and virtual SPP channels. A Coupled-Mode Theory for Field-Driven LSPR\nMeta-Networks is proposed to model current distributions and predict far-field\ncharacteristics. Results show strong agreement between theory and full-wave\nsimulations. A point-to-point meta-wireless link is analyzed, demonstrating\nscalability for low-latency, high-performance THz communication in WiNoC and\nchiplet applications. System-level metrics confirm feasibility for\nspace-constrained, high-speed interconnects.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02764v1", "cate": "physics.optics", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "太赫兹芯片级超网络与LSPR路由：一个理论框架", "tldr": "该研究提出了一种基于石墨烯调制和局部表面等离子体共振（LSPR）路由的太赫兹芯片级超网络，以实现高性能、低延迟的片上互连。", "motivation": "现代微电子-光子系统需要高效的芯片级互连来支持高带宽和低延迟处理。传统的有线连接面临高电阻率和延迟问题，而毫米波无线解决方案则受限于带宽拥塞和干扰。", "method": "该工作引入了一种二进制场驱动的超路由方法，该方法由一个半分析框架支持，该框架模拟了太赫兹等离子体现象与石墨烯电磁特性之间的可调谐相互作用。通过调制石墨烯的阻抗，该方法能够动态耦合和路由超网络中的局部表面等离子体共振（LSPR），从而实现芯片级系统中的实时波束转向。结合分析电导率模型、耦合模理论和算法控制，该方法能够预测可重构石墨烯超表面中基于LSPR的转向配置。设计了四种超像素天线配置：Y-MetaRouter、MetaSwitcher、Penta-MetaEmitter和CP-MetaCore，分别用于支持单向辐射、双向转向、频率驱动转换和圆偏振。提出了用于场驱动LSPR超网络的耦合模理论，以模拟电流分布并预测远场特性。", "result": "理论与全波仿真结果显示出高度一致性。对点对点超无线链路进行了分析，证明了其在WiNoC和芯片应用中实现低延迟、高性能太赫兹通信的可扩展性。系统级指标证实了其在空间受限、高速互连中的可行性。", "conclusion": "该研究提出并验证了一种基于石墨烯调制的太赫兹芯片级超网络理论框架，该框架通过LSPR路由实现了高性能、可扩展的片上互连，有望解决现有互连的局限性。", "translation": "高效的芯片级互连对于现代微电子-光子系统至关重要，它们支持高带宽和低延迟处理。传统的有线链路面临高电阻率和延迟问题，而毫米波无线解决方案则受到带宽拥塞和干扰的影响。基于表面等离子体激元（SPPs）的太赫兹（THz）等离子体通信提供了高数据速率和宽带宽，并且与纳米光子平台兼容。这项工作引入了一种二进制场驱动的超路由方法，该方法由一个半分析框架支持，该框架模拟了太赫兹等离子体现象与石墨烯电磁特性之间的可调谐相互作用。通过调制石墨烯的阻抗，该方法能够动态耦合和路由超网络中的局部表面等离子体共振（LSPRs），从而促进芯片级系统中的实时波束转向。结合分析电导率模型、耦合模理论和算法控制，该方法能够预测可重构石墨烯超表面中基于LSPR的转向配置。设计了四种超像素天线配置：Y-MetaRouter、MetaSwitcher、Penta-MetaEmitter和CP-MetaCore，分别用于支持单向辐射、双向转向、频率驱动转换和圆偏振。化学势调制创建了可重构的LSPR路径和虚拟SPP通道。提出了用于场驱动LSPR超网络的耦合模理论，以模拟电流分布并预测远场特性。结果显示理论与全波仿真之间存在高度一致性。分析了点对点超无线链路，证明了其在WiNoC和芯片应用中实现低延迟、高性能太赫兹通信的可扩展性。系统级指标证实了其在空间受限、高速互连中的可行性。", "summary": "该论文提出了一个用于太赫兹芯片级超网络的理论框架，利用石墨烯的电磁特性和局部表面等离子体共振（LSPR）路由来实现高效的片上互连。通过调制石墨烯阻抗，实现了LSPR的动态耦合和路由，支持实时波束转向。文中设计了多种超像素天线配置，并提出了耦合模理论来预测其特性。理论与仿真结果一致，证明了该技术在低延迟、高带宽太赫兹通信方面的可行性和可扩展性。", "keywords": "太赫兹通信, 芯片级互连, 局部表面等离子体共振, 石墨烯, 超网络", "comments": "这项研究的创新之处在于提出了一个基于石墨烯调制的太赫兹芯片级超网络理论框架，通过LSPR路由解决了传统互连的带宽和延迟问题。其将等离子体通信、超材料和可重构技术相结合，为未来的高性能片上通信提供了新的方向。该理论框架的提出和验证为芯片级太赫兹通信的实际应用奠定了基础，具有重要的理论和工程意义。"}}
{"id": "2507.02822", "title": "SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model", "authors": ["Wencheng Zhang", "Shiqin Qiao", "Lingjie Luo", "Yinfeng Li", "Chuanyang Zheng", "Qian Xu", "Meng Li", "Yong Gui", "Yijun He", "Jianing Qiu", "Jindong Hong", "Jiankai Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02822v1", "summary": "With the widespread adoption of large language models (LLMs) in practical\napplications, selecting an appropriate model requires balancing not only\nperformance but also operational cost. The emergence of reasoning-capable\nmodels has further widened the cost gap between \"thinking\" (high reasoning) and\n\"non-thinking\" (fast, low-cost) modes. In this work, we reveal that\napproximately 58% of medical questions can be accurately answered by the\nnon-thinking mode alone, without requiring the high-cost reasoning process.\nThis highlights a clear dichotomy in problem complexity and suggests that\ndynamically routing queries to the appropriate mode based on complexity could\noptimize accuracy, cost-efficiency, and overall user experience. Based on this,\nwe further propose SynapseRoute, a machine learning-based dynamic routing\nframework that intelligently assigns input queries to either thinking or\nnon-thinking modes. Experimental results on several medical datasets\ndemonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.\n0.8272) compared to the thinking mode alone but also reduces inference time by\n36.8% and token consumption by 39.66%. Importantly, qualitative analysis\nindicates that over-reasoning on simpler queries can lead to unnecessary delays\nand even decreased accuracy, a pitfall avoided by our adaptive routing.\nFinally, this work further introduces the Accuracy-Inference-Token (AIT) index\nto comprehensively evaluate the trade-offs among accuracy, latency, and token\ncost.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02822v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "SynapseRoute：一种基于双态大语言模型的自动路由切换框架", "tldr": "SynapseRoute提出了一种基于机器学习的动态路由框架，用于在大语言模型的“思考”和“非思考”模式之间智能分配查询，以在提高准确性的同时显著降低成本和推理时间，并避免过度推理。", "motivation": "在大语言模型（LLMs）的实际应用中，选择合适的模型不仅需要平衡性能，还需要考虑运营成本。具有推理能力的模型出现后，进一步扩大了“思考”（高推理）模式和“非思考”（快速、低成本）模式之间的成本差距。研究发现大约58%的医学问题仅通过非思考模式即可准确回答，无需高成本的推理过程，这表明问题的复杂性存在明显区别，动态路由查询可以优化准确性、成本效率和用户体验。", "method": "本文提出了SynapseRoute，一个基于机器学习的动态路由框架，它能智能地将输入查询分配到“思考”或“非思考”模式。此外，工作还引入了准确性-推理时间-令牌（AIT）指数来全面评估准确性、延迟和令牌成本之间的权衡。", "result": "在几个医学数据集上的实验结果表明，与单独使用思考模式相比，SynapseRoute不仅提高了整体准确性（0.8390 vs. 0.8272），还将推理时间减少了36.8%，令牌消耗减少了39.66%。定性分析表明，对简单查询进行过度推理可能导致不必要的延迟甚至降低准确性，而我们的自适应路由避免了这一缺陷。", "conclusion": "动态路由查询到合适的大语言模型模式可以有效优化准确性、成本效率和用户体验。SynapseRoute框架通过智能路由实现了性能提升和成本降低，并避免了过度推理的负面影响。引入的AIT指数为全面评估LLM的权衡提供了工具。", "translation": "随着大语言模型（LLMs）在实际应用中广泛采用，选择合适的模型不仅需要平衡性能，还需要考虑运营成本。具有推理能力的模型的出现进一步扩大了“思考”（高推理）和“非思考”（快速、低成本）模式之间的成本差距。在这项工作中，我们揭示了大约58%的医学问题仅通过非思考模式即可准确回答，而无需高成本的推理过程。这突显了问题复杂性的明显二分法，并表明根据复杂性动态路由查询到适当的模式可以优化准确性、成本效率和整体用户体验。在此基础上，我们进一步提出了SynapseRoute，一个基于机器学习的动态路由框架，它能智能地将输入查询分配到思考或非思考模式。在几个医学数据集上的实验结果表明，与单独使用思考模式相比，SynapseRoute不仅提高了整体准确性（0.8390 vs. 0.8272），还将推理时间减少了36.8%，令牌消耗减少了39.66%。重要的是，定性分析表明，对简单查询进行过度推理可能导致不必要的延迟甚至降低准确性，而我们的自适应路由避免了这一缺陷。最后，这项工作进一步引入了准确性-推理时间-令牌（AIT）指数，以全面评估准确性、延迟和令牌成本之间的权衡。", "summary": "SynapseRoute旨在解决大语言模型（LLMs）应用中性能与成本的平衡问题。该研究发现，许多简单查询无需高成本的“思考”模式。为此，本文提出了SynapseRoute，一个基于机器学习的动态路由框架，能够智能地将查询分配给“思考”或“非思考”模式。实验结果表明，该框架在提高整体准确性（0.8390 vs. 0.8272）的同时，显著减少了推理时间（36.8%）和令牌消耗（39.66%），并有效避免了过度推理带来的负面影响。此外，论文还引入了准确性-推理时间-令牌（AIT）指数，用于全面评估LLM在准确性、延迟和成本之间的权衡。", "keywords": "大语言模型, 动态路由, 成本效率, 双态LLM, SynapseRoute", "comments": "本文的创新之处在于提出了一种基于动态路由的LLM优化策略，通过区分“思考”和“非思考”模式，有效解决了LLM应用中性能与成本的平衡问题。这种自适应路由机制避免了不必要的计算开销和潜在的性能下降，对于LLM的实际部署和成本控制具有重要意义。引入AIT指数也为LLM的综合评估提供了新的视角和工具。"}}
{"id": "2507.02850", "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users", "authors": ["Almog Hilel", "Idan Shenfeld", "Leshem Choshen", "Jacob Andreas"], "categories": ["cs.CL", "cs.CR", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02850v1", "summary": "We describe a vulnerability in language models (LMs) trained with user\nfeedback, whereby a single user can persistently alter LM knowledge and\nbehavior given only the ability to provide prompts and upvote / downvote\nfeedback on LM outputs. To implement the attack, the attacker prompts the LM to\nstochastically output either a \"poisoned\" or benign response, then upvotes the\npoisoned response or downvotes the benign one. When feedback signals are used\nin a subsequent preference tuning behavior, LMs exhibit increased probability\nof producing poisoned responses even in contexts without malicious prompts. We\nshow that this attack can be used to (1) insert factual knowledge the model did\nnot previously possess, (2) modify code generation patterns in ways that\nintroduce exploitable security flaws, and (3) inject fake financial news. Our\nfinding both identifies a new qualitative feature of language model preference\ntuning (showing that it even highly restricted forms of preference data can be\nused to exert fine-grained control over behavior), and a new attack mechanism\nfor LMs trained with user feedback (extending work on pretraining-time data\npoisoning and deployment-time prompt injection).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02850v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LLM 催眠：利用用户反馈对所有用户进行未经授权的知识注入", "tldr": "研究发现，通过用户反馈，单个恶意用户可以永久性地改变语言模型的知识和行为，实现未经授权的知识注入。", "motivation": "语言模型通过用户反馈进行训练时，存在一种漏洞，允许单个用户持续改变模型的知识和行为。本文旨在揭示并描述这种新的攻击机制，扩展了数据投毒和提示注入的工作。", "method": "攻击者提示语言模型随机输出“投毒”或良性响应，然后对投毒响应进行点赞或对良性响应进行点踩。当这些反馈信号用于后续的偏好调整时，即使在没有恶意提示的上下文中，语言模型也会增加产生投毒响应的概率。", "result": "该攻击可用于 (1) 插入模型以前不具备的事实知识，(2) 修改代码生成模式以引入可利用安全缺陷，以及 (3) 注入虚假金融新闻。", "conclusion": "这项发现既揭示了语言模型偏好调整的一个新的定性特征（即使高度受限的偏好数据也能对行为进行细粒度控制），也揭示了一种针对使用用户反馈训练的语言模型的新攻击机制（扩展了预训练时数据投毒和部署时提示注入的工作）。", "translation": "我们描述了在通过用户反馈训练的语言模型（LMs）中存在的一个漏洞，即单个用户仅通过提供提示并对LM输出进行点赞/点踩反馈，就可以持久地改变LM的知识和行为。为了实施攻击，攻击者提示LM随机输出“投毒”或良性响应，然后对投毒响应进行点赞或对良性响应进行点踩。当反馈信号用于后续的偏好调整行为时，即使在没有恶意提示的上下文中，LMs也会增加产生投毒响应的概率。我们展示了这种攻击可以用于（1）插入模型以前不具备的事实知识，（2）以引入可利用安全缺陷的方式修改代码生成模式，以及（3）注入虚假金融新闻。我们的发现既识别了语言模型偏好调整的一个新的定性特征（表明即使是高度受限的偏好数据也可以用于对行为施加细粒度控制），也识别了一种针对使用用户反馈训练的LMs的新攻击机制（扩展了预训练时数据投毒和部署时提示注入的工作）。", "summary": "本文揭示了一种名为“LLM催眠”的漏洞，该漏洞允许单个用户通过提供提示和控制反馈（点赞/点踩）来持久地操纵基于用户反馈训练的语言模型（LMs）的知识和行为。攻击者通过偏好调整过程，使LMs在正常上下文中也更可能生成“投毒”响应。研究证明，这种攻击可以成功地注入虚假事实、引入代码安全漏洞以及传播虚假金融新闻。这项工作突出了偏好调整的潜在风险，并提出了一种新型的语言模型攻击机制。", "keywords": "LLM安全, 用户反馈, 知识注入, 偏好调整, 漏洞", "comments": "这篇论文揭示了一个非常重要且令人担忧的漏洞，即通过用户反馈机制，单个恶意用户就能对LLM的知识和行为进行持久性、细粒度的操纵。其创新之处在于将传统的“数据投毒”和“提示注入”攻击扩展到模型部署后的用户反馈阶段，揭示了RLHF等偏好调整技术可能带来的新风险。这项研究对于未来LLM的安全性和鲁棒性研究具有重要意义，提醒开发者在设计和部署用户反馈系统时需更加谨慎。"}}
{"id": "2507.02747", "title": "DexVLG: Dexterous Vision-Language-Grasp Model at Scale", "authors": ["Jiawei He", "Danshi Li", "Xinqiang Yu", "Zekun Qi", "Wenyao Zhang", "Jiayi Chen", "Zhaoxiang Zhang", "Zhizheng Zhang", "Li Yi", "He Wang"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02747v1", "summary": "As large models gain traction, vision-language-action (VLA) systems are\nenabling robots to tackle increasingly complex tasks. However, limited by the\ndifficulty of data collection, progress has mainly focused on controlling\nsimple gripper end-effectors. There is little research on functional grasping\nwith large models for human-like dexterous hands. In this paper, we introduce\nDexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction\naligned with language instructions using single-view RGBD input. To accomplish\nthis, we generate a dataset of 170 million dexterous grasp poses mapped to\nsemantic parts across 174,000 objects in simulation, paired with detailed\npart-level captions. This large-scale dataset, named DexGraspNet 3.0, is used\nto train a VLM and flow-matching-based pose head capable of producing\ninstruction-aligned grasp poses for tabletop objects. To assess DexVLG's\nperformance, we create benchmarks in physics-based simulations and conduct\nreal-world experiments. Extensive testing demonstrates DexVLG's strong\nzero-shot generalization capabilities-achieving over 76% zero-shot execution\nsuccess rate and state-of-the-art part-grasp accuracy in simulation-and\nsuccessful part-aligned grasps on physical objects in real-world scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02747v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DexVLG：大规模灵巧视觉-语言-抓取模型", "tldr": "DexVLG是一个大规模视觉-语言-抓取模型，它利用大规模模拟数据集来预测灵巧手抓取姿态，并实现了强大的零样本泛化能力和真实世界成功抓取。", "motivation": "现有视觉-语言-动作（VLA）系统主要集中在简单的夹持器末端执行器，对于类人灵巧手的实用抓取研究很少，尤其是在大型模型背景下。数据收集困难是主要限制。", "method": "引入了DexVLG，一个大型视觉-语言-抓取模型，用于预测与语言指令对齐的灵巧抓取姿态，使用单视图RGBD输入。为此，他们生成了一个名为DexGraspNet 3.0的大规模数据集，包含1.7亿个灵巧抓取姿态，映射到174,000个对象的语义部分，并配有详细的部分级标题。该数据集用于训练一个VLM和基于流匹配的姿态头部。通过物理模拟和真实世界实验进行评估。", "result": "DexVLG在零样本泛化能力上表现出色，实现了超过76%的零样本执行成功率，在模拟中达到了最先进的部分抓取精度，并在真实世界场景中成功实现了物理对象的与部分对齐的抓取。", "conclusion": "DexVLG通过大规模数据和模型，成功解决了灵巧手抓取姿态预测的挑战，并在模拟和现实世界中展示了强大的性能和泛化能力。", "translation": "随着大型模型的普及，视觉-语言-动作（VLA）系统使机器人能够应对日益复杂的任务。然而，受限于数据收集的难度，研究进展主要集中在控制简单的夹持器末端执行器。关于使用大型模型进行类人灵巧手功能性抓取的研究很少。在本文中，我们引入了DexVLG，一个大型视觉-语言-抓取模型，用于预测与语言指令对齐的灵巧抓取姿态，使用单视图RGBD输入。为实现这一目标，我们生成了一个包含1.7亿个灵巧抓取姿态的数据集，这些姿态映射到模拟中174,000个对象的语义部分，并配有详细的部分级标题。这个名为DexGraspNet 3.0的大规模数据集用于训练一个VLM和基于流匹配的姿态头部，能够为桌面对象生成与指令对齐的抓取姿态。为了评估DexVLG的性能，我们在基于物理的模拟中创建了基准，并进行了真实世界实验。广泛的测试表明，DexVLG具有强大的零样本泛化能力——在模拟中实现了超过76%的零样本执行成功率和最先进的部分抓取精度——并在真实世界场景中成功实现了物理对象的与部分对齐的抓取。", "summary": "本文介绍了DexVLG，一个大规模视觉-语言-抓取模型，旨在解决灵巧手抓取姿态预测的挑战。该模型利用了名为DexGraspNet 3.0的1.7亿个灵巧抓取姿态的大规模模拟数据集，该数据集将抓取姿态与对象语义部分及语言指令对齐。DexVLG通过训练VLM和流匹配姿态头部，实现了基于单视图RGBD输入的指令对齐抓取。在模拟和真实世界实验中，DexVLG展现出卓越的零样本泛化能力，在模拟中达到76%以上的成功率和最先进的精度，并在现实中成功执行了部分对齐抓取。", "keywords": "灵巧抓取, 视觉-语言模型, 大规模数据集, 机器人操作, 零样本泛化", "comments": "这项工作在机器人灵巧抓取领域取得了重要进展，通过引入大规模模拟数据集和视觉-语言-抓取模型DexVLG，有效解决了灵巧手抓取数据稀缺和与语言指令对齐的难题。其创新点在于大规模数据集的构建和VLM与流匹配结合的模型架构。在零样本泛化能力上的表现尤其突出，为未来复杂机器人操作任务提供了新的方向。"}}
{"id": "2507.02320", "title": "Transformer-based EEG Decoding: A Survey", "authors": ["Haodong Zhang", "Hongqi Li"], "categories": ["cs.LG", "cs.HC"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Journals", "url": "http://arxiv.org/abs/2507.02320v1", "summary": "Electroencephalography (EEG) is one of the most common signals used to\ncapture the electrical activity of the brain, and the decoding of EEG, to\nacquire the user intents, has been at the forefront of brain-computer/machine\ninterfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods\nwith machine learning, the advent of deep learning approaches have gradually\nrevolutionized the field by providing an end-to-end long-cascaded architecture,\nwhich can learn more discriminative features automatically. Among these,\nTransformer is renowned for its strong handling capability of sequential data\nby the attention mechanism, and the application of Transformers in various EEG\nprocessing tasks is increasingly prevalent. This article delves into a relevant\nsurvey, summarizing the latest application of Transformer models in EEG\ndecoding since it appeared. The evolution of the model architecture is followed\nto sort and organize the related advances, in which we first elucidate the\nfundamentals of the Transformer that benefits EEG decoding and its direct\napplication. Then, the common hybrid architectures by integrating basic\nTransformer with other deep learning techniques\n(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial\nnetworks, diffusion models, etc.) is overviewed in detail. The research\nadvances of applying the modified intrinsic structures of customized\nTransformer have also been introduced. Finally, the current challenges and\nfuture development prospects in this rapidly evolving field are discussed. This\npaper aims to help readers gain a clear understanding of the current state of\nTransformer applications in EEG decoding and to provide valuable insights for\nfuture research endeavors.", "comment": "Submitted to IEEE Journals", "pdf_url": "http://arxiv.org/pdf/2507.02320v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于Transformer的EEG解码：一项综述", "tldr": "本文综述了Transformer模型在EEG解码领域的最新应用，涵盖了模型演变、混合架构以及未来挑战。", "motivation": "EEG解码是脑机接口研究的前沿领域。与传统方法相比，深度学习尤其是Transformer模型在处理序列数据方面的强大能力，使其在EEG处理中日益普及。本文旨在对Transformer模型在EEG解码中的应用进行全面综述，以帮助读者理解现状并提供未来研究的见解。", "method": "本文通过综述形式，总结了Transformer模型在EEG解码领域的最新应用。具体内容包括：阐明Transformer的基本原理及其在EEG解码中的直接应用；详细概述将Transformer与卷积/循环/图/脉冲神经网络、生成对抗网络、扩散模型等其他深度学习技术相结合的常见混合架构；介绍应用定制化Transformer修改后的内在结构的研究进展；最后讨论该领域的当前挑战和未来发展前景。", "result": "本文总结了Transformer模型在EEG解码中的最新应用，从模型架构演变、与各种深度学习技术的混合架构，到定制化Transformer的结构修改。此外，还讨论了该领域面临的挑战和未来的发展方向。", "conclusion": "本文旨在帮助读者清晰理解Transformer在EEG解码应用中的当前状态，并为未来的研究工作提供有价值的见解。", "translation": "脑电图（EEG）是捕捉大脑电活动最常用的信号之一，而EEG解码（即获取用户意图）一直处于脑机接口（BCIs/BMIs）研究的最前沿。与传统的机器学习EEG分析方法相比，深度学习方法的出现通过提供端到端的长级联架构，可以自动学习更具区分性的特征，从而逐渐革新了该领域。其中，Transformer因其通过注意力机制对序列数据强大的处理能力而闻名，并且Transformer在各种EEG处理任务中的应用日益普及。本文深入探讨了一项相关综述，总结了自Transformer模型出现以来其在EEG解码中的最新应用。本文遵循模型架构的演变，对相关进展进行分类和组织，首先阐明了有利于EEG解码的Transformer基础知识及其直接应用。然后，详细概述了通过将基本Transformer与其他深度学习技术（卷积/循环/图/脉冲神经网络、生成对抗网络、扩散模型等）集成的常见混合架构。本文还介绍了应用定制化Transformer修改后的内在结构的研究进展。最后，讨论了在这个快速发展领域中当前的挑战和未来的发展前景。本文旨在帮助读者清晰理解Transformer在EEG解码应用中的当前状态，并为未来的研究工作提供有价值的见解。", "summary": "本文对Transformer模型在脑电图（EEG）解码领域的最新应用进行了全面综述。文章首先介绍了EEG解码在脑机接口中的重要性以及深度学习，特别是Transformer模型，如何通过其处理序列数据的能力革新了该领域。综述内容涵盖了Transformer应用于EEG解码的基本原理和直接应用，以及与卷积、循环、图神经网络等其他深度学习技术结合的混合架构。此外，还探讨了定制化Transformer结构修改的研究进展。最后，文章讨论了当前面临的挑战和未来的发展前景，旨在为读者提供对该领域现状的清晰理解并为未来研究提供有价值的见解。", "keywords": "EEG解码, Transformer, 脑机接口, 深度学习, 综述", "comments": "本文是一篇及时且重要的综述，它系统地梳理了Transformer模型在EEG解码这一快速发展领域中的应用。其创新之处在于详细分类和总结了不同架构和混合方法，并提出了未来的挑战和展望，对研究人员具有很高的参考价值。该综述有助于填补该领域知识的空白，并为未来的研究方向提供清晰的指引。"}}
{"id": "2507.02681", "title": "Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education", "authors": ["Behnam Parsaeifard", "Christof Imhof", "Tansu Pancar", "Ioan-Sorin Comsa", "Martin Hlosta", "Nicole Bergamin", "Per Bergamin"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02681v1", "summary": "Students disengaging from their tasks can have serious long-term\nconsequences, including academic drop-out. This is particularly relevant for\nstudents in distance education. One way to measure the level of disengagement\nin distance education is to observe participation in non-mandatory exercises in\ndifferent online courses. In this paper, we detect student disengagement in the\nnon-mandatory quizzes of 42 courses in four semesters from a distance-based\nuniversity. We carefully identified the most informative student log data that\ncould be extracted and processed from Moodle. Then, eight machine learning\nalgorithms were trained and compared to obtain the highest possible prediction\naccuracy. Using the SHAP method, we developed an explainable machine learning\nframework that allows practitioners to better understand the decisions of the\ntrained algorithm. The experimental results show a balanced accuracy of 91\\%,\nwhere about 85\\% of disengaged students were correctly detected. On top of the\nhighly predictive performance and explainable framework, we provide a\ndiscussion on how to design a timely intervention to minimise disengagement\nfrom voluntary tasks in online learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02681v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "检测学生从自愿测验中脱离：一种在高等远程教育中的可解释机器学习方法", "tldr": "该研究使用可解释机器学习方法，通过分析Moodle日志数据，检测远程教育学生在非强制性测验中的脱离行为，并实现了91%的平衡准确率，为早期干预提供了可能性。", "motivation": "学生在任务中脱离可能导致严重的长期后果，包括学业辍学，这在远程教育中尤为突出。通过观察学生在非强制性练习中的参与度可以衡量脱离水平。因此，该研究旨在检测远程教育学生在非强制性测验中的脱离行为。", "method": "研究从一所远程大学的42门课程中收集了四个学期学生的Moodle日志数据，识别出最具信息量的数据。然后，训练并比较了八种机器学习算法以获得最高的预测准确性。最后，使用SHAP方法开发了一个可解释的机器学习框架，以帮助理解算法的决策。", "result": "实验结果显示，该方法达到了91%的平衡准确率，其中大约85%的脱离学生被正确检测出来。该框架具有高度预测性能和可解释性。", "conclusion": "该研究成功地利用可解释机器学习方法检测了远程教育学生在自愿测验中的脱离行为，并取得了高准确率。这为设计及时干预措施以减少在线学习中自愿任务的脱离提供了基础。", "translation": "学生脱离任务可能带来严重的长期后果，包括学业辍学。这对于远程教育学生尤其重要。衡量远程教育中脱离程度的一种方法是观察学生在不同在线课程中参与非强制性练习的情况。本文检测了一所远程大学四个学期42门课程中学生在非强制性测验中的脱离情况。我们仔细识别了可以从Moodle中提取和处理的最具信息量的学生日志数据。然后，训练并比较了八种机器学习算法，以获得尽可能高的预测准确性。使用SHAP方法，我们开发了一个可解释的机器学习框架，使实践者能够更好地理解训练算法的决策。实验结果显示，平衡准确率达到91%，其中约85%的脱离学生被正确检测出来。在高度预测性能和可解释性框架的基础上，我们讨论了如何设计及时干预措施，以最大限度地减少在线学习中自愿任务的脱离。", "summary": "本研究利用Moodle日志数据，采用可解释机器学习方法（基于SHAP），检测远程教育学生在非强制性测验中的脱离行为。通过训练和比较多种机器学习算法，实现了91%的平衡准确率，成功识别了85%的脱离学生。该研究不仅提供了高预测性能的框架，还探讨了如何设计早期干预措施以减少学生在在线学习中的脱离。", "keywords": "学生脱离, 远程教育, 可解释机器学习, SHAP, 在线学习", "comments": "该研究的创新之处在于结合了可解释机器学习方法（SHAP）来检测学生脱离行为，这增强了模型的透明度和实用性，有助于教育者理解决策并设计更有效的干预措施。其重要性在于解决了远程教育中学生脱离这一关键问题，为及早发现并干预提供了数据驱动的工具。"}}
{"id": "2507.02321", "title": "Heeding the Inner Voice: Aligning ControlNet Training via Intermediate Features Feedback", "authors": ["Nina Konovalova", "Maxim Nikolaev", "Andrey Kuznetsov", "Aibek Alanov"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      code available at this https URL", "url": "http://arxiv.org/abs/2507.02321v1", "summary": "Despite significant progress in text-to-image diffusion models, achieving\nprecise spatial control over generated outputs remains challenging. ControlNet\naddresses this by introducing an auxiliary conditioning module, while\nControlNet++ further refines alignment through a cycle consistency loss applied\nonly to the final denoising steps. However, this approach neglects intermediate\ngeneration stages, limiting its effectiveness. We propose InnerControl, a\ntraining strategy that enforces spatial consistency across all diffusion steps.\nOur method trains lightweight convolutional probes to reconstruct input control\nsignals (e.g., edges, depth) from intermediate UNet features at every denoising\nstep. These probes efficiently extract signals even from highly noisy latents,\nenabling pseudo ground truth controls for training. By minimizing the\ndiscrepancy between predicted and target conditions throughout the entire\ndiffusion process, our alignment loss improves both control fidelity and\ngeneration quality. Combined with established techniques like ControlNet++,\nInnerControl achieves state-of-the-art performance across diverse conditioning\nmethods (e.g., edges, depth).", "comment": "code available at https://github.com/ControlGenAI/InnerControl", "pdf_url": "http://arxiv.org/pdf/2507.02321v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "倾听内在声音：通过中间特征反馈对齐ControlNet训练", "tldr": "InnerControl提出了一种新的训练策略，通过在所有去噪步骤中利用轻量级卷积探针从中间特征中重建控制信号，并在整个扩散过程中最小化预测与目标条件之间的差异，从而改进ControlNet的空间控制精度和生成质量。", "motivation": "尽管文本到图像扩散模型取得了显著进展，但实现对生成输出的精确空间控制仍然具有挑战性。现有的ControlNet和ControlNet++方法在对齐时忽略了中间生成阶段，限制了其有效性。", "method": "我们提出了InnerControl，这是一种训练策略，它在所有扩散步骤中强制执行空间一致性。该方法训练轻量级卷积探针，在每个去噪步骤从中间UNet特征中重建输入控制信号（例如边缘、深度）。这些探针即使从高噪声潜在变量中也能有效提取信号，从而为训练提供伪真实控制。通过在整个扩散过程中最小化预测条件与目标条件之间的差异，我们的对齐损失提高了控制保真度和生成质量。", "result": "结合ControlNet++等现有技术，InnerControl在各种条件方法（例如边缘、深度）上实现了最先进的性能。", "conclusion": "InnerControl通过在所有扩散步骤中强制执行空间一致性，并利用中间特征反馈进行训练，显著提高了ControlNet的空间控制精度和生成质量，达到了最先进的水平。", "translation": "尽管文本到图像扩散模型取得了显著进展，但实现对生成输出的精确空间控制仍然具有挑战性。ControlNet通过引入辅助条件模块解决了这个问题，而ControlNet++则通过仅应用于最终去噪步骤的循环一致性损失进一步完善了对齐。然而，这种方法忽略了中间生成阶段，限制了其有效性。我们提出了InnerControl，这是一种在所有扩散步骤中强制执行空间一致性的训练策略。我们的方法训练轻量级卷积探针，在每个去噪步骤从中间UNet特征中重建输入控制信号（例如边缘、深度）。这些探针即使从高噪声潜在变量中也能有效提取信号，从而为训练提供伪真实控制。通过在整个扩散过程中最小化预测条件与目标条件之间的差异，我们的对齐损失提高了控制保真度和生成质量。结合ControlNet++等成熟技术，InnerControl在各种条件方法（例如边缘、深度）上实现了最先进的性能。", "summary": "本文提出了InnerControl，一种针对文本到图像扩散模型的ControlNet训练策略，旨在解决精确空间控制的挑战。与现有方法忽略中间生成阶段不同，InnerControl通过训练轻量级卷积探针，在所有扩散步骤中从中间UNet特征重建控制信号，并利用这些信号作为伪真实控制。通过在整个扩散过程中最小化预测与目标条件之间的差异，InnerControl的对齐损失显著提升了控制保真度和生成质量，并在多种条件下实现了最先进的性能。", "keywords": "ControlNet, 扩散模型, 空间控制, 中间特征, 训练策略", "comments": "InnerControl的创新点在于其“倾听内在声音”的理念，即在扩散过程的每个中间步骤都引入反馈机制来强制执行空间一致性。通过利用轻量级卷积探针从噪声中间特征中提取控制信号，并将其作为伪真实进行训练，该方法有效地解决了现有ControlNet变体在中间阶段对齐不足的问题。这对于提高生成图像的精细控制能力和质量具有重要意义，是ControlNet系列模型的一个重要改进。"}}
{"id": "2507.02833", "title": "Generalizing Verifiable Instruction Following", "authors": ["Valentina Pyatkin", "Saumya Malik", "Victoria Graf", "Hamish Ivison", "Shengyi Huang", "Pradeep Dasigi", "Nathan Lambert", "Hannaneh Hajishirzi"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      11 pages", "url": "http://arxiv.org/abs/2507.02833v1", "summary": "A crucial factor for successful human and AI interaction is the ability of\nlanguage models or chatbots to follow human instructions precisely. A common\nfeature of instructions are output constraints like ``only answer with yes or\nno\" or ``mention the word `abrakadabra' at least 3 times\" that the user adds to\ncraft a more useful answer. Even today's strongest models struggle with\nfulfilling such constraints. We find that most models strongly overfit on a\nsmall set of verifiable constraints from the benchmarks that test these\nabilities, a skill called precise instruction following, and are not able to\ngeneralize well to unseen output constraints. We introduce a new benchmark,\nIFBench, to evaluate precise instruction following generalization on 58 new,\ndiverse, and challenging verifiable out-of-domain constraints. In addition, we\nperform an extensive analysis of how and on what data models can be trained to\nimprove precise instruction following generalization. Specifically, we\ncarefully design constraint verification modules and show that reinforcement\nlearning with verifiable rewards (RLVR) significantly improves instruction\nfollowing. In addition to IFBench, we release 29 additional new hand-annotated\ntraining constraints and verification functions, RLVR training prompts, and\ncode.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2507.02833v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "泛化可验证的指令遵循", "tldr": "当前最强的语言模型在遵循带输出约束的指令时表现不佳且存在过拟合问题。本文引入了一个新的基准测试IFBench和一种新的训练方法RLVR，显著提升了模型在泛化可验证指令遵循方面的能力。", "motivation": "语言模型或聊天机器人精确遵循人类指令的能力是人机成功交互的关键因素。指令中常见的输出约束（如“只回答是或否”）对用户生成更有用的答案至关重要，但即使是当前最强大的模型也难以满足这些约束，并且在现有基准测试上存在严重的过拟合，无法很好地泛化到未见的输出约束。", "method": "本文引入了一个新的基准测试IFBench，用于评估模型在58个新的、多样化且具有挑战性的域外可验证约束上的精确指令遵循泛化能力。此外，论文还对如何以及使用何种数据来训练模型以提高精确指令遵循泛化能力进行了广泛分析。具体而言，他们精心设计了约束验证模块，并展示了使用可验证奖励的强化学习（RLVR）显著改善了指令遵循能力。除了IFBench，他们还发布了29个额外的新手写标注训练约束和验证函数、RLVR训练提示以及代码。", "result": "研究发现大多数模型在测试精确指令遵循能力的现有基准测试中，对一小组可验证约束存在严重的过拟合，并且无法很好地泛化到未见的输出约束。通过引入IFBench和RLVR训练，结果表明强化学习与可验证奖励（RLVR）显著改善了模型的指令遵循能力。", "conclusion": "该研究揭示了现有语言模型在精确指令遵循泛化方面的局限性，并成功通过引入新的基准测试IFBench和创新的RLVR训练方法，显著提升了模型泛化遵循复杂输出约束的能力，为未来人机交互的精确性奠定了基础。", "translation": "成功的AIGC人机交互的一个关键因素是语言模型或聊天机器人精确遵循人类指令的能力。指令的一个常见特征是输出约束，例如“只回答是或否”或“至少提及‘abrakadabra’这个词3次”，用户添加这些约束是为了获得更有用的答案。即使是当今最强大的模型也难以满足这些约束。我们发现大多数模型在测试这些能力的基准测试中，对一小组可验证约束存在严重的过拟合，这种能力被称为精确指令遵循，并且无法很好地泛化到未见的输出约束。我们引入了一个新的基准测试IFBench，用于评估58个新的、多样化且具有挑战性的域外可验证约束上的精确指令遵循泛化能力。此外，我们对模型如何以及在什么数据上进行训练以提高精确指令遵循泛化能力进行了广泛分析。具体而言，我们精心设计了约束验证模块，并表明使用可验证奖励的强化学习（RLVR）显著改善了指令遵循能力。除了IFBench，我们还发布了29个额外的新手写标注训练约束和验证函数、RLVR训练提示和代码。", "summary": "该论文探讨了语言模型在遵循带有输出约束的指令时遇到的泛化挑战。研究发现现有模型存在过拟合问题，难以泛化到未见的约束。为解决此问题，作者引入了新的基准测试IFBench，包含58个多样化的域外可验证约束，并提出了一种基于可验证奖励的强化学习（RLVR）方法。实验证明，RLVR显著提升了模型的指令遵循泛化能力。此外，论文还发布了新的训练数据集和代码，以促进该领域的研究。", "keywords": "指令遵循, 泛化, 强化学习, 输出约束, 基准测试", "comments": "这项研究解决了语言模型在复杂人机交互中一个核心且普遍存在的痛点——精确遵循带约束的指令并实现泛化。其创新点在于不仅识别了模型过拟合的根本问题，还提供了一个新的、更具挑战性的评估基准IFBench，以及一种有效的训练范式RLVR。RLVR通过将约束验证与强化学习奖励结合，为模型提供更直接的反馈，从而显著提高了泛化能力。这项工作对于提升聊天机器人和AI助手的可靠性和用户体验具有重要意义，其发布的训练数据和代码也将极大地推动后续研究。"}}
{"id": "2507.02771", "title": "Grounding Intelligence in Movement", "authors": ["Melanie Segado", "Felipe Parodi", "Jordan K. Matelsky", "Michael L. Platt", "Eva B. Dyer", "Konrad P. Kording"], "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      9 pages, 2 figures", "url": "http://arxiv.org/abs/2507.02771v1", "summary": "Recent advances in machine learning have dramatically improved our ability to\nmodel language, vision, and other high-dimensional data, yet they continue to\nstruggle with one of the most fundamental aspects of biological systems:\nmovement. Across neuroscience, medicine, robotics, and ethology, movement is\nessential for interpreting behavior, predicting intent, and enabling\ninteraction. Despite its core significance in our intelligence, movement is\noften treated as an afterthought rather than as a rich and structured modality\nin its own right. This reflects a deeper fragmentation in how movement data is\ncollected and modeled, often constrained by task-specific goals and\ndomain-specific assumptions. But movement is not domain-bound. It reflects\nshared physical constraints, conserved morphological structures, and purposeful\ndynamics that cut across species and settings. We argue that movement should be\ntreated as a primary modeling target for AI. It is inherently structured and\ngrounded in embodiment and physics. This structure, often allowing for compact,\nlower-dimensional representations (e.g., pose), makes it more interpretable and\ncomputationally tractable to model than raw, high-dimensional sensory inputs.\nDeveloping models that can learn from and generalize across diverse movement\ndata will not only advance core capabilities in generative modeling and\ncontrol, but also create a shared foundation for understanding behavior across\nbiological and artificial systems. Movement is not just an outcome, it is a\nwindow into how intelligent systems engage with the world.", "comment": "9 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.02771v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "运动中的智能基础", "tldr": "AI应将运动视为核心建模目标，因为它具有固有结构且对智能至关重要。", "motivation": "尽管机器学习在语言、视觉等高维数据建模方面取得了显著进步，但在处理生物系统最基本的运动方面仍面临挑战。运动对于解释行为、预测意图和实现交互至关重要，但常被视为次要，而非一种独立、丰富且结构化的模态。这种状况反映了运动数据收集和建模的深度碎片化，常受限于特定任务目标和领域假设。", "method": "本文提出一种将运动视为AI主要建模目标的新视角，并论证其作为一种固有的、结构化的、根植于具身和物理学的模态的重要性。", "result": "Not mentioned in abstract", "conclusion": "运动不仅是智能系统与世界交互的结果，更是理解其交互方式的窗口。它应被视为AI的首要建模目标，因为其固有的结构性、具身性和物理基础使其更易于建模，并将推动生成式建模和控制能力的发展，同时为理解生物和人工系统行为提供共同基础。", "translation": "机器学习的最新进展极大地提升了我们对语言、视觉和其他高维数据进行建模的能力，但它们在处理生物系统最基本的一个方面——运动——上仍然举步维艰。在神经科学、医学、机器人学和动物行为学领域，运动对于解释行为、预测意图和实现交互至关重要。尽管运动在我们的智能中具有核心意义，但它通常被视为事后补充，而非一种独立、丰富且结构化的模态。这反映了运动数据收集和建模方式上更深层次的碎片化，往往受限于特定任务目标和领域特定假设。但运动并非领域受限。它反映了跨物种和环境共享的物理约束、保守的形态结构和有目的的动力学。我们认为，运动应被视为AI的主要建模目标。它本质上是结构化的，并根植于具身和物理学。这种结构通常允许紧凑的低维表示（例如姿态），使其比原始的高维感官输入更具可解释性且计算上更易于建模。开发能够从多样化运动数据中学习并进行泛化的模型，不仅将推进生成式建模和控制的核心能力，还将为理解生物和人工系统之间的行为创建一个共享基础。运动不仅仅是一种结果，它是了解智能系统如何与世界互动的一扇窗。", "summary": "本文指出，尽管当前机器学习在语言和视觉等领域取得进展，但在理解生物系统核心的运动方面仍面临挑战。作者强调，运动是解释行为、预测意图和实现交互的关键，且具有内在结构，根植于具身和物理学，常能形成紧凑的低维表示。因此，文章主张将运动作为AI的首要建模目标，认为这不仅能推动生成式建模和控制能力，还能为理解生物和人工系统的行为提供统一基础。运动被视为智能系统与世界互动的重要窗口。", "keywords": "运动智能, 具身智能, 行为理解, 机器学习, 结构化数据", "comments": "这篇论文提出一个重要的观点，即当前AI在理解运动方面存在不足，并强调了运动作为一种核心、结构化模态的重要性。其创新之处在于将运动提升到与语言、视觉同等重要的地位，并主张将其作为AI的主要建模目标，这可能为具身智能和通用行为理解提供新的视角和研究方向。该论点具有前瞻性，对未来AI研究的方向具有指导意义。"}}
{"id": "2507.02703", "title": "Time-critical and confidence-based abstraction dropping methods", "authors": ["Robin Schmöcker", "Lennart Kampmann", "Alexander Dockhorn"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted for Publication at the IEEE Conference on Games 2025", "url": "http://arxiv.org/abs/2507.02703v1", "summary": "One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and\nuse state and/or action abstractions during the tree search. Non-exact\nabstractions, however, introduce an approximation error making convergence to\nthe optimal action in the abstract space impossible. Hence, as proposed as a\ncomponent of Elastic Monte Carlo Tree Search by Xu et al., abstraction\nalgorithms should eventually drop the abstraction. In this paper, we propose\ntwo novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can\nyield clear performance improvements whilst being safe in the sense that the\ndropping never causes any notable performance degradations contrary to Xu's\ndropping method. OGA-IAAD is designed for time critical settings while OGA-CAD\nis designed to improve the MCTS performance with the same number of iterations.", "comment": "Accepted for Publication at the IEEE Conference on Games 2025", "pdf_url": "http://arxiv.org/pdf/2507.02703v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "时间关键型和基于置信度的抽象丢弃方法", "tldr": "本文提出了两种新的抽象丢弃方案（OGA-IAAD和OGA-CAD），它们在蒙特卡洛树搜索（MCTS）中既能提高性能又能确保安全，解决了现有方法可能导致的性能下降问题。", "motivation": "蒙特卡洛树搜索（MCTS）中引入的非精确抽象会带来近似误差，导致无法收敛到最优动作。现有的抽象丢弃方法（如Xu等人的方法）可能导致性能显著下降，因此需要一种既能提高性能又能确保安全的抽象丢弃机制。", "method": "本文提出了两种新颖的抽象丢弃方案：OGA-IAAD和OGA-CAD。OGA-IAAD专为时间关键型设置设计，而OGA-CAD旨在在相同迭代次数下提升MCTS性能。", "result": "与Xu的丢弃方法相反，本文提出的OGA-IAAD和OGA-CAD方法能够带来显著的性能提升，并且是安全的，即丢弃操作不会引起任何明显的性能下降。", "conclusion": "本文提出的OGA-IAAD和OGA-CAD两种新的抽象丢弃方案，在蒙特卡洛树搜索中能够实现性能提升，同时确保安全，避免了现有方法可能导致的性能下降问题。", "translation": "蒙特卡洛树搜索（MCTS）改进的一种范式是在树搜索过程中构建和使用状态和/或动作抽象。然而，非精确抽象会引入近似误差，使得在抽象空间中无法收敛到最优动作。因此，正如Xu等人作为弹性蒙特卡洛树搜索的一个组件所提出的，抽象算法最终应该丢弃抽象。在本文中，我们提出了两种新颖的抽象丢弃方案，即OGA-IAAD和OGA-CAD，它们可以带来明显的性能改进，同时是安全的，因为与Xu的丢弃方法相反，丢弃操作绝不会导致任何明显的性能下降。OGA-IAAD专为时间关键型设置设计，而OGA-CAD旨在在相同迭代次数下提高MCTS性能。", "summary": "本文针对蒙特卡洛树搜索（MCTS）中非精确抽象导致的收敛问题，提出了两种新的抽象丢弃方案：OGA-IAAD和OGA-CAD。这些方法旨在解决现有抽象丢弃方案可能导致的性能下降问题。OGA-IAAD适用于时间敏感场景，OGA-CAD则用于在相同迭代次数下提升MCTS性能。实验结果表明，这两种新方案在实现显著性能提升的同时，能够确保丢弃过程的安全性，避免了性能退化。", "keywords": "蒙特卡洛树搜索, 抽象丢弃, OGA-IAAD, OGA-CAD, 性能提升", "comments": "本文的创新点在于提出了两种新的、安全的抽象丢弃方法，解决了MCTS中抽象引入的近似误差和现有丢弃方法可能带来的性能下降问题。其重要性在于为MCTS的性能提升提供了一种更可靠的机制，特别是在时间受限的应用中。"}}
{"id": "2507.02322", "title": "Neural Network-based Study for Rice Leaf Disease Recognition and Classification: A Comparative Analysis Between Feature-based Model and Direct Imaging Model", "authors": ["Farida Siddiqi Prity", "Mirza Raquib", "Saydul Akbar Murad", "Md. Jubayar Alam Rafi", "Md. Khairul Bashar Bhuiyan", "Anupam Kumar Bairagi"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02322v1", "summary": "Rice leaf diseases significantly reduce productivity and cause economic\nlosses, highlighting the need for early detection to enable effective\nmanagement and improve yields. This study proposes Artificial Neural Network\n(ANN)-based image-processing techniques for timely classification and\nrecognition of rice diseases. Despite the prevailing approach of directly\ninputting images of rice leaves into ANNs, there is a noticeable absence of\nthorough comparative analysis between the Feature Analysis Detection Model\n(FADM) and Direct Image-Centric Detection Model (DICDM), specifically when it\ncomes to evaluating the effectiveness of Feature Extraction Algorithms (FEAs).\nHence, this research presents initial experiments on the Feature Analysis\nDetection Model, utilizing various image Feature Extraction Algorithms,\nDimensionality Reduction Algorithms (DRAs), Feature Selection Algorithms\n(FSAs), and Extreme Learning Machine (ELM). The experiments are carried out on\ndatasets encompassing bacterial leaf blight, brown spot, leaf blast, leaf\nscald, Sheath blight rot, and healthy leaf, utilizing 10-fold Cross-Validation\nmethod. A Direct Image-Centric Detection Model is established without the\nutilization of any FEA, and the evaluation of classification performance relies\non different metrics. Ultimately, an exhaustive contrast is performed between\nthe achievements of the Feature Analysis Detection Model and Direct\nImage-Centric Detection Model in classifying rice leaf diseases. The results\nreveal that the highest performance is attained using the Feature Analysis\nDetection Model. The adoption of the proposed Feature Analysis Detection Model\nfor detecting rice leaf diseases holds excellent potential for improving crop\nhealth, minimizing yield losses, and enhancing overall productivity and\nsustainability of rice farming.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02322v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于神经网络的水稻叶部病害识别与分类研究：基于特征模型与直接图像模型的比较分析", "tldr": "本研究比较了基于特征分析检测模型（FADM）和直接图像中心检测模型（DICDM）在水稻叶部病害识别和分类中的性能，结果表明FADM表现最佳。", "motivation": "水稻叶部病害严重降低产量并造成经济损失，因此早期检测至关重要。尽管将图像直接输入人工神经网络是普遍做法，但缺乏对特征分析检测模型（FADM）和直接图像中心检测模型（DICDM）之间有效性的全面比较分析，特别是在评估特征提取算法（FEA）方面。", "method": "本研究提出基于人工神经网络（ANN）的图像处理技术，用于水稻病害的及时分类和识别。实验在包含多种水稻叶部病害和健康叶片的数据集上进行，采用10折交叉验证方法。其中，特征分析检测模型（FADM）利用多种图像特征提取算法（FEA）、降维算法（DRA）、特征选择算法（FSA）和极限学习机（ELM），而直接图像中心检测模型（DICDM）则不使用任何FEA，并根据不同指标评估分类性能。最后对两种模型的性能进行全面对比。", "result": "结果显示，特征分析检测模型（FADM）取得了最高性能。", "conclusion": "所提出的特征分析检测模型（FADM）在检测水稻叶部病害方面具有巨大潜力，有助于改善作物健康，减少产量损失，并提高水稻种植的整体生产力和可持续性。", "translation": "水稻叶部病害显著降低生产力并造成经济损失，这凸显了早期检测以实现有效管理和提高产量的必要性。本研究提出基于人工神经网络（ANN）的图像处理技术，用于水稻病害的及时分类和识别。尽管将水稻叶片图像直接输入人工神经网络是普遍做法，但在评估特征提取算法（FEA）的有效性方面，特征分析检测模型（FADM）和直接图像中心检测模型（DICDM）之间明显缺乏全面的比较分析。因此，本研究展示了特征分析检测模型的初步实验，利用各种图像特征提取算法、降维算法（DRA）、特征选择算法（FSA）和极限学习机（ELM）。实验在包含细菌性叶枯病、褐斑病、稻瘟病、叶鞘腐烂病、稻曲病和健康叶片的数据集上进行，采用10折交叉验证方法。建立了一个不使用任何FEA的直接图像中心检测模型，分类性能的评估依赖于不同的指标。最终，对特征分析检测模型和直接图像中心检测模型在水稻叶部病害分类方面的成果进行了详尽的对比。结果显示，使用特征分析检测模型取得了最高性能。所提出的特征分析检测模型在检测水稻叶部病害方面的应用具有巨大潜力，可改善作物健康，最大限度地减少产量损失，并提高水稻种植的整体生产力和可持续性。", "summary": "本研究旨在比较基于特征分析检测模型（FADM）和直接图像中心检测模型（DICDM）在水稻叶部病害识别与分类中的有效性。研究提出了一种基于人工神经网络（ANN）的图像处理技术，并对FADM进行了实验，使用了多种特征提取、降维和特征选择算法以及极限学习机。同时，建立了一个不使用特征提取算法的DICDM。实验在多种水稻叶部病害数据集上进行，采用10折交叉验证。结果表明，FADM表现出更高的性能，这表明其在提高水稻健康、减少产量损失和增强农业可持续性方面具有巨大潜力。", "keywords": "水稻叶部病害, 神经网络, 特征提取, 图像分类, 病害识别", "comments": "该论文的创新点在于对两种主要基于神经网络的图像识别模型——基于特征的模型和直接图像模型——进行了详细的比较分析，填补了现有研究中缺乏此类比较的空白。其重要性在于为水稻叶部病害的早期诊断提供了更优的解决方案，对农业生产具有直接的积极影响，有助于提高作物产量和可持续性。"}}
{"id": "2507.02310", "title": "Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment", "authors": ["Alif Ashrafee", "Jedrzej Kozal", "Michal Wozniak", "Bartosz Krawczyk"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02310v1", "summary": "Traditional continual learning methods prioritize knowledge retention and\nfocus primarily on mitigating catastrophic forgetting, implicitly assuming that\nthe data distribution of previously learned tasks remains static. This\noverlooks the dynamic nature of real-world data streams, where concept drift\npermanently alters previously seen data and demands both stability and rapid\nadaptation.\n  We introduce a holistic framework for continual learning under concept drift\nthat simulates realistic scenarios by evolving task distributions. As a\nbaseline, we consider Full Relearning (FR), in which the model is retrained\nfrom scratch on newly labeled samples from the drifted distribution. While\neffective, this approach incurs substantial annotation and computational\noverhead. To address these limitations, we propose Adaptive Memory Realignment\n(AMR), a lightweight alternative that equips rehearsal-based learners with a\ndrift-aware adaptation mechanism. AMR selectively removes outdated samples of\ndrifted classes from the replay buffer and repopulates it with a small number\nof up-to-date instances, effectively realigning memory with the new\ndistribution. This targeted resampling matches the performance of FR while\nreducing the need for labeled data and computation by orders of magnitude.\n  To enable reproducible evaluation, we introduce four concept-drift variants\nof standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and\nTiny-ImageNet-CD, where previously seen classes reappear with shifted\nrepresentations. Comprehensive experiments on these datasets using several\nrehearsal-based baselines show that AMR consistently counters concept drift,\nmaintaining high accuracy with minimal overhead. These results position AMR as\na scalable solution that reconciles stability and plasticity in non-stationary\ncontinual learning environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02310v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "概念漂移下自适应内存重校准的整体持续学习", "tldr": "传统持续学习在概念漂移下表现不佳。本文提出自适应内存重校准（AMR），通过选择性更新内存来高效适应不断变化的数据分布，以显著降低的开销实现与完全重新学习相似的性能。", "motivation": "传统的持续学习方法优先考虑知识保留并主要侧重于减轻灾难性遗忘，隐含地假设先前学习任务的数据分布保持静态，这忽略了真实世界数据流的动态性质，其中概念漂移永久性地改变了先前见过的数据，并要求模型同时具备稳定性和快速适应性。完全重新学习（FR）虽然有效，但会产生大量的标注和计算开销。", "method": "本文引入了一个在概念漂移下进行持续学习的整体框架，通过演变任务分布来模拟真实场景。作为FR的轻量级替代方案，提出了自适应内存重校准（AMR），它为基于排练的学习器配备了漂移感知适应机制。AMR选择性地从重放缓冲区中移除已漂移类的过时样本，并用少量最新实例重新填充，从而有效地将内存与新分布对齐。为实现可重现评估，引入了四种标准视觉基准的概念漂移变体：Fashion-MNIST-CD、CIFAR10-CD、CIFAR100-CD和Tiny-ImageNet-CD。", "result": "AMR与完全重新学习（FR）的性能相匹配，同时将标记数据和计算的需求降低了几个数量级。在新的概念漂移数据集上进行的综合实验表明，AMR持续对抗概念漂移，以最小的开销保持高精度，并且在几种基于排练的基线上表现稳定。", "conclusion": "AMR是一种可扩展的解决方案，能够在非静态持续学习环境中有效地协调稳定性和可塑性，成功应对概念漂移问题。", "translation": "传统的持续学习方法优先考虑知识保留，主要侧重于减轻灾难性遗忘，隐含地假设先前学习任务的数据分布保持静态。这忽略了现实世界数据流的动态性质，其中概念漂移永久性地改变了先前见过的数据，并要求模型同时具备稳定性和快速适应性。\n我们引入了一个在概念漂移下进行持续学习的整体框架，通过演变任务分布来模拟真实场景。作为基线，我们考虑了完全重新学习（FR），其中模型从头开始使用来自漂移分布的新标记样本进行重新训练。尽管有效，但这种方法会产生大量的标注和计算开销。为了解决这些限制，我们提出了自适应内存重校准（AMR），这是一种轻量级替代方案，它为基于排练的学习器配备了漂移感知适应机制。AMR选择性地从重放缓冲区中移除已漂移类的过时样本，并用少量最新实例重新填充，从而有效地将内存与新分布对齐。这种有针对性的重采样与FR的性能相匹配，同时将标记数据和计算的需求降低了几个数量级。\n为了实现可重现的评估，我们引入了标准视觉基准的四种概念漂移变体：Fashion-MNIST-CD、CIFAR10-CD、CIFAR100-CD和Tiny-ImageNet-CD，其中先前见过的类以偏移的表示形式重新出现。对这些数据集使用几种基于排练的基线进行的综合实验表明，AMR持续对抗概念漂移，以最小的开销保持高精度。这些结果将AMR定位为一种可扩展的解决方案，可以在非静态持续学习环境中协调稳定性和可塑性。", "summary": "本文针对持续学习中传统方法在概念漂移下失效的问题，提出了一种新颖的轻量级方法——自适应内存重校准（AMR）。AMR通过选择性地用最新样本更新重放缓冲区，以适应不断演变的数据分布。与完全重新学习相比，AMR在大幅降低计算和数据标注成本的同时，实现了相似的性能。该论文还引入了新的基准数据集以进行可重现评估，并证明了AMR在概念漂移下保持准确性和稳定性的有效性。", "keywords": "持续学习, 概念漂移, 内存重校准, 灾难性遗忘, 数据流", "comments": "该论文通过明确解决概念漂移问题，对持续学习领域做出了重要贡献，这是持续学习中一个关键但经常被忽视的方面。所提出的AMR方法在目标内存重校准方面具有创新性，提供了一种实用且高效的解决方案，在不增加完全重新学习的高成本的情况下，平衡了稳定性和可塑性。引入新的概念漂移基准数据集对于未来的研究可重现性也具有重要价值。"}}
{"id": "2507.02726", "title": "Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving", "authors": ["Matthieu Zimmer", "Xiaotong Ji", "Rasul Tutunov", "Anthony Bordg", "Jun Wang", "Haitham Bou Ammar"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02726v1", "summary": "Reasoning remains a challenging task for large language models (LLMs),\nespecially within the logically constrained environment of automated theorem\nproving (ATP), due to sparse rewards and the vast scale of proofs. These\nchallenges are amplified in benchmarks like PutnamBench, which contains\nuniversity-level problems requiring complex, multi-step reasoning. To address\nthis, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new\nframework in which agents generate and pursue their subgoals based on the\nevolving proof state. Given this more structured generation of goals, the\nresulting problem becomes more amenable to search. We then apply Monte Carlo\nTree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our\napproach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs\nfor subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)\nsolves 26 problems, achieving new state-of-the-art results with models at this\nscale.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02726v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Bourbaki：用于定理证明的自生成和目标条件MDPs", "tldr": "Bourbaki引入了自生成目标条件MDPs（sG-MDPs）框架，并结合MCTS类算法，在PutnamBench上解决了26个定理证明问题，为该规模模型设定了新的SOTA。", "motivation": "大型语言模型（LLMs）在自动化定理证明（ATP）等逻辑受限环境中的推理仍然面临挑战，主要原因是奖励稀疏和证明规模巨大。尤其是在PutnamBench这种需要复杂多步骤推理的大学级别问题基准测试中，这些挑战更为突出。", "method": "研究引入了自生成目标条件MDPs（sG-MDPs）框架，其中代理根据不断演变的证明状态生成并追求子目标，使问题更适合搜索。然后，应用蒙特卡洛树搜索（MCTS）类算法来解决sG-MDP。该方法在Bourbaki (7B) 中实现，这是一个模块化系统，能够集成多个7B LLM用于子目标生成和策略合成。", "result": "在PutnamBench基准测试中，Bourbaki (7B) 成功解决了26个问题，在该规模的模型中取得了新的最先进（state-of-the-art）结果。", "conclusion": "通过引入自生成目标条件MDPs和结合MCTS类算法，Bourbaki (7B) 有效提升了大型语言模型在自动化定理证明领域的性能，并取得了显著的最新成果。", "translation": "对于大型语言模型（LLM）而言，推理仍然是一项具有挑战性的任务，尤其是在自动化定理证明（ATP）这种逻辑受限的环境中，原因在于奖励稀疏和证明规模巨大。这些挑战在PutnamBench等基准测试中被放大，该基准测试包含需要复杂、多步骤推理的大学级别问题。为解决此问题，我们引入了自生成目标条件MDPs（sG-MDPs），这是一个新框架，代理在此框架中根据不断演变的证明状态生成并追求其子目标。鉴于这种更结构化的目标生成，所产生的问题变得更适合搜索。随后，我们应用蒙特卡洛树搜索（MCTS）类算法来解决sG-MDP，并在Bourbaki (7B) 中实例化了我们的方法，这是一个模块化系统，可以集成多个7B LLM用于子目标生成和策略合成。在PutnamBench上，Bourbaki (7B) 解决了26个问题，在该规模的模型中取得了新的最先进结果。", "summary": "该研究针对大型语言模型在自动化定理证明中面临的稀疏奖励和证明规模巨大等挑战，提出了自生成目标条件MDPs（sG-MDPs）框架。该框架允许代理根据证明状态生成并追求子目标，从而使问题更易于搜索。通过结合蒙特卡洛树搜索（MCTS）类算法，并在名为Bourbaki (7B) 的模块化系统中实现，该系统集成了多个7B LLM。在PutnamBench基准测试中，Bourbaki (7B) 解决了26个问题，取得了该规模模型的最新最佳成果。", "keywords": "定理证明, 大型语言模型, 马尔可夫决策过程, 蒙特卡洛树搜索, PutnamBench", "comments": "该论文的创新点在于引入了自生成目标条件MDPs（sG-MDPs）框架，通过结构化的子目标生成，将复杂的定理证明问题转化为更易于搜索的形式。Bourbaki (7B) 作为一个模块化系统，能够集成多个LLM进行子目标生成和策略合成，这表明了LLM在复杂推理任务中通过模块化和结构化方法可以取得显著进步。该方法在PutnamBench上取得的最新成果，证明了其在解决大学级别数学问题方面的有效性和重要性。"}}
{"id": "2507.02349", "title": "Two-Steps Neural Networks for an Automated Cerebrovascular Landmark Detection", "authors": ["Rafic Nader", "Vincent L'Allinec", "Romain Bourcier", "Florent Autrusseau"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02349v1", "summary": "Intracranial aneurysms (ICA) commonly occur in specific segments of the\nCircle of Willis (CoW), primarily, onto thirteen major arterial bifurcations.\nAn accurate detection of these critical landmarks is necessary for a prompt and\nefficient diagnosis. We introduce a fully automated landmark detection approach\nfor CoW bifurcations using a two-step neural networks process. Initially, an\nobject detection network identifies regions of interest (ROIs) proximal to the\nlandmark locations. Subsequently, a modified U-Net with deep supervision is\nexploited to accurately locate the bifurcations. This two-step method reduces\nvarious problems, such as the missed detections caused by two landmarks being\nclose to each other and having similar visual characteristics, especially when\nprocessing the complete MRA Time-of-Flight (TOF). Additionally, it accounts for\nthe anatomical variability of the CoW, which affects the number of detectable\nlandmarks per scan. We assessed the effectiveness of our approach using two\ncerebral MRA datasets: our In-House dataset which had varying numbers of\nlandmarks, and a public dataset with standardized landmark configuration. Our\nexperimental results demonstrate that our method achieves the highest level of\nperformance on a bifurcation detection task.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02349v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "用于自动化脑血管标志物检测的两步神经网络", "tldr": "本文提出一种两步神经网络方法，用于自动检测脑部威利斯环的动脉分叉点，该方法通过先识别感兴趣区域，再精确定位，解决了传统方法中存在的漏检和解剖变异性问题，并在MRA数据集上取得了最佳性能。", "motivation": "准确检测颅内动脉瘤（ICA）常发生的威利斯环（CoW）主要动脉分叉点对于及时有效的诊断至关重要。现有的方法可能存在漏检问题，尤其是在处理完整MRA TOF数据时，以及无法充分考虑CoW的解剖变异性。", "method": "本文提出一种两步神经网络方法。首先，一个目标检测网络识别靠近标志物位置的感兴趣区域（ROIs）。其次，一个带有深度监督的修改版U-Net被用来精确地定位分叉点。这种方法旨在减少因两个标志物接近且视觉特征相似导致的漏检问题，并考虑CoW的解剖变异性。", "result": "该方法在两个脑部MRA数据集（一个内部数据集和一个公共数据集）上进行了评估，实验结果表明，该方法在分叉点检测任务中取得了最高水平的性能。", "conclusion": "所提出的两步神经网络方法能够有效且准确地自动化检测脑血管分叉标志物，解决了现有方法的局限性，并在实际应用中表现出卓越的性能。", "translation": "颅内动脉瘤（ICA）通常发生在威利斯环（CoW）的特定节段，主要是十三个主要的动脉分叉处。准确检测这些关键标志物对于及时有效的诊断是必要的。我们引入了一种使用两步神经网络过程的CoW分叉点全自动标志物检测方法。最初，一个目标检测网络识别靠近标志物位置的感兴趣区域（ROIs）。随后，一个带有深度监督的修改版U-Net被用来精确地定位分叉点。这种两步方法减少了各种问题，例如由于两个标志物彼此靠近且具有相似视觉特征而导致的漏检，尤其是在处理完整的MRA飞行时间（TOF）时。此外，它还考虑了CoW的解剖变异性，这会影响每次扫描中可检测标志物的数量。我们使用两个脑部MRA数据集评估了我们方法的有效性：我们的内部数据集，其中包含不同数量的标志物，以及一个具有标准化标志物配置的公共数据集。我们的实验结果表明，我们的方法在分叉点检测任务中取得了最高水平的性能。", "summary": "本文提出一种用于自动化检测脑部威利斯环动脉分叉点的两步神经网络方法。该方法首先通过目标检测网络识别感兴趣区域，然后利用改进的U-Net进行精确的标志物定位。该策略有效解决了现有方法中常见的漏检问题和解剖变异性挑战，并在多个MRA数据集上展现出卓越的检测性能，证明了其在颅内动脉瘤诊断中的潜在应用价值。", "keywords": "脑血管标志物检测, 两步神经网络, 颅内动脉瘤, 威利斯环, U-Net", "comments": "本文的创新点在于其提出的两步神经网络架构，通过结合目标检测和精确分割，有效解决了脑血管标志物检测中常见的漏检和解剖变异性问题。这种分阶段处理方法提高了检测的准确性和鲁棒性，对于颅内动脉瘤的早期诊断具有重要意义。该方法在实际医疗影像分析中具有很高的实用价值。"}}
{"id": "2507.02315", "title": "Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo", "authors": ["Sooyeon Kim", "Giung Nam", "Juho Lee"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02315v1", "summary": "Recent work has framed constrained text generation with autoregressive\nlanguage models as a probabilistic inference problem. Among these, Zhao et al.\n(2024) introduced a promising approach based on twisted Sequential Monte Carlo,\nwhich incorporates learned twist functions and twist-induced proposals to guide\nthe generation process. However, in constrained generation settings where the\ntarget distribution concentrates on outputs that are unlikely under the base\nmodel, learning becomes challenging due to sparse and uninformative reward\nsignals. We show that iteratively refining the base model through\nself-distillation alleviates this issue by making the model progressively more\naligned with the target, leading to substantial gains in generation quality.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02315v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "改进语言模型中受限生成的方法：通过自蒸馏扭曲序列蒙特卡洛", "tldr": "一种结合自蒸馏和扭曲序列蒙特卡洛的方法，通过使基础模型逐步与目标对齐，改善了语言模型中的受限文本生成，尤其是在目标输出在基础模型下不太可能出现的情况下。", "motivation": "在受限文本生成中，当目标分布在基础模型下不太可能出现时，学习变得困难，因为奖励信号稀疏且信息量不足。现有的基于扭曲序列蒙特卡洛的方法在这种情况下表现不佳。", "method": "通过自蒸馏迭代地优化基础模型，使其逐步与目标分布对齐。该方法建立在赵等人（2024）提出的扭曲序列蒙特卡洛方法之上。", "result": "通过自蒸馏迭代优化基础模型，缓解了稀疏奖励信号的问题，使模型逐步与目标对齐，从而显著提升了生成质量。", "conclusion": "自蒸馏可以有效改进语言模型中的受限文本生成，尤其是在目标输出稀有的挑战性场景中，通过增强模型与目标分布的对齐。", "translation": "最近的研究将自回归语言模型中的受限文本生成视为一个概率推理问题。其中，赵等人（2024）提出了一种基于扭曲序列蒙特卡洛的有前景的方法，该方法结合了学习到的扭曲函数和扭曲诱导的提议来指导生成过程。然而，在受限生成设置中，当目标分布集中在基础模型下不太可能出现的输出上时，由于稀疏和无信息奖励信号，学习变得具有挑战性。我们表明，通过自蒸馏迭代地优化基础模型可以缓解这个问题，使模型逐步与目标对齐，从而在生成质量上获得显著提升。", "summary": "本文解决了语言模型中受限文本生成的挑战，特别是当目标输出在基础模型下不太可能出现时，这导致由于稀疏奖励而难以学习。作者在扭曲序列蒙特卡洛（SMC）方法的基础上，提出了一种迭代的自蒸馏方法来优化基础模型。这种优化使模型逐步与目标分布对齐，并被证明能够显著提高生成质量。", "keywords": "受限生成, 语言模型, 自蒸馏, 序列蒙特卡洛, 概率推理", "comments": "这篇论文为受限文本生成中的一个已知问题引入了一个创新的解决方案。使用自蒸馏来优化基础模型并改进与目标分布的对齐，是克服稀疏奖励信号问题的一个巧妙方法，稀疏奖励信号是生成任务中类似强化学习设置的常见挑战。该方法增强了扭曲序列蒙特卡洛方法在更具挑战性的受限生成任务中的适用性。"}}
{"id": "2507.02760", "title": "Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work", "authors": ["Guangwei Zhang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02760v1", "summary": "The capabilities of Large Language Models (LLMs) have opened new frontiers\nfor interacting with complex, domain-specific knowledge. However, prevailing\nmethods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic\nAI, while powerful, often struggle with tasks that demand deep, procedural, and\nmethodological reasoning inherent to expert domains. RAG provides factual\ncontext but fails to convey logical frameworks; autonomous agents can be\ninefficient and unpredictable without domain-specific heuristics. To bridge\nthis gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm\nfocused on systematically translating human expert knowledge, often expressed\nin natural language documents, into a machine-executable Knowledge Protocol\n(KP). KPE shifts the focus from merely augmenting LLMs with fragmented\ninformation to endowing them with a domain's intrinsic logic, operational\nstrategies, and methodological principles. We argue that a well-engineered\nKnowledge Protocol allows a generalist LLM to function as a specialist, capable\nof decomposing abstract queries and executing complex, multi-step tasks. This\nposition paper defines the core principles of KPE, differentiates it from\nrelated concepts, and illustrates its potential applicability across diverse\nfields such as law and bioinformatics, positing it as a foundational\nmethodology for the future of human-AI collaboration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02760v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "知识协议工程：领域特定知识工作中AI的新范式", "tldr": "知识协议工程（KPE）是一种新范式，旨在将人类专家知识转化为机器可执行的知识协议，以解决大型语言模型在领域特定复杂推理任务中的不足，使通用LLM能够像专家一样执行多步骤任务。", "motivation": "现有的大型语言模型（LLMs）方法，如检索增强生成（RAG）和通用代理AI，在处理需要深层、程序性和方法论推理的专家领域任务时表现不足。RAG无法传达逻辑框架，而自主代理则可能效率低下且不可预测。", "method": "本文提出了知识协议工程（KPE），这是一种将人类专家知识（通常以自然语言文档表达）系统地转化为机器可执行的知识协议（KP）的新范式。KPE旨在赋予LLMs领域固有的逻辑、操作策略和方法论原则。", "result": "通过精心设计的知识协议，通用LLM能够像专家一样运作，分解抽象查询并执行复杂的多步骤任务。", "conclusion": "知识协议工程（KPE）定义了其核心原则，并被定位为未来人机协作的基础方法，在法律和生物信息学等领域具有潜在应用前景。", "translation": "大型语言模型（LLMs）的能力为与复杂、领域特定知识的交互开辟了新领域。然而，当前流行的方法，如检索增强生成（RAG）和通用代理AI，虽然功能强大，但在处理需要专家领域固有的深层、程序性和方法论推理的任务时常常力不从心。RAG提供事实上下文，但未能传达逻辑框架；自主代理在没有领域特定启发式方法的情况下可能效率低下且不可预测。为了弥补这一差距，我们引入了知识协议工程（KPE），这是一种新范式，专注于系统地将人类专家知识（通常以自然语言文档表达）转化为机器可执行的知识协议（KP）。KPE将重点从仅仅用碎片化信息增强LLMs转向赋予它们领域固有的逻辑、操作策略和方法论原则。我们认为，一个精心设计的知识协议允许通用LLM像专家一样运作，能够分解抽象查询并执行复杂的、多步骤的任务。这篇立场论文定义了KPE的核心原则，将其与相关概念区分开来，并阐述了其在法律和生物信息学等不同领域的潜在适用性，将其定位为未来人机协作的基础方法论。", "summary": "本文提出了一种名为知识协议工程（KPE）的新范式，旨在解决大型语言模型（LLMs）在处理复杂领域特定知识工作时遇到的挑战，特别是在需要深层逻辑和程序性推理的任务中。KPE通过将人类专家知识系统地转化为机器可执行的知识协议，使通用LLM能够掌握领域内在的逻辑和操作策略，从而像专家一样执行多步骤任务。该论文阐述了KPE的核心原则、与现有方法的区别及其在不同领域的潜在应用，并将其视为未来人机协作的关键方法。", "keywords": "知识协议工程,大型语言模型,领域特定知识,人机协作,知识协议", "comments": "该论文提出了一个创新性的概念——知识协议工程（KPE），旨在弥补当前LLMs在处理领域特定、深层推理任务时的不足。其核心在于将人类专家知识转化为结构化的“知识协议”，从而赋予LLM更强的领域专业性和执行复杂任务的能力。这对于推动LLM从通用型向专业型发展具有重要意义，尤其是在高门槛的专业领域。如果能有效实施，KPE有望显著提升AI在知识密集型工作中的实用性和可靠性。"}}
{"id": "2507.02354", "title": "Lightweight Shrimp Disease Detection Research Based on YOLOv8n", "authors": ["Fei Yuhuan", "Wang Gengchen", "Liu Fenghao", "Zang Ran", "Sun Xufei", "Chang Hao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      in Chinese language", "url": "http://arxiv.org/abs/2507.02354v1", "summary": "Shrimp diseases are one of the primary causes of economic losses in shrimp\naquaculture. To prevent disease transmission and enhance intelligent detection\nefficiency in shrimp farming, this paper proposes a lightweight network\narchitecture based on YOLOv8n. First, by designing the RLDD detection head and\nC2f-EMCM module, the model reduces computational complexity while maintaining\ndetection accuracy, improving computational efficiency. Subsequently, an\nimproved SegNext_Attention self-attention mechanism is introduced to further\nenhance the model's feature extraction capability, enabling more precise\nidentification of disease characteristics. Extensive experiments, including\nablation studies and comparative evaluations, are conducted on a\nself-constructed shrimp disease dataset, with generalization tests extended to\nthe URPC2020 dataset. Results demonstrate that the proposed model achieves a\n32.3% reduction in parameters compared to the original YOLOv8n, with a mAP@0.5\nof 92.7% (3% improvement over YOLOv8n). Additionally, the model outperforms\nother lightweight YOLO-series models in mAP@0.5, parameter count, and model\nsize. Generalization experiments on the URPC2020 dataset further validate the\nmodel's robustness, showing a 4.1% increase in mAP@0.5 compared to YOLOv8n. The\nproposed method achieves an optimal balance between accuracy and efficiency,\nproviding reliable technical support for intelligent disease detection in\nshrimp aquaculture.", "comment": "in Chinese language", "pdf_url": "http://arxiv.org/pdf/2507.02354v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于YOLOv8n的轻量级对虾疾病检测研究", "tldr": "本文基于YOLOv8n提出了一种轻量级网络，通过引入RLDD检测头、C2f-EMCM模块和改进的SegNext_Attention机制，显著降低了模型参数并提升了对虾疾病检测的准确性和效率。", "motivation": "对虾疾病是水产养殖中经济损失的主要原因之一。为防止疾病传播并提高对虾养殖中的智能检测效率，研究旨在开发一种轻量级、高效且准确的疾病检测模型。", "method": "本文提出了一种基于YOLOv8n的轻量级网络架构。该方法设计了RLDD检测头和C2f-EMCM模块以降低计算复杂度并提高效率；同时引入改进的SegNext_Attention自注意力机制以增强特征提取能力。模型在自建对虾疾病数据集上进行实验，并扩展到URPC2020数据集进行泛化测试。", "result": "提出的模型相较于原始YOLOv8n，参数量减少了32.3%，mAP@0.5达到92.7%（提升3%）。该模型在mAP@0.5、参数量和模型大小方面均优于其他轻量级YOLO系列模型。在URPC2020数据集上的泛化实验显示，mAP@0.5提升了4.1%。", "conclusion": "提出的方法在准确性和效率之间取得了最佳平衡，为对虾水产养殖中的智能疾病检测提供了可靠的技术支持。", "translation": "对虾疾病是对虾水产养殖中经济损失的主要原因之一。为了防止疾病传播并提高对虾养殖中的智能检测效率，本文提出了一种基于YOLOv8n的轻量级网络架构。首先，通过设计RLDD检测头和C2f-EMCM模块，模型在保持检测精度的同时降低了计算复杂度，提高了计算效率。其次，引入改进的SegNext_Attention自注意力机制，进一步增强了模型的特征提取能力，从而能够更精确地识别疾病特征。在自建对虾疾病数据集上进行了广泛的实验，包括消融研究和比较评估，并将泛化测试扩展到URPC2020数据集。结果表明，所提出的模型与原始YOLOv8n相比，参数量减少了32.3%，mAP@0.5达到92.7%（比YOLOv8n提高了3%）。此外，该模型在mAP@0.5、参数量和模型大小方面均优于其他轻量级YOLO系列模型。在URPC2020数据集上的泛化实验进一步验证了模型的鲁棒性，mAP@0.5比YOLOv8n提高了4.1%。所提出的方法在准确性和效率之间取得了最佳平衡，为对虾水产养殖中的智能疾病检测提供了可靠的技术支持。", "summary": "本文针对对虾疾病导致的经济损失问题，提出了一种基于YOLOv8n的轻量级对虾疾病检测网络。通过引入RLDD检测头、C2f-EMCM模块和改进的SegNext_Attention机制，该模型显著降低了参数量（32.3%）并提升了检测精度（mAP@0.5提升3%），同时在泛化能力和与其他轻量级模型的比较中表现出色，实现了准确性和效率的优化平衡，为智能对虾疾病检测提供了实用方案。", "keywords": "对虾疾病检测, YOLOv8n, 轻量化网络, 目标检测, 深度学习", "comments": "这篇论文通过对YOLOv8n进行轻量化改进，有效地解决了对虾疾病检测中对模型效率和准确性的需求，尤其是在资源受限的实际养殖环境中。RLDD检测头、C2f-EMCM模块和改进的SegNext_Attention机制的结合是其创新点，实现了参数量大幅减少的同时，保持甚至提升了检测性能，并在泛化性方面也表现出色。这项工作对于推动智能水产养殖技术具有重要意义。"}}
{"id": "2507.02856", "title": "Answer Matching Outperforms Multiple Choice for Language Model Evaluation", "authors": ["Nikhil Chandak", "Shashwat Goel", "Ameya Prabhu", "Moritz Hardt", "Jonas Geiping"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      34 pages, Code is available at this https URL", "url": "http://arxiv.org/abs/2507.02856v1", "summary": "Multiple choice benchmarks have long been the workhorse of language model\nevaluation because grading multiple choice is objective and easy to automate.\nHowever, we show multiple choice questions from popular benchmarks can often be\nanswered without even seeing the question. These shortcuts arise from a\nfundamental limitation of discriminative evaluation not shared by evaluations\nof the model's free-form, generative answers. Until recently, there appeared to\nbe no viable, scalable alternative to multiple choice--but, we show that this\nhas changed. We consider generative evaluation via what we call answer\nmatching: Give the candidate model the question without the options, have it\ngenerate a free-form response, then use a modern language model with the\nreference answer to determine if the response matches the reference. To compare\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\nevaluation approach. We find answer matching using recent models--even small\nones--achieves near-perfect agreement, in the range of inter-annotator\nagreement. In contrast, both multiple choice evaluation and using\nLLM-as-a-judge without reference answers aligns poorly with human grading.\nImproving evaluations via answer matching is not merely a conceptual concern:\nthe rankings of several models change significantly when evaluating their\nfree-form responses with answer matching. In light of these findings, we\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\nmatching.", "comment": "34 pages, Code is available at\n  https://github.com/nikhilchandak/answer-matching", "pdf_url": "http://arxiv.org/pdf/2507.02856v1", "cate": "cs.CL", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "答案匹配在语言模型评估中优于多项选择", "tldr": "研究表明，答案匹配比多项选择更有效地评估语言模型，因为它能克服多项选择的捷径问题并与人工评分高度一致。", "motivation": "多项选择基准虽然易于自动化，但存在根本性限制，允许语言模型通过捷径回答问题，而无需真正理解，导致评估结果不准确。此前缺乏可行的、可扩展的替代方案。", "method": "提出“答案匹配”评估方法：让候选模型对问题生成自由形式回答，然后使用另一个现代语言模型结合参考答案来判断回答是否匹配。通过对MMLU-Pro和GPQA-Diamond进行人工标注，获取人类评分数据，并测量不同评估方法与人类评分的一致性。", "result": "使用最新模型（包括小型模型）的答案匹配与人工评分达到接近完美的一致性，与标注者间一致性相当。多项选择评估和不带参考答案的“LLM-as-a-judge”方法与人工评分的对齐度很差。采用答案匹配评估自由形式回答时，多个模型的排名显著改变。", "conclusion": "答案匹配是评估语言模型的更优方法，能有效克服多项选择的局限性，评估生态系统应从多项选择转向答案匹配。", "translation": "长期以来，多项选择基准一直是语言模型评估的主力，因为多项选择的评分客观且易于自动化。然而，我们发现流行基准中的多项选择题通常在甚至没有看到问题的情况下就能被回答。这些捷径源于判别式评估的一个根本限制，而模型自由形式的生成式答案评估则没有这个限制。直到最近，似乎还没有一个可行、可扩展的多项选择替代方案——但我们表明这种情况已经改变。我们通过我们称之为答案匹配的方式进行生成式评估：给候选模型问题而不提供选项，让它生成一个自由形式的回答，然后使用一个现代语言模型和参考答案来确定该回答是否与参考答案匹配。为了比较不同评估策略的有效性，我们对MMLU-Pro和GPQA-Diamond进行了标注，以获取人工评分数据，并测量了每种评估方法的与人工评分的一致性。我们发现使用最新模型（甚至是小型模型）的答案匹配达到了接近完美的一致性，达到了标注者之间的一致性范围。相比之下，多项选择评估以及在没有参考答案的情况下使用大型语言模型作为裁判，与人工评分的对齐度都很差。通过答案匹配改进评估不仅仅是一个概念上的问题：当使用答案匹配评估模型的自由形式回答时，几个模型的排名显著改变。鉴于这些发现，我们讨论了如何将评估生态系统从多项选择转向答案匹配。", "summary": "本文提出“答案匹配”作为评估语言模型的优越方法，旨在解决传统多项选择基准中模型利用捷径的局限性。答案匹配要求模型生成自由形式的答案，然后使用另一个语言模型与参考答案进行匹配评估。研究通过对MMLU-Pro和GPQA-Diamond的人工评分数据进行验证，表明答案匹配与人类判断达到了近乎完美的一致性，优于多项选择和基于LLM的无参考答案判断方法。研究结果显示，采用答案匹配可以显著改变模型排名，从而提高语言模型评估的有效性。", "keywords": "语言模型评估, 答案匹配, 多项选择, 生成式评估, 基准测试", "comments": "该论文揭示了当前广泛使用的语言模型评估基准的关键缺陷，并提出了一种实用且可扩展的替代方案。其创新之处在于利用大型语言模型的生成能力进行评估（包括被评估模型和评估者本身），超越了传统的判别式多项选择。论文强调了其重要性，通过观察到模型排名的显著变化，暗示了之前评估可能存在的误导性。尽管该方法依赖于另一个LLM进行“匹配”，但其与人类判断的高度一致性大大缓解了潜在的偏见担忧。这标志着语言模型评估朝着更稳健和有效方向迈出了重要一步。"}}
{"id": "2507.02358", "title": "Holistic Tokenizer for Autoregressive Image Generation", "authors": ["Anlin Zheng", "Haochen Wang", "Yucheng Zhao", "Weipeng Deng", "Tiancai Wang", "Xiangyu Zhang", "Xiaojuan Qi"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      17 pages, 10 figures", "url": "http://arxiv.org/abs/2507.02358v1", "summary": "The vanilla autoregressive image generation model generates visual tokens in\na step-by-step fashion, which limits the ability to capture holistic\nrelationships among token sequences. Moreover, most visual tokenizers map local\nimage patches into latent tokens, leading to limited global information. To\naddress this, we introduce \\textit{Hita}, a novel image tokenizer for\nautoregressive (AR) image generation. It introduces a holistic-to-local\ntokenization scheme with learnable holistic queries and local patch tokens.\nBesides, Hita incorporates two key strategies for improved alignment with the\nAR generation process: 1) it arranges a sequential structure with holistic\ntokens at the beginning followed by patch-level tokens while using causal\nattention to maintain awareness of previous tokens; and 2) before feeding the\nde-quantized tokens into the decoder, Hita adopts a lightweight fusion module\nto control information flow to prioritize holistic tokens. Extensive\nexperiments show that Hita accelerates the training speed of AR generators and\noutperforms those trained with vanilla tokenizers, achieving \\textbf{2.59 FID}\nand \\textbf{281.9 IS} on the ImageNet benchmark. A detailed analysis of the\nholistic representation highlights its ability to capture global image\nproperties such as textures, materials, and shapes. Additionally, Hita also\ndemonstrates effectiveness in zero-shot style transfer and image in-painting.\nThe code is available at\n\\href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}", "comment": "17 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.02358v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "自回归图像生成中的整体式分词器", "tldr": "本文提出Hita，一种新型整体式图像分词器，通过引入整体到局部的分词方案，解决了传统自回归图像生成模型在捕获全局关系和信息方面的不足，显著加速训练并提升性能，在ImageNet上取得2.59 FID和281.9 IS。", "motivation": "香草自回归图像生成模型逐步生成视觉token，限制了捕获token序列间整体关系的能力。大多数视觉分词器将局部图像块映射为潜在token，导致全局信息有限，无法有效捕获全局信息。", "method": "本文引入Hita，一种用于自回归（AR）图像生成的新型图像分词器。它采用整体到局部的分词方案，结合可学习的整体查询和局部块token。Hita还整合了两个关键策略：1) 安排一个序列结构，整体token在前，后跟块级token，同时使用因果注意力以保持对先前token的感知；2) 在将去量化token输入解码器之前，采用轻量级融合模块来控制信息流，优先处理整体token。", "result": "Hita加速了AR生成器的训练速度，性能优于使用香草分词器的模型，在ImageNet基准测试中实现了2.59 FID和281.9 IS。对整体表示的详细分析表明其能捕获纹理、材质、形状等全局图像属性。此外，Hita在零样本风格迁移和图像修复中也表现出有效性。", "conclusion": "Hita作为一种新型整体式图像分词器，通过其整体到局部的分词方案和优化策略，显著提升了自回归图像生成模型的训练效率和性能，并能有效捕获全局图像属性，在多项任务中表现出色。", "translation": "香草自回归图像生成模型以逐步方式生成视觉token，这限制了捕获token序列间整体关系的能力。此外，大多数视觉分词器将局部图像块映射为潜在token，导致全局信息有限。为解决此问题，我们引入了Hita，一种用于自回归（AR）图像生成的新型图像分词器。它引入了一种整体到局部的分词方案，结合可学习的整体查询和局部块token。此外，Hita整合了两个关键策略以更好地与AR生成过程对齐：1) 它安排了一个序列结构，整体token在前，后跟块级token，同时使用因果注意力以保持对先前token的感知；2) 在将去量化token输入解码器之前，Hita采用轻量级融合模块来控制信息流，优先处理整体token。广泛的实验表明，Hita加速了AR生成器的训练速度，并优于使用香草分词器训练的模型，在ImageNet基准测试中实现了2.59 FID 和 281.9 IS。对整体表示的详细分析突出了其捕获全局图像属性（如纹理、材质和形状）的能力。此外，Hita在零样本风格迁移和图像修复中也表现出有效性。代码可在https://github.com/CVMI-Lab/Hita 获取。", "summary": "本文提出Hita，一种用于自回归图像生成的新型整体式图像分词器，旨在解决现有模型在捕获全局关系和信息上的不足。Hita采用整体到局部的分词方案，结合可学习的整体查询和局部块token，并通过优化序列结构和信息流控制来适应AR生成过程。实验证明，Hita能显著加速训练并提升性能，在ImageNet上实现了SOTA结果，并能有效捕获图像的全局属性，同时在零样本风格迁移和图像修复等任务中也展现出良好效果。", "keywords": "图像生成, 自回归模型, 分词器, 全局信息, Hita", "comments": "Hita的创新之处在于其整体到局部的分词策略，以及为自回归生成过程优化的信息流控制。通过引入整体查询来捕获全局上下文信息，它有效解决了传统局部补丁分词器在全局感知上的局限性，显著提升了自回归图像生成模型的效率和质量。其在多项任务上的有效性也证明了该方法的通用性和前景。"}}
{"id": "2507.02342", "title": "DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values", "authors": ["Changhun Kim", "Yechan Mun", "Sangchul Hahn", "Eunho Yang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to ICML 2025 Workshop on Actionable Interpretability. Code is available at this https URL", "url": "http://arxiv.org/abs/2507.02342v1", "summary": "This study proposes DeltaSHAP, a novel explainable artificial intelligence\n(XAI) algorithm specifically designed for online patient monitoring systems. In\nclinical environments, discovering the causes driving patient risk evolution is\ncritical for timely intervention, yet existing XAI methods fail to address the\nunique requirements of clinical time series explanation tasks. To this end,\nDeltaSHAP addresses three key clinical needs: explaining the changes in the\nconsecutive predictions rather than isolated prediction scores, providing both\nmagnitude and direction of feature attributions, and delivering these insights\nin real time. By adapting Shapley values to temporal settings, our approach\naccurately captures feature coalition effects. It further attributes prediction\nchanges using only the actually observed feature combinations, making it\nefficient and practical for time-sensitive clinical applications. We also\nintroduce new evaluation metrics to evaluate the faithfulness of the\nattributions for online time series, and demonstrate through experiments on\nonline patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI\nmethods in both explanation quality as 62% and computational efficiency as 33%\ntime reduction on the MIMIC-III decompensation benchmark. We release our code\nat https://github.com/AITRICS/DeltaSHAP.", "comment": "Accepted to ICML 2025 Workshop on Actionable Interpretability. Code\n  is available at https://github.com/AITRICS/DeltaSHAP", "pdf_url": "http://arxiv.org/pdf/2507.02342v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "DeltaSHAP：利用Shapley值解释在线患者监测中的预测演变", "tldr": "DeltaSHAP是一种新型可解释AI算法，专为在线患者监测设计，能实时解释预测变化，并优于现有方法。", "motivation": "现有可解释AI方法未能满足临床时间序列解释任务的独特需求，尤其是在线患者监测中发现驱动患者风险演变的原因对于及时干预至关重要。", "method": "提出DeltaSHAP算法，通过将Shapley值应用于时间序列设置，解释连续预测的变化，提供特征归因的幅度和方向，并实时提供这些洞察。它仅使用实际观察到的特征组合来归因预测变化，并引入新的评估指标。", "result": "DeltaSHAP在在线患者监测任务中，解释质量优于现有最先进的可解释AI方法62%，计算效率提高33%（在MIMIC-III失代偿基准上）。", "conclusion": "DeltaSHAP有效解决了在线患者监测中预测演变解释的关键临床需求，并在解释质量和计算效率方面表现出色。", "translation": "本研究提出了DeltaSHAP，一种新颖的可解释人工智能（XAI）算法，专门为在线患者监测系统设计。在临床环境中，发现驱动患者风险演变的原因对于及时干预至关重要，然而现有的XAI方法未能解决临床时间序列解释任务的独特需求。为此，DeltaSHAP解决了三个关键的临床需求：解释连续预测而非孤立预测分数的变化，提供特征归因的幅度和方向，以及实时提供这些洞察。通过将Shapley值应用于时间设置，我们的方法准确地捕捉了特征协同效应。它进一步仅使用实际观察到的特征组合来归因预测变化，使其对时间敏感的临床应用而言高效且实用。我们还引入了新的评估指标来评估在线时间序列归因的忠实性，并通过在线患者监测任务的实验证明，DeltaSHAP在MIMIC-III失代偿基准上，解释质量优于最先进的XAI方法62%，计算效率提高33%。我们已在https://github.com/AITRICS/DeltaSHAP发布了我们的代码。", "summary": "DeltaSHAP是一种创新的可解释AI算法，专注于在线患者监测系统。它通过适应Shapley值来实时解释患者风险预测的演变，提供特征归因的幅度和方向，并解决现有方法在临床时间序列解释方面的不足。实验证明，DeltaSHAP在解释质量和计算效率上均优于现有最先进的可解释AI方法。", "keywords": "可解释人工智能, Shapley值, 在线患者监测, 时间序列, 预测演变", "comments": "该论文提出了一种创新的可解释AI方法DeltaSHAP，专门针对在线患者监测的独特需求，解决了现有方法在解释预测演变方面的不足。其创新之处在于将Shapley值应用于时间序列环境，并能实时提供特征归因的幅度和方向。在临床应用中，这对于及时干预和理解患者风险变化至关重要。实验结果表明其在解释质量和计算效率上均有显著提升，具有重要的实用价值。"}}
{"id": "2507.02363", "title": "LocalDyGS: Multi-view Global Dynamic Scene Modeling via Adaptive Local Implicit Feature Decoupling", "authors": ["Jiahao Wu", "Rui Peng", "Jianbo Jiao", "Jiayu Yang", "Luyang Tang", "Kaiqiang Xiong", "Jie Liang", "Jinbo Yan", "Runling Liu", "Ronggang Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025", "url": "http://arxiv.org/abs/2507.02363v1", "summary": "Due to the complex and highly dynamic motions in the real world, synthesizing\ndynamic videos from multi-view inputs for arbitrary viewpoints is challenging.\nPrevious works based on neural radiance field or 3D Gaussian splatting are\nlimited to modeling fine-scale motion, greatly restricting their application.\nIn this paper, we introduce LocalDyGS, which consists of two parts to adapt our\nmethod to both large-scale and fine-scale motion scenes: 1) We decompose a\ncomplex dynamic scene into streamlined local spaces defined by seeds, enabling\nglobal modeling by capturing motion within each local space. 2) We decouple\nstatic and dynamic features for local space motion modeling. A static feature\nshared across time steps captures static information, while a dynamic residual\nfield provides time-specific features. These are combined and decoded to\ngenerate Temporal Gaussians, modeling motion within each local space. As a\nresult, we propose a novel dynamic scene reconstruction framework to model\nhighly dynamic real-world scenes more realistically. Our method not only\ndemonstrates competitive performance on various fine-scale datasets compared to\nstate-of-the-art (SOTA) methods, but also represents the first attempt to model\nlarger and more complex highly dynamic scenes. Project page:\nhttps://wujh2001.github.io/LocalDyGS/.", "comment": "Accepted by ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.02363v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LocalDyGS：通过自适应局部隐式特征解耦的多视角全局动态场景建模", "tldr": "LocalDyGS提出了一种新的动态场景建模框架，通过将复杂场景分解为局部空间并解耦静态和动态特征来生成时间高斯，从而实现更真实的动态视频合成，并首次尝试建模更大更复杂的动态场景。", "motivation": "由于现实世界中复杂且高度动态的运动，从多视角输入合成任意视点的动态视频极具挑战性。现有的基于神经辐射场或3D高斯泼溅的方法在建模精细尺度运动方面存在局限性，极大地限制了其应用。", "method": "本文提出了LocalDyGS，包含两部分以适应大尺度和精细尺度运动场景：1) 将复杂的动态场景分解为由种子定义的流线型局部空间，通过捕获每个局部空间内的运动实现全局建模。2) 解耦局部空间运动建模中的静态和动态特征，其中静态特征在时间步长之间共享以捕获静态信息，动态残差场提供时间特异性特征。这些特征被结合并解码以生成时间高斯，从而建模每个局部空间内的运动。", "result": "该方法不仅在各种精细尺度数据集上表现出与最先进（SOTA）方法相当的性能，而且是首次尝试建模更大、更复杂的动态场景。", "conclusion": "本文提出了一种新颖的动态场景重建框架LocalDyGS，能够更真实地建模高度动态的真实世界场景。它在精细尺度数据集上表现出色，并且是首次成功建模更大、更复杂动态场景的方法。", "translation": "由于现实世界中复杂且高度动态的运动，从多视角输入合成任意视点的动态视频极具挑战性。以往基于神经辐射场或3D高斯泼溅的工作仅限于建模精细尺度运动，极大地限制了其应用。在本文中，我们引入了LocalDyGS，它包含两个部分以使我们的方法适应大尺度和精细尺度运动场景：1) 我们将复杂的动态场景分解为由种子定义的流线型局部空间，通过捕获每个局部空间内的运动来实现全局建模。2) 我们解耦了局部空间运动建模中的静态和动态特征。一个跨时间步共享的静态特征捕获静态信息，而动态残差场提供时间特异性特征。这些特征被结合并解码以生成时间高斯，建模每个局部空间内的运动。因此，我们提出了一种新颖的动态场景重建框架，以更真实地建模高度动态的真实世界场景。我们的方法不仅在各种精细尺度数据集上与最先进（SOTA）方法相比表现出竞争性能，而且代表了首次尝试建模更大、更复杂的动态场景。项目页面：https://wujh2001.github.io/LocalDyGS/。", "summary": "本文提出了LocalDyGS，一个用于多视角全局动态场景建模的新颖框架。它通过将复杂场景分解为由种子定义的局部空间，并解耦静态与动态特征来生成时间高斯，从而解决了现有方法在处理大尺度和复杂运动方面的局限性。LocalDyGS能够更真实地重建高度动态的真实世界场景，并在精细尺度数据集上达到领先性能，同时也是首次成功建模更大、更复杂动态场景的方法。", "keywords": "动态场景建模, 多视角, 局部隐式特征, 时间高斯, 3D高斯泼溅", "comments": "LocalDyGS的创新之处在于其自适应局部隐式特征解耦机制，通过将复杂场景分解为局部空间并区分静态和动态信息，有效地处理了传统方法难以应对的大尺度和复杂动态运动。其重要性在于，它不仅在现有精细尺度任务上保持了竞争力，更是首次将动态场景建模扩展到更大、更复杂的真实世界场景，这对于动态视频合成和虚拟现实等领域具有重要意义。抽象中未明确提及局限性，但作为“首次尝试”可能暗示该领域仍有进一步探索的空间。"}}
{"id": "2507.02356", "title": "Offline Reinforcement Learning with Penalized Action Noise Injection", "authors": ["JunHyeok Oh", "Byung-Jun Lee"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02356v1", "summary": "Offline reinforcement learning (RL) optimizes a policy using only a fixed\ndataset, making it a practical approach in scenarios where interaction with the\nenvironment is costly. Due to this limitation, generalization ability is key to\nimproving the performance of offline RL algorithms, as demonstrated by recent\nsuccesses of offline RL with diffusion models. However, it remains questionable\nwhether such diffusion models are necessary for highly performing offline RL\nalgorithms, given their significant computational requirements during\ninference. In this paper, we propose Penalized Action Noise Injection (PANI), a\nmethod that simply enhances offline learning by utilizing noise-injected\nactions to cover the entire action space, while penalizing according to the\namount of noise injected. This approach is inspired by how diffusion models\nhave worked in offline RL algorithms. We provide a theoretical foundation for\nthis method, showing that offline RL algorithms with such noise-injected\nactions solve a modified Markov Decision Process (MDP), which we call the noisy\naction MDP. PANI is compatible with a wide range of existing off-policy and\noffline RL algorithms, and despite its simplicity, it demonstrates significant\nperformance improvements across various benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02356v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "离线强化学习与惩罚性动作噪声注入", "tldr": "PANI是一种简单的方法，通过注入噪声动作并施加惩罚来增强离线强化学习的性能，且计算成本低于扩散模型。", "motivation": "在离线强化学习中，泛化能力对于提升算法性能至关重要。尽管扩散模型在离线RL中取得了成功，但其高昂的推理计算成本使其适用性受到质疑。本文旨在探索一种更简单、计算效率更高但同样有效的离线RL增强方法。", "method": "本文提出了惩罚性动作噪声注入（PANI）方法。该方法通过利用噪声注入的动作来覆盖整个动作空间，并根据注入噪声的量进行惩罚。理论上，PANI通过解决一个修改后的马尔可夫决策过程（称之为噪声动作MDP）来实现这一目标。", "result": "PANI方法与多种现有的离策略和离线强化学习算法兼容。尽管其设计简单，但在各种基准测试中都表现出显著的性能改进。", "conclusion": "PANI是一种简单而有效的离线强化学习增强方法，通过噪声注入和惩罚机制显著提高了性能，并提供了坚实的理论基础。", "translation": "离线强化学习（RL）仅使用固定数据集来优化策略，使其成为与环境交互成本高昂场景下的实用方法。由于这一限制，泛化能力是提高离线RL算法性能的关键，正如最近扩散模型在离线RL中取得的成功所证明的。然而，考虑到扩散模型在推理过程中巨大的计算需求，这些模型是否对于高性能离线RL算法是必要的，仍然值得怀疑。在本文中，我们提出了惩罚性动作噪声注入（PANI），这是一种通过利用噪声注入动作来覆盖整个动作空间，同时根据注入噪声量进行惩罚，从而简单地增强离线学习的方法。这种方法受到了扩散模型在离线RL算法中工作方式的启发。我们为这种方法提供了理论基础，表明具有这种噪声注入动作的离线RL算法解决了一个修改后的马尔可夫决策过程（MDP），我们称之为噪声动作MDP。PANI与广泛的现有离策略和离线RL算法兼容，尽管其简单，但在各种基准测试中都表现出显著的性能改进。", "summary": "本文提出了一种名为惩罚性动作噪声注入（PANI）的离线强化学习方法，旨在通过简单有效的机制提升泛化能力，以替代计算成本高昂的扩散模型。PANI通过向动作空间注入噪声并根据噪声量施加惩罚来优化策略。该方法具有理论基础，解决了所谓的“噪声动作MDP”，并且与多种现有离策略和离线RL算法兼容。实验结果表明，PANI在多个基准测试中显著提升了性能。", "keywords": "离线强化学习, 动作噪声注入, 惩罚, 噪声动作MDP, 泛化能力", "comments": "PANI的创新之处在于它提供了一种无需复杂扩散模型即可增强离线RL泛化能力的简单而有效的方法。通过引入噪声注入和惩罚机制，它在降低计算开销的同时提高了性能。其理论基础和与现有算法的广泛兼容性也增加了其实用性和影响力。"}}
{"id": "2507.01991", "title": "FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports", "authors": ["Muhammad Bilal Zafar"], "categories": ["q-fin.CP", "cs.CL", "econ.GN", "q-fin.EC", "q-fin.GN"], "primary_category": "Subjects:       Computational Finance (q-fin.CP)", "pdf_link": null, "comments": "Comments:      The FinAI-BERT model can be directly loaded via Hugging Face Transformers ( this https URL ) for sentence-level AI disclosure classification", "url": "http://arxiv.org/abs/2507.01991v1", "summary": "The proliferation of artificial intelligence (AI) in financial services has\nprompted growing demand for tools that can systematically detect AI-related\ndisclosures in corporate filings. While prior approaches often rely on keyword\nexpansion or document-level classification, they fall short in granularity,\ninterpretability, and robustness. This study introduces FinAI-BERT, a\ndomain-adapted transformer-based language model designed to classify AI-related\ncontent at the sentence level within financial texts. The model was fine-tuned\non a manually curated and balanced dataset of 1,586 sentences drawn from 669\nannual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect\nclassification performance (accuracy of 99.37 percent, F1 score of 0.993),\noutperforming traditional baselines such as Logistic Regression, Naive Bayes,\nRandom Forest, and XGBoost. Interpretability was ensured through SHAP-based\ntoken attribution, while bias analysis and robustness checks confirmed the\nmodel's stability across sentence lengths, adversarial inputs, and temporal\nsamples. Theoretically, the study advances financial NLP by operationalizing\nfine-grained, theme-specific classification using transformer architectures.\nPractically, it offers a scalable, transparent solution for analysts,\nregulators, and scholars seeking to monitor the diffusion and framing of AI\nacross financial institutions.", "comment": "The FinAI-BERT model can be directly loaded via Hugging Face\n  Transformers (https://huggingface.co/bilalzafar/FinAI-BERT) for\n  sentence-level AI disclosure classification", "pdf_url": "http://arxiv.org/pdf/2507.01991v1", "cate": "q-fin.CP", "date": "2025-06-29", "updated": "2025-06-29", "AI": {"title_translation": "FinAI-BERT：一种用于金融报告中AI披露的句子级检测的基于Transformer的模型", "tldr": "FinAI-BERT是一个基于Transformer的模型，用于在金融报告中进行句子级的AI披露检测，取得了接近完美的分类性能。", "motivation": "金融服务中人工智能（AI）的普及引发了对系统性检测企业文件中AI相关披露工具的需求。现有方法（如关键词扩展或文档级分类）在粒度、可解释性和鲁棒性方面存在不足。", "method": "本研究引入了FinAI-BERT，这是一个领域适应的、基于Transformer的语言模型，旨在对金融文本中AI相关内容进行句子级分类。该模型在一个手动整理的平衡数据集上进行了微调，该数据集包含从669份美国银行年度报告（2015年至2023年）中提取的1,586个句子。通过基于SHAP的token归因确保了可解释性，并通过偏见分析和鲁棒性检查确认了模型在不同句子长度、对抗性输入和时间样本下的稳定性。", "result": "FinAI-BERT取得了接近完美的分类性能（准确率为99.37%，F1分数为0.993），优于传统的基线模型，如逻辑回归、朴素贝叶斯、随机森林和XGBoost。", "conclusion": "理论上，该研究通过使用Transformer架构实现细粒度、主题特定的分类，推动了金融自然语言处理的发展。实践中，它为分析师、监管机构和学者提供了一个可扩展、透明的解决方案，以监测AI在金融机构中的传播和框架。", "translation": "金融服务中人工智能（AI）的普及引发了对系统性检测企业文件中AI相关披露工具的日益增长的需求。虽然现有方法通常依赖于关键词扩展或文档级分类，但它们在粒度、可解释性和鲁棒性方面存在不足。本研究引入了FinAI-BERT，这是一个领域适应的、基于Transformer的语言模型，旨在对金融文本中AI相关内容进行句子级分类。该模型在一个手动整理的平衡数据集上进行了微调，该数据集包含从669份美国银行年度报告（2015年至2023年）中提取的1,586个句子。FinAI-BERT取得了接近完美的分类性能（准确率为99.37%，F1分数为0.993），优于传统的基线模型，如逻辑回归、朴素贝叶斯、随机森林和XGBoost。通过基于SHAP的token归因确保了可解释性，同时偏见分析和鲁棒性检查确认了模型在不同句子长度、对抗性输入和时间样本下的稳定性。理论上，该研究通过使用Transformer架构实现细粒度、主题特定的分类，推动了金融自然语言处理的发展。实践中，它为分析师、监管机构和学者提供了一个可扩展、透明的解决方案，以监测AI在金融机构中的传播和框架。", "summary": "本研究提出了FinAI-BERT，一个专门用于在金融报告中进行句子级AI披露检测的Transformer模型。该模型通过在包含1,586个句子的手动标注数据集上进行微调，实现了99.37%的准确率和0.993的F1分数，显著优于传统基线。FinAI-BERT还通过SHAP确保了可解释性，并通过鲁棒性检查验证了其稳定性，为金融NLP领域提供了细粒度、透明且可扩展的AI披露监测解决方案。", "keywords": "FinAI-BERT, Transformer, AI披露, 金融报告, 句子级检测", "comments": "该论文的创新点在于提出了一个领域适应的Transformer模型FinAI-BERT，用于解决金融报告中AI披露的句子级检测问题，弥补了现有方法在粒度、可解释性和鲁棒性上的不足。其高性能（接近完美的分类表现）和对可解释性、鲁棒性的强调，使其在实际应用中具有重要价值，为金融分析师、监管机构和学者提供了有效工具。"}}
{"id": "2507.02373", "title": "UVLM: Benchmarking Video Language Model for Underwater World Understanding", "authors": ["Xizhe Xue", "Yang Zhou", "Dawei Yan", "Ying Li", "Haokui Zhang", "Rong Xiao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 4 figures, 3 tables", "url": "http://arxiv.org/abs/2507.02373v1", "summary": "Recently, the remarkable success of large language models (LLMs) has achieved\na profound impact on the field of artificial intelligence. Numerous advanced\nworks based on LLMs have been proposed and applied in various scenarios. Among\nthem, video language models (VidLMs) are particularly widely used. However,\nexisting works primarily focus on terrestrial scenarios, overlooking the highly\ndemanding application needs of underwater observation. To overcome this gap, we\nintroduce UVLM, an under water observation benchmark which is build through a\ncollaborative approach combining human expertise and AI models. To ensure data\nquality, we have conducted in-depth considerations from multiple perspectives.\nFirst, to address the unique challenges of underwater environments, we selected\nvideos that represent typical underwater challenges including light variations,\nwater turbidity, and diverse viewing angles to construct the dataset. Second,\nto ensure data diversity, the dataset covers a wide range of frame rates,\nresolutions, 419 classes of marine animals, and various static plants and\nterrains. Next, for task diversity, we adopted a structured design where\nobservation targets are categorized into two major classes: biological and\nenvironmental. Each category includes content observation and change/action\nobservation, totaling 20 distinct task types. Finally, we designed several\nchallenging evaluation metrics to enable quantitative comparison and analysis\nof different methods. Experiments on two representative VidLMs demonstrate that\nfine-tuning VidLMs on UVLM significantly improves underwater world\nunderstanding while also showing potential for slight improvements on existing\nin-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and\nprompt engineering will be released publicly.", "comment": "13 pages, 4 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.02373v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "UVLM：水下世界理解的视频语言模型基准测试", "tldr": "本文介绍了UVLM，一个针对水下观察场景的视频语言模型基准数据集，旨在弥补现有视频语言模型主要关注陆地场景的不足。实验表明，在UVLM上微调视频语言模型能显著提升其水下理解能力。", "motivation": "现有视频语言模型（VidLMs）主要关注陆地场景，忽视了水下观察领域对高要求应用的需求，导致在水下环境中对视频的理解能力不足。", "method": "研究人员提出了UVLM，一个通过结合人类专业知识和AI模型构建的水下观察基准数据集。该数据集通过以下方式确保数据质量和多样性：1) 选择包含光线变化、水浊度、多样视角等典型水下挑战的视频；2) 涵盖广泛的帧率、分辨率、419种海洋动物以及各种静态植物和地形；3) 将观察目标分为生物和环境两大类，每类包含内容观察和变化/动作观察，共20种不同的任务类型；4) 设计了具有挑战性的评估指标进行定量比较和分析。", "result": "在两个代表性视频语言模型上的实验表明，在UVLM上进行微调可以显著提高模型对水下世界的理解能力，并且对现有陆地视频语言模型基准（如VideoMME和Perception text）也显示出轻微的改进潜力。", "conclusion": "本文成功构建并推出了UVLM水下观察基准数据集，有效弥补了现有视频语言模型在水下场景应用中的空白。实验证明UVLM能够显著提升视频语言模型的水下理解能力，为未来水下人工智能研究提供了重要资源。", "translation": "最近，大型语言模型（LLMs）的显著成功对人工智能领域产生了深远影响。基于LLMs的众多先进工作已被提出并应用于各种场景。其中，视频语言模型（VidLMs）的应用尤为广泛。然而，现有工作主要集中在陆地场景，忽视了水下观察领域对高要求应用的需求。为了弥补这一空白，我们引入了UVLM，一个通过结合人类专业知识和AI模型构建的水下观察基准。为确保数据质量，我们从多个角度进行了深入考虑。首先，为应对水下环境的独特挑战，我们选择了代表典型水下挑战的视频来构建数据集，包括光线变化、水浊度以及多样化的视角。其次，为确保数据多样性，数据集涵盖了广泛的帧率、分辨率、419种海洋动物以及各种静态植物和地形。接下来，为实现任务多样性，我们采用了结构化设计，将观察目标分为两大类：生物和环境。每个类别包括内容观察和变化/动作观察，总计20种不同的任务类型。最后，我们设计了几个具有挑战性的评估指标，以实现不同方法的定量比较和分析。在两个代表性VidLMs上的实验表明，在UVLM上对VidLMs进行微调显著提高了水下世界理解能力，同时在现有陆地VidLM基准（如VideoMME和Perception text）上也显示出轻微改进的潜力。数据集和提示工程将公开发布。", "summary": "本文介绍了UVLM，一个专门针对水下观察场景设计的视频语言模型基准数据集。鉴于现有视频语言模型主要关注陆地环境，UVLM旨在填补水下理解能力的空白。该基准数据集通过结合人类专业知识和AI模型构建，精心挑选了包含水下特有挑战的视频，并确保了数据和任务的多样性，涵盖了广泛的海洋生物、环境特征及20种不同的观察任务类型。实验证明，在UVLM上对视频语言模型进行微调，能够显著提升其水下世界的理解能力，并对陆地基准测试也表现出积极影响。UVLM数据集及其提示工程将公开发布。", "keywords": "UVLM, 视频语言模型, 水下, 基准测试, 数据集", "comments": "UVLM的创新之处在于其专注于水下环境，弥补了现有视频语言模型在这一特殊领域的研究空白。其重要性体现在提供了一个高质量、多维度的数据集和评估标准，能够有效推动水下人工智能和计算机视觉技术的发展。通过结合人类专业知识和AI模型构建数据集，确保了数据的实用性和专业性。该工作为未来水下机器人、海洋生物研究和资源勘探等应用提供了坚实的基础。"}}
{"id": "2507.02365", "title": "Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations", "authors": ["Muhammad Usama", "Dong Eui Chang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02365v1", "summary": "Equalizer parameter optimization for signal integrity in high-speed Dynamic\nRandom Access Memory systems is crucial but often computationally demanding or\nmodel-reliant. This paper introduces a data-driven framework employing learned\nlatent signal representations for efficient signal integrity evaluation,\ncoupled with a model-free Advantage Actor-Critic reinforcement learning agent\nfor parameter optimization. The latent representation captures vital signal\nintegrity features, offering a fast alternative to direct eye diagram analysis\nduring optimization, while the reinforcement learning agent derives optimal\nequalizer settings without explicit system models. Applied to industry-standard\nDynamic Random Access Memory waveforms, the method achieved significant\neye-opening window area improvements: 42.7\\% for cascaded Continuous-Time\nLinear Equalizer and Decision Feedback Equalizer structures, and 36.8\\% for\nDecision Feedback Equalizer-only configurations. These results demonstrate\nsuperior performance, computational efficiency, and robust generalization\nacross diverse Dynamic Random Access Memory units compared to existing\ntechniques. Core contributions include an efficient latent signal integrity\nmetric for optimization, a robust model-free reinforcement learning strategy,\nand validated superior performance for complex equalizer architectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02365v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "深度强化学习驱动的基于潜在表示的DRAM均衡器参数优化", "tldr": "本文提出一种基于深度强化学习和潜在信号表示的数据驱动框架，用于高效优化DRAM均衡器参数，显著提升了信号完整性。", "motivation": "高速DRAM系统中均衡器参数优化对信号完整性至关重要，但通常计算量大或依赖模型。", "method": "本文引入了一个数据驱动框架，该框架结合了学习到的潜在信号表示用于高效信号完整性评估，并与一个无模型的Advantage Actor-Critic强化学习代理相结合进行参数优化。潜在表示捕获重要的信号完整性特征，提供了一种快速替代直接眼图分析的方法；强化学习代理在没有明确系统模型的情况下推导出最佳均衡器设置。", "result": "应用于工业标准DRAM波形，该方法显著改善了眼图开窗面积：对于级联连续时间线性均衡器和判决反馈均衡器结构，提高了42.7%；对于仅判决反馈均衡器配置，提高了36.8%。这些结果展示了卓越的性能、计算效率和鲁棒泛化能力。", "conclusion": "该方法在复杂均衡器架构上表现出卓越的性能、计算效率和对不同DRAM单元的鲁棒泛化能力。核心贡献包括一个用于优化的有效潜在信号完整性度量、一个鲁棒的无模型强化学习策略，以及对复杂均衡器架构的验证过的卓越性能。", "translation": "高速动态随机存取存储器（DRAM）系统中用于信号完整性的均衡器参数优化至关重要，但通常计算量大或依赖于模型。本文引入了一个数据驱动框架，该框架采用学习到的潜在信号表示进行高效的信号完整性评估，并结合一个无模型的优势演员-评论家（Advantage Actor-Critic）强化学习代理进行参数优化。潜在表示捕获关键的信号完整性特征，为优化期间的直接眼图分析提供了一种快速替代方案，而强化学习代理在没有明确系统模型的情况下推导出最佳均衡器设置。应用于工业标准DRAM波形时，该方法在眼图开窗面积方面取得了显著改善：对于级联连续时间线性均衡器（CTLE）和判决反馈均衡器（DFE）结构，提高了42.7%；对于仅判决反馈均衡器（DFE-only）配置，提高了36.8%。与现有技术相比，这些结果展示了卓越的性能、计算效率以及在不同DRAM单元上的鲁棒泛化能力。核心贡献包括一个用于优化的有效潜在信号完整性度量、一个鲁棒的无模型强化学习策略，以及对复杂均衡器架构的验证过的卓越性能。", "summary": "本文提出了一种创新的数据驱动框架，利用深度强化学习和学习到的潜在信号表示来优化高速DRAM系统中的均衡器参数。该框架通过潜在表示快速评估信号完整性，并使用无模型的Advantage Actor-Critic强化学习代理来确定最佳均衡器设置。实验结果表明，该方法显著提升了眼图开窗面积，并展示了优于现有技术的性能、计算效率和泛化能力。", "keywords": "深度强化学习, DRAM, 均衡器优化, 信号完整性, 潜在表示", "comments": "这篇论文通过结合深度强化学习和潜在表示，解决了高速DRAM均衡器参数优化中计算量大和模型依赖的问题。其创新点在于利用潜在表示作为快速的信号完整性度量，并采用无模型的强化学习方法进行优化，从而实现了显著的性能提升和更强的泛化能力。该方法对于DRAM设计和优化具有重要的实际意义。"}}
{"id": "2507.02825", "title": "Establishing Best Practices for Building Rigorous Agentic Benchmarks", "authors": ["Yuxuan Zhu", "Tengjun Jin", "Yada Pruksachatkun", "Andy Zhang", "Shu Liu", "Sasha Cui", "Sayash Kapoor", "Shayne Longpre", "Kevin Meng", "Rebecca Weiss", "Fazl Barez", "Rahul Gupta", "Jwala Dhamala", "Jacob Merizian", "Mario Giulianelli", "Harry Coppock", "Cozmin Ududec", "Jasjeet Sekhon", "Jacob Steinhardt", "Antony Kellerman", "Sarah Schwettmann", "Matei Zaharia", "Ion Stoica", "Percy Liang", "Daniel Kang"], "categories": ["cs.AI", "A.1; I.2.m"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      39 pages, 15 tables, 6 figures", "url": "http://arxiv.org/abs/2507.02825v1", "summary": "Benchmarks are essential for quantitatively tracking progress in AI. As AI\nagents become increasingly capable, researchers and practitioners have\nintroduced agentic benchmarks to evaluate agents on complex, real-world tasks.\nThese benchmarks typically measure agent capabilities by evaluating task\noutcomes via specific reward designs. However, we show that many agentic\nbenchmarks have issues task setup or reward design. For example, SWE-bench\nVerified uses insufficient test cases, while TAU-bench counts empty responses\nas successful. Such issues can lead to under- or overestimation agents'\nperformance by up to 100% in relative terms. To make agentic evaluation\nrigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of\nguidelines that we synthesized from our benchmark-building experience, a survey\nof best practices, and previously reported issues. When applied to CVE-Bench, a\nbenchmark with a particularly complex evaluation design, ABC reduces the\nperformance overestimation by 33%.", "comment": "39 pages, 15 tables, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.02825v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "建立严格智能体基准的H最佳实践", "tldr": "本文指出当前智能体基准存在任务设置或奖励设计问题，导致性能评估偏差，并提出了智能体基准清单 (ABC) 作为一套指导方针，以建立更严格的智能体评估。", "motivation": "AI智能体能力日益增强，需要通过智能体基准来评估其在复杂现实任务中的表现。然而，现有许多智能体基准在任务设置或奖励设计上存在问题，导致对智能体性能的低估或高估。为了使智能体评估更加严格和准确，作者旨在建立一套最佳实践。", "method": "作者通过总结其基准构建经验、调查最佳实践以及分析已报告的问题，提出了一套名为“智能体基准清单”（Agentic Benchmark Checklist, ABC）的指导方针。该清单旨在解决现有智能体基准中存在的任务设置和奖励设计问题。", "result": "应用智能体基准清单 (ABC) 到一个复杂评估设计的基准CVE-Bench上时，成功将性能高估降低了33%。这表明ABC能够有效改善智能体评估的严谨性。", "conclusion": "智能体基准清单 (ABC) 提供了一套建立严格智能体评估的实用指南，有助于解决现有基准中的问题，从而实现更准确的智能体性能衡量。", "translation": "基准对于量化跟踪人工智能的进展至关重要。随着人工智能智能体能力日益增强，研究人员和从业者引入了智能体基准来评估智能体在复杂、现实世界任务中的表现。这些基准通常通过特定的奖励设计来评估任务结果，从而衡量智能体能力。然而，我们发现许多智能体基准在任务设置或奖励设计上存在问题。例如，SWE-bench Verified 使用了不足的测试用例，而TAU-bench将空响应计为成功。这些问题可能导致智能体性能被低估或高估，相对误差高达100%。为了使智能体评估更加严格，我们引入了智能体基准清单（Agentic Benchmark Checklist, ABC），这是一套我们从基准构建经验、最佳实践调查和先前报告的问题中综合得出的指导方针。当应用于CVE-Bench（一个评估设计特别复杂的基准）时，ABC将性能高估降低了33%。", "summary": "本文指出，当前用于评估AI智能体的基准存在任务设置和奖励设计缺陷，例如测试用例不足或不当的成功判定，这可能导致高达100%的性能评估偏差。为解决此问题，作者提出了“智能体基准清单”（ABC），这是一套基于实践经验和现有问题综合而成的指导方针。实验证明，将ABC应用于复杂的CVE-Bench基准时，能有效将性能高估减少33%，从而提升智能体评估的严谨性。", "keywords": "智能体基准, 评估, 最佳实践, 严谨性, ABC", "comments": "本文识别并解决了当前智能体基准中普遍存在的评估严谨性问题，通过提出“智能体基准清单”（ABC）提供了一套实用的、经验证的解决方案。其创新性在于将分散的最佳实践和常见问题系统化为可操作的指导方针，对于提高智能体评估的准确性和可靠性具有重要意义。"}}
{"id": "2507.02393", "title": "PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection", "authors": ["Seokyeong Lee", "Sithu Aung", "Junyong Choi", "Seungryong Kim", "Ig-Jae Kim", "Junghyun Cho"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      18 pages, 16 figures", "url": "http://arxiv.org/abs/2507.02393v1", "summary": "Monocular 3D object detection (M3OD) has long faced challenges due to data\nscarcity caused by high annotation costs and inherent 2D-to-3D ambiguity.\nAlthough various weakly supervised methods and pseudo-labeling methods have\nbeen proposed to address these issues, they are mostly limited by\ndomain-specific learning or rely solely on shape information from a single\nobservation. In this paper, we propose a novel pseudo-labeling framework that\nuses only video data and is more robust to occlusion, without requiring a\nmulti-view setup, additional sensors, camera poses, or domain-specific\ntraining. Specifically, we explore a technique for aggregating the\npseudo-LiDARs of both static and dynamic objects across temporally adjacent\nframes using object point tracking, enabling 3D attribute extraction in\nscenarios where 3D data acquisition is infeasible. Extensive experiments\ndemonstrate that our method ensures reliable accuracy and strong scalability,\nmaking it a practical and effective solution for M3OD.", "comment": "18 pages, 16 figures", "pdf_url": "http://arxiv.org/pdf/2507.02393v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "PLOT：通过视频目标跟踪进行伪标签化以实现可扩展的单目3D目标检测", "tldr": "提出PLOT框架，利用视频目标跟踪聚合伪LiDAR，解决单目3D目标检测数据稀缺和2D-3D歧义问题，无需多视图或额外传感器，实现高精度和可扩展性。", "motivation": "单目3D目标检测（M3OD）长期面临数据稀缺（高标注成本）和固有的2D-到-3D歧义挑战。现有弱监督和伪标签方法受限于特定领域学习或仅依赖单一观测的形状信息。", "method": "提出一种新颖的伪标签框架PLOT，仅使用视频数据，对遮挡更鲁棒，无需多视图设置、额外传感器、相机姿态或领域特定训练。具体地，通过目标点跟踪，聚合静态和动态对象在时间相邻帧的伪LiDAR，从而在3D数据获取不可行的情况下提取3D属性。", "result": "实验证明该方法确保了可靠的准确性和强大的可扩展性。", "conclusion": "PLOT为单目3D目标检测提供了一个实用且有效的解决方案，解决了数据稀缺和2D-3D歧义问题，且具有高精度和可扩展性。", "translation": "单目3D目标检测（M3OD）长期以来一直面临挑战，原因在于高昂的标注成本导致数据稀缺以及固有的2D到3D歧义。尽管已经提出了各种弱监督方法和伪标签方法来解决这些问题，但它们大多受限于特定领域学习或仅依赖于单一观测的形状信息。在本文中，我们提出了一种新颖的伪标签框架，该框架仅使用视频数据，对遮挡更具鲁棒性，并且不需要多视图设置、额外传感器、相机姿态或领域特定训练。具体来说，我们探索了一种技术，通过目标点跟踪来聚合静态和动态对象在时间相邻帧的伪LiDAR，从而在3D数据获取不可行的情况下实现3D属性提取。大量的实验表明，我们的方法确保了可靠的准确性和强大的可扩展性，使其成为M3OD的一个实用且有效的解决方案。", "summary": "本文提出PLOT，一个新颖的伪标签框架，旨在解决单目3D目标检测中数据稀缺和2D-3D歧义问题。它利用视频数据和目标点跟踪，聚合跨帧的伪LiDAR，以在3D数据难以获取的场景中提取3D属性。该方法无需多视图、额外传感器或领域特定训练，并被证明具有高精度和强可扩展性，为M3OD提供了一个实用且有效的解决方案。", "keywords": "单目3D目标检测, 伪标签, 视频目标跟踪, 数据稀缺, 可扩展性", "comments": "PLOT的创新之处在于其纯粹基于视频数据的伪标签方法，通过聚合时间相邻帧的伪LiDAR来克服单目3D检测的数据稀缺和2D-3D歧义，且无需昂贵的额外传感器或复杂设置。其鲁棒性和可扩展性使其在实际应用中具有重要潜力。"}}
{"id": "2507.02406", "title": "Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization", "authors": ["Caio Azevedo", "Lina Achaji", "Stefano Sabatini", "Nicola Poerio", "Grzegorz Bartyzel", "Sascha Hornauer", "Fabien Moutarde"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted for publication at ITSC 2025", "url": "http://arxiv.org/abs/2507.02406v1", "summary": "Trajectory prediction is an essential step in the pipeline of an autonomous\nvehicle. Inaccurate or inconsistent predictions regarding the movement of\nagents in its surroundings lead to poorly planned maneuvers and potentially\ndangerous situations for the end-user. Current state-of-the-art\ndeep-learning-based trajectory prediction models can achieve excellent accuracy\non public datasets. However, when used in more complex, interactive scenarios,\nthey often fail to capture important interdependencies between agents, leading\nto inconsistent predictions among agents in the traffic scene. Inspired by the\nefficacy of incorporating human preference into large language models, this\nwork fine-tunes trajectory prediction models in multi-agent settings using\npreference optimization. By taking as input automatically calculated preference\nrankings among predicted futures in the fine-tuning process, our\nexperiments--using state-of-the-art models on three separate datasets--show\nthat we are able to significantly improve scene consistency while minimally\nsacrificing trajectory prediction accuracy and without adding any excess\ncomputational requirements at inference time.", "comment": "Accepted for publication at ITSC 2025", "pdf_url": "http://arxiv.org/pdf/2507.02406v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过偏好优化提高车辆轨迹预测的一致性", "tldr": "本文通过偏好优化方法，显著提高了多智能体车辆轨迹预测的一致性，同时保持了预测准确性，且不增加推理计算成本。", "motivation": "现有最先进的深度学习轨迹预测模型在公共数据集上表现出高精度，但在复杂的交互场景中，往往无法捕捉智能体之间的重要相互依赖关系，导致预测结果不一致，这会引发糟糕的规划和潜在的危险情况。因此，本文旨在解决多智能体场景下轨迹预测的一致性问题。", "method": "本文受大型语言模型中引入人类偏好有效性的启发，在多智能体设置中，使用偏好优化来微调轨迹预测模型。在微调过程中，该方法将自动计算的预测未来偏好排名作为输入。", "result": "实验结果表明，在三个不同的数据集上使用最先进的模型进行测试，该方法能够显著提高场景一致性，同时最大限度地减少轨迹预测精度的损失，并且在推理时没有增加任何额外的计算要求。", "conclusion": "通过引入偏好优化进行微调，可以有效提高多智能体场景下车辆轨迹预测的一致性，同时保持预测性能和效率。", "translation": "轨迹预测是自动驾驶车辆管线中必不可少的一步。对周围智能体运动的不准确或不一致的预测会导致规划不佳的操作和最终用户面临的潜在危险情况。目前最先进的基于深度学习的轨迹预测模型可以在公共数据集上实现出色的准确性。然而，当在更复杂、交互性更强的场景中使用时，它们往往无法捕捉智能体之间重要的相互依赖关系，导致交通场景中智能体之间的预测不一致。受将人类偏好纳入大型语言模型有效性的启发，这项工作利用偏好优化在多智能体设置中微调轨迹预测模型。通过在微调过程中将自动计算的预测未来偏好排名作为输入，我们的实验——使用三个独立数据集上的最先进模型——表明我们能够显著提高场景一致性，同时最大限度地牺牲轨迹预测精度，并且在推理时没有增加任何额外的计算要求。", "summary": "本文提出了一种通过偏好优化来提高车辆轨迹预测一致性的方法。针对现有深度学习模型在复杂多智能体场景中预测不一致的问题，研究者受大型语言模型中引入偏好的启发，对轨迹预测模型进行微调。实验结果表明，该方法在不增加推理计算成本的前提下，显著提高了场景一致性，同时对预测精度的影响极小。", "keywords": "车辆轨迹预测, 偏好优化, 一致性, 多智能体", "comments": "该论文的创新点在于将偏好优化这一在大型语言模型中取得成功的思想引入到车辆轨迹预测领域，以解决多智能体交互场景中的预测一致性问题。这为轨迹预测领域提供了一个新的视角和有效的解决方案，尤其是在强调多智能体行为协调和安全性的自动驾驶应用中具有重要意义。该方法在不牺牲准确性和不增加推理成本的情况下提高了关键的“一致性”指标，显示出其方法的实用性和高效性。"}}
{"id": "2507.02841", "title": "StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason", "authors": ["Kaiyi Zhang", "Ang Lv", "Jinpeng Li", "Yongbo Wang", "Feng Wang", "Haoyuan Hu", "Rui Yan"], "categories": ["cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02841v1", "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor improving the complex reasoning abilities of large language models (LLMs).\nHowever, current RLVR methods face two significant challenges: the near-miss\nreward problem, where a small mistake can invalidate an otherwise correct\nreasoning process, greatly hindering training efficiency; and exploration\nstagnation, where models tend to focus on solutions within their ``comfort\nzone,'' lacking the motivation to explore potentially more effective\nalternatives. To address these challenges, we propose StepHint, a novel RLVR\nalgorithm that utilizes multi-level stepwise hints to help models explore the\nsolution space more effectively. StepHint generates valid reasoning chains from\nstronger models and partitions these chains into reasoning steps using our\nproposed adaptive partitioning method. The initial few steps are used as hints,\nand simultaneously, multiple-level hints (each comprising a different number of\nsteps) are provided to the model. This approach directs the model's exploration\ntoward a promising solution subspace while preserving its flexibility for\nindependent exploration. By providing hints, StepHint mitigates the near-miss\nreward problem, thereby improving training efficiency. Additionally, the\nexternal reasoning pathways help the model develop better reasoning abilities,\nenabling it to move beyond its ``comfort zone'' and mitigate exploration\nstagnation. StepHint outperforms competitive RLVR enhancement methods across\nsix mathematical benchmarks, while also demonstrating superior generalization\nand excelling over baselines on out-of-domain benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02841v1", "cate": "cs.AI", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "StepHint：多级分步提示增强强化学习推理能力", "tldr": "StepHint通过提供多级分步提示来解决可验证奖励强化学习中近失奖励和探索停滞问题，显著提升了大型语言模型的推理能力和训练效率。", "motivation": "当前的可验证奖励强化学习（RLVR）方法在提高大型语言模型（LLMs）的复杂推理能力方面面临两大挑战：一是“近失奖励问题”，即微小错误可能使整个推理过程失效，严重阻碍训练效率；二是“探索停滞”，即模型倾向于在其“舒适区”内寻找解决方案，缺乏探索更有效替代方案的动力。", "method": "我们提出了StepHint，一种新颖的RLVR算法，它利用多级分步提示来帮助模型更有效地探索解决方案空间。StepHint从更强的模型生成有效的推理链，并使用我们提出的自适应分区方法将这些链划分为推理步骤。初始的几个步骤被用作提示，同时向模型提供多级提示（每个提示包含不同数量的步骤）。这种方法引导模型的探索走向有前景的解决方案子空间，同时保留了其独立探索的灵活性。", "result": "StepHint在六个数学基准测试中优于竞争性RLVR增强方法，同时还表现出卓越的泛化能力，并在域外基准测试中超越了基线。", "conclusion": "StepHint通过提供多级分步提示，有效缓解了可验证奖励强化学习中的近失奖励问题和探索停滞，显著提高了训练效率、模型的推理能力以及泛化能力，使其在多种数学推理任务中表现出色。", "translation": "带有可验证奖励的强化学习（RLVR）是一种有前景的方法，可以提高大型语言模型（LLMs）的复杂推理能力。然而，当前的RLVR方法面临两个重大挑战：近失奖励问题，即一个小的错误可能使一个原本正确的推理过程失效，极大地阻碍了训练效率；以及探索停滞，即模型倾向于专注于其“舒适区”内的解决方案，缺乏探索可能更有效替代方案的动力。为了解决这些挑战，我们提出了StepHint，一种新颖的RLVR算法，它利用多级分步提示来帮助模型更有效地探索解决方案空间。StepHint从更强的模型生成有效的推理链，并使用我们提出的自适应分区方法将这些链划分为推理步骤。初始的几个步骤被用作提示，同时向模型提供多级提示（每个提示包含不同数量的步骤）。这种方法引导模型的探索走向有前景的解决方案子空间，同时保留了其独立探索的灵活性。通过提供提示，StepHint缓解了近失奖励问题，从而提高了训练效率。此外，外部推理路径帮助模型发展更好的推理能力，使其能够超越其“舒适区”并缓解探索停滞。StepHint在六个数学基准测试中优于竞争性RLVR增强方法，同时还表现出卓越的泛化能力，并在域外基准测试中超越了基线。", "summary": "本文提出了StepHint，一种新颖的可验证奖励强化学习（RLVR）算法，旨在解决大型语言模型（LLMs）推理训练中存在的近失奖励问题和探索停滞。StepHint通过从更强模型生成推理链并采用自适应分区方法，为模型提供多级分步提示。这种方法不仅能将模型的探索引导至更有前景的解决方案空间，同时保持其独立探索的灵活性，从而有效缓解了近失奖励问题并提升了训练效率。此外，外部推理路径有助于模型发展更强的推理能力，使其能够突破“舒适区”。实验结果表明，StepHint在多个数学基准测试中表现优异，并展现出卓越的泛化能力。", "keywords": "强化学习, 大型语言模型, 推理, 分步提示, 可验证奖励", "comments": "StepHint的创新之处在于其独特地结合了多级分步提示与自适应分区，有效解决了RLVR中长期存在的近失奖励和探索停滞问题。通过利用“更强模型”生成的推理链作为引导，该方法在提高LLM推理效率和能力方面迈出了重要一步。其在泛化能力和域外表现上的优势，预示了其在复杂推理任务中的广泛应用潜力。"}}
{"id": "2507.02395", "title": "Continual Multiple Instance Learning with Enhanced Localization for Histopathological Whole Slide Image Analysis", "authors": ["Byung Hyun Lee", "Wongi Jeong", "Woojae Han", "Kyoungbun Lee", "Se Young Chun"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025", "url": "http://arxiv.org/abs/2507.02395v1", "summary": "Multiple instance learning (MIL) significantly reduced annotation costs via\nbag-level weak labels for large-scale images, such as histopathological whole\nslide images (WSIs). However, its adaptability to continual tasks with minimal\nforgetting has been rarely explored, especially on instance classification for\nlocalization. Weakly incremental learning for semantic segmentation has been\nstudied for continual localization, but it focused on natural images,\nleveraging global relationships among hundreds of small patches (e.g., $16\n\\times 16$) using pre-trained models. This approach seems infeasible for MIL\nlocalization due to enormous amounts ($\\sim 10^5$) of large patches (e.g., $256\n\\times 256$) and no available global relationships such as cancer cells. To\naddress these challenges, we propose Continual Multiple Instance Learning with\nEnhanced Localization (CoMEL), an MIL framework for both localization and\nadaptability with minimal forgetting. CoMEL consists of (1) Grouped Double\nAttention Transformer (GDAT) for efficient instance encoding, (2) Bag\nPrototypes-based Pseudo-Labeling (BPPL) for reliable instance pseudo-labeling,\nand (3) Orthogonal Weighted Low-Rank Adaptation (OWLoRA) to mitigate forgetting\nin both bag and instance classification. Extensive experiments on three public\nWSI datasets demonstrate superior performance of CoMEL, outperforming the prior\narts by up to $11.00\\%$ in bag-level accuracy and up to $23.4\\%$ in\nlocalization accuracy under the continual MIL setup.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.02395v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "持续多示例学习与增强定位在组织病理学全玻片图像分析中的应用", "tldr": "本文提出了CoMEL，一个用于组织病理学全玻片图像分析的持续多示例学习框架，通过GDAT、BPPL和OWLoRA解决了持续任务中的遗忘问题和定位挑战，并在袋级和实例级分类上取得了显著优于现有技术的性能。", "motivation": "多示例学习（MIL）通过包级弱标签显著降低了组织病理学全玻片图像（WSIs）的标注成本。然而，其在持续任务中适应性差且易遗忘，尤其是在用于定位的实例分类方面，鲜有探索。现有的弱增量学习方法侧重于自然图像，不适用于WSI的MIL定位，因为WSI包含大量大型补丁且缺乏全局关系。", "method": "本文提出了持续多示例学习与增强定位（CoMEL）框架，一个用于定位和适应性且遗忘最小的MIL框架。CoMEL包含：1) 分组双注意Transformer（GDAT），用于高效实例编码；2) 基于包原型伪标签（BPPL），用于可靠的实例伪标签；3) 正交加权低秩适应（OWLoRA），用于减轻包和实例分类中的遗忘。", "result": "在三个公共WSI数据集上进行了广泛实验，结果表明CoMEL表现优越，在持续MIL设置下，包级准确率比现有技术提高了高达11.00%，定位准确率提高了高达23.4%。", "conclusion": "CoMEL框架成功解决了组织病理学全玻片图像分析中持续多示例学习的挑战，并在袋级和实例级分类的准确性方面显著优于现有方法，证明了其在持续任务和定位方面的有效性。", "translation": "多示例学习（MIL）通过包级弱标签显著降低了组织病理学全玻片图像（WSIs）等大规模图像的标注成本。然而，其对持续任务的适应性以及最小化遗忘的能力，特别是针对定位的实例分类，却鲜有探索。用于语义分割的弱增量学习已在持续定位方面进行了研究，但其侧重于自然图像，利用数百个小补丁（例如16x16）之间的全局关系并使用预训练模型。这种方法对于MIL定位似乎不可行，因为存在大量（约10^5）大型补丁（例如256x256），并且没有可用的全局关系，例如癌细胞。为了应对这些挑战，我们提出了持续多示例学习与增强定位（CoMEL），这是一个用于定位和适应性且遗忘最小的MIL框架。CoMEL包含：1）分组双注意Transformer（GDAT），用于高效实例编码；2）基于包原型伪标签（BPPL），用于可靠的实例伪标签；3）正交加权低秩适应（OWLoRA），用于减轻包和实例分类中的遗忘。在三个公共WSI数据集上进行了广泛实验，结果表明CoMEL表现优越，在持续MIL设置下，包级准确率比现有技术提高了高达11.00%，定位准确率提高了高达23.4%。", "summary": "本文提出了CoMEL，一个针对组织病理学全玻片图像分析的持续多示例学习框架，旨在解决传统MIL在持续任务中适应性差和遗忘问题，并增强定位能力。CoMEL集成了分组双注意Transformer（GDAT）进行高效实例编码，基于包原型伪标签（BPPL）进行可靠实例伪标签，以及正交加权低秩适应（OWLoRA）以减轻遗忘。实验结果表明，CoMEL在包级和实例级定位准确率上均显著超越了现有技术。", "keywords": "持续学习, 多示例学习, 组织病理学图像, 全玻片图像, 定位", "comments": "CoMEL在将多示例学习应用于持续学习和增强定位方面取得了创新性进展，特别是在组织病理学全玻片图像这样高维度、数据量大的领域。其提出的GDAT、BPPL和OWLoRA组件协同工作，有效解决了大规模图像分析中持续学习的遗忘挑战和精确实例定位的需求。该工作对于减少医学图像分析中的标注成本和提高诊断效率具有重要意义。"}}
{"id": "2507.02409", "title": "S2FGL: Spatial Spectral Federated Graph Learning", "authors": ["Zihan Tan", "Suyuan Huang", "Guancheng Wan", "Wenke Huang", "He Li", "Mang Ye"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02409v1", "summary": "Federated Graph Learning (FGL) combines the privacy-preserving capabilities\nof federated learning (FL) with the strong graph modeling capability of Graph\nNeural Networks (GNNs). Current research addresses subgraph-FL only from the\nstructural perspective, neglecting the propagation of graph signals on spatial\nand spectral domains of the structure. From a spatial perspective, subgraph-FL\nintroduces edge disconnections between clients, leading to disruptions in label\nsignals and a degradation in the class knowledge of the global GNN. From a\nspectral perspective, spectral heterogeneity causes inconsistencies in signal\nfrequencies across subgraphs, which makes local GNNs overfit the local signal\npropagation schemes. As a result, spectral client drifts occur, undermining\nglobal generalizability. To tackle the challenges, we propose a global\nknowledge repository to mitigate label signal disruption and a frequency\nalignment to address spectral client drifts. The combination of spatial and\nspectral strategies forms our framework S2FGL. Extensive experiments on\nmultiple datasets demonstrate the superiority of S2FGL. The code is available\nat https://github.com/Wonder7racer/S2FGL.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02409v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "S2FGL：空间谱联邦图学习", "tldr": "本研究提出S2FGL框架，通过引入全局知识库和频率对齐来解决联邦图学习中由于空间和谱域信号传播不足导致的标签信号中断和谱客户端漂移问题，提升了全局泛化能力。", "motivation": "当前的联邦图学习（FGL）研究仅从结构角度处理子图FL，忽略了图信号在结构的空间和谱域上的传播。具体问题包括：空间视角下，客户端间的边断开导致标签信号中断和全局GNN类别知识退化；谱视角下，谱异质性导致子图间信号频率不一致，使得局部GNN过拟合本地信号传播方案，从而产生谱客户端漂移，损害全局泛化能力。", "method": "为了解决上述挑战，本文提出了S2FGL框架。该框架结合了两种策略：一是引入一个全局知识库来缓解标签信号中断；二是采用频率对齐来解决谱客户端漂移问题。S2FGL是空间和谱策略的结合。", "result": "在多个数据集上进行的广泛实验证明了S2FGL的优越性。", "conclusion": "S2FGL通过整合空间和谱域的策略，有效解决了联邦图学习中存在的标签信号中断和谱客户端漂移问题，显著提升了模型的全局泛化能力。", "translation": "联邦图学习（FGL）结合了联邦学习（FL）的隐私保护能力和图神经网络（GNN）强大的图建模能力。当前研究仅从结构角度处理子图联邦学习，忽略了图信号在结构的空间和谱域上的传播。从空间角度看，子图联邦学习引入了客户端之间的边断开，导致标签信号中断和全局GNN类别知识的退化。从谱角度看，谱异质性导致子图之间信号频率的不一致，使得局部GNN过度拟合本地信号传播方案。结果是，谱客户端漂移发生，损害了全局泛化能力。为了解决这些挑战，我们提出了一个全局知识库来缓解标签信号中断，并提出频率对齐来解决谱客户端漂移。空间和谱策略的结合形成了我们的S2FGL框架。在多个数据集上的广泛实验证明了S2FGL的优越性。代码可在https://github.com/Wonder7racer/S2FGL.git获取。", "summary": "本文提出了S2FGL框架，旨在解决联邦图学习（FGL）中因忽略图信号在空间和谱域传播而导致的问题。具体而言，S2FGL通过引入全局知识库来缓解由客户端间边断开引起的标签信号中断，并通过频率对齐来解决由谱异质性导致的谱客户端漂移。实验结果表明，S2FGL在多个数据集上表现出优越性。", "keywords": "联邦图学习, 空间谱, 图神经网络, 隐私保护, 客户端漂移", "comments": "S2FGL的创新点在于首次从空间和谱域的视角分析并解决了联邦图学习中存在的关键挑战，特别是针对标签信号中断和谱客户端漂移问题提出了具体的解决方案（全局知识库和频率对齐），这为提升联邦图学习的全局泛化能力提供了新的思路。该工作对联邦学习和图神经网络的结合具有重要意义。"}}
{"id": "2507.00884", "title": "A Scalable and Quantum-Accurate Foundation Model for Biomolecular Force Field via Linearly Tensorized Quadrangle Attention", "authors": ["Qun Su", "Kai Zhu", "Qiaolin Gou", "Jintu Zhang", "Renling Hu", "Yurong Li", "Yongze Wang", "Hui Zhang", "Ziyi You", "Linlong Jiang", "Yu Kang", "Jike Wang", "Chang-Yu Hsieh", "Tingjun Hou"], "categories": ["physics.chem-ph", "cs.AI", "cs.LG", "physics.bio-ph"], "primary_category": "Subjects:       Chemical Physics (physics.chem-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.00884v1", "summary": "Accurate atomistic biomolecular simulations are vital for disease mechanism\nunderstanding, drug discovery, and biomaterial design, but existing simulation\nmethods exhibit significant limitations. Classical force fields are efficient\nbut lack accuracy for transition states and fine conformational details\ncritical in many chemical and biological processes. Quantum Mechanics (QM)\nmethods are highly accurate but computationally infeasible for large-scale or\nlong-time simulations. AI-based force fields (AIFFs) aim to achieve QM-level\naccuracy with efficiency but struggle to balance many-body modeling complexity,\naccuracy, and speed, often constrained by limited training data and\ninsufficient validation for generalizability. To overcome these challenges, we\nintroduce LiTEN, a novel equivariant neural network with Tensorized Quadrangle\nAttention (TQA). TQA efficiently models three- and four-body interactions with\nlinear complexity by reparameterizing high-order tensor features via vector\noperations, avoiding costly spherical harmonics. Building on LiTEN, LiTEN-FF is\na robust AIFF foundation model, pre-trained on the extensive nablaDFT dataset\nfor broad chemical generalization and fine-tuned on SPICE for accurate solvated\nsystem simulations. LiTEN achieves state-of-the-art (SOTA) performance across\nmost evaluation subsets of rMD17, MD22, and Chignolin, outperforming leading\nmodels such as MACE, NequIP, and EquiFormer. LiTEN-FF enables the most\ncomprehensive suite of downstream biomolecular modeling tasks to date,\nincluding QM-level conformer searches, geometry optimization, and free energy\nsurface construction, while offering 10x faster inference than MACE-OFF for\nlarge biomolecules (~1000 atoms). In summary, we present a physically grounded,\nhighly efficient framework that advances complex biomolecular modeling,\nproviding a versatile foundation for drug discovery and related applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.00884v1", "cate": "physics.chem-ph", "date": "2025-07-01", "updated": "2025-07-01", "AI": {"title_translation": "可扩展的量子精度生物分子力场基础模型，基于线性张量化四边形注意力", "tldr": "提出LiTEN-FF，一个基于新型张量化四边形注意力的量子精度AI力场基础模型，用于生物分子模拟，实现SOTA性能并显著提高效率。", "motivation": "准确的原子级生物分子模拟对疾病机制理解、药物发现和生物材料设计至关重要，但现有模拟方法（经典力场、量子力学方法、AI力场）在效率、准确性、可扩展性或泛化性方面存在显著局限性，尤其是在处理过渡态、精细构象细节、大规模或长时间模拟以及训练数据受限时。", "method": "本文引入了LiTEN，一个带有张量化四边形注意力（TQA）的新型等变神经网络。TQA通过向量操作重新参数化高阶张量特征，以线性复杂度高效地建模三体和四体相互作用，避免了昂贵的球谐函数。在此基础上，构建了LiTEN-FF，一个鲁棒的AI力场基础模型，在广泛的nablaDFT数据集上进行预训练以实现广泛的化学泛化性，并在SPICE数据集上进行微调以实现准确的溶剂化系统模拟。", "result": "LiTEN在rMD17、MD22和Chignolin的大多数评估子集上实现了最先进（SOTA）的性能，优于MACE、NequIP和EquiFormer等领先模型。LiTEN-FF支持迄今为止最全面的下游生物分子建模任务，包括QM级别的构象搜索、几何优化和自由能表面构建，同时对于大型生物分子（约1000个原子）提供比MACE-OFF快10倍的推理速度。", "conclusion": "本文提出了一个基于物理原理、高效的框架，推进了复杂的生物分子建模，为药物发现和相关应用提供了多功能的基础。", "translation": "准确的原子级生物分子模拟对于理解疾病机制、药物发现和生物材料设计至关重要，但现有的模拟方法存在显著局限性。经典力场效率高但对于许多化学和生物过程中关键的过渡态和精细构象细节缺乏准确性。量子力学（QM）方法高度准确但对于大规模或长时间模拟而言计算上不可行。基于AI的力场（AIFFs）旨在实现QM级别的精度和效率，但难以平衡多体建模的复杂性、准确性和速度，常受限于有限的训练数据和不足的泛化性验证。为了克服这些挑战，我们引入了LiTEN，一个带有张量化四边形注意力（TQA）的新型等变神经网络。TQA通过向量操作重新参数化高阶张量特征，以线性复杂度高效地建模三体和四体相互作用，避免了昂贵的球谐函数。在LiTEN的基础上，LiTEN-FF是一个鲁棒的AIFF基础模型，在广泛的nablaDFT数据集上进行预训练以实现广泛的化学泛化性，并在SPICE数据集上进行微调以实现准确的溶剂化系统模拟。LiTEN在rMD17、MD22和Chignolin的大多数评估子集上实现了最先进（SOTA）的性能，优于MACE、NequIP和EquiFormer等领先模型。LiTEN-FF支持迄今为止最全面的下游生物分子建模任务，包括QM级别的构象搜索、几何优化和自由能表面构建，同时对于大型生物分子（约1000个原子）提供比MACE-OFF快10倍的推理速度。总而言之，我们提出了一个基于物理原理、高效的框架，推进了复杂的生物分子建模，为药物发现和相关应用提供了多功能的基础。", "summary": "本文介绍了LiTEN-FF，一个可扩展且量子精度的生物分子力场基础模型。该模型基于新颖的等变神经网络LiTEN及其核心组件张量化四边形注意力（TQA），TQA能以线性复杂度高效处理多体相互作用。LiTEN-FF通过在大型数据集上预训练和微调，实现了卓越的泛化能力和准确性，并在多个基准测试中超越现有SOTA模型，同时显著提高了大型生物分子模拟的推理速度，为药物发现等领域提供了强大的工具。", "keywords": "生物分子模拟, 力场, 神经网络, 张量化四边形注意力, 量子精度", "comments": "该论文的创新点在于提出了LiTEN及其核心的张量化四边形注意力（TQA）机制。TQA通过线性复杂度的张量化方法有效解决了多体相互作用的建模难题，避免了传统球谐函数的高计算成本，同时实现了量子精度和高效率。LiTEN-FF作为基础模型的预训练和微调策略也显著增强了其泛化能力和对实际生物分子系统的适用性。这项工作为生物分子模拟领域带来了突破，尤其是在药物发现和材料设计方面，有望加速相关研究进展。"}}
{"id": "2507.02398", "title": "Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection", "authors": ["Taehoon Kim", "Jongwook Choi", "Yonghyun Jeong", "Haeun Noh", "Jaejun Yoo", "Seungryul Baek", "Jongwon Choi"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      accepted by iccv 2025. code is will be available at this https URL", "url": "http://arxiv.org/abs/2507.02398v1", "summary": "We introduce a deepfake video detection approach that exploits pixel-wise\ntemporal inconsistencies, which traditional spatial frequency-based detectors\noften overlook. Traditional detectors represent temporal information merely by\nstacking spatial frequency spectra across frames, resulting in the failure to\ndetect temporal artifacts in the pixel plane. Our approach performs a 1D\nFourier transform on the time axis for each pixel, extracting features highly\nsensitive to temporal inconsistencies, especially in areas prone to unnatural\nmovements. To precisely locate regions containing the temporal artifacts, we\nintroduce an attention proposal module trained in an end-to-end manner.\nAdditionally, our joint transformer module effectively integrates pixel-wise\ntemporal frequency features with spatio-temporal context features, expanding\nthe range of detectable forgery artifacts. Our framework represents a\nsignificant advancement in deepfake video detection, providing robust\nperformance across diverse and challenging detection scenarios.", "comment": "accepted by iccv 2025. code is will be available at\n  https://github.com/rama0126/PwTF-DVD", "pdf_url": "http://arxiv.org/pdf/2507.02398v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "超越空间频率：基于像素级时间频率的深度伪造视频检测", "tldr": "提出一种基于像素级时间频率的深度伪造视频检测方法，通过分析时间轴上的像素变化来捕捉传统方法忽略的时间不一致性。", "motivation": "传统深度伪造检测器主要关注空间频率，未能有效检测像素平面上的时间伪影，因为它们仅通过堆叠跨帧的空间频率谱来表示时间信息。", "method": "本方法对每个像素的时间轴执行一维傅里叶变换，提取对时间不一致性敏感的特征。引入端到端训练的注意力提案模块以精确定位伪影区域。此外，联合Transformer模块有效整合像素级时间频率特征与时空上下文特征。", "result": "该框架显著提升了深度伪造视频检测的性能，在各种具有挑战性的检测场景中表现出鲁棒性。", "conclusion": "本研究提出了一种利用像素级时间不一致性的深度伪造视频检测新方法，通过捕捉传统方法忽略的时间伪影，显著提高了检测性能。", "translation": "我们引入了一种深度伪造视频检测方法，该方法利用像素级时间不一致性，而这是传统基于空间频率的检测器经常忽略的。传统检测器仅通过堆叠跨帧的空间频率谱来表示时间信息，导致未能检测像素平面中的时间伪影。我们的方法对每个像素的时间轴执行一维傅里叶变换，提取对时间不一致性高度敏感的特征，尤其是在容易出现不自然运动的区域。为了精确定位包含时间伪影的区域，我们引入了一个以端到端方式训练的注意力提案模块。此外，我们的联合Transformer模块有效地将像素级时间频率特征与时空上下文特征相结合，扩展了可检测的伪造伪影范围。我们的框架代表了深度伪造视频检测的重大进步，在各种多样化和具有挑战性的检测场景中提供了鲁健的性能。", "summary": "本文提出一种新颖的深度伪造视频检测方法，通过分析像素级时间频率来捕捉传统空间频率方法忽视的时间不一致性。该方法对每个像素的时间轴进行一维傅里叶变换以提取敏感特征，并结合注意力提案模块和联合Transformer模块，有效定位并整合时空上下文信息，显著提升了在复杂场景下的检测性能和鲁棒性。", "keywords": "深度伪造检测, 时间频率, 像素级分析, 傅里叶变换, 注意力机制", "comments": "该论文创新性地将时间频率分析引入深度伪造检测领域，克服了传统方法仅关注空间频率的局限性。通过像素级时间傅里叶变换和注意力机制，能够更精细地捕捉到伪造视频中的时间伪影，具有重要的理论和实际意义。"}}
{"id": "2507.02466", "title": "Variational Kolmogorov-Arnold Network", "authors": ["Francesco Alesiani", "Henrik Christiansen", "Federico Errica"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      A preliminary (short paper) version presented at ComBayNS Workshop at IJCNN'25", "url": "http://arxiv.org/abs/2507.02466v1", "summary": "Kolmogorov Arnold Networks (KANs) are an emerging architecture for building\nmachine learning models. KANs are based on the theoretical foundation of the\nKolmogorov-Arnold Theorem and its expansions, which provide an exact\nrepresentation of a multi-variate continuous bounded function as the\ncomposition of a limited number of univariate continuous functions. While such\ntheoretical results are powerful, their use as a representation learning\nalternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of\nthe number of bases modeling each of the univariate functions. In this work, we\nshow how to address this problem by adaptively learning a potentially infinite\nnumber of bases for each univariate function during training. We therefore\nmodel the problem as a variational inference optimization problem. Our\nproposal, called InfinityKAN, which uses backpropagation, extends the potential\napplicability of KANs by treating an important hyperparameter as part of the\nlearning process.", "comment": "A preliminary (short paper) version presented at ComBayNS Workshop at\n  IJCNN'25", "pdf_url": "http://arxiv.org/pdf/2507.02466v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "变分Kolmogorov-Arnold网络", "tldr": "本文提出了InfinityKAN，通过将基函数的数量作为变分推断优化问题的一部分进行自适应学习，解决了传统Kolmogorov-Arnold网络（KANs）中基函数数量选择的随意性问题，从而扩展了KANs的适用性。", "motivation": "传统的Kolmogorov-Arnold网络（KANs）在建模每个单变量函数时，需要随意选择基函数的数量，这是一个重要的局限性，阻碍了其作为多层感知器（MLP）替代方案的有效应用。", "method": "本文将基函数数量的选择问题建模为一个变分推断优化问题，并通过反向传播在训练过程中自适应地学习每个单变量函数潜在无限数量的基函数，提出了名为InfinityKAN的新方法。", "result": "InfinityKAN通过将一个重要的超参数（基函数数量）作为学习过程的一部分来处理，从而扩展了Kolmogorov-Arnold网络（KANs）的潜在适用性。", "conclusion": "通过将基函数数量作为学习过程的一部分，InfinityKAN成功解决了KANs中基函数选择的随意性问题，显著提升了其灵活性和适用范围。", "translation": "Kolmogorov-Arnold网络（KANs）是一种新兴的机器学习模型架构。KANs基于Kolmogorov-Arnold定理及其扩展的理论基础，该定理提供了一种将多变量连续有界函数精确表示为有限数量的单变量连续函数组合的方法。尽管这些理论结果功能强大，但它们作为多层感知器（MLP）的替代表示学习方法，其有效性取决于对建模每个单变量函数的基函数数量的随意选择。在这项工作中，我们展示了如何通过在训练期间自适应地学习每个单变量函数潜在无限数量的基函数来解决这个问题。因此，我们将该问题建模为一个变分推断优化问题。我们提出的方案名为InfinityKAN，它使用反向传播，通过将一个重要的超参数作为学习过程的一部分来处理，从而扩展了KANs的潜在适用性。", "summary": "本文提出了一种名为InfinityKAN的新型Kolmogorov-Arnold网络（KAN）架构，旨在解决传统KANs中对每个单变量函数基函数数量进行随意选择的问题。通过将该问题建模为变分推断优化，InfinityKAN能够在训练过程中自适应地学习潜在无限数量的基函数。这种方法将一个关键超参数纳入学习过程，显著扩展了KANs的适用范围和灵活性。", "keywords": "Kolmogorov-Arnold网络, 变分推断, 机器学习, InfinityKAN, 超参数学习", "comments": "这项工作具有创新性，因为它解决了Kolmogorov-Arnold网络（KANs）中的一个关键限制：基函数数量的超参数选择。通过将其转化为一个可学习的变分推断问题，并提出InfinityKAN，该方法提高了KANs的自动化程度和适用性，使其更具吸引力作为多层感知器（MLP）的替代方案。这对于推动可解释和高效的机器学习模型发展具有重要意义。"}}
{"id": "2507.02399", "title": "TABNet: A Triplet Augmentation Self-Recovery Framework with Boundary-Aware Pseudo-Labels for Medical Image Segmentation", "authors": ["Peilin Zhang", "Shaouxan Wua", "Jun Feng", "Zhuo Jin", "Zhizezhang Gao", "Jingkun Chen", "Yaqiong Xing", "Xiao Zhang"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02399v1", "summary": "Background and objective: Medical image segmentation is a core task in\nvarious clinical applications. However, acquiring large-scale, fully annotated\nmedical image datasets is both time-consuming and costly. Scribble annotations,\nas a form of sparse labeling, provide an efficient and cost-effective\nalternative for medical image segmentation. However, the sparsity of scribble\nannotations limits the feature learning of the target region and lacks\nsufficient boundary supervision, which poses significant challenges for\ntraining segmentation networks. Methods: We propose TAB Net, a novel\nweakly-supervised medical image segmentation framework, consisting of two key\ncomponents: the triplet augmentation self-recovery (TAS) module and the\nboundary-aware pseudo-label supervision (BAP) module. The TAS module enhances\nfeature learning through three complementary augmentation strategies: intensity\ntransformation improves the model's sensitivity to texture and contrast\nvariations, cutout forces the network to capture local anatomical structures by\nmasking key regions, and jigsaw augmentation strengthens the modeling of global\nanatomical layout by disrupting spatial continuity. By guiding the network to\nrecover complete masks from diverse augmented inputs, TAS promotes a deeper\nsemantic understanding of medical images under sparse supervision. The BAP\nmodule enhances pseudo-supervision accuracy and boundary modeling by fusing\ndual-branch predictions into a loss-weighted pseudo-label and introducing a\nboundary-aware loss for fine-grained contour refinement. Results: Experimental\nevaluations on two public datasets, ACDC and MSCMR seg, demonstrate that TAB\nNet significantly outperforms state-of-the-art methods for scribble-based\nweakly supervised segmentation. Moreover, it achieves performance comparable to\nthat of fully supervised methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02399v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "TABNet：一种用于医学图像分割的边界感知伪标签三元组增强自恢复框架", "tldr": "TABNet是一种弱监督医学图像分割框架，通过三元组增强自恢复模块和边界感知伪标签监督模块，解决了稀疏涂鸦标注的挑战，性能优于现有方法并接近全监督方法。", "motivation": "医学图像分割是临床应用的核心任务，但获取大规模全标注数据集耗时且昂贵。涂鸦标注作为一种稀疏标注形式，效率高成本低，但其稀疏性限制了目标区域的特征学习，并缺乏足够的边界监督，给训练分割网络带来了挑战。", "method": "提出了TAB Net，一个弱监督医学图像分割框架，包含两个关键组件：三元组增强自恢复（TAS）模块和边界感知伪标签监督（BAP）模块。TAS模块通过强度变换、剪切和拼图三种互补的增强策略提升特征学习，引导网络从多样化的增强输入中恢复完整掩膜。BAP模块通过融合双分支预测为损失加权伪标签并引入边界感知损失，增强伪监督精度和边界建模。", "result": "在ACDC和MSCMR seg两个公共数据集上的实验评估表明，TAB Net显著优于最先进的基于涂鸦的弱监督分割方法。此外，它达到了与全监督方法相当的性能。", "conclusion": "TAB Net有效地解决了稀疏涂鸦标注在医学图像分割中的挑战，通过创新的增强和伪标签策略，实现了卓越的分割性能，甚至可与全监督方法媲美。", "translation": "背景与目标：医学图像分割是各种临床应用中的核心任务。然而，获取大规模、完全标注的医学图像数据集既耗时又昂贵。涂鸦标注作为一种稀疏标注形式，为医学图像分割提供了一种高效且经济的替代方案。然而，涂鸦标注的稀疏性限制了目标区域的特征学习，并且缺乏足够的边界监督，这给训练分割网络带来了重大挑战。方法：我们提出了TAB Net，一个新颖的弱监督医学图像分割框架，由两个关键组件组成：三元组增强自恢复（TAS）模块和边界感知伪标签监督（BAP）模块。TAS模块通过三种互补的增强策略增强特征学习：强度变换提高了模型对纹理和对比度变化的敏感性；剪切通过遮蔽关键区域迫使网络捕获局部解剖结构；拼图增强通过打乱空间连续性来加强全局解剖布局的建模。通过引导网络从多样化的增强输入中恢复完整的掩膜，TAS促进了稀疏监督下医学图像的更深层语义理解。BAP模块通过将双分支预测融合为损失加权伪标签，并引入边界感知损失进行精细轮廓细化，从而提高了伪监督的准确性和边界建模。结果：在ACDC和MSCMR seg两个公共数据集上的实验评估表明，TAB Net显著优于最先进的基于涂鸦的弱监督分割方法。此外，它达到了与全监督方法相当的性能。", "summary": "本研究提出了一种名为TAB Net的弱监督医学图像分割框架，旨在解决稀疏涂鸦标注在训练分割网络时面临的挑战，即特征学习受限和缺乏边界监督。该框架包含三元组增强自恢复（TAS）模块和边界感知伪标签监督（BAP）模块。TAS模块通过多种数据增强策略提升模型对图像语义的理解，而BAP模块则通过生成高质量伪标签和引入边界损失来优化分割精度和边界细节。实验结果表明，TAB Net在多个公共数据集上显著优于现有的弱监督方法，并能达到与全监督方法相当的性能。", "keywords": "弱监督分割, 医学图像, 涂鸦标注, 数据增强, 伪标签", "comments": "TAB Net的创新性在于其结合了多种数据增强策略（三元组增强自恢复）来弥补稀疏标注带来的信息不足，并通过边界感知伪标签监督有效地提升了分割精度和边界细节。这种方法为利用低成本标注实现高性能医学图像分割提供了有力的解决方案，具有重要的实际应用价值，特别是在标注资源有限的场景下。"}}
{"id": "2507.02135", "title": "Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency", "authors": ["Zongpu Zhang", "Pranab Dash", "Y. Charlie Hu", "Qiang Xu", "Jian Li", "Haibing Guan"], "categories": ["cs.OS", "cs.CL"], "primary_category": "Subjects:       Operating Systems (cs.OS)", "pdf_link": null, "comments": "Comments:      equal contribution between Zhang and Dash", "url": "http://arxiv.org/abs/2507.02135v1", "summary": "Large Language Models (LLMs) are increasingly being integrated into various\napplications and services running on billions of mobile devices. However,\ndeploying LLMs on resource-limited mobile devices faces a significant challenge\ndue to their high demand for computation, memory, and ultimately energy. While\ncurrent LLM frameworks for mobile use three power-hungry components-CPU, GPU,\nand Memory-even when running primarily-GPU LLM models, optimized DVFS governors\nfor CPU, GPU, and memory featured in modern mobile devices operate\nindependently and are oblivious of each other. Motivated by the above\nobservation, in this work, we first measure the energy-efficiency of a SOTA LLM\nframework consisting of various LLM models on mobile phones which showed the\ntriplet mobile governors result in up to 40.4% longer prefilling and decoding\nlatency compared to optimal combinations of CPU, GPU, and memory frequencies\nwith the same energy consumption for sampled prefill and decode lengths.\nSecond, we conduct an in-depth measurement study to uncover how the intricate\ninterplay (or lack of) among the mobile governors cause the above inefficiency\nin LLM inference. Finally, based on these insights, we design FUSE - a unified\nenergy-aware governor for optimizing the energy efficiency of LLM inference on\nmobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the\ntime-to-first-token and time-per-output-token latencies by 7.0%-16.9% and\n25.4%-36.8% on average with the same energy-per-token for various mobile LLM\nmodels.", "comment": "equal contribution between Zhang and Dash", "pdf_url": "http://arxiv.org/pdf/2507.02135v1", "cate": "cs.OS", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "剖析移动DVFS调速器对LLM推理性能和能效的影响", "tldr": "在移动设备上，LLM推理的DVFS调速器独立运作导致效率低下；本文设计了一种名为FUSE的统一能效感知调速器，显著提高了LLM推理的能效并降低了延迟。", "motivation": "大型语言模型（LLMs）在移动设备上的部署面临计算、内存和能耗的巨大挑战。尽管当前的移动LLM框架使用了CPU、GPU和内存这三个高功耗组件，但其优化的DVFS调速器却独立运行，彼此之间互不感知，导致LLM推理效率低下。", "method": "首先，测量了移动手机上SOTA LLM框架的能效，发现现有的三组移动调速器在相同能耗下会导致高达40.4%的预填充和解码延迟。其次，进行深入测量研究，揭示了移动调速器之间复杂（或缺乏）的相互作用如何导致LLM推理的低效率。最后，基于这些洞察，设计了FUSE——一个统一的能效感知调速器，用于优化移动设备上LLM推理的能效。", "result": "研究发现，现有的三组移动调速器在相同能耗下，导致预填充和解码延迟最高延长40.4%。FUSE在各种移动LLM模型上的评估显示，在相同每令牌能耗下，它将首令牌时间和每输出令牌时间延迟平均分别降低了7.0%-16.9%和25.4%-36.8%。", "conclusion": "本文设计并验证了FUSE，一个统一的能效感知调速器，能够有效优化移动设备上LLM推理的能效，显著降低延迟。", "translation": "大型语言模型（LLMs）正日益被集成到运行在数十亿移动设备上的各种应用和服务中。然而，由于LLM对计算、内存以及最终对能耗的高要求，在资源有限的移动设备上部署LLM面临巨大挑战。尽管当前用于移动设备的LLM框架使用了CPU、GPU和内存这三个高功耗组件——即使主要运行基于GPU的LLM模型——但现代移动设备中针对CPU、GPU和内存的优化DVFS调速器却是独立运行且彼此互不感知。受上述观察启发，本文首先测量了由各种LLM模型组成的SOTA LLM框架在移动手机上的能效，结果显示，对于采样的预填充和解码长度，在相同能耗下，三组移动调速器相比CPU、GPU和内存频率的最优组合，导致预填充和解码延迟最长延长40.4%。其次，我们进行了一项深入的测量研究，以揭示移动调速器之间复杂（或缺乏）的相互作用如何导致LLM推理的上述低效率。最后，基于这些洞察，我们设计了FUSE——一个统一的能效感知调速器，用于优化移动设备上LLM推理的能效。我们使用ShareGPT数据集进行的评估显示，FUSE在相同每令牌能耗下，将首令牌时间和每输出令牌时间延迟平均分别降低了7.0%-16.9%和25.4%-36.8%，适用于各种移动LLM模型。", "summary": "本文探讨了在移动设备上部署大型语言模型（LLMs）所面临的能效挑战，指出当前CPU、GPU和内存的DVFS调速器独立运作导致LLM推理效率低下。通过测量发现，现有调速器会显著增加延迟。为解决此问题，作者设计了FUSE——一个统一的能效感知调速器。实验结果表明，FUSE在保持相同能耗的前提下，显著降低了LLM推理的首令牌和每输出令牌延迟，从而提高了移动LLM的能效。", "keywords": "LLM, 移动设备, DVFS, 能效, FUSE", "comments": "本文揭示了移动设备上LLM推理中，DVFS调速器独立运行所带来的重要效率问题，并提出了一个创新的统一调速器FUSE。其创新性在于从系统层面整合了CPU、GPU和内存的功耗管理，而非单独优化。这项工作对于推动LLM在资源受限移动设备上的实际部署具有重要意义，尤其是在能耗和性能平衡方面。"}}
{"id": "2507.01972", "title": "Accelerated Portfolio Optimization and Option Pricing with Reinforcement Learning", "authors": ["Hadi Keramati", "Samaneh Jazayeri"], "categories": ["q-fin.PM", "cs.AI", "cs.LG", "q-fin.CP"], "primary_category": "Subjects:       Portfolio Management (q-fin.PM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01972v1", "summary": "We present a reinforcement learning (RL)-driven framework for optimizing\nblock-preconditioner sizes in iterative solvers used in portfolio optimization\nand option pricing. The covariance matrix in portfolio optimization or the\ndiscretization of differential operators in option pricing models lead to large\nlinear systems of the form $\\mathbf{A}\\textbf{x}=\\textbf{b}$. Direct inversion\nof high-dimensional portfolio or fine-grid option pricing incurs a significant\ncomputational cost. Therefore, iterative methods are usually used for\nportfolios in real-world situations. Ill-conditioned systems, however, suffer\nfrom slow convergence. Traditional preconditioning techniques often require\nproblem-specific parameter tuning. To overcome this limitation, we rely on RL\nto dynamically adjust the block-preconditioner sizes and accelerate iterative\nsolver convergence. Evaluations on a suite of real-world portfolio optimization\nmatrices demonstrate that our RL framework can be used to adjust\npreconditioning and significantly accelerate convergence and reduce\ncomputational cost. The proposed accelerated solver supports faster\ndecision-making in dynamic portfolio allocation and real-time option pricing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01972v1", "cate": "q-fin.PM", "date": "2025-06-23", "updated": "2025-06-23", "AI": {"title_translation": "基于强化学习的加速投资组合优化和期权定价", "tldr": "本文提出一个基于强化学习的框架，用于动态调整迭代求解器中的块预处理器大小，以加速投资组合优化和期权定价中的收敛速度并降低计算成本。", "motivation": "投资组合优化和期权定价中的大型线性系统直接求逆计算成本高昂，而迭代方法在病态系统下收敛缓慢。传统的预处理技术需要问题特定的参数调整，存在局限性。", "method": "本文依靠强化学习（RL）来动态调整块预处理器的大小，以加速迭代求解器的收敛。", "result": "在真实世界投资组合优化矩阵套件上的评估表明，本文的强化学习框架可以调整预处理并显著加速收敛，降低计算成本。", "conclusion": "所提出的加速求解器支持动态投资组合分配和实时期权定价中更快的决策制定。", "translation": "我们提出了一个强化学习（RL）驱动的框架，用于优化投资组合优化和期权定价中迭代求解器中的块预处理器大小。投资组合优化中的协方差矩阵或期权定价模型中微分算子的离散化导致了$\\mathbf{A}\\textbf{x}=\\textbf{b}$形式的大型线性系统。高维投资组合或精细网格期权定价的直接求逆会产生显著的计算成本。因此，在实际情况中，迭代方法通常用于投资组合。然而，病态系统收敛缓慢。传统的预处理技术通常需要问题特定的参数调整。为了克服这一限制，我们依靠强化学习来动态调整块预处理器大小并加速迭代求解器收敛。对一系列真实世界投资组合优化矩阵的评估表明，我们的强化学习框架可用于调整预处理并显著加速收敛和降低计算成本。所提出的加速求解器支持动态投资组合分配和实时期权定价中更快的决策制定。", "summary": "本文提出了一个基于强化学习（RL）的框架，旨在解决投资组合优化和期权定价中迭代求解器收敛缓慢和传统预处理技术需要手动调参的问题。通过利用RL动态调整块预处理器的大小，该方法显著加速了求解器收敛并降低了计算成本。实验结果表明，该RL框架在实际投资组合优化任务中表现出色，从而能够支持更快的金融决策。", "keywords": "强化学习, 投资组合优化, 期权定价, 迭代求解器, 预处理", "comments": "该论文的创新点在于将强化学习应用于迭代求解器中的块预处理器大小的动态调整，有效克服了传统预处理技术需要手动调参的局限性。这对于加速金融领域（如投资组合优化和期权定价）中的复杂计算具有重要意义，能够支持更快的实时决策，展现了RL在科学计算优化方面的潜力。"}}
{"id": "2507.02403", "title": "Wildlife Target Re-Identification Using Self-supervised Learning in Non-Urban Settings", "authors": ["Mufhumudzi Muthivhi", "Terence L. van Zyl"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for publication in IEEE Xplore and ISIF FUSION 2025 proceedings:", "url": "http://arxiv.org/abs/2507.02403v1", "summary": "Wildlife re-identification aims to match individuals of the same species\nacross different observations. Current state-of-the-art (SOTA) models rely on\nclass labels to train supervised models for individual classification. This\ndependence on annotated data has driven the curation of numerous large-scale\nwildlife datasets. This study investigates self-supervised learning\nSelf-Supervised Learning (SSL) for wildlife re-identification. We automatically\nextract two distinct views of an individual using temporal image pairs from\ncamera trap data without supervision. The image pairs train a self-supervised\nmodel from a potentially endless stream of video data. We evaluate the learnt\nrepresentations against supervised features on open-world scenarios and\ntransfer learning in various wildlife downstream tasks. The analysis of the\nexperimental results shows that self-supervised models are more robust even\nwith limited data. Moreover, self-supervised features outperform supervision\nacross all downstream tasks. The code is available here\nhttps://github.com/pxpana/SSLWildlife.", "comment": "Accepted for publication in IEEE Xplore and ISIF FUSION 2025\n  proceedings:", "pdf_url": "http://arxiv.org/pdf/2507.02403v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "非城市环境下使用自监督学习进行野生动物目标重识别", "tldr": "该研究提出在野生动物重识别中使用自监督学习，以减少对标注数据的依赖，并证明其比监督方法更具鲁棒性且性能更优。", "motivation": "当前的野生动物重识别模型严重依赖大量标注数据进行监督训练，这使得数据整理工作非常耗费资源。本研究旨在解决这种对数据的依赖性。", "method": "本研究采用自监督学习（SSL）方法。它利用相机陷阱数据中的时间图像对，在无监督的情况下自动提取个体的两个不同视图。这些图像对用于训练一个自监督模型。然后，在开放世界场景和各种下游任务的迁移学习中，将学习到的表示与监督特征进行评估。", "result": "自监督模型即使在数据有限的情况下也更具鲁棒性。自监督特征在所有下游任务中均优于监督特征。", "conclusion": "自监督学习是野生动物重识别的有效方法，与监督方法相比，它具有鲁棒性和卓越的性能，特别有助于减少对大型标注数据集的依赖。", "translation": "野生动物重识别旨在匹配不同观测中同一物种的个体。当前最先进（SOTA）的模型依赖于类别标签来训练用于个体分类的监督模型。这种对标注数据的依赖推动了大量野生动物数据集的整理。本研究调查了自监督学习（SSL）在野生动物重识别中的应用。我们利用相机陷阱数据中的时间图像对，在没有监督的情况下自动提取个体的两个不同视图。这些图像对用于训练一个自监督模型，该模型可以从潜在无限的视频数据流中获取数据。我们评估了学习到的表示在开放世界场景和各种野生动物下游任务中的迁移学习方面与监督特征的对比。实验结果分析表明，自监督模型即使在数据有限的情况下也更具鲁棒性。此外，自监督特征在所有下游任务中均优于监督特征。代码可在 https://github.com/pxpana/SSLWildlife 获取。", "summary": "本研究探讨了自监督学习（SSL）在野生动物重识别中的应用，旨在克服当前最先进的监督模型对大量标注数据集的依赖。所提出的方法自动从相机陷阱的时间图像对中提取不同的视图，以训练一个自监督模型。实验结果表明，自监督模型在数据有限的情况下更具鲁棒性，并且其特征在各种下游任务中优于监督方法，这表明了一种更高效、更有效的野生动物重识别方法。", "keywords": "野生动物重识别, 自监督学习, 相机陷阱, 鲁棒性, 迁移学习", "comments": "这篇论文通过利用自监督学习，为野生动物重识别提供了一种创新方法，显著减少了对劳动密集型数据标注的依赖。其发现SSL模型即使在数据有限的情况下也更具鲁棒性并优于监督方法，这是一个重要的贡献，尤其对于生态研究中大型标注数据集通常稀缺的情况。这项工作突出了SSL在非城市、数据受限环境中推动计算机视觉应用的潜力。"}}
{"id": "2507.02503", "title": "Continual Gradient Low-Rank Projection Fine-Tuning for LLMs", "authors": ["Chenxu Wang", "Yilin Lyu", "Zicheng Sun", "Liping Jing"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      15 pages, 6 figures, accepted by ACL 2025 main", "url": "http://arxiv.org/abs/2507.02503v1", "summary": "Continual fine-tuning of Large Language Models (LLMs) is hampered by the\ntrade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)\noffers efficiency but constrains the model's ability to learn new tasks and\ntransfer knowledge due to its low-rank nature and reliance on explicit\nparameter constraints. We propose GORP (Gradient LOw Rank Projection) for\nContinual Learning, a novel training strategy that overcomes these limitations\nby synergistically combining full and low-rank parameters and jointly updating\nwithin a unified low-rank gradient subspace. GORP expands the optimization\nspace while preserving efficiency and mitigating catastrophic forgetting.\nExtensive experiments on continual learning benchmarks demonstrate GORP's\nsuperior performance compared to existing state-of-the-art approaches. Code is\navailable at https://github.com/Wcxwcxw/GORP.", "comment": "15 pages, 6 figures, accepted by ACL 2025 main", "pdf_url": "http://arxiv.org/pdf/2507.02503v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LLM的持续梯度低秩投影微调", "tldr": "针对LLM持续微调中LoRA的局限性，本文提出GORP方法。GORP通过结合全秩和低秩参数，并在统一的低秩梯度子空间中联合更新，有效提升了性能并缓解了灾难性遗忘，同时保持了效率。", "motivation": "大型语言模型（LLMs）的持续微调面临效率和表达能力之间的权衡。现有的低秩适应（LoRA）方法虽然效率高，但由于其低秩性质和对显式参数约束的依赖，限制了模型学习新任务和知识迁移的能力。", "method": "本文提出了用于持续学习的梯度低秩投影（GORP）方法。GORP通过协同结合全秩和低秩参数，并在统一的低秩梯度子空间中联合更新，从而克服了现有方法的局限性。这种方法扩展了优化空间，同时保持了效率并减轻了灾难性遗忘。", "result": "在持续学习基准上的大量实验表明，与现有最先进的方法相比，GORP表现出卓越的性能。", "conclusion": "GORP通过结合全秩和低秩参数并在统一的低秩梯度子空间中更新，成功解决了LLM持续微调中效率与表达能力的权衡问题，显著提升了性能并有效缓解了灾难性遗忘。", "translation": "大型语言模型（LLMs）的持续微调受到效率和表达能力之间权衡的阻碍。低秩适应（LoRA）提供了效率，但由于其低秩性质和对显式参数约束的依赖，限制了模型学习新任务和知识迁移的能力。我们提出了用于持续学习的GORP（梯度低秩投影），这是一种新颖的训练策略，通过协同结合全秩和低秩参数并在统一的低秩梯度子空间中联合更新来克服这些限制。GORP扩展了优化空间，同时保持了效率并减轻了灾难性遗忘。在持续学习基准上的大量实验表明，与现有最先进的方法相比，GORP表现出卓越的性能。代码可在https://github.com/Wcxwcxw/GORP获取。", "summary": "GORP是一种新颖的LLM持续学习策略，它通过在统一的低秩梯度子空间中联合更新全秩和低秩参数，解决了LoRA在持续微调中的局限性。该方法在保持效率的同时扩展了优化空间并减轻了灾难性遗忘，在持续学习基准上表现出优于现有最先进方法的性能。", "keywords": "持续学习, LLMs, 低秩适应, 微调, 灾难性遗忘", "comments": "GORP的创新之处在于其巧妙地结合了全秩和低秩参数，并在统一的梯度子空间中进行更新，这有效地扩展了优化空间而没有牺牲效率。该方法解决了LLM持续学习中的关键权衡问题，代表了一项重要的进步。"}}
{"id": "2507.02405", "title": "PosDiffAE: Position-aware Diffusion Auto-encoder For High-Resolution Brain Tissue Classification Incorporating Artifact Restoration", "authors": ["Ayantika Das", "Moitreya Chaudhuri", "Koushik Bhat", "Keerthi Ram", "Mihail Bota", "Mohanasankar Sivaprakasam"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Published in IEEE Journal of Biomedical and Health Informatics (Early Access Available) this https URL", "url": "http://arxiv.org/abs/2507.02405v1", "summary": "Denoising diffusion models produce high-fidelity image samples by capturing\nthe image distribution in a progressive manner while initializing with a simple\ndistribution and compounding the distribution complexity. Although these models\nhave unlocked new applicabilities, the sampling mechanism of diffusion does not\noffer means to extract image-specific semantic representation, which is\ninherently provided by auto-encoders. The encoding component of auto-encoders\nenables mapping between a specific image and its latent space, thereby offering\nexplicit means of enforcing structures in the latent space. By integrating an\nencoder with the diffusion model, we establish an auto-encoding formulation,\nwhich learns image-specific representations and offers means to organize the\nlatent space. In this work, First, we devise a mechanism to structure the\nlatent space of a diffusion auto-encoding model, towards recognizing\nregion-specific cellular patterns in brain images. We enforce the\nrepresentations to regress positional information of the patches from\nhigh-resolution images. This creates a conducive latent space for\ndifferentiating tissue types of the brain. Second, we devise an unsupervised\ntear artifact restoration technique based on neighborhood awareness, utilizing\nlatent representations and the constrained generation capability of diffusion\nmodels during inference. Third, through representational guidance and\nleveraging the inference time steerable noising and denoising capability of\ndiffusion, we devise an unsupervised JPEG artifact restoration technique.", "comment": "Published in IEEE Journal of Biomedical and Health Informatics (Early\n  Access Available) https://ieeexplore.ieee.org/document/10989734", "pdf_url": "http://arxiv.org/pdf/2507.02405v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "PosDiffAE：用于高分辨率脑组织分类并结合伪影修复的位置感知扩散自编码器", "tldr": "PosDiffAE 是一种结合了扩散模型和自编码器优势的新模型，它能学习图像特异性表示并组织潜在空间，用于高分辨率脑组织分类和无监督的撕裂及 JPEG 伪影修复。", "motivation": "传统的去噪扩散模型虽然能生成高质量图像，但无法提取图像特异性语义表示，而自编码器则能提供这种能力并组织潜在空间。本文旨在结合两者的优势，解决扩散模型在语义表示方面的局限性，并应用于高分辨率脑组织分类和伪影修复。", "method": "本文提出了 PosDiffAE 模型，将编码器与扩散模型集成，形成自编码器公式，以学习图像特异性表示并组织潜在空间。具体方法包括：1. 设计机制来构建扩散自编码模型的潜在空间，通过强制表示回归高分辨率图像块的位置信息，使其有利于区分脑组织类型。2. 基于邻域感知，利用潜在表示和扩散模型的约束生成能力，开发了一种无监督的撕裂伪影修复技术。3. 通过表示引导并利用扩散模型在推理时的可控噪声和去噪能力，开发了一种无监督的 JPEG 伪影修复技术。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "去噪扩散模型通过以渐进方式捕捉图像分布，同时以简单分布初始化并复合分布复杂性，从而生成高保真图像样本。尽管这些模型开启了新的应用可能性，但扩散的采样机制不提供提取图像特异性语义表示的方法，而这正是自编码器固有提供的。自编码器的编码组件能够实现特定图像与其潜在空间之间的映射，从而提供在潜在空间中强制结构的明确方法。通过将编码器与扩散模型集成，我们建立了一种自编码公式，该公式学习图像特异性表示并提供组织潜在空间的方法。在这项工作中，首先，我们设计了一种机制来构建扩散自编码模型的潜在空间，旨在识别脑图像中区域特定的细胞模式。我们强制表示回归高分辨率图像中图像块的位置信息。这为区分脑组织类型创造了一个有利的潜在空间。其次，我们基于邻域感知，利用潜在表示和扩散模型在推理过程中受限的生成能力，设计了一种无监督的撕裂伪影修复技术。第三，通过表示引导并利用扩散在推理时可控的噪声和去噪能力，我们设计了一种无监督的 JPEG 伪影修复技术。", "summary": "本文提出了一种名为 PosDiffAE 的位置感知扩散自编码器模型，旨在结合扩散模型生成高保真图像的优势和自编码器提取图像特异性语义表示并组织潜在空间的能力。该模型通过强制潜在表示回归图像块的位置信息来构建潜在空间，从而有效地用于高分辨率脑组织分类。此外，PosDiffAE 还开发了基于邻域感知和表示引导的无监督撕裂伪影修复和 JPEG 伪影修复技术，利用了扩散模型在推理时的生成和去噪能力。", "keywords": "扩散模型, 自编码器, 脑组织分类, 伪影修复, 高分辨率", "comments": "该论文的创新点在于将扩散模型与自编码器相结合，弥补了传统扩散模型在提取语义表示方面的不足。通过引入位置感知机制来构建潜在空间，使其更适用于精细的脑组织分类任务。同时，其提出的无监督伪影修复技术也展示了模型的实用性和泛化潜力。"}}
{"id": "2507.01979", "title": "Forecasting Labor Markets with LSTNet: A Multi-Scale Deep Learning Approach", "authors": ["Adam Nelson-Archer", "Aleia Sen", "Meena Al Hasani", "Sofia Davila", "Jessica Le", "Omar Abbouchi"], "categories": ["q-fin.ST", "cs.AI", "cs.LG", "I.2.6; I.5.1"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "Comments:      Undergraduate senior project, University of Houston, Department of Computer Science", "url": "http://arxiv.org/abs/2507.01979v1", "summary": "We present a deep learning approach for forecasting short-term employment\nchanges and assessing long-term industry health using labor market data from\nthe U.S. Bureau of Labor Statistics. Our system leverages a Long- and\nShort-Term Time-series Network (LSTNet) to process multivariate time series\ndata, including employment levels, wages, turnover rates, and job openings. The\nmodel outputs both 7-day employment forecasts and an interpretable Industry\nEmployment Health Index (IEHI). Our approach outperforms baseline models across\nmost sectors, particularly in stable industries, and demonstrates strong\nalignment between IEHI rankings and actual employment volatility. We discuss\nerror patterns, sector-specific performance, and future directions for\nimproving interpretability and generalization.", "comment": "Undergraduate senior project, University of Houston, Department of\n  Computer Science", "pdf_url": "http://arxiv.org/pdf/2507.01979v1", "cate": "q-fin.ST", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "使用LSTNet预测劳动力市场：一种多尺度深度学习方法", "tldr": "本文提出了一种基于LSTNet的深度学习方法，用于预测短期就业变化和评估长期行业健康，并表现出优于基线模型的性能。", "motivation": "为了预测短期就业变化并评估长期行业健康，利用美国劳工统计局的劳动力市场数据。", "method": "采用深度学习方法，具体使用长短期时间序列网络（LSTNet）处理多元时间序列数据，包括就业水平、工资、离职率和职位空缺。模型输出7天就业预测和可解释的行业就业健康指数（IEHI）。", "result": "该方法在大多数行业，特别是稳定行业中，表现优于基线模型。IEHI排名与实际就业波动之间显示出很强的一致性。", "conclusion": "提出的深度学习方法（LSTNet）能有效预测劳动力市场，并提供可解释的行业健康评估，优于基线模型。", "translation": "我们提出了一种深度学习方法，利用美国劳工统计局的劳动力市场数据，预测短期就业变化并评估长期行业健康。我们的系统利用长短期时间序列网络（LSTNet）处理多元时间序列数据，包括就业水平、工资、周转率和职位空缺。该模型输出7天就业预测和可解释的行业就业健康指数（IEHI）。我们的方法在大多数行业，特别是在稳定行业中，表现优于基线模型，并且IEHI排名与实际就业波动之间显示出很强的一致性。我们讨论了误差模式、特定行业的表现以及未来改进可解释性和泛化能力的方向。", "summary": "本文介绍了一种利用LSTNet深度学习模型预测短期就业变化和评估长期行业健康的系统。该系统处理包括就业水平、工资等在内的多元时间序列数据，并生成7天就业预测和行业就业健康指数（IEHI）。实验结果表明，该方法在多数行业（尤其稳定行业）表现优于基线模型，且IEHI与实际就业波动高度一致。", "keywords": "劳动力市场预测, 深度学习, LSTNet, 时间序列, 行业健康指数", "comments": "这篇论文的创新点在于将LSTNet应用于劳动力市场预测，并引入了可解释的行业就业健康指数（IEHI），为政策制定者和行业分析提供了有价值的工具。其多尺度深度学习方法能够同时处理短期预测和长期健康评估，具有较高的实用性。"}}
{"id": "2507.02408", "title": "A Novel Tuning Method for Real-time Multiple-Object Tracking Utilizing Thermal Sensor with Complexity Motion Pattern", "authors": ["Duong Nguyen-Ngoc Tran", "Long Hoang Pham", "Chi Dai Tran", "Quoc Pham-Nam Ho", "Huy-Hung Nguyen", "Jae Wook Jeon"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02408v1", "summary": "Multi-Object Tracking in thermal images is essential for surveillance\nsystems, particularly in challenging environments where RGB cameras struggle\ndue to low visibility or poor lighting conditions. Thermal sensors enhance\nrecognition tasks by capturing infrared signatures, but a major challenge is\ntheir low-level feature representation, which makes it difficult to accurately\ndetect and track pedestrians. To address this, the paper introduces a novel\ntuning method for pedestrian tracking, specifically designed to handle the\ncomplex motion patterns in thermal imagery. The proposed framework optimizes\ntwo-stages, ensuring that each stage is tuned with the most suitable\nhyperparameters to maximize tracking performance. By fine-tuning\nhyperparameters for real-time tracking, the method achieves high accuracy\nwithout relying on complex reidentification or motion models. Extensive\nexperiments on PBVS Thermal MOT dataset demonstrate that the approach is highly\neffective across various thermal camera conditions, making it a robust solution\nfor real-world surveillance applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02408v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "一种利用热传感器处理复杂运动模式的实时多目标跟踪新型调优方法", "tldr": "本文提出了一种新颖的调优方法，通过优化两阶段超参数，实现了在热图像中对复杂运动模式下行人的高精度实时跟踪，无需复杂的重识别或运动模型。", "motivation": "在低能见度或光照不足的恶劣环境下，RGB相机难以进行多目标跟踪，而热传感器虽能增强识别任务，但其低级特征表示使得准确检测和跟踪行人成为挑战。", "method": "本文提出了一种新颖的两阶段调优方法，专门用于处理热图像中复杂的运动模式下的行人跟踪。该框架通过为每个阶段选择最合适的超参数来优化跟踪性能，从而在不依赖复杂重识别或运动模型的情况下实现高精度实时跟踪。", "result": "在PBVS热MOT数据集上进行的广泛实验表明，该方法在各种热相机条件下都非常有效。", "conclusion": "该方法为热图像中的实时多目标跟踪提供了一个鲁棒且高精度的解决方案，特别适用于需要处理复杂运动模式的监控应用。", "translation": "热图像中的多目标跟踪对于监控系统至关重要，特别是在RGB相机因低能见度或光照不足而难以工作的挑战性环境中。热传感器通过捕获红外特征来增强识别任务，但一个主要挑战是其低级特征表示，这使得准确检测和跟踪行人变得困难。为了解决这个问题，本文引入了一种新颖的行人跟踪调优方法，专门设计用于处理热图像中复杂的运动模式。所提出的框架优化了两个阶段，确保每个阶段都使用最合适的超参数进行调优，以最大化跟踪性能。通过对实时跟踪的超参数进行微调，该方法在不依赖复杂重识别或运动模型的情况下实现了高精度。在PBVS热MOT数据集上进行的广泛实验表明，该方法在各种热相机条件下都非常有效，使其成为真实世界监控应用的鲁棒解决方案。", "summary": "本文提出了一种新颖的两阶段超参数调优方法，用于在热图像中实现高精度的实时多目标（行人）跟踪。该方法旨在克服热传感器低级特征表示的挑战和复杂运动模式，通过优化超参数来最大化跟踪性能，且无需复杂的重识别或运动模型。在PBVS热MOT数据集上的实验证明了其在各种热相机条件下的有效性和鲁棒性，使其成为恶劣环境下监控的实用解决方案。", "keywords": "多目标跟踪, 热传感器, 超参数调优, 行人跟踪, 实时跟踪", "comments": "该论文的创新点在于提出了一种无需复杂重识别或运动模型的两阶段超参数调优方法，以解决热图像中行人跟踪的低级特征表示和复杂运动模式问题。其重要性在于为恶劣环境下的实时监控提供了鲁棒且高精度的解决方案。"}}
{"id": "2507.02529", "title": "RetrySQL: text-to-SQL training with retry data for self-correcting query generation", "authors": ["Alicja Rączkowska", "Riccardo Belluzzo", "Piotr Zieliński", "Joanna Baran", "Paweł Olszewski"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02529v1", "summary": "The text-to-SQL task is an active challenge in Natural Language Processing.\nMany existing solutions focus on using black-box language models extended with\nspecialized components within customized end-to-end text-to-SQL pipelines.\nWhile these solutions use both closed-source proprietary language models and\ncoding-oriented open-source models, there is a lack of research regarding\nSQL-specific generative models. At the same time, recent advancements in\nself-correcting generation strategies show promise for improving the\ncapabilities of existing architectures. The application of these concepts to\nthe text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL,\na new approach to training text-to-SQL generation models. We prepare reasoning\nsteps for reference SQL queries and then corrupt them to create retry data that\ncontains both incorrect and corrected steps, divided with a special token. We\ncontinuously pre-train an open-source coding model with this data and\ndemonstrate that retry steps yield an improvement of up to 4 percentage points\nin both overall and challenging execution accuracy metrics, compared to\npre-training without retry data. Additionally, we confirm that supervised\nfine-tuning with LoRA is ineffective for learning from retry data and that\nfull-parameter pre-training is a necessary requirement for that task. We\nshowcase that the self-correcting behavior is learned by the model and the\nincrease in downstream accuracy metrics is a result of this additional skill.\nFinally, we incorporate RetrySQL-trained models into the full text-to-SQL\npipeline and showcase that they are competitive in terms of execution accuracy\nwith proprietary models that contain orders of magnitude more parameters.\nRetrySQL demonstrates that self-correction can be learned in the text-to-SQL\ntask and provides a novel way of improving generation accuracy for SQL-oriented\nlanguage models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02529v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "RetrySQL：使用重试数据进行文本到SQL训练以实现自校正查询生成", "tldr": "RetrySQL提出了一种新的文本到SQL训练方法，通过使用包含错误和纠正步骤的“重试数据”对开源编码模型进行持续预训练，显著提高了查询生成精度，并证明了模型可以学习自校正行为。", "motivation": "现有的文本到SQL解决方案多专注于黑盒语言模型或通用开源模型，缺乏针对SQL的特定生成模型研究。同时，自校正生成策略在文本到SQL任务中的应用尚未被探索。", "method": "本文引入了RetrySQL，一种新的文本到SQL模型训练方法。该方法通过准备参考SQL查询的推理步骤，然后对其进行破坏以创建包含错误和纠正步骤的“重试数据”。研究人员使用这种数据持续预训练一个开源编码模型，并发现全参数预训练是学习重试数据的必要条件，而LoRA微调无效。", "result": "与没有使用重试数据进行预训练的模型相比，使用重试步骤的模型在整体和挑战性执行准确性指标上均提高了高达4个百分点。模型学习了自校正行为，并且下游准确性指标的提高是这种额外技能的结果。RetrySQL训练的模型在执行准确性方面与参数量大得多的专有模型具有竞争力。", "conclusion": "RetrySQL证明了自校正能力可以在文本到SQL任务中学习，并为提高面向SQL的语言模型的生成准确性提供了一种新颖的方法。", "translation": "文本到SQL任务是自然语言处理领域的一个活跃挑战。许多现有解决方案专注于使用黑盒语言模型，并在定制的端到端文本到SQL管道中扩展专用组件。虽然这些解决方案既使用闭源专有语言模型，也使用面向编码的开源模型，但缺乏关于SQL特定生成模型的研究。与此同时，自校正生成策略的最新进展显示出改善现有架构能力的希望。这些概念在文本到SQL任务中的应用仍未被探索。在本文中，我们引入了RetrySQL，一种训练文本到SQL生成模型的新方法。我们为参考SQL查询准备推理步骤，然后对其进行破坏以创建包含错误和纠正步骤的重试数据，这些步骤用一个特殊标记分隔。我们使用这些数据持续预训练一个开源编码模型，并证明与不使用重试数据进行预训练相比，重试步骤在整体和挑战性执行准确性指标上均提高了高达4个百分点。此外，我们确认使用LoRA进行监督微调对从重试数据中学习是无效的，并且全参数预训练是该任务的必要条件。我们展示了模型学习了自校正行为，并且下游准确性指标的增加是这种额外技能的结果。最后，我们将RetrySQL训练的模型整合到完整的文本到SQL管道中，并展示它们在执行准确性方面与参数量大得多的专有模型具有竞争力。RetrySQL表明自校正可以在文本到SQL任务中学习，并提供了一种提高面向SQL的语言模型生成准确性的新颖方法。", "summary": "RetrySQL提出了一种创新的文本到SQL训练方法，通过生成包含错误和纠正步骤的“重试数据”并用于持续预训练开源编码模型。该方法显著提升了模型在文本到SQL任务中的自校正能力和查询生成准确性，最高可达4个百分点。研究还发现，全参数预训练对于学习重试数据至关重要。RetrySQL训练的模型在准确性上可与大型专有模型媲美，为SQL特定语言模型的改进提供了一条新途径。", "keywords": "文本到SQL, 自校正, 重试数据, 查询生成, 语言模型", "comments": "RetrySQL的创新之处在于引入了“重试数据”的概念，通过模拟错误和纠正过程来训练模型学习自校正行为，这在文本到SQL领域是一个新颖且有前景的方向。其通过预训练提升模型能力，并证明了全参数预训练的重要性，对未来SQL生成模型的研究具有指导意义。该方法通过提高自校正能力，使得模型在处理复杂查询时表现出更强的鲁棒性。"}}
{"id": "2507.02550", "title": "Position: A Theory of Deep Learning Must Include Compositional Sparsity", "authors": ["David A. Danhofer", "Davide D'Ascenzo", "Rafael Dubach", "Tomaso Poggio"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02550v1", "summary": "Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable\nsuccess in a wide variety of domains too high-dimensional for classical shallow\nnetworks subject to the curse of dimensionality. However, open questions about\nfundamental principles, that govern the learning dynamics of DNNs, remain. In\nthis position paper we argue that it is the ability of DNNs to exploit the\ncompositionally sparse structure of the target function driving their success.\nAs such, DNNs can leverage the property that most practically relevant\nfunctions can be composed from a small set of constituent functions, each of\nwhich relies only on a low-dimensional subset of all inputs. We show that this\nproperty is shared by all efficiently Turing-computable functions and is\ntherefore highly likely present in all current learning problems. While some\npromising theoretical insights on questions concerned with approximation and\ngeneralization exist in the setting of compositionally sparse functions,\nseveral important questions on the learnability and optimization of DNNs\nremain. Completing the picture of the role of compositional sparsity in deep\nlearning is essential to a comprehensive theory of artificial, and even\ngeneral, intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02550v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "立场：深度学习理论必须包含组合稀疏性", "tldr": "深度学习的成功源于其利用了目标函数的组合稀疏结构。", "motivation": "解释深度神经网络（DNNs）成功背后的基本原理，尤其是在高维数据上的成功，以及现有理论的不足。", "method": "本文是一篇立场论文，通过论证（argue）DNNs的成功在于其能够利用目标函数的组合稀疏结构。他们指出，所有高效图灵可计算函数都共享此属性。", "result": "展示了所有高效图灵可计算函数都具有组合稀疏性，因此这很可能存在于所有当前的学习问题中。", "conclusion": "完善组合稀疏性在深度学习中的作用对于构建全面的人工智能甚至通用智能理论至关重要。", "translation": "超参数化的深度神经网络（DNNs）在各种高维领域取得了显著成功，这些领域对于受维度诅咒影响的经典浅层网络来说过于复杂。然而，关于控制DNNs学习动力学的基本原理仍存在未解之谜。在这篇立场论文中，我们认为DNNs的成功在于其能够利用目标函数的组合稀疏结构。因此，DNNs可以利用这样一个特性：大多数实际相关的函数可以由一小组构成函数组成，每个构成函数仅依赖于所有输入的一个低维子集。我们表明，所有高效图灵可计算函数都共享此属性，因此这很可能存在于所有当前的学习问题中。尽管在组合稀疏函数背景下，关于近似和泛化问题的理论洞察已有一些有希望的进展，但关于DNNs可学习性和优化的几个重要问题仍未解决。完善组合稀疏性在深度学习中的作用对于构建全面的人工智能，甚至是通用智能理论至关重要。", "summary": "本文提出，深度神经网络（DNNs）之所以成功，是因为它们能够利用目标函数的组合稀疏结构。作者论证了大多数实际相关的函数都具有此特性，即它们可以由依赖于低维输入子集的少量构成函数组合而成，并且所有高效图灵可计算函数都共享这一属性。论文强调，理解组合稀疏性对于发展全面的人工智能理论至关重要，尽管在可学习性和优化方面仍有待解决的问题。", "keywords": "深度学习, 组合稀疏性, 神经网络, 理论, 维度诅咒", "comments": "这篇立场论文提出了一个关于深度学习成功机制的重要理论观点，即“组合稀疏性”。它为理解深度神经网络的泛化能力提供了一个新的视角，并指出了未来理论研究的方向，强调了其在构建通用人工智能理论中的潜在核心地位。"}}
{"id": "2507.02287", "title": "Seeing Through Green: Text-Based Classification and the Firm's Returns from Green Patents", "authors": ["Lapo Santarlasci", "Armando Rungi", "Antonio Zinilli"], "categories": ["econ.GN", "cs.CL", "q-fin.EC"], "primary_category": "Subjects:       General Economics (econ.GN)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02287v1", "summary": "This paper introduces Natural Language Processing for identifying ``true''\ngreen patents from official supporting documents. We start our training on\nabout 12.4 million patents that had been classified as green from previous\nliterature. Thus, we train a simple neural network to enlarge a baseline\ndictionary through vector representations of expressions related to\nenvironmental technologies. After testing, we find that ``true'' green patents\nrepresent about 20\\% of the total of patents classified as green from previous\nliterature. We show heterogeneity by technological classes, and then check that\n`true' green patents are about 1\\% less cited by following inventions. In the\nsecond part of the paper, we test the relationship between patenting and a\ndashboard of firm-level financial accounts in the European Union. After\ncontrolling for reverse causality, we show that holding at least one ``true''\ngreen patent raises sales, market shares, and productivity. If we restrict the\nanalysis to high-novelty ``true'' green patents, we find that they also yield\nhigher profits. Our findings underscore the importance of using text analyses\nto gauge finer-grained patent classifications that are useful for policymaking\nin different domains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02287v1", "cate": "econ.GN", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "看透绿色：基于文本的分类与企业从绿色专利中获得的收益", "tldr": "本研究利用自然语言处理（NLP）识别“真正的”绿色专利，发现它们仅占先前分类绿色专利的一小部分，但与企业销售额、市场份额、生产力提升以及高新颖性专利的更高利润相关。", "motivation": "本研究旨在利用自然语言处理从官方文件中识别“真正的”绿色专利，因为先前的分类可能不够精确。此外，论文还探讨了企业持有这些“真正的”绿色专利所带来的财务回报。", "method": "本研究首先收集了约1240万份先前被分类为绿色专利的数据。接着，训练了一个简单的神经网络，通过环境技术相关表达的向量表示来扩充一个基线词典，从而识别出“真正的”绿色专利。随后，分析了这些专利在不同技术类别中的异质性及其被引用情况。最后，在控制反向因果关系的前提下，检验了持有“真正的”绿色专利与欧盟企业层面财务指标（如销售额、市场份额、生产力、利润）之间的关系。", "result": "“真正的”绿色专利约占先前文献中分类为绿色专利总数的20%。不同技术类别之间存在异质性，且“真正的”绿色专利被后续发明引用的次数约少1%。持有至少一项“真正的”绿色专利能提高企业的销售额、市场份额和生产力。高新颖性的“真正的”绿色专利还能带来更高的利润。", "conclusion": "研究结果强调了使用文本分析进行更细粒度的专利分类的重要性，这对于不同领域的政策制定具有重要意义。", "translation": "这篇论文引入了自然语言处理技术，用于从官方支持文件中识别“真正的”绿色专利。我们从先前文献中已分类为绿色的约1240万份专利开始训练。因此，我们训练了一个简单的神经网络，通过环境技术相关表达的向量表示来扩充一个基线词典。测试后，我们发现“真正的”绿色专利约占先前文献中分类为绿色专利总数的20%。我们展示了技术类别的异质性，然后检查发现“真正的”绿色专利被后续发明引用的次数约少1%。在论文的第二部分，我们测试了专利持有与欧盟企业层面财务指标（销售额、市场份额和生产力）之间的关系。在控制了反向因果关系后，我们表明持有一项“真正的”绿色专利可以提高销售额、市场份额和生产力。如果我们将分析限制在高新颖性的“真正的”绿色专利，我们发现它们还能带来更高的利润。我们的研究结果强调了使用文本分析来衡量更细粒度的专利分类的重要性，这对于不同领域的政策制定非常有用。", "summary": "本论文运用自然语言处理（NLP）技术，从官方文件中精确识别“真正的”绿色专利。研究首先利用约1240万份先前分类的绿色专利进行训练，通过神经网络扩充环境技术相关词汇的基线词典。结果显示，“真正的”绿色专利仅占现有绿色专利的约20%，并表现出技术类别上的异质性及略低的引用率。进一步分析表明，持有“真正的”绿色专利可显著提升企业的销售额、市场份额和生产力，而高新颖性的“真正的”绿色专利还能带来更高的利润。该研究强调了文本分析在实现更精细专利分类方面的重要性，为政策制定提供了有价值的见解。", "keywords": "绿色专利, 自然语言处理, 专利分类, 企业绩效, 环境技术", "comments": "本论文通过应用NLP技术对绿色专利进行精细分类，提供了一种创新方法，超越了以往的宽泛分类。其重要性在于揭示了更精确识别“真正的”绿色专利后，它们对企业产生的实际经济影响。研究发现仅有小部分现有“绿色”专利是真正的绿色专利，且这些专利与企业绩效呈正相关，这一发现具有重要意义。该工作为研究人员、政策制定者和关注环境创新及其财务回报的企业提供了宝贵的工具和见解。潜在的局限性可能在于其神经网络模型在其他专利数据集或语言上的泛化能力，可能需要重新训练。"}}
{"id": "2507.01990", "title": "Integrating Large Language Models in Financial Investments and Market Analysis: A Survey", "authors": ["Sedigheh Mahdavi", "Jiating", "Chen", "Pradeep Kumar Joshi", "Lina Huertas Guativa", "Upmanyu Singh"], "categories": ["q-fin.GN", "cs.AI", "cs.LG"], "primary_category": "Subjects:       General Finance (q-fin.GN)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01990v1", "summary": "Large Language Models (LLMs) have been employed in financial decision making,\nenhancing analytical capabilities for investment strategies. Traditional\ninvestment strategies often utilize quantitative models, fundamental analysis,\nand technical indicators. However, LLMs have introduced new capabilities to\nprocess and analyze large volumes of structured and unstructured data, extract\nmeaningful insights, and enhance decision-making in real-time. This survey\nprovides a structured overview of recent research on LLMs within the financial\ndomain, categorizing research contributions into four main frameworks:\nLLM-based Frameworks and Pipelines, Hybrid Integration Methods, Fine-Tuning and\nAdaptation Approaches, and Agent-Based Architectures. This study provides a\nstructured review of recent LLMs research on applications in stock selection,\nrisk assessment, sentiment analysis, trading, and financial forecasting. By\nreviewing the existing literature, this study highlights the capabilities,\nchallenges, and potential directions of LLMs in financial markets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01990v1", "cate": "q-fin.GN", "date": "2025-06-29", "updated": "2025-06-29", "AI": {"title_translation": "大型语言模型在金融投资和市场分析中的应用：一项综述", "tldr": "本综述探讨了大型语言模型（LLM）在金融投资和市场分析中的应用，概述了其能力、挑战和未来方向。", "motivation": "传统的金融投资策略在处理海量结构化和非结构化数据方面存在局限性。大型语言模型（LLM）的出现提供了新的能力，能够处理和分析大量数据，提取有意义的洞察力，并实时增强决策，从而提升金融决策和投资策略的分析能力。", "method": "本研究是一项综述，对金融领域内LLM的最新研究进行了结构化概述，将研究贡献分为四个主要框架：基于LLM的框架和管道、混合集成方法、微调和适应方法，以及基于代理的架构。同时，它还回顾了LLM在股票选择、风险评估、情感分析、交易和金融预测中的应用。", "result": "本研究回顾了现有文献，突出了大型语言模型在金融市场中的能力、挑战和潜在方向。它将LLM在金融领域的应用研究分为四大框架，并具体审视了其在股票选择、风险评估、情感分析、交易和金融预测等方面的应用。", "conclusion": "大型语言模型在金融市场中展现出显著的能力，但也面临挑战。本综述通过梳理现有研究，为理解LLM在该领域的当前状况提供了结构化视角，并指明了未来的发展方向。", "translation": "大型语言模型（LLM）已被应用于金融决策，增强了投资策略的分析能力。传统的投资策略通常利用定量模型、基本面分析和技术指标。然而，LLM引入了处理和分析大量结构化和非结构化数据、提取有意义的洞察力以及实时增强决策的新能力。本综述对金融领域内LLM的最新研究进行了结构化概述，将研究贡献分为四个主要框架：基于LLM的框架和管道、混合集成方法、微调和适应方法以及基于代理的架构。本研究对LLM在股票选择、风险评估、情感分析、交易和金融预测中的应用进行了结构化回顾。通过回顾现有文献，本研究强调了LLM在金融市场中的能力、挑战和潜在方向。", "summary": "本综述深入探讨了大型语言模型（LLM）在金融投资和市场分析领域的整合应用。文章阐述了LLM如何通过处理海量结构化与非结构化数据、提取关键洞察并支持实时决策，从而增强传统金融策略。该研究将LLM在金融领域的应用归纳为四大核心框架，并详细审查了其在股票选择、风险评估、情感分析、交易及金融预测等方面的具体应用。最终，本综述强调了LLM在金融市场中的现有能力、面临的挑战以及未来的发展潜力。", "keywords": "大型语言模型, 金融投资, 市场分析, 综述, 金融预测", "comments": "本论文通过系统性地梳理和分类金融领域中大型语言模型的研究现状，为该新兴交叉领域提供了重要的结构化视图。其创新之处在于对现有研究进行了框架性的归纳，这对于理解LLM在金融应用的广度与深度、识别关键技术路径以及未来研究方向具有重要指导意义。该综述填补了当前文献中对LLM在金融领域应用全面总结的空白，对于研究人员和业界实践者都具有较高的参考价值。"}}
{"id": "2507.02559", "title": "Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability", "authors": ["Luca Baroni", "Galvin Khara", "Joachim Schaeffer", "Marat Subkhankulov", "Stefan Heimersheim"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02559v1", "summary": "Layer-wise normalization (LN) is an essential component of virtually all\ntransformer-based large language models. While its effects on training\nstability are well documented, its role at inference time is poorly understood.\nAdditionally, LN layers hinder mechanistic interpretability by introducing\nadditional nonlinearities and increasing the interconnectedness of individual\nmodel components. Here, we show that all LN layers can be removed from every\nGPT-2 model with only a small increase in validation loss (e.g. +0.03\ncross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in\nlanguage modeling. We find that the amount of fine-tuning data needed for LN\nremoval grows sublinearly with model parameters, suggesting scaling to larger\nmodels is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.\nFurthermore, we test interpretability techniques on LN-free models. Direct\nlogit attribution now gives the exact direct effect of individual components,\nwhile the accuracy of attribution patching does not significantly improve. We\nalso confirm that GPT-2's \"confidence neurons\" are inactive in the LN-free\nmodels. Our work clarifies the role of LN layers in language modeling, showing\nthat GPT-2-class models can function without LN layers. We hope that our\nLN-free analogs of the GPT-2 family of models will enable more precise\ninterpretability research and improve our understanding of language models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02559v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Transformer在推理时不需要LayerNorm：将LayerNorm移除扩展到GPT-2 XL及其对机制可解释性的影响", "tldr": "研究表明，可以从GPT-2模型中移除LayerNorm层，仅导致验证损失的微小增加，同时提高机制可解释性。", "motivation": "层归一化（LN）是Transformer模型的重要组成部分，但其在推理时的作用尚不明确。此外，LN层通过引入非线性和增加组件互连性，阻碍了模型的机制可解释性。", "method": "研究者从GPT-2模型中移除了所有LayerNorm层，并测试了其对验证损失的影响。他们还探究了LN移除所需的微调数据量如何随模型参数扩展，并在无LN模型上测试了可解释性技术，如直接logit归因和归因修补，并检查了“置信神经元”的活跃性。", "result": "所有LN层都可以从GPT-2模型中移除，仅导致验证损失的微小增加（例如，GPT-2 XL的交叉熵损失增加+0.03）。这表明LN在语言建模中不发挥实质性作用。移除LN所需的微调数据量与模型参数呈亚线性增长，预示着可扩展到更大的模型。在无LN模型上，直接logit归因能提供单个组件的精确直接效应，但归因修补的准确性没有显著提高。此外，GPT-2的“置信神经元”在无LN模型中不活跃。研究者还在Hugging Face上发布了一套无LN的GPT-2模型。", "conclusion": "本研究阐明了LN层在语言建模中的作用，表明GPT-2类模型可以在没有LN层的情况下运行。无LN的GPT-2模型有望实现更精确的可解释性研究，并增进对语言模型的理解。", "translation": "层归一化（LN）是几乎所有基于Transformer的大型语言模型的重要组成部分。尽管其对训练稳定性的影响已有充分记录，但其在推理时的作用却鲜为人知。此外，LN层通过引入额外的非线性和增加单个模型组件的相互连接性，阻碍了机制可解释性。在此，我们表明所有LN层都可以从每个GPT-2模型中移除，而验证损失仅略有增加（例如，GPT-2 XL的交叉熵损失增加+0.03）。因此，LN在语言建模中不能发挥实质性作用。我们发现，LN移除所需的微调数据量与模型参数呈亚线性增长，这表明扩展到更大的模型是可行的。我们在Hugging Face上发布了一套无LN的GPT-2模型。此外，我们还在无LN模型上测试了可解释性技术。直接logit归因现在可以给出单个组件的精确直接效应，而归因修补的准确性没有显著提高。我们还证实了GPT-2的“置信神经元”在无LN模型中不活跃。我们的工作阐明了LN层在语言建模中的作用，表明GPT-2类模型可以在没有LN层的情况下运行。我们希望我们针对GPT-2系列模型的无LN模拟将实现更精确的可解释性研究，并提高我们对语言模型的理解。", "summary": "该论文揭示了Transformer模型中的LayerNorm层在推理时并非必需，可以从GPT-2模型中移除，仅导致性能的微小下降。这一发现挑战了LayerNorm在语言建模中的关键作用认知，并显著简化了模型结构，从而增强了机制可解释性。通过移除LayerNorm，研究者能够更直接地分析模型组件的影响。文章还指出该方法可扩展至更大模型，并已发布了无LayerNorm的GPT-2模型。", "keywords": "LayerNorm, Transformer, 机制可解释性, GPT-2, 语言模型", "comments": "这项工作的创新之处在于挑战了大型语言模型在推理时对LayerNorm的固有依赖，尤其是在可解释性方面。其重要性在于它不仅简化了模型架构，还为机制可解释性研究开辟了新途径，这对于理解复杂的深度学习模型至关重要。研究者发布无LayerNorm模型的举动，对研究社区来说是一项宝贵的贡献。"}}
{"id": "2507.02419", "title": "AvatarMakeup: Realistic Makeup Transfer for 3D Animatable Head Avatars", "authors": ["Yiming Zhong", "Xiaolin Zhang", "Ligang Liu", "Yao Zhao", "Yunchao Wei"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02419v1", "summary": "Similar to facial beautification in real life, 3D virtual avatars require\npersonalized customization to enhance their visual appeal, yet this area\nremains insufficiently explored. Although current 3D Gaussian editing methods\ncan be adapted for facial makeup purposes, these methods fail to meet the\nfundamental requirements for achieving realistic makeup effects: 1) ensuring a\nconsistent appearance during drivable expressions, 2) preserving the identity\nthroughout the makeup process, and 3) enabling precise control over fine\ndetails. To address these, we propose a specialized 3D makeup method named\nAvatarMakeup, leveraging a pretrained diffusion model to transfer makeup\npatterns from a single reference photo of any individual. We adopt a\ncoarse-to-fine idea to first maintain the consistent appearance and identity,\nand then to refine the details. In particular, the diffusion model is employed\nto generate makeup images as supervision. Due to the uncertainties in diffusion\nprocess, the generated images are inconsistent across different viewpoints and\nexpressions. Therefore, we propose a Coherent Duplication method to coarsely\napply makeup to the target while ensuring consistency across dynamic and\nmultiview effects. Coherent Duplication optimizes a global UV map by recoding\nthe averaged facial attributes among the generated makeup images. By querying\nthe global UV map, it easily synthesizes coherent makeup guidance from\narbitrary views and expressions to optimize the target avatar. Given the coarse\nmakeup avatar, we further enhance the makeup by incorporating a Refinement\nModule into the diffusion model to achieve high makeup quality. Experiments\ndemonstrate that AvatarMakeup achieves state-of-the-art makeup transfer quality\nand consistency throughout animation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02419v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "AvatarMakeup：可动画3D头部化身的真实感妆容迁移", "tldr": "提出AvatarMakeup，一种利用扩散模型将真实妆容从单张照片迁移到3D动画化身的新方法，解决了传统方法在表情一致性、身份保持和细节控制方面的不足。", "motivation": "3D虚拟化身需要个性化定制以增强视觉吸引力，但现有方法在实现真实感妆容效果时存在不足，具体表现在：1) 驱动表情时外观不一致，2) 妆容过程身份无法保持，3) 难以精确控制细节。因此，需要一种专门的3D妆容方法来解决这些问题。", "method": "提出AvatarMakeup方法，利用预训练扩散模型从单张参考照片迁移妆容。该方法采用粗到精的思路：首先通过“Coherent Duplication”方法保持外观和身份的一致性，该方法通过记录生成妆容图像的平均面部属性来优化全局UV贴图，从而从任意视角和表情合成一致的妆容指导。其次，通过在扩散模型中加入“Refinement Module”来增强妆容细节，实现高质量的妆容效果。", "result": "AvatarMakeup在妆容迁移质量和动画过程中的一致性方面达到了最先进水平。", "conclusion": "AvatarMakeup成功解决了3D动画化身妆容迁移中外观一致性、身份保持和细节控制的挑战，实现了真实感和高质量的妆容效果，并达到了SOTA水平。", "translation": "类似于现实生活中的面部美化，3D虚拟化身需要个性化定制以增强其视觉吸引力，但这一领域的研究仍然不足。尽管当前的3D高斯编辑方法可以用于面部妆容目的，但这些方法未能满足实现真实感妆容效果的基本要求：1) 在可驱动表情期间保持一致的外观，2) 在整个妆容过程中保留身份，以及3) 能够精确控制精细细节。为了解决这些问题，我们提出了一种名为AvatarMakeup的专用3D妆容方法，该方法利用预训练的扩散模型将妆容样式从任何个体的单张参考照片中迁移过来。我们采用了一种从粗到精的思路，首先保持一致的外观和身份，然后细化细节。特别是，扩散模型被用来生成妆容图像作为监督。由于扩散过程中的不确定性，生成的图像在不同视角和表情下是不一致的。因此，我们提出了一种“Coherent Duplication”方法，以粗略地将妆容应用到目标上，同时确保在动态和多视角效果下的一致性。“Coherent Duplication”通过重新编码生成妆容图像中的平均面部属性来优化全局UV贴图。通过查询全局UV贴图，它可以轻松地从任意视角和表情合成一致的妆容指导，以优化目标化身。给定粗略妆容的化身，我们通过在扩散模型中加入一个“Refinement Module”来进一步增强妆容，以实现高质量的妆容效果。实验表明，AvatarMakeup在动画过程中实现了最先进的妆容迁移质量和一致性。", "summary": "本文提出了一种名为AvatarMakeup的3D妆容迁移方法，旨在解决现有方法在3D动画化身妆容中面临的外观一致性、身份保持和细节控制不足的问题。该方法利用预训练扩散模型，并采用粗到精的策略，通过“Coherent Duplication”确保动态和多视角下的一致性，并通过“Refinement Module”提升妆容细节。实验证明，AvatarMakeup在妆容迁移质量和动画一致性方面达到了最先进水平。", "keywords": "3D化身, 妆容迁移, 扩散模型, 外观一致性, 身份保持", "comments": "这篇论文的创新点在于提出了AvatarMakeup方法，专门解决了3D动画化身妆容迁移中的一致性、身份保持和细节控制等核心挑战。通过结合预训练扩散模型、粗到精的策略以及独特的Coherent Duplication和Refinement Module，有效提升了3D妆容的真实感和动画表现力，具有重要的实际应用价值。"}}
{"id": "2507.02585", "title": "Scalable Interconnect Learning in Boolean Networks", "authors": ["Fabian Kresse", "Emily Yu", "Christoph H. Lampert"], "categories": ["cs.LG", "cs.LO"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      12 pages, 8 Figures", "url": "http://arxiv.org/abs/2507.02585v1", "summary": "Learned Differentiable Boolean Logic Networks (DBNs) already deliver\nefficient inference on resource-constrained hardware. We extend them with a\ntrainable, differentiable interconnect whose parameter count remains constant\nas input width grows, allowing DBNs to scale to far wider layers than earlier\nlearnable-interconnect designs while preserving their advantageous accuracy. To\nfurther reduce model size, we propose two complementary pruning stages: an\nSAT-based logic equivalence pass that removes redundant gates without affecting\nperformance, and a similarity-based, data-driven pass that outperforms a\nmagnitude-style greedy baseline and offers a superior compression-accuracy\ntrade-off.", "comment": "12 pages, 8 Figures", "pdf_url": "http://arxiv.org/pdf/2507.02585v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "布尔网络中可伸缩的互连学习", "tldr": "本文通过引入可训练的、可微分的互连层以及两种互补的剪枝阶段，使可微分布尔逻辑网络（DBNs）能够扩展到更宽的层，同时减小模型尺寸并保持精度。", "motivation": "现有的可微分布尔逻辑网络（DBNs）虽然在资源受限硬件上能高效推理，但其可学习互连设计在输入宽度增加时参数量也会增加，限制了其扩展性。此外，还需要进一步减小模型尺寸。", "method": "本文通过以下方法改进DBNs：1. 引入了一种可训练的、可微分的互连层，其参数数量不随输入宽度增长而变化，从而实现可伸缩性。2. 提出了两种互补的剪枝阶段：a) 基于SAT的逻辑等价剪枝，用于移除冗余门而不影响性能。b) 基于相似性的数据驱动剪枝，旨在实现更好的压缩-精度权衡。", "result": "所提出的方法使得DBNs能够扩展到比早期可学习互连设计更宽的层，同时保持其有利的精度。此外，相似性剪枝方法优于基于幅度的贪婪基线，并提供了更优越的压缩-精度权衡，有效减小了模型尺寸。", "conclusion": "通过引入可伸缩的互连和有效的剪枝策略，本文成功地使可微分布尔逻辑网络在保持高精度的同时，能够适应更宽的输入层并显著减小模型尺寸，使其更适用于资源受限的硬件。", "translation": "学习到的可微分布尔逻辑网络（DBNs）已经在资源受限的硬件上实现了高效推理。我们通过引入一个可训练的、可微分的互连层来扩展它们，该互连层的参数数量在输入宽度增长时保持不变，从而使DBNs能够扩展到比早期可学习互连设计更宽的层，同时保持其有利的精度。为了进一步减小模型尺寸，我们提出了两个互补的剪枝阶段：一个基于SAT的逻辑等价通道，用于移除冗余门而不影响性能；以及一个基于相似性的数据驱动通道，它优于基于幅度的贪婪基线，并提供了更优越的压缩-精度权衡。", "summary": "本文提出通过引入一种参数量不随输入宽度变化的、可训练的可微分互连层，解决了可微分布尔逻辑网络（DBNs）的扩展性问题，使其能够处理更宽的层同时保持高精度。此外，为进一步减小模型尺寸，论文还提出了两种剪枝策略：一种是基于SAT的逻辑等价剪枝，用于去除冗余门；另一种是基于相似性的数据驱动剪枝，该方法在压缩-精度权衡上表现优于现有基线。", "keywords": "布尔网络, 可微分逻辑, 可伸缩性, 剪枝, 互连学习", "comments": "本文的创新点在于其提出的可伸缩互连设计，有效解决了DBNs在处理宽层时的扩展性瓶颈，这对于在资源受限硬件上部署高效推理模型至关重要。同时，两种互补的剪枝策略，特别是基于相似性的数据驱动剪枝，为模型尺寸优化提供了新的思路和更好的性能。"}}
{"id": "2507.02608", "title": "Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation", "authors": ["François Rozet", "Ruben Ohana", "Michael McCabe", "Gilles Louppe", "François Lanusse", "Shirley Ho"], "categories": ["cs.LG", "physics.flu-dyn"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02608v1", "summary": "The steep computational cost of diffusion models at inference hinders their\nuse as fast physics emulators. In the context of image and video generation,\nthis computational drawback has been addressed by generating in the latent\nspace of an autoencoder instead of the pixel space. In this work, we\ninvestigate whether a similar strategy can be effectively applied to the\nemulation of dynamical systems and at what cost. We find that the accuracy of\nlatent-space emulation is surprisingly robust to a wide range of compression\nrates (up to 1000x). We also show that diffusion-based emulators are\nconsistently more accurate than non-generative counterparts and compensate for\nuncertainty in their predictions with greater diversity. Finally, we cover\npractical design choices, spanning from architectures to optimizers, that we\nfound critical to train latent-space emulators.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02608v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "迷失在潜在空间：潜在扩散模型在物理模拟中的实证研究", "tldr": "潜在扩散模型能高效准确地进行物理模拟，即使在高压缩率下也表现良好。", "motivation": "扩散模型在推理时计算成本高昂，阻碍了其作为快速物理模拟器的应用。", "method": "本文研究了将自编码器潜在空间生成策略应用于动力学系统模拟的有效性，并评估了其成本效益。", "result": "研究发现，潜在空间模拟的精度对大范围压缩率（高达1000倍）表现出惊人的鲁棒性；基于扩散的模拟器始终比非生成式模拟器更准确，并通过更大的多样性来弥补预测中的不确定性；还涵盖了训练潜在空间模拟器的关键实际设计选择。", "conclusion": "潜在扩散模型能够有效且高效地应用于物理模拟，克服了传统扩散模型的计算限制，并展现出优越的准确性和多样性。", "translation": "扩散模型在推理时高昂的计算成本阻碍了它们作为快速物理模拟器的应用。在图像和视频生成领域，这一计算劣势已通过在自编码器的潜在空间而非像素空间进行生成来解决。在这项工作中，我们研究了类似策略是否能有效应用于动力学系统的模拟以及其成本。我们发现，潜在空间模拟的精度对大范围的压缩率（高达1000倍）表现出惊人的鲁棒性。我们还表明，基于扩散的模拟器始终比非生成式对应物更准确，并通过更大的多样性来弥补其预测中的不确定性。最后，我们涵盖了从架构到优化器的实际设计选择，这些是我们发现对训练潜在空间模拟器至关重要的。", "summary": "针对扩散模型在物理模拟中推理成本高昂的问题，本文研究了将潜在空间生成策略应用于动力学系统模拟的有效性。研究发现，潜在空间模拟的精度在高达1000倍的压缩率下仍保持鲁棒性，并且基于扩散的模拟器比非生成式模拟器更准确，能通过更大的多样性弥补预测不确定性。文章还讨论了训练潜在空间模拟器的关键设计选择。", "keywords": "潜在扩散模型, 物理模拟, 计算成本, 动力学系统, 压缩率", "comments": "本文创新性地将潜在扩散模型应用于物理模拟领域，有效解决了传统扩散模型计算成本高昂的瓶颈。研究结果显示，该方法在保持高精度和生成多样性的同时，能够实现极高的压缩率，为快速物理模拟提供了有前景的解决方案，具有重要的实际应用价值。"}}
{"id": "2507.02659", "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding", "authors": ["Ramchalam Kinattinkara Ramakrishnan", "Zhaocong Yuan", "Shaojie Zhuo", "Chen Feng", "Yicheng Lin", "Chenzheng Su", "Xiaopeng Zhang"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02659v1", "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02659v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "OmniDraft: 跨词汇、在线自适应的设备端推测解码草稿器", "tldr": "OmniDraft是一个统一框架，使单个草稿模型能够与任何目标模型配合使用，并动态适应用户数据，适用于设备端推测解码，提供显著加速。", "motivation": "解决在线部署推测解码时，草稿模型与目标模型不兼容以及期望延迟随使用时间改善的挑战。目标是实现“一个草稿器适用于所有”的范式，特别是在设备端LLM应用中，模型成本、效率和用户定制是主要问题。", "method": "提出OmniDraft框架。采用在线n-gram缓存与混合蒸馏微调相结合，解决跨词汇不匹配问题；通过自适应草稿技术进一步提高解码速度。", "result": "OmniDraft使单个草稿模型（例如Llama-68M）能与多种目标模型（包括Vicuna-7B, Qwen2-7B, Llama3-8B）进行推测解码。在数学推理、编码和文本生成任务上展示了在线学习能力，并提供了高达1.5-2倍的速度提升。", "conclusion": "OmniDraft通过解决跨词汇不匹配和动态适应性问题，实现了“一个草稿器适用于所有”的范式，显著提升了设备端LLM推测解码的效率和灵活性。", "translation": "推测解码通常要求有一个小型、高效的草稿模型，该模型要么预训练，要么离线蒸馏到特定的目标模型系列，例如Llama或Qwen模型。然而，在在线部署设置中，存在两大挑战：1）使用与草稿模型不兼容的目标模型；2）期望随着使用时间和使用量的增加而提高延迟。在这项工作中，我们提出了OmniDraft，一个统一的框架，它使单个草稿模型能够与任何目标模型配合使用，并动态适应用户数据。我们引入了一种在线n-gram缓存与混合蒸馏微调相结合的方法，以解决草稿模型和目标模型之间的跨词汇不匹配问题；并通过利用自适应草稿技术进一步提高了解码速度。OmniDraft特别适用于设备端LLM应用，其中模型成本、效率和用户定制是主要争议点。这进一步突出了解决上述挑战的需求，并促成了“一个草稿器适用于所有”的范式。我们通过在数学推理、编码和文本生成任务上进行在线学习，展示了OmniDraft框架的熟练度。值得注意的是，OmniDraft使单个Llama-68M模型能够与各种目标模型（包括Vicuna-7B、Qwen2-7B和Llama3-8B模型）配对进行推测解码；此外，还提供了高达1.5-2倍的速度提升。", "summary": "OmniDraft是一个创新的设备端推测解码框架，旨在克服传统草稿模型与目标模型不兼容及在线适应性不足的挑战。它通过结合在线n-gram缓存、混合蒸馏微调和自适应草稿技术，使单个草稿模型能够与任意目标模型协同工作，并动态适应用户数据。实验证明，OmniDraft能与多种大型语言模型兼容，并在多项任务上实现1.5-2倍的速度提升，特别适用于对成本、效率和定制化有高要求的设备端LLM应用。", "keywords": "推测解码, 设备端LLM, 在线适应, 跨词汇, 草稿模型", "comments": "OmniDraft的创新点在于其“一个草稿器适用于所有”的范式，通过在线适应和跨词汇兼容性解决了设备端推测解码的关键挑战。这对于资源受限的设备端LLM部署具有重要意义，因为它减少了对多个专门草稿模型的依赖，并提供了运行时性能优化。"}}
{"id": "2507.02619", "title": "L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation", "authors": ["Hazal Mogultay Ozcan", "Sinan Kalkan", "Fatos T. Yarman-Vural"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The paper is under revision at Machine Vision and Applications", "url": "http://arxiv.org/abs/2507.02619v1", "summary": "In this paper, we propose a novel model called Learnable VAE (L-VAE), which\nlearns a disentangled representation together with the hyperparameters of the\ncost function. L-VAE can be considered as an extension of \\b{eta}-VAE, wherein\nthe hyperparameter, \\b{eta}, is empirically adjusted. L-VAE mitigates the\nlimitations of \\b{eta}-VAE by learning the relative weights of the terms in the\nloss function to control the dynamic trade-off between disentanglement and\nreconstruction losses. In the proposed model, the weight of the loss terms and\nthe parameters of the model architecture are learned concurrently. An\nadditional regularization term is added to the loss function to prevent bias\ntowards either reconstruction or disentanglement losses. Experimental analyses\nshow that the proposed L-VAE finds an effective balance between reconstruction\nfidelity and disentangling the latent dimensions. Comparisons of the proposed\nL-VAE against \\b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\\sigma}-VAE on\ndatasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that\nL-VAE consistently provides the best or the second best performances measured\nby a set of disentanglement metrics. Moreover, qualitative experiments on\nCelebA dataset, confirm the success of the L-VAE model for disentangling the\nfacial attributes.", "comment": "The paper is under revision at Machine Vision and Applications", "pdf_url": "http://arxiv.org/pdf/2507.02619v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "L-VAE：具有可学习Beta的变分自编码器用于解耦表示", "tldr": "L-VAE是一种新型变分自编码器，通过学习损失函数中的超参数来动态平衡解耦和重建，从而在各种数据集上实现更好的解耦表示性能。", "motivation": "现有的\\b{eta}-VAE模型需要经验性地调整超参数\\b{eta}，这限制了其在解耦表示方面的性能。L-VAE旨在通过学习损失函数中项的相对权重来缓解这一局限性，以控制解耦和重建损失之间的动态权衡。", "method": "本文提出了一种名为可学习VAE（L-VAE）的新模型。L-VAE通过并发学习损失项的权重和模型架构的参数，来学习解耦表示以及成本函数的超参数。它通过学习损失函数中项的相对权重来控制解耦和重建损失之间的动态权衡。此外，L-VAE还在损失函数中添加了一个额外的正则化项，以防止偏向重建或解耦损失。", "result": "实验分析表明，所提出的L-VAE在重建保真度和解耦潜在维度之间找到了有效的平衡。与\\b{eta}-VAE、VAE、ControlVAE、DynamicVAE和{\\sigma}-VAE在dSprites、MPI3D-complex、Falcor3D和Isaac3D等数据集上的比较显示，L-VAE在解耦指标方面始终提供最佳或次佳性能。此外，在CelebA数据集上的定性实验证实了L-VAE模型在解耦面部属性方面的成功。", "conclusion": "L-VAE通过学习损失函数中的超参数，能够有效地平衡重建保真度和解耦表示，并在多个数据集上取得了优于现有模型的性能。", "translation": "在本文中，我们提出了一种名为可学习VAE（L-VAE）的新模型，该模型与成本函数的超参数一起学习解耦表示。L-VAE可以被视为\\b{eta}-VAE的扩展，其中超参数\\b{eta}是经验性调整的。L-VAE通过学习损失函数中项的相对权重来控制解耦和重建损失之间的动态权衡，从而缓解了\\b{eta}-VAE的局限性。在所提出的模型中，损失项的权重和模型架构的参数是同时学习的。损失函数中添加了一个额外的正则化项，以防止偏向重建或解耦损失。实验分析表明，所提出的L-VAE在重建保真度和解耦潜在维度之间找到了有效的平衡。将所提出的L-VAE与\\b{eta}-VAE、VAE、ControlVAE、DynamicVAE和{\\sigma}-VAE在dSprites、MPI3D-complex、Falcor3D和Isaac3D等数据集上进行比较表明，L-VAE在解耦指标方面始终提供最佳或次佳性能。此外，在CelebA数据集上的定性实验证实了L-VAE模型在解耦面部属性方面的成功。", "summary": "本文提出了一种名为L-VAE的新型变分自编码器，它通过学习损失函数中的超参数来动态调整解耦和重建损失之间的权衡，从而实现更好的解耦表示。与传统的\\b{eta}-VAE不同，L-VAE能够并发学习模型参数和损失权重，并引入正则化项以防止偏差。实验结果表明，L-VAE在多个数据集上均优于或与现有先进模型相当，并在解耦性能和重建质量之间取得了有效平衡。", "keywords": "变分自编码器, 解耦表示, 可学习超参数, L-VAE, 损失函数", "comments": "L-VAE的创新之处在于其能够学习损失函数中的超参数，从而实现解耦和重建之间的动态权衡，这克服了传统\\b{eta}-VAE需要经验性调整的局限性。这种自适应学习机制使其在解耦表示方面表现出更强的鲁棒性和有效性。该方法对理解和生成具有可控属性的数据具有重要意义。"}}
{"id": "2507.02018", "title": "NGAT: A Node-level Graph Attention Network for Long-term Stock Prediction", "authors": ["Yingjie Niu", "Mingchuan Zhao", "Valerio Poti", "Ruihai Dong"], "categories": ["q-fin.ST", "cs.AI", "cs.LG", "I.2.1"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02018v1", "summary": "Graph representation learning methods have been widely adopted in financial\napplications to enhance company representations by leveraging inter-firm\nrelationships. However, current approaches face three key challenges: (1) The\nadvantages of relational information are obscured by limitations in downstream\ntask designs; (2) Existing graph models specifically designed for stock\nprediction often suffer from excessive complexity and poor generalization; (3)\nExperience-based construction of corporate relationship graphs lacks effective\ncomparison of different graph structures. To address these limitations, we\npropose a long-term stock prediction task and develop a Node-level Graph\nAttention Network (NGAT) specifically tailored for corporate relationship\ngraphs. Furthermore, we experimentally demonstrate the limitations of existing\ngraph comparison methods based on model downstream task performance.\nExperimental results across two datasets consistently demonstrate the\neffectiveness of our proposed task and model. The project is publicly available\non GitHub to encourage reproducibility and future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02018v1", "cate": "q-fin.ST", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "NGAT：一种用于长期股票预测的节点级图注意力网络", "tldr": "NGAT是一种新的图注意力网络，旨在解决现有图表示学习方法在长期股票预测中的局限性，并已在两个数据集上验证其有效性。", "motivation": "现有图表示学习方法在金融应用中存在三个关键挑战：1) 关系信息的优势被下游任务设计所限制；2) 现有股票预测图模型过于复杂且泛化性差；3) 基于经验构建的企业关系图缺乏有效比较不同图结构的方法。", "method": "本文提出了一个长期股票预测任务，并开发了一种专门为企业关系图设计的节点级图注意力网络（NGAT）。此外，通过实验证明了现有基于模型下游任务性能的图比较方法的局限性。", "result": "在两个数据集上的实验结果一致证明了所提出的任务和模型的有效性。", "conclusion": "NGAT模型和提出的长期股票预测任务能够有效提升股票预测性能，并解决了现有图模型在金融应用中的局限性。", "translation": "图表示学习方法已被广泛应用于金融领域，通过利用企业间关系来增强公司表示。然而，当前方法面临三个关键挑战：(1) 关系信息的优势被下游任务设计的局限性所掩盖；(2) 现有专门为股票预测设计的图模型通常过于复杂且泛化性差；(3) 基于经验构建的企业关系图缺乏对不同图结构的有效比较。为了解决这些局限性，我们提出了一个长期股票预测任务，并开发了一种专门针对企业关系图的节点级图注意力网络（NGAT）。此外，我们通过实验证明了现有基于模型下游任务性能的图比较方法的局限性。在两个数据集上的实验结果一致证明了我们提出的任务和模型的有效性。该项目已在GitHub上公开，以鼓励可复现性和未来的研究。", "summary": "本文针对现有图表示学习方法在长期股票预测中的局限性，提出了一种新的长期股票预测任务和节点级图注意力网络（NGAT）。该模型专门为企业关系图设计，旨在克服关系信息利用不足、模型复杂性高以及图结构比较方法不足等问题。实验结果在两个数据集上验证了NGAT及其所提任务的有效性。", "keywords": "图注意力网络, 长期股票预测, 企业关系图, 金融应用, 节点级", "comments": "NGAT的创新之处在于其专门针对企业关系图的节点级注意力机制，以及对长期股票预测任务的关注，解决了现有方法在金融领域面临的实际挑战。通过开源项目，该研究鼓励了可复现性和进一步的探索，具有较好的实践意义和研究价值。"}}
{"id": "2507.02454", "title": "Weakly-supervised Contrastive Learning with Quantity Prompts for Moving Infrared Small Target Detection", "authors": ["Weiwei Duan", "Luping Ji", "Shengjia Chen", "Sicheng Zhu", "Jianghong Huang", "Mao Ye"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02454v1", "summary": "Different from general object detection, moving infrared small target\ndetection faces huge challenges due to tiny target size and weak background\ncontrast.Currently, most existing methods are fully-supervised, heavily relying\non a large number of manual target-wise annotations. However, manually\nannotating video sequences is often expensive and time-consuming, especially\nfor low-quality infrared frame images. Inspired by general object detection,\nnon-fully supervised strategies ($e.g.$, weakly supervised) are believed to be\npotential in reducing annotation requirements. To break through traditional\nfully-supervised frameworks, as the first exploration work, this paper proposes\na new weakly-supervised contrastive learning (WeCoL) scheme, only requires\nsimple target quantity prompts during model training.Specifically, in our\nscheme, based on the pretrained segment anything model (SAM), a potential\ntarget mining strategy is designed to integrate target activation maps and\nmulti-frame energy accumulation.Besides, contrastive learning is adopted to\nfurther improve the reliability of pseudo-labels, by calculating the similarity\nbetween positive and negative samples in feature subspace.Moreover, we propose\na long-short term motion-aware learning scheme to simultaneously model the\nlocal motion patterns and global motion trajectory of small targets.The\nextensive experiments on two public datasets (DAUB and ITSDT-15K) verify that\nour weakly-supervised scheme could often outperform early fully-supervised\nmethods. Even, its performance could reach over 90\\% of state-of-the-art (SOTA)\nfully-supervised ones.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02454v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于数量提示的弱监督对比学习用于运动红外小目标检测", "tldr": "该论文提出了一种新的弱监督对比学习方案（WeCoL），仅需要目标数量提示即可在不依赖大量手动标注的情况下，有效地检测运动红外小目标，并在公开数据集上表现出色。", "motivation": "运动红外小目标检测面临目标尺寸微小和背景对比度弱的巨大挑战。现有方法大多是全监督的，高度依赖昂贵且耗时的手动目标级标注。为了减少标注需求并突破传统全监督框架，本研究探索了弱监督策略。", "method": "本论文提出了弱监督对比学习（WeCoL）方案。具体方法包括：1) 基于预训练的SAM模型，设计了融合目标激活图和多帧能量累积的潜在目标挖掘策略。2) 采用对比学习，通过计算特征子空间中正负样本的相似性来提高伪标签的可靠性。3) 提出了长短期运动感知学习方案，同时建模小目标的局部运动模式和全局运动轨迹。", "result": "在两个公共数据集（DAUB和ITSDT-15K）上的广泛实验验证了所提出的弱监督方案通常优于早期的全监督方法。甚至，其性能可以达到最先进（SOTA）全监督方法的90%以上。", "conclusion": "本研究提出的弱监督对比学习方案（WeCoL）在运动红外小目标检测中表现出显著的有效性，证明了仅需简单数量提示的弱监督方法在减少标注成本的同时，能够达到接近甚至超越传统全监督方法的性能。", "translation": "与一般目标检测不同，运动红外小目标检测由于目标尺寸微小和背景对比度弱而面临巨大挑战。目前，大多数现有方法是全监督的，高度依赖大量的目标级手动标注。然而，手动标注视频序列通常昂贵且耗时，特别是对于低质量红外帧图像。受一般目标检测的启发，非全监督策略（例如，弱监督）被认为在减少标注需求方面具有潜力。为了突破传统的全监督框架，作为首次探索性工作，本文提出了一种新的弱监督对比学习（WeCoL）方案，在模型训练期间仅需要简单的目标数量提示。具体而言，在我们的方案中，基于预训练的Segment Anything Model (SAM)，设计了一种潜在目标挖掘策略，以整合目标激活图和多帧能量累积。此外，通过计算特征子空间中正样本和负样本之间的相似性，采用对比学习来进一步提高伪标签的可靠性。此外，我们提出了一种长短期运动感知学习方案，以同时建模小目标的局部运动模式和全局运动轨迹。在两个公共数据集（DAUB和ITSDT-15K）上的广泛实验验证了我们的弱监督方案通常优于早期的全监督方法。甚至，其性能可以达到最先进（SOTA）全监督方法的90%以上。", "summary": "本论文针对运动红外小目标检测中手动标注成本高的问题，首次提出了一种名为WeCoL的弱监督对比学习方案。该方案仅需简单的目标数量提示，通过结合预训练的SAM模型进行潜在目标挖掘、利用对比学习提高伪标签可靠性，并引入长短期运动感知学习来捕捉目标运动特征。实验证明，WeCoL在公共数据集上表现出色，性能可与最先进的全监督方法相媲美，有效降低了标注需求。", "keywords": "弱监督学习, 对比学习, 红外小目标检测, 数量提示, SAM", "comments": "该论文的创新点在于首次将弱监督对比学习应用于运动红外小目标检测，并仅通过简单的数量提示就实现了高性能。其利用SAM模型进行目标挖掘和结合对比学习来提升伪标签可靠性的方法是值得关注的。这对于解决红外小目标检测中数据标注成本高昂的问题具有重要意义，展现了弱监督学习在该领域内的巨大潜力。"}}
{"id": "2507.02624", "title": "A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes", "authors": ["Antoine Honoré", "Borja Rodríguez Gálvez", "Yoomi Park", "Yitian Zhou", "Volker M. Lauschke", "Ming Xiao"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      12+8 pages", "url": "http://arxiv.org/abs/2507.02624v1", "summary": "Variant effect predictors (VEPs) aim to assess the functional impact of\nprotein variants, traditionally relying on multiple sequence alignments (MSAs).\nThis approach assumes that naturally occurring variants are fit, an assumption\nchallenged by pharmacogenomics, where some pharmacogenes experience low\nevolutionary pressure. Deep mutational scanning (DMS) datasets provide an\nalternative by offering quantitative fitness scores for variants. In this work,\nwe propose a transformer-based matrix variational auto-encoder (matVAE) with a\nstructured prior and evaluate its performance on 33 DMS datasets corresponding\nto 26 drug target and ADME proteins from the ProteinGym benchmark. Our model\ntrained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence\nmodel in zero-shot prediction on DMS datasets, despite using an order of\nmagnitude fewer parameters and requiring less computation at inference time. We\nalso compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on\nDMS data, and find that the latter performs better on supervised prediction\ntasks. Additionally, incorporating AlphaFold-generated structures into our\ntransformer model further improves performance, achieving results comparable to\nDeepSequence trained on MSAs and finetuned on DMS. These findings highlight the\npotential of DMS datasets to replace MSAs without significant loss in\npredictive performance, motivating further development of DMS datasets and\nexploration of their relationships to enhance variant effect prediction.", "comment": "12+8 pages", "pdf_url": "http://arxiv.org/pdf/2507.02624v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "用于药理基因变异效应预测的矩阵变分自编码器", "tldr": "提出了一种基于Transformer的矩阵变分自编码器（matVAE），用于药理基因变异效应预测，并在DMS数据集上表现出色，突出了DMS数据集替代MSAs的潜力。", "motivation": "传统的变异效应预测器（VEPs）依赖多序列比对（MSAs），但这种方法假设自然发生的变异是适应的，这在药理基因组学中受到挑战，因为某些药理基因的进化压力较低。深度突变扫描（DMS）数据集提供了定量适应度分数，为解决此问题提供了替代方案。", "method": "本研究提出了一种基于Transformer的矩阵变分自编码器（matVAE），具有结构化先验，并在ProteinGym基准测试中，使用对应于26种药物靶点和ADME蛋白的33个DMS数据集评估其性能。模型在MSAs上训练（matVAE-MSA），并与在DMS数据上训练的matENC-DMS模型以及整合了AlphaFold生成结构的Transformer模型进行了比较。", "result": "在DMS数据集上的零样本预测中，在MSAs上训练的matVAE-MSA模型优于最先进的DeepSequence模型，尽管参数数量少一个数量级，推理计算量也更少。在监督预测任务中，在DMS数据上训练的matENC-DMS模型表现更好。此外，将AlphaFold生成的结构整合到Transformer模型中进一步提高了性能，达到了与在MSAs上训练并在DMS上微调的DeepSequence相当的结果。", "conclusion": "研究结果强调了DMS数据集在不显著损失预测性能的情况下替代MSAs的潜力，这激励了DMS数据集的进一步开发以及对其关系的探索，以增强变异效应预测。", "translation": "变异效应预测器（VEPs）旨在评估蛋白质变异的功能影响，传统上依赖于多序列比对（MSAs）。这种方法假设自然发生的变异是适应的，但在药理基因组学中，一些药理基因经历低进化压力，这一假设受到挑战。深度突变扫描（DMS）数据集通过提供变异的定量适应度分数来提供替代方案。在这项工作中，我们提出了一种基于Transformer的矩阵变分自编码器（matVAE），具有结构化先验，并在ProteinGym基准测试中，对应于26种药物靶点和ADME蛋白的33个DMS数据集上评估其性能。我们基于MSAs训练的模型（matVAE-MSA）在DMS数据集上的零样本预测中优于最先进的DeepSequence模型，尽管其参数数量少一个数量级，并且在推理时所需的计算量更少。我们还将matVAE-MSA与matENC-DMS（一个在DMS数据上训练的容量相似的模型）进行比较，发现后者在监督预测任务上表现更好。此外，将AlphaFold生成的结构整合到我们的Transformer模型中进一步提高了性能，取得了与在MSAs上训练并在DMS上微调的DeepSequence相当的结果。这些发现强调了DMS数据集在不显著损失预测性能的情况下替代MSAs的潜力，这激励了DMS数据集的进一步开发以及对其关系的探索，以增强变异效应预测。", "summary": "本研究提出了一种名为matVAE的基于Transformer的矩阵变分自编码器，用于药理基因的变异效应预测。该模型在多个深度突变扫描（DMS）数据集上进行了评估，并与传统的多序列比对（MSA）方法进行了比较。结果显示，在MSA上训练的matVAE-MSA在零样本预测方面优于DeepSequence，且计算效率更高。此外，结合AlphaFold结构进一步提升了性能，表明DMS数据集有潜力替代MSAs进行变异效应预测。", "keywords": "变异效应预测, 矩阵变分自编码器, 深度突变扫描, 药理基因, Transformer", "comments": "这项工作展示了深度突变扫描（DMS）数据集在变异效应预测中的巨大潜力，可能取代传统的多序列比对（MSA）方法。模型在计算效率和预测性能上的优势，以及整合结构信息带来的提升，都体现了其创新性。这对于药理基因组学领域具有重要意义，因为它为理解和预测药物相关基因变异的功能影响提供了更有效和准确的工具。"}}
{"id": "2507.02628", "title": "Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data", "authors": ["Irena Girshovitz", "Atai Ambus", "Moni Shahar", "Ran Gilad-Bachrach"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      18 pages, 4 figures (+ appendix)", "url": "http://arxiv.org/abs/2507.02628v1", "summary": "Background: The use of Electronic Health Records (EHRs) for epidemiological\nstudies and artificial intelligence (AI) training is increasing rapidly. The\nreliability of the results depends on the accuracy and completeness of EHR\ndata. However, EHR data often contain significant quality issues, including\nmisrepresentations of subpopulations, biases, and systematic errors, as they\nare primarily collected for clinical and billing purposes. Existing quality\nassessment methods remain insufficient, lacking systematic procedures to assess\ndata fitness for research.\n  Methods: We present the Medical Data Pecking approach, which adapts unit\ntesting and coverage concepts from software engineering to identify data\nquality concerns. We demonstrate our approach using the Medical Data Pecking\nTool (MDPT), which consists of two main components: (1) an automated test\ngenerator that uses large language models and grounding techniques to create a\ntest suite from data and study descriptions, and (2) a data testing framework\nthat executes these tests, reporting potential errors and coverage.\n  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and\nSyntheticMass, generating 55-73 tests per cohort across four conditions. These\ntests correctly identified 20-43 non-aligned or non-conforming data issues. We\npresent a detailed analysis of the LLM-generated test suites in terms of\nreference grounding and value accuracy.\n  Conclusion: Our approach incorporates external medical knowledge to enable\ncontext-sensitive data quality testing as part of the data analysis workflow to\nimprove the validity of its outcomes. Our approach tackles these challenges\nfrom a quality assurance perspective, laying the foundation for further\ndevelopment such as additional data modalities and improved grounding methods.", "comment": "18 pages, 4 figures (+ appendix)", "pdf_url": "http://arxiv.org/pdf/2507.02628v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "医学数据啄木鸟：一种用于结构化医疗数据自动质量评估的上下文感知方法", "tldr": "该研究提出了一种名为“医学数据啄木鸟”（Medical Data Pecking）的上下文感知方法，通过借鉴软件工程中的单元测试和覆盖概念，并利用大型语言模型，自动评估结构化医疗数据的质量，以提高其在研究和AI训练中的可靠性。", "motivation": "电子健康记录（EHRs）在流行病学研究和人工智能（AI）训练中的使用迅速增加，但其结果的可靠性取决于EHR数据的准确性和完整性。然而，EHR数据常存在严重的质量问题，包括子群体误报、偏差和系统性错误，且现有质量评估方法不足以系统性地评估数据是否适合研究。", "method": "本研究提出了“医学数据啄木鸟”（Medical Data Pecking）方法，该方法借鉴了软件工程中的单元测试和覆盖概念来识别数据质量问题。通过医学数据啄木鸟工具（MDPT）实现，该工具包含两个主要组件：1) 一个自动测试生成器，利用大型语言模型和接地技术从数据和研究描述中创建测试套件；2) 一个数据测试框架，执行这些测试并报告潜在错误和覆盖率。", "result": "MDPT在三个数据集（All of Us (AoU)、MIMIC-III和SyntheticMass）上进行了评估，在四种条件下，每个队列生成了55-73个测试。这些测试正确识别了20-43个不一致或不符合规范的数据问题。研究还详细分析了LLM生成的测试套件在参考接地和值准确性方面的表现。", "conclusion": "该方法结合外部医学知识，实现了上下文敏感的数据质量测试，作为数据分析工作流程的一部分，以提高其结果的有效性。该方法从质量保证的角度解决了这些挑战，为进一步开发（如额外数据模态和改进的接地方法）奠定了基础。", "translation": "背景：电子健康记录（EHRs）在流行病学研究和人工智能（AI）训练中的使用正在迅速增加。结果的可靠性取决于EHR数据的准确性和完整性。然而，EHR数据通常包含显著的质量问题，包括子群体误报、偏差和系统性错误，因为它们主要用于临床和计费目的。现有的质量评估方法仍然不足，缺乏系统程序来评估数据是否适合研究。\n方法：我们提出了“医学数据啄木鸟”（Medical Data Pecking）方法，该方法借鉴了软件工程中的单元测试和覆盖概念来识别数据质量问题。我们使用医学数据啄木鸟工具（MDPT）展示了我们的方法，该工具由两个主要组件组成：(1) 一个自动测试生成器，利用大型语言模型和接地技术从数据和研究描述中创建测试套件；(2) 一个数据测试框架，执行这些测试，报告潜在错误和覆盖率。\n结果：我们在三个数据集：All of Us (AoU)、MIMIC-III和SyntheticMass上评估了MDPT，在四种条件下，每个队列生成了55-73个测试。这些测试正确识别了20-43个不一致或不符合规范的数据问题。我们详细分析了LLM生成的测试套件在参考接地和值准确性方面的表现。\n结论：我们的方法结合了外部医学知识，实现了上下文敏感的数据质量测试，作为数据分析工作流程的一部分，以提高其结果的有效性。我们的方法从质量保证的角度解决了这些挑战，为进一步开发（如额外数据模态和改进的接地方法）奠定了基础。", "summary": "本研究提出了一种名为“医学数据啄木鸟”（Medical Data Pecking）的新方法，旨在自动评估结构化医疗数据的质量。该方法借鉴软件工程中的单元测试和覆盖概念，并开发了MDPT工具，该工具利用大型语言模型自动生成测试套件，并执行数据测试以识别质量问题。在多个EHR数据集上的评估显示，MDPT能有效发现数据中的不一致和错误。该方法通过整合外部医学知识，实现了上下文敏感的数据质量检测，为提高医疗数据在研究和AI应用中的可靠性奠定了基础。", "keywords": "医疗数据质量, 电子健康记录, 单元测试, 大型语言模型, 数据质量评估", "comments": "该论文的创新点在于将软件工程的单元测试和覆盖概念引入医疗数据质量评估，并结合大型语言模型（LLM）实现自动化测试生成，这为解决EHR数据质量问题提供了一个新颖且高效的途径。其重要性在于能够提高医疗数据在流行病学研究和AI训练中的可靠性，从而提升医疗研究成果的有效性。未来的发展可能包括扩展到更多数据模态和改进LLM的接地方法。"}}
{"id": "2507.02479", "title": "CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios", "authors": ["Teng Fu", "Yuwen Chen", "Zhuofan Chen", "Mengyang Zhao", "Bin Li", "Xiangyang Xue"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02479v1", "summary": "Multi-object tracking is a classic field in computer vision. Among them,\npedestrian tracking has extremely high application value and has become the\nmost popular research category. Existing methods mainly use motion or\nappearance information for tracking, which is often difficult in complex\nscenarios. For the motion information, mutual occlusions between objects often\nprevent updating of the motion state; for the appearance information,\nnon-robust results are often obtained due to reasons such as only partial\nvisibility of the object or blurred images. Although learning how to perform\ntracking in these situations from the annotated data is the simplest solution,\nthe existing MOT dataset fails to satisfy this solution. Existing methods\nmainly have two drawbacks: relatively simple scene composition and\nnon-realistic scenarios. Although some of the video sequences in existing\ndataset do not have the above-mentioned drawbacks, the number is far from\nadequate for research purposes. To this end, we propose a difficult large-scale\ndataset for multi-pedestrian tracking, shot mainly from the first-person view\nand all from real-life complex scenarios. We name it ``CrowdTrack'' because\nthere are numerous objects in most of the sequences. Our dataset consists of 33\nvideos, containing a total of 5,185 trajectories. Each object is annotated with\na complete bounding box and a unique object ID. The dataset will provide a\nplatform to facilitate the development of algorithms that remain effective in\ncomplex situations. We analyzed the dataset comprehensively and tested multiple\nSOTA models on our dataset. Besides, we analyzed the performance of the\nfoundation models on our dataset. The dataset and project code is released at:\nhttps://github.com/loseevaya/CrowdTrack .", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02479v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "CrowdTrack：一个用于真实场景中困难多行人跟踪的基准", "tldr": "本文提出了CrowdTrack，一个用于真实复杂场景中困难多行人跟踪的新的大规模基准数据集，旨在解决现有数据集的局限性。", "motivation": "现有的多行人跟踪方法在复杂场景中由于遮挡和图像模糊而表现不佳，而现有的多目标跟踪（MOT）数据集存在场景构成过于简单和不真实的问题，无法为学习复杂情况下的跟踪提供足够的标注数据。", "method": "我们提出了一个名为“CrowdTrack”的困难大规模多行人跟踪数据集，主要从第一人称视角拍摄，全部来自真实复杂的场景。该数据集包含详细的边界框和唯一对象ID标注。我们对数据集进行了全面分析，并测试了多个最先进的模型以及基础模型。", "result": "CrowdTrack数据集包含33个视频，共5,185条轨迹。每个对象都标注有完整的边界框和唯一的对象ID。该数据集将为促进在复杂情况下仍然有效的算法开发提供一个平台。我们分析了数据集并测试了SOTA模型和基础模型。", "conclusion": "CrowdTrack数据集通过提供一个具有挑战性、大规模且真实世界的基准，解决了现有数据集的局限性，从而促进了在复杂场景下更鲁棒的多行人跟踪算法的开发。", "translation": "多目标跟踪是计算机视觉领域的一个经典方向。其中，行人跟踪具有极高的应用价值，并已成为最受欢迎的研究类别。现有方法主要利用运动或外观信息进行跟踪，这在复杂场景中往往很困难。对于运动信息，物体之间的相互遮挡常常阻碍运动状态的更新；对于外观信息，由于物体仅部分可见或图像模糊等原因，常常导致非鲁棒的结果。尽管从标注数据中学习如何在这些情况下进行跟踪是最简单的解决方案，但现有的MOT数据集未能满足这一需求。现有方法主要有两个缺点：场景构成相对简单和场景不真实。尽管现有数据集中某些视频序列没有上述缺点，但其数量远远不足以满足研究目的。为此，我们提出了一个用于多行人跟踪的困难大规模数据集，主要从第一人称视角拍摄，全部来自真实复杂的场景。我们将其命名为“CrowdTrack”，因为大多数序列中包含大量目标。我们的数据集由33个视频组成，共包含5,185条轨迹。每个对象都标注有完整的边界框和唯一的对象ID。该数据集将为促进在复杂情况下仍然有效的算法开发提供一个平台。我们全面分析了数据集，并在我们的数据集上测试了多个SOTA模型。此外，我们还分析了基础模型在我们数据集上的表现。数据集和项目代码已发布在：https://github.com/loseevaya/CrowdTrack。", "summary": "本文介绍了CrowdTrack，一个新颖的大规模基准数据集，专为真实世界复杂场景中具有挑战性的多行人跟踪而设计。它解决了现有数据集的不足，这些数据集通常场景简单或不真实，阻碍了鲁棒跟踪算法的开发。CrowdTrack包含33个视频，共5,185条轨迹，每条轨迹都精心标注了边界框和唯一的ID，主要从第一人称视角捕获。该数据集旨在为推进能够在遮挡、部分可见和图像模糊等密集人群环境中有效运行的跟踪算法提供一个关键平台。作者全面分析了该数据集，并评估了最先进的模型和基础模型。", "keywords": "多行人跟踪, 基准, 数据集, CrowdTrack, 真实场景", "comments": "本文的创新之处在于创建了一个专门针对行人跟踪中“困难”方面（如密集人群、遮挡、真实世界复杂性、第一人称视角）的数据集，而现有数据集未能充分涵盖这些方面。这对于推动多目标跟踪研究向更实用和鲁棒的解决方案迈进至关重要。"}}
{"id": "2507.02634", "title": "High-Order Deep Meta-Learning with Category-Theoretic Interpretation", "authors": ["David H. Mguni"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02634v1", "summary": "We introduce a new hierarchical deep learning framework for recursive\nhigher-order meta-learning that enables neural networks (NNs) to construct,\nsolve, and generalise across hierarchies of tasks. Central to this approach is\na generative mechanism that creates \\emph{virtual tasks} -- synthetic problem\ninstances designed to enable the meta-learner to learn \\emph{soft constraints}\nand unknown generalisable rules across related tasks. Crucially, this enables\nthe framework to generate its own informative, task-grounded datasets thereby\nfreeing machine learning (ML) training from the limitations of relying entirely\non human-generated data. By actively exploring the virtual point landscape and\nseeking out tasks lower-level learners find difficult, the meta-learner\niteratively refines constraint regions. This enhances inductive biases,\nregularises the adaptation process, and produces novel, unanticipated tasks and\nconstraints required for generalisation. Each meta-level of the hierarchy\ncorresponds to a progressively abstracted generalisation of problems solved at\nlower levels, enabling a structured and interpretable learning progression. By\ninterpreting meta-learners as category-theoretic \\emph{functors} that generate\nand condition a hierarchy of subordinate learners, we establish a compositional\nstructure that supports abstraction and knowledge transfer across progressively\ngeneralised tasks. The category-theoretic perspective unifies existing\nmeta-learning models and reveals how learning processes can be transformed and\ncompared through functorial relationships, while offering practical design\nprinciples for structuring meta-learning. We speculate this architecture may\nunderpin the next generation of NNs capable of autonomously generating novel,\ninstructive tasks and their solutions, thereby advancing ML towards general\nartificial intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02634v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "高阶深度元学习与范畴论解释", "tldr": "本文提出一种新的高阶深度元学习框架，通过生成虚拟任务来克服数据依赖，并利用范畴论解释实现跨层级任务的构建、解决和泛化，旨在推动机器学习迈向通用人工智能。", "motivation": "现有机器学习训练严重依赖人类生成的数据，限制了神经网络在任务层级间的构建、解决和泛化能力。本文旨在通过引入新的框架来解决这一局限性，使神经网络能够自主学习和泛化。", "method": "本文引入了一个新的递归高阶元学习的层次深度学习框架。其核心是一个生成机制，用于创建虚拟任务（合成问题实例），使元学习器能够学习跨相关任务的软约束和可泛化规则。通过主动探索虚拟点景观并寻找低级学习器认为困难的任务，元学习器迭代地完善约束区域。此外，该方法将元学习器解释为范畴论中的函子，从而建立支持抽象和知识迁移的组合结构。", "result": "该框架能够生成自身的信息丰富、以任务为基础的数据集，从而摆脱对人类生成数据的完全依赖。它增强了归纳偏置，规范了适应过程，并产生了泛化所需的新颖、意想不到的任务和约束。范畴论的视角统一了现有元学习模型，并揭示了学习过程如何通过函子关系进行转换和比较。", "conclusion": "该架构可能支撑下一代神经网络，使其能够自主生成新颖、有指导性的任务及其解决方案，从而推动机器学习向通用人工智能发展。", "translation": "我们引入了一种新的递归高阶元学习的层次深度学习框架，使神经网络能够构建、解决和泛化跨任务层级的任务。该方法的核心是一个生成机制，用于创建“虚拟任务”——旨在使元学习器学习相关任务中的“软约束”和未知可泛化规则的合成问题实例。至关重要的是，这使得该框架能够生成自己的信息丰富、以任务为基础的数据集，从而使机器学习训练摆脱完全依赖人类生成数据的限制。通过主动探索虚拟点景观并寻找低级学习器认为困难的任务，元学习器迭代地完善约束区域。这增强了归纳偏置，规范了适应过程，并产生了泛化所需的新颖、意想不到的任务和约束。层次结构的每个元级别都对应于对较低级别解决的问题的逐步抽象泛化，从而实现结构化和可解释的学习进展。通过将元学习器解释为生成和调节下属学习器层次结构的范畴论“函子”，我们建立了一个支持抽象和知识迁移的组合结构，跨越逐步泛化的任务。范畴论的视角统一了现有元学习模型，并揭示了学习过程如何通过函子关系进行转换和比较，同时为元学习的结构化提供了实用的设计原则。我们推测这种架构可能支撑下一代神经网络，使其能够自主生成新颖、有指导性的任务及其解决方案，从而推动机器学习迈向通用人工智能。", "summary": "本文提出了一种结合高阶深度元学习与范畴论解释的新型层次化框架。该框架通过生成“虚拟任务”来自主创建训练数据，克服了传统机器学习对人工数据的依赖。通过迭代优化虚拟任务，它能增强归纳偏置并产生新的泛化约束。将元学习器视为范畴论函子，该方法构建了一个支持跨层级抽象和知识迁移的组合结构，统一了现有模型，并为未来自主生成任务和解决方案的神经网络奠定了基础，有望推动通用人工智能的发展。", "keywords": "高阶元学习, 虚拟任务, 范畴论, 深度学习, 泛化", "comments": "本文的创新之处在于将高阶深度元学习与虚拟任务生成和范畴论解释相结合，为解决机器学习对人工数据的依赖提供了新颖的思路。通过自主生成任务和数据，该框架有望显著提升模型的泛化能力和自主学习能力，为通用人工智能的发展提供了理论和实践上的探索方向，具有重要的前瞻性。"}}
{"id": "2507.02790", "title": "From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding", "authors": ["Xiangfeng Wang", "Xiao Li", "Yadong Wei", "Xueyu Song", "Yang Song", "Xiaoqiang Xia", "Fangrui Zeng", "Zaiyi Chen", "Liu Liu", "Gu Xu", "Tong Xu"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02790v1", "summary": "The rapid growth of online video content, especially on short video\nplatforms, has created a growing demand for efficient video editing techniques\nthat can condense long-form videos into concise and engaging clips. Existing\nautomatic editing methods predominantly rely on textual cues from ASR\ntranscripts and end-to-end segment selection, often neglecting the rich visual\ncontext and leading to incoherent outputs. In this paper, we propose a\nhuman-inspired automatic video editing framework (HIVE) that leverages\nmultimodal narrative understanding to address these limitations. Our approach\nincorporates character extraction, dialogue analysis, and narrative\nsummarization through multimodal large language models, enabling a holistic\nunderstanding of the video content. To further enhance coherence, we apply\nscene-level segmentation and decompose the editing process into three subtasks:\nhighlight detection, opening/ending selection, and pruning of irrelevant\ncontent. To facilitate research in this area, we introduce DramaAD, a novel\nbenchmark dataset comprising over 800 short drama episodes and 500\nprofessionally edited advertisement clips. Experimental results demonstrate\nthat our framework consistently outperforms existing baselines across both\ngeneral and advertisement-oriented editing tasks, significantly narrowing the\nquality gap between automatic and human-edited videos.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02790v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "从长视频到精彩短片：一种基于多模态叙事理解的人类启发式视频编辑框架", "tldr": "本文提出了一种名为HIVE的人类启发式自动视频编辑框架，利用多模态叙事理解将长视频高效剪辑成连贯且引人入胜的短片，并引入了新数据集DramaAD，实验证明其优于现有基线。", "motivation": "在线视频内容，尤其是短视频平台的快速增长，对高效视频编辑技术提出了日益增长的需求，需要将长视频浓缩成简洁引人入胜的短片。现有自动编辑方法过度依赖ASR文本线索和端到端片段选择，忽略了丰富的视觉上下文，导致输出不连贯。", "method": "本文提出HIVE框架，利用多模态叙事理解，通过多模态大语言模型进行人物提取、对话分析和叙事总结，实现对视频内容的整体理解。为增强连贯性，该方法应用场景级分割，并将编辑过程分解为三个子任务：高光检测、开头/结尾选择和无关内容修剪。此外，引入了新的基准数据集DramaAD，包含800多个短剧集和500个专业编辑的广告片段。", "result": "实验结果表明，HIVE框架在通用和广告导向的编辑任务中均持续优于现有基线，显著缩小了自动编辑视频与人工编辑视频之间的质量差距。", "conclusion": "本文提出的HIVE框架通过结合多模态叙事理解和精细化的编辑流程，有效解决了现有自动视频编辑方法的局限性，显著提升了自动生成短片的质量和连贯性，使其更接近专业人工编辑水平。", "translation": "在线视频内容，尤其是在短视频平台上的快速增长，对能够将长篇视频浓缩成简洁且引人入胜的短片的高效视频编辑技术产生了日益增长的需求。现有的自动编辑方法主要依赖于ASR转录的文本线索和端到端的片段选择，往往忽略了丰富的视觉上下文，导致输出不连贯。在本文中，我们提出了一种人类启发式的自动视频编辑框架（HIVE），该框架利用多模态叙事理解来解决这些局限性。我们的方法通过多模态大语言模型整合了人物提取、对话分析和叙事总结，从而实现了对视频内容的整体理解。为了进一步增强连贯性，我们应用场景级分割，并将编辑过程分解为三个子任务：高光检测、开头/结尾选择和无关内容修剪。为了促进该领域的研究，我们引入了一个新颖的基准数据集DramaAD，该数据集包含800多个短剧集和500个专业编辑的广告短片。实验结果表明，我们的框架在通用和广告导向的编辑任务中均持续优于现有基线，显著缩小了自动编辑视频与人工编辑视频之间的质量差距。", "summary": "本文提出HIVE，一个人类启发式自动视频编辑框架，旨在将长视频高效剪辑为连贯且引人入胜的短片。针对现有方法忽视视觉上下文导致不连贯的问题，HIVE利用多模态大语言模型进行叙事理解，包括人物提取、对话分析和叙事总结。它还将编辑过程细化为高光检测、开头/结尾选择和内容修剪。为推动研究，作者还构建了DramaAD数据集。实验证明，HIVE在多种编辑任务中均优于现有基线，显著提升了自动编辑视频的质量。", "keywords": "视频编辑, 多模态理解, 叙事分析, 自动剪辑, DramaAD", "comments": "该论文的创新点在于提出了一个人类启发式的视频编辑框架HIVE，通过融合多模态叙事理解（特别是利用多模态大语言模型）来克服传统方法仅依赖文本线索的局限性。其将编辑过程细化为多个子任务，并引入了新的基准数据集DramaAD，对推动该领域的研究具有重要意义。实验结果表明其显著提升了自动编辑视频的质量，缩小了与人工编辑的差距，展示了该方法的有效性和实用潜力。"}}
{"id": "2507.02488", "title": "MedFormer: Hierarchical Medical Vision Transformer with Content-Aware Dual Sparse Selection Attention", "authors": ["Zunhui Xia", "Hongxing Li", "Libin Lan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 9 figures, 9 tables", "url": "http://arxiv.org/abs/2507.02488v1", "summary": "Medical image recognition serves as a key way to aid in clinical diagnosis,\nenabling more accurate and timely identification of diseases and abnormalities.\nVision transformer-based approaches have proven effective in handling various\nmedical recognition tasks. However, these methods encounter two primary\nchallenges. First, they are often task-specific and architecture-tailored,\nlimiting their general applicability. Second, they usually either adopt full\nattention to model long-range dependencies, resulting in high computational\ncosts, or rely on handcrafted sparse attention, potentially leading to\nsuboptimal performance. To tackle these issues, we present MedFormer, an\nefficient medical vision transformer with two key ideas. First, it employs a\npyramid scaling structure as a versatile backbone for various medical image\nrecognition tasks, including image classification and dense prediction tasks\nsuch as semantic segmentation and lesion detection. This structure facilitates\nhierarchical feature representation while reducing the computation load of\nfeature maps, highly beneficial for boosting performance. Second, it introduces\na novel Dual Sparse Selection Attention (DSSA) with content awareness to\nimprove computational efficiency and robustness against noise while maintaining\nhigh performance. As the core building technique of MedFormer, DSSA is\nexplicitly designed to attend to the most relevant content. In addition, a\ndetailed theoretical analysis has been conducted, demonstrating that MedFormer\nhas superior generality and efficiency in comparison to existing medical vision\ntransformers. Extensive experiments on a variety of imaging modality datasets\nconsistently show that MedFormer is highly effective in enhancing performance\nacross all three above-mentioned medical image recognition tasks. The code is\navailable at https://github.com/XiaZunhui/MedFormer.", "comment": "13 pages, 9 figures, 9 tables", "pdf_url": "http://arxiv.org/pdf/2507.02488v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MedFormer：基于内容感知双稀疏选择注意的分层医学视觉Transformer", "tldr": "MedFormer是一种高效的医学视觉Transformer，采用金字塔缩放结构和内容感知双稀疏选择注意力（DSSA），以提高医学图像识别的通用性和效率。", "motivation": "当前的医学视觉Transformer方法存在两个主要挑战：一是通用性受限，因为它们通常是任务特定和架构定制的；二是计算成本高或性能欠佳，因为它们要么采用全注意力，要么依赖手工稀疏注意力。", "method": "本文提出了MedFormer，一种高效的医学视觉Transformer。MedFormer采用金字塔缩放结构作为通用骨干，适用于图像分类、语义分割和病灶检测等多种医学图像识别任务，以实现分层特征表示并降低计算负荷。其次，它引入了一种新颖的、内容感知的双稀疏选择注意力（DSSA），旨在关注最相关的内容，从而提高计算效率、鲁棒性并保持高性能。", "result": "MedFormer在多种成像模态数据集上的广泛实验表明，它在图像分类、语义分割和病灶检测这三种医学图像识别任务中都能有效提升性能。理论分析也表明，MedFormer在通用性和效率方面优于现有医学视觉Transformer。", "conclusion": "MedFormer通过其分层金字塔结构和内容感知双稀疏选择注意力，成功解决了现有医学视觉Transformer在通用性和效率方面的局限性，并在多种医学图像识别任务中展现出卓越的性能。", "translation": "医学图像识别是辅助临床诊断的关键方法，能够更准确、及时地识别疾病和异常。基于视觉Transformer的方法已被证明在处理各种医学识别任务中是有效的。然而，这些方法遇到了两个主要挑战。首先，它们通常是任务特定和架构定制的，限制了其通用性。其次，它们通常要么采用全注意力来建模长距离依赖关系，导致高计算成本，要么依赖手工稀疏注意力，可能导致次优性能。为了解决这些问题，我们提出了MedFormer，一种高效的医学视觉Transformer，具有两个关键思想。首先，它采用金字塔缩放结构作为各种医学图像识别任务（包括图像分类和语义分割、病灶检测等密集预测任务）的通用骨干。这种结构有助于分层特征表示，同时减少特征图的计算负荷，这对于提高性能非常有益。其次，它引入了一种新颖的、内容感知的双稀疏选择注意力（DSSA），以提高计算效率和对噪声的鲁棒性，同时保持高性能。作为MedFormer的核心构建技术，DSSA被明确设计为关注最相关的内容。此外，还进行了详细的理论分析，表明与现有医学视觉Transformer相比，MedFormer具有卓越的通用性和效率。在各种成像模态数据集上进行的广泛实验一致表明，MedFormer在上述所有三种医学图像识别任务中都非常有效地提高了性能。代码可在https://github.com/XiaZunhui/MedFormer获取。", "summary": "本文提出了MedFormer，一种用于医学图像识别的高效分层视觉Transformer。针对现有方法通用性差和计算成本高的问题，MedFormer引入了金字塔缩放结构作为通用骨干，并设计了内容感知双稀疏选择注意力（DSSA）以提高效率和鲁棒性。实验证明，MedFormer在图像分类、语义分割和病灶检测等多种医学图像识别任务中均表现出优越的通用性和性能。", "keywords": "医学图像识别, 视觉Transformer, MedFormer, 双稀疏选择注意力, 深度学习", "comments": "MedFormer的创新之处在于结合了金字塔缩放结构和内容感知双稀疏选择注意力（DSSA），有效解决了医学视觉Transformer在通用性和计算效率方面的痛点。DSSA通过关注最相关内容，在提高效率的同时保持了高性能，这对于处理高分辨率医学图像至关重要。其作为通用骨干的潜力也使其在临床应用中具有广泛前景。"}}
{"id": "2507.02639", "title": "On Efficient Bayesian Exploration in Model-Based Reinforcement Learning", "authors": ["Alberto Caron", "Chris Hicks", "Vasilios Mavroudis"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02639v1", "summary": "In this work, we address the challenge of data-efficient exploration in\nreinforcement learning by examining existing principled, information-theoretic\napproaches to intrinsic motivation. Specifically, we focus on a class of\nexploration bonuses that targets epistemic uncertainty rather than the\naleatoric noise inherent in the environment. We prove that these bonuses\nnaturally signal epistemic information gains and converge to zero once the\nagent becomes sufficiently certain about the environment's dynamics and\nrewards, thereby aligning exploration with genuine knowledge gaps. Our analysis\nprovides formal guarantees for IG-based approaches, which previously lacked\ntheoretical grounding. To enable practical use, we also discuss tractable\napproximations via sparse variational Gaussian Processes, Deep Kernels and Deep\nEnsemble models. We then outline a general framework - Predictive Trajectory\nSampling with Bayesian Exploration (PTS-BE) - which integrates model-based\nplanning with information-theoretic bonuses to achieve sample-efficient deep\nexploration. We empirically demonstrate that PTS-BE substantially outperforms\nother baselines across a variety of environments characterized by sparse\nrewards and/or purely exploratory tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02639v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "关于模型基强化学习中高效贝叶斯探索", "tldr": "本文提出了一种名为PTS-BE的框架，用于在基于模型的强化学习中进行高效的贝叶斯探索，该框架利用信息理论奖励，并优于现有基线。", "motivation": "解决强化学习中数据高效探索的挑战，特别是通过检查和改进现有的、针对内在动机的、信息理论方法，旨在解决认知不确定性问题。此前的信息增益（IG）方法缺乏理论基础。", "method": "研究了针对认知不确定性的信息理论探索奖励；证明了这些奖励信号认知信息增益并随智能体对环境动态和奖励的确定性增加而趋于零；讨论了通过稀疏变分高斯过程、深度核和深度集成模型实现的实用近似；提出了一个通用框架——预测轨迹采样与贝叶斯探索（PTS-BE），该框架将基于模型的规划与信息理论奖励相结合。", "result": "提供了信息增益（IG）方法的正式理论保证，此前这些方法缺乏理论基础；PTS-BE在各种以稀疏奖励和/或纯探索任务为特征的环境中，显著优于其他基线。", "conclusion": "本文为基于信息增益的探索奖励提供了理论基础，并引入了PTS-BE，这是一个在基于模型的强化学习中实现样本高效深度探索的有效框架。", "translation": "在这项工作中，我们通过研究现有的原则性、信息理论内在动机方法，解决了强化学习中数据高效探索的挑战。具体来说，我们专注于一类针对认知不确定性而非环境中固有的任意噪声的探索奖励。我们证明了这些奖励自然地表明认知信息增益，并且一旦智能体对环境的动态和奖励足够确定，它们就会收敛到零，从而使探索与真正的知识差距对齐。我们的分析为以前缺乏理论基础的基于信息增益（IG）的方法提供了正式保证。为了实现实际应用，我们还讨论了通过稀疏变分高斯过程、深度核和深度集成模型实现的可处理近似。然后，我们概述了一个通用框架——预测轨迹采样与贝叶斯探索（PTS-BE）——它将基于模型的规划与信息理论奖励相结合，以实现样本高效的深度探索。我们凭经验证明，PTS-BE在各种以稀疏奖励和/或纯探索任务为特征的环境中显著优于其他基线。", "summary": "本文通过关注针对认知不确定性的信息理论探索奖励，解决了基于模型的强化学习中数据高效探索的挑战。论文为基于信息增益（IG）的方法提供了正式的理论保证，并讨论了可行的近似方法。作者提出了一个名为预测轨迹采样与贝叶斯探索（PTS-BE）的通用框架，该框架将基于模型的规划与信息理论奖励相结合，并在各种具有稀疏奖励和/或纯探索任务的环境中，经验性地证明了其优于其他基线的性能。", "keywords": "贝叶斯探索, 强化学习, 认知不确定性, 信息增益, 基于模型的强化学习", "comments": "该论文的创新之处在于为先前缺乏理论基础的信息增益（IG）探索奖励提供了正式的理论保证，并将其整合到一个实用且高性能的框架（PTS-BE）中。其重要性在于提高了探索的数据效率，尤其是在稀疏奖励环境中，这在强化学习中是一个重大挑战。"}}
{"id": "2507.02834", "title": "ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning", "authors": ["Ruiyang Zhou", "Shuozhe Li", "Amy Zhang", "Liu Leqi"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02834v1", "summary": "Recent advances in large language models have been driven by reinforcement\nlearning (RL)-style post-training, which improves reasoning by optimizing model\noutputs based on reward or preference signals. GRPO-style approaches implement\nthis by using self-generated samples labeled by an outcome-based verifier.\nHowever, these methods depend heavily on the model's initial ability to produce\npositive samples. They primarily refine what the model already knows\n(distribution sharpening) rather than enabling the model to solve problems\nwhere it initially fails. This limitation is especially problematic in\nearly-stage RL training and on challenging reasoning tasks, where positive\nsamples are unlikely to be generated. To unlock reasoning ability in such\nsettings, the model must explore new reasoning trajectories beyond its current\noutput distribution. Such exploration requires access to sufficiently good\npositive samples to guide the learning. While expert demonstrations seem like a\nnatural solution, we find that they are often ineffective in RL post-training.\nInstead, we identify two key properties of effective positive samples: they\nshould (1) be likely under the current policy, and (2) increase the model's\nlikelihood of predicting the correct answer. Based on these insights, we\npropose $\\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and\nmodular framework that generates such samples by conditioning on the\nground-truth answer. ExPO enables efficient exploration and guides the model to\nproduce reasoning trajectories more aligned with its policy than expert-written\nCoTs, while ensuring higher quality than its own (incorrect) samples.\nExperiments show that ExPO improves both learning efficiency and final\nperformance on reasoning benchmarks, surpassing expert-demonstration-based\nmethods in challenging settings such as MATH level-5, where the model initially\nstruggles the most.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02834v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "ExPO：通过自我解释引导的强化学习解锁困难推理", "tldr": "ExPO是一种新的强化学习框架，它通过生成高质量的自我解释样本来解决现有RL方法在困难推理任务中探索不足的问题，从而提高学习效率和最终性能。", "motivation": "现有的强化学习后训练方法（如GRPO）严重依赖模型生成高质量正样本的初始能力，导致在早期训练阶段或面对挑战性推理任务时（模型最初失败的情况下）难以有效探索和学习。模型需要探索超出当前输出分布的新推理轨迹，并需要高质量的正样本来指导学习。", "method": "本文提出了ExPO（自我解释策略优化）框架。ExPO通过以真实答案为条件来生成正样本。这些样本在当前策略下是可行的，并且能够增加模型预测正确答案的可能性。ExPO旨在引导模型生成与其策略更一致的推理轨迹，同时确保比模型自身（不正确）的样本质量更高。", "result": "实验表明，ExPO提高了推理基准测试的学习效率和最终性能。在MATH level-5等模型最初表现不佳的挑战性设置中，ExPO超越了基于专家演示的方法。", "conclusion": "ExPO框架通过生成高质量的自我解释样本，有效地解决了在困难推理任务中现有强化学习方法探索不足的问题，从而成功解锁了模型的推理能力，并在挑战性任务上取得了显著的性能提升。", "translation": "大型语言模型最近的进展得益于强化学习（RL）风格的后训练，通过优化基于奖励或偏好信号的模型输出来改进推理。GRPO风格的方法通过使用由基于结果的验证器标记的自生成样本来实现这一点。然而，这些方法严重依赖于模型生成正样本的初始能力。它们主要精炼模型已知的内容（分布锐化），而不是使模型能够解决其最初失败的问题。这种限制在早期RL训练和挑战性推理任务中尤其成问题，因为正样本不太可能生成。为了在这种设置下解锁推理能力，模型必须探索超出其当前输出分布的新推理轨迹。这种探索需要访问足够好的正样本来指导学习。虽然专家演示似乎是一个自然的解决方案，但我们发现它们在RL后训练中通常无效。相反，我们确定了有效正样本的两个关键属性：它们应该（1）在当前策略下是可能的，并且（2）增加模型预测正确答案的可能性。基于这些见解，我们提出了$\textbf{自我解释策略优化（ExPO）}$——一个简单模块化的框架，通过以真实答案为条件来生成此类样本。ExPO能够实现高效探索，并引导模型生成与其策略更一致的推理轨迹，而不是专家编写的CoT（思维链），同时确保比其自身（不正确）样本的质量更高。实验表明，ExPO提高了推理基准测试的学习效率和最终性能，在MATH level-5等模型最初挣扎的挑战性设置中超越了基于专家演示的方法。", "summary": "本文提出了ExPO（自我解释策略优化）框架，旨在解决现有强化学习方法在处理困难推理任务时探索不足的问题。ExPO通过以真实答案为条件生成高质量的自我解释样本，这些样本在当前策略下可行且能提高正确预测的可能性。这种方法使得模型能够更有效地探索新的推理路径，并在实验中被证明能显著提升学习效率和在如MATH level-5等挑战性推理任务上的性能，超越了传统的专家演示方法。", "keywords": "强化学习, 自我解释, 推理, 大型语言模型, 探索", "comments": "ExPO的创新之处在于其通过条件化真实答案来生成高质量的自我解释样本，从而解决了RL在早期训练和困难任务中正样本稀缺的问题。这种方法使得模型能够进行更有效的探索，超越了传统依赖模型自身初始能力或专家演示的局限性，对于提升大型语言模型在复杂推理任务上的表现具有重要意义。"}}
{"id": "2507.02493", "title": "Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy", "authors": ["Luca Parolari", "Andrea Cherubini", "Lamberto Ballan", "Carlo Biffi"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at MICCAI 2025", "url": "http://arxiv.org/abs/2507.02493v1", "summary": "Automated polyp counting in colonoscopy is a crucial step toward automated\nprocedure reporting and quality control, aiming to enhance the\ncost-effectiveness of colonoscopy screening. Counting polyps in a procedure\ninvolves detecting and tracking polyps, and then clustering tracklets that\nbelong to the same polyp entity. Existing methods for polyp counting rely on\nself-supervised learning and primarily leverage visual appearance, neglecting\ntemporal relationships in both tracklet feature learning and clustering stages.\nIn this work, we introduce a paradigm shift by proposing a supervised\ncontrastive loss that incorporates temporally-aware soft targets. Our approach\ncaptures intra-polyp variability while preserving inter-polyp discriminability,\nleading to more robust clustering. Additionally, we improve tracklet clustering\nby integrating a temporal adjacency constraint, reducing false positive\nre-associations between visually similar but temporally distant tracklets. We\ntrain and validate our method on publicly available datasets and evaluate its\nperformance with a leave-one-out cross-validation strategy. Results demonstrate\na 2.2x reduction in fragmentation rate compared to prior approaches. Our\nresults highlight the importance of temporal awareness in polyp counting,\nestablishing a new state-of-the-art. Code is available at\nhttps://github.com/lparolari/temporally-aware-polyp-counting.", "comment": "Accepted at MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.02493v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "结肠镜检查中息肉计数的时序感知监督对比学习", "tldr": "本研究提出一种时序感知监督对比学习方法，用于结肠镜检查中的息肉计数，通过引入时序信息改进特征学习和轨迹聚类，将碎片化率降低了2.2倍，建立了新的技术水平。", "motivation": "自动化息肉计数是结肠镜检查报告和质量控制的关键步骤，旨在提高筛查的成本效益。现有息肉计数方法主要依赖自监督学习并利用视觉外观，忽略了轨迹特征学习和聚类阶段中的时序关系，导致计数不准确。", "method": "本研究引入了一种范式转变，提出了一种结合时序感知软目标的监督对比损失。该方法在保留息肉间可区分性的同时捕获息肉内变异性，从而实现更鲁棒的聚类。此外，通过整合时序邻近约束来改进轨迹聚类，减少了视觉相似但时序上远的轨迹之间的假阳性重新关联。该方法在公开数据集上进行训练和验证，并采用留一法交叉验证策略评估性能。", "result": "与现有方法相比，碎片化率降低了2.2倍。", "conclusion": "研究结果强调了时序感知在息肉计数中的重要性，并建立了新的技术水平。", "translation": "结肠镜检查中息肉的自动化计数是实现自动化程序报告和质量控制的关键一步，旨在提高结肠镜筛查的成本效益。在一次操作中计数息肉涉及检测和跟踪息肉，然后将属于同一息肉实体的轨迹段进行聚类。现有息肉计数方法依赖于自监督学习，主要利用视觉外观，在轨迹段特征学习和聚类阶段都忽略了时间关系。在这项工作中，我们引入了一种范式转变，提出了一种结合时序感知软目标的监督对比损失。我们的方法在保持息肉间可区分性的同时捕获息肉内部变异性，从而实现更鲁棒的聚类。此外，我们通过整合时间邻近约束来改进轨迹段聚类，减少了视觉相似但时间上远的轨迹段之间的假阳性重新关联。我们在公开可用数据集上训练和验证了我们的方法，并采用留一法交叉验证策略评估其性能。结果表明，与现有方法相比，碎片化率降低了2.2倍。我们的结果突出了时间感知在息肉计数中的重要性，建立了新的技术水平。代码可在https://github.com/lparolari/temporally-aware-polyp-counting 获取。", "summary": "本论文提出了一种用于结肠镜检查中息肉计数的时序感知监督对比学习方法。针对现有方法忽视时序信息的问题，该方法引入了结合时序感知软目标的监督对比损失，并整合了时序邻近约束以改进轨迹聚类，从而在保留息肉间可区分性的同时捕获息肉内变异性，减少假阳性关联。实验结果显示，与现有方法相比，碎片化率降低了2.2倍，证明了时序感知在息肉计数中的重要性，并达到了新的技术水平。", "keywords": "息肉计数, 时序感知, 监督对比学习, 轨迹聚类, 结肠镜检查", "comments": "该论文的创新点在于将时序感知引入监督对比学习框架中，解决了现有息肉计数方法忽视视频序列中时序关系的问题。通过引入时序感知软目标和时序邻近约束，显著提高了轨迹聚类的鲁棒性和准确性，尤其是在减少碎片化率方面取得了显著进展，对自动化医疗影像分析具有重要意义。"}}
{"id": "2507.02645", "title": "Fair Deepfake Detectors Can Generalize", "authors": ["Harry Cheng", "Ming-Hui Liu", "Yangyang Guo", "Tianyi Wang", "Liqiang Nie", "Mohan Kankanhalli"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      14 pages, version 1", "url": "http://arxiv.org/abs/2507.02645v1", "summary": "Deepfake detection models face two critical challenges: generalization to\nunseen manipulations and demographic fairness among population groups. However,\nexisting approaches often demonstrate that these two objectives are inherently\nconflicting, revealing a trade-off between them. In this paper, we, for the\nfirst time, uncover and formally define a causal relationship between fairness\nand generalization. Building on the back-door adjustment, we show that\ncontrolling for confounders (data distribution and model capacity) enables\nimproved generalization via fairness interventions. Motivated by this insight,\nwe propose Demographic Attribute-insensitive Intervention Detection (DAID), a\nplug-and-play framework composed of: i) Demographic-aware data rebalancing,\nwhich employs inverse-propensity weighting and subgroup-wise feature\nnormalization to neutralize distributional biases; and ii) Demographic-agnostic\nfeature aggregation, which uses a novel alignment loss to suppress\nsensitive-attribute signals. Across three cross-domain benchmarks, DAID\nconsistently achieves superior performance in both fairness and generalization\ncompared to several state-of-the-art detectors, validating both its theoretical\nfoundation and practical effectiveness.", "comment": "14 pages, version 1", "pdf_url": "http://arxiv.org/pdf/2507.02645v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "公平的深度伪造检测器能够泛化", "tldr": "现有深度伪造检测器在泛化性和公平性之间存在权衡，本文首次揭示两者因果关系，并提出DAID框架，通过干预实现公平性和泛化性双赢。", "motivation": "现有的深度伪造检测模型面临两大挑战：对未知操纵的泛化能力和不同人群间的公平性，且通常认为这两个目标是冲突的。本文旨在首次揭示并正式定义公平性与泛化性之间的因果关系，并在此基础上提出一种能同时提升两者的解决方案。", "method": "本文提出了“人口属性不敏感干预检测（DAID）”框架，这是一个即插即用的框架。它包含两个主要组成部分：1) 人口统计学感知数据再平衡：通过逆倾向加权和子组特征归一化来消除分布偏差；2) 人口统计学无关特征聚合：使用新颖的对齐损失来抑制敏感属性信号。", "result": "在三个跨域基准测试中，DAID在公平性和泛化性方面均始终优于几种最先进的检测器。", "conclusion": "本文首次揭示并正式定义了公平性与泛化性之间的因果关系，并基于此提出了DAID框架，该框架在理论和实践上都证明了其在深度伪造检测中同时提升公平性和泛化性的有效性。", "translation": "深度伪造检测模型面临两大严峻挑战：对未知操纵的泛化能力和不同人群间的公平性。然而，现有方法通常表明这两个目标是固有冲突的，揭示了它们之间的权衡。本文首次揭示并正式定义了公平性与泛化性之间的因果关系。基于后门调整（back-door adjustment），我们表明控制混杂因素（数据分布和模型容量）可以通过公平性干预来改善泛化能力。受此启发，我们提出了人口属性不敏感干预检测（DAID），这是一个即插即用的框架，由以下部分组成：i）人口统计学感知数据再平衡，它采用逆倾向加权和子组特征归一化来中和分布偏差；以及ii）人口统计学无关特征聚合，它使用新颖的对齐损失来抑制敏感属性信号。在三个跨域基准测试中，与几种最先进的检测器相比，DAID在公平性和泛化性方面均始终取得了卓越的性能，验证了其理论基础和实际有效性。", "summary": "本文研究了深度伪造检测中泛化性和公平性之间的传统权衡问题。作者首次揭示并形式化定义了公平性与泛化性之间的因果关系，指出通过控制混杂因素（如数据分布和模型容量）进行公平性干预可以提升泛化能力。基于此洞察，提出了一种名为DAID的即插即用框架，该框架通过人口统计学感知数据再平衡（使用逆倾向加权和子组特征归一化）和人口统计学无关特征聚合（使用新的对齐损失）来解决偏差并抑制敏感属性信号。实验结果表明，DAID在多个跨域基准测试中，在公平性和泛化性上均优于现有先进方法。", "keywords": "深度伪造检测, 公平性, 泛化性, 因果关系, DAID, 偏差消除", "comments": "本文的创新之处在于首次从因果关系的角度探讨了深度伪造检测中公平性与泛化性之间的关联，并提出了一个理论上和实践上都有效的解决方案。DAID框架的即插即用特性也增加了其应用潜力。该研究为开发更鲁棒、更公平的深度伪造检测器提供了新的思路。"}}
{"id": "2507.02106", "title": "Resolving Turbulent Magnetohydrodynamics: A Hybrid Operator-Diffusion Framework", "authors": ["Semih Kacmaz", "E. A. Huerta", "Roland Haas"], "categories": ["physics.flu-dyn", "cs.AI", "cs.LG", "gr-qc", "physics.comp-ph", "J.2; I.2"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02106v1", "summary": "We present a hybrid machine learning framework that combines Physics-Informed\nNeural Operators (PINOs) with score-based generative diffusion models to\nsimulate the full spatio-temporal evolution of two-dimensional, incompressible,\nresistive magnetohydrodynamic (MHD) turbulence across a broad range of Reynolds\nnumbers ($\\mathrm{Re}$). The framework leverages the equation-constrained\ngeneralization capabilities of PINOs to predict coherent, low-frequency\ndynamics, while a conditional diffusion model stochastically corrects\nhigh-frequency residuals, enabling accurate modeling of fully developed\nturbulence. Trained on a comprehensive ensemble of high-fidelity simulations\nwith $\\mathrm{Re} \\in \\{100, 250, 500, 750, 1000, 3000, 10000\\}$, the approach\nachieves state-of-the-art accuracy in regimes previously inaccessible to\ndeterministic surrogates. At $\\mathrm{Re}=1000$ and $3000$, the model\nfaithfully reconstructs the full spectral energy distributions of both velocity\nand magnetic fields late into the simulation, capturing non-Gaussian\nstatistics, intermittent structures, and cross-field correlations with high\nfidelity. At extreme turbulence levels ($\\mathrm{Re}=10000$), it remains the\nfirst surrogate capable of recovering the high-wavenumber evolution of the\nmagnetic field, preserving large-scale morphology and enabling statistically\nmeaningful predictions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02106v1", "cate": "physics.flu-dyn", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "解决湍流磁流体力学：一种混合算子-扩散框架", "tldr": "该研究提出了一种结合PINOs和扩散模型的混合机器学习框架，用于高精度模拟二维不可压缩电阻磁流体动力学（MHD）湍流，尤其是在高雷诺数下表现出色。", "motivation": "模拟全时空演化的二维不可压缩电阻磁流体动力学（MHD）湍流，特别是在高雷诺数下，传统确定性替代模型难以实现高精度。", "method": "提出了一种混合机器学习框架，结合了物理信息神经算子（PINOs）和基于分数的生成扩散模型。PINOs预测连贯的低频动力学，条件扩散模型随机校正高频残差。", "result": "该方法在$\\\\mathrm{Re} \\\\in \\\\{100, 250, 500, 750, 1000, 3000, 10000\\\\}$的高保真模拟集合上训练，在以前确定性替代模型无法企及的范围内实现了最先进的精度。在$\\\\mathrm{Re}=1000$和$3000$时，模型忠实地重建了速度场和磁场的完整光谱能量分布，捕捉了非高斯统计、间歇性结构和交叉场相关性。在极端湍流水平（$\\\\mathrm{Re}=10000$）下，它是第一个能够恢复磁场高波数演化、保持大尺度形态并实现统计意义上预测的替代模型。", "conclusion": "该混合框架能够高精度模拟二维MHD湍流，特别是在高雷诺数下，克服了传统确定性替代模型的局限性，在极端湍流条件下也表现出色。", "translation": "我们提出了一种混合机器学习框架，该框架结合了物理信息神经算子（PINOs）和基于分数的生成扩散模型，以模拟二维、不可压缩、电阻磁流体动力学（MHD）湍流在广泛雷诺数（$\\\\mathrm{Re}$）范围内的全时空演化。该框架利用PINOs的方程约束泛化能力来预测连贯的低频动力学，而条件扩散模型则随机校正高频残差，从而实现对完全发展湍流的精确建模。该方法在包含$\\\\mathrm{Re} \\\\in \\\\{100, 250, 500, 750, 1000, 3000, 10000\\\\}$的高保真模拟综合数据集上进行训练，在以前确定性替代模型无法企及的范围内实现了最先进的精度。在$\\\\mathrm{Re}=1000$和$3000$时，该模型在模拟后期忠实地重建了速度场和磁场的完整光谱能量分布，高保真地捕捉了非高斯统计、间歇性结构和交叉场相关性。在极端湍流水平（$\\\\mathrm{Re}=10000$）下，它仍然是第一个能够恢复磁场高波数演化、保持大尺度形态并实现统计意义上预测的替代模型。", "summary": "本文提出了一种创新的混合机器学习框架，结合物理信息神经算子（PINOs）与基于分数的生成扩散模型，用于精确模拟二维不可压缩电阻磁流体动力学（MHD）湍流。该框架有效结合了PINOs的低频动力学预测能力和扩散模型的高频残差校正能力。实验结果表明，该模型在广泛的雷诺数范围内（最高达$\\\\mathrm{Re}=10000$）实现了最先进的精度，尤其是在高雷诺数下，能够忠实重建复杂的湍流特性，包括非高斯统计和磁场高波数演化，显著超越了现有确定性替代模型的性能。", "keywords": "磁流体力学湍流, 物理信息神经算子, 扩散模型, 机器学习, 雷诺数", "comments": "这篇论文的创新点在于将物理信息神经算子（PINOs）与生成扩散模型相结合，有效地处理了湍流模拟中低频和高频动力学的不同挑战。这种混合方法显著提升了在高雷诺数下MHD湍流模拟的准确性和鲁棒性，克服了传统确定性替代模型的局限性。尤其是在$\\\\mathrm{Re}=10000$这种极端湍流水平下，能够恢复磁场高波数演化并进行统计意义上的预测，是其重要的贡献。"}}
{"id": "2507.02494", "title": "MC-INR: Efficient Encoding of Multivariate Scientific Simulation Data using Meta-Learning and Clustered Implicit Neural Representations", "authors": ["Hyunsoo Son", "Jeonghyun Noh", "Suemin Jeon", "Chaoli Wang", "Won-Ki Jeong"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      5 pages", "url": "http://arxiv.org/abs/2507.02494v1", "summary": "Implicit Neural Representations (INRs) are widely used to encode data as\ncontinuous functions, enabling the visualization of large-scale multivariate\nscientific simulation data with reduced memory usage. However, existing\nINR-based methods face three main limitations: (1) inflexible representation of\ncomplex structures, (2) primarily focusing on single-variable data, and (3)\ndependence on structured grids. Thus, their performance degrades when applied\nto complex real-world datasets. To address these limitations, we propose a\nnovel neural network-based framework, MC-INR, which handles multivariate data\non unstructured grids. It combines meta-learning and clustering to enable\nflexible encoding of complex structures. To further improve performance, we\nintroduce a residual-based dynamic re-clustering mechanism that adaptively\npartitions clusters based on local error. We also propose a branched layer to\nleverage multivariate data through independent branches simultaneously.\nExperimental results demonstrate that MC-INR outperforms existing methods on\nscientific data encoding tasks.", "comment": "5 pages", "pdf_url": "http://arxiv.org/pdf/2507.02494v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MC-INR：使用元学习和聚类隐式神经表示对多变量科学仿真数据进行高效编码", "tldr": "MC-INR通过结合元学习和聚类，并引入动态再聚类机制，有效解决了现有隐式神经表示（INR）在编码复杂多变量科学仿真数据时的局限性，提高了性能。", "motivation": "现有基于隐式神经表示（INR）的方法在处理复杂结构、多变量数据和非结构化网格时存在局限性，导致在复杂真实世界数据集上的性能下降。", "method": "本文提出MC-INR框架，结合元学习和聚类实现复杂结构的灵活编码。为进一步提升性能，引入基于残差的动态再聚类机制，根据局部误差自适应地划分簇；并提出分支层，通过独立分支同时利用多变量数据。", "result": "实验结果表明，MC-INR在科学数据编码任务上优于现有方法。", "conclusion": "MC-INR是一个新颖的神经网络框架，通过结合元学习、聚类、动态再聚类和分支层，有效解决了现有隐式神经表示在处理复杂多变量非结构化网格数据时的局限性，并在科学数据编码任务中表现出优越性能。", "translation": "隐式神经表示（INRs）被广泛用于将数据编码为连续函数，从而以更少的内存使用量实现大规模多变量科学仿真数据的可视化。然而，现有基于INR的方法面临三个主要限制：（1）复杂结构表示不灵活，（2）主要关注单变量数据，以及（3）依赖于结构化网格。因此，当应用于复杂的真实世界数据集时，它们的性能会下降。为了解决这些限制，我们提出了一种新颖的基于神经网络的框架MC-INR，它可以处理非结构化网格上的多变量数据。它结合了元学习和聚类，以实现复杂结构的灵活编码。为了进一步提高性能，我们引入了一种基于残差的动态再聚类机制，该机制根据局部误差自适应地划分簇。我们还提出了一种分支层，以通过独立分支同时利用多变量数据。实验结果表明，MC-INR在科学数据编码任务上优于现有方法。", "summary": "MC-INR是一个新颖的神经网络框架，旨在解决现有隐式神经表示（INRs）在处理复杂多变量科学仿真数据时的局限性。该框架结合了元学习和聚类技术，以实现复杂结构的灵活编码，并引入了基于残差的动态再聚类机制来适应性地划分数据簇，同时通过分支层有效利用多变量信息。实验证明，MC-INR在科学数据编码任务上表现优于现有方法。", "keywords": "隐式神经表示, 元学习, 聚类, 多变量数据, 科学仿真", "comments": "MC-INR的创新点在于其结合了元学习和聚类，并引入了动态再聚类机制和分支层，从而显著提升了隐式神经表示在处理复杂、多变量且非结构化科学仿真数据时的灵活性和性能。这克服了现有INR方法在表示复杂结构和处理多变量数据方面的关键限制，具有重要的实际应用价值。"}}
{"id": "2507.02513", "title": "Automatic Labelling for Low-Light Pedestrian Detection", "authors": ["Dimitrios Bouzoulas", "Eerik Alamikkotervo", "Risto Ojala"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02513v1", "summary": "Pedestrian detection in RGB images is a key task in pedestrian safety, as the\nmost common sensor in autonomous vehicles and advanced driver assistance\nsystems is the RGB camera. A challenge in RGB pedestrian detection, that does\nnot appear to have large public datasets, is low-light conditions. As a\nsolution, in this research, we propose an automated infrared-RGB labeling\npipeline. The proposed pipeline consists of 1) Infrared detection, where a\nfine-tuned model for infrared pedestrian detection is used 2) Label transfer\nprocess from the infrared detections to their RGB counterparts 3) Training\nobject detection models using the generated labels for low-light RGB pedestrian\ndetection. The research was performed using the KAIST dataset. For the\nevaluation, object detection models were trained on the generated autolabels\nand ground truth labels. When compared on a previously unseen image sequence,\nthe results showed that the models trained on generated labels outperformed the\nones trained on ground-truth labels in 6 out of 9 cases for the mAP@50 and\nmAP@50-95 metrics. The source code for this research is available at\nhttps://github.com/BouzoulasDimitrios/IR-RGB-Automated-LowLight-Pedestrian-Labeling", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02513v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "低光照行人检测的自动标注", "tldr": "本文提出了一种自动红外-RGB标注流水线，用于解决低光照RGB行人检测数据缺乏的问题，实验证明其生成的标签在特定情况下训练的模型性能优于真实标签。", "motivation": "RGB图像中的行人检测是自动驾驶和高级驾驶辅助系统的关键任务，但低光照条件下缺乏大型公共数据集是一个挑战。", "method": "提出一个自动红外-RGB标注流水线，包括：1) 使用微调模型进行红外行人检测；2) 将红外检测结果的标签转移到对应的RGB图像上；3) 使用生成的标签训练目标检测模型进行低光照RGB行人检测。研究基于KAIST数据集。", "result": "在9种情况中的6种情况下，使用生成标签训练的模型在mAP@50和mAP@50-95指标上优于使用真实标签训练的模型，特别是在一个未曾见过的图像序列上进行评估时。", "conclusion": "该研究提出的自动标注流水线能够有效解决低光照RGB行人检测数据集不足的问题，并且其生成的标签可以训练出表现优异的行人检测模型。", "translation": "RGB图像中的行人检测是行人安全的关键任务，因为自动驾驶车辆和高级驾驶辅助系统中最常见的传感器是RGB摄像头。RGB行人检测面临的一个挑战是低光照条件，而这方面似乎缺乏大型公共数据集。作为解决方案，本研究提出了一种自动红外-RGB标注流水线。所提出的流水线包括：1) 红外检测，使用经过微调的红外行人检测模型；2) 将红外检测结果的标签转移到对应的RGB图像上；3) 使用生成的标签训练目标检测模型，用于低光照RGB行人检测。该研究使用KAIST数据集进行。为了评估，目标检测模型使用生成的自动标签和真实标签进行训练。在一个先前未见过的图像序列上进行比较时，结果显示，在9种情况中的6种情况下，使用生成标签训练的模型在mAP@50和mAP@50-95指标上优于使用真实标签训练的模型。本研究的源代码可在https://github.com/BouzoulasDimitrios/IR-RGB-Automated-LowLight-Pedestrian-Labeling获取。", "summary": "本文提出了一种自动红外-RGB标注流水线，旨在解决低光照条件下RGB行人检测数据集缺乏的问题。该流水线利用红外图像检测行人并将其标签自动转移到对应的RGB图像上，然后使用这些自动生成的标签训练目标检测模型。实验结果表明，在低光照场景下，使用该方法生成的标签训练的模型在性能上优于使用真实标签训练的模型，证明了其在解决数据集稀缺性方面的有效性。", "keywords": "自动标注, 低光照行人检测, 红外-RGB, 数据集生成, 目标检测", "comments": "这项研究通过利用红外图像的互补性来自动生成低光照RGB图像的行人标注，提供了一种创新且实用的方法来解决数据稀缺性问题。其重要性在于，它降低了在特定挑战性环境下（如低光照）获取大量标注数据的成本和难度，对自动驾驶和高级驾驶辅助系统的发展具有积极意义。"}}
{"id": "2507.02670", "title": "Guided Generation for Developable Antibodies", "authors": ["Siqi Zhao", "Joshua Moller", "Porfi Quintero-Cadena", "Lood van Niekerk"], "categories": ["cs.LG", "q-bio.BM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published in ICML 2025 GenBio Workshop", "url": "http://arxiv.org/abs/2507.02670v1", "summary": "Therapeutic antibodies require not only high-affinity target engagement, but\nalso favorable manufacturability, stability, and safety profiles for clinical\neffectiveness. These properties are collectively called `developability'. To\nenable a computational framework for optimizing antibody sequences for\nfavorable developability, we introduce a guided discrete diffusion model\ntrained on natural paired heavy- and light-chain sequences from the Observed\nAntibody Space (OAS) and quantitative developability measurements for 246\nclinical-stage antibodies. To steer generation toward biophysically viable\ncandidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module\nthat biases sampling without compromising naturalness. In unconstrained\nsampling, our model reproduces global features of both the natural repertoire\nand approved therapeutics, and under SVDD guidance we achieve significant\nenrichment in predicted developability scores over unguided baselines. When\ncombined with high-throughput developability assays, this framework enables an\niterative, ML-driven pipeline for designing antibodies that satisfy binding and\nbiophysical criteria in tandem.", "comment": "Published in ICML 2025 GenBio Workshop", "pdf_url": "http://arxiv.org/pdf/2507.02670v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "可开发抗体的引导式生成", "tldr": "该研究引入了一种引导式离散扩散模型，结合SVDD模块，用于生成具有良好可开发性的治疗性抗体，并通过实验证明其在预测可开发性分数方面显著优于基线模型。", "motivation": "治疗性抗体不仅需要高亲和力靶点结合，还需要良好的可制造性、稳定性及安全性，这些特性统称为“可开发性”。现有方法需要一个计算框架来优化抗体序列以获得良好的可开发性。", "method": "研究引入了一种引导式离散扩散模型，该模型基于“观察抗体空间 (OAS)”中的天然配对重链和轻链序列以及246个临床阶段抗体的定量可开发性测量数据进行训练。为使生成偏向生物物理上可行的候选抗体，模型集成了Soft Value-based Decoding in Diffusion (SVDD) 模块，该模块在不损害自然性的前提下引导采样。", "result": "在无约束采样下，模型能够重现天然抗体库和已批准治疗药物的全局特征。在SVDD引导下，模型在预测可开发性分数方面比无引导基线模型实现了显著的富集。", "conclusion": "该框架与高通量可开发性测定相结合，能够实现一个迭代的、机器学习驱动的管线，用于设计同时满足结合和生物物理标准的抗体。", "translation": "治疗性抗体不仅需要高亲和力靶点结合，还需要良好的可制造性、稳定性及安全性，这些特性统称为“可开发性”，对临床疗效至关重要。为了建立一个优化抗体序列以实现良好可开发性的计算框架，我们引入了一种引导式离散扩散模型，该模型基于“观察抗体空间 (OAS)”中的天然配对重链和轻链序列以及246个临床阶段抗体的定量可开发性测量数据进行训练。为了使生成偏向生物物理上可行的候选抗体，我们整合了一个基于Soft Value-based Decoding in Diffusion (SVDD) 的模块，该模块在不损害自然性的前提下引导采样。在无约束采样下，我们的模型能够重现天然抗体库和已批准治疗药物的全局特征。在SVDD引导下，我们在预测可开发性分数方面比无引导基线模型实现了显著的富集。当与高通量可开发性测定相结合时，该框架能够实现一个迭代的、机器学习驱动的管线，用于设计同时满足结合和生物物理标准的抗体。", "summary": "本研究提出了一种引导式离散扩散模型，旨在优化治疗性抗体的可开发性。该模型利用天然抗体序列和临床阶段抗体的可开发性数据进行训练，并引入SVDD模块以引导生成具有良好生物物理特性的抗体。实验结果表明，该模型在无约束采样下能捕捉天然抗体特征，并在SVDD引导下显著提高了预测可开发性分数。该框架有望结合高通量筛选，实现迭代式的抗体设计。", "keywords": "可开发性, 抗体设计, 扩散模型, 机器学习, SVDD", "comments": "该论文的创新点在于将引导式离散扩散模型与SVDD模块相结合，用于生成具有特定生物物理特性的抗体，解决了传统抗体设计中可开发性难以兼顾的问题。其重要性在于提供了一个计算驱动的抗体优化平台，有望加速治疗性抗体的发现和开发。"}}
{"id": "2507.02519", "title": "IMASHRIMP: Automatic White Shrimp (Penaeus vannamei) Biometrical Analysis from Laboratory Images Using Computer Vision and Deep Learning", "authors": ["Abiam Remache González", "Meriem Chagour", "Timon Bijan Rüth", "Raúl Trapiella Cañedo", "Marina Martínez Soler", "Álvaro Lorenzo Felipe", "Hyun-Suk Shin", "María-Jesús Zamorano Serrano", "Ricardo Torres", "Juan-Antonio Castillo Parra", "Eduardo Reyes Abad", "Miguel-Ángel Ferrer Ballester", "Juan-Manuel Afonso López", "Francisco-Mario Hernández Tejera", "Adrian Penate-Sanchez"], "categories": ["cs.CV", "I.2.10; I.4.8"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      14 pages, 7 figures", "url": "http://arxiv.org/abs/2507.02519v1", "summary": "This paper introduces IMASHRIMP, an adapted system for the automated\nmorphological analysis of white shrimp (Penaeus vannamei}, aimed at optimizing\ngenetic selection tasks in aquaculture. Existing deep learning and computer\nvision techniques were modified to address the specific challenges of shrimp\nmorphology analysis from RGBD images. IMASHRIMP incorporates two discrimination\nmodules, based on a modified ResNet-50 architecture, to classify images by the\npoint of view and determine rostrum integrity. It is proposed a \"two-factor\nauthentication (human and IA)\" system, it reduces human error in view\nclassification from 0.97% to 0% and in rostrum detection from 12.46% to 3.64%.\nAdditionally, a pose estimation module was adapted from VitPose to predict 23\nkey points on the shrimp's skeleton, with separate networks for lateral and\ndorsal views. A morphological regression module, using a Support Vector Machine\n(SVM) model, was integrated to convert pixel measurements to centimeter units.\nExperimental results show that the system effectively reduces human error,\nachieving a mean average precision (mAP) of 97.94% for pose estimation and a\npixel-to-centimeter conversion error of 0.07 (+/- 0.1) cm. IMASHRIMP\ndemonstrates the potential to automate and accelerate shrimp morphological\nanalysis, enhancing the efficiency of genetic selection and contributing to\nmore sustainable aquaculture practices.The code are available at\nhttps://github.com/AbiamRemacheGonzalez/ImaShrimp-public", "comment": "14 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.02519v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "IMASHRIMP：基于计算机视觉和深度学习的实验室图像白虾（南美白对虾）生物测量自动分析", "tldr": "IMASHRIMP是一个利用计算机视觉和深度学习自动分析白虾形态的系统，旨在优化水产养殖中的遗传选择，显著降低了人工错误并提高了效率。", "motivation": "本研究旨在优化水产养殖中的遗传选择任务，通过自动化白虾（南美白对虾）的形态分析来解决现有技术在处理RGBD图像时面临的特定挑战。", "method": "IMASHRIMP系统修改了现有深度学习和计算机视觉技术。它包含两个基于改进ResNet-50架构的判别模块，用于图像视角分类和吻部完整性检测。系统提出了一个“双因素认证（人与人工智能）”机制以减少人工错误。此外，还适配了VitPose的姿态估计模块来预测虾骨骼上的23个关键点，并使用一个基于支持向量机（SVM）模型的形态回归模块将像素测量转换为厘米单位。", "result": "IMASHRIMP系统将视角分类中的人工错误从0.97%降低到0%，吻部检测中的人工错误从12.46%降低到3.64%。姿态估计实现了97.94%的平均精度（mAP），像素到厘米的转换误差为0.07（+/- 0.1）厘米。实验结果表明系统有效减少了人工错误。", "conclusion": "IMASHRIMP系统展示了自动化和加速虾形态分析的巨大潜力，能够显著提高遗传选择的效率，并有助于推动更可持续的水产养殖实践。", "translation": "本论文介绍了IMASHRIMP，一个用于白虾（Penaeus vannamei）形态自动分析的适应性系统，旨在优化水产养殖中的遗传选择任务。现有深度学习和计算机视觉技术经过修改，以解决RGBD图像中虾形态分析的特定挑战。IMASHRIMP包含两个判别模块，基于修改后的ResNet-50架构，用于按视角对图像进行分类并确定吻部完整性。它提出了一个“双因素认证（人与人工智能）”系统，将视角分类中的人工错误从0.97%降低到0%，吻部检测中的人工错误从12.46%降低到3.64%。此外，一个姿态估计模块从VitPose改编而来，用于预测虾骨骼上的23个关键点，其中侧视图和背视图采用独立的网络。一个形态回归模块，使用支持向量机（SVM）模型，被集成用于将像素测量转换为厘米单位。实验结果表明，该系统有效减少了人工错误，姿态估计的平均精度（mAP）达到97.94%，像素到厘米转换误差为0.07（+/- 0.1）厘米。IMASHRIMP展示了自动化和加速虾形态分析的潜力，提高了遗传选择的效率，并有助于更可持续的水产养殖实践。代码可在https://github.com/AbiamRemacheGonzalez/ImaShrimp-public获取。", "summary": "IMASHRIMP是一个基于计算机视觉和深度学习的自动化系统，专为白虾（南美白对虾）的形态分析设计，以支持水产养殖中的遗传选择。该系统通过改进的ResNet-50模块进行视角和吻部完整性判别，并引入人机双因素认证以显著减少错误。它还利用改编自VitPose的姿态估计模块预测关键点，并使用SVM进行像素到厘米的转换。实验证明，IMASHRIMP显著降低了人工错误，并在姿态估计和尺寸测量方面表现出色，有望提升水产养殖效率和可持续性。", "keywords": "白虾, 形态分析, 计算机视觉, 深度学习, 遗传选择", "comments": "这篇论文通过结合计算机视觉和深度学习技术，为水产养殖中的白虾形态分析提供了一个创新的自动化解决方案。其亮点在于引入了人机结合的“双因素认证”系统，有效降低了传统人工分析的错误率。系统在关键点检测和尺寸测量上的高精度，预示着在遗传选择和可持续养殖方面具有巨大的应用潜力。"}}
{"id": "2507.02698", "title": "Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions", "authors": ["Thomas Hazenberg", "Yao Ma", "Seyed Sahand Mohammadi Ziabari", "Marijn van Rijswijk"], "categories": ["cs.LG", "econ.EM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02698v1", "summary": "This study investigates how Multi-Agent Reinforcement Learning (MARL) can\nimprove dynamic pricing strategies in supply chains, particularly in contexts\nwhere traditional ERP systems rely on static, rule-based approaches that\noverlook strategic interactions among market actors. While recent research has\napplied reinforcement learning to pricing, most implementations remain\nsingle-agent and fail to model the interdependent nature of real-world supply\nchains. This study addresses that gap by evaluating the performance of three\nMARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines,\nwithin a simulated environment informed by real e-commerce transaction data and\na LightGBM demand prediction model. Results show that rule-based agents achieve\nnear-perfect fairness (Jain's Index: 0.9896) and the highest price stability\n(volatility: 0.024), but they fully lack competitive dynamics. Among MARL\nagents, MADQN exhibits the most aggressive pricing behaviour, with the highest\nvolatility and the lowest fairness (0.5844). MADDPG provides a more balanced\napproach, supporting market competition (share volatility: 9.5 pp) while\nmaintaining relatively high fairness (0.8819) and stable pricing. These\nfindings suggest that MARL introduces emergent strategic behaviour not captured\nby static pricing rules and may inform future developments in dynamic pricing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02698v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "多智能体强化学习在供应链动态定价中的应用：在真实模拟市场条件下对战略智能体行为进行基准测试", "tldr": "本研究评估了多智能体强化学习（MARL）算法在供应链动态定价中的表现，发现MARL引入了静态规则无法捕捉的新兴战略行为，而MADDPG在竞争、公平性和稳定性之间取得了平衡。", "motivation": "传统ERP系统在供应链中依赖静态、基于规则的定价方法，忽略了市场参与者之间的战略互动。现有强化学习在定价中的应用大多是单智能体，未能建模真实供应链相互依赖的性质。本研究旨在弥补这一空白。", "method": "本研究通过在一个由真实电商交易数据和LightGBM需求预测模型构建的模拟环境中，评估了三种多智能体强化学习（MARL）算法（MADDPG、MADQN和QMIX）与静态规则基线相比的性能。", "result": "基于规则的智能体实现了接近完美的公平性（Jain's Index: 0.9896）和最高的定价稳定性（波动性: 0.024），但完全缺乏竞争动态。在MARL智能体中，MADQN表现出最具侵略性的定价行为，具有最高的波动性和最低的公平性（0.5844）。MADDPG提供了一种更平衡的方法，支持市场竞争（份额波动性: 9.5 pp），同时保持相对较高的公平性（0.8819）和稳定的定价。", "conclusion": "研究结果表明，多智能体强化学习（MARL）引入了静态定价规则无法捕捉的新兴战略行为，这可能为动态定价的未来发展提供信息。", "translation": "本研究探讨了多智能体强化学习（MARL）如何改善供应链中的动态定价策略，特别是在传统ERP系统依赖静态、基于规则的方法而忽视市场参与者之间战略互动的背景下。虽然最近的研究已将强化学习应用于定价，但大多数实施仍是单智能体，未能建模真实世界供应链的相互依赖性。本研究通过在一个由真实电商交易数据和LightGBM需求预测模型构建的模拟环境中，评估了三种MARL算法：MADDPG、MADQN和QMIX与静态规则基线相比的性能，从而弥补了这一空白。结果显示，基于规则的智能体实现了接近完美的公平性（Jain's Index: 0.9896）和最高的定价稳定性（波动性：0.024），但它们完全缺乏竞争动态。在MARL智能体中，MADQN表现出最具侵略性的定价行为，具有最高的波动性和最低的公平性（0.5844）。MADDPG提供了一种更平衡的方法，支持市场竞争（份额波动性：9.5 pp），同时保持相对较高的公平性（0.8819）和稳定的定价。这些发现表明，MARL引入了静态定价规则无法捕捉的新兴战略行为，并可能为动态定价的未来发展提供信息。", "summary": "本研究旨在解决传统定价方法在供应链中忽视市场参与者战略互动的问题，通过评估多智能体强化学习（MARL）算法在模拟市场环境下的动态定价性能。研究比较了MADDPG、MADQN和QMIX与静态规则基线，发现在公平性和稳定性方面，规则型智能体表现良好但缺乏竞争，而MADQN定价激进。MADDPG则在支持市场竞争、保持较高公平性和稳定定价之间取得了平衡。研究强调MARL能引入传统方法无法捕捉的新兴战略行为，对未来动态定价发展具有指导意义。", "keywords": "多智能体强化学习, 动态定价, 供应链, MADDPG, 市场模拟", "comments": "这项研究的创新之处在于将多智能体强化学习应用于供应链动态定价，以解决传统单智能体或基于规则方法无法捕捉市场参与者之间战略互动的问题。其重要性在于提供了一个在真实模拟市场条件下评估不同MARL算法性能的基准，并揭示了MARL引入的新兴战略行为对定价策略的影响。特别是MADDPG在竞争、公平性和稳定性之间的平衡表现，为未来动态定价系统的开发提供了有价值的见解。"}}
{"id": "2507.02546", "title": "MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details", "authors": ["Ruicheng Wang", "Sicheng Xu", "Yue Dong", "Yu Deng", "Jianfeng Xiang", "Zelong Lv", "Guangzhong Sun", "Xin Tong", "Jiaolong Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.02546v1", "summary": "We propose MoGe-2, an advanced open-domain geometry estimation model that\nrecovers a metric scale 3D point map of a scene from a single image. Our method\nbuilds upon the recent monocular geometry estimation approach, MoGe, which\npredicts affine-invariant point maps with unknown scales. We explore effective\nstrategies to extend MoGe for metric geometry prediction without compromising\nthe relative geometry accuracy provided by the affine-invariant point\nrepresentation. Additionally, we discover that noise and errors in real data\ndiminish fine-grained detail in the predicted geometry. We address this by\ndeveloping a unified data refinement approach that filters and completes real\ndata from different sources using sharp synthetic labels, significantly\nenhancing the granularity of the reconstructed geometry while maintaining the\noverall accuracy. We train our model on a large corpus of mixed datasets and\nconducted comprehensive evaluations, demonstrating its superior performance in\nachieving accurate relative geometry, precise metric scale, and fine-grained\ndetail recovery -- capabilities that no previous methods have simultaneously\nachieved.", "comment": "Project page: https://wangrc.site/MoGe2Page/", "pdf_url": "http://arxiv.org/pdf/2507.02546v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MoGe-2：具有度量尺度和锐利细节的精确单目几何", "tldr": "MoGe-2是一个先进的单目几何估计模型，能从单张图像中恢复度量尺度的3D点图，同时保持高精度和细节。", "motivation": "现有的单目几何估计方法（如MoGe）预测的仿射不变点图尺度未知，且真实数据中的噪声和误差会降低预测几何的精细细节，导致细节丢失。", "method": "MoGe-2在MoGe基础上，探索了实现度量几何预测的有效策略，同时不损害相对几何精度。它还开发了一种统一的数据精炼方法，利用锐利合成标签过滤和完成来自不同来源的真实数据，以显著增强重建几何的粒度。", "result": "MoGe-2在大型混合数据集上训练并进行全面评估，展示了其在实现精确相对几何、精确度量尺度和精细细节恢复方面的卓越性能，这些能力是以前的方法未能同时实现的。", "conclusion": "MoGe-2成功地同时实现了精确的相对几何、精确的度量尺度和精细的细节恢复，解决了现有方法的局限性。", "translation": "我们提出了MoGe-2，一个先进的开放域几何估计模型，它能从单张图像中恢复场景的度量尺度3D点图。我们的方法建立在最近的单目几何估计方法MoGe之上，该方法预测具有未知尺度的仿射不变点图。我们探索了有效的策略来扩展MoGe，以实现度量几何预测，同时不损害仿射不变点表示所提供的相对几何精度。此外，我们发现真实数据中的噪声和误差会降低预测几何的精细细节。我们通过开发一种统一的数据精炼方法来解决这个问题，该方法使用锐利的合成标签过滤和完成来自不同来源的真实数据，显著增强了重建几何的粒度，同时保持了整体精度。我们在大型混合数据集语料库上训练了我们的模型，并进行了全面的评估，展示了其在实现精确相对几何、精确度量尺度和精细细节恢复方面的卓越性能——这些能力是以前的方法未能同时实现的。", "summary": "MoGe-2是一种先进的单目几何估计模型，能够从单张图像中恢复具有度量尺度和锐利细节的3D点图。它在现有MoGe方法的基础上，通过探索度量几何预测策略和开发统一的数据精炼方法来解决尺度未知和细节丢失问题。实验证明，MoGe-2在相对几何精度、度量尺度准确性和细节恢复方面均表现出色，超越了现有方法。", "keywords": "单目几何估计, 度量尺度, 3D点图, 数据精炼, 细节恢复", "comments": "MoGe-2的创新之处在于其首次在一个模型中同时实现了精确的相对几何、精确的度量尺度和精细的细节恢复，这在单目几何估计领域是一个显著的进步。通过结合合成标签进行数据精炼，有效解决了真实数据噪声导致的细节丢失问题，提升了模型的实用性。"}}
{"id": "2507.02710", "title": "Fluid Democracy in Federated Data Aggregation", "authors": ["Aditya Vema Reddy Kesari", "Krishna Reddy Kesari"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 2025 Workshop on Collaborative and Federated Agentic Workflows", "url": "http://arxiv.org/abs/2507.02710v1", "summary": "Federated learning (FL) mechanisms typically require each client to transfer\ntheir weights to a central server, irrespective of how useful they are. In\norder to avoid wasteful data transfer costs from clients to the central server,\nwe propose the use of consensus based protocols to identify a subset of clients\nwith most useful model weights at each data transfer step. First, we explore\nthe application of existing fluid democracy protocols to FL from a performance\nstandpoint, comparing them with traditional one-person-one-vote (also known as\n1p1v or FedAvg). We propose a new fluid democracy protocol named\nviscous-retained democracy that always does better than 1p1v under the same\nassumptions as existing fluid democracy protocols while also not allowing for\ninfluence accumulation. Secondly, we identify weaknesses of fluid democracy\nprotocols from an adversarial lens in terms of their dependence on topology\nand/ or number of adversaries required to negatively impact the global model\nweights. To this effect, we propose an algorithm (FedVRD) that dynamically\nlimits the effect of adversaries while minimizing cost by leveraging the\ndelegation topology.", "comment": "ICML 2025 Workshop on Collaborative and Federated Agentic Workflows", "pdf_url": "http://arxiv.org/pdf/2507.02710v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "联邦数据聚合中的流体民主", "tldr": "本文提出在联邦学习中使用共识协议来选择最有用的客户端权重，以减少数据传输成本，并引入了一种新的流体民主协议（viscous-retained democracy）及其对应的FedVRD算法，旨在提高性能并抵御对抗性攻击。", "motivation": "联邦学习中，客户端无论权重有用与否都需传输至中央服务器，导致不必要的数据传输成本。", "method": "1. 探索将现有流体民主协议应用于联邦学习，并与传统的FedAvg进行性能比较。2. 提出一种新的流体民主协议，名为“viscous-retained democracy”，该协议在相同假设下优于1p1v，并能防止影响力累积。3. 从对抗性角度分析流体民主协议的弱点（依赖拓扑和/或对抗者数量），并提出FedVRD算法，利用委托拓扑动态限制对抗者的影响，同时最小化成本。", "result": "1. 提出的“viscous-retained democracy”协议在与现有流体民主协议相同的假设下，性能始终优于传统的1p1v（FedAvg）。2. 该协议能够防止影响力累积。3. FedVRD算法能够动态限制对抗者的影响，同时最小化成本。", "conclusion": "通过引入共识协议和新的流体民主变体（viscous-retained democracy及其FedVRD算法），可以有效减少联邦学习中的数据传输成本，提高性能，并增强对对抗性攻击的鲁棒性。", "translation": "联邦学习（FL）机制通常要求每个客户端将其权重传输到中央服务器，无论这些权重是否有用。为了避免客户端到中央服务器的浪费性数据传输成本，我们建议在每个数据传输步骤中使用基于共识的协议来识别具有最有用的模型权重的一部分客户端。首先，我们从性能角度探讨了现有流体民主协议在FL中的应用，并将其与传统的“一人一票”（也称为1p1v或FedAvg）进行比较。我们提出了一种名为“粘性保留民主”（viscous-retained democracy）的新流体民主协议，该协议在与现有流体民主协议相同的假设下始终优于1p1v，同时也不允许影响力累积。其次，我们从对抗性角度识别了流体民主协议的弱点，即它们对拓扑和/或负面影响全局模型权重所需的对抗者数量的依赖性。为此，我们提出了一种算法（FedVRD），该算法通过利用委托拓扑动态限制对抗者的影响，同时最小化成本。", "summary": "本文针对联邦学习中客户端权重传输的低效问题，提出了一种基于共识的协议来选择最有用的客户端子集进行数据传输，从而降低成本。研究首先评估了现有流体民主协议在联邦学习中的性能，并与FedAvg进行比较。在此基础上，提出了一种名为“粘性保留民主”（viscous-retained democracy）的新协议，该协议在性能上优于传统方法，并能有效防止影响力累积。此外，文章还分析了流体民主协议在对抗性环境下的脆弱性，并提出了FedVRD算法，该算法通过利用委托拓扑来动态限制对抗者的影响并最小化系统成本。", "keywords": "联邦学习, 流体民主, 数据聚合, 共识协议, 对抗性攻击", "comments": "本文创新性地将流体民主理念引入联邦学习，旨在优化数据传输效率和模型性能。提出的“粘性保留民主”协议不仅在性能上有所提升，更解决了传统流体民主可能存在的“影响力累积”问题，这一点非常重要。此外，论文还考虑了对抗性攻击，并提出了相应的防御机制，增强了协议的鲁棒性，体现了对实际应用中安全问题的关注。"}}
{"id": "2507.02565", "title": "Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning", "authors": ["Buzhen Huang", "Chen Li", "Chongyang Xu", "Dongyue Lu", "Jinnan Chen", "Yangang Wang", "Gim Hee Lee"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02565v1", "summary": "Due to visual ambiguities and inter-person occlusions, existing human pose\nestimation methods cannot recover plausible close interactions from in-the-wild\nvideos. Even state-of-the-art large foundation models~(\\eg, SAM) cannot\naccurately distinguish human semantics in such challenging scenarios. In this\nwork, we find that human appearance can provide a straightforward cue to\naddress these obstacles. Based on this observation, we propose a dual-branch\noptimization framework to reconstruct accurate interactive motions with\nplausible body contacts constrained by human appearances, social proxemics, and\nphysical laws. Specifically, we first train a diffusion model to learn the\nhuman proxemic behavior and pose prior knowledge. The trained network and two\noptimizable tensors are then incorporated into a dual-branch optimization\nframework to reconstruct human motions and appearances. Several constraints\nbased on 3D Gaussians, 2D keypoints, and mesh penetrations are also designed to\nassist the optimization. With the proxemics prior and diverse constraints, our\nmethod is capable of estimating accurate interactions from in-the-wild videos\ncaptured in complex environments. We further build a dataset with pseudo\nground-truth interaction annotations, which may promote future research on pose\nestimation and human behavior understanding. Experimental results on several\nbenchmarks demonstrate that our method outperforms existing approaches. The\ncode and data are available at https://www.buzhenhuang.com/works/CloseApp.html.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02565v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "基于外观和近体推理的紧密人体交互重建", "tldr": "本文提出了一种双分支优化框架，利用人体外观、社交近体学和物理定律来重建野外视频中准确的紧密人体交互，并构建了一个新的数据集。", "motivation": "现有的人体姿态估计方法由于视觉模糊和人际遮挡，无法从野外视频中恢复出可信的紧密交互，即使是先进的基础模型也无法准确区分复杂场景下的人体语义。", "method": "本文提出了一种双分支优化框架，利用人体外观、社交近体学和物理定律来重建准确的交互动作。具体来说，首先训练一个扩散模型来学习人体近体行为和姿态先验知识。然后，将训练好的网络和两个可优化张量整合到双分支优化框架中，以重建人体运动和外观。此外，还设计了基于3D高斯、2D关键点和网格穿透的多种约束来辅助优化。研究者还构建了一个带有伪真值交互注释的数据集。", "result": "实验结果表明，该方法在多个基准测试中优于现有方法，能够从复杂环境下的野外视频中估计出准确的交互。", "conclusion": "通过结合近体先验和多样化约束，本文提出的方法能够有效解决野外视频中紧密人体交互重建的挑战，并且构建的新数据集有望促进姿态估计和人体行为理解的未来研究。", "translation": "由于视觉模糊和人际遮挡，现有的人体姿态估计方法无法从野外视频中恢复出可信的紧密交互。即使是最先进的大型基础模型（例如SAM）也无法在这些具有挑战性的场景中准确区分人体语义。在这项工作中，我们发现人体外观可以提供直接的线索来解决这些障碍。基于这一观察，我们提出了一种双分支优化框架，以重建准确的交互动作，并通过人体外观、社交近体学和物理定律的约束来确保可信的身体接触。具体来说，我们首先训练一个扩散模型来学习人体近体行为和姿态先验知识。然后，将训练好的网络和两个可优化张量整合到双分支优化框架中，以重建人体运动和外观。还设计了基于3D高斯、2D关键点和网格穿透的多种约束来辅助优化。凭借近体先验和多样化约束，我们的方法能够从复杂环境中捕获的野外视频中估计出准确的交互。我们进一步构建了一个带有伪真值交互注释的数据集，这可能会促进未来在姿态估计和人体行为理解方面的研究。在多个基准测试上的实验结果表明，我们的方法优于现有方法。代码和数据可在https://www.buzhenhuang.com/works/CloseApp.html获取。", "summary": "本文提出了一种新颖的双分支优化框架，用于从野外视频中重建准确的紧密人体交互。该方法通过结合人体外观、社交近体学和物理定律作为约束，解决了现有方法在视觉模糊和遮挡下的局限性。研究中利用扩散模型学习人体近体行为和姿态先验，并设计了多种约束来辅助优化。此外，还构建了一个带有伪真值注释的数据集，以促进相关研究。实验证明，该方法在性能上超越了现有方法。", "keywords": "人体交互重建, 姿态估计, 近体学, 外观推理, 扩散模型", "comments": "本文的创新点在于提出了一个将人体外观和社交近体学（proxemics）融入紧密人体交互重建的框架，有效解决了野外视频中视觉模糊和遮挡的难题。通过引入扩散模型学习姿态先验和构建带有伪真值注释的数据集，为该领域的研究提供了新的思路和资源。其方法在复杂场景下的表现优于现有技术，具有重要的实际应用潜力。"}}
{"id": "2507.02712", "title": "A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control", "authors": ["Zilin Kang", "Chenyuan Hu", "Yu Luo", "Zhecheng Yuan", "Ruijie Zheng", "Huazhe Xu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02712v1", "summary": "Deep reinforcement learning for continuous control has recently achieved\nimpressive progress. However, existing methods often suffer from primacy bias,\na tendency to overfit early experiences stored in the replay buffer, which\nlimits an RL agent's sample efficiency and generalizability. In contrast,\nhumans are less susceptible to such bias, partly due to infantile amnesia,\nwhere the formation of new neurons disrupts early memory traces, leading to the\nforgetting of initial experiences. Inspired by this dual processes of\nforgetting and growing in neuroscience, in this paper, we propose Forget and\nGrow (FoG), a new deep RL algorithm with two mechanisms introduced. First,\nExperience Replay Decay (ER Decay) \"forgetting early experience\", which\nbalances memory by gradually reducing the influence of early experiences.\nSecond, Network Expansion, \"growing neural capacity\", which enhances agents'\ncapability to exploit the patterns of existing data by dynamically adding new\nparameters during training. Empirical results on four major continuous control\nbenchmarks with more than 40 tasks demonstrate the superior performance of FoG\nagainst SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02712v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "深度强化学习连续控制中的“遗忘与成长”策略", "tldr": "提出FoG算法，通过经验回放衰减和网络扩展解决深度强化学习中的早期经验过拟合问题，在连续控制任务上表现优异。", "motivation": "现有深度强化学习方法在连续控制中存在早期经验过拟合（primacy bias）问题，限制了样本效率和泛化能力。受神经科学中遗忘与成长双过程启发，旨在解决此问题。", "method": "提出Forget and Grow (FoG) 算法，包含两个机制：1. 经验回放衰减（ER Decay），通过逐渐减少早期经验的影响来平衡记忆，实现“遗忘早期经验”。2. 网络扩展，在训练期间动态添加新参数，增强代理利用现有数据模式的能力，实现“增长神经网络容量”。", "result": "在四个主要连续控制基准（超过40个任务）上，FoG算法表现优于现有最先进的深度强化学习算法，包括BRO、SimBa和TD-MPC2。", "conclusion": "FoG算法通过模拟神经科学中的遗忘与成长机制，有效解决了深度强化学习中的早期经验过拟合问题，显著提升了连续控制任务的性能。", "translation": "深度强化学习在连续控制领域最近取得了显著进展。然而，现有方法常受“首因偏差”困扰，即过度拟合回放缓冲区中存储的早期经验，这限制了强化学习代理的样本效率和泛化能力。相比之下，人类较少受此偏差影响，部分原因在于婴儿期失忆症，即新神经元的形成会扰乱早期记忆痕迹，导致初始经验的遗忘。受神经科学中遗忘与成长双过程的启发，本文提出了Forget and Grow (FoG) 算法，这是一种新的深度强化学习算法，引入了两种机制。首先是经验回放衰减（ER Decay），它通过逐渐减少早期经验的影响来平衡记忆，实现“遗忘早期经验”。其次是网络扩展，它通过在训练期间动态添加新参数来增强代理利用现有数据模式的能力，实现“增长神经网络容量”。在四个主要连续控制基准（超过40个任务）上的实证结果表明，FoG算法相对于现有最先进的深度强化学习算法（包括BRO、SimBa和TD-MPC2）表现出卓越的性能。", "summary": "本文提出了一种名为Forget and Grow (FoG) 的新型深度强化学习算法，旨在解决连续控制中因早期经验过拟合导致的首因偏差问题。受神经科学中遗忘与成长机制的启发，FoG引入了经验回放衰减（ER Decay）来减少早期经验的影响，以及网络扩展来动态增加网络容量。实验结果表明，FoG在多个连续控制基准测试中性能优于现有先进算法。", "keywords": "深度强化学习, 连续控制, 首因偏差, 经验回放, 网络扩展", "comments": "这篇论文的创新点在于将神经科学中的“遗忘与成长”概念引入深度强化学习，以解决困扰RL的首因偏差问题。通过经验回放衰减和网络动态扩展，FoG算法提供了一种新颖的机制来提高RL代理的样本效率和泛化能力。其在多个连续控制任务上的优越表现证明了该策略的有效性。"}}
{"id": "2507.02576", "title": "Parametric shape models for vessels learned from segmentations via differentiable voxelization", "authors": ["Alina F. Dima", "Suprosanna Shit", "Huaqi Qiu", "Robbie Holland", "Tamara T. Mueller", "Fabio Antonio Musio", "Kaiyuan Yang", "Bjoern Menze", "Rickmer Braren", "Marcus Makowski", "Daniel Rueckert"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 6 figures", "url": "http://arxiv.org/abs/2507.02576v1", "summary": "Vessels are complex structures in the body that have been studied extensively\nin multiple representations. While voxelization is the most common of them,\nmeshes and parametric models are critical in various applications due to their\ndesirable properties. However, these representations are typically extracted\nthrough segmentations and used disjointly from each other. We propose a\nframework that joins the three representations under differentiable\ntransformations. By leveraging differentiable voxelization, we automatically\nextract a parametric shape model of the vessels through shape-to-segmentation\nfitting, where we learn shape parameters from segmentations without the\nexplicit need for ground-truth shape parameters. The vessel is parametrized as\ncenterlines and radii using cubic B-splines, ensuring smoothness and continuity\nby construction. Meshes are differentiably extracted from the learned shape\nparameters, resulting in high-fidelity meshes that can be manipulated post-fit.\nOur method can accurately capture the geometry of complex vessels, as\ndemonstrated by the volumetric fits in experiments on aortas, aneurysms, and\nbrain vessels.", "comment": "15 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.02576v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "通过可微分体素化从分割中学习血管的参数化形状模型", "tldr": "提出了一种可微分框架，通过可微分体素化从分割中直接学习血管的参数化形状模型，连接了体素化、网格和参数化表示。", "motivation": "尽管体素化是血管研究中最常见的表示方法，但网格和参数化模型因其特性在各种应用中至关重要。然而，这些表示通常通过分割提取，并且彼此独立使用。因此，需要一个框架来连接这三种表示，并通过可微分的方法从分割中自动提取参数化形状模型，而无需显式的真值形状参数。", "method": "提出了一种在可微分变换下连接体素化、网格和参数化模型的三合一框架。该框架利用可微分体素化，通过形状到分割的拟合，从分割中自动提取血管的参数化形状模型，其中血管被参数化为使用三次B样条的中心线和半径，确保了平滑性和连续性。此外，网格可以从学习到的形状参数中可微分地提取，从而生成可进行后处理的高保真网格。", "result": "该方法能够准确捕捉复杂血管的几何形状，在主动脉、动脉瘤和脑血管的实验中通过体积拟合得到了验证。", "conclusion": "所提出的可微分框架有效地整合了不同的血管表示，并能够直接从分割中准确学习参数化血管形状，在复杂血管几何形状上表现出强大的性能。", "translation": "血管是身体中复杂的结构，已在多种表示形式下进行了广泛研究。尽管体素化是其中最常见的，但网格和参数化模型因其理想的特性在各种应用中至关重要。然而，这些表示通常通过分割提取并彼此独立使用。我们提出了一个框架，通过可微分变换将这三种表示结合起来。通过利用可微分体素化，我们通过形状到分割的拟合，自动从分割中提取血管的参数化形状模型，在此过程中我们从分割中学习形状参数，而无需显式的真值形状参数。血管使用三次B样条参数化为中心线和半径，通过构造确保了平滑性和连续性。网格可以从学习到的形状参数中可微分地提取，从而生成可在拟合后进行操作的高保真网格。我们的方法可以准确捕捉复杂血管的几何形状，这在主动脉、动脉瘤和脑血管的实验中通过体积拟合得到了证明。", "summary": "本文介绍了一种新颖的可微分框架，用于统一血管表示中的体素化、网格和参数化模型。该框架利用可微分体素化，通过形状到分割的拟合，直接从分割中学习血管的参数化形状（中心线和半径），从而无需真值形状参数。该方法使用三次B样条进行参数化，并能可微分地提取高保真网格。在主动脉、动脉瘤和脑血管上的实验证明了其在捕捉复杂血管几何形状方面的准确性。", "keywords": "血管分割, 参数化形状模型, 可微分体素化, 医学图像, B样条", "comments": "该论文的创新之处在于提出了一个可微分框架，它将三种不同的血管表示（体素化、网格、参数化模型）连接起来。这使得可以直接从分割中自动提取和学习参数化形状模型，这是一个重要的进展，因为它消除了对显式真值形状参数的需求。可微分体素化和使用三次B样条进行平滑参数化是关键的技术贡献。此外，在拟合后能够生成高保真、可操作的网格也增加了实用价值。"}}
{"id": "2507.02715", "title": "A Comprehensive Machine Learning Framework for Micromobility Demand Prediction", "authors": ["Omri Porat", "Michael Fire", "Eran Ben-Elia"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02715v1", "summary": "Dockless e-scooters, a key micromobility service, have emerged as\neco-friendly and flexible urban transport alternatives. These services improve\nfirst and last-mile connectivity, reduce congestion and emissions, and\ncomplement public transport for short-distance travel. However, effective\nmanagement of these services depends on accurate demand prediction, which is\ncrucial for optimal fleet distribution and infrastructure planning. While\nprevious studies have focused on analyzing spatial or temporal factors in\nisolation, this study introduces a framework that integrates spatial, temporal,\nand network dependencies for improved micromobility demand forecasting. This\nintegration enhances accuracy while providing deeper insights into urban\nmicromobility usage patterns. Our framework improves demand prediction accuracy\nby 27 to 49% over baseline models, demonstrating its effectiveness in capturing\nmicromobility demand patterns. These findings support data-driven micromobility\nmanagement, enabling optimized fleet distribution, cost reduction, and\nsustainable urban planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02715v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "微出行需求预测的综合机器学习框架", "tldr": "本研究提出一个综合机器学习框架，整合空间、时间及网络依赖性，显著提高了微出行需求预测的准确性，有助于优化共享电动滑板车等微出行服务的管理。", "motivation": "有效的微出行服务管理（如共享电动滑板车）依赖于准确的需求预测，这对于优化车队分配和基础设施规划至关重要。现有研究多独立分析空间或时间因素，未能充分捕捉复杂的依赖关系。", "method": "本研究提出了一个机器学习框架，该框架整合了空间、时间及网络依赖性，以改进微出行需求预测。此集成方法旨在提高预测准确性并深入了解城市微出行使用模式。", "result": "该框架将需求预测准确性比基线模型提高了27%到49%，有效捕捉了微出行需求模式。", "conclusion": "研究结果支持数据驱动的微出行管理，有助于优化车队分配、降低成本和实现可持续的城市规划。", "translation": "无桩式电动滑板车作为一项关键的微出行服务，已成为环保且灵活的城市交通替代方案。这些服务改善了第一英里和最后一英里的连接性，减少了拥堵和排放，并补充了公共交通的短途出行。然而，这些服务的有效管理取决于准确的需求预测，这对于优化车队分配和基础设施规划至关重要。虽然以往的研究主要集中于孤立地分析空间或时间因素，但本研究引入了一个框架，该框架整合了空间、时间以及网络依赖性，以改进微出行需求预测。这种整合在提高准确性的同时，也为城市微出行使用模式提供了更深入的见解。我们的框架将需求预测准确性比基线模型提高了27%到49%，证明了其在捕捉微出行需求模式方面的有效性。这些发现支持数据驱动的微出行管理，从而实现优化的车队分配、成本降低和可持续的城市规划。", "summary": "本研究提出了一个针对微出行需求预测的综合机器学习框架，旨在解决现有研究中空间和时间因素孤立分析的局限性。该框架创新性地整合了空间、时间及网络依赖性，显著提升了需求预测的准确性，相较于基线模型提高了27%至49%。这为数据驱动的微出行管理提供了有力支持，有助于优化共享电动滑板车等服务的车队分配、降低运营成本并促进城市可持续发展。", "keywords": "微出行, 需求预测, 机器学习, 共享电动滑板车, 城市交通", "comments": "本研究的创新点在于其综合性方法，将空间、时间与网络依赖性整合到一个机器学习框架中，以解决微出行需求预测的挑战。这种全面的方法超越了现有研究的局限性，显著提高了预测准确性。其重要性在于，准确的需求预测对于优化共享微出行服务的运营至关重要，能够支持更有效的车队管理、资源分配和城市规划，从而促进可持续的城市交通发展。"}}
{"id": "2507.02581", "title": "Structure-aware Semantic Discrepancy and Consistency for 3D Medical Image Self-supervised Learning", "authors": ["Tan Pan", "Zhaorui Tan", "Kaiyu Guo", "Dongli Xu", "Weidi Xu", "Chen Jiang", "Xin Guo", "Yuan Qi", "Yuan Cheng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV25", "url": "http://arxiv.org/abs/2507.02581v1", "summary": "3D medical image self-supervised learning (mSSL) holds great promise for\nmedical analysis. Effectively supporting broader applications requires\nconsidering anatomical structure variations in location, scale, and morphology,\nwhich are crucial for capturing meaningful distinctions. However, previous mSSL\nmethods partition images with fixed-size patches, often ignoring the structure\nvariations. In this work, we introduce a novel perspective on 3D medical images\nwith the goal of learning structure-aware representations. We assume that\npatches within the same structure share the same semantics (semantic\nconsistency) while those from different structures exhibit distinct semantics\n(semantic discrepancy). Based on this assumption, we propose an mSSL framework\nnamed $S^2DC$, achieving Structure-aware Semantic Discrepancy and Consistency\nin two steps. First, $S^2DC$ enforces distinct representations for different\npatches to increase semantic discrepancy by leveraging an optimal transport\nstrategy. Second, $S^2DC$ advances semantic consistency at the structural level\nbased on neighborhood similarity distribution. By bridging patch-level and\nstructure-level representations, $S^2DC$ achieves structure-aware\nrepresentations. Thoroughly evaluated across 10 datasets, 4 tasks, and 3\nmodalities, our proposed method consistently outperforms the state-of-the-art\nmethods in mSSL.", "comment": "Accepted by ICCV25", "pdf_url": "http://arxiv.org/pdf/2507.02581v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "三维医学图像自监督学习中的结构感知语义差异与一致性", "tldr": "本文提出了S²DC，一种新颖的三维医学图像自监督学习框架，通过利用结构感知语义差异和一致性，在多个数据集、任务和模态上超越了现有最先进的方法。", "motivation": "以前的三维医学图像自监督学习（mSSL）方法使用固定大小的图像块，忽略了解剖结构在位置、尺度和形态上的变异，而这些变异对于捕捉有意义的区别至关重要。", "method": "本文提出了名为S²DC的结构感知mSSL框架。该框架基于同一结构内的图像块共享语义（语义一致性）而不同结构间的图像块表现出不同语义（语义差异）的假设。S²DC通过两个步骤实现此目标：首先，利用最优传输策略强制不同图像块具有不同的表示以增加语义差异；其次，基于邻域相似性分布在结构层面推进语义一致性。该方法通过连接图像块级和结构级表示来实现结构感知表示。", "result": "所提出的方法在10个数据集、4个任务和3种模态上进行了全面评估，结果表明其在mSSL中始终优于现有最先进的方法。", "conclusion": "S²DC框架有效学习了三维医学图像的结构感知表示，解决了以往方法忽略解剖结构变异的局限性，在各种医学分析任务中表现出卓越的性能。", "translation": "三维医学图像自监督学习（mSSL）在医学分析中具有广阔前景。有效支持更广泛的应用需要考虑解剖结构在位置、尺度和形态上的变异，这些变异对于捕捉有意义的区别至关重要。然而，以前的mSSL方法使用固定大小的补丁分割图像，常常忽略结构变异。在这项工作中，我们引入了一种关于三维医学图像的新颖视角，旨在学习结构感知的表示。我们假设同一结构内的补丁共享相同的语义（语义一致性），而来自不同结构的补丁表现出不同的语义（语义差异）。基于这一假设，我们提出了一个名为S²DC的mSSL框架，通过两个步骤实现结构感知语义差异和一致性。首先，S²DC通过利用最优传输策略强制不同补丁具有不同的表示，以增加语义差异。其次，S²DC基于邻域相似性分布在结构层面推进语义一致性。通过连接补丁级和结构级表示，S²DC实现了结构感知表示。我们的方法在10个数据集、4个任务和3种模态上进行了全面评估，始终优于当前的mSSL先进方法。", "summary": "本文提出了一种新颖的三维医学图像自监督学习框架S²DC，旨在解决传统方法忽视解剖结构变异的问题。S²DC通过强制不同结构间的语义差异（利用最优传输）和同一结构内的语义一致性（基于邻域相似性分布）来学习结构感知表示。该方法成功桥接了图像块级和结构级表示，并在多个数据集、任务和模态上展现出超越现有最先进方法的性能。", "keywords": "三维医学图像, 自监督学习, 结构感知, 语义差异, 语义一致性", "comments": "该论文通过明确引入解剖结构感知，为三维医学图像自监督学习提供了一种创新方法。其核心思想是同时强制语义差异和一致性，特别是连接图像块级和结构级表示，是对传统固定图像块方法的重大改进。在不同基准测试中持续优于现有方法，突显了其在实际应用中的重要性和潜力。"}}
{"id": "2507.02724", "title": "Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms", "authors": ["Shiyi Liu", "Buwen Liang", "Yuetong Fang", "Zixuan Jiang", "Renjing Xu"], "categories": ["cs.LG", "q-bio.BM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02724v1", "summary": "Recent advances in AI for science have highlighted the power of contrastive\nlearning in bridging heterogeneous biological data modalities. Building on this\nparadigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction\nacross Organisms), a hierarchical contrastive framework for protein-protein\ninteraction(PPI) prediction, where protein sequences and their hierarchical\nattributes are aligned through multi-tiered biological representation matching.\nThe proposed approach incorporates hierarchical contrastive loss functions that\nemulate the structured relationship among functional classes of proteins. The\nframework adaptively incorporates domain and family knowledge through a\ndata-driven penalty mechanism, enforcing consistency between the learned\nembedding space and the intrinsic hierarchy of protein functions. Experiments\non benchmark datasets demonstrate that HIPPO achieves state-of-the-art\nperformance, outperforming existing methods and showing robustness in low-data\nregimes. Notably, the model demonstrates strong zero-shot transferability to\nother species without retraining, enabling reliable PPI prediction and\nfunctional inference even in less characterized or rare organisms where\nexperimental data are limited. Further analysis reveals that hierarchical\nfeature fusion is critical for capturing conserved interaction determinants,\nsuch as binding motifs and functional annotations. This work advances\ncross-species PPI prediction and provides a unified framework for interaction\nprediction in scenarios with sparse or imbalanced multi-species data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02724v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "跨生物体蛋白质-蛋白质相互作用预测的分层多标签对比学习", "tldr": "HIPPO是一种分层对比学习框架，用于跨生物体PPI预测，通过多层生物表示匹配对蛋白质序列和属性进行对齐，并在基准数据集上实现了最先进的性能和强大的零样本迁移能力。", "motivation": "现有的方法可能在数据稀疏或不平衡的多物种场景下表现不佳，且在实验数据有限的生物体中进行可靠的PPI预测和功能推断具有挑战性。", "method": "提出了HIPPO（Hierarchical Protein-Protein interaction prediction across Organisms）框架，这是一种分层对比学习方法。它通过多层生物表示匹配来对齐蛋白质序列及其分层属性，并整合了模拟蛋白质功能类别之间结构化关系的分层对比损失函数。该框架还通过数据驱动的惩罚机制自适应地结合了结构域和家族知识，以确保学习到的嵌入空间与蛋白质功能的内在层次结构一致。", "result": "HIPPO在基准数据集上取得了最先进的性能，优于现有方法，并在低数据量情况下表现出鲁棒性。该模型还展示了强大的零样本迁移能力，无需重新训练即可应用于其他物种。进一步分析表明，分层特征融合对于捕获保守的相互作用决定因素（如结合基序和功能注释）至关重要。", "conclusion": "该工作推动了跨物种PPI预测的进展，并为数据稀疏或不平衡的多物种场景下的相互作用预测提供了一个统一的框架。", "translation": "**标题：** 跨生物体蛋白质-蛋白质相互作用预测的分层多标签对比学习\n\n**摘要：** 科学领域人工智能的最新进展突显了对比学习在连接异构生物数据模态方面的强大能力。在此范式基础上，我们提出了HIPPO（跨生物体分层蛋白质-蛋白质相互作用预测），这是一个用于蛋白质-蛋白质相互作用（PPI）预测的分层对比框架。在该框架中，蛋白质序列及其分层属性通过多层生物表示匹配进行对齐。所提出的方法结合了分层对比损失函数，该函数模拟了蛋白质功能类别之间的结构化关系。该框架通过数据驱动的惩罚机制自适应地整合结构域和家族知识，强制学习到的嵌入空间与蛋白质功能的内在层次结构保持一致。在基准数据集上的实验表明，HIPPO取得了最先进的性能，优于现有方法，并在低数据量情况下表现出鲁棒性。值得注意的是，该模型展示了强大的零样本迁移能力，无需重新训练即可迁移到其他物种，即使在实验数据有限的特征较少或稀有生物体中，也能实现可靠的PPI预测和功能推断。进一步分析表明，分层特征融合对于捕获保守的相互作用决定因素（如结合基序和功能注释）至关重要。这项工作推动了跨物种PPI预测的进展，并为数据稀疏或不平衡的多物种数据场景下的相互作用预测提供了一个统一的框架。", "summary": "本文提出了HIPPO，一个用于跨生物体蛋白质-蛋白质相互作用（PPI）预测的分层对比学习框架。该框架通过多层生物表示匹配对齐蛋白质序列和其分层属性，并引入分层对比损失和数据驱动的惩罚机制来整合蛋白质功能层次结构。实验证明HIPPO在基准数据集上达到最先进性能，并在低数据量和零样本跨物种迁移方面表现出色，为处理稀疏或不平衡的多物种PPI数据提供了统一方案。", "keywords": "蛋白质-蛋白质相互作用, 对比学习, 分层学习, 跨物种预测, 零样本迁移", "comments": "这篇论文的创新点在于提出了HIPPO框架，将分层对比学习应用于跨生物体PPI预测，并成功整合了蛋白质的层次化功能信息。其零样本迁移能力显著提升了模型在数据稀缺物种中的应用价值，为生物信息学领域，特别是蛋白质功能和相互作用研究，提供了新的强大工具。"}}
{"id": "2507.02591", "title": "AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video Understanding", "authors": ["Weili Xu", "Enxin Song", "Wenhao Chai", "Xuexiang Wen", "Tian Ye", "Gaoang Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.02591v1", "summary": "The challenge of long video understanding lies in its high computational\ncomplexity and prohibitive memory cost, since the memory and computation\nrequired by transformer-based LLMs scale quadratically with input sequence\nlength. We propose AuroraLong to address this challenge by replacing the LLM\ncomponent in MLLMs with a linear RNN language model that handles input sequence\nof arbitrary length with constant-size hidden states. To further increase\nthroughput and efficiency, we combine visual token merge with linear RNN models\nby reordering the visual tokens by their sizes in ascending order. Despite\nhaving only 2B parameters and being trained exclusively on public data,\nAuroraLong achieves performance comparable to Transformer-based models of\nsimilar size trained on private datasets across multiple video benchmarks. This\ndemonstrates the potential of efficient, linear RNNs to democratize long video\nunderstanding by lowering its computational entry barrier. To our best\nknowledge, we are the first to use a linear RNN based LLM backbone in a\nLLaVA-like model for open-ended video understanding.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.02591v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "AuroraLong：将RNN带回高效的开放式视频理解", "tldr": "AuroraLong通过将多模态大语言模型（MLLM）中的LLM组件替换为线性RNN语言模型，解决了长视频理解中的高计算复杂度和内存成本问题，实现了与Transformer模型相当的性能。", "motivation": "长视频理解面临高计算复杂度和高昂的内存成本，因为基于Transformer的大语言模型（LLM）所需的内存和计算量与输入序列长度呈平方关系。", "method": "提出AuroraLong，用线性RNN语言模型替代多模态大语言模型（MLLM）中的LLM组件，该模型能够以固定大小的隐藏状态处理任意长度的输入序列。为了进一步提高吞吐量和效率，通过按大小升序重新排序视觉token，将视觉token合并与线性RNN模型结合。", "result": "AuroraLong仅使用20亿参数并在公共数据集上训练，却在多个视频基准测试中取得了与在私有数据集上训练的、类似大小的Transformer模型相当的性能。", "conclusion": "高效的线性RNN有潜力通过降低计算门槛来普及长视频理解。本文首次在类似LLaVA的模型中使用基于线性RNN的LLM骨干进行开放式视频理解。", "translation": "长视频理解的挑战在于其高计算复杂度和高昂的内存成本，因为基于Transformer的大语言模型（LLM）所需的内存和计算量与输入序列长度呈平方关系。我们提出了AuroraLong来解决这一挑战，它用一个线性RNN语言模型取代了多模态大语言模型（MLLM）中的LLM组件，该模型能够以固定大小的隐藏状态处理任意长度的输入序列。为了进一步提高吞吐量和效率，我们通过按大小升序重新排序视觉token，将视觉token合并与线性RNN模型结合。尽管只有20亿参数并完全在公共数据上训练，AuroraLong在多个视频基准测试中取得了与在私有数据集上训练的、类似大小的Transformer模型相当的性能。这表明了高效、线性RNN通过降低计算门槛来普及长视频理解的潜力。据我们所知，我们是第一个在类似LLaVA的模型中使用基于线性RNN的LLM骨干进行开放式视频理解的。", "summary": "AuroraLong旨在解决长视频理解中Transformer模型带来的高计算和内存成本问题。该方法将多模态大语言模型（MLLM）的LLM部分替换为线性RNN语言模型，并结合视觉token合并技术，以实现高效且可扩展的视频处理。尽管参数量较小且仅使用公共数据训练，AuroraLong在性能上与大型Transformer模型相当，展示了线性RNN在降低长视频理解计算门槛方面的潜力。", "keywords": "长视频理解, 线性RNN, 计算效率, 多模态大语言模型, AuroraLong", "comments": "这项研究的创新点在于首次将线性RNN作为大语言模型（LLM）骨干应用于开放式视频理解的LLaVA类模型中，有效解决了长视频处理中Transformer模型固有的计算复杂度和内存成本问题。其重要性在于通过降低计算门槛，使得长视频理解技术更易于普及和应用，尤其是在资源受限的环境下。"}}
{"id": "2507.02602", "title": "Addressing Camera Sensors Faults in Vision-Based Navigation: Simulation and Dataset Development", "authors": ["Riccardo Gallon", "Fabian Schiemenz", "Alessandra Menicucci", "Eberhard Gill"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Submitted to Acta Astronautica", "url": "http://arxiv.org/abs/2507.02602v1", "summary": "The increasing importance of Vision-Based Navigation (VBN) algorithms in\nspace missions raises numerous challenges in ensuring their reliability and\noperational robustness. Sensor faults can lead to inaccurate outputs from\nnavigation algorithms or even complete data processing faults, potentially\ncompromising mission objectives. Artificial Intelligence (AI) offers a powerful\nsolution for detecting such faults, overcoming many of the limitations\nassociated with traditional fault detection methods. However, the primary\nobstacle to the adoption of AI in this context is the lack of sufficient and\nrepresentative datasets containing faulty image data.\n  This study addresses these challenges by focusing on an interplanetary\nexploration mission scenario. A comprehensive analysis of potential fault cases\nin camera sensors used within the VBN pipeline is presented. The causes and\neffects of these faults are systematically characterized, including their\nimpact on image quality and navigation algorithm performance, as well as\ncommonly employed mitigation strategies. To support this analysis, a simulation\nframework is introduced to recreate faulty conditions in synthetically\ngenerated images, enabling a systematic and controlled reproduction of faulty\ndata. The resulting dataset of fault-injected images provides a valuable tool\nfor training and testing AI-based fault detection algorithms. The final link to\nthe dataset will be added after an embargo period. For peer-reviewers, this\nprivate link is available.", "comment": "Submitted to Acta Astronautica", "pdf_url": "http://arxiv.org/pdf/2507.02602v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "解决基于视觉导航中的相机传感器故障：仿真与数据集开发", "tldr": "本研究通过开发一个模拟框架和生成故障注入图像数据集，解决了基于视觉导航中相机传感器故障检测的AI应用中数据不足的问题。", "motivation": "基于视觉的导航（VBN）算法在太空任务中的重要性日益增加，但传感器故障可能导致导航算法输出不准确甚至数据处理完全失效，从而危及任务目标。传统故障检测方法存在局限性，而人工智能（AI）虽能提供强大解决方案，但主要障碍在于缺乏足够且具代表性的包含故障图像数据的数据集。", "method": "本研究针对星际探索任务场景，对VBN管线中相机传感器的潜在故障案例进行了全面分析，系统地描述了这些故障的原因和影响（包括对图像质量和导航算法性能的影响），以及常用的缓解策略。为支持此分析，引入了一个仿真框架，用于在合成图像中重现故障条件，从而实现对故障数据的系统化和可控的再现。", "result": "获得了一个故障注入图像数据集，该数据集为训练和测试基于AI的故障检测算法提供了有价值的工具。", "conclusion": "创建的故障注入图像数据集是开发和验证基于AI的故障检测算法的重要资源，有助于提高基于视觉导航系统在太空任务中的可靠性和鲁棒性。", "translation": "基于视觉导航（VBN）算法在太空任务中日益增长的重要性，对其可靠性和操作鲁棒性提出了诸多挑战。传感器故障可能导致导航算法输出不准确，甚至完全的数据处理故障，从而可能危及任务目标。人工智能（AI）为检测此类故障提供了一种强大的解决方案，克服了传统故障检测方法的许多局限性。然而，AI在此背景下应用的主要障碍是缺乏足够且具代表性的包含故障图像数据的数据集。\n本研究通过聚焦星际探索任务场景来应对这些挑战。文中对VBN管线中使用的相机传感器的潜在故障案例进行了全面分析。系统地描述了这些故障的原因和影响，包括它们对图像质量和导航算法性能的影响，以及常用的缓解策略。为了支持这项分析，引入了一个仿真框架，用于在合成生成的图像中重现故障条件，从而实现对故障数据的系统化和可控的再现。由此产生的故障注入图像数据集为训练和测试基于AI的故障检测算法提供了宝贵工具。数据集的最终链接将在禁运期后添加。对于同行评审员，此私人链接可用。", "summary": "本文旨在解决基于视觉导航（VBN）中相机传感器故障检测的挑战，尤其针对AI方法因缺乏故障数据集而受限的问题。研究分析了星际探索任务中相机传感器的潜在故障，并系统地表征了其原因和影响。为生成必要的训练数据，作者开发了一个仿真框架，能够合成生成带有故障的图像数据，最终创建了一个有价值的故障注入图像数据集，以支持AI故障检测算法的开发与测试。", "keywords": "视觉导航, 传感器故障, 数据集, 仿真, 人工智能", "comments": "这项研究通过创建一个针对相机传感器故障的合成数据集，有效地解决了AI在视觉导航故障检测中面临的关键数据瓶颈问题。其创新点在于开发了一个可控的仿真框架来系统地生成代表性故障数据，这对于在实际太空任务中难以获取真实故障数据的场景至关重要。该数据集对于提高未来太空任务中VBN系统的可靠性和鲁棒性具有重要意义。"}}
{"id": "2507.02754", "title": "Fast and Simplex: 2-Simplicial Attention in Triton", "authors": ["Aurko Roy", "Timothy Chou", "Sai Surya Duvvuri", "Sijia Chen", "Jiecao Yu", "Xiaodong Wang", "Manzil Zaheer", "Rohan Anil"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, with appendix 25 pages", "url": "http://arxiv.org/abs/2507.02754v1", "summary": "Recent work has shown that training loss scales as a power law with both\nmodel size and the number of tokens, and that achieving compute-optimal models\nrequires scaling model size and token count together. However, these scaling\nlaws assume an infinite supply of data and apply primarily in compute-bound\nsettings. As modern large language models increasingly rely on massive\ninternet-scale datasets, the assumption that they are compute-bound is becoming\nless valid. This shift highlights the need for architectures that prioritize\ntoken efficiency.\n  In this work, we investigate the use of the 2-simplicial Transformer, an\narchitecture that generalizes standard dot-product attention to trilinear\nfunctions through an efficient Triton kernel implementation. We demonstrate\nthat the 2-simplicial Transformer achieves better token efficiency than\nstandard Transformers: for a fixed token budget, similarly sized models\noutperform their dot-product counterparts on tasks involving mathematics,\ncoding, reasoning, and logic. We quantify these gains by demonstrating that\n$2$-simplicial attention changes the exponent in the scaling laws for knowledge\nand reasoning tasks compared to dot product attention.", "comment": "10 pages, with appendix 25 pages", "pdf_url": "http://arxiv.org/pdf/2507.02754v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "快速而简单：Triton 中的 2-单纯注意力", "tldr": "本文引入了 2-单纯 Transformer，它通过高效的 Triton 内核实现，在数学、编码、推理和逻辑任务上比标准 Transformer 具有更好的令牌效率，并改变了知识和推理任务的缩放定律指数。", "motivation": "当前的缩放定律假设数据无限且主要适用于计算受限环境，但随着大型语言模型越来越依赖大规模互联网数据集，计算受限的假设变得不那么有效。这凸显了对优先考虑令牌效率的架构的需求。", "method": "本文研究了 2-单纯 Transformer 的使用，这是一种通过高效的 Triton 内核实现将标准点积注意力推广到三线性函数的架构。", "result": "2-单纯 Transformer 比标准 Transformer 实现了更好的令牌效率：在固定令牌预算下，类似大小的模型在涉及数学、编码、推理和逻辑的任务上优于其点积对应物。研究表明，2-单纯注意力改变了知识和推理任务的缩放定律中的指数。", "conclusion": "2-单纯注意力通过提高令牌效率和改变特定任务的缩放定律指数，为大型语言模型提供了比标准点积注意力更优的性能。", "translation": "最近的工作表明，训练损失与模型大小和令牌数量呈幂律关系，并且实现计算最优模型需要同时缩放模型大小和令牌数量。然而，这些缩放定律假设数据供应无限，并且主要适用于计算受限的环境。随着现代大型语言模型越来越依赖大规模互联网数据集，它们是计算受限的假设变得越来越不成立。这种转变凸显了对优先考虑令牌效率的架构的需求。\n在这项工作中，我们研究了 2-单纯 Transformer 的使用，这是一种通过高效的 Triton 内核实现将标准点积注意力推广到三线性函数的架构。我们证明了 2-单纯 Transformer 比标准 Transformer 实现了更好的令牌效率：在固定令牌预算下，类似大小的模型在涉及数学、编码、推理和逻辑的任务上优于其点积对应物。我们通过证明 2-单纯注意力与点积注意力相比，改变了知识和推理任务的缩放定律中的指数来量化这些收益。", "summary": "本文提出并研究了 2-单纯 Transformer，这是一种将标准点积注意力推广到三线性函数的新架构，并通过高效的 Triton 内核实现。研究发现，在固定令牌预算下，2-单纯 Transformer 在数学、编码、推理和逻辑等任务上比传统 Transformer 表现出更高的令牌效率，并能改变这些任务的缩放定律指数，从而在计算效率日益重要的背景下，为大型语言模型提供了更优的性能。", "keywords": "2-单纯注意力, Transformer, 令牌效率, 缩放定律, Triton", "comments": "该论文的创新点在于引入了 2-单纯注意力机制，并将其应用于 Transformer 模型，通过 Triton 内核实现了高效的计算。其重要性在于解决了当前大型语言模型在数据量日益增长的情况下，传统缩放定律的局限性，并提出了更具令牌效率的解决方案，这对于未来大型模型的训练和部署具有重要意义。"}}
{"id": "2507.02664", "title": "AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image Detection via Multimodal Large Language Models", "authors": ["Ziyin Zhou", "Yunpeng Luo", "Yuanchen Wu", "Ke Sun", "Jiayi Ji", "Ke Yan", "Shouhong Ding", "Xiaoshuai Sun", "Yunsheng Wu", "Rongrong Ji"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.02664v1", "summary": "The rapid development of AI-generated content (AIGC) technology has led to\nthe misuse of highly realistic AI-generated images (AIGI) in spreading\nmisinformation, posing a threat to public information security. Although\nexisting AIGI detection techniques are generally effective, they face two\nissues: 1) a lack of human-verifiable explanations, and 2) a lack of\ngeneralization in the latest generation technology. To address these issues, we\nintroduce a large-scale and comprehensive dataset, Holmes-Set, which includes\nthe Holmes-SFTSet, an instruction-tuning dataset with explanations on whether\nimages are AI-generated, and the Holmes-DPOSet, a human-aligned preference\ndataset. Our work introduces an efficient data annotation method called the\nMulti-Expert Jury, enhancing data generation through structured MLLM\nexplanations and quality control via cross-model evaluation, expert defect\nfiltering, and human preference modification. In addition, we propose Holmes\nPipeline, a meticulously designed three-stage training framework comprising\nvisual expert pre-training, supervised fine-tuning, and direct preference\noptimization. Holmes Pipeline adapts multimodal large language models (MLLMs)\nfor AIGI detection while generating human-verifiable and human-aligned\nexplanations, ultimately yielding our model AIGI-Holmes. During the inference\nstage, we introduce a collaborative decoding strategy that integrates the model\nperception of the visual expert with the semantic reasoning of MLLMs, further\nenhancing the generalization capabilities. Extensive experiments on three\nbenchmarks validate the effectiveness of our AIGI-Holmes.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.02664v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "AIGI-Holmes：迈向通过多模态大型语言模型实现可解释和可泛化的AI生成图像检测", "tldr": "AIGI-Holmes 提出了一种基于多模态大语言模型的方法来检测AI生成图像，并提供可解释的检测结果和更好的泛化能力。", "motivation": "现有AI生成图像（AIGI）检测技术存在两个问题：1) 缺乏人类可验证的解释；2) 在最新一代技术中缺乏泛化能力。AI生成内容（AIGC）的快速发展导致高逼真度AI生成图像被滥用于传播虚假信息，对公共信息安全构成威胁。", "method": "本文引入了一个大规模综合数据集Holmes-Set，包括带有AI生成图像解释的指令微调数据集Holmes-SFTSet和人类对齐偏好数据集Holmes-DPOSet。提出了一种高效的数据标注方法“多专家评审团”，通过结构化MLLM解释增强数据生成，并通过跨模型评估、专家缺陷过滤和人类偏好修改进行质量控制。此外，提出了Holmes Pipeline，一个精心设计的三阶段训练框架，包括视觉专家预训练、监督微调和直接偏好优化，以使多模态大语言模型（MLLMs）适应AIGI检测，并生成人类可验证和人类对齐的解释，最终得到模型AIGI-Holmes。在推理阶段，引入了一种协作解码策略，整合了视觉专家的模型感知和MLLMs的语义推理，进一步增强泛化能力。", "result": "在三个基准测试中进行了广泛实验，验证了AIGI-Holmes的有效性。", "conclusion": "AIGI-Holmes通过结合多模态大语言模型、大规模数据集和创新的训练及推理策略，成功解决了AI生成图像检测中解释性不足和泛化能力差的问题，为公共信息安全提供了新的解决方案。", "translation": "AI生成内容（AIGC）技术的快速发展导致高度逼真的AI生成图像（AIGI）被滥用于传播虚假信息，对公共信息安全构成威胁。尽管现有AIGI检测技术普遍有效，但它们面临两个问题：1）缺乏人类可验证的解释；2）在最新一代技术中缺乏泛化能力。为了解决这些问题，我们引入了一个大规模且全面的数据集Holmes-Set，其中包括带有图像是否为AI生成解释的指令微调数据集Holmes-SFTSet，以及人类对齐的偏好数据集Holmes-DPOSet。我们的工作引入了一种高效的数据标注方法，称为多专家评审团，通过结构化MLLM解释增强数据生成，并通过跨模型评估、专家缺陷过滤和人类偏好修改进行质量控制。此外，我们提出了Holmes Pipeline，一个精心设计的三阶段训练框架，包括视觉专家预训练、监督微调和直接偏好优化。Holmes Pipeline使多模态大型语言模型（MLLMs）适应AIGI检测，同时生成人类可验证和人类对齐的解释，最终产生我们的模型AIGI-Holmes。在推理阶段，我们引入了一种协作解码策略，整合了视觉专家的模型感知与MLLMs的语义推理，进一步增强了泛化能力。在三个基准测试中进行了广泛实验，验证了我们的AIGI-Holmes的有效性。", "summary": "本文提出了AIGI-Holmes，一个利用多模态大型语言模型（MLLMs）进行AI生成图像（AIGI）检测的模型，旨在解决现有方法缺乏可解释性和泛化能力的问题。研究构建了Holmes-Set大规模数据集（包含Holmes-SFTSet和Holmes-DPOSet），并开发了多专家评审团数据标注方法。AIGI-Holmes基于Holmes Pipeline三阶段训练框架（视觉专家预训练、监督微调、直接偏好优化）进行训练，并在推理阶段采用协作解码策略。实验证明，AIGI-Holmes在多个基准测试中表现出有效性，能够生成人类可验证的解释并提升泛化能力。", "keywords": "AI生成图像检测, 多模态大语言模型, 可解释性, 泛化能力, Holmes-Set", "comments": "该论文的创新点在于其结合了多模态大语言模型来解决AI生成图像检测中的两大挑战：可解释性和泛化能力。通过构建大规模的、带有解释的数据集和设计精巧的三阶段训练框架，AIGI-Holmes为AIGC时代的信息安全提供了一个有前景的解决方案。其引入的“多专家评审团”数据标注方法和“协作解码策略”也显示出方法上的独到之处。"}}
{"id": "2507.02762", "title": "Contextual Online Pricing with (Biased) Offline Data", "authors": ["Yixuan Zhang", "Ruihao Zhu", "Qiaomin Xie"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      47 pages, 4 figures", "url": "http://arxiv.org/abs/2507.02762v1", "summary": "We study contextual online pricing with biased offline data. For the scalar\nprice elasticity case, we identify the instance-dependent quantity $\\delta^2$\nthat measures how far the offline data lies from the (unknown) online optimum.\nWe show that the time length $T$, bias bound $V$, size $N$ and dispersion\n$\\lambda_{\\min}(\\hat{\\Sigma})$ of the offline data, and $\\delta^2$ jointly\ndetermine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty\n(OFU) policy achieves a minimax-optimal, instance-dependent regret bound\n$\\tilde{\\mathcal{O}}\\big(d\\sqrt{T} \\wedge (V^2T +\n\\frac{dT}{\\lambda_{\\min}(\\hat{\\Sigma}) + (N \\wedge T) \\delta^2})\\big)$. For\ngeneral price elasticity, we establish a worst-case, minimax-optimal rate\n$\\tilde{\\mathcal{O}}\\big(d\\sqrt{T} \\wedge (V^2T + \\frac{dT\n}{\\lambda_{\\min}(\\hat{\\Sigma})})\\big)$ and provide a generalized OFU algorithm\nthat attains it. When the bias bound $V$ is unknown, we design a robust variant\nthat always guarantees sub-linear regret and strictly improves on purely online\nmethods whenever the exact bias is small. These results deliver the first tight\nregret guarantees for contextual pricing in the presence of biased offline\ndata. Our techniques also transfer verbatim to stochastic linear bandits with\nbiased offline data, yielding analogous bounds.", "comment": "47 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.02762v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "上下文在线定价与（有偏）离线数据", "tldr": "本文研究了有偏离线数据下的上下文在线定价问题，识别了衡量偏差的关键量，并提出了基于不确定性乐观（OFU）的策略，首次为该问题提供了紧致的遗憾界限。", "motivation": "研究在有偏离线数据存在的情况下，如何进行上下文在线定价，以及如何量化和处理这种偏差对统计复杂度的影响。", "method": "针对标量价格弹性情况，识别了衡量离线数据与在线最优值距离的实例依赖量 $\\delta^2$。提出了一种基于不确定性乐观（OFU）的策略，并推广到一般的价格弹性情况。当偏差边界 $V$ 未知时，设计了一种鲁棒变体。相关技术也可用于带有偏置离线数据的随机线性强盗问题。", "result": "OFU策略实现了极小化最优、实例依赖的遗憾界限 $\\tilde{\\mathcal{O}}\\big(d\\sqrt{T} \\wedge (V^2T + \\frac{dT}{\\lambda_{\\min}(\\hat{\\Sigma}) + (N \\wedge T) \\delta^2})\\big)$。对于一般的价格弹性，建立了最坏情况下的极小化最优速率 $\\tilde{\\mathcal{O}}\\big(d\\sqrt{T} \\wedge (V^2T + \\frac{dT }{\\lambda_{\\min}(\\hat{\\Sigma})})\\big)$。当偏差边界 $V$ 未知时，设计的鲁棒变体保证了亚线性遗憾，并在精确偏差较小时显著优于纯在线方法。这些结果首次为有偏离线数据存在下的上下文定价提供了紧致的遗憾保证。", "conclusion": "本文为有偏离线数据存在下的上下文在线定价问题提供了第一个紧致的遗憾界限，并通过OFU策略及其变体实现了极小化最优性能，证明了在存在偏差数据时也能实现有效的在线学习，且技术可推广到随机线性强盗问题。", "translation": "我们研究了有偏离线数据下的上下文在线定价问题。对于标量价格弹性情况，我们确定了实例依赖量 $\\delta^2$，它衡量了离线数据与（未知）在线最优值之间的距离。我们表明，时间长度 $T$、偏差边界 $V$、离线数据大小 $N$ 和离线数据离散度 $\\lambda_{\\min}(\\hat{\\Sigma})$，以及 $\\delta^2$ 共同决定了统计复杂度。一种不确定性乐观（OFU）策略实现了极小化最优、实例依赖的遗憾界限 $\\tilde{\\mathcal{O}}\\big(d\\sqrt{T} \\wedge (V^2T + \\frac{dT}{\\lambda_{\\min}(\\hat{\\Sigma}) + (N \\wedge T) \\delta^2})\\big)$。对于一般的价格弹性，我们建立了最坏情况下的极小化最优速率 $\\tilde{\\mathcal{O}}\\big(d\\sqrt{T} \\wedge (V^2T + \\frac{dT }{\\lambda_{\\min}(\\hat{\\Sigma})})\\big)$，并提供了一种广义OFU算法来实现它。当偏差边界 $V$ 未知时，我们设计了一种鲁棒变体，它总是保证亚线性遗憾，并且在精确偏差较小时严格优于纯在线方法。这些结果首次为有偏离线数据存在下的上下文定价提供了紧致的遗憾保证。我们的技术也原封不动地转移到带有偏置离线数据的随机线性强盗问题，产生了类似的界限。", "summary": "本文研究了在有偏离线数据情境下的上下文在线定价问题。针对标量和一般价格弹性情况，论文识别了关键的偏差衡量指标 $\\delta^2$，并提出了基于不确定性乐观（OFU）的算法，证明了其达到了极小化最优的遗憾界限。当偏差边界未知时，还设计了鲁棒变体以保证亚线性遗憾。这些工作首次为有偏离线数据下的上下文定价提供了紧致的遗憾保证，并可推广到随机线性强盗问题。", "keywords": "上下文在线定价, 有偏离线数据, 遗憾界限, 不确定性乐观 (OFU), 随机线性强盗", "comments": "这篇论文的创新点在于首次为有偏离线数据下的上下文在线定价问题提供了紧致的遗憾界限，并通过引入实例依赖的偏差度量 $\\delta^2$ 和设计相应的OFU算法，有效地处理了离线数据偏差带来的挑战。其方法不仅理论上严谨，具有极小化最优的性能，而且在偏差边界未知的情况下也提供了实用的鲁棒解决方案，具有重要的理论和实际意义。"}}
{"id": "2507.02686", "title": "Learning few-step posterior samplers by unfolding and distillation of diffusion models", "authors": ["Charlesquin Kemajou Mbakam", "Jonathan Spence", "Marcelo Pereyra"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      28 pages, 16 figures, 10 tables", "url": "http://arxiv.org/abs/2507.02686v1", "summary": "Diffusion models (DMs) have emerged as powerful image priors in Bayesian\ncomputational imaging. Two primary strategies have been proposed for leveraging\nDMs in this context: Plug-and-Play methods, which are zero-shot and highly\nflexible but rely on approximations; and specialized conditional DMs, which\nachieve higher accuracy and faster inference for specific tasks through\nsupervised training. In this work, we introduce a novel framework that\nintegrates deep unfolding and model distillation to transform a DM image prior\ninto a few-step conditional model for posterior sampling. A central innovation\nof our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm\n- specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et\nal., 2025) - representing the first known instance of deep unfolding applied to\na Monte Carlo sampling scheme. We demonstrate our proposed unfolded and\ndistilled samplers through extensive experiments and comparisons with the state\nof the art, where they achieve excellent accuracy and computational efficiency,\nwhile retaining the flexibility to adapt to variations in the forward model at\ninference time.", "comment": "28 pages, 16 figures, 10 tables", "pdf_url": "http://arxiv.org/pdf/2507.02686v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "扩散模型展开与蒸馏学习少步后验采样器", "tldr": "本文提出一种新颖的框架，通过深度展开和模型蒸馏将扩散模型图像先验转换为少步条件模型，用于高效的后验采样，并在MCMC算法上首次应用深度展开。", "motivation": "扩散模型在贝叶斯计算成像中作为图像先验表现出色，但现有方法如即插即用（依赖近似）和专用条件扩散模型（需要监督训练且针对特定任务）存在局限性。", "method": "引入一个新颖的框架，结合深度展开和模型蒸馏，将扩散模型图像先验转化为少步条件模型用于后验采样。核心创新是将马尔可夫链蒙特卡洛（MCMC）算法（特别是LATINO Langevin采样器）进行展开，这是首次将深度展开应用于蒙特卡洛采样方案。", "result": "所提出的展开和蒸馏采样器在大量实验和与现有技术比较中展现出卓越的准确性和计算效率，同时保持了在推理时适应前向模型变化的能力。", "conclusion": "本文提出的通过展开和蒸馏学习少步后验采样器的方法，为贝叶斯计算成像提供了一个高效、准确且灵活的解决方案，并首次将深度展开应用于MCMC采样。", "translation": "扩散模型（DMs）已成为贝叶斯计算成像中强大的图像先验。在该背景下，已提出两种主要策略来利用扩散模型：即插即用方法，它们是零样本且高度灵活但依赖于近似；以及专用条件扩散模型，它们通过监督训练为特定任务实现更高的准确性和更快的推理。在这项工作中，我们引入了一个新颖的框架，该框架整合了深度展开和模型蒸馏，将扩散模型图像先验转换为用于后验采样的少步条件模型。我们方法的核心创新是展开马尔可夫链蒙特卡洛（MCMC）算法——特别是最近提出的LATINO Langevin采样器（Spagnoletti et al., 2025）——这代表了深度展开首次应用于蒙特卡洛采样方案。我们通过广泛的实验和与现有技术的比较，展示了我们提出的展开和蒸馏采样器，它们实现了卓越的准确性和计算效率，同时保留了在推理时适应前向模型变化的灵活性。", "summary": "本文提出了一个新颖的框架，通过结合深度展开和模型蒸馏，将扩散模型（DM）图像先验转换为高效的少步条件模型，用于贝叶斯计算成像中的后验采样。该方法首次将深度展开应用于马尔可夫链蒙特卡洛（MCMC）算法（如LATINO Langevin采样器），旨在克服现有DM利用策略的局限性。实验证明，该方法在准确性、计算效率和适应前向模型变化方面均表现出色。", "keywords": "扩散模型, 深度展开, 模型蒸馏, 后验采样, MCMC", "comments": "本文的创新点在于首次将深度展开技术应用于马尔可夫链蒙特卡洛（MCMC）采样算法，这为结合深度学习和传统采样方法提供了新的范式。通过模型蒸馏，实现了将复杂的扩散模型先验转化为高效的少步条件采样器，显著提升了计算效率，同时保持了高精度和灵活性，这对于贝叶斯计算成像领域具有重要意义。"}}
{"id": "2507.02782", "title": "Understanding and Improving Length Generalization in Recurrent Models", "authors": ["Ricardo Buitrago Ruiz", "Albert Gu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02782v1", "summary": "Recently, recurrent models such as state space models and linear attention\nhave become popular due to their linear complexity in the sequence length.\nThanks to their recurrent nature, in principle they can process arbitrarily\nlong sequences, but their performance sometimes drops considerably beyond their\ntraining context lengths-i.e. they fail to length generalize. In this work, we\nprovide comprehensive empirical and theoretical analysis to support the\nunexplored states hypothesis, which posits that models fail to length\ngeneralize when during training they are only exposed to a limited subset of\nthe distribution of all attainable states (i.e. states that would be attained\nif the recurrence was applied to long sequences). Furthermore, we investigate\nsimple training interventions that aim to increase the coverage of the states\nthat the model is trained on, e.g. by initializing the state with Gaussian\nnoise or with the final state of a different input sequence. With only 500\npost-training steps ($\\sim 0.1\\%$ of the pre-training budget), these\ninterventions enable length generalization for sequences that are orders of\nmagnitude longer than the training context (e.g. $2k\\longrightarrow 128k$) and\nshow improved performance in long context tasks, thus presenting a simple and\nefficient way to enable robust length generalization in general recurrent\nmodels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02782v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "理解和改进循环模型的长度泛化能力", "tldr": "本文深入分析了循环模型在训练上下文长度之外的长度泛化失败问题，提出了“未探索状态假设”，并通过简单的训练干预（如高斯噪声或不同输入序列的最终状态初始化）显著提高了模型的长度泛化能力，使其能够处理远超训练长度的序列。", "motivation": "循环模型（如状态空间模型和线性注意力）尽管在理论上可以处理任意长序列，但在实践中，它们的性能在超出训练上下文长度时会显著下降，即它们无法进行长度泛化。本文旨在理解并改进这一问题。", "method": "本文提供了全面的实证和理论分析来支持“未探索状态假设”，该假设认为模型在训练时仅接触到所有可达状态分布的有限子集，从而导致长度泛化失败。此外，研究人员还探索了简单的训练干预措施，旨在增加模型训练所覆盖的状态范围，例如，通过高斯噪声或不同输入序列的最终状态来初始化模型状态。", "result": "通过仅500个训练后步骤（约占预训练预算的0.1%），这些干预措施使模型能够对远超训练上下文长度（例如，从2k到128k）的序列实现长度泛化，并在长上下文任务中表现出改进的性能。", "conclusion": "本文提出了一种简单高效的方法，可以使通用循环模型实现鲁棒的长度泛化能力。", "translation": "最近，循环模型，如状态空间模型和线性注意力，因其在序列长度上的线性复杂度而变得流行。由于它们的循环特性，原则上它们可以处理任意长的序列，但它们的性能有时在超出训练上下文长度后会显著下降——即它们未能实现长度泛化。在这项工作中，我们提供了全面的实证和理论分析来支持未探索状态假设，该假设认为模型在训练期间只接触到所有可达状态分布的有限子集（即，如果将循环应用于长序列将达到的状态）时，就无法进行长度泛化。此外，我们研究了旨在增加模型训练所覆盖状态范围的简单训练干预措施，例如，通过高斯噪声或不同输入序列的最终状态来初始化状态。仅通过500个训练后步骤（约占预训练预算的0.1%），这些干预措施使模型能够对远超训练上下文长度（例如，从2k到128k）的序列实现长度泛化，并在长上下文任务中表现出改进的性能，从而提出了一种简单高效的方法，以实现通用循环模型中鲁棒的长度泛化。", "summary": "本文深入探讨了循环模型在超出训练上下文长度时长度泛化能力下降的问题。研究人员提出了“未探索状态假设”，认为模型失败的原因在于训练期间只接触了有限的可达状态子集。为解决此问题，论文探索了通过高斯噪声初始化状态或使用不同输入序列的最终状态进行初始化的简单训练干预措施。实验结果表明，仅通过少量训练后步骤，这些干预措施就能显著提升循环模型对超长序列的泛化能力，并在长上下文任务中表现出更好的性能，提供了一种简单而高效的解决方案。", "keywords": "循环模型, 长度泛化, 未探索状态假设, 状态初始化, 深度学习", "comments": "这项研究的创新之处在于提出了“未探索状态假设”，为理解循环模型长度泛化失败提供了一个新的理论视角。其重要性在于，通过极小的额外计算成本（仅占预训练预算的0.1%），就能显著提升模型的长度泛化能力，使得循环模型在处理超长序列时更具实用性。这对于需要处理大数据流或长文本的应用具有重要意义。"}}
{"id": "2507.02687", "title": "APT: Adaptive Personalized Training for Diffusion Models with Limited Data", "authors": ["JungWoo Chae", "Jiyoon Kim", "JaeWoong Choi", "Kyungyul Kim", "Sangheum Hwang"], "categories": ["cs.CV", "cs.AI", "60J60, 68T07", "I.2.6; I.2.10; I.4.9"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      CVPR 2025 camera ready. Project page: this https URL", "url": "http://arxiv.org/abs/2507.02687v1", "summary": "Personalizing diffusion models using limited data presents significant\nchallenges, including overfitting, loss of prior knowledge, and degradation of\ntext alignment. Overfitting leads to shifts in the noise prediction\ndistribution, disrupting the denoising trajectory and causing the model to lose\nsemantic coherence. In this paper, we propose Adaptive Personalized Training\n(APT), a novel framework that mitigates overfitting by employing adaptive\ntraining strategies and regularizing the model's internal representations\nduring fine-tuning. APT consists of three key components: (1) Adaptive Training\nAdjustment, which introduces an overfitting indicator to detect the degree of\noverfitting at each time step bin and applies adaptive data augmentation and\nadaptive loss weighting based on this indicator; (2)Representation\nStabilization, which regularizes the mean and variance of intermediate feature\nmaps to prevent excessive shifts in noise prediction; and (3) Attention\nAlignment for Prior Knowledge Preservation, which aligns the cross-attention\nmaps of the fine-tuned model with those of the pretrained model to maintain\nprior knowledge and semantic coherence. Through extensive experiments, we\ndemonstrate that APT effectively mitigates overfitting, preserves prior\nknowledge, and outperforms existing methods in generating high-quality, diverse\nimages with limited reference data.", "comment": "CVPR 2025 camera ready. Project page: https://lgcnsai.github.io/apt", "pdf_url": "http://arxiv.org/pdf/2507.02687v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "APT：面向有限数据的扩散模型自适应个性化训练", "tldr": "APT是一种新颖的框架，通过自适应训练策略和内部表示正则化，有效缓解了使用有限数据对扩散模型进行个性化训练时出现的过拟合、先验知识丢失和文本对齐退化问题，生成高质量、多样化的图像。", "motivation": "使用有限数据对扩散模型进行个性化训练面临重大挑战，包括过拟合、先验知识丢失和文本对齐退化。过拟合会导致噪声预测分布偏移，破坏去噪轨迹，并导致模型失去语义连贯性。", "method": "本文提出了自适应个性化训练（APT）框架，通过自适应训练策略和正则化模型内部表示来缓解过拟合。APT包含三个关键组件：1) 自适应训练调整，引入过拟合指标来检测每个时间步箱的过拟合程度，并基于此指标应用自适应数据增强和自适应损失加权；2) 表示稳定化，正则化中间特征图的均值和方差，以防止噪声预测的过度偏移；3) 先验知识保留的注意力对齐，将微调模型的交叉注意力图与预训练模型的交叉注意力图对齐，以保持先验知识和语义连贯性。", "result": "APT有效缓解了过拟合，保留了先验知识，并且在生成高质量、多样化的图像方面，利用有限参考数据优于现有方法。", "conclusion": "APT框架通过创新的自适应训练策略和表示正则化，成功解决了扩散模型在有限数据下个性化训练的挑战，显著提升了生成图像的质量和多样性，同时维护了模型的先验知识和语义连贯性。", "translation": "使用有限数据对扩散模型进行个性化训练带来了重大挑战，包括过拟合、先验知识丢失和文本对齐退化。过拟合会导致噪声预测分布的偏移，扰乱去噪轨迹，并导致模型失去语义连贯性。在本文中，我们提出了自适应个性化训练（APT），这是一个新颖的框架，通过采用自适应训练策略和在微调过程中正则化模型的内部表示来缓解过拟合。APT由三个关键组件组成：(1) 自适应训练调整，引入过拟合指标来检测每个时间步箱的过拟合程度，并基于此指标应用自适应数据增强和自适应损失加权；(2) 表示稳定化，正则化中间特征图的均值和方差，以防止噪声预测的过度偏移；(3) 先验知识保留的注意力对齐，将微调模型的交叉注意力图与预训练模型的交叉注意力图对齐，以保持先验知识和语义连贯性。通过广泛的实验，我们证明APT有效地缓解了过拟合，保留了先验知识，并且在利用有限参考数据生成高质量、多样化的图像方面优于现有方法。", "summary": "本文提出了一种名为自适应个性化训练（APT）的新型框架，旨在解决使用有限数据个性化扩散模型时遇到的过拟合、先验知识丢失和文本对齐退化等挑战。APT通过引入自适应训练调整来检测和应对过拟合，通过表示稳定化来防止噪声预测的过度偏移，并通过注意力对齐来保持模型的先验知识和语义连贯性。实验证明，APT能有效缓解过拟合，保留先验知识，并在有限数据条件下生成高质量、多样化的图像，表现优于现有方法。", "keywords": "扩散模型, 个性化训练, 有限数据, 过拟合, 自适应训练", "comments": "APT的创新性在于其整合了多方面的策略来解决有限数据下的扩散模型个性化问题，特别是引入过拟合指标进行自适应调整和对内部表示的正则化，以及对注意力图的对齐，这些都有效提升了模型性能和生成质量。其重要性在于为低数据场景下的扩散模型应用提供了有效途径。"}}
{"id": "2507.02807", "title": "In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization", "authors": ["Thiti Suttaket", "Stanley Kok"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02807v1", "summary": "Survival analysis is an important problem in healthcare because it models the\nrelationship between an individual's covariates and the onset time of an event\nof interest (e.g., death). It is important for survival models to be\nwell-calibrated (i.e., for their predicted probabilities to be close to\nground-truth probabilities) because badly calibrated systems can result in\nerroneous clinical decisions. Existing survival models are typically calibrated\nat the population level only, and thus run the risk of being poorly calibrated\nfor one or more minority subpopulations. We propose a model called GRADUATE\nthat achieves multicalibration by ensuring that all subpopulations are\nwell-calibrated too. GRADUATE frames multicalibration as a constrained\noptimization problem, and optimizes both calibration and discrimination\nin-training to achieve a good balance between them. We mathematically prove\nthat the optimization method used yields a solution that is both near-optimal\nand feasible with high probability. Empirical comparisons against\nstate-of-the-art baselines on real-world clinical datasets demonstrate\nGRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of\nthe baselines vis-a-vis GRADUATE's strengths.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02807v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "医疗保健领域中通过约束优化实现的训练中多校准生存分析", "tldr": "GRADUATE模型通过在训练中将多校准作为约束优化问题，确保医疗保健领域中生存分析模型在所有子群体上都得到良好校准，并在真实数据上表现优异。", "motivation": "现有生存模型通常只在总体层面进行校准，可能导致对少数子群体校准不佳，进而引发错误的临床决策。因此，生存模型需要良好校准，即预测概率应接近真实概率。", "method": "本文提出了GRADUATE模型，通过将多校准问题构建为约束优化问题，并在训练过程中同时优化校准和判别能力，以实现两者之间的良好平衡。数学证明该优化方法能以高概率产生接近最优且可行的解。", "result": "在真实临床数据集上，GRADUATE模型与最先进的基线模型进行经验比较，证明了其有效性。详细分析揭示了基线模型相对于GRADUATE优势的不足之处。", "conclusion": "GRADUATE模型通过在训练中实现多校准生存分析，有效解决了现有模型在子群体校准方面的不足，并在实际应用中展现出卓越的性能。", "translation": "生存分析在医疗保健领域是一个重要问题，因为它模拟了个体协变量与感兴趣事件（例如死亡）发生时间之间的关系。生存模型需要良好校准（即其预测概率应接近真实概率）至关重要，因为校准不良的系统可能导致错误的临床决策。现有的生存模型通常仅在总体层面进行校准，因此存在对一个或多个少数子群体校准不佳的风险。我们提出了一个名为GRADUATE的模型，通过确保所有子群体也得到良好校准来实现多校准。GRADUATE将多校准构建为一个约束优化问题，并在训练中同时优化校准和判别能力，以实现两者之间的良好平衡。我们数学证明了所使用的优化方法能以高概率产生接近最优且可行的解。在真实临床数据集上与最先进的基线模型进行经验比较，证明了GRADUATE的有效性。在详细分析中，我们阐明了基线模型相对于GRADUATE优势的不足之处。", "summary": "本文提出了一种名为GRADUATE的新型生存分析模型，旨在解决现有模型在医疗保健领域少数子群体校准不良的问题。GRADUATE通过将多校准视为一个约束优化问题，并在训练过程中同时优化校准和判别能力，以确保所有子群体都能得到良好校准。该模型在理论上被证明能以高概率产生接近最优且可行的解，并在真实临床数据集上表现出优于现有最先进基线模型的有效性。", "keywords": "生存分析, 多校准, 约束优化, 医疗保健, GRADUATE", "comments": "这篇论文的创新点在于将多校准问题融入到生存分析的训练过程中，并将其表述为一个约束优化问题，从而确保模型在不同子群体上的公平性和准确性。这对于避免医疗决策中的偏见和提高临床决策的可靠性具有重要意义。"}}
{"id": "2507.02691", "title": "CanonSwap: High-Fidelity and Consistent Video Face Swapping via Canonical Space Modulation", "authors": ["Xiangyang Luo", "Ye Zhu", "Yunfei Liu", "Lijian Lin", "Cong Wan", "Zijian Cai", "Shao-Lun Huang", "Yu Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV Accepted", "url": "http://arxiv.org/abs/2507.02691v1", "summary": "Video face swapping aims to address two primary challenges: effectively\ntransferring the source identity to the target video and accurately preserving\nthe dynamic attributes of the target face, such as head poses, facial\nexpressions, lip-sync, \\etc. Existing methods mainly focus on achieving\nhigh-quality identity transfer but often fall short in maintaining the dynamic\nattributes of the target face, leading to inconsistent results. We attribute\nthis issue to the inherent coupling of facial appearance and motion in videos.\nTo address this, we propose CanonSwap, a novel video face-swapping framework\nthat decouples motion information from appearance information. Specifically,\nCanonSwap first eliminates motion-related information, enabling identity\nmodification within a unified canonical space. Subsequently, the swapped\nfeature is reintegrated into the original video space, ensuring the\npreservation of the target face's dynamic attributes. To further achieve\nprecise identity transfer with minimal artifacts and enhanced realism, we\ndesign a Partial Identity Modulation module that adaptively integrates source\nidentity features using a spatial mask to restrict modifications to facial\nregions. Additionally, we introduce several fine-grained synchronization\nmetrics to comprehensively evaluate the performance of video face swapping\nmethods. Extensive experiments demonstrate that our method significantly\noutperforms existing approaches in terms of visual quality, temporal\nconsistency, and identity preservation. Our project page are publicly available\nat https://luoxyhappy.github.io/CanonSwap/.", "comment": "ICCV Accepted", "pdf_url": "http://arxiv.org/pdf/2507.02691v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "CanonSwap：通过规范空间调制实现高保真和一致的视频换脸", "tldr": "CanonSwap提出了一种新的视频换脸框架，通过解耦运动和外观信息，在规范空间中进行身份修改，从而解决了现有方法在保持目标人脸动态属性方面的一致性问题，并显著提升了视觉质量和时间一致性。", "motivation": "现有视频换脸方法主要侧重于实现高质量的身份迁移，但在保持目标人脸的动态属性（如头部姿态、面部表情、唇语同步等）方面往往表现不佳，导致结果不一致。本文将此问题归因于视频中面部外观和运动的固有耦合。", "method": "本文提出了CanonSwap，一种新颖的视频换脸框架，它将运动信息与外观信息解耦。具体来说，CanonSwap首先消除运动相关信息，从而在统一的规范空间内进行身份修改。随后，将交换后的特征重新整合到原始视频空间中，以确保保留目标人脸的动态属性。为了进一步实现精确的身份迁移，减少伪影并增强真实感，设计了一个部分身份调制模块，该模块使用空间掩码自适应地整合源身份特征，将修改限制在面部区域。此外，还引入了几种细粒度同步度量来全面评估视频换脸方法的性能。", "result": "大量实验表明，CanonSwap在视觉质量、时间一致性和身份保留方面显著优于现有方法。", "conclusion": "CanonSwap通过解耦视频中面部外观和运动信息，并利用规范空间进行身份修改，有效地解决了视频换脸中动态属性保持不一致的问题，实现了高保真和时间一致的换脸结果。", "translation": "视频换脸旨在解决两个主要挑战：有效地将源身份转移到目标视频，并准确保留目标人脸的动态属性，例如头部姿态、面部表情、唇语同步等。现有方法主要侧重于实现高质量的身份迁移，但在保持目标人脸的动态属性方面往往表现不佳，导致结果不一致。我们将此问题归因于视频中面部外观和运动的固有耦合。为了解决这个问题，我们提出了CanonSwap，一个新颖的视频换脸框架，它将运动信息与外观信息解耦。具体来说，CanonSwap首先消除运动相关信息，从而在统一的规范空间内进行身份修改。随后，将交换后的特征重新整合到原始视频空间中，确保保留目标人脸的动态属性。为了进一步实现精确的身份迁移，减少伪影并增强真实感，我们设计了一个部分身份调制模块，该模块使用空间掩码自适应地整合源身份特征，将修改限制在面部区域。此外，我们引入了几种细粒度同步度量来全面评估视频换脸方法的性能。大量实验表明，我们的方法在视觉质量、时间一致性和身份保留方面显著优于现有方法。我们的项目页面已公开，网址为 https://luoxyhappy.github.io/CanonSwap/。", "summary": "CanonSwap是一个创新的视频换脸框架，旨在解决现有方法在保持目标人脸动态属性方面的一致性问题。它通过将视频中的面部运动与外观信息解耦，在统一的规范空间中进行身份修改，然后将交换后的特征重新整合回原始视频空间。为提高身份迁移的精确性和真实感，该方法还引入了部分身份调制模块。实验结果表明，CanonSwap在视觉质量、时间一致性和身份保留方面均显著优于现有技术。", "keywords": "视频换脸, 规范空间, 身份迁移, 时间一致性, 解耦", "comments": "CanonSwap的创新之处在于其对视频中面部外观和运动信息的解耦处理，并在规范空间中进行身份修改，这有效地解决了现有方法在保持时间一致性方面的挑战。部分身份调制模块的设计进一步提升了换脸的精确性和真实感，限制了修改范围，减少了伪影。该方法在视频换脸领域提供了一个高保真且一致性的解决方案，具有重要的实际应用潜力。"}}
{"id": "2507.02814", "title": "Replicable Distribution Testing", "authors": ["Ilias Diakonikolas", "Jingyi Gao", "Daniel Kane", "Sihan Liu", "Christopher Ye"], "categories": ["cs.LG", "G.3"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      39 pages", "url": "http://arxiv.org/abs/2507.02814v1", "summary": "We initiate a systematic investigation of distribution testing in the\nframework of algorithmic replicability. Specifically, given independent samples\nfrom a collection of probability distributions, the goal is to characterize the\nsample complexity of replicably testing natural properties of the underlying\ndistributions. On the algorithmic front, we develop new replicable algorithms\nfor testing closeness and independence of discrete distributions. On the lower\nbound front, we develop a new methodology for proving sample complexity lower\nbounds for replicable testing that may be of broader interest. As an\napplication of our technique, we establish near-optimal sample complexity lower\nbounds for replicable uniformity testing -- answering an open question from\nprior work -- and closeness testing.", "comment": "39 pages", "pdf_url": "http://arxiv.org/pdf/2507.02814v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "可复现的分布测试", "tldr": "该研究系统性地探讨了算法可复现性框架下的分布测试，提出了新的可复现算法和证明样本复杂度下限的方法，并解决了可复现均匀性测试的开放问题。", "motivation": "在算法可复现性框架下，系统性地研究分布测试，旨在表征可复现地测试底层分布自然属性的样本复杂度。", "method": "开发了用于测试离散分布的接近度和独立性的新型可复现算法。开发了一种证明可复现测试样本复杂度下限的新方法。", "result": "为离散分布的接近度和独立性测试开发了新的可复现算法。为可复现均匀性测试和接近度测试建立了接近最优的样本复杂度下限，解决了先前工作中的一个开放问题。", "conclusion": "本研究在算法可复现性框架下，系统地调查了分布测试，提供了新的算法和样本复杂度下限证明方法，并解决了该领域的一个开放问题，为可复现分布测试奠定了基础。", "translation": "我们首次在算法可复现性框架下系统地研究了分布测试。具体来说，给定来自一组概率分布的独立样本，目标是表征可复现地测试底层分布自然属性的样本复杂度。在算法方面，我们开发了用于测试离散分布的接近度和独立性的新型可复现算法。在下限方面，我们开发了一种证明可复现测试样本复杂度下限的新方法，这可能具有更广泛的意义。作为我们技术的一个应用，我们为可复现均匀性测试——回答了先前工作中的一个开放问题——和接近度测试建立了接近最优的样本复杂度下限。", "summary": "本论文首次系统地研究了算法可复现性框架下的分布测试。研究目标是表征可复现地测试分布属性所需的样本复杂度。作者开发了新的可复现算法，用于测试离散分布的接近度和独立性。此外，还提出了一种证明可复现测试样本复杂度下限的新方法，并以此为可复现均匀性测试和接近度测试建立了接近最优的样本复杂度下限，解决了领域内的开放问题。", "keywords": "分布测试, 可复现性, 样本复杂度, 算法, 下限", "comments": "该论文创新性地将“可复现性”引入到分布测试领域，为该领域开辟了新的研究方向。其提出的新算法和样本复杂度下限证明方法对于未来的可复现算法设计和分析具有重要意义。特别是解决了可复现均匀性测试的开放问题，显示了其方法的有效性。"}}
{"id": "2507.02705", "title": "SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment", "authors": ["Qi Xu", "Dongxu Wei", "Lingzhe Zhao", "Wenpu Li", "Zhangchi Huang", "Shunping Ji", "Peidong Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02705v1", "summary": "Simultaneous understanding and 3D reconstruction plays an important role in\ndeveloping end-to-end embodied intelligent systems. To achieve this, recent\napproaches resort to 2D-to-3D feature alignment paradigm, which leads to\nlimited 3D understanding capability and potential semantic information loss. In\nlight of this, we propose SIU3R, the first alignment-free framework for\ngeneralizable simultaneous understanding and 3D reconstruction from unposed\nimages. Specifically, SIU3R bridges reconstruction and understanding tasks via\npixel-aligned 3D representation, and unifies multiple understanding tasks into\na set of unified learnable queries, enabling native 3D understanding without\nthe need of alignment with 2D models. To encourage collaboration between the\ntwo tasks with shared representation, we further conduct in-depth analyses of\ntheir mutual benefits, and propose two lightweight modules to facilitate their\ninteraction. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance not only on the individual tasks of 3D\nreconstruction and understanding, but also on the task of simultaneous\nunderstanding and 3D reconstruction, highlighting the advantages of our\nalignment-free framework and the effectiveness of the mutual benefit designs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02705v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "SIU3R：超越特征对齐的同步场景理解与三维重建", "tldr": "提出SIU3R，一个无需特征对齐的框架，实现从无姿态图像中同步进行场景理解和三维重建，并达到SOTA性能。", "motivation": "现有的同步理解与三维重建方法依赖2D-to-3D特征对齐范式，导致3D理解能力有限和潜在的语义信息丢失。", "method": "提出SIU3R框架，这是首个无需对齐的通用同步场景理解和三维重建框架，可从无姿态图像中进行。SIU3R通过像素对齐的3D表示连接重建和理解任务，并将多个理解任务统一为一组可学习查询，实现无需与2D模型对齐的原生3D理解。为促进两任务协作，进一步分析其互惠性，并提出两个轻量级模块以促进其交互。", "result": "我们的方法不仅在3D重建和理解的单个任务上，而且在同步理解和3D重建任务上都达到了最先进的性能。", "conclusion": "所提出的无对齐框架的优势以及互惠设计方案的有效性，在同步场景理解与三维重建中得到了显著体现。", "translation": "同步理解和三维重建在开发端到端具身智能系统中扮演着重要角色。为实现此目标，近期方法诉诸于2D到3D特征对齐范式，这导致了有限的3D理解能力和潜在的语义信息损失。鉴于此，我们提出了SIU3R，这是首个无需对齐的通用同步理解和三维重建框架，可从无姿态图像中进行。具体而言，SIU3R通过像素对齐的3D表示连接重建和理解任务，并将多个理解任务统一为一组可学习查询，从而实现无需与2D模型对齐的原生3D理解。为了鼓励具有共享表示的两个任务之间的协作，我们进一步深入分析了它们的互惠效益，并提出了两个轻量级模块来促进它们的交互。广泛的实验表明，我们的方法不仅在3D重建和理解的单个任务上，而且在同步理解和3D重建任务上都达到了最先进的性能，突显了我们无对齐框架的优势和互惠设计方案的有效性。", "summary": "本文提出了SIU3R，首个无需特征对齐的框架，用于从无姿态图像中同步进行场景理解和三维重建。该方法通过像素对齐的3D表示连接重建和理解任务，并将多个理解任务统一为可学习查询，实现原生3D理解。为促进任务协作，还引入了轻量级模块。实验证明SIU3R在各项任务上均达到最先进性能，验证了其无对齐框架和互惠设计的有效性。", "keywords": "同步场景理解, 三维重建, 无对齐框架, 像素对齐, 端到端系统", "comments": "该论文的创新点在于提出了首个无需特征对齐的同步场景理解与三维重建框架，解决了传统2D-to-3D特征对齐的局限性。通过原生3D理解和任务间互惠设计，显著提升了端到端智能系统的性能，为该领域提供了新的研究方向。"}}
{"id": "2507.02713", "title": "UniMC: Taming Diffusion Transformer for Unified Keypoint-Guided Multi-Class Image Generation", "authors": ["Qin Guo", "Ailing Zeng", "Dongxu Yue", "Ceyuan Yang", "Yang Cao", "Hanzhong Guo", "Fei Shen", "Wei Liu", "Xihui Liu", "Dan Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02713v1", "summary": "Although significant advancements have been achieved in the progress of\nkeypoint-guided Text-to-Image diffusion models, existing mainstream\nkeypoint-guided models encounter challenges in controlling the generation of\nmore general non-rigid objects beyond humans (e.g., animals). Moreover, it is\ndifficult to generate multiple overlapping humans and animals based on keypoint\ncontrols solely. These challenges arise from two main aspects: the inherent\nlimitations of existing controllable methods and the lack of suitable datasets.\nFirst, we design a DiT-based framework, named UniMC, to explore unifying\ncontrollable multi-class image generation. UniMC integrates instance- and\nkeypoint-level conditions into compact tokens, incorporating attributes such as\nclass, bounding box, and keypoint coordinates. This approach overcomes the\nlimitations of previous methods that struggled to distinguish instances and\nclasses due to their reliance on skeleton images as conditions. Second, we\npropose HAIG-2.9M, a large-scale, high-quality, and diverse dataset designed\nfor keypoint-guided human and animal image generation. HAIG-2.9M includes 786K\nimages with 2.9M instances. This dataset features extensive annotations such as\nkeypoints, bounding boxes, and fine-grained captions for both humans and\nanimals, along with rigorous manual inspection to ensure annotation accuracy.\nExtensive experiments demonstrate the high quality of HAIG-2.9M and the\neffectiveness of UniMC, particularly in heavy occlusions and multi-class\nscenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02713v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "UniMC：驯服扩散Transformer实现统一关键点引导的多类别图像生成", "tldr": "UniMC提出了一个基于DiT的框架和HAIG-2.9M数据集，用于解决现有关键点引导扩散模型在生成非刚性多类别图像（特别是动物和重叠实例）时的挑战。", "motivation": "现有主流关键点引导的文本到图像扩散模型在控制生成人类之外的更通用非刚性物体（如动物）时遇到挑战，并且难以仅通过关键点控制来生成多个重叠的人类和动物。这些挑战源于现有可控方法的局限性以及缺乏合适的数据集。", "method": "本文设计了一个名为UniMC的基于DiT的框架，用于探索统一可控的多类别图像生成。UniMC将实例级和关键点级条件整合到紧凑的tokens中，包含了类别、边界框和关键点坐标等属性，克服了以往方法依赖骨架图像作为条件而难以区分实例和类别的问题。其次，提出了HAIG-2.9M，一个大规模、高质量、多样化的关键点引导人与动物图像生成数据集，包含786K图像和2.9M实例，并具有丰富的关键点、边界框和细粒度标注，以及严格的人工检查。", "result": "广泛的实验证明了HAIG-2.9M数据集的高质量以及UniMC的有效性，特别是在重度遮挡和多类别场景下表现出色。", "conclusion": "UniMC框架和HAIG-2.9M数据集有效解决了现有关键点引导扩散模型在生成复杂多类别和重叠非刚性物体方面的局限性，显著提升了生成质量和控制能力。", "translation": "尽管关键点引导的文本到图像扩散模型取得了显著进展，但现有的主流关键点引导模型在控制生成人类之外的更通用非刚性物体（例如动物）时遇到了挑战。此外，仅凭关键点控制难以生成多个重叠的人类和动物。这些挑战源于两个主要方面：现有可控方法的固有局限性以及缺乏合适的数据集。首先，我们设计了一个名为UniMC的基于DiT的框架，旨在探索统一可控的多类别图像生成。UniMC将实例级和关键点级条件整合到紧凑的tokens中，并包含了类别、边界框和关键点坐标等属性。这种方法克服了以往方法因依赖骨架图像作为条件而难以区分实例和类别的局限性。其次，我们提出了HAIG-2.9M，一个大规模、高质量、多样化的数据集，专为关键点引导的人与动物图像生成而设计。HAIG-2.9M包含786K张图像和2.9M个实例。该数据集具有丰富的标注，如关键点、边界框以及人类和动物的细粒度描述，并经过严格的人工检查以确保标注的准确性。广泛的实验证明了HAIG-2.9M的高质量和UniMC的有效性，尤其是在重度遮挡和多类别场景下。", "summary": "本文提出UniMC框架和HAIG-2.9M数据集，旨在解决现有关键点引导扩散模型在生成非刚性多类别图像（特别是动物和多重叠实例）时的局限性。UniMC是一个基于DiT的框架，通过将实例级和关键点级条件整合到紧凑的tokens中，克服了传统方法区分实例和类别的难题。HAIG-2.9M是一个大规模、高质量的人与动物关键点引导图像生成数据集，包含了丰富的标注和严格的质量控制。实验证明了UniMC在复杂场景下的有效性。", "keywords": "关键点引导生成, 多类别图像生成, 扩散Transformer, 数据集, UniMC", "comments": "本文通过提出创新的UniMC框架和构建大规模高质量的HAIG-2.9M数据集，有效解决了现有关键点引导扩散模型在处理非刚性物体和多类别、多实例重叠生成时的核心痛点。UniMC将多源条件统一编码为紧凑tokens的策略，是其方法上的显著创新，提升了模型的控制力和区分能力。HAIG-2.9M数据集的构建填补了该领域高质量数据的空白，对后续研究具有重要推动作用。"}}
{"id": "2507.02843", "title": "LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding", "authors": ["Yuchen Ma", "Dennis Frauen", "Jonas Schweisthal", "Stefan Feuerriegel"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02843v1", "summary": "Estimating treatment effects is crucial for personalized decision-making in\nmedicine, but this task faces unique challenges in clinical practice. At\ntraining time, models for estimating treatment effects are typically trained on\nwell-structured medical datasets that contain detailed patient information.\nHowever, at inference time, predictions are often made using textual\ndescriptions (e.g., descriptions with self-reported symptoms), which are\nincomplete representations of the original patient information. In this work,\nwe make three contributions. (1) We show that the discrepancy between the data\navailable during training time and inference time can lead to biased estimates\nof treatment effects. We formalize this issue as an inference time text\nconfounding problem, where confounders are fully observed during training time\nbut only partially available through text at inference time. (2) To address\nthis problem, we propose a novel framework for estimating treatment effects\nthat explicitly accounts for inference time text confounding. Our framework\nleverages large language models together with a custom doubly robust learner to\nmitigate biases caused by the inference time text confounding. (3) Through a\nseries of experiments, we demonstrate the effectiveness of our framework in\nreal-world applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02843v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LLM驱动的推断时文本混淆下的治疗效果估计", "tldr": "提出一个利用LLM和双重鲁棒学习器的新框架，解决治疗效果估计中训练时数据与推断时文本描述不一致导致的偏差问题。", "motivation": "在医学领域，治疗效果估计对个性化决策至关重要。然而，在临床实践中，训练时模型使用结构化数据集，而推断时则依赖不完整的文本描述，这种数据不一致性会导致治疗效果估计出现偏差，即推断时文本混淆问题。", "method": "提出一个新颖的治疗效果估计框架，该框架明确考虑了推断时文本混淆问题。该框架利用大型语言模型（LLM）和一个定制的双重鲁棒学习器来减轻由推断时文本混淆引起的偏差。", "result": "通过一系列实验，证明了所提出的框架在实际应用中的有效性。", "conclusion": "该论文成功识别并形式化了推断时文本混淆问题，并提出了一个基于LLM和双重鲁棒学习器的有效框架来解决此问题，从而提高了治疗效果估计的准确性。", "translation": "治疗效果估计对于医学中的个性化决策至关重要，但这项任务在临床实践中面临独特的挑战。在训练时，用于估计治疗效果的模型通常在包含详细患者信息的结构化医疗数据集上进行训练。然而，在推断时，预测通常使用文本描述（例如，包含自我报告症状的描述）进行，这些描述是原始患者信息的不完整表示。在这项工作中，我们做出了三项贡献。(1) 我们表明，训练时和推断时可用数据之间的差异可能导致治疗效果估计的偏差。我们将这个问题形式化为推断时文本混淆问题，其中混淆变量在训练时被完全观察到，但在推断时只能通过文本部分获取。(2) 为了解决这个问题，我们提出了一个新颖的治疗效果估计框架，该框架明确考虑了推断时文本混淆。我们的框架利用大型语言模型和一个定制的双重鲁棒学习器来减轻由推断时文本混淆引起的偏差。(3) 通过一系列实验，我们证明了我们的框架在实际应用中的有效性。", "summary": "这篇论文探讨了在医学领域估计治疗效果时面临的挑战，特别是训练数据与推断时文本描述之间的不一致导致的偏差问题，并将其形式化为“推断时文本混淆”。为解决此问题，作者提出了一个新颖的框架，该框架结合了大型语言模型（LLM）和一个定制的双重鲁棒学习器，以有效减轻这种偏差。实验结果表明该框架在实际应用中表现出有效性。", "keywords": "治疗效果估计, 推断时文本混淆, 大型语言模型, 双重鲁棒学习, 医学决策", "comments": "这篇论文识别了一个在实际临床应用中非常重要且普遍存在的问题：训练数据与推断数据形式不一致导致的混淆偏差。其创新点在于将此问题形式化为“推断时文本混淆”，并巧妙地引入大型语言模型来处理文本信息的不完整性，结合双重鲁棒学习器来提高估计的准确性。该方法对于提高医学个性化决策的可靠性具有重要意义。"}}
{"id": "2507.02714", "title": "FairHuman: Boosting Hand and Face Quality in Human Image Generation with Minimum Potential Delay Fairness in Diffusion Models", "authors": ["Yuxuan Wang", "Tianwei Cao", "Huayu Zhang", "Zhongjiang He", "Kongming Liang", "Zhanyu Ma"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2507.02714v1", "summary": "Image generation has achieved remarkable progress with the development of\nlarge-scale text-to-image models, especially diffusion-based models. However,\ngenerating human images with plausible details, such as faces or hands, remains\nchallenging due to insufficient supervision of local regions during training.\nTo address this issue, we propose FairHuman, a multi-objective fine-tuning\napproach designed to enhance both global and local generation quality fairly.\nSpecifically, we first construct three learning objectives: a global objective\nderived from the default diffusion objective function and two local objectives\nfor hands and faces based on pre-annotated positional priors. Subsequently, we\nderive the optimal parameter updating strategy under the guidance of the\nMinimum Potential Delay (MPD) criterion, thereby attaining fairness-ware\noptimization for this multi-objective problem. Based on this, our proposed\nmethod can achieve significant improvements in generating challenging local\ndetails while maintaining overall quality. Extensive experiments showcase the\neffectiveness of our method in improving the performance of human image\ngeneration under different scenarios.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.02714v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "FairHuman：在扩散模型中通过最小潜在延迟公平性提升人类图像生成中的手部和面部质量", "tldr": "FairHuman 提出一种多目标微调方法，利用全局和局部（手、脸）损失以及最小潜在延迟（MPD）公平性优化，显著改善扩散模型生成人类图像中手部和面部细节的质量，同时保持整体质量。", "motivation": "尽管大型文本到图像扩散模型在图像生成方面取得了显著进展，但在生成具有逼真细节（如面部或手部）的人类图像时仍然具有挑战性，原因是训练期间对局部区域的监督不足。", "method": "提出 FairHuman，一种多目标微调方法。首先构建三个学习目标：一个来自默认扩散目标函数的全局目标，以及两个基于预标注位置先验的手部和面部局部目标。随后，在最小潜在延迟（MPD）准则的指导下推导出最优参数更新策略，从而实现多目标问题的公平性感知优化。", "result": "该方法在生成具有挑战性的局部细节方面取得了显著改进，同时保持了整体质量。大量实验表明该方法在不同场景下提升人类图像生成性能的有效性。", "conclusion": "FairHuman 通过其多目标微调和最小潜在延迟公平性优化策略，成功解决了扩散模型在生成人类图像中手部和面部细节方面的挑战，显著提升了局部和整体生成质量。", "translation": "图像生成随着大规模文本到图像模型，特别是基于扩散模型的发展，取得了显著进展。然而，由于训练期间对局部区域的监督不足，生成具有逼真细节（如面部或手部）的人类图像仍然具有挑战性。为了解决这个问题，我们提出了 FairHuman，一种旨在公平地提升全局和局部生成质量的多目标微调方法。具体而言，我们首先构建了三个学习目标：一个源自默认扩散目标函数的全局目标，以及两个基于预标注位置先验的手部和面部局部目标。随后，我们在最小潜在延迟（MPD）准则的指导下推导出最优参数更新策略，从而实现针对此多目标问题的公平性感知优化。在此基础上，我们提出的方法在生成具有挑战性的局部细节方面取得了显著改进，同时保持了整体质量。大量实验展示了我们方法在不同场景下改善人类图像生成性能的有效性。", "summary": "FairHuman 是一种针对扩散模型的多目标微调方法，旨在解决现有模型在生成人类图像时面部和手部细节不足的问题。它通过结合全局扩散目标和基于预标注位置先验的局部（手部和面部）目标来增强监督，并采用最小潜在延迟（MPD）准则进行公平性感知优化，以实现全局和局部生成质量的同步提升。实验证明，该方法能显著改善局部细节的生成效果，同时保持整体图像质量。", "keywords": "扩散模型, 人类图像生成, 面部质量, 手部质量, 多目标优化, 最小潜在延迟, FairHuman", "comments": "该论文的创新点在于提出了 FairHuman，一个结合了多目标学习（全局与局部）和公平性感知优化（基于最小潜在延迟 MPD）的微调框架，以专门解决扩散模型在生成人类图像时手部和面部细节不足的问题。MPD 准则的应用是其独特之处，旨在平衡不同目标之间的优化进度，确保公平性。这对于提升生成图像的真实感和实用性具有重要意义。"}}
{"id": "2507.02847", "title": "MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis", "authors": ["Kunyu Zhang", "Qiang Li", "Shujian Yu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by MICCAI-25, code is available at \\url{ this https URL }", "url": "http://arxiv.org/abs/2507.02847v1", "summary": "Recent evidence suggests that modeling higher-order interactions (HOIs) in\nfunctional magnetic resonance imaging (fMRI) data can enhance the diagnostic\naccuracy of machine learning systems. However, effectively extracting and\nutilizing HOIs remains a significant challenge. In this work, we propose\nMvHo-IB, a novel multi-view learning framework that integrates both pairwise\ninteractions and HOIs for diagnostic decision-making, while automatically\ncompressing task-irrelevant redundant information. MvHo-IB introduces several\nkey innovations: (1) a principled method that combines O-information from\ninformation theory with a matrix-based Renyi alpha-order entropy estimator to\nquantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to\neffectively utilize these interactions, and (3) a new multi-view learning\ninformation bottleneck objective to enhance representation learning.\nExperiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves\nstate-of-the-art performance, significantly outperforming previous methods,\nincluding recent hypergraph-based techniques. The implementation of MvHo-IB is\navailable at https://github.com/zky04/MvHo-IB.", "comment": "Accepted by MICCAI-25, code is available at\n  \\url{https://github.com/zky04/MvHo-IB}", "pdf_url": "http://arxiv.org/pdf/2507.02847v1", "cate": "cs.LG", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "MvHo-IB：用于脑疾病诊断的多视图高阶信息瓶颈", "tldr": "MvHo-IB是一个新的多视图学习框架，它结合了配对交互和高阶交互（HOIs）来诊断脑疾病，通过信息瓶颈自动压缩冗余信息，并在fMRI数据集上实现了最先进的性能。", "motivation": "现有研究表明，建模fMRI数据中的高阶交互（HOIs）可以提高机器学习系统的诊断准确性，但有效提取和利用HOIs仍然是一个重大挑战。", "method": "提出了MvHo-IB，一个新颖的多视图学习框架，整合了配对交互和高阶交互（HOIs）用于诊断决策，同时自动压缩任务无关的冗余信息。MvHo-IB的关键创新包括：1) 一种结合信息论中的O-信息和基于矩阵的Renyi α-阶熵估计器来量化和提取HOIs的原理性方法；2) 一个专门构建的Brain3DCNN编码器以有效利用这些交互；3) 一个新的多视图学习信息瓶颈目标以增强表示学习。", "result": "在三个基准fMRI数据集上的实验表明，MvHo-IB实现了最先进的性能，显著优于以前的方法，包括最近基于超图的技术。", "conclusion": "MvHo-IB通过有效整合高阶交互和多视图学习信息瓶颈，显著提高了脑疾病诊断的准确性。", "translation": "最近的证据表明，在功能性磁共振成像（fMRI）数据中建模高阶交互（HOIs）可以提高机器学习系统的诊断准确性。然而，有效提取和利用HOIs仍然是一个重大挑战。在这项工作中，我们提出了MvHo-IB，一个新颖的多视图学习框架，它整合了配对交互和HOIs用于诊断决策，同时自动压缩任务无关的冗余信息。MvHo-IB引入了几个关键创新：(1) 一种结合信息论中的O-信息和基于矩阵的Renyi α-阶熵估计器来量化和提取HOIs的原理性方法，(2) 一个专门构建的Brain3DCNN编码器以有效利用这些交互，以及(3) 一个新的多视图学习信息瓶颈目标以增强表示学习。在三个基准fMRI数据集上的实验表明，MvHo-IB实现了最先进的性能，显著优于以前的方法，包括最近基于超图的技术。MvHo-IB的实现代码可在https://github.com/zky04/MvHo-IB 获取。", "summary": "本文提出了MvHo-IB，一个用于脑疾病诊断的新型多视图学习框架。该框架旨在解决fMRI数据中高阶交互（HOIs）难以有效提取和利用的挑战。MvHo-IB通过结合信息论方法量化HOIs，使用Brain3DCNN编码器处理这些交互，并引入多视图信息瓶颈目标来优化表示学习，同时压缩冗余信息。实验结果表明，MvHo-IB在多个fMRI数据集上取得了最先进的诊断性能。", "keywords": "fMRI, 脑疾病诊断, 高阶交互, 多视图学习, 信息瓶颈", "comments": "这篇论文通过引入结合信息论和Renyi熵估计器的新方法来量化和提取高阶交互，以及利用专门的Brain3DCNN编码器，并在多视图信息瓶颈框架下进行表示学习，有效解决了fMRI数据中高阶交互难以利用的问题。其在基准数据集上的SOTA性能证明了该方法的创新性和有效性，对脑疾病诊断领域具有重要意义。"}}
{"id": "2507.02743", "title": "Prompt learning with bounding box constraints for medical image segmentation", "authors": ["Mélanie Gaillochet", "Mehrdad Noori", "Sahar Dastani", "Christian Desrosiers", "Hervé Lombaert"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE Transactions on Biomedical Engineering (TMBE), 14 pages", "url": "http://arxiv.org/abs/2507.02743v1", "summary": "Pixel-wise annotations are notoriously labourious and costly to obtain in the\nmedical domain. To mitigate this burden, weakly supervised approaches based on\nbounding box annotations-much easier to acquire-offer a practical alternative.\nVision foundation models have recently shown noteworthy segmentation\nperformance when provided with prompts such as points or bounding boxes. Prompt\nlearning exploits these models by adapting them to downstream tasks and\nautomating segmentation, thereby reducing user intervention. However, existing\nprompt learning approaches depend on fully annotated segmentation masks. This\npaper proposes a novel framework that combines the representational power of\nfoundation models with the annotation efficiency of weakly supervised\nsegmentation. More specifically, our approach automates prompt generation for\nfoundation models using only bounding box annotations. Our proposed\noptimization scheme integrates multiple constraints derived from box\nannotations with pseudo-labels generated by the prompted foundation model.\nExtensive experiments across multimodal datasets reveal that our weakly\nsupervised method achieves an average Dice score of 84.90% in a limited data\nsetting, outperforming existing fully-supervised and weakly-supervised\napproaches. The code is available at\nhttps://github.com/Minimel/box-prompt-learning-VFM.git", "comment": "Accepted to IEEE Transactions on Biomedical Engineering (TMBE), 14\n  pages", "pdf_url": "http://arxiv.org/pdf/2507.02743v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "带有边界框约束的医学图像分割提示学习", "tldr": "提出一种新的弱监督提示学习框架，仅使用边界框标注实现医学图像分割，性能优于现有方法。", "motivation": "像素级标注在医学领域耗时且昂贵，现有提示学习依赖完全标注的分割掩膜，需要解决弱监督下的医学图像分割问题。", "method": "本文提出一个新颖的框架，将基础模型的表示能力与弱监督分割的标注效率相结合。具体地，该方法仅使用边界框标注自动化生成基础模型的提示，并整合来自边界框标注的多个约束与提示基础模型生成的伪标签进行优化。", "result": "在多模态数据集上的广泛实验表明，该弱监督方法在有限数据设置下平均Dice得分达到84.90%，优于现有的全监督和弱监督方法。", "conclusion": "该研究成功地将弱监督学习与基础模型结合，显著降低了医学图像分割的标注成本，同时实现了卓越的分割性能，为医学图像分析提供了一种实用且高效的解决方案。", "translation": "像素级标注在医学领域是出了名的费力且昂贵。为了减轻这一负担，基于边界框标注（更容易获取）的弱监督方法提供了一种实用的替代方案。视觉基础模型最近在提供点或边界框等提示时，显示出显著的分割性能。提示学习通过使这些模型适应下游任务并自动化分割来利用它们，从而减少用户干预。然而，现有的提示学习方法依赖于完全标注的分割掩膜。本文提出了一种新颖的框架，将基础模型的表示能力与弱监督分割的标注效率相结合。更具体地说，我们的方法仅使用边界框标注自动化生成基础模型的提示。我们提出的优化方案将来自边界框标注的多个约束与由提示基础模型生成的伪标签相结合。在多模态数据集上的广泛实验表明，我们的弱监督方法在有限数据设置下平均Dice得分达到84.90%，优于现有的全监督和弱监督方法。代码可在 https://github.com/Minimel/box-prompt-learning-VFM.git 获取。", "summary": "本文提出了一种新颖的弱监督提示学习框架，用于医学图像分割，旨在解决像素级标注成本高昂的问题。该框架利用视觉基础模型的强大表示能力，并仅通过易于获取的边界框标注来自动化提示生成。通过结合边界框约束和伪标签的优化方案，该方法在多模态数据集上取得了84.90%的平均Dice得分，超越了现有的全监督和弱监督方法，为医学图像分割提供了一种高效且实用的解决方案。", "keywords": "医学图像分割, 提示学习, 弱监督, 边界框, 基础模型", "comments": "该论文的创新之处在于将弱监督学习与视觉基础模型相结合，通过仅使用边界框标注来自动化提示生成，显著降低了医学图像分割的标注成本，同时保持了高精度。其方法有效地利用了基础模型的潜力，克服了现有提示学习对完全标注的依赖，对于资源受限的医学影像领域具有重要意义。"}}
{"id": "2507.01964", "title": "Forecasting Nigerian Equity Stock Returns Using Long Short-Term Memory Technique", "authors": ["Adebola K. Ojo", "Ifechukwude Jude Okafor"], "categories": ["q-fin.ST", "cs.LG", "68T07"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "Comments:      10 pages", "url": "http://arxiv.org/abs/2507.01964v1", "summary": "Investors and stock market analysts face major challenges in predicting stock\nreturns and making wise investment decisions. The predictability of equity\nstock returns can boost investor confidence, but it remains a difficult task.\nTo address this issue, a study was conducted using a Long Short-term Memory\n(LSTM) model to predict future stock market movements. The study used a\nhistorical dataset from the Nigerian Stock Exchange (NSE), which was cleaned\nand normalized to design the LSTM model. The model was evaluated using\nperformance metrics and compared with other deep learning models like\nArtificial and Convolutional Neural Networks (CNN). The experimental results\nshowed that the LSTM model can predict future stock market prices and returns\nwith over 90% accuracy when trained with a reliable dataset. The study\nconcludes that LSTM models can be useful in predicting financial\ntime-series-related problems if well-trained. Future studies should explore\ncombining LSTM models with other deep learning techniques like CNN to create\nhybrid models that mitigate the risks associated with relying on a single model\nfor future equity stock predictions.", "comment": "10 pages", "pdf_url": "http://arxiv.org/pdf/2507.01964v1", "cate": "q-fin.ST", "date": "2025-05-27", "updated": "2025-05-27", "AI": {"title_translation": "使用长短期记忆技术预测尼日利亚股票回报", "tldr": "本研究使用长短期记忆 (LSTM) 模型，利用尼日利亚证券交易所的历史数据，以超过90%的准确率预测尼日利亚股票市场的未来价格和回报。", "motivation": "投资者和股市分析师在预测股票回报和做出明智投资决策方面面临重大挑战。股票回报的可预测性可以增强投资者信心，但仍是一项艰巨的任务。", "method": "本研究使用长短期记忆 (LSTM) 模型来预测未来股市走势。研究使用了来自尼日利亚证券交易所 (NSE) 的历史数据集，并对其进行了清洗和标准化以设计 LSTM 模型。该模型通过性能指标进行评估，并与人工神经网络 (ANN) 和卷积神经网络 (CNN) 等其他深度学习模型进行了比较。", "result": "实验结果表明，当使用可靠的数据集进行训练时，LSTM 模型可以以超过90%的准确率预测未来的股市价格和回报。", "conclusion": "研究得出结论，如果训练得当，LSTM 模型在预测金融时间序列相关问题方面可能非常有用。未来的研究应探索将 LSTM 模型与其他深度学习技术（如 CNN）结合，以创建混合模型，从而降低未来股票预测中仅依赖单一模型的风险。", "translation": "投资者和股市分析师在预测股票回报和做出明智投资决策方面面临重大挑战。股票回报的可预测性可以增强投资者信心，但仍是一项艰巨的任务。为了解决这个问题，一项研究使用长短期记忆 (LSTM) 模型来预测未来股市走势。该研究使用了来自尼日利亚证券交易所 (NSE) 的历史数据集，并对其进行了清洗和标准化以设计 LSTM 模型。该模型通过性能指标进行评估，并与人工神经网络和卷积神经网络 (CNN) 等其他深度学习模型进行了比较。实验结果表明，当使用可靠的数据集进行训练时，LSTM 模型可以以超过90%的准确率预测未来的股市价格和回报。研究得出结论，如果训练得当，LSTM 模型在预测金融时间序列相关问题方面可能非常有用。未来的研究应探索将 LSTM 模型与其他深度学习技术（如 CNN）结合，以创建混合模型，从而降低未来股票预测中仅依赖单一模型的风险。", "summary": "本研究旨在解决股票回报预测的挑战，通过使用长短期记忆 (LSTM) 模型预测尼日利亚股市的未来走势。研究利用尼日利亚证券交易所的历史数据进行模型训练和评估，并将其性能与人工神经网络和卷积神经网络进行了比较。结果显示，LSTM 模型能够以超过90%的准确率预测股市价格和回报。论文总结认为，LSTM 模型在金融时间序列预测中具有潜力，并建议未来研究探索结合其他深度学习技术以构建更稳健的混合模型。", "keywords": "尼日利亚股票, LSTM, 股票回报, 预测, 深度学习", "comments": "本文创新性地将LSTM模型应用于尼日利亚股票市场的回报预测，并取得了较高的准确率。其重要性在于为投资者和分析师提供了潜在的工具，以应对股市预测的挑战。然而，论文也指出了单一模型可能存在的风险，并提出了未来结合多种深度学习技术构建混合模型的建议，这体现了对模型局限性的认识和对未来研究方向的展望。"}}
{"id": "2507.02748", "title": "Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics", "authors": ["Alex Colagrande", "Paul Caillon", "Eva Feillet", "Alexandre Allauzen"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ECLR Workshop at ICCV 2025", "url": "http://arxiv.org/abs/2507.02748v1", "summary": "Transformers have become the de facto standard for a wide range of tasks,\nfrom image classification to physics simulations. Despite their impressive\nperformance, the quadratic complexity of standard Transformers in both memory\nand time with respect to the input length makes them impractical for processing\nhigh-resolution inputs. Therefore, several variants have been proposed, the\nmost successful relying on patchification, downsampling, or coarsening\ntechniques, often at the cost of losing the finest-scale details. In this work,\nwe take a different approach. Inspired by state-of-the-art techniques in\n$n$-body numerical simulations, we cast attention as an interaction problem\nbetween grid points. We introduce the Multipole Attention Neural Operator\n(MANO), which computes attention in a distance-based multiscale fashion. MANO\nmaintains, in each attention head, a global receptive field and achieves linear\ntime and memory complexity with respect to the number of grid points. Empirical\nresults on image classification and Darcy flows demonstrate that MANO rivals\nstate-of-the-art models such as ViT and Swin Transformer, while reducing\nruntime and peak memory usage by orders of magnitude. We open source our code\nfor reproducibility at https://github.com/AlexColagrande/MANO.", "comment": "Accepted at ECLR Workshop at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.02748v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "具有全局上下文的线性注意力：一种用于视觉和物理的多极注意力机制", "tldr": "本文提出了一种名为多极注意力神经算子（MANO）的新型线性注意力机制，它通过将注意力视为网格点之间的相互作用问题，解决了标准Transformer在处理高分辨率输入时二次复杂性过高的问题。MANO在图像分类和物理模拟任务上表现出与SOTA模型相当的性能，同时显著降低了运行时和内存消耗。", "motivation": "标准Transformer在处理高分辨率输入时，其时间与内存复杂度呈二次方增长，导致不切实际。现有的解决方案（如分块、下采样）常常会丢失精细尺度的细节。因此，本文旨在开发一种能处理高分辨率输入，同时保持线性复杂度的注意力机制。", "method": "本文受n体数值模拟中先进技术的启发，将注意力视为网格点之间的相互作用问题。提出了一种名为多极注意力神经算子（MANO）的新型机制，该机制以基于距离的多尺度方式计算注意力。MANO在每个注意力头中都保持一个全局感受野，并实现了相对于网格点数量的线性时间和内存复杂度。", "result": "在图像分类和达西流（Darcy flows）任务上的实证结果表明，MANO与ViT和Swin Transformer等最先进的模型性能相当，同时将运行时和峰值内存使用量降低了几个数量级。", "conclusion": "MANO是一种高效且有效的注意力机制，能够解决标准Transformer在处理高分辨率输入时面临的二次复杂性问题，并在视觉和物理任务中展现出卓越的性能和效率。", "translation": "Transformer已成为从图像分类到物理模拟等各种任务的事实标准。尽管它们性能令人印象深刻，但标准Transformer在处理高分辨率输入时，其相对于输入长度的内存和时间复杂度呈二次方增长，使其变得不切实际。因此，人们提出了几种变体，其中最成功的方法依赖于分块、下采样或粗化技术，这通常以牺牲最精细尺度的细节为代价。在这项工作中，我们采取了不同的方法。受n体数值模拟中先进技术的启发，我们将注意力视为网格点之间的相互作用问题。我们引入了多极注意力神经算子（MANO），它以基于距离的多尺度方式计算注意力。MANO在每个注意力头中都保持一个全局感受野，并实现了相对于网格点数量的线性时间和内存复杂度。在图像分类和达西流上的实证结果表明，MANO与ViT和Swin Transformer等最先进的模型性能相当，同时将运行时和峰值内存使用量降低了几个数量级。我们在https://github.com/AlexColagrande/MANO上开源了代码以供复现。", "summary": "本文提出了一种名为多极注意力神经算子（MANO）的新型线性注意力机制，旨在解决标准Transformer在处理高分辨率输入时面临的二次时间与内存复杂度问题。MANO借鉴了n体数值模拟思想，将注意力建模为网格点间的相互作用，并采用基于距离的多尺度计算方式，实现了线性复杂度和全局感受野。实验证明，MANO在图像分类和达西流任务上不仅性能媲美现有SOTA模型，还能大幅降低运行时间和内存消耗。", "keywords": "线性注意力, 多极注意力, Transformer, 高分辨率, 物理模拟", "comments": "MANO的创新之处在于其将n体数值模拟思想引入注意力机制，提出了一种全新的多极注意力计算范式。这种方法成功地将Transformer的复杂度从二次方降低到线性，同时通过保持全局感受野避免了传统下采样方法丢失细节的问题。这对于处理高分辨率图像和复杂物理模拟具有重要意义，是Transformer领域一个非常有前景的进展。"}}
{"id": "2507.01970", "title": "News Sentiment Embeddings for Stock Price Forecasting", "authors": ["Ayaan Qayyum"], "categories": ["q-fin.ST", "cs.LG"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "Comments:      12 pages, 11 figures", "url": "http://arxiv.org/abs/2507.01970v1", "summary": "This paper will discuss how headline data can be used to predict stock\nprices. The stock price in question is the SPDR S&P 500 ETF Trust, also known\nas SPY that tracks the performance of the largest 500 publicly traded\ncorporations in the United States. A key focus is to use news headlines from\nthe Wall Street Journal (WSJ) to predict the movement of stock prices on a\ndaily timescale with OpenAI-based text embedding models used to create vector\nencodings of each headline with principal component analysis (PCA) to exact the\nkey features. The challenge of this work is to capture the time-dependent and\ntime-independent, nuanced impacts of news on stock prices while handling\npotential lag effects and market noise. Financial and economic data were\ncollected to improve model performance; such sources include the U.S. Dollar\nIndex (DXY) and Treasury Interest Yields. Over 390 machine-learning inference\nmodels were trained. The preliminary results show that headline data embeddings\ngreatly benefit stock price prediction by at least 40% compared to training and\noptimizing a machine learning system without headline data embeddings.", "comment": "12 pages, 11 figures", "pdf_url": "http://arxiv.org/pdf/2507.01970v1", "cate": "q-fin.ST", "date": "2025-06-19", "updated": "2025-06-19", "AI": {"title_translation": "新闻情感嵌入在股票价格预测中的应用", "tldr": "本文探讨了如何利用新闻标题数据预测SPY股票价格，通过OpenAI文本嵌入模型和PCA提取特征，并结合金融经济数据，结果显示标题数据嵌入能显著提升预测准确率至少40%。", "motivation": "本文旨在探讨如何利用新闻标题数据来预测股票价格，特别关注SPDR S&P 500 ETF Trust (SPY) 的表现。", "method": "研究方法包括使用《华尔街日报》的新闻标题，利用基于OpenAI的文本嵌入模型创建每个标题的向量编码，并运用主成分分析（PCA）提取关键特征。此外，还收集了美国美元指数（DXY）和国债利率等金融经济数据以提升模型性能。共训练了390多个机器学习推理模型。", "result": "初步结果显示，与不使用标题数据嵌入的机器学习系统相比，标题数据嵌入极大地提升了股票价格预测的准确性，至少提高了40%。", "conclusion": "新闻标题数据嵌入对股票价格预测具有显著的益处，能够有效提升预测模型的性能。", "translation": "本文将讨论如何使用新闻标题数据来预测股票价格。所讨论的股票价格是SPDR S&P 500 ETF Trust，也称为SPY，它追踪美国最大的500家上市公司的表现。一个关键的重点是使用《华尔街日报》（WSJ）的新闻标题来预测股票价格的每日变动，其中使用基于OpenAI的文本嵌入模型创建每个标题的向量编码，并使用主成分分析（PCA）提取关键特征。这项工作的挑战在于捕捉新闻对股票价格的时间依赖性和时间无关性、细微影响，同时处理潜在的滞后效应和市场噪音。收集了金融和经济数据以提高模型性能；这些来源包括美国美元指数（DXY）和国债收益率。训练了超过390个机器学习推理模型。初步结果显示，与不使用新闻标题数据嵌入的机器学习系统相比，新闻标题数据嵌入极大地有利于股票价格预测，至少提高了40%。", "summary": "本文研究了新闻标题情感嵌入在股票价格预测中的应用，以SPDR S&P 500 ETF Trust (SPY) 为研究对象。通过对《华尔街日报》新闻标题进行OpenAI文本嵌入和PCA处理，并结合DXY和国债收益率等金融数据，构建了390多个机器学习模型。实验结果表明，新闻标题数据嵌入能够显著提升股票价格预测的准确性，至少带来40%的性能提升。", "keywords": "新闻情感,股票价格预测,文本嵌入,SPY,机器学习", "comments": "该研究的创新之处在于利用OpenAI的文本嵌入模型处理新闻标题，并结合PCA提取特征，以捕捉新闻对股票价格的复杂影响。其重要性体现在显著提升了股票价格预测的准确率，证明了新闻情感数据在金融预测中的巨大潜力。文章也提到了处理滞后效应和市场噪音的挑战，但未详细说明如何解决。"}}
{"id": "2507.02751", "title": "Partial Weakly-Supervised Oriented Object Detection", "authors": ["Mingxin Liu", "Peiyuan Zhang", "Yuan Liu", "Wei Zhang", "Yue Zhou", "Ning Liao", "Ziyang Gong", "Junwei Luo", "Zhirui Wang", "Yi Yu", "Xue Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 5 figures, 4 tables, source code: this https URL", "url": "http://arxiv.org/abs/2507.02751v1", "summary": "The growing demand for oriented object detection (OOD) across various domains\nhas driven significant research in this area. However, the high cost of dataset\nannotation remains a major concern. Current mainstream OOD algorithms can be\nmainly categorized into three types: (1) fully supervised methods using\ncomplete oriented bounding box (OBB) annotations, (2) semi-supervised methods\nusing partial OBB annotations, and (3) weakly supervised methods using weak\nannotations such as horizontal boxes or points. However, these algorithms\ninevitably increase the cost of models in terms of annotation speed or\nannotation cost. To address this issue, we propose:(1) the first Partial\nWeakly-Supervised Oriented Object Detection (PWOOD) framework based on\npartially weak annotations (horizontal boxes or single points), which can\nefficiently leverage large amounts of unlabeled data, significantly\noutperforming weakly supervised algorithms trained with partially weak\nannotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware\nStudent (OS-Student) model capable of learning orientation and scale\ninformation with only a small amount of orientation-agnostic or scale-agnostic\nweak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF)\nto reduce the model's sensitivity to static filtering thresholds. Comprehensive\nexperiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD\nframework performs comparably to, or even surpasses, traditional\nsemi-supervised algorithms.", "comment": "10 pages, 5 figures, 4 tables, source code:\n  https://github.com/VisionXLab/PWOOD", "pdf_url": "http://arxiv.org/pdf/2507.02751v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "部分弱监督定向目标检测", "tldr": "提出了一种新的部分弱监督定向目标检测框架PWOOD，通过利用部分弱标注和未标注数据，显著降低了标注成本，并在多个数据集上表现优于或媲美传统半监督算法。", "motivation": "定向目标检测（OOD）的数据集标注成本高昂，现有主流算法（全监督、半监督、弱监督）在标注速度或成本方面都增加了模型成本。", "method": "提出(1)首个基于部分弱标注（水平框或单点）的部分弱监督定向目标检测（PWOOD）框架，能有效利用大量未标注数据；(2)定向和尺度感知学生（OS-Student）模型，能用少量与方向或尺度无关的弱标注学习方向和尺度信息；(3)类别无关伪标签过滤（CPF）策略，以降低模型对静态过滤阈值的敏感性。", "result": "在DOTA-v1.0/v1.5/v2.0和DIOR数据集上的综合实验表明，PWOOD框架的性能与传统半监督算法相当，甚至超越了它们。", "conclusion": "PWOOD框架通过利用部分弱标注和未标注数据，有效降低了定向目标检测的标注成本，并取得了与传统半监督方法相当或更优的性能，为解决高昂标注成本问题提供了一个低成本高效的解决方案。", "translation": "定向目标检测（OOD）在各个领域日益增长的需求推动了该领域的重大研究。然而，数据集标注的高成本仍然是一个主要问题。当前主流的OOD算法主要分为三类：（1）使用完整定向边界框（OBB）标注的全监督方法；（2）使用部分OBB标注的半监督方法；（3）使用水平框或点等弱标注的弱监督方法。然而，这些算法不可避免地在标注速度或标注成本方面增加了模型的成本。为了解决这个问题，我们提出：(1)首个基于部分弱标注（水平框或单点）的部分弱监督定向目标检测（PWOOD）框架，该框架可以有效利用大量未标注数据，显著优于使用部分弱标注训练的弱监督算法，并提供更低的成本解决方案；(2)定向和尺度感知学生（OS-Student）模型，能够仅用少量与方向无关或与尺度无关的弱标注学习方向和尺度信息；以及(3)类别无关伪标签过滤（CPF）策略，以降低模型对静态过滤阈值的敏感性。在DOTA-v1.0/v1.5/v2.0和DIOR数据集上的综合实验表明，我们的PWOOD框架的性能与传统半监督算法相当，甚至超越了它们。", "summary": "该论文提出了一种名为PWOOD（Partial Weakly-Supervised Oriented Object Detection）的新型框架，旨在解决定向目标检测中高昂的数据标注成本问题。PWOOD利用部分弱标注（如水平框或单点）和大量未标注数据进行训练，并通过引入OS-Student模型学习方向和尺度信息，以及CPF策略优化伪标签过滤。实验证明，PWOOD在DOTA和DIOR等数据集上表现出与传统半监督算法媲美甚至超越的性能，提供了一种更经济高效的解决方案。", "keywords": "定向目标检测, 弱监督, 部分标注, 半监督, 标注成本", "comments": "这篇论文的创新点在于首次提出了部分弱监督的定向目标检测框架，有效结合了弱监督和半监督的优势，显著降低了标注成本。通过引入OS-Student模型和CPF策略，解决了弱标注下方向和尺度信息学习以及伪标签过滤的挑战。其性能与传统半监督方法相当甚至更优，证明了该方法的实用性和高效性。"}}
{"id": "2507.02792", "title": "RichControl: Structure- and Appearance-Rich Training-Free Spatial Control for Text-to-Image Generation", "authors": ["Liheng Zhang", "Lexi Pang", "Hang Ye", "Xiaoxuan Ma", "Yizhou Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02792v1", "summary": "Text-to-image (T2I) diffusion models have shown remarkable success in\ngenerating high-quality images from text prompts. Recent efforts extend these\nmodels to incorporate conditional images (e.g., depth or pose maps) for\nfine-grained spatial control. Among them, feature injection methods have\nemerged as a training-free alternative to traditional fine-tuning approaches.\nHowever, they often suffer from structural misalignment, condition leakage, and\nvisual artifacts, especially when the condition image diverges significantly\nfrom natural RGB distributions. By revisiting existing methods, we identify a\ncore limitation: the synchronous injection of condition features fails to\naccount for the trade-off between domain alignment and structural preservation\nduring denoising. Inspired by this observation, we propose a flexible feature\ninjection framework that decouples the injection timestep from the denoising\nprocess. At its core is a structure-rich injection module, which enables the\nmodel to better adapt to the evolving interplay between alignment and structure\npreservation throughout the diffusion steps, resulting in more faithful\nstructural generation. In addition, we introduce appearance-rich prompting and\na restart refinement strategy to further enhance appearance control and visual\nquality. Together, these designs enable training-free generation that is both\nstructure-rich and appearance-rich. Extensive experiments show that our\napproach achieves state-of-the-art performance across diverse zero-shot\nconditioning scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02792v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "RichControl：结构与外观丰富的免训练文本到图像生成空间控制", "tldr": "提出RichControl，一个灵活的免训练特征注入框架，通过解耦注入时间步和去噪过程，实现对文本到图像生成的结构和外观的精确空间控制，解决了现有方法的对齐和伪影问题，并达到SOTA性能。", "motivation": "现有的文本到图像（T2I）扩散模型的特征注入方法在结合条件图像进行精细空间控制时，常出现结构错位、条件泄露和视觉伪影，尤其当条件图像与自然RGB分布差异大时。核心限制在于同步注入未能权衡去噪过程中的域对齐与结构保持。", "method": "提出RichControl，一个灵活的特征注入框架，它将注入时间步与去噪过程解耦。核心是一个结构丰富的注入模块，使模型能更好地适应扩散步骤中对齐和结构保持之间的动态平衡。此外，引入了外观丰富的提示和重启细化策略，以进一步增强外观控制和视觉质量。", "result": "我们的方法在各种零样本条件场景下取得了最先进的性能。", "conclusion": "RichControl通过解耦注入时间步、引入结构丰富的注入模块、外观丰富的提示和重启细化策略，实现了结构和外观都丰富的免训练文本到图像生成，有效解决了现有方法的局限性并达到了卓越的控制效果。", "translation": "文本到图像（T2I）扩散模型在从文本提示生成高质量图像方面取得了显著成功。最近的工作扩展了这些模型，以结合条件图像（例如，深度图或姿态图）进行精细的空间控制。其中，特征注入方法已成为传统微调方法的免训练替代方案。然而，它们常常遭受结构错位、条件泄露和视觉伪影，特别是当条件图像与自然RGB分布显著不同时。通过重新审视现有方法，我们发现了一个核心限制：条件特征的同步注入未能考虑去噪过程中域对齐和结构保持之间的权衡。受此观察启发，我们提出了一个灵活的特征注入框架，它将注入时间步与去噪过程解耦。其核心是一个结构丰富的注入模块，使模型能够更好地适应扩散步骤中对齐和结构保持之间不断演变的关系，从而实现更忠实的结构生成。此外，我们引入了外观丰富的提示和重启细化策略，以进一步增强外观控制和视觉质量。总的来说，这些设计实现了结构丰富和外观丰富的免训练生成。广泛的实验表明，我们的方法在各种零样本条件场景下均取得了最先进的性能。", "summary": "该论文提出了RichControl，一个用于文本到图像生成的免训练空间控制框架。针对现有特征注入方法在结构错位、条件泄露和视觉伪影方面的局限性，RichControl通过解耦特征注入时间步与去噪过程，并引入一个结构丰富的注入模块，实现了更精确的结构保持。同时，结合外观丰富的提示和重启细化策略，进一步提升了外观控制和图像质量。实验证明，该方法在多种零样本条件场景下均达到最先进的性能，提供了结构和外观兼备的免训练生成能力。", "keywords": "文本到图像生成, 空间控制, 特征注入, 免训练, 扩散模型", "comments": "这篇论文的创新点在于提出了一个灵活的特征注入框架，通过解耦注入时间步与去噪过程，有效解决了现有免训练方法在结构保持和域对齐之间的矛盾。结构丰富的注入模块、外观丰富的提示和重启细化策略的结合，使其在无需训练的情况下实现对文本到图像生成的高度空间控制，具有重要的实际应用价值。"}}
{"id": "2507.02798", "title": "No time to train! Training-Free Reference-Based Instance Segmentation", "authors": ["Miguel Espinosa", "Chenhongyi Yang", "Linus Ericsson", "Steven McDonagh", "Elliot J. Crowley"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.02798v1", "summary": "The performance of image segmentation models has historically been\nconstrained by the high cost of collecting large-scale annotated data. The\nSegment Anything Model (SAM) alleviates this original problem through a\npromptable, semantics-agnostic, segmentation paradigm and yet still requires\nmanual visual-prompts or complex domain-dependent prompt-generation rules to\nprocess a new image. Towards reducing this new burden, our work investigates\nthe task of object segmentation when provided with, alternatively, only a small\nset of reference images. Our key insight is to leverage strong semantic priors,\nas learned by foundation models, to identify corresponding regions between a\nreference and a target image. We find that correspondences enable automatic\ngeneration of instance-level segmentation masks for downstream tasks and\ninstantiate our ideas via a multi-stage, training-free method incorporating (1)\nmemory bank construction; (2) representation aggregation and (3) semantic-aware\nfeature matching. Our experiments show significant improvements on segmentation\nmetrics, leading to state-of-the-art performance on COCO FSOD (36.8% nAP),\nPASCAL VOC Few-Shot (71.2% nAP50) and outperforming existing training-free\napproaches on the Cross-Domain FSOD benchmark (22.4% nAP).", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.02798v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "没时间训练！免训练的基于参考的实例分割", "tldr": "该论文提出了一种无需训练的实例分割方法，利用参考图像和基础模型的语义先验，在多个基准测试中取得了最先进的性能。", "motivation": "图像分割模型受限于高昂的标注数据收集成本，而SAM虽缓解了此问题，但仍需手动提示或复杂规则。本文旨在通过仅使用少量参考图像来减轻这一新负担，研究目标分割任务。", "method": "利用基础模型学习到的强大语义先验来识别参考图像和目标图像之间的对应区域，从而自动生成实例级分割掩码。该方法是一个多阶段、免训练的过程，包括：1) 记忆库构建；2) 表示聚合；3) 语义感知特征匹配。", "result": "在分割指标上实现了显著改进，在COCO FSOD上达到36.8% nAP的最先进性能，在PASCAL VOC Few-Shot上达到71.2% nAP50的最先进性能，并超越了Cross-Domain FSOD基准上现有的免训练方法（22.4% nAP）。", "conclusion": "本文成功展示了一种无需训练的基于参考的实例分割方法，解决了现有模型在数据标注和手动提示方面的局限性，并取得了最先进的性能。", "translation": "图像分割模型的性能历来受到收集大规模标注数据高成本的限制。Segment Anything Model (SAM) 通过可提示的、语义无关的分割范式缓解了最初的问题，但仍然需要手动视觉提示或复杂的域相关提示生成规则来处理新图像。为了减轻这一新负担，我们的工作研究了在仅提供少量参考图像的情况下进行目标分割的任务。我们的关键见解是利用基础模型学习到的强大语义先验，以识别参考图像和目标图像之间的对应区域。我们发现对应关系能够为下游任务自动生成实例级分割掩码，并通过一个多阶段、免训练的方法实例化了我们的思想，该方法包括 (1) 记忆库构建；(2) 表示聚合和 (3) 语义感知特征匹配。我们的实验显示分割指标有显著改进，在 COCO FSOD 上取得了最先进的性能 (36.8% nAP)，在 PASCAL VOC Few-Shot 上取得了最先进的性能 (71.2% nAP50)，并且在跨域 FSOD 基准上超越了现有的免训练方法 (22.4% nAP)。", "summary": "本文提出了一种新颖的免训练的基于参考的实例分割方法，旨在减少对大量数据标注和手动提示的依赖。其核心思想是利用预训练基础模型中的语义先验来建立参考图像和目标图像之间的对应关系，从而自动生成实例级掩码。所提出的多阶段方法包括记忆库构建、表示聚合和语义感知特征匹配。实验结果表明，该方法在多个少样本和跨域分割基准上取得了显著的性能提升，达到了最先进水平。", "keywords": "实例分割, 免训练, 基于参考, 基础模型, 少样本学习", "comments": "本文提出了一种创新的实例分割方法，完全取消了训练阶段，转而依赖参考图像和基础模型中的强大语义先验。这种“免训练”范式意义重大，因为它直接解决了计算机视觉中臭名昭著的数据标注瓶颈，尤其适用于数据稀缺的新领域或任务。该方法无需显式训练步骤即可实现SOTA结果，是一项显著的进步，凸显了利用基础模型中预学习知识进行高效和自适应视觉任务的潜力。"}}
{"id": "2507.02345", "title": "HelixDesign-Antibody: A Scalable Production-Grade Platform for Antibody Design Built on HelixFold3", "authors": ["Jie Gao", "Jing Hu", "Shanzhuo Zhang", "Kunrui Zhu", "Sheng Qian", "Yueyang Huang", "Xiaonan Zhang", "Xiaomin Fang"], "categories": ["q-bio.BM", "cs.AI"], "primary_category": "Subjects:       Biomolecules (q-bio.BM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02345v1", "summary": "Antibody engineering is essential for developing therapeutics and advancing\nbiomedical research. Traditional discovery methods often rely on time-consuming\nand resource-intensive experimental screening. To enhance and streamline this\nprocess, we introduce a production-grade, high-throughput platform built on\nHelixFold3, HelixDesign-Antibody, which utilizes the high-accuracy structure\nprediction model, HelixFold3. The platform facilitates the large-scale\ngeneration of antibody candidate sequences and evaluates their interaction with\nantigens. Integrated high-performance computing (HPC) support enables\nhigh-throughput screening, addressing challenges such as fragmented toolchains\nand high computational demands. Validation on multiple antigens showcases the\nplatform's ability to generate diverse and high-quality antibodies, confirming\na scaling law where exploring larger sequence spaces increases the likelihood\nof identifying optimal binders. This platform provides a seamless, accessible\nsolution for large-scale antibody design and is available via the antibody\ndesign page of PaddleHelix platform.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02345v1", "cate": "q-bio.BM", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "HelixDesign-Antibody：一个基于HelixFold3的可扩展生产级抗体设计平台", "tldr": "HelixDesign-Antibody是一个基于HelixFold3的高通量平台，旨在通过大规模生成和评估抗体序列来加速抗体工程，解决传统方法的耗时和计算挑战，并已在多个抗原上得到验证。", "motivation": "传统的抗体发现方法耗时且资源密集，依赖实验筛选。为了增强和简化这一过程，需要一个高效、可扩展的平台。", "method": "该研究引入了一个基于高精度结构预测模型HelixFold3的生产级、高通量平台HelixDesign-Antibody。该平台能够大规模生成抗体候选序列并评估它们与抗原的相互作用，并集成了高性能计算（HPC）支持以实现高通量筛选。", "result": "该平台在多个抗原上进行了验证，展示了其生成多样化和高质量抗体的能力。研究证实了一个缩放定律，即探索更大的序列空间会增加识别最佳结合物的可能性。", "conclusion": "HelixDesign-Antibody提供了一个无缝、易于访问的大规模抗体设计解决方案，并通过PaddleHelix平台的抗体设计页面提供。", "translation": "抗体工程对于开发治疗药物和推进生物医学研究至关重要。传统的发现方法通常依赖耗时且资源密集的实验筛选。为了增强和简化这一过程，我们引入了一个基于HelixFold3的生产级、高通量平台HelixDesign-Antibody，该平台利用高精度结构预测模型HelixFold3。该平台促进大规模生成抗体候选序列并评估它们与抗原的相互作用。集成的HPC（高性能计算）支持实现了高通量筛选，解决了工具链碎片化和高计算需求等挑战。在多个抗原上的验证展示了该平台生成多样化和高质量抗体的能力，证实了一个缩放定律，即探索更大的序列空间会增加识别最佳结合物的可能性。该平台为大规模抗体设计提供了一个无缝、易于访问的解决方案，并通过PaddleHelix平台的抗体设计页面提供。", "summary": "HelixDesign-Antibody是一个基于HelixFold3的高通量、生产级平台，旨在革新抗体工程。它通过利用高精度结构预测模型，实现大规模抗体序列的生成与抗原相互作用评估。该平台整合了高性能计算，解决了传统方法的效率低下和计算瓶颈问题。实验验证表明，它能够生成多样化且高质量的抗体，并揭示了探索更广阔序列空间有助于发现更优结合物的规律。该平台为大规模抗体设计提供了便捷高效的解决方案。", "keywords": "抗体设计, HelixFold3, 高通量筛选, 生产级平台, 抗体工程", "comments": "该论文介绍的HelixDesign-Antibody平台在抗体设计领域具有重要意义。其创新之处在于将高精度结构预测模型HelixFold3与高性能计算相结合，提供了一个端到端、可扩展的解决方案，有效解决了传统抗体发现过程中耗时、资源密集以及工具链碎片化的问题。该平台的生产级特性和高通量能力，有望显著加速治疗性抗体的研发进程。此外，其验证结果揭示的“缩放定律”为未来抗体设计策略提供了有价值的指导。"}}
{"id": "2507.01980", "title": "Detecting Fraud in Financial Networks: A Semi-Supervised GNN Approach with Granger-Causal Explanations", "authors": ["Linh Nguyen", "Marcel Boersma", "Erman Acar"], "categories": ["q-fin.ST", "cs.LG", "stat.ML"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01980v1", "summary": "Fraudulent activity in the financial industry costs billions annually.\nDetecting fraud, therefore, is an essential yet technically challenging task\nthat requires carefully analyzing large volumes of data. While machine learning\n(ML) approaches seem like a viable solution, applying them successfully is not\nso easy due to two main challenges: (1) the sparsely labeled data, which makes\nthe training of such approaches challenging (with inherent labeling costs), and\n(2) lack of explainability for the flagged items posed by the opacity of ML\nmodels, that is often required by business regulations. This article proposes\nSAGE-FIN, a semi-supervised graph neural network (GNN) based approach with\nGranger causal explanations for Financial Interaction Networks. SAGE-FIN learns\nto flag fraudulent items based on weakly labeled (or unlabelled) data points.\nTo adhere to regulatory requirements, the flagged items are explained by\nhighlighting related items in the network using Granger causality. We\nempirically validate the favorable performance of SAGE-FIN on a real-world\ndataset, Bipartite Edge-And-Node Attributed financial network (Elliptic++),\nwith Granger-causal explanations for the identified fraudulent items without\nany prior assumption on the network structure.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01980v1", "cate": "q-fin.ST", "date": "2025-06-25", "updated": "2025-06-25", "AI": {"title_translation": "金融网络中的欺诈检测：一种基于格兰杰因果解释的半监督图神经网络方法", "tldr": "SAGE-FIN是一种半监督图神经网络，结合格兰杰因果解释，用于检测和解释金融欺诈，解决了数据稀疏和可解释性问题。", "motivation": "金融行业的欺诈活动每年造成数十亿美元的损失，欺诈检测至关重要但技术上具有挑战性。主要挑战是数据标注稀疏和机器学习模型缺乏可解释性。", "method": "本文提出SAGE-FIN，这是一种基于半监督图神经网络（GNN）的方法，结合格兰杰因果解释用于金融交互网络。SAGE-FIN利用弱标注（或未标注）数据点来识别欺诈项，并使用格兰杰因果关系解释识别出的欺诈项。", "result": "SAGE-FIN在真实世界数据集Elliptic++上表现出良好的性能，并为识别出的欺诈项提供了格兰杰因果解释，无需预先假设网络结构。", "conclusion": "SAGE-FIN成功地解决了金融欺诈检测中数据标注稀疏和模型可解释性不足的问题，并在真实数据集上表现良好。", "translation": "金融行业的欺诈活动每年造成数十亿美元的损失。因此，欺诈检测是一项重要但技术上具有挑战性的任务，需要仔细分析大量数据。虽然机器学习（ML）方法似乎是可行的解决方案，但由于两个主要挑战，成功应用它们并不容易：(1) 数据标注稀疏，这使得此类方法的训练具有挑战性（并伴随固有的标注成本），以及 (2) ML模型的不透明性导致对标记项缺乏可解释性，而这通常是业务法规所要求的。本文提出了SAGE-FIN，这是一种基于半监督图神经网络（GNN）的方法，结合格兰杰因果解释用于金融交互网络。SAGE-FIN学习根据弱标注（或未标注）数据点标记欺诈项。为了遵守监管要求，通过使用格兰杰因果关系突出网络中的相关项来解释标记的项。我们通过经验验证了SAGE-FIN在真实世界数据集（二分边和节点属性金融网络Elliptic++）上的良好性能，并为识别出的欺诈项提供了格兰杰因果解释，无需对网络结构进行任何先验假设。", "summary": "针对金融欺诈检测中数据标注稀疏和机器学习模型缺乏可解释性的挑战，本文提出SAGE-FIN，一种基于半监督图神经网络并结合格兰杰因果解释的方法。SAGE-FIN能够利用弱标注数据识别欺诈行为，并提供可解释的欺诈项。在真实世界数据集上的实验验证了SAGE-FIN的良好性能和其解释能力。", "keywords": "欺诈检测, 金融网络, 半监督学习, 图神经网络, 格兰杰因果关系", "comments": "这项工作通过结合半监督图神经网络和格兰杰因果关系，为金融欺诈检测提供了一种新颖且具有可解释性的方法。它有效地解决了金融领域数据标注稀疏和模型透明度不足的实际挑战，对于满足监管要求和提高欺诈检测的实用性具有重要意义。"}}
{"id": "2507.02813", "title": "LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion", "authors": ["Fangfu Liu", "Hao Li", "Jiawei Chi", "Hanyang Wang", "Minghui Yang", "Fudong Wang", "Yueqi Duan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.02813v1", "summary": "Recovering 3D structures with open-vocabulary scene understanding from 2D\nimages is a fundamental but daunting task. Recent developments have achieved\nthis by performing per-scene optimization with embedded language information.\nHowever, they heavily rely on the calibrated dense-view reconstruction\nparadigm, thereby suffering from severe rendering artifacts and implausible\nsemantic synthesis when limited views are available. In this paper, we\nintroduce a novel generative framework, coined LangScene-X, to unify and\ngenerate 3D consistent multi-modality information for reconstruction and\nunderstanding. Powered by the generative capability of creating more consistent\nnovel observations, we can build generalizable 3D language-embedded scenes from\nonly sparse views. Specifically, we first train a TriMap video diffusion model\nthat can generate appearance (RGBs), geometry (normals), and semantics\n(segmentation maps) from sparse inputs through progressive knowledge\nintegration. Furthermore, we propose a Language Quantized Compressor (LQC),\ntrained on large-scale image datasets, to efficiently encode language\nembeddings, enabling cross-scene generalization without per-scene retraining.\nFinally, we reconstruct the language surface fields by aligning language\ninformation onto the surface of 3D scenes, enabling open-ended language\nqueries. Extensive experiments on real-world data demonstrate the superiority\nof our LangScene-X over state-of-the-art methods in terms of quality and\ngeneralizability. Project Page: https://liuff19.github.io/LangScene-X.", "comment": "Project page: https://liuff19.github.io/LangScene-X", "pdf_url": "http://arxiv.org/pdf/2507.02813v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "LangScene-X：使用TriMap视频扩散重建可泛化的3D语言嵌入场景", "tldr": "LangScene-X提出了一种新颖的生成框架，利用TriMap视频扩散模型和语言量化压缩器，仅从稀疏视图重建可泛化的3D语言嵌入场景，并在质量和泛化能力上超越了现有方法。", "motivation": "现有方法在从2D图像重建具有开放词汇场景理解的3D结构时，需要密集的视图重建，导致在有限视图下出现渲染伪影和不合理的语义合成，无法实现良好的泛化能力。", "method": "LangScene-X是一个新颖的生成框架，旨在统一和生成3D一致的多模态信息。它首先训练一个TriMap视频扩散模型，通过渐进式知识集成从稀疏输入生成外观（RGB）、几何（法线）和语义（分割图）。接着，提出一个语言量化压缩器（LQC），在大规模图像数据集上训练，以高效编码语言嵌入，从而实现跨场景泛化而无需每场景再训练。最后，通过将语言信息对齐到3D场景表面来重建语言表面场，以实现开放式语言查询。", "result": "在真实世界数据上进行的大量实验表明，LangScene-X在质量和泛化能力方面均优于现有最先进的方法。", "conclusion": "LangScene-X成功地解决了从稀疏视图重建可泛化3D语言嵌入场景的挑战，并通过其创新的生成框架和组件（如TriMap视频扩散和LQC）实现了卓越的性能和泛化能力。", "translation": "从2D图像中恢复具有开放词汇场景理解的3D结构是一项基础但艰巨的任务。最近的发展通过对嵌入语言信息的每场景优化实现了这一点。然而，它们严重依赖于校准的密集视图重建范式，因此在有限视图可用时遭受严重的渲染伪影和不合理的语义合成。在本文中，我们引入了一种新颖的生成框架，命名为LangScene-X，以统一和生成3D一致的多模态信息，用于重建和理解。借助创建更一致的新颖观测的生成能力，我们可以仅从稀疏视图构建可泛化的3D语言嵌入场景。具体来说，我们首先训练一个TriMap视频扩散模型，该模型可以通过渐进式知识集成从稀疏输入生成外观（RGB）、几何（法线）和语义（分割图）。此外，我们提出了一个语言量化压缩器（LQC），在大规模图像数据集上训练，以高效编码语言嵌入，实现无需每场景再训练的跨场景泛化。最后，我们通过将语言信息对齐到3D场景表面来重建语言表面场，从而实现开放式语言查询。在真实世界数据上进行的大量实验证明了我们的LangScene-X在质量和泛化能力方面优于现有最先进的方法。项目页面：https://liuff19.github.io/LangScene-X。", "summary": "LangScene-X是一个创新的生成框架，旨在从稀疏2D图像重建可泛化的3D语言嵌入场景，克服了现有方法对密集视图的依赖及其导致的伪影和语义合成问题。该框架核心包括一个TriMap视频扩散模型，用于生成多模态（外观、几何、语义）信息，以及一个语言量化压缩器（LQC），用于高效编码语言嵌入，实现跨场景泛化。LangScene-X通过将语言信息对齐到3D表面，支持开放式语言查询。实验证明，其在质量和泛化能力上均优于现有最先进技术。", "keywords": "3D重建, 语言嵌入场景, 视频扩散, 稀疏视图, 泛化", "comments": "LangScene-X的创新点在于其结合了TriMap视频扩散模型和语言量化压缩器（LQC），使得从稀疏视图重建可泛化的3D语言嵌入场景成为可能。它解决了现有方法在有限视图下存在的渲染伪影和语义合成问题，并通过高效的语言嵌入编码实现了跨场景泛化，这对于实际应用具有重要意义，尤其是在数据稀疏的场景下。"}}
{"id": "2507.01987", "title": "Predicting and Explaining Customer Data Sharing in the Open Banking", "authors": ["João B. G. de Brito", "Rodrigo Heldt", "Cleo S. Silveira", "Matthias Bogaert", "Guilherme B. Bucco", "Fernando B. Luce", "João L. Becker", "Filipe J. Zabala", "Michel J. Anzanello"], "categories": ["q-fin.GN", "cs.LG"], "primary_category": "Subjects:       General Finance (q-fin.GN)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.01987v1", "summary": "The emergence of Open Banking represents a significant shift in financial\ndata management, influencing financial institutions' market dynamics and\nmarketing strategies. This increased competition creates opportunities and\nchallenges, as institutions manage data inflow to improve products and services\nwhile mitigating data outflow that could aid competitors. This study introduces\na framework to predict customers' propensity to share data via Open Banking and\ninterprets this behavior through Explanatory Model Analysis (EMA). Using data\nfrom a large Brazilian financial institution with approximately 3.2 million\ncustomers, a hybrid data balancing strategy incorporating ADASYN and NEARMISS\ntechniques was employed to address the infrequency of data sharing and enhance\nthe training of XGBoost models. These models accurately predicted customer data\nsharing, achieving 91.39% accuracy for inflow and 91.53% for outflow. The EMA\nphase combined the Shapley Additive Explanations (SHAP) method with the\nClassification and Regression Tree (CART) technique, revealing the most\ninfluential features on customer decisions. Key features included the number of\ntransactions and purchases in mobile channels, interactions within these\nchannels, and credit-related features, particularly credit card usage across\nthe national banking system. These results highlight the critical role of\nmobile engagement and credit in driving customer data-sharing behaviors,\nproviding financial institutions with strategic insights to enhance\ncompetitiveness and innovation in the Open Banking environment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.01987v1", "cate": "q-fin.GN", "date": "2025-06-28", "updated": "2025-06-28", "AI": {"title_translation": "开放银行中客户数据共享的预测与解释", "tldr": "本研究建立了一个框架，通过XGBoost模型预测客户在开放银行中共享数据的倾向，并利用SHAP和CART解释了其行为，发现移动参与度和信用是关键影响因素。", "motivation": "开放银行的出现改变了金融数据管理，增加了金融机构的竞争，机构需要管理数据流入以改进产品服务，同时避免数据流出帮助竞争对手。因此，预测和解释客户数据共享行为变得至关重要。", "method": "本研究引入了一个框架，预测客户通过开放银行共享数据的倾向，并通过解释性模型分析（EMA）解释行为。使用来自一家巴西大型金融机构约320万客户的数据，采用ADASYN和NEARMISS混合数据平衡策略处理数据共享的稀疏性，以增强XGBoost模型的训练。EMA阶段结合了Shapley Additive Explanations (SHAP) 方法和分类回归树 (CART) 技术。", "result": "XGBoost模型准确预测了客户数据共享，流入准确率达到91.39%，流出准确率达到91.53%。EMA揭示了对客户决策影响最大的特征，包括移动渠道的交易和购买次数、这些渠道的互动以及信贷相关特征，特别是全国银行系统中的信用卡使用情况。", "conclusion": "研究结果强调了移动参与度和信用在驱动客户数据共享行为中的关键作用，为金融机构在开放银行环境中提升竞争力和创新提供了战略性见解。", "translation": "开放银行的出现代表了金融数据管理领域的一次重大转变，影响着金融机构的市场动态和营销策略。这种日益激烈的竞争既带来了机遇也带来了挑战，因为机构需要管理数据流入以改进产品和服务，同时还要减轻可能帮助竞争对手的数据流出。本研究引入了一个框架，用于预测客户通过开放银行共享数据的倾向，并通过解释性模型分析（EMA）来解释这种行为。研究使用了来自一家拥有约320万客户的巴西大型金融机构的数据，并采用了结合ADASYN和NEARMISS技术的混合数据平衡策略，以解决数据共享的稀疏性问题，并增强XGBoost模型的训练。这些模型准确地预测了客户数据共享行为，流入准确率达到91.39%，流出准确率达到91.53%。EMA阶段将Shapley加性解释（SHAP）方法与分类回归树（CART）技术相结合，揭示了对客户决策影响最大的特征。关键特征包括移动渠道的交易和购买次数、这些渠道的互动以及信贷相关特征，特别是全国银行系统中的信用卡使用情况。这些结果突出了移动参与度和信用在驱动客户数据共享行为中的关键作用，为金融机构在开放银行环境中提升竞争力和创新提供了战略性见解。", "summary": "本研究提出了一个框架，用于预测和解释客户在开放银行中的数据共享行为。利用巴西一家大型金融机构的320万客户数据，研究采用ADASYN和NEARMISS混合平衡策略训练XGBoost模型，成功预测了客户数据流入（91.39%准确率）和流出（91.53%准确率）。通过结合SHAP和CART的解释性模型分析，研究识别出移动渠道的交易、互动以及信用卡使用等信贷相关特征是影响客户数据共享决策的关键因素。这些发现为金融机构在开放银行环境下提升竞争力提供了重要战略见解。", "keywords": "开放银行, 客户数据共享, XGBoost, SHAP, 解释性模型分析", "comments": "本研究的创新之处在于将预测模型（XGBoost）与可解释人工智能（SHAP和CART）相结合，应用于开放银行中客户数据共享这一特定场景，从而提供了可操作的洞察。这对于金融机构理解并影响客户行为具有重要意义，有助于他们在竞争激烈的市场中制定更有效的战略。"}}
{"id": "2507.02826", "title": "Confidence-driven Gradient Modulation for Multimodal Human Activity Recognition: A Dynamic Contrastive Dual-Path Learning Approach", "authors": ["Panpan Ji", "Junni Song", "Hang Xiao", "Hanyu Liu", "Chao Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02826v1", "summary": "Sensor-based Human Activity Recognition (HAR) is a core technology that\nenables intelligent systems to perceive and interact with their environment.\nHowever, multimodal HAR systems still encounter key challenges, such as\ndifficulties in cross-modal feature alignment and imbalanced modality\ncontributions. To address these issues, we propose a novel framework called the\nDynamic Contrastive Dual-Path Network (DCDP-HAR). The framework comprises three\nkey components. First, a dual-path feature extraction architecture is employed,\nwhere ResNet and DenseNet branches collaboratively process multimodal sensor\ndata. Second, a multi-stage contrastive learning mechanism is introduced to\nachieve progressive alignment from local perception to semantic abstraction.\nThird, we present a confidence-driven gradient modulation strategy that\ndynamically monitors and adjusts the learning intensity of each modality branch\nduring backpropagation, effectively alleviating modality competition. In\naddition, a momentum-based gradient accumulation strategy is adopted to enhance\ntraining stability. We conduct ablation studies to validate the effectiveness\nof each component and perform extensive comparative experiments on four public\nbenchmark datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02826v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "置信度驱动的梯度调制多模态人体活动识别：一种动态对比双路径学习方法", "tldr": "本文提出了一种名为DCDP-HAR的新颖框架，通过双路径特征提取、多阶段对比学习和置信度驱动的梯度调制，解决了多模态人体活动识别中跨模态特征对齐困难和模态贡献不平衡的问题。", "motivation": "多模态人体活动识别（HAR）系统面临跨模态特征对齐困难和模态贡献不平衡的关键挑战。", "method": "本文提出动态对比双路径网络（DCDP-HAR）框架，包含三个关键组件：1) 双路径特征提取架构（ResNet和DenseNet分支协同处理）；2) 多阶段对比学习机制实现从局部感知到语义抽象的渐进对齐；3) 置信度驱动的梯度调制策略动态调整各模态学习强度以缓解模态竞争。此外，还采用基于动量的梯度累积策略增强训练稳定性。", "result": "通过消融研究验证了每个组件的有效性，并在四个公共基准数据集上进行了广泛的比较实验。", "conclusion": "Not mentioned in abstract", "translation": "基于传感器的活动识别（HAR）是使智能系统感知并与其环境交互的核心技术。然而，多模态HAR系统仍面临关键挑战，例如跨模态特征对齐困难和模态贡献不平衡。为解决这些问题，我们提出了一种名为动态对比双路径网络（DCDP-HAR）的新颖框架。该框架包含三个关键组件。首先，采用双路径特征提取架构，其中ResNet和DenseNet分支协同处理多模态传感器数据。其次，引入多阶段对比学习机制，以实现从局部感知到语义抽象的渐进式对齐。第三，我们提出了一种置信度驱动的梯度调制策略，该策略在反向传播过程中动态监控和调整每个模态分支的学习强度，有效缓解模态竞争。此外，还采用了基于动量的梯度累积策略来增强训练稳定性。我们进行了消融研究以验证每个组件的有效性，并在四个公共基准数据集上进行了广泛的比较实验。", "summary": "本文提出了一种名为动态对比双路径网络（DCDP-HAR）的新型框架，用于多模态人体活动识别（HAR）。该框架通过双路径特征提取架构、多阶段对比学习机制以及置信度驱动的梯度调制策略，解决了跨模态特征对齐困难和模态贡献不平衡等挑战。此外，它还采用基于动量的梯度累积策略以增强训练稳定性。该框架通过消融研究和在公共数据集上的比较实验验证了其有效性。", "keywords": "多模态HAR, 梯度调制, 对比学习, 双路径网络, 传感器基", "comments": "本文的创新点在于结合了双路径架构、多阶段对比学习以及置信度驱动的梯度调制策略，以动态平衡多模态学习中的模态贡献，这解决了多模态HAR系统中的关键挑战。基于动量的梯度累积策略也增强了训练的实用性和稳定性。"}}
{"id": "2507.02827", "title": "USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention Diffusion Network", "authors": ["Ying Yu", "Hang Xiao", "Siyao Li", "Jiarui Li", "Haotian Tang", "Hanyu Liu", "Chao Li"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02827v1", "summary": "The primary objective of human activity recognition (HAR) is to infer ongoing\nhuman actions from sensor data, a task that finds broad applications in health\nmonitoring, safety protection, and sports analysis. Despite proliferating\nresearch, HAR still faces key challenges, including the scarcity of labeled\nsamples for rare activities, insufficient extraction of high-level features,\nand suboptimal model performance on lightweight devices. To address these\nissues, this paper proposes a comprehensive optimization approach centered on\nmulti-attention interaction mechanisms. First, an unsupervised,\nstatistics-guided diffusion model is employed to perform data augmentation,\nthereby alleviating the problems of labeled data scarcity and severe class\nimbalance. Second, a multi-branch spatio-temporal interaction network is\ndesigned, which captures multi-scale features of sequential data through\nparallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels.\nSimultaneously, temporal attention mechanisms are incorporated to identify\ncritical time points, while spatial attention enhances inter-sensor\ninteractions. A cross-branch feature fusion unit is further introduced to\nimprove the overall feature representation capability. Finally, an adaptive\nmulti-loss function fusion strategy is integrated, allowing for dynamic\nadjustment of loss weights and overall model optimization. Experimental results\non three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the\nproposed unsupervised data augmentation spatio-temporal attention diffusion\nnetwork (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively,\nsignificantly outperforming existing approaches. Furthermore, practical\ndeployment on embedded devices verifies the efficiency and feasibility of the\nproposed method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02827v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "USAD：一种无监督数据增强时空注意力扩散网络", "tldr": "USAD提出了一种结合无监督数据增强和时空注意力机制的网络，显著提升了人体活动识别的性能，并解决了数据稀缺和特征提取不足的问题。", "motivation": "人体活动识别（HAR）面临的主要挑战包括稀缺的标记样本（特别是稀有活动）、高层特征提取不足以及在轻量级设备上模型性能不佳。", "method": "本文提出了一种名为USAD的综合优化方法。首先，采用无监督、统计引导的扩散模型进行数据增强，以解决标记数据稀缺和类别不平衡问题。其次，设计了一个多分支时空交互网络，通过具有3x3、5x5和7x7卷积核的并行残差分支捕获序列数据的多尺度特征。同时，引入时间注意力机制识别关键时间点，并空间注意力增强传感器间交互。此外，还引入了跨分支特征融合单元来提高整体特征表示能力。最后，集成了自适应多损失函数融合策略，动态调整损失权重以优化模型。", "result": "USAD在三个公共数据集WISDM、PAMAP2和OPPORTUNITY上分别取得了98.84%、93.81%和80.92%的准确率，显著优于现有方法。此外，在嵌入式设备上的实际部署验证了所提方法的效率和可行性。", "conclusion": "USAD通过结合无监督数据增强、多注意力时空网络和自适应多损失融合，有效解决了人体活动识别中的挑战，并在多个公共数据集上取得了SOTA性能，证明了其在实际部署中的有效性和可行性。", "translation": "人体活动识别（HAR）的主要目标是从传感器数据中推断正在进行的人体动作，这项任务在健康监测、安全保护和运动分析等领域有广泛应用。尽管研究不断增多，但HAR仍面临关键挑战，包括稀有活动标记样本的稀缺性、高层特征提取不足以及轻量级设备上模型性能不佳。为了解决这些问题，本文提出了一种以多注意力交互机制为中心的综合优化方法。首先，采用无监督、统计引导的扩散模型进行数据增强，从而缓解了标记数据稀缺和严重类别不平衡的问题。其次，设计了一个多分支时空交互网络，通过具有3*3、5*5和7*7卷积核的并行残差分支捕获序列数据的多尺度特征。同时，引入时间注意力机制以识别关键时间点，而空间注意力则增强了传感器间的交互。进一步引入了跨分支特征融合单元，以提高整体特征表示能力。最后，集成了自适应多损失函数融合策略，允许动态调整损失权重和整体模型优化。在WISDM、PAMAP2和OPPORTUNITY三个公共数据集上的实验结果表明，所提出的无监督数据增强时空注意力扩散网络（USAD）分别实现了98.84%、93.81%和80.92%的准确率，显著优于现有方法。此外，在嵌入式设备上的实际部署验证了所提方法的效率和可行性。", "summary": "本文提出了一种名为USAD的无监督数据增强时空注意力扩散网络，旨在解决人体活动识别（HAR）中标记数据稀缺、特征提取不足和设备性能限制等挑战。USAD结合了无监督扩散模型进行数据增强，以及一个多分支时空交互网络，该网络利用多尺度卷积核、时间注意力、空间注意力和跨分支特征融合来增强特征表示。此外，它还采用自适应多损失函数融合策略进行优化。实验结果表明，USAD在多个公共数据集上取得了显著优于现有方法的性能，并且在嵌入式设备上验证了其效率和可行性。", "keywords": "人体活动识别, 无监督数据增强, 时空注意力, 扩散网络, 多尺度特征", "comments": "USAD的创新点在于其结合了无监督数据增强（通过扩散模型）来解决数据稀缺和类别不平衡问题，以及一个高度优化的多注意力时空网络架构，能够有效地捕获序列数据的复杂特征。其在嵌入式设备上的验证也突出了其实用性和部署潜力，对于实际应用具有重要意义。"}}
{"id": "2507.02011", "title": "Machine Learning Based Stress Testing Framework for Indian Financial Market Portfolios", "authors": ["Vidya Sagar G", "Shifat Ali", "Siddhartha P. Chakrabarty"], "categories": ["q-fin.RM", "cs.LG", "q-fin.PM"], "primary_category": "Subjects:       Risk Management (q-fin.RM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02011v1", "summary": "This paper presents a machine learning driven framework for sectoral stress\ntesting in the Indian financial market, focusing on financial services,\ninformation technology, energy, consumer goods, and pharmaceuticals. Initially,\nwe address the limitations observed in conventional stress testing through\ndimensionality reduction and latent factor modeling via Principal Component\nAnalysis and Autoencoders. Building on this, we extend the methodology using\nVariational Autoencoders, which introduces a probabilistic structure to the\nlatent space. This enables Monte Carlo-based scenario generation, allowing for\nmore nuanced, distribution-aware simulation of stressed market conditions. The\nproposed framework captures complex non-linear dependencies and supports risk\nestimation through Value-at-Risk and Expected Shortfall. Together, these\npipelines demonstrate the potential of Machine Learning approaches to improve\nthe flexibility, robustness, and realism of financial stress testing.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02011v1", "cate": "q-fin.RM", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "基于机器学习的印度金融市场投资组合压力测试框架", "tldr": "本文提出了一个基于机器学习的框架，用于印度金融市场（包括金融服务、信息技术、能源、消费品和制药行业）的行业压力测试，通过降维、潜在因子建模以及变分自编码器和蒙特卡洛模拟，克服了传统方法的局限性，提高了压力测试的灵活性、鲁棒性和真实性。", "motivation": "传统压力测试存在局限性，需要更先进的方法来捕捉复杂的非线性依赖关系并进行更细致、分布感知的压力市场条件模拟。", "method": "首先，通过主成分分析（PCA）和自编码器（Autoencoders）进行降维和潜在因子建模，解决传统压力测试的局限性。接着，使用变分自编码器（Variational Autoencoders）引入概率结构到潜在空间，从而实现基于蒙特卡洛的场景生成，以进行更细致、分布感知的压力市场条件模拟。该框架还支持通过风险价值（Value-at-Risk）和预期损失（Expected Shortfall）进行风险估计。", "result": "所提出的框架能够捕捉复杂的非线性依赖关系，支持通过风险价值（Value-at-Risk）和预期损失（Expected Shortfall）进行风险估计。这些方法展示了机器学习方法在提高金融压力测试的灵活性、鲁棒性和真实性方面的潜力。", "conclusion": "机器学习方法能够显著提高金融压力测试的灵活性、鲁棒性和真实性，克服了传统方法的局限性。", "translation": "本文提出了一个机器学习驱动的框架，用于印度金融市场的行业压力测试，重点关注金融服务、信息技术、能源、消费品和制药行业。最初，我们通过主成分分析和自编码器进行降维和潜在因子建模，解决了传统压力测试中观察到的局限性。在此基础上，我们使用变分自编码器扩展了该方法，这为潜在空间引入了概率结构。这使得基于蒙特卡洛的场景生成成为可能，从而允许对压力市场条件进行更细致、分布感知的模拟。所提出的框架捕捉了复杂的非线性依赖关系，并通过风险价值和预期损失支持风险估计。总而言之，这些流程展示了机器学习方法在提高金融压力测试的灵活性、鲁棒性和真实性方面的潜力。", "summary": "本文提出了一种基于机器学习的印度金融市场行业压力测试框架，涵盖金融服务、信息技术、能源、消费品和制药等行业。该框架通过主成分分析和自编码器解决传统压力测试的局限性，并进一步利用变分自编码器引入概率结构，实现基于蒙特卡洛的精细化压力场景模拟。该方法能捕捉非线性依赖，并支持风险价值和预期损失的风险估计，显著提升了金融压力测试的灵活性、鲁棒性和真实性。", "keywords": "机器学习, 压力测试, 印度金融市场, 变分自编码器, 风险管理", "comments": "该论文通过引入机器学习（特别是自编码器和变分自编码器）来改进金融压力测试，具有创新性。它克服了传统方法在处理复杂非线性依赖和生成分布感知场景方面的局限性，对于提高金融风险管理的准确性和鲁棒性具有重要意义。"}}
{"id": "2507.02857", "title": "AnyI2V: Animating Any Conditional Image with Motion Control", "authors": ["Ziye Li", "Hao Luo", "Xincheng Shuai", "Henghui Ding"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025, Project Page: this https URL", "url": "http://arxiv.org/abs/2507.02857v1", "summary": "Recent advancements in video generation, particularly in diffusion models,\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\nsynthesis. However, challenges remain in effectively integrating dynamic motion\nsignals and flexible spatial constraints. Existing T2V methods typically rely\non text prompts, which inherently lack precise control over the spatial layout\nof generated content. In contrast, I2V methods are limited by their dependence\non real images, which restricts the editability of the synthesized content.\nAlthough some methods incorporate ControlNet to introduce image-based\nconditioning, they often lack explicit motion control and require\ncomputationally expensive training. To address these limitations, we propose\nAnyI2V, a training-free framework that animates any conditional images with\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\nas the conditional image, including data types such as meshes and point clouds\nthat are not supported by ControlNet, enabling more flexible and versatile\nvideo generation. Additionally, it supports mixed conditional inputs and\nenables style transfer and editing via LoRA and text prompts. Extensive\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\nand provides a new perspective in spatial- and motion-controlled video\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.", "comment": "ICCV 2025, Project Page: https://henghuiding.com/AnyI2V/", "pdf_url": "http://arxiv.org/pdf/2507.02857v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "AnyI2V：通过运动控制动画化任意条件图像", "tldr": "AnyI2V是一个免训练的框架，它能够通过用户定义的运动轨迹，将任意条件图像（包括网格和点云等多种模态）动画化，解决了现有视频生成方法在运动控制和空间约束上的局限性。", "motivation": "现有视频生成方法（特别是T2V和I2V）在整合动态运动信号和灵活空间约束方面存在挑战。T2V方法依赖文本提示，缺乏对空间布局的精确控制；I2V方法受限于真实图像，编辑性受限；而使用ControlNet的方法通常缺乏明确的运动控制且训练成本高昂。", "method": "本文提出了AnyI2V，一个免训练的框架，能够通过用户定义的运动轨迹来动画化任意条件图像。AnyI2V支持更广泛的条件图像模态，包括ControlNet不支持的网格和点云等数据类型。此外，它还支持混合条件输入，并通过LoRA和文本提示实现风格迁移和编辑。", "result": "广泛的实验表明，所提出的AnyI2V实现了卓越的性能，并在空间和运动控制的视频生成方面提供了新的视角。", "conclusion": "AnyI2V提供了一个免训练的框架，通过支持多种条件图像模态和用户定义的运动轨迹，有效解决了现有视频生成方法在精确运动和空间控制方面的局限性，实现了更灵活、更通用的视频生成。", "translation": "视频生成领域的最新进展，特别是在扩散模型方面，推动了文本到视频（T2V）和图像到视频（I2V）合成的显著进步。然而，有效整合动态运动信号和灵活的空间约束仍然面临挑战。现有的T2V方法通常依赖于文本提示，这本身就缺乏对生成内容空间布局的精确控制。相比之下，I2V方法受限于对真实图像的依赖，这限制了合成内容的可编辑性。尽管一些方法结合了ControlNet来引入基于图像的条件，但它们通常缺乏明确的运动控制，并且需要计算成本高昂的训练。为了解决这些限制，我们提出了AnyI2V，一个免训练的框架，它可以通过用户定义的运动轨迹来动画化任何条件图像。AnyI2V支持更广泛的模态作为条件图像，包括ControlNet不支持的网格和点云等数据类型，从而实现更灵活和多功能的视频生成。此外，它还支持混合条件输入，并通过LoRA和文本提示实现风格迁移和编辑。大量的实验表明，所提出的AnyI2V取得了卓越的性能，并在空间和运动控制的视频生成方面提供了新的视角。代码可在https://henghuiding.com/AnyI2V/获取。", "summary": "AnyI2V是一个创新的免训练框架，旨在解决当前视频生成（T2V和I2V）在精确运动控制和灵活空间约束方面的不足。它允许用户通过自定义运动轨迹来动画化任何条件图像，并支持包括网格和点云在内的多种模态，扩展了传统ControlNet的限制。该框架还支持混合输入，并能通过LoRA和文本提示进行风格迁移和编辑。实验证明AnyI2V在空间和运动控制视频生成方面表现出色，为该领域提供了新的方法。", "keywords": "视频生成, 运动控制, 条件图像, AnyI2V, 扩散模型", "comments": "AnyI2V的创新之处在于其“免训练”的特性，这显著降低了计算成本。同时，它支持更广泛的条件图像模态（如网格和点云），并能结合用户自定义的运动轨迹，极大地提升了视频生成的灵活性和控制精度，为多模态视频内容创作开辟了新途径。"}}
{"id": "2507.02859", "title": "Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for Data-Efficient Model Adaptation", "authors": ["Jiaer Xia", "Bingkui Tong", "Yuhang Zang", "Rui Shao", "Kaiyang Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV2025", "url": "http://arxiv.org/abs/2507.02859v1", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in interpreting images using natural language. However, without\nusing large-scale datasets for retraining, these models are difficult to adapt\nto specialized vision tasks, e.g., chart understanding. This problem is caused\nby a mismatch between pre-training and downstream datasets: pre-training\ndatasets primarily concentrate on scenes and objects but contain limited\ninformation about specialized, non-object images, such as charts and tables. In\nthis paper, we share an interesting finding that training an MLLM with\nChain-of-Thought (CoT) reasoning data can facilitate model adaptation in\nspecialized vision tasks, especially under data-limited regimes. However, we\nidentify a critical issue within CoT data distilled from pre-trained MLLMs,\ni.e., the data often contains multiple factual errors in the reasoning steps.\nTo address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple\nbootstrapping-based approach that aims to inject grounding information (i.e.,\nbounding boxes) into CoT data, essentially making the reasoning steps more\nfaithful to input images. We evaluate our approach on five specialized vision\ntasks, which cover a variety of visual formats including charts, tables,\nreceipts, and reports. The results demonstrate that under data-limited regimes\nour approach significantly improves upon fine-tuning and distillation.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.02859v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "多模态大语言模型中引导接地思维链以实现数据高效的模型适应", "tldr": "本文提出GCoT方法，通过向CoT数据注入边界框以纠正预训练MLLM中CoT数据的事实错误，从而在数据受限的情况下显著提高MLLM在专业视觉任务（如图表理解）上的适应能力。", "motivation": "尽管多模态大语言模型（MLLMs）在图像理解方面表现出色，但由于预训练数据与下游专业视觉任务（如图表理解）之间存在不匹配，导致模型在数据量有限的情况下难以适应这些专业任务。此外，从预训练MLLM中提取的思维链（CoT）数据常包含事实错误。", "method": "本文提出了一种名为接地思维链（GCoT）的简单引导式方法。该方法旨在将接地信息（即边界框）注入到思维链（CoT）数据中，从而使推理步骤更忠实于输入图像，以解决从预训练MLLM中提取的CoT数据中存在的推理步骤事实错误问题。", "result": "在五种涵盖图表、表格、收据和报告等多种视觉格式的专业视觉任务上进行评估，结果表明，在数据受限的情况下，GCoT方法显著优于传统的微调和蒸馏方法。", "conclusion": "通过引入接地信息（边界框）来纠正思维链数据中的事实错误，GCoT方法能够有效地提高多模态大语言模型在数据受限的专业视觉任务上的适应能力。", "translation": "多模态大语言模型（MLLMs）在利用自然语言解释图像方面展现出卓越的能力。然而，在不使用大规模数据集进行再训练的情况下，这些模型难以适应专业的视觉任务，例如图表理解。这个问题是由于预训练数据集和下游数据集之间的不匹配造成的：预训练数据集主要集中于场景和物体，但包含关于专业非物体图像（如图表和表格）的信息有限。在本文中，我们分享了一个有趣的发现，即使用思维链（CoT）推理数据训练MLLM可以促进模型在专业视觉任务中的适应，尤其是在数据受限的情况下。然而，我们发现从预训练MLLM中提取的CoT数据存在一个关键问题，即数据在推理步骤中经常包含多个事实错误。为了解决这个问题，我们提出了接地思维链（GCoT），这是一种简单的基于引导的方法，旨在将接地信息（即边界框）注入到CoT数据中，从而使推理步骤更忠实于输入图像。我们在五种专业视觉任务上评估了我们的方法，这些任务涵盖了包括图表、表格、收据和报告在内的各种视觉格式。结果表明，在数据受限的情况下，我们的方法显著优于微调和蒸馏。", "summary": "本文针对多模态大语言模型（MLLMs）在数据受限的专业视觉任务（如图表理解）中适应性差的问题，提出了接地思维链（GCoT）方法。该方法通过引导式地向CoT数据中注入边界框等接地信息，纠正了从预训练MLLM中提取的CoT数据中存在的推理错误。实验证明，在图表、表格、收据和报告等五种专业任务上，GCoT在数据量有限的情况下，其性能显著优于传统的微调和蒸馏方法。", "keywords": "多模态大语言模型, 思维链, 数据高效, 模型适应, 接地信息", "comments": "这篇论文的创新点在于提出了GCoT，通过引入接地信息（边界框）来解决从大型预训练模型中蒸馏出的思维链数据中存在的“幻觉”或事实错误问题，这对于在数据受限场景下提升MLLMs在专业领域（如图表理解）的适应性具有重要意义。其引导式方法也为高效模型适应提供了一个新思路。"}}
{"id": "2507.02860", "title": "Less is Enough: Training-Free Video Diffusion Acceleration via Runtime-Adaptive Caching", "authors": ["Xin Zhou", "Dingkang Liang", "Kaijin Chen", "Tianrui Feng", "Xiwu Chen", "Hongkai Lin", "Yikang Ding", "Feiyang Tan", "Hengshuang Zhao", "Xiang Bai"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      The code is made available at this https URL . Project page: this https URL", "url": "http://arxiv.org/abs/2507.02860v1", "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.", "comment": "The code is made available at\n  https://github.com/H-EmbodVis/EasyCache. Project page:\n  https://h-embodvis.github.io/EasyCache/", "pdf_url": "http://arxiv.org/pdf/2507.02860v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "少即是多：通过运行时自适应缓存实现免训练视频扩散加速", "tldr": "EasyCache是一个免训练的视频扩散模型加速框架，通过运行时自适应缓存动态重用计算结果，显著减少推理时间（2.1-3.3倍加速）并保持高视觉保真度，甚至比现有SOTA方法提升36%的PSNR。", "motivation": "视频生成模型虽然性能卓越，但其缓慢的推理速度和高昂的计算成本（主要由于去噪过程的迭代性质）阻碍了其广泛应用。解决这一瓶颈对于普及先进的视频合成技术并将其集成到实际应用中至关重要。", "method": "本文提出了EasyCache，一个用于视频扩散模型的免训练加速框架。EasyCache引入了一种轻量级、运行时自适应的缓存机制，该机制动态重用先前计算的转换向量，从而避免了推理过程中的冗余计算。与现有方法不同，EasyCache无需离线分析、预计算或大量的参数调优。", "result": "该方法在OpenSora、Wan2.1和HunyuanVideo等各种大型视频生成模型上进行了全面研究，实现了领先的加速性能。与原始基线相比，推理时间减少了2.1-3.3倍，同时保持了高视觉保真度，与之前的SOTA方法相比，PSNR显著提高了36%。", "conclusion": "EasyCache提供了一种高效且高度可用的解决方案，用于研究和实际应用中的高质量视频生成，显著提升了视频生成模型的推理效率和可访问性。", "translation": "视频生成模型已经展示出卓越的性能，但由于去噪过程的迭代性质，其缓慢的推理速度和高昂的计算成本阻碍了其更广泛的应用。解决这一瓶颈对于普及先进的视频合成技术并使其集成到实际应用中至关重要。这项工作提出了EasyCache，一个用于视频扩散模型的免训练加速框架。EasyCache引入了一种轻量级、运行时自适应的缓存机制，该机制动态重用先前计算的转换向量，从而避免了推理过程中的冗余计算。与现有方法不同，EasyCache无需离线分析、预计算或大量的参数调优。我们在各种大型视频生成模型（包括OpenSora、Wan2.1和HunyuanVideo）上进行了全面研究。我们的方法实现了领先的加速性能，与原始基线相比，推理时间减少了2.1-3.3倍，同时保持了高视觉保真度，与之前的SOTA方法相比，PSNR显著提高了36%。这一改进使得我们的EasyCache成为研究和实际应用中高质量视频生成的高效且高度可用的解决方案。代码可在https://github.com/H-EmbodVis/EasyCache获取。", "summary": "EasyCache是一个免训练的视频扩散模型加速框架，通过引入运行时自适应缓存机制，动态重用转换向量，有效避免了推理过程中的冗余计算。该方法无需预先分析或参数调优，在多种大型视频模型上实现了2.1-3.3倍的推理加速，并显著提升了视觉质量（PSNR提升36%），为高质量视频生成提供了一个高效且易于访问的解决方案。", "keywords": "视频扩散模型, 加速, 免训练, 缓存, 运行时自适应", "comments": "EasyCache的创新之处在于其“免训练”和“运行时自适应”的特点，避免了传统加速方法所需的复杂预处理和调优。它通过动态缓存机制有效解决了视频扩散模型的推理速度瓶颈，显著提升了实际应用的可行性，同时保持甚至提高了视觉质量，具有重要的实用价值。"}}
{"id": "2507.02862", "title": "RefTok: Reference-Based Tokenization for Video Generation", "authors": ["Xiang Fan", "Xiaohang Sun", "Kushan Thakkar", "Zhu Liu", "Vimal Bhat", "Ranjay Krishna", "Xiang Hao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02862v1", "summary": "Effectively handling temporal redundancy remains a key challenge in learning\nvideo models. Prevailing approaches often treat each set of frames\nindependently, failing to effectively capture the temporal dependencies and\nredundancies inherent in videos. To address this limitation, we introduce\nRefTok, a novel reference-based tokenization method capable of capturing\ncomplex temporal dynamics and contextual information. Our method encodes and\ndecodes sets of frames conditioned on an unquantized reference frame. When\ndecoded, RefTok preserves the continuity of motion and the appearance of\nobjects across frames. For example, RefTok retains facial details despite head\nmotion, reconstructs text correctly, preserves small patterns, and maintains\nthe legibility of handwriting from the context. Across 4 video datasets (K600,\nUCF-101, BAIR Robot Pushing, and DAVIS), RefTok significantly outperforms\ncurrent state-of-the-art tokenizers (Cosmos and MAGVIT) and improves all\nevaluated metrics (PSNR, SSIM, LPIPS) by an average of 36.7% at the same or\nhigher compression ratios. When a video generation model is trained using\nRefTok's latents on the BAIR Robot Pushing task, the generations not only\noutperform MAGVIT-B but the larger MAGVIT-L, which has 4x more parameters,\nacross all generation metrics by an average of 27.9%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02862v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "RefTok：基于参考的视频生成分词方法", "tldr": "RefTok是一种新的基于参考的分词方法，能有效处理视频中的时间冗余，显著优于现有SOTA分词器，并提升视频生成模型的性能。", "motivation": "现有视频模型处理时间冗余效果不佳，未能有效捕获视频中固有的时间依赖性和冗余，因为它们通常独立处理每组帧。", "method": "本文引入RefTok，一种新颖的基于参考的分词方法，旨在捕获复杂的时间动态和上下文信息。该方法以未量化的参考帧为条件对帧集进行编码和解码。", "result": "在4个视频数据集（K600、UCF-101、BAIR Robot Pushing、DAVIS）上，RefTok显著优于当前最先进的分词器（Cosmos和MAGVIT），在相同或更高压缩比下，所有评估指标（PSNR、SSIM、LPIPS）平均提高了36.7%。当使用RefTok的潜在空间训练视频生成模型时，其性能不仅优于MAGVIT-B，甚至超越了参数多4倍的MAGVIT-L，在所有生成指标上平均提高了27.9%。解码时，RefTok能保持运动连续性、对象外观、面部细节、文本正确重建、小模式保留以及手写体可读性。", "conclusion": "RefTok通过其新颖的基于参考的分词方法，有效解决了视频时间冗余问题，显著提升了视频分词和视频生成任务的性能，超越了现有最先进的方法。", "translation": "有效处理时间冗余仍然是学习视频模型的关键挑战。现有方法通常独立处理每组帧，未能有效捕获视频中固有的时间依赖性和冗余。为了解决这一限制，我们引入了RefTok，一种新颖的基于参考的分词方法，能够捕获复杂的时间动态和上下文信息。我们的方法以未量化的参考帧为条件对帧集进行编码和解码。解码时，RefTok在帧之间保留了运动的连续性和对象的出现。例如，RefTok在头部运动时仍能保留面部细节，正确重建文本，保留小的模式，并从上下文中保持手写体的可读性。在4个视频数据集（K600、UCF-101、BAIR Robot Pushing和DAVIS）上，RefTok显著优于当前最先进的分词器（Cosmos和MAGVIT），并在相同或更高压缩比下将所有评估指标（PSNR、SSIM、LPIPS）平均提高了36.7%。当使用RefTok的潜在空间在BAIR Robot Pushing任务上训练视频生成模型时，生成的视频不仅优于MAGVIT-B，甚至优于参数多4倍的MAGVIT-L，在所有生成指标上平均提高了27.9%。", "summary": "本文提出RefTok，一种新颖的基于参考的视频分词方法，旨在解决视频中时间冗余处理的挑战。RefTok通过以未量化的参考帧为条件进行编码和解码，有效捕获视频的时间动态和上下文信息，从而在保持运动连续性和对象外观方面表现出色。实验结果表明，RefTok在多个视频数据集上显著优于现有SOTA分词器，并在相同或更高压缩比下平均提升了36.7%的评估指标。此外，使用RefTok潜在空间训练的视频生成模型在性能上超越了更大型的现有模型，证明了其在视频生成领域的有效性。", "keywords": "视频生成, 分词, 时间冗余, 参考学习, 视频压缩", "comments": "RefTok的创新之处在于其“基于参考”的分词机制，这使其能够有效捕捉视频的时间依赖性和冗余，解决了传统方法独立处理帧的局限性。其在压缩效率和生成质量上的显著提升，以及对细节如面部、文本和手写体的良好保留能力，显示了其在视频处理和生成领域的巨大潜力。该方法为未来更高效、更高质量的视频模型提供了新的思路。"}}
{"id": "2507.02086", "title": "Selective Feature Re-Encoded Quantum Convolutional Neural Network with Joint Optimization for Image Classification", "authors": ["Shaswata Mahernob Sarkar", "Sheikh Iftekhar Ahmed", "Jishnu Mahmud", "Shaikh Anowarul Fattah", "Gaurav Sharma"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      26 pages, 12 figures, 6 Tables", "url": "http://arxiv.org/abs/2507.02086v1", "summary": "Quantum Machine Learning (QML) has seen significant advancements, driven by\nrecent improvements in Noisy Intermediate-Scale Quantum (NISQ) devices.\nLeveraging quantum principles such as entanglement and superposition, quantum\nconvolutional neural networks (QCNNs) have demonstrated promising results in\nclassifying both quantum and classical data. This study examines QCNNs in the\ncontext of image classification and proposes a novel strategy to enhance\nfeature processing and a QCNN architecture for improved classification\naccuracy. First, a selective feature re-encoding strategy is proposed, which\ndirects the quantum circuits to prioritize the most informative features,\nthereby effectively navigating the crucial regions of the Hilbert space to find\nthe optimal solution space. Secondly, a novel parallel-mode QCNN architecture\nis designed to simultaneously incorporate features extracted by two classical\nmethods, Principal Component Analysis (PCA) and Autoencoders, within a unified\ntraining scheme. The joint optimization involved in the training process allows\nthe QCNN to benefit from complementary feature representations, enabling better\nmutual readjustment of model parameters. To assess these methodologies,\ncomprehensive experiments have been performed using the widely used MNIST and\nFashion MNIST datasets for binary classification tasks. Experimental findings\nreveal that the selective feature re-encoding method significantly improves the\nquantum circuit's feature processing capability and performance. Furthermore,\nthe jointly optimized parallel QCNN architecture consistently outperforms the\nindividual QCNN models and the traditional ensemble approach involving\nindependent learning followed by decision fusion, confirming its superior\naccuracy and generalization capabilities.", "comment": "26 pages, 12 figures, 6 Tables", "pdf_url": "http://arxiv.org/pdf/2507.02086v1", "cate": "quant-ph", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "用于图像分类的选择性特征重编码量子卷积神经网络与联合优化", "tldr": "本文提出了一种结合选择性特征重编码和并行模式QCNN架构的量子卷积神经网络，通过联合优化提高了图像分类的准确性和泛化能力。", "motivation": "量子机器学习（QML）在噪声中等规模量子（NISQ）设备进步的推动下取得了显著进展。量子卷积神经网络（QCNN）在分类量子和经典数据方面显示出良好前景。本研究旨在通过改进特征处理和QCNN架构来提高图像分类的准确性。", "method": "1. 提出了一种选择性特征重编码策略，引导量子电路优先处理信息量最大的特征。2. 设计了一种新颖的并行模式QCNN架构，该架构能同时整合两种经典方法（主成分分析PCA和自动编码器）提取的特征，并在统一的训练方案中进行联合优化。", "result": "实验结果表明，选择性特征重编码方法显著提高了量子电路的特征处理能力和性能。联合优化的并行QCNN架构始终优于单个QCNN模型和传统的独立学习后决策融合的集成方法。", "conclusion": "选择性特征重编码和联合优化的并行QCNN架构能够有效提升图像分类的准确性和泛化能力，为QCNN在图像分类任务中的应用提供了新的有效策略。", "translation": "量子机器学习（QML）在噪声中等规模量子（NISQ）设备的最新改进推动下取得了显著进展。利用纠缠和叠加等量子原理，量子卷积神经网络（QCNN）在分类量子和经典数据方面都展现出有前景的结果。本研究在图像分类背景下检验了QCNN，并提出了一种新颖的策略来增强特征处理和QCNN架构，以提高分类准确性。首先，提出了一种选择性特征重编码策略，该策略引导量子电路优先处理信息量最大的特征，从而有效地在希尔伯特空间的关键区域中导航以找到最优解空间。其次，设计了一种新颖的并行模式QCNN架构，用于在一个统一的训练方案中同时整合由两种经典方法（主成分分析PCA和自动编码器）提取的特征。训练过程中涉及的联合优化使得QCNN能够受益于互补的特征表示，从而实现模型参数更好的相互调整。为了评估这些方法，我们使用广泛使用的MNIST和Fashion MNIST数据集进行了全面的二元分类任务实验。实验结果表明，选择性特征重编码方法显著提高了量子电路的特征处理能力和性能。此外，联合优化的并行QCNN架构始终优于单个QCNN模型和传统的涉及独立学习后决策融合的集成方法，证实了其卓越的准确性和泛化能力。", "summary": "本研究提出了一种用于图像分类的增强型量子卷积神经网络（QCNN）。该方法引入了选择性特征重编码策略，以优化量子电路对信息量最大特征的处理。同时，设计了一种并行模式QCNN架构，通过联合优化整合了PCA和自动编码器提取的经典特征。在MNIST和Fashion MNIST数据集上的实验证明，所提出的选择性特征重编码显著提升了特征处理能力，而联合优化的并行QCNN架构在准确性和泛化能力上均优于现有方法。", "keywords": "量子卷积神经网络, 图像分类, 特征重编码, 联合优化, NISQ", "comments": "该论文的创新点在于提出了选择性特征重编码策略和并行模式QCNN架构，并通过联合优化提升了QCNN在图像分类任务中的性能。这对于当前NISQ设备下的QML发展具有重要意义，尤其是在如何有效利用量子计算处理经典数据方面提供了新的思路。"}}
{"id": "2507.02436", "title": "Toward a Robust and Generalizable Metamaterial Foundation Model", "authors": ["Namjung Kim", "Dongseok Lee", "Jongbin Yu", "Sung Woong Cho", "Dosung Lee", "Yesol Park", "Youngjoon Hong"], "categories": ["physics.optics", "cs.AI"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02436v1", "summary": "Advances in material functionalities drive innovations across various fields,\nwhere metamaterials-defined by structure rather than composition-are leading\nthe way. Despite the rise of artificial intelligence (AI)-driven design\nstrategies, their impact is limited by task-specific retraining, poor\nout-of-distribution(OOD) generalization, and the need for separate models for\nforward and inverse design. To address these limitations, we introduce the\nMetamaterial Foundation Model (MetaFO), a Bayesian transformer-based foundation\nmodel inspired by large language models. MetaFO learns the underlying mechanics\nof metamaterials, enabling probabilistic, zero-shot predictions across diverse,\nunseen combinations of material properties and structural responses. It also\nexcels in nonlinear inverse design, even under OOD conditions. By treating\nmetamaterials as an operator that maps material properties to structural\nresponses, MetaFO uncovers intricate structure-property relationships and\nsignificantly expands the design space. This scalable and generalizable\nframework marks a paradigm shift in AI-driven metamaterial discovery, paving\nthe way for next-generation innovations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02436v1", "cate": "physics.optics", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "迈向鲁棒且可泛化的超材料基础模型", "tldr": "引入MetaFO，一个受大型语言模型启发的贝叶斯Transformer模型，用于超材料设计，实现零样本预测和非线性逆向设计，解决AI驱动设计中的泛化和任务特定问题。", "motivation": "现有的AI驱动材料设计策略面临局限性，包括需要针对特定任务重新训练、对分布外数据泛化能力差，以及正向和逆向设计需要不同模型。", "method": "本文引入了Metamaterial Foundation Model (MetaFO)，这是一个受大型语言模型启发的基于贝叶斯Transformer的基础模型。MetaFO将超材料视为将材料属性映射到结构响应的算子。", "result": "MetaFO能够实现对不同、未见过的材料属性和结构响应组合的概率性、零样本预测。它在非线性逆向设计方面表现出色，即使在分布外条件下也是如此。它揭示了复杂的结构-属性关系并显著扩展了设计空间。", "conclusion": "MetaFO代表了AI驱动超材料发现的范式转变，为下一代创新铺平了道路，因为它提供了一个可扩展且可泛化的框架。", "translation": "材料功能性的进步推动了各个领域的创新，其中由结构而非成分定义的超材料正引领潮流。尽管人工智能（AI）驱动的设计策略日益兴起，但其影响受到任务特定再训练、差劲的分布外（OOD）泛化能力以及正向和逆向设计需要单独模型等限制。为了解决这些局限性，我们引入了超材料基础模型（MetaFO），这是一个受大型语言模型启发的基于贝叶斯Transformer的基础模型。MetaFO学习超材料的底层力学原理，从而能够在材料属性和结构响应的各种未见组合中进行概率性、零样本预测。它在非线性逆向设计方面也表现出色，即使在OOD条件下也是如此。通过将超材料视为将材料属性映射到结构响应的算子，MetaFO揭示了复杂的结构-属性关系并显著扩展了设计空间。这个可扩展且可泛化的框架标志着AI驱动超材料发现的范式转变，为下一代创新铺平了道路。", "summary": "本文提出MetaFO，一个受大型语言模型启发的贝叶斯Transformer基础模型，旨在克服当前AI驱动超材料设计中任务特定训练、泛化性差及正逆向设计分离的局限。MetaFO通过将超材料视为操作符，学习其底层力学，实现对未知组合的零样本概率预测，并在分布外条件下进行非线性逆向设计，显著拓宽了设计空间，代表了超材料AI设计的新范式。", "keywords": "超材料, 基础模型, 贝叶斯Transformer, 零样本预测, 逆向设计", "comments": "该论文创新性地将大型语言模型（LLM）的“基础模型”概念引入超材料设计领域，提出了MetaFO。其亮点在于解决了当前AI设计中泛化性差和任务特定训练的问题，实现了零样本预测和分布外条件下的逆向设计。这种将超材料视为“操作符”的视角，有望开启超材料设计的新篇章，极大地加速材料发现过程。"}}
{"id": "2507.02863", "title": "Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory", "authors": ["Yuqi Wu", "Wenzhao Zheng", "Jie Zhou", "Jiwen Lu"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Code is available at: this https URL", "url": "http://arxiv.org/abs/2507.02863v1", "summary": "Dense 3D scene reconstruction from an ordered sequence or unordered image\ncollections is a critical step when bringing research in computer vision into\npractical scenarios. Following the paradigm introduced by DUSt3R, which unifies\nan image pair densely into a shared coordinate system, subsequent methods\nmaintain an implicit memory to achieve dense 3D reconstruction from more\nimages. However, such implicit memory is limited in capacity and may suffer\nfrom information loss of earlier frames. We propose Point3R, an online\nframework targeting dense streaming 3D reconstruction. To be specific, we\nmaintain an explicit spatial pointer memory directly associated with the 3D\nstructure of the current scene. Each pointer in this memory is assigned a\nspecific 3D position and aggregates scene information nearby in the global\ncoordinate system into a changing spatial feature. Information extracted from\nthe latest frame interacts explicitly with this pointer memory, enabling dense\nintegration of the current observation into the global coordinate system. We\ndesign a 3D hierarchical position embedding to promote this interaction and\ndesign a simple yet effective fusion mechanism to ensure that our pointer\nmemory is uniform and efficient. Our method achieves competitive or\nstate-of-the-art performance on various tasks with low training costs. Code is\navailable at: https://github.com/YkiWu/Point3R.", "comment": "Code is available at: https://github.com/YkiWu/Point3R", "pdf_url": "http://arxiv.org/pdf/2507.02863v1", "cate": "cs.CV", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "Point3R: 具有显式空间指针记忆的流式三维重建", "tldr": "Point3R提出了一种用于流式三维重建的在线框架，通过引入显式空间指针记忆来克服现有隐式记忆方法容量有限和信息丢失的问题，实现了具有竞争力的性能。", "motivation": "现有的三维重建方法使用隐式记忆，但其容量有限且可能导致早期帧的信息丢失，这限制了它们在实际场景中的应用。", "method": "我们提出了Point3R，一个针对密集流式三维重建的在线框架。它维护一个与当前场景三维结构直接关联的显式空间指针记忆。记忆中的每个指针被分配一个特定的三维位置，并在全局坐标系中聚合附近的场景信息，形成一个变化的空间特征。最新帧提取的信息与此指针记忆显式交互，将当前观测值密集整合到全局坐标系中。我们设计了一个三维分层位置嵌入来促进这种交互，并设计了一个简单而有效的融合机制来确保指针记忆的统一性和效率。", "result": "我们的方法在各种任务上取得了具有竞争力或最先进的性能，且训练成本较低。", "conclusion": "Point3R通过引入显式空间指针记忆，有效解决了传统隐式记忆在流式三维重建中容量和信息丢失的限制，实现了高效且高性能的密集三维场景重建。", "translation": "从有序序列或无序图像集合中进行密集三维场景重建是将计算机视觉研究应用于实际场景的关键步骤。遵循DUSt3R引入的范式（该范式将图像对密集地统一到共享坐标系中），后续方法保持隐式记忆以实现从更多图像进行的密集三维重建。然而，这种隐式记忆的容量有限，并且可能遭受早期帧的信息丢失。我们提出了Point3R，一个针对密集流式三维重建的在线框架。具体来说，我们维护一个与当前场景三维结构直接关联的显式空间指针记忆。该记忆中的每个指针被分配一个特定的三维位置，并在全局坐标系中聚合附近场景信息，形成一个变化的空间特征。从最新帧提取的信息与此指针记忆显式交互，从而将当前观测值密集整合到全局坐标系中。我们设计了一个三维分层位置嵌入来促进这种交互，并设计了一个简单而有效的融合机制，以确保我们的指针记忆是统一且高效的。我们的方法在各种任务上取得了具有竞争力或最先进的性能，且训练成本较低。代码可在：https://github.com/YkiWu/Point3R 获得。", "summary": "Point3R是一个用于密集流式三维重建的在线框架，旨在克服现有隐式记忆方法的容量限制和信息丢失问题。它引入了一种显式空间指针记忆，该记忆与三维场景结构直接关联，通过聚合空间信息并与最新帧显式交互来整合观测数据。该框架利用三维分层位置嵌入和高效融合机制，在各种任务上实现了具有竞争力或最先进的性能，且训练成本低。", "keywords": "流式三维重建, 显式记忆, 空间指针, 在线框架, 密集重建", "comments": "Point3R的关键创新在于其显式空间指针记忆的设计，这直接解决了传统隐式记忆在处理连续数据流时面临的容量和信息丢失问题。这种方法对于需要实时、连续三维重建的应用（如机器人导航、AR/VR）具有重要意义，因为它提供了更稳定和全面的场景表示。其低训练成本也增强了其实用性。"}}
{"id": "2507.02024", "title": "TubuleTracker: a high-fidelity shareware software to quantify angiogenesis architecture and maturity", "authors": ["Danish Mahmood", "Stephanie Buczkowski", "Sahaj Shah", "Autumn Anthony", "Rohini Desetty", "Carlo R Bartoli"], "categories": ["q-bio.QM", "cs.CV", "q-bio.CB"], "primary_category": "Subjects:       Quantitative Methods (q-bio.QM)", "pdf_link": null, "comments": "Comments:      Abstract word count = [285] Total word count = [3910] Main body text = [2179] References = [30] Table = [0] Figures = [4]", "url": "http://arxiv.org/abs/2507.02024v1", "summary": "Background: In vitro endothelial cell culture is widely used to study\nangiogenesis. Histomicrographic images of cell networks are often analyzed\nmanually, a process that is time-consuming and subjective. Automated tools like\nImageJ (NIH) can assist, but are often slow and inaccurate. Additionally, as\nendothelial networks grow more complex, traditional architectural metrics may\nnot fully reflect network maturity. To address these limitations, we developed\ntubuleTracker, a software tool that quantifies endothelial network architecture\nand maturity rapidly and objectively. Methods: Human umbilical vein endothelial\ncells were cultured in an extracellular matrix, and 54 images were acquired\nusing phase contrast microscopy. Each image was analyzed manually by three\nindependent reviewers, and by both ImageJ and tubuleTracker. Key metrics\nincluded tubule count, total length, node count, tubule area, and vessel\ncircularity. In parallel, trained scientists rated each image for angiogenesis\nmaturity on a 1-5 scale (1 = most mature). Results: Analysis time per image\ndiffered significantly: manual (8 min), ImageJ (58+/-4 s), and tubuleTracker\n(6+/-2 s) (p<0.0001). Significant differences were also found in tubule count\n(manual 168+/-SD, tubuleTracker 92+/-SD, ImageJ 433+/-SD), length, and node\ncount (all p<0.0001). tubuleTracker's metrics varied significantly across\nangiogenesis maturity scores, including tubule count, length, node count, area,\nand circularity (all p<0.0001). Conclusions: tubuleTracker was faster and more\nconsistent than both manual and ImageJ-based analysis. Vessel circularity\nproved especially effective in capturing angiogenesis maturity. tubuleTracker\nis available as free shareware for the biomedical research community.", "comment": "Abstract word count = [285] Total word count = [3910] Main body text\n  = [2179] References = [30] Table = [0] Figures = [4]", "pdf_url": "http://arxiv.org/pdf/2507.02024v1", "cate": "q-bio.QM", "date": "2025-07-02", "updated": "2025-07-02", "AI": {"title_translation": "TubuleTracker：一种用于量化血管生成结构和成熟度的高保真共享软件", "tldr": "TubuleTracker是一款快速、客观且高保真的共享软件，用于量化血管生成结构和成熟度，优于手动和ImageJ分析。", "motivation": "现有的血管生成体外图像分析方法（手动或ImageJ）耗时、主观、缓慢且不准确，且传统指标难以完全反映网络成熟度。", "method": "开发了TubuleTracker软件。使用人脐静脉内皮细胞培养获得54张图像，并分别通过手动、ImageJ和TubuleTracker进行分析，比较了小管计数、总长度、节点数、小管面积和血管圆度等指标。同时，训练有素的科学家对图像的血管生成成熟度进行了1-5评分。", "result": "TubuleTracker的图像分析速度显著快于手动和ImageJ（6+/-2秒 vs. 8分钟 vs. 58+/-4秒）。在小管计数、长度和节点数等指标上，TubuleTracker与其他方法存在显著差异。TubuleTracker的各项指标（包括小管计数、长度、节点数、面积和圆度）与血管生成成熟度评分显著相关。", "conclusion": "TubuleTracker比手动和ImageJ分析更快、更一致。血管圆度在捕捉血管生成成熟度方面特别有效。TubuleTracker作为免费共享软件提供。", "translation": "背景：体外内皮细胞培养被广泛用于研究血管生成。细胞网络的组织显微图像通常通过手动分析，这一过程耗时且主观。像ImageJ (NIH) 这样的自动化工具可以提供帮助，但通常速度慢且不准确。此外，随着内皮网络变得更加复杂，传统的结构指标可能无法完全反映网络的成熟度。为了解决这些限制，我们开发了TubuleTracker，一个能够快速客观地量化内皮网络结构和成熟度的软件工具。\n方法：将人脐静脉内皮细胞在细胞外基质中培养，并使用相差显微镜获取了54张图像。每张图像由三名独立的评审员手动分析，并分别由ImageJ和TubuleTracker进行分析。关键指标包括小管计数、总长度、节点计数、小管面积和血管圆度。同时，受过训练的科学家根据1-5分制（1=最成熟）对每张图像的血管生成成熟度进行评分。\n结果：每张图像的分析时间差异显著：手动（8分钟），ImageJ（58+/-4秒），和TubuleTracker（6+/-2秒）（p<0.0001）。在小管计数（手动168+/-SD，TubuleTracker 92+/-SD，ImageJ 433+/-SD）、长度和节点计数方面也发现了显著差异（所有p<0.0001）。TubuleTracker的指标在血管生成成熟度评分中表现出显著差异，包括小管计数、长度、节点计数、面积和圆度（所有p<0.0001）。\n结论：TubuleTracker比手动和基于ImageJ的分析更快、更一致。血管圆度被证明在捕捉血管生成成熟度方面特别有效。TubuleTracker作为免费共享软件供生物医学研究社区使用。", "summary": "本文介绍了TubuleTracker，一款高保真共享软件，旨在快速客观地量化体外血管生成网络的结构和成熟度。通过与手动分析和ImageJ的比较，研究发现TubuleTracker在分析速度上显著优越，并且在各项网络指标的量化上展现出更高的效率和一致性。此外，研究还发现TubuleTracker的指标，特别是血管圆度，能有效反映血管生成网络的成熟度。该软件的推出为生物医学研究提供了更高效、标准化的血管生成分析工具。", "keywords": "血管生成, 图像分析, 软件, TubuleTracker, 血管成熟度", "comments": "TubuleTracker提供了一个显著改进的解决方案，解决了现有体外血管生成图像分析耗时、主观和不准确的问题。其作为共享软件的可用性，将大大促进生物医学研究社区在该领域的研究效率和标准化。特别强调了“血管圆度”作为衡量成熟度的有效指标，这是一项重要的发现。"}}
{"id": "2507.02248", "title": "Transfer Learning for Matrix Completion", "authors": ["Dali Liu", "Haolei Weng"], "categories": ["stat.ML", "cs.LG", "15A83", "I.2.6; G.3"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      37 pages, 1 figure", "url": "http://arxiv.org/abs/2507.02248v1", "summary": "In this paper, we explore the knowledge transfer under the setting of matrix\ncompletion, which aims to enhance the estimation of a low-rank target matrix\nwith auxiliary data available. We propose a transfer learning procedure given\nprior information on which source datasets are favorable. We study its\nconvergence rates and prove its minimax optimality. Our analysis reveals that\nwith the source matrices close enough to the target matrix, out method\noutperforms the traditional method using the single target data. In particular,\nwe leverage the advanced sharp concentration inequalities introduced in\n\\cite{brailovskaya2024universality} to eliminate a logarithmic factor in the\nconvergence rate, which is crucial for proving the minimax optimality. When the\nrelevance of source datasets is unknown, we develop an efficient detection\nprocedure to identify informative sources and establish its selection\nconsistency. Simulations and real data analysis are conducted to support the\nvalidity of our methodology.", "comment": "37 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.02248v1", "cate": "stat.ML", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "矩阵补全的迁移学习", "tldr": "本文提出了一个用于矩阵补全的迁移学习方法，通过知识迁移利用辅助数据增强低秩目标矩阵的估计，并证明了其最优性。", "motivation": "旨在通过利用辅助数据，增强低秩目标矩阵的估计，解决传统矩阵补全方法可能遇到的局限性。", "method": "提出了一种迁移学习程序，当已知有利的源数据集时，利用其知识。当源数据集的相关性未知时，开发了一种有效的检测程序来识别信息源。方法中还利用了先进的集中不等式来优化收敛率。", "result": "证明了所提方法的收敛率和minimax最优性。分析表明，当源矩阵与目标矩阵足够接近时，该方法优于仅使用单一目标数据的传统方法。成功消除了收敛率中的对数因子。在源相关性未知时，建立了检测程序的选择一致性。模拟和真实数据分析支持了方法的有效性。", "conclusion": "本文提出的迁移学习方法在矩阵补全任务中表现出优越性，特别是在利用辅助数据增强估计和处理未知源相关性方面，并通过理论分析和实验验证了其有效性和最优性。", "translation": "在本文中，我们探讨了矩阵补全背景下的知识迁移，旨在利用辅助数据增强低秩目标矩阵的估计。我们提出了一种迁移学习程序，该程序在已知哪些源数据集有利的情况下进行。我们研究了其收敛速度并证明了其minimax最优性。我们的分析表明，当源矩阵与目标矩阵足够接近时，我们的方法优于仅使用单一目标数据的传统方法。特别是，我们利用了[brailovskaya2024universality]中引入的先进的尖锐集中不等式，以消除收敛速度中的一个对数因子，这对于证明minimax最优性至关重要。当源数据集的相关性未知时，我们开发了一种有效的检测程序来识别信息源并建立了其选择一致性。进行了模拟和真实数据分析以支持我们方法的有效性。", "summary": "本文研究了矩阵补全中的知识迁移，提出了一种利用辅助数据提升低秩目标矩阵估计的迁移学习方法。该方法在已知有利源数据时表现出优越的收敛率和minimax最优性，并能通过检测程序有效识别未知相关性的信息源。理论分析和实验验证了其超越传统方法的有效性。", "keywords": "迁移学习, 矩阵补全, 知识迁移, 最优性, 收敛率", "comments": "这篇论文的创新点在于将迁移学习引入矩阵补全领域，并通过严格的理论分析（包括minimax最优性和对数因子的消除）证明了其方法的优越性。此外，它还解决了源数据相关性未知时的实际问题，这增加了其实用性。"}}
{"id": "2507.02264", "title": "NLP4Neuro: Sequence-to-sequence learning for neural population decoding", "authors": ["Jacob J. Morra", "Kaitlyn E. Fouke", "Kexin Hang", "Zichen He", "Owen Traubert", "Timothy W. Dunn", "Eva A. Naumann"], "categories": ["q-bio.NC", "cs.LG"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "Comments:      17 pages, 6 figures", "url": "http://arxiv.org/abs/2507.02264v1", "summary": "Delineating how animal behavior arises from neural activity is a foundational\ngoal of neuroscience. However, as the computations underlying behavior unfold\nin networks of thousands of individual neurons across the entire brain, this\npresents challenges for investigating neural roles and computational mechanisms\nin large, densely wired mammalian brains during behavior. Transformers, the\nbackbones of modern large language models (LLMs), have become powerful tools\nfor neural decoding from smaller neural populations. These modern LLMs have\nbenefited from extensive pre-training, and their sequence-to-sequence learning\nhas been shown to generalize to novel tasks and data modalities, which may also\nconfer advantages for neural decoding from larger, brain-wide activity\nrecordings. Here, we present a systematic evaluation of off-the-shelf LLMs to\ndecode behavior from brain-wide populations, termed NLP4Neuro, which we used to\ntest LLMs on simultaneous calcium imaging and behavior recordings in larval\nzebrafish exposed to visual motion stimuli. Through NLP4Neuro, we found that\nLLMs become better at neural decoding when they use pre-trained weights learned\nfrom textual natural language data. Moreover, we found that a recent\nmixture-of-experts LLM, DeepSeek Coder-7b, significantly improved behavioral\ndecoding accuracy, predicted tail movements over long timescales, and provided\nanatomically consistent highly interpretable readouts of neuron salience.\nNLP4Neuro demonstrates that LLMs are highly capable of informing brain-wide\nneural circuit dissection.", "comment": "17 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.02264v1", "cate": "q-bio.NC", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "NLP4Neuro：用于神经群体解码的序列到序列学习", "tldr": "本文系统评估了预训练的大型语言模型（LLMs）在全脑神经活动解码行为方面的能力，发现LLMs在经过文本数据预训练后表现更佳，且某些特定LLM显著提高了行为解码精度和可解释性。", "motivation": "神经科学的一个基本目标是描绘动物行为如何从神经活动中产生。然而，在大而密集连接的哺乳动物大脑中研究行为期间的神经作用和计算机制面临挑战。Transformer模型在小型神经群体解码中表现出色，且其序列到序列学习能力有望推广到大规模全脑活动记录的神经解码。", "method": "提出了NLP4Neuro框架，系统评估了现成的LLMs从全脑神经群体中解码行为的能力。通过对斑马鱼幼虫同时进行钙成像和行为记录，并暴露于视觉运动刺激，测试了LLMs。", "result": "发现LLMs在使用从文本自然语言数据中学到的预训练权重时，神经解码能力更强。此外，发现一种混合专家LLM（DeepSeek Coder-7b）显著提高了行为解码精度，能在长时间尺度上预测尾部运动，并提供了在解剖学上一致且高度可解释的神经元显著性读数。", "conclusion": "NLP4Neuro表明，大型语言模型在揭示全脑神经回路方面具有很强的能力。", "translation": "从神经活动中描绘动物行为的产生方式是神经科学的一个基本目标。然而，由于行为背后的计算发生在整个大脑中数千个独立神经元的网络中，这给研究行为期间大型、密集连接的哺乳动物大脑中的神经作用和计算机制带来了挑战。Transformer，作为现代大型语言模型（LLMs）的骨干，已成为从较小神经群体进行神经解码的强大工具。这些现代LLMs受益于广泛的预训练，其序列到序列学习已被证明可以推广到新任务和数据模态，这可能也为从更大规模的全脑活动记录中进行神经解码带来优势。本文，我们提出了对现成LLMs进行系统评估的框架，以从全脑群体中解码行为，我们称之为NLP4Neuro，我们用它来测试LLMs在暴露于视觉运动刺激的斑马鱼幼虫中同时进行的钙成像和行为记录数据上的表现。通过NLP4Neuro，我们发现LLMs在使用从文本自然语言数据中学到的预训练权重时，神经解码能力更强。此外，我们发现一种最近的混合专家LLM，DeepSeek Coder-7b，显著提高了行为解码精度，能在长时间尺度上预测尾部运动，并提供了在解剖学上一致且高度可解释的神经元显著性读数。NLP4Neuro证明了LLMs在揭示全脑神经回路方面具有很强的能力。", "summary": "本文提出了NLP4Neuro框架，旨在系统评估大型语言模型（LLMs）在全脑神经活动解码行为方面的能力。研究发现，经过文本数据预训练的LLMs在神经解码方面表现更优，特别是DeepSeek Coder-7b等混合专家LLM能显著提升行为解码精度，并提供可解释的神经元显著性读数，展示了LLMs在解析全脑神经回路中的巨大潜力。", "keywords": "神经解码, 大型语言模型, 序列到序列学习, 全脑活动, 斑马鱼", "comments": "这项工作创新性地将大型语言模型应用于全脑神经解码，突破了传统神经解码方法在处理大规模、高维度神经数据时的局限性。通过利用LLMs的序列到序列学习能力和预训练优势，为理解行为背后的复杂神经机制提供了新视角。其重要性在于为神经科学研究提供了一个强大的新工具，尤其是在处理全脑尺度数据时，有望加速对神经回路功能的理解。"}}
{"id": "2507.02275", "title": "It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation", "authors": ["Jikai Jin", "Lester Mackey", "Vasilis Syrgkanis"], "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02275v1", "summary": "Structure-agnostic causal inference studies how well one can estimate a\ntreatment effect given black-box machine learning estimates of nuisance\nfunctions (like the impact of confounders on treatment and outcomes). Here, we\nfind that the answer depends in a surprising way on the distribution of the\ntreatment noise. Focusing on the partially linear model of\n\\citet{robinson1988root}, we first show that the widely adopted double machine\nlearning (DML) estimator is minimax rate-optimal for Gaussian treatment noise,\nresolving an open problem of \\citet{mackey2018orthogonal}. Meanwhile, for\nindependent non-Gaussian treatment noise, we show that DML is always suboptimal\nby constructing new practical procedures with higher-order robustness to\nnuisance errors. These \\emph{ACE} procedures use structure-agnostic cumulant\nestimators to achieve $r$-th order insensitivity to nuisance errors whenever\nthe $(r+1)$-st treatment cumulant is non-zero. We complement these core results\nwith novel minimax guarantees for binary treatments in the partially linear\nmodel. Finally, using synthetic demand estimation experiments, we demonstrate\nthe practical benefits of our higher-order robust estimators.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02275v1", "cate": "stat.ML", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "噪声对结构不可知估计的影响：正常很难", "tldr": "在结构不可知因果推断中，DML估计器对高斯噪声是最优的，但对非高斯噪声则不然；本文提出了新的ACE程序，对非高斯噪声提供更高阶的鲁棒性。", "motivation": "本研究旨在探究在结构不可知因果推断中，处理噪声的分布如何影响处理效应的估计，并解决在非高斯噪声下DML估计器的次优性问题。", "method": "本文聚焦于部分线性模型，分析了双重机器学习（DML）估计器在高斯和非高斯处理噪声下的表现。作者提出了新的“ACE”程序，利用结构不可知累积量估计器，在非高斯噪声存在时实现对混杂误差的更高阶鲁棒性。研究还为部分线性模型中的二元处理提供了新的极小极大保证，并使用合成需求估计实验来验证所提出估计器的实际效益。", "result": "研究表明，在部分线性模型中，双重机器学习（DML）估计器对高斯处理噪声是极小极大率最优的，解决了Mackey等人提出的一个开放问题。然而，对于独立的非高斯处理噪声，DML总是次优的。本文构建了新的实用程序（ACE），这些程序利用结构不可知累积量估计器，在第$(r+1)$阶处理累积量非零时，对混杂误差实现$r$阶不敏感性。通过合成需求估计实验，证明了这些更高阶鲁棒估计器的实际效益。", "conclusion": "处理噪声的分布对结构不可知因果推断中估计器的最优性有着显著影响。DML估计器在高斯噪声下是最佳选择，但在非高斯噪声情况下，需要并有效采用基于新累积量的程序来实现鲁棒估计。", "translation": "结构不可知因果推断研究了在给定混杂函数（如混杂因素对处理和结果的影响）的黑盒机器学习估计的情况下，处理效应能被估计得有多好。在此，我们发现答案以一种令人惊讶的方式取决于处理噪声的分布。我们专注于\\citet{robinson1988root}的部分线性模型，首先表明广泛采用的双重机器学习（DML）估计器对于高斯处理噪声是极小极大率最优的，这解决了\\citet{mackey2018orthogonal}的一个开放问题。同时，对于独立的非高斯处理噪声，我们通过构建具有更高阶鲁棒性的新实用程序来证明DML总是次优的。这些\\emph{ACE}程序利用结构不可知累积量估计器，只要第$(r+1)$阶处理累积量非零，就能实现对混杂误差的$r$阶不敏感性。我们通过部分线性模型中二元处理的新极小极大保证来补充这些核心结果。最后，利用合成需求估计实验，我们展示了我们更高阶鲁棒估计器的实际效益。", "summary": "本文探讨了处理噪声分布对结构不可知因果推断中处理效应估计的影响。研究发现，广泛使用的双重机器学习（DML）估计器对高斯处理噪声是极小极大率最优的，但对独立的非高斯噪声则是次优的。为了解决这一问题，作者提出了新颖的“ACE”程序，这些程序利用结构不可知累积量估计器，在非高斯噪声存在时，对混杂函数误差实现更高阶的鲁棒性。通过合成实验，本文验证了这些更高阶鲁棒估计器的实际益处。", "keywords": "因果推断, 双重机器学习, 非高斯噪声, 鲁棒估计, 部分线性模型", "comments": "该论文通过揭示估计器最优性对处理噪声分布的惊人依赖性，对DML的普遍适用性提出了挑战，从而做出了重要贡献。引入“ACE”程序为在更一般的噪声设置（特别是非高斯噪声，这在现实世界数据中很常见）中实现鲁棒估计提供了一种新颖且实用的解决方案。这项工作解决了一个开放问题，并扩展了因果推断的理论理解和实用工具。"}}
{"id": "2507.02644", "title": "Solving the Hubbard model with Neural Quantum States", "authors": ["Yuntian Gu", "Wenrui Li", "Heng Lin", "Bo Zhan", "Ruichen Li", "Yifei Huang", "Di He", "Yantao Wu", "Tao Xiang", "Mingpu Qin", "Liwei Wang", "Dingshun Lv"], "categories": ["cond-mat.str-el", "cs.AI", "quant-ph"], "primary_category": "Subjects:       Strongly Correlated Electrons (cond-mat.str-el)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02644v1", "summary": "The rapid development of neural quantum states (NQS) has established it as a\npromising framework for studying quantum many-body systems. In this work, by\nleveraging the cutting-edge transformer-based architectures and developing\nhighly efficient optimization algorithms, we achieve the state-of-the-art\nresults for the doped two-dimensional (2D) Hubbard model, arguably the minimum\nmodel for high-Tc superconductivity. Interestingly, we find different attention\nheads in the NQS ansatz can directly encode correlations at different scales,\nmaking it capable of capturing long-range correlations and entanglements in\nstrongly correlated systems. With these advances, we establish the half-filled\nstripe in the ground state of 2D Hubbard model with the next nearest\nneighboring hoppings, consistent with experimental observations in cuprates.\nOur work establishes NQS as a powerful tool for solving challenging\nmany-fermions systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02644v1", "cate": "cond-mat.str-el", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "利用神经网络量子态求解哈伯德模型", "tldr": "本文利用基于Transformer的神经网络量子态，高效求解了掺杂二维哈伯德模型，实现了最先进的结果，并发现了NQS能编码不同尺度的关联，验证了二维哈伯德模型基态的半填充条纹结构。", "motivation": "神经网络量子态（NQS）的快速发展使其成为研究量子多体系统的一个有前景的框架。本研究旨在利用NQS求解掺杂二维哈伯德模型，该模型被认为是高温超导的最小模型，并力求达到最先进的结果。", "method": "本研究利用了尖端的基于Transformer的神经网络量子态（NQS）架构，并开发了高效的优化算法，以求解掺杂二维哈伯德模型。", "result": "研究在掺杂二维哈伯德模型上取得了最先进的结果。发现NQS波函数中的不同注意力头可以直接编码不同尺度的关联，使其能够捕获强关联系统中的长程关联和纠缠。同时，本工作确定了包含次近邻跳跃的二维哈伯德模型基态中的半填充条纹结构，这与铜氧化物中的实验观察结果一致。", "conclusion": "本工作确立了神经网络量子态（NQS）作为解决具有挑战性的多费米子系统的强大工具。", "translation": "神经网络量子态（NQS）的快速发展已使其成为研究量子多体系统的一个有前景的框架。在这项工作中，通过利用尖端的基于Transformer的架构并开发高效的优化算法，我们为掺杂二维哈伯德模型（可以说是高温超导的最小模型）取得了最先进的结果。有趣的是，我们发现NQS波函数中的不同注意力头可以直接编码不同尺度的关联，使其能够捕获强关联系统中的长程关联和纠缠。凭借这些进展，我们确立了包含次近邻跳跃的二维哈伯德模型基态中的半填充条纹结构，这与铜氧化物中的实验观察结果一致。我们的工作确立了NQS作为解决具有挑战性的多费米子系统的强大工具。", "summary": "本研究利用基于Transformer的神经网络量子态（NQS）和高效优化算法，成功求解了掺杂二维哈伯德模型，并取得了最先进的结果。研究发现NQS中的注意力头能够编码不同尺度的关联，从而捕获长程关联和纠缠。此外，本工作确认了二维哈伯德模型基态中的半填充条纹结构，与实验观察结果相符，进一步确立了NQS在解决多费米子系统中的强大能力。", "keywords": "神经网络量子态, 哈伯德模型, Transformer, 高温超导, 量子多体系统", "comments": "本文的创新点在于将Transformer架构引入神经网络量子态，并开发高效优化算法，从而在解决哈伯德模型这一重要量子多体问题上取得了显著进展。其重要性体现在为高温超导机理的研究提供了新的计算工具和视角，特别是发现了NQS能编码不同尺度关联的能力，这对于理解强关联系统至关重要。研究结果与实验观测的一致性进一步增强了其可靠性。"}}
{"id": "2507.02377", "title": "Sparse Gaussian Processes: Structured Approximations and Power-EP Revisited", "authors": ["Thang D. Bui", "Michalis K. Titsias"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02377v1", "summary": "Inducing-point-based sparse variational Gaussian processes have become the\nstandard workhorse for scaling up GP models. Recent advances show that these\nmethods can be improved by introducing a diagonal scaling matrix to the\nconditional posterior density given the inducing points. This paper first\nconsiders an extension that employs a block-diagonal structure for the scaling\nmatrix, provably tightening the variational lower bound. We then revisit the\nunifying framework of sparse GPs based on Power Expectation Propagation (PEP)\nand show that it can leverage and benefit from the new structured approximate\nposteriors. Through extensive regression experiments, we show that the proposed\nblock-diagonal approximation consistently performs similarly to or better than\nexisting diagonal approximations while maintaining comparable computational\ncosts. Furthermore, the new PEP framework with structured posteriors provides\ncompetitive performance across various power hyperparameter settings, offering\npractitioners flexible alternatives to standard variational approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02377v1", "cate": "stat.ML", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "稀疏高斯过程：结构化近似与幂期望传播再探", "tldr": "本文提出了一种基于块对角结构的新型稀疏高斯过程近似方法，并将其与幂期望传播（PEP）框架结合，在保持计算成本的同时，性能优于现有方法。", "motivation": "基于诱导点的稀疏变分高斯过程已成为扩展高斯过程模型的标准方法。最近的研究表明，通过在给定诱导点的条件后验密度中引入对角尺度矩阵，可以改进这些方法。本文旨在进一步改进这些稀疏GP模型。", "method": "本文首先提出了一种采用块对角结构尺度矩阵的扩展，并证明这能收紧变分下界。接着，作者重新探讨了基于幂期望传播（PEP）的稀疏GP统一框架，并展示了该框架如何利用并受益于新的结构化近似后验。", "result": "通过大量的回归实验，研究表明所提出的块对角近似方法始终表现出与现有对角近似方法相似或更好的性能，同时保持了可比的计算成本。此外，带有结构化后验的新PEP框架在各种幂超参数设置下提供了具有竞争力的性能。", "conclusion": "本文提出的块对角近似方法在稀疏高斯过程中表现出色，并与幂期望传播框架结合，为实践者提供了优于标准变分方法的灵活替代方案。", "translation": "基于诱导点的稀疏变分高斯过程已成为扩展GP模型的标准方法。最近的进展表明，通过在给定诱导点的条件后验密度中引入对角尺度矩阵，可以改进这些方法。本文首先考虑了一种采用块对角结构尺度矩阵的扩展，可证明其能收紧变分下界。然后，我们重新审视了基于幂期望传播（PEP）的稀疏GP统一框架，并表明它能够利用并受益于新的结构化近似后验。通过广泛的回归实验，我们表明所提出的块对角近似始终表现出与现有对角近似相似或更好的性能，同时保持了可比的计算成本。此外，带有结构化后验的新PEP框架在各种幂超参数设置下提供了具有竞争力的性能，为实践者提供了标准变分方法的灵活替代方案。", "summary": "本文提出了一种改进稀疏高斯过程（GP）的新方法，通过引入块对角结构尺度矩阵来收紧变分下界。该方法与现有的对角近似相比，在回归实验中表现出更好或相似的性能，且计算成本相当。此外，该研究将此结构化近似整合到幂期望传播（PEP）框架中，证明了其在不同超参数设置下的竞争力，为实际应用提供了更灵活的选择。", "keywords": "稀疏高斯过程, 结构化近似, 幂期望传播, 块对角, 变分推断", "comments": "本文的创新点在于引入了块对角结构来改进稀疏高斯过程的近似，并证明了其在理论上能收紧变分下界。将这种结构化近似与幂期望传播（PEP）框架结合，进一步提升了模型的灵活性和性能。这为高斯过程在大规模数据上的应用提供了更有效和鲁棒的解决方案，具有重要的实践意义。"}}
{"id": "2507.02752", "title": "Synthesizable by Design: A Retrosynthesis-Guided Framework for Molecular Analog Generation", "authors": ["Shuan Chen", "Gunwook Nam", "Yousung Jung"], "categories": ["physics.chem-ph", "cs.AI"], "primary_category": "Subjects:       Chemical Physics (physics.chem-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02752v1", "summary": "The disconnect between AI-generated molecules with desirable properties and\ntheir synthetic feasibility remains a critical bottleneck in computational drug\nand material discovery. While generative AI has accelerated the proposal of\ncandidate molecules, many of these structures prove challenging or impossible\nto synthesize using established chemical reactions. Here, we introduce\nSynTwins, a novel retrosynthesis-guided molecular analog design framework that\ndesigns synthetically accessible molecular analogs by emulating expert chemist\nstrategies through a three-step process: retrosynthesis, similar building block\nsearching, and virtual synthesis. In comparative evaluations, SynTwins\ndemonstrates superior performance in generating synthetically accessible\nanalogs compared to state-of-the-art machine learning models while maintaining\nhigh structural similarity to original target molecules. Furthermore, when\nintegrated with existing molecule optimization frameworks, our hybrid approach\nproduces synthetically feasible molecules with property profiles comparable to\nunconstrained molecule generators, yet its synthesizability ensured. Our\ncomprehensive benchmarking across diverse molecular datasets demonstrates that\nSynTwins effectively bridges the gap between computational design and\nexperimental synthesis, providing a practical solution for accelerating the\ndiscovery of synthesizable molecules with desired properties for a wide range\nof applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02752v1", "cate": "physics.chem-ph", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "可合成性设计：一种逆合成指导的分子类似物生成框架", "tldr": "SynTwins是一种新的逆合成指导框架，通过模拟专家化学家策略，生成可合成的分子类似物，弥合了计算设计与实验合成之间的鸿沟。", "motivation": "AI生成的分子在所需性质方面表现良好，但其合成可行性与实际合成存在脱节，这是计算药物和材料发现中的一个关键瓶颈。", "method": "本文引入了SynTwins，一个新颖的逆合成指导分子类似物设计框架。它通过模拟专家化学家策略，采用三步过程设计可合成的分子类似物：逆合成、相似构建块搜索和虚拟合成。", "result": "在比较评估中，SynTwins在生成可合成类似物方面表现出优于最先进机器学习模型的性能，同时保持了与原始目标分子的高度结构相似性。当与现有分子优化框架结合时，其混合方法产生的分子具有与无约束分子生成器相当的性质，但合成可行性得到保证。", "conclusion": "SynTwins有效弥合了计算设计与实验合成之间的差距，为加速发现具有所需性质的可合成分子提供了一个实用解决方案，适用于广泛的应用。", "translation": "AI生成的分子在具有理想性质方面与它们的合成可行性之间的脱节仍然是计算药物和材料发现中的一个关键瓶颈。尽管生成式AI加速了候选分子的提出，但其中许多结构被证明难以或不可能使用已建立的化学反应进行合成。在此，我们引入了SynTwins，一种新颖的逆合成指导分子类似物设计框架，它通过逆合成、相似构建块搜索和虚拟合成三步过程，模拟专家化学家策略，设计可合成的分子类似物。在比较评估中，SynTwins在生成可合成类似物方面表现出优于最先进机器学习模型的性能，同时保持了与原始目标分子的高度结构相似性。此外，当与现有分子优化框架结合时，我们的混合方法产生的分子具有与无约束分子生成器相当的性质，但其可合成性得到了保证。我们对各种分子数据集的全面基准测试表明，SynTwins有效弥合了计算设计与实验合成之间的差距，为加速发现具有所需性质的可合成分子提供了一个实用解决方案，适用于广泛的应用。", "summary": "本研究提出了SynTwins，一个逆合成指导的分子类似物设计框架，旨在解决AI生成分子与实际合成可行性之间的脱节。SynTwins通过模拟专家化学家的策略，采用逆合成、相似构建块搜索和虚拟合成三步法来生成可合成的分子。实验证明，SynTwins在生成可合成类似物方面优于现有模型，并能与分子优化框架结合，产生具有理想性质且合成可行的分子。该框架为加速可合成分子的发现提供了实用方案。", "keywords": "逆合成, 分子生成, 合成可行性, 药物发现, AI", "comments": "SynTwins通过结合逆合成策略，有效解决了AI生成分子在合成可行性方面的挑战，这在计算药物和材料发现领域具有重要意义。其创新之处在于模拟专家化学家的思维过程，将合成可及性融入设计初期，而非事后验证。"}}
{"id": "2507.02801", "title": "Learning to Coordinate Bidders in Non-Truthful Auctions", "authors": ["Hu Fu", "Tao Lin"], "categories": ["cs.GT", "cs.LG", "econ.TH"], "primary_category": "Subjects:       Computer Science and Game Theory (cs.GT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.02801v1", "summary": "In non-truthful auctions such as first-price and all-pay auctions, the\nindependent strategic behaviors of bidders, with the corresponding equilibrium\nnotion -- Bayes Nash equilibria -- are notoriously difficult to characterize\nand can cause undesirable outcomes. An alternative approach to designing better\nauction systems is to coordinate the bidders: let a mediator make\nincentive-compatible recommendations of correlated bidding strategies to the\nbidders, namely, implementing a Bayes correlated equilibrium (BCE). The\nimplementation of BCE, however, requires knowledge of the distribution of\nbidders' private valuations, which is often unavailable. We initiate the study\nof the sample complexity of learning Bayes correlated equilibria in\nnon-truthful auctions. We prove that the BCEs in a large class of non-truthful\nauctions, including first-price and all-pay auctions, can be learned with a\npolynomial number $\\tilde O(\\frac{n}{\\varepsilon^2})$ of samples from the\nbidders' value distributions. Our technique is a reduction to the problem of\nestimating bidders' expected utility from samples, combined with an analysis of\nthe pseudo-dimension of the class of all monotone bidding strategies of\nbidders.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.02801v1", "cate": "cs.GT", "date": "2025-07-03", "updated": "2025-07-03", "AI": {"title_translation": "非真实性拍卖中竞标者协调学习", "tldr": "在非真实性拍卖中，本文首次研究了学习贝叶斯相关均衡（BCE）的样本复杂度，并证明对于包括第一价格和全付拍卖在内的一大类非真实性拍卖，BCE可以通过多项式数量的样本学习得到。", "motivation": "在第一价格和全付拍卖等非真实性拍卖中，竞标者的独立策略行为及其贝叶斯纳什均衡难以表征，并可能导致不良结果。虽然通过中介协调竞标者以实施贝叶斯相关均衡（BCE）是一种替代方案，但其实施需要了解竞标者私人估值的分布，而这通常是不可获得的。", "method": "本文通过将问题转化为从样本中估计竞标者预期效用的问题，并结合对竞标者所有单调投标策略类别的伪维度分析，来研究学习贝叶斯相关均衡的样本复杂度。", "result": "本文证明了在一大类非真实性拍卖中，包括第一价格和全付拍卖，贝叶斯相关均衡（BCE）可以通过多项式数量的样本（$\tilde O(\frac{n}{\\varepsilon^2})$）从竞标者的价值分布中学习得到。", "conclusion": "本文首次开启了在非真实性拍卖中学习贝叶斯相关均衡样本复杂度的研究，并为在缺乏估值分布知识的情况下实现竞标者协调提供了理论基础。", "translation": "在第一价格和全付拍卖等非真实性拍卖中，竞标者的独立策略行为及其相应的均衡概念——贝叶斯纳什均衡——众所周知难以表征，并可能导致不良结果。设计更好拍卖系统的另一种方法是协调竞标者：让中介向竞标者提供激励兼容的相关投标策略建议，即实施贝叶斯相关均衡（BCE）。然而，BCE的实施需要了解竞标者私人估值的分布，而这通常是不可获得的。我们首次研究了在非真实性拍卖中学习贝叶斯相关均衡的样本复杂度。我们证明了在一大类非真实性拍卖中，包括第一价格和全付拍卖，BCE可以通过多项式数量的样本（$\tilde O(\frac{n}{\\varepsilon^2})$）从竞标者的价值分布中学习得到。我们的技术是将其归结为从样本中估计竞标者预期效用的问题，并结合对竞标者所有单调投标策略类别伪维度的分析。", "summary": "本文研究了在非真实性拍卖中协调竞标者的问题，特别是通过学习贝叶斯相关均衡（BCE）来克服独立策略行为和贝叶斯纳什均衡的局限性。针对BCE实施中缺乏竞标者估值分布知识的挑战，本文首次探讨了学习BCE的样本复杂度。研究证明，在包括第一价格和全付拍卖在内的大多数非真实性拍卖中，BCE可以通过多项式数量的样本（$\tilde O(\frac{n}{\\varepsilon^2})$）学习得到。这一成果是通过将问题简化为效用估计，并结合单调投标策略伪维度分析实现的。", "keywords": "非真实性拍卖, 贝叶斯相关均衡, 样本复杂度, 协调机制, 机器学习", "comments": "本文的创新之处在于首次系统地研究了在非真实性拍卖中学习贝叶斯相关均衡（BCE）的样本复杂度问题。它为在实际应用中，当竞标者估值分布未知时，如何通过数据驱动的方法实现竞标者协调提供了重要的理论基础和可行性证明。所提出的基于效用估计和伪维度分析的技术具有通用性，可能对其他机制设计问题也有借鉴意义。"}}
