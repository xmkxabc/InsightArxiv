{"id": "2504.17039", "pdf": "https://arxiv.org/pdf/2504.17039", "abs": "https://arxiv.org/abs/2504.17039", "authors": ["Ruben Gonzalez Avil\u00e9s", "Linus Scheibenreif", "Damian Borth"], "title": "Dense Air Pollution Estimation from Sparse in-situ Measurements and Satellite Data", "categories": ["cs.CV"], "comment": null, "summary": "This paper addresses the critical environmental challenge of estimating\nambient Nitrogen Dioxide (NO$_2$) concentrations, a key issue in public health\nand environmental policy. Existing methods for satellite-based air pollution\nestimation model the relationship between satellite and in-situ measurements at\nselect point locations. While these approaches have advanced our ability to\nprovide air quality estimations on a global scale, they come with inherent\nlimitations. The most notable limitation is the computational intensity\nrequired for generating comprehensive estimates over extensive areas. Motivated\nby these limitations, this study introduces a novel dense estimation technique.\nOur approach seeks to balance the accuracy of high-resolution estimates with\nthe practicality of computational constraints, thereby enabling efficient and\nscalable global environmental assessment. By utilizing a uniformly random\noffset sampling strategy, our method disperses the ground truth data pixel\nlocation evenly across a larger patch. At inference, the dense estimation\nmethod can then generate a grid of estimates in a single step, significantly\nreducing the computational resources required to provide estimates for larger\nareas. Notably, our approach also surpasses the results of existing point-wise\nmethods by a significant margin of $9.45\\%$, achieving a Mean Absolute Error\n(MAE) of $4.98\\ \\mu\\text{g}/\\text{m}^3$. This demonstrates both high accuracy\nand computational efficiency, highlighting the applicability of our method for\nglobal environmental assessment. Furthermore, we showcase the method's\nadaptability and robustness by applying it to diverse geographic regions. Our\nmethod offers a viable solution to the computational challenges of large-scale\nenvironmental monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bc6\u96c6\u4f30\u8ba1\u6280\u672f\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u5730\u4f30\u7b97\u5168\u7403\u73af\u5883\u4e2d\u7684\u6c2e\u6c27\u5316\u7269\uff08NO$_2$\uff09\u6d53\u5ea6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u536b\u661f\u7a7a\u6c14\u8d28\u91cf\u4f30\u7b97\u65b9\u6cd5\u5728\u8ba1\u7b97\u5927\u8303\u56f4\u533a\u57df\u65f6\u5b58\u5728\u8ba1\u7b97\u5f3a\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u5747\u5300\u968f\u673a\u504f\u79fb\u91c7\u6837\u7b56\u7565\uff0c\u5c06\u5730\u9762\u771f\u5b9e\u6570\u636e\u50cf\u7d20\u4f4d\u7f6e\u5747\u5300\u5206\u6563\u5230\u66f4\u5927\u7684\u533a\u57df\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u4e00\u6b65\u751f\u6210\u5bc6\u96c6\u4f30\u8ba1\u7f51\u683c\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4e0a\u6bd4\u73b0\u6709\u70b9\u65b9\u6cd5\u663e\u8457\u964d\u4f4e9.45%\uff0c\u8fbe\u52304.98 \u03bcg/m\u00b3\uff0c\u517c\u5177\u9ad8\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u73af\u5883\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5176\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.17040", "pdf": "https://arxiv.org/pdf/2504.17040", "abs": "https://arxiv.org/abs/2504.17040", "authors": ["Zhenhailong Wang", "Senthil Purushwalkam", "Caiming Xiong", "Silvio Savarese", "Heng Ji", "Ran Xu"], "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We present DyMU, an efficient, training-free framework that dynamically\nreduces the computational burden of vision-language models (VLMs) while\nmaintaining high task performance. Our approach comprises two key components.\nFirst, Dynamic Token Merging (DToMe) reduces the number of visual token\nembeddings by merging similar tokens based on image complexity, addressing the\ninherent inefficiency of fixed-length outputs in vision transformers. Second,\nVirtual Token Unmerging (VTU) simulates the expected token sequence for large\nlanguage models (LLMs) by efficiently reconstructing the attention dynamics of\na full sequence, thus preserving the downstream performance without additional\nfine-tuning. Unlike previous approaches, our method dynamically adapts token\ncompression to the content of the image and operates completely training-free,\nmaking it readily applicable to most state-of-the-art VLM architectures.\nExtensive experiments on image and video understanding tasks demonstrate that\nDyMU can reduce the average visual token count by 32%-85% while achieving\ncomparable performance to full-length models across diverse VLM architectures,\nincluding the recently popularized AnyRes-based visual encoders. Furthermore,\nthrough qualitative analyses, we demonstrate that DToMe effectively adapts\ntoken reduction based on image complexity and, unlike existing systems,\nprovides users more control over computational costs. Project page:\nhttps://mikewangwzhl.github.io/dymu/.", "AI": {"tldr": "DyMU\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u52a8\u6001\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u56fa\u5b9a\u957f\u5ea6\u8f93\u51fa\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u52a8\u6001\u9002\u5e94\u56fe\u50cf\u5185\u5bb9\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u7ed3\u5408\u52a8\u6001\u4ee4\u724c\u5408\u5e76\uff08DToMe\uff09\u548c\u865a\u62df\u4ee4\u724c\u89e3\u5408\u5e76\uff08VTU\uff09\uff0c\u52a8\u6001\u8c03\u6574\u89c6\u89c9\u4ee4\u724c\u6570\u91cf\u5e76\u6a21\u62df\u5b8c\u6574\u5e8f\u5217\u7684\u6ce8\u610f\u529b\u52a8\u6001\u3002", "result": "\u5728\u56fe\u50cf\u548c\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\uff0c\u89c6\u89c9\u4ee4\u724c\u6570\u91cf\u51cf\u5c1132%-85%\uff0c\u6027\u80fd\u4e0e\u5b8c\u6574\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "DyMU\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u52a8\u6001\u4ee4\u724c\u538b\u7f29\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u63a7\u5236\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2504.17067", "pdf": "https://arxiv.org/pdf/2504.17067", "abs": "https://arxiv.org/abs/2504.17067", "authors": ["Xinqi Xiong", "Andrea Dunn Beltran", "Jun Myeong Choi", "Marc Niethammer", "Roni Sengupta"], "title": "PPS-Ctrl: Controllable Sim-to-Real Translation for Colonoscopy Depth Estimation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate depth estimation enhances endoscopy navigation and diagnostics, but\nobtaining ground-truth depth in clinical settings is challenging. Synthetic\ndatasets are often used for training, yet the domain gap limits generalization\nto real data. We propose a novel image-to-image translation framework that\npreserves structure while generating realistic textures from clinical data. Our\nkey innovation integrates Stable Diffusion with ControlNet, conditioned on a\nlatent representation extracted from a Per-Pixel Shading (PPS) map. PPS\ncaptures surface lighting effects, providing a stronger structural constraint\nthan depth maps. Experiments show our approach produces more realistic\ntranslations and improves depth estimation over GAN-based MI-CycleGAN. Our code\nis publicly accessible at https://github.com/anaxqx/PPS-Ctrl.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Stable Diffusion\u548cControlNet\u7684\u56fe\u50cf\u7ffb\u8bd1\u6846\u67b6\uff0c\u5229\u7528Per-Pixel Shading (PPS) \u56fe\u751f\u6210\u66f4\u771f\u5b9e\u7684\u7eb9\u7406\uff0c\u63d0\u5347\u6df1\u5ea6\u4f30\u8ba1\u6548\u679c\u3002", "motivation": "\u4e34\u5e8a\u73af\u5883\u4e2d\u83b7\u53d6\u771f\u5b9e\u6df1\u5ea6\u6570\u636e\u56f0\u96be\uff0c\u5408\u6210\u6570\u636e\u8bad\u7ec3\u5b58\u5728\u9886\u57df\u5dee\u8ddd\uff0c\u9700\u6539\u8fdb\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5\u4ee5\u63d0\u5347\u6df1\u5ea6\u4f30\u8ba1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6574\u5408Stable Diffusion\u4e0eControlNet\uff0c\u4ee5PPS\u56fe\u63d0\u53d6\u7684\u6f5c\u5728\u8868\u793a\u4f5c\u4e3a\u6761\u4ef6\uff0c\u4fdd\u7559\u7ed3\u6784\u5e76\u751f\u6210\u771f\u5b9e\u7eb9\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u4e8eGAN\u7684MI-CycleGAN\u751f\u6210\u66f4\u771f\u5b9e\u7684\u56fe\u50cf\uff0c\u5e76\u63d0\u5347\u4e86\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7PPS\u56fe\u7684\u7ed3\u6784\u7ea6\u675f\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u50cf\u7ffb\u8bd1\u7684\u771f\u5b9e\u6027\u548c\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2504.17069", "pdf": "https://arxiv.org/pdf/2504.17069", "abs": "https://arxiv.org/abs/2504.17069", "authors": ["Rishav Pramanik", "Antoine Poupon", "Juan A. Rodriguez", "Masih Aminbeidokhti", "David Vazquez", "Christopher Pal", "Zhaozheng Yin", "Marco Pedersoli"], "title": "Distilling semantically aware orders for autoregressive image generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autoregressive patch-based image generation has recently shown competitive\nresults in terms of image quality and scalability. It can also be easily\nintegrated and scaled within Vision-Language models. Nevertheless,\nautoregressive models require a defined order for patch generation. While a\nnatural order based on the dictation of the words makes sense for text\ngeneration, there is no inherent generation order that exists for image\ngeneration. Traditionally, a raster-scan order (from top-left to bottom-right)\nguides autoregressive image generation models. In this paper, we argue that\nthis order is suboptimal, as it fails to respect the causality of the image\ncontent: for instance, when conditioned on a visual description of a sunset, an\nautoregressive model may generate clouds before the sun, even though the color\nof clouds should depend on the color of the sun and not the inverse. In this\nwork, we show that first by training a model to generate patches in\nany-given-order, we can infer both the content and the location (order) of each\npatch during generation. Secondly, we use these extracted orders to finetune\nthe any-given-order model to produce better-quality images. Through our\nexperiments, we show on two datasets that this new generation method produces\nbetter images than the traditional raster-scan approach, with similar training\ncosts and no extra annotations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u4ee5\u4efb\u610f\u987a\u5e8f\u751f\u6210\u56fe\u50cf\u5757\uff0c\u5e76\u5229\u7528\u63a8\u65ad\u7684\u987a\u5e8f\u4f18\u5316\u751f\u6210\u8d28\u91cf\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u6805\u683c\u626b\u63cf\u987a\u5e8f\u3002", "motivation": "\u4f20\u7edf\u7684\u6805\u683c\u626b\u63cf\u987a\u5e8f\uff08\u4ece\u5de6\u5230\u53f3\u3001\u4ece\u4e0a\u5230\u4e0b\uff09\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u5ffd\u7565\u4e86\u5185\u5bb9\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5bfc\u81f4\u751f\u6210\u987a\u5e8f\u4e0d\u5408\u7406\uff08\u5982\u5148\u4e91\u540e\u592a\u9633\uff09\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "1. \u8bad\u7ec3\u6a21\u578b\u4ee5\u4efb\u610f\u987a\u5e8f\u751f\u6210\u56fe\u50cf\u5757\uff1b2. \u63a8\u65ad\u751f\u6210\u987a\u5e8f\uff1b3. \u5229\u7528\u63a8\u65ad\u7684\u987a\u5e8f\u5fae\u8c03\u6a21\u578b\u4ee5\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u4f18\u4e8e\u4f20\u7edf\u6805\u683c\u626b\u63cf\u987a\u5e8f\uff0c\u4e14\u8bad\u7ec3\u6210\u672c\u548c\u989d\u5916\u6807\u6ce8\u9700\u6c42\u76f8\u540c\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u751f\u6210\u987a\u5e8f\uff0c\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u66f4\u5408\u7406\u5730\u53cd\u6620\u56fe\u50cf\u5185\u5bb9\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2504.16956", "pdf": "https://arxiv.org/pdf/2504.16956", "abs": "https://arxiv.org/abs/2504.16956", "authors": ["Cong Qi", "Hanzhang Fang", "Tianxing Hu", "Siqi Jiang", "Wei Zhi"], "title": "Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity", "categories": ["cs.CL", "cs.LG", "q-bio.GN"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of\ncellular heterogeneity, but its complexity, which is marked by high\ndimensionality, sparsity, and batch effects, which poses major computational\nchallenges. Transformer-based models have made significant advances in this\ndomain but are often limited by their quadratic complexity and suboptimal\nhandling of long-range dependencies. In this work, we introduce GeneMamba, a\nscalable and efficient foundation model for single-cell transcriptomics built\non state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba\ncaptures bidirectional gene context with linear-time complexity, offering\nsubstantial computational gains over transformer baselines. The model is\npretrained on nearly 30 million cells and incorporates biologically informed\nobjectives, including pathway-aware contrastive loss and rank-based gene\nencoding. We evaluate GeneMamba across diverse tasks, including multi-batch\nintegration, cell type annotation, and gene-gene correlation, demonstrating\nstrong performance, interpretability, and robustness. These results position\nGeneMamba as a practical and powerful alternative to transformer-based methods,\nadvancing the development of biologically grounded, scalable tools for\nlarge-scale single-cell data analysis.", "AI": {"tldr": "GeneMamba\u662f\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u7684\u5355\u7ec6\u80de\u8f6c\u5f55\u7ec4\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7Bi-Mamba\u67b6\u6784\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u53cc\u5411\u57fa\u56e0\u4e0a\u4e0b\u6587\u6355\u6349\uff0c\u4f18\u4e8e\u4f20\u7edfTransformer\u65b9\u6cd5\u3002", "motivation": "\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\uff08scRNA-seq\uff09\u7684\u9ad8\u7ef4\u5ea6\u3001\u7a00\u758f\u6027\u548c\u6279\u6b21\u6548\u5e94\u5e26\u6765\u4e86\u8ba1\u7b97\u6311\u6218\uff0c\u73b0\u6709Transformer\u6a21\u578b\u56e0\u4e8c\u6b21\u590d\u6742\u5ea6\u548c\u957f\u7a0b\u4f9d\u8d56\u5904\u7406\u4e0d\u8db3\u800c\u53d7\u9650\u3002", "method": "GeneMamba\u91c7\u7528Bi-Mamba\u67b6\u6784\uff0c\u7ed3\u5408\u751f\u7269\u5b66\u76ee\u6807\uff08\u5982\u901a\u8def\u611f\u77e5\u5bf9\u6bd4\u635f\u5931\u548c\u57fa\u4e8e\u6392\u540d\u7684\u57fa\u56e0\u7f16\u7801\uff09\uff0c\u57283000\u4e07\u7ec6\u80de\u4e0a\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u6279\u6b21\u6574\u5408\u3001\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u548c\u57fa\u56e0-\u57fa\u56e0\u76f8\u5173\u6027\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5f3a\u5065\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "GeneMamba\u662fTransformer\u65b9\u6cd5\u7684\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5927\u89c4\u6a21\u5355\u7ec6\u80de\u6570\u636e\u5206\u6790\u5de5\u5177\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.17062", "pdf": "https://arxiv.org/pdf/2504.17062", "abs": "https://arxiv.org/abs/2504.17062", "authors": ["Yu Guo", "Zhiqiang Lao", "Xiyun Song", "Yubin Zhou", "Zongfang Lin", "Heather Yu"], "title": "ePBR: Extended PBR Materials in Image Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": "8 pages without references, 7 figures, accepted in CVPRW 2025", "summary": "Realistic indoor or outdoor image synthesis is a core challenge in computer\nvision and graphics. The learning-based approach is easy to use but lacks\nphysical consistency, while traditional Physically Based Rendering (PBR) offers\nhigh realism but is computationally expensive. Intrinsic image representation\noffers a well-balanced trade-off, decomposing images into fundamental\ncomponents (intrinsic channels) such as geometry, materials, and illumination\nfor controllable synthesis. However, existing PBR materials struggle with\ncomplex surface models, particularly high-specular and transparent surfaces. In\nthis work, we extend intrinsic image representations to incorporate both\nreflection and transmission properties, enabling the synthesis of transparent\nmaterials such as glass and windows. We propose an explicit intrinsic\ncompositing framework that provides deterministic, interpretable image\nsynthesis. With the Extended PBR (ePBR) Materials, we can effectively edit the\nmaterials with precise controls.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684\u7269\u7406\u6e32\u67d3\uff08ePBR\uff09\u6750\u6599\u65b9\u6cd5\uff0c\u7ed3\u5408\u53cd\u5c04\u548c\u900f\u5c04\u7279\u6027\uff0c\u7528\u4e8e\u5408\u6210\u900f\u660e\u6750\u6599\uff08\u5982\u73bb\u7483\u548c\u7a97\u6237\uff09\uff0c\u5e76\u901a\u8fc7\u660e\u786e\u7684\u672c\u5f81\u5408\u6210\u6846\u67b6\u5b9e\u73b0\u53ef\u63a7\u7684\u56fe\u50cf\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3\uff08PBR\uff09\u6750\u6599\u96be\u4ee5\u5904\u7406\u9ad8\u955c\u9762\u548c\u900f\u660e\u8868\u9762\uff0c\u800c\u672c\u5f81\u56fe\u50cf\u8868\u793a\u5728\u53ef\u63a7\u5408\u6210\u65b9\u9762\u63d0\u4f9b\u4e86\u5e73\u8861\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6269\u5c55\u672c\u5f81\u56fe\u50cf\u8868\u793a\u4ee5\u5305\u542b\u53cd\u5c04\u548c\u900f\u5c04\u7279\u6027\uff0c\u63d0\u51fa\u660e\u786e\u7684\u5408\u6210\u6846\u67b6\uff08ePBR\u6750\u6599\uff09\u4ee5\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u3002", "result": "\u80fd\u591f\u6709\u6548\u7f16\u8f91\u900f\u660e\u6750\u6599\uff0c\u63d0\u4f9b\u786e\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u5408\u6210\u3002", "conclusion": "ePBR\u6750\u6599\u4e3a\u590d\u6742\u8868\u9762\uff08\u5982\u900f\u660e\u6750\u6599\uff09\u7684\u5408\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17076", "pdf": "https://arxiv.org/pdf/2504.17076", "abs": "https://arxiv.org/abs/2504.17076", "authors": ["Jens Petersen", "Davide Abati", "Amirhossein Habibian", "Auke Wiggers"], "title": "Scene-Aware Location Modeling for Data Augmentation in Automotive Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Generative image models are increasingly being used for training data\naugmentation in vision tasks. In the context of automotive object detection,\nmethods usually focus on producing augmented frames that look as realistic as\npossible, for example by replacing real objects with generated ones. Others try\nto maximize the diversity of augmented frames, for example by pasting lots of\ngenerated objects onto existing backgrounds. Both perspectives pay little\nattention to the locations of objects in the scene. Frame layouts are either\nreused with little or no modification, or they are random and disregard realism\nentirely. In this work, we argue that optimal data augmentation should also\ninclude realistic augmentation of layouts. We introduce a scene-aware\nprobabilistic location model that predicts where new objects can realistically\nbe placed in an existing scene. By then inpainting objects in these locations\nwith a generative model, we obtain much stronger augmentation performance than\nexisting approaches. We set a new state of the art for generative data\naugmentation on two automotive object detection tasks, achieving up to\n$2.8\\times$ higher gains than the best competing approach ($+1.4$ vs. $+0.5$\nmAP boost). We also demonstrate significant improvements for instance\nsegmentation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u573a\u666f\u611f\u77e5\u7684\u6982\u7387\u4f4d\u7f6e\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u65b0\u7269\u4f53\u5728\u73b0\u6709\u573a\u666f\u4e2d\u7684\u5408\u7406\u4f4d\u7f6e\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u6a21\u578b\u5728\u8fd9\u4e9b\u4f4d\u7f6e\u586b\u5145\u7269\u4f53\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u6570\u636e\u589e\u5f3a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u56fe\u50cf\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u4e2d\uff0c\u901a\u5e38\u5ffd\u89c6\u7269\u4f53\u5728\u573a\u666f\u4e2d\u7684\u5408\u7406\u5e03\u5c40\uff0c\u5bfc\u81f4\u589e\u5f3a\u6548\u679c\u4e0d\u4f73\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u5e03\u5c40\u589e\u5f3a\uff0c\u63d0\u5347\u6570\u636e\u589e\u5f3a\u6027\u80fd\u3002", "method": "\u5f15\u5165\u573a\u666f\u611f\u77e5\u7684\u6982\u7387\u4f4d\u7f6e\u6a21\u578b\uff0c\u9884\u6d4b\u65b0\u7269\u4f53\u5728\u573a\u666f\u4e2d\u7684\u5408\u7406\u4f4d\u7f6e\uff0c\u5e76\u5229\u7528\u751f\u6210\u6a21\u578b\u5728\u8fd9\u4e9b\u4f4d\u7f6e\u586b\u5145\u7269\u4f53\u3002", "result": "\u5728\u4e24\u9879\u6c7d\u8f66\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u7684\u6027\u80fd\u63d0\u5347\uff08+1.4 vs. +0.5 mAP\uff09\uff0c\u5e76\u5728\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u5e03\u5c40\u589e\u5f3a\uff0c\u751f\u6210\u6570\u636e\u589e\u5f3a\u7684\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4e3a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002"}}
{"id": "2504.16977", "pdf": "https://arxiv.org/pdf/2504.16977", "abs": "https://arxiv.org/abs/2504.16977", "authors": ["Priyaranjan Pattnayak", "Hitesh Laxmichand Patel", "Amit Agarwal"], "title": "Tokenization Matters: Improving Zero-Shot NER for Indic Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tokenization is a critical component of Natural Language Processing (NLP),\nespecially for low resource languages, where subword segmentation influences\nvocabulary structure and downstream task accuracy. Although Byte Pair Encoding\n(BPE) is a standard tokenization method in multilingual language models, its\nsuitability for Named Entity Recognition (NER) in low resource Indic languages\nremains underexplored due to its limitations in handling morphological\ncomplexity. In this work, we systematically compare BPE, SentencePiece, and\nCharacter Level tokenization strategies using IndicBERT for NER tasks in low\nresource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as\nextremely low resource Indic languages like Santali, Manipuri, and Sindhi. We\nassess both intrinsic linguistic properties tokenization efficiency, out of\nvocabulary (OOV) rates, and morphological preservation as well as extrinsic\ndownstream performance, including fine tuning and zero shot cross lingual\ntransfer.\n  Our experiments show that SentencePiece is a consistently better performing\napproach than BPE for NER in low resource Indic Languages, particularly in zero\nshot cross lingual settings, as it better preserves entity consistency. While\nBPE provides the most compact tokenization form, it is not capable of\ngeneralization because it misclassifies or even fails to recognize entity\nlabels when tested on unseen languages. In contrast, SentencePiece constitutes\na better linguistic structural preservation model, benefiting extremely low\nresource and morphologically rich Indic languages, such as Santali and\nManipuri, for superior entity recognition, as well as high generalization\nacross scripts, such as Sindhi, written in Arabic. The results point to\nSentencePiece as the more effective tokenization strategy for NER within\nmultilingual and low resource Indic NLP applications.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86BPE\u3001SentencePiece\u548c\u5b57\u7b26\u7ea7\u5206\u8bcd\u7b56\u7565\u5728\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00NER\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0SentencePiece\u5728\u8de8\u8bed\u8a00\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7814\u7a76BPE\u5728\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00NER\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5bf9\u5f62\u6001\u590d\u6742\u8bed\u8a00\u7684\u5904\u7406\u3002", "method": "\u4f7f\u7528IndicBERT\u5bf9\u591a\u79cd\u5370\u5ea6\u8bed\u8a00\uff08\u5982\u963f\u8428\u59c6\u8bed\u3001\u5b5f\u52a0\u62c9\u8bed\u7b49\uff09\u8fdb\u884cBPE\u3001SentencePiece\u548c\u5b57\u7b26\u7ea7\u5206\u8bcd\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "SentencePiece\u5728\u8de8\u8bed\u8a00\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u4e8eBPE\uff0c\u5c24\u5176\u5728\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\uff08\u5982Santali\u3001Manipuri\uff09\u4e2d\u3002", "conclusion": "SentencePiece\u662f\u4f4e\u8d44\u6e90\u5370\u5ea6\u8bed\u8a00NER\u4efb\u52a1\u4e2d\u66f4\u6709\u6548\u7684\u5206\u8bcd\u7b56\u7565\u3002"}}
{"id": "2504.17614", "pdf": "https://arxiv.org/pdf/2504.17614", "abs": "https://arxiv.org/abs/2504.17614", "authors": ["Jonathan Leaf", "David Sebastian Minor", "Gilles Daviet", "Nuttapong Chentanez", "Greg Klar", "Ed Quigley"], "title": "Bolt: Clothing Virtual Characters at Scale", "categories": ["cs.GR"], "comment": null, "summary": "Clothing virtual characters is a time-consuming and often manual process.\nOutfits can be composed of multiple garments, and each garment must be fitted\nto the unique shape of a character. Since characters can vary widely in size\nand shape, fitting outfits to many characters is a combinatorially large\nproblem. We present Bolt, a system designed to take outfits originally authored\non a source body and fit them to new body shapes via a three stage transfer,\ndrape, and rig process. First, our new garment transfer method transforms each\ngarment's 3D mesh positions to the new character, then optimizes the garment's\n2D sewing pattern while maintaining key features of the original seams and\nboundaries. Second, our system simulates the transferred garments to\nprogressively drape and untangle each garment in the outfit. Finally, the\ngarments are rigged to the new character. This entire process is automatic,\nmaking it feasible to clothe characters at scale with no human intervention.\nClothed characters are then ready for immediate use in applications such as\ngaming, animation, synthetic generation, and more.", "AI": {"tldr": "Bolt\u7cfb\u7edf\u901a\u8fc7\u4e09\u9636\u6bb5\uff08\u8f6c\u79fb\u3001\u60ac\u6302\u3001\u7ed1\u5b9a\uff09\u81ea\u52a8\u5c06\u670d\u88c5\u4ece\u6e90\u8eab\u4f53\u9002\u914d\u5230\u65b0\u8eab\u4f53\u5f62\u72b6\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u670d\u88c5\u865a\u62df\u89d2\u8272\u662f\u8017\u65f6\u4e14\u901a\u5e38\u9700\u8981\u624b\u52a8\u5b8c\u6210\u7684\u4efb\u52a1\uff0c\u5c24\u5176\u662f\u5f53\u89d2\u8272\u5f62\u72b6\u5dee\u5f02\u5927\u65f6\uff0c\u9002\u914d\u95ee\u9898\u53d8\u5f97\u590d\u6742\u3002", "method": "1. \u8f6c\u79fb\uff1a\u4f18\u53163D\u7f51\u683c\u548c2D\u7f1d\u7eab\u56fe\u6848\uff1b2. \u60ac\u6302\uff1a\u6a21\u62df\u670d\u88c5\u9010\u6b65\u60ac\u6302\u548c\u89e3\u7f20\uff1b3. \u7ed1\u5b9a\uff1a\u5c06\u670d\u88c5\u7ed1\u5b9a\u5230\u65b0\u89d2\u8272\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u670d\u88c5\u9002\u914d\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u89d2\u8272\u670d\u88c5\u9700\u6c42\u3002", "conclusion": "Bolt\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u670d\u88c5\u9002\u914d\u6548\u7387\uff0c\u9002\u7528\u4e8e\u6e38\u620f\u3001\u52a8\u753b\u7b49\u591a\u79cd\u5e94\u7528\u3002"}}
{"id": "2504.17111", "pdf": "https://arxiv.org/pdf/2504.17111", "abs": "https://arxiv.org/abs/2504.17111", "authors": ["Tekin Gunasar", "Virginia de Sa"], "title": "Transferring Spatial Filters via Tangent Space Alignment in Motor Imagery BCIs", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "We propose a method to improve subject transfer in motor imagery BCIs by\naligning covariance matrices on a Riemannian manifold, followed by computing a\nnew common spatial patterns (CSP) based spatial filter. We explore various ways\nto integrate information from multiple subjects and show improved performance\ncompared to standard CSP. Across three datasets, our method shows marginal\nimprovements over standard CSP; however, when training data are limited, the\nimprovements become more significant.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u9ece\u66fc\u6d41\u5f62\u5bf9\u9f50\u534f\u65b9\u5dee\u77e9\u9635\u5e76\u8ba1\u7b97\u65b0\u7684CSP\u7a7a\u95f4\u6ee4\u6ce2\u5668\u7684\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u8fd0\u52a8\u60f3\u8c61BCI\u4e2d\u7684\u4e3b\u9898\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u8fd0\u52a8\u60f3\u8c61BCI\u4e2d\u4e3b\u9898\u8fc1\u79fb\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u65f6\u3002", "method": "\u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u5bf9\u9f50\u534f\u65b9\u5dee\u77e9\u9635\uff0c\u7136\u540e\u8ba1\u7b97\u65b0\u7684CSP\u7a7a\u95f4\u6ee4\u6ce2\u5668\uff0c\u5e76\u63a2\u7d22\u591a\u4e3b\u9898\u4fe1\u606f\u6574\u5408\u65b9\u5f0f\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7565\u4f18\u4e8e\u6807\u51c6CSP\uff0c\u8bad\u7ec3\u6570\u636e\u6709\u9650\u65f6\u6539\u8fdb\u66f4\u663e\u8457\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u6709\u9650\u65f6\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e3aBCI\u4e3b\u9898\u8fc1\u79fb\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17025", "pdf": "https://arxiv.org/pdf/2504.17025", "abs": "https://arxiv.org/abs/2504.17025", "authors": ["Luca Moroni", "Giovanni Puccetti", "Pere-Lluis Huguet Cabot", "Andrei Stefan Bejgu", "Edoardo Barba", "Alessio Miaschi", "Felice Dell'Orletta", "Andrea Esuli", "Roberto Navigli"], "title": "Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "The number of pretrained Large Language Models (LLMs) is increasing steadily,\nthough the majority are designed predominantly for the English language. While\nstate-of-the-art LLMs can handle other languages, due to language contamination\nor some degree of multilingual pretraining data, they are not optimized for\nnon-English languages, leading to inefficient encoding (high token \"fertility\")\nand slower inference speed. In this work, we thoroughly compare a variety of\nvocabulary adaptation techniques for optimizing English LLMs for the Italian\nlanguage, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a\nnovel method that leverages neural mapping for vocabulary substitution. SAVA\nachieves competitive performance across multiple downstream tasks, enhancing\ngrounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing\ntoken fertility by 25\\%, and Llama-3.1-8B, optimizing the vocabulary and\nreducing the number of parameters by 1 billion. We show that, following the\nadaptation of the vocabulary, these models can recover their performance with a\nrelatively limited stage of continual training on the target language. Finally,\nwe test the capabilities of the adapted models on various multi-choice and\ngenerative tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAVA\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u82f1\u8bed\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u610f\u5927\u5229\u8bed\u4e2d\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u8bcd\u6c47\u66ff\u6362\u51cf\u5c11token\u6570\u91cf\u5e76\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570LLM\u4e3b\u8981\u9488\u5bf9\u82f1\u8bed\u8bbe\u8ba1\uff0c\u5bf9\u5176\u4ed6\u8bed\u8a00\u7684\u652f\u6301\u4e0d\u8db3\uff0c\u5bfc\u81f4\u7f16\u7801\u6548\u7387\u4f4e\u548c\u63a8\u7406\u901f\u5ea6\u6162\u3002", "method": "\u63d0\u51fa\u4e86SAVA\u65b9\u6cd5\uff0c\u5229\u7528\u795e\u7ecf\u6620\u5c04\u8fdb\u884c\u8bcd\u6c47\u66ff\u6362\uff0c\u4f18\u5316\u8bcd\u6c47\u8868\u5e76\u51cf\u5c11\u53c2\u6570\u3002", "result": "\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51cf\u5c11\u4e86token\u6570\u91cf\uff08\u5982Mistral-7b-v0.1\u51cf\u5c1125%\uff09\u548c\u53c2\u6570\uff08\u5982Llama-3.1-8B\u51cf\u5c1110\u4ebf\uff09\u3002", "conclusion": "SAVA\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316LLM\u5bf9\u975e\u82f1\u8bed\u8bed\u8a00\u7684\u652f\u6301\uff0c\u5e76\u901a\u8fc7\u5c11\u91cf\u6301\u7eed\u8bad\u7ec3\u6062\u590d\u6027\u80fd\u3002"}}
{"id": "2504.17728", "pdf": "https://arxiv.org/pdf/2504.17728", "abs": "https://arxiv.org/abs/2504.17728", "authors": ["Shucheng Gong", "Lingzhe Zhao", "Wenpu Li", "Hong Xie", "Yin Zhang", "Shiyu Zhao", "Peidong Liu"], "title": "CasualHDRSplat: Robust High Dynamic Range 3D Gaussian Splatting from Casually Captured Videos", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": "Source Code: https://github.com/WU-CVGL/CasualHDRSplat", "summary": "Recently, photo-realistic novel view synthesis from multi-view images, such\nas neural radiance field (NeRF) and 3D Gaussian Splatting (3DGS), have garnered\nwidespread attention due to their superior performance. However, most works\nrely on low dynamic range (LDR) images, which limits the capturing of richer\nscene details. Some prior works have focused on high dynamic range (HDR) scene\nreconstruction, typically require capturing of multi-view sharp images with\ndifferent exposure times at fixed camera positions during exposure times, which\nis time-consuming and challenging in practice. For a more flexible data\nacquisition, we propose a one-stage method: \\textbf{CasualHDRSplat} to easily\nand robustly reconstruct the 3D HDR scene from casually captured videos with\nauto-exposure enabled, even in the presence of severe motion blur and varying\nunknown exposure time. \\textbf{CasualHDRSplat} contains a unified\ndifferentiable physical imaging model which first applies continuous-time\ntrajectory constraint to imaging process so that we can jointly optimize\nexposure time, camera response function (CRF), camera poses, and sharp 3D HDR\nscene. Extensive experiments demonstrate that our approach outperforms existing\nmethods in terms of robustness and rendering quality. Our source code will be\navailable at https://github.com/WU-CVGL/CasualHDRSplat", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCasualHDRSplat\u7684\u5355\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u968f\u610f\u62cd\u6444\u7684\u89c6\u9891\u4e2d\u91cd\u5efa3D HDR\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u66dd\u5149\u548c\u591a\u89c6\u89d2\u56fe\u50cf\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709HDR\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u9700\u8981\u56fa\u5b9a\u76f8\u673a\u4f4d\u7f6e\u548c\u591a\u66dd\u5149\u56fe\u50cf\uff0c\u8017\u65f6\u4e14\u4e0d\u7075\u6d3b\u3002CasualHDRSplat\u65e8\u5728\u901a\u8fc7\u968f\u610f\u62cd\u6444\u7684\u89c6\u9891\u5b9e\u73b0\u9ad8\u6548\u3001\u9c81\u68d2\u7684HDR\u91cd\u5efa\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5fae\u5206\u7269\u7406\u6210\u50cf\u6a21\u578b\uff0c\u7ed3\u5408\u8fde\u7eed\u65f6\u95f4\u8f68\u8ff9\u7ea6\u675f\uff0c\u8054\u5408\u4f18\u5316\u66dd\u5149\u65f6\u95f4\u3001\u76f8\u673a\u54cd\u5e94\u51fd\u6570\u3001\u76f8\u673a\u4f4d\u59ff\u548c\u6e05\u66703D HDR\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCasualHDRSplat\u5728\u9c81\u68d2\u6027\u548c\u6e32\u67d3\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CasualHDRSplat\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u9ad8\u6548\u7684HDR\u573a\u666f\u91cd\u5efa\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.17132", "pdf": "https://arxiv.org/pdf/2504.17132", "abs": "https://arxiv.org/abs/2504.17132", "authors": ["Ning Li", "Antai Andy Liu", "Jingran Zhang", "Justin Cui"], "title": "Latent Video Dataset Distillation", "categories": ["cs.CV"], "comment": "https://openreview.net/forum?id=i665TIHv92", "summary": "Dataset distillation has demonstrated remarkable effectiveness in\nhigh-compression scenarios for image datasets. While video datasets inherently\ncontain greater redundancy, existing video dataset distillation methods\nprimarily focus on compression in the pixel space, overlooking advances in the\nlatent space that have been widely adopted in modern text-to-image and\ntext-to-video models. In this work, we bridge this gap by introducing a novel\nvideo dataset distillation approach that operates in the latent space using a\nstate-of-the-art variational encoder. Furthermore, we employ a diversity-aware\ndata selection strategy to select both representative and diverse samples.\nAdditionally, we introduce a simple, training-free method to further compress\nthe distilled latent dataset. By combining these techniques, our approach\nachieves a new state-of-the-art performance in dataset distillation,\noutperforming prior methods on all datasets, e.g. on HMDB51 IPC 1, we achieve a\n2.6% performance increase; on MiniUCF IPC 5, we achieve a 7.8% performance\nincrease.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u7ed3\u5408\u591a\u6837\u6027\u611f\u77e5\u6570\u636e\u9009\u62e9\u548c\u8bad\u7ec3\u514d\u8d39\u538b\u7f29\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u5728\u50cf\u7d20\u7a7a\u95f4\u538b\u7f29\uff0c\u5ffd\u7565\u4e86\u6f5c\u5728\u7a7a\u95f4\u7684\u8fdb\u5c55\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u7f16\u7801\u5668\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u84b8\u998f\uff0c\u91c7\u7528\u591a\u6837\u6027\u611f\u77e5\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u8bad\u7ec3\u514d\u8d39\u7684\u538b\u7f29\u65b9\u6cd5\u3002", "result": "\u5728HMDB51 IPC 1\u4e0a\u6027\u80fd\u63d0\u53472.6%\uff0c\u5728MiniUCF IPC 5\u4e0a\u63d0\u53477.8%\uff0c\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u548c\u591a\u6837\u6027\u9009\u62e9\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u6570\u636e\u96c6\u84b8\u998f\u7684\u6027\u80fd\u3002"}}
{"id": "2504.17052", "pdf": "https://arxiv.org/pdf/2504.17052", "abs": "https://arxiv.org/abs/2504.17052", "authors": ["Shariar Kabir", "Kevin Esterling", "Yue Dong"], "title": "Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models", "categories": ["cs.CL"], "comment": "20 pages, 9 figures", "summary": "Large Language Models (LLMs) are increasingly shaping political discourse,\nyet their responses often display inconsistency when subjected to scrutiny.\nWhile prior research has primarily categorized LLM outputs as left- or\nright-leaning to assess their political stances, a critical question remains:\nDo these responses reflect genuine internal beliefs or merely surface-level\nalignment with training data? To address this, we propose a novel framework for\nevaluating belief depth by analyzing (1) argumentative consistency and (2)\nuncertainty quantification. We evaluate 12 LLMs on 19 economic policies from\nthe Political Compass Test, challenging their belief stability with both\nsupportive and opposing arguments. Our analysis reveals that LLMs exhibit\ntopic-specific belief stability rather than a uniform ideological stance.\nNotably, up to 95% of left-leaning models' responses and 89% of right-leaning\nmodels' responses remain consistent under the challenge, enabling semantic\nentropy to achieve high accuracy (AUROC=0.78), effectively distinguishing\nbetween surface-level alignment from genuine belief. These findings call into\nquestion the assumption that LLMs maintain stable, human-like political\nideologies, emphasizing the importance of conducting topic-specific reliability\nassessments for real-world applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u653f\u6cbb\u7acb\u573a\u6df1\u5ea6\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u5176\u54cd\u5e94\u66f4\u591a\u662f\u4e3b\u9898\u7279\u5b9a\u7684\u7a33\u5b9a\u6027\uff0c\u800c\u975e\u7edf\u4e00\u7684\u610f\u8bc6\u5f62\u6001\u7acb\u573a\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7a76LLM\u7684\u653f\u6cbb\u54cd\u5e94\u662f\u771f\u5b9e\u4fe1\u5ff5\u8fd8\u662f\u8868\u9762\u4e0e\u8bad\u7ec3\u6570\u636e\u5bf9\u9f50\u3002", "method": "\u901a\u8fc7\u5206\u6790\uff081\uff09\u8bba\u8bc1\u4e00\u81f4\u6027\u548c\uff082\uff09\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u8bc4\u4f3012\u4e2aLLM\u572819\u9879\u7ecf\u6d4e\u653f\u7b56\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5de6\u503e\u548c\u53f3\u503e\u6a21\u578b\u7684\u54cd\u5e94\u5728\u6311\u6218\u4e0b\u5206\u522b\u670995%\u548c89%\u7684\u4e00\u81f4\u6027\uff0c\u8bed\u4e49\u71b5\u80fd\u6709\u6548\u533a\u5206\u8868\u9762\u5bf9\u9f50\u4e0e\u771f\u5b9e\u4fe1\u5ff5\uff08AUROC=0.78\uff09\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51faLLM\u672a\u5fc5\u5177\u6709\u7a33\u5b9a\u7684\u4eba\u7c7b\u610f\u8bc6\u5f62\u6001\uff0c\u5f3a\u8c03\u9700\u8fdb\u884c\u4e3b\u9898\u7279\u5b9a\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u3002"}}
{"id": "2504.17162", "pdf": "https://arxiv.org/pdf/2504.17162", "abs": "https://arxiv.org/abs/2504.17162", "authors": ["Cece Zhang", "Xuehuan Zhu", "Nick Peterson", "Jieqiong Wang", "Shibiao Wan"], "title": "A Comprehensive Review on RNA Subcellular Localization Prediction", "categories": ["cs.CV", "cs.AI", "q-bio.GN", "q-bio.SC"], "comment": null, "summary": "The subcellular localization of RNAs, including long non-coding RNAs\n(lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs,\nplays a critical role in determining their biological functions. For instance,\nlncRNAs are predominantly associated with chromatin and act as regulators of\ngene transcription and chromatin structure, while mRNAs are distributed across\nthe nucleus and cytoplasm, facilitating the transport of genetic information\nfor protein synthesis. Understanding RNA localization sheds light on processes\nlike gene expression regulation with spatial and temporal precision. However,\ntraditional wet lab methods for determining RNA localization, such as in situ\nhybridization, are often time-consuming, resource-demanding, and costly. To\novercome these challenges, computational methods leveraging artificial\nintelligence (AI) and machine learning (ML) have emerged as powerful\nalternatives, enabling large-scale prediction of RNA subcellular localization.\nThis paper provides a comprehensive review of the latest advancements in\nAI-based approaches for RNA subcellular localization prediction, covering\nvarious RNA types and focusing on sequence-based, image-based, and hybrid\nmethodologies that combine both data types. We highlight the potential of these\nmethods to accelerate RNA research, uncover molecular pathways, and guide\ntargeted disease treatments. Furthermore, we critically discuss the challenges\nin AI/ML approaches for RNA subcellular localization, such as data scarcity and\nlack of benchmarks, and opportunities to address them. This review aims to\nserve as a valuable resource for researchers seeking to develop innovative\nsolutions in the field of RNA subcellular localization and beyond.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8eAI/ML\u7684RNA\u4e9a\u7ec6\u80de\u5b9a\u4f4d\u9884\u6d4b\u65b9\u6cd5\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u591a\u79cdRNA\u7c7b\u578b\u53ca\u4e0d\u540c\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u6311\u6218\u4e0e\u673a\u9047\u3002", "motivation": "\u4f20\u7edf\u6e7f\u5b9e\u9a8c\u5ba4\u65b9\u6cd5\u8017\u65f6\u8017\u529b\uff0cAI/ML\u65b9\u6cd5\u4e3aRNA\u4e9a\u7ec6\u80de\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5e8f\u5217\u3001\u56fe\u50cf\u53ca\u6df7\u5408\u65b9\u6cd5\u7684AI/ML\u6280\u672f\u3002", "result": "AI/ML\u65b9\u6cd5\u80fd\u52a0\u901fRNA\u7814\u7a76\uff0c\u63ed\u793a\u5206\u5b50\u901a\u8def\uff0c\u6307\u5bfc\u75be\u75c5\u6cbb\u7597\u3002", "conclusion": "\u672c\u6587\u4e3aRNA\u4e9a\u7ec6\u80de\u5b9a\u4f4d\u9886\u57df\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2504.17075", "pdf": "https://arxiv.org/pdf/2504.17075", "abs": "https://arxiv.org/abs/2504.17075", "authors": ["Arjun Subramonian", "Vagrant Gautam", "Preethi Seshadri", "Dietrich Klakow", "Kai-Wei Chang", "Yizhou Sun"], "title": "Agree to Disagree? A Meta-Evaluation of LLM Misgendering", "categories": ["cs.CL", "cs.CY"], "comment": "Work in progress", "summary": "Numerous methods have been proposed to measure LLM misgendering, including\nprobability-based evaluations (e.g., automatically with templatic sentences)\nand generation-based evaluations (e.g., with automatic heuristics or human\nvalidation). However, it has gone unexamined whether these evaluation methods\nhave convergent validity, that is, whether their results align. Therefore, we\nconduct a systematic meta-evaluation of these methods across three existing\ndatasets for LLM misgendering. We propose a method to transform each dataset to\nenable parallel probability- and generation-based evaluation. Then, by\nautomatically evaluating a suite of 6 models from 3 families, we find that\nthese methods can disagree with each other at the instance, dataset, and model\nlevels, conflicting on 20.2% of evaluation instances. Finally, with a human\nevaluation of 2400 LLM generations, we show that misgendering behaviour is\ncomplex and goes far beyond pronouns, which automatic evaluations are not\ncurrently designed to capture, suggesting essential disagreement with human\nevaluations. Based on our findings, we provide recommendations for future\nevaluations of LLM misgendering. Our results are also more widely relevant, as\nthey call into question broader methodological conventions in LLM evaluation,\nwhich often assume that different evaluation methods agree.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86LLM\u6027\u522b\u9519\u8bef\u8bc4\u4f30\u65b9\u6cd5\u7684\u6536\u655b\u6548\u5ea6\uff0c\u53d1\u73b0\u4e0d\u540c\u65b9\u6cd5\u5728\u5b9e\u4f8b\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u5c42\u9762\u4e0a\u5b58\u5728\u4e0d\u4e00\u81f4\uff0c\u4e14\u81ea\u52a8\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8\u73b0\u6709LLM\u6027\u522b\u9519\u8bef\u8bc4\u4f30\u65b9\u6cd5\u662f\u5426\u5177\u6709\u6536\u655b\u6548\u5ea6\uff0c\u5373\u4e0d\u540c\u65b9\u6cd5\u7684\u7ed3\u679c\u662f\u5426\u4e00\u81f4\u3002", "method": "\u5bf9\u4e09\u4e2a\u73b0\u6709\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u5143\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\u4ee5\u652f\u6301\u5e76\u884c\u6982\u7387\u548c\u751f\u6210\u8bc4\u4f30\uff0c\u5e76\u81ea\u52a8\u8bc4\u4f306\u4e2a\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u8bc4\u4f30\u65b9\u6cd5\u4e4b\u95f4\u5b58\u572820.2%\u7684\u5b9e\u4f8b\u51b2\u7a81\uff0c\u81ea\u52a8\u8bc4\u4f30\u672a\u80fd\u6355\u6349\u6027\u522b\u9519\u8bef\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u5efa\u8bae\u672a\u6765\u8bc4\u4f30\u9700\u6539\u8fdb\u65b9\u6cd5\uff0c\u5e76\u8d28\u7591LLM\u8bc4\u4f30\u4e2d\u5e7f\u6cdb\u5047\u8bbe\u4e0d\u540c\u65b9\u6cd5\u4e00\u81f4\u6027\u7684\u60ef\u4f8b\u3002"}}
{"id": "2504.17163", "pdf": "https://arxiv.org/pdf/2504.17163", "abs": "https://arxiv.org/abs/2504.17163", "authors": ["Kai Cui", "Jia Li", "Yu Liu", "Xuesong Zhang", "Zhenzhen Hu", "Meng Wang"], "title": "PhysioSync: Temporal and Cross-Modal Contrastive Learning Inspired by Physiological Synchronization for EEG-Based Emotion Recognition", "categories": ["cs.CV"], "comment": "The source code will be publicly available at\n  https://github.com/MSA-LMC/PhysioSync", "summary": "Electroencephalography (EEG) signals provide a promising and involuntary\nreflection of brain activity related to emotional states, offering significant\nadvantages over behavioral cues like facial expressions. However, EEG signals\nare often noisy, affected by artifacts, and vary across individuals,\ncomplicating emotion recognition. While multimodal approaches have used\nPeripheral Physiological Signals (PPS) like GSR to complement EEG, they often\noverlook the dynamic synchronization and consistent semantics between the\nmodalities. Additionally, the temporal dynamics of emotional fluctuations\nacross different time resolutions in PPS remain underexplored. To address these\nchallenges, we propose PhysioSync, a novel pre-training framework leveraging\ntemporal and cross-modal contrastive learning, inspired by physiological\nsynchronization phenomena. PhysioSync incorporates Cross-Modal Consistency\nAlignment (CM-CA) to model dynamic relationships between EEG and complementary\nPPS, enabling emotion-related synchronizations across modalities. Besides, it\nintroduces Long- and Short-Term Temporal Contrastive Learning (LS-TCL) to\ncapture emotional synchronization at different temporal resolutions within\nmodalities. After pre-training, cross-resolution and cross-modal features are\nhierarchically fused and fine-tuned to enhance emotion recognition. Experiments\non DEAP and DREAMER datasets demonstrate PhysioSync's advanced performance\nunder uni-modal and cross-modal conditions, highlighting its effectiveness for\nEEG-centered emotion recognition.", "AI": {"tldr": "PhysioSync\u662f\u4e00\u4e2a\u65b0\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u65f6\u95f4\u548c\u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\uff0c\u901a\u8fc7EEG\u548cPPS\u7684\u52a8\u6001\u540c\u6b65\u63d0\u5347\u60c5\u7eea\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "EEG\u4fe1\u53f7\u867d\u7136\u80fd\u53cd\u6620\u60c5\u7eea\u72b6\u6001\uff0c\u4f46\u5b58\u5728\u566a\u58f0\u548c\u4e2a\u4f53\u5dee\u5f02\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u5ffd\u7565\u4e86\u6a21\u6001\u95f4\u7684\u52a8\u6001\u540c\u6b65\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faPhysioSync\u6846\u67b6\uff0c\u7ed3\u5408\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5bf9\u9f50\uff08CM-CA\uff09\u548c\u957f\u77ed\u65f6\u65f6\u95f4\u5bf9\u6bd4\u5b66\u4e60\uff08LS-TCL\uff09\uff0c\u6355\u6349\u6a21\u6001\u95f4\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u4e0a\u7684\u60c5\u7eea\u540c\u6b65\u3002", "result": "\u5728DEAP\u548cDREAMER\u6570\u636e\u96c6\u4e0a\uff0cPhysioSync\u5728\u5355\u6a21\u6001\u548c\u8de8\u6a21\u6001\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "PhysioSync\u901a\u8fc7\u52a8\u6001\u540c\u6b65\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86EEG\u4e3a\u4e2d\u5fc3\u7684\u60c5\u7eea\u8bc6\u522b\u6548\u679c\u3002"}}
{"id": "2504.17083", "pdf": "https://arxiv.org/pdf/2504.17083", "abs": "https://arxiv.org/abs/2504.17083", "authors": ["Rendi Chevi", "Kentaro Inui", "Thamar Solorio", "Alham Fikri Aji"], "title": "How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study", "categories": ["cs.CL"], "comment": "Accepted at GenAICHI 2025 @ ACM CHI 2025", "summary": "What makes an interaction with the LLM more preferable for the user? While it\nis intuitive to assume that information accuracy in the LLM's responses would\nbe one of the influential variables, recent studies have found that inaccurate\nLLM's responses could still be preferable when they are perceived to be more\nauthoritative, certain, well-articulated, or simply verbose. These variables\ninterestingly fall under the broader category of language style, implying that\nthe style in the LLM's responses might meaningfully influence users'\npreferences. This hypothesized dynamic could have double-edged consequences:\nenhancing the overall user experience while simultaneously increasing their\nsusceptibility to risks such as LLM's misinformation or hallucinations. In this\nshort paper, we present our preliminary studies in exploring this subject.\nThrough a series of exploratory and experimental user studies, we found that\nLLM's language style does indeed influence user's preferences, but how and\nwhich language styles influence the preference varied across different user\npopulations, and more interestingly, moderated by the user's very own\nindividual traits. As a preliminary work, the findings in our studies should be\ninterpreted with caution, particularly given the limitations in our samples,\nwhich still need wider demographic diversity and larger sample sizes. Our\nfuture directions will first aim to address these limitations, which would\nenable a more comprehensive joint effect analysis between the language style,\nindividual traits, and preferences, and further investigate the potential\ncausal relationship between and beyond these variables.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cLLM\u7684\u8bed\u8a00\u98ce\u683c\uff08\u5982\u6743\u5a01\u6027\u3001\u786e\u5b9a\u6027\u3001\u8868\u8fbe\u6e05\u6670\u5ea6\u7b49\uff09\u663e\u8457\u5f71\u54cd\u7528\u6237\u504f\u597d\uff0c\u4f46\u5177\u4f53\u5f71\u54cd\u56e0\u7528\u6237\u7fa4\u4f53\u548c\u4e2a\u4f53\u7279\u8d28\u800c\u5f02\u3002\u9700\u6ce8\u610f\u6837\u672c\u5c40\u9650\uff0c\u672a\u6765\u5c06\u6269\u5927\u7814\u7a76\u8303\u56f4\u548c\u591a\u6837\u6027\u3002", "motivation": "\u63a2\u8ba8LLM\u7684\u8bed\u8a00\u98ce\u683c\u5982\u4f55\u5f71\u54cd\u7528\u6237\u504f\u597d\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5f71\u54cd\u7684\u6f5c\u5728\u53cc\u5203\u5251\u6548\u5e94\uff08\u63d0\u5347\u4f53\u9a8c\u4e0e\u589e\u52a0\u98ce\u9669\uff09\u3002", "method": "\u901a\u8fc7\u63a2\u7d22\u6027\u548c\u5b9e\u9a8c\u6027\u7528\u6237\u7814\u7a76\uff0c\u5206\u6790\u4e0d\u540c\u8bed\u8a00\u98ce\u683c\u5bf9\u7528\u6237\u504f\u597d\u7684\u5f71\u54cd\u3002", "result": "\u8bed\u8a00\u98ce\u683c\u786e\u5b9e\u5f71\u54cd\u7528\u6237\u504f\u597d\uff0c\u4f46\u5177\u4f53\u5f71\u54cd\u56e0\u7528\u6237\u7fa4\u4f53\u548c\u4e2a\u4f53\u7279\u8d28\u800c\u5f02\u3002\u6837\u672c\u5c40\u9650\u9700\u8c28\u614e\u89e3\u8bfb\u3002", "conclusion": "\u521d\u6b65\u7814\u7a76\u8868\u660e\u8bed\u8a00\u98ce\u683c\u5bf9\u7528\u6237\u504f\u597d\u6709\u5f71\u54cd\uff0c\u672a\u6765\u9700\u6269\u5927\u6837\u672c\u5e76\u6df1\u5165\u5206\u6790\u53d8\u91cf\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002"}}
{"id": "2504.17177", "pdf": "https://arxiv.org/pdf/2504.17177", "abs": "https://arxiv.org/abs/2504.17177", "authors": ["Kevin Lane", "Morteza Karimzadeh"], "title": "A Genealogy of Multi-Sensor Foundation Models in Remote Sensing", "categories": ["cs.CV", "cs.LG", "I.4.7; I.4.8"], "comment": "20 pages, submitted to ACM SigSpatial, currently under peer review", "summary": "Foundation models have garnered increasing attention for representation\nlearning in remote sensing, primarily adopting approaches that have\ndemonstrated success in computer vision with minimal domain-specific\nmodification. However, the development and application of foundation models in\nthis field are still burgeoning, as there are a variety of competing approaches\nthat each come with significant benefits and drawbacks. This paper examines\nthese approaches along with their roots in the computer vision field in order\nto characterize potential advantages and pitfalls while outlining future\ndirections to further improve remote sensing-specific foundation models. We\ndiscuss the quality of the learned representations and methods to alleviate the\nneed for massive compute resources. We place emphasis on the multi-sensor\naspect of Earth observations, and the extent to which existing approaches\nleverage multiple sensors in training foundation models in relation to\nmulti-modal foundation models. Finally, we identify opportunities for further\nharnessing the vast amounts of unlabeled, seasonal, and multi-sensor remote\nsensing observations.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u9065\u611f\u9886\u57df\u4e2d\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u73b0\u72b6\uff0c\u63a2\u8ba8\u4e86\u5176\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u7684\u5f02\u540c\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u5206\u6790\u9065\u611f\u9886\u57df\u4e2d\u57fa\u7840\u6a21\u578b\u7684\u73b0\u6709\u65b9\u6cd5\u53ca\u5176\u4f18\u7f3a\u70b9\uff0c\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6bd4\u8f83\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u5728\u9065\u611f\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63a2\u8ba8\u5982\u4f55\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u591a\u4f20\u611f\u5668\u6570\u636e\u7684\u5229\u7528\u548c\u5b63\u8282\u6027\u6570\u636e\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u5f00\u53d1\u3002", "conclusion": "\u7ed3\u8bba\u6307\u51fa\uff0c\u672a\u6765\u5e94\u66f4\u5145\u5206\u5229\u7528\u672a\u6807\u8bb0\u548c\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u4ee5\u4f18\u5316\u9065\u611f\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2504.17091", "pdf": "https://arxiv.org/pdf/2504.17091", "abs": "https://arxiv.org/abs/2504.17091", "authors": ["Seunghyun Yoo"], "title": "Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning", "categories": ["cs.CL", "68T05"], "comment": "5 page", "summary": "Due to the proliferation of short-form content and the rapid adoption of AI,\nopportunities for deep, reflective thinking have significantly diminished,\nundermining users' critical thinking and reducing engagement with the reasoning\nbehind AI-generated outputs. To address this issue, we propose an Interactive\nChain-of-Thought (CoT) Framework that enhances human-centered explainability\nand responsible AI usage by making the model's inference process transparent,\nmodular, and user-editable. The framework decomposes reasoning into clearly\ndefined blocks that users can inspect, modify, and re-execute, encouraging\nactive cognitive engagement rather than passive consumption. It further\nintegrates a lightweight edit-adaptation mechanism inspired by preference\nlearning, allowing the system to align with diverse cognitive styles and user\nintentions. Ethical transparency is ensured through explicit metadata\ndisclosure, built-in bias checkpoint functionality, and privacy-preserving\nsafeguards. This work outlines the design principles and architecture necessary\nto promote critical engagement, responsible interaction, and inclusive\nadaptation in AI systems aimed at addressing complex societal challenges.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u4e92\u5f0f\u601d\u7ef4\u94fe\u6846\u67b6\uff0c\u901a\u8fc7\u900f\u660e\u5316\u3001\u6a21\u5757\u5316\u548c\u7528\u6237\u53ef\u7f16\u8f91\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u5347AI\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8d1f\u8d23\u4efb\u4f7f\u7528\u3002", "motivation": "\u77ed\u5185\u5bb9\u6cdb\u6ee5\u548cAI\u5feb\u901f\u666e\u53ca\u5bfc\u81f4\u6df1\u5ea6\u601d\u8003\u673a\u4f1a\u51cf\u5c11\uff0c\u524a\u5f31\u7528\u6237\u6279\u5224\u6027\u601d\u7ef4\u548c\u5bf9AI\u8f93\u51fa\u7684\u7406\u89e3\u3002", "method": "\u8bbe\u8ba1\u4ea4\u4e92\u5f0f\u601d\u7ef4\u94fe\u6846\u67b6\uff0c\u5206\u89e3\u63a8\u7406\u4e3a\u53ef\u68c0\u67e5\u3001\u4fee\u6539\u548c\u91cd\u65b0\u6267\u884c\u7684\u6a21\u5757\uff0c\u5e76\u96c6\u6210\u8f7b\u91cf\u7ea7\u7f16\u8f91\u9002\u5e94\u673a\u5236\u3002", "result": "\u6846\u67b6\u589e\u5f3a\u4e86\u7528\u6237\u8ba4\u77e5\u53c2\u4e0e\uff0c\u652f\u6301\u591a\u6837\u5316\u8ba4\u77e5\u98ce\u683c\u548c\u610f\u56fe\uff0c\u540c\u65f6\u786e\u4fdd\u4f26\u7406\u900f\u660e\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4fc3\u8fdbAI\u7cfb\u7edf\u4e2d\u7684\u6279\u5224\u6027\u53c2\u4e0e\u3001\u8d1f\u8d23\u4efb\u4e92\u52a8\u548c\u5305\u5bb9\u6027\u9002\u5e94\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u539f\u5219\u548c\u67b6\u6784\u3002"}}
{"id": "2504.17180", "pdf": "https://arxiv.org/pdf/2504.17180", "abs": "https://arxiv.org/abs/2504.17180", "authors": ["Minkyu Choi", "S P Sharan", "Harsh Goel", "Sahil Shah", "Sandeep Chinchali"], "title": "We'll Fix it in Post: Improving Text-to-Video Generation with Neuro-Symbolic Feedback", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Current text-to-video (T2V) generation models are increasingly popular due to\ntheir ability to produce coherent videos from textual prompts. However, these\nmodels often struggle to generate semantically and temporally consistent videos\nwhen dealing with longer, more complex prompts involving multiple objects or\nsequential events. Additionally, the high computational costs associated with\ntraining or fine-tuning make direct improvements impractical. To overcome these\nlimitations, we introduce \\(\\projectname\\), a novel zero-training video\nrefinement pipeline that leverages neuro-symbolic feedback to automatically\nenhance video generation, achieving superior alignment with the prompts. Our\napproach first derives the neuro-symbolic feedback by analyzing a formal video\nrepresentation and pinpoints semantically inconsistent events, objects, and\ntheir corresponding frames. This feedback then guides targeted edits to the\noriginal video. Extensive empirical evaluations on both open-source and\nproprietary T2V models demonstrate that \\(\\projectname\\) significantly enhances\ntemporal and logical alignment across diverse prompts by almost $40\\%$.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u89c6\u9891\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u53cd\u9988\u63d0\u5347\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u8bed\u4e49\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u63d0\u793a\u65f6\u96be\u4ee5\u4fdd\u8bc1\u8bed\u4e49\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u53cd\u9988\u9a71\u52a8\u7684\u89c6\u9891\u4f18\u5316\u6d41\u7a0b\uff0c\u901a\u8fc7\u5206\u6790\u89c6\u9891\u8868\u793a\u5e76\u5b9a\u4f4d\u4e0d\u4e00\u81f4\u4e8b\u4ef6\u548c\u5bf9\u8c61\uff0c\u6307\u5bfc\u9488\u5bf9\u6027\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u4e0e\u903b\u8f91\u5bf9\u9f50\u6548\u679c\uff0c\u6539\u8fdb\u5e45\u5ea6\u8fbe40%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aT2V\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2504.17119", "pdf": "https://arxiv.org/pdf/2504.17119", "abs": "https://arxiv.org/abs/2504.17119", "authors": ["Muskan Garg", "Shaina Raza", "Shebuti Rayana", "Xingyi Liu", "Sunghwan Sohn"], "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "categories": ["cs.CL", "cs.AI"], "comment": "35 pages, 7 tables, 5 figures", "summary": "Despite substantial progress in healthcare applications driven by large\nlanguage models (LLMs), growing concerns around data privacy, and limited\nresources; the small language models (SLMs) offer a scalable and clinically\nviable solution for efficient performance in resource-constrained environments\nfor next-generation healthcare informatics. Our comprehensive survey presents a\ntaxonomic framework to identify and categorize them for healthcare\nprofessionals and informaticians. The timeline of healthcare SLM contributions\nestablishes a foundational framework for analyzing models across three\ndimensions: NLP tasks, stakeholder roles, and the continuum of care. We present\na taxonomic framework to identify the architectural foundations for building\nmodels from scratch; adapting SLMs to clinical precision through prompting,\ninstruction fine-tuning, and reasoning; and accessibility and sustainability\nthrough compression techniques. Our primary objective is to offer a\ncomprehensive survey for healthcare professionals, introducing recent\ninnovations in model optimization and equipping them with curated resources to\nsupport future research and development in the field. Aiming to showcase the\ngroundbreaking advancements in SLMs for healthcare, we present a comprehensive\ncompilation of experimental results across widely studied NLP tasks in\nhealthcare to highlight the transformative potential of SLMs in healthcare. The\nupdated repository is available at Github", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6846\u67b6\uff0c\u5206\u6790\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u4f18\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e86\u6a21\u578b\u4f18\u5316\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u7597\u4fdd\u5065\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u6570\u636e\u9690\u79c1\u548c\u8d44\u6e90\u9650\u5236\u95ee\u9898\u4fc3\u4f7f\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u6210\u4e3a\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u4e34\u5e8a\u53ef\u884c\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u6846\u67b6\u5206\u6790SLMs\u5728\u4e09\u4e2a\u7ef4\u5ea6\uff08NLP\u4efb\u52a1\u3001\u5229\u76ca\u76f8\u5173\u8005\u89d2\u8272\u548c\u62a4\u7406\u8fde\u7eed\u6027\uff09\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8\u4e86\u6a21\u578b\u6784\u5efa\u3001\u4f18\u5316\u548c\u538b\u7f29\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSLMs\u5728\u533b\u7597\u4fdd\u5065NLP\u4efb\u52a1\u4e2d\u5177\u6709\u53d8\u9769\u6027\u6f5c\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u65b0\u7684\u8d44\u6e90\u5e93\u3002", "conclusion": "SLMs\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u533b\u7597\u4fdd\u5065\u73af\u5883\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u7814\u7a76\u548c\u5f00\u53d1\u53ef\u57fa\u4e8e\u6b64\u6846\u67b6\u8fdb\u4e00\u6b65\u63a8\u8fdb\u3002"}}
{"id": "2504.17207", "pdf": "https://arxiv.org/pdf/2504.17207", "abs": "https://arxiv.org/abs/2504.17207", "authors": ["Phillip Y. Lee", "Jihyeon Je", "Chanho Park", "Mikaela Angelina Uy", "Leonidas Guibas", "Minhyuk Sung"], "title": "Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation", "categories": ["cs.CV"], "comment": "Project Page: https://apc-vlm.github.io/", "summary": "We present a framework for perspective-aware reasoning in vision-language\nmodels (VLMs) through mental imagery simulation. Perspective-taking, the\nability to perceive an environment or situation from an alternative viewpoint,\nis a key benchmark for human-level visual understanding, essential for\nenvironmental interaction and collaboration with autonomous agents. Despite\nadvancements in spatial reasoning within VLMs, recent research has shown that\nmodern VLMs significantly lack perspective-aware reasoning capabilities and\nexhibit a strong bias toward egocentric interpretations. To bridge the gap\nbetween VLMs and human perception, we focus on the role of mental imagery,\nwhere humans perceive the world through abstracted representations that\nfacilitate perspective shifts. Motivated by this, we propose a framework for\nperspective-aware reasoning, named Abstract Perspective Change (APC), that\neffectively leverages vision foundation models, such as object detection,\nsegmentation, and orientation estimation, to construct scene abstractions and\nenable perspective transformations. Our experiments on synthetic and real-image\nbenchmarks, compared with various VLMs, demonstrate significant improvements in\nperspective-aware reasoning with our framework, further outperforming\nfine-tuned spatial reasoning models and novel-view-synthesis-based approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fc3\u7406\u610f\u8c61\u6a21\u62df\u7684\u89c6\u89d2\u611f\u77e5\u63a8\u7406\u6846\u67b6\uff08APC\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u89c6\u89d2\u8f6c\u6362\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3VLM\u5728\u89c6\u89d2\u611f\u77e5\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u4e14\u504f\u5411\u81ea\u6211\u4e2d\u5fc3\u89e3\u91ca\uff0c\u9700\u8981\u5f25\u8865\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u62bd\u8c61\u89c6\u89d2\u53d8\u6362\uff08APC\uff09\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u3001\u5206\u5272\u548c\u65b9\u5411\u4f30\u8ba1\uff09\u6784\u5efa\u573a\u666f\u62bd\u8c61\u5e76\u5b9e\u73b0\u89c6\u89d2\u8f6c\u6362\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAPC\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89d2\u611f\u77e5\u63a8\u7406\u80fd\u529b\uff0c\u4f18\u4e8e\u5fae\u8c03\u7684\u7a7a\u95f4\u63a8\u7406\u6a21\u578b\u548c\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u3002", "conclusion": "APC\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86VLM\u7684\u89c6\u89d2\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u73af\u5883\u4ea4\u4e92\u548c\u81ea\u4e3b\u4ee3\u7406\u534f\u4f5c\u63d0\u4f9b\u4e86\u66f4\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u7684\u89c6\u89c9\u7406\u89e3\u3002"}}
{"id": "2504.17130", "pdf": "https://arxiv.org/pdf/2504.17130", "abs": "https://arxiv.org/abs/2504.17130", "authors": ["Hannah Cyberey", "David Evans"], "title": "Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control", "categories": ["cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) have transformed the way we access information.\nThese models are often tuned to refuse to comply with requests that are\nconsidered harmful and to produce responses that better align with the\npreferences of those who control the models. To understand how this\n\"censorship\" works. We use representation engineering techniques to study\nopen-weights safety-tuned models. We present a method for finding a\nrefusal--compliance vector that detects and controls the level of censorship in\nmodel outputs. We also analyze recent reasoning LLMs, distilled from\nDeepSeek-R1, and uncover an additional dimension of censorship through \"thought\nsuppression\". We show a similar approach can be used to find a vector that\nsuppresses the model's reasoning process, allowing us to remove censorship by\napplying the negative multiples of this vector", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5ba1\u67e5\u673a\u5236\uff0c\u901a\u8fc7\u8868\u793a\u5de5\u7a0b\u6280\u672f\u627e\u5230\u63a7\u5236\u6a21\u578b\u8f93\u51fa\u5ba1\u67e5\u7684\u5411\u91cf\uff0c\u5e76\u63ed\u793a\u4e86\u201c\u601d\u7ef4\u6291\u5236\u201d\u8fd9\u4e00\u989d\u5916\u7ef4\u5ea6\u3002", "motivation": "\u7406\u89e3LLMs\u5982\u4f55\u901a\u8fc7\u5ba1\u67e5\u673a\u5236\u62d2\u7edd\u6709\u5bb3\u8bf7\u6c42\uff0c\u5e76\u7814\u7a76\u5982\u4f55\u68c0\u6d4b\u548c\u63a7\u5236\u8fd9\u79cd\u5ba1\u67e5\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u8868\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u627e\u5230\u62d2\u7edd-\u670d\u4ece\u5411\u91cf\u4ee5\u68c0\u6d4b\u548c\u63a7\u5236\u6a21\u578b\u8f93\u51fa\u7684\u5ba1\u67e5\u6c34\u5e73\uff0c\u5e76\u5206\u6790\u63a8\u7406LLMs\u4e2d\u7684\u201c\u601d\u7ef4\u6291\u5236\u201d\u73b0\u8c61\u3002", "result": "\u53d1\u73b0\u4e86\u4e00\u79cd\u53ef\u4ee5\u63a7\u5236\u6a21\u578b\u5ba1\u67e5\u6c34\u5e73\u7684\u5411\u91cf\uff0c\u5e76\u63ed\u793a\u4e86\u201c\u601d\u7ef4\u6291\u5236\u201d\u4f5c\u4e3a\u5ba1\u67e5\u7684\u989d\u5916\u7ef4\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u8868\u793a\u5de5\u7a0b\u6280\u672f\u53ef\u4ee5\u6709\u6548\u68c0\u6d4b\u548c\u63a7\u5236LLMs\u7684\u5ba1\u67e5\u884c\u4e3a\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u6027\u548c\u900f\u660e\u5ea6\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2504.17213", "pdf": "https://arxiv.org/pdf/2504.17213", "abs": "https://arxiv.org/abs/2504.17213", "authors": ["Shiwen Cao", "Zhaoxing Zhang", "Junming Jiao", "Juyi Qiao", "Guowen Song", "Rong Shen"], "title": "MCAF: Efficient Agent-based Video Understanding Framework through Multimodal Coarse-to-Fine Attention Focusing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Even in the era of rapid advances in large models, video understanding,\nparticularly long videos, remains highly challenging. Compared with textual or\nimage-based information, videos commonly contain more information with\nredundancy, requiring large models to strategically allocate attention at a\nglobal level for accurate comprehension. To address this, we propose MCAF, an\nagent-based, training-free framework perform video understanding through\nMultimodal Coarse-to-fine Attention Focusing. The key innovation lies in its\nability to sense and prioritize segments of the video that are highly relevant\nto the understanding task. First, MCAF hierarchically concentrates on highly\nrelevant frames through multimodal information, enhancing the correlation\nbetween the acquired contextual information and the query. Second, it employs a\ndilated temporal expansion mechanism to mitigate the risk of missing crucial\ndetails when extracting information from these concentrated frames. In\naddition, our framework incorporates a self-reflection mechanism utilizing the\nconfidence level of the model's responses as feedback. By iteratively applying\nthese two creative focusing strategies, it adaptively adjusts attention to\ncapture highly query-connected context and thus improves response accuracy.\nMCAF outperforms comparable state-of-the-art methods on average. On the\nEgoSchema dataset, it achieves a remarkable 5% performance gain over the\nleading approach. Meanwhile, on Next-QA and IntentQA datasets, it outperforms\nthe current state-of-the-art standard by 0.2% and 0.3% respectively. On the\nVideo-MME dataset, which features videos averaging nearly an hour in length,\nMCAF also outperforms other agent-based methods.", "AI": {"tldr": "MCAF\u662f\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u7684\u65e0\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7c97\u5230\u7ec6\u6ce8\u610f\u529b\u805a\u7126\u5b9e\u73b0\u89c6\u9891\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u7406\u89e3\uff0c\u5c24\u5176\u662f\u957f\u89c6\u9891\uff0c\u4fe1\u606f\u5197\u4f59\u4e14\u590d\u6742\uff0c\u9700\u8981\u5168\u5c40\u6ce8\u610f\u529b\u5206\u914d\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "method": "MCAF\u901a\u8fc7\u591a\u6a21\u6001\u4fe1\u606f\u5206\u5c42\u805a\u7126\u76f8\u5173\u5e27\uff0c\u5e76\u91c7\u7528\u6269\u5f20\u65f6\u95f4\u6269\u5c55\u673a\u5236\u907f\u514d\u9057\u6f0f\u7ec6\u8282\uff0c\u7ed3\u5408\u81ea\u53cd\u9988\u673a\u5236\u8fed\u4ee3\u4f18\u5316\u6ce8\u610f\u529b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5982EgoSchema\u63d0\u53475%\uff0cNext-QA\u548cIntentQA\u5206\u522b\u63d0\u53470.2%\u548c0.3%\uff0c\u5728\u957f\u89c6\u9891\u6570\u636e\u96c6Video-MME\u4e0a\u4e5f\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "MCAF\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u805a\u7126\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2504.17137", "pdf": "https://arxiv.org/pdf/2504.17137", "abs": "https://arxiv.org/abs/2504.17137", "authors": ["Chanhee Park", "Hyeonseok Moon", "Chanjun Park", "Heuiseok Lim"], "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to NAACL2025 Findings", "summary": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective\nmethod for enhancing the generative capabilities of Large Language Models\n(LLMs) through the incorporation of external knowledge. However, the evaluation\nof RAG systems remains a challenge, due to the intricate interplay between\nretrieval and generation components. This limitation has resulted in a scarcity\nof benchmarks that facilitate a detailed, component-specific assessment. In\nthis work, we present MIRAGE, a Question Answering dataset specifically\ndesigned for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped\nto a retrieval pool of 37,800 entries, enabling an efficient and precise\nevaluation of both retrieval and generation tasks. We also introduce novel\nevaluation metrics aimed at measuring RAG adaptability, encompassing dimensions\nsuch as noise vulnerability, context acceptability, context insensitivity, and\ncontext misinterpretation. Through comprehensive experiments across various\nretriever-LLM configurations, we provide new insights into the optimal\nalignment of model pairs and the nuanced dynamics within RAG systems. The\ndataset and evaluation code are publicly available, allowing for seamless\nintegration and customization in diverse research settings\\footnote{The MIRAGE\ncode and data are available at https://github.com/nlpai-lab/MIRAGE.", "AI": {"tldr": "MIRAGE\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3aRAG\u7cfb\u7edf\u8bc4\u4f30\u8bbe\u8ba1\u7684\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5305\u542b7,560\u4e2a\u5b9e\u4f8b\u548c37,800\u6761\u68c0\u7d22\u6761\u76ee\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u4ee5\u8861\u91cfRAG\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u7531\u4e8eRAG\u7cfb\u7edf\u4e2d\u68c0\u7d22\u4e0e\u751f\u6210\u7ec4\u4ef6\u7684\u590d\u6742\u4ea4\u4e92\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7f3a\u4e4f\u8be6\u7ec6\u7684\u7ec4\u4ef6\u7279\u5b9a\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51fa\u4e86MIRAGE\u6570\u636e\u96c6\uff0c\u5305\u542b\u5927\u91cf\u5b9e\u4f8b\u548c\u68c0\u7d22\u6761\u76ee\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5982\u566a\u58f0\u8106\u5f31\u6027\u3001\u4e0a\u4e0b\u6587\u53ef\u63a5\u53d7\u6027\u7b49\uff09\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u4e86RAG\u7cfb\u7edf\u4e2d\u6a21\u578b\u5bf9\u7684\u6700\u4f18\u5bf9\u9f50\u53ca\u5176\u5185\u90e8\u52a8\u6001\u3002", "conclusion": "MIRAGE\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u516c\u5f00\uff0c\u4fbf\u4e8e\u7814\u7a76\u4f7f\u7528\u3002"}}
{"id": "2504.17223", "pdf": "https://arxiv.org/pdf/2504.17223", "abs": "https://arxiv.org/abs/2504.17223", "authors": ["Mengyu Qiao", "Runze Tian", "Yang Wang"], "title": "Towards Generalizable Deepfake Detection with Spatial-Frequency Collaborative Learning and Hierarchical Cross-Modal Fusion", "categories": ["cs.CV"], "comment": null, "summary": "The rapid evolution of deep generative models poses a critical challenge to\ndeepfake detection, as detectors trained on forgery-specific artifacts often\nsuffer significant performance degradation when encountering unseen forgeries.\nWhile existing methods predominantly rely on spatial domain analysis, frequency\ndomain operations are primarily limited to feature-level augmentation, leaving\nfrequency-native artifacts and spatial-frequency interactions insufficiently\nexploited. To address this limitation, we propose a novel detection framework\nthat integrates multi-scale spatial-frequency analysis for universal deepfake\ndetection. Our framework comprises three key components: (1) a local spectral\nfeature extraction pipeline that combines block-wise discrete cosine transform\nwith cascaded multi-scale convolutions to capture subtle spectral artifacts;\n(2) a global spectral feature extraction pipeline utilizing scale-invariant\ndifferential accumulation to identify holistic forgery distribution patterns;\nand (3) a multi-stage cross-modal fusion mechanism that incorporates\nshallow-layer attention enhancement and deep-layer dynamic modulation to model\nspatial-frequency interactions. Extensive evaluations on widely adopted\nbenchmarks demonstrate that our method outperforms state-of-the-art deepfake\ndetection methods in both accuracy and generalizability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u5c3a\u5ea6\u7a7a\u95f4-\u9891\u7387\u5206\u6790\u7684\u65b0\u578b\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7a7a\u95f4\u57df\u5206\u6790\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u9891\u7387\u57df\u7279\u5f81\u548c\u7a7a\u95f4-\u9891\u7387\u4ea4\u4e92\uff0c\u5bfc\u81f4\u5bf9\u672a\u89c1\u4f2a\u9020\u7684\u68c0\u6d4b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u6846\u67b6\u5305\u542b\u5c40\u90e8\u548c\u5168\u5c40\u9891\u8c31\u7279\u5f81\u63d0\u53d6\u7ba1\u9053\uff0c\u4ee5\u53ca\u591a\u9636\u6bb5\u8de8\u6a21\u6001\u878d\u5408\u673a\u5236\uff0c\u7ed3\u5408\u5757\u72b6\u79bb\u6563\u4f59\u5f26\u53d8\u6362\u548c\u591a\u5c3a\u5ea6\u5377\u79ef\u3002", "result": "\u5728\u5e7f\u6cdb\u91c7\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u5c3a\u5ea6\u7a7a\u95f4-\u9891\u7387\u5206\u6790\u6709\u6548\u63d0\u5347\u4e86\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2504.17192", "pdf": "https://arxiv.org/pdf/2504.17192", "abs": "https://arxiv.org/abs/2504.17192", "authors": ["Minju Seo", "Jinheon Baek", "Seongyun Lee", "Sung Ju Hwang"], "title": "Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning", "categories": ["cs.CL"], "comment": null, "summary": "Despite the rapid growth of machine learning research, corresponding code\nimplementations are often unavailable, making it slow and labor-intensive for\nresearchers to reproduce results and build upon prior work. In the meantime,\nrecent Large Language Models (LLMs) excel at understanding scientific documents\nand generating high-quality code. Inspired by this, we introduce PaperCoder, a\nmulti-agent LLM framework that transforms machine learning papers into\nfunctional code repositories. PaperCoder operates in three stages: planning,\nwhere it constructs a high-level roadmap, designs the system architecture with\ndiagrams, identifies file dependencies, and generates configuration files;\nanalysis, which focuses on interpreting implementation-specific details; and\ngeneration, where modular, dependency-aware code is produced. Moreover, each\nphase is instantiated through a set of specialized agents designed to\ncollaborate effectively across the pipeline. We then evaluate PaperCoder on\ngenerating code implementations from machine learning papers based on both\nmodel-based and human evaluations, specifically from the original paper\nauthors, with author-released repositories as ground truth if available. Our\nresults demonstrate the effectiveness of PaperCoder in creating high-quality,\nfaithful implementations. Furthermore, it consistently shows strengths in the\nrecently released PaperBench benchmark, surpassing strong baselines by\nsubstantial margins.", "AI": {"tldr": "PaperCoder\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53LLM\u6846\u67b6\uff0c\u5c06\u673a\u5668\u5b66\u4e60\u8bba\u6587\u8f6c\u5316\u4e3a\u529f\u80fd\u6027\u4ee3\u7801\u4ed3\u5e93\uff0c\u5206\u4e3a\u89c4\u5212\u3001\u5206\u6790\u548c\u751f\u6210\u4e09\u4e2a\u9636\u6bb5\uff0c\u5e76\u901a\u8fc7\u534f\u4f5c\u7684\u667a\u80fd\u4f53\u5b9e\u73b0\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7814\u7a76\u4e2d\u4ee3\u7801\u5b9e\u73b0\u5e38\u4e0d\u53ef\u7528\uff0c\u5bfc\u81f4\u590d\u73b0\u548c\u6269\u5c55\u5de5\u4f5c\u8017\u65f6\u8d39\u529b\uff0c\u800cLLMs\u5728\u7406\u89e3\u79d1\u5b66\u6587\u6863\u548c\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "method": "PaperCoder\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u5b9e\u73b0\uff1a\u89c4\u5212\uff08\u8bbe\u8ba1\u67b6\u6784\u3001\u4f9d\u8d56\u5173\u7cfb\u548c\u914d\u7f6e\u6587\u4ef6\uff09\u3001\u5206\u6790\uff08\u89e3\u6790\u5b9e\u73b0\u7ec6\u8282\uff09\u548c\u751f\u6210\uff08\u751f\u6210\u6a21\u5757\u5316\u4ee3\u7801\uff09\uff0c\u6bcf\u4e2a\u9636\u6bb5\u7531\u4e13\u95e8\u667a\u80fd\u4f53\u534f\u4f5c\u5b8c\u6210\u3002", "result": "PaperCoder\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u4ee3\u7801\u5b9e\u73b0\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5728PaperBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PaperCoder\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4ee3\u7801\u5b9e\u73b0\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2504.17224", "pdf": "https://arxiv.org/pdf/2504.17224", "abs": "https://arxiv.org/abs/2504.17224", "authors": ["Zhifeng Wang", "Qixuan Zhang", "Peter Zhang", "Wenjia Niu", "Kaihao Zhang", "Ramesh Sankaranarayana", "Sabrina Caldwell", "Tom Gedeon"], "title": "Visual and textual prompts for enhancing emotion recognition in video", "categories": ["cs.CV"], "comment": "12 pages, 10 figures", "summary": "Vision Large Language Models (VLLMs) exhibit promising potential for\nmulti-modal understanding, yet their application to video-based emotion\nrecognition remains limited by insufficient spatial and contextual awareness.\nTraditional approaches, which prioritize isolated facial features, often\nneglect critical non-verbal cues such as body language, environmental context,\nand social interactions, leading to reduced robustness in real-world scenarios.\nTo address this gap, we propose Set-of-Vision-Text Prompting (SoVTP), a novel\nframework that enhances zero-shot emotion recognition by integrating spatial\nannotations (e.g., bounding boxes, facial landmarks), physiological signals\n(facial action units), and contextual cues (body posture, scene dynamics,\nothers' emotions) into a unified prompting strategy. SoVTP preserves holistic\nscene information while enabling fine-grained analysis of facial muscle\nmovements and interpersonal dynamics. Extensive experiments show that SoVTP\nachieves substantial improvements over existing visual prompting methods,\ndemonstrating its effectiveness in enhancing VLLMs' video emotion recognition\ncapabilities.", "AI": {"tldr": "SoVTP\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7a7a\u95f4\u6807\u6ce8\u3001\u751f\u7406\u4fe1\u53f7\u548c\u4e0a\u4e0b\u6587\u63d0\u793a\uff0c\u63d0\u5347VLLMs\u5728\u89c6\u9891\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u96f6\u6837\u672c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709VLLMs\u5728\u89c6\u9891\u60c5\u611f\u8bc6\u522b\u4e2d\u56e0\u7a7a\u95f4\u548c\u4e0a\u4e0b\u6587\u610f\u8bc6\u4e0d\u8db3\u800c\u53d7\u9650\uff0c\u4f20\u7edf\u65b9\u6cd5\u5ffd\u89c6\u975e\u8bed\u8a00\u7ebf\u7d22\u3002", "method": "\u63d0\u51faSoVTP\u6846\u67b6\uff0c\u7ed3\u5408\u7a7a\u95f4\u6807\u6ce8\u3001\u751f\u7406\u4fe1\u53f7\u548c\u4e0a\u4e0b\u6587\u63d0\u793a\uff0c\u5f62\u6210\u7edf\u4e00\u63d0\u793a\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793aSoVTP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89c6\u89c9\u63d0\u793a\u65b9\u6cd5\uff0c\u63d0\u5347VLLMs\u7684\u60c5\u611f\u8bc6\u522b\u80fd\u529b\u3002", "conclusion": "SoVTP\u901a\u8fc7\u591a\u6a21\u6001\u6574\u5408\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2504.17200", "pdf": "https://arxiv.org/pdf/2504.17200", "abs": "https://arxiv.org/abs/2504.17200", "authors": ["Yangxinyu Xie", "Bowen Jiang", "Tanwi Mallick", "Joshua David Bergerson", "John K. Hutchison", "Duane R. Verner", "Jordan Branham", "M. Ross Alexander", "Robert B. Ross", "Yan Feng", "Leslie-Anne Levy", "Weijie Su", "Camillo J. Taylor"], "title": "A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are a transformational capability at the\nfrontier of artificial intelligence and machine learning that can support\ndecision-makers in addressing pressing societal challenges such as extreme\nnatural hazard events. As generalized models, LLMs often struggle to provide\ncontext-specific information, particularly in areas requiring specialized\nknowledge. In this work we propose a retrieval-augmented generation (RAG)-based\nmulti-agent LLM system to support analysis and decision-making in the context\nof natural hazards and extreme weather events. As a proof of concept, we\npresent WildfireGPT, a specialized system focused on wildfire hazards. The\narchitecture employs a user-centered, multi-agent design to deliver tailored\nrisk insights across diverse stakeholder groups. By integrating natural hazard\nand extreme weather projection data, observational datasets, and scientific\nliterature through an RAG framework, the system ensures both the accuracy and\ncontextual relevance of the information it provides. Evaluation across ten\nexpert-led case studies demonstrates that WildfireGPT significantly outperforms\nexisting LLM-based solutions for decision support.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edfWildfireGPT\uff0c\u7528\u4e8e\u81ea\u7136\u707e\u5bb3\u51b3\u7b56\u652f\u6301\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLM\u65b9\u6848\u3002", "motivation": "\u901a\u7528LLM\u5728\u63d0\u4f9b\u7279\u5b9a\u9886\u57df\uff08\u5982\u81ea\u7136\u707e\u5bb3\uff09\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u4ee5\u652f\u6301\u51b3\u7b56\u3002", "method": "\u91c7\u7528RAG\u6846\u67b6\u6574\u5408\u707e\u5bb3\u6570\u636e\u3001\u89c2\u6d4b\u8d44\u6599\u548c\u79d1\u5b66\u6587\u732e\uff0c\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53\u7cfb\u7edfWildfireGPT\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u5b9a\u5236\u5316\u98ce\u9669\u5206\u6790\u3002", "result": "\u5728\u5341\u9879\u4e13\u5bb6\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cWildfireGPT\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLM\u51b3\u7b56\u652f\u6301\u65b9\u6848\u3002", "conclusion": "WildfireGPT\u901a\u8fc7RAG\u548c\u591a\u667a\u80fd\u4f53\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u81ea\u7136\u707e\u5bb3\u9886\u57df\u7684\u5b9e\u7528\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.17229", "pdf": "https://arxiv.org/pdf/2504.17229", "abs": "https://arxiv.org/abs/2504.17229", "authors": ["Akihiro Kuwabara", "Sorachi Kato", "Takuya Fujihashi", "Toshiaki Koike-Akino", "Takashi Watanabe"], "title": "Range Image-Based Implicit Neural Compression for LiDAR Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a novel scheme to efficiently compress Light Detection\nand Ranging~(LiDAR) point clouds, enabling high-precision 3D scene archives,\nand such archives pave the way for a detailed understanding of the\ncorresponding 3D scenes. We focus on 2D range images~(RIs) as a lightweight\nformat for representing 3D LiDAR observations. Although conventional image\ncompression techniques can be adapted to improve compression efficiency for\nRIs, their practical performance is expected to be limited due to differences\nin bit precision and the distinct pixel value distribution characteristics\nbetween natural images and RIs. We propose a novel implicit neural\nrepresentation~(INR)--based RI compression method that effectively handles\nfloating-point valued pixels. The proposed method divides RIs into depth and\nmask images and compresses them using patch-wise and pixel-wise INR\narchitectures with model pruning and quantization, respectively. Experiments on\nthe KITTI dataset show that the proposed method outperforms existing image,\npoint cloud, RI, and INR-based compression methods in terms of 3D\nreconstruction and detection quality at low bitrates and decoding latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7684LiDAR\u70b9\u4e91\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u548c\u63a9\u7801\u56fe\u50cf\u7684\u5206\u5757\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u6bd4\u7279\u7387\u4e0b\u76843D\u91cd\u5efa\u548c\u68c0\u6d4b\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u538b\u7f29\u6280\u672f\u5728\u5904\u7406LiDAR\u76842D\u8303\u56f4\u56fe\u50cf\uff08RIs\uff09\u65f6\u6548\u7387\u6709\u9650\uff0c\u56e0\u4e3a\u5176\u4e0e\u81ea\u7136\u56fe\u50cf\u5728\u6bd4\u7279\u7cbe\u5ea6\u548c\u50cf\u7d20\u503c\u5206\u5e03\u4e0a\u5b58\u5728\u5dee\u5f02\u3002", "method": "\u5c06RIs\u5206\u4e3a\u6df1\u5ea6\u548c\u63a9\u7801\u56fe\u50cf\uff0c\u5206\u522b\u91c7\u7528\u5206\u5757\u548c\u9010\u50cf\u7d20\u7684INR\u67b6\u6784\uff0c\u7ed3\u5408\u6a21\u578b\u526a\u679d\u548c\u91cf\u5316\u8fdb\u884c\u538b\u7f29\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u4f4e\u6bd4\u7279\u7387\u548c\u89e3\u7801\u5ef6\u8fdf\u4e0b\uff0c\u4f18\u4e8e\u73b0\u6709\u56fe\u50cf\u3001\u70b9\u4e91\u3001RI\u548cINR\u538b\u7f29\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u6548\u538b\u7f29LiDAR\u70b9\u4e91\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u652f\u6301\u9ad8\u7cbe\u5ea63D\u573a\u666f\u5b58\u6863\u3002"}}
{"id": "2504.17220", "pdf": "https://arxiv.org/pdf/2504.17220", "abs": "https://arxiv.org/abs/2504.17220", "authors": ["Kaidong Feng", "Zhu Sun", "Jie Yang", "Hui Fang", "Xinghua Qu", "Wenyuan Liu"], "title": "Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "LLMs are increasingly explored for bundle generation, thanks to their\nreasoning capabilities and knowledge. However, deploying large-scale LLMs\nintroduces significant efficiency challenges, primarily high computational\ncosts during fine-tuning and inference due to their massive parameterization.\nKnowledge distillation (KD) offers a promising solution, transferring expertise\nfrom large teacher models to compact student models. This study systematically\ninvestigates knowledge distillation approaches for bundle generation, aiming to\nminimize computational demands while preserving performance. We explore three\ncritical research questions: (1) how does the format of KD impact bundle\ngeneration performance? (2) to what extent does the quantity of distilled\nknowledge influence performance? and (3) how do different ways of utilizing the\ndistilled knowledge affect performance? We propose a comprehensive KD framework\nthat (i) progressively extracts knowledge (patterns, rules, deep thoughts);\n(ii) captures varying quantities of distilled knowledge through different\nstrategies; and (iii) exploits complementary LLM adaptation techniques\n(in-context learning, supervised fine-tuning, combination) to leverage\ndistilled knowledge in small student models for domain-specific adaptation and\nenhanced efficiency. Extensive experiments provide valuable insights into how\nknowledge format, quantity, and utilization methodologies collectively shape\nLLM-based bundle generation performance, exhibiting KD's significant potential\nfor more efficient yet effective LLM-based bundle generation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u5728LLM\u6346\u7ed1\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u6846\u67b6\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u89c4\u6a21LLM\u5728\u6346\u7ed1\u751f\u6210\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7efc\u5408KD\u6846\u67b6\uff0c\u9010\u6b65\u63d0\u53d6\u77e5\u8bc6\u3001\u6355\u83b7\u4e0d\u540c\u91cf\u7684\u77e5\u8bc6\uff0c\u5e76\u7ed3\u5408LLM\u9002\u5e94\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u77e5\u8bc6\u683c\u5f0f\u3001\u6570\u91cf\u53ca\u5229\u7528\u65b9\u6cd5\u5171\u540c\u5f71\u54cd\u6027\u80fd\uff0cKD\u5728\u9ad8\u6548\u6346\u7ed1\u751f\u6210\u4e2d\u6f5c\u529b\u663e\u8457\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u80fd\u663e\u8457\u63d0\u5347LLM\u6346\u7ed1\u751f\u6210\u7684\u6548\u7387\u4e0e\u6548\u679c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.17234", "pdf": "https://arxiv.org/pdf/2504.17234", "abs": "https://arxiv.org/abs/2504.17234", "authors": ["Zhiqiang Lao", "Heather Yu"], "title": "Scene Perceived Image Perceptual Score (SPIPS): combining global and local perception for image quality assessment", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of artificial intelligence and widespread use of\nsmartphones have resulted in an exponential growth of image data, both real\n(camera-captured) and virtual (AI-generated). This surge underscores the\ncritical need for robust image quality assessment (IQA) methods that accurately\nreflect human visual perception. Traditional IQA techniques primarily rely on\nspatial features - such as signal-to-noise ratio, local structural distortions,\nand texture inconsistencies - to identify artifacts. While effective for\nunprocessed or conventionally altered images, these methods fall short in the\ncontext of modern image post-processing powered by deep neural networks (DNNs).\nThe rise of DNN-based models for image generation, enhancement, and restoration\nhas significantly improved visual quality, yet made accurate assessment\nincreasingly complex. To address this, we propose a novel IQA approach that\nbridges the gap between deep learning methods and human perception. Our model\ndisentangles deep features into high-level semantic information and low-level\nperceptual details, treating each stream separately. These features are then\ncombined with conventional IQA metrics to provide a more comprehensive\nevaluation framework. This hybrid design enables the model to assess both\nglobal context and intricate image details, better reflecting the human visual\nprocess, which first interprets overall structure before attending to\nfine-grained elements. The final stage employs a multilayer perceptron (MLP) to\nmap the integrated features into a concise quality score. Experimental results\ndemonstrate that our method achieves improved consistency with human perceptual\njudgments compared to existing IQA models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u4f20\u7edfIQA\u65b9\u6cd5\u7684\u65b0\u578b\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u79bb\u9ad8\u3001\u4f4e\u5c42\u6b21\u7279\u5f81\u5e76\u6574\u5408\uff0c\u66f4\u8d34\u8fd1\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u3002", "motivation": "\u968f\u7740AI\u548c\u667a\u80fd\u624b\u673a\u7684\u666e\u53ca\uff0c\u56fe\u50cf\u6570\u636e\u6fc0\u589e\uff0c\u4f20\u7edfIQA\u65b9\u6cd5\u96be\u4ee5\u8bc4\u4f30DNN\u5904\u7406\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u9700\u66f4\u8d34\u8fd1\u4eba\u7c7b\u611f\u77e5\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6a21\u578b\u5c06\u6df1\u5ea6\u7279\u5f81\u5206\u89e3\u4e3a\u9ad8\u5c42\u6b21\u8bed\u4e49\u548c\u4f4e\u5c42\u6b21\u611f\u77e5\u7ec6\u8282\uff0c\u7ed3\u5408\u4f20\u7edfIQA\u6307\u6807\uff0c\u901a\u8fc7MLP\u751f\u6210\u8d28\u91cf\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709IQA\u6a21\u578b\u66f4\u7b26\u5408\u4eba\u7c7b\u89c6\u89c9\u5224\u65ad\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u591a\u5c42\u6b21\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.17238", "pdf": "https://arxiv.org/pdf/2504.17238", "abs": "https://arxiv.org/abs/2504.17238", "authors": ["Jinfeng Zhou", "Yuxuan Chen", "Jianing Yin", "Yongkang Huang", "Yihan Shi", "Xikun Zhang", "Libiao Peng", "Rongsheng Zhang", "Tangjie Lv", "Zhipeng Hu", "Hongning Wang", "Minlie Huang"], "title": "Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Cognitive Restructuring (CR) is a psychotherapeutic process aimed at\nidentifying and restructuring an individual's negative thoughts, arising from\nmental health challenges, into more helpful and positive ones via multi-turn\ndialogues. Clinician shortage and stigma urge the development of human-LLM\ninteractive psychotherapy for CR. Yet, existing efforts implement CR via simple\ntext rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to\nalign with the psychotherapeutic process for effective CR. To address this gap,\nwe propose CRDial, a novel framework for CR, which creates multi-turn dialogues\nwith specifically designed identification and restructuring stages of negative\nthoughts, integrates sentence-level supportive conversation strategies, and\nadopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we\ndistill Crisp, a large-scale and high-quality bilingual dialogue dataset, from\nLLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and\n14B scales. Extensive human studies show the superiority of Crispers in\npointwise, pairwise, and intervention evaluations.", "AI": {"tldr": "CRDial\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u5b9e\u73b0\u8ba4\u77e5\u91cd\u6784\uff0c\u7ed3\u5408\u652f\u6301\u6027\u5bf9\u8bdd\u7b56\u7565\u548c\u591a\u901a\u9053\u5faa\u73af\u673a\u5236\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u53cc\u8bed\u6570\u636e\u96c6Crisp\uff0c\u5e76\u8bad\u7ec3\u51fa\u8868\u73b0\u4f18\u5f02\u7684\u5bf9\u8bdd\u6a21\u578bCrispers\u3002", "motivation": "\u4e34\u5e8a\u533b\u751f\u77ed\u7f3a\u548c\u5fc3\u7406\u5065\u5eb7\u6c61\u540d\u5316\u4fc3\u4f7f\u5f00\u53d1\u4eba\u673a\u4ea4\u4e92\u5fc3\u7406\u6cbb\u7597\u5de5\u5177\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u6a21\u62df\u5fc3\u7406\u6cbb\u7597\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faCRDial\u6846\u67b6\uff0c\u8bbe\u8ba1\u591a\u8f6e\u5bf9\u8bdd\u9636\u6bb5\uff08\u8bc6\u522b\u4e0e\u91cd\u6784\u8d1f\u9762\u601d\u60f3\uff09\uff0c\u6574\u5408\u652f\u6301\u6027\u5bf9\u8bdd\u7b56\u7565\u548c\u591a\u901a\u9053\u5faa\u73af\u673a\u5236\uff0c\u751f\u6210\u6570\u636e\u96c6Crisp\u5e76\u8bad\u7ec3\u5bf9\u8bdd\u6a21\u578bCrispers\u3002", "result": "Crispers\u5728\u70b9\u5bf9\u70b9\u3001\u6210\u5bf9\u548c\u5e72\u9884\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CRDial\u548cCrispers\u4e3a\u8ba4\u77e5\u91cd\u6784\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17253", "pdf": "https://arxiv.org/pdf/2504.17253", "abs": "https://arxiv.org/abs/2504.17253", "authors": ["Yinqi Li", "Hong Chang", "Ruibing Hou", "Shiguang Shan", "Xilin Chen"], "title": "DIVE: Inverting Conditional Diffusion Models for Discriminative Tasks", "categories": ["cs.CV", "cs.MM"], "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "Diffusion models have shown remarkable progress in various generative tasks\nsuch as image and video generation. This paper studies the problem of\nleveraging pretrained diffusion models for performing discriminative tasks.\nSpecifically, we extend the discriminative capability of pretrained frozen\ngenerative diffusion models from the classification task to the more complex\nobject detection task, by \"inverting\" a pretrained layout-to-image diffusion\nmodel. To this end, a gradient-based discrete optimization approach for\nreplacing the heavy prediction enumeration process, and a prior distribution\nmodel for making more accurate use of the Bayes' rule, are proposed\nrespectively. Empirical results show that this method is on par with basic\ndiscriminative object detection baselines on COCO dataset. In addition, our\nmethod can greatly speed up the previous diffusion-based method for\nclassification without sacrificing accuracy. Code and models are available at\nhttps://github.com/LiYinqi/DIVE .", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5224\u522b\u6027\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cd\u8f6c\u5e03\u5c40\u5230\u56fe\u50cf\u7684\u6269\u6563\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u68af\u5ea6\u4f18\u5316\u548c\u5148\u9a8c\u5206\u5e03\u6a21\u578b\uff0c\u6027\u80fd\u4e0e\u57fa\u7840\u5224\u522b\u6027\u57fa\u7ebf\u76f8\u5f53\uff0c\u4e14\u901f\u5ea6\u66f4\u5feb\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u9884\u8bad\u7ec3\u7684\u751f\u6210\u6269\u6563\u6a21\u578b\u6267\u884c\u5224\u522b\u6027\u4efb\u52a1\uff0c\u6269\u5c55\u5176\u4ece\u5206\u7c7b\u5230\u66f4\u590d\u6742\u7684\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u68af\u5ea6\u79bb\u6563\u4f18\u5316\u65b9\u6cd5\u66ff\u4ee3\u7e41\u91cd\u7684\u9884\u6d4b\u679a\u4e3e\u8fc7\u7a0b\uff0c\u5e76\u8bbe\u8ba1\u5148\u9a8c\u5206\u5e03\u6a21\u578b\u4ee5\u66f4\u51c6\u786e\u5730\u5e94\u7528\u8d1d\u53f6\u65af\u89c4\u5219\u3002", "result": "\u5728COCO\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u4e0e\u57fa\u7840\u5224\u522b\u6027\u76ee\u6807\u68c0\u6d4b\u57fa\u7ebf\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u52a0\u901f\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5728\u5224\u522b\u6027\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e14\u6548\u7387\u66f4\u9ad8\u3002"}}
{"id": "2504.17252", "pdf": "https://arxiv.org/pdf/2504.17252", "abs": "https://arxiv.org/abs/2504.17252", "authors": ["Ocheme Anthony Ekle", "Biswarup Das"], "title": "Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo", "categories": ["cs.CL", "cs.LG", "68T50, 68T01", "I.2.7; I.2.1"], "comment": "25 pages, 14 combined figures (19 total), includes horizontal\n  layouts. Submitted to arXiv for open access", "summary": "In this study, we develop Neural Machine Translation (NMT) and\nTransformer-based transfer learning models for English-to-Igbo translation - a\nlow-resource African language spoken by over 40 million people across Nigeria\nand West Africa. Our models are trained on a curated and benchmarked dataset\ncompiled from Bible corpora, local news, Wikipedia articles, and Common Crawl,\nall verified by native language experts. We leverage Recurrent Neural Network\n(RNN) architectures, including Long Short-Term Memory (LSTM) and Gated\nRecurrent Units (GRU), enhanced with attention mechanisms to improve\ntranslation accuracy. To further enhance performance, we apply transfer\nlearning using MarianNMT pre-trained models within the SimpleTransformers\nframework. Our RNN-based system achieves competitive results, closely matching\nexisting English-Igbo benchmarks. With transfer learning, we observe a\nperformance gain of +4.83 BLEU points, reaching an estimated translation\naccuracy of 70%. These findings highlight the effectiveness of combining RNNs\nwith transfer learning to address the performance gap in low-resource language\ntranslation tasks.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u57fa\u4e8eRNN\u548cTransformer\u7684\u82f1\u8bed-\u4f0a\u535a\u8bed\u7ffb\u8bd1\u6a21\u578b\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u6027\u80fd\uff0cBLEU\u5206\u6570\u63d0\u9ad84.83\uff0c\u8fbe\u523070%\u7684\u7ffb\u8bd1\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u975e\u6d32\u8bed\u8a00\u4f0a\u535a\u8bed\u7684\u673a\u5668\u7ffb\u8bd1\u95ee\u9898\uff0c\u586b\u8865\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528RNN\u67b6\u6784\uff08LSTM\u548cGRU\uff09\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5e94\u7528\u8fc1\u79fb\u5b66\u4e60\uff08MarianNMT\u9884\u8bad\u7ec3\u6a21\u578b\uff09\u3002", "result": "RNN\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u73b0\u6709\u57fa\u51c6\uff0c\u8fc1\u79fb\u5b66\u4e60\u4f7fBLEU\u5206\u6570\u63d0\u53474.83\uff0c\u51c6\u786e\u7387\u8fbe70%\u3002", "conclusion": "RNN\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u6027\u80fd\u3002"}}
{"id": "2504.17263", "pdf": "https://arxiv.org/pdf/2504.17263", "abs": "https://arxiv.org/abs/2504.17263", "authors": ["Wenqiang Zhou", "Zhendong Yu", "Xinyu Liu", "Jiaming Yang", "Rong Xiao", "Tao Wang", "Chenwei Tang", "Jiancheng Lv"], "title": "Precision Neural Network Quantization via Learnable Adaptive Modules", "categories": ["cs.CV", "cs.CC"], "comment": null, "summary": "Quantization Aware Training (QAT) is a neural network quantization technique\nthat compresses model size and improves operational efficiency while\neffectively maintaining model performance. The paradigm of QAT is to introduce\nfake quantization operators during the training process, allowing the model to\nautonomously compensate for information loss caused by quantization. Making\nquantization parameters trainable can significantly improve the performance of\nQAT, but at the cost of compromising the flexibility during inference,\nespecially when dealing with activation values with substantially different\ndistributions. In this paper, we propose an effective learnable adaptive neural\nnetwork quantization method, called Adaptive Step Size Quantization (ASQ), to\nresolve this conflict. Specifically, the proposed ASQ method first dynamically\nadjusts quantization scaling factors through a trained module capable of\naccommodating different activations. Then, to address the rigid resolution\nissue inherent in Power of Two (POT) quantization, we propose an efficient\nnon-uniform quantization scheme. We utilize the Power Of Square root of Two\n(POST) as the basis for exponential quantization, effectively handling the\nbell-shaped distribution of neural network weights across various bit-widths\nwhile maintaining computational efficiency through a Look-Up Table method\n(LUT). Extensive experimental results demonstrate that the proposed ASQ method\nis superior to the state-of-the-art QAT approaches. Notably that the ASQ is\neven competitive compared to full precision baselines, with its 4-bit quantized\nResNet34 model improving accuracy by 1.2\\% on ImageNet.", "AI": {"tldr": "ASQ\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u91cf\u5316\u53c2\u6570\u548c\u975e\u5747\u5300\u91cf\u5316\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u91cf\u5316\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfQAT\u4e2d\u91cf\u5316\u53c2\u6570\u56fa\u5b9a\u5bfc\u81f4\u6fc0\u6d3b\u503c\u5206\u5e03\u5dee\u5f02\u5927\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51faASQ\u65b9\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u91cf\u5316\u7f29\u653e\u56e0\u5b50\uff0c\u5e76\u91c7\u7528POST\u975e\u5747\u5300\u91cf\u5316\u65b9\u6848\u5904\u7406\u6743\u91cd\u5206\u5e03\u3002", "result": "ASQ\u57284\u4f4d\u91cf\u5316ResNet34\u4e0a\u6bd4\u5168\u7cbe\u5ea6\u57fa\u7ebf\u63d0\u53471.2%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709QAT\u65b9\u6cd5\u3002", "conclusion": "ASQ\u901a\u8fc7\u81ea\u9002\u5e94\u91cf\u5316\u53c2\u6570\u548c\u975e\u5747\u5300\u91cf\u5316\uff0c\u6709\u6548\u5e73\u8861\u4e86\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2504.17264", "pdf": "https://arxiv.org/pdf/2504.17264", "abs": "https://arxiv.org/abs/2504.17264", "authors": ["Zhaolu Kang", "Hongtian Cai", "Xiangyang Ji", "Jinzhe Li", "Nanfei Gu"], "title": "JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in International Joint Conference on Neural Networks (IJCNN)\n  2025", "summary": "In recent years, Unsupervised Domain Adaptation (UDA) has gained significant\nattention in the field of Natural Language Processing (NLP) owing to its\nability to enhance model generalization across diverse domains. However, its\napplication for knowledge transfer between distinct legal domains remains\nlargely unexplored. To address the challenges posed by lengthy and complex\nlegal texts and the limited availability of large-scale annotated datasets, we\npropose JurisCTC, a novel model designed to improve the accuracy of Legal\nJudgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC\nfacilitates effective knowledge transfer across various legal domains and\nemploys contrastive learning to distinguish samples from different domains.\nSpecifically, for the LJP task, we enable knowledge transfer between civil and\ncriminal law domains. Compared to other models and specific large language\nmodels (LLMs), JurisCTC demonstrates notable advancements, achieving peak\naccuracies of 76.59% and 78.83%, respectively.", "AI": {"tldr": "JurisCTC\u662f\u4e00\u79cd\u65b0\u578b\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u5347\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5b9e\u73b0\u8de8\u6cd5\u5f8b\u9886\u57df\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u6cd5\u5f8b\u6587\u672c\u590d\u6742\u4e14\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u65e0\u76d1\u7763\u9886\u57df\u9002\u5e94\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faJurisCTC\u6a21\u578b\uff0c\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u533a\u5206\u4e0d\u540c\u6cd5\u5f8b\u9886\u57df\u7684\u6837\u672c\uff0c\u5b9e\u73b0\u6c11\u4e8b\u4e0e\u5211\u4e8b\u6cd5\u5f8b\u9886\u57df\u7684\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "JurisCTC\u5728\u51c6\u786e\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u522b\u8fbe\u523076.59%\u548c78.83%\u3002", "conclusion": "JurisCTC\u5728\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u8de8\u6cd5\u5f8b\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17269", "pdf": "https://arxiv.org/pdf/2504.17269", "abs": "https://arxiv.org/abs/2504.17269", "authors": ["Yu Hong", "Xiao Cai", "Pengpeng Zeng", "Shuai Zhang", "Jingkuan Song", "Lianli Gao", "Heng Tao Shen"], "title": "Towards Generalized and Training-Free Text-Guided Semantic Manipulation", "categories": ["cs.CV"], "comment": null, "summary": "Text-guided semantic manipulation refers to semantically editing an image\ngenerated from a source prompt to match a target prompt, enabling the desired\nsemantic changes (e.g., addition, removal, and style transfer) while preserving\nirrelevant contents. With the powerful generative capabilities of the diffusion\nmodel, the task has shown the potential to generate high-fidelity visual\ncontent. Nevertheless, existing methods either typically require time-consuming\nfine-tuning (inefficient), fail to accomplish multiple semantic manipulations\n(poorly extensible), and/or lack support for different modality tasks (limited\ngeneralizability). Upon further investigation, we find that the geometric\nproperties of noises in the diffusion model are strongly correlated with the\nsemantic changes. Motivated by this, we propose a novel $\\textit{GTF}$ for\ntext-guided semantic manipulation, which has the following attractive\ncapabilities: 1) $\\textbf{Generalized}$: our $\\textit{GTF}$ supports multiple\nsemantic manipulations (e.g., addition, removal, and style transfer) and can be\nseamlessly integrated into all diffusion-based methods (i.e., Plug-and-play)\nacross different modalities (i.e., modality-agnostic); and 2)\n$\\textbf{Training-free}$: $\\textit{GTF}$ produces high-fidelity results via\nsimply controlling the geometric relationship between noises without tuning or\noptimization. Our extensive experiments demonstrate the efficacy of our\napproach, highlighting its potential to advance the state-of-the-art in\nsemantics manipulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGTF\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6587\u672c\u5f15\u5bfc\u7684\u8bed\u4e49\u56fe\u50cf\u7f16\u8f91\uff0c\u652f\u6301\u591a\u79cd\u8bed\u4e49\u64cd\u4f5c\u4e14\u65e0\u9700\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4f4e\u3001\u6269\u5c55\u6027\u5dee\u4e14\u901a\u7528\u6027\u6709\u9650\uff0c\u800c\u6269\u6563\u6a21\u578b\u4e2d\u566a\u58f0\u7684\u51e0\u4f55\u7279\u6027\u4e0e\u8bed\u4e49\u53d8\u5316\u5f3a\u76f8\u5173\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u566a\u58f0\u7684\u51e0\u4f55\u5173\u7cfb\u5b9e\u73b0\u8bed\u4e49\u7f16\u8f91\uff0c\u652f\u6301\u591a\u79cd\u64cd\u4f5c\u4e14\u65e0\u9700\u8c03\u4f18\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eGTF\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u4fdd\u771f\u7ed3\u679c\uff0c\u652f\u6301\u591a\u79cd\u8bed\u4e49\u64cd\u4f5c\u3002", "conclusion": "GTF\u5728\u8bed\u4e49\u7f16\u8f91\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u63a8\u52a8\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2504.17279", "pdf": "https://arxiv.org/pdf/2504.17279", "abs": "https://arxiv.org/abs/2504.17279", "authors": ["Xiuying Chen", "Tairan Wang", "Juexiao Zhou", "Zirui Song", "Xin Gao", "Xiangliang Zhang"], "title": "Evaluating and Mitigating Bias in AI-Based Medical Text Generation", "categories": ["cs.CL"], "comment": "12 pages, 8 figures, published in Nature Computational Science", "summary": "Artificial intelligence (AI) systems, particularly those based on deep\nlearning models, have increasingly achieved expert-level performance in medical\napplications. However, there is growing concern that such AI systems may\nreflect and amplify human bias, and reduce the quality of their performance in\nhistorically under-served populations. The fairness issue has attracted\nconsiderable research interest in the medical imaging classification field, yet\nit remains understudied in the text generation domain. In this study, we\ninvestigate the fairness problem in text generation within the medical field\nand observe significant performance discrepancies across different races,\nsexes, and age groups, including intersectional groups, various model scales,\nand different evaluation metrics. To mitigate this fairness issue, we propose\nan algorithm that selectively optimizes those underperformed groups to reduce\nbias. The selection rules take into account not only word-level accuracy but\nalso the pathology accuracy to the target reference, while ensuring that the\nentire process remains fully differentiable for effective model training. Our\nevaluations across multiple backbones, datasets, and modalities demonstrate\nthat our proposed algorithm enhances fairness in text generation without\ncompromising overall performance. Specifically, the disparities among various\ngroups across different metrics were diminished by more than 30% with our\nalgorithm, while the relative change in text generation accuracy was typically\nwithin 2%. By reducing the bias generated by deep learning models, our proposed\napproach can potentially alleviate concerns about the fairness and reliability\nof text generation diagnosis in medical domain.\n  Our code is publicly available to facilitate further research at\nhttps://github.com/iriscxy/GenFair.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u533b\u7597\u9886\u57df\u6587\u672c\u751f\u6210\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9009\u62e9\u6027\u4f18\u5316\u7b97\u6cd5\u4ee5\u51cf\u5c11\u504f\u89c1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4e0d\u540c\u7fa4\u4f53\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "AI\u7cfb\u7edf\u5728\u533b\u7597\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53ef\u80fd\u653e\u5927\u4eba\u7c7b\u504f\u89c1\uff0c\u5c24\u5176\u5728\u6587\u672c\u751f\u6210\u9886\u57df\u516c\u5e73\u6027\u95ee\u9898\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9009\u62e9\u6027\u4f18\u5316\u7b97\u6cd5\uff0c\u8003\u8651\u8bcd\u7ea7\u51c6\u786e\u6027\u548c\u75c5\u7406\u51c6\u786e\u6027\uff0c\u786e\u4fdd\u8fc7\u7a0b\u53ef\u5fae\u5206\u4ee5\u6709\u6548\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u7b97\u6cd5\u5c06\u4e0d\u540c\u7fa4\u4f53\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u51cf\u5c1130%\u4ee5\u4e0a\uff0c\u6574\u4f53\u751f\u6210\u51c6\u786e\u7387\u53d8\u5316\u57282%\u4ee5\u5185\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u751f\u6210\u7684\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6574\u4f53\u6027\u80fd\uff0c\u6709\u671b\u7f13\u89e3\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u504f\u89c1\u95ee\u9898\u3002"}}
{"id": "2504.17280", "pdf": "https://arxiv.org/pdf/2504.17280", "abs": "https://arxiv.org/abs/2504.17280", "authors": ["Haodi Yao", "Fenghua He", "Ning Hao", "Chen Xie"], "title": "EdgePoint2: Compact Descriptors for Superior Efficiency and Accuracy", "categories": ["cs.CV"], "comment": null, "summary": "The field of keypoint extraction, which is essential for vision applications\nlike Structure from Motion (SfM) and Simultaneous Localization and Mapping\n(SLAM), has evolved from relying on handcrafted methods to leveraging deep\nlearning techniques. While deep learning approaches have significantly improved\nperformance, they often incur substantial computational costs, limiting their\ndeployment in real-time edge applications. Efforts to create lightweight neural\nnetworks have seen some success, yet they often result in trade-offs between\nefficiency and accuracy. Additionally, the high-dimensional descriptors\ngenerated by these networks poses challenges for distributed applications\nrequiring efficient communication and coordination, highlighting the need for\ncompact yet competitively accurate descriptors. In this paper, we present\nEdgePoint2, a series of lightweight keypoint detection and description neural\nnetworks specifically tailored for edge computing applications on embedded\nsystem. The network architecture is optimized for efficiency without\nsacrificing accuracy. To train compact descriptors, we introduce a combination\nof Orthogonal Procrustes loss and similarity loss, which can serve as a general\napproach for hypersphere embedding distillation tasks. Additionally, we offer\n14 sub-models to satisfy diverse application requirements. Our experiments\ndemonstrate that EdgePoint2 consistently achieves state-of-the-art (SOTA)\naccuracy and efficiency across various challenging scenarios while employing\nlower-dimensional descriptors (32/48/64). Beyond its accuracy, EdgePoint2\noffers significant advantages in flexibility, robustness, and versatility.\nConsequently, EdgePoint2 emerges as a highly competitive option for visual\ntasks, especially in contexts demanding adaptability to diverse computational\nand communication constraints.", "AI": {"tldr": "EdgePoint2\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5173\u952e\u70b9\u68c0\u6d4b\u548c\u63cf\u8ff0\u795e\u7ecf\u7f51\u7edc\uff0c\u4e13\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4f18\u5316\u67b6\u6784\u548c\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5173\u952e\u70b9\u63d0\u53d6\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u9ad8\u7ef4\u63cf\u8ff0\u7b26\u4e0d\u9002\u7528\u4e8e\u5206\u5e03\u5f0f\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u7d27\u51d1\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4f18\u5316\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u6b63\u4ea4Procrustes\u635f\u5931\u548c\u76f8\u4f3c\u6027\u635f\u5931\u8bad\u7ec3\u7d27\u51d1\u63cf\u8ff0\u7b26\uff0c\u5e76\u63d0\u4f9b14\u4e2a\u5b50\u6a21\u578b\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u663e\u793aEdgePoint2\u5728\u591a\u79cd\u573a\u666f\u4e0b\u5747\u8fbe\u5230SOTA\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u4e14\u4f7f\u7528\u4f4e\u7ef4\u63cf\u8ff0\u7b26\uff0832/48/64\uff09\u3002", "conclusion": "EdgePoint2\u5728\u7075\u6d3b\u6027\u3001\u9c81\u68d2\u6027\u548c\u591a\u529f\u80fd\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u662f\u9002\u5e94\u591a\u6837\u5316\u8ba1\u7b97\u548c\u901a\u4fe1\u9650\u5236\u7684\u7406\u60f3\u9009\u62e9\u3002"}}
{"id": "2504.17309", "pdf": "https://arxiv.org/pdf/2504.17309", "abs": "https://arxiv.org/abs/2504.17309", "authors": ["Junyan Zhang", "Shuliang Liu", "Aiwei Liu", "Yubo Gao", "Jungang Li", "Xiaojie Gu", "Xuming Hu"], "title": "CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality", "categories": ["cs.CL"], "comment": "Published at the 1st workshop on GenAI Watermarking, collocated with\n  ICLR 2025", "summary": "Watermarking technology is a method used to trace the usage of content\ngenerated by large language models. Sentence-level watermarking aids in\npreserving the semantic integrity within individual sentences while maintaining\ngreater robustness. However, many existing sentence-level watermarking\ntechniques depend on arbitrary segmentation or generation processes to embed\nwatermarks, which can limit the availability of appropriate sentences. This\nlimitation, in turn, compromises the quality of the generated response. To\naddress the challenge of balancing high text quality with robust watermark\ndetection, we propose CoheMark, an advanced sentence-level watermarking\ntechnique that exploits the cohesive relationships between sentences for better\nlogical fluency. The core methodology of CoheMark involves selecting sentences\nthrough trained fuzzy c-means clustering and applying specific next sentence\nselection criteria. Experimental evaluations demonstrate that CoheMark achieves\nstrong watermark strength while exerting minimal impact on text quality.", "AI": {"tldr": "CoheMark\u662f\u4e00\u79cd\u9ad8\u7ea7\u53e5\u5b50\u7ea7\u6c34\u5370\u6280\u672f\uff0c\u901a\u8fc7\u5229\u7528\u53e5\u5b50\u95f4\u7684\u8fde\u8d2f\u5173\u7cfb\u63d0\u5347\u903b\u8f91\u6d41\u7545\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6587\u672c\u8d28\u91cf\u548c\u5f3a\u6c34\u5370\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u53e5\u5b50\u7ea7\u6c34\u5370\u6280\u672f\u4f9d\u8d56\u968f\u610f\u5206\u5272\u6216\u751f\u6210\u8fc7\u7a0b\uff0c\u53ef\u80fd\u9650\u5236\u5408\u9002\u53e5\u5b50\u7684\u53ef\u7528\u6027\uff0c\u4ece\u800c\u5f71\u54cd\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u3002", "method": "CoheMark\u91c7\u7528\u6a21\u7ccac\u5747\u503c\u805a\u7c7b\u9009\u62e9\u53e5\u5b50\uff0c\u5e76\u5e94\u7528\u7279\u5b9a\u4e0b\u4e00\u53e5\u9009\u62e9\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoheMark\u5728\u4fdd\u6301\u9ad8\u6587\u672c\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u6c34\u5370\u5f3a\u5ea6\u3002", "conclusion": "CoheMark\u6709\u6548\u5e73\u8861\u4e86\u6587\u672c\u8d28\u91cf\u4e0e\u6c34\u5370\u68c0\u6d4b\u7684\u6311\u6218\u3002"}}
{"id": "2504.17306", "pdf": "https://arxiv.org/pdf/2504.17306", "abs": "https://arxiv.org/abs/2504.17306", "authors": ["Meher Boulaabi", "Takwa Ben A\u00efcha Gader", "Afef Kacem Echi", "Sameh Mbarek"], "title": "Advanced Segmentation of Diabetic Retinopathy Lesions Using DeepLabv3+", "categories": ["cs.CV", "cs.AI"], "comment": "This work was accepted at the ACS/IEEE International Conference on\n  Computer Systems and Applications (AICCSA) 2024", "summary": "To improve the segmentation of diabetic retinopathy lesions (microaneurysms,\nhemorrhages, exudates, and soft exudates), we implemented a binary segmentation\nmethod specific to each type of lesion. As post-segmentation, we combined the\nindividual model outputs into a single image to better analyze the lesion\ntypes. This approach facilitated parameter optimization and improved accuracy,\neffectively overcoming challenges related to dataset limitations and annotation\ncomplexity. Specific preprocessing steps included cropping and applying\ncontrast-limited adaptive histogram equalization to the L channel of the LAB\nimage. Additionally, we employed targeted data augmentation techniques to\nfurther refine the model's efficacy. Our methodology utilized the DeepLabv3+\nmodel, achieving a segmentation accuracy of 99%. These findings highlight the\nefficacy of innovative strategies in advancing medical image analysis,\nparticularly in the precise segmentation of diabetic retinopathy lesions. The\nIDRID dataset was utilized to validate and demonstrate the robustness of our\napproach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u75c5\u7076\u7684\u4e8c\u5143\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u75c5\u7076\u7c7b\u578b\u7684\u6a21\u578b\u8f93\u51fa\uff0c\u4f18\u5316\u53c2\u6570\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u96c6\u9650\u5236\u548c\u6807\u6ce8\u590d\u6742\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u63d0\u5347\u75c5\u7076\u5206\u5272\u7684\u7cbe\u786e\u5ea6\u3002", "method": "\u91c7\u7528DeepLabv3+\u6a21\u578b\uff0c\u7ed3\u5408\u7279\u5b9a\u9884\u5904\u7406\uff08\u88c1\u526a\u548cCLAHE\uff09\u53ca\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002", "result": "\u5206\u5272\u51c6\u786e\u7387\u8fbe\u523099%\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u521b\u65b0\u7b56\u7565\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u75c5\u7076\u7684\u7cbe\u786e\u5206\u5272\u4e0a\u3002"}}
{"id": "2504.17311", "pdf": "https://arxiv.org/pdf/2504.17311", "abs": "https://arxiv.org/abs/2504.17311", "authors": ["Yulia Otmakhova", "Hung Thinh Truong", "Rahmad Mahendra", "Zenan Zhai", "Rongxin Zhu", "Daniel Beck", "Jey Han Lau"], "title": "FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present FLUKE (Framework for LingUistically-driven and tasK-agnostic\nrobustness Evaluation), a task-agnostic framework for assessing model\nrobustness through systematic minimal variations of test data. FLUKE introduces\ncontrolled variations across linguistic levels - from orthography to dialect\nand style varieties - and leverages large language models (LLMs) with human\nvalidation to generate modifications. We demonstrate FLUKE's utility by\nevaluating both fine-tuned models and LLMs across four diverse NLP tasks, and\nreveal that (1) the impact of linguistic variations is highly task-dependent,\nwith some tests being critical for certain tasks but irrelevant for others; (2)\nwhile LLMs have better overall robustness compared to fine-tuned models, they\nstill exhibit significant brittleness to certain linguistic variations; (3) all\nmodels show substantial vulnerability to negation modifications across most\ntasks. These findings highlight the importance of systematic robustness testing\nfor understanding model behaviors.", "AI": {"tldr": "FLUKE\u662f\u4e00\u4e2a\u4efb\u52a1\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u751f\u6210\u6d4b\u8bd5\u6570\u636e\u7684\u5fae\u5c0f\u53d8\u5316\u6765\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u6db5\u76d6\u4ece\u62fc\u5199\u5230\u65b9\u8a00\u548c\u98ce\u683c\u7684\u591a\u5c42\u6b21\u8bed\u8a00\u53d8\u5316\u3002", "motivation": "\u5f53\u524d\u6a21\u578b\u5728\u8bed\u8a00\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0cFLUKE\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "FLUKE\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u4eba\u5de5\u9a8c\u8bc1\u751f\u6210\u591a\u5c42\u6b21\u7684\u53d7\u63a7\u8bed\u8a00\u53d8\u5316\uff0c\u5e76\u5728\u56db\u4e2aNLP\u4efb\u52a1\u4e2d\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1)\u8bed\u8a00\u53d8\u5316\u7684\u5f71\u54cd\u9ad8\u5ea6\u4f9d\u8d56\u4efb\u52a1\uff1b(2)LLMs\u6574\u4f53\u9c81\u68d2\u6027\u66f4\u5f3a\uff0c\u4f46\u5bf9\u67d0\u4e9b\u53d8\u5316\u4ecd\u8106\u5f31\uff1b(3)\u6240\u6709\u6a21\u578b\u5bf9\u5426\u5b9a\u4fee\u6539\u666e\u904d\u8106\u5f31\u3002", "conclusion": "\u7cfb\u7edf\u6027\u7684\u9c81\u68d2\u6027\u6d4b\u8bd5\u5bf9\u7406\u89e3\u6a21\u578b\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.17315", "pdf": "https://arxiv.org/pdf/2504.17315", "abs": "https://arxiv.org/abs/2504.17315", "authors": ["Zhanglin Wu", "Tengfei Song", "Ning Xie", "Weidong Zhang", "Pengfei Li", "Shuang Wu", "Chong Li", "Junhao Zhu", "Hao Yang"], "title": "DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model", "categories": ["cs.CV", "cs.AI"], "comment": "7 pages, 1 figures, 2 tables", "summary": "This paper presents the technical solution proposed by Huawei Translation\nService Center (HW-TSC) for the \"End-to-End Document Image Machine Translation\nfor Complex Layouts\" competition at the 19th International Conference on\nDocument Analysis and Recognition (DIMT25@ICDAR2025). Leveraging\nstate-of-the-art open-source large vision-language model (LVLM), we introduce a\ntraining framework that combines multi-task learning with perceptual\nchain-of-thought to develop a comprehensive end-to-end document translation\nsystem. During the inference phase, we apply minimum Bayesian decoding and\npost-processing strategies to further enhance the system's translation\ncapabilities. Our solution uniquely addresses both OCR-based and OCR-free\ndocument image translation tasks within a unified framework. This paper\nsystematically details the training methods, inference strategies, LVLM base\nmodels, training data, experimental setups, and results, demonstrating an\neffective approach to document image machine translation.", "AI": {"tldr": "\u534e\u4e3a\u7ffb\u8bd1\u670d\u52a1\u4e2d\u5fc3\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f00\u6e90\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7aef\u5230\u7aef\u6587\u6863\u56fe\u50cf\u7ffb\u8bd1\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u611f\u77e5\u94fe\u5f0f\u601d\u7ef4\uff0c\u652f\u6301OCR\u548c\u65e0OCR\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u5e03\u5c40\u6587\u6863\u56fe\u50cf\u7684\u7aef\u5230\u7aef\u673a\u5668\u7ffb\u8bd1\u95ee\u9898\uff0c\u63d0\u5347\u7ffb\u8bd1\u7cfb\u7edf\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u611f\u77e5\u94fe\u5f0f\u601d\u7ef4\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u6700\u5c0f\u8d1d\u53f6\u65af\u89e3\u7801\u548c\u540e\u5904\u7406\u7b56\u7565\u3002", "result": "\u5c55\u793a\u4e86\u6709\u6548\u7684\u6587\u6863\u56fe\u50cf\u673a\u5668\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u652f\u6301\u7edf\u4e00\u6846\u67b6\u4e0b\u7684OCR\u548c\u65e0OCR\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u590d\u6742\u5e03\u5c40\u6587\u6863\u56fe\u50cf\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17332", "pdf": "https://arxiv.org/pdf/2504.17332", "abs": "https://arxiv.org/abs/2504.17332", "authors": ["Zihan Wang", "Lu Yuan", "Zhengxuan Zhang", "Qing Zhao"], "title": "Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection", "categories": ["cs.CL"], "comment": null, "summary": "In the digital era, social media has become a major conduit for information\ndissemination, yet it also facilitates the rapid spread of misinformation.\nTraditional misinformation detection methods primarily focus on surface-level\nfeatures, overlooking the crucial roles of human empathy in the propagation\nprocess. To address this gap, we propose the Dual-Aspect Empathy Framework\n(DAE), which integrates cognitive and emotional empathy to analyze\nmisinformation from both the creator and reader perspectives. By examining\ncreators' cognitive strategies and emotional appeals, as well as simulating\nreaders' cognitive judgments and emotional responses using Large Language\nModels (LLMs), DAE offers a more comprehensive and human-centric approach to\nmisinformation detection. Moreover, we further introduce an empathy-aware\nfiltering mechanism to enhance response authenticity and diversity.\nExperimental results on benchmark datasets demonstrate that DAE outperforms\nexisting methods, providing a novel paradigm for multimodal misinformation\ndetection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8ba4\u77e5\u4e0e\u60c5\u611f\u5171\u60c5\u7684\u53cc\u65b9\u9762\u5171\u60c5\u6846\u67b6\uff08DAE\uff09\uff0c\u7528\u4e8e\u66f4\u5168\u9762\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u89c6\u4eba\u7c7b\u5171\u60c5\u5728\u4f20\u64ad\u4e2d\u7684\u4f5c\u7528\uff0cDAE\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "DAE\u6574\u5408\u8ba4\u77e5\u4e0e\u60c5\u611f\u5171\u60c5\uff0c\u5206\u6790\u865a\u5047\u4fe1\u606f\u7684\u521b\u4f5c\u8005\u548c\u8bfb\u8005\u89c6\u89d2\uff0c\u5e76\u5f15\u5165\u5171\u60c5\u611f\u77e5\u8fc7\u6ee4\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDAE\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "conclusion": "DAE\u4e3a\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u4eba\u6027\u5316\u548c\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17343", "pdf": "https://arxiv.org/pdf/2504.17343", "abs": "https://arxiv.org/abs/2504.17343", "authors": ["Linli Yao", "Yicheng Li", "Yuancheng Wei", "Lei Li", "Shuhuai Ren", "Yuanxin Liu", "Kun Ouyang", "Lean Wang", "Shicheng Li", "Sida Li", "Lingpeng Kong", "Qi Liu", "Yuanxing Zhang", "Xu Sun"], "title": "TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos", "categories": ["cs.CV"], "comment": null, "summary": "The rapid growth of online video platforms, particularly live streaming\nservices, has created an urgent need for real-time video understanding systems.\nThese systems must process continuous video streams and respond to user queries\ninstantaneously, presenting unique challenges for current Video Large Language\nModels (VideoLLMs). While existing VideoLLMs excel at processing complete\nvideos, they face significant limitations in streaming scenarios due to their\ninability to handle dense, redundant frames efficiently. We introduce\nTimeChat-Online, a novel online VideoLLM that revolutionizes real-time video\ninteraction. At its core lies our innovative Differential Token Drop (DTD)\nmodule, which addresses the fundamental challenge of visual redundancy in\nstreaming videos. Drawing inspiration from human visual perception's Change\nBlindness phenomenon, DTD preserves meaningful temporal changes while filtering\nout static, redundant content between frames. Remarkably, our experiments\ndemonstrate that DTD achieves an 82.8% reduction in video tokens while\nmaintaining 98% performance on StreamingBench, revealing that over 80% of\nvisual content in streaming videos is naturally redundant without requiring\nlanguage guidance. To enable seamless real-time interaction, we present\nTimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse\ninteraction patterns including backward-tracing, current-perception, and\nfuture-responding scenarios. TimeChat-Online's unique Proactive Response\ncapability, naturally achieved through continuous monitoring of video scene\ntransitions via DTD, sets it apart from conventional approaches. Our extensive\nevaluation demonstrates TimeChat-Online's superior performance on streaming\nbenchmarks (StreamingBench and OvOBench) and maintaining competitive results on\nlong-form video tasks such as Video-MME and MLVU.", "AI": {"tldr": "TimeChat-Online\u662f\u4e00\u79cd\u65b0\u578b\u5728\u7ebf\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5dee\u5206\u4ee4\u724c\u4e22\u5f03\uff08DTD\uff09\u6a21\u5757\u9ad8\u6548\u5904\u7406\u5b9e\u65f6\u89c6\u9891\u6d41\uff0c\u51cf\u5c11\u5197\u4f59\u5e27\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u89c6\u9891\u5e73\u53f0\u7684\u5feb\u901f\u589e\u957f\u9700\u8981\u5b9e\u65f6\u89c6\u9891\u7406\u89e3\u7cfb\u7edf\uff0c\u4f46\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6d41\u5a92\u4f53\u573a\u666f\u4e2d\u56e0\u65e0\u6cd5\u9ad8\u6548\u5904\u7406\u5197\u4f59\u5e27\u800c\u53d7\u9650\u3002", "method": "\u63d0\u51faDTD\u6a21\u5757\uff0c\u53d7\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u542f\u53d1\uff0c\u4fdd\u7559\u6709\u610f\u4e49\u7684\u65f6\u95f4\u53d8\u5316\uff0c\u8fc7\u6ee4\u9759\u6001\u5197\u4f59\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDTD\u51cf\u5c1182.8%\u7684\u89c6\u9891\u4ee4\u724c\uff0c\u540c\u65f6\u4fdd\u630198%\u7684\u6027\u80fd\uff0c\u8868\u660e\u6d41\u5a92\u4f53\u89c6\u9891\u4e2d80%\u4ee5\u4e0a\u5185\u5bb9\u5197\u4f59\u3002", "conclusion": "TimeChat-Online\u5728\u6d41\u5a92\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u957f\u89c6\u9891\u4efb\u52a1\u4e2d\u4fdd\u6301\u7ade\u4e89\u529b\u3002"}}
{"id": "2504.17353", "pdf": "https://arxiv.org/pdf/2504.17353", "abs": "https://arxiv.org/abs/2504.17353", "authors": ["Chengguang Gan", "Sunbowen Lee", "Zhixi Cai", "Yanbin Wei", "Lei Zheng", "Yunhao Liang", "Shiwen Ni", "Tatsunori Mori"], "title": "M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction", "categories": ["cs.CL", "cs.CV", "cs.MM"], "comment": null, "summary": "Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection\nof information extraction and model interpretability. MRE aims to leverage the\nmutual understanding between tasks of different granularities, enhancing the\nperformance of both coarse-grained and fine-grained tasks through joint\nmodeling. While MRE has been explored and validated in the textual domain, its\napplicability to visual and multimodal domains remains unexplored. In this\nwork, we extend MRE to the multimodal information extraction domain for the\nfirst time. Specifically, we introduce a new task: Multimodal Mutual\nReinforcement Effect (M-MRE), and construct a corresponding dataset to support\nthis task. To address the challenges posed by M-MRE, we further propose a\nPrompt Format Adapter (PFA) that is fully compatible with various Large\nVision-Language Models (LVLMs). Experimental results demonstrate that MRE can\nalso be observed in the M-MRE task, a multimodal text-image understanding\nscenario. This provides strong evidence that MRE facilitates mutual gains\nacross three interrelated tasks, confirming its generalizability beyond the\ntextual domain.", "AI": {"tldr": "\u8bba\u6587\u5c06\u4e92\u589e\u5f3a\u6548\u5e94\uff08MRE\uff09\u6269\u5c55\u5230\u591a\u6a21\u6001\u9886\u57df\uff0c\u63d0\u51fa\u591a\u6a21\u6001\u4e92\u589e\u5f3a\u6548\u5e94\uff08M-MRE\uff09\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1Prompt Format Adapter\uff08PFA\uff09\u9002\u914d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86MRE\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u4e92\u589e\u5f3a\u6548\u5e94\uff08MRE\uff09\u5728\u591a\u6a21\u6001\u4fe1\u606f\u63d0\u53d6\u9886\u57df\u7684\u9002\u7528\u6027\uff0c\u586b\u8865\u89c6\u89c9\u548c\u591a\u6a21\u6001\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u4e92\u589e\u5f3a\u6548\u5e94\uff08M-MRE\uff09\u4efb\u52a1\uff0c\u6784\u5efa\u76f8\u5e94\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1Prompt Format Adapter\uff08PFA\uff09\u9002\u914d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMRE\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u540c\u6837\u6709\u6548\uff0c\u652f\u6301\u591a\u4efb\u52a1\u95f4\u7684\u4e92\u589e\u5f3a\u3002", "conclusion": "MRE\u5728\u591a\u6a21\u6001\u9886\u57df\u5177\u6709\u666e\u9002\u6027\uff0c\u80fd\u591f\u4fc3\u8fdb\u591a\u4efb\u52a1\u95f4\u7684\u76f8\u4e92\u589e\u76ca\u3002"}}
{"id": "2504.17349", "pdf": "https://arxiv.org/pdf/2504.17349", "abs": "https://arxiv.org/abs/2504.17349", "authors": ["Yiyan Xu", "Wuqiang Zheng", "Wenjie Wang", "Fengbin Zhu", "Xinting Hu", "Yang Zhang", "Fuli Feng", "Tat-Seng Chua"], "title": "DRC: Enhancing Personalized Image Generation via Disentangled Representation Composition", "categories": ["cs.CV", "cs.IR"], "comment": null, "summary": "Personalized image generation has emerged as a promising direction in\nmultimodal content creation. It aims to synthesize images tailored to\nindividual style preferences (e.g., color schemes, character appearances,\nlayout) and semantic intentions (e.g., emotion, action, scene contexts) by\nleveraging user-interacted history images and multimodal instructions. Despite\nnotable progress, existing methods -- whether based on diffusion models, large\nlanguage models, or Large Multimodal Models (LMMs) -- struggle to accurately\ncapture and fuse user style preferences and semantic intentions. In particular,\nthe state-of-the-art LMM-based method suffers from the entanglement of visual\nfeatures, leading to Guidance Collapse, where the generated images fail to\npreserve user-preferred styles or reflect the specified semantics.\n  To address these limitations, we introduce DRC, a novel personalized image\ngeneration framework that enhances LMMs through Disentangled Representation\nComposition. DRC explicitly extracts user style preferences and semantic\nintentions from history images and the reference image, respectively, to form\nuser-specific latent instructions that guide image generation within LMMs.\nSpecifically, it involves two critical learning stages: 1) Disentanglement\nlearning, which employs a dual-tower disentangler to explicitly separate style\nand semantic features, optimized via a reconstruction-driven paradigm with\ndifficulty-aware importance sampling; and 2) Personalized modeling, which\napplies semantic-preserving augmentations to effectively adapt the disentangled\nrepresentations for robust personalized generation. Extensive experiments on\ntwo benchmarks demonstrate that DRC shows competitive performance while\neffectively mitigating the guidance collapse issue, underscoring the importance\nof disentangled representation learning for controllable and effective\npersonalized image generation.", "AI": {"tldr": "DRC\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u8868\u793a\u7ec4\u5408\u63d0\u5347\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u98ce\u683c\u548c\u8bed\u4e49\u610f\u56fe\u878d\u5408\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u6355\u6349\u548c\u878d\u5408\u7528\u6237\u7684\u98ce\u683c\u504f\u597d\u4e0e\u8bed\u4e49\u610f\u56fe\uff0c\u5c24\u5176\u662fLMM\u65b9\u6cd5\u5b58\u5728\u89c6\u89c9\u7279\u5f81\u7ea0\u7f20\u95ee\u9898\u3002", "method": "DRC\u91c7\u7528\u89e3\u8026\u8868\u793a\u7ec4\u5408\uff0c\u5206\u4e24\u9636\u6bb5\u5b66\u4e60\uff1a\u89e3\u8026\u5b66\u4e60\u548c\u4e2a\u6027\u5316\u5efa\u6a21\uff0c\u5206\u522b\u5206\u79bb\u98ce\u683c\u4e0e\u8bed\u4e49\u7279\u5f81\u5e76\u4f18\u5316\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDRC\u5728\u6027\u80fd\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6307\u5bfc\u5d29\u6e83\u95ee\u9898\u3002", "conclusion": "\u89e3\u8026\u8868\u793a\u5b66\u4e60\u5bf9\u53ef\u63a7\u4e14\u6709\u6548\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.17360", "pdf": "https://arxiv.org/pdf/2504.17360", "abs": "https://arxiv.org/abs/2504.17360", "authors": ["Jose G. Moreno", "Jesus Lovon", "M'Rick Robin-Charlet", "Christine Damase-Michel", "Lynda Tamine"], "title": "PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning of Large Language Models (LLMs) has become the default practice\nfor improving model performance on a given task. However, performance\nimprovement comes at the cost of training on vast amounts of annotated data\nwhich could be sensitive leading to significant data privacy concerns. In\nparticular, the healthcare domain is one of the most sensitive domains exposed\nto data privacy issues. In this paper, we present PatientDx, a framework of\nmodel merging that allows the design of effective LLMs for health-predictive\ntasks without requiring fine-tuning nor adaptation on patient data. Our\nproposal is based on recently proposed techniques known as merging of LLMs and\naims to optimize a building block merging strategy. PatientDx uses a pivotal\nmodel adapted to numerical reasoning and tunes hyperparameters on examples\nbased on a performance metric but without training of the LLM on these data.\nExperiments using the mortality tasks of the MIMIC-IV dataset show improvements\nup to 7% in terms of AUROC when compared to initial models. Additionally, we\nconfirm that when compared to fine-tuned models, our proposal is less prone to\ndata leak problems without hurting performance. Finally, we qualitatively show\nthe capabilities of our proposal through a case study. Our best model is\npublicly available at https://huggingface.co/ Jgmorenof/mistral\\_merged\\_0\\_4.", "AI": {"tldr": "PatientDx\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u6280\u672f\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u63d0\u5347LLM\u5728\u533b\u7597\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u907f\u514d\u6570\u636e\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3LLM\u5fae\u8c03\u9700\u8981\u5927\u91cf\u654f\u611f\u6570\u636e\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u9886\u57df\uff0c\u6570\u636e\u9690\u79c1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u57fa\u4e8e\u6a21\u578b\u5408\u5e76\u6280\u672f\uff0c\u4f18\u5316\u6784\u5efa\u5757\u5408\u5e76\u7b56\u7565\uff0c\u4f7f\u7528\u6570\u503c\u63a8\u7406\u6a21\u578b\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\uff0cAUROC\u63d0\u53477%\uff0c\u4e14\u6bd4\u5fae\u8c03\u6a21\u578b\u66f4\u5c11\u51fa\u73b0\u6570\u636e\u6cc4\u9732\u95ee\u9898\u3002", "conclusion": "PatientDx\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9690\u79c1\u5b89\u5168\u7684LLM\u4f18\u5316\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u533b\u7597\u9884\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2504.17364", "pdf": "https://arxiv.org/pdf/2504.17364", "abs": "https://arxiv.org/abs/2504.17364", "authors": ["Ali Haider", "Muhammad Salman Ali", "Maryam Qamar", "Tahir Khalil", "Soo Ye Kim", "Jihyong Oh", "Enzo Tartaglione", "Sung-Ho Bae"], "title": "I-INR: Iterative Implicit Neural Representations", "categories": ["cs.CV"], "comment": null, "summary": "Implicit Neural Representations (INRs) have revolutionized signal processing\nand computer vision by modeling signals as continuous, differentiable functions\nparameterized by neural networks. However, their inherent formulation as a\nregression problem makes them prone to regression to the mean, limiting their\nability to capture fine details, retain high-frequency information, and handle\nnoise effectively. To address these challenges, we propose Iterative Implicit\nNeural Representations (I-INRs) a novel plug-and-play framework that enhances\nsignal reconstruction through an iterative refinement process. I-INRs\neffectively recover high-frequency details, improve robustness to noise, and\nachieve superior reconstruction quality. Our framework seamlessly integrates\nwith existing INR architectures, delivering substantial performance gains\nacross various tasks. Extensive experiments show that I-INRs outperform\nbaseline methods, including WIRE, SIREN, and Gauss, in diverse computer vision\napplications such as image restoration, image denoising, and object occupancy\nprediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aI-INRs\u7684\u8fed\u4ee3\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7ec6\u5316\u63d0\u5347\u4fe1\u53f7\u91cd\u5efa\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfINRs\u5728\u7ec6\u8282\u4fdd\u7559\u548c\u9ad8\u9891\u4fe1\u606f\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INRs\uff09\u56e0\u56de\u5f52\u95ee\u9898\u7684\u56fa\u6709\u7279\u6027\uff0c\u5bb9\u6613\u56de\u5f52\u5747\u503c\uff0c\u96be\u4ee5\u6355\u6349\u7ec6\u8282\u548c\u9ad8\u9891\u4fe1\u606f\uff0c\u4e14\u5bf9\u566a\u58f0\u654f\u611f\u3002", "method": "\u63d0\u51faI-INRs\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7ec6\u5316\u8fc7\u7a0b\u589e\u5f3a\u4fe1\u53f7\u91cd\u5efa\uff0c\u517c\u5bb9\u73b0\u6709INRs\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cI-INRs\u5728\u56fe\u50cf\u6062\u590d\u3001\u53bb\u566a\u548c\u7269\u4f53\u5360\u636e\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982WIRE\u3001SIREN\u548cGauss\uff09\u3002", "conclusion": "I-INRs\u663e\u8457\u63d0\u5347\u4e86\u4fe1\u53f7\u91cd\u5efa\u8d28\u91cf\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.17366", "pdf": "https://arxiv.org/pdf/2504.17366", "abs": "https://arxiv.org/abs/2504.17366", "authors": ["Yongxuan Wu", "Runyu Chen", "Peiyu Liu", "Hongjin Qian"], "title": "LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context understanding poses significant challenges in natural language\nprocessing, particularly for real-world dialogues characterized by speech-based\nelements, high redundancy, and uneven information density. Although large\nlanguage models (LLMs) achieve impressive results on existing benchmarks, these\ndatasets fail to reflect the complexities of such texts, limiting their\napplicability to practical scenarios. To bridge this gap, we construct the\nfirst spoken long-text dataset, derived from live streams, designed to reflect\nthe redundancy-rich and conversational nature of real-world scenarios. We\nconstruct tasks in three categories: retrieval-dependent, reasoning-dependent,\nand hybrid. We then evaluate both popular LLMs and specialized methods to\nassess their ability to understand long-contexts in these tasks. Our results\nshow that current methods exhibit strong task-specific preferences and perform\npoorly on highly redundant inputs, with no single method consistently\noutperforming others. We propose a new baseline that better handles redundancy\nin spoken text and achieves strong performance across tasks. Our findings\nhighlight key limitations of current methods and suggest future directions for\nimproving long-context understanding. Finally, our benchmark fills a gap in\nevaluating long-context spoken language understanding and provides a practical\nfoundation for developing real-world e-commerce systems. The code and benchmark\nare available at https://github.com/Yarayx/livelongbench.", "AI": {"tldr": "\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u76f4\u64ad\u7684\u5197\u4f59\u4e30\u5bcc\u7684\u53e3\u8bed\u957f\u6587\u672c\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u5bf9\u8bdd\u7684\u590d\u6742\u6027\uff0c\u9650\u5236\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6784\u5efa\u4e86\u9996\u4e2a\u53e3\u8bed\u957f\u6587\u672c\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u4e86\u68c0\u7d22\u4f9d\u8d56\u3001\u63a8\u7406\u4f9d\u8d56\u548c\u6df7\u5408\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e86\u6d41\u884cLLM\u548c\u4e13\u7528\u65b9\u6cd5\u3002", "result": "\u73b0\u6709\u65b9\u6cd5\u5728\u5197\u4f59\u8f93\u5165\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u65b0\u57fa\u7ebf\u65b9\u6cd5\u5728\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u6539\u8fdb\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u540c\u65f6\u586b\u8865\u4e86\u53e3\u8bed\u957f\u6587\u672c\u8bc4\u4f30\u7684\u7a7a\u767d\u3002"}}
{"id": "2504.17365", "pdf": "https://arxiv.org/pdf/2504.17365", "abs": "https://arxiv.org/abs/2504.17365", "authors": ["Ling You", "Wenxuan Huang", "Xinni Xie", "Xiangyi Wei", "Bangyan Li", "Shaohui Lin", "Yang Li", "Changbo Wang"], "title": "TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Soccer is a globally popular sporting event, typically characterized by long\nmatches and distinctive highlight moments. Recent advances in Multimodal Large\nLanguage Models (MLLMs) offer promising capabilities in temporal grounding and\nvideo understanding, soccer commentary generation often requires precise\ntemporal localization and semantically rich descriptions over long-form video.\nHowever, existing soccer MLLMs often rely on the temporal a priori for caption\ngeneration, so they cannot process the soccer video end-to-end. While some\ntraditional approaches follow a two-step paradigm that is complex and fails to\ncapture the global context to achieve suboptimal performance. To solve the\nabove issues, we present TimeSoccer, the first end-to-end soccer MLLM for\nSingle-anchor Dense Video Captioning (SDVC) in full-match soccer videos.\nTimeSoccer jointly predicts timestamps and generates captions in a single pass,\nenabling global context modeling across 45-minute matches. To support long\nvideo understanding of soccer matches, we introduce MoFA-Select, a\ntraining-free, motion-aware frame compression module that adaptively selects\nrepresentative frames via a coarse-to-fine strategy, and incorporates\ncomplementary training paradigms to strengthen the model's ability to handle\nlong temporal sequences. Extensive experiments demonstrate that our TimeSoccer\nachieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end\nform, generating high-quality commentary with accurate temporal alignment and\nstrong semantic relevance.", "AI": {"tldr": "TimeSoccer\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u7684\u8db3\u7403MLLM\uff0c\u7528\u4e8e\u5168\u573a\u6bd4\u8d5b\u89c6\u9891\u7684\u5355\u951a\u70b9\u5bc6\u96c6\u89c6\u9891\u5b57\u5e55\u751f\u6210\uff08SDVC\uff09\uff0c\u901a\u8fc7\u8054\u5408\u9884\u6d4b\u65f6\u95f4\u6233\u548c\u751f\u6210\u5b57\u5e55\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u8db3\u7403MLLM\u4f9d\u8d56\u65f6\u95f4\u5148\u9a8c\u751f\u6210\u5b57\u5e55\uff0c\u65e0\u6cd5\u7aef\u5230\u7aef\u5904\u7406\u89c6\u9891\uff1b\u4f20\u7edf\u65b9\u6cd5\u590d\u6742\u4e14\u65e0\u6cd5\u6355\u6349\u5168\u5c40\u4e0a\u4e0b\u6587\u3002TimeSoccer\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faTimeSoccer\uff0c\u7ed3\u5408MoFA-Select\uff08\u8bad\u7ec3\u65e0\u5173\u7684\u8fd0\u52a8\u611f\u77e5\u5e27\u538b\u7f29\u6a21\u5757\uff09\u548c\u4e92\u8865\u8bad\u7ec3\u8303\u5f0f\uff0c\u652f\u6301\u957f\u89c6\u9891\u7406\u89e3\u3002", "result": "TimeSoccer\u5728SDVC\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u8bc4\u8bba\uff0c\u65f6\u95f4\u5bf9\u9f50\u51c6\u786e\u4e14\u8bed\u4e49\u76f8\u5173\u6027\u5f3a\u3002", "conclusion": "TimeSoccer\u4e3a\u8db3\u7403\u89c6\u9891\u7684\u7aef\u5230\u7aef\u5b57\u5e55\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2504.17390", "pdf": "https://arxiv.org/pdf/2504.17390", "abs": "https://arxiv.org/abs/2504.17390", "authors": ["Jihyun Lee", "Yejin Jeon", "Seungyeon Seo", "Gary Geunbae Lee"], "title": "PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona", "categories": ["cs.CL"], "comment": "Accepted in NAACL 2025 main", "summary": "Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests\nthrough natural language interactions, yet existing systems often produce\ngeneric, monotonic responses that lack individuality and fail to adapt to\nusers' personal attributes. To address this, we introduce PicPersona-TOD, a\nnovel dataset that incorporates user images as part of the persona, enabling\npersonalized responses tailored to user-specific factors such as age or\nemotional context. This is facilitated by first impressions, dialogue\npolicy-guided prompting, and the use of external knowledge to reduce\nhallucinations. Human evaluations confirm that our dataset enhances user\nexperience, with personalized responses contributing to a more engaging\ninteraction. Additionally, we introduce a new NLG model, Pictor, which not only\npersonalizes responses, but also demonstrates robust performance across unseen\ndomains https://github.com/JihyunLee1/PicPersona.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PicPersona-TOD\u6570\u636e\u96c6\u548cPictor\u6a21\u578b\uff0c\u901a\u8fc7\u7528\u6237\u56fe\u50cf\u5b9e\u73b0\u4e2a\u6027\u5316\u5bf9\u8bdd\u54cd\u5e94\uff0c\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u5e38\u751f\u6210\u5355\u8c03\u3001\u901a\u7528\u7684\u54cd\u5e94\uff0c\u7f3a\u4e4f\u4e2a\u6027\u5316\u548c\u5bf9\u7528\u6237\u5c5e\u6027\u7684\u9002\u5e94\u3002", "method": "\u7ed3\u5408\u7528\u6237\u56fe\u50cf\u4f5c\u4e3a\u4eba\u7269\u8bbe\u5b9a\uff0c\u5229\u7528\u7b2c\u4e00\u5370\u8c61\u3001\u5bf9\u8bdd\u7b56\u7565\u5f15\u5bfc\u63d0\u793a\u548c\u5916\u90e8\u77e5\u8bc6\u51cf\u5c11\u5e7b\u89c9\uff0c\u6784\u5efaPicPersona-TOD\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1Pictor\u6a21\u578b\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u8bc1\u5b9e\u4e2a\u6027\u5316\u54cd\u5e94\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\uff0cPictor\u6a21\u578b\u5728\u672a\u89c1\u9886\u57df\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "PicPersona-TOD\u548cPictor\u6a21\u578b\u6709\u6548\u5b9e\u73b0\u4e86\u4e2a\u6027\u5316\u5bf9\u8bdd\uff0c\u589e\u5f3a\u4e86\u4ea4\u4e92\u4f53\u9a8c\u3002"}}
{"id": "2504.17371", "pdf": "https://arxiv.org/pdf/2504.17371", "abs": "https://arxiv.org/abs/2504.17371", "authors": ["Oussema Dhaouadi", "Johannes Meier", "Luca Wahl", "Jacques Kaiser", "Luca Scalerandi", "Nick Wandelburg", "Zhuolun Zhou", "Nijanthan Berinpanathan", "Holger Banzhaf", "Daniel Cremers"], "title": "Highly Accurate and Diverse Traffic Data: The DeepScenario Open 3D Dataset", "categories": ["cs.CV"], "comment": null, "summary": "Accurate 3D trajectory data is crucial for advancing autonomous driving. Yet,\ntraditional datasets are usually captured by fixed sensors mounted on a car and\nare susceptible to occlusion. Additionally, such an approach can precisely\nreconstruct the dynamic environment in the close vicinity of the measurement\nvehicle only, while neglecting objects that are further away. In this paper, we\nintroduce the DeepScenario Open 3D Dataset (DSC3D), a high-quality,\nocclusion-free dataset of 6 degrees of freedom bounding box trajectories\nacquired through a novel monocular camera drone tracking pipeline. Our dataset\nincludes more than 175,000 trajectories of 14 types of traffic participants and\nsignificantly exceeds existing datasets in terms of diversity and scale,\ncontaining many unprecedented scenarios such as complex vehicle-pedestrian\ninteraction on highly populated urban streets and comprehensive parking\nmaneuvers from entry to exit. DSC3D dataset was captured in five various\nlocations in Europe and the United States and include: a parking lot, a crowded\ninner-city, a steep urban intersection, a federal highway, and a suburban\nintersection. Our 3D trajectory dataset aims to enhance autonomous driving\nsystems by providing detailed environmental 3D representations, which could\nlead to improved obstacle interactions and safety. We demonstrate its utility\nacross multiple applications including motion prediction, motion planning,\nscenario mining, and generative reactive traffic agents. Our interactive online\nvisualization platform and the complete dataset are publicly available at\napp.deepscenario.com, facilitating research in motion prediction, behavior\nmodeling, and safety validation.", "AI": {"tldr": "DSC3D\u662f\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u65e0\u906e\u6321\u76843D\u8f68\u8ff9\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u65e0\u4eba\u673a\u6355\u6349\uff0c\u8986\u76d6\u591a\u6837\u573a\u666f\uff0c\u65e8\u5728\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u96c6\u56e0\u56fa\u5b9a\u4f20\u611f\u5668\u548c\u906e\u6321\u95ee\u9898\u53d7\u9650\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u52a8\u6001\u73af\u5883\u3002", "method": "\u91c7\u7528\u5355\u76ee\u76f8\u673a\u65e0\u4eba\u673a\u8ffd\u8e2a\u7ba1\u9053\uff0c\u91c7\u96c614\u7c7b\u4ea4\u901a\u53c2\u4e0e\u8005\u7684175,000\u591a\u6761\u8f68\u8ff9\u3002", "result": "\u6570\u636e\u96c6\u5728\u591a\u6837\u6027\u548c\u89c4\u6a21\u4e0a\u8d85\u8d8a\u73b0\u6709\u6570\u636e\u96c6\uff0c\u5305\u542b\u590d\u6742\u573a\u666f\u5982\u9ad8\u5bc6\u5ea6\u57ce\u5e02\u8857\u9053\u4ea4\u4e92\u3002", "conclusion": "DSC3D\u901a\u8fc7\u8be6\u7ec63D\u73af\u5883\u8868\u5f81\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u9879\u5e94\u7528\u7814\u7a76\uff0c\u6570\u636e\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2504.17445", "pdf": "https://arxiv.org/pdf/2504.17445", "abs": "https://arxiv.org/abs/2504.17445", "authors": ["Anna Lieb", "Maneesh Arora", "Eni Mustafaraj"], "title": "Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation", "categories": ["cs.CL"], "comment": "Presented at IC2S2 2024 in Philadelphia, USA", "summary": "Unsupervised machine learning techniques, such as topic modeling and\nclustering, are often used to identify latent patterns in unstructured text\ndata in fields such as political science and sociology. These methods overcome\ncommon concerns about reproducibility and costliness involved in the\nlabor-intensive process of human qualitative analysis. However, two major\nlimitations of topic models are their interpretability and their practicality\nfor answering targeted, domain-specific social science research questions. In\nthis work, we investigate opportunities for using LLM-generated text\naugmentation to improve the usefulness of topic modeling output. We use a\npolitical science case study to evaluate our results in a domain-specific\napplication, and find that topic modeling using GPT-4 augmentations creates\nhighly interpretable categories that can be used to investigate domain-specific\nresearch questions with minimal human guidance.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528LLM\u751f\u6210\u6587\u672c\u589e\u5f3a\u4e3b\u9898\u6a21\u578b\u7684\u5b9e\u7528\u6027\uff0c\u901a\u8fc7\u653f\u6cbb\u5b66\u6848\u4f8b\u9a8c\u8bc1\u4e86GPT-4\u589e\u5f3a\u7684\u4e3b\u9898\u6a21\u578b\u80fd\u751f\u6210\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u7c7b\u522b\u3002", "motivation": "\u89e3\u51b3\u4e3b\u9898\u6a21\u578b\u5728\u89e3\u91ca\u6027\u548c\u9488\u5bf9\u7279\u5b9a\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u95ee\u9898\u5b9e\u7528\u6027\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528LLM\uff08\u5982GPT-4\uff09\u751f\u6210\u6587\u672c\u589e\u5f3a\u4e3b\u9898\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u653f\u6cbb\u5b66\u6848\u4f8b\u8bc4\u4f30\u6548\u679c\u3002", "result": "GPT-4\u589e\u5f3a\u7684\u4e3b\u9898\u6a21\u578b\u751f\u6210\u4e86\u9ad8\u5ea6\u53ef\u89e3\u91ca\u7684\u7c7b\u522b\uff0c\u9002\u7528\u4e8e\u7279\u5b9a\u9886\u57df\u7814\u7a76\u95ee\u9898\u3002", "conclusion": "LLM\u751f\u6210\u7684\u6587\u672c\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u9898\u6a21\u578b\u7684\u5b9e\u7528\u6027\u548c\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u3002"}}
{"id": "2504.17395", "pdf": "https://arxiv.org/pdf/2504.17395", "abs": "https://arxiv.org/abs/2504.17395", "authors": ["Yiming Zhao", "Guorong Li", "Laiyun Qing", "Amin Beheshti", "Jian Yang", "Michael Sheng", "Yuankai Qi", "Qingming Huang"], "title": "SDVPT: Semantic-Driven Visual Prompt Tuning for Open-World Object Counting", "categories": ["cs.CV"], "comment": null, "summary": "Open-world object counting leverages the robust text-image alignment of\npre-trained vision-language models (VLMs) to enable counting of arbitrary\ncategories in images specified by textual queries. However, widely adopted\nnaive fine-tuning strategies concentrate exclusively on text-image consistency\nfor categories contained in training, which leads to limited generalizability\nfor unseen categories. In this work, we propose a plug-and-play Semantic-Driven\nVisual Prompt Tuning framework (SDVPT) that transfers knowledge from the\ntraining set to unseen categories with minimal overhead in parameters and\ninference time. First, we introduce a two-stage visual prompt learning strategy\ncomposed of Category-Specific Prompt Initialization (CSPI) and Topology-Guided\nPrompt Refinement (TGPR). The CSPI generates category-specific visual prompts,\nand then TGPR distills latent structural patterns from the VLM's text encoder\nto refine these prompts. During inference, we dynamically synthesize the visual\nprompts for unseen categories based on the semantic correlation between unseen\nand training categories, facilitating robust text-image alignment for unseen\ncategories. Extensive experiments integrating SDVPT with all available\nopen-world object counting models demonstrate its effectiveness and\nadaptability across three widely used datasets: FSC-147, CARPK, and PUCPR+.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSDVPT\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u89c6\u89c9\u63d0\u793a\u5b66\u4e60\u7b56\u7565\u63d0\u5347\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u4e16\u754c\u7269\u4f53\u8ba1\u6570\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u4ec5\u5173\u6ce8\u6587\u672c-\u56fe\u50cf\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u5bf9\u672a\u89c1\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "SDVPT\u6846\u67b6\u5305\u542b\u7c7b\u522b\u7279\u5b9a\u63d0\u793a\u521d\u59cb\u5316\uff08CSPI\uff09\u548c\u62d3\u6251\u5f15\u5bfc\u63d0\u793a\u7ec6\u5316\uff08TGPR\uff09\u4e24\u9636\u6bb5\u7b56\u7565\uff0c\u52a8\u6001\u5408\u6210\u672a\u89c1\u7c7b\u522b\u7684\u89c6\u89c9\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSDVPT\u5728FSC-147\u3001CARPK\u548cPUCPR+\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "SDVPT\u901a\u8fc7\u8bed\u4e49\u9a71\u52a8\u7684\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u4e16\u754c\u7269\u4f53\u8ba1\u6570\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.17480", "pdf": "https://arxiv.org/pdf/2504.17480", "abs": "https://arxiv.org/abs/2504.17480", "authors": ["Xin Yi", "Shunfan Zhengc", "Linlin Wanga", "Xiaoling Wang", "Liang He"], "title": "Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Watermarking has emerged as a critical technique for combating misinformation\nand protecting intellectual property in large language models (LLMs). A recent\ndiscovery, termed watermark radioactivity, reveals that watermarks embedded in\nteacher models can be inherited by student models through knowledge\ndistillation. On the positive side, this inheritance allows for the detection\nof unauthorized knowledge distillation by identifying watermark traces in\nstudent models. However, the robustness of watermarks against scrubbing attacks\nand their unforgeability in the face of spoofing attacks under unauthorized\nknowledge distillation remain largely unexplored. Existing watermark attack\nmethods either assume access to model internals or fail to simultaneously\nsupport both scrubbing and spoofing attacks. In this work, we propose\nContrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified\nframework that enables bidirectional attacks under unauthorized knowledge\ndistillation. Our approach employs contrastive decoding to extract corrupted or\namplified watermark texts via comparing outputs from the student model and\nweakly watermarked references, followed by bidirectional distillation to train\nnew student models capable of watermark removal and watermark forgery,\nrespectively. Extensive experiments show that CDG-KD effectively performs\nattacks while preserving the general performance of the distilled model. Our\nfindings underscore critical need for developing watermarking schemes that are\nrobust and unforgeable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDG-KD\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u672a\u7ecf\u6388\u6743\u7684\u77e5\u8bc6\u84b8\u998f\u4e2d\u8fdb\u884c\u53cc\u5411\u653b\u51fb\uff08\u64e6\u9664\u548c\u4f2a\u9020\u6c34\u5370\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u901a\u7528\u6027\u80fd\u3002", "motivation": "\u6c34\u5370\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7528\u4e8e\u5bf9\u6297\u9519\u8bef\u4fe1\u606f\u548c\u4fdd\u62a4\u77e5\u8bc6\u4ea7\u6743\uff0c\u4f46\u5176\u5728\u672a\u7ecf\u6388\u6743\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u4e0d\u53ef\u4f2a\u9020\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u5bf9\u6bd4\u89e3\u7801\u4ece\u5b66\u751f\u6a21\u578b\u4e2d\u63d0\u53d6\u88ab\u7834\u574f\u6216\u653e\u5927\u7684\u6c34\u5370\u6587\u672c\uff0c\u5e76\u901a\u8fc7\u53cc\u5411\u84b8\u998f\u8bad\u7ec3\u65b0\u7684\u5b66\u751f\u6a21\u578b\uff0c\u5206\u522b\u5b9e\u73b0\u6c34\u5370\u64e6\u9664\u548c\u4f2a\u9020\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCDG-KD\u80fd\u6709\u6548\u6267\u884c\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u84b8\u998f\u6a21\u578b\u7684\u901a\u7528\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5f00\u53d1\u9c81\u68d2\u4e14\u4e0d\u53ef\u4f2a\u9020\u7684\u6c34\u5370\u65b9\u6848\u7684\u8feb\u5207\u9700\u6c42\u3002"}}
{"id": "2504.17397", "pdf": "https://arxiv.org/pdf/2504.17397", "abs": "https://arxiv.org/abs/2504.17397", "authors": ["Francesc Marti-Escofet", "Benedikt Blumenstiel", "Linus Scheibenreif", "Paolo Fraccaro", "Konrad Schindler"], "title": "Fine-tune Smarter, Not Harder: Parameter-Efficient Fine-Tuning for Geospatial Foundation Models", "categories": ["cs.CV"], "comment": "Code available at https://github.com/IBM/peft-geofm", "summary": "Earth observation (EO) is crucial for monitoring environmental changes,\nresponding to disasters, and managing natural resources. In this context,\nfoundation models facilitate remote sensing image analysis to retrieve relevant\ngeoinformation accurately and efficiently. However, as these models grow in\nsize, fine-tuning becomes increasingly challenging due to the associated\ncomputational resources and costs, limiting their accessibility and\nscalability. Furthermore, full fine-tuning can lead to forgetting pre-trained\nfeatures and even degrade model generalization. To address this,\nParameter-Efficient Fine-Tuning (PEFT) techniques offer a promising solution.\nIn this paper, we conduct extensive experiments with various foundation model\narchitectures and PEFT techniques to evaluate their effectiveness on five\ndifferent EO datasets. Our results provide a comprehensive comparison, offering\ninsights into when and how PEFT methods support the adaptation of pre-trained\ngeospatial models. We demonstrate that PEFT techniques match or even exceed\nfull fine-tuning performance and enhance model generalisation to unseen\ngeographic regions, while reducing training time and memory requirements.\nAdditional experiments investigate the effect of architecture choices such as\nthe decoder type or the use of metadata, suggesting UNet decoders and\nfine-tuning without metadata as the recommended configuration. We have\nintegrated all evaluated foundation models and techniques into the open-source\npackage TerraTorch to support quick, scalable, and cost-effective model\nadaptation.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672f\u5728\u5730\u7403\u89c2\u6d4b\uff08EO\uff09\u9886\u57df\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u7684\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8d8a\u4f20\u7edf\u5fae\u8c03\u6027\u80fd\u7684\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u89c4\u6a21\u7684\u589e\u5927\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u9762\u4e34\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u3001\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4e14\u53ef\u80fd\u5bfc\u81f4\u9884\u8bad\u7ec3\u7279\u5f81\u9057\u5fd8\u548c\u6cdb\u5316\u80fd\u529b\u4e0b\u964d\u3002PEFT\u6280\u672f\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u901a\u8fc7\u591a\u79cd\u57fa\u7840\u6a21\u578b\u67b6\u6784\u548cPEFT\u6280\u672f\u5728\u4e94\u4e2a\u4e0d\u540cEO\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u5176\u6709\u6548\u6027\uff0c\u5e76\u63a2\u8ba8\u67b6\u6784\u9009\u62e9\uff08\u5982\u89e3\u7801\u5668\u7c7b\u578b\u548c\u5143\u6570\u636e\u4f7f\u7528\uff09\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPEFT\u6280\u672f\u4e0d\u4ec5\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u4f20\u7edf\u5fae\u8c03\u6027\u80fd\uff0c\u8fd8\u80fd\u589e\u5f3a\u6a21\u578b\u5bf9\u672a\u89c1\u5730\u7406\u533a\u57df\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u9700\u6c42\u3002UNet\u89e3\u7801\u5668\u548c\u4e0d\u4f7f\u7528\u5143\u6570\u636e\u7684\u914d\u7f6e\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "PEFT\u6280\u672f\u4e3aEO\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6a21\u578b\u5fae\u8c03\u65b9\u6848\uff0c\u76f8\u5173\u6a21\u578b\u548c\u6280\u672f\u5df2\u96c6\u6210\u5230\u5f00\u6e90\u5de5\u5177TerraTorch\u4e2d\uff0c\u652f\u6301\u5feb\u901f\u3001\u4f4e\u6210\u672c\u7684\u6a21\u578b\u9002\u914d\u3002"}}
{"id": "2504.17550", "pdf": "https://arxiv.org/pdf/2504.17550", "abs": "https://arxiv.org/abs/2504.17550", "authors": ["Yejin Bang", "Ziwei Ji", "Alan Schelten", "Anthony Hartshorn", "Tara Fowler", "Cheng Zhang", "Nicola Cancedda", "Pascale Fung"], "title": "HalluLens: LLM Hallucination Benchmark", "categories": ["cs.CL", "cs.AI"], "comment": "42 pages", "summary": "Large language models (LLMs) often generate responses that deviate from user\ninput or training data, a phenomenon known as \"hallucination.\" These\nhallucinations undermine user trust and hinder the adoption of generative AI\nsystems. Addressing hallucinations is essential for the advancement of LLMs.\nThis paper introduces a comprehensive hallucination benchmark, incorporating\nboth new extrinsic and existing intrinsic evaluation tasks, built upon clear\ntaxonomy of hallucination. A major challenge in benchmarking hallucinations is\nthe lack of a unified framework due to inconsistent definitions and\ncategorizations. We disentangle LLM hallucination from \"factuality,\" proposing\na clear taxonomy that distinguishes between extrinsic and intrinsic\nhallucinations, to promote consistency and facilitate research. Extrinsic\nhallucinations, where the generated content is not consistent with the training\ndata, are increasingly important as LLMs evolve. Our benchmark includes dynamic\ntest set generation to mitigate data leakage and ensure robustness against such\nleakage. We also analyze existing benchmarks, highlighting their limitations\nand saturation. The work aims to: (1) establish a clear taxonomy of\nhallucinations, (2) introduce new extrinsic hallucination tasks, with data that\ncan be dynamically regenerated to prevent saturation by leakage, (3) provide a\ncomprehensive analysis of existing benchmarks, distinguishing them from\nfactuality evaluations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5e7b\u89c9\u57fa\u51c6\uff0c\u901a\u8fc7\u65b0\u5b9a\u4e49\u7684\u5916\u5728\u548c\u5185\u5728\u8bc4\u4f30\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86LLM\u751f\u6210\u5185\u5bb9\u504f\u79bb\u7528\u6237\u8f93\u5165\u6216\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\u3002", "motivation": "LLM\u751f\u6210\u7684\u5e7b\u89c9\u5185\u5bb9\u635f\u5bb3\u7528\u6237\u4fe1\u4efb\u5e76\u963b\u788d\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u91c7\u7528\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u4ee5\u63a8\u52a8LLM\u53d1\u5c55\u3002", "method": "\u8bba\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6e05\u6670\u5206\u7c7b\u7684\u5e7b\u89c9\u57fa\u51c6\uff0c\u5305\u62ec\u52a8\u6001\u6d4b\u8bd5\u96c6\u751f\u6210\u4ee5\u9632\u6b62\u6570\u636e\u6cc4\u6f0f\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709\u57fa\u51c6\u7684\u5c40\u9650\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u533a\u5206\u5916\u5728\u548c\u5185\u5728\u5e7b\u89c9\u7684\u6e05\u6670\u5206\u7c7b\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u52a8\u6001\u751f\u6210\u6d4b\u8bd5\u96c6\u7684\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u57fa\u51c6\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aLLM\u5e7b\u89c9\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6846\u67b6\u548c\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u672a\u6765\u7814\u7a76\u7684\u8fdb\u5c55\u3002"}}
{"id": "2504.17399", "pdf": "https://arxiv.org/pdf/2504.17399", "abs": "https://arxiv.org/abs/2504.17399", "authors": ["Sven Teufel", "J\u00f6rg Gamerdinger", "Oliver Bringmann"], "title": "S2S-Net: Addressing the Domain Gap of Heterogeneous Sensor Systems in LiDAR-Based Collective Perception", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Collective Perception (CP) has emerged as a promising approach to overcome\nthe limitations of individual perception in the context of autonomous driving.\nVarious approaches have been proposed to realize collective perception;\nhowever, the Sensor2Sensor domain gap that arises from the utilization of\ndifferent sensor systems in Connected and Automated Vehicles (CAVs) remains\nmostly unaddressed. This is primarily due to the paucity of datasets containing\nheterogeneous sensor setups among the CAVs. The recently released SCOPE\ndatasets address this issue by providing data from three different LiDAR\nsensors for each CAV. This study is the first to tackle the Sensor2Sensor\ndomain gap in vehicle to vehicle (V2V) collective perception. First, we present\nour sensor-domain robust architecture S2S-Net. Then an in-depth analysis of the\nSensor2Sensor domain adaptation capabilities of S2S-Net on the SCOPE dataset is\nconducted. S2S-Net demonstrates the capability to maintain very high\nperformance in unseen sensor domains and achieved state-of-the-art results on\nthe SCOPE dataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aS2S-Net\u7684\u4f20\u611f\u5668\u57df\u9c81\u68d2\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u8f66\u8f86\u95f4\u96c6\u4f53\u611f\u77e5\u4e2d\u7684Sensor2Sensor\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u5e76\u5728SCOPE\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u96c6\u4f53\u611f\u77e5\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4e0d\u540c\u4f20\u611f\u5668\u7cfb\u7edf\u5bfc\u81f4\u7684Sensor2Sensor\u57df\u5dee\u8ddd\u95ee\u9898\u5c1a\u672a\u89e3\u51b3\u3002\u7f3a\u4e4f\u5f02\u6784\u4f20\u611f\u5668\u8bbe\u7f6e\u7684\u6570\u636e\u96c6\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51faS2S-Net\u67b6\u6784\uff0c\u5e76\u5728SCOPE\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6df1\u5165\u7684Sensor2Sensor\u57df\u9002\u5e94\u80fd\u529b\u5206\u6790\u3002", "result": "S2S-Net\u5728\u672a\u89c1\u8fc7\u7684\u4f20\u611f\u5668\u57df\u4e2d\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\uff0c\u5e76\u5728SCOPE\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "S2S-Net\u6709\u6548\u89e3\u51b3\u4e86Sensor2Sensor\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u4e3a\u8f66\u8f86\u95f4\u96c6\u4f53\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17562", "pdf": "https://arxiv.org/pdf/2504.17562", "abs": "https://arxiv.org/abs/2504.17562", "authors": ["Rei Higuchi", "Ryotaro Kawata", "Naoki Nishikawa", "Kazusato Oko", "Shoichiro Yamaguchi", "Sosuke Kobayashi", "Seiya Tokui", "Kohei Hayashi", "Daisuke Okanohara", "Taiji Suzuki"], "title": "When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The ability to acquire latent semantics is one of the key properties that\ndetermines the performance of language models. One convenient approach to\ninvoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at\nthe beginning of texts in the pre-training data, making it easier for the model\nto access latent semantics before observing the entire text. Previous studies\nhave reported that this technique actually improves the performance of trained\nmodels in downstream tasks; however, this improvement has been observed only in\nspecific downstream tasks, without consistent enhancement in average next-token\nprediction loss. To understand this phenomenon, we closely investigate how\nprepending metadata during pre-training affects model performance by examining\nits behavior using artificial data. Interestingly, we found that this approach\nproduces both positive and negative effects on the downstream tasks. We\ndemonstrate that the effectiveness of the approach depends on whether latent\nsemantics can be inferred from the downstream task's prompt. Specifically,\nthrough investigations using data generated by probabilistic context-free\ngrammars, we show that training with metadata helps improve model's performance\nwhen the given context is long enough to infer the latent semantics. In\ncontrast, the technique negatively impacts performance when the context lacks\nthe necessary information to make an accurate posterior inference.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u9884\u8bad\u7ec3\u6570\u636e\u524d\u6dfb\u52a0\u5143\u6570\u636e\u5bf9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u6548\u679c\u53d6\u51b3\u4e8e\u4e0b\u6e38\u4efb\u52a1\u63d0\u793a\u662f\u5426\u80fd\u63a8\u65ad\u6f5c\u5728\u8bed\u4e49\u3002", "motivation": "\u7406\u89e3\u9884\u8bad\u7ec3\u65f6\u6dfb\u52a0\u5143\u6570\u636e\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4e00\u81f4\u7684\u73b0\u8c61\u3002", "method": "\u4f7f\u7528\u4eba\u5de5\u751f\u6210\u7684\u6570\u636e\uff08\u5982\u6982\u7387\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\uff09\u5206\u6790\u6a21\u578b\u884c\u4e3a\uff0c\u7814\u7a76\u5143\u6570\u636e\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u7684\u6548\u679c\u3002", "result": "\u5143\u6570\u636e\u5728\u4e0a\u4e0b\u6587\u8db3\u591f\u957f\u65f6\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5728\u4fe1\u606f\u4e0d\u8db3\u65f6\u53cd\u800c\u4f1a\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u5143\u6570\u636e\u7684\u6709\u6548\u6027\u4f9d\u8d56\u4e8e\u4e0b\u6e38\u4efb\u52a1\u63d0\u793a\u662f\u5426\u80fd\u63a8\u65ad\u6f5c\u5728\u8bed\u4e49\uff0c\u9700\u6839\u636e\u4efb\u52a1\u7279\u70b9\u8c28\u614e\u4f7f\u7528\u3002"}}
{"id": "2504.17401", "pdf": "https://arxiv.org/pdf/2504.17401", "abs": "https://arxiv.org/abs/2504.17401", "authors": ["Xu Wang", "Jialang Xu", "Shuai Zhang", "Baoru Huang", "Danail Stoyanov", "Evangelos B. Mazomenos"], "title": "StereoMamba: Real-time and Robust Intraoperative Stereo Disparity Estimation via Long-range Spatial Dependencies", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Stereo disparity estimation is crucial for obtaining depth information in\nrobot-assisted minimally invasive surgery (RAMIS). While current deep learning\nmethods have made significant advancements, challenges remain in achieving an\noptimal balance between accuracy, robustness, and inference speed. To address\nthese challenges, we propose the StereoMamba architecture, which is\nspecifically designed for stereo disparity estimation in RAMIS. Our approach is\nbased on a novel Feature Extraction Mamba (FE-Mamba) module, which enhances\nlong-range spatial dependencies both within and across stereo images. To\neffectively integrate multi-scale features from FE-Mamba, we then introduce a\nnovel Multidimensional Feature Fusion (MFF) module. Experiments against the\nstate-of-the-art on the ex-vivo SCARED benchmark demonstrate that StereoMamba\nachieves superior performance on EPE of 2.64 px and depth MAE of 2.55 mm, the\nsecond-best performance on Bad2 of 41.49% and Bad3 of 26.99%, while maintaining\nan inference speed of 21.28 FPS for a pair of high-resolution images\n(1280*1024), striking the optimum balance between accuracy, robustness, and\nefficiency. Furthermore, by comparing synthesized right images, generated from\nwarping left images using the generated disparity maps, with the actual right\nimage, StereoMamba achieves the best average SSIM (0.8970) and PSNR (16.0761),\nexhibiting strong zero-shot generalization on the in-vivo RIS2017 and StereoMIS\ndatasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faStereoMamba\u67b6\u6784\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u8f85\u52a9\u5fae\u521b\u624b\u672f\u4e2d\u7684\u7acb\u4f53\u89c6\u5dee\u4f30\u8ba1\uff0c\u901a\u8fc7FE-Mamba\u548cMFF\u6a21\u5757\u63d0\u5347\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u7acb\u4f53\u89c6\u5dee\u4f30\u8ba1\u4e2d\u96be\u4ee5\u5e73\u8861\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u63a8\u7406\u901f\u5ea6\uff0cStereoMamba\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528FE-Mamba\u6a21\u5757\u589e\u5f3a\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u7ed3\u5408MFF\u6a21\u5757\u878d\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u3002", "result": "\u5728SCARED\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEPE\u4e3a2.64 px\uff0c\u6df1\u5ea6MAE\u4e3a2.55 mm\uff0c\u63a8\u7406\u901f\u5ea6\u4e3a21.28 FPS\uff0cSSIM\u548cPSNR\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "StereoMamba\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u4e0a\u8fbe\u5230\u6700\u4f18\u5e73\u8861\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.17565", "pdf": "https://arxiv.org/pdf/2504.17565", "abs": "https://arxiv.org/abs/2504.17565", "authors": ["Xiaoyu Tian", "Sitong Zhao", "Haotian Wang", "Shuaiting Chen", "Yiping Peng", "Yunjie Ji", "Han Zhao", "Xiangang Li"], "title": "DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training", "categories": ["cs.CL"], "comment": null, "summary": "Although large language models (LLMs) have recently achieved remarkable\nperformance on various complex reasoning benchmarks, the academic community\nstill lacks an in-depth understanding of base model training processes and data\nquality. To address this, we construct a large-scale, difficulty-graded\nreasoning dataset containing approximately 3.34 million unique queries of\nvarying difficulty levels and about 40 million distilled responses generated by\nmultiple models over several passes. Leveraging pass rate and Coefficient of\nVariation (CV), we precisely select the most valuable training data to enhance\nreasoning capability. Notably, we observe a training pattern shift, indicating\nthat reasoning-focused training based on base models requires higher learning\nrates for effective training. Using this carefully selected data, we\nsignificantly improve the reasoning capabilities of the base model, achieving a\npass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This\nresult surpasses most current distilled models and closely approaches\nstate-of-the-art performance. We provide detailed descriptions of our data\nprocessing, difficulty assessment, and training methodology, and have publicly\nreleased all datasets and methods to promote rapid progress in open-source\nlong-reasoning LLMs. The dataset is available at:\nhttps://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u96be\u5ea6\u5206\u7ea7\u7684\u63a8\u7406\u6570\u636e\u96c6\uff0c\u4f18\u5316\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u5b66\u672f\u754c\u5bf9\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u548c\u6570\u636e\u8d28\u91cf\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u5305\u542b340\u4e07\u72ec\u7279\u67e5\u8be2\u548c4000\u4e07\u84b8\u998f\u54cd\u5e94\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5229\u7528\u901a\u8fc7\u7387\u548c\u53d8\u5f02\u7cfb\u6570\u9009\u62e9\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u5e76\u8c03\u6574\u5b66\u4e60\u7387\u3002", "result": "\u5728AIME2024\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8fbe\u523079.2%\u7684\u901a\u8fc7\u7387\uff0c\u8d85\u8d8a\u591a\u6570\u84b8\u998f\u6a21\u578b\u3002", "conclusion": "\u516c\u5f00\u6570\u636e\u96c6\u548c\u65b9\u6cd5\uff0c\u63a8\u52a8\u5f00\u6e90\u957f\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\u3002"}}
{"id": "2504.17414", "pdf": "https://arxiv.org/pdf/2504.17414", "abs": "https://arxiv.org/abs/2504.17414", "authors": ["Min Wei", "Chaohui Yu", "Jingkai Zhou", "Fan Wang"], "title": "3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models", "categories": ["cs.CV"], "comment": "Project page: https://2y7c3.github.io/3DV-TON/", "summary": "Video try-on replaces clothing in videos with target garments. Existing\nmethods struggle to generate high-quality and temporally consistent results\nwhen handling complex clothing patterns and diverse body poses. We present\n3DV-TON, a novel diffusion-based framework for generating high-fidelity and\ntemporally consistent video try-on results. Our approach employs generated\nanimatable textured 3D meshes as explicit frame-level guidance, alleviating the\nissue of models over-focusing on appearance fidelity at the expanse of motion\ncoherence. This is achieved by enabling direct reference to consistent garment\ntexture movements throughout video sequences. The proposed method features an\nadaptive pipeline for generating dynamic 3D guidance: (1) selecting a keyframe\nfor initial 2D image try-on, followed by (2) reconstructing and animating a\ntextured 3D mesh synchronized with original video poses. We further introduce a\nrobust rectangular masking strategy that successfully mitigates artifact\npropagation caused by leaking clothing information during dynamic human and\ngarment movements. To advance video try-on research, we introduce HR-VVT, a\nhigh-resolution benchmark dataset containing 130 videos with diverse clothing\ntypes and scenarios. Quantitative and qualitative results demonstrate our\nsuperior performance over existing methods. The project page is at this link\nhttps://2y7c3.github.io/3DV-TON/", "AI": {"tldr": "3DV-TON\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u89c6\u9891\u8bd5\u7a7f\u6548\u679c\uff0c\u901a\u8fc7\u52a8\u60013D\u7f51\u683c\u548c\u77e9\u5f62\u63a9\u7801\u7b56\u7565\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u670d\u88c5\u56fe\u6848\u548c\u591a\u6837\u8eab\u4f53\u59ff\u52bf\u4e0b\u96be\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u7ed3\u679c\u3002", "method": "\u4f7f\u7528\u751f\u6210\u7684\u52a8\u753b\u7eb9\u74063D\u7f51\u683c\u4f5c\u4e3a\u5e27\u7ea7\u6307\u5bfc\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7ba1\u9053\u751f\u6210\u52a8\u60013D\u6307\u5bfc\uff0c\u5e76\u5f15\u5165\u77e9\u5f62\u63a9\u7801\u7b56\u7565\u51cf\u5c11\u4f2a\u5f71\u4f20\u64ad\u3002", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u7ed3\u679c\u8868\u660e\uff0c3DV-TON\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "3DV-TON\u901a\u8fc7\u52a8\u60013D\u6307\u5bfc\u548c\u63a9\u7801\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u8bd5\u7a7f\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2504.17574", "pdf": "https://arxiv.org/pdf/2504.17574", "abs": "https://arxiv.org/abs/2504.17574", "authors": ["Zhenkai Qin", "Guifang Yang", "Dongze Wu"], "title": "RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "As false information continues to proliferate across social media platforms,\neffective rumor detection has emerged as a pressing challenge in natural\nlanguage processing. This paper proposes RAGAT-Mind, a multi-granular modeling\napproach for Chinese rumor detection, built upon the MindSpore deep learning\nframework. The model integrates TextCNN for local semantic extraction,\nbidirectional GRU for sequential context learning, Multi-Head Self-Attention\nfor global dependency focusing, and Bidirectional Graph Convolutional Networks\n(BiGCN) for structural representation of word co-occurrence graphs. Experiments\non the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior\nclassification performance, attaining 99.2% accuracy and a macro-F1 score of\n0.9919. The results validate the effectiveness of combining hierarchical\nlinguistic features with graph-based semantic structures. Furthermore, the\nmodel exhibits strong generalization and interpretability, highlighting its\npractical value for real-world rumor detection applications.", "AI": {"tldr": "RAGAT-Mind\u662f\u4e00\u79cd\u57fa\u4e8eMindSpore\u6846\u67b6\u7684\u4e2d\u6587\u8c23\u8a00\u68c0\u6d4b\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u7c92\u5ea6\u5efa\u6a21\u65b9\u6cd5\uff0c\u5728\u5fae\u535a\u8c23\u8a00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e0a\u865a\u5047\u4fe1\u606f\u6cdb\u6ee5\uff0c\u8c23\u8a00\u68c0\u6d4b\u6210\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u8feb\u5207\u9700\u6c42\u3002", "method": "\u6a21\u578b\u6574\u5408\u4e86TextCNN\u3001\u53cc\u5411GRU\u3001\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u53cc\u5411\u56fe\u5377\u79ef\u7f51\u7edc\uff08BiGCN\uff09\uff0c\u7528\u4e8e\u63d0\u53d6\u5c40\u90e8\u8bed\u4e49\u3001\u5b66\u4e60\u5e8f\u5217\u4e0a\u4e0b\u6587\u3001\u805a\u7126\u5168\u5c40\u4f9d\u8d56\u548c\u8868\u793a\u8bcd\u5171\u73b0\u56fe\u7ed3\u6784\u3002", "result": "\u5728Weibo1-Rumor\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe99.2%\uff0c\u5b8fF1\u5206\u6570\u4e3a0.9919\u3002", "conclusion": "RAGAT-Mind\u901a\u8fc7\u7ed3\u5408\u5c42\u6b21\u5316\u8bed\u8a00\u7279\u5f81\u548c\u56fe\u8bed\u4e49\u7ed3\u6784\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.17432", "pdf": "https://arxiv.org/pdf/2504.17432", "abs": "https://arxiv.org/abs/2504.17432", "authors": ["Tiancheng Gu", "Kaicheng Yang", "Ziyong Feng", "Xingjun Wang", "Yanzhao Zhang", "Dingkun Long", "Yingda Chen", "Weidong Cai", "Jiankang Deng"], "title": "Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs", "categories": ["cs.CV"], "comment": "13 pages, 8 figures, Project page: https://garygutc.github.io/UniME", "summary": "The Contrastive Language-Image Pre-training (CLIP) framework has become a\nwidely used approach for multimodal representation learning, particularly in\nimage-text retrieval and clustering. However, its efficacy is constrained by\nthree key limitations: (1) text token truncation, (2) isolated image-text\nencoding, and (3) deficient compositionality due to bag-of-words behavior.\nWhile recent Multimodal Large Language Models (MLLMs) have demonstrated\nsignificant advances in generalized vision-language understanding, their\npotential for learning transferable multimodal representations remains\nunderexplored.In this work, we present UniME (Universal Multimodal Embedding),\na novel two-stage framework that leverages MLLMs to learn discriminative\nrepresentations for diverse downstream tasks. In the first stage, we perform\ntextual discriminative knowledge distillation from a powerful LLM-based teacher\nmodel to enhance the embedding capability of the MLLM\\'s language component. In\nthe second stage, we introduce hard negative enhanced instruction tuning to\nfurther advance discriminative representation learning. Specifically, we\ninitially mitigate false negative contamination and then sample multiple hard\nnegatives per instance within each batch, forcing the model to focus on\nchallenging samples. This approach not only improves discriminative power but\nalso enhances instruction-following ability in downstream tasks. We conduct\nextensive experiments on the MMEB benchmark and multiple retrieval tasks,\nincluding short and long caption retrieval and compositional retrieval. Results\ndemonstrate that UniME achieves consistent performance improvement across all\ntasks, exhibiting superior discriminative and compositional capabilities.", "AI": {"tldr": "UniME\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5229\u7528MLLM\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u591a\u6a21\u6001\u8868\u793a\uff0c\u89e3\u51b3\u4e86CLIP\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "CLIP\u6846\u67b6\u5728\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4e2d\u5b58\u5728\u6587\u672c\u622a\u65ad\u3001\u5b64\u7acb\u7f16\u7801\u548c\u7ec4\u5408\u6027\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u800cMLLM\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002", "method": "UniME\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4eceLLM\u6559\u5e08\u6a21\u578b\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\uff1b2) \u5f15\u5165\u786c\u8d1f\u6837\u672c\u589e\u5f3a\u7684\u6307\u4ee4\u8c03\u4f18\uff0c\u63d0\u5347\u5224\u522b\u6027\u548c\u7ec4\u5408\u6027\u3002", "result": "\u5728MMEB\u57fa\u51c6\u548c\u591a\u4e2a\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0cUniME\u8868\u73b0\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u66f4\u5f3a\u7684\u5224\u522b\u548c\u7ec4\u5408\u80fd\u529b\u3002", "conclusion": "UniME\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5d4c\u5165\u80fd\u529b\u3002"}}
{"id": "2504.17653", "pdf": "https://arxiv.org/pdf/2504.17653", "abs": "https://arxiv.org/abs/2504.17653", "authors": ["Samaneh Hosseini Moghaddam", "Kelly Lyons", "Cheryl Regehr", "Vivek Goel", "Kaitlyn Regehr"], "title": "Towards a comprehensive taxonomy of online abusive language informed by machine leaning", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of abusive language in online communications has posed\nsignificant risks to the health and wellbeing of individuals and communities.\nThe growing concern regarding online abuse and its consequences necessitates\nmethods for identifying and mitigating harmful content and facilitating\ncontinuous monitoring, moderation, and early intervention. This paper presents\na taxonomy for distinguishing key characteristics of abusive language within\nonline text. Our approach uses a systematic method for taxonomy development,\nintegrating classification systems of 18 existing multi-label datasets to\ncapture key characteristics relevant to online abusive language classification.\nThe resulting taxonomy is hierarchical and faceted, comprising 5 categories and\n17 dimensions. It classifies various facets of online abuse, including context,\ntarget, intensity, directness, and theme of abuse. This shared understanding\ncan lead to more cohesive efforts, facilitate knowledge exchange, and\naccelerate progress in the field of online abuse detection and mitigation among\nresearchers, policy makers, online platform owners, and other stakeholders.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u533a\u5206\u5728\u7ebf\u6587\u672c\u4e2d\u8fb1\u9a82\u8bed\u8a00\u5173\u952e\u7279\u5f81\u7684\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u6574\u540818\u4e2a\u591a\u6807\u7b7e\u6570\u636e\u96c6\u7684\u5206\u7c7b\u7cfb\u7edf\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b5\u4e2a\u7c7b\u522b\u548c17\u4e2a\u7ef4\u5ea6\u7684\u5c42\u6b21\u5316\u5206\u7c7b\u6cd5\u3002", "motivation": "\u5728\u7ebf\u8fb1\u9a82\u8bed\u8a00\u7684\u6cdb\u6ee5\u5bf9\u4e2a\u4eba\u548c\u793e\u533a\u7684\u5065\u5eb7\u4e0e\u798f\u7949\u6784\u6210\u91cd\u5927\u98ce\u9669\uff0c\u4e9f\u9700\u8bc6\u522b\u548c\u51cf\u8f7b\u6709\u5bb3\u5185\u5bb9\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u5206\u7c7b\u6cd5\u5f00\u53d1\u65b9\u6cd5\uff0c\u6574\u540818\u4e2a\u73b0\u6709\u591a\u6807\u7b7e\u6570\u636e\u96c6\u7684\u5206\u7c7b\u7cfb\u7edf\uff0c\u6784\u5efa\u5c42\u6b21\u5316\u5206\u7c7b\u6cd5\u3002", "result": "\u6700\u7ec8\u5206\u7c7b\u6cd5\u5305\u542b5\u4e2a\u7c7b\u522b\u548c17\u4e2a\u7ef4\u5ea6\uff0c\u6db5\u76d6\u8fb1\u9a82\u8bed\u8a00\u7684\u80cc\u666f\u3001\u76ee\u6807\u3001\u5f3a\u5ea6\u3001\u76f4\u63a5\u6027\u548c\u4e3b\u9898\u7b49\u7279\u5f81\u3002", "conclusion": "\u8be5\u5206\u7c7b\u6cd5\u4e3a\u7814\u7a76\u8005\u3001\u653f\u7b56\u5236\u5b9a\u8005\u7b49\u63d0\u4f9b\u4e86\u5171\u4eab\u7406\u89e3\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u5728\u7ebf\u8fb1\u9a82\u68c0\u6d4b\u4e0e\u51cf\u8f7b\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2504.17441", "pdf": "https://arxiv.org/pdf/2504.17441", "abs": "https://arxiv.org/abs/2504.17441", "authors": ["Mingxuan Wu", "Huang Huang", "Justin Kerr", "Chung Min Kim", "Anthony Zhang", "Brent Yi", "Angjoo Kanazawa"], "title": "Predict-Optimize-Distill: A Self-Improving Cycle for 4D Object Understanding", "categories": ["cs.CV"], "comment": "See our website at:\n  https://predict-optimize-distill.github.io/pod.github.io First two authors\n  contributed equally", "summary": "Humans can resort to long-form inspection to build intuition on predicting\nthe 3D configurations of unseen objects. The more we observe the object motion,\nthe better we get at predicting its 3D state immediately. Existing systems\neither optimize underlying representations from multi-view observations or\ntrain a feed-forward predictor from supervised datasets. We introduce\nPredict-Optimize-Distill (POD), a self-improving framework that interleaves\nprediction and optimization in a mutually reinforcing cycle to achieve better\n4D object understanding with increasing observation time. Given a multi-view\nobject scan and a long-form monocular video of human-object interaction, POD\niteratively trains a neural network to predict local part poses from RGB\nframes, uses this predictor to initialize a global optimization which refines\noutput poses through inverse rendering, then finally distills the results of\noptimization back into the model by generating synthetic self-labeled training\ndata from novel viewpoints. Each iteration improves both the predictive model\nand the optimized motion trajectory, creating a virtuous cycle that bootstraps\nits own training data to learn about the pose configurations of an object. We\nalso introduce a quasi-multiview mining strategy for reducing depth ambiguity\nby leveraging long video. We evaluate POD on 14 real-world and 5 synthetic\nobjects with various joint types, including revolute and prismatic joints as\nwell as multi-body configurations where parts detach or reattach independently.\nPOD demonstrates significant improvement over a pure optimization baseline\nwhich gets stuck in local minima, particularly for longer videos. We also find\nthat POD's performance improves with both video length and successive\niterations of the self-improving cycle, highlighting its ability to scale\nperformance with additional observations and looped refinement.", "AI": {"tldr": "POD\u6846\u67b6\u901a\u8fc7\u9884\u6d4b-\u4f18\u5316-\u84b8\u998f\u7684\u5faa\u73af\u81ea\u6211\u63d0\u5347\u673a\u5236\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u626b\u63cf\u548c\u957f\u89c6\u9891\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7269\u4f534D\u72b6\u6001\u7684\u6301\u7eed\u4f18\u5316\u7406\u89e3\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u957f\u65f6\u95f4\u89c2\u5bdf\u7269\u4f53\u8fd0\u52a8\u6765\u9884\u6d4b\u51763D\u72b6\u6001\uff0c\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u591a\u89c6\u89d2\u89c2\u5bdf\u6216\u76d1\u7763\u6570\u636e\u96c6\u8bad\u7ec3\u3002POD\u65e8\u5728\u901a\u8fc7\u81ea\u6211\u63d0\u5347\u6846\u67b6\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "POD\u6846\u67b6\u901a\u8fc7\u9884\u6d4b\u5c40\u90e8\u59ff\u6001\u3001\u5168\u5c40\u4f18\u5316\u548c\u84b8\u998f\u5408\u6210\u6570\u636e\u7684\u5faa\u73af\uff0c\u7ed3\u5408\u51c6\u591a\u89c6\u89d2\u6316\u6398\u7b56\u7565\u51cf\u5c11\u6df1\u5ea6\u6a21\u7cca\u6027\u3002", "result": "\u572814\u4e2a\u771f\u5b9e\u548c5\u4e2a\u5408\u6210\u5bf9\u8c61\u4e0a\u6d4b\u8bd5\uff0cPOD\u663e\u8457\u4f18\u4e8e\u7eaf\u4f18\u5316\u57fa\u7ebf\uff0c\u6027\u80fd\u968f\u89c6\u9891\u957f\u5ea6\u548c\u8fed\u4ee3\u6b21\u6570\u63d0\u5347\u3002", "conclusion": "POD\u5c55\u793a\u4e86\u901a\u8fc7\u5faa\u73af\u81ea\u6211\u63d0\u5347\u548c\u957f\u89c6\u9891\u5229\u7528\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u590d\u6742\u7269\u4f53\u59ff\u6001\u914d\u7f6e\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2504.17665", "pdf": "https://arxiv.org/pdf/2504.17665", "abs": "https://arxiv.org/abs/2504.17665", "authors": ["Zena Al-Khalili", "Nick Howell", "Dietrich Klakow"], "title": "Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics", "categories": ["cs.CL"], "comment": null, "summary": "Assisting LLMs with code generation improved their performance on\nmathematical reasoning tasks. However, the evaluation of code-assisted LLMs is\ngenerally restricted to execution correctness, lacking a rigorous evaluation of\ntheir generated programs. In this work, we bridge this gap by conducting an\nin-depth analysis of code-assisted LLMs' generated programs in response to math\nreasoning tasks. Our evaluation focuses on the extent to which LLMs ground\ntheir programs to math rules, and how that affects their end performance. For\nthis purpose, we assess the generations of five different LLMs, on two\ndifferent math datasets, both manually and automatically. Our results reveal\nthat the distribution of grounding depends on LLMs' capabilities and the\ndifficulty of math problems. Furthermore, mathematical grounding is more\neffective for closed-source models, while open-source models fail to employ\nmath rules in their solutions correctly. On MATH500, the percentage of grounded\nprograms decreased to half, while the ungrounded generations doubled in\ncomparison to ASDiv grade-school problems. Our work highlights the need for\nin-depth evaluation beyond execution accuracy metrics, toward a better\nunderstanding of code-assisted LLMs' capabilities and limits in the math\ndomain.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u4ee3\u7801\u8f85\u52a9LLMs\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u751f\u6210\u7684\u7a0b\u5e8f\uff0c\u53d1\u73b0\u5176\u6570\u5b66\u89c4\u5219\u57fa\u7840\u6027\u5bf9\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u4e14\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u4ee3\u7801\u8f85\u52a9LLMs\u7684\u6267\u884c\u6b63\u786e\u6027\uff0c\u7f3a\u4e4f\u5bf9\u5176\u751f\u6210\u7a0b\u5e8f\u7684\u6df1\u5165\u8bc4\u4f30\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u624b\u52a8\u548c\u81ea\u52a8\u8bc4\u4f30\u4e94\u79cdLLMs\u5728\u4e24\u4e2a\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u751f\u6210\u7a0b\u5e8f\uff0c\u5206\u6790\u5176\u6570\u5b66\u89c4\u5219\u57fa\u7840\u6027\u3002", "result": "\u6570\u5b66\u89c4\u5219\u57fa\u7840\u6027\u56e0\u6a21\u578b\u80fd\u529b\u548c\u95ee\u9898\u96be\u5ea6\u800c\u5f02\uff0c\u95ed\u6e90\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u9700\u8d85\u8d8a\u6267\u884c\u51c6\u786e\u6027\uff0c\u6df1\u5165\u8bc4\u4f30\u4ee3\u7801\u8f85\u52a9LLMs\u5728\u6570\u5b66\u9886\u57df\u7684\u8868\u73b0\uff0c\u4ee5\u5168\u9762\u7406\u89e3\u5176\u80fd\u529b\u4e0e\u5c40\u9650\u3002"}}
{"id": "2504.17447", "pdf": "https://arxiv.org/pdf/2504.17447", "abs": "https://arxiv.org/abs/2504.17447", "authors": ["De-An Huang", "Subhashree Radhakrishnan", "Zhiding Yu", "Jan Kautz"], "title": "FRAG: Frame Selection Augmented Generation for Long Video and Long Document Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "There has been impressive progress in Large Multimodal Models (LMMs). Recent\nworks extend these models to long inputs, including multi-page documents and\nlong videos. However, the model size and performance of these long context\nmodels are still limited due to the computational cost in both training and\ninference. In this work, we explore an orthogonal direction and process long\ninputs without long context LMMs. We propose Frame Selection Augmented\nGeneration (FRAG), where the model first selects relevant frames within the\ninput, and then only generates the final outputs based on the selected frames.\nThe core of the selection process is done by scoring each frame independently,\nwhich does not require long context processing. The frames with the highest\nscores are then selected by a simple Top-K selection. We show that this\nfrustratingly simple framework is applicable to both long videos and multi-page\ndocuments using existing LMMs without any fine-tuning. We consider two models,\nLLaVA-OneVision and InternVL2, in our experiments and show that FRAG\nconsistently improves the performance and achieves state-of-the-art\nperformances for both long video and long document understanding. For videos,\nFRAG substantially improves InternVL2-76B by 5.8% on MLVU and 3.7% on\nVideo-MME. For documents, FRAG achieves over 20% improvements on MP-DocVQA\ncompared with recent LMMs specialized in long document understanding. Code is\navailable at: https://github.com/NVlabs/FRAG", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFRAG\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u8f93\u5165\u4e2d\u7684\u76f8\u5173\u5e27\u800c\u975e\u5904\u7406\u957f\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u957f\u89c6\u9891\u548c\u591a\u9875\u6587\u6863\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u957f\u4e0a\u4e0b\u6587\u591a\u6a21\u6001\u6a21\u578b\u56e0\u8ba1\u7b97\u6210\u672c\u9ad8\u800c\u53d7\u9650\uff0c\u4f5c\u8005\u63a2\u7d22\u4e86\u4e00\u79cd\u65e0\u9700\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "FRAG\u6846\u67b6\u901a\u8fc7\u72ec\u7acb\u8bc4\u5206\u9009\u62e9\u76f8\u5173\u5e27\uff08Top-K\u9009\u62e9\uff09\uff0c\u4ec5\u57fa\u4e8e\u9009\u5b9a\u5e27\u751f\u6210\u8f93\u51fa\uff0c\u65e0\u9700\u5fae\u8c03\u73b0\u6709\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFRAG\u5728\u957f\u89c6\u9891\u548c\u6587\u6863\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5982InternVL2-76B\u5728MLVU\u4e0a\u63d0\u53475.8%\uff0c\u5728MP-DocVQA\u4e0a\u63d0\u5347\u8d8520%\u3002", "conclusion": "FRAG\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u957f\u8f93\u5165\u4efb\u52a1\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u3002"}}
{"id": "2504.17671", "pdf": "https://arxiv.org/pdf/2504.17671", "abs": "https://arxiv.org/abs/2504.17671", "authors": ["Yuanchang Ye", "Weiyan Wen"], "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSplit Conformal Prediction\uff08SCP\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u51cf\u5c11\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u9608\u503c\u6821\u51c6\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u9a8c\u8bc1\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "LVLM\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8f93\u51fa\u5e38\u4f34\u968f\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u5bf9\u5b89\u5168\u5173\u952e\u5e94\u7528\u6784\u6210\u98ce\u9669\u3002", "method": "\u91c7\u7528SCP\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u5206\u533a\uff08\u6821\u51c6\u96c6\u548c\u6d4b\u8bd5\u96c6\uff09\u8ba1\u7b97\u975e\u4e00\u81f4\u6027\u5206\u6570\uff0c\u6784\u5efa\u5177\u6709\u7edf\u8ba1\u4fdd\u8bc1\u7684\u9884\u6d4b\u96c6\uff0c\u52a8\u6001\u8c03\u6574\u9884\u6d4b\u96c6\u5927\u5c0f\u5e76\u6d88\u9664\u5148\u9a8c\u5206\u5e03\u5047\u8bbe\u3002", "result": "\u5728ScienceQA\u548cMMMU\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSCP\u6846\u67b6\u5728\u6240\u6709\u03b1\u503c\u4e0b\u5747\u5b9e\u73b0\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u4e0d\u540c\u6821\u51c6-\u6d4b\u8bd5\u5206\u5272\u6bd4\u4f8b\u4e0b\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u6a21\u6001AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5e7b\u89c9\u68c0\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u533b\u7597\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b89\u5168\u654f\u611f\u9886\u57df\u3002"}}
{"id": "2504.17457", "pdf": "https://arxiv.org/pdf/2504.17457", "abs": "https://arxiv.org/abs/2504.17457", "authors": ["Zhiying Li", "Yeying Jin", "Fan Shen", "Zhi Liu", "Weibin Chen", "Pengju Zhang", "Xiaomei Zhang", "Boyu Chen", "Michael Shen", "Kejian Wu", "Zhaoxin Fan", "Jin Dong"], "title": "Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks", "categories": ["cs.CV"], "comment": "14 pages, 7 figures", "summary": "Expressive human pose and shape estimation (EHPS) is crucial for digital\nhuman generation, especially in applications like live streaming. While\nexisting research primarily focuses on reducing estimation errors, it largely\nneglects robustness and security aspects, leaving these systems vulnerable to\nadversarial attacks. To address this significant challenge, we propose the\n\\textbf{Tangible Attack (TBA)}, a novel framework designed to generate\nadversarial examples capable of effectively compromising any digital human\ngeneration model. Our approach introduces a \\textbf{Dual Heterogeneous Noise\nGenerator (DHNG)}, which leverages Variational Autoencoders (VAE) and\nControlNet to produce diverse, targeted noise tailored to the original image\nfeatures. Additionally, we design a custom \\textbf{adversarial loss function}\nto optimize the noise, ensuring both high controllability and potent\ndisruption. By iteratively refining the adversarial sample through\nmulti-gradient signals from both the noise and the state-of-the-art EHPS model,\nTBA substantially improves the effectiveness of adversarial attacks. Extensive\nexperiments demonstrate TBA's superiority, achieving a remarkable 41.0\\%\nincrease in estimation error, with an average improvement of approximately\n17.0\\%. These findings expose significant security vulnerabilities in current\nEHPS models and highlight the need for stronger defenses in digital human\ngeneration systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTangible Attack (TBA)\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7Dual Heterogeneous Noise Generator (DHNG)\u548c\u81ea\u5b9a\u4e49\u5bf9\u6297\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u6297\u653b\u51fb\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524dEHPS\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u51cf\u5c11\u4f30\u8ba1\u8bef\u5dee\uff0c\u4f46\u5ffd\u89c6\u4e86\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u3002", "method": "\u63d0\u51faTBA\u6846\u67b6\uff0c\u7ed3\u5408DHNG\uff08\u5229\u7528VAE\u548cControlNet\u751f\u6210\u591a\u6837\u5316\u566a\u58f0\uff09\u548c\u81ea\u5b9a\u4e49\u5bf9\u6297\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u591a\u68af\u5ea6\u4fe1\u53f7\u8fed\u4ee3\u4f18\u5316\u5bf9\u6297\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTBA\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\u6548\u679c\uff0c\u4f30\u8ba1\u8bef\u5dee\u589e\u52a0\u4e8641.0%\uff0c\u5e73\u5747\u63d0\u5347\u7ea617.0%\u3002", "conclusion": "\u5f53\u524dEHPS\u6a21\u578b\u5b58\u5728\u91cd\u5927\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u52a0\u5f3a\u6570\u5b57\u4eba\u751f\u6210\u7cfb\u7edf\u7684\u9632\u5fa1\u80fd\u529b\u3002"}}
{"id": "2504.17674", "pdf": "https://arxiv.org/pdf/2504.17674", "abs": "https://arxiv.org/abs/2504.17674", "authors": ["Jared Fernandez", "Clara Na", "Vashisth Tiwari", "Yonatan Bisk", "Sasha Luccioni", "Emma Strubell"], "title": "Energy Considerations of Large Language Model Inference and Efficiency Optimizations", "categories": ["cs.CL", "cs.LG"], "comment": "16 pages", "summary": "As large language models (LLMs) scale in size and adoption, their\ncomputational and environmental costs continue to rise. Prior benchmarking\nefforts have primarily focused on latency reduction in idealized settings,\noften overlooking the diverse real-world inference workloads that shape energy\nuse. In this work, we systematically analyze the energy implications of common\ninference efficiency optimizations across diverse Natural Language Processing\n(NLP) and generative Artificial Intelligence (AI) workloads, including\nconversational AI and code generation. We introduce a modeling approach that\napproximates real-world LLM workflows through a binning strategy for\ninput-output token distributions and batch size variations. Our empirical\nanalysis spans software frameworks, decoding strategies, GPU architectures,\nonline and offline serving settings, and model parallelism configurations. We\nshow that the effectiveness of inference optimizations is highly sensitive to\nworkload geometry, software stack, and hardware accelerators, demonstrating\nthat naive energy estimates based on FLOPs or theoretical GPU utilization\nsignificantly underestimate real-world energy consumption. Our findings reveal\nthat the proper application of relevant inference efficiency optimizations can\nreduce total energy use by up to 73% from unoptimized baselines. These insights\nprovide a foundation for sustainable LLM deployment and inform energy-efficient\ndesign strategies for future AI infrastructure.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u6548\u7387\u4f18\u5316\u5bf9\u80fd\u6e90\u6d88\u8017\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u4f18\u5316\u7b56\u7565\u53ef\u663e\u8457\u964d\u4f4e\u80fd\u6e90\u4f7f\u7528\u3002", "motivation": "\u968f\u7740LLM\u89c4\u6a21\u548c\u4f7f\u7528\u7684\u589e\u52a0\uff0c\u5176\u8ba1\u7b97\u548c\u73af\u5883\u6210\u672c\u4e0a\u5347\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u7406\u60f3\u5316\u573a\u666f\u7684\u5ef6\u8fdf\u4f18\u5316\uff0c\u5ffd\u89c6\u4e86\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u5bf9\u80fd\u6e90\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u8f93\u5165\u8f93\u51fa\u4ee4\u724c\u5206\u5e03\u548c\u6279\u91cf\u5927\u5c0f\u7684\u5206\u7bb1\u7b56\u7565\uff0c\u5efa\u6a21\u771f\u5b9eLLM\u5de5\u4f5c\u6d41\uff0c\u5e76\u5206\u6790\u8f6f\u4ef6\u6846\u67b6\u3001\u89e3\u7801\u7b56\u7565\u3001GPU\u67b6\u6784\u7b49\u591a\u79cd\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u4f18\u5316\u7684\u6548\u679c\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u3001\u8f6f\u4ef6\u548c\u786c\u4ef6\u9ad8\u5ea6\u654f\u611f\uff0c\u4f18\u5316\u7b56\u7565\u53ef\u51cf\u5c11\u9ad8\u8fbe73%\u7684\u80fd\u6e90\u6d88\u8017\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53ef\u6301\u7eedLLM\u90e8\u7f72\u548c\u672a\u6765AI\u57fa\u7840\u8bbe\u65bd\u7684\u8282\u80fd\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2504.17474", "pdf": "https://arxiv.org/pdf/2504.17474", "abs": "https://arxiv.org/abs/2504.17474", "authors": ["Weiran Pan", "Wei Wei", "Feida Zhu", "Yong Deng"], "title": "Enhanced Sample Selection with Confidence Tracking: Identifying Correctly Labeled yet Hard-to-Learn Samples in Noisy Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose a novel sample selection method for image classification in the\npresence of noisy labels. Existing methods typically consider small-loss\nsamples as correctly labeled. However, some correctly labeled samples are\ninherently difficult for the model to learn and can exhibit high loss similar\nto mislabeled samples in the early stages of training. Consequently, setting a\nthreshold on per-sample loss to select correct labels results in a trade-off\nbetween precision and recall in sample selection: a lower threshold may miss\nmany correctly labeled hard-to-learn samples (low recall), while a higher\nthreshold may include many mislabeled samples (low precision). To address this\nissue, our goal is to accurately distinguish correctly labeled yet\nhard-to-learn samples from mislabeled ones, thus alleviating the trade-off\ndilemma. We achieve this by considering the trends in model prediction\nconfidence rather than relying solely on loss values. Empirical observations\nshow that only for correctly labeled samples, the model's prediction confidence\nfor the annotated labels typically increases faster than for any other classes.\nBased on this insight, we propose tracking the confidence gaps between the\nannotated labels and other classes during training and evaluating their trends\nusing the Mann-Kendall Test. A sample is considered potentially correctly\nlabeled if all its confidence gaps tend to increase. Our method functions as a\nplug-and-play component that can be seamlessly integrated into existing sample\nselection techniques. Experiments on several standard benchmarks and real-world\ndatasets demonstrate that our method enhances the performance of existing\nmethods for learning with noisy labels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u8d8b\u52bf\u7684\u6837\u672c\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u566a\u58f0\u6807\u7b7e\u4e0b\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6837\u672c\u9009\u62e9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u5c0f\u635f\u5931\u6837\u672c\u89c6\u4e3a\u6b63\u786e\u6807\u7b7e\uff0c\u4f46\u90e8\u5206\u6b63\u786e\u6807\u7b7e\u6837\u672c\u56e0\u96be\u4ee5\u5b66\u4e60\u800c\u8868\u73b0\u51fa\u9ad8\u635f\u5931\uff0c\u5bfc\u81f4\u6837\u672c\u9009\u62e9\u5728\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u901a\u8fc7\u8ddf\u8e2a\u6807\u6ce8\u6807\u7b7e\u4e0e\u5176\u4ed6\u7c7b\u522b\u4e4b\u95f4\u7684\u7f6e\u4fe1\u5ea6\u5dee\u8ddd\u8d8b\u52bf\uff0c\u5229\u7528Mann-Kendall Test\u8bc4\u4f30\u8d8b\u52bf\uff0c\u9009\u62e9\u7f6e\u4fe1\u5ea6\u5dee\u8ddd\u589e\u52a0\u7684\u6837\u672c\u4f5c\u4e3a\u6f5c\u5728\u6b63\u786e\u6807\u7b7e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u73b0\u6709\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u6837\u672c\u9009\u62e9\u6280\u672f\u4e2d\uff0c\u7f13\u89e3\u4e86\u6837\u672c\u9009\u62e9\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2504.17685", "pdf": "https://arxiv.org/pdf/2504.17685", "abs": "https://arxiv.org/abs/2504.17685", "authors": ["Haru-Tada Sato", "Fuka Matsuzaki", "Jun-ichiro Takahashi"], "title": "Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 2 figures", "summary": "This study explores the potential of small language model(SLM) ensembles to\nachieve accuracy comparable to proprietary large language models (LLMs). We\npropose Ensemble Bayesian Inference (EBI), a novel approach that applies\nBayesian estimation to combine judgments from multiple SLMs, allowing them to\nexceed the performance limitations of individual models. Our experiments on\ndiverse tasks(aptitude assessments and consumer profile analysis in both\nJapanese and English) demonstrate EBI's effectiveness. Notably, we analyze\ncases where incorporating models with negative Lift values into ensembles\nimproves overall performance, and we examine the method's efficacy across\ndifferent languages. These findings suggest new possibilities for constructing\nhigh-performance AI systems with limited computational resources and for\neffectively utilizing models with individually lower performance. Building on\nexisting research on LLM performance evaluation, ensemble methods, and\nopen-source LLM utilization, we discuss the novelty and significance of our\napproach.", "AI": {"tldr": "\u901a\u8fc7\u96c6\u6210\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u548c\u8d1d\u53f6\u65af\u63a8\u7406\uff08EBI\uff09\uff0c\u7814\u7a76\u5b9e\u73b0\u4e86\u4e0e\u5927\u578b\u4e13\u6709\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u7d22\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u6784\u5efa\u9ad8\u6027\u80fdAI\u7cfb\u7edf\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u6709\u6548\u5229\u7528\u6027\u80fd\u8f83\u4f4e\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faEnsemble Bayesian Inference\uff08EBI\uff09\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f30\u8ba1\u7ed3\u5408\u591a\u4e2aSLM\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eEBI\u5728\u591a\u79cd\u4efb\u52a1\uff08\u5982\u80fd\u529b\u8bc4\u4f30\u548c\u6d88\u8d39\u8005\u5206\u6790\uff09\u4e2d\u6709\u6548\uff0c\u751a\u81f3\u80fd\u901a\u8fc7\u96c6\u6210\u6027\u80fd\u8f83\u5dee\u7684\u6a21\u578b\u63d0\u5347\u6574\u4f53\u8868\u73b0\u3002", "conclusion": "EBI\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u9ad8\u6027\u80fdAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u5c55\u793a\u4e86\u4f4e\u6027\u80fd\u6a21\u578b\u7684\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2504.17502", "pdf": "https://arxiv.org/pdf/2504.17502", "abs": "https://arxiv.org/abs/2504.17502", "authors": ["Aviv Slobodkin", "Hagai Taitelbaum", "Yonatan Bitton", "Brian Gordon", "Michal Sokolik", "Nitzan Bitton Guetta", "Almog Gueta", "Royi Rassin", "Itay Laish", "Dani Lischinski", "Idan Szpektor"], "title": "RefVNLI: Towards Scalable Evaluation of Subject-driven Text-to-image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Subject-driven text-to-image (T2I) generation aims to produce images that\nalign with a given textual description, while preserving the visual identity\nfrom a referenced subject image. Despite its broad downstream applicability --\nranging from enhanced personalization in image generation to consistent\ncharacter representation in video rendering -- progress in this field is\nlimited by the lack of reliable automatic evaluation. Existing methods either\nassess only one aspect of the task (i.e., textual alignment or subject\npreservation), misalign with human judgments, or rely on costly API-based\nevaluation. To address this, we introduce RefVNLI, a cost-effective metric that\nevaluates both textual alignment and subject preservation in a single\nprediction. Trained on a large-scale dataset derived from video-reasoning\nbenchmarks and image perturbations, RefVNLI outperforms or matches existing\nbaselines across multiple benchmarks and subject categories (e.g.,\n\\emph{Animal}, \\emph{Object}), achieving up to 6.4-point gains in textual\nalignment and 8.5-point gains in subject consistency. It also excels with\nlesser-known concepts, aligning with human preferences at over 87\\% accuracy.", "AI": {"tldr": "RefVNLI\u662f\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e3b\u9898\u9a71\u52a8\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6587\u672c\u5bf9\u9f50\u548c\u4e3b\u9898\u4e00\u81f4\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3b\u9898\u9a71\u52a8\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7f3a\u4e4f\u53ef\u9760\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4ec5\u8bc4\u4f30\u5355\u4e00\u4efb\u52a1\uff0c\u8981\u4e48\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e0d\u4e00\u81f4\uff0c\u6216\u4f9d\u8d56\u6602\u8d35\u7684API\u8bc4\u4f30\u3002", "method": "\u5f15\u5165RefVNLI\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u89c6\u9891\u63a8\u7406\u57fa\u51c6\u548c\u56fe\u50cf\u6270\u52a8\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u540c\u65f6\u8bc4\u4f30\u6587\u672c\u5bf9\u9f50\u548c\u4e3b\u9898\u4e00\u81f4\u6027\u3002", "result": "RefVNLI\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e3b\u9898\u7c7b\u522b\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u57fa\u7ebf\uff0c\u6587\u672c\u5bf9\u9f50\u63d0\u53476.4\u5206\uff0c\u4e3b\u9898\u4e00\u81f4\u6027\u63d0\u53478.5\u5206\uff0c\u4e14\u5728\u51b7\u95e8\u6982\u5ff5\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "RefVNLI\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2504.17704", "pdf": "https://arxiv.org/pdf/2504.17704", "abs": "https://arxiv.org/abs/2504.17704", "authors": ["Cheng Wang", "Yue Liu", "Baolong Li", "Duzhen Zhang", "Zhongzhi Li", "Junfeng Fang"], "title": "Safety in Large Reasoning Models: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks\nlike mathematics and coding, leveraging their advanced reasoning capabilities.\nNevertheless, as these capabilities progress, significant concerns regarding\ntheir vulnerabilities and safety have arisen, which can pose challenges to\ntheir deployment and application in real-world settings. This paper presents a\ncomprehensive survey of LRMs, meticulously exploring and summarizing the newly\nemerged safety risks, attacks, and defense strategies. By organizing these\nelements into a detailed taxonomy, this work aims to offer a clear and\nstructured understanding of the current safety landscape of LRMs, facilitating\nfuture research and development to enhance the security and reliability of\nthese powerful models.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u5b89\u5168\u98ce\u9669\u3001\u653b\u51fb\u65b9\u5f0f\u548c\u9632\u5fa1\u7b56\u7565\uff0c\u65e8\u5728\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6e05\u6670\u7684\u7ed3\u6784\u5316\u7406\u89e3\u3002", "motivation": "\u968f\u7740LRMs\u5728\u6570\u5b66\u548c\u7f16\u7801\u7b49\u4efb\u52a1\u4e2d\u5c55\u73b0\u5f3a\u5927\u63a8\u7406\u80fd\u529b\uff0c\u5176\u5b89\u5168\u6f0f\u6d1e\u548c\u98ce\u9669\u6210\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u91cd\u5927\u6311\u6218\u3002", "method": "\u901a\u8fc7\u8be6\u7ec6\u5206\u7c7b\u6cd5\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86LRMs\u7684\u5b89\u5168\u98ce\u9669\u3001\u653b\u51fb\u65b9\u5f0f\u548c\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5b89\u5168\u5206\u7c7b\u6846\u67b6\uff0c\u4e3aLRMs\u7684\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u6307\u5bfc\u3002", "conclusion": "\u672c\u6587\u4e3a\u63d0\u5347LRMs\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u63a8\u52a8\u4e86\u672a\u6765\u76f8\u5173\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.17515", "pdf": "https://arxiv.org/pdf/2504.17515", "abs": "https://arxiv.org/abs/2504.17515", "authors": ["Zihan Cheng", "Jintao Guo", "Jian Zhang", "Lei Qi", "Luping Zhou", "Yinghuan Shi", "Yang Gao"], "title": "Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence Augmentation for Generalizable Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted by IEEE TMI 2025. The code is available at\n  https://github.com/orange-czh/Mamba-Sea", "summary": "To segment medical images with distribution shifts, domain generalization\n(DG) has emerged as a promising setting to train models on source domains that\ncan generalize to unseen target domains. Existing DG methods are mainly based\non CNN or ViT architectures. Recently, advanced state space models, represented\nby Mamba, have shown promising results in various supervised medical image\nsegmentation. The success of Mamba is primarily owing to its ability to capture\nlong-range dependencies while keeping linear complexity with input sequence\nlength, making it a promising alternative to CNNs and ViTs. Inspired by the\nsuccess, in the paper, we explore the potential of the Mamba architecture to\naddress distribution shifts in DG for medical image segmentation. Specifically,\nwe propose a novel Mamba-based framework, Mamba-Sea, incorporating\nglobal-to-local sequence augmentation to improve the model's generalizability\nunder domain shift issues. Our Mamba-Sea introduces a global augmentation\nmechanism designed to simulate potential variations in appearance across\ndifferent sites, aiming to suppress the model's learning of domain-specific\ninformation. At the local level, we propose a sequence-wise augmentation along\ninput sequences, which perturbs the style of tokens within random continuous\nsub-sequences by modeling and resampling style statistics associated with\ndomain shifts. To our best knowledge, Mamba-Sea is the first work to explore\nthe generalization of Mamba for medical image segmentation, providing an\nadvanced and promising Mamba-based architecture with strong robustness to\ndomain shifts. Remarkably, our proposed method is the first to surpass a Dice\ncoefficient of 90% on the Prostate dataset, which exceeds previous SOTA of\n88.61%. The code is available at https://github.com/orange-czh/Mamba-Sea.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u67b6\u6784\u7684\u65b0\u6846\u67b6Mamba-Sea\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u5168\u5c40\u5230\u5c40\u90e8\u7684\u5e8f\u5217\u589e\u5f3a\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728Prostate\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u8d85\u8fc790%\u7684Dice\u7cfb\u6570\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\uff0c\u5206\u5e03\u504f\u79fb\u95ee\u9898\u5f71\u54cd\u6a21\u578b\u5728\u672a\u89c1\u76ee\u6807\u57df\u4e0a\u7684\u8868\u73b0\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8eCNN\u6216ViT\u67b6\u6784\uff0c\u800cMamba\u56e0\u5176\u957f\u7a0b\u4f9d\u8d56\u6355\u6349\u80fd\u529b\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u663e\u793a\u51fa\u6f5c\u529b\u3002", "method": "\u63d0\u51faMamba-Sea\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u5e8f\u5217\u589e\u5f3a\u673a\u5236\uff1a\u5168\u5c40\u589e\u5f3a\u6a21\u62df\u4e0d\u540c\u7ad9\u70b9\u95f4\u7684\u5916\u89c2\u53d8\u5316\uff0c\u6291\u5236\u57df\u7279\u5b9a\u4fe1\u606f\u5b66\u4e60\uff1b\u5c40\u90e8\u589e\u5f3a\u901a\u8fc7\u6270\u52a8\u8fde\u7eed\u5b50\u5e8f\u5217\u7684\u98ce\u683c\u7edf\u8ba1\u63d0\u5347\u9c81\u68d2\u6027\u3002", "result": "\u5728Prostate\u6570\u636e\u96c6\u4e0a\uff0cMamba-Sea\u9996\u6b21\u8d85\u8fc790%\u7684Dice\u7cfb\u6570\uff0c\u4f18\u4e8e\u4e4b\u524d88.61%\u7684SOTA\u3002", "conclusion": "Mamba-Sea\u662f\u9996\u4e2a\u63a2\u7d22Mamba\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6cdb\u5316\u80fd\u529b\u7684\u5de5\u4f5c\uff0c\u63d0\u4f9b\u4e86\u5bf9\u5206\u5e03\u504f\u79fb\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u7684\u5148\u8fdb\u67b6\u6784\u3002"}}
{"id": "2504.17720", "pdf": "https://arxiv.org/pdf/2504.17720", "abs": "https://arxiv.org/abs/2504.17720", "authors": ["Vansh Gupta", "Sankalan Pal Chowdhury", "Vil\u00e9m Zouhar", "Donya Rooein", "Mrinmaya Sachan"], "title": "Multilingual Performance Biases of Large Language Models in Education", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly being adopted in educational\nsettings. These applications expand beyond English, though current LLMs remain\nprimarily English-centric. In this work, we ascertain if their use in education\nsettings in non-English languages is warranted. We evaluated the performance of\npopular LLMs on four educational tasks: identifying student misconceptions,\nproviding targeted feedback, interactive tutoring, and grading translations in\nsix languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to\nEnglish. We find that the performance on these tasks somewhat corresponds to\nthe amount of language represented in training data, with lower-resource\nlanguages having poorer task performance. Although the models perform\nreasonably well in most languages, the frequent performance drop from English\nis significant. Thus, we recommend that practitioners first verify that the LLM\nworks well in the target language for their educational task before deployment.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u975e\u82f1\u8bed\u6559\u80b2\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u4e0e\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u8bed\u8a00\u8d44\u6e90\u91cf\u76f8\u5173\uff0c\u5efa\u8bae\u90e8\u7f72\u524d\u9a8c\u8bc1\u76ee\u6807\u8bed\u8a00\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u975e\u82f1\u8bed\u6559\u80b2\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\uff0c\u586b\u8865\u5f53\u524d\u4ee5\u82f1\u8bed\u4e3a\u4e3b\u7684LLMs\u5728\u591a\u8bed\u8a00\u6559\u80b2\u4efb\u52a1\u4e2d\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u8bc4\u4f30\u4e86\u591a\u4e2a\u6d41\u884cLLMs\u5728\u516d\u79cd\u975e\u82f1\u8bed\u8bed\u8a00\uff08\u5370\u5730\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\u3001\u6ce2\u65af\u8bed\u3001\u6cf0\u5362\u56fa\u8bed\u3001\u4e4c\u514b\u5170\u8bed\u3001\u6377\u514b\u8bed\uff09\u53ca\u82f1\u8bed\u4e2d\u7684\u56db\u79cd\u6559\u80b2\u4efb\u52a1\u8868\u73b0\u3002", "result": "\u6a21\u578b\u6027\u80fd\u4e0e\u8bed\u8a00\u8bad\u7ec3\u6570\u636e\u91cf\u76f8\u5173\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u8868\u73b0\u8f83\u5dee\uff0c\u4e14\u975e\u82f1\u8bed\u4efb\u52a1\u6027\u80fd\u666e\u904d\u4f4e\u4e8e\u82f1\u8bed\u3002", "conclusion": "\u5efa\u8bae\u5728\u6559\u80b2\u4efb\u52a1\u90e8\u7f72\u524d\u9a8c\u8bc1LLMs\u5728\u76ee\u6807\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u786e\u4fdd\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.17522", "pdf": "https://arxiv.org/pdf/2504.17522", "abs": "https://arxiv.org/abs/2504.17522", "authors": ["Anyi Xiao", "Cihui Yang"], "title": "Towards One-Stage End-to-End Table Structure Recognition with Parallel Regression for Diverse Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "Table structure recognition aims to parse tables in unstructured data into\nmachine-understandable formats. Recent methods address this problem through a\ntwo-stage process or optimized one-stage approaches. However, these methods\neither require multiple networks to be serially trained and perform more\ntime-consuming sequential decoding, or rely on complex post-processing\nalgorithms to parse the logical structure of tables. They struggle to balance\ncross-scenario adaptability, robustness, and computational efficiency. In this\npaper, we propose a one-stage end-to-end table structure parsing network called\nTableCenterNet. This network unifies the prediction of table spatial and\nlogical structure into a parallel regression task for the first time, and\nimplicitly learns the spatial-logical location mapping laws of cells through a\nsynergistic architecture of shared feature extraction layers and task-specific\ndecoding. Compared with two-stage methods, our method is easier to train and\nfaster to infer. Experiments on benchmark datasets show that TableCenterNet can\neffectively parse table structures in diverse scenarios and achieve\nstate-of-the-art performance on the TableGraph-24k dataset. Code is available\nat https://github.com/dreamy-xay/TableCenterNet.", "AI": {"tldr": "TableCenterNet\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u9636\u6bb5\u7aef\u5230\u7aef\u7684\u8868\u683c\u7ed3\u6784\u89e3\u6790\u7f51\u7edc\uff0c\u7edf\u4e00\u4e86\u8868\u683c\u7a7a\u95f4\u548c\u903b\u8f91\u7ed3\u6784\u7684\u9884\u6d4b\uff0c\u901a\u8fc7\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u5c42\u548c\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u7684\u534f\u540c\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u573a\u666f\u9002\u5e94\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0cTableCenterNet\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5355\u9636\u6bb5\u7aef\u5230\u7aef\u7f51\u7edcTableCenterNet\uff0c\u7edf\u4e00\u9884\u6d4b\u8868\u683c\u7a7a\u95f4\u548c\u903b\u8f91\u7ed3\u6784\uff0c\u901a\u8fc7\u5171\u4eab\u7279\u5f81\u63d0\u53d6\u5c42\u548c\u4efb\u52a1\u7279\u5b9a\u89e3\u7801\u5b9e\u73b0\u3002", "result": "\u5728TableGraph-24k\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e14\u8bad\u7ec3\u548c\u63a8\u7406\u66f4\u9ad8\u6548\u3002", "conclusion": "TableCenterNet\u5728\u591a\u6837\u573a\u666f\u4e2d\u6709\u6548\u89e3\u6790\u8868\u683c\u7ed3\u6784\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002"}}
{"id": "2504.17753", "pdf": "https://arxiv.org/pdf/2504.17753", "abs": "https://arxiv.org/abs/2504.17753", "authors": ["Anuja Tayal", "Devika Salunke", "Barbara Di Eugenio", "Paula Allen-Meares", "Eulalia Puig Abril", "Olga Garcia", "Carolyn Dickens", "Andrew Boyd"], "title": "Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT", "categories": ["cs.CL"], "comment": null, "summary": "Conversational assistants are becoming more and more popular, including in\nhealthcare, partly because of the availability and capabilities of Large\nLanguage Models. There is a need for controlled, probing evaluations with real\nstakeholders which can highlight advantages and disadvantages of more\ntraditional architectures and those based on generative AI. We present a\nwithin-group user study to compare two versions of a conversational assistant\nthat allows heart failure patients to ask about salt content in food. One\nversion of the system was developed in-house with a neurosymbolic architecture,\nand one is based on ChatGPT. The evaluation shows that the in-house system is\nmore accurate, completes more tasks and is less verbose than the one based on\nChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors\nand requires fewer clarifications to complete the task. Patients show no\npreference for one over the other.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u4e24\u79cd\u5bf9\u8bdd\u52a9\u624b\uff08\u57fa\u4e8e\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u548c\u57fa\u4e8eChatGPT\uff09\u5728\u5e2e\u52a9\u5fc3\u8870\u60a3\u8005\u67e5\u8be2\u98df\u7269\u76d0\u542b\u91cf\u65f6\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5404\u6709\u4f18\u52a3\uff0c\u4f46\u60a3\u8005\u65e0\u660e\u663e\u504f\u597d\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u5bf9\u8bdd\u52a9\u624b\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\u589e\u591a\uff0c\u9700\u901a\u8fc7\u771f\u5b9e\u7528\u6237\u8bc4\u4f30\u6bd4\u8f83\u4f20\u7edf\u67b6\u6784\u4e0e\u751f\u6210\u5f0fAI\u7684\u4f18\u7f3a\u70b9\u3002", "method": "\u91c7\u7528\u7ec4\u5185\u7528\u6237\u7814\u7a76\uff0c\u6bd4\u8f83\u57fa\u4e8e\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u7684\u81ea\u7814\u7cfb\u7edf\u548c\u57fa\u4e8eChatGPT\u7684\u52a9\u624b\u5728\u4efb\u52a1\u5b8c\u6210\u5ea6\u3001\u51c6\u786e\u6027\u7b49\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u81ea\u7814\u7cfb\u7edf\u66f4\u51c6\u786e\u3001\u4efb\u52a1\u5b8c\u6210\u5ea6\u66f4\u9ad8\u4e14\u66f4\u7b80\u6d01\uff1bChatGPT\u7248\u672c\u8bed\u97f3\u9519\u8bef\u66f4\u5c11\u3001\u9700\u6f84\u6e05\u6b21\u6570\u66f4\u5c11\u3002\u60a3\u8005\u5bf9\u4e24\u8005\u65e0\u504f\u597d\u3002", "conclusion": "\u4e24\u79cd\u67b6\u6784\u5404\u6709\u4f18\u52bf\uff0c\u9700\u6839\u636e\u5177\u4f53\u9700\u6c42\u9009\u62e9\uff1b\u60a3\u8005\u65e0\u660e\u663e\u504f\u597d\uff0c\u8868\u660e\u4e24\u8005\u5747\u53ef\u63a5\u53d7\u3002"}}
{"id": "2504.17524", "pdf": "https://arxiv.org/pdf/2504.17524", "abs": "https://arxiv.org/abs/2504.17524", "authors": ["Junyan Zhang", "Yan Li", "Mengxiao Geng", "Liu Shi", "Qiegen Liu"], "title": "ESDiff: Encoding Strategy-inspired Diffusion Model with Few-shot Learning for Color Image Inpainting", "categories": ["cs.CV"], "comment": "11 pages,10 figures,Submit to tcsvt", "summary": "Image inpainting is a technique used to restore missing or damaged regions of\nan image. Traditional methods primarily utilize information from adjacent\npixels for reconstructing missing areas, while they struggle to preserve\ncomplex details and structures. Simultaneously, models based on deep learning\nnecessitate substantial amounts of training data. To address this challenge, an\nencoding strategy-inspired diffusion model with few-shot learning for color\nimage inpainting is proposed in this paper. The main idea of this novel\nencoding strategy is the deployment of a \"virtual mask\" to construct\nhigh-dimensional objects through mutual perturbations between channels. This\napproach enables the diffusion model to capture diverse image representations\nand detailed features from limited training samples. Moreover, the encoding\nstrategy leverages redundancy between channels, integrates with low-rank\nmethods during iterative inpainting, and incorporates the diffusion model to\nachieve accurate information output. Experimental results indicate that our\nmethod exceeds current techniques in quantitative metrics, and the\nreconstructed images quality has been improved in aspects of texture and\nstructural integrity, leading to more precise and coherent results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f16\u7801\u7b56\u7565\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u5c11\u6837\u672c\u5b66\u4e60\u7684\u5f69\u8272\u56fe\u50cf\u4fee\u590d\uff0c\u901a\u8fc7\u865a\u62df\u63a9\u7801\u548c\u9ad8\u7ef4\u5bf9\u8c61\u6784\u5efa\uff0c\u63d0\u5347\u4e86\u7ec6\u8282\u548c\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u4fdd\u7559\u590d\u6742\u7ec6\u8282\uff0c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u56e0\u6b64\u63d0\u51fa\u4e00\u79cd\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7f16\u7801\u7b56\u7565\uff0c\u5229\u7528\u865a\u62df\u63a9\u7801\u6784\u5efa\u9ad8\u7ef4\u5bf9\u8c61\uff0c\u7ed3\u5408\u4f4e\u79e9\u65b9\u6cd5\u548c\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u7cbe\u786e\u4fee\u590d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u6307\u6807\u548c\u56fe\u50cf\u8d28\u91cf\uff08\u7eb9\u7406\u548c\u7ed3\u6784\u5b8c\u6574\u6027\uff09\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c11\u6837\u672c\u5b66\u4e60\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u548c\u4e00\u81f4\u7684\u56fe\u50cf\u4fee\u590d\u6548\u679c\u3002"}}
{"id": "2504.17768", "pdf": "https://arxiv.org/pdf/2504.17768", "abs": "https://arxiv.org/abs/2504.17768", "authors": ["Piotr Nawrot", "Robert Li", "Renjie Huang", "Sebastian Ruder", "Kelly Marchisio", "Edoardo M. Ponti"], "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sparse attention offers a promising strategy to extend long-context\ncapabilities in Transformer LLMs, yet its viability, its efficiency-accuracy\ntrade-offs, and systematic scaling studies remain unexplored. To address this\ngap, we perform a careful comparison of training-free sparse attention methods\nat varying model scales, sequence lengths, and sparsity levels on a diverse\ncollection of long-sequence tasks-including novel ones that rely on natural\nlanguage while remaining controllable and easy to evaluate. Based on our\nexperiments, we report a series of key findings: 1) an isoFLOPS analysis\nreveals that for very long sequences, larger and highly sparse models are\npreferable to smaller and dense ones. 2) The level of sparsity attainable while\nstatistically guaranteeing accuracy preservation is higher during decoding than\nprefilling, and correlates with model size in the former. 3) There is no clear\nstrategy that performs best across tasks and phases, with different units of\nsparsification or budget adaptivity needed for different scenarios. Even\nmoderate sparsity levels often result in significant performance degradation on\nat least one task, highlighting that sparse attention is not a universal\nsolution. 4) We introduce and validate novel scaling laws specifically tailored\nfor sparse attention, providing evidence that our findings are likely to hold\ntrue beyond our range of experiments. Through these insights, we demonstrate\nthat sparse attention is a key tool to enhance the capabilities of Transformer\nLLMs for processing longer sequences, but requires careful evaluation of\ntrade-offs for performance-sensitive applications.", "AI": {"tldr": "\u7a00\u758f\u6ce8\u610f\u529b\u662f\u6269\u5c55Transformer LLMs\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u5176\u53ef\u884c\u6027\u3001\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\u53ca\u7cfb\u7edf\u6027\u6269\u5c55\u7814\u7a76\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u3001\u5e8f\u5217\u957f\u5ea6\u548c\u7a00\u758f\u5ea6\u7684\u8bad\u7ec3\u65e0\u5173\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5173\u952e\u53d1\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u7a00\u758f\u6ce8\u610f\u529b\u7684\u65b0\u7f29\u653e\u89c4\u5f8b\u3002", "motivation": "\u63a2\u7d22\u7a00\u758f\u6ce8\u610f\u529b\u5728Transformer LLMs\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u5176\u5728\u53ef\u884c\u6027\u3001\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\u53ca\u7cfb\u7edf\u6027\u6269\u5c55\u7814\u7a76\u4e0a\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u3001\u5e8f\u5217\u957f\u5ea6\u548c\u7a00\u758f\u5ea6\u7684\u8bad\u7ec3\u65e0\u5173\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u5728\u591a\u6837\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "1) \u5bf9\u4e8e\u8d85\u957f\u5e8f\u5217\uff0c\u66f4\u5927\u4e14\u9ad8\u5ea6\u7a00\u758f\u7684\u6a21\u578b\u4f18\u4e8e\u5c0f\u4e14\u5bc6\u96c6\u7684\u6a21\u578b\uff1b2) \u89e3\u7801\u9636\u6bb5\u7684\u7a00\u758f\u5ea6\u4e0a\u9650\u9ad8\u4e8e\u9884\u586b\u5145\u9636\u6bb5\uff0c\u4e14\u4e0e\u6a21\u578b\u89c4\u6a21\u76f8\u5173\uff1b3) \u4e0d\u540c\u4efb\u52a1\u548c\u9636\u6bb5\u9700\u8981\u4e0d\u540c\u7684\u7a00\u758f\u5316\u7b56\u7565\uff1b4) \u63d0\u51fa\u4e86\u9488\u5bf9\u7a00\u758f\u6ce8\u610f\u529b\u7684\u65b0\u7f29\u653e\u89c4\u5f8b\u3002", "conclusion": "\u7a00\u758f\u6ce8\u610f\u529b\u662f\u589e\u5f3aTransformer LLMs\u5904\u7406\u957f\u5e8f\u5217\u80fd\u529b\u7684\u5173\u952e\u5de5\u5177\uff0c\u4f46\u9700\u4ed4\u7ec6\u6743\u8861\u6027\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2504.17525", "pdf": "https://arxiv.org/pdf/2504.17525", "abs": "https://arxiv.org/abs/2504.17525", "authors": ["Paul Grimal", "Herv\u00e9 Le Borgne", "Olivier Ferret"], "title": "Text-to-Image Alignment in Denoising-Based Models through Step Selection", "categories": ["cs.CV"], "comment": null, "summary": "Visual generative AI models often encounter challenges related to text-image\nalignment and reasoning limitations. This paper presents a novel method for\nselectively enhancing the signal at critical denoising steps, optimizing image\ngeneration based on input semantics. Our approach addresses the shortcomings of\nearly-stage signal modifications, demonstrating that adjustments made at later\nstages yield superior results. We conduct extensive experiments to validate the\neffectiveness of our method in producing semantically aligned images on\nDiffusion and Flow Matching model, achieving state-of-the-art performance. Our\nresults highlight the importance of a judicious choice of sampling stage to\nimprove performance and overall image alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u5173\u952e\u53bb\u566a\u6b65\u9aa4\u9009\u62e9\u6027\u589e\u5f3a\u4fe1\u53f7\u7684\u65b9\u6cd5\uff0c\u4f18\u5316\u57fa\u4e8e\u8f93\u5165\u8bed\u4e49\u7684\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u751f\u6210AI\u6a21\u578b\u4e2d\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u548c\u63a8\u7406\u9650\u5236\u7684\u6311\u6218\u3002", "method": "\u5728\u540e\u671f\u53bb\u566a\u9636\u6bb5\u8c03\u6574\u4fe1\u53f7\uff0c\u800c\u975e\u65e9\u671f\u9636\u6bb5\uff0c\u4ee5\u4f18\u5316\u56fe\u50cf\u751f\u6210\u3002", "result": "\u5728Diffusion\u548cFlow Matching\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u9009\u62e9\u5408\u9002\u7684\u91c7\u6837\u9636\u6bb5\u5bf9\u63d0\u5347\u6027\u80fd\u548c\u56fe\u50cf\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.16939", "pdf": "https://arxiv.org/pdf/2504.16939", "abs": "https://arxiv.org/abs/2504.16939", "authors": ["Emre Can Acikgoz", "Cheng Qian", "Hongru Wang", "Vardhan Dongre", "Xiusi Chen", "Heng Ji", "Dilek Hakkani-T\u00fcr", "Gokhan Tur"], "title": "A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have propelled conversational\nAI from traditional dialogue systems into sophisticated agents capable of\nautonomous actions, contextual awareness, and multi-turn interactions with\nusers. Yet, fundamental questions about their capabilities, limitations, and\npaths forward remain open. This survey paper presents a desideratum for\nnext-generation Conversational Agents - what has been achieved, what challenges\npersist, and what must be done for more scalable systems that approach\nhuman-level intelligence. To that end, we systematically analyze LLM-driven\nConversational Agents by organizing their capabilities into three primary\ndimensions: (i) Reasoning - logical, systematic thinking inspired by human\nintelligence for decision making, (ii) Monitor - encompassing self-awareness\nand user interaction monitoring, and (iii) Control - focusing on tool\nutilization and policy following. Building upon this, we introduce a novel\ntaxonomy by classifying recent work on Conversational Agents around our\nproposed desideratum. We identify critical research gaps and outline key\ndirections, including realistic evaluations, long-term multi-turn reasoning\nskills, self-evolution capabilities, collaborative and multi-agent task\ncompletion, personalization, and proactivity. This work aims to provide a\nstructured foundation, highlight existing limitations, and offer insights into\npotential future research directions for Conversational Agents, ultimately\nadvancing progress toward Artificial General Intelligence (AGI). We maintain a\ncurated repository of papers at:\nhttps://github.com/emrecanacikgoz/awesome-conversational-agents.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5bf9\u8bdd\u4ee3\u7406\u7684\u73b0\u72b6\u3001\u6311\u6218\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u5173\u952e\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u5c3d\u7ba1LLM\u9a71\u52a8\u7684\u5bf9\u8bdd\u4ee3\u7406\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u80fd\u529b\u3001\u5c40\u9650\u6027\u548c\u672a\u6765\u53d1\u5c55\u8def\u5f84\u4ecd\u5b58\u5728\u8bb8\u591a\u672a\u89e3\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u5206\u6790\u8fd9\u4e9b\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e0b\u4e00\u4ee3\u5bf9\u8bdd\u4ee3\u7406\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5c06\u5bf9\u8bdd\u4ee3\u7406\u7684\u80fd\u529b\u5206\u4e3a\u4e09\u4e2a\u7ef4\u5ea6\uff08\u63a8\u7406\u3001\u76d1\u63a7\u3001\u63a7\u5236\uff09\uff0c\u5e76\u56f4\u7ed5\u8fd9\u4e9b\u7ef4\u5ea6\u5bf9\u73b0\u6709\u7814\u7a76\u8fdb\u884c\u5206\u7c7b\uff0c\u63d0\u51fa\u65b0\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u8bc6\u522b\u4e86\u5173\u952e\u7814\u7a76\u7a7a\u767d\uff0c\u5982\u957f\u671f\u591a\u8f6e\u63a8\u7406\u80fd\u529b\u3001\u81ea\u6211\u8fdb\u5316\u80fd\u529b\u3001\u591a\u4ee3\u7406\u534f\u4f5c\u7b49\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u5bf9\u8bdd\u4ee3\u7406\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u57fa\u7840\uff0c\u6307\u51fa\u4e86\u73b0\u6709\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u4ee5\u63a8\u52a8\u4eba\u5de5\u901a\u7528\u667a\u80fd\uff08AGI\uff09\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.17540", "pdf": "https://arxiv.org/pdf/2504.17540", "abs": "https://arxiv.org/abs/2504.17540", "authors": ["Ahmadreza Shateri", "Negar Nourani", "Morteza Dorrigiv", "Hamid Nasiri"], "title": "An Explainable Nature-Inspired Framework for Monkeypox Diagnosis: Xception Features Combined with NGBoost and African Vultures Optimization Algorithm", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "comment": null, "summary": "The recent global spread of monkeypox, particularly in regions where it has\nnot historically been prevalent, has raised significant public health concerns.\nEarly and accurate diagnosis is critical for effective disease management and\ncontrol. In response, this study proposes a novel deep learning-based framework\nfor the automated detection of monkeypox from skin lesion images, leveraging\nthe power of transfer learning, dimensionality reduction, and advanced machine\nlearning techniques. We utilize the newly developed Monkeypox Skin Lesion\nDataset (MSLD), which includes images of monkeypox, chickenpox, and measles, to\ntrain and evaluate our models. The proposed framework employs the Xception\narchitecture for deep feature extraction, followed by Principal Component\nAnalysis (PCA) for dimensionality reduction, and the Natural Gradient Boosting\n(NGBoost) algorithm for classification. To optimize the model's performance and\ngeneralization, we introduce the African Vultures Optimization Algorithm (AVOA)\nfor hyperparameter tuning, ensuring efficient exploration of the parameter\nspace. Our results demonstrate that the proposed AVOA-NGBoost model achieves\nstate-of-the-art performance, with an accuracy of 97.53%, F1-score of 97.72%\nand an AUC of 97.47%. Additionally, we enhance model interpretability using\nGrad-CAM and LIME techniques, providing insights into the decision-making\nprocess and highlighting key features influencing classification. This\nframework offers a highly precise and efficient diagnostic tool, potentially\naiding healthcare providers in early detection and diagnosis, particularly in\nresource-constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u76ae\u80a4\u75c5\u53d8\u56fe\u50cf\u4e2d\u81ea\u52a8\u68c0\u6d4b\u7334\u75d8\uff0c\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u3001\u964d\u7ef4\u548c\u4f18\u5316\u7b97\u6cd5\uff0c\u53d6\u5f97\u4e86\u9ad8\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7334\u75d8\u5728\u5168\u7403\u975e\u4f20\u7edf\u6d41\u884c\u5730\u533a\u7684\u4f20\u64ad\u5f15\u53d1\u516c\u5171\u536b\u751f\u62c5\u5fe7\uff0c\u65e9\u671f\u51c6\u786e\u8bca\u65ad\u5bf9\u75be\u75c5\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528Xception\u67b6\u6784\u63d0\u53d6\u7279\u5f81\uff0cPCA\u964d\u7ef4\uff0cNGBoost\u5206\u7c7b\uff0c\u5e76\u5f15\u5165AVOA\u7b97\u6cd5\u4f18\u5316\u8d85\u53c2\u6570\u3002", "result": "\u6a21\u578b\u51c6\u786e\u7387\u8fbe97.53%\uff0cF1\u5206\u657097.72%\uff0cAUC 97.47%\uff0c\u5e76\u901a\u8fc7Grad-CAM\u548cLIME\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d44\u6e90\u6709\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u9ad8\u6548\u8bca\u65ad\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u65e9\u671f\u68c0\u6d4b\u548c\u8bca\u65ad\u3002"}}
{"id": "2504.17004", "pdf": "https://arxiv.org/pdf/2504.17004", "abs": "https://arxiv.org/abs/2504.17004", "authors": ["Amin Karbasi", "Omar Montasser", "John Sous", "Grigoris Velegkas"], "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Is automated hallucination detection possible? In this work, we introduce a\ntheoretical framework to analyze the feasibility of automatically detecting\nhallucinations produced by large language models (LLMs). Inspired by the\nclassical Gold-Angluin framework for language identification and its recent\nadaptation to language generation by Kleinberg and Mullainathan, we investigate\nwhether an algorithm, trained on examples drawn from an unknown target language\n$K$ (selected from a countable collection) and given access to an LLM, can\nreliably determine whether the LLM's outputs are correct or constitute\nhallucinations.\n  First, we establish an equivalence between hallucination detection and the\nclassical task of language identification. We prove that any hallucination\ndetection method can be converted into a language identification method, and\nconversely, algorithms solving language identification can be adapted for\nhallucination detection. Given the inherent difficulty of language\nidentification, this implies that hallucination detection is fundamentally\nimpossible for most language collections if the detector is trained using only\ncorrect examples from the target language.\n  Second, we show that the use of expert-labeled feedback, i.e., training the\ndetector with both positive examples (correct statements) and negative examples\n(explicitly labeled incorrect statements), dramatically changes this\nconclusion. Under this enriched training regime, automated hallucination\ndetection becomes possible for all countable language collections.\n  These results highlight the essential role of expert-labeled examples in\ntraining hallucination detectors and provide theoretical support for\nfeedback-based methods, such as reinforcement learning with human feedback\n(RLHF), which have proven critical for reliable LLM deployment.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u81ea\u52a8\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e7b\u89c9\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u4ec5\u4f7f\u7528\u6b63\u786e\u6837\u672c\u8bad\u7ec3\u65f6\u68c0\u6d4b\u4e0d\u53ef\u884c\uff0c\u4f46\u52a0\u5165\u4e13\u5bb6\u6807\u6ce8\u53cd\u9988\u540e\u53ef\u884c\u3002", "motivation": "\u7814\u7a76\u81ea\u52a8\u68c0\u6d4bLLM\u5e7b\u89c9\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u53ef\u9760\u90e8\u7f72LLM\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u57fa\u4e8eGold-Angluin\u6846\u67b6\uff0c\u5c06\u5e7b\u89c9\u68c0\u6d4b\u4e0e\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\u7b49\u4ef7\uff0c\u5206\u6790\u4e0d\u540c\u8bad\u7ec3\u6570\u636e\uff08\u4ec5\u6b63\u786e\u6837\u672c vs. \u4e13\u5bb6\u6807\u6ce8\u6b63\u8d1f\u6837\u672c\uff09\u7684\u5f71\u54cd\u3002", "result": "\u4ec5\u7528\u6b63\u786e\u6837\u672c\u8bad\u7ec3\u65f6\uff0c\u5e7b\u89c9\u68c0\u6d4b\u4e0d\u53ef\u884c\uff1b\u52a0\u5165\u4e13\u5bb6\u6807\u6ce8\u53cd\u9988\u540e\uff0c\u5bf9\u6240\u6709\u53ef\u6570\u8bed\u8a00\u96c6\u5408\u5747\u53ef\u884c\u3002", "conclusion": "\u4e13\u5bb6\u6807\u6ce8\u53cd\u9988\u5bf9\u8bad\u7ec3\u5e7b\u89c9\u68c0\u6d4b\u5668\u81f3\u5173\u91cd\u8981\uff0c\u652f\u6301\u57fa\u4e8e\u53cd\u9988\u7684\u65b9\u6cd5\uff08\u5982RLHF\uff09\u3002"}}
{"id": "2504.17545", "pdf": "https://arxiv.org/pdf/2504.17545", "abs": "https://arxiv.org/abs/2504.17545", "authors": ["Keyang Ye", "Tianjia Shao", "Kun Zhou"], "title": "When Gaussian Meets Surfel: Ultra-fast High-fidelity Radiance Field Rendering", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation for\nradiance field rendering, wherein a set of 2D opaque surfels with\nview-dependent colors represent the coarse-scale geometry and appearance of\nscenes, and a few 3D Gaussians surrounding the surfels supplement fine-scale\nappearance details. The rendering with GESs consists of two passes -- surfels\nare first rasterized through a standard graphics pipeline to produce depth and\ncolor maps, and then Gaussians are splatted with depth testing and color\naccumulation on each pixel order independently. The optimization of GESs from\nmulti-view images is performed through an elaborate coarse-to-fine procedure,\nfaithfully capturing rich scene appearance. The entirely sorting-free rendering\nof GESs not only achieves very fast rates, but also produces view-consistent\nimages, successfully avoiding popping artifacts under view changes. The basic\nGES representation can be easily extended to achieve anti-aliasing in rendering\n(Mip-GES), boosted rendering speeds (Speedy-GES) and compact storage\n(Compact-GES), and reconstruct better scene geometries by replacing 3D\nGaussians with 2D Gaussians (2D-GES). Experimental results show that GESs\nadvance the state-of-the-arts as a compelling representation for ultra-fast\nhigh-fidelity radiance field rendering.", "AI": {"tldr": "Gaussian-enhanced Surfels (GESs) \u662f\u4e00\u79cd\u53cc\u5c3a\u5ea6\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u8f90\u5c04\u573a\u6e32\u67d3\uff0c\u901a\u8fc72D\u4e0d\u900f\u660e\u9762\u5143\u548c3D\u9ad8\u65af\u5206\u5e03\u7ed3\u5408\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8f90\u5c04\u573a\u6e32\u67d3\u4e2d\u901f\u5ea6\u4e0e\u8d28\u91cf\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u9ad8\u6548\u4e14\u9ad8\u4fdd\u771f\u7684\u6e32\u67d3\u65b9\u6848\u3002", "method": "\u4f7f\u75282D\u9762\u5143\u8868\u793a\u7c97\u5c3a\u5ea6\u51e0\u4f55\u548c\u5916\u89c2\uff0c3D\u9ad8\u65af\u5206\u5e03\u8865\u5145\u7ec6\u8282\uff1b\u4e24\u9636\u6bb5\u6e32\u67d3\uff08\u9762\u5143\u5149\u6805\u5316+\u9ad8\u65af\u5206\u5e03\u53e0\u52a0\uff09\uff1b\u901a\u8fc7\u591a\u89c6\u56fe\u56fe\u50cf\u4f18\u5316\u5b9e\u73b0\u7c97\u5230\u7ec6\u7684\u6355\u6349\u3002", "result": "GESs \u5b9e\u73b0\u4e86\u8d85\u5feb\u901f\u7684\u9ad8\u4fdd\u771f\u6e32\u67d3\uff0c\u907f\u514d\u4e86\u89c6\u89d2\u53d8\u5316\u65f6\u7684\u95ea\u70c1\u95ee\u9898\uff0c\u5e76\u652f\u6301\u591a\u79cd\u6269\u5c55\uff08\u5982\u6297\u952f\u9f7f\u3001\u52a0\u901f\u6e32\u67d3\u7b49\uff09\u3002", "conclusion": "GESs \u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u8f90\u5c04\u573a\u8868\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6e32\u67d3\u901f\u5ea6\u548c\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2504.17038", "pdf": "https://arxiv.org/pdf/2504.17038", "abs": "https://arxiv.org/abs/2504.17038", "authors": ["Christian D. Newman", "Brandon Scholten", "Sophia Testa", "Joshua A. C. Behler", "Syreen Banabilah", "Michael L. Collard", "Michael J. Decker", "Mohamed Wiem Mkaouer", "Marcos Zampieri", "Eman Abdullah AlOmar", "Reem Alsuhaibani", "Anthony Peruma", "Jonathan I. Maletic"], "title": "SCALAR: A Part-of-speech Tagger for Identifiers", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "The paper presents the Source Code Analysis and Lexical Annotation Runtime\n(SCALAR), a tool specialized for mapping (annotating) source code identifier\nnames to their corresponding part-of-speech tag sequence (grammar pattern).\nSCALAR's internal model is trained using scikit-learn's\nGradientBoostingClassifier in conjunction with a manually-curated oracle of\nidentifier names and their grammar patterns. This specializes the tagger to\nrecognize the unique structure of the natural language used by developers to\ncreate all types of identifiers (e.g., function names, variable names etc.).\nSCALAR's output is compared with a previous version of the tagger, as well as a\nmodern off-the-shelf part-of-speech tagger to show how it improves upon other\ntaggers' output for annotating identifiers. The code is available on Github", "AI": {"tldr": "SCALAR\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u5c06\u6e90\u4ee3\u7801\u6807\u8bc6\u7b26\u540d\u79f0\u6620\u5c04\u5230\u5176\u5bf9\u5e94\u8bcd\u6027\u6807\u8bb0\u5e8f\u5217\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u63d0\u5347\u6807\u6ce8\u51c6\u786e\u6027\u3002", "motivation": "\u5f00\u53d1\u8005\u4f7f\u7528\u7684\u81ea\u7136\u8bed\u8a00\u7ed3\u6784\u72ec\u7279\uff0c\u73b0\u6709\u8bcd\u6027\u6807\u6ce8\u5de5\u5177\u65e0\u6cd5\u51c6\u786e\u6807\u6ce8\u6e90\u4ee3\u7801\u6807\u8bc6\u7b26\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u5de5\u5177\u3002", "method": "\u4f7f\u7528scikit-learn\u7684GradientBoostingClassifier\u8bad\u7ec3\u6a21\u578b\uff0c\u7ed3\u5408\u624b\u52a8\u6574\u7406\u7684\u6807\u8bc6\u7b26\u540d\u79f0\u548c\u8bed\u6cd5\u6a21\u5f0f\u6570\u636e\u96c6\u3002", "result": "SCALAR\u5728\u6807\u6ce8\u6807\u8bc6\u7b26\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7248\u672c\u548c\u73b0\u4ee3\u901a\u7528\u8bcd\u6027\u6807\u6ce8\u5de5\u5177\u3002", "conclusion": "SCALAR\u4e3a\u6e90\u4ee3\u7801\u6807\u8bc6\u7b26\u7684\u8bcd\u6027\u6807\u6ce8\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.17547", "pdf": "https://arxiv.org/pdf/2504.17547", "abs": "https://arxiv.org/abs/2504.17547", "authors": ["Jiaqi Deng", "Zonghan Wu", "Huan Huo", "Guandong Xu"], "title": "A Comprehensive Survey of Knowledge-Based Vision Question Answering Systems: The Lifecycle of Knowledge in Visual Reasoning Task", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": "20 pages, 5 figures, 4 tables", "summary": "Knowledge-based Vision Question Answering (KB-VQA) extends general Vision\nQuestion Answering (VQA) by not only requiring the understanding of visual and\ntextual inputs but also extensive range of knowledge, enabling significant\nadvancements across various real-world applications. KB-VQA introduces unique\nchallenges, including the alignment of heterogeneous information from diverse\nmodalities and sources, the retrieval of relevant knowledge from noisy or\nlarge-scale repositories, and the execution of complex reasoning to infer\nanswers from the combined context. With the advancement of Large Language\nModels (LLMs), KB-VQA systems have also undergone a notable transformation,\nwhere LLMs serve as powerful knowledge repositories, retrieval-augmented\ngenerators and strong reasoners. Despite substantial progress, no comprehensive\nsurvey currently exists that systematically organizes and reviews the existing\nKB-VQA methods. This survey aims to fill this gap by establishing a structured\ntaxonomy of KB-VQA approaches, and categorizing the systems into main stages:\nknowledge representation, knowledge retrieval, and knowledge reasoning. By\nexploring various knowledge integration techniques and identifying persistent\nchallenges, this work also outlines promising future research directions,\nproviding a foundation for advancing KB-VQA models and their applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u662f\u4e00\u7bc7\u5173\u4e8e\u77e5\u8bc6\u9a71\u52a8\u7684\u89c6\u89c9\u95ee\u7b54\uff08KB-VQA\uff09\u7684\u7efc\u8ff0\uff0c\u7cfb\u7edf\u6574\u7406\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "KB-VQA\u7ed3\u5408\u89c6\u89c9\u3001\u6587\u672c\u548c\u5e7f\u6cdb\u77e5\u8bc6\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7efc\u8ff0\u3002", "method": "\u901a\u8fc7\u5efa\u7acb\u5206\u7c7b\u6846\u67b6\uff0c\u5c06KB-VQA\u65b9\u6cd5\u5206\u4e3a\u77e5\u8bc6\u8868\u793a\u3001\u77e5\u8bc6\u68c0\u7d22\u548c\u77e5\u8bc6\u63a8\u7406\u4e09\u4e2a\u9636\u6bb5\u3002", "result": "\u7efc\u8ff0\u4e86\u73b0\u6709\u6280\u672f\uff0c\u603b\u7ed3\u4e86\u77e5\u8bc6\u6574\u5408\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u6301\u7eed\u6311\u6218\u3002", "conclusion": "\u4e3aKB-VQA\u6a21\u578b\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.17551", "pdf": "https://arxiv.org/pdf/2504.17551", "abs": "https://arxiv.org/abs/2504.17551", "authors": ["Lin Che", "Yizi Chen", "Tanhua Jin", "Martin Raubal", "Konrad Schindler", "Peter Kiefer"], "title": "Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 7 figures, preprint version", "summary": "Urban land use classification and mapping are critical for urban planning,\nresource management, and environmental monitoring. Existing remote sensing\ntechniques often lack precision in complex urban environments due to the\nabsence of ground-level details. Unlike aerial perspectives, street view images\nprovide a ground-level view that captures more human and social activities\nrelevant to land use in complex urban scenes. Existing street view-based\nmethods primarily rely on supervised classification, which is challenged by the\nscarcity of high-quality labeled data and the difficulty of generalizing across\ndiverse urban landscapes. This study introduces an unsupervised contrastive\nclustering model for street view images with a built-in geographical prior, to\nenhance clustering performance. When combined with a simple visual assignment\nof the clusters, our approach offers a flexible and customizable solution to\nland use mapping, tailored to the specific needs of urban planners. We\nexperimentally show that our method can generate land use maps from geotagged\nstreet view image datasets of two cities. As our methodology relies on the\nuniversal spatial coherence of geospatial data (\"Tobler's law\"), it can be\nadapted to various settings where street view images are available, to enable\nscalable, unsupervised land use mapping and updating. The code will be\navailable at https://github.com/lin102/CCGP.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5730\u7406\u5148\u9a8c\u7684\u65e0\u76d1\u7763\u5bf9\u6bd4\u805a\u7c7b\u6a21\u578b\uff0c\u7528\u4e8e\u8857\u666f\u56fe\u50cf\u7684\u571f\u5730\u5229\u7528\u5206\u7c7b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7f3a\u4e4f\u7cbe\u5ea6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u6280\u672f\u56e0\u7f3a\u4e4f\u5730\u9762\u7ec6\u8282\u800c\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u800c\u8857\u666f\u56fe\u50cf\u80fd\u6355\u6349\u66f4\u591a\u4eba\u7c7b\u548c\u793e\u4f1a\u6d3b\u52a8\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u76d1\u7763\u5206\u7c7b\u65b9\u6cd5\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u7684\u7a00\u7f3a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5730\u7406\u5148\u9a8c\u7684\u65e0\u76d1\u7763\u5bf9\u6bd4\u805a\u7c7b\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u7b80\u5355\u7684\u89c6\u89c9\u5206\u914d\u5b9e\u73b0\u571f\u5730\u5229\u7528\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u4e24\u4e2a\u57ce\u5e02\u7684\u8857\u666f\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u751f\u6210\u571f\u5730\u5229\u7528\u5730\u56fe\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u5730\u7406\u6570\u636e\u7684\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u53ef\u9002\u7528\u4e8e\u5404\u79cd\u8857\u666f\u56fe\u50cf\u53ef\u7528\u573a\u666f\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u65e0\u76d1\u7763\u571f\u5730\u5229\u7528\u5206\u7c7b\u4e0e\u66f4\u65b0\u3002"}}
{"id": "2504.17449", "pdf": "https://arxiv.org/pdf/2504.17449", "abs": "https://arxiv.org/abs/2504.17449", "authors": ["Jun Zhang", "Jue Wang", "Huan Li", "Lidan Shou", "Ke Chen", "Gang Chen", "Qin Xie", "Guiming Xie", "Xuejian Gong"], "title": "HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by VLDBJ 2025", "summary": "The significant computational demands of pretrained language models (PLMs),\nwhich often require dedicated hardware, present a substantial challenge in\nserving them efficiently, especially in multi-tenant environments. To address\nthis, we introduce HMI, a Hierarchical knowledge management-based Multi-tenant\nInference system, designed to manage tenants with distinct PLMs\nresource-efficiently. Our approach is three-fold: Firstly, we categorize PLM\nknowledge into general, domain-specific, and task-specific. Leveraging insights\non knowledge acquisition across different model layers, we construct\nhierarchical PLMs (hPLMs) by extracting and storing knowledge at different\nlevels, significantly reducing GPU memory usage per tenant. Secondly, we\nestablish hierarchical knowledge management for hPLMs generated by various\ntenants in HMI. We manage domain-specific knowledge with acceptable storage\nincreases by constructing and updating domain-specific knowledge trees based on\nfrequency. We manage task-specific knowledge within limited GPU memory through\nparameter swapping. Finally, we propose system optimizations to enhance\nresource utilization and inference throughput. These include fine-grained\npipelining via hierarchical knowledge prefetching to overlap CPU and I/O\noperations with GPU computations, and optimizing parallel implementations with\nbatched matrix multiplications. Our experimental results demonstrate that the\nproposed HMI can efficiently serve up to 10,000 hPLMs (hBERTs and hGPTs) on a\nsingle GPU, with only a negligible compromise in accuracy.", "AI": {"tldr": "HMI\u662f\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u77e5\u8bc6\u7ba1\u7406\u7684\u591a\u79df\u6237\u63a8\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u9ad8\u6548\u7ba1\u7406\u4e0d\u540cPLM\u7684\u79df\u6237\uff0c\u901a\u8fc7\u5206\u5c42\u77e5\u8bc6\u63d0\u53d6\u548c\u5b58\u50a8\u51cf\u5c11GPU\u5185\u5b58\u4f7f\u7528\uff0c\u652f\u6301\u5355GPU\u4e0a\u8fd0\u884c\u591a\u8fbe10,000\u4e2ahPLM\u3002", "motivation": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u7684\u9ad8\u8ba1\u7b97\u9700\u6c42\u5728\u591a\u79df\u6237\u73af\u5883\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u4e13\u7528\u786c\u4ef6\u652f\u6301\u3002HMI\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u7ba1\u7406\u3002", "method": "1. \u5c06PLM\u77e5\u8bc6\u5206\u4e3a\u901a\u7528\u3001\u9886\u57df\u7279\u5b9a\u548c\u4efb\u52a1\u7279\u5b9a\u4e09\u7c7b\uff0c\u6784\u5efa\u5206\u5c42PLM\uff08hPLM\uff09\uff1b2. \u901a\u8fc7\u9891\u7387\u66f4\u65b0\u9886\u57df\u77e5\u8bc6\u6811\u548c\u53c2\u6570\u4ea4\u6362\u7ba1\u7406\u4efb\u52a1\u77e5\u8bc6\uff1b3. \u7cfb\u7edf\u4f18\u5316\u5305\u62ec\u5206\u5c42\u77e5\u8bc6\u9884\u53d6\u548c\u6279\u91cf\u77e9\u9635\u4e58\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cHMI\u5728\u5355GPU\u4e0a\u53ef\u9ad8\u6548\u8fd0\u884c10,000\u4e2ahPLM\uff08hBERT\u548chGPT\uff09\uff0c\u51c6\u786e\u6027\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "HMI\u901a\u8fc7\u5206\u5c42\u77e5\u8bc6\u7ba1\u7406\u548c\u7cfb\u7edf\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u79df\u6237\u73af\u5883\u4e2dPLM\u7684\u8d44\u6e90\u5229\u7528\u7387\u548c\u63a8\u7406\u541e\u5410\u91cf\u3002"}}
{"id": "2504.17582", "pdf": "https://arxiv.org/pdf/2504.17582", "abs": "https://arxiv.org/abs/2504.17582", "authors": ["Zebo Huang", "Yinghui Wang"], "title": "Occlusion-Aware Self-Supervised Monocular Depth Estimation for Weak-Texture Endoscopic Images", "categories": ["cs.CV"], "comment": null, "summary": "We propose a self-supervised monocular depth estimation network tailored for\nendoscopic scenes, aiming to infer depth within the gastrointestinal tract from\nmonocular images. Existing methods, though accurate, typically assume\nconsistent illumination, which is often violated due to dynamic lighting and\nocclusions caused by GI motility. These variations lead to incorrect geometric\ninterpretations and unreliable self-supervised signals, degrading depth\nreconstruction quality. To address this, we introduce an occlusion-aware\nself-supervised framework. First, we incorporate an occlusion mask for data\naugmentation, generating pseudo-labels by simulating viewpoint-dependent\nocclusion scenarios. This enhances the model's ability to learn robust depth\nfeatures under partial visibility. Second, we leverage semantic segmentation\nguided by non-negative matrix factorization, clustering convolutional\nactivations to generate pseudo-labels in texture-deprived regions, thereby\nimproving segmentation accuracy and mitigating information loss from lighting\nchanges. Experimental results on the SCARED dataset show that our method\nachieves state-of-the-art performance in self-supervised depth estimation.\nAdditionally, evaluations on the Endo-SLAM and SERV-CT datasets demonstrate\nstrong generalization across diverse endoscopic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5185\u7aa5\u955c\u573a\u666f\u7684\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7f51\u7edc\uff0c\u901a\u8fc7\u906e\u6321\u611f\u77e5\u6846\u67b6\u548c\u8bed\u4e49\u5206\u5272\u63d0\u5347\u6df1\u5ea6\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u5149\u7167\u4e00\u81f4\uff0c\u4f46\u5185\u7aa5\u955c\u573a\u666f\u4e2d\u52a8\u6001\u5149\u7167\u548c\u906e\u6321\u5bfc\u81f4\u51e0\u4f55\u89e3\u91ca\u9519\u8bef\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5f15\u5165\u906e\u6321\u611f\u77e5\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7ed3\u5408\u906e\u6321\u63a9\u6a21\u6570\u636e\u589e\u5f3a\u548c\u8bed\u4e49\u5206\u5272\uff0c\u63d0\u5347\u7279\u5f81\u5b66\u4e60\u548c\u5206\u5272\u7cbe\u5ea6\u3002", "result": "\u5728SCARED\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u5728Endo-SLAM\u548cSERV-CT\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5185\u7aa5\u955c\u573a\u666f\u4e2d\u7684\u5149\u7167\u548c\u906e\u6321\u95ee\u9898\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.17594", "pdf": "https://arxiv.org/pdf/2504.17594", "abs": "https://arxiv.org/abs/2504.17594", "authors": ["Zhaofeng Si", "Siwei Lyu"], "title": "Tamper-evident Image using JPEG Fixed Points", "categories": ["cs.CV", "I.4.7"], "comment": "6 pages, 6 figures", "summary": "An intriguing phenomenon about JPEG compression has been observed since two\ndecades ago- after repeating JPEG compression and decompression, it leads to a\nstable image that does not change anymore, which is a fixed point. In this\nwork, we prove the existence of fixed points in the essential JPEG procedures.\nWe analyze JPEG compression and decompression processes, revealing the\nexistence of fixed points that can be reached within a few iterations. These\nfixed points are diverse and preserve the image's visual quality, ensuring\nminimal distortion. This result is used to develop a method to create a\ntamper-evident image from the original authentic image, which can expose\ntampering operations by showing deviations from the fixed point image.", "AI": {"tldr": "JPEG\u91cd\u590d\u538b\u7f29\u548c\u89e3\u538b\u4f1a\u8fbe\u5230\u4e00\u4e2a\u7a33\u5b9a\u70b9\uff08\u56fa\u5b9a\u70b9\uff09\uff0c\u672c\u6587\u8bc1\u660e\u4e86\u56fa\u5b9a\u70b9\u7684\u5b58\u5728\u6027\uff0c\u5e76\u5229\u7528\u8fd9\u4e00\u7279\u6027\u5f00\u53d1\u4e86\u4e00\u79cd\u9632\u7be1\u6539\u56fe\u50cf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76JPEG\u538b\u7f29\u8fc7\u7a0b\u4e2d\u7684\u56fa\u5b9a\u70b9\u73b0\u8c61\uff0c\u4ee5\u5f00\u53d1\u9632\u7be1\u6539\u56fe\u50cf\u6280\u672f\u3002", "method": "\u5206\u6790JPEG\u538b\u7f29\u548c\u89e3\u538b\u8fc7\u7a0b\uff0c\u8bc1\u660e\u56fa\u5b9a\u70b9\u7684\u5b58\u5728\u6027\uff0c\u5e76\u5229\u7528\u56fa\u5b9a\u70b9\u5f00\u53d1\u9632\u7be1\u6539\u65b9\u6cd5\u3002", "result": "\u56fa\u5b9a\u70b9\u5b58\u5728\u4e14\u53ef\u5728\u51e0\u6b21\u8fed\u4ee3\u5185\u8fbe\u5230\uff0c\u56fe\u50cf\u89c6\u89c9\u8d28\u91cf\u4fdd\u6301\u826f\u597d\uff0c\u5931\u771f\u6700\u5c0f\u3002", "conclusion": "\u56fa\u5b9a\u70b9\u53ef\u7528\u4e8e\u5f00\u53d1\u9632\u7be1\u6539\u56fe\u50cf\u6280\u672f\uff0c\u901a\u8fc7\u68c0\u6d4b\u4e0e\u56fa\u5b9a\u70b9\u7684\u504f\u5dee\u6765\u66b4\u9732\u7be1\u6539\u64cd\u4f5c\u3002"}}
{"id": "2504.17595", "pdf": "https://arxiv.org/pdf/2504.17595", "abs": "https://arxiv.org/abs/2504.17595", "authors": ["Boyue Xu", "Yi Xu", "Ruichao Hou", "Jia Bei", "Tongwei Ren", "Gangshan Wu"], "title": "RGB-D Tracking via Hierarchical Modality Aggregation and Distribution Network", "categories": ["cs.CV"], "comment": null, "summary": "The integration of dual-modal features has been pivotal in advancing\nRGB-Depth (RGB-D) tracking. However, current trackers are less efficient and\nfocus solely on single-level features, resulting in weaker robustness in fusion\nand slower speeds that fail to meet the demands of real-world applications. In\nthis paper, we introduce a novel network, denoted as HMAD (Hierarchical\nModality Aggregation and Distribution), which addresses these challenges. HMAD\nleverages the distinct feature representation strengths of RGB and depth\nmodalities, giving prominence to a hierarchical approach for feature\ndistribution and fusion, thereby enhancing the robustness of RGB-D tracking.\nExperimental results on various RGB-D datasets demonstrate that HMAD achieves\nstate-of-the-art performance. Moreover, real-world experiments further validate\nHMAD's capacity to effectively handle a spectrum of tracking challenges in\nreal-time scenarios.", "AI": {"tldr": "HMAD\u7f51\u7edc\u901a\u8fc7\u5c42\u6b21\u5316\u6a21\u6001\u805a\u5408\u4e0e\u5206\u5e03\uff0c\u63d0\u5347\u4e86RGB-D\u8ddf\u8e2a\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u5f53\u524dRGB-D\u8ddf\u8e2a\u5668\u6548\u7387\u4f4e\u4e14\u4ec5\u5173\u6ce8\u5355\u5c42\u7279\u5f81\uff0c\u5bfc\u81f4\u878d\u5408\u9c81\u68d2\u6027\u5dee\u4e14\u901f\u5ea6\u6162\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51faHMAD\u7f51\u7edc\uff0c\u5229\u7528RGB\u548c\u6df1\u5ea6\u6a21\u6001\u7684\u72ec\u7279\u7279\u5f81\u8868\u793a\u4f18\u52bf\uff0c\u91c7\u7528\u5c42\u6b21\u5316\u7279\u5f81\u5206\u5e03\u4e0e\u878d\u5408\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2aRGB-D\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u6709\u6548\u5e94\u5bf9\u591a\u79cd\u8ddf\u8e2a\u6311\u6218\u3002", "conclusion": "HMAD\u901a\u8fc7\u5c42\u6b21\u5316\u6a21\u6001\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86RGB-D\u8ddf\u8e2a\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6027\u3002"}}
{"id": "2504.17609", "pdf": "https://arxiv.org/pdf/2504.17609", "abs": "https://arxiv.org/abs/2504.17609", "authors": ["Fengchun Liu", "Tong Zhang", "Chunying Zhang"], "title": "STCL:Curriculum learning Strategies for deep learning image steganography models", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": null, "summary": "Aiming at the problems of poor quality of steganographic images and slow\nnetwork convergence of image steganography models based on deep learning, this\npaper proposes a Steganography Curriculum Learning training strategy (STCL) for\ndeep learning image steganography models. So that only easy images are selected\nfor training when the model has poor fitting ability at the initial stage, and\ngradually expand to more difficult images, the strategy includes a difficulty\nevaluation strategy based on the teacher model and an knee point-based training\nscheduling strategy. Firstly, multiple teacher models are trained, and the\nconsistency of the quality of steganographic images under multiple teacher\nmodels is used as the difficulty score to construct the training subsets from\neasy to difficult. Secondly, a training control strategy based on knee points\nis proposed to reduce the possibility of overfitting on small training sets and\naccelerate the training process. Experimental results on three large public\ndatasets, ALASKA2, VOC2012 and ImageNet, show that the proposed image\nsteganography scheme is able to improve the model performance under multiple\nalgorithmic frameworks, which not only has a high PSNR, SSIM score, and\ndecoding accuracy, but also the steganographic images generated by the model\nunder the training of the STCL strategy have a low steganography analysis\nscores. You can find our code at\n\\href{https://github.com/chaos-boops/STCL}{https://github.com/chaos-boops/STCL}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684\u9690\u5199\u8bad\u7ec3\u7b56\u7565\uff08STCL\uff09\uff0c\u901a\u8fc7\u9010\u6b65\u589e\u52a0\u8bad\u7ec3\u96be\u5ea6\uff0c\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u9690\u5199\u6a21\u578b\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u9690\u5199\u6a21\u578b\u56fe\u50cf\u8d28\u91cf\u5dee\u548c\u7f51\u7edc\u6536\u655b\u6162\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "method": "1. \u57fa\u4e8e\u6559\u5e08\u6a21\u578b\u7684\u96be\u5ea6\u8bc4\u4f30\u7b56\u7565\uff1b2. \u57fa\u4e8e\u62d0\u70b9\u7684\u8bad\u7ec3\u8c03\u5ea6\u7b56\u7565\u3002\u901a\u8fc7\u9010\u6b65\u6269\u5c55\u8bad\u7ec3\u5b50\u96c6\u4ece\u6613\u5230\u96be\uff0c\u907f\u514d\u8fc7\u62df\u5408\u5e76\u52a0\u901f\u8bad\u7ec3\u3002", "result": "\u5728ALASKA2\u3001VOC2012\u548cImageNet\u6570\u636e\u96c6\u4e0a\uff0cSTCL\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684PSNR\u3001SSIM\u548c\u89e3\u7801\u51c6\u786e\u7387\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u9690\u5199\u5206\u6790\u5f97\u5206\u3002", "conclusion": "STCL\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u6df1\u5ea6\u5b66\u4e60\u9690\u5199\u6a21\u578b\u7684\u6027\u80fd\uff0c\u751f\u6210\u7684\u9690\u5199\u56fe\u50cf\u8d28\u91cf\u9ad8\u4e14\u5b89\u5168\u6027\u5f3a\u3002"}}
{"id": "2504.17619", "pdf": "https://arxiv.org/pdf/2504.17619", "abs": "https://arxiv.org/abs/2504.17619", "authors": ["Catarina P. Coutinho", "Aneeqa Merhab", "Janko Petkovic", "Ferdinando Zanchetta", "Rita Fioresi"], "title": "Enhancing CNNs robustness to occlusions with bioinspired filters for border completion", "categories": ["cs.CV", "cs.AI"], "comment": "Submitted to the 7th International Conference on Geometric Science of\n  Information", "summary": "We exploit the mathematical modeling of the visual cortex mechanism for\nborder completion to define custom filters for CNNs. We see a consistent\nimprovement in performance, particularly in accuracy, when our modified LeNet 5\nis tested with occluded MNIST images.", "AI": {"tldr": "\u5229\u7528\u89c6\u89c9\u76ae\u5c42\u8fb9\u754c\u8865\u5168\u673a\u5236\u8bbe\u8ba1CNN\u6ee4\u6ce2\u5668\uff0c\u6539\u8fdbLeNet 5\u5728\u906e\u6321MNIST\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u76ae\u5c42\u673a\u5236\u5728CNN\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u906e\u6321\u56fe\u50cf\u7684\u8bc6\u522b\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u89c6\u89c9\u76ae\u5c42\u8fb9\u754c\u8865\u5168\u7684\u6570\u5b66\u6a21\u578b\u8bbe\u8ba1\u81ea\u5b9a\u4e49\u6ee4\u6ce2\u5668\uff0c\u6539\u8fdbLeNet 5\u3002", "result": "\u5728\u906e\u6321MNIST\u56fe\u50cf\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u89c6\u89c9\u76ae\u5c42\u673a\u5236\u53ef\u6709\u6548\u4f18\u5316CNN\u6ee4\u6ce2\u5668\u8bbe\u8ba1\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.17626", "pdf": "https://arxiv.org/pdf/2504.17626", "abs": "https://arxiv.org/abs/2504.17626", "authors": ["Ashish Singh", "Michael J. Jones", "Kuan-Chuan Peng", "Anoop Cherian", "Moitreya Chatterjee", "Erik Learned-Miller"], "title": "Improving Open-World Object Localization by Discovering Background", "categories": ["cs.CV"], "comment": null, "summary": "Our work addresses the problem of learning to localize objects in an\nopen-world setting, i.e., given the bounding box information of a limited\nnumber of object classes during training, the goal is to localize all objects,\nbelonging to both the training and unseen classes in an image, during\ninference. Towards this end, recent work in this area has focused on improving\nthe characterization of objects either explicitly by proposing new objective\nfunctions (localization quality) or implicitly using object-centric\nauxiliary-information, such as depth information, pixel/region affinity map\netc. In this work, we address this problem by incorporating background\ninformation to guide the learning of the notion of objectness. Specifically, we\npropose a novel framework to discover background regions in an image and train\nan object proposal network to not detect any objects in these regions. We\nformulate the background discovery task as that of identifying image regions\nthat are not discriminative, i.e., those that are redundant and constitute low\ninformation content. We conduct experiments on standard benchmarks to showcase\nthe effectiveness of our proposed approach and observe significant improvements\nover the previous state-of-the-art approaches for this task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u5b9a\u4f4d\u5bf9\u8c61\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u80cc\u666f\u4fe1\u606f\u6307\u5bfc\u5bf9\u8c61\u6027\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u8bad\u7ec3\u65f6\u4ec5\u6709\u9650\u7c7b\u522b\u5bf9\u8c61\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u5728\u63a8\u7406\u65f6\u5b9a\u4f4d\u6240\u6709\u5bf9\u8c61\uff08\u5305\u62ec\u672a\u89c1\u7c7b\u522b\uff09\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u53d1\u73b0\u56fe\u50cf\u4e2d\u7684\u80cc\u666f\u533a\u57df\u5e76\u8bad\u7ec3\u5bf9\u8c61\u63d0\u8bae\u7f51\u7edc\u4e0d\u5728\u8fd9\u4e9b\u533a\u57df\u68c0\u6d4b\u5bf9\u8c61\uff0c\u80cc\u666f\u53d1\u73b0\u4efb\u52a1\u88ab\u5b9a\u4e49\u4e3a\u8bc6\u522b\u975e\u5224\u522b\u6027\u533a\u57df\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u80cc\u666f\u4fe1\u606f\u6307\u5bfc\u5bf9\u8c61\u6027\u5b66\u4e60\uff0c\u8be5\u65b9\u6cd5\u5728\u5f00\u653e\u4e16\u754c\u5bf9\u8c61\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2504.17636", "pdf": "https://arxiv.org/pdf/2504.17636", "abs": "https://arxiv.org/abs/2504.17636", "authors": ["Vojtech Panek", "Qunjie Zhou", "Yaqing Ding", "S\u00e9rgio Agostinho", "Zuzana Kukelova", "Torsten Sattler", "Laura Leal-Taix\u00e9"], "title": "A Guide to Structureless Visual Localization", "categories": ["cs.CV", "I.2.10; I.4.8; I.4.9"], "comment": null, "summary": "Visual localization algorithms, i.e., methods that estimate the camera pose\nof a query image in a known scene, are core components of many applications,\nincluding self-driving cars and augmented / mixed reality systems.\nState-of-the-art visual localization algorithms are structure-based, i.e., they\nstore a 3D model of the scene and use 2D-3D correspondences between the query\nimage and 3D points in the model for camera pose estimation. While such\napproaches are highly accurate, they are also rather inflexible when it comes\nto adjusting the underlying 3D model after changes in the scene. Structureless\nlocalization approaches represent the scene as a database of images with known\nposes and thus offer a much more flexible representation that can be easily\nupdated by adding or removing images. Although there is a large amount of\nliterature on structure-based approaches, there is significantly less work on\nstructureless methods. Hence, this paper is dedicated to providing the, to the\nbest of our knowledge, first comprehensive discussion and comparison of\nstructureless methods. Extensive experiments show that approaches that use a\nhigher degree of classical geometric reasoning generally achieve higher pose\naccuracy. In particular, approaches based on classical absolute or\nsemi-generalized relative pose estimation outperform very recent methods based\non pose regression by a wide margin. Compared with state-of-the-art\nstructure-based approaches, the flexibility of structureless methods comes at\nthe cost of (slightly) lower pose accuracy, indicating an interesting direction\nfor future work.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u5e76\u6bd4\u8f83\u4e86\u65e0\u7ed3\u6784\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u53d1\u73b0\u57fa\u4e8e\u7ecf\u5178\u51e0\u4f55\u63a8\u7406\u7684\u65b9\u6cd5\u5728\u59ff\u6001\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u59ff\u6001\u56de\u5f52\u7684\u65b9\u6cd5\uff0c\u4f46\u7075\u6d3b\u6027\u4ee5\u7a0d\u4f4e\u7684\u7cbe\u5ea6\u4e3a\u4ee3\u4ef7\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ed3\u6784\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u867d\u7cbe\u786e\u4f46\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u800c\u65e0\u7ed3\u6784\u65b9\u6cd5\u66f4\u6613\u66f4\u65b0\u4f46\u7814\u7a76\u8f83\u5c11\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u591a\u79cd\u65e0\u7ed3\u6784\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u7ecf\u5178\u51e0\u4f55\u63a8\u7406\u548c\u59ff\u6001\u56de\u5f52\u7684\u65b9\u6cd5\u3002", "result": "\u57fa\u4e8e\u7ecf\u5178\u51e0\u4f55\u63a8\u7406\u7684\u65b9\u6cd5\uff08\u5982\u7edd\u5bf9\u6216\u534a\u5e7f\u4e49\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\uff09\u5728\u59ff\u6001\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u59ff\u6001\u56de\u5f52\u7684\u65b9\u6cd5\u3002", "conclusion": "\u65e0\u7ed3\u6784\u65b9\u6cd5\u5728\u7075\u6d3b\u6027\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u4f46\u7cbe\u5ea6\u7a0d\u4f4e\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.17643", "pdf": "https://arxiv.org/pdf/2504.17643", "abs": "https://arxiv.org/abs/2504.17643", "authors": ["Steve G\u00f6ring"], "title": "CLIPSE -- a minimalistic CLIP-based image search engine for research", "categories": ["cs.CV"], "comment": null, "summary": "A brief overview of CLIPSE, a self-hosted image search engine with the main\napplication of research, is provided. In general, CLIPSE uses CLIP embeddings\nto process the images and also the text queries. The overall framework is\ndesigned with simplicity to enable easy extension and usage. Two benchmark\nscenarios are described and evaluated, covering indexing and querying time. It\nis shown that CLIPSE is capable of handling smaller datasets; for larger\ndatasets, a distributed approach with several instances should be considered.", "AI": {"tldr": "CLIPSE\u662f\u4e00\u4e2a\u81ea\u6258\u7ba1\u7684\u56fe\u50cf\u641c\u7d22\u5f15\u64ce\uff0c\u4e3b\u8981\u7528\u4e8e\u7814\u7a76\uff0c\u5229\u7528CLIP\u5d4c\u5165\u5904\u7406\u56fe\u50cf\u548c\u6587\u672c\u67e5\u8be2\uff0c\u8bbe\u8ba1\u7b80\u5355\u6613\u6269\u5c55\u3002", "motivation": "\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5355\u9ad8\u6548\u7684\u56fe\u50cf\u641c\u7d22\u5de5\u5177\u3002", "method": "\u4f7f\u7528CLIP\u5d4c\u5165\u5904\u7406\u56fe\u50cf\u548c\u6587\u672c\u67e5\u8be2\uff0c\u8bbe\u8ba1\u7b80\u6d01\u6846\u67b6\u3002", "result": "\u5728\u5c0f\u578b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5927\u578b\u6570\u636e\u96c6\u9700\u5206\u5e03\u5f0f\u5904\u7406\u3002", "conclusion": "CLIPSE\u9002\u5408\u5c0f\u578b\u6570\u636e\u96c6\uff0c\u5927\u578b\u6570\u636e\u96c6\u9700\u6269\u5c55\u4e3a\u5206\u5e03\u5f0f\u67b6\u6784\u3002"}}
{"id": "2504.17670", "pdf": "https://arxiv.org/pdf/2504.17670", "abs": "https://arxiv.org/abs/2504.17670", "authors": ["Lutao Jiang", "Jiantao Lin", "Kanghao Chen", "Wenhang Ge", "Xin Yang", "Yifan Jiang", "Yuanhuiyi Lyu", "Xu Zheng", "Yingcong Chen"], "title": "DiMeR: Disentangled Mesh Reconstruction Model", "categories": ["cs.CV"], "comment": "Project Page: https://lutao2021.github.io/DiMeR_page/", "summary": "With the advent of large-scale 3D datasets, feed-forward 3D generative\nmodels, such as the Large Reconstruction Model (LRM), have gained significant\nattention and achieved remarkable success. However, we observe that RGB images\noften lead to conflicting training objectives and lack the necessary clarity\nfor geometry reconstruction. In this paper, we revisit the inductive biases\nassociated with mesh reconstruction and introduce DiMeR, a novel disentangled\ndual-stream feed-forward model for sparse-view mesh reconstruction. The key\nidea is to disentangle both the input and framework into geometry and texture\nparts, thereby reducing the training difficulty for each part according to the\nPrinciple of Occam's Razor. Given that normal maps are strictly consistent with\ngeometry and accurately capture surface variations, we utilize normal maps as\nexclusive input for the geometry branch to reduce the complexity between the\nnetwork's input and output. Moreover, we improve the mesh extraction algorithm\nto introduce 3D ground truth supervision. As for texture branch, we use RGB\nimages as input to obtain the textured mesh. Overall, DiMeR demonstrates robust\ncapabilities across various tasks, including sparse-view reconstruction,\nsingle-image-to-3D, and text-to-3D. Numerous experiments show that DiMeR\nsignificantly outperforms previous methods, achieving over 30% improvement in\nChamfer Distance on the GSO and OmniObject3D dataset.", "AI": {"tldr": "DiMeR\u662f\u4e00\u79cd\u65b0\u578b\u7684\u53cc\u6d41\u89e3\u8026\u524d\u9988\u6a21\u578b\uff0c\u7528\u4e8e\u7a00\u758f\u89c6\u56fe\u7f51\u683c\u91cd\u5efa\uff0c\u901a\u8fc7\u5206\u79bb\u51e0\u4f55\u548c\u7eb9\u7406\u90e8\u5206\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "RGB\u56fe\u50cf\u5728\u51e0\u4f55\u91cd\u5efa\u4e2d\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u76ee\u6807\u51b2\u7a81\u4e14\u7f3a\u4e4f\u6e05\u6670\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5206\u79bb\u51e0\u4f55\u548c\u7eb9\u7406\u4fe1\u606f\u3002", "method": "DiMeR\u5c06\u8f93\u5165\u548c\u6846\u67b6\u89e3\u8026\u4e3a\u51e0\u4f55\u548c\u7eb9\u7406\u4e24\u90e8\u5206\uff0c\u51e0\u4f55\u5206\u652f\u4f7f\u7528\u6cd5\u7ebf\u56fe\u4f5c\u4e3a\u8f93\u5165\uff0c\u7eb9\u7406\u5206\u652f\u4f7f\u7528RGB\u56fe\u50cf\uff0c\u5e76\u6539\u8fdb\u4e86\u7f51\u683c\u63d0\u53d6\u7b97\u6cd5\u3002", "result": "DiMeR\u5728\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u3001\u5355\u56fe\u50cf\u52303D\u548c\u6587\u672c\u52303D\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cChamfer Distance\u5728GSO\u548cOmniObject3D\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e8630%\u4ee5\u4e0a\u3002", "conclusion": "DiMeR\u901a\u8fc7\u89e3\u8026\u51e0\u4f55\u548c\u7eb9\u7406\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u96be\u5ea6\u5e76\u63d0\u5347\u4e86\u91cd\u5efa\u6548\u679c\uff0c\u4e3a3D\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.17695", "pdf": "https://arxiv.org/pdf/2504.17695", "abs": "https://arxiv.org/abs/2504.17695", "authors": ["Alp\u00e1r Cseke", "Shashank Tripathi", "Sai Kumar Dwivedi", "Arjun Lakshmipathy", "Agniv Chatterjee", "Michael J. Black", "Dimitrios Tzionas"], "title": "PICO: Reconstructing 3D People In Contact with Objects", "categories": ["cs.CV"], "comment": "Accepted in CVPR'25. Project Page: https://pico.is.tue.mpg.de", "summary": "Recovering 3D Human-Object Interaction (HOI) from single color images is\nchallenging due to depth ambiguities, occlusions, and the huge variation in\nobject shape and appearance. Thus, past work requires controlled settings such\nas known object shapes and contacts, and tackles only limited object classes.\nInstead, we need methods that generalize to natural images and novel object\nclasses. We tackle this in two main ways: (1) We collect PICO-db, a new dataset\nof natural images uniquely paired with dense 3D contact on both body and object\nmeshes. To this end, we use images from the recent DAMON dataset that are\npaired with contacts, but these contacts are only annotated on a canonical 3D\nbody. In contrast, we seek contact labels on both the body and the object. To\ninfer these given an image, we retrieve an appropriate 3D object mesh from a\ndatabase by leveraging vision foundation models. Then, we project DAMON's body\ncontact patches onto the object via a novel method needing only 2 clicks per\npatch. This minimal human input establishes rich contact correspondences\nbetween bodies and objects. (2) We exploit our new dataset of contact\ncorrespondences in a novel render-and-compare fitting method, called PICO-fit,\nto recover 3D body and object meshes in interaction. PICO-fit infers contact\nfor the SMPL-X body, retrieves a likely 3D object mesh and contact from PICO-db\nfor that object, and uses the contact to iteratively fit the 3D body and object\nmeshes to image evidence via optimization. Uniquely, PICO-fit works well for\nmany object categories that no existing method can tackle. This is crucial to\nenable HOI understanding to scale in the wild. Our data and code are available\nat https://pico.is.tue.mpg.de.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u5f20\u5f69\u8272\u56fe\u50cf\u4e2d\u6062\u590d3D\u4eba-\u7269\u4ea4\u4e92\uff08HOI\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u65b0\u6570\u636e\u96c6PICO-db\u548c\u5f00\u53d1\u4f18\u5316\u65b9\u6cd5PICO-fit\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u6a21\u7cca\u3001\u906e\u6321\u548c\u7269\u4f53\u591a\u6837\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u5df2\u77e5\u7269\u4f53\u5f62\u72b6\u548c\u63a5\u89e6\u4fe1\u606f\uff0c\u4e14\u4ec5\u9002\u7528\u4e8e\u6709\u9650\u7269\u4f53\u7c7b\u522b\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u9002\u7528\u4e8e\u81ea\u7136\u56fe\u50cf\u548c\u65b0\u578b\u7269\u4f53\u7c7b\u522b\u7684\u901a\u7528\u65b9\u6cd5\u3002", "method": "1. \u6784\u5efaPICO-db\u6570\u636e\u96c6\uff0c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u68c0\u7d223D\u7269\u4f53\u7f51\u683c\uff0c\u5e76\u901a\u8fc72\u6b21\u70b9\u51fb\u6295\u5f71\u63a5\u89e6\u6807\u7b7e\u30022. \u63d0\u51faPICO-fit\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e32\u67d3-\u6bd4\u8f83\u4f18\u5316\u62df\u54083D\u4eba\u4f53\u548c\u7269\u4f53\u7f51\u683c\u3002", "result": "PICO-fit\u80fd\u5904\u7406\u591a\u79cd\u7269\u4f53\u7c7b\u522b\uff0c\u663e\u8457\u63d0\u5347\u4e86HOI\u7406\u89e3\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u7136\u573a\u666f\u4e2d\u7684HOI\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.17696", "pdf": "https://arxiv.org/pdf/2504.17696", "abs": "https://arxiv.org/abs/2504.17696", "authors": ["Ghazal Kaviani", "Yavuz Yarici", "Seulgi Kim", "Mohit Prabhushankar", "Ghassan AlRegib", "Mashhour Solh", "Ameya Patil"], "title": "Hierarchical and Multimodal Data for Daily Activity Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Daily Activity Recordings for Artificial Intelligence (DARai, pronounced\n\"Dahr-ree\") is a multimodal, hierarchically annotated dataset constructed to\nunderstand human activities in real-world settings. DARai consists of\ncontinuous scripted and unscripted recordings of 50 participants in 10\ndifferent environments, totaling over 200 hours of data from 20 sensors\nincluding multiple camera views, depth and radar sensors, wearable inertial\nmeasurement units (IMUs), electromyography (EMG), insole pressure sensors,\nbiomonitor sensors, and gaze tracker.\n  To capture the complexity in human activities, DARai is annotated at three\nlevels of hierarchy: (i) high-level activities (L1) that are independent tasks,\n(ii) lower-level actions (L2) that are patterns shared between activities, and\n(iii) fine-grained procedures (L3) that detail the exact execution steps for\nactions. The dataset annotations and recordings are designed so that 22.7% of\nL2 actions are shared between L1 activities and 14.2% of L3 procedures are\nshared between L2 actions. The overlap and unscripted nature of DARai allows\ncounterfactual activities in the dataset.\n  Experiments with various machine learning models showcase the value of DARai\nin uncovering important challenges in human-centered applications.\nSpecifically, we conduct unimodal and multimodal sensor fusion experiments for\nrecognition, temporal localization, and future action anticipation across all\nhierarchical annotation levels. To highlight the limitations of individual\nsensors, we also conduct domain-variant experiments that are enabled by DARai's\nmulti-sensor and counterfactual activity design setup.\n  The code, documentation, and dataset are available at the dedicated DARai\nwebsite:\nhttps://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/", "AI": {"tldr": "DARai\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u3001\u5206\u5c42\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u4eba\u7c7b\u6d3b\u52a8\uff0c\u5305\u542b50\u540d\u53c2\u4e0e\u8005\u572810\u79cd\u73af\u5883\u4e2d\u7684200\u591a\u5c0f\u65f6\u6570\u636e\uff0c\u8986\u76d620\u79cd\u4f20\u611f\u5668\u3002", "motivation": "\u7406\u89e3\u4eba\u7c7b\u6d3b\u52a8\u7684\u590d\u6742\u6027\uff0c\u652f\u6301\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u548c\u5206\u5c42\u4efb\u52a1\u5206\u6790\u3002", "method": "\u6784\u5efa\u5305\u542b\u811a\u672c\u548c\u975e\u811a\u672c\u8bb0\u5f55\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u5206\u4e3a\u4e09\u4e2a\u5c42\u6b21\uff08\u6d3b\u52a8\u3001\u52a8\u4f5c\u3001\u6b65\u9aa4\uff09\uff0c\u5e76\u8fdb\u884c\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u5c55\u793a\u4e86DARai\u5728\u8bc6\u522b\u3001\u65f6\u95f4\u5b9a\u4f4d\u548c\u672a\u6765\u52a8\u4f5c\u9884\u6d4b\u4e2d\u7684\u4ef7\u503c\uff0c\u5e76\u63ed\u793a\u4e86\u5355\u4e2a\u4f20\u611f\u5668\u7684\u5c40\u9650\u6027\u3002", "conclusion": "DARai\u4e3a\u4eba\u7c7b\u4e2d\u5fc3\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6311\u6218\u548c\u89e3\u51b3\u65b9\u6848\u7684\u6570\u636e\u652f\u6301\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2504.17712", "pdf": "https://arxiv.org/pdf/2504.17712", "abs": "https://arxiv.org/abs/2504.17712", "authors": ["Zhuo He", "Paul Henderson", "Nicolas Pugeault"], "title": "Generative Fields: Uncovering Hierarchical Feature Control for StyleGAN via Inverted Receptive Fields", "categories": ["cs.CV"], "comment": null, "summary": "StyleGAN has demonstrated the ability of GANs to synthesize highly-realistic\nfaces of imaginary people from random noise. One limitation of GAN-based image\ngeneration is the difficulty of controlling the features of the generated\nimage, due to the strong entanglement of the low-dimensional latent space.\nPrevious work that aimed to control StyleGAN with image or text prompts\nmodulated sampling in W latent space, which is more expressive than Z latent\nspace. However, W space still has restricted expressivity since it does not\ncontrol the feature synthesis directly; also the feature embedding in W space\nrequires a pre-training process to reconstruct the style signal, limiting its\napplication. This paper introduces the concept of \"generative fields\" to\nexplain the hierarchical feature synthesis in StyleGAN, inspired by the\nreceptive fields of convolution neural networks (CNNs). Additionally, we\npropose a new image editing pipeline for StyleGAN using generative field theory\nand the channel-wise style latent space S, utilizing the intrinsic structural\nfeature of CNNs to achieve disentangled control of feature synthesis at\nsynthesis time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u573a\u7406\u8bba\u7684\u65b0\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86StyleGAN\u4e2d\u7279\u5f81\u5408\u6210\u7684\u63a7\u5236\u80fd\u529b\uff0c\u5229\u7528\u901a\u9053\u98ce\u683c\u6f5c\u5728\u7a7a\u95f4S\u5b9e\u73b0\u89e3\u8026\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3StyleGAN\u4e2d\u7531\u4e8e\u6f5c\u5728\u7a7a\u95f4\u7ea0\u7f20\u5bfc\u81f4\u7684\u751f\u6210\u56fe\u50cf\u7279\u5f81\u63a7\u5236\u56f0\u96be\u95ee\u9898\uff0c\u63d0\u5347\u7279\u5f81\u5408\u6210\u7684\u76f4\u63a5\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u5f15\u5165\u751f\u6210\u573a\u7406\u8bba\u89e3\u91caStyleGAN\u7684\u5c42\u6b21\u7279\u5f81\u5408\u6210\uff0c\u63d0\u51fa\u57fa\u4e8e\u901a\u9053\u98ce\u683c\u6f5c\u5728\u7a7a\u95f4S\u7684\u56fe\u50cf\u7f16\u8f91\u6d41\u7a0b\uff0c\u5229\u7528CNN\u7684\u56fa\u6709\u7ed3\u6784\u7279\u5f81\u5b9e\u73b0\u89e3\u8026\u63a7\u5236\u3002", "result": "\u901a\u8fc7\u751f\u6210\u573a\u7406\u8bba\u548cS\u7a7a\u95f4\uff0c\u5b9e\u73b0\u4e86\u5bf9StyleGAN\u7279\u5f81\u5408\u6210\u7684\u66f4\u76f4\u63a5\u548c\u7075\u6d3b\u63a7\u5236\u3002", "conclusion": "\u751f\u6210\u573a\u7406\u8bba\u548cS\u7a7a\u95f4\u7684\u5e94\u7528\u4e3aStyleGAN\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u7279\u5f81\u63a7\u5236\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.17732", "pdf": "https://arxiv.org/pdf/2504.17732", "abs": "https://arxiv.org/abs/2504.17732", "authors": ["Zhanwen Liu", "Sai Zhou", "Yuchao Dai", "Yang Wang", "Yisheng An", "Xiangmo Zhao"], "title": "DPMambaIR:All-in-One Image Restoration via Degradation-Aware Prompt State Space Model", "categories": ["cs.CV", "I.4.4"], "comment": null, "summary": "All-in-One image restoration aims to address multiple image degradation\nproblems using a single model, significantly reducing training costs and\ndeployment complexity compared to traditional methods that design dedicated\nmodels for each degradation type. Existing approaches typically rely on\nDegradation-specific models or coarse-grained degradation prompts to guide\nimage restoration. However, they lack fine-grained modeling of degradation\ninformation and face limitations in balancing multi-task conflicts. To overcome\nthese limitations, we propose DPMambaIR, a novel All-in-One image restoration\nframework. By integrating a Degradation-Aware Prompt State Space Model (DP-SSM)\nand a High-Frequency Enhancement Block (HEB), DPMambaIR enables fine-grained\nmodeling of complex degradation information and efficient global integration,\nwhile mitigating the loss of high-frequency details caused by task competition.\nSpecifically, the DP-SSM utilizes a pre-trained degradation extractor to\ncapture fine-grained degradation features and dynamically incorporates them\ninto the state space modeling process, enhancing the model's adaptability to\ndiverse degradation types. Concurrently, the HEB supplements high-frequency\ninformation, effectively addressing the loss of critical details, such as edges\nand textures, in multi-task image restoration scenarios. Extensive experiments\non a mixed dataset containing seven degradation types show that DPMambaIR\nachieves the best performance, with 27.69dB and 0.893 in PSNR and SSIM,\nrespectively. These results highlight the potential and superiority of\nDPMambaIR as a unified solution for All-in-One image restoration.", "AI": {"tldr": "DPMambaIR\u662f\u4e00\u4e2a\u65b0\u578b\u7684All-in-One\u56fe\u50cf\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7Degradation-Aware Prompt State Space Model\u548cHigh-Frequency Enhancement Block\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u9000\u5316\u5efa\u6a21\u548c\u9ad8\u9891\u7ec6\u8282\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u591a\u4efb\u52a1\u4fee\u590d\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9488\u5bf9\u6bcf\u79cd\u9000\u5316\u7c7b\u578b\u8bbe\u8ba1\u4e13\u7528\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u4e14\u590d\u6742\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u9000\u5316\u5efa\u6a21\u548c\u591a\u4efb\u52a1\u51b2\u7a81\u5e73\u8861\u80fd\u529b\u3002", "method": "\u7ed3\u5408DP-SSM\uff08\u7ec6\u7c92\u5ea6\u9000\u5316\u7279\u5f81\u5efa\u6a21\uff09\u548cHEB\uff08\u9ad8\u9891\u7ec6\u8282\u8865\u5145\uff09\uff0c\u52a8\u6001\u6574\u5408\u9000\u5316\u4fe1\u606f\u5e76\u589e\u5f3a\u5168\u5c40\u4fee\u590d\u6548\u679c\u3002", "result": "\u5728\u5305\u542b\u4e03\u79cd\u9000\u5316\u7c7b\u578b\u7684\u6df7\u5408\u6570\u636e\u96c6\u4e0a\uff0cDPMambaIR\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff08PSNR 27.69dB\uff0cSSIM 0.893\uff09\u3002", "conclusion": "DPMambaIR\u4f5c\u4e3a\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86All-in-One\u56fe\u50cf\u4fee\u590d\u7684\u6f5c\u529b\u548c\u4f18\u8d8a\u6027\u3002"}}
{"id": "2504.17735", "pdf": "https://arxiv.org/pdf/2504.17735", "abs": "https://arxiv.org/abs/2504.17735", "authors": ["Akhil Padmanabha", "Saravanan Govindarajan", "Hwanmun Kim", "Sergio Ortiz", "Rahul Rajan", "Doruk Senkal", "Sneha Kadetotad"], "title": "EgoCHARM: Resource-Efficient Hierarchical Activity Recognition using an Egocentric IMU Sensor", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Human activity recognition (HAR) on smartglasses has various use cases,\nincluding health/fitness tracking and input for context-aware AI assistants.\nHowever, current approaches for egocentric activity recognition suffer from low\nperformance or are resource-intensive. In this work, we introduce a resource\n(memory, compute, power, sample) efficient machine learning algorithm,\nEgoCHARM, for recognizing both high level and low level activities using a\nsingle egocentric (head-mounted) Inertial Measurement Unit (IMU). Our\nhierarchical algorithm employs a semi-supervised learning strategy, requiring\nprimarily high level activity labels for training, to learn generalizable low\nlevel motion embeddings that can be effectively utilized for low level activity\nrecognition. We evaluate our method on 9 high level and 3 low level activities\nachieving 0.826 and 0.855 F1 scores on high level and low level activity\nrecognition respectively, with just 63k high level and 22k low level model\nparameters, allowing the low level encoder to be deployed directly on current\nIMU chips with compute. Lastly, we present results and insights from a\nsensitivity analysis and highlight the opportunities and limitations of\nactivity recognition using egocentric IMUs.", "AI": {"tldr": "EgoCHARM\u662f\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u901a\u8fc7\u5934\u6234\u5f0fIMU\u8bc6\u522b\u9ad8\u3001\u4f4e\u5c42\u6b21\u6d3b\u52a8\uff0c\u91c7\u7528\u534a\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u6a21\u578b\u8f7b\u91cf\u3002", "motivation": "\u5f53\u524d\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u6d3b\u52a8\u8bc6\u522b\u65b9\u6cd5\u6027\u80fd\u4f4e\u6216\u8d44\u6e90\u6d88\u8017\u5927\uff0cEgoCHARM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7b97\u6cd5\u548c\u534a\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u5229\u7528\u9ad8\u5c42\u6b21\u6d3b\u52a8\u6807\u7b7e\u8bad\u7ec3\uff0c\u5b66\u4e60\u901a\u7528\u4f4e\u5c42\u6b21\u8fd0\u52a8\u5d4c\u5165\u3002", "result": "\u57289\u79cd\u9ad8\u5c42\u6b21\u548c3\u79cd\u4f4e\u5c42\u6b21\u6d3b\u52a8\u4e0a\uff0cF1\u5206\u6570\u5206\u522b\u4e3a0.826\u548c0.855\uff0c\u6a21\u578b\u53c2\u6570\u4ec563k\u548c22k\u3002", "conclusion": "EgoCHARM\u5c55\u793a\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u9ad8\u6548\u6d3b\u52a8\u8bc6\u522b\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u5206\u6790\u4e86\u5176\u673a\u4f1a\u4e0e\u9650\u5236\u3002"}}
{"id": "2504.17761", "pdf": "https://arxiv.org/pdf/2504.17761", "abs": "https://arxiv.org/abs/2504.17761", "authors": ["Shiyu Liu", "Yucheng Han", "Peng Xing", "Fukun Yin", "Rui Wang", "Wei Cheng", "Jiaqi Liao", "Yingming Wang", "Honghao Fu", "Chunrui Han", "Guopeng Li", "Yuang Peng", "Quan Sun", "Jingwei Wu", "Yan Cai", "Zheng Ge", "Ranchen Ming", "Lei Xia", "Xianfang Zeng", "Yibo Zhu", "Binxing Jiao", "Xiangyu Zhang", "Gang Yu", "Daxin Jiang"], "title": "Step1X-Edit: A Practical Framework for General Image Editing", "categories": ["cs.CV"], "comment": "code: https://github.com/stepfun-ai/Step1X-Edit", "summary": "In recent years, image editing models have witnessed remarkable and rapid\ndevelopment. The recent unveiling of cutting-edge multimodal models such as\nGPT-4o and Gemini2 Flash has introduced highly promising image editing\ncapabilities. These models demonstrate an impressive aptitude for fulfilling a\nvast majority of user-driven editing requirements, marking a significant\nadvancement in the field of image manipulation. However, there is still a large\ngap between the open-source algorithm with these closed-source models. Thus, in\nthis paper, we aim to release a state-of-the-art image editing model, called\nStep1X-Edit, which can provide comparable performance against the closed-source\nmodels like GPT-4o and Gemini2 Flash. More specifically, we adopt the\nMultimodal LLM to process the reference image and the user's editing\ninstruction. A latent embedding has been extracted and integrated with a\ndiffusion image decoder to obtain the target image. To train the model, we\nbuild a data generation pipeline to produce a high-quality dataset. For\nevaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world\nuser instructions. Experimental results on GEdit-Bench demonstrate that\nStep1X-Edit outperforms existing open-source baselines by a substantial margin\nand approaches the performance of leading proprietary models, thereby making\nsignificant contributions to the field of image editing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStep1X-Edit\u7684\u5f00\u6e90\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\uff0c\u65e8\u5728\u7f29\u5c0f\u4e0e\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4o\u548cGemini2 Flash\uff09\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u901a\u8fc7\u591a\u6a21\u6001LLM\u5904\u7406\u56fe\u50cf\u548c\u7528\u6237\u6307\u4ee4\uff0c\u7ed3\u5408\u6269\u6563\u56fe\u50cf\u89e3\u7801\u5668\u751f\u6210\u76ee\u6807\u56fe\u50cf\uff0c\u5e76\u5728\u65b0\u57fa\u51c6GEdit-Bench\u4e0a\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u9886\u57df\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5f00\u6e90\u7b97\u6cd5\u4e0e\u95ed\u6e90\u6a21\u578b\u4e4b\u95f4\u4ecd\u5b58\u5728\u8f83\u5927\u5dee\u8ddd\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u6027\u80fd\u63a5\u8fd1\u95ed\u6e90\u6a21\u578b\u7684\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001LLM\u5904\u7406\u53c2\u8003\u56fe\u50cf\u548c\u7528\u6237\u6307\u4ee4\uff0c\u63d0\u53d6\u6f5c\u5728\u5d4c\u5165\u5e76\u4e0e\u6269\u6563\u56fe\u50cf\u89e3\u7801\u5668\u7ed3\u5408\u751f\u6210\u76ee\u6807\u56fe\u50cf\u3002\u901a\u8fc7\u6570\u636e\u751f\u6210\u7ba1\u9053\u6784\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5728GEdit-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cStep1X-Edit\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u57fa\u7ebf\uff0c\u5e76\u63a5\u8fd1\u9886\u5148\u95ed\u6e90\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "Step1X-Edit\u4e3a\u56fe\u50cf\u7f16\u8f91\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u7684\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\uff0c\u7f29\u5c0f\u4e86\u4e0e\u95ed\u6e90\u6a21\u578b\u7684\u5dee\u8ddd\u3002"}}
{"id": "2504.17787", "pdf": "https://arxiv.org/pdf/2504.17787", "abs": "https://arxiv.org/abs/2504.17787", "authors": ["Anton Obukhov", "Matteo Poggi", "Fabio Tosi", "Ripudaman Singh Arora", "Jaime Spencer", "Chris Russell", "Simon Hadfield", "Richard Bowden", "Shuaihang Wang", "Zhenxin Ma", "Weijie Chen", "Baobei Xu", "Fengyu Sun", "Di Xie", "Jiang Zhu", "Mykola Lavreniuk", "Haining Guan", "Qun Wu", "Yupei Zeng", "Chao Lu", "Huanran Wang", "Guangyuan Zhou", "Haotian Zhang", "Jianxiong Wang", "Qiang Rao", "Chunjie Wang", "Xiao Liu", "Zhiqiang Lou", "Hualie Jiang", "Yihao Chen", "Rui Xu", "Minglang Tan", "Zihan Qin", "Yifan Mao", "Jiayang Liu", "Jialei Xu", "Yifan Yang", "Wenbo Zhao", "Junjun Jiang", "Xianming Liu", "Mingshuai Zhao", "Anlong Ming", "Wu Chen", "Feng Xue", "Mengying Yu", "Shida Gao", "Xiangfeng Wang", "Gbenga Omotara", "Ramy Farag", "Jacket Demby", "Seyed Mohamad Ali Tousi", "Guilherme N DeSouza", "Tuan-Anh Yang", "Minh-Quang Nguyen", "Thien-Phuc Tran", "Albert Luginov", "Muhammad Shahzad"], "title": "The Fourth Monocular Depth Estimation Challenge", "categories": ["cs.CV"], "comment": "To appear in CVPRW2025", "summary": "This paper presents the results of the fourth edition of the Monocular Depth\nEstimation Challenge (MDEC), which focuses on zero-shot generalization to the\nSYNS-Patches benchmark, a dataset featuring challenging environments in both\nnatural and indoor settings. In this edition, we revised the evaluation\nprotocol to use least-squares alignment with two degrees of freedom to support\ndisparity and affine-invariant predictions. We also revised the baselines and\nincluded popular off-the-shelf methods: Depth Anything v2 and Marigold. The\nchallenge received a total of 24 submissions that outperformed the baselines on\nthe test set; 10 of these included a report describing their approach, with\nmost leading methods relying on affine-invariant predictions. The challenge\nwinners improved the 3D F-Score over the previous edition's best result,\nraising it from 22.58% to 23.05%.", "AI": {"tldr": "\u7b2c\u56db\u7248\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6311\u6218\u8d5b\uff08MDEC\uff09\u805a\u7126\u4e8e\u96f6\u6837\u672c\u6cdb\u5316\u5230SYNS-Patches\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6539\u8fdb\u4e86\u8bc4\u4f30\u534f\u8bae\u548c\u57fa\u7ebf\u65b9\u6cd5\uff0c24\u4e2a\u63d0\u4ea4\u7ed3\u679c\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5176\u4e2d10\u4e2a\u63d0\u4f9b\u4e86\u65b9\u6cd5\u63cf\u8ff0\uff0c\u83b7\u80dc\u8005\u5c063D F-Score\u4ece22.58%\u63d0\u5347\u81f323.05%\u3002", "motivation": "\u6311\u6218\u8d5b\u65e8\u5728\u63a8\u52a8\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5728\u590d\u6742\u81ea\u7136\u548c\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4fee\u8ba2\u4e86\u8bc4\u4f30\u534f\u8bae\uff0c\u91c7\u7528\u6700\u5c0f\u4e8c\u4e58\u5bf9\u9f50\u652f\u6301\u89c6\u5dee\u548c\u4eff\u5c04\u4e0d\u53d8\u9884\u6d4b\uff0c\u5e76\u5f15\u5165Depth Anything v2\u548cMarigold\u4f5c\u4e3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "24\u4e2a\u63d0\u4ea4\u7ed3\u679c\u4f18\u4e8e\u57fa\u7ebf\uff0c\u83b7\u80dc\u65b9\u6cd5\u5c063D F-Score\u4ece22.58%\u63d0\u5347\u81f323.05%\u3002", "conclusion": "\u6311\u6218\u8d5b\u5c55\u793a\u4e86\u4eff\u5c04\u4e0d\u53d8\u9884\u6d4b\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a8\u52a8\u4e86\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6280\u672f\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.17788", "pdf": "https://arxiv.org/pdf/2504.17788", "abs": "https://arxiv.org/abs/2504.17788", "authors": ["Chris Rockwell", "Joseph Tung", "Tsung-Yi Lin", "Ming-Yu Liu", "David F. Fouhey", "Chen-Hsuan Lin"], "title": "Dynamic Camera Poses and Where to Find Them", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025. Project Page:\n  https://research.nvidia.com/labs/dir/dynpose-100k", "summary": "Annotating camera poses on dynamic Internet videos at scale is critical for\nadvancing fields like realistic video generation and simulation. However,\ncollecting such a dataset is difficult, as most Internet videos are unsuitable\nfor pose estimation. Furthermore, annotating dynamic Internet videos present\nsignificant challenges even for state-of-theart methods. In this paper, we\nintroduce DynPose-100K, a large-scale dataset of dynamic Internet videos\nannotated with camera poses. Our collection pipeline addresses filtering using\na carefully combined set of task-specific and generalist models. For pose\nestimation, we combine the latest techniques of point tracking, dynamic\nmasking, and structure-from-motion to achieve improvements over the\nstate-of-the-art approaches. Our analysis and experiments demonstrate that\nDynPose-100K is both large-scale and diverse across several key attributes,\nopening up avenues for advancements in various downstream applications.", "AI": {"tldr": "DynPose-100K\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u52a8\u6001\u4e92\u8054\u7f51\u89c6\u9891\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86\u76f8\u673a\u4f4d\u59ff\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u91c7\u96c6\u548c\u6807\u6ce8\u6d41\u7a0b\u3002", "motivation": "\u52a8\u6001\u4e92\u8054\u7f51\u89c6\u9891\u7684\u76f8\u673a\u4f4d\u59ff\u6807\u6ce8\u5bf9\u89c6\u9891\u751f\u6210\u548c\u6a21\u62df\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u548c\u901a\u7528\u6a21\u578b\u8fdb\u884c\u89c6\u9891\u7b5b\u9009\uff0c\u5e76\u91c7\u7528\u70b9\u8ddf\u8e2a\u3001\u52a8\u6001\u63a9\u7801\u548c\u8fd0\u52a8\u7ed3\u6784\u6062\u590d\u6280\u672f\u8fdb\u884c\u4f4d\u59ff\u4f30\u8ba1\u3002", "result": "DynPose-100K\u6570\u636e\u96c6\u89c4\u6a21\u5927\u4e14\u591a\u6837\u5316\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u4e0b\u6e38\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "conclusion": "DynPose-100K\u4e3a\u52a8\u6001\u89c6\u9891\u4f4d\u59ff\u6807\u6ce8\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u76f8\u5173\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.17789", "pdf": "https://arxiv.org/pdf/2504.17789", "abs": "https://arxiv.org/abs/2504.17789", "authors": ["Xu Ma", "Peize Sun", "Haoyu Ma", "Hao Tang", "Chih-Yao Ma", "Jialiang Wang", "Kunpeng Li", "Xiaoliang Dai", "Yujun Shi", "Xuan Ju", "Yushi Hu", "Artsiom Sanakoyeu", "Felix Juefei-Xu", "Ji Hou", "Junjiao Tian", "Tao Xu", "Tingbo Hou", "Yen-Cheng Liu", "Zecheng He", "Zijian He", "Matt Feiszli", "Peizhao Zhang", "Peter Vajda", "Sam Tsai", "Yun Fu"], "title": "Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models", "categories": ["cs.CV"], "comment": null, "summary": "Autoregressive (AR) models, long dominant in language generation, are\nincreasingly applied to image synthesis but are often considered less\ncompetitive than Diffusion-based models. A primary limitation is the\nsubstantial number of image tokens required for AR models, which constrains\nboth training and inference efficiency, as well as image resolution. To address\nthis, we present Token-Shuffle, a novel yet simple method that reduces the\nnumber of image tokens in Transformer. Our key insight is the dimensional\nredundancy of visual vocabularies in Multimodal Large Language Models (MLLMs),\nwhere low-dimensional visual codes from visual encoder are directly mapped to\nhigh-dimensional language vocabularies. Leveraging this, we consider two key\noperations: token-shuffle, which merges spatially local tokens along channel\ndimension to decrease the input token number, and token-unshuffle, which\nuntangles the inferred tokens after Transformer blocks to restore the spatial\narrangement for output. Jointly training with textual prompts, our strategy\nrequires no additional pretrained text-encoder and enables MLLMs to support\nextremely high-resolution image synthesis in a unified next-token prediction\nway while maintaining efficient training and inference. For the first time, we\npush the boundary of AR text-to-image generation to a resolution of 2048x2048\nwith gratifying generation performance. In GenAI-benchmark, our 2.7B model\nachieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen\nby 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human\nevaluations also demonstrate our prominent image generation ability in terms of\ntext-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle\ncan serve as a foundational design for efficient high-resolution image\ngeneration within MLLMs.", "AI": {"tldr": "Token-Shuffle\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11Transformer\u4e2d\u7684\u56fe\u50cf\u6807\u8bb0\u6570\u91cf\uff0c\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u6a21\u578b\u5728\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u652f\u6301\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u5728\u56fe\u50cf\u5408\u6210\u4e2d\u56e0\u6807\u8bb0\u6570\u91cf\u591a\u800c\u6548\u7387\u4f4e\u4e0b\uff0c\u9650\u5236\u4e86\u5206\u8fa8\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51faToken-Shuffle\u548cToken-Unshuffle\u64cd\u4f5c\uff0c\u5229\u7528\u89c6\u89c9\u8bcd\u6c47\u7684\u7ef4\u5ea6\u5197\u4f59\u51cf\u5c11\u6807\u8bb0\u6570\u91cf\uff0c\u7ed3\u5408\u6587\u672c\u63d0\u793a\u8054\u5408\u8bad\u7ec3\u3002", "result": "\u57282048x2048\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u9ad8\u6548\u751f\u6210\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\u3002", "conclusion": "Token-Shuffle\u4e3aMLLMs\u4e2d\u7684\u9ad8\u6548\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u57fa\u7840\u8bbe\u8ba1\u3002"}}
{"id": "2504.17791", "pdf": "https://arxiv.org/pdf/2504.17791", "abs": "https://arxiv.org/abs/2504.17791", "authors": ["Tetiana Martyniuk", "Gilles Puy", "Alexandre Boulch", "Renaud Marlet", "Raoul de Charette"], "title": "LiDPM: Rethinking Point Diffusion for Lidar Scene Completion", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted to IEEE IV 2025", "summary": "Training diffusion models that work directly on lidar points at the scale of\noutdoor scenes is challenging due to the difficulty of generating fine-grained\ndetails from white noise over a broad field of view. The latest works\naddressing scene completion with diffusion models tackle this problem by\nreformulating the original DDPM as a local diffusion process. It contrasts with\nthe common practice of operating at the level of objects, where vanilla DDPMs\nare currently used. In this work, we close the gap between these two lines of\nwork. We identify approximations in the local diffusion formulation, show that\nthey are not required to operate at the scene level, and that a vanilla DDPM\nwith a well-chosen starting point is enough for completion. Finally, we\ndemonstrate that our method, LiDPM, leads to better results in scene completion\non SemanticKITTI. The project page is https://astra-vision.github.io/LiDPM .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLiDPM\u65b9\u6cd5\uff0c\u8bc1\u660e\u5728\u573a\u666f\u7ea7\u522b\u5b8c\u6210\u65f6\uff0c\u65e0\u9700\u5c40\u90e8\u6269\u6563\u8fd1\u4f3c\uff0c\u6807\u51c6DDPM\u5373\u53ef\u5b9e\u73b0\u66f4\u597d\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u5728\u5ba4\u5916\u573a\u666f\u4e2d\u76f4\u63a5\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u7cbe\u7ec6\u70b9\u4e91\u7684\u6311\u6218\uff0c\u5f25\u5408\u5c40\u90e8\u6269\u6563\u4e0e\u5bf9\u8c61\u7ea7\u522b\u6269\u6563\u7684\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u6807\u51c6DDPM\uff0c\u9009\u62e9\u5408\u9002\u7684\u8d77\u59cb\u70b9\uff0c\u907f\u514d\u5c40\u90e8\u6269\u6563\u7684\u8fd1\u4f3c\u3002", "result": "\u5728SemanticKITTI\u4e0a\u53d6\u5f97\u66f4\u597d\u7684\u573a\u666f\u5b8c\u6210\u6548\u679c\u3002", "conclusion": "LiDPM\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u6807\u51c6DDPM\u5728\u573a\u666f\u7ea7\u522b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.16936", "pdf": "https://arxiv.org/pdf/2504.16936", "abs": "https://arxiv.org/abs/2504.16936", "authors": ["Yusheng Zhao", "Junyu Luo", "Xiao Luo", "Weizhi Zhang", "Zhiping Xiao", "Wei Ju", "Philip S. Yu", "Ming Zhang"], "title": "Multifaceted Evaluation of Audio-Visual Capability for MLLMs: Effectiveness, Efficiency, Generalizability and Robustness", "categories": ["cs.MM", "cs.CV", "cs.SD", "eess.AS"], "comment": null, "summary": "Multi-modal large language models (MLLMs) have recently achieved great\nsuccess in processing and understanding information from diverse modalities\n(e.g., text, audio, and visual signals). Despite their growing popularity,\nthere remains a lack of comprehensive evaluation measuring the audio-visual\ncapabilities of these models, especially in diverse scenarios (e.g.,\ndistribution shifts and adversarial attacks). In this paper, we present a\nmultifaceted evaluation of the audio-visual capability of MLLMs, focusing on\nfour key dimensions: effectiveness, efficiency, generalizability, and\nrobustness. Through extensive experiments, we find that MLLMs exhibit strong\nzero-shot and few-shot generalization abilities, enabling them to achieve great\nperformance with limited data. However, their success relies heavily on the\nvision modality, which impairs performance when visual input is corrupted or\nmissing. Additionally, while MLLMs are susceptible to adversarial samples, they\ndemonstrate greater robustness compared to traditional models. The experimental\nresults and our findings provide insights into the audio-visual capabilities of\nMLLMs, highlighting areas for improvement and offering guidance for future\nresearch.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u97f3\u9891-\u89c6\u89c9\u80fd\u529b\u8fdb\u884c\u4e86\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u53d1\u73b0\u5176\u5728\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u89c6\u89c9\u6a21\u6001\u4f9d\u8d56\u6027\u5f3a\uff0c\u4e14\u5bf9\u6297\u6837\u672c\u653b\u51fb\u4e0b\u4ecd\u663e\u8106\u5f31\u3002", "motivation": "\u5c3d\u7ba1MLLMs\u5728\u591a\u6a21\u6001\u4fe1\u606f\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u97f3\u9891-\u89c6\u89c9\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30\uff0c\u5c24\u5176\u662f\u5728\u5206\u5e03\u504f\u79fb\u548c\u5bf9\u6297\u653b\u51fb\u7b49\u591a\u6837\u5316\u573a\u666f\u4e2d\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff08\u6709\u6548\u6027\u3001\u6548\u7387\u3001\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\uff09\u5bf9MLLMs\u8fdb\u884c\u591a\u89d2\u5ea6\u8bc4\u4f30\uff0c\u5e76\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u3002", "result": "MLLMs\u5728\u96f6\u6837\u672c\u548c\u5c0f\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u89c6\u89c9\u6a21\u6001\u4f9d\u8d56\u6027\u5f3a\uff0c\u89c6\u89c9\u8f93\u5165\u53d7\u635f\u65f6\u6027\u80fd\u4e0b\u964d\uff1b\u5bf9\u6297\u6837\u672c\u653b\u51fb\u4e0b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86MLLMs\u7684\u97f3\u9891-\u89c6\u89c9\u80fd\u529b\u73b0\u72b6\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u548c\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.16940", "pdf": "https://arxiv.org/pdf/2504.16940", "abs": "https://arxiv.org/abs/2504.16940", "authors": ["Drew Linsley", "Pinyuan Feng", "Thomas Serre"], "title": "Can deep neural networks learn biological vision?", "categories": ["q-bio.NC", "cs.AI", "cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) once showed increasing alignment with primate\nneural responses as they improved on computer vision benchmarks. This trend\nraised the exciting possibility that better models of biological vision would\ncome as a byproduct of the deep learning revolution in artificial intelligence.\nHowever, the trend has reversed over recent years as DNNs have scaled to human\nor superhuman recognition accuracy, a divergence that may stem from modern DNNs\nlearning to rely on different visual features than primates to solve tasks.\nWhere will better computational models of biological vision come from? We\npropose that vision science must break from artificial intelligence to develop\nalgorithms that are designed with biological visual systems in mind instead of\ninternet data benchmarks. We predict that the next generation of deep learning\nmodels of biological vision will be trained with data diets, training routines,\nand objectives that are closer to those that shape human vision than those that\nare in use today.", "AI": {"tldr": "\u73b0\u4ee3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u4e0e\u7075\u957f\u7c7b\u795e\u7ecf\u53cd\u5e94\u7684\u5339\u914d\u5ea6\u4e0b\u964d\uff0c\u8868\u660e\u5176\u4f9d\u8d56\u7684\u7279\u5f81\u4e0e\u751f\u7269\u89c6\u89c9\u7cfb\u7edf\u4e0d\u540c\u3002\u672a\u6765\u751f\u7269\u89c6\u89c9\u6a21\u578b\u9700\u8131\u79bb\u4eba\u5de5\u667a\u80fd\u8303\u5f0f\uff0c\u4e13\u6ce8\u4e8e\u751f\u7269\u89c6\u89c9\u7684\u6570\u636e\u548c\u8bad\u7ec3\u65b9\u5f0f\u3002", "motivation": "\u63a2\u8ba8DNNs\u4e0e\u751f\u7269\u89c6\u89c9\u7cfb\u7edf\u5339\u914d\u5ea6\u4e0b\u964d\u7684\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "method": "\u63d0\u51fa\u9700\u8bbe\u8ba1\u66f4\u8d34\u8fd1\u751f\u7269\u89c6\u89c9\u7cfb\u7edf\u7684\u7b97\u6cd5\uff0c\u5305\u62ec\u6570\u636e\u3001\u8bad\u7ec3\u548c\u76ee\u6807\u3002", "result": "\u73b0\u4ee3DNNs\u4f9d\u8d56\u7684\u7279\u5f81\u4e0e\u7075\u957f\u7c7b\u4e0d\u540c\uff0c\u5bfc\u81f4\u5339\u914d\u5ea6\u4e0b\u964d\u3002", "conclusion": "\u672a\u6765\u751f\u7269\u89c6\u89c9\u6a21\u578b\u9700\u57fa\u4e8e\u751f\u7269\u89c6\u89c9\u7684\u6570\u636e\u548c\u8bad\u7ec3\u65b9\u5f0f\uff0c\u800c\u975e\u4f20\u7edfAI\u8303\u5f0f\u3002"}}
{"id": "2504.16942", "pdf": "https://arxiv.org/pdf/2504.16942", "abs": "https://arxiv.org/abs/2504.16942", "authors": ["Shushman Choudhury", "Elad Aharoni", "Chandrakumari Suvarna", "Iveel Tsogsuren", "Abdul Rahman Kreidieh", "Chun-Ta Lu", "Neha Arora"], "title": "S2Vec: Self-Supervised Geospatial Embeddings", "categories": ["cs.SI", "cs.AI", "cs.CV"], "comment": "To be submitted to ACM Transactions on Spatial Algorithms and Systems", "summary": "Scalable general-purpose representations of the built environment are crucial\nfor geospatial artificial intelligence applications. This paper introduces\nS2Vec, a novel self-supervised framework for learning such geospatial\nembeddings. S2Vec uses the S2 Geometry library to partition large areas into\ndiscrete S2 cells, rasterizes built environment feature vectors within cells as\nimages, and applies masked autoencoding on these rasterized images to encode\nthe feature vectors. This approach yields task-agnostic embeddings that capture\nlocal feature characteristics and broader spatial relationships. We evaluate\nS2Vec on three large-scale socioeconomic prediction tasks, showing its\ncompetitive performance against state-of-the-art image-based embeddings. We\nalso explore the benefits of combining S2Vec embeddings with image-based\nembeddings downstream, showing that such multimodal fusion can often improve\nperformance. Our results highlight how S2Vec can learn effective\ngeneral-purpose geospatial representations and how it can complement other data\nmodalities in geospatial artificial intelligence.", "AI": {"tldr": "S2Vec\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u901a\u7528\u7684\u5730\u7406\u7a7a\u95f4\u5d4c\u5165\uff0c\u901a\u8fc7S2\u51e0\u4f55\u5e93\u5206\u533a\u548c\u63a9\u7801\u81ea\u7f16\u7801\u6280\u672f\u751f\u6210\u4efb\u52a1\u65e0\u5173\u7684\u5d4c\u5165\uff0c\u5e76\u5728\u793e\u4f1a\u7ecf\u6d4e\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6784\u5efa\u53ef\u6269\u5c55\u7684\u901a\u7528\u5730\u7406\u7a7a\u95f4\u8868\u793a\u5bf9\u5730\u7406\u7a7a\u95f4\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528S2\u51e0\u4f55\u5e93\u5c06\u5927\u533a\u57df\u5212\u5206\u4e3a\u79bb\u6563\u7684S2\u5355\u5143\uff0c\u5c06\u7279\u5f81\u5411\u91cf\u6805\u683c\u5316\u4e3a\u56fe\u50cf\uff0c\u5e76\u5e94\u7528\u63a9\u7801\u81ea\u7f16\u7801\u6280\u672f\u751f\u6210\u5d4c\u5165\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u89c4\u6a21\u793e\u4f1a\u7ecf\u6d4e\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u4e0e\u56fe\u50cf\u5d4c\u5165\u7ed3\u5408\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "S2Vec\u80fd\u751f\u6210\u6709\u6548\u7684\u5730\u7406\u7a7a\u95f4\u8868\u793a\uff0c\u5e76\u4e0e\u5176\u4ed6\u6570\u636e\u6a21\u6001\u4e92\u8865\u3002"}}
{"id": "2504.16972", "pdf": "https://arxiv.org/pdf/2504.16972", "abs": "https://arxiv.org/abs/2504.16972", "authors": ["Hossein Ahmadi", "Sajjad Emdadi Mahdimahalleh", "Arman Farahat", "Banafsheh Saffari"], "title": "Unsupervised Time-Series Signal Analysis with Autoencoders and Vision Transformers: A Review of Architectures and Applications", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": null, "summary": "The rapid growth of unlabeled time-series data in domains such as wireless\ncommunications, radar, biomedical engineering, and the Internet of Things (IoT)\nhas driven advancements in unsupervised learning. This review synthesizes\nrecent progress in applying autoencoders and vision transformers for\nunsupervised signal analysis, focusing on their architectures, applications,\nand emerging trends. We explore how these models enable feature extraction,\nanomaly detection, and classification across diverse signal types, including\nelectrocardiograms, radar waveforms, and IoT sensor data. The review highlights\nthe strengths of hybrid architectures and self-supervised learning, while\nidentifying challenges in interpretability, scalability, and domain\ngeneralization. By bridging methodological innovations and practical\napplications, this work offers a roadmap for developing robust, adaptive models\nfor signal intelligence.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u81ea\u7f16\u7801\u5668\u548c\u89c6\u89c9\u53d8\u6362\u5668\u5728\u65e0\u76d1\u7763\u4fe1\u53f7\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86\u5176\u67b6\u6784\u3001\u5e94\u7528\u53ca\u8d8b\u52bf\uff0c\u5e76\u6307\u51fa\u4e86\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u9886\u57df\u6cdb\u5316\u7b49\u6311\u6218\u3002", "motivation": "\u968f\u7740\u65e0\u7ebf\u901a\u4fe1\u3001\u96f7\u8fbe\u3001\u751f\u7269\u533b\u5b66\u5de5\u7a0b\u548c\u7269\u8054\u7f51\u7b49\u9886\u57df\u4e2d\u672a\u6807\u8bb0\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u5feb\u901f\u589e\u957f\uff0c\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u8fdb\u5c55\u63a8\u52a8\u4e86\u4fe1\u53f7\u5206\u6790\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u81ea\u7f16\u7801\u5668\u548c\u89c6\u89c9\u53d8\u6362\u5668\u7684\u67b6\u6784\u548c\u5e94\u7528\uff0c\u5206\u6790\u5176\u5728\u7279\u5f81\u63d0\u53d6\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u8ba8\u6df7\u5408\u67b6\u6784\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u4f18\u52bf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u6a21\u578b\u5728\u591a\u79cd\u4fe1\u53f7\u7c7b\u578b\uff08\u5982\u5fc3\u7535\u56fe\u3001\u96f7\u8fbe\u6ce2\u5f62\u548c\u7269\u8054\u7f51\u4f20\u611f\u5668\u6570\u636e\uff09\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9762\u4e34\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u9886\u57df\u6cdb\u5316\u7684\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a\u5f00\u53d1\u9c81\u68d2\u3001\u81ea\u9002\u5e94\u7684\u4fe1\u53f7\u667a\u80fd\u6a21\u578b\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u4e86\u65b9\u6cd5\u521b\u65b0\u4e0e\u5b9e\u9645\u5e94\u7528\u7684\u7ed3\u5408\u3002"}}
{"id": "2504.16974", "pdf": "https://arxiv.org/pdf/2504.16974", "abs": "https://arxiv.org/abs/2504.16974", "authors": ["Hidde Makimei", "Shuai Wang", "Willem van Peursen"], "title": "Seeing The Words: Evaluating AI-generated Biblical Art", "categories": ["cs.CY", "cs.CV", "cs.MM", "I.4.8; I.4.0; I.3.3; I.3.0"], "comment": null, "summary": "The past years witnessed a significant amount of Artificial Intelligence (AI)\ntools that can generate images from texts. This triggers the discussion of\nwhether AI can generate accurate images using text from the Bible with respect\nto the corresponding biblical contexts and backgrounds. Despite some existing\nattempts at a small scale, little work has been done to systematically evaluate\nthese generated images. In this work, we provide a large dataset of over 7K\nimages using biblical text as prompts. These images were evaluated with\nmultiple neural network-based tools on various aspects. We provide an\nassessment of accuracy and some analysis from the perspective of religion and\naesthetics. Finally, we discuss the use of the generated images and reflect on\nthe performance of the AI generators.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u751f\u6210\u56fe\u50cf\u662f\u5426\u80fd\u51c6\u786e\u53cd\u6620\u5723\u7ecf\u6587\u672c\u7684\u80cc\u666f\u548c\u5185\u5bb9\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b7K\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u79cd\u795e\u7ecf\u7f51\u7edc\u5de5\u5177\u8bc4\u4f30\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u7814\u7a76AI\u751f\u6210\u56fe\u50cf\u5728\u5723\u7ecf\u6587\u672c\u80cc\u666f\u4e0b\u7684\u51c6\u786e\u6027\uff0c\u586b\u8865\u7cfb\u7edf\u6027\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u5723\u7ecf\u6587\u672c\u4f5c\u4e3a\u63d0\u793a\u751f\u62107K\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u795e\u7ecf\u7f51\u7edc\u5de5\u5177\u8bc4\u4f30\u5176\u51c6\u786e\u6027\u3001\u5b97\u6559\u6027\u548c\u7f8e\u5b66\u6027\u3002", "result": "\u63d0\u4f9b\u4e86\u56fe\u50cf\u51c6\u786e\u6027\u8bc4\u4f30\uff0c\u5e76\u4ece\u5b97\u6559\u548c\u7f8e\u5b66\u89d2\u5ea6\u8fdb\u884c\u4e86\u5206\u6790\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u751f\u6210\u56fe\u50cf\u7684\u7528\u9014\uff0c\u5e76\u53cd\u601d\u4e86AI\u751f\u6210\u5668\u7684\u8868\u73b0\u3002"}}
{"id": "2504.16979", "pdf": "https://arxiv.org/pdf/2504.16979", "abs": "https://arxiv.org/abs/2504.16979", "authors": ["Masoud Tafavvoghi", "Lars Ailo Bongo", "Andr\u00e9 Berli Delgado", "Nikita Shvetsov", "Anders Sildnes", "Line Moi", "Lill-Tove Rasmussen Busund", "Kajsa M\u00f8llersen"], "title": "Automating tumor-infiltrating lymphocyte assessment in breast cancer histopathology images using QuPath: a transparent and accessible machine learning pipeline", "categories": ["q-bio.QM", "cs.AI", "cs.CV"], "comment": "16 Pages, 9 Figures, 3 tables", "summary": "In this study, we built an end-to-end tumor-infiltrating lymphocytes (TILs)\nassessment pipeline within QuPath, demonstrating the potential of easily\naccessible tools to perform complex tasks in a fully automatic fashion. First,\nwe trained a pixel classifier to segment tumor, tumor-associated stroma, and\nother tissue compartments in breast cancer H&E-stained whole-slide images (WSI)\nto isolate tumor-associated stroma for subsequent analysis. Next, we applied a\npre-trained StarDist deep learning model in QuPath for cell detection and used\nthe extracted cell features to train a binary classifier distinguishing TILs\nfrom other cells. To evaluate our TILs assessment pipeline, we calculated the\nTIL density in each WSI and categorized them as low, medium, or high TIL\nlevels. Our pipeline was evaluated against pathologist-assigned TIL scores,\nachieving a Cohen's kappa of 0.71 on the external test set, corroborating\nprevious research findings. These results confirm that existing software can\noffer a practical solution for the assessment of TILs in H&E-stained WSIs of\nbreast cancer.", "AI": {"tldr": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u80bf\u7624\u6d78\u6da6\u6dcb\u5df4\u7ec6\u80de\uff08TILs\uff09\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5229\u7528QuPath\u5b9e\u73b0\u5168\u81ea\u52a8\u590d\u6742\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u73b0\u6709\u8f6f\u4ef6\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u63a2\u7d22\u5229\u7528\u6613\u83b7\u53d6\u5de5\u5177\uff08\u5982QuPath\uff09\u5b9e\u73b0\u590d\u6742\u4efb\u52a1\uff08\u5982TILs\u8bc4\u4f30\uff09\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u8bad\u7ec3\u50cf\u7d20\u5206\u7c7b\u5668\u5206\u5272\u80bf\u7624\u76f8\u5173\u57fa\u8d28\uff1b2. \u4f7f\u7528\u9884\u8bad\u7ec3StarDist\u6a21\u578b\u68c0\u6d4b\u7ec6\u80de\u5e76\u8bad\u7ec3\u4e8c\u5143\u5206\u7c7b\u5668\u533a\u5206TILs\uff1b3. \u8ba1\u7b97TIL\u5bc6\u5ea6\u5e76\u5206\u7c7b\u4e3a\u4f4e\u3001\u4e2d\u3001\u9ad8\u3002", "result": "\u4e0e\u75c5\u7406\u5b66\u5bb6\u8bc4\u5206\u5bf9\u6bd4\uff0cCohen's kappa\u8fbe0.71\uff0c\u9a8c\u8bc1\u4e86\u6d41\u7a0b\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u73b0\u6709\u8f6f\u4ef6\u53ef\u63d0\u4f9b\u4e73\u817a\u764cH&E\u67d3\u8272\u5168\u5207\u7247\u56fe\u50cf\u4e2dTILs\u8bc4\u4f30\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.17114", "pdf": "https://arxiv.org/pdf/2504.17114", "abs": "https://arxiv.org/abs/2504.17114", "authors": ["Valentin Langer", "Kartikay Tehlan", "Thomas Wendler"], "title": "Anatomy-constrained modelling of image-derived input functions in dynamic PET using multi-organ segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "comment": "The code is available under\n  https://github.com/tinolan/curve_fit_multi_idif", "summary": "Accurate kinetic analysis of [$^{18}$F]FDG distribution in dynamic positron\nemission tomography (PET) requires anatomically constrained modelling of\nimage-derived input functions (IDIFs). Traditionally, IDIFs are obtained from\nthe aorta, neglecting anatomical variations and complex vascular contributions.\nThis study proposes a multi-organ segmentation-based approach that integrates\nIDIFs from the aorta, portal vein, pulmonary artery, and ureters. Using\nhigh-resolution CT segmentations of the liver, lungs, kidneys, and bladder, we\nincorporate organ-specific blood supply sources to improve kinetic modelling.\nOur method was evaluated on dynamic [$^{18}$F]FDG PET data from nine patients,\nresulting in a mean squared error (MSE) reduction of $13.39\\%$ for the liver\nand $10.42\\%$ for the lungs. These initial results highlight the potential of\nmultiple IDIFs in improving anatomical modelling and fully leveraging dynamic\nPET imaging. This approach could facilitate the integration of tracer kinetic\nmodelling into clinical routine.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u5668\u5b98\u5206\u5272\u7684\u65b9\u6cd5\uff0c\u6574\u5408\u4e3b\u52a8\u8109\u3001\u95e8\u9759\u8109\u3001\u80ba\u52a8\u8109\u548c\u8f93\u5c3f\u7ba1\u7684\u56fe\u50cf\u884d\u751f\u8f93\u5165\u51fd\u6570\uff08IDIFs\uff09\uff0c\u4ee5\u63d0\u9ad8\u52a8\u6001PET\u6210\u50cf\u7684\u52a8\u529b\u5b66\u5efa\u6a21\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfIDIFs\u4ec5\u4ece\u4e3b\u52a8\u8109\u83b7\u53d6\uff0c\u5ffd\u7565\u4e86\u89e3\u5256\u53d8\u5f02\u548c\u590d\u6742\u8840\u7ba1\u8d21\u732e\uff0c\u9650\u5236\u4e86\u52a8\u529b\u5b66\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "method": "\u5229\u7528\u9ad8\u5206\u8fa8\u7387CT\u5206\u5272\u809d\u810f\u3001\u80ba\u3001\u80be\u810f\u548c\u8180\u80f1\uff0c\u6574\u5408\u5668\u5b98\u7279\u5f02\u6027\u8840\u6db2\u4f9b\u5e94\u6e90\uff0c\u6539\u8fdb\u52a8\u529b\u5b66\u5efa\u6a21\u3002", "result": "\u57289\u4f8b\u60a3\u8005\u7684\u52a8\u6001PET\u6570\u636e\u4e2d\uff0c\u809d\u810f\u548c\u80ba\u7684\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u5206\u522b\u964d\u4f4e\u4e8613.39%\u548c10.42%\u3002", "conclusion": "\u591aIDIFs\u65b9\u6cd5\u6709\u671b\u6539\u5584\u89e3\u5256\u5efa\u6a21\uff0c\u63a8\u52a8\u793a\u8e2a\u52a8\u529b\u5b66\u5efa\u6a21\u5728\u4e34\u5e8a\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2504.17122", "pdf": "https://arxiv.org/pdf/2504.17122", "abs": "https://arxiv.org/abs/2504.17122", "authors": ["Kartikay Tehlan", "Thomas Wendler"], "title": "Physiological neural representation for personalised tracer kinetic parameter estimation from dynamic PET", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "The code is available at: https://github.com/tkartikay/PhysNRPET", "summary": "Dynamic positron emission tomography (PET) with [$^{18}$F]FDG enables\nnon-invasive quantification of glucose metabolism through kinetic analysis,\noften modelled by the two-tissue compartment model (TCKM). However, voxel-wise\nkinetic parameter estimation using conventional methods is computationally\nintensive and limited by spatial resolution. Deep neural networks (DNNs) offer\nan alternative but require large training datasets and significant\ncomputational resources. To address these limitations, we propose a\nphysiological neural representation based on implicit neural representations\n(INRs) for personalized kinetic parameter estimation. INRs, which learn\ncontinuous functions, allow for efficient, high-resolution parametric imaging\nwith reduced data requirements. Our method also integrates anatomical priors\nfrom a 3D CT foundation model to enhance robustness and precision in kinetic\nmodelling. We evaluate our approach on an [$^{18}$F]FDG dynamic PET/CT dataset\nand compare it to state-of-the-art DNNs. Results demonstrate superior spatial\nresolution, lower mean-squared error, and improved anatomical consistency,\nparticularly in tumour and highly vascularized regions. Our findings highlight\nthe potential of INRs for personalized, data-efficient tracer kinetic\nmodelling, enabling applications in tumour characterization, segmentation, and\nprognostic assessment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INRs\uff09\u7684\u751f\u7406\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u52a8\u529b\u5b66\u53c2\u6570\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u5728\u8ba1\u7b97\u548c\u6570\u636e\u9700\u6c42\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u52a8\u6001PET\u6210\u50cf\u4e2d\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u4e14\u7a7a\u95f4\u5206\u8fa8\u7387\u6709\u9650\uff0cDNNs\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u6570\u636e\u9700\u6c42\u66f4\u5c11\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528INRs\u5b66\u4e60\u8fde\u7eed\u51fd\u6570\uff0c\u7ed3\u54083D CT\u57fa\u7840\u6a21\u578b\u7684\u89e3\u5256\u5148\u9a8c\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u53c2\u6570\u6210\u50cf\u3002", "result": "\u5728[$^{18}$F]FDG\u52a8\u6001PET/CT\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u66f4\u9ad8\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\u3001\u66f4\u4f4e\u7684\u5747\u65b9\u8bef\u5dee\u548c\u66f4\u597d\u7684\u89e3\u5256\u4e00\u81f4\u6027\uff0c\u5c24\u5176\u5728\u80bf\u7624\u548c\u9ad8\u8840\u7ba1\u533a\u57df\u3002", "conclusion": "INRs\u5728\u4e2a\u6027\u5316\u3001\u6570\u636e\u9ad8\u6548\u7684\u793a\u8e2a\u52a8\u529b\u5b66\u5efa\u6a21\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u80bf\u7624\u8868\u5f81\u3001\u5206\u5272\u548c\u9884\u540e\u8bc4\u4f30\u3002"}}
{"id": "2504.17160", "pdf": "https://arxiv.org/pdf/2504.17160", "abs": "https://arxiv.org/abs/2504.17160", "authors": ["Alberto Fern\u00e1ndez-Hern\u00e1ndez", "Jose I. Mestre", "Manuel F. Dolz", "Jose Duato", "Enrique S. Quintana-Ort\u00ed"], "title": "OUI Need to Talk About Weight Decay: A New Perspective on Overfitting Detection", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "10 pages, 3 figures", "summary": "We introduce the Overfitting-Underfitting Indicator (OUI), a novel tool for\nmonitoring the training dynamics of Deep Neural Networks (DNNs) and identifying\noptimal regularization hyperparameters. Specifically, we validate that OUI can\neffectively guide the selection of the Weight Decay (WD) hyperparameter by\nindicating whether a model is overfitting or underfitting during training\nwithout requiring validation data. Through experiments on DenseNet-BC-100 with\nCIFAR- 100, EfficientNet-B0 with TinyImageNet and ResNet-34 with ImageNet-1K,\nwe show that maintaining OUI within a prescribed interval correlates strongly\nwith improved generalization and validation scores. Notably, OUI converges\nsignificantly faster than traditional metrics such as loss or accuracy,\nenabling practitioners to identify optimal WD (hyperparameter) values within\nthe early stages of training. By leveraging OUI as a reliable indicator, we can\ndetermine early in training whether the chosen WD value leads the model to\nunderfit the training data, overfit, or strike a well-balanced trade-off that\nmaximizes validation scores. This enables more precise WD tuning for optimal\nperformance on the tested datasets and DNNs. All code for reproducing these\nexperiments is available at https://github.com/AlbertoFdezHdez/OUI.", "AI": {"tldr": "OUI\u662f\u4e00\u79cd\u65b0\u5de5\u5177\uff0c\u7528\u4e8e\u76d1\u63a7DNN\u8bad\u7ec3\u52a8\u6001\u5e76\u8bc6\u522b\u6700\u4f73\u6b63\u5219\u5316\u8d85\u53c2\u6570\uff0c\u65e0\u9700\u9a8c\u8bc1\u6570\u636e\u5373\u53ef\u5224\u65ad\u8fc7\u62df\u5408\u6216\u6b20\u62df\u5408\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9a8c\u8bc1\u6570\u636e\u8c03\u6574\u8d85\u53c2\u6570\uff0cOUI\u65e8\u5728\u63d0\u4f9b\u66f4\u5feb\u901f\u3001\u66f4\u76f4\u63a5\u7684\u8fc7\u62df\u5408/\u6b20\u62df\u5408\u6307\u793a\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1OUI\u5728\u591a\u79cdDNN\u548c\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\uff0c\u6307\u5bfcWeight Decay\u8d85\u53c2\u6570\u7684\u9009\u62e9\u3002", "result": "OUI\u80fd\u66f4\u5feb\u6536\u655b\u5e76\u663e\u8457\u63d0\u5347\u6cdb\u5316\u6027\u80fd\uff0c\u5e2e\u52a9\u65e9\u671f\u786e\u5b9a\u6700\u4f73\u8d85\u53c2\u6570\u3002", "conclusion": "OUI\u662f\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u53ef\u4f18\u5316DNN\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.17179", "pdf": "https://arxiv.org/pdf/2504.17179", "abs": "https://arxiv.org/abs/2504.17179", "authors": ["Mohammad Zarei", "Melanie A Jutras", "Eliana Evans", "Mike Tan", "Omid Aaramoon"], "title": "AUTHENTICATION: Identifying Rare Failure Modes in Autonomous Vehicle Perception Systems using Adversarially Guided Diffusion Models", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO", "68T45, 68T05 68T45, 68T05 68T45, 68T05", "I.2.6; I.2.10; I.4.8"], "comment": "8 pages, 10 figures. Accepted to IEEE Conference on Artificial\n  Intelligence (CAI), 2025", "summary": "Autonomous Vehicles (AVs) rely on artificial intelligence (AI) to accurately\ndetect objects and interpret their surroundings. However, even when trained\nusing millions of miles of real-world data, AVs are often unable to detect rare\nfailure modes (RFMs). The problem of RFMs is commonly referred to as the\n\"long-tail challenge\", due to the distribution of data including many instances\nthat are very rarely seen. In this paper, we present a novel approach that\nutilizes advanced generative and explainable AI techniques to aid in\nunderstanding RFMs. Our methods can be used to enhance the robustness and\nreliability of AVs when combined with both downstream model training and\ntesting. We extract segmentation masks for objects of interest (e.g., cars) and\ninvert them to create environmental masks. These masks, combined with carefully\ncrafted text prompts, are fed into a custom diffusion model. We leverage the\nStable Diffusion inpainting model guided by adversarial noise optimization to\ngenerate images containing diverse environments designed to evade object\ndetection models and expose vulnerabilities in AI systems. Finally, we produce\nnatural language descriptions of the generated RFMs that can guide developers\nand policymakers to improve the safety and reliability of AV systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u751f\u6210\u548c\u53ef\u89e3\u91caAI\u6280\u672f\u7406\u89e3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AVs\uff09\u4e2d\u7f55\u89c1\u6545\u969c\u6a21\u5f0f\uff08RFMs\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u73af\u5883\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u63d0\u5347AV\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AVs\uff09\u5728\u68c0\u6d4b\u7f55\u89c1\u6545\u969c\u6a21\u5f0f\uff08RFMs\uff09\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u88ab\u79f0\u4e3a\u201c\u957f\u5c3e\u6311\u6218\u201d\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u751f\u6210\u548c\u53ef\u89e3\u91caAI\u6280\u672f\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u53d6\u5bf9\u8c61\u5206\u5272\u63a9\u7801\u5e76\u53cd\u8f6c\u751f\u6210\u73af\u5883\u63a9\u7801\uff0c\u7ed3\u5408\u6587\u672c\u63d0\u793a\u8f93\u5165\u5b9a\u5236\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u7a33\u5b9a\u6269\u6563\u4fee\u590d\u6a21\u578b\u548c\u5bf9\u6297\u566a\u58f0\u4f18\u5316\u751f\u6210\u591a\u6837\u5316\u56fe\u50cf\uff0c\u66b4\u9732AI\u7cfb\u7edf\u7684\u6f0f\u6d1e\u3002", "result": "\u751f\u6210\u7684\u73af\u5883\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u80fd\u591f\u5e2e\u52a9\u5f00\u53d1\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u8bc6\u522b\u5e76\u6539\u8fdbAV\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u548c\u89e3\u51b3AVs\u4e2d\u7684\u7f55\u89c1\u6545\u969c\u6a21\u5f0f\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u6709\u671b\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2504.17258", "pdf": "https://arxiv.org/pdf/2504.17258", "abs": "https://arxiv.org/abs/2504.17258", "authors": ["Md Ashiqur Rahman", "Raymond A. Yeh"], "title": "Group Downsampling with Equivariant Anti-aliasing", "categories": ["cs.LG", "cs.CV", "math.GR"], "comment": null, "summary": "Downsampling layers are crucial building blocks in CNN architectures, which\nhelp to increase the receptive field for learning high-level features and\nreduce the amount of memory/computation in the model. In this work, we study\nthe generalization of the uniform downsampling layer for group equivariant\narchitectures, e.g., G-CNNs. That is, we aim to downsample signals (feature\nmaps) on general finite groups with anti-aliasing. This involves the following:\n(a) Given a finite group and a downsampling rate, we present an algorithm to\nform a suitable choice of subgroup. (b) Given a group and a subgroup, we study\nthe notion of bandlimited-ness and propose how to perform anti-aliasing.\nNotably, our method generalizes the notion of downsampling based on classical\nsampling theory. When the signal is on a cyclic group, i.e., periodic, our\nmethod recovers the standard downsampling of an ideal low-pass filter followed\nby a subsampling operation. Finally, we conducted experiments on image\nclassification tasks demonstrating that the proposed downsampling operation\nimproves accuracy, better preserves equivariance, and reduces model size when\nincorporated into G-equivariant networks", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7fa4\u7b49\u53d8\u67b6\u6784\uff08\u5982G-CNNs\uff09\u4e2d\u63a8\u5e7f\u5747\u5300\u4e0b\u91c7\u6837\u5c42\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6709\u9650\u7fa4\u7684\u4e0b\u91c7\u6837\u7b97\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u4e0b\u91c7\u6837\u5c42\u662fCNN\u67b6\u6784\u4e2d\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u7fa4\u7b49\u53d8\u67b6\u6784\u4e2d\u7684\u901a\u7528\u6027\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u9002\u7528\u4e8e\u6709\u9650\u7fa4\u7684\u4e0b\u91c7\u6837\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5\uff0c\u9488\u5bf9\u6709\u9650\u7fa4\u548c\u7ed9\u5b9a\u7684\u4e0b\u91c7\u6837\u7387\u9009\u62e9\u5408\u9002\u7684\u5b50\u7fa4\uff0c\u5e76\u7814\u7a76\u4e86\u5e26\u9650\u6027\u548c\u6297\u6df7\u53e0\u65b9\u6cd5\u3002\u65b9\u6cd5\u57fa\u4e8e\u7ecf\u5178\u91c7\u6837\u7406\u8bba\uff0c\u9002\u7528\u4e8e\u5468\u671f\u6027\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u4e0b\u91c7\u6837\u64cd\u4f5c\u5728G-\u7b49\u53d8\u7f51\u7edc\u4e2d\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u66f4\u597d\u5730\u4fdd\u6301\u4e86\u7b49\u53d8\u6027\uff0c\u5e76\u51cf\u5c11\u4e86\u6a21\u578b\u5927\u5c0f\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u63a8\u5e7f\u4e86\u7ecf\u5178\u4e0b\u91c7\u6837\u7406\u8bba\uff0c\u9002\u7528\u4e8e\u7fa4\u7b49\u53d8\u67b6\u6784\uff0c\u5e76\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.17314", "pdf": "https://arxiv.org/pdf/2504.17314", "abs": "https://arxiv.org/abs/2504.17314", "authors": ["Miaoyun Zhao", "Qiang Zhang", "Chenrong Li"], "title": "Class-Conditional Distribution Balancing for Group Robust Classification", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Spurious correlations that lead models to correct predictions for the wrong\nreasons pose a critical challenge for robust real-world generalization.\nExisting research attributes this issue to group imbalance and addresses it by\nmaximizing group-balanced or worst-group accuracy, which heavily relies on\nexpensive bias annotations. A compromise approach involves predicting bias\ninformation using extensively pretrained foundation models, which requires\nlarge-scale data and becomes impractical for resource-limited rare domains. To\naddress these challenges, we offer a novel perspective by reframing the\nspurious correlations as imbalances or mismatches in class-conditional\ndistributions, and propose a simple yet effective robust learning method that\neliminates the need for both bias annotations and predictions. With the goal of\nreducing the mutual information between spurious factors and label information,\nour method leverages a sample reweighting strategy to achieve class-conditional\ndistribution balancing, which automatically highlights minority groups and\nclasses, effectively dismantling spurious correlations and producing a debiased\ndata distribution for classification. Extensive experiments and analysis\ndemonstrate that our approach consistently delivers state-of-the-art\nperformance, rivaling methods that rely on bias supervision.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u504f\u5dee\u6807\u6ce8\u6216\u9884\u6d4b\u7684\u9c81\u68d2\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u6837\u672c\u5e73\u8861\u7c7b\u6761\u4ef6\u5206\u5e03\uff0c\u6709\u6548\u6d88\u9664\u865a\u5047\u76f8\u5173\u6027\u3002", "motivation": "\u865a\u5047\u76f8\u5173\u6027\u5bfc\u81f4\u6a21\u578b\u57fa\u4e8e\u9519\u8bef\u539f\u56e0\u505a\u51fa\u9884\u6d4b\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u504f\u5dee\u6807\u6ce8\u6216\u5927\u89c4\u6a21\u6570\u636e\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u6709\u9650\u9886\u57df\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u51cf\u5c11\u865a\u5047\u56e0\u7d20\u4e0e\u6807\u7b7e\u4fe1\u606f\u7684\u4e92\u4fe1\u606f\uff0c\u91c7\u7528\u6837\u672c\u91cd\u65b0\u52a0\u6743\u7b56\u7565\u5e73\u8861\u7c7b\u6761\u4ef6\u5206\u5e03\uff0c\u81ea\u52a8\u7a81\u51fa\u5c11\u6570\u7fa4\u4f53\u548c\u7c7b\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u5f02\uff0c\u5ab2\u7f8e\u4f9d\u8d56\u504f\u5dee\u76d1\u7763\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u6216\u6570\u636e\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u6709\u9650\u573a\u666f\u3002"}}
{"id": "2504.17379", "pdf": "https://arxiv.org/pdf/2504.17379", "abs": "https://arxiv.org/abs/2504.17379", "authors": ["Hassan Keshvarikhojasteh", "Mihail Tifrea", "Sibylle Hess", "Josien P. W. Pluim", "Mitko Veta"], "title": "A Spatially-Aware Multiple Instance Learning Framework for Digital Pathology", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Multiple instance learning (MIL) is a promising approach for weakly\nsupervised classification in pathology using whole slide images (WSIs).\nHowever, conventional MIL methods such as Attention-Based Deep Multiple\nInstance Learning (ABMIL) typically disregard spatial interactions among\npatches that are crucial to pathological diagnosis. Recent advancements, such\nas Transformer based MIL (TransMIL), have incorporated spatial context and\ninter-patch relationships. However, it remains unclear whether explicitly\nmodeling patch relationships yields similar performance gains in ABMIL, which\nrelies solely on Multi-Layer Perceptrons (MLPs). In contrast, TransMIL employs\nTransformer-based layers, introducing a fundamental architectural shift at the\ncost of substantially increased computational complexity. In this work, we\nenhance the ABMIL framework by integrating interaction-aware representations to\naddress this question. Our proposed model, Global ABMIL (GABMIL), explicitly\ncaptures inter-instance dependencies while preserving computational efficiency.\nExperimental results on two publicly available datasets for tumor subtyping in\nbreast and lung cancers demonstrate that GABMIL achieves up to a 7 percentage\npoint improvement in AUPRC and a 5 percentage point increase in the Kappa score\nover ABMIL, with minimal or no additional computational overhead. These\nfindings underscore the importance of incorporating patch interactions within\nMIL frameworks.", "AI": {"tldr": "GABMIL\u6539\u8fdbABMIL\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u6355\u6349\u5b9e\u4f8b\u95f4\u4f9d\u8d56\u5173\u7cfb\u63d0\u5347\u6027\u80fd\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u4f18\u4e8eABMIL\u3002", "motivation": "\u4f20\u7edfMIL\u65b9\u6cd5\uff08\u5982ABMIL\uff09\u5ffd\u89c6\u75c5\u7406\u8bca\u65ad\u4e2d\u5173\u952e\u7684patch\u95f4\u7a7a\u95f4\u4ea4\u4e92\uff0c\u800cTransMIL\u867d\u5f15\u5165\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3002", "method": "\u63d0\u51faGABMIL\uff0c\u5728ABMIL\u6846\u67b6\u4e2d\u96c6\u6210\u4ea4\u4e92\u611f\u77e5\u8868\u793a\uff0c\u663e\u5f0f\u5efa\u6a21\u5b9e\u4f8b\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u4e73\u817a\u764c\u548c\u80ba\u764c\u4e9a\u578b\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0cGABMIL\u7684AUPRC\u63d0\u53477%\uff0cKappa\u5206\u6570\u63d0\u53475%\uff0c\u8ba1\u7b97\u5f00\u9500\u51e0\u4e4e\u4e0d\u53d8\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21patch\u4ea4\u4e92\u5bf9MIL\u6846\u67b6\u6027\u80fd\u63d0\u5347\u81f3\u5173\u91cd\u8981\uff0cGABMIL\u5728\u6548\u7387\u548c\u6027\u80fd\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2504.17618", "pdf": "https://arxiv.org/pdf/2504.17618", "abs": "https://arxiv.org/abs/2504.17618", "authors": ["Nikita Gabdullin"], "title": "The effects of Hessian eigenvalue spectral density type on the applicability of Hessian analysis to generalization capability assessment of neural networks", "categories": ["cs.LG", "cs.CV"], "comment": "11 pages, 10 figures, 4 tables, 4 equations", "summary": "Hessians of neural network (NN) contain essential information about the\ncurvature of NN loss landscapes which can be used to estimate NN generalization\ncapabilities. We have previously proposed generalization criteria that rely on\nthe observation that Hessian eigenvalue spectral density (HESD) behaves\nsimilarly for a wide class of NNs. This paper further studies their\napplicability by investigating factors that can result in different types of\nHESD. We conduct a wide range of experiments showing that HESD mainly has\npositive eigenvalues (MP-HESD) for NN training and fine-tuning with various\noptimizers on different datasets with different preprocessing and augmentation\nprocedures. We also show that mainly negative HESD (MN-HESD) is a consequence\nof external gradient manipulation, indicating that the previously proposed\nHessian analysis methodology cannot be applied in such cases. We also propose\ncriteria and corresponding conditions to determine HESD type and estimate NN\ngeneralization potential. These HESD types and previously proposed\ngeneralization criteria are combined into a unified HESD analysis methodology.\nFinally, we discuss how HESD changes during training, and show the occurrence\nof quasi-singular (QS) HESD and its influence on the proposed methodology and\non the conventional assumptions about the relation between Hessian eigenvalues\nand NN loss landscape curvature.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u795e\u7ecf\u7f51\u7edcHessian\u77e9\u9635\u7279\u5f81\u503c\u8c31\u5bc6\u5ea6\uff08HESD\uff09\u7684\u884c\u4e3a\u53ca\u5176\u5bf9\u6cdb\u5316\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u7684HESD\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2dHESD\u7684\u53d8\u5316\u3002", "motivation": "Hessian\u77e9\u9635\u7279\u5f81\u503c\u8c31\u5bc6\u5ea6\uff08HESD\uff09\u80fd\u53cd\u6620\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u66f2\u9762\u7684\u66f2\u7387\uff0c\u8fdb\u800c\u4f30\u8ba1\u6cdb\u5316\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u8fdb\u4e00\u6b65\u7814\u7a76HESD\u7684\u9002\u7528\u6027\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5206\u6790\u4e0d\u540c\u4f18\u5316\u5668\u3001\u6570\u636e\u96c6\u3001\u9884\u5904\u7406\u548c\u6570\u636e\u589e\u5f3a\u5bf9HESD\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u5224\u65adHESD\u7c7b\u578b\u7684\u6807\u51c6\uff0c\u5e76\u7ed3\u5408\u5148\u524d\u63d0\u51fa\u7684\u6cdb\u5316\u51c6\u5219\u5f62\u6210\u7edf\u4e00\u5206\u6790\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0HESD\u4e3b\u8981\u4e3a\u6b63\u503c\uff08MP-HESD\uff09\u65f6\u9002\u7528\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u800c\u8d1f\u503c\uff08MN-HESD\uff09\u5219\u4e0e\u5916\u90e8\u68af\u5ea6\u64cd\u4f5c\u76f8\u5173\u3002\u6b64\u5916\uff0c\u8bad\u7ec3\u4e2d\u4f1a\u51fa\u73b0\u51c6\u5947\u5f02\uff08QS\uff09HESD\uff0c\u5f71\u54cd\u4f20\u7edf\u5047\u8bbe\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684HESD\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86HESD\u7c7b\u578b\u53ca\u5176\u53d8\u5316\u5bf9\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.17628", "pdf": "https://arxiv.org/pdf/2504.17628", "abs": "https://arxiv.org/abs/2504.17628", "authors": ["Abderrachid Hamrani", "Daniela Leizaola", "Renato Sousa", "Jose P. Ponce", "Stanley Mathis", "David G. Armstrong", "Anuradha Godavarty"], "title": "Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with Self-attention Diffusion Models and the Potential for Text-Guided Customization", "categories": ["eess.IV", "cs.CV"], "comment": "12 pages, 8 figures, journal article", "summary": "Diabetic foot ulcers (DFUs) pose a significant challenge in healthcare,\nrequiring precise and efficient wound assessment to enhance patient outcomes.\nThis study introduces the Attention Diffusion Zero-shot Unsupervised System\n(ADZUS), a novel text-guided diffusion model that performs wound segmentation\nwithout relying on labeled training data. Unlike conventional deep learning\nmodels, which require extensive annotation, ADZUS leverages zero-shot learning\nto dynamically adapt segmentation based on descriptive prompts, offering\nenhanced flexibility and adaptability in clinical applications. Experimental\nevaluations demonstrate that ADZUS surpasses traditional and state-of-the-art\nsegmentation models, achieving an IoU of 86.68\\% and the highest precision of\n94.69\\% on the chronic wound dataset, outperforming supervised approaches such\nas FUSegNet. Further validation on a custom-curated DFU dataset reinforces its\nrobustness, with ADZUS achieving a median DSC of 75\\%, significantly surpassing\nFUSegNet's 45\\%. The model's text-guided segmentation capability enables\nreal-time customization of segmentation outputs, allowing targeted analysis of\nwound characteristics based on clinical descriptions. Despite its competitive\nperformance, the computational cost of diffusion-based inference and the need\nfor potential fine-tuning remain areas for future improvement. ADZUS represents\na transformative step in wound segmentation, providing a scalable, efficient,\nand adaptable AI-driven solution for medical imaging.", "AI": {"tldr": "ADZUS\u662f\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u7cd6\u5c3f\u75c5\u8db3\u6e83\u75a1\u7684\u65e0\u76d1\u7763\u5206\u5272\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u8db3\u6e83\u75a1\u7684\u7cbe\u786e\u8bc4\u4f30\u5bf9\u60a3\u8005\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "ADZUS\u5229\u7528\u96f6\u6837\u672c\u5b66\u4e60\u548c\u6587\u672c\u5f15\u5bfc\u7684\u52a8\u6001\u5206\u5272\uff0c\u65e0\u9700\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\uff0c\u76f4\u63a5\u6839\u636e\u63cf\u8ff0\u6027\u63d0\u793a\u8fdb\u884c\u5206\u5272\u3002", "result": "ADZUS\u5728\u6162\u6027\u4f24\u53e3\u6570\u636e\u96c6\u4e0a\u8fbe\u523086.68%\u7684IoU\u548c94.69%\u7684\u7cbe\u786e\u5ea6\uff0c\u663e\u8457\u4f18\u4e8eFUSegNet\u7b49\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "ADZUS\u4e3a\u533b\u5b66\u5f71\u50cf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u548c\u6f5c\u5728\u5fae\u8c03\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2504.17655", "pdf": "https://arxiv.org/pdf/2504.17655", "abs": "https://arxiv.org/abs/2504.17655", "authors": ["Farhad Pourkamali-Anaraki"], "title": "Aerial Image Classification in Scarce and Unconstrained Environments via Conformal Prediction", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "17 pages, 5 figures, and 2 tables", "summary": "This paper presents a comprehensive empirical analysis of conformal\nprediction methods on a challenging aerial image dataset featuring diverse\nevents in unconstrained environments. Conformal prediction is a powerful\npost-hoc technique that takes the output of any classifier and transforms it\ninto a set of likely labels, providing a statistical guarantee on the coverage\nof the true label. Unlike evaluations on standard benchmarks, our study\naddresses the complexities of data-scarce and highly variable real-world\nsettings. We investigate the effectiveness of leveraging pretrained models\n(MobileNet, DenseNet, and ResNet), fine-tuned with limited labeled data, to\ngenerate informative prediction sets. To further evaluate the impact of\ncalibration, we consider two parallel pipelines (with and without temperature\nscaling) and assess performance using two key metrics: empirical coverage and\naverage prediction set size. This setup allows us to systematically examine how\ncalibration choices influence the trade-off between reliability and efficiency.\nOur findings demonstrate that even with relatively small labeled samples and\nsimple nonconformity scores, conformal prediction can yield valuable\nuncertainty estimates for complex tasks. Moreover, our analysis reveals that\nwhile temperature scaling is often employed for calibration, it does not\nconsistently lead to smaller prediction sets, underscoring the importance of\ncareful consideration in its application. Furthermore, our results highlight\nthe significant potential of model compression techniques within the conformal\nprediction pipeline for deployment in resource-constrained environments. Based\non our observations, we advocate for future research to delve into the impact\nof noisy or ambiguous labels on conformal prediction performance and to explore\neffective model reduction strategies.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\u5728\u590d\u6742\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\uff0c\u53d1\u73b0\u5373\u4f7f\u6570\u636e\u6709\u9650\uff0c\u5171\u5f62\u9884\u6d4b\u4ecd\u80fd\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u9884\u6d4b\u96c6\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u6e29\u5ea6\u7f29\u653e\u5bf9\u9884\u6d4b\u96c6\u5927\u5c0f\u7684\u5f71\u54cd\u4e0d\u4e00\u81f4\u3002", "motivation": "\u7814\u7a76\u5171\u5f62\u9884\u6d4b\u5728\u6570\u636e\u7a00\u7f3a\u4e14\u9ad8\u5ea6\u53d8\u5316\u7684\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u9a8c\u8bc1\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff08MobileNet\u3001DenseNet\u3001ResNet\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u751f\u6210\u9884\u6d4b\u96c6\uff0c\u5e76\u901a\u8fc7\u4e24\u79cd\u6821\u51c6\u7ba1\u9053\uff08\u5e26/\u4e0d\u5e26\u6e29\u5ea6\u7f29\u653e\uff09\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5171\u5f62\u9884\u6d4b\u5728\u6709\u9650\u6807\u8bb0\u6570\u636e\u4e0b\u4ecd\u80fd\u63d0\u4f9b\u53ef\u9760\u7684\u9884\u6d4b\u96c6\uff0c\u4f46\u6e29\u5ea6\u7f29\u653e\u5bf9\u9884\u6d4b\u96c6\u5927\u5c0f\u7684\u5f71\u54cd\u4e0d\u4e00\u81f4\u3002\u6a21\u578b\u538b\u7f29\u6280\u672f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u566a\u58f0\u6216\u6a21\u7cca\u6807\u7b7e\u5bf9\u5171\u5f62\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u7b56\u7565\u3002"}}
{"id": "2504.17693", "pdf": "https://arxiv.org/pdf/2504.17693", "abs": "https://arxiv.org/abs/2504.17693", "authors": ["Asier Bikandi", "Muhammad Shaheer", "Hriday Bavle", "Jayan Jevanesan", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "BIM-Constrained Optimization for Accurate Localization and Deviation Correction in Construction Monitoring", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Augmented reality (AR) applications for construction monitoring rely on\nreal-time environmental tracking to visualize architectural elements. However,\nconstruction sites present significant challenges for traditional tracking\nmethods due to featureless surfaces, dynamic changes, and drift accumulation,\nleading to misalignment between digital models and the physical world. This\npaper proposes a BIM-aware drift correction method to address these challenges.\nInstead of relying solely on SLAM-based localization, we align ``as-built\"\ndetected planes from the real-world environment with ``as-planned\"\narchitectural planes in BIM. Our method performs robust plane matching and\ncomputes a transformation (TF) between SLAM (S) and BIM (B) origin frames using\noptimization techniques, minimizing drift over time. By incorporating BIM as\nprior structural knowledge, we can achieve improved long-term localization and\nenhanced AR visualization accuracy in noisy construction environments. The\nmethod is evaluated through real-world experiments, showing significant\nreductions in drift-induced errors and optimized alignment consistency. On\naverage, our system achieves a reduction of 52.24% in angular deviations and a\nreduction of 60.8% in the distance error of the matched walls compared to the\ninitial manual alignment by the user.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eBIM\u7684\u6f02\u79fb\u6821\u6b63\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5efa\u7b51\u5de5\u5730AR\u5e94\u7528\u4e2d\u56e0\u73af\u5883\u7279\u5f81\u4e0d\u8db3\u548c\u52a8\u6001\u53d8\u5316\u5bfc\u81f4\u7684\u8ddf\u8e2a\u6f02\u79fb\u95ee\u9898\u3002", "motivation": "\u5efa\u7b51\u5de5\u5730\u73af\u5883\u7279\u5f81\u5c11\u3001\u52a8\u6001\u53d8\u5316\u591a\uff0c\u4f20\u7edf\u8ddf\u8e2a\u65b9\u6cd5\u6613\u4ea7\u751f\u6f02\u79fb\uff0c\u5bfc\u81f4\u6570\u5b57\u6a21\u578b\u4e0e\u7269\u7406\u4e16\u754c\u5bf9\u9f50\u4e0d\u51c6\u786e\u3002", "method": "\u901a\u8fc7\u5c06\u5b9e\u9645\u68c0\u6d4b\u5230\u7684\u5e73\u9762\u4e0eBIM\u4e2d\u7684\u8ba1\u5212\u5e73\u9762\u5bf9\u9f50\uff0c\u5229\u7528\u4f18\u5316\u6280\u672f\u8ba1\u7b97SLAM\u4e0eBIM\u5750\u6807\u7cfb\u95f4\u7684\u53d8\u6362\uff0c\u51cf\u5c11\u6f02\u79fb\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u51cf\u5c1152.24%\u7684\u89d2\u5ea6\u504f\u5dee\u548c60.8%\u7684\u8ddd\u79bb\u8bef\u5dee\u3002", "conclusion": "\u7ed3\u5408BIM\u7684\u5148\u9a8c\u7ed3\u6784\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86AR\u53ef\u89c6\u5316\u5728\u5608\u6742\u5efa\u7b51\u73af\u5883\u4e2d\u7684\u957f\u671f\u5b9a\u4f4d\u51c6\u786e\u6027\u3002"}}
{"id": "2504.17710", "pdf": "https://arxiv.org/pdf/2504.17710", "abs": "https://arxiv.org/abs/2504.17710", "authors": ["Yoeri Poels", "Alessandro Pau", "Christian Donner", "Giulio Romanelli", "Olivier Sauter", "Cristina Venturini", "Vlado Menkovski", "the TCV team", "the WPTE team"], "title": "Plasma State Monitoring and Disruption Characterization using Multimodal VAEs", "categories": ["physics.plasm-ph", "cs.CV", "cs.LG"], "comment": null, "summary": "When a plasma disrupts in a tokamak, significant heat and electromagnetic\nloads are deposited onto the surrounding device components. These forces scale\nwith plasma current and magnetic field strength, making disruptions one of the\nkey challenges for future devices. Unfortunately, disruptions are not fully\nunderstood, with many different underlying causes that are difficult to\nanticipate. Data-driven models have shown success in predicting them, but they\nonly provide limited interpretability. On the other hand, large-scale\nstatistical analyses have been a great asset to understanding disruptive\npatterns. In this paper, we leverage data-driven methods to find an\ninterpretable representation of the plasma state for disruption\ncharacterization. Specifically, we use a latent variable model to represent\ndiagnostic measurements as a low-dimensional, latent representation. We build\nupon the Variational Autoencoder (VAE) framework, and extend it for (1)\ncontinuous projections of plasma trajectories; (2) a multimodal structure to\nseparate operating regimes; and (3) separation with respect to disruptive\nregimes. Subsequently, we can identify continuous indicators for the disruption\nrate and the disruptivity based on statistical properties of measurement data.\nThe proposed method is demonstrated using a dataset of approximately 1600 TCV\ndischarges, selecting for flat-top disruptions or regular terminations. We\nevaluate the method with respect to (1) the identified disruption risk and its\ncorrelation with other plasma properties; (2) the ability to distinguish\ndifferent types of disruptions; and (3) downstream analyses. For the latter, we\nconduct a demonstrative study on identifying parameters connected to\ndisruptions using counterfactual-like analysis. Overall, the method can\nadequately identify distinct operating regimes characterized by varying\nproximity to disruptions in an interpretable manner.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u8868\u5f81\u6258\u5361\u9a6c\u514b\u4e2d\u7b49\u79bb\u5b50\u4f53\u72b6\u6001\u7684\u53ef\u89e3\u91ca\u8868\u793a\uff0c\u4ee5\u9884\u6d4b\u548c\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684\u7b49\u79bb\u5b50\u4f53\u7834\u88c2\u3002", "motivation": "\u7b49\u79bb\u5b50\u4f53\u7834\u88c2\u662f\u672a\u6765\u6258\u5361\u9a6c\u514b\u88c5\u7f6e\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u4e4b\u4e00\uff0c\u4f46\u76ee\u524d\u5bf9\u5176\u7406\u89e3\u6709\u9650\uff0c\u4e14\u6570\u636e\u9a71\u52a8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u8868\u793a\u65b9\u6cd5\u6539\u8fdb\u7834\u88c2\u9884\u6d4b\u548c\u7279\u5f81\u5206\u6790\u3002", "method": "\u6269\u5c55\u4e86VAE\u6846\u67b6\uff0c\u5305\u62ec\u8fde\u7eed\u6295\u5f71\u7b49\u79bb\u5b50\u4f53\u8f68\u8ff9\u3001\u591a\u6a21\u6001\u7ed3\u6784\u5206\u79bb\u64cd\u4f5c\u72b6\u6001\uff0c\u4ee5\u53ca\u57fa\u4e8e\u7edf\u8ba1\u7279\u6027\u7684\u7834\u88c2\u7387\u6307\u6807\u3002\u65b9\u6cd5\u57281600\u6b21TCV\u653e\u7535\u6570\u636e\u4e0a\u9a8c\u8bc1\u3002", "result": "\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u4e0d\u540c\u64cd\u4f5c\u72b6\u6001\u53ca\u5176\u4e0e\u7834\u88c2\u7684\u5173\u8054\uff0c\u5e76\u901a\u8fc7\u53cd\u4e8b\u5b9e\u5206\u6790\u8bc6\u522b\u4e0e\u7834\u88c2\u76f8\u5173\u7684\u53c2\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ee5\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u533a\u5206\u4e86\u4e0d\u540c\u64cd\u4f5c\u72b6\u6001\uff0c\u4e3a\u7834\u88c2\u9884\u6d4b\u548c\u7279\u5f81\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
