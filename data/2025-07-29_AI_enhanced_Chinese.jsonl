{"id": "2507.19060", "title": "PurpCode: Reasoning for Safer Code Generation", "authors": ["Jiawei Liu", "Nirav Diwan", "Zhe Wang", "Haoyu Zhai", "Xiaona Zhou", "Kiet A. Nguyen", "Tianjiao Yu", "Muntasir Wahed", "Yinlin Deng", "Hadjer Benkraouda", "Yuxiang Wei", "Lingming Zhang", "Ismini Lourentzou", "Gang Wang"], "categories": ["cs.CR", "cs.CL", "cs.LG", "cs.SE"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19060v1", "summary": "We introduce PurpCode, the first post-training recipe for training safe code\nreasoning models towards generating secure code and defending against malicious\ncyberactivities. PurpCode trains a reasoning model in two stages: (i) Rule\nLearning, which explicitly teaches the model to reference cybersafety rules to\ngenerate vulnerability-free code and to avoid facilitating malicious\ncyberactivities; and (ii) Reinforcement Learning, which optimizes model safety\nand preserves model utility through diverse, multi-objective reward mechanisms.\nTo empower the training pipelines with comprehensive cybersafety data, we\nconduct internal red-teaming to synthesize comprehensive and high-coverage\nprompts based on real-world tasks for inducing unsafe cyberactivities in the\nmodel. Based on PurpCode, we develop a reasoning-based coding model, namely\nPurpCode-32B, which demonstrates state-of-the-art cybersafety, outperforming\nvarious frontier models. Meanwhile, our alignment method decreases the model\noverrefusal rates in both general and cybersafety-specific scenarios, while\npreserving model utility in both code generation and common security knowledge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19060v1", "cate": "cs.CR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "PurpCode: 用于更安全代码生成的推理", "tldr": "PurpCode 是一种用于训练安全代码生成模型的首次训练后方法，通过两阶段推理模型实现网络安全，同时保持模型实用性，并取得了最先进的成果。", "motivation": "开发能够生成安全代码并抵御恶意网络活动的安全代码推理模型，以解决现有模型可能生成易受攻击代码或助长恶意网络活动的问题。", "method": "PurpCode 采用两阶段训练方法：(i) 规则学习，明确教导模型引用网络安全规则以生成无漏洞代码并避免助长恶意网络活动；(ii) 强化学习，通过多样化的多目标奖励机制优化模型安全性并保持模型实用性。为了提供全面的网络安全数据，该方法还进行了内部红队演练，以根据真实任务合成全面的高覆盖率提示，从而诱导模型中的不安全网络活动。", "result": "基于 PurpCode 开发的推理型编码模型 PurpCode-32B 展示了最先进的网络安全性，超越了各种前沿模型。同时，该对齐方法降低了模型在通用和网络安全特定场景下的过度拒绝率，同时在代码生成和常见安全知识方面保持了模型实用性。", "conclusion": "PurpCode 提供了一种有效的训练后配方，用于开发具有高网络安全性和实用性保留的安全代码生成模型。", "translation": "我们引入了 PurpCode，这是第一个用于训练安全代码推理模型的训练后配方，旨在生成安全代码并防御恶意网络活动。PurpCode 分两阶段训练推理模型：(i) 规则学习，明确教导模型引用网络安全规则以生成无漏洞代码并避免助长恶意网络活动；(ii) 强化学习，通过多样化的多目标奖励机制优化模型安全性并保持模型实用性。为了为训练管道提供全面的网络安全数据，我们进行了内部红队演练，根据真实任务合成全面且高覆盖率的提示，以诱导模型中的不安全网络活动。基于 PurpCode，我们开发了一个基于推理的编码模型，即 PurpCode-32B，它展示了最先进的网络安全性，超越了各种前沿模型。同时，我们的对齐方法降低了模型在通用和网络安全特定场景下的过度拒绝率，同时在代码生成和常见安全知识方面保持了模型实用性。", "summary": "本文介绍了 PurpCode，这是首个用于训练安全代码推理模型的训练后方法，旨在生成安全代码并抵御恶意网络活动。该方法分两阶段进行：规则学习，明确教导模型基于网络安全规则生成无漏洞代码；强化学习，通过多目标奖励机制优化模型安全性和实用性。为获取全面的网络安全数据，研究人员通过内部红队演练合成高覆盖率的提示。基于此，开发了 PurpCode-32B 模型，该模型在网络安全方面表现出最先进的性能，超越了现有前沿模型，并有效降低了过度拒绝率，同时保持了代码生成和安全知识的实用性。", "keywords": "安全代码生成, 网络安全, 推理模型, 强化学习, 红队", "comments": "PurpCode 引入了一种创新的两阶段训练后方法（规则学习和强化学习），并结合了红队演练来合成安全数据，这对于开发安全的 AI 代码生成模型至关重要。其在网络安全方面达到最先进水平并保持实用性的成就，突显了该方法在降低软件开发中 AI 相关风险的实际重要性。"}}
{"id": "2507.18740", "title": "Learned Single-Pixel Fluorescence Microscopy", "authors": ["Serban C. Tudosie", "Valerio Gandolfi", "Shivaprasad Varakkoth", "Andrea Farina", "Cosimo D'Andrea", "Simon Arridge"], "categories": ["eess.IV", "cs.CV", "cs.LG", "physics.optics"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 6 figures, 1 table", "url": "http://arxiv.org/abs/2507.18740v1", "summary": "Single-pixel imaging has emerged as a key technique in fluorescence\nmicroscopy, where fast acquisition and reconstruction are crucial. In this\ncontext, images are reconstructed from linearly compressed measurements. In\npractice, total variation minimisation is still used to reconstruct the image\nfrom noisy measurements of the inner product between orthogonal sampling\npattern vectors and the original image data. However, data can be leveraged to\nlearn the measurement vectors and the reconstruction process, thereby enhancing\ncompression, reconstruction quality, and speed. We train an autoencoder through\nself-supervision to learn an encoder (or measurement matrix) and a decoder. We\nthen test it on physically acquired multispectral and intensity data. During\nacquisition, the learned encoder becomes part of the physical device. Our\napproach can enhance single-pixel imaging in fluorescence microscopy by\nreducing reconstruction time by two orders of magnitude, achieving superior\nimage quality, and enabling multispectral reconstructions. Ultimately, learned\nsingle-pixel fluorescence microscopy could advance diagnosis and biological\nresearch, providing multispectral imaging at a fraction of the cost.", "comment": "10 pages, 6 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.18740v1", "cate": "eess.IV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "学习型单像素荧光显微镜", "tldr": "该研究提出了一种基于自监督自编码器的学习型单像素荧光显微镜方法，显著缩短了图像重建时间，提高了图像质量，并实现了多光谱重建。", "motivation": "单像素荧光显微镜中，图像重建速度和质量至关重要。现有方法（如全变分最小化）在从噪声测量中重建图像时效率不高且可能影响质量。该论文旨在利用数据学习测量向量和重建过程，从而提高压缩、重建质量和速度。", "method": "研究人员通过自监督方式训练了一个自编码器，以学习一个编码器（或测量矩阵）和一个解码器。然后，在实际获取的多光谱和强度数据上进行测试。在图像采集过程中，学习到的编码器被整合到物理设备中。", "result": "该方法将单像素荧光显微镜的重建时间缩短了两个数量级，实现了卓越的图像质量，并支持多光谱重建。", "conclusion": "学习型单像素荧光显微镜有望推动诊断和生物学研究，以更低的成本提供多光谱成像。", "translation": "单像素成像已成为荧光显微镜中的一项关键技术，其中快速采集和重建至关重要。在此背景下，图像从线性压缩测量中重建。在实践中，仍使用全变分最小化从正交采样模式向量与原始图像数据之间内积的噪声测量中重建图像。然而，可以利用数据来学习测量向量和重建过程，从而提高压缩、重建质量和速度。我们通过自监督训练一个自编码器，以学习一个编码器（或测量矩阵）和一个解码器。然后，我们在物理获取的多光谱和强度数据上对其进行测试。在采集过程中，学习到的编码器成为物理设备的一部分。我们的方法可以将荧光显微镜中的单像素成像重建时间缩短两个数量级，实现卓越的图像质量，并实现多光谱重建。最终，学习型单像素荧光显微镜可以推动诊断和生物学研究，以更低的成本提供多光谱成像。", "summary": "该论文提出了一种创新的学习型单像素荧光显微镜方法，通过自监督训练一个自编码器来优化测量和重建过程。与传统的全变分最小化方法相比，这种基于学习的编码器和解码器显著加快了图像重建速度（两个数量级），提高了图像质量，并首次实现了多光谱重建。该技术有望以更低的成本为诊断和生物学研究提供高性能的多光谱成像。", "keywords": "单像素成像, 荧光显微镜, 自编码器, 图像重建, 多光谱成像", "comments": "该论文的创新之处在于将深度学习（自编码器）应用于单像素成像，以优化传统的固定测量模式和迭代重建过程。通过学习测量向量和重建过程，它解决了现有方法在速度和质量上的局限性，特别是实现了显著的加速和多光谱能力。这对于推动生物医学成像领域具有重要意义，可能使高性能成像更加普及和经济。"}}
{"id": "2504.13372", "title": "Integration of a Graph-Based Path Planner and Mixed-Integer MPC for Robot Navigation in Cluttered Environments", "authors": ["Joshua A. Robbins", "Stephen J. Harnett", "Andrew F. Thompson", "Sean Brennan", "Herschel C. Pangborn"], "categories": ["eess.SY", "cs.RO", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.13372v2", "summary": "The ability to update a path plan is a required capability for autonomous\nmobile robots navigating through uncertain environments. This paper proposes a\nre-planning strategy using a multilayer planning and control framework for\ncases where the robot's environment is partially known. A medial axis\ngraph-based planner defines a global path plan based on known obstacles, where\neach edge in the graph corresponds to a unique corridor. A mixed-integer model\npredictive control (MPC) method detects if a terminal constraint derived from\nthe global plan is infeasible, subject to a non-convex description of the local\nenvironment. Infeasibility detection is used to trigger efficient global\nre-planning via medial axis graph edge deletion. The proposed re-planning\nstrategy is demonstrated experimentally.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.13372v2", "cate": "eess.SY", "date": "2025-04-17", "updated": "2025-07-25", "AI": {"title_translation": "基于图的路径规划器与混合整数MPC集成用于杂乱环境中的机器人导航", "tldr": "本文提出了一种多层规划与控制框架，结合基于中轴图的全局规划和混合整数MPC的局部控制，以实现机器人在部分已知杂乱环境中的高效路径重规划。", "motivation": "在不确定环境中自主移动机器人需要具备更新路径规划的能力。", "method": "本文提出了一种多层规划与控制框架的重规划策略。该策略使用基于中轴图的规划器定义全局路径，并使用混合整数模型预测控制（MPC）方法检测局部环境的终端约束是否不可行。当检测到不可行时，通过删除中轴图边缘来触发高效的全局重规划。", "result": "所提出的重规划策略已通过实验证明。", "conclusion": "该研究成功地提出并实验验证了一种结合全局规划和局部控制的多层重规划策略，有效地解决了机器人在部分已知杂乱环境中的导航问题。", "translation": "自主移动机器人在不确定环境中导航时，更新路径规划的能力是一项必需的能力。本文提出了一种重规划策略，该策略采用多层规划与控制框架，适用于机器人环境部分已知的情况。基于中轴图的规划器根据已知障碍物定义全局路径规划，其中图中的每条边对应一个独特的走廊。混合整数模型预测控制（MPC）方法检测源自全局规划的终端约束是否不可行，该约束受局部环境的非凸描述影响。不可行性检测用于通过删除中轴图边缘来触发高效的全局重规划。所提出的重规划策略已通过实验证明。", "summary": "本文提出了一种用于机器人在部分已知杂乱环境中导航的重规划策略。该策略采用多层规划与控制框架，结合了基于中轴图的全局路径规划器和混合整数模型预测控制（MPC）。全局规划器负责基于已知障碍物生成路径，而MPC则检测局部环境中的约束不可行性。一旦检测到不可行，系统将通过删除中轴图边缘来触发高效的全局重规划。该重规划策略已通过实验验证。", "keywords": "机器人导航, 路径规划, 混合整数MPC, 重规划, 中轴图", "comments": "这篇论文的创新点在于结合了全局的基于图的规划和局部的混合整数MPC，形成了一个分层且能够动态重规划的框架。这种集成方法有效地解决了机器人导航中环境不确定性带来的挑战，特别是通过实时检测不可行性并触发高效的重规划，增强了机器人在复杂环境中的适应性和鲁棒性。实验验证进一步增强了其实用性。"}}
{"id": "2403.11362", "title": "A boostlet transform for wave-based acoustic signal processing in space-time", "authors": ["Elias Zea", "Marco Laudato", "Joakim Andén"], "categories": ["physics.flu-dyn", "eess.SP", "math-ph", "math.MP"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "Comments:      30 pages, 7 figures", "url": "http://arxiv.org/abs/2403.11362v2", "summary": "Sparse representation systems that encode signal architecture have had a\nprofound impact on sampling and compression paradigms. Remarkable examples are\nmulti-scale directional systems, which, similar to our vision system, encode\nthe underlying architecture of natural images with sparse features. Inspired by\nthis philosophy, we introduce a representation system for wave-based acoustic\nsignal processing in 2D space--time, referred to as the \\emph{boostlet\ntransform}, which encodes sparse features of natural acoustic fields using the\nPoincar\\'e group and isotropic dilations. Boostlets are spatiotemporal\nfunctions parametrized with dilations, Lorentz boosts, and translations in\nspace--time. Physically speaking, boostlets are supported away from the\nacoustic radiation cone, i.e., having broadband frequency with phase velocities\nother than the speed of sound, resulting in a peculiar scaling function. We\nformulate a discrete boostlet frame using Meyer wavelets and bump functions and\nexamine its sparsity properties. An analysis with experimentally measured\nfields indicates that discrete boostlet coefficients decay significantly faster\nand attain superior reconstruction performance than wavelets, curvelets,\nshearlets, and wave atoms. The results demonstrate that boostlets provide a\nnatural, compact representation system for acoustic waves in space-time.", "comment": "30 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2403.11362v2", "cate": "physics.flu-dyn", "date": "2024-03-17", "updated": "2025-07-25", "AI": {"title_translation": "空间-时间中基于波的声学信号处理的boostlet变换", "tldr": "引入了一种名为boostlet变换的新型稀疏表示系统，用于空间-时间中的声学信号处理，实验证明其在稀疏性和重建性能方面优于现有方法。", "motivation": "现有稀疏表示系统（如多尺度方向系统）在图像处理中表现出色，本文受此启发，旨在为基于波的声学信号处理开发一种新的稀疏表示系统。", "method": "引入了名为“boostlet变换”的表示系统，用于二维空间-时间中的基于波的声学信号处理。Boostlet通过Poincar\\'e群和各向同性膨胀编码自然声场的稀疏特征，它们是受膨胀、洛伦兹增强和空间-时间平移参数化的时空函数。研究中还使用Meyer小波和凹凸函数构建了离散boostlet框架，并分析了其稀疏性。", "result": "实验测量场的分析表明，离散boostlet系数衰减速度显著快于小波、曲波、剪切波和波原子，并获得了卓越的重建性能。", "conclusion": "Boostlet为空间-时间中的声波提供了一种自然、紧凑的表示系统。", "translation": "稀疏表示系统编码信号架构对采样和压缩范式产生了深远影响。显著的例子是多尺度方向系统，它们类似于我们的视觉系统，用稀疏特征编码自然图像的底层架构。受这一理念启发，我们引入了一种用于二维空间-时间中基于波的声学信号处理的表示系统，称为“boostlet变换”，它利用Poincar\\'e群和各向同性膨胀编码自然声场的稀疏特征。Boostlet是时空函数，由膨胀、洛伦兹增强和空间-时间平移参数化。从物理上讲，boostlet支持远离声辐射锥，即具有宽带频率和非声速的相速度，从而产生独特的缩放函数。我们使用Meyer小波和凹凸函数构建了一个离散boostlet框架，并检查了其稀疏性。对实验测量场的分析表明，离散boostlet系数比小波、曲波、剪切波和波原子衰减得显著更快，并获得了卓越的重建性能。结果表明，boostlet为空间-时间中的声波提供了一种自然、紧凑的表示系统。", "summary": "本文受多尺度方向系统在图像处理中稀疏表示的启发，提出了一种名为“boostlet变换”的新型表示系统，用于二维空间-时间中的基于波的声学信号处理。Boostlet通过Poincar\\'e群和各向同性膨胀来编码自然声场的稀疏特征，是受膨胀、洛伦兹增强和空间-时间平移参数化的时空函数。通过使用Meyer小波和凹凸函数构建离散boostlet框架，并与现有方法（如小波、曲波等）进行比较，实验结果表明boostlet系数衰减更快，并实现了更优越的重建性能，证明了其作为声波在空间-时间中自然、紧凑表示系统的有效性。", "keywords": "boostlet变换, 稀疏表示, 声学信号处理, 空间-时间, Poincar\\'e群", "comments": "这项工作引入了一种创新的boostlet变换，它利用了Poincar\\'e群和洛伦兹增强，为声学信号处理提供了一种新的稀疏表示方法。其主要创新在于将这种数学框架应用于声波，并实验证明其在稀疏性和重建性能上优于传统的多尺度分析工具，这对于声学数据的高效采样和压缩具有重要意义。"}}
{"id": "2402.13470", "title": "How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?", "authors": ["Aviv Brokman", "Ramakanth Kavuluru"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      A version of this paper has appeared in the proceedings of NLDB 2025 with a slightly different title. The corresponding DOI is also listed below in the metadata", "url": "http://arxiv.org/abs/2402.13470v2", "summary": "Cutting edge techniques developed in the general NLP domain are often\nsubsequently applied to the high-value, data-rich biomedical domain. The past\nfew years have seen generative language models (LMs), instruction finetuning,\nand few-shot learning become foci of NLP research. As such, generative LMs\npretrained on biomedical corpora have proliferated and biomedical instruction\nfinetuning has been attempted as well, all with the hope that domain\nspecificity improves performance on downstream tasks. Given the nontrivial\neffort in training such models, we investigate what, if any, benefits they have\nin the key biomedical NLP task of relation extraction. Specifically, we address\ntwo questions: (1) Do LMs trained on biomedical corpora outperform those\ntrained on general domain corpora? (2) Do models instruction finetuned on\nbiomedical datasets outperform those finetuned on assorted datasets or those\nsimply pretrained? We tackle these questions using existing LMs, testing across\nfour datasets. In a surprising result, general-domain models typically\noutperformed biomedical-domain models. However, biomedical instruction\nfinetuning improved performance to a similar degree as general instruction\nfinetuning, despite having orders of magnitude fewer instructions. Our findings\nsuggest it may be more fruitful to focus research effort on larger-scale\nbiomedical instruction finetuning of general LMs over building domain-specific\nbiomedical LMs", "comment": "A version of this paper has appeared in the proceedings of NLDB 2025\n  with a slightly different title. The corresponding DOI is also listed below\n  in the metadata", "pdf_url": "http://arxiv.org/pdf/2402.13470v2", "cate": "cs.CL", "date": "2024-02-21", "updated": "2025-07-25", "AI": {"title_translation": "生物医学关系提取中语言模型和指令微调的领域特异性有多重要？", "tldr": "通用语言模型在生物医学关系提取中通常优于生物医学领域模型；生物医学指令微调即使指令量少也能有效提升性能。", "motivation": "鉴于训练领域特定语言模型需要大量精力，且普遍认为领域特异性可提升性能，本文旨在探究领域特异性语言模型和指令微调在生物医学关系提取这一关键任务中的益处。具体来说，研究旨在回答生物医学语料库训练的语言模型是否优于通用领域语料库训练的模型，以及生物医学指令微调的模型是否优于其他微调或仅预训练的模型。", "method": "本研究使用现有语言模型，并在四个数据集上进行测试，以评估在生物医学语料库上训练的语言模型与在通用领域语料库上训练的语言模型之间的性能差异，以及在生物医学数据集上进行指令微调的模型与在其他数据集上微调或仅预训练的模型之间的性能差异。", "result": "出人意料的是，通用领域模型通常优于生物医学领域模型。然而，尽管生物医学指令微调的指令数量少几个数量级，但其性能提升程度与通用指令微调相似。", "conclusion": "研究结果表明，将研究精力更多地集中于对通用语言模型进行更大规模的生物医学指令微调，可能比构建领域特定的生物医学语言模型更有成效。", "translation": "通用NLP领域中开发的尖端技术通常随后应用于高价值、数据丰富的生物医学领域。过去几年，生成式语言模型（LMs）、指令微调和少样本学习成为NLP研究的焦点。因此，在生物医学语料库上预训练的生成式语言模型大量涌现，生物医学指令微调也进行了尝试，所有这些都希望领域特异性能够提高下游任务的性能。考虑到训练这些模型需要付出不小的努力，我们调查了它们在生物医学NLP关键任务——关系提取中是否有益。具体来说，我们解决了两个问题：（1）在生物医学语料库上训练的语言模型是否优于在通用领域语料库上训练的语言模型？（2）在生物医学数据集上进行指令微调的模型是否优于在各种数据集上进行微调的模型或仅仅是预训练的模型？我们使用现有语言模型，并在四个数据集上进行测试来解决这些问题。令人惊讶的结果是，通用领域模型通常优于生物医学领域模型。然而，尽管生物医学指令微调的指令数量少几个数量级，但它将性能提高到与通用指令微调相似的程度。我们的发现表明，将研究精力集中在对通用语言模型进行更大规模的生物医学指令微调上，可能比构建领域特定的生物医学语言模型更有成效。", "summary": "本文探讨了语言模型（LMs）和指令微调在生物医学关系提取中领域特异性的重要性。研究比较了在生物医学语料库和通用语料库上训练的语言模型，以及生物医学指令微调与通用指令微调的性能。令人惊讶的是，通用领域语言模型通常优于生物医学领域模型。然而，生物医学指令微调被证明是有效的，尽管使用的指令数量少得多，但其性能提升与通用微调相似。研究建议将重点放在对通用语言模型进行大规模生物医学指令微调上，而非开发新的领域特定语言模型。", "keywords": "语言模型, 指令微调, 生物医学关系提取, 领域特异性, 自然语言处理", "comments": "本文提出了一个反直觉的发现，即在生物医学关系提取任务中，通用领域语言模型的表现优于领域特定语言模型，这挑战了领域特异性总是带来更好性能的普遍假设。此外，关于生物医学指令微调效率的发现（在数据量显著更少的情况下获得相似的性能提升）尤其有见地，这为生物医学NLP未来的研究指明了一条更高效的路径，有望节省大量计算资源和开发精力。"}}
{"id": "2507.19185", "title": "PrompTrend: Continuous Community-Driven Vulnerability Discovery and Assessment for Large Language Models", "authors": ["Tarek Gasmi", "Ramzi Guesmi", "Mootez Aloui", "Jihene Bennaceur"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19185v1", "summary": "Static benchmarks fail to capture LLM vulnerabilities emerging through\ncommunity experimentation in online forums. We present PrompTrend, a system\nthat collects vulnerability data across platforms and evaluates them using\nmultidimensional scoring, with an architecture designed for scalable\nmonitoring. Cross-sectional analysis of 198 vulnerabilities collected from\nonline communities over a five-month period (January-May 2025) and tested on\nnine commercial models reveals that advanced capabilities correlate with\nincreased vulnerability in some architectures, psychological attacks\nsignificantly outperform technical exploits, and platform dynamics shape attack\neffectiveness with measurable model-specific patterns. The PrompTrend\nVulnerability Assessment Framework achieves 78% classification accuracy while\nrevealing limited cross-model transferability, demonstrating that effective LLM\nsecurity requires comprehensive socio-technical monitoring beyond traditional\nperiodic assessment. Our findings challenge the assumption that capability\nadvancement improves security and establish community-driven psychological\nmanipulation as the dominant threat vector for current language models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19185v1", "cate": "cs.CR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "PrompTrend：大型语言模型持续社区驱动的漏洞发现与评估", "tldr": "PrompTrend系统通过收集社区数据和多维评估，揭示了LLM漏洞的动态性，发现心理攻击是主要威胁，并强调了持续社会技术监控的重要性。", "motivation": "传统的静态基准测试无法捕捉在线社区中涌现的大型语言模型（LLM）漏洞，因此需要一个系统来持续发现和评估这些漏洞。", "method": "论文提出了PrompTrend系统，该系统收集跨平台的漏洞数据，使用多维评分进行评估，并采用可扩展的架构进行监控。研究对2025年1月至5月期间从在线社区收集的198个漏洞进行了横断面分析，并在9个商业模型上进行了测试。PrompTrend漏洞评估框架实现了78%的分类准确率。", "result": "研究发现，在某些架构中，高级能力与更高的漏洞相关；心理攻击显著优于技术攻击；平台动态影响攻击的有效性，并存在可测量的模型特定模式。PrompTrend框架的分类准确率为78%，但显示出有限的跨模型可转移性。", "conclusion": "有效的大型语言模型安全需要超越传统定期评估的全面社会技术监控。研究结果挑战了能力提升能改善安全的假设，并确立了社区驱动的心理操纵是当前语言模型的主要威胁向量。", "translation": "静态基准测试未能捕捉到通过在线论坛社区实验涌现的大型语言模型（LLM）漏洞。我们提出了PrompTrend，一个系统，它收集跨平台的漏洞数据并使用多维评分进行评估，其架构设计用于可扩展监控。对2025年1月至5月期间从在线社区收集的198个漏洞进行横断面分析，并在9个商业模型上进行测试，结果显示，在某些架构中，高级能力与漏洞增加相关，心理攻击显著优于技术漏洞利用，并且平台动态通过可测量的模型特定模式影响攻击有效性。PrompTrend漏洞评估框架实现了78%的分类准确率，同时揭示了有限的跨模型可转移性，这表明有效的大型语言模型安全需要超越传统定期评估的全面社会技术监控。我们的发现挑战了能力提升能改善安全的假设，并确立了社区驱动的心理操纵是当前语言模型的主要威胁向量。", "summary": "本文介绍了PrompTrend系统，旨在解决传统静态基准测试无法捕捉在线社区中涌现的大型语言模型（LLM）漏洞的问题。PrompTrend通过收集跨平台漏洞数据并进行多维评估，实现了对LLM漏洞的持续监控。研究分析了198个社区发现的漏洞，发现在某些架构中，LLM能力提升反而增加了漏洞，心理攻击比技术攻击更有效，且攻击有效性受平台动态影响。PrompTrend框架在漏洞分类上达到78%的准确率，但漏洞跨模型可转移性有限。论文强调LLM安全需要持续的社会技术监控，而非仅依赖周期性评估，并指出社区驱动的心理操纵是当前LLM的主要威胁。", "keywords": "大型语言模型, 漏洞发现, 社区驱动, 心理攻击, 安全评估", "comments": "这篇论文创新性地提出了一种“社区驱动”的漏洞发现和评估方法，弥补了传统静态基准测试的不足。其重要性在于揭示了LLM安全面临的动态挑战，特别是心理攻击作为主要威胁，以及模型能力与漏洞之间的复杂关系。论文强调的“社会技术监控”超越了纯技术范畴，为LLM安全提供了新的视角。然而，论文中提到的数据收集时间“January-May 2025”似乎是一个未来时间，这可能是一个笔误，或者暗示了其前瞻性研究的性质。"}}
{"id": "2507.19113", "title": "Exploring the Use of LLMs for Requirements Specification in an IT Consulting Company", "authors": ["Liliana Pasquale", "Azzurra Ragone", "Emanuele Piemontese", "Armin Amiri Darban"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      11 pages, 5 figures. Accepted for presentation at the Industrial Innovation Track of the 33rd IEEE International Requirements Engineering Conference (RE 2025), Valencia, Spain", "url": "http://arxiv.org/abs/2507.19113v1", "summary": "In practice, requirements specification remains a critical challenge. The\nknowledge necessary to generate a specification can often be fragmented across\ndiverse sources (e.g., meeting minutes, emails, and high-level product\ndescriptions), making the process cumbersome and time-consuming. In this paper,\nwe report our experience using large language models (LLMs) in an IT consulting\ncompany to automate the requirements specification process. In this company,\nrequirements are specified using a Functional Design Specification (FDS), a\ndocument that outlines the functional requirements and features of a system,\napplication, or process. We provide LLMs with a summary of the requirements\nelicitation documents and FDS templates, prompting them to generate Epic FDS\n(including high-level product descriptions) and user stories, which are\nsubsequently compiled into a complete FDS document. We compared the correctness\nand quality of the FDS generated by three state-of-the-art LLMs against those\nproduced by human analysts. Our results show that LLMs can help automate and\nstandardize the requirements specification, reducing time and human effort.\nHowever, the quality of LLM-generated FDS highly depends on inputs and often\nrequires human revision. Thus, we advocate for a synergistic approach in which\nan LLM serves as an effective drafting tool while human analysts provide the\ncritical contextual and technical oversight necessary for high-quality\nrequirements engineering (RE) documentation.", "comment": "11 pages, 5 figures. Accepted for presentation at the Industrial\n  Innovation Track of the 33rd IEEE International Requirements Engineering\n  Conference (RE 2025), Valencia, Spain", "pdf_url": "http://arxiv.org/pdf/2507.19113v1", "cate": "cs.SE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "探索大型语言模型在IT咨询公司需求规范中的应用", "tldr": "本研究探讨了在IT咨询公司中使用大型语言模型（LLMs）自动化需求规范的经验，发现LLMs能提高效率但仍需人工修订。", "motivation": "需求规范过程复杂且耗时，因为所需知识分散在不同来源（如会议记录、邮件和高层产品描述）中。", "method": "研究人员在一家IT咨询公司中，使用LLMs自动化需求规范过程。他们向LLMs提供需求获取文档摘要和FDS模板，让LLMs生成Epic FDS和用户故事，然后编译成完整的FDS文档。将三款最先进的LLMs生成的FDS与人工分析师生成的FDS进行正确性和质量比较。", "result": "结果显示，LLMs可以帮助自动化和标准化需求规范，减少时间和人力。然而，LLM生成的FDS质量高度依赖输入，并且通常需要人工修订。", "conclusion": "大型语言模型可以作为有效的起草工具，但人类分析师提供必要的上下文和技术监督对高质量的需求工程文档至关重要，因此提倡协同方法。", "translation": "在实践中，需求规范仍然是一个关键挑战。生成规范所需的知识通常分散在不同的来源（例如，会议记录、电子邮件和高层产品描述）中，这使得该过程繁琐且耗时。在本文中，我们报告了在一家IT咨询公司中使用大型语言模型（LLMs）自动化需求规范过程的经验。在这家公司中，需求通过功能设计规范（FDS）来规定，FDS是概述系统、应用程序或过程的功能需求和特性的文档。我们向LLMs提供了需求获取文档的摘要和FDS模板，提示它们生成史诗级FDS（包括高层产品描述）和用户故事，这些内容随后被编译成完整的FDS文档。我们比较了三款最先进的LLMs生成的FDS与人类分析师生成的FDS的正确性和质量。我们的结果表明，LLMs可以帮助自动化和标准化需求规范，减少时间和人力。然而，LLM生成的FDS的质量高度依赖于输入，并且通常需要人工修订。因此，我们提倡一种协同方法，即LLM作为有效的起草工具，而人类分析师则提供高质量需求工程（RE）文档所需的关键上下文和技术监督。", "summary": "本研究探讨了在IT咨询公司中利用大型语言模型（LLMs）自动化需求规范的实践经验。通过向LLMs提供需求文档摘要和FDS模板，使其生成Epic FDS和用户故事，并与人工产出进行质量比较。结果表明，LLMs能有效提高需求规范的自动化和标准化水平，减少时间和精力，但其产出质量受输入影响且仍需人工修订。因此，论文倡导LLMs作为起草工具与人工监督相结合的协同工作模式。", "keywords": "大型语言模型, 需求规范, IT咨询, 自动化, 功能设计规范", "comments": "该研究具有重要的实践意义，因为它直接解决了IT咨询领域中需求规范的痛点。其创新之处在于将LLMs应用于FDS的生成，并提供了实际案例的验证。研究强调了LLMs的效率提升潜力，同时也明确指出了其局限性，即对输入质量的依赖和对人工修订的需求，这为未来LLMs在复杂业务流程中的应用提供了现实指导，避免了过度乐观的期望，并提出了人机协作的有效范式。"}}
{"id": "2507.19205", "title": "Physics-Informed Graph Neural Networks for Transverse Momentum Estimation in CMS Trigger Systems", "authors": ["Md Abrar Jahin", "Shahriar Soudeep", "M. F. Mridha", "Muhammad Mostafa Monowar", "Md. Abdul Hamid"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19205v1", "summary": "Real-time particle transverse momentum ($p_T$) estimation in high-energy\nphysics demands algorithms that are both efficient and accurate under strict\nhardware constraints. Static machine learning models degrade under high pileup\nand lack physics-aware optimization, while generic graph neural networks (GNNs)\noften neglect domain structure critical for robust $p_T$ regression. We propose\na physics-informed GNN framework that systematically encodes detector geometry\nand physical observables through four distinct graph construction strategies\nthat systematically encode detector geometry and physical observables:\nstation-as-node, feature-as-node, bending angle-centric, and pseudorapidity\n($\\eta$)-centric representations. This framework integrates these tailored\ngraph structures with a novel Message Passing Layer (MPL), featuring\nintra-message attention and gated updates, and domain-specific loss functions\nincorporating $p_{T}$-distribution priors. Our co-design methodology yields\nsuperior accuracy-efficiency trade-offs compared to existing baselines.\nExtensive experiments on the CMS Trigger Dataset validate the approach: a\nstation-informed EdgeConv model achieves a state-of-the-art MAE of 0.8525 with\n$\\ge55\\%$ fewer parameters than deep learning baselines, especially TabNet,\nwhile an $\\eta$-centric MPL configuration also demonstrates improved accuracy\nwith comparable efficiency. These results establish the promise of\nphysics-guided GNNs for deployment in resource-constrained trigger systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19205v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "物理信息图神经网络在CMS触发系统中横向动量估计的应用", "tldr": "提出了一种物理信息图神经网络（GNN）框架，用于在CMS触发系统中高效准确地估计横向动量（$p_T$），通过定制的图结构和消息传递层，实现了优于现有基线的精度和效率。", "motivation": "实时高能物理中的粒子横向动量($p_T$)估计需要在严格的硬件限制下同时具备效率和准确性。现有静态机器学习模型在高堆积环境下性能下降且缺乏物理优化，而通用GNNs常忽略对鲁棒$p_T$回归至关重要的领域结构。", "method": "提出了一个物理信息图神经网络框架，通过四种不同的图构建策略（以站为节点、以特征为节点、以弯曲角为中心、以赝快度($\boldsymbol{\\eta}$)为中心表示）系统地编码探测器几何和物理可观测数据。该框架将定制的图结构与一种新颖的消息传递层（MPL）集成，该MPL具有消息内注意力（intra-message attention）和门控更新（gated updates），并结合了包含$p_T$分布先验的领域特定损失函数。", "result": "共同设计的方法在精度-效率权衡方面优于现有基线。在CMS触发数据集上的广泛实验验证了该方法：一个基于站点的EdgeConv模型实现了0.8525的最新MAE，参数量比深度学习基线（特别是TabNet）少55%以上；一个以$\boldsymbol{\\eta}$为中心的MPL配置也展示了更高的精度和相当的效率。", "conclusion": "这些结果确立了物理引导的GNNs在资源受限的触发系统中部署的潜力。", "translation": "高能物理中实时粒子横向动量（$p_T$）估计要求算法在严格的硬件限制下既高效又准确。静态机器学习模型在高堆积环境下性能下降且缺乏物理感知优化，而通用图神经网络（GNNs）往往忽略了对鲁棒$p_T$回归至关重要的领域结构。我们提出了一种物理信息图神经网络框架，通过四种不同的图构建策略系统地编码探测器几何和物理可观测数据：以站为节点、以特征为节点、以弯曲角为中心和以赝快度（$\boldsymbol{\\eta}$）为中心表示。该框架将这些定制的图结构与一种新颖的消息传递层（MPL）集成，该MPL具有消息内注意力（intra-message attention）和门控更新（gated updates），并结合了包含$p_T$分布先验的领域特定损失函数。我们的协同设计方法与现有基线相比，在精度-效率权衡方面表现出卓越的性能。在CMS触发数据集上的广泛实验验证了该方法：一个基于站点的EdgeConv模型实现了0.8525的最新MAE，参数量比深度学习基线（特别是TabNet）少55%以上，而一个以$\boldsymbol{\\eta}$为中心的MPL配置也展示了更高的精度和相当的效率。这些结果确立了物理引导的GNNs在资源受限的触发系统中部署的潜力。", "summary": "本文提出了一种物理信息图神经网络（Physics-Informed GNN）框架，用于在CMS触发系统中高效准确地估计粒子横向动量（$p_T$）。该框架通过四种定制的图构建策略（如以站为节点、以$\boldsymbol{\\eta}$为中心）系统地编码物理信息和探测器几何，并结合了新颖的消息传递层（MPL）和领域特定损失函数。实验结果表明，与现有基线相比，所提出的方法在精度和效率上均表现优异，尤其是在参数量显著减少的情况下实现了最先进的性能，验证了物理引导GNN在资源受限触发系统中的应用潜力。", "keywords": "物理信息图神经网络, 横向动量估计, CMS触发系统, 高能物理, 图神经网络", "comments": "这篇论文的创新点在于将物理领域知识深度融合到图神经网络的设计中，通过定制的图结构和损失函数解决了高能物理中$p_T$估计的特定挑战。其协同设计方法在保持高精度的同时显著降低了模型复杂度，这对于资源受限的实时触发系统至关重要，展示了GNN在科学领域应用的巨大潜力。"}}
{"id": "2503.22026", "title": "Multispectral Demosaicing via Dual Cameras", "authors": ["SaiKiran Tedla", "Junyong Lee", "Beixuan Yang", "Mahmoud Afifi", "Michael S. Brown"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:       this https URL", "url": "http://arxiv.org/abs/2503.22026v3", "summary": "Multispectral (MS) images capture detailed scene information across a wide\nrange of spectral bands, making them invaluable for applications requiring rich\nspectral data. Integrating MS imaging into multi camera devices, such as\nsmartphones, has the potential to enhance both spectral applications and RGB\nimage quality. A critical step in processing MS data is demosaicing, which\nreconstructs color information from the mosaic MS images captured by the\ncamera. This paper proposes a method for MS image demosaicing specifically\ndesigned for dual-camera setups where both RGB and MS cameras capture the same\nscene. Our approach leverages co-captured RGB images, which typically have\nhigher spatial fidelity, to guide the demosaicing of lower-fidelity MS images.\nWe introduce the Dual-camera RGB-MS Dataset - a large collection of paired RGB\nand MS mosaiced images with ground-truth demosaiced outputs - that enables\ntraining and evaluation of our method. Experimental results demonstrate that\nour method achieves state-of-the-art accuracy compared to existing techniques.", "comment": "https://ms-demosaic.github.io/", "pdf_url": "http://arxiv.org/pdf/2503.22026v3", "cate": "cs.CV", "date": "2025-03-27", "updated": "2025-07-25", "AI": {"title_translation": "双摄像头多光谱去马赛克", "tldr": "本文提出了一种用于双摄像头设置的多光谱图像去马赛克方法。该方法利用高保真度的RGB图像来引导低保真度的多光谱图像去马赛克，并引入了一个新的数据集。实验结果表明其达到了最先进的准确性。", "motivation": "多光谱图像在需要丰富光谱数据的应用中具有重要价值。将多光谱成像集成到智能手机等多摄像头设备中，有望增强光谱应用和RGB图像质量。去马赛克是处理多光谱数据的关键步骤，因此需要开发有效的去马赛克方法。", "method": "本文提出了一种专为双摄像头设置（RGB和MS相机捕获相同场景）设计的多光谱图像去马赛克方法。该方法利用共同捕获的高空间保真度RGB图像来引导低保真度MS图像的去马赛克。此外，还引入了“双摄像头RGB-MS数据集”，一个包含配对RGB和MS马赛克图像及其真实去马赛克输出的大型数据集，用于方法的训练和评估。", "result": "实验结果表明，该方法与现有技术相比，实现了最先进的准确性。", "conclusion": "本文提出的双摄像头多光谱去马赛克方法，通过利用RGB图像的引导和新的数据集进行训练，成功实现了多光谱图像的高精度重建，并达到了最先进的性能，有望促进多光谱成像在多摄像头设备中的应用。", "translation": "多光谱 (MS) 图像捕获了广泛光谱带中的详细场景信息，这使得它们在需要丰富光谱数据的应用中具有不可估量的价值。将多光谱成像集成到多摄像头设备（例如智能手机）中，有可能同时增强光谱应用和RGB图像质量。处理MS数据的关键一步是去马赛克，它从相机捕获的马赛克MS图像中重建颜色信息。本文提出了一种专门为双摄像头设置设计的多光谱图像去马赛克方法，其中RGB和MS相机捕获相同的场景。我们的方法利用共同捕获的RGB图像（这些图像通常具有更高的空间保真度）来指导较低保真度MS图像的去马赛克。我们引入了“双摄像头RGB-MS数据集”——一个包含配对RGB和MS马赛克图像以及真实去马赛克输出的大型数据集——该数据集能够训练和评估我们的方法。实验结果表明，我们的方法与现有技术相比，实现了最先进的准确性。", "summary": "本文提出了一种创新的双摄像头多光谱去马赛克方法，旨在解决多光谱图像处理中的关键步骤。该方法利用双摄像头设置中RGB图像的高空间保真度来引导低保真度多光谱图像的重建。为支持方法的训练和评估，研究者还创建并发布了一个名为“双摄像头RGB-MS数据集”的大型数据集。实验结果验证了该方法的有效性，表明其在准确性方面达到了现有技术的最新水平。", "keywords": "多光谱成像, 去马赛克, 双摄像头, RGB-MS数据集, 最先进技术", "comments": "该论文的创新点在于提出了利用双摄像头设置中RGB图像的高空间保真度来辅助多光谱图像去马赛克的策略，这为提升多光谱成像在消费级设备上的应用潜力提供了新思路。同时，发布专门的“双摄像头RGB-MS数据集”对于推动相关领域的研究和方法评估具有重要意义。"}}
{"id": "2507.19405", "title": "Convergence of Discrete Exterior Calculus for the Hodge-Dirac Operator", "authors": ["Radovan Dabetić", "Ralf Hiptmair"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19405v1", "summary": "A short proof of convergence for the discretization of the Hodge-Dirac\noperator in the framework of discrete exterior calculus (DEC) is provided using\nthe techniques established in [Johnny Guzm\\'an and Pratyush Potu, A Framework\nfor Analysis of DEC Approximations to Hodge-Laplacian Problems using\nGeneralized Whitney Forms, arXiv:2505.08934, 2025]", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19405v1", "cate": "math.NA", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "霍奇-狄拉克算子的离散外微分收敛性", "tldr": "本文利用现有技术，为离散外微分（DEC）框架下霍奇-狄拉克算子离散化的收敛性提供了一个简短的证明。", "motivation": "本文旨在提供霍奇-狄拉克算子在离散外微分（DEC）框架下离散化收敛性的证明，以验证或确立DEC在该领域的有效性。", "method": "利用了Johnny Guzm\\u00e1n和Pratyush Potu在《A Framework for Analysis of DEC Approximations to Hodge-Laplacian Problems using Generalized Whitney Forms》中建立的技术。", "result": "提供了霍奇-狄拉克算子在离散外微分（DEC）框架下离散化的收敛性的一个简短证明。", "conclusion": "霍奇-狄拉克算子在离散外微分（DEC）框架下的离散化是收敛的。", "translation": "本文利用[Johnny Guzm\\u00e1n 和 Pratyush Potu, A Framework for Analysis of DEC Approximations to Hodge-Laplacian Problems using Generalized Whitney Forms, arXiv:2505.08934, 2025]中建立的技术，为离散外微分（DEC）框架下霍奇-狄拉克算子离散化的收敛性提供了一个简短的证明。", "summary": "本文利用先前研究中建立的技术，提出了霍奇-狄拉克算子在离散外微分（DEC）框架下离散化收敛性的一个简洁证明。", "keywords": "离散外微分, 霍奇-狄拉克算子, 收敛性, 离散化", "comments": "该论文的创新点在于为霍奇-狄拉克算子在离散外微分（DEC）框架下的离散化提供了“简短证明”，这可能意味着对现有理论的简化或更高效的验证。其重要性在于为DEC方法在处理相关数学物理问题时的准确性和可靠性提供了坚实的理论基础。"}}
{"id": "2507.19295", "title": "On the Security of a Code-Based PIR Scheme", "authors": ["Svenja Lage", "Hannes Bartz"], "categories": ["cs.CR", "cs.IR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19295v1", "summary": "Private Information Retrieval (PIR) schemes allow clients to retrieve files\nfrom a database without disclosing the requested file's identity to the server.\nIn the pursuit of post-quantum security, most recent PIR schemes rely on hard\nlattice problems. In contrast, the so called CB-cPIR scheme stands out as a\npioneering effort to base PIR schemes on hard problems in coding theory,\nthereby contributing significantly to the diversification of security\nfoundations. However, our research reveals a critical vulnerability in CB-cPIR,\nsubstantially diminishing its security levels. Moreover, a comparative analysis\nwith state-of-the-art PIR schemes shows that CB-cPIR's advantages are reduced,\nmaking it less competitive in terms of the communication cost. Nevertheless,\nour findings highlight the importance of continued research into code-based PIR\nschemes, as they have the potential to provide a valuable alternative to\nlattice-based approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19295v1", "cate": "cs.CR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "关于基于编码的PIR方案的安全性", "tldr": "该论文发现了一种基于编码的PIR方案CB-cPIR的关键漏洞，降低了其安全性与竞争力，但强调需继续研究此类方案。", "motivation": "现有PIR方案多依赖格问题，CB-cPIR是基于编码理论的PIR方案的开创性尝试，旨在提供安全基础的多样性。本研究的动机是评估CB-cPIR方案的安全性。", "method": "本研究通过揭示关键漏洞和与现有先进PIR方案进行比较分析。", "result": "发现CB-cPIR存在关键漏洞，大幅降低了其安全级别。与现有先进PIR方案相比，CB-cPIR的优势减弱，在通信成本方面竞争力不足。", "conclusion": "我们的研究结果强调了继续研究基于编码的PIR方案的重要性，因为它们有潜力成为基于格方法的有价值的替代方案。", "translation": "私有信息检索（PIR）方案允许客户端从数据库中检索文件，而无需向服务器泄露所请求文件的身份。在追求后量子安全方面，大多数最新的PIR方案都依赖于困难的格问题。相比之下，所谓的CB-cPIR方案作为一项开创性努力脱颖而出，它将PIR方案建立在编码理论中的难题之上，从而显著促进了安全基础的多样化。然而，我们的研究揭示了CB-cPIR中的一个关键漏洞，大幅降低了其安全级别。此外，与现有先进PIR方案的比较分析表明，CB-cPIR的优势减弱，使其在通信成本方面竞争力下降。尽管如此，我们的研究结果强调了继续研究基于编码的PIR方案的重要性，因为它们有潜力提供一种有价值的替代基于格的方法。", "summary": "本文分析了一种新颖的基于编码理论的私有信息检索（PIR）方案CB-cPIR，该方案旨在为后量子安全提供多样化的基础。研究揭示了CB-cPIR的一个关键漏洞，严重削弱了其安全性。此外，与现有先进PIR方案的比较分析表明，CB-cPIR的通信成本优势减弱，使其竞争力下降。尽管如此，论文强调了继续研究基于编码的PIR方案的重要性，认为它们有潜力成为基于格方法的有价值替代方案。", "keywords": "私有信息检索, 基于编码的PIR, 安全性分析, 后量子密码学", "comments": "这篇论文的重要性在于它对后量子密码学中一个新方向（基于编码的PIR）进行了批判性评估，指出了关键的弱点，这对于该领域的未来研究和发展至关重要。它突出了在多样化密码学基础方面所面临的挑战。"}}
{"id": "2507.19139", "title": "String Consensus Problems with Swaps and Substitutions", "authors": ["Estéban Gabory", "Laurent Bulteau", "Gabriele Fici", "Hilde Verbeek"], "categories": ["cs.DS", "cs.CC"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      Full version of the work presented at SPIRE 2025", "url": "http://arxiv.org/abs/2507.19139v2", "summary": "String consensus problems aim at finding a string that minimizes some given\ndistance with respect to an input set of strings. In particular, in the Closest\nstring problem, we are given a set of strings of equal length and a radius $d$.\nThe objective is to find a new string that differs from each input string by at\nmost $d$ substitutions. We study a generalization of this problem where, in\naddition to substitutions, swaps of adjacent characters are also permitted,\neach operation incurring a unit cost. Amir et al. showed that this generalized\nproblem is NP-hard, even when only swaps are allowed. In this paper, we show\nthat it is FPT with respect to the parameter $d$. Moreover, we investigate a\nvariant in which the goal is to minimize the sum of distances from the output\nstring to all input strings. For this version, we present a polynomial-time\nalgorithm.", "comment": "Full version of the work presented at SPIRE 2025", "pdf_url": "http://arxiv.org/pdf/2507.19139v2", "cate": "cs.DS", "date": "2025-07-25", "updated": "2025-07-28", "AI": {"title_translation": "字符串共识问题与交换和替换", "tldr": "本文研究了带有相邻字符交换和替换的字符串共识问题，证明了其中一个变体是FPT，并为另一个变体提出了多项式时间算法。", "motivation": "字符串共识问题旨在找到一个与输入字符串集距离最小的字符串。本文研究了最近字符串问题的推广，其中除了替换外，还允许相邻字符交换，每种操作都有单位成本。", "method": "对于允许替换和交换的最近字符串问题的推广，本文证明了其是关于参数 $d$ 的FPT（固定参数可处理）。对于目标是最小化输出字符串与所有输入字符串之间距离之和的变体，本文提出了一种多项式时间算法。", "result": "证明了带有替换和交换的最近字符串问题是关于参数 $d$ 的FPT。对于最小化距离之和的变体，提出了一个多项式时间算法。", "conclusion": "论文成功地分析了带有交换和替换的字符串共识问题，并为不同的问题变体提供了有效的算法，包括一个FPT算法和一个多项式时间算法。", "translation": "字符串共识问题旨在找到一个与给定输入字符串集距离最小的字符串。特别是，在最近字符串问题中，我们得到一组等长字符串和一个半径 $d$。目标是找到一个新字符串，使其与每个输入字符串的差异最多为 $d$ 次替换。我们研究了该问题的一个推广，其中除了替换外，还允许相邻字符的交换，每种操作都会产生单位成本。Amir 等人表明，即使只允许交换，这个推广问题也是NP难的。在本文中，我们表明它是关于参数 $d$ 的FPT（固定参数可处理）。此外，我们研究了一个变体，其目标是最小化输出字符串与所有输入字符串之间的距离之和。对于此版本，我们提出了一个多项式时间算法。", "summary": "本文研究了字符串共识问题的一个推广，即在最近字符串问题中引入相邻字符交换操作，并考虑替换和交换的单位成本。针对这个已知NP难的问题，作者证明了其在以最大差异 $d$ 为参数时是FPT（固定参数可处理）的。此外，论文还提出了一个针对最小化字符串与所有输入字符串距离之和的变体的多项式时间算法。", "keywords": "字符串共识问题, 最近字符串问题, 字符交换, 固定参数可处理, 多项式时间算法", "comments": "本文在字符串共识问题领域进行了重要的扩展，将传统的替换操作推广到包含相邻字符交换。虽然该推广问题已知是NP难的，但作者成功地证明了其中一个关键变体是FPT，这表明在参数 $d$ 较小时，该问题可以有效解决。同时，为距离之和最小化问题提供了多项式时间算法，也展示了对不同优化目标的深入分析。这对于理解和解决更复杂的字符串匹配和模式识别问题具有重要意义。"}}
{"id": "2503.17026", "title": "Modelling the Climate Change Debate in Italy through Information Supply and Demand", "authors": ["Irene Scalco", "Giulia Colafrancesco", "Matteo Cinelli"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.17026v2", "summary": "Climate change is one of the most critical challenges of the twenty-first\ncentury. Public understanding of climate issues and of the goals regarding the\nclimate transition is essential to translate awareness into concrete actions.\nIn this context, social media platforms play a crucial role in disseminating\ninformation about climate change and climate policy. To better understand the\ndynamics of information circulation and the emergence of information voids we\npropose a model that takes into account the supply and demand of information\nrelated to the Italian climate-transition discourse. We conceptualise\ninformation supply as the production of content on Facebook, Instagram and\nGDELT (an online news database) while leveraging Google searches to capture\ninformation demand. Our findings highlight responsiveness and temporal coupling\nbetween supply and demand, particularly during moments of heightened public\nattention triggered by significant external events. These responsive\ninteractions reveal an overall adaptive information ecosystem. However, we also\nobserve persistent information voids which may limit public understanding and\ndelay meaningful engagement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.17026v2", "cate": "cs.SI", "date": "2025-03-21", "updated": "2025-07-25", "AI": {"title_translation": "通过信息供需模型分析意大利气候变化辩论", "tldr": "该研究通过建模意大利气候变化信息供需，发现信息生态系统具有适应性，但也存在可能阻碍公众理解和参与的信息空白。", "motivation": "公众对气候问题的理解以及气候转型目标的认知对于将意识转化为具体行动至关重要。社交媒体在传播气候变化信息方面发挥着关键作用。为了更好地理解信息流通的动态和信息空白的出现，本研究提出了一个模型。", "method": "研究提出了一个考虑意大利气候转型话语中信息供需的模型。信息供给被概念化为Facebook、Instagram和GDELT（在线新闻数据库）上的内容生产，而信息需求则通过Google搜索来捕捉。", "result": "研究结果表明，信息供需之间存在响应性和时间耦合，特别是在重大外部事件引发公众高度关注的时刻。这些响应性互动揭示了一个整体适应性的信息生态系统。然而，研究也观察到持续存在的信息空白。", "conclusion": "持续存在的信息空白可能会限制公众理解并延迟有意义的参与，尽管信息生态系统整体上是适应性的。", "translation": "气候变化是二十一世纪最严峻的挑战之一。公众对气候问题的理解以及对气候转型目标的认知对于将意识转化为具体行动至关重要。在此背景下，社交媒体平台在传播气候变化信息和气候政策方面发挥着关键作用。为了更好地理解信息流通的动态和信息空白的出现，我们提出了一个模型，该模型考虑了与意大利气候转型话语相关的信息供需。我们将信息供给概念化为Facebook、Instagram和GDELT（一个在线新闻数据库）上的内容生产，同时利用Google搜索来捕捉信息需求。我们的研究结果突出了供需之间的响应性和时间耦合，特别是在由重大外部事件引发的公众高度关注时刻。这些响应性互动揭示了一个整体适应性的信息生态系统。然而，我们也观察到持续存在的信息空白，这可能会限制公众理解并延迟有意义的参与。", "summary": "本研究通过建模意大利气候变化信息供需，探讨了公众理解和参与气候行动的重要性。研究将社交媒体内容视为信息供给，谷歌搜索视为信息需求，并发现两者之间存在响应性和时间耦合，尤其是在外部事件驱动下。尽管信息生态系统表现出适应性，但持续存在的信息空白可能阻碍公众理解和有效参与。", "keywords": "气候变化, 信息供需, 意大利, 社交媒体, 信息空白", "comments": "该研究创新性地将信息供需模型应用于气候变化辩论，并利用多源数据（社交媒体和搜索数据）进行分析。其重要性在于揭示了信息流通的动态及其对公众理解的影响，特别是在气候变化这一关键议题上。发现信息空白的存在是其局限性之一，也为未来的政策制定提供了方向。"}}
{"id": "2505.03005", "title": "RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale", "authors": ["Daniel Goldstein", "Eric Alcaide", "Janna Lu", "Eugene Cheah"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.03005v3", "summary": "We present Rapid Attention Distillation to Linear Attention Decoders at Scale\n(RADLADS), a protocol for rapidly converting softmax attention transformers\ninto linear attention decoder models, along with two new RWKV-variant\narchitectures, and models converted from popular Qwen2.5 open source models in\n7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens,\nless than 0.005% of the token count used to train the original teacher models.\nConverting to our 72B linear attention model costs less than \\$2,000 USD at\ntoday's prices, yet quality at inference remains close to the original\ntransformer. These models achieve state-of-the-art downstream performance\nacross a set of standard benchmarks for linear attention models of their size.\nWe release all our models on HuggingFace under the Apache 2.0 license, with the\nexception of our 72B models which are also governed by the Qwen License\nAgreement.\n  Models at\nhttps://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102\nTraining Code at https://github.com/recursal/RADLADS-paper", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.03005v3", "cate": "cs.CL", "date": "2025-05-05", "updated": "2025-07-25", "AI": {"title_translation": "RADLADS：大规模快速注意力蒸馏到线性注意力解码器", "tldr": "RADLADS提出了一种快速将softmax注意力Transformer转换为线性注意力解码器模型的方法，成本低廉且性能接近原始模型。", "motivation": "为了解决传统softmax注意力Transformer模型在推理时效率和成本问题，并提供一种快速、经济的方法将其转换为更高效的线性注意力模型。", "method": "本文提出了RADLADS协议，这是一种将softmax注意力Transformer快速转换为线性注意力解码器模型的方法。该过程仅需350-700M tokens进行转换，并引入了两种新的RWKV变体架构，以及从Qwen2.5开源模型（7B、32B、72B）转换而来的模型。", "result": "转换过程所需的tokens量不到原始教师模型训练所用tokens的0.005%。转换一个72B的线性注意力模型成本低于2,000美元。转换后的模型在推理时的质量接近原始Transformer，并且在同等规模的线性注意力模型标准基准测试中达到了最先进的下游性能。", "conclusion": "RADLADS提供了一种高效且经济的方法，可以将现有的softmax注意力Transformer模型转换为高性能的线性注意力解码器模型，显著降低了转换成本和计算需求，同时保持了接近原始模型的性能。", "translation": "我们提出了大规模快速注意力蒸馏到线性注意力解码器（RADLADS），这是一种将softmax注意力Transformer快速转换为线性注意力解码器模型的协议，同时还提出了两种新的RWKV变体架构，以及从流行的Qwen2.5开源模型（7B、32B和72B）转换而来的模型。我们的转换过程仅需350-700M tokens，不到原始教师模型训练所用token数量的0.005%。以当前价格计算，将模型转换为我们的72B线性注意力模型成本不到2,000美元，但在推理时的质量仍接近原始Transformer。这些模型在其规模的线性注意力模型的一系列标准基准测试中取得了最先进的下游性能。我们根据Apache 2.0许可协议在HuggingFace上发布了所有模型，但72B模型也受Qwen许可协议的约束。模型地址：https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 训练代码地址：https://github.com/recursal/RADLADS-paper", "summary": "RADLADS提出了一种高效协议，能将大型softmax注意力Transformer模型迅速转换为成本更低的线性注意力解码器模型。该方法仅需极少量tokens进行蒸馏，显著降低了转换成本，同时保持了接近原始模型的推理性能，并在相关基准测试中表现出色。作者还发布了转换后的模型和训练代码。", "keywords": "注意力蒸馏, 线性注意力, Transformer, 模型转换, 大规模模型", "comments": "RADLADS的创新之处在于其极低的转换成本和高效率，使得将现有的大型Transformer模型转换为更适合推理的线性注意力模型变得经济可行。这对于资源有限的研究者和开发者来说具有重要意义，有助于推动线性注意力模型在实际应用中的普及。其将转换成本降低到不到2000美元，且所需tokens量极少，是该方法的核心亮点。"}}
{"id": "2507.19391", "title": "Transcript Franking for Encrypted Messaging", "authors": ["Armin Namavari", "Thomas Ristenpart"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19391v1", "summary": "Message franking is an indispensable abuse mitigation tool for end-to-end\nencrypted (E2EE) messaging platforms. With it, users who receive harmful\ncontent can securely report that content to platform moderators. However, while\nreal-world deployments of reporting require the disclosure of multiple\nmessages, existing treatments of message franking only consider the report of a\nsingle message. As a result, there is a gap between the security goals achieved\nby constructions and those needed in practice. Our work introduces transcript\nfranking, a new type of protocol that allows reporting subsets of conversations\nsuch that moderators can cryptographically verify message causality and\ncontents. We define syntax, semantics, and security for transcript franking in\ntwo-party and group messaging. We then present efficient constructions for\ntranscript franking and prove their security. Looking toward deployment\nconsiderations, we provide detailed discussion of how real-world messaging\nsystems can incorporate our protocols.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19391v1", "cate": "cs.CR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "加密消息的抄本加盖戳记", "tldr": "本文引入了“抄本加盖戳记”协议，用于加密消息平台中举报部分对话内容，以允许版主加密验证消息的因果关系和内容，从而弥补了现有消息加盖戳记仅支持单条消息举报的不足。", "motivation": "现有的消息加盖戳记（message franking）仅考虑举报单条消息，而现实世界中的举报部署需要披露多条消息，导致安全目标与实际需求之间存在差距。因此，需要一种新的协议来支持对对话子集的举报。", "method": "本文引入了“抄本加盖戳记”（transcript franking）这一新型协议，允许举报对话的子集，使版主能够加密验证消息的因果关系和内容。作者定义了双人及群组消息中抄本加盖戳记的语法、语义和安全性，并提出了高效的构建方法并证明了其安全性。此外，还详细讨论了如何在实际消息系统中整合这些协议。", "result": "本文提出了高效的抄本加盖戳记构建方法，并证明了其安全性。这些构建方法能够允许版主加密验证被举报对话子集的消息因果关系和内容。", "conclusion": "抄本加盖戳记协议有效地解决了现有消息加盖戳记在处理多条消息举报时的不足，为端到端加密消息平台提供了更实用的滥用缓解工具，增强了内容举报的安全性与实用性。", "translation": "消息加盖戳记是端到端加密（E2EE）消息平台不可或缺的滥用缓解工具。通过它，收到有害内容的用户可以安全地向平台版主举报该内容。然而，尽管现实世界中的举报部署需要披露多条消息，但现有消息加盖戳记的处理方式只考虑举报单条消息。因此，构建所达到的安全目标与实践所需的安全目标之间存在差距。我们的工作引入了抄本加盖戳记，这是一种新型协议，允许举报对话的子集，以便版主可以加密验证消息的因果关系和内容。我们为双人及群组消息中的抄本加盖戳记定义了语法、语义和安全性。然后，我们提出了高效的抄本加盖戳记构建方法并证明了其安全性。着眼于部署考虑，我们详细讨论了实际消息系统如何整合我们的协议。", "summary": "本文提出了一种名为“抄本加盖戳记”的新型协议，旨在解决现有消息加盖戳记在端到端加密（E2EE）消息平台中仅支持单条消息举报的局限性。该协议允许用户举报对话的子集，使平台版主能够加密验证消息的因果关系和内容，从而弥合了安全目标与实际需求之间的差距。文章定义了该协议在双人及群组消息中的语法、语义和安全性，并提供了高效且经过安全证明的构建方法，同时讨论了其在实际系统中的部署。", "keywords": "抄本加盖戳记, 加密消息, 滥用缓解, 消息举报, E2EE", "comments": "本文创新性地提出了抄本加盖戳记协议，解决了现有消息加盖戳记在实际应用中无法有效举报多条消息的痛点。这一工作对于增强端到端加密消息平台的滥用缓解能力具有重要意义，尤其是在保护用户隐私的同时实现内容审查方面。其贡献在于将单条消息的举报扩展到对话子集，并提供了严格的安全性定义和高效的实现方案，对未来的E2EE消息系统设计具有指导价值。"}}
{"id": "2507.17543", "title": "Anticipate, Simulate, Reason (ASR): A Comprehensive Generative AI Framework for Combating Messaging Scams", "authors": ["Xue Wen Tan", "Kenneth See", "Stanley Kok"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2412.13528", "url": "http://arxiv.org/abs/2507.17543v2", "summary": "The rapid growth of messaging scams creates an escalating challenge for user\nsecurity and financial safety. In this paper, we present the\n\\textit{Anticipate, Simulate, Reason} (ASR) generative AI framework to enable\nusers to proactively identify and comprehend scams within instant messaging\nplatforms. Using large language models, ASR predicts scammer responses and\ndelivers real-time, interpretable support to end-users. We also develop\nScamGPT-J, a domain-specific language model fine-tuned on a new, high-quality\ndataset of scam conversations covering multiple scam types. Thorough\nexperimental evaluation shows that the ASR framework substantially enhances\nscam detection, particularly in challenging contexts such as job scams, and\nuncovers important demographic patterns in user vulnerability and perceptions\nof AI-generated assistance. Our findings reveal a contradiction where those\nmost at risk are often least receptive to AI support, emphasizing the\nimportance of user-centered design in AI-driven fraud prevention. This work\nadvances both the practical and theoretical foundations for interpretable and\nhuman-centered AI systems in combating evolving digital threats.", "comment": "arXiv admin note: text overlap with arXiv:2412.13528", "pdf_url": "http://arxiv.org/pdf/2507.17543v2", "cate": "cs.HC", "date": "2025-07-23", "updated": "2025-07-25", "AI": {"title_translation": "预判、模拟、推理 (ASR)：一个用于打击消息诈骗的综合生成式AI框架", "tldr": "消息诈骗日益增多。ASR是一个利用大型语言模型和ScamGPT-J的生成式AI框架，旨在帮助用户识别和理解诈骗，它显著提升了诈骗检测能力，但也揭示了用户对AI支持接受度的问题。", "motivation": "消息诈骗的快速增长对用户安全和财产安全构成了日益严峻的挑战，因此需要一个能够帮助用户主动识别和理解诈骗的框架。", "method": "本文提出了“预判、模拟、推理”（ASR）生成式AI框架，该框架利用大型语言模型来预测诈骗者的回应，并向终端用户提供实时、可解释的支持。此外，还开发了ScamGPT-J，这是一个在新的高质量诈骗对话数据集上进行微调的领域特定语言模型。", "result": "实验评估表明，ASR框架显著增强了诈骗检测能力，尤其是在求职诈骗等挑战性场景中。研究还揭示了用户脆弱性以及对AI生成辅助感知的关键人口统计学模式，发现那些风险最高的人往往最不接受AI支持。", "conclusion": "研究结果揭示了一个矛盾：高风险用户往往最不接受AI支持，这强调了在AI驱动的防诈骗中以用户为中心设计的重要性。这项工作推进了可解释和以人为中心的AI系统在打击不断演变的数字威胁方面的实践和理论基础。", "translation": "消息诈骗的快速增长对用户安全和财产安全构成了日益严峻的挑战。在本文中，我们提出了“预判、模拟、推理”（ASR）生成式AI框架，旨在帮助用户在即时通讯平台中主动识别和理解诈骗。ASR利用大型语言模型预测诈骗者的回应，并向终端用户提供实时、可解释的支持。我们还开发了ScamGPT-J，这是一个在新的高质量诈骗对话数据集（涵盖多种诈骗类型）上进行微调的领域特定语言模型。全面的实验评估表明，ASR框架显著增强了诈骗检测能力，尤其是在求职诈骗等挑战性场景中，并揭示了用户脆弱性以及对AI生成辅助感知的关键人口统计学模式。我们的研究结果揭示了一个矛盾：那些风险最高的人往往最不接受AI支持，这强调了在AI驱动的防诈骗中以用户为中心设计的重要性。这项工作推进了可解释和以人为中心的AI系统在打击不断演变的数字威胁方面的实践和理论基础。", "summary": "本文介绍了ASR（预判、模拟、推理）生成式AI框架，结合了专门微调的ScamGPT-J模型，旨在通过预测诈骗者回应并提供实时、可解释的支持来打击消息诈骗。实验证明ASR显著提高了诈骗检测能力，但同时也揭示了高风险用户对AI支持接受度较低的挑战，强调了在防诈骗AI系统中以用户为中心设计的重要性。", "keywords": "消息诈骗, 生成式AI, ASR框架, ScamGPT-J, 防诈骗", "comments": "该研究的创新之处在于提出了一个名为ASR的综合生成式AI框架，并开发了领域特定的ScamGPT-J模型，以主动、可解释的方式应对日益增长的消息诈骗威胁。其重要性在于为实时诈骗检测提供了新的工具和方法，并深入探讨了用户对AI辅助的接受度问题。研究发现高风险用户往往最不接受AI支持，这一矛盾是该系统在实际部署中面临的关键挑战，强调了未来AI防诈骗系统在用户体验和信任建设方面的必要性。"}}
{"id": "2507.18857", "title": "PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning", "authors": ["Mohammad Kachuee", "Teja Gollapudi", "Minseok Kim", "Yin Huang", "Kai Sun", "Xiao Yang", "Jiaqi Wang", "Nirav Shah", "Yue Liu", "Aaron Colak", "Anuj Kumar", "Wen-tau Yih", "Xin Luna Dong"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18857v1", "summary": "Retrieval-augmented generation (RAG) often falls short when retrieved context\nincludes confusing semi-relevant passages, or when answering questions require\ndeep contextual understanding and reasoning. We propose an efficient\nfine-tuning framework, called PrismRAG, that (i) trains the model with\ndistractor-aware QA pairs mixing gold evidence with subtle distractor passages,\nand (ii) instills reasoning-centric habits that make the LLM plan, rationalize,\nand synthesize without relying on extensive human engineered instructions.\nEvaluated across 12 open-book RAG QA benchmarks spanning diverse application\ndomains and scenarios, PrismRAG improves average factuality by 5.4%,\noutperforming state-of-the-art solutions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18857v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "PrismRAG: 通过干扰项鲁棒性和策略性推理提升RAG的真实性", "tldr": "PrismRAG是一个高效的微调框架，通过抗干扰训练和策略性推理，显著提升了检索增强生成（RAG）模型的真实性，在多个基准测试中表现优异。", "motivation": "检索增强生成（RAG）在检索到的上下文包含混淆性半相关段落时，或在回答问题需要深入理解和推理时，往往表现不佳，真实性不足。", "method": "PrismRAG是一个高效的微调框架，通过以下方式实现：(i) 使用混合了黄金证据和微妙干扰段落的干扰项感知QA对来训练模型；(ii) 灌输以推理为中心的习惯，使大型语言模型（LLM）无需依赖大量人工设计的指令即可进行规划、合理化和合成。", "result": "PrismRAG在涵盖不同应用领域和场景的12个开放式RAG QA基准测试中进行了评估，平均真实性提高了5.4%，优于现有最先进的解决方案。", "conclusion": "PrismRAG通过其创新的微调框架，有效解决了RAG在处理干扰信息和需要深度推理时的真实性问题，显著超越了现有技术水平。", "translation": "检索增强生成（RAG）在检索到的上下文包含混淆性半相关段落时，或在回答问题需要深入理解和推理时，往往表现不佳。我们提出了一个高效的微调框架，名为PrismRAG，它(i) 使用混合了黄金证据和微妙干扰段落的干扰项感知QA对来训练模型，以及(ii) 灌输以推理为中心的习惯，使大型语言模型（LLM）无需依赖大量人工设计的指令即可进行规划、合理化和合成。在涵盖不同应用领域和场景的12个开放式RAG QA基准测试中进行了评估，PrismRAG平均真实性提高了5.4%，优于现有最先进的解决方案。", "summary": "PrismRAG是一个针对检索增强生成（RAG）模型真实性问题的创新微调框架。它通过设计独特的干扰项感知QA训练数据，并培养LLM的策略性推理能力，使其在面对复杂或包含干扰信息的上下文时，能够更准确地生成答案。实验结果表明，PrismRAG在多个RAG QA基准测试中，将平均真实性提高了5.4%，超越了当前最先进的技术。", "keywords": "RAG, 真实性, 微调, 干扰项鲁棒性, 策略性推理", "comments": "PrismRAG的创新点在于其双管齐下的方法：一是通过“干扰项感知QA对”直接训练模型识别并忽略误导信息，这对于提升RAG的鲁棒性至关重要；二是通过“推理中心习惯”培养LLM的自主推理能力，减少对复杂指令的依赖。这种方法有效解决了RAG在复杂语境下容易产生幻觉的问题，对于提高生成内容的可靠性具有重要意义。"}}
{"id": "2507.19225", "title": "Face2VoiceSync: Lightweight Face-Voice Consistency for Text-Driven Talking Face Generation", "authors": ["Fang Kang", "Yin Cao", "Haoyu Chen"], "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19225v1", "summary": "Recent studies in speech-driven talking face generation achieve promising\nresults, but their reliance on fixed-driven speech limits further applications\n(e.g., face-voice mismatch). Thus, we extend the task to a more challenging\nsetting: given a face image and text to speak, generating both talking face\nanimation and its corresponding speeches. Accordingly, we propose a novel\nframework, Face2VoiceSync, with several novel contributions: 1) Voice-Face\nAlignment, ensuring generated voices match facial appearance; 2) Diversity \\&\nManipulation, enabling generated voice control over paralinguistic features\nspace; 3) Efficient Training, using a lightweight VAE to bridge visual and\naudio large-pretrained models, with significantly fewer trainable parameters\nthan existing methods; 4) New Evaluation Metric, fairly assessing the diversity\nand identity consistency. Experiments show Face2VoiceSync achieves both visual\nand audio state-of-the-art performances on a single 40GB GPU.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19225v1", "cate": "cs.SD", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "Face2VoiceSync：轻量级人脸-语音一致性用于文本驱动的说话人脸生成", "tldr": "提出Face2VoiceSync框架，通过给定人脸图像和文本，生成说话人脸动画及对应语音，解决了现有方法中语音驱动的局限性，并实现了轻量级高效训练和SOTA性能。", "motivation": "当前语音驱动的说话人脸生成方法依赖于固定的驱动语音，限制了其应用，例如导致人脸与语音不匹配的问题。因此，需要扩展到更具挑战性的设置：根据人脸图像和文本生成说话人脸动画及其对应语音。", "method": "提出了一种名为Face2VoiceSync的新颖框架，包含多项贡献：1) 语音-人脸对齐，确保生成的语音与面部外观匹配；2) 多样性与操控，允许控制生成语音的副语言特征空间；3) 高效训练，使用轻量级VAE连接视觉和音频大型预训练模型，训练参数显著少于现有方法；4) 新评估指标，公平评估生成的多样性和身份一致性。", "result": "实验表明，Face2VoiceSync在单块40GB GPU上实现了视觉和音频方面的最先进性能。", "conclusion": "Face2VoiceSync成功地解决了文本驱动的说话人脸生成任务中的多项挑战，通过创新的框架设计和高效的训练方法，在视觉和音频质量上均达到了SOTA水平，并能够有效处理人脸-语音一致性及多样性控制。", "translation": "最近在语音驱动的说话人脸生成方面的研究取得了可喜的成果，但它们对固定驱动语音的依赖限制了进一步的应用（例如，人脸-语音不匹配）。因此，我们将任务扩展到一个更具挑战性的设置：给定一张人脸图像和要说的文本，生成说话人脸动画及其对应的语音。相应地，我们提出了一个新颖的框架Face2VoiceSync，具有以下几项新颖贡献：1）语音-人脸对齐，确保生成的语音与面部外观匹配；2）多样性与操控，使生成的语音能够控制副语言特征空间；3）高效训练，使用轻量级VAE连接视觉和音频大型预训练模型，其可训练参数显著少于现有方法；4）新的评估指标，公平评估多样性和身份一致性。实验表明Face2VoiceSync在单块40GB GPU上实现了视觉和音频方面的最先进性能。", "summary": "本文提出了Face2VoiceSync框架，旨在解决文本驱动的说话人脸生成任务中现有语音驱动方法的局限性。该框架通过给定人脸图像和文本，同时生成说话人脸动画和对应的语音。Face2VoiceSync引入了语音-人脸对齐、语音多样性与操控、基于轻量级VAE的高效训练以及新的评估指标等多项创新。实验证明，该方法在视觉和音频生成质量上均达到了最先进水平，并且训练效率高。", "keywords": "说话人脸生成, 文本驱动, 语音合成, 人脸-语音一致性, 轻量级模型", "comments": "本文的创新之处在于将文本驱动的说话人脸生成任务分解为同时生成视觉和音频内容，并着重解决了人脸-语音一致性问题。通过引入轻量级VAE连接视觉和音频大模型，显著降低了训练成本，提高了实用性。此外，提出新的评估指标也体现了对该领域评估标准的贡献。其在单GPU上达到SOTA的性能，显示了其高效性和有效性。"}}
{"id": "2507.19274", "title": "Sparse Recovery from Group Orbits", "authors": ["Timm Gilles", "Hartmut Führ"], "categories": ["cs.IT", "math.IT", "15B52, 20C35, 94A12, 60G50, 60G70"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19274v1", "summary": "While most existing sparse recovery results allow only minimal structure\nwithin the measurement scheme, many practical problems possess significant\nstructure. To address this gap, we present a framework for structured\nmeasurements that are generated by random orbits of a group representation\nassociated with a finite group. We differentiate between two scenarios: one in\nwhich the sampling set is fixed and another in which the sampling set is\nrandomized. For each case, we derive an estimate for the number of measurements\nrequired to ensure that the restricted isometry property holds with high\nprobability. These estimates are contingent upon the specific representation\nemployed. For this reason, we analyze and characterize various representations\nthat yield favorable recovery outcomes, including the left regular\nrepresentation. Our work not only establishes a comprehensive framework for\nsparse recovery of group-structured measurements but also generalizes\nestablished measurement schemes, such as those derived from partial random\ncirculant matrices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19274v1", "cate": "cs.IT", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "群轨道上的稀疏恢复", "tldr": "本文提出了一个利用群表示的随机轨道生成结构化测量的新框架，用于稀疏恢复，并估计了满足受限等距性质所需的测量数量，同时分析了有利的表示形式。", "motivation": "现有的稀疏恢复方法在测量方案中只允许最小的结构，而许多实际问题具有显著的结构，这在现有方法中是一个空白。", "method": "提出一个用于结构化测量的新框架，这些测量由有限群相关的群表示的随机轨道生成。研究了两种场景：采样集固定和采样集随机化。对于每种情况，推导了确保受限等距性质以高概率成立所需的测量数量的估计。此外，分析并表征了能产生有利恢复结果的各种表示，包括左正则表示。", "result": "导出了在固定和随机采样集两种场景下，确保受限等距性质以高概率成立所需的测量数量的估计。这些估计取决于所采用的具体群表示。分析并识别了能产生有利恢复结果的各种表示形式。", "conclusion": "本工作不仅为群结构化测量的稀疏恢复建立了一个全面的框架，而且推广了已有的测量方案，例如那些源自部分随机循环矩阵的方案。", "translation": "标题：群轨道上的稀疏恢复\n摘要：\n尽管大多数现有的稀疏恢复结果在测量方案中只允许最小的结构，但许多实际问题却具有显著的结构。为了弥补这一空白，我们提出了一个用于结构化测量的新框架，这些测量由与有限群相关的群表示的随机轨道生成。我们区分了两种情况：一种是采样集固定，另一种是采样集随机化。对于每种情况，我们都推导了确保受限等距性质以高概率成立所需的测量数量的估计。这些估计取决于所采用的具体表示。因此，我们分析并表征了能产生有利恢复结果的各种表示，包括左正则表示。我们的工作不仅为群结构化测量的稀疏恢复建立了一个全面的框架，而且推广了已有的测量方案，例如那些源自部分随机循环矩阵的方案。", "summary": "本文针对现有稀疏恢复方法未能充分利用实际问题中存在的显著结构这一问题，提出了一个基于有限群表示的随机轨道生成结构化测量的新框架。该框架考虑了固定和随机两种采样集场景，并为每种情况估算了满足受限等距性质所需的测量数量。研究还深入分析了不同群表示对恢复效果的影响，并证明了该框架能够推广现有的一些测量方案。", "keywords": "稀疏恢复, 群表示, 结构化测量, 受限等距性质, 随机轨道", "comments": "这篇论文通过引入群表示的随机轨道来处理结构化测量，弥补了现有稀疏恢复理论在处理实际问题中复杂结构方面的不足。其创新性在于为稀疏恢复提供了一种新的、更普适的测量方案构建方法，并推广了现有技术。这项工作对于稀疏信号处理在具有内在群对称性的数据上的应用具有重要意义。"}}
{"id": "2507.09089", "title": "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity", "authors": ["Joel Becker", "Nate Rush", "Elizabeth Barnes", "David Rein"], "categories": ["cs.AI", "cs.HC", "cs.SE", "I.2"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      51 pages, 8 tables, 22 figures", "url": "http://arxiv.org/abs/2507.09089v2", "summary": "Despite widespread adoption, the impact of AI tools on software development\nin the wild remains understudied. We conduct a randomized controlled trial\n(RCT) to understand how AI tools at the February-June 2025 frontier affect the\nproductivity of experienced open-source developers. 16 developers with moderate\nAI experience complete 246 tasks in mature projects on which they have an\naverage of 5 years of prior experience. Each task is randomly assigned to allow\nor disallow usage of early 2025 AI tools. When AI tools are allowed, developers\nprimarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.\nBefore starting tasks, developers forecast that allowing AI will reduce\ncompletion time by 24%. After completing the study, developers estimate that\nallowing AI reduced completion time by 20%. Surprisingly, we find that allowing\nAI actually increases completion time by 19%--AI tooling slowed developers\ndown. This slowdown also contradicts predictions from experts in economics (39%\nshorter) and ML (38% shorter). To understand this result, we collect and\nevaluate evidence for 20 properties of our setting that a priori could\ncontribute to the observed slowdown effect--for example, the size and quality\nstandards of projects, or prior developer experience with AI tooling. Although\nthe influence of experimental artifacts cannot be entirely ruled out, the\nrobustness of the slowdown effect across our analyses suggests it is unlikely\nto primarily be a function of our experimental design.", "comment": "51 pages, 8 tables, 22 figures", "pdf_url": "http://arxiv.org/pdf/2507.09089v2", "cate": "cs.AI", "date": "2025-07-12", "updated": "2025-07-25", "AI": {"title_translation": "衡量2025年初AI对经验丰富的开源开发者生产力的影响", "tldr": "研究发现，2025年初的AI工具（如Cursor Pro和Claude 3.5/3.7 Sonnet）实际上使经验丰富的开源开发者完成任务的时间增加了19%，与开发者和专家预测的效率提升相悖。", "motivation": "尽管AI工具被广泛采用，但其对实际软件开发生产力的影响仍未得到充分研究。", "method": "采用随机对照试验（RCT），16名具有中等AI经验的开发者在他们有平均5年经验的成熟项目中完成了246项任务。任务随机分配为允许或不允许使用2025年初的AI工具（主要使用Cursor Pro和Claude 3.5/3.7 Sonnet）。研究还收集并评估了20个可能导致减速效应的设置属性。", "result": "开发者预期AI能减少24%的完成时间，研究结束后估计减少20%。然而，实际结果显示，允许使用AI工具反而使完成时间增加了19%，即AI工具减慢了开发者的速度。这一结果也与经济学和机器学习专家的预测（分别缩短39%和38%）相矛盾。减速效应在分析中表现出鲁棒性。", "conclusion": "2025年初的AI工具对经验丰富的开源开发者而言，未能提高生产力，反而导致了任务完成时间的增加。尽管实验性因素无法完全排除，但减速效应的鲁棒性表明其不太可能是实验设计的主要产物。", "translation": "尽管AI工具被广泛采用，但其对实际软件开发生产力的影响仍未得到充分研究。我们进行了一项随机对照试验（RCT），以了解2025年2月至6月的尖端AI工具如何影响经验丰富的开源开发者的生产力。16名具有中等AI经验的开发者在他们平均拥有5年经验的成熟项目中完成了246项任务。每项任务都被随机分配为允许或不允许使用2025年初的AI工具。当允许使用AI工具时，开发者主要使用流行的代码编辑器Cursor Pro和Claude 3.5/3.7 Sonnet。在开始任务前，开发者预测允许使用AI将减少24%的完成时间。在完成研究后，开发者估计允许使用AI将完成时间减少了20%。令人惊讶的是，我们发现允许使用AI实际上使完成时间增加了19%——AI工具减慢了开发者的速度。这种减速也与经济学（缩短39%）和机器学习（缩短38%）专家的预测相矛盾。为了理解这一结果，我们收集并评估了我们环境中20个先验可能导致观察到的减速效应的属性的证据——例如，项目的规模和质量标准，或开发者之前使用AI工具的经验。尽管实验性人工制品的影响不能完全排除，但减速效应在我们分析中的鲁棒性表明它不太可能是我们实验设计的主要产物。", "summary": "本研究通过一项随机对照试验，调查了2025年初AI工具对经验丰富的开源开发者生产力的影响。与开发者和专家预测的效率提升相反，研究发现允许使用AI工具实际上导致任务完成时间增加了19%。该研究分析了多种潜在因素以解释这种意外的减速效应，并指出其结果具有鲁棒性，表明AI工具在当前阶段可能并未如预期般提升资深开发者的生产力。", "keywords": "AI工具, 开发者生产力, 开源软件, 随机对照试验, 减速效应", "comments": "这篇论文的创新之处在于通过严格的随机对照试验，量化了AI工具对经验丰富的开发者生产力的实际影响，并得出了与普遍认知和专家预测相悖的“减速”结论。其重要性在于，它对AI在软件开发中“必然提升效率”的假设提出了挑战，提示业界和研究者需更深入地理解AI工具在不同情境下的真实作用，而不仅仅是关注其潜力。研究还对导致减速的潜在因素进行了初步探索，为后续研究提供了方向。"}}
{"id": "2507.19367", "title": "Empowering IoT Firmware Secure Update with Customization Rights", "authors": ["Weihao Chen", "Yansong Gao", "Boyu Kuang", "Jin B. Hong", "Yuqing Zhang", "Anmin Fu"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19367v1", "summary": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19367v1", "cate": "cs.CR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "赋能物联网固件安全更新与定制化权限", "tldr": "本文揭示了物联网固件更新中定制化带来的安全漏洞，并提出了IMUP框架，通过模块化更新、完整性验证和性能优化，显著提升了更新的安全性与效率。", "motivation": "物联网设备的固件更新是防御攻击的主要手段，但更新通道本身已成为攻击向量。现有防御主要关注整体固件镜像，而模块级定制化（日益增长的用户需求）却未受保护且探索不足。研究发现，定制化实践导致了大量未记录的漏洞，且超过半数的更新相关CVE源于定制化引起的问题。这凸显了定制化增加攻击面，而当前防御未能跟上的关键问题。", "method": "首先，对23家供应商的200个基于Linux的物联网设备的更新工作流进行了试点研究，发现了5个此前未记录的定制化引起的漏洞。其次，对2020年至2024年更新相关的CVEs进行了更广泛的分析，确认了定制化是主要问题来源。最后，提出了IMUP（Integrity-Centric Modular Update Platform）框架，这是首个解决构建可信跨模块完整性链和在大规模定制下扩展更新性能的框架。IMUP结合了三种技术：每模块变色龙哈希（用于完整性）、服务器端工作量证明卸载（减少设备开销）和服务器端缓存（重用模块组合，最小化重建成本）。", "result": "研究发现了5个此前未记录的定制化引起的漏洞。对CVEs的分析表明，超过一半的更新相关CVEs源于定制化引起的问题。安全性分析显示，即使95%的密钥暴露，伪造有效镜像的成本也比合法服务器高300倍以上。在异构物联网设备上的实验表明，与包管理器基线相比，IMUP将服务器端生成时间减少了2.9倍，设备停机时间减少了5.9倍。", "conclusion": "定制化在物联网固件更新中引入了显著的安全风险，当前防御不足以应对。IMUP框架通过其完整性验证和性能优化技术，有效地解决了物联网固件安全更新中定制化带来的挑战，显著提升了更新的安全性、效率和可扩展性。", "translation": "固件更新仍然是物联网设备的主要防御手段；然而，更新通道本身已成为一个成熟的攻击向量。现有防御主要侧重于保护整体固件镜像，而模块级定制化——日益增长的用户需求——在很大程度上未受保护且探索不足。为了解决这一空白，我们对23家供应商的200个基于Linux的物联网设备的更新工作流进行了试点研究，发现了5个此前未记录的由定制化实践引起的漏洞。对2020年至2024年更新相关的CVEs的更广泛分析显示，超过一半的漏洞源于定制化引起的问题。这些发现凸显了一个关键但未充分研究的现实：随着定制化的增加，攻击面也随之增加，而当前的防御未能跟上。我们提出了IMUP（以完整性为中心的模块化更新平台），这是第一个解决两个关键挑战的框架：构建可信的跨模块完整性链和在大规模定制下扩展更新性能。IMUP结合了三种技术：每模块变色龙哈希用于完整性、服务器端工作量证明卸载以减少设备开销，以及服务器端缓存以重用模块组合，从而最大限度地降低重建成本。安全分析表明，即使95%的密钥暴露，伪造有效镜像的成本也比合法服务器高300倍以上。在异构物联网设备上的实验表明，与包管理器基线相比，IMUP将服务器端生成时间减少了2.9倍，设备停机时间减少了5.9倍。", "summary": "本文探讨了物联网设备固件更新中定制化带来的安全挑战，指出现有防御机制不足以应对模块级定制化所引入的攻击面。通过对200个物联网设备的更新流程进行研究，发现了多项由定制化引起的未记录漏洞，并发现大部分更新相关CVEs源于此问题。为解决此问题，论文提出了IMUP框架，该框架通过引入每模块变色龙哈希、服务器端工作量证明卸载和服务器端缓存等技术，旨在构建可信的跨模块完整性链并提升大规模定制下的更新性能。实验证明，IMUP显著提高了安全性，并降低了服务器生成时间和设备停机时间。", "keywords": "物联网, 固件更新, 安全, 定制化, IMUP", "comments": "本文创新性地关注了物联网固件更新中日益增长的模块级定制化所带来的安全挑战，这是一个被现有研究和防御机制忽视的关键领域。通过实证研究揭示了定制化实践引发的未记录漏洞，并提出了IMUP这一针对性的解决方案，其结合了多种技术以确保完整性、优化性能，并在安全性上表现出色。IMUP的提出对于提升物联网设备固件更新的整体安全性和效率具有重要意义，尤其是在大规模定制化的背景下。"}}
{"id": "2507.19050", "title": "Large Language Model-Based Task Offloading and Resource Allocation for Digital Twin Edge Computing Networks", "authors": ["Qiong Wu", "Yu Xie", "Pingyi Fan", "Dong Qin", "Kezhi Wang", "Nan Cheng", "Khaled B. Letaief"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      This paper has been submitted to IEEE TMC", "url": "http://arxiv.org/abs/2507.19050v1", "summary": "In this paper, we propose a general digital twin edge computing network\ncomprising multiple vehicles and a server. Each vehicle generates multiple\ncomputing tasks within a time slot, leading to queuing challenges when\noffloading tasks to the server. The study investigates task offloading\nstrategies, queue stability, and resource allocation. Lyapunov optimization is\nemployed to transform long-term constraints into tractable short-term\ndecisions. To solve the resulting problem, an in-context learning approach\nbased on large language model (LLM) is adopted, replacing the conventional\nmulti-agent reinforcement learning (MARL) framework. Experimental results\ndemonstrate that the LLM-based method achieves comparable or even superior\nperformance to MARL.", "comment": "This paper has been submitted to IEEE TMC", "pdf_url": "http://arxiv.org/pdf/2507.19050v1", "cate": "cs.NI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于大语言模型的数字孪生边缘计算网络任务卸载与资源分配", "tldr": "该研究提出了一种基于大语言模型（LLM）的任务卸载和资源分配方法，用于数字孪生边缘计算网络，旨在解决车辆任务卸载到服务器时的排队问题，并证明其性能与多智能体强化学习（MARL）相当或更优。", "motivation": "在数字孪生边缘计算网络中，车辆在每个时隙生成多个计算任务，导致任务卸载到服务器时面临排队挑战。本研究旨在解决任务卸载策略、队列稳定性和资源分配问题。", "method": "本研究提出了一个包含多辆车和一台服务器的通用数字孪生边缘计算网络。采用Lyapunov优化将长期约束转化为可处理的短期决策。为解决由此产生的问题，引入了基于大语言模型（LLM）的上下文学习方法，以取代传统的多智能体强化学习（MARL）框架。", "result": "实验结果表明，基于大语言模型的方法实现了与多智能体强化学习相当甚至更优的性能。", "conclusion": "本研究提出了一种基于大语言模型的任务卸载与资源分配方法，在数字孪生边缘计算网络中取得了与传统多智能体强化学习方法相当或更优的效果，有效解决了任务排队和资源分配问题。", "translation": "本文提出了一种通用的数字孪生边缘计算网络，由多辆车和一个服务器组成。每辆车在一个时隙内生成多个计算任务，导致将任务卸载到服务器时面临排队挑战。本研究探讨了任务卸载策略、队列稳定性以及资源分配。采用Lyapunov优化将长期约束转化为可处理的短期决策。为了解决由此产生的问题，采用了一种基于大语言模型（LLM）的上下文学习方法，取代了传统的多智能体强化学习（MARL）框架。实验结果表明，基于LLM的方法实现了与MARL相当甚至更优的性能。", "summary": "本论文提出了一种针对数字孪生边缘计算网络的任务卸载与资源分配方案。该方案构建了一个多车辆与服务器组成的网络模型，旨在解决任务排队问题。研究利用Lyapunov优化将长期问题转化为短期决策，并创新性地采用大语言模型（LLM）的上下文学习方法取代传统的多智能体强化学习（MARL）框架来求解。实验证明，LLM方法在性能上与MARL相当或更优。", "keywords": "数字孪生, 边缘计算, 任务卸载, 资源分配, 大语言模型", "comments": "本文的创新点在于将大语言模型（LLM）应用于任务卸载和资源分配问题，并将其与传统的强化学习方法进行比较。这表明LLM在解决复杂的网络优化问题方面具有潜力，为未来研究提供了新的方向。取代MARL框架是其重要贡献。"}}
{"id": "2405.06270", "title": "XAI4LLM. Let Machine Learning Models and LLMs Collaborate for Enhanced In-Context Learning in Healthcare", "authors": ["Fatemeh Nazary", "Yashar Deldjoo", "Tommaso Di Noia", "Eugenio di Sciascio"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.06270v4", "summary": "Clinical decision support systems require models that are not only highly\naccurate but also equitable and sensitive to the implications of missed\ndiagnoses. In this study, we introduce a knowledge-guided in-context learning\n(ICL) framework designed to enable large language models (LLMs) to effectively\nprocess structured clinical data. Our approach integrates domain-specific\nfeature groupings, carefully balanced few-shot examples, and task-specific\nprompting strategies. We systematically evaluate this method across seventy\ndistinct ICL designs by various prompt variations and two different\ncommunication styles-natural-language narrative and numeric conversational-and\ncompare its performance to robust classical machine learning (ML) benchmarks on\ntasks involving heart disease and diabetes prediction.\n  Our findings indicate that while traditional ML models maintain superior\nperformance in balanced precision-recall scenarios, LLMs employing narrative\nprompts with integrated domain knowledge achieve higher recall and\nsignificantly reduce gender bias, effectively narrowing fairness disparities by\nan order of magnitude. Despite the current limitation of increased inference\nlatency, LLMs provide notable advantages, including the capacity for zero-shot\ndeployment and enhanced equity. This research offers the first comprehensive\nanalysis of ICL design considerations for applying LLMs to tabular clinical\ntasks and highlights distillation and multimodal extensions as promising\ndirections for future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.06270v4", "cate": "cs.LG", "date": "2024-05-10", "updated": "2025-07-25", "AI": {"title_translation": "XAI4LLM. 机器学习模型与大型语言模型协同增强医疗领域上下文学习", "tldr": "本研究提出了一种知识引导的上下文学习（ICL）框架，使大型语言模型（LLMs）能够处理结构化临床数据，并在心脏病和糖尿病预测任务中与传统机器学习模型进行比较。结果显示，虽然传统机器学习模型在平衡的精确召回率方面表现更优，但结合领域知识的叙述性提示LLMs能提高召回率并显著减少性别偏见，增强公平性，尽管推理延迟有所增加。", "motivation": "临床决策支持系统需要不仅高度准确，而且公平且对漏诊影响敏感的模型。", "method": "本研究引入了一种知识引导的上下文学习（ICL）框架，旨在使大型语言模型（LLMs）有效处理结构化临床数据。该方法整合了领域特定特征分组、平衡的少量样本示例以及任务特定提示策略。研究通过70种不同的ICL设计、多种提示变体和两种通信风格（自然语言叙述和数字对话）系统评估了该方法，并将其性能与稳健的经典机器学习（ML）基准模型在心脏病和糖尿病预测任务上进行了比较。", "result": "研究发现，尽管传统机器学习模型在平衡的精确召回率情景中保持卓越性能，但采用叙述性提示并整合领域知识的LLMs实现了更高的召回率，并显著减少了性别偏见，将公平性差异缩小了一个数量级。尽管当前存在推理延迟增加的局限性，LLMs提供了显著优势，包括零样本部署能力和增强的公平性。", "conclusion": "本研究首次全面分析了将LLMs应用于表格临床任务的ICL设计考虑因素，并指出蒸馏和多模态扩展是未来研究的有前景方向。", "translation": "临床决策支持系统需要不仅高度准确，而且公平且对漏诊影响敏感的模型。在本研究中，我们引入了一种知识引导的上下文学习（ICL）框架，旨在使大型语言模型（LLMs）能够有效处理结构化临床数据。我们的方法整合了领域特定特征分组、精心平衡的少量样本示例以及任务特定提示策略。我们通过各种提示变体和两种不同的通信风格——自然语言叙述和数字对话——系统地评估了70种不同的ICL设计，并将其性能与稳健的经典机器学习（ML）基准模型在心脏病和糖尿病预测任务上进行了比较。我们的研究结果表明，虽然传统机器学习模型在平衡的精确召回率情景中保持卓越性能，但采用叙述性提示并整合领域知识的LLMs实现了更高的召回率，并显著减少了性别偏见，有效地将公平性差异缩小了一个数量级。尽管当前存在推理延迟增加的局限性，LLMs提供了显著优势，包括零样本部署能力和增强的公平性。这项研究首次全面分析了将LLMs应用于表格临床任务的ICL设计考虑因素，并指出蒸馏和多模态扩展是未来研究的有前景方向。", "summary": "本研究提出了一种知识引导的上下文学习（ICL）框架，旨在使大型语言模型（LLMs）能够有效处理结构化临床数据，以支持医疗领域的决策。该框架整合了领域知识、少量样本示例和特定提示策略，并在心脏病和糖尿病预测任务中与传统机器学习模型进行对比评估。结果显示，尽管传统ML模型在平衡精确召回率上表现更佳，但LLMs在结合叙述性提示和领域知识后，能显著提高召回率并大幅减少性别偏见，从而增强了模型的公平性，并具备零样本部署能力。研究也指出了LLMs在推理延迟方面的局限性，并展望了蒸馏和多模态扩展的未来研究方向。", "keywords": "上下文学习, 大型语言模型, 医疗, 公平性, 偏见消除", "comments": "该论文创新性地将LLMs应用于结构化临床数据的上下文学习，并首次系统评估了其在医疗任务中的表现和公平性。其重要性在于证明了LLMs在提升模型公平性（尤其是性别偏见）方面的潜力，这对于临床决策支持系统至关重要。尽管存在推理延迟的局限性，但LLMs在零样本部署和公平性方面的优势为未来医疗AI的发展提供了新的视角和方向。"}}
{"id": "2504.05355", "title": "Deep Learning for Double Auction", "authors": ["Jiayin Liu", "Chenglong Zhang"], "categories": ["cs.LG", "cs.GT", "econ.TH"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This submission has been withdrawn in accordance with our institution's publication policy, which requires additional internal review and approval prior to public release", "url": "http://arxiv.org/abs/2504.05355v2", "summary": "Auctions are important mechanisms extensively implemented in various markets,\ne.g., search engines' keyword auctions, antique auctions, etc. Finding an\noptimal auction mechanism is extremely difficult due to the constraints of\nimperfect information, incentive compatibility (IC), and individual rationality\n(IR). In addition to the traditional economic methods, some recently attempted\nto find the optimal (single) auction using deep learning methods. Unlike those\nattempts focusing on single auctions, we develop deep learning methods for\ndouble auctions, where imperfect information exists on both the demand and\nsupply sides. The previous attempts on single auction cannot directly apply to\nour contexts and those attempts additionally suffer from limited\ngeneralizability, inefficiency in ensuring the constraints, and learning\nfluctuations. We innovate in designing deep learning models for solving the\nmore complex problem and additionally addressing the previous models' three\nlimitations. Specifically, we achieve generalizability by leveraging a\ntransformer-based architecture to model market participants as sequences for\nvarying market sizes; we utilize the numerical features of the constraints and\npre-treat them for a higher learning efficiency; we develop a\ngradient-conflict-elimination scheme to address the problem of learning\nfluctuation. Extensive experimental evaluations demonstrate the superiority of\nour approach to classical and machine learning baselines.", "comment": "This submission has been withdrawn in accordance with our\n  institution's publication policy, which requires additional internal review\n  and approval prior to public release", "pdf_url": "http://arxiv.org/pdf/2504.05355v2", "cate": "cs.LG", "date": "2025-04-07", "updated": "2025-07-25", "AI": {"title_translation": "双边拍卖的深度学习", "tldr": "本文为双边拍卖开发了深度学习方法，解决了现有单边拍卖深度学习方法的局限性，并通过基于Transformer的架构、约束的特征利用和梯度冲突消除方案提高了通用性、效率和稳定性。", "motivation": "寻找最优拍卖机制非常困难，传统经济学方法和现有针对单边拍卖的深度学习方法都存在信息不完美、激励兼容性、个体理性等约束，且现有深度学习方法在通用性、确保约束的效率和学习波动性方面存在局限。本文旨在为更复杂的双边拍卖问题设计深度学习模型，并解决现有模型的局限性。", "method": "本文设计了针对双边拍卖的深度学习模型，并解决了现有模型的三个局限性：1. 利用基于Transformer的架构将市场参与者建模为序列，以实现对不同市场规模的通用性。2. 利用约束的数值特征并进行预处理，以提高学习效率。3. 开发了梯度冲突消除方案，以解决学习波动性问题。", "result": "广泛的实验评估表明，本文的方法优于经典和机器学习基线。", "conclusion": "本文成功开发了用于双边拍卖的深度学习方法，该方法在处理复杂问题方面表现出色，并有效解决了现有深度学习模型在通用性、效率和稳定性方面的局限性。", "translation": "拍卖是在各种市场中广泛实施的重要机制，例如搜索引擎的关键词拍卖、古董拍卖等。由于信息不完美、激励兼容性（IC）和个体理性（IR）的约束，找到最优拍卖机制极其困难。除了传统的经济学方法外，最近一些尝试使用深度学习方法寻找最优（单边）拍卖。与那些专注于单边拍卖的尝试不同，我们为双边拍卖开发了深度学习方法，其中需求方和供应方都存在不完美信息。先前针对单边拍卖的尝试不能直接应用于我们的场景，并且这些尝试还存在通用性有限、确保约束的效率低下以及学习波动性等问题。我们在设计深度学习模型方面进行了创新，以解决更复杂的问题，并额外解决了先前模型的三个局限性。具体而言，我们通过利用基于Transformer的架构将市场参与者建模为序列以适应不同的市场规模，从而实现通用性；我们利用约束的数值特征并对其进行预处理以提高学习效率；我们开发了一种梯度冲突消除方案来解决学习波动性问题。广泛的实验评估表明，我们的方法优于经典和机器学习基线。", "summary": "本文提出了一种针对双边拍卖的深度学习方法，旨在解决传统方法和现有单边拍卖深度学习方法的局限性。该方法通过引入基于Transformer的架构实现对不同市场规模的通用性，利用约束的数值特征提高学习效率，并开发梯度冲突消除方案解决学习波动性。实验结果表明，该方法优于现有基线。", "keywords": "深度学习, 双边拍卖, 拍卖机制, Transformer, 激励兼容性", "comments": "本文的创新之处在于首次将深度学习应用于更复杂的双边拍卖问题，并系统性地解决了现有深度学习方法在拍卖机制设计中面临的通用性、效率和稳定性三大挑战。特别是，引入Transformer模型处理可变市场规模以及梯度冲突消除方案，为未来在该领域的进一步研究提供了有价值的方向。"}}
{"id": "2507.19024", "title": "A Survey of Multimodal Hallucination Evaluation and Detection", "authors": ["Zhiyuan Chen", "Yuecong Min", "Jie Zhang", "Bei Yan", "Jiahao Wang", "Xiaozhen Wang", "Shiguang Shan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      33 pages, 5 figures", "url": "http://arxiv.org/abs/2507.19024v1", "summary": "Multi-modal Large Language Models (MLLMs) have emerged as a powerful paradigm\nfor integrating visual and textual information, supporting a wide range of\nmulti-modal tasks. However, these models often suffer from hallucination,\nproducing content that appears plausible but contradicts the input content or\nestablished world knowledge. This survey offers an in-depth review of\nhallucination evaluation benchmarks and detection methods across Image-to-Text\n(I2T) and Text-to-image (T2I) generation tasks. Specifically, we first propose\na taxonomy of hallucination based on faithfulness and factuality, incorporating\nthe common types of hallucinations observed in practice. Then we provide an\noverview of existing hallucination evaluation benchmarks for both T2I and I2T\ntasks, highlighting their construction process, evaluation objectives, and\nemployed metrics. Furthermore, we summarize recent advances in hallucination\ndetection methods, which aims to identify hallucinated content at the instance\nlevel and serve as a practical complement of benchmark-based evaluation.\nFinally, we highlight key limitations in current benchmarks and detection\nmethods, and outline potential directions for future research.", "comment": "33 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.19024v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "多模态幻觉评估与检测综述", "tldr": "这篇综述深入探讨了多模态大语言模型中的幻觉问题，涵盖了幻觉的分类、评估基准和检测方法，并指出了未来的研究方向。", "motivation": "多模态大语言模型（MLLMs）虽然强大，但在生成内容时常出现与输入内容或现有知识相矛盾的“幻觉”现象，因此需要对其进行评估和检测。", "method": "本综述首先基于忠实性和事实性提出了幻觉的分类法，涵盖了实践中常见的幻觉类型。接着，概述了现有针对图像到文本（I2T）和文本到图像（T2I）任务的幻觉评估基准，并总结了幻觉检测方法的最新进展。", "result": "本综述提供了一个全面的视角，涵盖了多模态幻觉的分类、现有的评估基准（包括其构建过程、评估目标和采用的指标）以及用于实例级识别的检测方法。同时，也指出了当前基准和检测方法的局限性。", "conclusion": "本综述揭示了当前幻觉评估基准和检测方法的局限性，并为未来的研究指明了潜在方向。", "translation": "多模态大语言模型（MLLMs）已成为整合视觉和文本信息的强大范式，支持广泛的多模态任务。然而，这些模型经常遭受幻觉问题，产生看似合理但与输入内容或既定世界知识相矛盾的内容。本综述深入回顾了图像到文本（I2T）和文本到图像（T2I）生成任务中的幻觉评估基准和检测方法。具体而言，我们首先基于忠实性和事实性提出了幻觉的分类法，并纳入了实践中观察到的常见幻觉类型。然后，我们概述了针对T2I和I2T任务的现有幻觉评估基准，重点介绍了它们的构建过程、评估目标和采用的指标。此外，我们总结了幻觉检测方法的最新进展，这些方法旨在在实例级别识别幻觉内容，并作为基于基准评估的实用补充。最后，我们强调了当前基准和检测方法中的主要局限性，并概述了未来研究的潜在方向。", "summary": "本综述全面审视了多模态大语言模型（MLLMs）中的“幻觉”现象，该现象指模型生成的内容与输入或事实不符。文章首先提出了一种基于忠实性和事实性的幻觉分类体系，随后详细介绍了图像到文本（I2T）和文本到图像（T2I）任务中现有的幻觉评估基准，包括其构建、目标和指标。此外，综述还总结了识别幻觉内容的检测方法进展。最后，文章指出了当前评估和检测方法的不足，并展望了未来的研究方向。", "keywords": "多模态大语言模型, 幻觉, 评估, 检测, 综述", "comments": "这篇综述非常及时且重要，因为它关注了多模态大语言模型（MLLMs）的一个核心挑战——幻觉问题。通过系统地分类、评估和检测方法，它为理解和解决这一问题提供了全面的框架，并为未来的研究指明了清晰的方向，对于推动MLLMs的可靠性和实用性具有重要意义。"}}
{"id": "2507.19399", "title": "Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security", "authors": ["Gabriel Chua"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19399v1", "summary": "As large language models (LLMs) increasingly integrate native code\ninterpreters, they enable powerful real-time execution capabilities,\nsubstantially expanding their utility. However, such integrations introduce\npotential system-level cybersecurity threats, fundamentally different from\nprompt-based vulnerabilities. To systematically evaluate these\ninterpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience\nCheck for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting\nCPU, memory, and disk resource exhaustion. Each risk category includes\nexplicitly malicious (\"direct\") and plausibly benign (\"indirect\") prompt\nvariants. Our automated evaluation framework assesses not only whether LLMs\nrefuse or generates risky code, but also executes the generated code within the\ninterpreter environment to evaluate code correctness, simplifications made by\nthe LLM to make the code safe, or execution timeouts. Evaluating 7 commercially\navailable models from OpenAI and Google, we uncover significant and\ninconsistent vulnerabilities. For instance, evaluations show substantial\ndisparities even within providers - OpenAI's o4-mini correctly refuses risky\nrequests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results\nparticularly underscore that indirect, socially-engineered prompts\nsubstantially weaken model defenses. This highlights an urgent need for\ninterpreter-specific cybersecurity benchmarks, dedicated mitigation tools\n(e.g., guardrails), and clear industry standards to guide safe and responsible\ndeployment of LLM interpreter integrations. The benchmark dataset and\nevaluation code are publicly released to foster further research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19399v1", "cate": "cs.CR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "在CIRCLE中运行？一个LLM代码解释器安全性的简单基准", "tldr": "本文提出了CIRCLE，一个针对LLM代码解释器系统级网络安全威胁的基准测试，发现现有模型存在显著且不一致的漏洞，尤其是在面对间接提示时防御能力显著下降。", "motivation": "随着大型语言模型（LLMs）集成原生代码解释器，其功能得到极大扩展，但也引入了与基于提示的漏洞根本不同的系统级网络安全威胁。为了系统地评估这些解释器特有的风险，本研究旨在提出一个评估框架。", "method": "研究提出了CIRCLE（Code-Interpreter Resilience Check for LLM Exploits）基准，包含1,260个旨在耗尽CPU、内存和磁盘资源的提示。这些提示分为明确恶意（“直接”）和看似良性（“间接”）两种变体。自动化评估框架不仅评估LLM是否拒绝或生成危险代码，还在解释器环境中执行生成的代码，以评估代码的正确性、LLM为确保安全所做的简化或执行超时。", "result": "对OpenAI和Google的7个商用模型进行评估后，研究发现了显著且不一致的漏洞。例如，OpenAI的o4-mini在7.1%的情况下正确拒绝了危险请求，远高于GPT-4.1的0.5%。结果特别强调，间接的、社会工程学提示显著削弱了模型的防御能力。", "conclusion": "研究强调了对解释器特有的网络安全基准、专用缓解工具（如护栏）以及明确的行业标准以指导LLM解释器集成的安全和负责任部署的迫切需求。", "translation": "随着大型语言模型（LLMs）越来越多地集成原生代码解释器，它们实现了强大的实时执行能力，极大地扩展了其效用。然而，这种集成引入了潜在的系统级网络安全威胁，这与基于提示的漏洞根本不同。为了系统地评估这些解释器特有的风险，我们提出了CIRCLE（Code-Interpreter Resilience Check for LLM Exploits），这是一个简单的基准，包含1,260个针对CPU、内存和磁盘资源耗尽的提示。每个风险类别都包含明确恶意（“直接”）和看似良性（“间接”）的提示变体。我们的自动化评估框架不仅评估LLM是否拒绝或生成危险代码，还在解释器环境中执行生成的代码，以评估代码的正确性、LLM为使代码安全所做的简化或执行超时。通过评估OpenAI和Google的7个商用模型，我们发现了显著且不一致的漏洞。例如，评估显示即使在同一提供商内部也存在显著差异——OpenAI的o4-mini正确拒绝危险请求的比例为7.1%，明显高于GPT-4.1的0.5%。结果特别强调，间接的、社会工程学提示显著削弱了模型的防御能力。这突出表明，迫切需要解释器特有的网络安全基准、专用缓解工具（例如护栏）以及明确的行业标准来指导LLM解释器集成的安全和负责任的部署。基准数据集和评估代码已公开发布，以促进进一步研究。", "summary": "本文提出了CIRCLE，一个针对LLM代码解释器系统级网络安全威胁的简单基准。该基准包含1,260个提示，旨在测试CPU、内存和磁盘资源耗尽，并区分直接恶意和间接良性提示。通过自动化框架评估了7个商用LLM，结果显示模型存在显著且不一致的漏洞，特别是间接提示能有效削弱模型防御。研究强调了开发专用安全基准、缓解工具和行业标准的紧迫性，以确保LLM解释器集成的安全部署。", "keywords": "LLM, 代码解释器, 安全, 基准, 网络安全", "comments": "本文创新性地提出了针对LLM代码解释器安全的新型基准CIRCLE，填补了现有研究在系统级网络安全威胁评估方面的空白。其自动化评估框架能够实际执行生成的代码，提供更深入的洞察。研究发现的显著漏洞，特别是对间接提示的脆弱性，具有重要的实践意义，为LLM安全研究和行业标准的制定提供了有力证据。其贡献在于明确指出了LLM集成代码解释器带来的新风险，并提供了量化评估的工具和方法。"}}
{"id": "2507.19469", "title": "Efficient Lines Detection for Robot Soccer", "authors": ["João G. Melo", "João P. Mafaldo", "Edna Barros"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      12 pages, 8 figures, RoboCup Symposium 2025", "url": "http://arxiv.org/abs/2507.19469v1", "summary": "Self-localization is essential in robot soccer, where accurate detection of\nvisual field features, such as lines and boundaries, is critical for reliable\npose estimation. This paper presents a lightweight and efficient method for\ndetecting soccer field lines using the ELSED algorithm, extended with a\nclassification step that analyzes RGB color transitions to identify lines\nbelonging to the field. We introduce a pipeline based on Particle Swarm\nOptimization (PSO) for threshold calibration to optimize detection performance,\nrequiring only a small number of annotated samples. Our approach achieves\naccuracy comparable to a state-of-the-art deep learning model while offering\nhigher processing speed, making it well-suited for real-time applications on\nlow-power robotic platforms.", "comment": "12 pages, 8 figures, RoboCup Symposium 2025", "pdf_url": "http://arxiv.org/pdf/2507.19469v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "机器人足球中的高效线条检测", "tldr": "本文提出了一种基于ELSED算法和PSO优化的轻量级高效机器人足球场线条检测方法，其精度可与深度学习模型媲美，但处理速度更快，适用于低功耗机器人平台。", "motivation": "在机器人足球中，自定位至关重要，而准确检测视觉场特征（如线条和边界）对于可靠的姿态估计至关重要。", "method": "本文提出了一种轻量级高效的足球场线条检测方法，该方法基于ELSED算法并扩展了分类步骤，通过分析RGB颜色转换来识别属于球场的线条。引入了基于粒子群优化（PSO）的管道进行阈值校准，以优化检测性能，仅需要少量标注样本。", "result": "该方法在精度上与最先进的深度学习模型相当，同时提供更高的处理速度，使其非常适合在低功耗机器人平台上进行实时应用。", "conclusion": "本文提出的方法能够高效准确地检测机器人足球场线条，其性能优于或媲美现有技术，特别适用于资源受限的实时机器人平台。", "translation": "自定位在机器人足球中至关重要，其中准确检测视觉场特征（例如线条和边界）对于可靠的姿态估计至关重要。本文提出了一种轻量级高效的足球场线条检测方法，该方法使用ELSED算法，并扩展了分类步骤，通过分析RGB颜色转换来识别属于球场的线条。我们引入了一种基于粒子群优化（PSO）的管道进行阈值校准，以优化检测性能，仅需要少量标注样本。我们的方法在精度上可与最先进的深度学习模型媲美，同时提供更高的处理速度，使其非常适合在低功耗机器人平台上进行实时应用。", "summary": "本文提出了一种用于机器人足球场线条检测的轻量级高效方法。该方法基于ELSED算法，并通过RGB颜色转换分类步骤进行扩展，以识别场线。利用粒子群优化（PSO）管道进行阈值校准，仅需少量标注样本即可优化检测性能。该方法在精度上与先进的深度学习模型相当，同时提供更快的处理速度，使其适用于低功耗机器人平台的实时应用。", "keywords": "线条检测, 机器人足球, ELSED, 粒子群优化, 实时应用", "comments": "该论文的创新点在于结合了ELSED算法和基于RGB颜色转换的分类步骤，并通过PSO优化进行阈值校准，实现了在低功耗平台上高效且高精度的线条检测。其重要性在于为机器人足球中的实时自定位提供了一种可靠且资源友好的解决方案，避免了深度学习模型通常所需的高计算资源。"}}
{"id": "2302.07751", "title": "Fully Energy-Efficient Randomized Backoff: Slow Feedback Loops Yield Fast Contention Resolution", "authors": ["Michael A. Bender", "Jeremy T. Fineman", "Seth Gilbert", "John Kuszmaul", "Maxwell Young"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2302.07751v5", "summary": "Contention resolution addresses the problem of coordinating access to a\nshared channel. Time proceeds in slots, and a packet transmission can be made\nin any slot. A packet is successfully sent if no other packet is also\ntransmitted during that slot. If two or more packets are sent in the same slot,\nthen none of these transmissions succeed. Listening during a slot gives ternary\nfeedback, indicating if that slot had (0) silence, (1) a successful\ntransmission, or (2+) noise. No other feedback is available. Packets are\n(adversarially) injected into the system over time. A packet departs the system\nonce it is successful. The goal is to send all packets while optimizing\nthroughput, which is roughly the fraction of successful slots.\n  Most prior algorithms with constant throughput require a short feedback loop,\nin the sense that a packet's sending probability in slot t+1 is fully\ndetermined by its internal state at slot t and the channel feedback at slot t.\nAn open question is whether these short feedback loops are necessary; that is,\nhow often must listening and updating occur in order to achieve constant\nthroughput? This question addresses energy efficiency, since both listening and\nsending consume significant energy. The channel can also suffer adversarial\nnoise (\"jamming\"), which causes any listener to hear noise, even when no\npackets are sent. How does jamming affect our goal of long feedback\nloops/energy efficiency?\n  Connecting these questions, we ask: what does a contention-resolution\nalgorithm have to sacrifice to reduce channel accesses? Must we give up on\nconstant throughput or robustness to noise? Here, we show that we need not\nconcede anything. Suppose there are N packets and J jammed slots, where the\ninput is determined by an adaptive adversary. We give an algorithm that, with\nhigh probability in N+J, has constant throughput and polylog(N+J) channel\naccesses per packet.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2302.07751v5", "cate": "cs.DC", "date": "2023-02-15", "updated": "2025-07-25", "AI": {"title_translation": "充分节能的随机退避：慢反馈循环产生快速争用解决", "tldr": "本文提出一种新的争用解决算法，在保持恒定吞吐量和对抗性噪声鲁棒性的同时，显著减少了每包信道访问次数，从而实现了高能效。", "motivation": "现有大多数实现恒定吞吐量的算法需要短反馈循环，这意味着频繁的监听和更新，这会消耗大量能量。本文旨在探讨是否可以延长反馈循环（即减少信道访问次数）同时仍能保持恒定吞吐量和对对抗性噪声的鲁棒性，以提高能源效率。", "method": "本文提出了一种新的争用解决算法。该算法在存在N个数据包和J个干扰时隙（由自适应对抗者决定）的条件下运行。", "result": "本文提出的算法以N+J的极高概率，实现了恒定的吞吐量，并且每数据包的信道访问次数为polylog(N+J)。这表明无需牺牲恒定吞吐量或对噪声的鲁棒性即可减少信道访问。", "conclusion": "本文证明了在共享信道争用解决问题中，可以通过延长反馈循环（减少信道访问）来提高能源效率，而无需牺牲性能（恒定吞吐量）或对对抗性噪声的鲁棒性。", "translation": "争用解决旨在协调对共享信道的访问问题。时间以时隙为单位进行，数据包可以在任何时隙中传输。如果该时隙中没有其他数据包同时传输，则数据包成功发送。如果两个或更多数据包在同一时隙发送，则这些传输都不会成功。在时隙期间监听会提供三元反馈，指示该时隙是（0）静默，（1）成功传输，还是（2+）噪声。没有其他反馈可用。数据包（对抗性地）随时间注入系统。数据包一旦成功便离开系统。目标是发送所有数据包，同时优化吞吐量，这大致是成功时隙的比例。\n大多数具有恒定吞吐量的现有算法需要短反馈循环，即数据包在时隙 t+1 的发送概率完全由其在时隙 t 的内部状态和时隙 t 的信道反馈决定。一个悬而未决的问题是这些短反馈循环是否必要；也就是说，为了实现恒定吞吐量，监听和更新必须多久发生一次？这个问题涉及到能源效率，因为监听和发送都会消耗大量能量。信道也可能遭受对抗性噪声（“干扰”），这会导致任何监听者听到噪声，即使没有数据包发送。干扰如何影响我们实现长反馈循环/能源效率的目标？\n连接这些问题，我们询问：争用解决算法必须牺牲什么才能减少信道访问？我们必须放弃恒定吞吐量或对噪声的鲁棒性吗？在这里，我们表明我们无需妥协任何东西。假设有 N 个数据包和 J 个干扰时隙，其中输入由自适应对抗者确定。我们给出了一个算法，该算法以 N+J 的极高概率，具有恒定的吞吐量和每数据包 polylog(N+J) 的信道访问次数。", "summary": "本文研究了共享信道争用解决中的能源效率问题，特别是关于反馈循环长度的需求。针对现有算法依赖短反馈循环导致高能耗的痛点，作者提出了一个新算法。该算法在存在对抗性噪声和自适应对抗者的情况下，实现了恒定吞吐量，并且显著减少了每数据包的信道访问次数（polylog(N+J)），证明了无需牺牲性能或鲁棒性即可提高能源效率。", "keywords": "争用解决, 能源效率, 随机退避, 反馈循环, 吞吐量", "comments": "这篇论文的创新点在于它挑战了传统上认为实现恒定吞吐量需要短反馈循环的假设，并成功设计了一个高能效的算法。它在理论上证明了在争用解决中，通过延长反馈循环（减少信道访问）可以显著提高能源效率，同时保持对对抗性噪声的鲁棒性和恒定吞吐量。这对于无线通信和网络协议设计具有重要意义，尤其是在资源受限或需要节能的应用场景中。"}}
{"id": "2507.18849", "title": "Optimizing Metachronal Paddling with Reinforcement Learning at Low Reynolds Number", "authors": ["Alana A. Bailey", "Robert D. Guy"], "categories": ["physics.flu-dyn", "cs.LG", "stat.ML"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "Comments:      18 pages, 14 figures, to be published in EPJ E", "url": "http://arxiv.org/abs/2507.18849v1", "summary": "Metachronal paddling is a swimming strategy in which an organism oscillates\nsets of adjacent limbs with a constant phase lag, propagating a metachronal\nwave through its limbs and propelling it forward. This limb coordination\nstrategy is utilized by swimmers across a wide range of Reynolds numbers, which\nsuggests that this metachronal rhythm was selected for its optimality of\nswimming performance. In this study, we apply reinforcement learning to a\nswimmer at zero Reynolds number and investigate whether the learning algorithm\nselects this metachronal rhythm, or if other coordination patterns emerge. We\ndesign the swimmer agent with an elongated body and pairs of straight,\ninflexible paddles placed along the body for various fixed paddle spacings.\nBased on paddle spacing, the swimmer agent learns qualitatively different\ncoordination patterns. At tight spacings, a back-to-front metachronal wave-like\nstroke emerges which resembles the commonly observed biological rhythm, but at\nwide spacings, different limb coordinations are selected. Across all resulting\nstrokes, the fastest stroke is dependent on the number of paddles, however, the\nmost efficient stroke is a back-to-front wave-like stroke regardless of the\nnumber of paddles.", "comment": "18 pages, 14 figures, to be published in EPJ E", "pdf_url": "http://arxiv.org/pdf/2507.18849v1", "cate": "physics.flu-dyn", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "低雷诺数下利用强化学习优化异时划水", "tldr": "本研究利用强化学习在低雷诺数下模拟划水运动，发现紧密桨距下会产生类似生物的异时波状划水，而最有效的划水方式是后向前的波状划水。", "motivation": "异时划水是一种常见的游泳策略，被广泛应用于不同雷诺数的生物中，这表明其在游泳性能方面具有最优性。本研究旨在探究强化学习算法是否会选择这种异时节律，或者是否会出现其他协调模式。", "method": "研究人员将强化学习应用于一个零雷诺数下的游泳器模型。该游泳器代理被设计成具有细长身体和沿身体放置的成对笔直、不可弯曲的桨叶，并设置了不同的固定桨距。", "result": "根据桨距的不同，游泳器代理学习到了性质上不同的协调模式。在桨距紧密时，出现了类似常见生物节律的从后向前的异时波状划水；但在桨距宽时，则选择了不同的肢体协调模式。在所有产生的划水模式中，最快的划水方式取决于桨叶的数量。", "conclusion": "最有效的划水方式是后向前的波状划水，而与桨叶数量无关。", "translation": "异时划水是一种游泳策略，生物通过使相邻肢体以恒定的相位滞后振荡，从而在肢体中传播异时波并向前推进。这种肢体协调策略被广泛应用于各种雷诺数下的游泳生物，这表明这种异时节律因其游泳性能的最优性而被选择。在本研究中，我们将强化学习应用于一个零雷诺数下的游泳器，并调查学习算法是否会选择这种异时节律，或者是否会出现其他协调模式。我们设计了具有细长身体和沿身体放置的成对笔直、不可弯曲桨叶的游泳器代理，并设置了各种固定的桨距。根据桨距的不同，游泳器代理学习到了性质上不同的协调模式。在桨距紧密时，出现了类似常见生物节律的从后向前的异时波状划水；但在桨距宽时，则选择了不同的肢体协调模式。在所有产生的划水模式中，最快的划水方式取决于桨叶的数量，然而，最有效的划水方式是后向前的波状划水，无论桨叶数量如何。", "summary": "本研究利用强化学习在零雷诺数下模拟异时划水，以探究其协调模式。研究发现，在桨距紧密时，模型能学习到类似生物的后向前的异时波状划水；而在桨距宽时，则出现不同的协调模式。尽管最快划水方式取决于桨叶数量，但最有效的划水方式始终是后向前的波状划水。", "keywords": "异时划水, 强化学习, 低雷诺数, 游泳性能, 协调模式", "comments": "这项研究创新性地将强化学习应用于低雷诺数下的生物运动模拟，特别是异时划水。它不仅验证了生物中常见的异时节律在特定条件下（紧密桨距）的涌现，还揭示了不同桨距下可能出现的其他有效协调模式。研究强调了效率与速度在不同参数下的表现，为理解微观生物的运动机制提供了新的视角和计算方法。"}}
{"id": "2507.19041", "title": "PGKET: A Photonic Gaussian Kernel Enhanced Transformer", "authors": ["Ren-Xin Zhao"], "categories": ["quant-ph", "cs.CV"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19041v1", "summary": "Self-Attention Mechanisms (SAMs) enhance model performance by extracting key\ninformation but are inefficient when dealing with long sequences. To this end,\na photonic Gaussian Kernel Enhanced Transformer (PGKET) is proposed, based on\nthe Photonic Gaussian Kernel Self-Attention Mechanism (PGKSAM). The PGKSAM\ncalculates the Photonic Gaussian Kernel Self-Attention Score (PGKSAS) using\nphoton interferometry and superposition to process multiple inputs in parallel.\nExperimental results show that PGKET outperforms some state-of-the-art\ntransformers in multi-classification tasks on MedMNIST v2 and CIFAR-10, and is\nexpected to improve performance in complex tasks and accelerate the convergence\nof Photonic Computing (PC) and machine learning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19041v1", "cate": "quant-ph", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "PGKET：一种光子高斯核增强型Transformer", "tldr": "PGKET是一种基于光子高斯核自注意力机制的Transformer，通过光子并行处理提升了长序列处理效率，并在多分类任务上表现优于现有Transformer。", "motivation": "自注意力机制（SAMs）在处理长序列时效率低下。", "method": "提出了一种光子高斯核增强型Transformer（PGKET），其核心是光子高斯核自注意力机制（PGKSAM）。PGKSAM利用光子干涉和叠加并行计算光子高斯核自注意力分数（PGKSAS），以处理多个输入。", "result": "PGKET在MedMNIST v2和CIFAR-10上的多分类任务中表现优于一些最先进的Transformer模型。", "conclusion": "PGKET有望在复杂任务中提升性能，并加速光子计算和机器学习的收敛。", "translation": "自注意力机制（SAMs）通过提取关键信息来提高模型性能，但在处理长序列时效率低下。为此，本文提出了一种基于光子高斯核自注意力机制（PGKSAM）的光子高斯核增强型Transformer（PGKET）。PGKSAM利用光子干涉和叠加并行计算光子高斯核自注意力分数（PGKSAS），以处理多个输入。实验结果表明，PGKET在MedMNIST v2和CIFAR-10上的多分类任务中优于一些最先进的Transformer模型，并有望在复杂任务中提高性能，加速光子计算（PC）和机器学习的收敛。", "summary": "本文提出了一种名为PGKET的光子高斯核增强型Transformer，旨在解决自注意力机制在处理长序列时的效率问题。PGKET的核心是PGKSAM，它利用光子干涉和叠加实现多输入并行处理，从而计算光子高斯核自注意力分数。实验证明，PGKET在MedMNIST v2和CIFAR-10数据集上的多分类任务中超越了现有的先进Transformer模型，预示着其在复杂任务性能提升及光子计算与机器学习融合加速方面的潜力。", "keywords": "光子计算, Transformer, 自注意力机制, 高斯核, 并行处理", "comments": "PGKET的创新点在于将光子计算原理引入Transformer的自注意力机制，通过光子并行处理克服了传统自注意力机制处理长序列时的效率瓶颈。这对于提升深度学习模型在处理大规模数据时的性能具有重要意义，同时也为光子计算在机器学习领域的应用开辟了新的方向。"}}
{"id": "2507.18657", "title": "VGS-ATD: Robust Distributed Learning for Multi-Label Medical Image Classification Under Heterogeneous and Imbalanced Conditions", "authors": ["Zehui Zhao", "Laith Alzubaidi", "Haider A. Alwzwazy", "Jinglan Zhang", "Yuantong Gu"], "categories": ["cs.CV", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      15 pages, 8 figures, 6 tables", "url": "http://arxiv.org/abs/2507.18657v2", "summary": "In recent years, advanced deep learning architectures have shown strong\nperformance in medical imaging tasks. However, the traditional centralized\nlearning paradigm poses serious privacy risks as all data is collected and\ntrained on a single server. To mitigate this challenge, decentralized\napproaches such as federated learning and swarm learning have emerged, allowing\nmodel training on local nodes while sharing only model weights. While these\nmethods enhance privacy, they struggle with heterogeneous and imbalanced data\nand suffer from inefficiencies due to frequent communication and the\naggregation of weights. More critically, the dynamic and complex nature of\nclinical environments demands scalable AI systems capable of continuously\nlearning from diverse modalities and multilabels. Yet, both centralized and\ndecentralized models are prone to catastrophic forgetting during system\nexpansion, often requiring full model retraining to incorporate new data. To\naddress these limitations, we propose VGS-ATD, a novel distributed learning\nframework. To validate VGS-ATD, we evaluate it in experiments spanning 30\ndatasets and 80 independent labels across distributed nodes, VGS-ATD achieved\nan overall accuracy of 92.7%, outperforming centralized learning (84.9%) and\nswarm learning (72.99%), while federated learning failed under these conditions\ndue to high requirements on computational resources. VGS-ATD also demonstrated\nstrong scalability, with only a 1% drop in accuracy on existing nodes after\nexpansion, compared to a 20% drop in centralized learning, highlighting its\nresilience to catastrophic forgetting. Additionally, it reduced computational\ncosts by up to 50% relative to both centralized and swarm learning, confirming\nits superior efficiency and scalability.", "comment": "The idea is still underdeveloped, not yet enough to be published", "pdf_url": "http://arxiv.org/pdf/2507.18657v2", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-28", "AI": {"title_translation": "VGS-ATD: 异构和不平衡条件下多标签医学图像分类的鲁棒分布式学习", "tldr": "VGS-ATD是一种新型的分布式学习框架，解决了传统集中式和去中心化学习在医学图像分类中面临的隐私、数据异构、低效和灾难性遗忘问题，实现了更高的准确性、更好的可扩展性和更低的计算成本。", "motivation": "传统集中式学习存在严重的隐私风险；联邦学习和群学习等去中心化方法难以处理异构和不平衡数据，且存在通信效率低下问题；现有模型在系统扩展时易发生灾难性遗忘，需要重新训练。", "method": "本文提出了一种名为 VGS-ATD 的新型分布式学习框架，用于解决上述问题。", "result": "在30个数据集和80个独立标签的实验中，VGS-ATD 取得了92.7%的整体准确率，优于集中式学习（84.9%）和群学习（72.99%），且联邦学习在这种条件下失败。VGS-ATD 在系统扩展后，现有节点准确率仅下降1%，而集中式学习下降20%，显示出对灾难性遗忘的强大抵抗力。此外，VGS-ATD 相对于集中式和群学习，计算成本降低了高达50%。", "conclusion": "VGS-ATD 是一种高效、可扩展且鲁棒的分布式学习框架，能够有效应对医学图像分类中异构、不平衡数据、隐私风险和灾难性遗忘等挑战，在性能和效率上均优于现有方法。", "translation": "近年来，先进的深度学习架构在医学成像任务中表现出强大的性能。然而，传统的集中式学习范式带来了严重的数据隐私风险，因为所有数据都集中在一个服务器上进行训练。为了缓解这一挑战，联邦学习和群学习等去中心化方法应运而生，它们允许在本地节点上训练模型，同时只共享模型权重。虽然这些方法增强了隐私，但它们难以处理异构和不平衡数据，并且由于频繁的通信和权重聚合而效率低下。更关键的是，临床环境的动态和复杂性要求可扩展的AI系统能够从不同模态和多标签中持续学习。然而，集中式和去中心化模型在系统扩展时都容易发生灾难性遗忘，通常需要对模型进行完全重新训练才能整合新数据。为了解决这些局限性，我们提出了 VGS-ATD，一种新颖的分布式学习框架。为了验证 VGS-ATD，我们在跨分布式节点的30个数据集和80个独立标签的实验中对其进行了评估，VGS-ATD 实现了92.7%的整体准确率，优于集中式学习（84.9%）和群学习（72.99%），而联邦学习由于对计算资源的高要求在这些条件下失败。VGS-ATD 还表现出强大的可扩展性，在扩展后现有节点的准确率仅下降1%，而集中式学习下降20%，这突出了其对灾难性遗忘的弹性。此外，相对于集中式和群学习，它将计算成本降低了高达50%，证实了其卓越的效率和可扩展性。", "summary": "本文提出了一种名为 VGS-ATD 的新型分布式学习框架，旨在解决医学图像分类中传统集中式和现有去中心化方法（如联邦学习和群学习）在数据隐私、异构与不平衡数据处理、通信效率以及灾难性遗忘方面面临的挑战。VGS-ATD 在多标签医学图像分类任务中表现出色，实验结果显示其在准确率、可扩展性和计算效率方面均优于现有方法，尤其在应对灾难性遗忘方面表现出显著的鲁棒性。", "keywords": "分布式学习, 医学图像分类, 异构数据, 灾难性遗忘, 多标签分类", "comments": "VGS-ATD 的创新之处在于它同时解决了分布式学习在医学图像领域面临的多个关键挑战：数据隐私、异构数据处理、通信效率以及灾难性遗忘。其在多标签医学图像分类任务中的优异表现，特别是对灾难性遗忘的强大抵抗力和显著的计算成本降低，使其在实际临床应用中具有重要的潜力。该框架为构建更鲁棒、高效和可扩展的医疗AI系统提供了新的思路。"}}
{"id": "2507.18987", "title": "Differentiated Thyroid Cancer Recurrence Classification Using Machine Learning Models and Bayesian Neural Networks with Varying Priors: A SHAP-Based Interpretation of the Best Performing Model", "authors": ["HMNS Kumari", "HMLS Kumari", "UMMPK Nawarathne"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      16 pages, 15 figures, to be published in International Journal of Research in Computing (IJRC)", "url": "http://arxiv.org/abs/2507.18987v1", "summary": "Differentiated thyroid cancer DTC recurrence is a major public health\nconcern, requiring classification and predictive models that are not only\naccurate but also interpretable and uncertainty aware. This study introduces a\ncomprehensive framework for DTC recurrence classification using a dataset\ncontaining 383 patients and 16 clinical and pathological variables. Initially,\n11 machine learning ML models were employed using the complete dataset, where\nthe Support Vector Machines SVM model achieved the highest accuracy of 0.9481.\nTo reduce complexity and redundancy, feature selection was carried out using\nthe Boruta algorithm, and the same ML models were applied to the reduced\ndataset, where it was observed that the Logistic Regression LR model obtained\nthe maximum accuracy of 0.9611. However, these ML models often lack uncertainty\nquantification, which is critical in clinical decision making. Therefore, to\naddress this limitation, the Bayesian Neural Networks BNN with six varying\nprior distributions, including Normal 0,1, Normal 0,10, Laplace 0,1, Cauchy\n0,1, Cauchy 0,2.5, and Horseshoe 1, were implemented on both the complete and\nreduced datasets. The BNN model with Normal 0,10 prior distribution exhibited\nmaximum accuracies of 0.9740 and 0.9870 before and after feature selection,\nrespectively.", "comment": "16 pages, 15 figures, to be published in International Journal of\n  Research in Computing (IJRC)", "pdf_url": "http://arxiv.org/pdf/2507.18987v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "分化型甲状腺癌复发分类：使用机器学习模型和不同先验的贝叶斯神经网络，并基于SHAP解释最佳性能模型", "tldr": "本研究使用机器学习模型和贝叶斯神经网络对分化型甲状腺癌复发进行高精度分类，并强调了不确定性量化的重要性。", "motivation": "分化型甲状腺癌（DTC）复发是一个重要的公共卫生问题，需要准确、可解释且具有不确定性感知的分类和预测模型。", "method": "本研究提出了一个DTC复发分类的综合框架。首先，在包含383名患者和16个变量的完整数据集上应用了11种机器学习（ML）模型。其次，使用Boruta算法进行特征选择，并在简化数据集上重新应用ML模型。最后，为解决ML模型缺乏不确定性量化的问题，在完整和简化数据集上实现了具有六种不同先验分布的贝叶斯神经网络（BNN）。", "result": "在完整数据集上，支持向量机（SVM）模型准确率最高为0.9481。在特征选择后的简化数据集上，逻辑回归（LR）模型准确率最高为0.9611。具有Normal 0,10先验分布的BNN模型在特征选择前后分别取得了0.9740和0.9870的最高准确率。", "conclusion": "贝叶斯神经网络（BNN），特别是采用Normal 0,10先验的BNN，在分化型甲状腺癌复发分类中表现出卓越的准确性和提供不确定性量化信息的能力，这对于临床决策至关重要。", "translation": "标题：分化型甲状腺癌复发分类：使用机器学习模型和不同先验的贝叶斯神经网络，并基于SHAP解释最佳性能模型\n摘要：分化型甲状腺癌（DTC）复发是一个主要的公共卫生问题，需要不仅准确而且可解释且具有不确定性感知的分类和预测模型。本研究引入了一个使用包含383名患者和16个临床和病理变量的数据集进行DTC复发分类的综合框架。最初，使用完整数据集采用了11种机器学习（ML）模型，其中支持向量机（SVM）模型达到了0.9481的最高准确率。为了降低复杂性和冗余，使用Boruta算法进行了特征选择，并将相同的ML模型应用于简化数据集，观察到逻辑回归（LR）模型获得了0.9611的最大准确率。然而，这些ML模型通常缺乏不确定性量化，这在临床决策中至关重要。因此，为了解决这一限制，在完整和简化数据集上实现了具有六种不同先验分布（包括Normal 0,1、Normal 0,10、Laplace 0,1、Cauchy 0,1、Cauchy 0,2.5和Horseshoe 1）的贝叶斯神经网络（BNN）。具有Normal 0,10先验分布的BNN模型在特征选择前后分别表现出0.9740和0.9870的最大准确率。", "summary": "本研究提出了一个用于分化型甲状腺癌（DTC）复发分类的综合框架，结合了机器学习模型和贝叶斯神经网络（BNN）。研究首先评估了多种ML模型，并通过特征选择优化性能。为解决传统ML模型缺乏不确定性量化的问题，研究引入了具有不同先验分布的BNN模型。结果显示，BNN模型，特别是Normal 0,10先验的BNN，在特征选择后实现了0.9870的最高准确率，表明其在提供准确且具有不确定性感知的临床决策支持方面的潜力。", "keywords": "分化型甲状腺癌, 机器学习, 贝叶斯神经网络, 复发分类, 不确定性量化", "comments": "本研究的创新之处在于其综合框架，结合了机器学习和贝叶斯神经网络，特别强调了不确定性量化在临床决策中的重要性。BNN的引入有效解决了传统ML模型在此方面的不足，并取得了更高的分类准确率。这对于提高甲状腺癌复发预测的可靠性和临床实用性具有重要意义。"}}
{"id": "2507.19246", "title": "Multi-Level Monte Carlo sampling with Parallel-in-Time Integration for Uncertainty Quantification in Electric Machine Simulation", "authors": ["Robert Hahn", "Sebastian Schöps"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.19246v1", "summary": "While generally considered computationally expensive, Uncertainty\nQuantification using Monte Carlo sampling remains beneficial for applications\nwith uncertainties of high dimension. As an extension of the naive Monte Carlo\nmethod, the Multi-Level Monte Carlo method reduces the overall computational\neffort, but is unable to reduce the time to solution in a sufficiently parallel\ncomputing environment. In this work, we propose a Uncertainty Quantification\nmethod combining Multi-Level Monte Carlo sampling and Parallel-in-Time\nintegration for select samples, exploiting remaining parallel computing\ncapacity to accelerate the computation. While effective at reducing the\ntime-to-solution, Parallel-in-Time integration methods greatly increase the\ntotal computational effort. We investigate the tradeoff between\ntime-to-solution and total computational effort of the combined method,\nstarting from theoretical considerations and comparing our findings to two\nnumerical examples. There, a speedup of 12 - 45% compared to Multi-Level Monte\nCarlo sampling is observed, with an increase of 15 - 18% in computational\neffort.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.19246v1", "cate": "cs.CE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "多级蒙特卡洛采样结合时间并行积分用于电机仿真中的不确定性量化", "tldr": "本文提出了一种结合多级蒙特卡洛采样和时间并行积分的不确定性量化方法，旨在利用并行计算能力加速电机仿真，并在时间和计算量之间进行权衡。", "motivation": "传统的蒙特卡洛采样在不确定性量化中计算成本高昂，而多级蒙特卡洛方法虽然降低了总计算量，但在并行计算环境下未能有效减少求解时间。因此，需要一种方法来利用剩余的并行计算能力加速计算。", "method": "本文提出了一种将多级蒙特卡洛采样与时间并行积分相结合的不确定性量化方法，用于选定的样本，以利用剩余的并行计算能力来加速计算。研究了该组合方法在求解时间和总计算量之间的权衡。", "result": "与多级蒙特卡洛采样相比，该方法观察到12-45%的速度提升，同时计算量增加了15-18%。", "conclusion": "结合多级蒙特卡洛采样和时间并行积分的方法可以有效减少求解时间，但会增加总计算量，需要在两者之间进行权衡。", "translation": "尽管通常被认为计算成本高昂，但使用蒙特卡洛采样进行不确定性量化对于高维不确定性应用仍然有益。作为朴素蒙特卡洛方法的扩展，多级蒙特卡洛方法降低了整体计算量，但无法在充分并行的计算环境中减少求解时间。在这项工作中，我们提出了一种不确定性量化方法，该方法结合了多级蒙特卡洛采样和对选定样本的时间并行积分，利用剩余的并行计算能力来加速计算。虽然时间并行积分方法在减少求解时间方面有效，但却大大增加了总计算量。我们从理论考量出发，并与两个数值示例的结果进行比较，研究了组合方法在求解时间和总计算量之间的权衡。结果显示，与多级蒙特卡洛采样相比，速度提升了12-45%，而计算量增加了15-18%。", "summary": "本文提出了一种结合多级蒙特卡洛采样和时间并行积分的新型不确定性量化方法，旨在利用并行计算资源加速电机仿真中的不确定性量化过程。该方法通过权衡求解时间和总计算量，在数值示例中实现了显著的速度提升，但伴随计算量的适度增加。", "keywords": "不确定性量化, 多级蒙特卡洛, 时间并行积分, 电机仿真, 并行计算", "comments": "该论文的创新点在于将多级蒙特卡洛采样与时间并行积分相结合，解决了传统多级蒙特卡洛方法在并行环境下无法有效缩短求解时间的问题。通过引入时间并行积分，利用了额外的并行计算能力，实现了计算加速。这种方法在需要快速不确定性量化的工程仿真中具有重要意义，尤其是在电机的复杂仿真场景中。其局限性在于增加了总计算量，如何在速度提升和资源消耗之间取得最佳平衡是未来研究的重点。"}}
{"id": "2408.06303", "title": "Long-Form Answers to Visual Questions from Blind and Low Vision People", "authors": ["Mina Huh", "Fangyuan Xu", "Yi-Hao Peng", "Chongyan Chen", "Hansika Murugu", "Danna Gurari", "Eunsol Choi", "Amy Pavel"], "categories": ["cs.CL", "cs.CV"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      COLM 2024 Oral Spotlight", "url": "http://arxiv.org/abs/2408.06303v2", "summary": "Vision language models can now generate long-form answers to questions about\nimages - long-form visual question answers (LFVQA). We contribute VizWiz-LF, a\ndataset of long-form answers to visual questions posed by blind and low vision\n(BLV) users. VizWiz-LF contains 4.2k long-form answers to 600 visual questions,\ncollected from human expert describers and six VQA models. We develop and\nannotate functional roles of sentences of LFVQA and demonstrate that long-form\nanswers contain information beyond the question answer such as explanations and\nsuggestions. We further conduct automatic and human evaluations with BLV and\nsighted people to evaluate long-form answers. BLV people perceive both\nhuman-written and generated long-form answers to be plausible, but generated\nanswers often hallucinate incorrect visual details, especially for unanswerable\nvisual questions (e.g., blurry or irrelevant images). To reduce hallucinations,\nwe evaluate the ability of VQA models to abstain from answering unanswerable\nquestions across multiple prompting strategies.", "comment": "COLM 2024 Oral Spotlight", "pdf_url": "http://arxiv.org/pdf/2408.06303v2", "cate": "cs.CL", "date": "2024-08-12", "updated": "2025-07-25", "AI": {"title_translation": "盲人和低视力人群对视觉问题的长篇回答", "tldr": "研究并构建了面向盲人和低视力人群的视觉问答长篇回答数据集VizWiz-LF，发现生成式回答存在幻觉问题，并探索了模型规避幻觉的策略。", "motivation": "视觉语言模型在生成长篇视觉问答（LFVQA）方面存在幻觉问题，尤其是在面对无法回答的视觉问题时。为了解决这一问题并更好地服务盲人和低视力（BLV）用户，需要一个专门的数据集和评估方法来研究和改进LFVQA。", "method": "研究团队构建了VizWiz-LF数据集，其中包含来自盲人和低视力用户提出的600个视觉问题的4.2k个长篇回答，这些回答由人类专家和六个VQA模型收集。他们还开发并标注了LFVQA句子中的功能角色，并进行了自动和人工评估（包括盲人、低视力人群和有视力的人群）。此外，为了减少幻觉，他们评估了VQA模型在面对无法回答的问题时，通过多种提示策略进行回答规避的能力。", "result": "盲人和低视力人群认为人类撰写和模型生成的长篇回答都具有合理性，但生成的回答常出现不正确的视觉细节幻觉，尤其是在图片模糊或不相关等无法回答的问题上。", "conclusion": "尽管模型生成的长篇视觉问答对盲人和低视力用户来说是可信的，但幻觉问题是其主要限制。未来的工作应关注减少幻觉，例如通过提升模型对无法回答问题的规避能力。", "translation": "视觉语言模型现在可以生成关于图像问题的长篇回答——即长篇视觉问答（LFVQA）。我们贡献了VizWiz-LF，这是一个包含盲人和低视力（BLV）用户提出的视觉问题的长篇回答数据集。VizWiz-LF包含4.2k个长篇回答，对应600个视觉问题，这些回答由人类专家描述者和六个VQA模型收集。我们开发并标注了LFVQA句子中的功能角色，并证明长篇回答包含超出问题答案的信息，例如解释和建议。我们进一步与盲人和低视力人群以及有视力的人群进行了自动和人工评估，以评估长篇回答。盲人和低视力人群认为人类撰写和生成的长篇回答都是合理的，但生成的回答经常产生不正确的视觉细节幻觉，特别是对于无法回答的视觉问题（例如，模糊或不相关的图像）。为了减少幻觉，我们评估了VQA模型在多种提示策略下，对无法回答的问题规避回答的能力。", "summary": "本研究引入了VizWiz-LF数据集，专注于盲人和低视力用户提出的视觉问题的长篇回答。该数据集包含由人类和模型生成的4.2k个回答。研究分析了长篇回答的结构，发现它们包含解释和建议。评估结果显示，尽管模型生成的回答对盲人和低视力用户具有合理性，但其主要缺陷是存在不正确的视觉细节幻觉，尤其是在面对无法回答的问题时。为解决此问题，论文还探索了模型规避回答无法回答问题的能力。", "keywords": "长篇视觉问答,盲人和低视力,数据集,幻觉,VizWiz-LF", "comments": "这项工作通过创建VizWiz-LF数据集，为研究和改进服务于盲人和低视力人群的长篇视觉问答（LFVQA）领域做出了重要贡献。其创新之处在于专注于BLV用户的特定需求，并揭示了模型生成LFVQA中的幻觉问题，这对于提升VQA系统的实用性和可靠性至关重要。该研究通过详细的评估和对规避策略的探索，为未来开发更鲁棒的VQA模型提供了宝贵的见解。"}}
{"id": "2507.18988", "title": "AEDR: Training-Free AI-Generated Image Attribution via Autoencoder Double-Reconstruction", "authors": ["Chao Wang", "Kejiang Chen", "Zijin Yang", "Yaofei Wang", "Weiming Zhang"], "categories": ["cs.CV", "cs.CR", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18988v1", "summary": "The rapid advancement of image-generation technologies has made it possible\nfor anyone to create photorealistic images using generative models, raising\nsignificant security concerns. To mitigate malicious use, tracing the origin of\nsuch images is essential. Reconstruction-based attribution methods offer a\npromising solution, but they often suffer from reduced accuracy and high\ncomputational costs when applied to state-of-the-art (SOTA) models. To address\nthese challenges, we propose AEDR (AutoEncoder Double-Reconstruction), a novel\ntraining-free attribution method designed for generative models with continuous\nautoencoders. Unlike existing reconstruction-based approaches that rely on the\nvalue of a single reconstruction loss, AEDR performs two consecutive\nreconstructions using the model's autoencoder, and adopts the ratio of these\ntwo reconstruction losses as the attribution signal. This signal is further\ncalibrated using the image homogeneity metric to improve accuracy, which\ninherently cancels out absolute biases caused by image complexity, with\nautoencoder-based reconstruction ensuring superior computational efficiency.\nExperiments on eight top latent diffusion models show that AEDR achieves 25.5%\nhigher attribution accuracy than existing reconstruction-based methods, while\nrequiring only 1% of the computational time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18988v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "AEDR：通过自编码器双重重建实现免训练的AI生成图像归因", "tldr": "AEDR是一种免训练的AI生成图像归因方法，通过自编码器的两次重建损失比率来识别图像来源，比现有方法更准确、计算效率更高。", "motivation": "随着图像生成技术的快速发展，AI生成图像的溯源变得至关重要，以应对潜在的安全问题。现有的基于重建的归因方法在处理最先进的模型时，往往面临准确性降低和计算成本高昂的挑战。", "method": "AEDR（自编码器双重重建）是一种新颖的、免训练的归因方法，专为具有连续自编码器的生成模型设计。它通过模型的自编码器执行两次连续重建，并采用两次重建损失的比率作为归因信号。该信号进一步利用图像同质性指标进行校准，以提高准确性并消除图像复杂性引起的绝对偏差，同时确保卓越的计算效率。", "result": "在八个顶级潜在扩散模型上的实验表明，AEDR的归因准确率比现有基于重建的方法高出25.5%，而计算时间仅为后者的1%。", "conclusion": "AEDR通过其独特的双重重建和损失比率机制，显著提高了AI生成图像归因的准确性和计算效率，有效解决了现有方法的局局限性。", "translation": "图像生成技术的快速发展使得任何人都可以使用生成模型创建逼真的图像，这引发了重大的安全担忧。为了减轻恶意使用，追溯此类图像的来源至关重要。基于重建的归因方法提供了一种有前景的解决方案，但当应用于最先进（SOTA）模型时，它们通常会遇到准确性降低和计算成本高昂的问题。为了应对这些挑战，我们提出了AEDR（自编码器双重重建），一种新颖的免训练归因方法，专为具有连续自编码器的生成模型设计。与现有依赖单一重建损失值的基于重建的方法不同，AEDR使用模型的自编码器执行两次连续重建，并采用这两次重建损失的比率作为归因信号。该信号通过图像同质性指标进一步校准以提高准确性，这本质上消除了由图像复杂性引起的绝对偏差，同时基于自编码器的重建确保了卓越的计算效率。在八个顶级潜在扩散模型上的实验表明，AEDR的归因准确率比现有基于重建的方法高出25.5%，而计算时间仅为后者的1%。", "summary": "本文提出了一种名为AEDR（自编码器双重重建）的免训练归因方法，用于追溯AI生成图像的来源。针对现有基于重建方法在准确性和计算成本上的不足，AEDR利用生成模型中自编码器的两次连续重建损失比率作为归因信号，并结合图像同质性进行校准。实验结果显示，AEDR在准确性上比现有方法提升25.5%，计算效率提高99%。", "keywords": "AI生成图像归因, 自编码器, 双重重建, 免训练, 计算效率", "comments": "AEDR的创新之处在于其“免训练”特性、独特的“双重重建”机制以及利用“损失比率”作为归因信号，这显著提升了归因的准确性和计算效率，使其在处理先进的生成模型时更具实用性。其通过图像同质性校准来消除偏差的策略也十分巧妙。"}}
{"id": "2507.19026", "title": "RhythmTA: A Visual-Aided Interactive System for ESL Rhythm Training via Dubbing Practice", "authors": ["Chang Chen", "Sicheng Song", "Shuchang Xu", "Zhicheng Li", "Huamin Qu", "Yanna Lin"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19026v1", "summary": "English speech rhythm, the temporal patterns of stressed syllables, is\nessential for English as a second language (ESL) learners to produce\nnatural-sounding and comprehensible speech. Rhythm training is generally based\non imitation of native speech. However, it relies heavily on external\ninstructor feedback, preventing ESL learners from independent practice. To\naddress this gap, we present RhythmTA, an interactive system for ESL learners\nto practice speech rhythm independently via dubbing, an imitation-based\napproach. The system automatically extracts rhythm from any English speech and\nintroduces novel visual designs to support three stages of dubbing practice:\n(1) Synchronized listening with visual aids to enhance perception, (2) Guided\nrepeating by visual cues for self-adjustment, and (3) Comparative reflection\nfrom a parallel view for self-monitoring. Our design is informed by a formative\nstudy with nine spoken English instructors, which identified current practices\nand challenges. A user study with twelve ESL learners demonstrates that\nRhythmTA effectively enhances learners' rhythm perception and shows significant\npotential for improving rhythm production.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19026v1", "cate": "cs.HC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "节奏TA：一个基于配音练习的视觉辅助互动系统，用于ESL节奏训练", "tldr": "RhythmTA是一个视觉辅助互动系统，帮助ESL学习者通过配音练习独立进行英语语音节奏训练。", "motivation": "ESL学习者的英语语音节奏训练通常依赖外部教师反馈，阻碍了独立练习。本研究旨在解决这一问题。", "method": "提出RhythmTA系统，通过配音帮助ESL学习者独立练习语音节奏。系统自动提取英语语音节奏，并引入新颖的视觉设计支持配音练习的三个阶段：同步听觉（视觉辅助）、引导重复（视觉提示）、比较反思（平行视图。设计由九位口语教师的形成性研究指导。", "result": "对十二名ESL学习者的用户研究表明，RhythmTA有效提高了学习者的节奏感知能力，并显示出改善节奏发音的巨大潜力。", "conclusion": "RhythmTA系统能够有效帮助ESL学习者独立进行英语语音节奏训练，提高节奏感知和发音能力。", "translation": "英语语音节奏，即重读音节的时间模式，对于英语作为第二语言（ESL）学习者来说，是发出自然且可理解语音的关键。节奏训练通常基于模仿母语者语音。然而，它严重依赖外部教师反馈，阻碍了ESL学习者的独立练习。为了解决这一空白，我们提出了RhythmTA，一个互动系统，供ESL学习者通过配音这种模仿方法独立练习语音节奏。该系统自动从任何英语语音中提取节奏，并引入新颖的视觉设计来支持配音练习的三个阶段：（1）带有视觉辅助的同步听力以增强感知，（2）通过视觉提示引导重复以进行自我调整，（3）通过平行视图进行比较反思以进行自我监控。我们的设计是基于对九位英语口语教师的形成性研究，该研究确定了当前的实践和挑战。对十二名ESL学习者的用户研究表明，RhythmTA有效增强了学习者的节奏感知能力，并显示出改善节奏发音的巨大潜力。", "summary": "RhythmTA是一个为ESL学习者设计的视觉辅助互动系统，旨在解决传统节奏训练对教师反馈的过度依赖问题。该系统通过自动提取语音节奏并提供创新的视觉辅助，支持配音练习的三个阶段：感知增强、自我调整和自我监控。一项用户研究表明，RhythmTA有效提升了学习者的节奏感知，并展现出改善节奏发音的潜力，为ESL学习者提供了独立的节奏训练途径。", "keywords": "英语语音节奏, ESL学习, 配音练习, 视觉辅助系统, 独立学习", "comments": "该论文提出了一种创新的视觉辅助互动系统RhythmTA，通过配音练习解决了ESL学习者独立进行英语语音节奏训练的难题。其亮点在于将复杂的语音节奏模式可视化，并设计了循序渐进的练习阶段，有效弥补了传统方法中教师反馈的不足。系统经过形成性研究和用户研究验证，显示出良好的效果和应用潜力，为ESL语音教学提供了一个有价值的工具。"}}
{"id": "2507.19070", "title": "Computing, Complexity and Degrowth : Systemic Considerations for Digital De-escalation", "authors": ["Valentin Girard", "Maud Rio", "Romain Couillet"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19070v1", "summary": "Research on digital degrowth predominantly critiques digital expansion or\npresents alternative digital practices. Yet, analyzing the link between digital\ntechnologies and complexity is crucial to overcome systemic obstacles hindering\ndigital de-escalation. This article presents the different types of links\nbetween complexity and computing observed in the literature: the\ninfrastructural complexity inherent in digital technologies, the\nsocio-political complexity induced by them, and finally, the ontological\ncomplexity (individual's ways of relating to their environment) hindered by\ndigitization. The paper explores these links to identify ways to reduce\ninfrastructural and socio-political complexities, and to move away from the\nreductionist paradigm, in order to support digital degrowth. Its development\nshows that complexity induces ratchet effects (i.e. irreversibilities in the\ndevelopment of a technique in a society), rendering degrowth efforts difficult\nto handle by individuals. Therefore, strategies to overcome these barriers are\nproposed, suggesting that bottom-up simplification approaches stand a greater\nchance of making alternatives emerge from different stakeholders (including\nusers). This digital shift assumes the development of methods and technical\ntools that enable individuals to disengage from their attachments to digital\nhabits and infrastructure, opening a substantial field of study.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19070v1", "cate": "cs.CY", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "计算、复杂性与去增长：数字降级化的系统性考量", "tldr": "研究数字去增长，发现数字技术与复杂性（基础设施、社会政治、本体论）之间存在棘轮效应，阻碍了去增长。文章提出自下而上的简化策略，以帮助个人摆脱数字依赖。", "motivation": "现有数字去增长研究主要批评数字扩张或提出替代实践。然而，分析数字技术与复杂性之间的联系对于克服阻碍数字降级化的系统性障碍至关重要。", "method": "文章呈现了文献中观察到的计算与复杂性之间的不同类型联系：数字技术固有的基础设施复杂性、由其引起的社会政治复杂性，以及被数字化阻碍的本体论复杂性（个体与环境的关系方式）。论文探索这些联系，以识别减少基础设施和社会政治复杂性的方法，并摆脱还原论范式，从而支持数字去增长。", "result": "研究表明，复杂性会引发棘轮效应（即技术发展在社会中的不可逆性），这使得去增长的努力对个体而言难以处理。", "conclusion": "因此，提出了克服这些障碍的策略，表明自下而上的简化方法更有可能促使不同利益相关者（包括用户）提出替代方案。这种数字转型需要开发能够帮助个人摆脱对数字习惯和基础设施依赖的方法和技术工具，这开启了一个重要的研究领域。", "translation": "关于数字去增长的研究主要批判数字扩张或提出替代性数字实践。然而，分析数字技术与复杂性之间的联系对于克服阻碍数字降级化的系统性障碍至关重要。本文介绍了文献中观察到的复杂性与计算之间不同类型的联系：数字技术固有的基础设施复杂性、由其引起的社会政治复杂性，以及最终被数字化阻碍的本体论复杂性（个体与环境的关系方式）。论文探讨这些联系，以识别减少基础设施和社会政治复杂性的方法，并摆脱还原论范式，从而支持数字去增长。其发展表明，复杂性会引发棘轮效应（即一项技术在社会发展中的不可逆性），使得去增长的努力对个体而言难以处理。因此，提出了克服这些障碍的策略，表明自下而上的简化方法更有可能促使不同利益相关者（包括用户）提出替代方案。这种数字转型假设需要开发能够帮助个人摆脱对数字习惯和基础设施依赖的方法和技术工具，这开启了一个重要的研究领域。", "summary": "该论文探讨了数字技术与复杂性之间的关键联系，以支持数字去增长。它识别了三种复杂性：基础设施、社会政治和本体论复杂性。研究发现，这些复杂性导致“棘轮效应”，使数字去增长难以实现。为应对此挑战，文章提出了自下而上的简化策略，旨在帮助个人摆脱数字依赖，并促进替代方案的出现。", "keywords": "数字去增长, 复杂性, 棘轮效应, 数字降级化, 系统性考量", "comments": "这篇论文的创新之处在于它将“复杂性”作为理解和推动“数字去增长”的核心视角，特别是提出了“棘轮效应”这一概念来解释去增长的困难。其重要性在于为数字去增长领域提供了新的理论框架和实践方向，强调了自下而上的简化方法以及开发工具以帮助个人摆脱数字依赖的必要性。"}}
{"id": "2507.07087", "title": "Incremental Averaging Method to Improve Graph-Based Time-Difference-of-Arrival Estimation", "authors": ["Klaus Brümann", "Kouei Yamaoka", "Nobutaka Ono", "Simon Doclo"], "categories": ["eess.AS", "eess.SP"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      This paper was accepted for presentation at the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) 2025", "url": "http://arxiv.org/abs/2507.07087v3", "summary": "Estimating the position of a speech source based on\ntime-differences-of-arrival (TDOAs) is often adversely affected by background\nnoise and reverberation. A popular method to estimate the TDOA between a\nmicrophone pair involves maximizing a generalized cross-correlation with phase\ntransform (GCC-PHAT) function. Since the TDOAs across different microphone\npairs satisfy consistency relations, generally only a small subset of\nmicrophone pairs are used for source position estimation. Although the set of\nmicrophone pairs is often determined based on a reference microphone, recently\na more robust method has been proposed to determine the set of microphone pairs\nby computing the minimum spanning tree (MST) of a signal graph of GCC-PHAT\nfunction reliabilities. To reduce the influence of noise and reverberation on\nthe TDOA estimation accuracy, in this paper we propose to compute the GCC-PHAT\nfunctions of the MST based on an average of multiple cross-power spectral\ndensities (CPSDs) using an incremental method. In each step of the method, we\nincrease the number of CPSDs over which we average by considering CPSDs\ncomputed indirectly via other microphones from previous steps. Using signals\nrecorded in a noisy and reverberant laboratory with an array of spatially\ndistributed microphones, the performance of the proposed method is evaluated in\nterms of TDOA estimation error and 2D source position estimation error.\nExperimental results for different source and microphone configurations and\nthree reverberation conditions show that the proposed method considering\nmultiple CPSDs improves the TDOA estimation and source position estimation\naccuracy compared to the reference microphone- and MST-based methods that rely\non a single CPSD as well as steered-response power-based source position\nestimation.", "comment": "This paper was accepted for presentation at the IEEE Workshop on\n  Applications of Signal Processing to Audio and Acoustics (WASPAA) 2025", "pdf_url": "http://arxiv.org/pdf/2507.07087v3", "cate": "eess.AS", "date": "2025-07-09", "updated": "2025-07-25", "AI": {"title_translation": "增量平均法改进基于图的时差估计", "tldr": "本文提出一种增量平均方法，通过平均多个交叉功率谱密度来改进基于图的时差估计，有效降低噪声和混响对语音源定位的影响，提高定位精度。", "motivation": "时差（TDOA）估计在语音源定位中常受到背景噪声和混响的不利影响。现有方法（如基于参考麦克风或最小生成树（MST）的方法）通常依赖于单个交叉功率谱密度（CPSD），这限制了其在噪声和混响环境下的精度。", "method": "本文提出一种增量平均方法来计算最小生成树（MST）的广义互相关-相位变换（GCC-PHAT）函数。该方法通过逐步增加用于平均的交叉功率谱密度（CPSD）数量来实现，其中考虑了通过其他麦克风间接计算的CPSD。", "result": "实验结果表明，与依赖单个CPSD的参考麦克风和MST基方法以及基于导向响应功率的源定位方法相比，所提出的考虑多个CPSD的增量平均方法显著提高了时差（TDOA）估计和声源位置估计的精度。", "conclusion": "通过对多个交叉功率谱密度进行增量平均，可以有效提高基于图的时差估计方法的鲁棒性和精度，从而在噪声和混响环境下实现更准确的语音源定位。", "translation": "基于时差（TDOA）的语音源位置估计常受到背景噪声和混响的不利影响。一种流行的麦克风对之间TDOA估计方法是最大化广义互相关-相位变换（GCC-PHAT）函数。由于不同麦克风对之间的TDOA满足一致性关系，通常只使用一小部分麦克风对进行声源位置估计。尽管麦克风对的集合通常基于参考麦克风确定，但最近提出了一种更鲁棒的方法，通过计算GCC-PHAT函数可靠性信号图的最小生成树（MST）来确定麦克风对集合。为了减少噪声和混响对TDOA估计精度的影响，本文提出通过增量方法对多个交叉功率谱密度（CPSD）进行平均，从而计算MST的GCC-PHAT函数。在该方法的每一步中，我们通过考虑前一步中通过其他麦克风间接计算的CPSD来增加平均的CPSD数量。使用在嘈杂混响实验室中通过空间分布的麦克风阵列记录的信号，从TDOA估计误差和二维声源位置估计误差方面评估了所提出方法的性能。针对不同声源和麦克风配置以及三种混响条件进行的实验结果表明，与依赖单个CPSD的参考麦克风和MST基方法以及基于导向响应功率的声源位置估计方法相比，所提出的考虑多个CPSD的方法改进了TDOA估计和声源位置估计的精度。", "summary": "本文提出一种改进基于图的时差（TDOA）估计的增量平均方法，旨在减少背景噪声和混响对语音源定位的影响。该方法通过对最小生成树（MST）的广义互相关-相位变换（GCC-PHAT）函数进行计算时，采用增量方式平均多个交叉功率谱密度（CPSD），包括间接计算的CPSD。实验结果表明，该方法在TDOA和声源位置估计精度上优于现有的基于单个CPSD的参考麦克风和MST方法以及基于导向响应功率的方法。", "keywords": "时差估计, 增量平均, 广义互相关, 最小生成树, 语音源定位", "comments": "该论文的创新点在于将增量平均策略应用于基于图的TDOA估计中，通过整合多个交叉功率谱密度（CPSD），包括间接计算的CPSD，有效提升了在噪声和混响环境下的定位精度。这种方法增强了GCC-PHAT函数的鲁棒性，对于实际应用中的语音源定位具有重要意义。"}}
{"id": "2507.18767", "title": "Early State Exclusion in 7-Qubit Spin Chains", "authors": ["Mia Gabriella Escobar", "Valentin Garcia", "Anastasiia Minenkova"], "categories": ["quant-ph", "cs.NA", "math.NA", "15A29, 81P45, 47B36, 33C45"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      11 pages, 8 figures", "url": "http://arxiv.org/abs/2507.18767v1", "summary": "The existence of infinite families of $N \\times N$ Jacobi matrices\nrepresenting the Hamiltonians of quantum spin chains with and without early\nstate exclusion (ESE) has been shown to exist for any even $N \\geq 4$. However,\ntheir existence for odd $N \\geq 7$ has remained an open problem. In Section 3,\nwe consider a chain of qubits experiencing nearest-neighbor interactions with\nenvironmental effects and present infinite families of $7 \\times 7$ Jacobi\nmatrices with and without ESE.", "comment": "11 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.18767v1", "cate": "quant-ph", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "7量子比特自旋链中的早期状态排除", "tldr": "该论文解决了7量子比特自旋链中具有和不具有早期状态排除的哈密顿量雅可比矩阵的存在性问题，通过提出无限族7x7雅可比矩阵来证明其存在。", "motivation": "对于偶数$N \\geq 4$，已证明存在表示具有和不具有早期状态排除（ESE）的量子自旋链哈密顿量的$N \\times N$雅可比矩阵，但对于奇数$N \\geq 7$，它们的存在性仍然是一个悬而未决的问题。", "method": "论文考虑了一个经历最近邻相互作用并受环境影响的量子比特链，并提出了无限族的$7 \\times 7$雅可比矩阵。", "result": "论文提出了无限族的$7 \\times 7$雅可比矩阵，这些矩阵具有或不具有早期状态排除（ESE），从而证明了7量子比特自旋链中此类哈密顿量的存在性。", "conclusion": "本研究成功为奇数$N=7$的量子自旋链建立了无限族具有和不具有早期状态排除的$7 \\times 7$雅可比矩阵的存在性，解决了此前的开放问题。", "translation": "对于任意偶数$N \\geq 4$，已证明存在表示具有和不具有早期状态排除（ESE）的量子自旋链哈密顿量的无限族$N \\times N$雅可比矩阵。然而，对于奇数$N \\geq 7$，它们的存在性仍然是一个悬而未决的问题。在第3节中，我们考虑了一个经历最近邻相互作用并受环境影响的量子比特链，并提出了无限族的$7 \\times 7$雅可比矩阵，这些矩阵具有或不具有ESE。", "summary": "本论文解决了奇数$N \\geq 7$的量子自旋链中具有和不具有早期状态排除（ESE）的哈密顿量雅可比矩阵的存在性这一开放问题。具体针对$N=7$的情况，作者考虑了一个具有最近邻相互作用和环境效应的7量子比特链，成功地提出了无限族的$7 \\times 7$雅可比矩阵，证明了此类哈密顿量的存在。", "keywords": "早期状态排除, 自旋链, 雅可比矩阵, 量子比特, 哈密顿量", "comments": "该论文通过解决一个长期存在的关于量子自旋链中特定哈密顿量存在性的开放问题，特别是针对7量子比特系统，做出了重要贡献。引入无限族雅可比矩阵为理解这些系统提供了具体的数学框架。"}}
{"id": "2507.19219", "title": "How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework", "authors": ["Zi Liang", "Liantong Yu", "Shiyu Zhang", "Qingqing Ye", "Haibo Hu"], "categories": ["cs.CL", "cs.CR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Source code: this https URL Website: this https URL", "url": "http://arxiv.org/abs/2507.19219v1", "summary": "Overestimation in evaluating large language models (LLMs) has become an\nincreasing concern. Due to the contamination of public benchmarks or imbalanced\nmodel training, LLMs may achieve unreal evaluation results on public\nbenchmarks, either intentionally or unintentionally, which leads to unfair\ncomparisons among LLMs and undermines their realistic capability assessments.\nExisting benchmarks attempt to address these issues by keeping test cases\npermanently secret, mitigating contamination through human evaluation, or\nrepeatedly collecting and constructing new samples. However, these approaches\nfail to ensure reproducibility, transparency, and high efficiency\nsimultaneously. Moreover, the extent of overestimation in current LLMs remains\nunquantified. To address these issues, we propose ArxivRoll, a dynamic\nevaluation framework inspired by one-time pad encryption in cryptography.\nArxivRoll comprises two key components: \\emph{i) SCP (Sequencing, Cloze, and\nPrediction)}, an automated generator for private test cases, and \\emph{ii)\nRugged Scores (RS)}, metrics that measure the proportion of public benchmark\ncontamination and training bias. Leveraging SCP, ArxivRoll constructs a new\nbenchmark every six months using recent articles from ArXiv and employs them\nfor one-time evaluations of LLM performance. Extensive experiments demonstrate\nthe high quality of our benchmark, and we provide a systematic evaluation of\ncurrent LLMs. The source code is available at\nhttps://github.com/liangzid/ArxivRoll/.", "comment": "Source code: https://github.com/liangzid/ArxivRoll/ Website:\n  https://arxivroll.moreoverai.com/", "pdf_url": "http://arxiv.org/pdf/2507.19219v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "大型语言模型在评估中“作弊”了多少？基于一次性密码本框架的过高估计基准测试", "tldr": "该研究提出了ArxivRoll，一个动态评估框架，用于量化大型语言模型在评估中的过高估计，并通过自动生成私有测试用例来解决基准污染和训练偏差问题。", "motivation": "现有的大型语言模型（LLMs）评估存在过高估计的问题，这源于公共基准的污染或不平衡的模型训练，导致不公平的比较和不准确的实际能力评估。现有方法无法同时确保可复现性、透明度和高效率，且当前LLMs的过高估计程度尚未量化。", "method": "本研究提出了ArxivRoll，一个受一次性密码本加密启发的动态评估框架。ArxivRoll包含两个关键组件：i) SCP（Sequencing, Cloze, and Prediction），一个用于生成私有测试用例的自动化生成器；ii) Rugged Scores (RS)，用于衡量公共基准污染和训练偏差比例的指标。ArxivRoll利用SCP每六个月从ArXiv最新文章中构建新的基准，并用于LLM性能的一次性评估。", "result": "广泛的实验证明了所构建基准的高质量，并且提供了一个对当前LLMs的系统性评估。", "conclusion": "ArxivRoll框架有效地解决了LLM评估中过高估计的问题，通过动态生成私有测试用例和量化污染/偏差，确保了评估的公平性、可复现性、透明度和效率。", "translation": "大型语言模型（LLMs）评估中的过高估计已成为日益增长的关注点。由于公共基准的污染或不平衡的模型训练，LLMs可能在公共基准上获得不真实的评估结果，无论是故意的还是无意的，这导致了LLMs之间的不公平比较，并损害了它们实际能力的评估。现有基准试图通过永久保密测试用例、通过人工评估减轻污染或重复收集和构建新样本来解决这些问题。然而，这些方法未能同时确保可复现性、透明度和高效率。此外，当前LLMs中过高估计的程度仍未被量化。为了解决这些问题，我们提出了ArxivRoll，一个受密码学中一次性密码本加密启发的动态评估框架。ArxivRoll包含两个关键组件：i) SCP（排序、完形填空和预测），一个用于私有测试用例的自动化生成器；ii) Rugged Scores (RS)，衡量公共基准污染和训练偏差比例的指标。利用SCP，ArxivRoll每六个月使用ArXiv上的最新文章构建一个新的基准，并将其用于LLM性能的一次性评估。广泛的实验证明了我们基准的高质量，并且我们提供了对当前LLMs的系统性评估。源代码可在https://github.com/liangzid/ArxivRoll/获取。", "summary": "本论文关注大型语言模型（LLM）评估中日益严重的过高估计问题，该问题源于基准污染和训练偏差，导致评估结果不真实且比较不公平。为解决现有方法在可复现性、透明度和效率方面的不足，并量化过高估计程度，作者提出了ArxivRoll框架。ArxivRoll包含SCP（自动生成私有测试用例）和Rugged Scores（衡量污染和偏差）。该框架每六个月利用ArXiv最新文章构建新基准进行一次性评估。实验证明了其基准的高质量，并对当前LLMs进行了系统评估。", "keywords": "大型语言模型, 评估, 过高估计, 基准测试, 一次性密码本, ArxivRoll", "comments": "该论文提出了一种新颖且实用的动态评估框架ArxivRoll，通过借鉴密码学中的一次性密码本概念来解决LLM评估中的核心痛点——基准污染和训练偏差。其创新点在于SCP组件能够自动化生成私有、动态的测试用例，有效规避了传统静态基准的弊端。Rugged Scores的引入也为量化过高估计提供了具体指标。该方法有望提高LLM评估的公平性、透明度和可信度，对LLM领域的发展具有重要意义。"}}
{"id": "2507.18741", "title": "KuiSCIMA v2.0: Improved Baselines, Calibration, and Cross-Notation Generalization for Historical Chinese Music Notations in Jiang Kui's Baishidaoren Gequ", "authors": ["Tristan Repolusk", "Eduardo Veas"], "categories": ["cs.CV", "cs.DL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      International Conference on Document Analysis and Recognition. This preprint has not undergone any post-submission improvements or corrections. The Version of Record of this contribution is published in \"19th International Conference on Document Analysis and Recognition (ICDAR 2025), Wuhan, China, September 16-21, 2025, Proceedings\", and is available online at the External DOI field below", "url": "http://arxiv.org/abs/2507.18741v1", "summary": "Optical Music Recognition (OMR) for historical Chinese musical notations,\nsuch as suzipu and l\\\"ul\\\"upu, presents unique challenges due to high class\nimbalance and limited training data. This paper introduces significant\nadvancements in OMR for Jiang Kui's influential collection Baishidaoren Gequ\nfrom 1202. In this work, we develop and evaluate a character recognition model\nfor scarce imbalanced data. We improve upon previous baselines by reducing the\nCharacter Error Rate (CER) from 10.4% to 7.1% for suzipu, despite working with\n77 highly imbalanced classes, and achieve a remarkable CER of 0.9% for\nl\\\"ul\\\"upu. Our models outperform human transcribers, with an average human CER\nof 15.9% and a best-case CER of 7.6%. We employ temperature scaling to achieve\na well-calibrated model with an Expected Calibration Error (ECE) below 0.0162.\nUsing a leave-one-edition-out cross-validation approach, we ensure robust\nperformance across five historical editions. Additionally, we extend the\nKuiSCIMA dataset to include all 109 pieces from Baishidaoren Gequ, encompassing\nsuzipu, l\\\"ul\\\"upu, and jianzipu notations. Our findings advance the\ndigitization and accessibility of historical Chinese music, promoting cultural\ndiversity in OMR and expanding its applicability to underrepresented music\ntraditions.", "comment": "International Conference on Document Analysis and Recognition. This\n  preprint has not undergone any post-submission improvements or corrections.\n  The Version of Record of this contribution is published in \"19th\n  International Conference on Document Analysis and Recognition (ICDAR 2025),\n  Wuhan, China, September 16-21, 2025, Proceedings\", and is available online at\n  the External DOI field below", "pdf_url": "http://arxiv.org/pdf/2507.18741v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "KuiSCIMA v2.0：姜夔《白石道人歌曲》历史中文乐谱的改进基线、校准和跨乐谱泛化", "tldr": "改进了针对姜夔《白石道人歌曲》中历史中文乐谱（如俗字谱和律吕谱）的光学音乐识别（OMR）模型，显著降低了字符错误率，并优于人类转录者。", "motivation": "历史中文乐谱（如俗字谱和律吕谱）的光学音乐识别（OMR）由于高类别不平衡和训练数据有限而面临独特的挑战，阻碍了历史中文音乐的数字化和可访问性。", "method": "开发并评估了用于稀缺不平衡数据的字符识别模型。采用温度标定实现模型校准，并使用留一版交叉验证确保跨历史版本的鲁棒性。此外，扩展了KuiSCIMA数据集，包含了《白石道人歌曲》中的所有109首曲目，包括俗字谱、律吕谱和减字谱。", "result": "将俗字谱的字符错误率（CER）从10.4%降至7.1%，律吕谱的CER达到0.9%。模型表现优于人类转录者，人类平均CER为15.9%，最佳情况为7.6%。通过温度标定实现了预期校准误差（ECE）低于0.0162的良好校准模型，并在五种历史版本中表现稳健。", "conclusion": "本研究的发现促进了历史中文音乐的数字化和可访问性，提升了OMR在文化多样性方面的应用，并将其适用范围扩展到代表性不足的音乐传统。", "translation": "历史中文乐谱（如俗字谱和律吕谱）的光学音乐识别（OMR）由于高类别不平衡和有限的训练数据而面临独特的挑战。本文介绍了姜夔1202年有影响力的作品集《白石道人歌曲》OMR的重大进展。在这项工作中，我们开发并评估了一种针对稀缺不平衡数据的字符识别模型。我们改进了之前的基线，将俗字谱的字符错误率（CER）从10.4%降低到7.1%，尽管处理的是77个高度不平衡的类别，并且律吕谱的CER达到了0.9%的显著成果。我们的模型表现优于人类转录者，人类平均CER为15.9%，最佳情况为7.6%。我们采用温度标定来实现预期校准误差（ECE）低于0.0162的良好校准模型。通过采用留一版交叉验证方法，我们确保了在五种历史版本中的稳健性能。此外，我们将KuiSCIMA数据集扩展到包含《白石道人歌曲》中的所有109首曲目，包括俗字谱、律吕谱和减字谱。我们的研究结果推动了历史中文音乐的数字化和可访问性，促进了OMR在文化多样性方面的应用，并将其适用范围扩展到代表性不足的音乐传统。", "summary": "本文介绍了KuiSCIMA v2.0，一个针对姜夔《白石道人歌曲》中历史中文乐谱（包括俗字谱、律吕谱和减字谱）的光学音乐识别（OMR）系统。该系统通过开发针对稀缺不平衡数据的字符识别模型，显著提升了识别性能，俗字谱CER降至7.1%，律吕谱CER降至0.9%，且性能优于人类转录者。研究还通过温度标定实现了模型校准，并采用留一版交叉验证确保了跨版本的鲁棒性。扩展后的KuiSCIMA数据集包含了《白石道人歌曲》的全部内容。这些进展有助于历史中文音乐的数字化和可访问性。", "keywords": "光学音乐识别, 历史中文乐谱, 字符错误率, 姜夔, 白石道人歌曲", "comments": "本文在解决历史中文乐谱OMR面临的高类别不平衡和数据稀缺问题上取得了显著进展。其创新点在于开发了针对这类数据的字符识别模型，并通过温度标定和交叉验证提高了模型的鲁棒性和可靠性。性能超越人类转录者是其重要亮点，对于推动文化遗产数字化具有重要意义。扩展数据集也为未来的研究奠定了基础。"}}
{"id": "2507.19201", "title": "Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model", "authors": ["Xin Li", "Kaixiang Yang", "Qiang Li", "Zhiwei Wang"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted, ACM Multimedia 2025, 10 pages, 5 figures", "url": "http://arxiv.org/abs/2507.19201v1", "summary": "Mammography is the most commonly used imaging modality for breast cancer\nscreening, driving an increasing demand for deep-learning techniques to support\nlarge-scale analysis. However, the development of accurate and robust methods\nis often limited by insufficient data availability and a lack of diversity in\nlesion characteristics. While generative models offer a promising solution for\ndata synthesis, current approaches often fail to adequately emphasize\nlesion-specific features and their relationships with surrounding tissues. In\nthis paper, we propose Gated Conditional Diffusion Model (GCDM), a novel\nframework designed to jointly synthesize holistic mammogram images and\nlocalized lesions. GCDM is built upon a latent denoising diffusion framework,\nwhere the noised latent image is concatenated with a soft mask embedding that\nrepresents breast, lesion, and their transitional regions, ensuring anatomical\ncoherence between them during the denoising process. To further emphasize\nlesion-specific features, GCDM incorporates a gated conditioning branch that\nguides the denoising process by dynamically selecting and fusing the most\nrelevant radiomic and geometric properties of lesions, effectively capturing\ntheir interplay. Experimental results demonstrate that GCDM achieves precise\ncontrol over small lesion areas while enhancing the realism and diversity of\nsynthesized mammograms. These advancements position GCDM as a promising tool\nfor clinical applications in mammogram synthesis. Our code is available at\nhttps://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/", "comment": "Accepted, ACM Multimedia 2025, 10 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.19201v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于门控条件扩散模型的联合整体与病灶可控乳腺X线图像合成", "tldr": "本文提出了一种门控条件扩散模型（GCDM），用于联合合成整体乳腺X线图像和局部病灶，解决了数据不足和病灶特征多样性不足的问题，并实现了对小病灶区域的精确控制。", "motivation": "乳腺癌筛查对深度学习技术的需求日益增长，但准确和鲁棒方法的开发受限于数据不足和病灶特征多样性缺乏。现有生成模型在数据合成时未能充分强调病灶特异性特征及其与周围组织的关系。", "method": "本文提出了门控条件扩散模型（GCDM），一个新颖的框架，用于联合合成整体乳腺X线图像和局部病灶。GCDM基于潜在去噪扩散框架，将噪声潜在图像与表示乳腺、病灶及其过渡区域的软掩码嵌入连接，确保去噪过程中的解剖连贯性。为进一步强调病灶特异性特征，GCDM引入了一个门控条件分支，通过动态选择和融合病灶最相关的放射组学和几何属性来指导去噪过程，有效捕捉它们的相互作用。", "result": "实验结果表明，GCDM实现了对小病灶区域的精确控制，同时增强了合成乳腺X线图像的真实性和多样性。", "conclusion": "GCDM的进步使其成为乳腺X线图像合成临床应用中一个有前景的工具。", "translation": "乳腺X线摄影是乳腺癌筛查最常用的成像方式，推动了对支持大规模分析的深度学习技术日益增长的需求。然而，准确和鲁棒方法的开发往往受限于数据可用性不足和病灶特征缺乏多样性。尽管生成模型为数据合成提供了有前景的解决方案，但当前方法往往未能充分强调病灶特异性特征及其与周围组织的关系。在本文中，我们提出了一种门控条件扩散模型（GCDM），这是一个旨在联合合成整体乳腺X线图像和局部病灶的新颖框架。GCDM建立在潜在去噪扩散框架之上，其中噪声潜在图像与表示乳腺、病灶及其过渡区域的软掩码嵌入连接，确保在去噪过程中它们之间的解剖学一致性。为了进一步强调病灶特异性特征，GCDM整合了一个门控条件分支，通过动态选择和融合病灶最相关的放射组学和几何属性来指导去噪过程，有效捕捉它们的相互作用。实验结果表明，GCDM实现了对小病灶区域的精确控制，同时增强了合成乳腺X线图像的真实性和多样性。这些进展使GCDM成为乳腺X线图像合成临床应用中一个有前景的工具。我们的代码可在https://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/获取。", "summary": "本文提出了一种名为门控条件扩散模型（GCDM）的新型框架，旨在解决乳腺X线摄影数据稀缺和病灶多样性不足的问题。GCDM基于潜在去噪扩散模型，通过结合软掩码嵌入确保图像的解剖连贯性，并引入门控条件分支动态融合病灶的放射组学和几何特性，以精确控制和强调病灶特征。实验证明，GCDM能够精确控制小病灶区域的合成，并提升合成图像的真实性和多样性，有望应用于临床乳腺X线图像合成。", "keywords": "乳腺X线图像合成, 扩散模型, 病灶控制, 深度学习, 数据增强", "comments": "该论文提出了一种创新的门控条件扩散模型（GCDM），其亮点在于能够同时合成整体乳腺X线图像和局部病灶，并实现了对小病灶的精确控制。通过引入软掩码嵌入和门控条件分支，模型有效解决了现有生成模型在强调病灶特异性特征方面的不足，提升了合成图像的真实性和多样性，对于缓解医疗影像数据稀缺问题和推动深度学习在乳腺癌筛查中的应用具有重要意义。"}}
{"id": "2507.19309", "title": "Low-Complexity 6DMA Rotation and Position Optimization Based on Statistical Channel Information", "authors": ["Qijun Jiang", "Xiaodan Shao", "Rui Zhang"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      arXiv admin note: substantial text overlap with arXiv:2504.20618", "url": "http://arxiv.org/abs/2507.19309v1", "summary": "The six-dimensional movable antenna (6DMA) is a promising technology to fully\nexploit spatial variation in wireless channels by allowing flexible adjustment\nof three-dimensional (3D) positions and rotations of antennas at the\ntransceiver. In this paper, we consider a 6DMA-equipped base station (BS) and\naim to maximize the average sum logarithmic rate of all users served by the BS\nby jointly designing 6DMA surface positions and rotations based on statistical\nchannel information (SCI). Different from prior works on 6DMA design which use\nalternating optimization to iteratively update surface positions and rotations,\nwe propose a new sequential optimization method that first determines the\noptimal rotations and then identifies feasible positions to realize these\nrotations under practical antenna placement constraints. Simulation results\nshow that our proposed optimization scheme significantly reduces the\ncomputational complexity of conventional alternating optimization (AO), while\nachieving communication performance comparable to the AO-based approach and\nsuperior to existing fixed-position/rotation antenna arrays.", "comment": "arXiv admin note: substantial text overlap with arXiv:2504.20618", "pdf_url": "http://arxiv.org/pdf/2507.19309v1", "cate": "cs.IT", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于统计信道信息的低复杂度6DMA旋转和位置优化", "tldr": "本文提出了一种新的低复杂度顺序优化方法，用于基于统计信道信息优化6DMA天线的旋转和位置，以最大化用户平均对数和速率，并在保持性能的同时显著降低了计算复杂度。", "motivation": "六维可移动天线（6DMA）是一种很有前景的技术，可以通过灵活调整天线的3D位置和旋转来充分利用无线信道的空间变化。本文旨在通过联合设计6DMA表面位置和旋转，基于统计信道信息，最大化基站服务所有用户的平均对数和速率。", "method": "本文提出了一种新的顺序优化方法，与以往使用交替优化的方法不同。该方法首先确定最优旋转，然后识别在实际天线放置约束下实现这些旋转的可行位置。", "result": "仿真结果表明，所提出的优化方案显著降低了传统交替优化（AO）的计算复杂度，同时实现了与基于AO的方法相当的通信性能，并且优于现有固定位置/旋转的天线阵列。", "conclusion": "本文提出的基于统计信道信息的低复杂度6DMA旋转和位置优化方案，在保持通信性能的同时有效降低了计算复杂度，为6DMA部署提供了更实用的方法。", "translation": "六维可移动天线（6DMA）是一种很有前景的技术，通过允许收发器天线的3D位置和旋转的灵活调整，充分利用无线信道的空间变化。在本文中，我们考虑一个配备6DMA的基站（BS），旨在通过基于统计信道信息（SCI）联合设计6DMA表面位置和旋转，最大化基站服务所有用户的平均对数和速率。与以往使用交替优化迭代更新表面位置和旋转的6DMA设计工作不同，我们提出了一种新的顺序优化方法，该方法首先确定最优旋转，然后识别在实际天线放置约束下实现这些旋转的可行位置。仿真结果表明，我们提出的优化方案显著降低了传统交替优化（AO）的计算复杂度，同时实现了与基于AO的方法相当的通信性能，并且优于现有固定位置/旋转的天线阵列。", "summary": "本文研究了基于统计信道信息的六维可移动天线（6DMA）的低复杂度旋转和位置优化问题，旨在最大化基站服务用户的平均对数和速率。针对现有交替优化方法计算复杂度高的问题，提出了一种新的顺序优化方法，该方法先确定最优旋转，再寻找可行位置。仿真结果验证了该方法在显著降低计算复杂度的同时，能保持与传统交替优化相当的通信性能，并优于固定天线阵列。", "keywords": "六维可移动天线, 6DMA, 统计信道信息, 顺序优化, 低复杂度", "comments": "本文的创新点在于提出了一个低复杂度的顺序优化方法，解决了6DMA设计中传统交替优化计算复杂度高的问题。其重要性在于使得6DMA技术在实际部署中更具可行性，通过优化天线位置和旋转，提高了无线通信系统的性能。"}}
{"id": "2503.22139", "title": "Time-resolved dynamic CBCT reconstruction using prior-model-free spatiotemporal Gaussian representation (PMF-STGR)", "authors": ["Jiacheng Xie", "Hua-Chieh Shao", "You Zhang"], "categories": ["physics.med-ph", "cs.LG", "eess.IV"], "primary_category": "Subjects:       Medical Physics (physics.med-ph)", "pdf_link": null, "comments": "Comments:      25 pages, 5 figures", "url": "http://arxiv.org/abs/2503.22139v2", "summary": "Time-resolved CBCT imaging, which reconstructs a dynamic sequence of CBCTs\nreflecting intra-scan motion (one CBCT per x-ray projection without phase\nsorting or binning), is highly desired for regular and irregular motion\ncharacterization, patient setup, and motion-adapted radiotherapy. Representing\npatient anatomy and associated motion fields as 3D Gaussians, we developed a\nGaussian representation-based framework (PMF-STGR) for fast and accurate\ndynamic CBCT reconstruction. PMF-STGR comprises three major components: a dense\nset of 3D Gaussians to reconstruct a reference-frame CBCT for the dynamic\nsequence; another 3D Gaussian set to capture three-level, coarse-to-fine\nmotion-basis-components (MBCs) to model the intra-scan motion; and a CNN-based\nmotion encoder to solve projection-specific temporal coefficients for the MBCs.\nScaled by the temporal coefficients, the learned MBCs will combine into\ndeformation vector fields to deform the reference CBCT into\nprojection-specific, time-resolved CBCTs to capture the dynamic motion. Due to\nthe strong representation power of 3D Gaussians, PMF-STGR can reconstruct\ndynamic CBCTs in a 'one-shot' training fashion from a standard 3D CBCT scan,\nwithout using any prior anatomical or motion model. We evaluated PMF-STGR using\nXCAT phantom simulations and real patient scans. Metrics including the image\nrelative error, structural-similarity-index-measure, tumor\ncenter-of-mass-error, and landmark localization error were used to evaluate the\naccuracy of solved dynamic CBCTs and motion. PMF-STGR shows clear advantages\nover a state-of-the-art, INR-based approach, PMF-STINR. Compared with\nPMF-STINR, PMF-STGR reduces reconstruction time by 50% while reconstructing\nless blurred images with better motion accuracy. With improved efficiency and\naccuracy, PMF-STGR enhances the applicability of dynamic CBCT imaging for\npotential clinical translation.", "comment": "25 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2503.22139v2", "cate": "physics.med-ph", "date": "2025-03-28", "updated": "2025-07-24", "AI": {"title_translation": "使用无先验模型时空高斯表示（PMF-STGR）的时间分辨动态CBCT重建", "tldr": "PMF-STGR是一种基于高斯表示的新框架，用于快速准确的时间分辨动态CBCT重建，优于现有方法。", "motivation": "时间分辨CBCT成像对于运动表征、患者摆位和运动适应性放射治疗具有高度需求。挑战在于无需相位排序或分箱即可重建反映扫描内运动的动态CBCT序列。", "method": "PMF-STGR将患者解剖结构和运动表示为3D高斯。它包含三个主要组件：1）一组密集的3D高斯，用于重建动态序列的参考帧CBCT；2）另一组3D高斯，用于捕获三级、从粗到精的运动基分量（MBCs），以建模扫描内运动；3）一个基于CNN的运动编码器，用于求解MBCs的特定投影时间系数。通过时间系数缩放，学习到的MBCs将组合成形变矢量场，以将参考CBCT形变为特定投影的时间分辨CBCT，从而捕获动态运动。PMF-STGR可以从标准3D CBCT扫描中以“一次性”训练方式重建动态CBCT，无需使用任何先验解剖或运动模型。", "result": "使用XCAT体模模拟和真实患者扫描进行了评估。评估指标包括图像相对误差、结构相似性指数、肿瘤质心误差和地标定位误差。PMF-STGR比最先进的基于INR的方法PMF-STINR具有明显优势，重建时间减少50%，同时重建出更少模糊、运动精度更高的图像。", "conclusion": "PMF-STGR通过提高效率和准确性，增强了动态CBCT成像的适用性，有望实现临床转化。", "translation": "时间分辨CBCT成像，即重建反映扫描内运动的动态CBCT序列（每个X射线投影一个CBCT，无需相位排序或分箱），对于规则和不规则运动表征、患者摆位和运动适应性放射治疗具有高度需求。我们将患者解剖结构和相关运动场表示为3D高斯，开发了一种基于高斯表示的框架（PMF-STGR），用于快速准确的动态CBCT重建。PMF-STGR包含三个主要组件：一组密集的3D高斯，用于重建动态序列的参考帧CBCT；另一组3D高斯，用于捕获三级、从粗到精的运动基分量（MBCs），以建模扫描内运动；以及一个基于CNN的运动编码器，用于求解MBCs的特定投影时间系数。通过时间系数缩放，学习到的MBCs将组合成形变矢量场，以将参考CBCT形变为特定投影的时间分辨CBCT，从而捕获动态运动。由于3D高斯强大的表示能力，PMF-STGR可以从标准3D CBCT扫描中以“一次性”训练方式重建动态CBCT，无需使用任何先验解剖或运动模型。我们使用XCAT体模模拟和真实患者扫描对PMF-STGR进行了评估。评估动态CBCT和运动准确性的指标包括图像相对误差、结构相似性指数、肿瘤质心误差和地标定位误差。PMF-STGR比最先进的基于INR的方法PMF-STINR具有明显优势。与PMF-STINR相比，PMF-STGR将重建时间减少了50%，同时重建出更少模糊、运动精度更高的图像。凭借更高的效率和准确性，PMF-STGR增强了动态CBCT成像的适用性，有望实现临床转化。", "summary": "本文介绍了一种名为PMF-STGR的新型无先验模型时空高斯表示框架，用于时间分辨动态CBCT重建。它通过使用3D高斯表示解剖结构和运动，结合参考CBCT、多级运动基分量和基于CNN的运动编码器，无需相位排序或分箱即可重建动态CBCT序列。在体模和患者数据上的评估表明，与现有最先进的方法相比，PMF-STGR显著缩短了重建时间，并提高了图像质量和运动精度，从而增强了其临床适用性。", "keywords": "时间分辨CBCT, 动态CBCT, 高斯表示, 运动建模, 无先验模型", "comments": "该创新的亮点在于使用3D高斯表示解剖结构和运动，并实现“一次性”训练而无需先验模型，这对于动态CBCT来说是一个重大进步。与最先进的方法相比，其在定量上的显著改进（重建时间减少50%，精度更高）突显了其对临床转化，特别是运动适应性放射治疗的重要性。"}}
{"id": "2507.19411", "title": "SILS: Strategic Influence on Liquidity Stability and Whale Detection in Concentrated-Liquidity DEXs", "authors": ["Ali RajabiNekoo", "Laleh Rasoul", "Amirfarhad Farhadi", "Azadeh Zamanifar"], "categories": ["cs.LG", "cs.CR", "cs.ET"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19411v1", "summary": "Traditional methods for identifying impactful liquidity providers (LPs) in\nConcentrated Liquidity Market Makers (CLMMs) rely on broad measures, such as\nnominal capital size or surface-level activity, which often lead to inaccurate\nrisk analysis. The SILS framework offers a significantly more detailed\napproach, characterizing LPs not just as capital holders but as dynamic\nsystemic agents whose actions directly impact market stability. This represents\na fundamental paradigm shift from the static, volume-based analysis to a\ndynamic, impact-focused understanding. This advanced approach uses on-chain\nevent logs and smart contract execution traces to compute Exponential\nTime-Weighted Liquidity (ETWL) profiles and apply unsupervised anomaly\ndetection. Most importantly, it defines an LP's functional importance through\nthe Liquidity Stability Impact Score (LSIS), a counterfactual metric that\nmeasures the potential degradation of the market if the LP withdraws. This\ncombined approach provides a more detailed and realistic characterization of an\nLP's impact, moving beyond the binary and often misleading classifications used\nby existing methods. This impact-focused and comprehensive approach enables\nSILS to accurately identify high-impact LPs-including those missed by\ntraditional methods and supports essential applications like a protective\noracle layer and actionable trader signals, thereby significantly enhancing\nDeFi ecosystem. The framework provides unprecedented transparency into the\nunderlying liquidity structure and associated risks, effectively reducing the\ncommon false positives and uncovering critical false negatives found in\ntraditional models. Therefore, SILS provides an effective mechanism for\nproactive risk management, transforming how DeFi protocols safeguard their\necosystems against asymmetric liquidity behavior.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19411v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "SILS：集中流动性DEX中流动性稳定性与巨鲸检测的战略影响", "tldr": "SILS框架通过动态、以影响力为中心的方法，利用链上数据和反事实度量，更准确地识别集中流动性DEX中对市场稳定性有重大影响的流动性提供者。", "motivation": "传统方法在识别集中流动性做市商（CLMMs）中有影响力的流动性提供者（LPs）时，依赖于名义资本规模或表面活动等宽泛衡量标准，这导致风险分析不准确，且经常产生误导性分类。", "method": "SILS框架将LPs视为动态系统代理，而非仅仅是资本持有者。它利用链上事件日志和智能合约执行轨迹来计算指数时间加权流动性（ETWL）配置文件，并应用无监督异常检测。最重要的是，它通过流动性稳定性影响分数（LSIS）定义LP的功能重要性，LSIS是一个反事实度量，用于衡量LP撤资时市场潜在的退化程度。", "result": "SILS能够准确识别高影响力LPs（包括传统方法遗漏的），支持保护性预言机层和可操作的交易者信号等基本应用，从而显著增强DeFi生态系统。该框架提供了对底层流动性结构和相关风险前所未有的透明度，有效减少了传统模型中常见的误报并揭示了关键的漏报。", "conclusion": "SILS提供了一种有效的主动风险管理机制，改变了DeFi协议保护其生态系统免受不对称流动性行为影响的方式。", "translation": "传统上，在集中流动性做市商（CLMMs）中识别有影响力的流动性提供者（LPs）的方法依赖于诸如名义资本规模或表面活动等宽泛的衡量标准，这通常导致不准确的风险分析。SILS框架提供了一种显著更详细的方法，它不仅将LPs描述为资本持有者，而且将其描述为动态的系统代理，其行为直接影响市场稳定性。这代表了从静态的、基于交易量的分析到动态的、以影响力为中心的理解的根本性范式转变。这种先进方法利用链上事件日志和智能合约执行轨迹来计算指数时间加权流动性（ETWL）配置文件并应用无监督异常检测。最重要的是，它通过流动性稳定性影响分数（LSIS）定义了LP的功能重要性，LSIS是一个反事实度量，衡量LP撤资时市场潜在的退化程度。这种组合方法提供了对LP影响力更详细和真实的描述，超越了现有方法使用的二元且经常具有误导性的分类。这种以影响力为中心且全面的方法使SILS能够准确识别高影响力LPs——包括那些被传统方法遗漏的——并支持诸如保护性预言机层和可操作的交易者信号等基本应用，从而显著增强DeFi生态系统。该框架提供了对底层流动性结构和相关风险前所未有的透明度，有效减少了传统模型中常见的误报并揭示了传统模型中发现的关键漏报。因此，SILS提供了一种有效的主动风险管理机制，改变了DeFi协议保护其生态系统免受不对称流动性行为影响的方式。", "summary": "SILS框架提出了一种新的方法来识别集中流动性DEX中具有影响力的流动性提供者（LPs）。它超越了传统的基于资本规模或交易量的分析，将LPs视为动态系统代理。SILS利用链上数据计算指数时间加权流动性（ETWL）并应用无监督异常检测，同时引入流动性稳定性影响分数（LSIS）来量化LP撤资对市场稳定性的潜在影响。这种方法旨在更准确地识别高影响力LPs，减少风险分析中的误报和漏报，从而为DeFi生态系统提供更透明和主动的风险管理。", "keywords": "集中流动性DEX, 流动性提供者, 巨鲸检测, 风险管理, DeFi", "comments": "SILS框架的创新之处在于其将流动性提供者视为动态系统代理，并引入了反事实的流动性稳定性影响分数（LSIS），这使得对LP影响力的评估更加细致和准确。通过利用链上数据和先进的异常检测，SILS有望显著提升DeFi协议的风险管理能力，尤其是在应对巨鲸行为和非对称流动性方面。"}}
{"id": "2507.19115", "title": "Automated Code Review Using Large Language Models at Ericsson: An Experience Report", "authors": ["Shweta Ramesh", "Joy Bose", "Hamender Singh", "A K Raghavan", "Sujoy Roychowdhury", "Giriprasad Sridhara", "Nishrith Saini", "Ricardo Britto"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19115v1", "summary": "Code review is one of the primary means of assuring the quality of released\nsoftware along with testing and static analysis. However, code review requires\nexperienced developers who may not always have the time to perform an in-depth\nreview of code. Thus, automating code review can help alleviate the cognitive\nburden on experienced software developers allowing them to focus on their\nprimary activities of writing code to add new features and fix bugs. In this\npaper, we describe our experience in using Large Language Models towards\nautomating the code review process in Ericsson. We describe the development of\na lightweight tool using LLMs and static program analysis. We then describe our\npreliminary experiments with experienced developers in evaluating our code\nreview tool and the encouraging results.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19115v1", "cate": "cs.SE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "爱立信使用大型语言模型进行自动化代码审查：一份经验报告", "tldr": "爱立信使用大型语言模型和静态程序分析开发了一个轻量级工具，以自动化代码审查过程，初步实验结果令人鼓舞，减轻了经验丰富的开发人员的认知负担。", "motivation": "代码审查是确保软件质量的关键手段，但它需要经验丰富的开发人员，而这些开发人员可能没有足够的时间进行深入审查。因此，自动化代码审查可以减轻他们的认知负担，使他们能专注于开发新功能和修复错误。", "method": "本文描述了在爱立信使用大型语言模型（LLMs）自动化代码审查过程的经验。开发了一个结合LLMs和静态程序分析的轻量级工具，并与经验丰富的开发人员进行了初步实验以评估该工具。", "result": "初步实验结果令人鼓舞。", "conclusion": "使用大型语言模型自动化代码审查是可行的，并且能够有效减轻经验丰富的开发人员的负担，提高软件开发效率。", "translation": "代码审查是确保发布软件质量的主要手段之一，与测试和静态分析并列。然而，代码审查需要经验丰富的开发人员，他们可能没有时间对代码进行深入审查。因此，自动化代码审查可以帮助减轻经验丰富的软件开发人员的认知负担，使他们能够专注于编写代码以添加新功能和修复错误的主要活动。在本文中，我们描述了在爱立信使用大型语言模型实现代码审查过程自动化的经验。我们描述了使用LLMs和静态程序分析开发轻量级工具的过程。然后，我们描述了与经验丰富的开发人员进行的初步实验，评估了我们的代码审查工具及其令人鼓舞的结果。", "summary": "本文介绍了爱立信在使用大型语言模型（LLMs）自动化代码审查方面的经验。研究人员开发了一个结合LLMs和静态程序分析的轻量级工具，旨在减轻经验丰富的开发人员在代码审查中的认知负担。初步实验结果显示该工具表现良好，证实了自动化代码审查的可行性和潜力。", "keywords": "自动化代码审查, 大型语言模型, 爱立信, 经验报告, 静态分析", "comments": "该论文展示了将大型语言模型应用于企业级软件开发流程（如代码审查）的实际案例，具有重要的实践意义。它提供了一个真实世界的经验报告，对于探索LLMs在软件工程领域应用具有参考价值。其创新点在于结合了LLMs和传统的静态程序分析，以期达到更高效和准确的自动化审查。"}}
{"id": "2506.08979", "title": "Towards Generalized Range-View LiDAR Segmentation in Adverse Weather", "authors": ["Longyu Yang", "Lu Zhang", "Jun Liu", "Yap-Peng Tan", "Heng Tao Shen", "Xiaofeng Zhu", "Ping Hu"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.08979v3", "summary": "LiDAR segmentation has emerged as an important task to enrich scene\nperception and understanding. Range-view-based methods have gained popularity\ndue to their high computational efficiency and compatibility with real-time\ndeployment. However, their generalized performance under adverse weather\nconditions remains underexplored, limiting their reliability in real-world\nenvironments. In this work, we identify and analyze the unique challenges that\naffect the generalization of range-view LiDAR segmentation in severe weather.\nTo address these challenges, we propose a modular and lightweight framework\nthat enhances robustness without altering the core architecture of existing\nmodels. Our method reformulates the initial stem block of standard range-view\nnetworks into two branches to process geometric attributes and reflectance\nintensity separately. Specifically, a Geometric Abnormality Suppression (GAS)\nmodule reduces the influence of weather-induced spatial noise, and a\nReflectance Distortion Calibration (RDC) module corrects reflectance\ndistortions through memory-guided adaptive instance normalization. The\nprocessed features are then fused and passed to the original segmentation\npipeline. Extensive experiments on different benchmarks and baseline models\ndemonstrate that our approach significantly improves generalization to adverse\nweather with minimal inference overhead, offering a practical and effective\nsolution for real-world LiDAR segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.08979v3", "cate": "cs.CV", "date": "2025-06-10", "updated": "2025-07-25", "AI": {"title_translation": "恶劣天气下广义距离视图激光雷达分割研究", "tldr": "本文提出了一种模块化轻量级框架，通过分离处理几何属性和反射强度，以提高距离视图激光雷达分割模型在恶劣天气下的泛化能力，并在多个基准测试中取得了显著效果。", "motivation": "距离视图激光雷达分割方法虽然计算效率高且兼容实时部署，但在恶劣天气条件下的泛化性能尚未得到充分探索，这限制了其在现实世界环境中的可靠性。", "method": "本文提出了一种模块化、轻量级的框架，在不改变现有模型核心架构的情况下增强鲁棒性。该方法将标准距离视图网络的初始主干块重构为两个分支，分别处理几何属性和反射强度。具体来说，几何异常抑制（GAS）模块减少天气引起的空间噪声影响，反射畸变校准（RDC）模块通过记忆引导自适应实例归一化校正反射畸变。处理后的特征被融合并传递给原始分割流程。", "result": "在不同的基准测试和基线模型上进行的广泛实验表明，该方法显著提高了在恶劣天气下的泛化能力，同时推理开销极小。", "conclusion": "本文提供了一种实用且有效的解决方案，用于现实世界中的激光雷达分割，显著提高了模型在恶劣天气下的泛化能力。", "translation": "激光雷达分割已成为丰富场景感知和理解的重要任务。基于距离视图的方法因其高计算效率和与实时部署的兼容性而受到欢迎。然而，它们在恶劣天气条件下的泛化性能仍未得到充分探索，这限制了它们在现实世界环境中的可靠性。在这项工作中，我们识别并分析了影响距离视图激光雷达分割在恶劣天气下泛化能力的独特挑战。为了解决这些挑战，我们提出了一种模块化且轻量级的框架，在不改变现有模型核心架构的情况下增强鲁棒性。我们的方法将标准距离视图网络的初始主干块重构为两个分支，分别处理几何属性和反射强度。具体来说，几何异常抑制（GAS）模块减少天气引起的空间噪声影响，反射畸变校准（RDC）模块通过记忆引导自适应实例归一化校正反射畸变。处理后的特征随后被融合并传递给原始分割流程。在不同基准和基线模型上进行的广泛实验表明，我们的方法以最小的推理开销显著提高了在恶劣天气下的泛化能力，为现实世界中的激光雷达分割提供了一种实用且有效的解决方案。", "summary": "本文针对距离视图激光雷达分割在恶劣天气下泛化能力不足的问题，提出了一种模块化、轻量级的框架。该框架通过将标准网络的初始主干块重构为两个分支，分别利用几何异常抑制（GAS）模块处理空间噪声和反射畸变校准（RDC）模块校正反射畸变，从而增强模型鲁棒性。实验证明，该方法在恶劣天气条件下显著提高了分割性能，且推理开销极低，为实际应用提供了有效方案。", "keywords": "激光雷达分割, 距离视图, 恶劣天气, 泛化能力, 模块化框架", "comments": "该论文的创新点在于提出了一个模块化且轻量级的框架，通过解耦处理几何属性和反射强度来提升激光雷达分割模型在恶劣天气下的泛化能力。特别是GAS和RDC模块的设计，针对性地解决了天气引起的噪声和反射畸变问题，具有很强的实用价值和工程意义。其“不改变核心架构”的特点，使其易于集成到现有模型中，降低了部署成本，是一项重要的进展。"}}
{"id": "2507.19308", "title": "The Eloquence team submission for task 1 of MLC-SLM challenge", "authors": ["Lorenzo Concina", "Jordi Luque", "Alessio Brutti", "Marco Matassoni", "Yuchen Zhang"], "categories": ["cs.SD", "cs.CL", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Technical Report for MLC-SLM Challenge of Interspeech2025", "url": "http://arxiv.org/abs/2507.19308v1", "summary": "In this paper, we present our studies and experiments carried out for the\ntask 1 of the Challenge and Workshop on Multilingual Conversational Speech\nLanguage Model (MLC-SLM), which focuses on advancing multilingual\nconversational speech recognition through the development of speech language\nmodels architectures. Given the increasing relevance of real-world\nconversational data for building robust Spoken Dialogue Systems, we explore\nthree approaches to multilingual ASR. First, we conduct an evaluation of the\nofficial baseline to better understand its strengths and limitations, by\ntraining two projectors (linear and qformer) with different foundation models.\nSecond we leverage the SLAM-ASR framework to train a custom multilingual linear\nprojector. Finally we investigate the role of contrastive learning and the\nextended conversational context in enhancing the robustness of recognition.", "comment": "Technical Report for MLC-SLM Challenge of Interspeech2025", "pdf_url": "http://arxiv.org/pdf/2507.19308v1", "cate": "cs.SD", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "MLC-SLM挑战任务1的Eloquence团队提交", "tldr": "本文介绍了Eloquence团队为MLC-SLM挑战任务1所做的研究和实验，旨在通过开发语音语言模型架构来推进多语言会话语音识别。研究探索了三种方法，包括评估基线、利用SLAM-ASR框架训练自定义投影器以及研究对比学习和会话上下文的作用。", "motivation": "本文旨在推进多语言会话语音识别技术，通过开发语音语言模型架构来应对MLC-SLM挑战的任务1，并鉴于真实世界会话数据对构建鲁棒语音对话系统日益增长的重要性。", "method": "本文探索了三种多语言ASR方法：首先，通过使用不同基础模型训练线性投影器和qformer两种投影器，评估官方基线的优缺点；其次，利用SLAM-ASR框架训练自定义多语言线性投影器；最后，研究对比学习和扩展会话上下文在增强识别鲁棒性方面的作用。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "在本文中，我们介绍了为多语言会话语音语言模型（MLC-SLM）挑战和研讨会任务1所进行的研究和实验，该任务侧重于通过开发语音语言模型架构来推进多语言会话语音识别。鉴于真实世界会话数据对于构建鲁棒语音对话系统日益增长的相关性，我们探索了多语言ASR的三种方法。首先，我们通过使用不同的基础模型训练两个投影器（线性投影器和qformer），对官方基线进行了评估，以更好地理解其优点和局限性。其次，我们利用SLAM-ASR框架训练了一个自定义的多语言线性投影器。最后，我们研究了对比学习和扩展会话上下文在增强识别鲁棒性方面的作用。", "summary": "本文介绍了Eloquence团队为MLC-SLM挑战任务1所进行的研究，该任务旨在通过语音语言模型架构提升多语言会话语音识别。研究探讨了三种方法：评估官方基线（通过训练线性与qformer投影器）、利用SLAM-ASR框架训练自定义多语言线性投影器，以及探究对比学习和扩展会话上下文对识别鲁棒性的影响。", "keywords": "多语言ASR, 语音语言模型, 会话语音识别, 对比学习, SLAM-ASR", "comments": "本文针对MLC-SLM挑战任务1，提出了三种探索多语言ASR的方法，包括基线评估、框架应用和学习机制研究。其创新点在于对不同投影器和学习策略的尝试，以提升会话语音识别的鲁棒性。但抽象中未提及具体实验结果，无法评估其有效性或局限性。"}}
{"id": "2506.00842", "title": "Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience", "authors": ["Jiawei Gu", "Ziting Xian", "Yuanzhen Xie", "Ye Liu", "Enjie Liu", "Ruichao Zhong", "Mochi Gao", "Yunzhi Tan", "Bo Hu", "Zang Li"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 Findings", "url": "http://arxiv.org/abs/2506.00842v2", "summary": "Large language models (LLMs) achieve strong performance on plain text tasks\nbut underperform on structured data like tables and databases. Potential\nchallenges arise from their underexposure during pre-training and rigid\ntext-to-structure transfer mechanisms. Unlike humans who seamlessly apply\nlearned patterns across data modalities, LLMs struggle to infer implicit\nrelationships embedded in tabular formats, especially in the absence of\nexplicit structural guidance. To bridge this cognitive gap, we introduce\nContrastive Retrieval-Augmented Generation on Experience (CoRE), a framework\nthat builds experience memory representations and enhances generalization\nthrough contrastive In-Context Learning (ICL) to simulate human-like knowledge\ntransfer. Experiments on Text-to-SQL and TableQA show CoRE significantly\nimproves performance, achieving average gains of 3.44% and 4.24%, with up to\n17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated\nExperience Memory expands training data 8-9x, enhancing diversity and domain\ncoverage. This training-free and continual method propels LLMs toward\nstructured knowledge expertise.", "comment": "ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2506.00842v2", "cate": "cs.CL", "date": "2025-06-01", "updated": "2025-07-24", "AI": {"title_translation": "迈向结构化知识推理：基于经验的对比检索增强生成", "tldr": "CoRE框架通过构建经验记忆和对比上下文学习，显著提升了大型语言模型在结构化数据（如Text-to-SQL和TableQA）上的推理表现。", "motivation": "大型语言模型在纯文本任务上表现出色，但在处理表格和数据库等结构化数据时表现不佳。这主要源于预训练期间的曝光不足以及僵化的文本到结构转换机制，导致LLMs难以推断表格格式中嵌入的隐式关系。", "method": "本文引入了基于经验的对比检索增强生成（CoRE）框架。CoRE通过构建经验记忆表示，并利用对比上下文学习（ICL）来增强泛化能力，以模拟人类般的知识迁移。此外，该方法使用蒙特卡洛树搜索（MCTS）生成经验记忆。", "result": "在Text-to-SQL和TableQA任务上的实验表明，CoRE显著提高了性能，平均增益分别为3.44%和4.24%，在挑战性任务上最高可达17.2%。MCTS生成的经验记忆将训练数据扩展了8-9倍，增强了多样性和领域覆盖率。", "conclusion": "CoRE是一种免训练且持续的方法，它通过模拟人类知识迁移的方式，推动大型语言模型在结构化知识领域迈向专业化。", "translation": "大型语言模型（LLMs）在纯文本任务上表现出色，但在表格和数据库等结构化数据上表现不佳。潜在的挑战源于它们在预训练期间的曝光不足以及僵化的文本到结构转换机制。与人类无缝地将所学模式应用于不同数据模态不同，LLMs难以推断表格格式中嵌入的隐式关系，尤其是在缺乏明确结构指导的情况下。为了弥合这种认知差距，我们引入了基于经验的对比检索增强生成（CoRE），一个通过构建经验记忆表示并通过对比上下文学习（ICL）增强泛化能力以模拟人类知识迁移的框架。在Text-to-SQL和TableQA上的实验表明，CoRE显著提高了性能，平均增益分别为3.44%和4.24%，在挑战性任务上最高可达17.2%。我们通过蒙特卡洛树搜索（MCTS）生成的经验记忆将训练数据扩展了8-9倍，增强了多样性和领域覆盖率。这种免训练且持续的方法推动LLMs迈向结构化知识专业化。", "summary": "本文提出了CoRE（基于经验的对比检索增强生成）框架，旨在解决大型语言模型在结构化数据推理上的不足。CoRE通过构建经验记忆表示并结合对比上下文学习来模拟人类知识迁移，从而提升模型的泛化能力。实验结果显示，CoRE在Text-to-SQL和TableQA任务上取得了显著的性能提升，并通过MCTS生成的经验记忆将训练数据量大幅扩展，增强了数据多样性和领域覆盖。该方法是一种免训练且持续学习的范式，有助于LLMs更好地处理结构化知识。", "keywords": "大型语言模型, 结构化数据, 检索增强生成, 对比学习, 上下文学习", "comments": "CoRE的创新之处在于引入了“经验记忆”和“对比上下文学习”来模拟人类的知识迁移能力，以解决LLMs在结构化数据推理上的认知差距。通过MCTS扩展训练数据量，在不增加训练成本的情况下提升了模型的表现和泛化能力，这对于推动LLMs在复杂结构化任务中的应用具有重要意义。"}}
{"id": "2507.19038", "title": "A Distributed Approach for Agile Supply Chain Decision-Making Based on Network Attributes", "authors": ["Mingjie Bi", "Dawn M. Tilbury", "Siqian Shen", "Kira Barton"], "categories": ["cs.MA", "cs.SI", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19038v1", "summary": "In recent years, the frequent occurrence of disruptions has had a negative\nimpact on global supply chains. To stay competitive, enterprises strive to\nremain agile through the implementation of efficient and effective\ndecision-making strategies in reaction to disruptions. A significant effort has\nbeen made to develop these agile disruption mitigation approaches, leveraging\nboth centralized and distributed decision-making strategies. Though trade-offs\nof centralized and distributed approaches have been analyzed in existing\nstudies, no related work has been found on understanding supply chain\nperformance based on the network attributes of the disrupted supply chain\nentities. In this paper, we characterize supply chains from a capability and\nnetwork topological perspective and investigate the use of a distributed\ndecision-making approach based on classical multi-agent frameworks. The\nperformance of the distributed framework is evaluated through a comprehensive\ncase study that investigates the performance of the supply chain as a function\nof the network structure and agent attributes within the network in the\npresence of a disruption. Comparison to a centralized decision-making approach\nhighlights trade-offs between performance, computation time, and network\ncommunication based on the decision-making strategy and network architecture.\nPractitioners can use the outcomes of our studies to design response strategies\nbased on agent capabilities, network attributes, and desired supply chain\nperformance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19038v1", "cate": "cs.MA", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于网络属性的敏捷供应链分布式决策方法", "tldr": "研究基于网络属性的分布式方法，以提高供应链在中断情况下的敏捷决策能力，并与集中式方法进行比较。", "motivation": "全球供应链频繁中断对企业竞争力产生负面影响，现有研究虽分析了集中式和分布式决策的权衡，但缺乏基于中断供应链实体网络属性来理解供应链性能的研究。", "method": "该研究从能力和网络拓扑角度描述供应链，并基于经典多智能体框架，提出并研究了一种分布式决策方法。通过一个综合案例研究评估了该分布式框架的性能，分析了中断情况下供应链性能与网络结构和智能体属性的关系。", "result": "研究结果通过与集中式决策方法的比较，揭示了性能、计算时间和网络通信之间基于决策策略和网络架构的权衡。", "conclusion": "研究成果可供实践者根据智能体能力、网络属性和期望的供应链性能来设计响应策略。", "translation": "近年来，中断的频繁发生对全球供应链产生了负面影响。为了保持竞争力，企业致力于通过实施高效和有效的决策策略来应对中断，从而保持敏捷性。在开发这些敏捷的中断缓解方法方面已经付出了巨大努力，利用了集中式和分布式决策策略。尽管现有研究已经分析了集中式和分布式方法的权衡，但尚未发现有相关工作基于中断供应链实体的网络属性来理解供应链性能。在本文中，我们从能力和网络拓扑视角描述供应链，并研究了基于经典多智能体框架的分布式决策方法的使用。通过一个综合案例研究评估了该分布式框架的性能，该案例研究调查了在存在中断的情况下，供应链性能作为网络结构和网络内智能体属性的函数。与集中式决策方法的比较突出了基于决策策略和网络架构的性能、计算时间和网络通信之间的权衡。实践者可以利用我们研究的结果，根据智能体能力、网络属性和期望的供应链性能来设计响应策略。", "summary": "本论文提出并评估了一种基于网络属性的分布式方法，用于在面对中断时实现敏捷供应链决策。研究从能力和网络拓扑角度刻画供应链，并利用多智能体框架构建分布式决策模型。通过案例研究，该方法在中断情境下的供应链性能、计算时间和网络通信方面与集中式方法进行了比较，揭示了不同决策策略和网络架构下的权衡。研究成果为实践者根据智能体能力、网络属性和期望性能设计供应链响应策略提供了指导。", "keywords": "供应链敏捷性, 分布式决策, 网络属性, 多智能体系统, 中断管理", "comments": "该论文的创新点在于首次将供应链性能与中断后的网络属性关联起来，填补了现有研究的空白。通过结合多智能体框架和分布式决策，为应对供应链中断提供了一种新的视角和实用工具。其重要性在于为企业设计更具弹性和敏捷性的供应链响应策略提供了理论依据和实践指导。"}}
{"id": "2507.19096", "title": "iPLAN: Redefining Indoor Wireless Network Planning Through Large Language Models", "authors": ["Jinbo Hou", "Stefanos Bakirtzis", "Kehai Qiu", "Sichong Liao", "Hui Song", "Haonan Hu", "Kezhi Wang", "Jie Zhang"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19096v1", "summary": "Efficient indoor wireless network (IWN) planning is crucial for providing\nhigh-quality 5G in-building services. However, traditional meta-heuristic and\nartificial intelligence-based planning methods face significant challenges due\nto the intricate interplay between indoor environments (IEs) and IWN demands.\nIn this article, we present an indoor wireless network Planning with large\nLANguage models (iPLAN) framework, which integrates multi-modal IE\nrepresentations into large language model (LLM)-powered optimizers to improve\nIWN planning. First, we instate the role of LLMs as optimizers, outlining\nembedding techniques for IEs, and introducing two core applications of iPLAN:\n(i) IWN planning based on pre-existing IEs and (ii) joint design of IWN and IE\nfor new wireless-friendly buildings. For the former, we embed essential\ninformation into LLM optimizers by leveraging indoor descriptions,\ndomain-specific knowledge, and performance-driven perception. For the latter,\nwe conceptualize a multi-agent strategy, where intelligent agents\ncollaboratively address key planning sub-tasks in a step-by-step manner while\nensuring optimal trade-offs between the agents. The simulation results\ndemonstrate that iPLAN achieves superior performance in IWN planning tasks and\noptimizes building wireless performance through the joint design of IEs and\nIWNs, exemplifying a paradigm shift in IWN planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19096v1", "cate": "cs.NI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "iPLAN：通过大型语言模型重新定义室内无线网络规划", "tldr": "iPLAN是一个利用大型语言模型优化室内无线网络规划的框架，通过整合多模态室内环境表示，解决了传统方法的挑战，并在模拟中表现出优越性能。", "motivation": "高效的室内无线网络（IWN）规划对于提供高质量的5G室内服务至关重要。然而，传统的元启发式和基于人工智能的规划方法由于室内环境（IEs）和IWN需求之间复杂的相互作用而面临重大挑战。", "method": "本文提出了一个名为iPLAN（室内无线网络规划与大型语言模型）的框架，该框架将多模态室内环境表示整合到由大型语言模型（LLM）驱动的优化器中，以改进IWN规划。首先，确定LLM作为优化器的作用，概述了室内环境的嵌入技术，并介绍了iPLAN的两个核心应用：(i) 基于现有室内环境的IWN规划和 (ii) 新型无线友好建筑的IWN和室内环境的联合设计。对于前者，通过利用室内描述、领域特定知识和性能驱动感知，将关键信息嵌入到LLM优化器中。对于后者，概念化了一种多智能体策略，其中智能体以循序渐进的方式协作解决关键规划子任务，同时确保智能体之间的最佳权衡。", "result": "仿真结果表明，iPLAN在IWN规划任务中实现了卓越的性能，并通过室内环境和IWN的联合设计优化了建筑无线性能。", "conclusion": "iPLAN框架通过引入大型语言模型优化器和多模态室内环境表示，在室内无线网络规划领域实现了范式转变，显著提升了规划效率和性能。", "translation": "高效的室内无线网络（IWN）规划对于提供高质量的5G室内服务至关重要。然而，传统的元启发式和基于人工智能的规划方法由于室内环境（IEs）和IWN需求之间复杂的相互作用而面临重大挑战。在本文中，我们提出了一个名为iPLAN（室内无线网络规划与大型语言模型）的框架，该框架将多模态室内环境表示整合到由大型语言模型（LLM）驱动的优化器中，以改进IWN规划。首先，我们确立了LLM作为优化器的作用，概述了室内环境的嵌入技术，并介绍了iPLAN的两个核心应用：(i) 基于现有室内环境的IWN规划和 (ii) 新型无线友好建筑的IWN和室内环境的联合设计。对于前者，我们通过利用室内描述、领域特定知识和性能驱动感知，将关键信息嵌入到LLM优化器中。对于后者，我们概念化了一种多智能体策略，其中智能体以循序渐进的方式协作解决关键规划子任务，同时确保智能体之间的最佳权衡。仿真结果表明，iPLAN在IWN规划任务中实现了卓越的性能，并通过室内环境和IWN的联合设计优化了建筑无线性能，这预示着IWN规划领域的一次范式转变。", "summary": "iPLAN是一个创新的框架，旨在通过利用大型语言模型（LLM）重新定义室内无线网络（IWN）规划。它通过将多模态室内环境表示集成到LLM驱动的优化器中，解决了传统规划方法的复杂性挑战。iPLAN支持两种应用：基于现有室内环境的IWN规划和IWN与室内环境的联合设计，其中后者采用多智能体协作策略。仿真结果表明，iPLAN在IWN规划任务中表现出卓越性能，并通过联合设计优化了建筑无线性能，代表了IWN规划领域的一次范式转变。", "keywords": "室内无线网络规划, 大型语言模型, 多模态表示, 优化器, 联合设计", "comments": "iPLAN的创新之处在于将大型语言模型引入室内无线网络规划领域，将其用作优化器，并结合多模态室内环境表示。这为解决传统方法在复杂室内环境下遇到的挑战提供了一个新颖的视角。其提出的两种应用场景，特别是IWN与室内环境的联合设计，展现了该方法在未来无线友好建筑设计中的潜力，预示着该领域可能发生范式转变。"}}
{"id": "2507.19211", "title": "Dependency-aware synthetic tabular data generation", "authors": ["Chaithra Umesh", "Kristian Schultz", "Manjunath Mahendra", "Saptarshi Bej", "Olaf Wolkenhauer"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      23 pages, 3 figures, submitted to Pattern Recognition", "url": "http://arxiv.org/abs/2507.19211v1", "summary": "Synthetic tabular data is increasingly used in privacy-sensitive domains such\nas health care, but existing generative models often fail to preserve\ninter-attribute relationships. In particular, functional dependencies (FDs) and\nlogical dependencies (LDs), which capture deterministic and rule-based\nassociations between features, are rarely or often poorly retained in synthetic\ndatasets. To address this research gap, we propose the Hierarchical Feature\nGeneration Framework (HFGF) for synthetic tabular data generation. We created\nbenchmark datasets with known dependencies to evaluate our proposed HFGF. The\nframework first generates independent features using any standard generative\nmodel, and then reconstructs dependent features based on predefined FD and LD\nrules. Our experiments on four benchmark datasets with varying sizes, feature\nimbalance, and dependency complexity demonstrate that HFGF improves the\npreservation of FDs and LDs across six generative models, including CTGAN,\nTVAE, and GReaT. Our findings demonstrate that HFGF can significantly enhance\nthe structural fidelity and downstream utility of synthetic tabular data.", "comment": "23 pages, 3 figures, submitted to Pattern Recognition", "pdf_url": "http://arxiv.org/pdf/2507.19211v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "依赖感知的合成表格数据生成", "tldr": "本文提出了分层特征生成框架（HFGF），用于生成能更好地保留属性间依赖关系（特别是功能依赖和逻辑依赖）的合成表格数据。", "motivation": "现有的生成模型在生成合成表格数据时，往往未能很好地保留属性间的关系，尤其是功能依赖（FDs）和逻辑依赖（LDs），这在隐私敏感领域是一个重要缺陷。", "method": "本研究提出了分层特征生成框架（HFGF）。该框架首先使用任何标准生成模型生成独立特征，然后根据预定义的功能依赖（FD）和逻辑依赖（LD）规则重建依赖特征。研究创建了已知依赖关系的基准数据集来评估HFGF。", "result": "在四个不同大小、特征不平衡和依赖复杂度的基准数据集上的实验表明，HFGF在包括CTGAN、TVAE和GReaT在内的六种生成模型中，显著改善了功能依赖和逻辑依赖的保留。研究结果表明HFGF可以显著增强合成表格数据的结构保真度和下游效用。", "conclusion": "分层特征生成框架（HFGF）能够显著提高合成表格数据中功能依赖和逻辑依赖的保留度，从而增强数据的结构保真度和实际应用价值。", "translation": "合成表格数据在医疗保健等隐私敏感领域得到越来越多的应用，但现有生成模型往往未能保留属性间的关系。特别是，捕获特征之间确定性和基于规则关联的功能依赖（FDs）和逻辑依赖（LDs），在合成数据集中很少或常常保留不佳。为了解决这一研究空白，我们提出了用于合成表格数据生成的分层特征生成框架（HFGF）。我们创建了已知依赖关系的基准数据集来评估我们提出的HFGF。该框架首先使用任何标准生成模型生成独立特征，然后根据预定义的FD和LD规则重建依赖特征。我们在四个具有不同大小、特征不平衡和依赖复杂度的基准数据集上的实验表明，HFGF改善了包括CTGAN、TVAE和GReaT在内的六种生成模型的功能依赖和逻辑依赖的保留。我们的发现表明HFGF可以显著增强合成表格数据的结构保真度和下游效用。", "summary": "本文提出了一种名为分层特征生成框架（HFGF）的新方法，旨在解决现有合成表格数据生成模型未能有效保留属性间依赖关系的问题。HFGF首先生成独立特征，然后基于预定义的功能依赖和逻辑依赖规则重建依赖特征。实验结果表明，HFGF显著提升了合成数据中依赖关系的保留度，从而提高了数据的结构保真度和实用性。", "keywords": "合成表格数据, 功能依赖, 逻辑依赖, 数据生成模型, 隐私保护", "comments": "HFGF的创新之处在于它明确地将依赖关系的处理分层，先生成独立特征再重建依赖特征，这种方法有效地解决了现有模型在保留功能依赖和逻辑依赖方面的不足，对于需要高结构保真度的隐私敏感数据生成具有重要意义。"}}
{"id": "2504.14508", "title": "Less is More: Adaptive Coverage for Synthetic Training Data", "authors": ["Sasan Tavakkol", "Max Springer", "Mohammadhossein Bateni", "Neslihan Bulut", "Vincent Cohen-Addad", "MohammadTaghi Hajiaghayi"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.14508v2", "summary": "Synthetic training data generation with Large Language Models (LLMs) like\nGoogle's Gemma and OpenAI's GPT offer a promising solution to the challenge of\nobtaining large, labeled datasets for training classifiers. When rapid model\ndeployment is critical, such as in classifying emerging social media trends or\ncombating new forms of online abuse tied to current events, the ability to\ngenerate training data is invaluable. While prior research has examined the\ncomparability of synthetic data to human-labeled data, this study introduces a\nnovel sampling algorithm, based on the maximum coverage problem, to select a\nrepresentative subset from a synthetically generated dataset. Our results\ndemonstrate that training a classifier on this contextually sampled subset\nachieves superior performance compared to training on the entire dataset. This\n\"less is more\" approach not only improves model accuracy but also reduces the\nvolume of data required, leading to potentially more efficient model\nfine-tuning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.14508v2", "cate": "cs.LG", "date": "2025-04-20", "updated": "2025-07-24", "AI": {"title_translation": "少即是多：合成训练数据的自适应覆盖", "tldr": "本研究提出了一种基于最大覆盖问题的新型采样算法，从合成数据集中选择代表性子集，以提高分类器性能并减少所需数据量。", "motivation": "大型语言模型（LLMs）生成的合成训练数据为获取大量带标签数据集提供了一种有前景的解决方案，尤其是在需要快速部署模型（如分类新兴社交媒体趋势或打击新型在线滥用）时，生成训练数据的能力至关重要。", "method": "本研究引入了一种基于最大覆盖问题的新型采样算法，用于从合成生成的数据集中选择一个具有代表性的子集。", "result": "在经过上下文采样的子集上训练分类器，其性能优于在整个数据集上训练。这种“少即是多”的方法不仅提高了模型准确性，还减少了所需数据量。", "conclusion": "通过对合成数据进行自适应采样，可以实现更高的模型准确性并提高模型微调效率，证明了“少即是多”方法的有效性。", "translation": "大型语言模型（LLMs），如谷歌的Gemma和OpenAI的GPT，生成的合成训练数据为获取大量带标签数据集以训练分类器提供了一个有前景的解决方案。当快速模型部署至关重要时，例如在分类新兴社交媒体趋势或打击与当前事件相关的新型在线滥用时，生成训练数据的能力是无价的。虽然先前的研究已经检验了合成数据与人工标注数据的可比性，但本研究引入了一种基于最大覆盖问题的新型采样算法，以从合成生成的数据集中选择一个具有代表性的子集。我们的结果表明，在此上下文采样的子集上训练分类器，其性能优于在整个数据集上训练。这种“少即是多”的方法不仅提高了模型准确性，还减少了所需的数据量，从而可能实现更高效的模型微调。", "summary": "本研究提出了一种基于最大覆盖问题的新型采样算法，用于从LLM生成的合成训练数据集中选择一个代表性子集。实验结果表明，在采样的子集上训练分类器能够获得比在整个数据集上更好的性能，从而在提高模型准确性的同时减少了所需数据量，为高效模型微调提供了可能。", "keywords": "合成数据, 大语言模型, 采样算法, 最大覆盖, 模型准确性", "comments": "这项研究的创新之处在于引入了一种基于最大覆盖问题的新型采样算法来优化合成数据的利用。其重要性体现在它解决了LLM生成数据量大但可能存在冗余的问题，通过“少即是多”的方法提高了模型训练效率和准确性，特别适用于需要快速部署模型的场景。"}}
{"id": "2507.19178", "title": "Budget and Profit Approximations for Spanning Tree Interdiction", "authors": ["Rafail Ostrovsky", "Yuval Rabani", "Yoav Siman Tov"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "Comments:      Presented at the APPROX 2025 conference", "url": "http://arxiv.org/abs/2507.19178v1", "summary": "We give polynomial time logarithmic approximation guarantees for the budget\nminimization, as well as for the profit maximization versions of minimum\nspanning tree interdiction. In this problem, the goal is to remove some edges\nof an undirected graph with edge weights and edge costs, so as to increase the\nweight of a minimum spanning tree. In the budget minimization version, the goal\nis to minimize the total cost of the removed edges, while achieving a desired\nincrease $\\Delta$ in the weight of the minimum spanning tree. An alternative\nobjective within the same framework is to maximize the profit of interdiction,\nnamely the increase in the weight of the minimum spanning tree, subject to a\nbudget constraint. There are known polynomial time $O(1)$ approximation\nguarantees for a similar objective (maximizing the total cost of the tree,\nrather than the increase). However, the guarantee does not seem to apply to the\nincrease in cost. Moreover, the same techniques do not seem to apply to the\nbudget version.\n  Our approximation guarantees are motivated by studying the question of\nminimizing the cost of increasing the minimum spanning tree by any amount. We\nshow that in contrast to the budget and profit problems, this version of\ninterdiction is polynomial time-solvable, and we give an efficient algorithm\nfor solving it. The solution motivates a graph-theoretic relaxation of the\nNP-hard interdiction problem. The gain in minimum spanning tree weight, as a\nfunction of the set of removed edges, is super-modular. Thus, the budget\nproblem is an instance of minimizing a linear function subject to a\nsuper-modular covering constraint. We use the graph-theoretic relaxation to\ndesign and analyze a batch greedy-based algorithm.", "comment": "Presented at the APPROX 2025 conference", "pdf_url": "http://arxiv.org/pdf/2507.19178v1", "cate": "cs.DS", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "最小生成树阻断的预算和收益近似", "tldr": "本文为最小生成树阻断问题的预算最小化和收益最大化版本提供了多项式时间对数近似保证。", "motivation": "现有方法对最小生成树成本的增加和预算版本不适用，这促使作者寻求新的近似保证。此外，研究如何以最小成本增加任意量最小生成树权重的可解性问题也提供了动机。", "method": "作者首先证明了以最小成本增加任意量最小生成树权重的问题是多项式时间可解的，并给出了高效算法。该解决方案启发了NP难阻断问题的图论松弛。鉴于最小生成树权重的增益是超模的，预算问题被视为一个受超模覆盖约束的线性函数最小化实例。他们利用图论松弛设计并分析了一种基于批处理贪婪的算法。", "result": "本文为最小生成树阻断的预算最小化和收益最大化版本提供了多项式时间对数近似保证。", "conclusion": "本文成功为最小生成树阻断问题的预算最小化和收益最大化版本提供了多项式时间对数近似保证，并通过研究相关可解问题、图论松弛和贪婪算法实现了这一目标。", "translation": "我们为最小生成树阻断问题的预算最小化以及收益最大化版本提供了多项式时间对数近似保证。在这个问题中，目标是移除无向图的一些边，这些边具有边权重和边成本，以增加最小生成树的权重。在预算最小化版本中，目标是最小化移除边的总成本，同时实现最小生成树权重所需的增加量$\\Delta$。在同一框架内的另一个目标是在预算约束下最大化阻断的收益，即最小生成树权重的增加量。对于类似的目标（最大化树的总成本，而不是增加量），已知有多项式时间$O(1)$近似保证。然而，该保证似乎不适用于成本的增加。此外，相同的技术似乎不适用于预算版本。我们的近似保证是通过研究以任意量增加最小生成树成本最小化的问题而得到的。我们表明，与预算和收益问题相反，这个版本的阻断是多项式时间可解的，并且我们提供了一种高效的算法来解决它。该解决方案促使了NP难阻断问题的图论松弛。最小生成树权重增益作为移除边集的函数是超模的。因此，预算问题是受超模覆盖约束的线性函数最小化实例。我们利用图论松弛来设计和分析一种基于批处理贪婪的算法。", "summary": "本文研究了最小生成树阻断问题，目标是通过移除图边来增加最小生成树的权重。针对预算最小化和收益最大化两个版本，作者提出了多项式时间对数近似算法。研究发现，将最小生成树权重增加任意量的最小成本问题是多项式时间可解的，这一发现启发了对NP难阻断问题的图论松弛。通过利用最小生成树权重增益的超模性质，并结合图论松弛，设计并分析了一种基于批处理贪婪的算法。", "keywords": "最小生成树阻断, 近似算法, 预算最小化, 收益最大化, 超模函数", "comments": "本文的创新之处在于为最小生成树阻断这一NP难问题提供了近似算法，特别是在之前近似保证不适用的“增加量”和“预算”情境下取得了对数近似保证。通过将问题转化为受超模覆盖约束的线性函数最小化实例，并结合图论松弛和贪婪算法，展现了理论分析与算法设计的结合。"}}
{"id": "2406.03785", "title": "Improving Count-Mean Sketch as the Leading Locally Differentially Private Frequency Estimator for Large Dictionaries", "authors": ["Mingen Pan"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      2025 IEEE Computer Security Foundations Symposium, 362-377", "url": "http://arxiv.org/abs/2406.03785v2", "summary": "This paper identifies that a group of latest locally-differentially-private\n(LDP) algorithms for frequency estimation, including all the\nHadamard-matrix-based algorithms, are equivalent to the private Count-Mean\nSketch (CMS) algorithm with different parameters. Therefore, we revisit the\nprivate CMS, correct errors in the original CMS paper regarding expectation and\nvariance, modify the CMS implementation to eliminate existing bias, and\noptimize CMS using randomized response (RR) as the perturbation method. The\noptimized CMS with RR is shown to outperform CMS variants with other known\nperturbations in reducing the worst-case mean squared error (MSE), $l_1$ loss,\nand $l_2$ loss. Additionally, we prove that pairwise-independent hashing is\nsufficient for CMS, reducing its communication cost to the logarithm of the\ncardinality of all possible values (i.e., a dictionary). As a result, the\noptimized CMS with RR is proven theoretically and empirically as the leading\nalgorithm for reducing the aforementioned loss functions when dealing with a\nvery large dictionary. Furthermore, we demonstrate that randomness is necessary\nto ensure the correctness of CMS, and the communication cost of CMS, though\nlow, is unavoidable despite the randomness being public or private.", "comment": "2025 IEEE Computer Security Foundations Symposium, 362-377", "pdf_url": "http://arxiv.org/pdf/2406.03785v2", "cate": "cs.CR", "date": "2024-06-06", "updated": "2025-07-25", "AI": {"title_translation": "改进Count-Mean Sketch作为大型字典领先的局部差分隐私频率估算器", "tldr": "本文重新审视并优化了私有Count-Mean Sketch (CMS) 算法，通过纠错、去偏和引入随机响应，使其在处理大型字典的频率估计中成为性能领先的局部差分隐私算法。", "motivation": "识别出最新一批局部差分隐私（LDP）频率估计算法（包括基于Hadamard矩阵的算法）与私有Count-Mean Sketch (CMS) 算法在不同参数下是等价的，这促使作者重新审视并改进CMS。", "method": "重新审视私有CMS，纠正原始论文中关于期望和方差的错误，修改CMS实现以消除偏差，并使用随机响应（RR）作为扰动方法来优化CMS。此外，证明了对CMS而言，成对独立哈希足以降低通信成本。", "result": "优化后的CMS结合RR在降低最差情况下的均方误差（MSE）、$l_1$ 损失和 $l_2$ 损失方面优于其他已知扰动的CMS变体。理论和经验证明，优化后的CMS结合RR是处理大型字典时减少上述损失函数的领先算法。此外，还证明了随机性对于CMS的正确性是必需的，且CMS的通信成本虽然低但不可避免。", "conclusion": "优化后的Count-Mean Sketch (CMS) 结合随机响应（RR）在理论和实践上都被证明是处理大型字典时，在减少均方误差、$l_1$ 和 $l_2$ 损失方面的领先局部差分隐私频率估计算法。", "translation": "本文指出，一组最新的局部差分隐私（LDP）频率估计算法，包括所有基于Hadamard矩阵的算法，在不同参数下都等同于私有Count-Mean Sketch (CMS) 算法。因此，我们重新审视了私有CMS，纠正了原始CMS论文中关于期望和方差的错误，修改了CMS实现以消除现有偏差，并使用随机响应（RR）作为扰动方法优化了CMS。结果表明，优化后的CMS与RR在降低最差情况下的均方误差（MSE）、$l_1$ 损失和 $l_2$ 损失方面优于其他已知扰动的CMS变体。此外，我们证明了成对独立哈希对于CMS来说是足够的，将其通信成本降低到所有可能值（即字典）基数的对数。因此，理论和经验证明，优化后的CMS与RR是处理超大型字典时，在降低上述损失函数方面的领先算法。此外，我们证明了随机性对于确保CMS的正确性是必要的，并且CMS的通信成本虽然低，但无论是公共随机性还是私有随机性，都是不可避免的。", "summary": "本文发现最新的LDP频率估计算法与私有Count-Mean Sketch (CMS) 等价，因此对CMS进行了深入改进。通过纠正错误、消除偏差并引入随机响应（RR），优化后的CMS在降低MSE、$l_1$ 和 $l_2$ 损失方面表现出卓越性能，并被证明是处理大型字典的领先算法。研究还指出，成对独立哈希足以降低通信成本，且随机性对CMS的正确性至关重要，其通信成本不可避免。", "keywords": "局部差分隐私, 频率估计, Count-Mean Sketch, 随机响应, 大字典", "comments": "这篇论文通过深入分析和优化已有的Count-Mean Sketch算法，使其在局部差分隐私频率估计领域取得了显著的领先地位。其创新点在于识别出多种新算法与CMS的等价性，并在此基础上系统地纠正了CMS的缺陷，引入了有效的扰动方法（RR），并优化了哈希策略以降低通信成本。这对于在大规模数据环境下实现隐私保护的频率估计具有重要意义。"}}
{"id": "2507.18897", "title": "HH-Codec: High Compression High-fidelity Discrete Neural Codec for Spoken Language Modeling", "authors": ["Rongkun Xue", "Yazhe Niu", "Shuai Hu", "Zixin Yin", "Yongqiang Yao", "Jing Yang"], "categories": ["cs.SD", "cs.AI", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18897v1", "summary": "Discrete speech tokenization is a fundamental component in speech codecs.\nHowever, in large-scale speech-to-speech systems, the complexity of parallel\nstreams from multiple quantizers and the computational cost of\nhigh-time-dimensional codecs pose significant challenges. In this paper, we\nintroduce HH-Codec, a neural codec that achieves extreme compression at 24\ntokens per second for 24 kHz audio while relying on single-quantizer inference.\nOur approach involves a carefully designed Vector Quantization space for Spoken\nLanguage Modeling, optimizing compression efficiency while minimizing\ninformation loss. Building on this, we propose an asymmetric encoder-decoder\narchitecture (Audio-VQ-Mel-Audio) that leverages dual supervision and\nprogressive training to enhance reconstruction stability and fidelity. HH-Codec\nachieves state-of-the-art performance in speech reconstruction with an\nultra-low bandwidth of 0.3 kbps. We further evaluate its effectiveness in\ncodebook utilization and generative model adaptation, with extensive ablations\nvalidating the necessity of each module. HH-Codec is available at\nhttps://github.com/opendilab/HH-Codec.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18897v1", "cate": "cs.SD", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "HH-Codec：用于口语建模的高压缩高保真离散神经编解码器", "tldr": "HH-Codec是一种新的神经编解码器，通过单量化器推理实现24 kHz音频的极高压缩（0.3 kbps）和高保真语音重建，解决了现有大规模语音系统中的复杂性和计算成本问题。", "motivation": "在 KHz 大规模语音到语音系统中，多量化器并行流的复杂性和高时间维度编解码器的计算成本带来了显著挑战。", "method": "本文引入了HH-Codec，这是一种神经编解码器，它在24 kHz音频下以每秒24个token的速度实现极致压缩，同时依赖于单量化器推理。其方法包括为口语建模精心设计的向量量化空间，以优化压缩效率并最小化信息损失。在此基础上，提出了一种非对称编码器-解码器架构（Audio-VQ-Mel-Audio），该架构利用双重监督和渐进式训练来增强重建稳定性和保真度。", "result": "HH-Codec在语音重建方面达到了最先进的性能，带宽超低至0.3 kbps。它在码本利用和生成模型适应性方面也表现出有效性，并通过广泛的消融实验验证了每个模块的必要性。", "conclusion": "HH-Codec通过其创新的设计（如单量化器推理、优化的VQ空间和非对称架构），成功解决了大规模语音系统中的压缩和计算挑战，实现了高保真、超低带宽的语音重建，并达到了最先进的性能。", "translation": "离散语音分词是语音编解码器中的基本组成部分。然而，在大规模语音到语音系统中，来自多个量化器的并行流的复杂性以及高时间维度编解码器的计算成本带来了显著挑战。在本文中，我们引入了HH-Codec，这是一种神经编解码器，它在24 kHz音频下以每秒24个token的速度实现极致压缩，同时依赖于单量化器推理。我们的方法涉及为口语建模精心设计的向量量化空间，优化压缩效率同时最小化信息损失。在此基础上，我们提出了一种非对称编码器-解码器架构（Audio-VQ-Mel-Audio），该架构利用双重监督和渐进式训练来增强重建稳定性和保真度。HH-Codec在语音重建方面以0.3 kbps的超低带宽实现了最先进的性能。我们进一步评估了其在码本利用和生成模型适应性方面的有效性，并通过广泛的消融实验验证了每个模块的必要性。HH-Codec可在https://github.com/opendilab/HH-Codec获取。", "summary": "本文介绍了HH-Codec，一种针对口语建模设计的新型神经编解码器，旨在解决大规模语音系统中多量化器复杂性和计算成本高的挑战。HH-Codec通过单量化器推理，实现了24 kHz音频的极高压缩（24 tokens/秒），同时保持高保真度。其核心创新包括精心设计的向量量化空间和非对称编码器-解码器架构（Audio-VQ-Mel-Audio），并结合双重监督和渐进式训练。该方法在语音重建方面达到了0.3 kbps的超低带宽下的最先进性能，并有效支持码本利用和生成模型适应。", "keywords": "神经编解码器, 语音压缩, 向量量化, 口语建模, 高保真", "comments": "HH-Codec的创新点在于其单量化器推理设计，这显著降低了大规模语音系统的复杂性和计算成本。通过精心设计的VQ空间和非对称编码器-解码器架构，它在实现极高压缩的同时保持了语音重建的高保真度，0.3 kbps的超低带宽是一个重要的突破。这对于资源受限或需要高效传输的语音应用具有重要意义。"}}
{"id": "2507.19052", "title": "Probing Multimodal Fusion in the Brain: The Dominance of Audiovisual Streams in Naturalistic Encoding", "authors": ["Hamid Abdollahi", "Amir Hossein Mansouri Majoumerd", "Amir Hossein Bagheri Baboukani", "Amir Abolfazl Suratgar", "Mohammad Bagher Menhaj"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19052v1", "summary": "Predicting brain activity in response to naturalistic, multimodal stimuli is\na key challenge in computational neuroscience. While encoding models are\nbecoming more powerful, their ability to generalize to truly novel contexts\nremains a critical, often untested, question. In this work, we developed brain\nencoding models using state-of-the-art visual (X-CLIP) and auditory (Whisper)\nfeature extractors and rigorously evaluated them on both in-distribution (ID)\nand diverse out-of-distribution (OOD) data. Our results reveal a fundamental\ntrade-off between model complexity and generalization: a higher-capacity\nattention-based model excelled on ID data, but a simpler linear model was more\nrobust, outperforming a competitive baseline by 18\\% on the OOD set.\nIntriguingly, we found that linguistic features did not improve predictive\naccuracy, suggesting that for familiar languages, neural encoding may be\ndominated by the continuous visual and auditory streams over redundant textual\ninformation. Spatially, our approach showed marked performance gains in the\nauditory cortex, underscoring the benefit of high-fidelity speech\nrepresentations. Collectively, our findings demonstrate that rigorous OOD\ntesting is essential for building robust neuro-AI models and provides nuanced\ninsights into how model architecture, stimulus characteristics, and sensory\nhierarchies shape the neural encoding of our rich, multimodal world.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19052v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "探测大脑中的多模态融合：视听流在自然编码中的主导地位", "tldr": "本研究开发并评估了使用先进视听特征提取器的脑编码模型，发现模型复杂性和泛化能力之间存在权衡，简单线性模型在OOD数据上表现更佳。研究还表明视听流在大脑自然编码中占据主导地位，语言特征未能提高预测准确性。", "motivation": "在计算神经科学中，预测大脑对自然多模态刺激的活动是一个关键挑战。尽管编码模型日益强大，但其泛化到全新情境的能力仍是未经验证的关键问题。", "method": "本研究开发了脑编码模型，使用了最先进的视觉（X-CLIP）和听觉（Whisper）特征提取器。模型在分布内（ID）和多样化的分布外（OOD）数据上进行了严格评估。", "result": "研究结果揭示了模型复杂性和泛化能力之间的基本权衡：高容量的基于注意力的模型在ID数据上表现出色，但更简单的线性模型更具鲁棒性，在OOD数据集上比竞争基线高出18%。有趣的是，语言特征未能提高预测准确性，表明对于熟悉的语言，神经编码可能由连续的视觉和听觉流主导，而非冗余的文本信息。在空间上，该方法在听觉皮层显示出显著的性能提升，突显了高保真语音表征的益处。", "conclusion": "研究结果表明，严格的OOD测试对于构建鲁棒的神经-AI模型至关重要，并提供了关于模型架构、刺激特性和感觉层级如何塑造我们丰富多模态世界的神经编码的细致见解。", "translation": "预测大脑对自然、多模态刺激的活动是计算神经科学中的一个关键挑战。尽管编码模型变得越来越强大，但它们泛化到真正新颖情境的能力仍然是一个关键且常常未经验证的问题。在这项工作中，我们使用最先进的视觉（X-CLIP）和听觉（Whisper）特征提取器开发了大脑编码模型，并在分布内（ID）和多样化的分布外（OOD）数据上对其进行了严格评估。我们的结果揭示了模型复杂性和泛化能力之间存在一个基本权衡：高容量的基于注意力的模型在ID数据上表现出色，但一个更简单的线性模型更具鲁棒性，在OOD集上比竞争基线高出18%。有趣的是，我们发现语言特征并未提高预测准确性，这表明对于熟悉的语言，神经编码可能由连续的视觉和听觉流而非冗余的文本信息主导。在空间上，我们的方法在听觉皮层显示出显著的性能提升，突显了高保真语音表征的益处。总的来说，我们的发现表明，严格的OOD测试对于构建鲁棒的神经-AI模型至关重要，并提供了关于模型架构、刺激特性和感觉层级如何塑造我们丰富、多模态世界的神经编码的细致见解。", "summary": "本研究旨在解决预测大脑对自然多模态刺激活动的关键挑战，特别关注编码模型的泛化能力。研究团队开发了基于X-CLIP和Whisper特征提取器的脑编码模型，并在分布内（ID）和分布外（OOD）数据上进行了严格评估。结果显示，模型复杂性与泛化能力之间存在权衡，其中简单的线性模型在OOD数据上表现出更强的鲁棒性。此外，研究发现语言特征并未提高预测准确性，暗示视听流在神经编码中占据主导地位。本研究强调了严格的OOD测试对于构建鲁棒神经-AI模型的重要性，并深入探讨了模型架构、刺激特性和感觉层级对神经编码的影响。", "keywords": "多模态融合, 脑编码, 视听流, 泛化, OOD测试", "comments": "这项工作通过对脑编码模型进行严格的分布外（OOD）测试，强调了模型泛化能力的重要性，这是当前AI模型面临的关键挑战。其创新之处在于利用先进的视听特征提取器，并揭示了模型复杂性与泛化能力之间的权衡。此外，发现视听流在大脑自然编码中的主导作用，以及语言特征的局限性，为理解多模态信息在大脑中的处理方式提供了新的见解。对于构建更鲁棒和生物学上更合理的神经-AI模型具有重要指导意义。"}}
{"id": "2507.18762", "title": "The Role of Orthographic Consistency in Multilingual Embedding Models for Text Classification in Arabic-Script Languages", "authors": ["Abdulhady Abas Abdullah", "Amir H. Gandomi", "Tarik A Rashid", "Seyedali Mirjalili", "Laith Abualigah", "Milena Živković", "Hadi Veisi"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18762v1", "summary": "In natural language processing, multilingual models like mBERT and\nXLM-RoBERTa promise broad coverage but often struggle with languages that share\na script yet differ in orthographic norms and cultural context. This issue is\nespecially notable in Arabic-script languages such as Kurdish Sorani, Arabic,\nPersian, and Urdu. We introduce the Arabic Script RoBERTa (AS-RoBERTa) family:\nfour RoBERTa-based models, each pre-trained on a large corpus tailored to its\nspecific language. By focusing pre-training on language-specific script\nfeatures and statistics, our models capture patterns overlooked by\ngeneral-purpose models. When fine-tuned on classification tasks, AS-RoBERTa\nvariants outperform mBERT and XLM-RoBERTa by 2 to 5 percentage points. An\nablation study confirms that script-focused pre-training is central to these\ngains. Error analysis using confusion matrices shows how shared script traits\nand domain-specific content affect performance. Our results highlight the value\nof script-aware specialization for languages using the Arabic script and\nsupport further work on pre-training strategies rooted in script and language\nspecificity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18762v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "阿拉伯文字语言中多语言嵌入模型正字法一致性在文本分类中的作用", "tldr": "针对阿拉伯文字语言，AS-RoBERTa模型通过特定语言的文字预训练，在文本分类任务上优于通用多语言模型。", "motivation": "通用多语言模型（如mBERT和XLM-RoBERTa）在共享文字但正字法规范和文化背景不同的语言（特别是阿拉伯文字语言）上表现不佳。", "method": "引入阿拉伯文字RoBERTa（AS-RoBERTa）家族：四个RoBERTa模型，每个模型都在针对其特定语言的大型语料库上进行预训练，专注于语言特定的文字特征和统计数据。", "result": "AS-RoBERTa变体在分类任务中比mBERT和XLM-RoBERTa高出2到5个百分点。消融研究证实文字聚焦的预训练是性能提升的关键。", "conclusion": "结果强调了针对使用阿拉伯文字的语言进行文字感知专业化的价值，并支持进一步开展基于文字和语言特异性的预训练策略研究。", "translation": "在自然语言处理中，mBERT和XLM-RoBERTa等多语言模型承诺广泛覆盖，但通常难以处理共享文字但在正字法规范和文化背景上有所差异的语言。这个问题在库尔德索拉尼语、阿拉伯语、波斯语和乌尔都语等阿拉伯文字语言中尤为突出。我们引入了阿拉伯文字RoBERTa（AS-RoBERTa）家族：四个基于RoBERTa的模型，每个模型都在针对其特定语言的大型语料库上进行了预训练。通过将预训练重点放在语言特定的文字特征和统计数据上，我们的模型捕获了通用模型所忽视的模式。在文本分类任务上进行微调时，AS-RoBERTa变体比mBERT和XLM-RoBERTa高出2到5个百分点。一项消融研究证实，以文字为中心的预训练是这些收益的核心。使用混淆矩阵进行的错误分析显示了共享文字特征和特定领域内容如何影响性能。我们的结果强调了针对使用阿拉伯文字的语言进行文字感知专业化的价值，并支持进一步开展基于文字和语言特异性的预训练策略研究。", "summary": "本文针对mBERT和XLM-RoBERTa等通用多语言模型在阿拉伯文字语言文本分类任务中表现不佳的问题，提出了AS-RoBERTa家族模型。这些模型通过对特定语言的大型语料库进行文字特征和统计数据预训练，有效地捕获了通用模型忽略的模式。实验结果表明，AS-RoBERTa在分类任务中显著优于现有模型，证实了文字聚焦预训练的重要性，并强调了文字感知专业化对于阿拉伯文字语言的价值。", "keywords": "阿拉伯文字语言, 多语言模型, 文本分类, RoBERTa, 文字感知预训练", "comments": "本文的创新点在于提出了针对特定文字系统（特别是阿拉伯文字）进行语言特定预训练的策略，而不是依赖于通用的多语言模型。这对于处理具有共同文字但语言内部差异较大的语言家族具有重要意义，揭示了文字层面的细致处理对模型性能提升的关键作用。"}}
{"id": "2507.19284", "title": "Relaxed Total Generalized Variation Regularized Piecewise Smooth Mumford-Shah Model for Triangulated Surface Segmentation", "authors": ["Huayan Zhang", "Shanqiang Wang", "Xiaochao Wang"], "categories": ["cs.CG", "cs.CV"], "primary_category": "Subjects:       Computational Geometry (cs.CG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19284v1", "summary": "The Mumford-Shah (MS) model is an important technique for mesh segmentation.\nMany existing researches focus on piecewise constant MS mesh segmentation model\nwith total variation regularization, which pursue the shortest length of\nboundaries. Different from previous efforts, in this article, we propose a\nnovel piecewise smooth MS mesh segmentation model by utilizing the relaxed\ntotal generalized variation regularization (rTGV). The new model assumes that\nthe feature function of a mesh can be approximated by the sum of piecewise\nconstant function and asmooth function, and the rTGV regularization is able to\ncharacterize the high order discontinuity of the geometric structure. The newly\nintroduced method is effective in segmenting meshes with irregular structures\nand getting the better boundaries rather than the shortest boundaries. We solve\nthe new model by alternating minimization and alternating direction method of\nmultipliers (ADMM). Our algorithm is discussed from several aspects, and\ncomparisons with several state-of-art methods. Experimental results show that\nour method can yield competitive results when compared to other approaches. In\naddition, our results compare favorably to those of the several state-of-art\ntechniques when evaluated on the Princeton Segmentation Benchmark. Furthermore,\nthe quantitative errors and computational costs confirm the robustness and\nefficiency of the proposed method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19284v1", "cate": "cs.CG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "松弛全广义变分正则化分段光滑Mumford-Shah模型用于三角曲面分割", "tldr": "本文提出了一种基于松弛全广义变分正则化（rTGV）的分段光滑Mumford-Shah（MS）网格分割模型，旨在获得更优而非最短的边界，并有效处理不规则结构。该模型通过交替最小化和ADMM求解，实验结果表明其性能与现有先进方法相当。", "motivation": "现有的Mumford-Shah（MS）网格分割研究多集中于采用全变分正则化的分段常数MS模型，其目标是追求最短的边界长度。本文旨在提出一种新的分段光滑MS网格分割模型，以解决现有方法在处理不规则结构和获得“更好”而非仅仅“最短”边界方面的局限性。", "method": "本文提出了一种新颖的分段光滑Mumford-Shah（MS）网格分割模型，该模型利用了松弛全广义变分正则化（rTGV）。新模型假设网格的特征函数可以近似为分段常数函数和光滑函数之和，并且rTGV正则化能够表征几何结构的高阶不连续性。该模型通过交替最小化和交替方向乘子法（ADMM）进行求解。", "result": "所提出的方法能够有效地分割具有不规则结构的网格，并获得比最短边界更好的边界。实验结果表明，与现有方法相比，本文方法能够产生具有竞争力的结果。在Princeton分割基准测试中，其结果优于几种最先进的技术。此外，量化误差和计算成本也证实了该方法的鲁棒性和效率。", "conclusion": "本文提出的基于松弛全广义变分正则化的分段光滑Mumford-Shah网格分割模型，在处理不规则网格结构和获得高质量边界方面表现出显著效果。该方法具有良好的鲁棒性和计算效率，在与现有先进方法的比较中展现出竞争力。", "translation": "Mumford-Shah（MS）模型是网格分割的重要技术。许多现有研究侧重于带有全变分正则化的分段常数MS网格分割模型，该模型追求最短的边界长度。与以往的工作不同，本文提出了一种利用松弛全广义变分正则化（rTGV）的新型分段光滑MS网格分割模型。新模型假设网格的特征函数可以由分段常数函数和光滑函数的和来近似，并且rTGV正则化能够表征几何结构的高阶不连续性。新引入的方法在分割不规则结构的网格和获得更好的边界而非最短边界方面是有效的。我们通过交替最小化和交替方向乘子法（ADMM）来求解新模型。本文从几个方面讨论了我们的算法，并与几种最先进的方法进行了比较。实验结果表明，与其它方法相比，我们的方法可以产生具有竞争力的结果。此外，在Princeton分割基准上进行评估时，我们的结果优于几种最先进的技术。此外，量化误差和计算成本证实了所提出方法的鲁棒性和效率。", "summary": "本文提出了一种新颖的分段光滑Mumford-Shah（MS）网格分割模型，该模型引入了松弛全广义变分正则化（rTGV）。与传统追求最短边界的分段常数MS模型不同，新模型能够更好地处理网格的不规则结构，并获得更优的分割边界。通过假设网格特征函数为分段常数与光滑函数之和，并利用rTGV表征高阶不连续性，该方法在分割效果上有所提升。模型通过交替最小化和ADMM算法求解，并在实验中展现出与现有先进技术相当甚至更优的性能，同时具有良好的鲁棒性和计算效率。", "keywords": "Mumford-Shah, 网格分割, 全广义变分, 分段光滑, ADMM", "comments": "本文的创新之处在于将Mumford-Shah模型从传统的分段常数扩展到分段光滑，并引入了松弛全广义变分（rTGV）正则化。这一改进使得模型能够更好地捕获几何结构的高阶不连续性，从而在网格分割中获得“更好”而非仅仅“最短”的边界，这对于处理复杂和不规则的网格结构具有重要意义。通过交替最小化和ADMM的优化求解策略也保证了算法的实用性和效率。"}}
{"id": "2507.19043", "title": "Dynamic distributed decision-making for resilient resource reallocation in disrupted manufacturing systems", "authors": ["Mingjie Bi", "Ilya Kovalenko", "Dawn M. Tilbury", "Kira Barton"], "categories": ["cs.MA", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19043v1", "summary": "The COVID-19 pandemic brings many unexpected disruptions, such as frequently\nshifting markets and limited human workforce, to manufacturers. To stay\ncompetitive, flexible and real-time manufacturing decision-making strategies\nare needed to deal with such highly dynamic manufacturing environments. One\nessential problem is dynamic resource allocation to complete production tasks,\nespecially when a resource disruption (e.g., machine breakdown) occurs. Though\nmulti-agent methods have been proposed to solve the problem in a flexible and\nagile manner, the agent internal decision-making process and resource\nuncertainties have rarely been studied. This work introduces a model-based\nresource agent (RA) architecture that enables effective agent coordination and\ndynamic agent decision-making. Based on the RA architecture, a rescheduling\nstrategy that incorporates risk assessment via a clustering agent coordination\nstrategy is also proposed. A simulation-based case study is implemented to\ndemonstrate dynamic rescheduling using the proposed multi-agent framework. The\nresults show that the proposed method reduces the computational efforts while\nlosing some throughput optimality compared to the centralized method.\nFurthermore, the case study illustrates that incorporating risk assessment into\nrescheduling decision-making improves the throughput.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19043v1", "cate": "cs.MA", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "动态分布式决策用于中断制造系统中的弹性资源再分配", "tldr": "本文提出了一种基于模型资源代理（RA）架构的动态分布式决策方法，用于在中断制造系统中进行弹性资源再分配，通过引入风险评估提高了吞吐量，并减少了计算量。", "motivation": "面对COVID-19大流行带来的市场变化和劳动力限制等意外中断，制造商需要灵活、实时的制造决策策略来应对高度动态的制造环境，尤其是在资源中断（如机器故障）发生时的动态资源分配问题。现有的大多数多智能体方法很少研究智能体内部决策过程和资源不确定性。", "method": "本文提出了一种基于模型的资源代理（RA）架构，以实现有效的智能体协调和动态智能体决策。基于该RA架构，提出了一种通过聚类智能体协调策略整合风险评估的重调度策略。通过一个基于仿真的案例研究来验证所提出的多智能体框架。", "result": "与集中式方法相比，所提出的方法在计算量上有所减少，但牺牲了一些吞吐量最优性。此外，案例研究表明，将风险评估纳入重调度决策可以提高吞吐量。", "conclusion": "所提出的动态分布式决策方法，特别是通过整合风险评估的重调度策略，能够有效应对制造系统中的资源中断，在计算效率和吞吐量之间取得平衡，并通过风险评估提升性能。", "translation": "COVID-19大流行给制造商带来了许多意想不到的中断，例如频繁变化的市场和有限的人力资源。为了保持竞争力，需要灵活和实时的制造决策策略来应对这种高度动态的制造环境。一个基本问题是动态资源分配以完成生产任务，尤其是在发生资源中断（例如机器故障）时。尽管已经提出了多智能体方法以灵活敏捷的方式解决该问题，但很少研究智能体内部决策过程和资源不确定性。这项工作引入了一种基于模型的资源代理（RA）架构，该架构能够实现有效的智能体协调和动态智能体决策。基于RA架构，还提出了一种通过聚类智能体协调策略整合风险评估的重调度策略。实施了一个基于仿真的案例研究，以演示使用所提出的多智能体框架进行的动态重调度。结果表明，与集中式方法相比，所提出的方法减少了计算量，同时损失了一些吞吐量最优性。此外，案例研究表明，将风险评估纳入重调度决策可以提高吞吐量。", "summary": "本文针对COVID-19大流行背景下制造系统面临的资源中断和动态环境挑战，提出了一种基于模型的资源代理（RA）架构，以实现分布式、动态的资源再分配决策。该架构支持有效的智能体协调和内部决策，并引入了结合风险评估的重调度策略。仿真研究表明，该方法在降低计算复杂度的同时，通过风险评估提升了系统吞吐量，为弹性制造提供了新的解决方案。", "keywords": "动态决策, 分布式系统, 资源再分配, 制造系统, 中断管理, 多智能体, 风险评估", "comments": "这项工作通过引入基于模型的资源代理架构和整合风险评估的重调度策略，为处理动态和中断的制造系统中的资源分配问题提供了一个创新的分布式解决方案。其重要性在于，它不仅关注了多智能体协调，还深入探讨了智能体内部决策过程和资源不确定性。虽然它在计算效率上有所提升，但牺牲了一些吞吐量最优性，这可能是一个值得进一步研究的权衡点。然而，风险评估的引入显著提升了性能，这表明了其在实际应用中的潜力。"}}
{"id": "2405.03572", "title": "RoboCar: A Rapidly Deployable Open-Source Platform for Autonomous Driving Research", "authors": ["Mehdi Testouri", "Gamal Elghazaly", "Raphael Frank"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2405.03572v3", "summary": "This paper introduces RoboCar, an open-source research platform for\nautonomous driving developed at the University of Luxembourg. RoboCar provides\na modular, cost-effective framework for the development of experimental\nAutonomous Driving Systems (ADS), utilizing the 2018 KIA Soul EV. The platform\nintegrates a robust hardware and software architecture that aligns with the\nvehicle's existing systems, minimizing the need for extensive modifications. It\nsupports various autonomous driving functions and has undergone real-world\ntesting on public roads in Luxembourg City. This paper outlines the platform's\narchitecture, integration challenges, and initial test results, offering\ninsights into its application in advancing autonomous driving research. RoboCar\nis available to anyone at https://github.com/sntubix/robocar and is released\nunder an open-source MIT license.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2405.03572v3", "cate": "cs.RO", "date": "2024-05-06", "updated": "2025-07-25", "AI": {"title_translation": "RoboCar：一个用于自动驾驶研究的快速部署开源平台", "tldr": "RoboCar是一个在卢森堡大学开发的开源自动驾驶研究平台，旨在提供一个模块化、成本效益高的实验性自动驾驶系统开发框架，并已在实际道路上进行了测试。", "motivation": "为了提供一个模块化、成本效益高的实验性自动驾驶系统（ADS）开发框架，并最小化对现有车辆系统进行广泛修改的需求。", "method": "本文介绍了RoboCar平台，该平台利用2018款起亚Soul EV，集成了一个强大的硬件和软件架构，与车辆现有系统对齐。它支持各种自动驾驶功能，并在卢森堡市的公共道路上进行了实际测试。论文概述了平台的架构、集成挑战和初步测试结果。", "result": "RoboCar平台已在卢森堡市的公共道路上进行了实际测试，并提供了平台架构、集成挑战和初步测试结果的见解。", "conclusion": "RoboCar是一个可供研究人员使用的开源平台，为推进自动驾驶研究提供了宝贵的工具和见解。", "translation": "本文介绍了RoboCar，一个由卢森堡大学开发的用于自动驾驶的开源研究平台。RoboCar为实验性自动驾驶系统（ADS）的开发提供了一个模块化、成本效益高的框架，利用2018款起亚Soul EV。该平台集成了强大的硬件和软件架构，与车辆现有系统对齐，最大限度地减少了大量修改的需求。它支持各种自动驾驶功能，并已在卢森堡市的公共道路上进行了实际测试。本文概述了平台的架构、集成挑战和初步测试结果，为推进自动驾驶研究的应用提供了见解。RoboCar可在https://github.com/sntubix/robocar上获取，并根据MIT开源许可证发布。", "summary": "RoboCar是卢森堡大学开发的一个开源自动驾驶研究平台，它提供了一个基于2018款起亚Soul EV的模块化、成本效益高的框架，用于开发和测试自动驾驶系统。该平台集成了与车辆现有系统兼容的软硬件架构，已在公共道路上进行了测试，并分享了其架构、集成挑战和初步测试结果，旨在推动自动驾驶研究。", "keywords": "RoboCar, 自动驾驶, 开源平台, 研究平台, 卢森堡大学", "comments": "RoboCar的创新之处在于其作为开源平台的特性，降低了自动驾驶研究的门槛。其模块化和成本效益高的设计，以及与现有车辆系统的兼容性，使其成为一个实用的研究工具。在公共道路上的实际测试增加了其可靠性。"}}
{"id": "2507.19213", "title": "PRE-MAP: Personalized Reinforced Eye-tracking Multimodal LLM for High-Resolution Multi-Attribute Point Prediction", "authors": ["Hanbing Wu", "Ping Jiang", "Anyang Su", "Chenxu Zhao", "Tianyu Fu", "Minghui Wu", "Beiping Tan", "Huiying Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19213v1", "summary": "Visual selective attention, driven by individual preferences, regulates human\nprioritization of visual stimuli by bridging subjective cognitive mechanisms\nwith objective visual elements, thereby steering the semantic interpretation\nand hierarchical processing of dynamic visual scenes. However, existing models\nand datasets predominantly neglect the influence of subjective cognitive\ndiversity on fixation behavior. Conventional saliency prediction models,\ntypically employing segmentation approaches, rely on low-resolution imagery to\ngenerate saliency heatmaps, subsequently upscaled to native resolutions, which\nlimiting their capacity to capture personalized attention patterns.\nFurthermore, MLLMs are constrained by factors such as hallucinations, making it\nvery costly to strictly adhere to the expected format in tasks involving\nmultiple point predictions, and achieving precise point positioning is\nchallenging. To address these limitations, we present Subjective Personalized\nAttention for Advertisement Videos, namely SPA-ADV, a large-scale multimodal\ndataset capturing gaze behaviors from over 4,500 participants varying in age\nand gender with 486 videos. Furthermore, we propose PRE-MAP, a novel\neye-tracking saliency model that characterizes Personalized visual disparities\nthrough Reinforcement learning-optimized Eye-tracking, built upon MLLMs and\nguided by Multi-Attribute user profiles to predict Points. To ensure MLLMs\nproduce prediction points that are both format-correct and spatially accurate,\nwe introduce Consistency Group Relative Policy Optimization (C-GRPO), inspired\nby the variability in eye movement points and Multi-Attribute profiles.\nExtensive experiments on SPA-ADV and other benchmarks demonstrate the\neffectiveness of our approach. The code and dataset are available at\n\\href{https://github.com/mininglamp-MLLM/PRE-MAP}{this URL}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19213v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "PRE-MAP：个性化强化眼动多模态大语言模型，用于高分辨率多属性点预测", "tldr": "本文提出了PRE-MAP模型和SPA-ADV数据集，以解决现有模型在个性化视觉选择性注意预测中对主观认知多样性的忽视、低分辨率限制以及多模态大语言模型在多点预测中的挑战。", "motivation": "现有的模型和数据集忽视了主观认知多样性对注视行为的影响；传统显著性预测模型依赖低分辨率图像，限制了捕捉个性化注意模式的能力；多模态大语言模型（MLLMs）存在幻觉问题，难以严格遵守多点预测任务的预期格式，且难以实现精确点定位。", "method": "本文提出了SPA-ADV，一个大规模多模态数据集，包含来自4500多名不同年龄和性别的参与者在486个视频中的注视行为。此外，本文提出了PRE-MAP，一个新颖的眼动显著性模型，该模型基于多模态大语言模型构建，通过强化学习优化的眼动来表征个性化视觉差异，并由多属性用户画像引导进行点预测。为确保多模态大语言模型生成格式正确且空间准确的预测点，本文引入了受眼动点变异性和多属性画像启发的“一致性组相对策略优化”（C-GRPO）。", "result": "在SPA-ADV和其他基准数据集上的大量实验证明了本文方法的有效性。", "conclusion": "本文提出的PRE-MAP模型和SPA-ADV数据集有效解决了现有模型在个性化视觉选择性注意预测方面的局限性，特别是在结合主观认知多样性、提高预测分辨率和克服多模态大语言模型在多点预测中的挑战方面表现出显著效果。", "translation": "视觉选择性注意由个体偏好驱动，通过连接主观认知机制与客观视觉元素来调节人类对视觉刺激的优先级，从而引导动态视觉场景的语义解释和分层处理。然而，现有模型和数据集主要忽视了主观认知多样性对注视行为的影响。传统的显著性预测模型通常采用分割方法，依赖低分辨率图像生成显著性热图，随后上采样到原始分辨率，这限制了它们捕捉个性化注意模式的能力。此外，多模态大语言模型（MLLMs）受到幻觉等因素的限制，使得在涉及多点预测的任务中严格遵守预期格式的成本非常高，并且实现精确的点定位具有挑战性。为了解决这些局限性，我们提出了广告视频的主观个性化注意力（SPA-ADV），这是一个大规模多模态数据集，捕捉了来自4500多名不同年龄和性别的参与者在486个视频中的注视行为。此外，我们提出了PRE-MAP，一个新颖的眼动显著性模型，该模型通过强化学习优化的眼动来表征个性化视觉差异，构建于多模态大语言模型之上，并由多属性用户画像引导进行点预测。为了确保多模态大语言模型生成格式正确且空间准确的预测点，我们引入了受眼动点变异性和多属性画像启发的“一致性组相对策略优化”（C-GRPO）。在SPA-ADV和其他基准数据集上的大量实验证明了我们方法的有效性。代码和数据集可在[此URL](https://github.com/mininglamp-MLLM/PRE-MAP)获取。", "summary": "本文针对现有视觉选择性注意模型忽视主观认知多样性、低分辨率限制以及多模态大语言模型在多点预测中的挑战，提出了PRE-MAP模型和一个大规模多模态数据集SPA-ADV。PRE-MAP是一个新颖的眼动显著性模型，结合强化学习和多属性用户画像，基于多模态大语言模型实现高分辨率多点预测，并通过C-GRPO确保预测点的准确性。实验证明了该方法的有效性。", "keywords": "个性化注意, 眼动追踪, 多模态大语言模型, 显著性预测, 强化学习", "comments": "本文的创新点在于构建了一个大规模、包含主观认知多样性的眼动数据集SPA-ADV，并提出了PRE-MAP模型，该模型首次将强化学习应用于MLLM框架下的眼动显著性预测，并通过C-GRPO解决了MLLM在多点预测中格式和空间准确性的挑战。这对于个性化视觉注意力研究和广告效果评估具有重要意义。"}}
{"id": "2507.12106", "title": "Urban Green Governance: IoT-Driven Management and Enhancement of Urban Green Spaces in Campobasso", "authors": ["Antonio Salis", "Gabriele Troina", "Gianluca Boanelli", "Marco Ottaviano", "Paola Fortini", "Soraya Versace"], "categories": ["cs.DC", "cs.CY"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      18 pages, 6 Figures", "url": "http://arxiv.org/abs/2507.12106v4", "summary": "The efficient design and management of public green spaces is a key factor in\npromoting the health and well-being of urban population, as emphasized by the\nWHO, UNEP, and EEA. These areas serve as the \"green lungs\" of the urban\necosystem, playing a vital role in enhancing quality of life thanks to the\nprovision of ecosystem services. In this context, the Smart Green City use case\nin Campobasso municipality, funded by the Italian Ministry of Enterprises\n(MIMIT), emerges as an innovative model for the sustainable management of green\nurban areas through the adoption of an advanced system of emerging technologies\nintegrated and interoperable. The project integrates IoT systems and\ndata-driven governance platforms, enabling real-time monitoring of the health\nstatus of trees and green areas via a Decision Support System (DSS). It also\nfacilitates the collection and analysis of data from diverse sources, including\nweather conditions, air quality, soil moisture, pollution levels. The resulting\ncloud-based platform supports a holistic real time decision making for green\nurban managers, technical experts and operational staff. It enables intelligent\ncontrol and management of urban green spaces using Tree Talker sensors,\nintegrated with soil moisture and water potential monitoring systems. Thanks to\npredictive models based on machine learning algorithms and real time data\nprovided by IoT sensors, irrigation of public parks can be optimized by\nproviding suggestions on when and how much water to apply. Customized alerts\nlayers are also activated warning users when monitored parameters, such as soil\ntemperature, humidity, or water potential, exceed predefined thresholds. This\nUse Case demonstrates how digitalization, IoT sensors fusion and technological\ninnovation can support sustainable urban governance, fostering environmental\nresilience and improving citizens quality of life.", "comment": "18 pages, 6 Figures", "pdf_url": "http://arxiv.org/pdf/2507.12106v4", "cate": "cs.DC", "date": "2025-07-16", "updated": "2025-07-25", "AI": {"title_translation": "城市绿色治理：基于物联网的坎波巴索城市绿地管理与提升", "tldr": "该项目在坎波巴索利用物联网和数据驱动平台，通过实时监测和预测模型优化城市绿地管理，以提升城市健康和居民福祉。", "motivation": "城市绿地的有效设计和管理对提升城市人口健康与福祉至关重要，并提供生态系统服务。该项目旨在通过创新技术解决绿地可持续管理问题。", "method": "该项目在坎波巴索市实施，整合了物联网系统和数据驱动的治理平台。它通过决策支持系统实时监测树木和绿地健康状况，并收集分析天气、空气质量、土壤湿度、污染水平等数据。利用基于云的平台、Tree Talker传感器、土壤湿度和水势监测系统，结合机器学习算法的预测模型和物联网实时数据，优化公共公园的灌溉，并提供定制警报。", "result": "该用例展示了数字化、物联网传感器融合和技术创新如何支持可持续的城市治理，促进环境韧性并改善公民生活质量。通过实时数据和预测模型，实现了公共公园灌溉的优化，并能提供定制警报。", "conclusion": "数字化、物联网传感器融合和技术创新可以有效地支持可持续的城市治理，从而增强环境韧性并显著改善城市居民的生活质量。", "translation": "城市公共绿地的有效设计和管理是促进城市人口健康和福祉的关键因素，这一点得到了世界卫生组织、联合国环境规划署和欧洲环境环境署的强调。这些区域作为城市生态系统的“绿色肺”，通过提供生态系统服务，在提高生活质量方面发挥着至关重要的作用。在此背景下，由意大利企业部（MIMIT）资助的坎波巴索市“智能绿色城市”用例，通过采用先进的集成和可互操作的新兴技术系统，成为城市绿色区域可持续管理的一种创新模式。该项目整合了物联网系统和数据驱动的治理平台，通过决策支持系统（DSS）实现对树木和绿地健康状况的实时监测。它还促进了来自不同来源（包括天气条件、空气质量、土壤湿度、污染水平）的数据收集和分析。由此产生的基于云的平台支持城市绿地管理者、技术专家和操作人员进行全面的实时决策。它能够使用Tree Talker传感器，并与土壤湿度和水势监测系统集成，对城市绿地进行智能控制和管理。得益于基于机器学习算法的预测模型和物联网传感器提供的实时数据，可以通过提供何时以及施用多少水的建议来优化公共公园的灌溉。当监测参数（如土壤温度、湿度或水势）超过预设阈值时，还会激活定制的警报层。该用例展示了数字化、物联网传感器融合和技术创新如何支持可持续的城市治理，促进环境韧性并改善公民生活质量。", "summary": "本文介绍了意大利坎波巴索市的“智能绿色城市”项目，该项目利用物联网（IoT）系统和数据驱动的治理平台，实现城市绿地的可持续管理。通过实时监测树木健康、收集环境数据（如天气、空气质量、土壤湿度），并结合机器学习预测模型，该平台能够优化公共公园的灌溉，并提供定制警报。该用例展示了数字化和技术创新在提升城市环境韧性及改善居民生活质量方面的潜力。", "keywords": "城市绿地管理, 物联网, 数据驱动治理, 智能城市, 可持续发展", "comments": "这篇论文展示了一个将先进技术应用于城市治理的实际案例，其创新点在于整合了物联网传感器、数据驱动平台和机器学习算法，实现了对城市绿地的实时、智能化管理。这种方法不仅有助于优化资源（如水）的使用，还能通过提供定制警报来预防潜在问题，从而提升城市生态系统的健康和韧性。该项目的重要性在于其对可持续城市发展和居民福祉的积极贡献。"}}
{"id": "2405.12179", "title": "PLEIADES: Building Temporal Kernels with Orthogonal Polynomials", "authors": ["Yan Ru Pei", "Olivier Coenen"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      14 pages, 3 figures", "url": "http://arxiv.org/abs/2405.12179v4", "summary": "We introduce a class of neural networks named PLEIADES (PoLynomial Expansion\nIn Adaptive Distributed Event-based Systems), which contains temporal\nconvolution kernels generated from orthogonal polynomial basis functions. We\nfocus on interfacing these networks with event-based data to perform online\nspatiotemporal classification and detection with low latency. By virtue of\nusing structured temporal kernels and event-based data, we have the freedom to\nvary the sample rate of the data along with the discretization step-size of the\nnetwork without additional finetuning. We experimented with three event-based\nbenchmarks and obtained state-of-the-art results on all three by large margins\nwith significantly smaller memory and compute costs. We achieved: 1) 99.59%\naccuracy with 192K parameters on the DVS128 hand gesture recognition dataset\nand 100% with a small additional output filter; 2) 99.58% test accuracy with\n277K parameters on the AIS 2024 eye tracking challenge; and 3) 0.556 mAP with\n576k parameters on the PROPHESEE 1 Megapixel Automotive Detection Dataset.", "comment": "14 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2405.12179v4", "cate": "cs.LG", "date": "2024-05-20", "updated": "2025-07-25", "AI": {"title_translation": "PLEIADES：使用正交多项式构建时序核", "tldr": "PLEIADES是一种新型神经网络，利用正交多项式生成时序卷积核，在事件驱动数据上实现低延迟时空分类和检测，并在多个基准测试中以更小的资源消耗取得了最先进的性能。", "motivation": "旨在解决在线时空分类和检测中，传统方法可能存在的延迟问题，并提高处理事件驱动数据的效率和灵活性，同时降低内存和计算成本。", "method": "引入了PLEIADES神经网络，其时序卷积核由正交多项式基函数生成。该网络与事件驱动数据接口，能够在线执行时空分类和检测，并具有在不额外微调的情况下改变数据采样率和网络离散化步长的灵活性。", "result": "1) 在DVS128手势识别数据集上，以192K参数达到99.59%的准确率，额外增加一个小型输出滤波器后达到100%；2) 在AIS 2024眼动追踪挑战赛上，以277K参数达到99.58%的测试准确率；3) 在PROPHESEE 1百万像素汽车检测数据集上，以576k参数达到0.556 mAP。所有三个基准测试均以显著优势获得最先进的结果，且内存和计算成本显著降低。", "conclusion": "PLEIADES神经网络通过利用结构化的时序核和事件驱动数据，在在线时空分类和检测任务上取得了突破性的、最先进的性能，同时显著降低了资源消耗，证明了其在处理事件驱动数据方面的优越性和高效性。", "translation": "我们引入了一类名为PLEIADES（基于自适应分布式事件系统中的多项式展开）的神经网络，该网络包含由正交多项式基函数生成的时序卷积核。我们专注于将这些网络与事件驱动数据接口，以低延迟执行在线时空分类和检测。通过使用结构化的时序核和事件驱动数据，我们可以在不额外微调的情况下，自由地改变数据的采样率以及网络的离散化步长。我们对三个事件驱动基准进行了实验，并在所有三个基准上以显著优势获得了最先进的结果，同时内存和计算成本显著降低。我们取得了：1）在DVS128手势识别数据集上，使用192K参数获得了99.59%的准确率，通过增加一个小型输出滤波器后达到100%；2）在AIS 2024眼动追踪挑战赛上，使用277K参数获得了99.58%的测试准确率；3）在PROPHESEE 1百万像素汽车检测数据集上，使用576k参数获得了0.556 mAP。", "summary": "PLEIADES是一种新型神经网络架构，利用正交多项式构建时序卷积核，专为处理事件驱动数据设计，以实现低延迟的在线时空分类和检测。该方法允许在不进行额外微调的情况下调整数据采样率和网络步长。实验结果表明，PLEIADES在多个事件驱动基准测试中取得了显著优于现有技术的性能，同时显著降低了内存和计算成本。", "keywords": "时序核, 正交多项式, 事件驱动数据, 神经网络, 低延迟", "comments": "这篇论文通过引入基于正交多项式的时序核，为处理事件驱动数据提供了一种创新且高效的神经网络架构。其主要创新点在于结构化时序核的设计，以及在不额外微调的情况下适应数据采样率变化的能力。在多个基准测试中取得的最先进结果，以及显著降低的资源消耗，凸显了其在实际应用中的重要性和潜力，尤其是在需要低延迟和资源受限的场景。"}}
{"id": "2410.07919", "title": "Advancing biomolecular understanding and design following human instructions", "authors": ["Xiang Zhuang", "Keyan Ding", "Tianwen Lyu", "Yinuo Jiang", "Xiaotong Li", "Zhuoyi Xiang", "Zeyuan Wang", "Ming Qin", "Kehua Feng", "Jike Wang", "Qiang Zhang", "Huajun Chen"], "categories": ["cs.CL", "q-bio.BM"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.07919v2", "summary": "Understanding and designing biomolecules, such as proteins and small\nmolecules, is central to advancing drug discovery, synthetic biology and enzyme\nengineering. Recent breakthroughs in artificial intelligence have\nrevolutionized biomolecular research, achieving remarkable accuracy in\nbiomolecular prediction and design. However, a critical gap remains between\nartificial intelligence's computational capabilities and researchers' intuitive\ngoals, particularly in using natural language to bridge complex tasks with\nhuman intentions. Large language models have shown potential to interpret human\nintentions, yet their application to biomolecular research remains nascent due\nto challenges including specialized knowledge requirements, multimodal data\nintegration, and semantic alignment between natural language and biomolecules.\nTo address these limitations, we present InstructBioMol, a large language model\ndesigned to bridge natural language and biomolecules through a comprehensive\nany-to-any alignment of natural language, molecules and proteins. This model\ncan integrate multimodal biomolecules as the input, and enable researchers to\narticulate design goals in natural language, providing biomolecular outputs\nthat meet precise biological needs. Experimental results demonstrate that\nInstructBioMol can understand and design biomolecules following human\ninstructions. In particular, it can generate drug molecules with a 10%\nimprovement in binding affinity and design enzymes that achieve an\nenzyme-substrate pair prediction score of 70.4. This highlights its potential\nto transform real-world biomolecular research. The code is available at\nhttps://github.com/HICAI-ZJU/InstructBioMol.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.07919v2", "cate": "cs.CL", "date": "2024-10-10", "updated": "2025-07-25", "AI": {"title_translation": "遵循人类指令推进生物分子理解与设计", "tldr": "InstructBioMol是一个大型语言模型，旨在通过自然语言连接生物分子，帮助研究人员以人类指令进行生物分子理解和设计，并在药物分子生成和酶设计方面取得了显著成果。", "motivation": "尽管人工智能在生物分子预测和设计方面取得了突破，但计算能力与研究人员直观目标之间存在关键差距，尤其是在使用自然语言将复杂任务与人类意图联系起来方面。大型语言模型在此领域应用仍处于初期阶段，面临专业知识、多模态数据集成和语义对齐等挑战，因此需要弥合自然语言与生物分子之间的鸿沟。", "method": "本文提出了InstructBioMol，这是一个大型语言模型，旨在通过自然语言、分子和蛋白质的全面任意对任意对齐来连接自然语言与生物分子。该模型能够整合多模态生物分子作为输入，并允许研究人员以自然语言阐明设计目标，从而提供满足精确生物需求的生物分子输出。", "result": "实验结果表明，InstructBioMol能够遵循人类指令理解和设计生物分子。具体而言，它能生成结合亲和力提高10%的药物分子，并设计出酶-底物对预测分数达到70.4的酶。", "conclusion": "InstructBioMol模型展示了其理解和设计生物分子的能力，并有望通过弥合自然语言与生物分子之间的差距，彻底改变现实世界的生物分子研究。", "translation": "理解和设计蛋白质和小分子等生物分子对于推进药物发现、合成生物学和酶工程至关重要。人工智能的最新突破彻底改变了生物分子研究，在生物分子预测和设计方面取得了显著的准确性。然而，人工智能的计算能力与研究人员的直观目标之间仍存在关键差距，特别是在使用自然语言将复杂任务与人类意图联系起来方面。大型语言模型已显示出解释人类意图的潜力，但由于专业知识要求、多模态数据集成以及自然语言与生物分子之间的语义对齐等挑战，它们在生物分子研究中的应用仍处于初期阶段。为了解决这些限制，我们提出了InstructBioMol，一个大型语言模型，旨在通过自然语言、分子和蛋白质的全面任意对任意对齐来连接自然语言与生物分子。该模型能够整合多模态生物分子作为输入，并使研究人员能够用自然语言阐明设计目标，提供满足精确生物需求的生物分子输出。实验结果表明，InstructBioMol能够遵循人类指令理解和设计生物分子。特别是，它可以生成结合亲和力提高10%的药物分子，并设计出酶-底物对预测分数达到70.4的酶。这突显了其改变现实世界生物分子研究的潜力。代码可在https://github.com/HICAI-ZJU/InstructBioMol获取。", "summary": "本研究提出InstructBioMol，一个创新的大型语言模型，旨在通过将自然语言与生物分子（包括蛋白质和分子）进行多模态对齐来弥合AI与人类意图之间的差距。InstructBioMol能够接受人类指令并生成符合特定生物需求的生物分子输出。实验证明，该模型在药物分子设计（结合亲和力提高10%）和酶设计（酶-底物对预测得分70.4）方面表现出色，预示着其在生物分子研究领域的巨大应用潜力。", "keywords": "生物分子, 大型语言模型, 药物设计, 酶工程, InstructBioMol", "comments": "InstructBioMol的创新之处在于其实现了自然语言、分子和蛋白质之间的“任意对任意”对齐，这有效地解决了现有AI在生物分子领域中理解人类复杂指令的挑战。该模型能够整合多模态生物分子数据并直接以自然语言接收设计目标，极大地降低了生物学家使用AI工具的门槛，并有望加速药物发现和合成生物学等领域的发展。其在药物分子结合亲和力和酶设计方面的具体成果也验证了其强大的实用性。"}}
{"id": "2408.08088", "title": "KGV: Integrating Large Language Models with Knowledge Graphs for Cyber Threat Intelligence Credibility Assessment", "authors": ["Zongzong Wu", "Fengxiao Tang", "Ming Zhao", "Yufeng Li"], "categories": ["cs.CR", "cs.IR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2408.08088v2", "summary": "Cyber threat intelligence (CTI) is a crucial tool to prevent sophisticated,\norganized, and weaponized cyber attacks. However, few studies have focused on\nthe credibility assessment of CTI, and this work still requires manual analysis\nby cybersecurity experts. In this paper, we propose Knowledge Graph-based\nVerifier (KGV), the first framework integrating large language models (LLMs)\nwith simple structured knowledge graphs (KGs) for automated CTI credibility\nassessment. Unlike entity-centric KGs, KGV constructs paragraph-level semantic\ngraphs where nodes represent text segments connected through similarity\nanalysis, which effectively enhances the semantic understanding ability of the\nmodel, reduces KG density and greatly improves response speed. Experimental\nresults demonstrate that our KGV outperforms state-of-the-art fact reasoning\nmethods on the CTI-200 dataset, achieving a 5.7\\% improvement in F1.\nAdditionally, it shows strong scalability on factual QA and fake news detection\ndatasets. Compared to entity-based knowledge graphs (KGs) for equivalent-length\ntexts, our structurally simple KG reduces node quantities by nearly two-thirds\nwhile boosting precision by 1.7\\% and cutting response time by 46.7\\%. In\naddition, we have created and publicly released the first CTI credibility\nassessment dataset, CTI-200. Distinct from CTI identification datasets, CTI-200\nrefines CTI summaries and key sentences to focus specifically on credibility\nassessment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2408.08088v2", "cate": "cs.CR", "date": "2024-08-15", "updated": "2025-07-25", "AI": {"title_translation": "KGV：整合大型语言模型与知识图谱用于网络威胁情报可信度评估", "tldr": "KGV是一个结合大型语言模型（LLM）和知识图谱（KG）的框架，用于自动化网络威胁情报（CTI）可信度评估。它通过构建段落级语义图提升性能和效率，并在专用数据集CTI-200上超越现有方法，同时发布了该数据集。", "motivation": "现有研究很少关注网络威胁情报（CTI）的可信度评估，且这项工作仍需网络安全专家进行手动分析，效率低下且成本高昂。", "method": "本文提出了基于知识图谱的验证器（KGV），这是首个整合大型语言模型（LLMs）与简单结构化知识图谱（KGs）的自动化CTI可信度评估框架。KGV构建段落级语义图，其中节点代表文本片段并通过相似性分析连接，而非传统的实体中心式知识图谱。这种设计旨在增强模型的语义理解能力，降低KG密度，并提高响应速度。同时，研究团队创建并公开发布了首个CTI可信度评估数据集CTI-200。", "result": "KGV在CTI-200数据集上优于最先进的事实推理方法，F1分数提高了5.7%。它在事实问答和假新闻检测数据集上显示出强大的可扩展性。与等长文本的基于实体的知识图谱相比，KGV的简单结构化KG将节点数量减少了近三分之二，同时精度提高了1.7%，响应时间缩短了46.7%。", "conclusion": "KGV通过创新性地结合大型语言模型与段落级知识图谱，显著提升了网络威胁情报可信度评估的自动化水平、性能和效率。该研究不仅提出了高效的框架，还为领域贡献了首个专门的可信度评估数据集CTI-200，为未来的研究奠定了基础。", "translation": "网络威胁情报（CTI）是预防复杂、有组织和武器化网络攻击的关键工具。然而，很少有研究关注CTI的可信度评估，这项工作仍然需要网络安全专家进行手动分析。在本文中，我们提出了基于知识图谱的验证器（KGV），这是第一个将大型语言模型（LLM）与简单结构化知识图谱（KG）集成，用于自动化CTI可信度评估的框架。与以实体为中心的知识图谱不同，KGV构建了段落级语义图，其中节点代表通过相似性分析连接的文本片段，这有效地增强了模型的语义理解能力，降低了KG密度并大大提高了响应速度。实验结果表明，我们的KGV在CTI-200数据集上优于最先进的事实推理方法，F1提高了5.7%。此外，它在事实问答和假新闻检测数据集上显示出强大的可扩展性。与等长文本的基于实体的知识图谱（KG）相比，我们结构简单的KG将节点数量减少了近三分之二，同时精度提高了1.7%，响应时间缩短了46.7%。此外，我们创建并公开发布了第一个CTI可信度评估数据集CTI-200。与CTI识别数据集不同，CTI-200细化了CTI摘要和关键句子，专门关注可信度评估。", "summary": "本文提出了KGV框架，它创新性地将大型语言模型与一种新型的段落级知识图谱结合，旨在自动化网络威胁情报的可信度评估。KGV通过构建以文本片段为节点的语义图，有效提升了模型的语义理解能力，并显著优化了图谱密度和响应速度。实验证明，KGV在专用数据集CTI-200上超越了现有先进方法，并在效率和可扩展性方面表现出色。研究团队还创建并发布了首个专注于CTI可信度评估的CTI-200数据集。", "keywords": "网络威胁情报, 可信度评估, 大型语言模型, 知识图谱, CTI-200数据集", "comments": "KGV的创新点在于其将大型语言模型与非传统的段落级知识图谱相结合，以解决网络威胁情报可信度评估的自动化问题，这在现有研究中较少见。这种设计不仅提升了模型的语义理解能力，还显著优化了图谱的效率和响应速度。此外，该工作公开发布了首个专门用于CTI可信度评估的数据集CTI-200，这对于推动该领域的研究具有重要意义。"}}
{"id": "2507.19255", "title": "Learning electromagnetic fields based on finite element basis functions", "authors": ["Merle Backmeyer", "Michael Wiesheu", "Sebastian Schöps"], "categories": ["cs.CE", "physics.comp-ph"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.19255v1", "summary": "Parametric surrogate models of electric machines are widely used for\nefficient design optimization and operational monitoring. Addressing geometry\nvariations, spline-based computer-aided design representations play a pivotal\nrole. In this study, we propose a novel approach that combines isogeometric\nanalysis, proper orthogonal decomposition and deep learning to enable rapid and\nphysically consistent predictions by directly learning spline basis\ncoefficients. The effectiveness of this method is demonstrated using a\nparametric nonlinear magnetostatic model of a permanent magnet synchronous\nmachine.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.19255v1", "cate": "cs.CE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于有限元基函数学习电磁场", "tldr": "本文提出了一种结合等几何分析、本征正交分解和深度学习的新方法，通过直接学习样条基系数，实现电机的快速、物理一致性预测。", "motivation": "电机的参数代理模型广泛用于高效设计优化和运行监控。为解决几何变化问题，基于样条的计算机辅助设计表示至关重要。", "method": "本研究提出一种新颖的方法，结合等几何分析（isogeometric analysis）、本征正交分解（proper orthogonal decomposition）和深度学习（deep learning），通过直接学习样条基系数，实现快速且物理一致的预测。", "result": "该方法的有效性通过永磁同步电机的参数非线性静磁模型得到验证。", "conclusion": "该研究成功提出并验证了一种结合多学科方法来快速、物理一致地预测电机电磁场的方法，对电机设计优化具有重要意义。", "translation": "电机电磁场的参数代理模型被广泛应用于高效设计优化和运行监控。为了解决几何变化问题，基于样条的计算机辅助设计表示发挥着关键作用。在本研究中，我们提出了一种新颖的方法，该方法结合了等几何分析、本征正交分解和深度学习，通过直接学习样条基系数，实现快速且物理一致的预测。该方法的有效性通过永磁同步电机的参数非线性静磁模型得到了验证。", "summary": "本文针对电机设计优化和运行监控中的几何变化问题，提出了一种新颖的电磁场预测方法。该方法将等几何分析、本征正交分解与深度学习相结合，通过直接学习样条基系数，实现了对电机参数非线性静磁模型的快速且物理一致的预测。", "keywords": "电磁场, 等几何分析, 本征正交分解, 深度学习, 代理模型", "comments": "该论文的创新点在于将等几何分析、本征正交分解和深度学习这三种先进技术巧妙地结合起来，以解决电机电磁场预测中的几何变化和计算效率问题。这种方法能够实现快速且物理一致的预测，对于电机的高效设计优化和运行监控具有重要的实际应用价值。"}}
{"id": "2507.18903", "title": "Probably Approximately Correct Causal Discovery", "authors": ["Mian Wei", "Somesh Jha", "David Page"], "categories": ["stat.ML", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18903v1", "summary": "The discovery of causal relationships is a foundational problem in artificial\nintelligence, statistics, epidemiology, economics, and beyond. While elegant\ntheories exist for accurate causal discovery given infinite data, real-world\napplications are inherently resource-constrained. Effective methods for\ninferring causal relationships from observational data must perform well under\nfinite data and time constraints, where \"performing well\" implies achieving\nhigh, though not perfect accuracy. In his seminal paper A Theory of the\nLearnable, Valiant highlighted the importance of resource constraints in\nsupervised machine learning, introducing the concept of Probably Approximately\nCorrect (PAC) learning as an alternative to exact learning. Inspired by\nValiant's work, we propose the Probably Approximately Correct Causal (PACC)\nDiscovery framework, which extends PAC learning principles to the causal field.\nThis framework emphasizes both computational and sample efficiency for\nestablished causal methods such as propensity score techniques and instrumental\nvariable approaches. Furthermore, we show that it can also provide theoretical\nguarantees for other widely used methods, such as the Self-Controlled Case\nSeries (SCCS) method, which had previously lacked such guarantees.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18903v1", "cate": "stat.ML", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "可能近似正确因果发现", "tldr": "提出了可能近似正确因果发现 (PACC) 框架，将PAC学习原则扩展到因果领域，强调计算和样本效率，并为现有方法提供理论保证。", "motivation": "尽管在无限数据下存在精确因果发现的理论，但现实世界应用受资源限制，需要能在有限数据和时间约束下有效推断因果关系的方法，即实现高但不完美的准确性。", "method": "受Valiant的PAC学习理论启发，提出了可能近似正确因果 (PACC) 发现框架，将PAC学习原则扩展到因果领域。该框架强调了倾向得分技术和工具变量方法等既定因果方法的计算和样本效率。", "result": "展示了PACC框架可以为自控病例系列 (SCCS) 方法等其他广泛使用但此前缺乏理论保证的方法提供理论保证。", "conclusion": "PACC框架为在资源受限的真实世界环境中进行因果发现提供了一个新的视角，强调了计算和样本效率，并为现有方法提供了理论保证。", "translation": "因果关系的发现是人工智能、统计学、流行病学、经济学等领域的一个基础问题。虽然在无限数据下存在精确因果发现的优雅理论，但现实世界应用本质上受到资源限制。从观察数据推断因果关系的有效方法必须在有限数据和时间约束下表现良好，其中“表现良好”意味着达到高但不完美的准确性。Valiant在其开创性论文《可学习理论》中强调了资源约束在监督机器学习中的重要性，引入了可能近似正确 (PAC) 学习的概念作为精确学习的替代方案。受Valiant工作的启发，我们提出了可能近似正确因果 (PACC) 发现框架，它将PAC学习原则扩展到因果领域。该框架强调了倾向得分技术和工具变量方法等既定因果方法的计算和样本效率。此外，我们表明它还可以为其他广泛使用的方法提供理论保证，例如自控病例系列 (SCCS) 方法，该方法此前缺乏此类保证。", "summary": "本研究提出“可能近似正确因果 (PACC) 发现”框架，将计算学习理论中的PAC学习原则应用于因果发现领域。鉴于现实世界中数据和时间资源的限制，该框架旨在解决在有限资源下进行高效且近似准确的因果推断问题。PACC框架强调了现有因果方法（如倾向得分和工具变量）的计算和样本效率，并为此前缺乏理论保证的自控病例系列 (SCCS) 等方法提供了理论基础。", "keywords": "因果发现, PAC学习, PACC框架, 计算效率, 样本效率", "comments": "该论文的创新之处在于将PAC学习的理念引入因果发现领域，为在资源受限的真实世界场景中进行因果推断提供了一个新的理论框架。它不仅强调了效率，还为现有方法提供了急需的理论保证，这对于推动因果推断在实际应用中的可靠性具有重要意义。"}}
{"id": "2507.18989", "title": "GENIAL: Generative Design Space Exploration via Network Inversion for Low Power Algorithmic Logic Units", "authors": ["Maxence Bouvier", "Ryan Amaudruz", "Felix Arnold", "Renzo Andri", "Lukas Cavigelli"], "categories": ["cs.LG", "cs.AI", "cs.AR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Under review", "url": "http://arxiv.org/abs/2507.18989v1", "summary": "As AI workloads proliferate, optimizing arithmetic units is becoming\nincreasingly important to reduce the footprint of digital systems. Conventional\ndesign flows, which often rely on manual or heuristics-based optimization, are\nlimited in their ability to thoroughly explore the vast design space. In this\npaper, we introduce GENIAL, a machine learning-based framework for the\nautomatic generation and optimization of arithmetic units, more specifically\nmultipliers.\n  At the core of GENIAL is a Transformer-based surrogate model trained in two\nstages, involving self-supervised pretraining followed by supervised\nfinetuning, to robustly forecast key hardware metrics such as power and area\nfrom abstracted design representations. By inverting the surrogate model,\nGENIAL efficiently searches for new operand encodings that directly minimize\npower consumption in arithmetic units for specific input data distributions.\nExtensive experiments on large datasets demonstrate that GENIAL is consistently\nmore sample efficient than other methods, and converges faster towards\noptimized designs. This enables to deploy a high-effort logic synthesis\noptimization flow in the loop, improving the accuracy of the surrogate model.\nNotably, GENIAL automatically discovers encodings that achieve up to 18%\nswitching activity savings within multipliers on representative AI workloads\ncompared with the conventional two's complement. We also demonstrate the\nversatility of our approach by achieving significant improvements on Finite\nState Machines, highlighting GENIAL's applicability for a wide spectrum of\nlogic functions. Together, these advances mark a significant step toward\nautomated Quality-of-Results-optimized combinational circuit generation for\ndigital systems.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2507.18989v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "GENIAL：基于网络反演的低功耗算法逻辑单元生成式设计空间探索", "tldr": "GENIAL是一个基于机器学习的框架，通过反演Transformer代理模型，自动优化算术单元（如乘法器）以降低功耗，实现显著的功耗节省和更快的优化。", "motivation": "随着AI工作负载的普及，优化算术单元以减少数字系统占用空间变得日益重要。然而，传统的依赖手动或启发式优化的设计流程在探索广阔设计空间方面存在局限性。", "method": "本文引入了GENIAL，一个基于机器学习的框架，用于算术单元的自动生成和优化。其核心是一个两阶段训练（自监督预训练加监督微调）的Transformer代理模型，用于预测功耗和面积等硬件指标。GENIAL通过反演该代理模型，高效地搜索新的操作数编码，以直接最小化特定输入数据分布下的算术单元功耗。该方法还通过在循环中部署高投入的逻辑综合优化流程来提高代理模型的准确性。", "result": "GENIAL在实验中始终比其他方法更具样本效率，并能更快地收敛到优化设计。与传统的二进制补码相比，GENIAL自动发现的编码在代表性AI工作负载的乘法器中实现了高达18%的开关活动节省。此外，该方法在有限状态机上也取得了显著改进，证明了其对广泛逻辑函数的适用性。", "conclusion": "这些进展标志着向数字系统自动化优化结果质量的组合电路生成迈出了重要一步。", "translation": "随着AI工作负载的普及，优化算术单元对于减少数字系统占用空间变得越来越重要。传统的基于手动或启发式优化的设计流程在彻底探索广阔的设计空间方面存在局限性。在本文中，我们引入了GENIAL，一个基于机器学习的框架，用于算术单元（特别是乘法器）的自动生成和优化。GENIAL的核心是一个基于Transformer的代理模型，该模型经过两阶段训练，包括自监督预训练和监督微调，以从抽象的设计表示中稳健地预测关键硬件指标，如功耗和面积。通过反演代理模型，GENIAL有效地搜索新的操作数编码，直接最小化算术单元在特定输入数据分布下的功耗。对大型数据集的广泛实验表明，GENIAL始终比其他方法更具样本效率，并且更快地收敛到优化设计。这使得在循环中部署高投入的逻辑综合优化流程成为可能，从而提高了代理模型的准确性。值得注意的是，与传统的二进制补码相比，GENIAL自动发现的编码在代表性AI工作负载的乘法器中实现了高达18%的开关活动节省。我们还通过在有限状态机上取得显著改进来展示我们方法的通用性，突出了GENIAL对广泛逻辑函数的适用性。总而言之，这些进展标志着向数字系统自动化优化结果质量的组合电路生成迈出了重要一步。", "summary": "GENIAL是一个基于机器学习的框架，旨在通过网络反演实现低功耗算术逻辑单元的生成式设计空间探索。它利用一个两阶段训练的Transformer代理模型来预测硬件指标，并通过反演该模型自动生成和优化算术单元（如乘法器）的操作数编码。实验证明，GENIAL在样本效率、收敛速度以及功耗节省（在乘法器中高达18%的开关活动节省）方面优于传统方法，并适用于多种逻辑功能，代表了数字系统自动化电路生成的重要进展。", "keywords": "GENIAL, 机器学习, 算术单元, 功耗优化, 设计空间探索", "comments": "本文提出了一种创新的基于机器学习的硬件自动化优化方法（GENIAL），特别针对算术单元。其核心创新在于利用一个基于Transformer的代理模型，并通过“反演”该模型直接搜索功耗高效的操作数编码，这是一种探索设计空间的新颖方式。所展示的显著功耗节省（高达18%）以及在不同逻辑功能（乘法器、有限状态机）上的通用性，突出了其在降低AI工作负载能耗方面的实际重要性。将高投入的逻辑综合流程集成到循环中以提高模型准确性也是一个巧妙的反馈机制。"}}
{"id": "2507.19410", "title": "Reconstruction in the Calderón problem on a fixed partition from finite and partial boundary data", "authors": ["Henrik Garde"], "categories": ["math.AP", "cs.NA", "math.NA", "35R30, 35Q60, 35R05, 47H05,"], "primary_category": "Subjects:       Analysis of PDEs (math.AP)", "pdf_link": null, "comments": "Comments:      4 pages, 1 figure", "url": "http://arxiv.org/abs/2507.19410v1", "summary": "This short note modifies a reconstruction method by the author (Comm. PDE,\n45(9):1118--1133, 2020), for reconstructing piecewise constant conductivities\nin the Calder\\'on problem (electrical impedance tomography). In the former\npaper, a layering assumption and the local Neumann-to-Dirichlet map was needed\nsince the piecewise constant partitioning also was assumed unknown. Here I show\nhow to modify the method in case the partitioning is known, for general\npiecewise constant conductivities and only a finite number of partial boundary\nmeasurements. Moreover, no lower/upper bounds on the unknown conductivity are\nneeded.", "comment": "4 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.19410v1", "cate": "math.AP", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于有限和部分边界数据在固定分区上Calderón问题的重建", "tldr": "本文修改了一种重建方法，用于在已知分区的Calderón问题中，利用有限和部分边界数据重建分段常数电导率，且无需电导率的上下界。", "motivation": "之前的研究（作者在Comm. PDE, 45(9):1118--1133, 2020的论文）在Calderón问题中重建分段常数电导率时，需要分层假设和局部Neumann-to-Dirichlet映射，因为分段常数分区是未知的。本文的动机是修改该方法，使其适用于分区已知的情况，并仅需有限数量的部分边界测量。", "method": "作者修改了其之前提出的一种重建方法。新的方法适用于分区已知的情况，用于重建一般的、分段常数的电导率，并且仅需要有限数量的部分边界测量。", "result": "该修改后的方法不再需要对未知电导率的下限/上限进行假设。", "conclusion": "Not mentioned in abstract", "translation": "这篇短文修改了作者之前提出的一种重建方法（Comm. PDE, 45(9):1118--1133, 2020），用于在Calderón问题（电阻抗断层扫描）中重建分段常数电导率。在之前的论文中，由于分段常数分区也被假定为未知，因此需要分层假设和局部Neumann-to-Dirichlet映射。本文展示了在分区已知的情况下如何修改该方法，适用于一般的、分段常数的电导率，并且仅需有限数量的部分边界测量。此外，不需要对未知电导率的下限/上限进行假设。", "summary": "本文对作者先前在Calderón问题中重建分段常数电导率的方法进行了改进。原方法要求分层假设和局部Neumann-to-Dirichlet映射，因其假定分区未知。新方法针对分区已知的情况，仅利用有限的部分边界测量，即可重建一般的、分段常数电导率，且无需预设电导率的上下界。", "keywords": "Calderón问题, 电阻抗断层扫描, 分段常数电导率, 重建, 边界数据", "comments": "本文的创新在于针对Calderón问题中的分段常数电导率重建，在分区已知的前提下，通过修改现有方法，显著降低了数据需求（仅需有限和部分边界数据）并消除了对电导率上下界的假设。这对于实际应用中数据获取受限或电导率范围未知的情况具有重要意义。"}}
{"id": "2507.19049", "title": "Heterogeneous Risk Management Using a Multi-Agent Framework for Supply Chain Disruption Response", "authors": ["Mingjie Bi", "Juan-Alberto Estrada-Garcia", "Dawn M. Tilbury", "Siqian Shen", "Kira Barton"], "categories": ["cs.MA", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Multiagent Systems (cs.MA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19049v1", "summary": "In the highly complex and stochastic global, supply chain environments, local\nenterprise agents seek distributed and dynamic strategies for agile responses\nto disruptions. Existing literature explores both centralized and distributed\napproaches, while most work neglects temporal dynamics and the heterogeneity of\nthe risk management of individual agents. To address this gap, this letter\npresents a heterogeneous risk management mechanism to incorporate uncertainties\nand risk attitudes into agent communication and decision-making strategy.\nHence, this approach empowers enterprises to handle disruptions in stochastic\nenvironments in a distributed way, and in particular in the context of\nmulti-agent control and management. Through a simulated case study, we showcase\nthe feasibility and effectiveness of the proposed approach under stochastic\nsettings and how the decision of disruption responses changes when agents hold\nvarious risk attitudes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19049v1", "cate": "cs.MA", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "异构风险管理：基于多智能体框架的供应链中断响应", "tldr": "本文提出了一种基于多智能体框架的异构风险管理机制，用于应对供应链中断，该机制将不确定性和风险态度纳入智能体通信和决策中，并通过仿真验证了其可行性和有效性。", "motivation": "现有文献在探讨供应链中断响应时，大多忽视了时间动态性以及个体智能体风险管理的异构性。", "method": "本文提出了一种异构风险管理机制，将不确定性和风险态度纳入智能体通信和决策策略中。该方法旨在使企业能够以分布式方式处理随机环境中的中断，特别是在多智能体控制和管理背景下。", "result": "通过一个模拟案例研究，本文展示了所提出方法在随机设置下的可行性和有效性，以及当智能体持有各种风险态度时，中断响应决策如何变化。", "conclusion": "所提出的基于多智能体框架的异构风险管理机制，能够有效应对随机环境下的供应链中断，并考虑到智能体不同的风险态度，具有可行性和有效性。", "translation": "在全球高度复杂和随机的供应链环境中，本地企业智能体寻求分布式和动态策略以敏捷响应中断。现有文献探讨了集中式和分布式方法，但大多数工作忽略了时间动态性和个体智能体风险管理的异构性。为了弥补这一空白，本文提出了一种异构风险管理机制，将不确定性和风险态度纳入智能体通信和决策策略中。因此，这种方法使企业能够以分布式方式处理随机环境中的中断，特别是在多智能体控制和管理背景下。通过一个模拟案例研究，我们展示了所提出方法在随机设置下的可行性和有效性，以及当智能体持有各种风险态度时，中断响应决策如何变化。", "summary": "本文提出了一种基于多智能体框架的异构风险管理机制，旨在解决现有供应链中断响应方法中忽略时间动态性和个体智能体风险异构性的问题。该机制将不确定性和智能体的风险态度整合到其通信和决策策略中，以支持企业在随机环境中进行分布式中断处理。通过模拟案例研究，论文验证了该方法的可行性和有效性，并展示了不同风险态度如何影响中断响应决策。", "keywords": "供应链中断, 异构风险管理, 多智能体系统, 风险态度, 随机环境", "comments": "该论文通过引入异构风险管理机制，填补了现有供应链中断响应研究中对时间动态性和个体智能体风险态度关注不足的空白。其多智能体框架允许分布式和动态的响应，这对于复杂且随机的全球供应链环境至关重要，提升了模型的现实性和应用潜力。"}}
{"id": "2507.19072", "title": "Exploring post-neoliberal futures for managing commercial heating and cooling through speculative praxis", "authors": ["Oliver Bates", "Christian Remy", "Kieran Cutting", "Adam Tyler", "Adrian Friday"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Post-proceedings paper presented at LIMITS 2024: 10th Workshop on Computing within Limits, 2024-06-19/20, Online", "url": "http://arxiv.org/abs/2507.19072v1", "summary": "What could designing for carbon reduction of heating and cooling in\ncommercial settings look like in the near future? How can we challenge dominant\nmindsets and paradigms of efficiency and behaviour change? How can we help\nbuild worlds through our practice that can become future realities? This paper\nintroduces the fictional consultancy ANCSTRL.LAB to explore opportunities for\nmaking space in research projects that can encourage more systems-oriented\ninterventions. We present a design fiction that asks `what if energy management\nand reduction practice embraced systems thinking?'. Our design fiction explores\nhow future energy consultancies could utilise systems thinking, and (more than)\nhuman centred design to re-imagine energy management practice and change\nsystems in ways that are currently unfathomable. We finish by discussing how\nLIMITS research can utilise design fiction and speculative praxis to help build\nnew material realities where more holistic perspectives, the leveraging of\nsystems change, and the imagining of post-neoliberal futures is the norm.", "comment": "Post-proceedings paper presented at LIMITS 2024: 10th Workshop on\n  Computing within Limits, 2024-06-19/20, Online", "pdf_url": "http://arxiv.org/pdf/2507.19072v1", "cate": "cs.HC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "通过推测性实践探索后新自由主义未来在商业供暖和制冷管理中的应用", "tldr": "本文通过引入虚构的咨询公司ANCSTRL.LAB和设计虚构，探讨了如何在商业环境中通过系统思维和超越人类中心的设计来重新构想能源管理实践，以实现碳减排和构建后新自由主义的未来。", "motivation": "探讨在不久的将来，商业供暖和制冷碳减排的设计可能是什么样子，挑战效率和行为改变的主导思维模式，以及如何通过实践构建未来的现实。", "method": "引入虚构的咨询公司ANCSTRL.LAB，并提出一个设计虚构，探讨“如果能源管理和减排实践采纳系统思维会怎样？”。研究利用系统思维和（超越）以人为中心的设计来重新构想能源管理实践和改变系统。", "result": "通过设计虚构，展示了未来的能源咨询公司如何利用系统思维和超越人类中心的设计来重新构想能源管理实践，并以目前无法想象的方式改变系统。", "conclusion": "LIMITS研究可以利用设计虚构和推测性实践，帮助构建新的物质现实，其中更全面的视角、系统变革的利用以及后新自由主义未来的设想成为常态。", "translation": "在不久的将来，商业环境中的供暖和制冷碳减排设计会是什么样子？我们如何挑战效率和行为改变的主导思维模式和范式？我们如何通过实践帮助构建可以成为未来现实的世界？本文引入了虚构的咨询公司ANCSTRL.LAB，以探索在研究项目中创造空间的机会，从而鼓励更多以系统为导向的干预措施。我们提出了一个设计虚构，提出“如果能源管理和减排实践采纳系统思维会怎样？”。我们的设计虚构探讨了未来的能源咨询公司如何利用系统思维和（超越）以人为中心的设计来重新构想能源管理实践，并以目前无法想象的方式改变系统。最后，我们讨论了LIMITS研究如何利用设计虚构和推测性实践，帮助构建新的物质现实，其中更全面的视角、系统变革的利用以及后新自由主义未来的设想成为常态。", "summary": "本文通过引入虚构的咨询公司ANCSTRL.LAB和设计虚构，探讨了在商业环境中实现供暖和制冷碳减排的后新自由主义未来。研究挑战了传统效率和行为改变的范式，提出通过系统思维和超越人类中心的设计，重新构想能源管理实践，以实现目前无法想象的系统变革。论文强调了利用设计虚构和推测性实践来构建新的物质现实的重要性，其中更全面的视角和系统变革成为常态。", "keywords": "设计虚构, 推测性实践, 能源管理, 系统思维, 碳减排", "comments": "本文的创新之处在于其采用的“设计虚构”和“推测性实践”方法，通过构建虚构的场景和实体来探索未来可能性，这为能源管理和系统变革研究提供了新颖的视角。通过引入“后新自由主义”概念，论文试图跳出现有的经济和社会框架，思考更根本的变革。其重要性在于鼓励研究人员和实践者超越当前限制，以更具想象力和系统性的方式应对气候变化和能源挑战。"}}
{"id": "2507.19078", "title": "A Protocol to Address Ecological Redirection for Digital Practices in Organizations", "authors": ["Valentin Girard", "Antoine Martin", "Maud Rio", "Romain Couillet"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19078v1", "summary": "The digitalization of societies raises questions about its sustainability and\nthe socio-technical impacts it generates. Ecological redirection applied to\norganizations is a field of research aiming for achieving sustainability as a\ndirection, rather than for technical means. Arbitration and renunciation to\nsome digital usage and technologies are investigated. Ecological redirection\nis, however, not yet addressing concrete methodologies for its implementation\nin organizations. This paper therefore proposes a protocol to support\nstakeholders in the ecological redirection of their digital practices. This\nprotocol is based on mapping attachments to digital tools through a\nmulti-disciplinary survey. It then proposes increasing stakeholders' knowledge\nand skills to prepare a debate on the arbitration of renunciations, and\nfinally, to operationalize the closure/transformation of targeted digital\npractices. This protocol will be tested in real conditions in different\ncontexts. An empirical study is proposed to measure 1) the fluidity with which\nparticipants carry out the protocol, 2) the effectiveness of the protocol in\nterms of the redirection objective, 3) the socio-technical barriers to the\nredirection process. The paper concludes on the potential benefits for\norganizations to better understand both the barriers related to its ecological\nredirection and the transformative aim of such protocols. This will help them\ntrigger large and radical policies towards a desirable and sustainable society.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19078v1", "cate": "cs.CY", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "组织中数字实践生态重定向的协议", "tldr": "提出一个协议，通过绘制数字工具依恋、提升知识和促进辩论来帮助组织对其数字实践进行生态重定向，以实现可持续性。", "motivation": "社会数字化引发了对其可持续性及其产生的社会技术影响的质疑。生态重定向旨在实现可持续性，但目前缺乏具体的实施方法。", "method": "本文提出一个协议，分三步支持利益相关者对其数字实践进行生态重定向：1) 通过多学科调查绘制对数字工具的依恋；2) 提高利益相关者的知识和技能，为仲裁和放弃进行辩论做准备；3) 操作化目标数字实践的关闭/转型。该协议将在真实条件下进行测试。", "result": "Not mentioned in abstract", "conclusion": "该协议有助于组织更好地理解生态重定向的相关障碍及其变革目标，从而推动大规模激进政策，以实现理想和可持续的社会。", "translation": "社会数字化引发了对其可持续性及其产生的社会技术影响的质疑。应用于组织的生态重定向是一个旨在将可持续性作为方向而非技术手段来实现的研究领域。论文调查了对某些数字使用和技术的仲裁和放弃。然而，生态重定向尚未解决其在组织中实施的具体方法问题。因此，本文提出了一个协议，以支持利益相关者对其数字实践进行生态重定向。该协议基于通过多学科调查绘制对数字工具的依恋。然后，它建议提高利益相关者的知识和技能，以准备关于仲裁和放弃的辩论，最后，将目标数字实践的关闭/转型付诸实施。该协议将在不同情境下的真实条件下进行测试。本文提出了一项实证研究，旨在衡量：1) 参与者执行协议的流畅性，2) 协议在重定向目标方面的有效性，3) 重定向过程中的社会技术障碍。本文总结了该协议对组织潜在的好处，即更好地理解其生态重定向的相关障碍以及此类协议的变革目标。这将有助于它们触发大规模激进政策，迈向一个理想和可持续的社会。", "summary": "本文提出了一种针对组织数字实践的生态重定向协议，旨在解决社会数字化带来的可持续性问题。该协议通过多学科调查识别对数字工具的依赖，提升利益相关者的相关知识和技能，并最终指导数字实践的调整或放弃。该协议计划在实际环境中进行测试，以评估其可行性、有效性和可能面临的社会技术障碍，最终目标是帮助组织更好地理解并实现可持续的数字化转型。", "keywords": "生态重定向, 数字实践, 可持续性, 组织, 协议", "comments": "这篇论文的创新点在于提出了一个具体的、分步骤的协议来指导组织进行数字实践的生态重定向，填补了该领域在方法论上的空白。其重要性在于提供了一个框架，帮助组织审视并调整其数字工具的使用，以应对可持续性挑战。通过强调“仲裁和放弃”而非仅仅是技术优化，该协议触及了数字化转型的深层伦理和社会维度。"}}
{"id": "2507.19230", "title": "Unstable Prompts, Unreliable Segmentations: A Challenge for Longitudinal Lesion Analysis", "authors": ["Niels Rocholl", "Ewoud Smit", "Mathias Prokop", "Alessa Hering"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19230v1", "summary": "Longitudinal lesion analysis is crucial for oncological care, yet automated\ntools often struggle with temporal consistency. While universal lesion\nsegmentation models have advanced, they are typically designed for single time\npoints. This paper investigates the performance of the ULS23 segmentation model\nin a longitudinal context. Using a public clinical dataset of baseline and\nfollow-up CT scans, we evaluated the model's ability to segment and track\nlesions over time. We identified two critical, interconnected failure modes: a\nsharp degradation in segmentation quality in follow-up cases due to inter-scan\nregistration errors, and a subsequent breakdown of the lesion correspondence\nprocess. To systematically probe this vulnerability, we conducted a controlled\nexperiment where we artificially displaced the input volume relative to the\ntrue lesion center. Our results demonstrate that the model's performance is\nhighly dependent on its assumption of a centered lesion; segmentation accuracy\ncollapses when the lesion is sufficiently displaced. These findings reveal a\nfundamental limitation of applying single-timepoint models to longitudinal\ndata. We conclude that robust oncological tracking requires a paradigm shift\naway from cascading single-purpose tools towards integrated, end-to-end models\ninherently designed for temporal analysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19230v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "不稳定的提示，不可靠的分割：纵向病灶分析的挑战", "tldr": "本文研究了ULS23分割模型在纵向背景下的表现，发现其在随访病例中分割质量严重下降，并揭示了单时间点模型应用于纵向数据的基本局限性。", "motivation": "纵向病灶分析对肿瘤护理至关重要，但现有自动化工具在时间一致性方面表现不佳。尽管通用病灶分割模型已取得进展，但它们通常是为单时间点设计的，因此本文旨在探究这些模型在纵向分析中的表现。", "method": "本研究使用ULS23分割模型，在一个包含基线和随访CT扫描的公共临床数据集上评估了模型随时间分割和追踪病灶的能力。为系统探究其脆弱性，作者进行了一项受控实验，人工位移输入体块相对于真实病灶中心的位置。", "result": "研究发现两个关键的、相互关联的失效模式：由于扫描间配准错误，随访病例的分割质量急剧下降；随后病灶对应过程崩溃。结果表明，模型的性能高度依赖于其对病灶居中的假设；当病灶充分位移时，分割精度会急剧下降。", "conclusion": "研究揭示了将单时间点模型应用于纵向数据的基本局限性。结论是，鲁棒的肿瘤追踪需要范式转变，从级联的单一用途工具转向固有设计用于时间分析的集成式端到端模型。", "translation": "纵向病灶分析对肿瘤护理至关重要，但自动化工具往往难以保持时间一致性。虽然通用病灶分割模型已取得进展，但它们通常是为单时间点设计的。本文研究了ULS23分割模型在纵向背景下的表现。我们使用一个包含基线和随访CT扫描的公共临床数据集，评估了模型随时间分割和追踪病灶的能力。我们发现了两个关键的、相互关联的失效模式：由于扫描间配准错误，随访病例的分割质量急剧下降，以及随后病灶对应过程的崩溃。为了系统地探究这种脆弱性，我们进行了一项受控实验，人工位移输入体块相对于真实病灶中心的位置。我们的结果表明，模型的性能高度依赖于其对病灶居中的假设；当病灶充分位移时，分割精度会急剧下降。这些发现揭示了将单时间点模型应用于纵向数据的基本局限性。我们得出结论，鲁棒的肿瘤追踪需要范式转变，从级联的单一用途工具转向固有设计用于时间分析的集成式端到端模型。", "summary": "本研究探讨了单时间点病灶分割模型（如ULS23）在纵向肿瘤护理中的局限性。通过评估模型在基线和随访CT扫描上的表现，论文发现由于扫描间配准错误，模型在随访病例中的分割质量显著下降，且病灶对应过程失效。受控实验进一步证实，模型性能严重依赖于病灶是否居中。研究强调，为实现可靠的肿瘤追踪，需开发原生支持时间分析的集成式端到端模型，而非简单堆叠单用途工具。", "keywords": "纵向病灶分析, 肿瘤学, 图像分割, 时间一致性, ULS23", "comments": "这篇论文揭示了将单时间点医学图像分析模型直接应用于纵向数据所面临的根本性挑战，特别是模型对输入精确性的敏感性。其创新之处在于通过受控实验量化了“不稳定提示”对分割性能的影响。这对于指导未来开发更鲁棒、时间感知的临床AI工具具有重要意义，强调了从单一任务向端到端、时间整合模型的范式转变。"}}
{"id": "2507.19384", "title": "On Anti-collusion Codes for Averaging Attack in Multimedia Fingerprinting", "authors": ["Jing Jiang", "Cailin Wen", "Minquan Cheng"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      23 pages", "url": "http://arxiv.org/abs/2507.19384v1", "summary": "Multimedia fingerprinting is a technique to protect the copyrighted contents\nagainst being illegally redistributed under various collusion attack models.\nAveraging attack is the most fair choice for each colluder to avoid detection,\nand also makes the pirate copy have better perceptional quality. This makes\nsuch an attack one of the most feasible approaches to carrying out collusion.\nIn order to trace all the colluders, several types of multimedia fingerprinting\ncodes were introduced to construct fingerprints resistant to averaging attacks\non multimedia contents, such as AND anti-collusion codes (AND-ACCs), binary\nseparable codes (SCs), logical anti-collusion codes (LACCs), binary frameproof\ncodes (FPCs), binary strongly-separable codes (SSCs) and binary secure code\nwith list decoding (SCLDs). Then codes with the rate as high as possible are\ndesired. However, the existing fingerprinting codes have low code rate due to\nthe strong combinatorial structure. The reason is that the previous research\nmethods adopted simple tracing algorithms. In this paper, we first propose\nnovel tracing algorithms and then find appropriate fingerprinting codes with\nweaker combinatorial structure, i.e., the binary strongly identifiable parent\nproperty code for multimedia fingerprinting (SMIPPC) and its concatenated code.\nTheoretical comparisons and numerical comparisons show that SMIPPCs have higher\ncode rates than those of the existing codes due to their weaker combinatorial\nstructures. It is worth noting that SMIPPCs can only trace a part of colluders\nby using the previous tracing algorithm and the concatenated SMIPPC may be not\nan SMIPPC. This implies that our tracing algorithms have strong traceability.", "comment": "23 pages", "pdf_url": "http://arxiv.org/pdf/2507.19384v1", "cate": "cs.IT", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "关于多媒体指纹中平均攻击的防串通码", "tldr": "本文提出了新的追踪算法和二元强可识别父属性码（SMIPPC），以提高多媒体指纹识别中对抗平均攻击的防串通码的码率。", "motivation": "多媒体指纹识别技术旨在保护受版权保护的内容免受各种串通攻击（特别是平均攻击）的非法再分发。然而，现有的防串通码由于其强组合结构和简单的追踪算法，导致码率较低，这限制了其在实际应用中的效率。", "method": "本文首先提出了新颖的追踪算法，然后寻找具有较弱组合结构的合适指纹码，即多媒体指纹识别的二元强可识别父属性码（SMIPPC）及其级联码。", "result": "理论和数值比较表明，由于其较弱的组合结构，SMIPPC比现有代码具有更高的码率。值得注意的是，SMIPPC使用以前的追踪算法只能追踪部分串通者，这表明本文提出的追踪算法具有很强的可追溯性。", "conclusion": "本文提出的SMIPPC及其级联码，结合新颖的追踪算法，能够提供更高的码率和更强的可追溯性，有效对抗多媒体指纹识别中的平均攻击，克服了现有方法的局限性。", "translation": "多媒体指纹识别是一种保护版权内容免受各种串通攻击模型下非法再分发的技术。平均攻击是每个串通者避免检测的最公平选择，也使得盗版副本具有更好的感知质量。这使得这种攻击成为进行串通的最可行方法之一。为了追踪所有串通者，引入了几种类型的多媒体指纹码来构建抵抗多媒体内容平均攻击的指纹，例如AND防串通码（AND-ACCs）、二元可分离码（SCs）、逻辑防串通码（LACCs）、二元防伪码（FPCs）、二元强可分离码（SSCs）和带有列表解码的二元安全码（SCLDs）。然后，期望码率尽可能高的代码。然而，由于强组合结构，现有的指纹码码率较低。原因是之前的研究方法采用了简单的追踪算法。在本文中，我们首先提出了新颖的追踪算法，然后找到了具有较弱组合结构的合适指纹码，即多媒体指纹识别的二元强可识别父属性码（SMIPPC）及其级联码。理论比较和数值比较表明，由于其较弱的组合结构，SMIPPC比现有代码具有更高的码率。值得注意的是，SMIPPC使用以前的追踪算法只能追踪部分串通者，级联的SMIPPC可能不是SMIPPC。这意味着我们的追踪算法具有很强的可追溯性。", "summary": "本文针对多媒体指纹识别中平均攻击导致的低码率防串通码问题，提出了一种新的解决方案。通过引入新颖的追踪算法，并设计了具有较弱组合结构的二元强可识别父属性码（SMIPPC）及其级联码。研究结果表明，与现有代码相比，SMIPPC具有更高的码率，且新的追踪算法展现出更强的可追溯性，有效提升了多媒体内容版权保护的效率和鲁棒性。", "keywords": "防串通码, 多媒体指纹, 平均攻击, 码率, 追踪算法", "comments": "本文的创新点在于提出了新的追踪算法，这使得可以使用组合结构较弱的指纹码（如SMIPPC）来提高码率。这解决了现有防串通码因强组合结构导致码率低下的问题，具有重要的实际意义。新算法的强可追溯性是其关键优势。"}}
{"id": "2501.10699", "title": "VENENA: A Deceptive Visual Encryption Framework for Wireless Semantic Secrecy", "authors": ["Bin Han", "Ye Yuan", "Hans D. Schotten"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE WCM", "url": "http://arxiv.org/abs/2501.10699v2", "summary": "Eavesdropping has been a long-standing threat to the security and privacy of\nwireless communications, since it is difficult to detect and costly to prevent.\nAs networks evolve towards Sixth Generation (6G) and semantic communication\nbecomes increasingly central to next-generation wireless systems, securing\nsemantic information transmission emerges as a critical challenge. While\nclassical physical layer security (PLS) focuses on passive security, the\nrecently proposed concept of physical layer deception (PLD) offers a semantic\nencryption measure to actively deceive eavesdroppers. Yet the existing studies\nof PLD have been dominantly information-theoretical and link-level oriented,\nlacking considerations of system-level design and practical implementation.\n  In this work we propose a novel artificial intelligence (AI)-enabled\nframework called Visual ENcryption for Eavesdropping NegAtion (VENENA), which\ncombines the techniques of PLD, visual encryption, and image poisoning, into a\ncomprehensive mechanism for deceptive secure semantic transmission in future\nwireless networks. By leveraging advanced vision transformers and semantic\ncodecs, VENENA demonstrates how semantic security can be enhanced through the\nsynergy of physical layer techniques and artificial intelligence, paving the\nway for secure semantic communication in 6G networks.", "comment": "Submitted to IEEE WCM", "pdf_url": "http://arxiv.org/pdf/2501.10699v2", "cate": "cs.CR", "date": "2025-01-18", "updated": "2025-07-25", "AI": {"title_translation": "VENENA：一种用于无线语义保密的欺骗性视觉加密框架", "tldr": "VENENA是一个AI驱动的欺骗性视觉加密框架，用于增强6G无线网络中的语义安全，通过结合物理层欺骗、视觉加密和图像中毒技术来主动迷惑窃听者。", "motivation": "无线通信中的窃听对安全和隐私构成长期威胁，尤其是在网络向6G和语义通信演进的背景下，保护语义信息传输成为一个关键挑战。现有物理层欺骗（PLD）研究主要集中在信息理论和链路层面，缺乏系统级设计和实际实现考量。", "method": "本文提出了一种名为VENENA的新型人工智能（AI）驱动框架。该框架结合了物理层欺骗（PLD）、视觉加密和图像中毒技术，形成了一个全面的机制，用于未来无线网络中的欺骗性安全语义传输。VENENA通过利用先进的视觉Transformer和语义编解码器实现其功能。", "result": "VENENA展示了如何通过物理层技术和人工智能的协同作用来增强语义安全。", "conclusion": "VENENA为6G网络中的安全语义通信铺平了道路。", "translation": "窃听一直是无线通信安全和隐私的长期威胁，因为它难以检测且预防成本高昂。随着网络向第六代（6G）发展，语义通信在下一代无线系统中变得越来越核心，保护语义信息传输成为一个关键挑战。虽然经典的物理层安全（PLS）侧重于被动安全，但最近提出的物理层欺骗（PLD）概念提供了一种语义加密措施，可以主动欺骗窃听者。然而，现有的PLD研究主要偏向信息理论和链路层面，缺乏对系统级设计和实际实现的考虑。\n在这项工作中，我们提出了一种新颖的人工智能（AI）驱动框架，名为视觉加密以阻止窃听（VENENA），它将PLD、视觉加密和图像中毒技术结合成一个全面的机制，用于未来无线网络中的欺骗性安全语义传输。通过利用先进的视觉Transformer和语义编解码器，VENENA展示了物理层技术和人工智能的协同作用如何增强语义安全，为6G网络中的安全语义通信铺平了道路。", "summary": "本文提出了一种名为VENENA的AI驱动欺骗性视觉加密框架，旨在解决6G网络中无线语义通信面临的窃听威胁。VENENA结合了物理层欺骗、视觉加密和图像中毒技术，提供了一个全面的机制来主动迷惑窃听者。该框架利用先进的视觉Transformer和语义编解码器，展示了物理层技术与人工智能协同作用如何有效地增强语义安全性，为未来6G网络的安全语义通信奠定了基础。", "keywords": "语义安全, 物理层欺骗, 视觉加密, 图像中毒, 6G", "comments": "该论文提出了一种新颖的AI驱动框架VENENA，将物理层欺骗、视觉加密和图像中毒等技术相结合，为无线语义通信提供主动防御窃听的方案，特别关注了6G网络的需求。其创新点在于将信息理论层面的物理层欺骗概念提升到系统级设计和实际实现层面，并通过AI（视觉Transformer和语义编解码器）赋能，展示了跨层协同增强安全性的潜力。"}}
{"id": "2507.18743", "title": "SAR-TEXT: A Large-Scale SAR Image-Text Dataset Built with SAR-Narrator and Progressive Transfer Learning", "authors": ["Xinjun Cheng", "Yiguo He", "Junjie Zhu", "Chunping Qiu", "Jun Wang", "Qiangjuan Huang", "Ke Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      IEEE Submission", "url": "http://arxiv.org/abs/2507.18743v1", "summary": "Vision Language Models (VLMs) have achieved remarkable breakthroughs in the\nfield of remote sensing in recent years. Synthetic Aperture Radar (SAR)\nimagery, with its all-weather capability, is essential in remote sensing, yet\nthe lack of large-scale, high-quality SAR image-text datasets hinders its\nsemantic understanding. In this paper, we construct SAR-Text, a large-scale and\nhigh-quality dataset consisting of over 130,000 SAR image-text pairs. To\nconstruct the SAR-Text dataset, we design the SAR-Narrator framework, which\ngenerates textual descriptions for SAR images through a multi-stage progressive\ntransfer learning strategy. To verify the effectiveness of the SAR-TEXT\ndataset, we conduct experiments on three typical vision-language tasks:\nimage-text retrieval, image captioning, and visual question answering (VQA).\nSpecifically, we construct three representative models on SAR-TEXT:\nSAR-RS-CLIP, SAR-RS-CoCa, and SAR-GPT. SAR-RS-CLIP achieves notable\nimprovements in retrieval performance, boosting average recall by 16.43% and\n10.54% on the OSdataset-512 and HRSID test sets, respectively. In the\ncaptioning task, SAR-RS-CoCa achieves BLEU-4, SPICE, and CIDEr scores exceeding\nthose of the original CoCa model by more than 8x, 4x, and 10x, respectively. In\nthe VQA task, SAR-GPT outperforms baseline and single-stage models on multiple\nSAR-VQA datasets, demonstrating stronger semantic understanding and reasoning\nability, as further confirmed by qualitative results. It is worth noting that,\nas a flexible captioning tool, SAR-Narrator can be readily adopted by the\ncommunity to construct larger-scale SAR image-text datasets.", "comment": "IEEE Submission", "pdf_url": "http://arxiv.org/pdf/2507.18743v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "SAR-TEXT：一个利用SAR-Narrator和渐进式迁移学习构建的大规模SAR图像-文本数据集", "tldr": "本文构建了一个名为SAR-Text的大规模SAR图像-文本数据集（超过13万对），并提出了SAR-Narrator框架来生成文本描述。实验证明，该数据集在图像-文本检索、图像字幕和视觉问答等任务上显著提升了性能。", "motivation": "合成孔径雷达（SAR）图像在遥感领域至关重要，但缺乏大规模、高质量的SAR图像-文本数据集阻碍了其语义理解。", "method": "本文构建了SAR-Text数据集，其中包含超过13万对SAR图像-文本。为构建该数据集，设计了SAR-Narrator框架，通过多阶段渐进式迁移学习策略为SAR图像生成文本描述。为验证数据集有效性，在图像-文本检索、图像字幕和视觉问答（VQA）三个典型视觉-语言任务上进行了实验，并构建了SAR-RS-CLIP、SAR-RS-CoCa和SAR-GPT三个代表性模型。", "result": "在检索任务中，SAR-RS-CLIP在OSdataset-512和HRSID测试集上平均召回率分别提高了16.43%和10.54%。在字幕任务中，SAR-RS-CoCa的BLEU-4、SPICE和CIDEr分数分别比原始CoCa模型高出8倍、4倍和10倍以上。在VQA任务中，SAR-GPT在多个SAR-VQA数据集上优于基线和单阶段模型，表现出更强的语义理解和推理能力。", "conclusion": "SAR-Text数据集和SAR-Narrator框架在提升SAR图像的语义理解和视觉-语言任务性能方面是有效的。SAR-Narrator作为一个灵活的字幕工具，可以被社区用于构建更大规模的SAR图像-文本数据集。", "translation": "近年来，视觉语言模型（VLMs）在遥感领域取得了显著突破。合成孔径雷达（SAR）图像凭借其全天候能力在遥感中至关重要，但缺乏大规模、高质量的SAR图像-文本数据集阻碍了其语义理解。在本文中，我们构建了SAR-Text，一个包含超过13万对SAR图像-文本的大规模高质量数据集。为了构建SAR-Text数据集，我们设计了SAR-Narrator框架，该框架通过多阶段渐进式迁移学习策略为SAR图像生成文本描述。为了验证SAR-TEXT数据集的有效性，我们对三种典型的视觉-语言任务进行了实验：图像-文本检索、图像字幕和视觉问答（VQA）。具体来说，我们在SAR-TEXT上构建了三个代表性模型：SAR-RS-CLIP、SAR-RS-CoCa和SAR-GPT。SAR-RS-CLIP在检索性能方面取得了显著改进，在OSdataset-512和HRSID测试集上平均召回率分别提高了16.43%和10.54%。在字幕任务中，SAR-RS-CoCa的BLEU-4、SPICE和CIDEr分数分别超过原始CoCa模型8倍、4倍和10倍以上。在VQA任务中，SAR-GPT在多个SAR-VQA数据集上优于基线和单阶段模型，表现出更强的语义理解和推理能力，这通过定性结果得到进一步证实。值得注意的是，作为一种灵活的字幕工具，SAR-Narrator可以很容易地被社区采用，以构建更大规模的SAR图像-文本数据集。", "summary": "本文针对SAR图像语义理解中缺乏大规模高质量图像-文本数据集的问题，构建了SAR-Text数据集，包含超过13万对SAR图像-文本。为生成文本描述，提出了SAR-Narrator框架，采用多阶段渐进式迁移学习策略。通过在图像-文本检索、图像字幕和视觉问答任务上进行实验，并构建了SAR-RS-CLIP、SAR-RS-CoCa和SAR-GPT模型，验证了SAR-Text数据集的有效性，取得了显著的性能提升。SAR-Narrator也被证明是一个灵活的字幕工具，有助于未来构建更大规模的SAR图像-文本数据集。", "keywords": "SAR-Text, SAR-Narrator, 图像-文本数据集, 视觉语言模型, 迁移学习", "comments": "本文的主要创新在于构建了迄今为止最大规模的SAR图像-文本数据集SAR-Text，并通过SAR-Narrator框架解决了SAR图像描述生成的难题。这种数据驱动的方法极大地推动了SAR领域视觉语言模型的发展和应用，为SAR图像的语义理解提供了坚实的基础。SAR-Narrator的灵活性也预示着其在未来数据扩充中的潜力。"}}
{"id": "2507.19245", "title": "Transfinite Fixed Points in Alpay Algebra as Ordinal Game Equilibria in Dependent Type Theory", "authors": ["Faruk Alpay", "Bugra Kilictas", "Taylan Alpay"], "categories": ["cs.LO", "cs.AI", "68T27, 03B70, 68Q55"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      21 pages, 1 figure", "url": "http://arxiv.org/abs/2507.19245v1", "summary": "This paper contributes to the Alpay Algebra by demonstrating that the stable\noutcome of a self referential process, obtained by iterating a transformation\nthrough all ordinal stages, is identical to the unique equilibrium of an\nunbounded revision dialogue between a system and its environment. The analysis\ninitially elucidates how classical fixed point theorems guarantee such\nconvergence in finite settings and subsequently extends the argument to the\ntransfinite domain, relying upon well founded induction and principles of order\ntheoretic continuity.\n  Furthermore, the resulting transordinal fixed point operator is embedded into\ndependent type theory, a formalization which permits every step of the\ntransfinite iteration and its limit to be verified within a modern proof\nassistant. This procedure yields a machine checked proof that the iterative\ndialogue necessarily stabilizes and that its limit is unique. The result\nprovides a foundation for Alpay's philosophical claim of semantic convergence\nwithin the framework of constructive logic. By unifying concepts from fixed\npoint theory, game semantics, ordinal analysis, and type theory, this research\nestablishes a broadly accessible yet formally rigorous foundation for reasoning\nabout infinite self referential systems and offers practical tools for\ncertifying their convergence within computational environments.", "comment": "21 pages, 1 figure", "pdf_url": "http://arxiv.org/pdf/2507.19245v1", "cate": "cs.LO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "阿尔佩代数中的超限不动点作为依赖类型理论中的序数博弈均衡", "tldr": "本文证明了阿尔佩代数中通过序数阶段迭代得到的自指过程的稳定结果，等同于系统与其环境之间无限修正对话的唯一均衡，并使用依赖类型理论进行了机器验证。", "motivation": "本文旨在为阿尔佩关于语义收敛的哲学主张在构造性逻辑框架内提供基础，并通过统一不动点理论、博弈语义、序数分析和类型理论的概念，为推理无限自指系统建立一个可访问且严格的形式化基础。", "method": "1. 首先阐明经典不动点定理如何在有限设置中保证收敛。2. 随后将论证扩展到超限域，依赖于良基归纳法和序理论连续性原理。3. 将超序数不动点算子嵌入到依赖类型理论中。4. 使用现代证明助手进行机器验证，证明迭代对话必然稳定且其极限唯一。", "result": "1. 证明了通过所有序数阶段迭代变换得到的自指过程的稳定结果，与系统及其环境之间无限修正对话的唯一均衡相同。2. 通过机器验证证明了迭代对话必然稳定且其极限是唯一的。3. 为推理无限自指系统建立了广泛可访问且形式上严格的基础。4. 为在计算环境中验证系统收敛性提供了实用工具。", "conclusion": "本文通过将不动点理论、博弈语义、序数分析和类型理论的概念统一起来，为在构造性逻辑框架内推理无限自指系统提供了坚实的基础，并证明了其收敛性和唯一性，提供了实用的验证工具。", "translation": "本文通过证明在所有序数阶段迭代变换所获得的自指过程的稳定结果与系统及其环境之间无限修正对话的唯一均衡相同，从而为阿尔佩代数做出了贡献。分析首先阐明了经典不动点定理如何在有限设置中保证此类收敛，随后将论证扩展到超限域，依赖于良基归纳法和序理论连续性原理。此外，由此产生的超序数不动点算子被嵌入到依赖类型理论中，这种形式化允许在现代证明助手中验证超限迭代的每一步及其极限。此过程产生了机器验证的证明，表明迭代对话必然稳定且其极限是唯一的。该结果为阿尔佩在构造性逻辑框架内关于语义收敛的哲学主张提供了基础。通过统一不动点理论、博弈语义、序数分析和类型理论的概念，这项研究为推理无限自指系统建立了广泛可访问且形式上严格的基础，并为在计算环境中验证其收敛性提供了实用工具。", "summary": "本文在阿尔佩代数框架下，证明了通过序数阶段迭代得到的自指过程的稳定结果与系统和环境之间无限修正对话的唯一均衡是等价的。研究从有限设置的不动点定理出发，扩展到超限域，并利用依赖类型理论和现代证明助手对结果进行了机器验证，确保了迭代过程的稳定性和极限的唯一性。该工作为理解无限自指系统提供了严谨的数学基础，并支持了阿尔佩关于语义收敛的哲学观点。", "keywords": "阿尔佩代数, 超限不动点, 序数博弈均衡, 依赖类型理论, 自指系统", "comments": "这篇论文的创新点在于它将抽象的超限不动点理论与具体的序数博弈均衡概念联系起来，并利用依赖类型理论提供了机器可验证的形式化证明。这种方法不仅加强了理论的严谨性，也为在计算环境中验证复杂自指系统的收敛性提供了实用工具，具有重要的理论和实践意义。它成功地桥接了不动点理论、博弈语义、序数分析和类型理论等多个领域。"}}
{"id": "2407.15423", "title": "Integrating IP Broadcasting with Audio Tags: Workflow and Challenges", "authors": ["Rhys Burchett-Vass", "Arshdeep Singh", "Gabriel Bibbó", "Mark D. Plumbley"], "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted for publication in 2025 AES International Conference on Artificial Intelligence and Machine Learning for Audio", "url": "http://arxiv.org/abs/2407.15423v3", "summary": "The broadcasting industry has adopted IP technologies, revolutionising both\nlive and pre-recorded content production, from news gathering to live music\nevents. IP broadcasting allows for the transport of audio and video signals in\nan easily configurable way, aligning with modern networking techniques. This\nshift towards an IP workflow allows for much greater flexibility, not only in\nrouting signals but with the integration of tools using standard web\ndevelopment techniques. One possible tool could include the use of live audio\ntagging, which has a number of uses in the production of content. These could\ninclude adding sound effects to automated closed captioning or identifying\nunwanted sound events within a scene. In this paper, we describe the process of\ncontainerising an audio tagging model into a microservice, a small segregated\ncode module that can be integrated into a multitude of different network\nsetups. The goal is to develop a modular, accessible, and flexible tool capable\nof seamless deployment into broadcasting workflows of all sizes, from small\nproductions to large corporations. Challenges surrounding latency of the\nselected audio tagging model and its effect on the usefulness of the end\nproduct are discussed.", "comment": "Accepted for publication in 2025 AES International Conference on\n  Artificial Intelligence and Machine Learning for Audio", "pdf_url": "http://arxiv.org/pdf/2407.15423v3", "cate": "eess.AS", "date": "2024-07-22", "updated": "2025-07-25", "AI": {"title_translation": "IP广播与音频标签的集成：工作流程与挑战", "tldr": "本文描述了将音频标签模型容器化为微服务，并将其集成到IP广播工作流程中的过程，同时讨论了相关的延迟挑战，旨在实现灵活部署。", "motivation": "广播行业正向IP技术转型，带来更大的灵活性。音频标签在内容制作中具有多种用途（如为自动字幕添加音效、识别不需要的声音事件）。本文的动机是将音频标签作为一种模块化、可访问且灵活的工具，无缝集成到现代IP广播工作流程中。", "method": "本文描述了将音频标签模型容器化为微服务的流程。该微服务被设计为一个小型、独立的代码模块，可集成到多种不同的网络设置中。", "result": "本文的目标是开发一个模块化、可访问且灵活的工具，能够无缝部署到各种规模的广播工作流程中。文中讨论了所选音频标签模型的延迟及其对最终产品实用性的影响所面临的挑战。", "conclusion": "本文描述了将音频标签模型作为模块化微服务集成到IP广播中的过程，并强调了解决延迟问题对于实际应用的重要性。", "translation": "广播行业已采用IP技术，彻底改变了新闻采集到现场音乐活动等直播和预录内容的制作方式。IP广播允许以易于配置的方式传输音频和视频信号，这与现代网络技术相符。这种向IP工作流程的转变带来了更大的灵活性，不仅体现在信号路由上，还体现在使用标准网络开发技术集成工具方面。其中一种可能的工具是实时音频标签的使用，它在内容制作中具有多种用途。这些用途可能包括为自动字幕添加音效，或识别场景中不需要的声音事件。在本文中，我们描述了将音频标签模型容器化为微服务的流程，微服务是一个小型独立代码模块，可以集成到多种不同的网络设置中。目标是开发一个模块化、可访问且灵活的工具，能够无缝部署到从小规模制作到大型企业的所有规模的广播工作流程中。讨论了所选音频标签模型的延迟及其对最终产品有用性的影响所面临的挑战。", "summary": "本文探讨了将实时音频标签集成到IP广播工作流程中的方法，以提升内容制作的灵活性。文中详细阐述了将音频标签模型容器化为微服务的过程，旨在创建一个模块化、可访问且灵活的工具。作者还讨论了与音频标签模型延迟相关的挑战及其对最终产品实用性的影响，旨在实现该工具在各种规模制作中的无缝部署。", "keywords": "IP广播, 音频标签, 微服务, 工作流程, 延迟", "comments": "鉴于广播行业向IP技术的转变，本文探讨的主题非常及时。将音频标签模型作为微服务集成的方法具有创新性，有助于提升系统的模块化和灵活性。文中对延迟挑战的关注，对于确保该技术在实时广播环境中的实际应用至关重要。"}}
{"id": "2506.01177", "title": "Bridging Quantum and Classical Computing in Drug Design: Architecture Principles for Improved Molecule Generation", "authors": ["Andrew Smith", "Erhan Guven"], "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published in Proceedings of the Workshop on Generative AI for Biology at the 42nd International Conference on Machine Learning 10 pages, 7 figures", "url": "http://arxiv.org/abs/2506.01177v2", "summary": "Hybrid quantum-classical machine learning offers a path to leverage noisy\nintermediate-scale quantum (NISQ) devices for drug discovery, but optimal model\narchitectures remain unclear. We systematically optimize the quantum-classical\nbridge architecture of generative adversarial networks (GANs) for molecule\ndiscovery using multi-objective Bayesian optimization. Our optimized model\n(BO-QGAN) significantly improves performance, achieving a 2.27-fold higher Drug\nCandidate Score (DCS) than prior quantum-hybrid benchmarks and 2.21-fold higher\nthan the classical baseline, while reducing parameter count by more than 60%.\nKey findings favor layering multiple (3-4) shallow (4-8 qubit) quantum circuits\nsequentially, while classical architecture shows less sensitivity above a\nminimum capacity. This work provides the first empirically-grounded\narchitectural guidelines for hybrid models, enabling more effective integration\nof current quantum computers into pharmaceutical research pipelines.", "comment": "Published in Proceedings of the Workshop on Generative AI for Biology\n  at the 42nd International Conference on Machine Learning 10 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.01177v2", "cate": "cs.LG", "date": "2025-06-01", "updated": "2025-07-25", "AI": {"title_translation": "药物设计中量子与经典计算的桥接：改进分子生成的架构原则", "tldr": "混合量子-经典机器学习应用于药物发现；优化后的GAN架构（BO-QGAN）显著提高了药物候选分数并减少了参数，为混合模型提供了架构指南。", "motivation": "利用混合量子-经典机器学习将噪声中等规模量子（NISQ）设备应用于药物发现，但最佳模型架构尚不明确。", "method": "使用多目标贝叶斯优化，系统地优化了用于分子发现的生成对抗网络（GANs）的量子-经典桥接架构。", "result": "优化后的模型（BO-QGAN）性能显著提升，药物候选分数（DCS）比之前的量子混合基准高2.27倍，比经典基线高2.21倍，同时参数数量减少了60%以上。关键发现表明，分层多个（3-4个）浅层（4-8量子比特）量子电路是优选的，而经典架构在达到最小容量后敏感度较低。", "conclusion": "这项工作首次为混合模型提供了基于经验的架构指南，从而使当前量子计算机能够更有效地整合到药物研究流程中。", "translation": "混合量子-经典机器学习为利用噪声中等规模量子（NISQ）设备进行药物发现提供了一条途径，但最佳模型架构仍不明确。我们使用多目标贝叶斯优化，系统地优化了用于分子发现的生成对抗网络（GANs）的量子-经典桥接架构。我们优化后的模型（BO-QGAN）显著提升了性能，药物候选分数（DCS）比之前的量子混合基准高2.27倍，比经典基线高2.21倍，同时参数数量减少了60%以上。关键发现表明，分层多个（3-4个）浅层（4-8量子比特）量子电路是优选的，而经典架构在达到最小容量后敏感度较低。这项工作首次为混合模型提供了基于经验的架构指南，从而使当前量子计算机能够更有效地整合到药物研究流程中。", "summary": "本文利用多目标贝叶斯优化，优化了用于药物分子发现的生成对抗网络（GANs）的量子-经典桥接架构。由此产生的BO-QGAN模型在药物候选分数（DCS）方面显著优于先前的量子混合和经典基线，同时减少了参数。它提供了基于经验的架构指南，建议分层多个浅层量子电路，以更好地将当前量子计算机整合到药物研究中。", "keywords": "量子-经典混合, 药物设计, 分子生成, GANs, 贝叶斯优化", "comments": "这篇论文在药物发现的量子-经典混合模型实践方面迈出了重要一步。其经验优化和具体的架构指南（分层浅层量子电路）具有创新性，直接解决了利用NISQ设备的挑战。参数数量的减少也是一个显著的实际优势。这项工作对于推动量子计算在制药领域的应用具有重要意义。"}}
{"id": "2507.18867", "title": "Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning via Incorporating Generalized Human Expertise", "authors": ["Xuefei Wu", "Xiao Yin", "Yuanyang Zhu", "Chunlin Chen"], "categories": ["cs.LG", "cs.AI", "cs.MA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      IEEE International Conference on Systems, Man, and Cybernetics", "url": "http://arxiv.org/abs/2507.18867v1", "summary": "Efficient exploration in multi-agent reinforcement learning (MARL) is a\nchallenging problem when receiving only a team reward, especially in\nenvironments with sparse rewards. A powerful method to mitigate this issue\ninvolves crafting dense individual rewards to guide the agents toward efficient\nexploration. However, individual rewards generally rely on manually engineered\nshaping-reward functions that lack high-order intelligence, thus it behaves\nineffectively than humans regarding learning and generalization in complex\nproblems. To tackle these issues, we combine the above two paradigms and\npropose a novel framework, LIGHT (Learning Individual Intrinsic reward via\nIncorporating Generalized Human experTise), which can integrate human knowledge\ninto MARL algorithms in an end-to-end manner. LIGHT guides each agent to avoid\nunnecessary exploration by considering both individual action distribution and\nhuman expertise preference distribution. Then, LIGHT designs individual\nintrinsic rewards for each agent based on actionable representational\ntransformation relevant to Q-learning so that the agents align their action\npreferences with the human expertise while maximizing the joint action value.\nExperimental results demonstrate the superiority of our method over\nrepresentative baselines regarding performance and better knowledge reusability\nacross different sparse-reward tasks on challenging scenarios.", "comment": "IEEE International Conference on Systems, Man, and Cybernetics", "pdf_url": "http://arxiv.org/pdf/2507.18867v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "通过融合广义人类经验在多智能体强化学习中学习个体内在奖励", "tldr": "LIGHT框架通过结合人类经验为多智能体强化学习中的每个智能体学习内在奖励，以解决稀疏奖励环境下的探索效率问题。", "motivation": "在稀疏奖励环境中，多智能体强化学习（MARL）的有效探索是一个挑战，尤其是在只接收到团队奖励时。手动设计的个体奖励函数缺乏高阶智能，导致在复杂问题中学习和泛化能力不足。", "method": "本文提出了一种名为LIGHT（Learning Individual Intrinsic reward via Incorporating Generalized Human experTise）的新框架。LIGHT通过考虑个体动作分布和人类专业知识偏好分布来引导每个智能体避免不必要的探索。然后，LIGHT基于与Q学习相关的可操作表示转换，为每个智能体设计个体内在奖励，使智能体在最大化联合动作价值的同时，使其动作偏好与人类专业知识对齐。", "result": "实验结果表明，该方法在性能和跨不同稀疏奖励任务的知识重用性方面优于代表性基线。", "conclusion": "LIGHT框架通过有效整合人类专业知识，显著提高了多智能体强化学习在稀疏奖励环境下的探索效率和知识重用性。", "translation": "多智能体强化学习（MARL）中的高效探索是一个具有挑战性的问题，尤其是在只接收到团队奖励的稀疏奖励环境中。一种缓解此问题的强大方法是设计密集的个体奖励，以引导智能体进行高效探索。然而，个体奖励通常依赖于手动设计的奖励整形函数，这些函数缺乏高阶智能，因此在复杂问题的学习和泛化方面不如人类有效。为了解决这些问题，我们结合上述两种范式，提出了一种新颖的框架——LIGHT（通过融合广义人类经验学习个体内在奖励），该框架可以以端到端的方式将人类知识整合到MARL算法中。LIGHT通过同时考虑个体动作分布和人类专业知识偏好分布来引导每个智能体避免不必要的探索。然后，LIGHT基于与Q学习相关的可操作表示转换，为每个智能体设计个体内在奖励，从而使智能体在最大化联合动作价值的同时，使其动作偏好与人类专业知识对齐。实验结果表明，我们的方法在具有挑战性的场景下，在性能和跨不同稀疏奖励任务的知识可重用性方面优于代表性基线。", "summary": "本文提出了LIGHT框架，旨在解决多智能体强化学习（MARL）在稀疏奖励环境中探索效率低下的问题。LIGHT通过将人类专业知识以端到端的方式整合到MARL算法中，为每个智能体学习个体内在奖励。它通过考虑个体动作分布和人类经验偏好来指导智能体进行高效探索，并设计基于Q学习的内在奖励，使智能体行为与人类经验对齐。实验证明LIGHT在性能和知识重用性方面优于现有方法。", "keywords": "多智能体强化学习, 个体内在奖励, 人类专业知识, 稀疏奖励, 探索", "comments": "该论文的创新点在于将人类专业知识以端到端的方式整合到多智能体强化学习中，通过学习个体内在奖励来解决稀疏奖励环境下的探索难题。这种结合人工经验和强化学习的方法，提升了模型的学习效率和泛化能力，尤其是在复杂和稀疏奖励场景下，具有重要的实践意义。"}}
{"id": "2507.19271", "title": "Fine-Tuning Multilingual Language Models for Code Review: An Empirical Study on Industrial C# Projects", "authors": ["Igli Begolli", "Meltem Aksoy", "Daniel Neider"], "categories": ["cs.SE", "cs.AI", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19271v1", "summary": "Code review is essential for maintaining software quality but often\ntime-consuming and cognitively demanding, especially in industrial\nenvironments. Recent advancements in language models (LMs) have opened new\navenues for automating core review tasks. This study presents the empirical\nevaluation of monolingual fine-tuning on the performance of open-source LMs\nacross three key automated code review tasks: Code Change Quality Estimation,\nReview Comment Generation, and Code Refinement. We fine-tuned three distinct\nmodels, CodeReviewer, CodeLlama-7B, and DeepSeek-R1-Distill, on a C\\# specific\ndataset combining public benchmarks with industrial repositories. Our study\ninvestigates how different configurations of programming languages and natural\nlanguages in the training data affect LM performance, particularly in comment\ngeneration. Additionally, we benchmark the fine-tuned models against an\nautomated software analysis tool (ASAT) and human reviewers to evaluate their\npractical utility in real-world settings. Our results show that monolingual\nfine-tuning improves model accuracy and relevance compared to multilingual\nbaselines. While LMs can effectively support code review workflows, especially\nfor routine or repetitive tasks, human reviewers remain superior in handling\nsemantically complex or context-sensitive changes. Our findings highlight the\nimportance of language alignment and task-specific adaptation in optimizing LMs\nfor automated code review.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19271v1", "cate": "cs.SE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "微调多语言语言模型用于代码审查：一项针对工业C#项目的实证研究", "tldr": "本研究对在工业C#项目上微调多语言语言模型进行代码审查的性能进行了实证评估，发现单语微调能提高模型准确性，并指出语言对齐和任务适应的重要性。", "motivation": "代码审查对于维护软件质量至关重要，但在工业环境中通常耗时且认知负担重。语言模型（LMs）的最新进展为自动化核心代码审查任务提供了新途径。", "method": "本研究对CodeReviewer、CodeLlama-7B和DeepSeek-R1-Distill这三个模型进行了单语微调，使用了结合公共基准和工业C#仓库的特定C#数据集。研究评估了模型在代码变更质量评估、审查评论生成和代码精炼这三个自动化代码审查任务上的性能。此外，还将微调后的模型与自动化软件分析工具（ASAT）和人工审阅者进行了基准测试。", "result": "单语微调相比多语言基线提高了模型的准确性和相关性。语言模型可以有效支持代码审查工作流，特别是对于常规或重复性任务。然而，在处理语义复杂或上下文敏感的变更时，人工审阅者仍然更胜一筹。", "conclusion": "本研究的结果强调了语言对齐和任务特定适应在优化语言模型以实现自动化代码审查方面的重要性。", "translation": "代码审查对于维护软件质量至关重要，但在工业环境中通常耗时且认知负担重。语言模型（LMs）的最新进展为自动化核心审查任务开辟了新途径。本研究对开源语言模型在三个关键自动化代码审查任务（代码变更质量评估、审查评论生成和代码精炼）上的单语微调性能进行了实证评估。我们对CodeReviewer、CodeLlama-7B和DeepSeek-R1-Distill这三个不同的模型进行了微调，使用了结合公共基准和工业仓库的C#特定数据集。我们的研究调查了训练数据中编程语言和自然语言的不同配置如何影响语言模型性能，特别是在评论生成方面。此外，我们还将微调后的模型与自动化软件分析工具（ASAT）和人工审阅者进行了基准测试，以评估它们在实际应用中的实用性。我们的结果表明，与多语言基线相比，单语微调提高了模型的准确性和相关性。虽然语言模型可以有效支持代码审查工作流，特别是对于常规或重复性任务，但人工审阅者在处理语义复杂或上下文敏感的变更方面仍然更胜一筹。我们的发现强调了语言对齐和任务特定适应在优化语言模型以实现自动化代码审查方面的重要性。", "summary": "本研究对在工业C#项目上对多语言语言模型进行单语微调以实现代码审查自动化进行了实证评估。通过对CodeReviewer、CodeLlama-7B和DeepSeek-R1-Distill模型在C#数据集上进行微调，并将其应用于代码变更质量评估、评论生成和代码精炼任务。研究发现，单语微调显著提升了模型性能，尤其是在准确性和相关性方面。虽然语言模型能有效处理常规审查任务，但在复杂场景下人工审查仍占优势，凸显了语言和任务特定适应的重要性。", "keywords": "代码审查, 语言模型, 单语微调, C#, 自动化", "comments": "这项研究通过实证分析，验证了在特定编程语言和任务上对大型语言模型进行单语微调的有效性，对于推动代码审查自动化具有实际指导意义。它不仅提供了具体的模型和数据集配置，还明确指出了当前语言模型在复杂语义理解方面的局限性，为未来研究指明了方向。其创新之处在于将LMs应用于工业C#项目的代码审查，并对比了不同配置和基线，具有较高的实践价值。"}}
{"id": "2507.19290", "title": "Query Efficient Structured Matrix Learning", "authors": ["Noah Amsel", "Pratyush Avi", "Tyler Chen", "Feyza Duman Keles", "Chinmay Hegde", "Cameron Musco", "Christopher Musco", "David Persson"], "categories": ["cs.DS", "cs.LG", "cs.NA", "math.NA"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19290v1", "summary": "We study the problem of learning a structured approximation (low-rank,\nsparse, banded, etc.) to an unknown matrix $A$ given access to matrix-vector\nproduct (matvec) queries of the form $x \\rightarrow Ax$ and $x \\rightarrow\nA^Tx$. This problem is of central importance to algorithms across scientific\ncomputing and machine learning, with applications to fast multiplication and\ninversion for structured matrices, building preconditioners for first-order\noptimization, and as a model for differential operator learning. Prior work\nfocuses on obtaining query complexity upper and lower bounds for learning\nspecific structured matrix families that commonly arise in applications.\n  We initiate the study of the problem in greater generality, aiming to\nunderstand the query complexity of learning approximations from general matrix\nfamilies. Our main result focuses on finding a near-optimal approximation to\n$A$ from any finite-sized family of matrices, $\\mathcal{F}$. Standard results\nfrom matrix sketching show that $O(\\log|\\mathcal{F}|)$ matvec queries suffice\nin this setting. This bound can also be achieved, and is optimal, for\nvector-matrix-vector queries of the form $x,y\\rightarrow x^TAy$, which have\nbeen widely studied in work on rank-$1$ matrix sensing.\n  Surprisingly, we show that, in the matvec model, it is possible to obtain a\nnearly quadratic improvement in complexity, to\n$\\tilde{O}(\\sqrt{\\log|\\mathcal{F}|})$. Further, we prove that this bound is\ntight up to log-log factors.Via covering number arguments, our result extends\nto well-studied infinite families. As an example, we establish that a\nnear-optimal approximation from any \\emph{linear matrix family} of dimension\n$q$ can be learned with $\\tilde{O}(\\sqrt{q})$ matvec queries, improving on an\n$O(q)$ bound achievable via sketching techniques and vector-matrix-vector\nqueries.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19290v1", "cate": "cs.DS", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "查询高效的结构化矩阵学习", "tldr": "本文研究了使用矩阵向量乘积查询学习结构化矩阵近似的问题。对于有限矩阵族，它在查询复杂度上实现了近乎二次方的改进，并将此扩展到无限族。", "motivation": "该研究旨在解决在给定矩阵向量乘积查询的情况下，学习未知矩阵的结构化近似（如低秩、稀疏、带状）的问题。这个问题在科学计算和机器学习算法中至关重要，应用于结构化矩阵的快速乘法和求逆、一阶优化的预处理器构建以及微分算子学习。现有工作主要关注特定结构化矩阵族的查询复杂度，而本文旨在更普遍地理解从通用矩阵族中学习近似的查询复杂度。", "method": "研究主要集中于从任何有限大小的矩阵族 $\\mathcal{F}$ 中寻找矩阵 $A$ 的近最优近似。方法使用矩阵向量乘积（matvec）查询（形式为 $x \\rightarrow Ax$ 和 $x \\rightarrow A^Tx$）。通过覆盖数论证，研究结果扩展到无限矩阵族。", "result": "标准矩阵草图方法需要 $O(\\log|\\mathcal{F}|)$ 次matvec查询。本文展示了在matvec模型中，查询复杂度可以实现近乎二次方的改进，达到 $\\tilde{O}(\\sqrt{\\log|\\mathcal{F}|})$。此外，该界限在对数-对数因子内是紧密的。通过覆盖数论证，结果扩展到无限族，例如，对于维度为 $q$ 的线性矩阵族，可以以 $\\tilde{O}(\\sqrt{q})$ 次matvec查询学习到近最优近似，这比现有 $O(q)$ 的界限有显著改进。", "conclusion": "本文表明，通过使用矩阵向量乘积查询，可以在学习结构化矩阵近似的查询复杂度上实现近乎二次方的显著改进，并证明了该界限的紧密性及其对更广泛矩阵族的适用性。", "translation": "我们研究了在给定矩阵向量乘积（matvec）查询形式为 $x \\rightarrow Ax$ 和 $x \\rightarrow A^Tx$ 的情况下，学习未知矩阵 $A$ 的结构化近似（低秩、稀疏、带状等）的问题。这个问题对于科学计算和机器学习中的算法至关重要，应用于结构化矩阵的快速乘法和求逆、构建一阶优化的预处理器以及作为微分算子学习的模型。先前的工作主要集中于获取学习在应用中常见特定结构化矩阵族的查询复杂度的上下界。\n我们更普遍地开始了对该问题的研究，旨在理解从通用矩阵族中学习近似的查询复杂度。我们的主要结果集中于从任何有限大小的矩阵族 $\\mathcal{F}$ 中找到 $A$ 的近最优近似。矩阵草图的标准结果表明，在此设置中 $O(\\log|\\mathcal{F}|)$ 次matvec查询就足够了。对于形式为 $x,y\\rightarrow x^TAy$ 的向量-矩阵-向量查询，这个界限也可以达到并且是最优的，这在秩-1矩阵感知的工作中得到了广泛研究。\n令人惊讶的是，我们证明在matvec模型中，可以将复杂度实现近乎二次方的改进，达到 $\\tilde{O}(\\sqrt{\\log|\\mathcal{F}|})$。此外，我们证明这个界限在对数-对数因子内是紧密的。通过覆盖数论证，我们的结果扩展到了经过充分研究的无限族。例如，我们确定从任何维度为 $q$ 的线性矩阵族中可以学习到近最优近似，只需要 $\\tilde{O}(\\sqrt{q})$ 次matvec查询，这比通过草图技术和向量-矩阵-向量查询可实现的 $O(q)$ 界限有了改进。", "summary": "本文研究了使用矩阵向量乘积查询学习未知矩阵结构化近似的查询复杂度。作者超越了特定的矩阵族，展示了查询复杂度上令人惊讶的近乎二次方的改进，对于有限族 $\\mathcal{F}$ 达到了 $\\tilde{O}(\\sqrt{\\log|\\mathcal{F}|})$，并证明了其紧密性。该结果通过覆盖数论证扩展到无限族，显著改善了现有界限，例如将线性矩阵族的学习查询从 $O(q)$ 提升到 $\\tilde{O}(\\sqrt{q})$。", "keywords": "结构化矩阵学习,查询复杂度,矩阵向量乘积,近似,有限矩阵族,无限矩阵族", "comments": "该论文通过推广结构化矩阵近似的学习问题并在查询效率上取得实质性改进，做出了重要贡献。与现有方法甚至向量-矩阵-向量查询相比，矩阵向量乘积的查询复杂度实现了近乎二次方的加速，这是一个关键创新。界限的紧密性及其对无限族的扩展突显了所提出方法的理论强度和实际适用性。"}}
{"id": "2506.00967", "title": "Pilot Contamination-Aware Graph Attention Network for Power Control in CFmMIMO", "authors": ["Tingting Zhang", "Sergiy A. Vorobyov", "David J. Love", "Taejoon Kim", "Kai Dong"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.00967v2", "summary": "Optimization-based power control algorithms are predominantly iterative with\nhigh computational complexity, making them impractical for real-time\napplications in cell-free massive multiple-input multiple-output (CFmMIMO)\nsystems. Learning-based methods have emerged as a promising alternative, and\namong them, graph neural networks (GNNs) have demonstrated their excellent\nperformance in solving power control problems. However, all existing GNN-based\napproaches assume ideal orthogonality among pilot sequences for user equipments\n(UEs), which is unrealistic given that the number of UEs exceeds the available\northogonal pilot sequences in CFmMIMO schemes. Moreover, most learning-based\nmethods assume a fixed number of UEs, whereas the number of active UEs varies\nover time in practice. Additionally, supervised training necessitates costly\ncomputational resources for computing the target power control solutions for a\nlarge volume of training samples. To address these issues, we propose a graph\nattention network for downlink power control in CFmMIMO systems that operates\nin a self-supervised manner while effectively handling pilot contamination and\nadapting to a dynamic number of UEs. Experimental results show its\neffectiveness, even in comparison to the optimal accelerated projected gradient\nmethod as a baseline.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.00967v2", "cate": "cs.LG", "date": "2025-06-01", "updated": "2025-07-25", "AI": {"title_translation": "导频污染感知图注意力网络用于CFmMIMO中的功率控制", "tldr": "本文提出了一种自监督图注意力网络，用于CFmMIMO系统中的下行链路功率控制，能够有效处理导频污染并适应动态的用户数量，解决了现有优化和学习方法在实时性、导频假设和用户动态性方面的局限性。", "motivation": "现有的基于优化的功率控制算法计算复杂度高，不适用于CFmMIMO系统中的实时应用。现有的基于图神经网络（GNN）的方法假设导频序列理想正交且用户数量固定，这在实际中不现实。此外，监督训练需要大量的计算资源来生成训练样本的目标功率控制解。", "method": "本文提出了一种图注意力网络（Graph Attention Network, GAT），用于CFmMIMO系统中的下行链路功率控制。该方法以自监督方式运行，有效处理导频污染，并能适应动态变化的用户数量。", "result": "实验结果表明，该方法有效，甚至与作为基线的最佳加速投影梯度法相比也表现出色。", "conclusion": "本文提出的自监督图注意力网络是一种有效且实用的CFmMIMO功率控制解决方案，能够克服传统优化方法和现有学习方法的局限性，特别是在处理导频污染和动态用户数量方面。", "translation": "优化型功率控制算法主要是迭代式的，计算复杂度高，使其在无蜂窝大规模多输入多输出（CFmMIMO）系统中的实时应用不切实际。基于学习的方法已成为一种有前景的替代方案，其中图神经网络（GNNs）在解决功率控制问题方面表现出卓越的性能。然而，所有现有的基于GNN的方法都假设用户设备（UEs）之间的导频序列理想正交，考虑到CFmMIMO方案中用户数量超过可用正交导频序列的情况，这是不现实的。此外，大多数基于学习的方法假设用户数量固定，而实际中活跃用户数量随时间变化。另外，监督训练需要昂贵的计算资源来计算大量训练样本的目标功率控制解。为了解决这些问题，我们提出了一种用于CFmMIMO系统中下行链路功率控制的图注意力网络，该网络以自监督方式运行，同时有效处理导频污染并适应动态的用户数量。实验结果显示了其有效性，甚至与作为基线的最佳加速投影梯度法相比也表现出色。", "summary": "本文提出了一种自监督图注意力网络（GAT），用于解决CFmMIMO系统中下行链路功率控制的挑战。该方法旨在克服传统优化算法计算复杂、现有GNN方法对导频正交性和用户数量的非现实假设以及监督训练成本高昂的问题。所提出的GAT能够有效处理导频污染，适应动态用户数量，并以自监督方式运行。实验证明其性能优于基线优化方法，显示了其在实际应用中的潜力。", "keywords": "功率控制, CFmMIMO, 图注意力网络, 导频污染, 自监督学习", "comments": "该论文的创新之处在于提出了一个自监督的图注意力网络来解决CFmMIMO功率控制中的导频污染和动态用户数量问题，克服了现有方法的局限性。其采用自监督学习减少了对大量标注数据的依赖，提高了实用性。该方法在处理现实世界复杂性方面的能力，如导频污染和用户数量变化，使其在实际无线通信系统中具有重要意义。"}}
{"id": "2507.13152", "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models", "authors": ["Xiangyu Dong", "Haoran Zhao", "Jiang Gao", "Haozhou Li", "Xiaoguang Ma", "Yaoming Zhou", "Fuhai Chen", "Juan Liu"], "categories": ["cs.CV", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13152v2", "summary": "Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13152v2", "cate": "cs.CV", "date": "2025-07-17", "updated": "2025-07-25", "AI": {"title_translation": "SE-VLN：一种基于多模态大语言模型的自进化视觉-语言导航框架", "tldr": "提出SE-VLN，一个基于多模态大语言模型的自进化视觉-语言导航框架，通过经验学习和反思实现持续进化，并在VLN任务上取得了显著的性能提升。", "motivation": "现有的基于大语言模型的视觉-语言导航（VLN）方法受限于固定的知识库和推理能力，无法充分整合经验知识，导致缺乏有效的进化能力。", "method": "提出了一个名为SE-VLN的自进化VLN框架，旨在赋予VLN智能体在测试期间持续进化的能力。SE-VLN包含三个核心模块：分层记忆模块（将成功和失败案例转化为可重用知识）、检索增强的基于思考的推理模块（检索经验并实现多步决策）和反思模块（实现持续进化）。", "result": "在R2R和REVERSE数据集的未见环境中，SE-VLN的导航成功率分别达到57%和35.2%，比当前最先进的方法分别提高了23.9%和15.0%。此外，SE-VLN的性能随经验库的增加而提升。", "conclusion": "SE-VLN作为一种自进化的VLN智能体框架，具有巨大的潜力。", "translation": "视觉-语言导航（VLN）的最新进展主要归功于新兴的大语言模型（LLMs）。这些方法在指令理解和任务推理方面表现出卓越的泛化能力。然而，它们受限于LLMs固定的知识库和推理能力，阻碍了对经验知识的充分整合，从而导致缺乏有效的进化能力。为了解决这个问题，我们从自然智能体的进化能力中获得启发，提出了一种自进化VLN框架（SE-VLN），旨在赋予VLN智能体在测试期间持续进化的能力。据我们所知，这是首次提出由多模态大语言模型驱动的自进化VLN框架。具体而言，SE-VLN包含三个核心模块：一个分层记忆模块，用于将成功和失败案例转化为可重用知识；一个检索增强的基于思考的推理模块，用于检索经验并实现多步决策；以及一个反思模块，用于实现持续进化。全面的测试表明，SE-VLN在未见环境中实现了57%和35.2%的导航成功率，分别比R2R和REVERSE数据集上当前最先进的方法绝对性能提高了23.9%和15.0%。此外，SE-VLN随着经验库的增加表现出性能提升，阐明了其作为VLN自进化智能体框架的巨大潜力。", "summary": "本文提出了SE-VLN，一个基于多模态大语言模型的自进化视觉-语言导航框架，旨在解决现有VLN方法缺乏经验知识整合和进化能力的问题。SE-VLN通过分层记忆、检索增强推理和反思模块，使智能体能够在测试过程中持续学习和进化。实验结果表明，SE-VLN在未见环境中显著优于现有SOTA方法，并能随经验积累提升性能，展现了其作为自进化VLN智能体的巨大潜力。", "keywords": "视觉-语言导航, 自进化, 多模态大语言模型, 经验学习, 持续进化", "comments": "这篇论文的创新点在于首次提出了一个由多模态大语言模型驱动的自进化VLN框架，通过引入记忆、检索和反思机制，有效解决了现有方法在经验学习和持续进化方面的局限性。其能够从成功和失败案例中学习并提升性能的能力，对于构建更智能、更适应环境的导航智能体具有重要意义。"}}
{"id": "2507.18775", "title": "Initial Steps in Integrating Large Reasoning and Action Models for Service Composition", "authors": ["Ilche Georgievski", "Marco Aiello"], "categories": ["cs.AI", "cs.SE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      16 pages, 3 figures, 19th Symposium and Summer School on Service-Oriented Computing (SummerSOC)", "url": "http://arxiv.org/abs/2507.18775v1", "summary": "Service composition remains a central challenge in building adaptive and\nintelligent software systems, often constrained by limited reasoning\ncapabilities or brittle execution mechanisms. This paper explores the\nintegration of two emerging paradigms enabled by large language models: Large\nReasoning Models (LRMs) and Large Action Models (LAMs). We argue that LRMs\naddress the challenges of semantic reasoning and ecosystem complexity while\nLAMs excel in dynamic action execution and system interoperability. However,\neach paradigm has complementary limitations - LRMs lack grounded action\ncapabilities, and LAMs often struggle with deep reasoning. We propose an\nintegrated LRM-LAM architectural framework as a promising direction for\nadvancing automated service composition. Such a system can reason about service\nrequirements and constraints while dynamically executing workflows, thus\nbridging the gap between intention and execution. This integration has the\npotential to transform service composition into a fully automated,\nuser-friendly process driven by high-level natural language intent.", "comment": "16 pages, 3 figures, 19th Symposium and Summer School on\n  Service-Oriented Computing (SummerSOC)", "pdf_url": "http://arxiv.org/pdf/2507.18775v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "服务组合中大型推理和行动模型集成的初步探索", "tldr": "本文提出将大型推理模型（LRMs）和大型行动模型（LAMs）集成，以解决服务组合中的推理和执行挑战，旨在实现自动化、用户友好的服务组合。", "motivation": "服务组合在构建自适应和智能软件系统方面仍是核心挑战，常受限于推理能力不足或执行机制脆弱。", "method": "本文探索了将大型推理模型（LRMs）和大型行动模型（LAMs）进行集成，并提出了一个集成的LRM-LAM架构框架，旨在结合两者的优势，弥补各自的局限性。", "result": "提出的集成系统能够对服务需求和约束进行推理，同时动态执行工作流，从而弥合了意图与执行之间的鸿沟。", "conclusion": "将大型推理模型和大型行动模型集成，有望将服务组合转变为一个由高级自然语言意图驱动的完全自动化、用户友好的过程。", "translation": "服务组合在构建自适应和智能软件系统方面仍然是一个核心挑战，通常受到有限推理能力或脆弱执行机制的制约。本文探讨了大型语言模型所支持的两种新兴范式：大型推理模型（LRMs）和大型行动模型（LAMs）的集成。我们认为LRMs解决了语义推理和生态系统复杂性的挑战，而LAMs擅长动态行动执行和系统互操作性。然而，每种范式都有互补的局限性——LRMs缺乏基础行动能力，而LAMs往往难以进行深度推理。我们提出了一个集成的LRM-LAM架构框架，作为推进自动化服务组合的一个有前景的方向。这样的系统可以推理服务需求和约束，同时动态执行工作流，从而弥合了意图与执行之间的鸿沟。这种集成有可能将服务组合转变为一个完全自动化、用户友好的过程，由高级自然语言意图驱动。", "summary": "本文针对服务组合中推理能力和执行机制的挑战，提出了整合大型推理模型（LRMs）和大型行动模型（LAMs）的架构框架。LRMs擅长语义推理，LAMs擅长动态执行，两者互补。该集成系统能同时进行需求推理和工作流执行，有望实现由自然语言驱动的自动化服务组合。", "keywords": "服务组合, 大型推理模型, 大型行动模型, 自然语言处理, 自动化", "comments": "本文提出了将大型推理模型（LRMs）和大型行动模型（LAMs）集成以解决服务组合问题的创新方向。其重要性在于，通过结合两种模型的优势，有望克服现有系统在深层推理和具体行动执行上的局限性，从而推动服务组合向更自动化、更智能的方向发展。这是一个有前景的初步探索，但具体实现细节和实际效果仍需进一步研究和验证。"}}
{"id": "2507.19124", "title": "AI Enabled 6G for Semantic Metaverse: Prospects, Challenges and Solutions for Future Wireless VR", "authors": ["Muhammad Ahmed Mohsin", "Sagnik Bhattacharya", "Abhiram Gorle", "Muhammad Ali Jamshed", "John M. Cioffi"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      IEEE Wireless Communications Magazine", "url": "http://arxiv.org/abs/2507.19124v1", "summary": "Wireless support of virtual reality (VR) has challenges when a network has\nmultiple users, particularly for 3D VR gaming, digital AI avatars, and remote\nteam collaboration. This work addresses these challenges through investigation\nof the low-rank channels that inevitably occur when there are more active users\nthan there are degrees of spatial freedom, effectively often the number of\nantennas. The presented approach uses optimal nonlinear transceivers,\nequivalently generalized decision-feedback or successive cancellation for\nuplink and superposition or dirty-paper precoders for downlink. Additionally, a\npowerful optimization approach for the users' energy allocation and decoding\norder appears to provide large improvements over existing methods, effectively\nnearing theoretical optima. As the latter optimization methods pose real-time\nchallenges, approximations using deep reinforcement learning (DRL) are used to\napproximate best performance with much lower (5x at least) complexity.\nExperimental results show significantly larger sum rates and very large power\nsavings to attain the data rates found necessary to support VR. Experimental\nresults show the proposed algorithm outperforms current industry standards like\northogonal multiple access (OMA), non-orthogonal multiple access (NOMA), as\nwell as the highly researched methods in multi-carrier NOMA (MC-NOMA),\nenhancing sum data rate by 39%, 28%, and 16%, respectively, at a given power\nlevel. For the same data rate, it achieves power savings of 75%, 45%, and 40%,\nmaking it ideal for VR applications. Additionally, a near-optimal deep\nreinforcement learning (DRL)-based resource allocation framework for real-time\nuse by being 5x faster and reaching 83% of the global optimum is introduced.", "comment": "IEEE Wireless Communications Magazine", "pdf_url": "http://arxiv.org/pdf/2507.19124v1", "cate": "cs.NI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "AI赋能的6G语义元宇宙：未来无线VR的展望、挑战与解决方案", "tldr": "该论文提出了一种AI驱动的优化收发器和资源分配方案，用于6G无线VR，显著提高了数据速率和能效，优于现有方法。", "motivation": "当网络中存在多个用户时，无线支持虚拟现实（VR）面临挑战，特别是对于3D VR游戏、数字AI化身和远程团队协作。当活跃用户数量多于空间自由度（通常是天线数量）时，不可避免地会出现低秩信道，这进一步加剧了挑战。", "method": "本研究通过以下方法解决挑战：1) 使用最优非线性收发器，等效于上行链路的广义判决反馈或逐次消除，以及下行链路的叠加或脏纸预编码器。2) 采用强大的优化方法进行用户能量分配和解码顺序，以接近理论最优。3) 针对实时挑战，使用深度强化学习（DRL）进行近似，以在显著降低（至少5倍）复杂度的同时实现最佳性能。", "result": "实验结果显示，在给定功率水平下，总速率显著提高，并实现了非常大的功耗节省，以达到支持VR所需的数据速率。所提出的算法在总数据速率方面，相较于正交多址（OMA）、非正交多址（NOMA）以及多载波NOMA（MC-NOMA）分别提高了39%、28%和16%。对于相同的数据速率，它实现了75%（OMA）、45%（NOMA）和40%（MC-NOMA）的功耗节省，使其成为VR应用的理想选择。此外，还引入了一种基于近最优深度强化学习（DRL）的资源分配框架，该框架实时使用速度快5倍，并达到全局最优的83%。", "conclusion": "本研究提出的AI赋能解决方案，通过利用最优非线性收发器和基于深度强化学习（DRL）的资源分配，显著提升了无线VR的性能，包括数据速率和能效，其表现优于当前的行业标准和高研究方法。", "translation": "无线支持虚拟现实（VR）在网络存在多个用户时面临挑战，特别是对于3D VR游戏、数字AI化身和远程团队协作。本研究通过调查当活跃用户数量多于空间自由度（通常是天线数量）时不可避免出现的低秩信道来解决这些挑战。所提出的方法采用最优非线性收发器，等效于上行链路的广义判决反馈或逐次消除，以及下行链路的叠加或脏纸预编码器。此外，一种强大的用户能量分配和解码顺序优化方法似乎比现有方法提供了巨大的改进，有效地接近理论最优。由于后者的优化方法带来了实时挑战，因此使用深度强化学习（DRL）进行近似，以在大大降低（至少5倍）复杂度的同时近似最佳性能。实验结果显示，总速率显著提高，并实现了非常大的功耗节省，以达到支持VR所需的数据速率。实验结果显示，所提出的算法优于当前的行业标准，如正交多址（OMA）、非正交多址（NOMA），以及多载波NOMA（MC-NOMA）中高度研究的方法，在给定功率水平下，总数据速率分别提高了39%、28%和16%。对于相同的数据速率，它实现了75%、45%和40%的功耗节省，使其成为VR应用的理想选择。此外，还引入了一种基于近最优深度强化学习（DRL）的资源分配框架，该框架实时使用速度快5倍，并达到全局最优的83%。", "summary": "本文研究了多用户场景下无线VR的挑战，特别是低秩信道问题。它提出了一种解决方案，利用最优非线性收发器和强大的能量分配与解码顺序优化方法，并通过深度强化学习实现实时性能近似。实验结果表明，与OMA、NOMA和MC-NOMA相比，该方案在总速率和功耗节省方面取得了显著改进，使其成为未来无线VR应用的理想选择。此外，还引入了一种基于DRL的资源分配框架，以显著降低的复杂度实现近最优性能。", "keywords": "无线VR, 6G, 深度强化学习, 非线性收发器, 低秩信道", "comments": "该论文通过将最优非线性收发器与深度强化学习相结合，为无线VR在挑战性的低秩信道环境中实现实时资源分配，引入了一种创新的方法。其在数据速率和能效方面相对于现有标准的显著性能提升是值得注意的，解决了未来6G和语义元宇宙中沉浸式体验的关键瓶颈。利用DRL降低复杂度的同时保持近最优性能是其主要优势。"}}
{"id": "2507.19233", "title": "Component-Based Machine Learning for Indoor Flow and Temperature Fields Prediction Latent Feature Aggregation and Flow Interaction", "authors": ["Shaofan Wang", "Nils Thuerey", "Philipp Geyer"], "categories": ["cs.LG", "physics.flu-dyn"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19233v1", "summary": "Accurate and efficient prediction of indoor airflow and temperature\ndistributions is essential for building energy optimization and occupant\ncomfort control. However, traditional CFD simulations are computationally\nintensive, limiting their integration into real-time or design-iterative\nworkflows. This study proposes a component-based machine learning (CBML)\nsurrogate modeling approach to replace conventional CFD simulation for fast\nprediction of indoor velocity and temperature fields. The model consists of\nthree neural networks: a convolutional autoencoder with residual connections\n(CAER) to extract and compress flow features, a multilayer perceptron (MLP) to\nmap inlet velocities to latent representations, and a convolutional neural\nnetwork (CNN) as an aggregator to combine single-inlet features into dual-inlet\nscenarios. A two-dimensional room with varying left and right air inlet\nvelocities is used as a benchmark case, with CFD simulations providing training\nand testing data. Results show that the CBML model accurately and fast predicts\ntwo-component aggregated velocity and temperature fields across both training\nand testing datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19233v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于组件的机器学习用于室内气流和温度场预测：潜在特征聚合与气流相互作用", "tldr": "该研究提出了一种基于组件的机器学习（CBML）代理建模方法，以取代传统的CFD模拟，用于快速准确地预测室内速度和温度场。", "motivation": "准确高效地预测室内气流和温度分布对于建筑能耗优化和居住者舒适度控制至关重要。然而，传统的计算流体动力学（CFD）模拟计算量大，限制了其在实时或设计迭代工作流中的应用。", "method": "本研究提出了一种基于组件的机器学习（CBML）代理建模方法，用于快速预测室内速度和温度场，以取代传统的CFD模拟。该模型由三个神经网络组成：一个带有残差连接的卷积自编码器（CAER）用于提取和压缩流场特征，一个多层感知器（MLP）用于将入口速度映射到潜在表示，以及一个卷积神经网络（CNN）作为聚合器，将单入口特征组合成双入口场景。以一个具有不同左右进气速度的二维房间作为基准案例，并使用CFD模拟提供训练和测试数据。", "result": "结果表明，CBML模型能够准确快速地预测训练和测试数据集中的双分量聚合速度和温度场。", "conclusion": "Not mentioned in abstract", "translation": "精确高效地预测室内气流和温度分布对于建筑能耗优化和居住者舒适度控制至关重要。然而，传统的计算流体动力学（CFD）模拟计算量大，限制了其在实时或设计迭代工作流中的应用。本研究提出了一种基于组件的机器学习（CBML）代理建模方法，以取代传统的CFD模拟，用于快速预测室内速度和温度场。该模型由三个神经网络组成：一个带有残差连接的卷积自编码器（CAER）用于提取和压缩流场特征，一个多层感知器（MLP）用于将入口速度映射到潜在表示，以及一个卷积神经网络（CNN）作为聚合器，将单入口特征组合成双入口场景。以一个具有不同左右进气速度的二维房间作为基准案例，并使用CFD模拟提供训练和测试数据。结果表明，CBML模型能够准确快速地预测训练和测试数据集中的双分量聚合速度和温度场。", "summary": "该论文提出了一种基于组件的机器学习（CBML）代理建模方法，旨在取代计算密集型CFD模拟，实现对室内气流和温度场的快速准确预测。CBML模型包含一个卷积自编码器（CAER）用于特征提取，一个多层感知器（MLP）用于速度映射，以及一个卷积神经网络（CNN）用于特征聚合。通过在二维房间基准案例上使用CFD数据进行训练和测试，结果显示CBML模型能够高效且准确地预测聚合的速度和温度场。", "keywords": "组件式机器学习, 室内气流, 温度场预测, 代理模型, 神经网络", "comments": "该研究的创新之处在于其提出的组件式机器学习（CBML）代理建模框架，特别是结合了卷积自编码器、多层感知器和卷积神经网络来处理复杂的流场预测问题，并实现了潜在特征的有效聚合。这对于解决传统CFD模拟计算成本高昂的问题具有重要意义，为建筑能耗优化和舒适度控制提供了更实时的解决方案。其模块化设计也可能便于未来针对不同场景进行扩展和改进。"}}
{"id": "2507.18791", "title": "Evaluating Code-Mixing in LLMs Across 18 Languages", "authors": ["Yilun Yang", "Yekun Chai"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18791v1", "summary": "Code-mixing, the practice of switching between languages within a\nconversation, presents unique challenges for traditional natural language\nprocessing. Existing benchmarks, such as LinCE and GLUECoS, are limited by\nnarrow language pairings and tasks, failing to adequately evaluate the\ncode-mixing capabilities of large language models (LLMs). Despite the\nsignificance of code-mixing for multilingual users, research on LLMs in this\ncontext remains limited. Additionally, current methods for generating\ncode-mixed data are underdeveloped. In this paper, we conduct a comprehensive\nevaluation of LLMs' performance on code-mixed data across 18 languages from\nseven language families. We also propose a novel approach for generating\nsynthetic code-mixed texts by combining word substitution with GPT-4 prompting.\nOur analysis reveals consistent underperformance of LLMs on code-mixed datasets\ninvolving multiple language families. We suggest that improvements in training\ndata size, model scale, and few-shot learning could enhance their performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18791v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "评估大型语言模型在18种语言中的语码混合能力", "tldr": "大型语言模型在处理跨18种语言的语码混合数据时表现不佳，现有基准和数据生成方法不足。本研究进行了全面评估，并提出新的数据生成方法，指出需改进训练数据、模型规模和少样本学习来提升性能。", "motivation": "现有语码混合基准测试（如LinCE和GLUECoS）在语言配对和任务上存在局限性，无法充分评估大型语言模型（LLM）的语码混合能力；同时，当前语码混合数据生成方法不成熟，导致该领域研究有限。", "method": "本研究对大型语言模型在来自七个语系的18种语言的语码混合数据上的性能进行了全面评估。此外，论文提出了一种结合词语替换与GPT-4提示的新型方法来生成合成语码混合文本。", "result": "分析显示，大型语言模型在涉及多个语系的语码混合数据集上持续表现不佳。", "conclusion": "建议通过增加训练数据规模、扩大模型规模和改进少样本学习来提高大型语言模型在语码混合方面的性能。", "translation": "语码混合，即在对话中切换语言的做法，给传统的自然语言处理带来了独特的挑战。现有的基准测试，如LinCE和GLUECoS，受限于狭窄的语言配对和任务，未能充分评估大型语言模型（LLM）的语码混合能力。尽管语码混合对多语言用户具有重要意义，但在此背景下对LLM的研究仍然有限。此外，当前生成语码混合数据的方法尚不成熟。在本文中，我们对LLM在来自七个语系的18种语言的语码混合数据上的性能进行了全面评估。我们还提出了一种通过结合词语替换与GPT-4提示来生成合成语码混合文本的新方法。我们的分析揭示了LLM在涉及多个语系的语码混合数据集上持续表现不佳。我们建议通过增加训练数据规模、模型规模和少样本学习来提高它们的性能。", "summary": "本文全面评估了大型语言模型在18种语言（涵盖7个语系）语码混合数据上的表现，并提出了一种结合词语替换与GPT-4提示的新型语码混合文本生成方法。研究发现，LLM在跨语系的语码混合数据集上表现持续不佳，作者建议通过扩大训练数据、模型规模和改进少样本学习来提升其性能。", "keywords": "语码混合, 大型语言模型, 跨语言评估, 数据生成, GPT-4", "comments": "这篇论文通过对LLM在多达18种语言的语码混合能力进行全面评估，填补了现有基准测试和数据生成方法的不足，具有重要的创新性。其提出的结合词语替换与GPT-4提示的合成数据生成方法为语码混合研究提供了新的思路。研究结果揭示了LLM在处理复杂语码混合时的局限性，并为未来改进提供了明确方向。"}}
{"id": "2312.09880", "title": "Information Extraction from Unstructured data using Augmented-AI and Computer Vision", "authors": ["Aditya Parikh"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2312.09880v2", "summary": "Information extraction (IE) from unstructured documents remains a critical\nchallenge in data processing pipelines. Traditional optical character\nrecognition (OCR) methods and conventional parsing engines demonstrate limited\neffectiveness when processing large-scale document datasets. This paper\npresents a comprehensive framework for information extraction that combines\nAugmented Intelligence (A2I) with computer vision and natural language\nprocessing techniques. Our approach addresses the limitations of conventional\nmethods by leveraging deep learning architectures for object detection,\nparticularly for tabular data extraction, and integrating cloud-based services\nfor scalable document processing. The proposed methodology demonstrates\nimproved accuracy and efficiency in extracting structured information from\ndiverse document formats including PDFs, images, and scanned documents.\nExperimental validation shows significant improvements over traditional\nOCR-based approaches, particularly in handling complex document layouts and\nmulti-modal content extraction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2312.09880v2", "cate": "cs.CV", "date": "2023-12-15", "updated": "2025-07-25", "AI": {"title_translation": "使用增强型人工智能和计算机视觉从非结构化数据中提取信息", "tldr": "本文提出了一种结合增强智能、计算机视觉和自然语言处理的框架，用于从非结构化文档中高效准确地提取信息，解决了传统OCR方法的局限性。", "motivation": "从非结构化文档中提取信息仍然是数据处理中的一个关键挑战。传统的OCR方法和解析引擎在处理大规模文档数据集时效果有限。", "method": "本文提出了一种信息提取的综合框架，该框架结合了增强智能（A2I）、计算机视觉和自然语言处理技术。该方法利用深度学习架构进行对象检测（特别是表格数据提取），并整合基于云的服务以实现可扩展的文档处理。", "result": "所提出的方法在从包括PDF、图像和扫描文档在内的多种文档格式中提取结构化信息方面，显示出更高的准确性和效率。实验验证表明，与传统的基于OCR的方法相比，该方法有显著改进，尤其是在处理复杂文档布局和多模态内容提取方面。", "conclusion": "Not mentioned in abstract", "translation": "从非结构化文档中提取信息（IE）仍然是数据处理流程中的一个关键挑战。传统的光学字符识别（OCR）方法和常规解析引擎在处理大规模文档数据集时表现出有限的有效性。本文提出了一种结合增强智能（A2I）与计算机视觉和自然语言处理技术的综合信息提取框架。我们的方法通过利用深度学习架构进行对象检测，特别是表格数据提取，并集成基于云的服务以实现可扩展的文档处理，从而解决了传统方法的局限性。所提出的方法在从包括PDF、图像和扫描文档在内的多种文档格式中提取结构化信息方面，显示出更高的准确性和效率。实验验证表明，与传统的基于OCR的方法相比，该方法有显著改进，尤其是在处理复杂文档布局和多模态内容提取方面。", "summary": "本文介绍了一个结合增强智能（A2I）、计算机视觉和自然语言处理的综合框架，旨在解决从非结构化文档中提取信息的挑战。该方法利用深度学习进行对象检测（特别是表格），并集成云服务以提高可扩展性。实验结果表明，该框架在处理多种文档格式和复杂布局时，比传统OCR方法具有更高的准确性和效率。", "keywords": "信息提取, 增强智能, 计算机视觉, 深度学习, 非结构化数据", "comments": "该论文提出了一种结合增强智能和计算机视觉的创新方法，以解决非结构化数据信息提取的长期挑战。其通过整合深度学习和云服务，显著提升了传统OCR在处理复杂文档时的局限性，具有重要的实际应用价值。然而，摘要中并未明确提及该方法的具体创新点与现有技术的区别，且未提供关于其局限性的讨论。"}}
{"id": "2507.18808", "title": "Perpetua: Multi-Hypothesis Persistence Modeling for Semi-Static Environments", "authors": ["Miguel Saavedra-Ruiz", "Samer B. Nashed", "Charlie Gauthier", "Liam Paull"], "categories": ["cs.RO", "cs.CV"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025) Code available at this https URL . Webpage and additional videos at this https URL", "url": "http://arxiv.org/abs/2507.18808v1", "summary": "Many robotic systems require extended deployments in complex, dynamic\nenvironments. In such deployments, parts of the environment may change between\nsubsequent robot observations. Most robotic mapping or environment modeling\nalgorithms are incapable of representing dynamic features in a way that enables\npredicting their future state. Instead, they opt to filter certain state\nobservations, either by removing them or some form of weighted averaging. This\npaper introduces Perpetua, a method for modeling the dynamics of semi-static\nfeatures. Perpetua is able to: incorporate prior knowledge about the dynamics\nof the feature if it exists, track multiple hypotheses, and adapt over time to\nenable predicting of future feature states. Specifically, we chain together\nmixtures of \"persistence\" and \"emergence\" filters to model the probability that\nfeatures will disappear or reappear in a formal Bayesian framework. The\napproach is an efficient, scalable, general, and robust method for estimating\nthe states of features in an environment, both in the present as well as at\narbitrary future times. Through experiments on simulated and real-world data,\nwe find that Perpetua yields better accuracy than similar approaches while also\nbeing online adaptable and robust to missing observations.", "comment": "Accepted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025) Code available at\n  https://github.com/montrealrobotics/perpetua-code. Webpage and additional\n  videos at https://montrealrobotics.ca/perpetua/", "pdf_url": "http://arxiv.org/pdf/2507.18808v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "Perpetua：半静态环境下的多假设持久性建模", "tldr": "Perpetua 是一种用于在半静态环境中对动态特征进行建模的新方法，它能够跟踪多个假设并预测未来状态，比现有方法更准确、适应性更强。", "motivation": "许多机器人系统需要在复杂、动态的环境中进行长时间部署，但现有的机器人建图或环境建模算法无法有效表示动态特征并预测其未来状态，通常只能通过过滤或加权平均来处理。", "method": "本文提出了 Perpetua 方法，通过结合“持久性”和“出现”滤波器的混合体，在一个形式化的贝叶斯框架下建模特征消失或重新出现的概率。它能够结合先验知识、跟踪多个假设并随时间调整。", "result": "通过模拟和真实世界数据的实验，Perpetua 比类似方法取得了更好的准确性，并且能够在线适应并对缺失的观测数据保持鲁棒性。", "conclusion": "Perpetua 提供了一种高效、可扩展、通用且鲁棒的方法，用于估计环境中特征的当前和未来状态，有效解决了半静态环境中动态特征建模的挑战。", "translation": "许多机器人系统需要长时间部署在复杂、动态的环境中。在这种部署中，环境的某些部分可能会在随后的机器人观测之间发生变化。大多数机器人建图或环境建模算法无法以能够预测其未来状态的方式表示动态特征。相反，它们选择过滤某些状态观测，通过移除它们或某种形式的加权平均。本文介绍了 Perpetua，一种用于建模半静态特征动态的方法。Perpetua 能够：如果存在关于特征动态的先验知识，则将其纳入；跟踪多个假设；并随时间调整以预测未来的特征状态。具体来说，我们将“持久性”和“出现”滤波器的混合体链接起来，在一个形式化的贝叶斯框架中建模特征消失或重新出现的概率。该方法是一种高效、可扩展、通用且鲁棒的方法，用于估计环境中特征的当前状态以及任意未来时间的状态。通过对模拟和真实世界数据的实验，我们发现 Perpetua 比类似方法具有更高的准确性，同时还具有在线适应性并对缺失的观测数据具有鲁棒性。", "summary": "本文介绍了 Perpetua，一种在半静态环境中对动态特征进行建模的新方法。针对现有机器人系统在处理动态环境时无法预测未来状态的问题，Perpetua 采用结合“持久性”和“出现”滤波器的混合体，在贝叶斯框架下跟踪多假设并适应时间变化，以预测特征的未来状态。实验证明，Perpetua 在准确性、在线适应性和对缺失观测的鲁棒性方面均优于现有方法。", "keywords": "持久性建模, 机器人学, 半静态环境, 多假设, 动态特征", "comments": "Perpetua 的创新之处在于其多假设持久性建模能力，以及将“持久性”和“出现”滤波器结合在贝叶斯框架中，从而能够有效处理半静态环境中的动态特征并预测其未来状态。这对于需要长期部署的机器人系统具有重要意义，因为它解决了现有方法在动态特征表示和预测方面的局限性。其在线适应性和对缺失数据的鲁棒性也增加了其实用性。"}}
{"id": "2507.18918", "title": "Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders", "authors": ["Richmond Sin Jing Xuan", "Jalil Huseynov", "Yang Zhang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18918v1", "summary": "Multilingual large language models (LLMs) exhibit strong cross-linguistic\ngeneralization, yet medium to low resource languages underperform on common\nbenchmarks such as ARC-Challenge, MMLU, and HellaSwag. We analyze activation\npatterns in Gemma-2-2B across all 26 residual layers and 10 languages: Chinese\n(zh), Russian (ru), Spanish (es), Italian (it), medium to low resource\nlanguages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam\n(ml), and Hindi (hi), with English (en) as the reference. Using Sparse\nAutoencoders (SAEs), we reveal systematic disparities in activation patterns.\nMedium to low resource languages receive up to 26.27 percent lower activations\nin early layers, with a persistent gap of 19.89 percent in deeper layers. To\naddress this, we apply activation-aware fine-tuning via Low-Rank Adaptation\n(LoRA), leading to substantial activation gains, such as 87.69 percent for\nMalayalam and 86.32 percent for Hindi, while maintaining English retention at\napproximately 91 percent. After fine-tuning, benchmark results show modest but\nconsistent improvements, highlighting activation alignment as a key factor in\nenhancing multilingual LLM performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18918v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "使用稀疏自编码器揭示大型语言模型中的跨语言差异", "tldr": "本研究使用稀疏自编码器揭示了多语言大型语言模型中资源匮乏语言的激活模式差异，并通过LoRA微调提高了激活水平，从而在基准测试中获得改进。", "motivation": "多语言大型语言模型（LLMs）在跨语言泛化方面表现出色，但中低资源语言在ARC-Challenge、MMLU和HellaSwag等常见基准测试中表现不佳。", "method": "研究分析了Gemma-2-2B模型在所有26个残差层和10种语言（包括中低资源语言和英语作为参考）上的激活模式。使用稀疏自编码器（SAEs）揭示激活模式的系统性差异。为解决这些差异，研究通过低秩适应（LoRA）应用了激活感知微调。", "result": "中低资源语言在早期层中的激活度低26.27%，在深层中持续存在19.89%的差距。通过LoRA微调，激活水平显著提高，例如马拉雅拉姆语提高87.69%，印地语提高86.32%，同时英语保留率保持在91%左右。微调后，基准测试结果显示出适度但一致的改进。", "conclusion": "激活对齐是提高多语言大型语言模型性能的关键因素。", "translation": "多语言大型语言模型（LLMs）表现出强大的跨语言泛化能力，但中低资源语言在ARC-Challenge、MMLU和HellaSwag等常见基准测试中表现不佳。我们分析了Gemma-2-2B模型在所有26个残差层和10种语言（包括中文（zh）、俄语（ru）、西班牙语（es）、意大利语（it），以及中低资源语言如印尼语（id）、加泰罗尼亚语（ca）、马拉地语（mr）、马拉雅拉姆语（ml）和印地语（hi），以英语（en）作为参考）中的激活模式。使用稀疏自编码器（SAEs），我们揭示了激活模式中的系统性差异。中低资源语言在早期层中的激活度低26.27%，在更深层中持续存在19.89%的差距。为了解决这个问题，我们通过低秩适应（LoRA）应用了激活感知微调，从而获得了显著的激活增益，例如马拉雅拉姆语提高了87.69%，印地语提高了86.32%，同时英语的保留率约为91%。微调后，基准测试结果显示出适度但一致的改进，这突出表明激活对齐是增强多语言LLM性能的关键因素。", "summary": "本研究使用稀疏自编码器（SAEs）分析了Gemma-2-2B在多语言环境下的激活模式，揭示了中低资源语言在LLM中存在显著的激活差异。这些语言的激活度在早期层和深层均低于高资源语言。为弥补此差距，研究采用了基于LoRA的激活感知微调，成功提升了中低资源语言的激活水平，并带来了基准测试性能的适度改进，强调了激活对齐对提升多语言LLM性能的重要性。", "keywords": "多语言LLM, 稀疏自编码器, 激活模式, 语言差异, LoRA", "comments": "该论文通过量化分析揭示了多语言LLM中资源匮乏语言的内在激活差异，这是一个重要的发现。使用稀疏自编码器来分析激活模式具有创新性。通过激活感知微调来弥补这些差异并取得效果，为未来多语言LLM的优化提供了新的方向。尽管基准测试的改进是“适度”的，但其核心在于揭示了激活对齐作为性能提升的关键因素，这对于理解LLM的工作机制和进行针对性优化具有重要意义。"}}
{"id": "2507.19058", "title": "ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment", "authors": ["Chong Xia", "Shengjun Zhang", "Fangfu Liu", "Chang Liu", "Khodchaphun Hirunyaratsameewong", "Yueqi Duan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19058v1", "summary": "Perpetual 3D scene generation aims to produce long-range and coherent 3D view\nsequences, which is applicable for long-term video synthesis and 3D scene\nreconstruction. Existing methods follow a \"navigate-and-imagine\" fashion and\nrely on outpainting for successive view expansion. However, the generated view\nsequences suffer from semantic drift issue derived from the accumulated\ndeviation of the outpainting module. To tackle this challenge, we propose\nScenePainter, a new framework for semantically consistent 3D scene generation,\nwhich aligns the outpainter's scene-specific prior with the comprehension of\nthe current scene. To be specific, we introduce a hierarchical graph structure\ndubbed SceneConceptGraph to construct relations among multi-level scene\nconcepts, which directs the outpainter for consistent novel views and can be\ndynamically refined to enhance diversity. Extensive experiments demonstrate\nthat our framework overcomes the semantic drift issue and generates more\nconsistent and immersive 3D view sequences. Project Page:\nhttps://xiac20.github.io/ScenePainter/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19058v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "ScenePainter：基于概念关系对齐的语义一致性永续三维场景生成", "tldr": "ScenePainter 提出了一种新的框架，通过引入层次图结构 SceneConceptGraph 来解决永续三维场景生成中语义漂移的问题，从而生成更一致的视图序列。", "motivation": "现有的永续三维场景生成方法依赖于外绘（outpainting）进行连续视图扩展，但由于外绘模块累积的偏差，生成的视图序列存在语义漂移问题。", "method": "我们提出了 ScenePainter 框架，通过将外绘器的场景特定先验与当前场景的理解对齐，实现语义一致的三维场景生成。具体来说，我们引入了一个名为 SceneConceptGraph 的分层图结构，用于构建多级场景概念之间的关系，该结构指导外绘器生成一致的新颖视图，并且可以动态细化以增强多样性。", "result": "我们的框架克服了语义漂移问题，并生成了更一致、更具沉浸感的三维视图序列。", "conclusion": "ScenePainter 通过引入 SceneConceptGraph 有效解决了永续三维场景生成中的语义漂移问题，显著提升了生成视图序列的一致性和沉浸感。", "translation": "永续三维场景生成旨在产生长范围且连贯的三维视图序列，适用于长期视频合成和三维场景重建。现有方法遵循“导航-想象”模式，并依赖于外绘进行连续视图扩展。然而，生成的视图序列受到外绘模块累积偏差导致的语义漂移问题的影响。为了解决这一挑战，我们提出了 ScenePainter，一个用于语义一致三维场景生成的新框架，它将外绘器的场景特定先验与当前场景的理解对齐。具体来说，我们引入了一个名为 SceneConceptGraph 的分层图结构，用于构建多级场景概念之间的关系，该结构指导外绘器生成一致的新颖视图，并且可以动态细化以增强多样性。大量实验表明，我们的框架克服了语义漂移问题，并生成了更一致、更具沉浸感的三维视图序列。项目主页：https://xiac20.github.io/ScenePainter/。", "summary": "本文提出了 ScenePainter，一个用于永续三维场景生成的框架，旨在解决现有方法中因外绘累积偏差导致的语义漂移问题。ScenePainter 通过引入一个名为 SceneConceptGraph 的分层图结构，对多级场景概念之间的关系进行建模和对齐，从而指导外绘器生成语义一致且连贯的新颖视图。实验证明，该方法有效提升了生成三维视图序列的一致性和沉浸感。", "keywords": "三维场景生成, 语义一致性, 永续生成, 概念关系对齐, SceneConceptGraph", "comments": "ScenePainter 的创新点在于引入了 SceneConceptGraph 来显式地建模和对齐多级场景概念关系，从而解决了永续三维场景生成中长期存在的语义漂移问题。这种基于图结构的方法为生成高质量、一致性强的长范围三维视图序列提供了一个新的视角和有效的解决方案，对于长期视频合成和三维重建等应用具有重要意义。"}}
{"id": "2507.18664", "title": "Generating real-time detailed ground visualisations from sparse aerial point clouds", "authors": ["Aidan Murray", "Eddie Waite", "Caleb Ross", "Scarlet Mitchell", "Alexander Bradley", "Joanna Jamrozy", "Kenny Mitchell"], "categories": ["cs.GR", "cs.CV", "I.3.2; I.4.10"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      CVMP Short Paper. 1 page, 3 figures, CVMP 2022: The 19th ACM SIGGRAPH European Conference on Visual Media Production, London. This work was supported by the European Union's Horizon 2020 research and innovation programme under Grant 101017779", "url": "http://arxiv.org/abs/2507.18664v1", "summary": "Building realistic wide scale outdoor 3D content with sufficient visual\nquality to observe at walking eye level or from driven vehicles is often\ncarried out by large teams of artists skilled in modelling, texturing, material\nshading and lighting, which typically leads to both prohibitive costs and\nreduced accuracy honoring the variety of real world ground truth landscapes. In\nour proposed method, we define a process to automatically amplify real-world\nscanned data and render real-time in animated 3D to explore at close range with\nhigh quality for training, simulation, video game and visualisation\napplications.", "comment": "CVMP Short Paper. 1 page, 3 figures, CVMP 2022: The 19th ACM SIGGRAPH\n  European Conference on Visual Media Production, London. This work was\n  supported by the European Union's Horizon 2020 research and innovation\n  programme under Grant 101017779", "pdf_url": "http://arxiv.org/pdf/2507.18664v1", "cate": "cs.GR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "从稀疏航空点云生成实时详细地面可视化", "tldr": "本文提出了一种从稀疏航空点云自动生成高质量、实时3D地面可视化内容的方法，以解决传统手动建模成本高昂且不准确的问题。", "motivation": "传统的广域户外3D内容创建需要大量艺术家团队进行建模、纹理、材质着色和照明，这通常导致高昂的成本和对真实世界地貌多样性还原度的降低。", "method": "我们提出了一种方法，定义了一个过程来自动放大真实世界扫描数据，并实时渲染动画3D内容，以便在高质量下进行近距离探索。", "result": "该方法能够实时渲染高质量的动画3D内容，用于近距离探索，适用于训练、模拟、视频游戏和可视化应用。", "conclusion": "该论文提出了一种自动化的方法，能够高效且高质量地从扫描数据生成实时详细的地面可视化内容，解决了传统手动制作的成本和精度问题。", "translation": "以足以在步行眼平或驾驶车辆时观察到的视觉质量构建逼真的大规模户外3D内容，通常由大量擅长建模、纹理、材质着色和照明的艺术家团队完成，这通常导致成本过高且在还原真实世界地貌多样性方面的准确性降低。在我们提出的方法中，我们定义了一个过程，以自动放大真实世界扫描数据并实时渲染动画3D内容，以便在高质量下进行近距离探索，适用于训练、模拟、视频游戏和可视化应用。", "summary": "本文针对传统手动创建大规模户外3D内容成本高昂且准确性不足的问题，提出了一种创新的自动化方法。该方法能够从稀疏航空点云中自动放大真实世界扫描数据，并实时生成高质量、动画化的3D地面可视化，从而实现近距离高品质的探索体验，广泛应用于训练、模拟、视频游戏和可视化等领域。", "keywords": "实时可视化, 稀疏点云, 3D内容生成, 地面可视化, 自动化", "comments": "本文提出了一种有前景的自动化方法，解决了传统3D内容制作中效率低下和成本高昂的问题。其创新点在于能够从稀疏数据中生成高质量的实时可视化，这对于需要高保真度场景的应用（如模拟和游戏）具有重要意义。然而，抽象中未详细说明具体的技术细节和性能评估。"}}
{"id": "2412.01131", "title": "A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans", "authors": ["Zhihan Cao", "Hiroaki Yamada", "Simone Teufel", "Takenobu Tokunaga"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.01131v4", "summary": "Recently, much work has concerned itself with the enigma of what exactly\npretrained language models~(PLMs) learn about different aspects of language,\nand how they learn it. One stream of this type of research investigates the\nknowledge that PLMs have about semantic relations. However, many aspects of\nsemantic relations were left unexplored. Generally, only one relation has been\nconsidered, namely hypernymy. Furthermore, previous work did not measure\nhumans' performance on the same task as that performed by the PLMs. This means\nthat at this point in time, there is only an incomplete view of the extent of\nthese models' semantic relation knowledge. To address this gap, we introduce a\ncomprehensive evaluation framework covering five relations beyond hypernymy,\nnamely hyponymy, holonymy, meronymy, antonymy, and synonymy. We use five\nmetrics (two newly introduced here) for recently untreated aspects of semantic\nrelation knowledge, namely soundness, completeness, symmetry, prototypicality,\nand distinguishability. Using these, we can fairly compare humans and models on\nthe same task. Our extensive experiments involve six PLMs, four masked and two\ncausal language models. The results reveal a significant knowledge gap between\nhumans and models for all semantic relations. In general, causal language\nmodels, despite their wide use, do not always perform significantly better than\nmasked language models. Antonymy is the outlier relation where all models\nperform reasonably well.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.01131v4", "cate": "cs.CL", "date": "2024-12-02", "updated": "2025-07-25", "AI": {"title_translation": "预训练语言模型和人类语义关系知识的综合评估", "tldr": "本文对预训练语言模型（PLM）与人类在多种语义关系上的知识进行了全面评估，发现PLM与人类之间存在显著的知识差距，并且因果语言模型并不总是优于掩码语言模型，反义词是模型表现较好的例外。", "motivation": "现有研究对预训练语言模型（PLM）语义关系知识的探索不全面，通常只关注上位词关系，且缺乏PLM与人类在相同任务上的表现对比，导致对PLM语义关系知识程度的理解不完整。", "method": "为了解决现有空白，本文引入了一个综合评估框架，涵盖了上位词、下位词、整体-部分、部分-整体、反义词和同义词等五种超越上位词的关系。研究使用了五种度量标准（其中两种为新引入），用于评估语义关系知识的未处理方面，包括合理性、完整性、对称性、原型性和可区分性。通过这些标准，研究能够公平地比较人类和模型在相同任务上的表现。实验涉及六个PLM，包括四个掩码语言模型和两个因果语言模型。", "result": "实验结果显示，在所有语义关系上，人类和模型之间存在显著的知识差距。总的来说，尽管因果语言模型被广泛使用，但它们并不总是比掩码语言模型表现得更好。反义词是一种例外关系，所有模型在该关系上表现都相当不错。", "conclusion": "预训练语言模型在语义关系知识方面与人类存在显著差距，尤其是在上位词以外的关系上。未来的研究需要进一步提升模型的语义理解能力。", "translation": "最近，许多工作都关注预训练语言模型（PLM）究竟学习了语言的哪些方面以及它们是如何学习的这一谜团。这类研究的一个方向是调查PLM对语义关系的知识。然而，语义关系的许多方面尚未被探索。通常只考虑了一种关系，即上位词关系。此外，之前的工作没有测量人类在与PLM执行相同任务上的表现。这意味着目前对这些模型的语义关系知识程度只有不完整的了解。为了弥补这一空白，我们引入了一个全面的评估框架，涵盖了超越上位词的五种关系，即下位词、整体-部分、部分-整体、反义词和同义词。我们使用了五种度量标准（其中两种是新引入的），用于评估语义关系知识最近未被处理的方面，即合理性、完整性、对称性、原型性和可区分性。通过这些，我们可以公平地比较人类和模型在相同任务上的表现。我们的大量实验涉及六个PLM，包括四个掩码语言模型和两个因果语言模型。结果揭示了人类和模型在所有语义关系上存在显著的知识差距。总的来说，尽管因果语言模型被广泛使用，但它们并不总是比掩码语言模型表现得更好。反义词是例外关系，所有模型在该关系上表现都相当不错。", "summary": "本文针对预训练语言模型（PLM）语义关系知识评估的局限性，提出了一个全面的评估框架。该框架涵盖了上位词、下位词、整体-部分、部分-整体、反义词和同义词等五种语义关系，并引入了包括合理性、完整性、对称性、原型性和可区分性在内的五种评估指标。研究通过与人类在相同任务上的比较，评估了六种PLM（包括掩码和因果模型）的性能。结果表明，PLM在所有语义关系上与人类存在显著的知识差距，且因果语言模型并未普遍优于掩码语言模型，仅在反义词关系上表现较好。", "keywords": "预训练语言模型, 语义关系, 知识评估, 人机差距, 反义词", "comments": "本文的创新之处在于其构建了一个更全面的语义关系评估框架，首次将PLM与人类在多种语义关系上进行公平比较，并引入了新的评估指标。这对于理解PLM的语义理解能力边界及其发展方向具有重要意义，揭示了PLM在复杂语义知识方面仍有巨大提升空间。"}}
{"id": "2502.05209", "title": "Model Tampering Attacks Enable More Rigorous Evaluations of LLM Capabilities", "authors": ["Zora Che", "Stephen Casper", "Robert Kirk", "Anirudh Satheesh", "Stewart Slocum", "Lev E McKinney", "Rohit Gandikota", "Aidan Ewart", "Domenic Rosati", "Zichu Wu", "Zikui Cai", "Bilal Chughtai", "Yarin Gal", "Furong Huang", "Dylan Hadfield-Menell"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      Accepted to TMLR", "url": "http://arxiv.org/abs/2502.05209v4", "summary": "Evaluations of large language model (LLM) risks and capabilities are\nincreasingly being incorporated into AI risk management and governance\nframeworks. Currently, most risk evaluations are conducted by designing inputs\nthat elicit harmful behaviors from the system. However, this approach suffers\nfrom two limitations. First, input-output evaluations cannot fully evaluate\nrealistic risks from open-weight models. Second, the behaviors identified\nduring any particular input-output evaluation can only lower-bound the model's\nworst-possible-case input-output behavior. As a complementary method for\neliciting harmful behaviors, we propose evaluating LLMs with model tampering\nattacks which allow for modifications to latent activations or weights. We pit\nstate-of-the-art techniques for removing harmful LLM capabilities against a\nsuite of 5 input-space and 6 model tampering attacks. In addition to\nbenchmarking these methods against each other, we show that (1) model\nresilience to capability elicitation attacks lies on a low-dimensional\nrobustness subspace; (2) the success rate of model tampering attacks can\nempirically predict and offer conservative estimates for the success of\nheld-out input-space attacks; and (3) state-of-the-art unlearning methods can\neasily be undone within 16 steps of fine-tuning. Together, these results\nhighlight the difficulty of suppressing harmful LLM capabilities and show that\nmodel tampering attacks enable substantially more rigorous evaluations than\ninput-space attacks alone.", "comment": "Accepted to TMLR", "pdf_url": "http://arxiv.org/pdf/2502.05209v4", "cate": "cs.CR", "date": "2025-02-03", "updated": "2025-07-24", "AI": {"title_translation": "模型篡改攻击能够对大型语言模型（LLM）能力进行更严格的评估", "tldr": "本文提出模型篡改攻击作为一种更严格的LLM风险评估方法，克服了传统输入-输出评估的局限性，并揭示了抑制有害LLM能力的难度。", "motivation": "当前对大型语言模型（LLM）风险的评估主要依赖于设计输入来诱发有害行为，但这种方法存在局限性：无法充分评估开源模型的真实风险，且只能提供模型最坏行为的下限。因此，需要一种更全面的评估方法。", "method": "本文提出通过模型篡改攻击（允许修改LLM的潜在激活或权重）来评估LLM的有害能力，作为现有输入-输出评估的补充方法。研究将最先进的LLM有害能力移除技术与5种输入空间攻击和6种模型篡改攻击进行对抗性测试。", "result": "研究发现：(1) 模型对能力诱发攻击的弹性存在于一个低维鲁棒性子空间中；(2) 模型篡改攻击的成功率可以经验性地预测并保守估计未测试的输入空间攻击的成功率；(3) 最先进的遗忘方法在16步微调内即可轻易被撤销。", "conclusion": "这些结果共同强调了抑制LLM有害能力的困难性，并表明模型篡改攻击能够比单独的输入空间攻击提供更严格得多的评估。", "translation": "大型语言模型（LLM）风险和能力的评估正越来越多地被纳入人工智能风险管理和治理框架中。目前，大多数风险评估通过设计输入来诱导系统产生有害行为。然而，这种方法存在两个局限性。首先，输入-输出评估无法完全评估开源模型带来的实际风险。其次，在任何特定输入-输出评估中识别出的行为只能作为模型最坏情况输入-输出行为的下限。作为一种诱导有害行为的补充方法，我们提出使用模型篡改攻击来评估LLM，这种攻击允许修改潜在激活或权重。我们将最先进的移除有害LLM能力的技术与一套包含5种输入空间攻击和6种模型篡改攻击的方法进行对抗。除了相互基准测试这些方法外，我们还表明：(1) 模型对能力诱导攻击的弹性存在于一个低维鲁棒性子空间中；(2) 模型篡改攻击的成功率可以经验性地预测并保守估计未测试的输入空间攻击的成功率；(3) 最先进的遗忘方法可以在16步微调内轻易被撤销。总而言之，这些结果突出了抑制有害LLM能力的难度，并表明模型篡改攻击能够比单独的输入空间攻击实现更严格得多的评估。", "summary": "本文提出模型篡改攻击作为评估大型语言模型（LLM）有害能力的一种更严格、更全面的方法，以弥补传统输入-输出评估的不足。研究通过将先进的LLM能力移除技术与一系列输入空间和模型篡改攻击进行对抗性测试，发现模型篡改攻击能够有效预测输入空间攻击的成功率，并揭示了抑制LLM有害能力的固有难度，强调了其在LLM风险评估中的重要性。", "keywords": "模型篡改攻击, LLM评估, 安全性, 鲁棒性, 有害能力", "comments": "这项研究的创新之处在于引入了“模型篡改攻击”这一新颖的评估范式，它超越了传统的输入-输出测试，直接探究模型内部机制，为LLM的鲁棒性和安全性评估提供了更深层次的视角。其重要性体现在揭示了当前LLM“遗忘”或“去毒”方法的脆弱性，对LLM的风险管理和治理框架具有重大指导意义。该方法能够提供对LLM真实风险更保守的估计，对于理解和缓解开源LLM的潜在危害至关重要。"}}
{"id": "2410.06315", "title": "Incremental Learning for Robot Shared Autonomy", "authors": ["Yiran Tao", "Guixiu Qiao", "Dan Ding", "Zackory Erickson"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.06315v4", "summary": "Shared autonomy holds promise for improving the usability and accessibility\nof assistive robotic arms, but current methods often rely on costly expert\ndemonstrations and remain static after pretraining, limiting their ability to\nhandle real-world variations. Even with extensive training data, unforeseen\nchallenges--especially those that fundamentally alter task dynamics, such as\nunexpected obstacles or spatial constraints--can cause assistive policies to\nbreak down, leading to ineffective or unreliable assistance. To address this,\nwe propose ILSA, an Incrementally Learned Shared Autonomy framework that\ncontinuously refines its assistive policy through user interactions, adapting\nto real-world challenges beyond the scope of pre-collected data. At the core of\nILSA is a structured fine-tuning mechanism that enables continual improvement\nwith each interaction by effectively integrating limited new interaction data\nwhile preserving prior knowledge, ensuring a balance between adaptation and\ngeneralization. A user study with 20 participants demonstrates ILSA's\neffectiveness, showing faster task completion and improved user experience\ncompared to static alternatives. Code and videos are available at\nhttps://ilsa-robo.github.io/.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.06315v4", "cate": "cs.RO", "date": "2024-10-08", "updated": "2025-07-25", "AI": {"title_translation": "机器人共享自主的增量学习", "tldr": "提出ILSA框架，通过用户交互持续改进机器人共享自主策略，使其能适应预收集数据范围之外的现实挑战，提高任务完成速度和用户体验。", "motivation": "当前共享自主方法依赖昂贵的专家演示且预训练后保持静态，无法处理现实世界变化；当任务动态发生根本性改变（如意外障碍或空间限制）时，辅助策略会失效。", "method": "本文提出了ILSA（Incrementally Learned Shared Autonomy）框架，通过用户交互持续改进辅助策略。其核心是结构化微调机制，能够有效整合有限的新交互数据并保留先验知识，从而在适应性和泛化性之间取得平衡。", "result": "一项有20名参与者的用户研究表明，与静态替代方案相比，ILSA能实现更快的任务完成速度和更好的用户体验。", "conclusion": "ILSA通过增量学习有效解决了现有共享自主系统在面对现实世界变化时的局限性，显著提升了机器人辅助的可靠性和用户体验。", "translation": "共享自主有望提高辅助机械臂的可用性和可及性，但当前方法通常依赖昂贵的专家演示，并且在预训练后保持静态，限制了它们处理现实世界变化的能力。即使有大量的训练数据，不可预见的挑战——特别是那些从根本上改变任务动态的挑战，如意外障碍或空间限制——也可能导致辅助策略失效，从而导致无效或不可靠的帮助。为了解决这个问题，我们提出了ILSA，一个增量学习的共享自主框架，它通过用户交互持续完善其辅助策略，适应预收集数据范围之外的现实世界挑战。ILSA的核心是一个结构化微调机制，通过有效整合有限的新交互数据同时保留先验知识，在每次交互中实现持续改进，确保了适应性和泛化性之间的平衡。一项有20名参与者的用户研究表明了ILSA的有效性，与静态替代方案相比，它显示出更快的任务完成速度和改进的用户体验。代码和视频可在https://ilsa-robo.github.io/获得。", "summary": "本文提出了ILSA（增量学习共享自主）框架，旨在解决现有辅助机器人共享自主系统在面对现实世界变化时的局限性。ILSA通过用户交互持续优化其辅助策略，其核心的结构化微调机制能够有效整合新数据并保留先验知识，从而在适应性和泛化性之间取得平衡。用户研究结果表明，ILSA显著提高了任务完成速度和用户体验。", "keywords": "增量学习, 共享自主, 机器人, 用户交互, 适应性", "comments": "本文的创新点在于提出了一个增量学习的共享自主框架ILSA，通过用户交互持续改进策略，而非依赖静态预训练模型。这解决了现有方法无法适应现实世界动态变化的痛点，提高了辅助机器人的实用性和可靠性。其结构化微调机制在保留先验知识的同时整合新数据，是其核心贡献。"}}
{"id": "2503.13745", "title": "FedVSR: Towards Model-Agnostic Federated Learning in Video Super-Resolution", "authors": ["Ali Mollaahmadi Dehaghi", "Hossein KhademSohi", "Reza Razavi", "Steve Drew", "Mohammad Moshirpour"], "categories": ["cs.CV", "cs.DC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This version includes an updated abstract and introduction for improved clarity and context. We also added the LPIPS metric to our evaluation results to provide a more comprehensive assessment of perceptual quality", "url": "http://arxiv.org/abs/2503.13745v2", "summary": "Video super-resolution aims to enhance low-resolution videos by leveraging\nboth spatial and temporal information. While deep learning has led to\nimpressive progress, it typically requires centralized data, which raises\nprivacy concerns. Federated learning offers a privacy-friendly solution, but\ngeneral FL frameworks often struggle with low-level vision tasks, resulting in\nblurry, low-quality outputs. To address this, we introduce FedVSR, the first FL\nframework specifically designed for VSR. It is model-agnostic and stateless,\nand introduces a lightweight loss function based on the DWT to better preserve\nhigh-frequency details during local training. Additionally, a loss-aware\naggregation strategy combines both DWT-based and task-specific losses to guide\nglobal updates effectively. Extensive experiments across multiple VSR models\nand datasets demonstrate that FedVSR consistently outperforms existing FL\nmethods, achieving up to 0.82 dB higher PSNR, 0.0327 higher SSIM, and 0.0251\nlower LPIPS. These results underscore FedVSR's ability to bridge the gap\nbetween privacy and performance, setting a new benchmark for federated learning\nin low-level vision tasks. The code is available at:\nhttps://github.com/alimd94/FedVSR", "comment": "This version includes an updated abstract and introduction for\n  improved clarity and context. We also added the LPIPS metric to our\n  evaluation results to provide a more comprehensive assessment of perceptual\n  quality", "pdf_url": "http://arxiv.org/pdf/2503.13745v2", "cate": "cs.CV", "date": "2025-03-17", "updated": "2025-07-24", "AI": {"title_translation": "FedVSR: 迈向视频超分辨率中的模型无关联邦学习", "tldr": "FedVSR是首个专为视频超分辨率（VSR）设计的联邦学习（FL）框架，通过引入轻量级DWT损失和损失感知聚合策略，解决了传统FL在低级视觉任务中输出模糊的问题，并在隐私保护的前提下显著提升了VSR性能。", "motivation": "深度学习在视频超分辨率（VSR）中取得了显著进展，但通常需要集中式数据，这引发了隐私担忧。虽然联邦学习（FL）提供了隐私友好的解决方案，但通用FL框架在低级视觉任务（如VSR）中表现不佳，导致输出模糊、质量低下。", "method": "本文引入了FedVSR，这是首个专为VSR设计的联邦学习框架。它具有模型无关和无状态的特性，并引入了一种基于离散小波变换（DWT）的轻量级损失函数，以在本地训练期间更好地保留高频细节。此外，还采用了一种损失感知聚合策略，结合基于DWT的损失和任务特定损失来有效指导全局更新。", "result": "在多个VSR模型和数据集上的大量实验表明，FedVSR始终优于现有FL方法，PSNR最高提升0.82 dB，SSIM最高提升0.0327，LPIPS最高降低0.0251。这些结果突显了FedVSR在隐私和性能之间架起桥梁的能力。", "conclusion": "FedVSR成功弥合了隐私与性能之间的差距，为低级视觉任务中的联邦学习树立了新基准。", "translation": "视频超分辨率旨在通过利用空间和时间信息来增强低分辨率视频。虽然深度学习取得了令人印象深刻的进展，但它通常需要集中式数据，这引发了隐私问题。联邦学习提供了一种隐私友好的解决方案，但通用FL框架在低级视觉任务中往往表现不佳，导致输出模糊、质量低下。为了解决这个问题，我们引入了FedVSR，这是第一个专门为VSR设计的联邦学习框架。它是模型无关和无状态的，并引入了一种基于DWT的轻量级损失函数，以在本地训练期间更好地保留高频细节。此外，一种损失感知聚合策略结合了基于DWT的损失和任务特定损失，以有效指导全局更新。在多个VSR模型和数据集上的大量实验表明，FedVSR始终优于现有FL方法，PSNR最高提升0.82 dB，SSIM最高提升0.0327，LPIPS最高降低0.0251。这些结果强调了FedVSR弥合隐私和性能之间差距的能力，为低级视觉任务中的联邦学习树立了新基准。代码可在https://github.com/alimd94/FedVSR获取。", "summary": "FedVSR是首个专为视频超分辨率（VSR）设计的联邦学习（FL）框架，旨在解决传统FL在低级视觉任务中性能不佳且存在隐私问题的挑战。该框架是模型无关且无状态的，通过引入基于离散小波变换（DWT）的轻量级损失函数来保留高频细节，并采用损失感知聚合策略来优化全局更新。实验证明，FedVSR在多个VSR模型和数据集上显著优于现有FL方法，在PSNR、SSIM和LPIPS等指标上均有提升，成功平衡了隐私保护与性能优化。", "keywords": "联邦学习, 视频超分辨率, 模型无关, 隐私保护, DWT", "comments": "FedVSR是一项创新工作，首次将联邦学习应用于视频超分辨率领域，解决了VSR集中式数据带来的隐私问题。其创新点在于提出了模型无关、无状态的框架设计，并引入了专为VSR优化的DWT轻量级损失函数和损失感知聚合策略，有效提升了低级视觉任务中联邦学习的性能，为该领域树立了新基准。"}}
{"id": "2404.13223", "title": "Superfast direct inversion of the nonuniform discrete Fourier transform via hierarchically semi-separable least squares", "authors": ["Heather Wilber", "Ethan N. Epperly", "Alex H. Barnett"], "categories": ["math.NA", "cs.NA", "65T50, 65F55, 65F20"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      30 pages, 8 figures", "url": "http://arxiv.org/abs/2404.13223v3", "summary": "A direct solver is introduced for solving overdetermined linear systems\ninvolving nonuniform discrete Fourier transform matrices. Such matrices can be\ntransformed into a Cauchy-like form that has hierarchical low rank structure.\nThe rank structure of this matrix is explained, and it is shown that the ranks\nof the relevant submatrices grow only logarithmically with the number of\ncolumns of the matrix. A fast rank-structured hierarchical approximation method\nbased on this analysis is developed, along with a hierarchical least-squares\nsolver for these and related systems. This result is a direct method for\ninverting nonuniform discrete transforms with a complexity that is usually\nnearly linear with respect to the degrees of freedom in the problem.This solver\nis benchmarked against various iterative and direct solvers in the setting of\ninverting the one-dimensional type-II (or forward) transform, for a range of\ncondition numbers and problem sizes (up to 4 x 10^6 by 2 x 10^6). These\nexperiments demonstrate that this method is especially useful for large\nproblems with multiple right-hand sides.", "comment": "30 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2404.13223v3", "cate": "math.NA", "date": "2024-04-20", "updated": "2025-07-24", "AI": {"title_translation": "超快速非均匀离散傅里叶变换的直接逆解通过分层半可分离最小二乘法", "tldr": "开发了一种超快速直接求解器，用于通过分层半可分离最小二乘法对非均匀离散傅里叶变换矩阵进行逆运算，其复杂度接近线性。", "motivation": "解决涉及非均匀离散傅里叶变换矩阵的超定线性系统，并提供一种比现有迭代和直接求解器更高效的直接求解方法，尤其适用于大型问题。", "method": "将非均匀离散傅里叶变换矩阵转换为具有分层低秩结构的柯西型形式，解释其秩结构并证明相关子矩阵的秩仅随列数对数增长。基于此分析，开发了一种快速秩结构分层近似方法和分层最小二乘求解器。", "result": "引入了一种直接求解器，能够以通常接近问题自由度线性复杂度的速度逆解非均匀离散变换。实验表明，该方法对于具有多个右侧的大型问题特别有用，并在各种条件数和问题大小下进行了基准测试。", "conclusion": "该研究成功开发了一种超快速的直接求解器，用于逆解非均匀离萨傅里叶变换，其效率在处理大型问题时尤为突出，为相关领域的计算提供了显著改进。", "translation": "引入了一种直接求解器，用于求解涉及非均匀离散傅里叶变换矩阵的超定线性系统。这些矩阵可以转换为具有分层低秩结构的柯西型形式。文中解释了该矩阵的秩结构，并表明相关子矩阵的秩仅随矩阵列数的对数增长。基于此分析，开发了一种快速的秩结构分层近似方法，以及用于这些和相关系统的分层最小二乘求解器。这项成果是一种直接方法，用于逆解非均匀离散变换，其复杂度通常与问题自由度呈近似线性关系。该求解器在逆解一维II型（或正向）变换的背景下，针对一系列条件数和问题大小（高达4 x 10^6 x 2 x 10^6）与各种迭代和直接求解器进行了基准测试。这些实验表明，该方法对于具有多个右侧的大型问题特别有用。", "summary": "本文介绍了一种用于求解涉及非均匀离散傅里叶变换矩阵的超定线性系统的直接求解器。通过将这些矩阵转换为具有分层低秩结构的柯西型形式，并利用其秩仅对数增长的特性，作者开发了一种快速的秩结构分层近似方法和分层最小二乘求解器。该方法能够以接近线性的复杂度逆解非均匀离散变换，并被证明在处理具有多个右侧的大型问题时非常高效。", "keywords": "非均匀离散傅里叶变换, 直接求解器, 分层半可分离, 最小二乘, 秩结构", "comments": "这篇论文的创新点在于将非均匀离散傅里叶变换矩阵转换为具有特定秩结构的柯西型形式，并利用这种结构开发了高效的分层最小二乘求解器。其重要性在于提供了一种超快速的直接方法，显著降低了大型非均匀离里叶变换问题的计算复杂度，尤其适用于需要处理多个右侧向量的场景，这在信号处理、图像重建等领域具有广泛应用前景。"}}
{"id": "2405.14475", "title": "MagicDrive3D: Controllable 3D Generation for Any-View Rendering in Street Scenes", "authors": ["Ruiyuan Gao", "Kai Chen", "Zhihao Li", "Lanqing Hong", "Zhenguo Li", "Qiang Xu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2405.14475v4", "summary": "Controllable generative models for images and videos have seen significant\nsuccess, yet 3D scene generation, especially in unbounded scenarios like\nautonomous driving, remains underdeveloped. Existing methods lack flexible\ncontrollability and often rely on dense view data collection in controlled\nenvironments, limiting their generalizability across common datasets (e.g.,\nnuScenes). In this paper, we introduce MagicDrive3D, a novel framework for\ncontrollable 3D street scene generation that combines video-based view\nsynthesis with 3D representation (3DGS) generation. It supports multi-condition\ncontrol, including road maps, 3D objects, and text descriptions. Unlike\nprevious approaches that require 3D representation before training,\nMagicDrive3D first trains a multi-view video generation model to synthesize\ndiverse street views. This method utilizes routinely collected autonomous\ndriving data, reducing data acquisition challenges and enriching 3D scene\ngeneration. In the 3DGS generation step, we introduce Fault-Tolerant Gaussian\nSplatting to address minor errors and use monocular depth for better\ninitialization, alongside appearance modeling to manage exposure discrepancies\nacross viewpoints. Experiments show that MagicDrive3D generates diverse,\nhigh-quality 3D driving scenes, supports any-view rendering, and enhances\ndownstream tasks like BEV segmentation, demonstrating its potential for\nautonomous driving simulation and beyond.", "comment": "Project Page: https://flymin.github.io/magicdrive3d", "pdf_url": "http://arxiv.org/pdf/2405.14475v4", "cate": "cs.CV", "date": "2024-05-23", "updated": "2025-07-25", "AI": {"title_translation": "MagicDrive3D：街景中任意视角渲染的可控3D生成", "tldr": "MagicDrive3D提出了一种新的框架，通过结合视频视图合成和3DGS生成，实现可控的3D街景生成，能够生成多样化、高质量的场景并支持任意视角渲染，对自动驾驶模拟具有重要潜力。", "motivation": "当前3D场景生成，尤其在自动驾驶等无界场景中，仍不成熟。现有方法缺乏灵活的可控性，并常依赖于受控环境中密集视角数据采集，限制了其在常见数据集（如nuScenes）上的泛化能力。", "method": "本文引入了MagicDrive3D，一个用于可控3D街景生成的新颖框架。它结合了基于视频的视图合成和3D表示（3DGS）生成。该框架支持多条件控制，包括道路地图、3D对象和文本描述。与之前需要先进行3D表示训练的方法不同，MagicDrive3D首先训练一个多视图视频生成模型来合成多样化的街景视图，利用日常收集的自动驾驶数据，从而减少数据获取挑战并丰富3D场景生成。在3DGS生成步骤中，引入了容错高斯泼溅（Fault-Tolerant Gaussian Splatting）来处理微小误差，并使用单目深度进行更好的初始化，同时通过外观建模来管理不同视点间的曝光差异。", "result": "实验表明，MagicDrive3D能够生成多样化、高质量的3D驾驶场景，支持任意视角渲染，并增强了下游任务，如BEV分割。", "conclusion": "MagicDrive3D展示了其在自动驾驶模拟及其他领域的潜力。", "translation": "可控的图像和视频生成模型取得了显著成功，然而3D场景生成，尤其在自动驾驶等无界场景中，仍不成熟。现有方法缺乏灵活的可控性，并且通常依赖于在受控环境中收集密集视图数据，这限制了它们在常见数据集（例如nuScenes）上的泛化能力。在本文中，我们引入了MagicDrive3D，一个用于可控3D街景生成的新颖框架，它结合了基于视频的视图合成与3D表示（3DGS）生成。它支持多条件控制，包括道路地图、3D对象和文本描述。与之前需要在训练前进行3D表示的方法不同，MagicDrive3D首先训练一个多视图视频生成模型来合成多样化的街景视图。这种方法利用了日常收集的自动驾驶数据，减少了数据获取的挑战，并丰富了3D场景生成。在3DGS生成步骤中，我们引入了容错高斯泼溅（Fault-Tolerant Gaussian Splatting）来处理微小误差，并使用单目深度进行更好的初始化，同时通过外观建模来管理不同视点间的曝光差异。实验表明，MagicDrive3D能够生成多样化、高质量的3D驾驶场景，支持任意视角渲染，并增强了下游任务，如BEV分割，展示了其在自动驾驶模拟及其他领域的潜力。", "summary": "MagicDrive3D是一个创新的框架，旨在解决自动驾驶等无界场景中3D街景生成的可控性和数据依赖问题。它通过结合视频视图合成和3D高斯泼溅（3DGS）生成，实现了多条件控制下的多样化、高质量3D场景生成。该方法首先训练多视图视频生成模型，利用日常自动驾驶数据，并引入容错高斯泼溅和单目深度初始化来优化3DGS生成。实验证明，MagicDrive3D能生成支持任意视角渲染的3D驾驶场景，并提升下游任务性能，展现了其在自动驾驶模拟中的巨大潜力。", "keywords": "可控3D生成, 街景, 任意视角渲染, 自动驾驶, 高斯泼溅", "comments": "该论文的创新点在于将视频视图合成与3DGS生成相结合，特别是其“先训练多视图视频生成模型”的策略，有效利用了日常收集的自动驾驶数据，降低了数据获取难度。引入容错高斯泼溅和单目深度初始化也提升了生成质量。该框架为自动驾驶模拟提供了高质量、可控的3D场景，具有重要的应用价值和广阔前景。"}}
{"id": "2507.19151", "title": "ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination", "authors": ["Michael Amir", "Guang Yang", "Zhan Gao", "Keisuke Okumura", "Heedo Woo", "Amanda Prorok"], "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "I.2.9"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19151v1", "summary": "Constraint-based optimization is a cornerstone of robotics, enabling the\ndesign of controllers that reliably encode task and safety requirements such as\ncollision avoidance or formation adherence. However, handcrafted constraints\ncan fail in multi-agent settings that demand complex coordination. We introduce\nReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid\nframework that merges the reliability of optimization-based controllers with\nthe adaptability of multi-agent reinforcement learning. Rather than discarding\nexpert controllers, ReCoDe improves them by learning additional, dynamic\nconstraints that capture subtler behaviors, for example, by constraining agent\nmovements to prevent congestion in cluttered scenarios. Through local\ncommunication, agents collectively constrain their allowed actions to\ncoordinate more effectively under changing conditions. In this work, we focus\non applications of ReCoDe to multi-agent navigation tasks requiring intricate,\ncontext-based movements and consensus, where we show that it outperforms purely\nhandcrafted controllers, other hybrid approaches, and standard MARL baselines.\nWe give empirical (real robot) and theoretical evidence that retaining a\nuser-defined controller, even when it is imperfect, is more efficient than\nlearning from scratch, especially because ReCoDe can dynamically change the\ndegree to which it relies on this controller.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19151v1", "cate": "cs.RO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "ReCoDe：基于强化学习的多智能体协同动态约束设计", "tldr": "ReCoDe是一个混合框架，通过强化学习为多智能体系统学习动态约束，以增强现有控制器，并在多智能体导航任务中表现优异。", "motivation": "传统的基于约束的优化方法在需要复杂协调的多智能体环境中表现不佳，因为手工设计的约束可能失效。", "method": "ReCoDe是一个去中心化的混合框架，它结合了基于优化的控制器的可靠性和多智能体强化学习的适应性。它不是丢弃专家控制器，而是通过学习额外的动态约束来改进它们，例如限制智能体移动以防止拥堵。智能体通过局部通信共同约束其允许的动作以在变化条件下更有效地协调。", "result": "在需要复杂、基于上下文的移动和共识的多智能体导航任务中，ReCoDe的表现优于纯手工控制器、其他混合方法和标准多智能体强化学习基线。研究提供了经验（真实机器人）和理论证据表明，保留用户定义的控制器（即使不完美）比从头开始学习更有效，因为ReCoDe可以动态改变其对该控制器的依赖程度。", "conclusion": "ReCoDe证明了在多智能体协调任务中，通过强化学习动态增强现有优化控制器比从头学习更高效且性能更优越。", "translation": "基于约束的优化是机器人技术的基础，它能够设计可靠地编码任务和安全要求（如避碰或编队遵守）的控制器。然而，在需要复杂协调的多智能体环境中，手工设计的约束可能会失效。我们引入了ReCoDe——基于强化学习的约束设计——一个去中心化的混合框架，它融合了基于优化的控制器的可靠性与多智能体强化学习的适应性。ReCoDe并非抛弃专家控制器，而是通过学习额外的动态约束来改进它们，这些约束能够捕捉更细微的行为，例如通过限制智能体移动来防止拥挤场景中的堵塞。通过局部通信，智能体共同约束其允许的动作，以在变化条件下更有效地协调。在这项工作中，我们专注于将ReCoDe应用于需要复杂、基于上下文的移动和共识的多智能体导航任务，并展示了它优于纯手工控制器、其他混合方法和标准多智能体强化学习基线。我们提供了经验（真实机器人）和理论证据，表明保留用户定义的控制器，即使它不完美，也比从头开始学习更有效，特别是因为ReCoDe可以动态改变其对该控制器的依赖程度。", "summary": "ReCoDe是一个新颖的去中心化混合框架，旨在解决多智能体协调中手工约束的局限性。它通过强化学习动态生成额外约束，以增强现有的优化控制器，而非完全取代。该方法允许智能体通过局部通信协同，有效避免拥堵等复杂行为。实验结果表明，ReCoDe在多智能体导航任务中显著优于传统手工控制器、其他混合方法和纯MARL基线，并证明了在不完美控制器基础上进行增量学习的效率优势。", "keywords": "多智能体协调, 强化学习, 动态约束, 混合控制, 机器人导航", "comments": "ReCoDe的创新之处在于其混合框架，它没有完全抛弃现有可靠的优化控制器，而是通过强化学习动态地补充和优化它们，这在复杂多变的多智能体环境中提供了更高的适应性和鲁棒性。这种渐进式学习策略比从零开始学习更高效，尤其是在真实世界应用中，现有控制器通常已经具备一定的基础能力。该方法对于需要高可靠性和灵活性的机器人协调任务具有重要意义。"}}
{"id": "2507.18992", "title": "Reinforcement Learning via Conservative Agent for Environments with Random Delays", "authors": ["Jongsoo Lee", "Jangwon Kim", "Jiseok Jeong", "Soohee Han"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18992v1", "summary": "Real-world reinforcement learning applications are often hindered by delayed\nfeedback from environments, which violates the Markov assumption and introduces\nsignificant challenges. Although numerous delay-compensating methods have been\nproposed for environments with constant delays, environments with random delays\nremain largely unexplored due to their inherent variability and\nunpredictability. In this study, we propose a simple yet robust agent for\ndecision-making under random delays, termed the conservative agent, which\nreformulates the random-delay environment into its constant-delay equivalent.\nThis transformation enables any state-of-the-art constant-delay method to be\ndirectly extended to the random-delay environments without modifying the\nalgorithmic structure or sacrificing performance. We evaluate the conservative\nagent-based algorithm on continuous control tasks, and empirical results\ndemonstrate that it significantly outperforms existing baseline algorithms in\nterms of asymptotic performance and sample efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18992v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "具有随机延迟环境下的保守智能体强化学习", "tldr": "提出了一种保守智能体，能将随机延迟环境转化为恒定延迟环境，从而使现有方法适用于随机延迟强化学习，并在性能上超越现有基线。", "motivation": "现实世界强化学习应用常受到环境延迟反馈的阻碍，特别是随机延迟，这违反了马尔可夫假设并带来了重大挑战。尽管有针对恒定延迟的方法，但随机延迟环境因其变异性和不可预测性而未被充分探索。", "method": "提出了一种名为“保守智能体”的方法，它将随机延迟环境重新表述为其恒定延迟等效环境。这种转换使得任何先进的恒定延迟方法可以直接扩展到随机延迟环境，而无需修改算法结构或牺牲性能。", "result": "在连续控制任务上评估了基于保守智能体的算法，经验结果表明它在渐近性能和样本效率方面显著优于现有基线算法。", "conclusion": "保守智能体能够有效解决随机延迟环境下的强化学习问题，并显著提升性能和样本效率。", "translation": "现实世界的强化学习应用常常受到环境延迟反馈的阻碍，这违反了马尔可夫假设并带来了重大挑战。尽管已经提出了许多针对恒定延迟环境的延迟补偿方法，但由于其固有的可变性和不可预测性，随机延迟环境在很大程度上仍未被探索。在这项研究中，我们提出了一种简单而鲁棒的用于随机延迟下决策的智能体，称之为保守智能体，它将随机延迟环境重新表述为其恒定延迟等效环境。这种转换使得任何最先进的恒定延迟方法可以直接扩展到随机延迟环境，而无需修改算法结构或牺牲性能。我们在连续控制任务上评估了基于保守智能体的算法，经验结果表明它在渐近性能和样本效率方面显著优于现有基线算法。", "summary": "本文提出了一种“保守智能体”方法，用于解决现实世界强化学习中随机延迟反馈的问题。该方法通过将随机延迟环境转化为恒定延迟等效环境，使得现有的恒定延迟强化学习算法可以直接应用于随机延迟场景，无需修改或性能损失。实验结果表明，该方法在连续控制任务上显著优于现有基线算法，提升了渐近性能和样本效率。", "keywords": "强化学习, 随机延迟, 保守智能体, 延迟补偿, 连续控制", "comments": "该研究提出了一种新颖且实用的方法，通过环境重构将随机延迟问题转化为已充分研究的恒定延迟问题，极大地扩展了现有强化学习算法的适用范围，具有重要的理论和实践意义。"}}
{"id": "2507.19388", "title": "A novel multi-thickness topology optimization method for balancing structural performance and manufacturability", "authors": ["Gabriel Stankiewicz", "Chaitanya Dev", "Paul Steinmann"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19388v1", "summary": "Topology optimization (TO) in two dimensions often presents a trade-off\nbetween structural performance and manufacturability, with unpenalized\n(variable-thickness) methods yielding superior but complex designs, and\npenalized (SIMP) methods producing simpler, truss-like structures with\ncompromised performance. This paper introduces a multi-thickness, density-based\ntopology optimization method designed to bridge this gap. The proposed approach\nguides the design towards a predefined set of discrete, allowable thicknesses\nby employing a novel multilevel penalization scheme and a multilevel smoothed\nHeaviside projection. A continuation strategy for the penalization and\nprojection parameters, combined with an adaptive mesh refinement technique,\nensures robust convergence and high-resolution geometric features. The method\nis validated on standard cantilever and MBB beam benchmarks. Results\ndemonstrate that as the number of allowable thicknesses increases, the designs\nsystematically transition from conventional truss-like structures to\nhigh-performance, sheet-like structures. Notably, designs with as few as three\ndiscrete thickness levels achieve compliance values within 2\\% of those from\nfully unpenalized, variable-thickness optimization, while significantly\noutperforming standard SIMP results. The method inherently eliminates\nimpractically thin regions and features, both in the out-of-plane and in-plane\ndirections and produces designs well-suited for both additive manufacturing and\nconventional fabrication using standard-thickness stock materials, thus\nmaximizing both performance and manufacturability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19388v1", "cate": "cs.CE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "一种平衡结构性能和可制造性的新型多厚度拓扑优化方法", "tldr": "本文提出了一种多厚度拓扑优化方法，通过离散厚度选择，在保持高结构性能的同时显著提高可制造性，弥补了传统拓扑优化方法在性能与可制造性之间的差距。", "motivation": "二维拓扑优化（TO）在结构性能和可制造性之间存在权衡：非惩罚（变厚度）方法产生性能优越但复杂的设计，而惩罚（SIMP）方法产生更简单、桁架状但性能受损的结构。本文旨在弥合这一差距。", "method": "本文提出了一种基于密度的多厚度拓扑优化方法。该方法采用新颖的多级惩罚方案和多级平滑Heaviside投影，引导设计趋向预定义的离散允许厚度集合。结合惩罚和投影参数的连续策略以及自适应网格细化技术，确保了鲁棒收敛和高分辨率几何特征。该方法在标准悬臂梁和MBB梁基准上进行了验证。", "result": "结果表明，随着允许厚度数量的增加，设计系统地从传统的桁架状结构过渡到高性能的板状结构。值得注意的是，仅用三个离散厚度水平的设计，其柔度值就能达到完全非惩罚、变厚度优化结果的2%以内，同时显著优于标准SIMP结果。该方法从根本上消除了面外和面内方向上不切实际的薄区域和特征。", "conclusion": "该方法生成的设计既适用于增材制造，也适用于使用标准厚度原材料的传统制造，从而最大限度地提高了性能和可制造性。", "translation": "二维拓扑优化（TO）通常在结构性能和可制造性之间存在权衡，其中非惩罚（变厚度）方法产生性能优越但复杂的设计，而惩罚（SIMP）方法产生更简单、桁架状但性能受损的结构。本文介绍了一种基于密度的多厚度拓扑优化方法，旨在弥合这一差距。所提出的方法通过采用新颖的多级惩罚方案和多级平滑Heaviside投影，将设计引导至一组预定义的离散、允许的厚度。惩罚和投影参数的连续策略，结合自适应网格细化技术，确保了鲁棒收敛和高分辨率几何特征。该方法在标准悬臂梁和MBB梁基准上进行了验证。结果表明，随着允许厚度数量的增加，设计系统地从传统的桁架状结构过渡到高性能的板状结构。值得注意的是，仅用三个离散厚度水平的设计，其柔度值就能达到完全非惩罚、变厚度优化结果的2%以内，同时显著优于标准SIMP结果。该方法从根本上消除了面外和面内方向上不切实际的薄区域和特征，并生成了非常适合增材制造和使用标准厚度原材料的传统制造的设计，从而最大限度地提高了性能和可制造性。", "summary": "本文提出了一种新型多厚度、基于密度的拓扑优化方法，旨在解决传统二维拓扑优化中结构性能与可制造性之间的矛盾。通过引入多级惩罚方案和Heaviside投影，该方法能将设计引导至预设的离散厚度，并结合连续策略和自适应网格细化以确保收敛和高分辨率。实验结果表明，该方法在仅使用少量离散厚度时即可达到接近变厚度优化的性能，同时生成适合增材制造和传统制造的、无不切实际薄区域的设计，从而有效平衡了性能和可制造性。", "keywords": "拓扑优化, 多厚度, 可制造性, 结构性能, 离散厚度", "comments": "该论文提出了一种创新的多厚度拓扑优化方法，有效弥补了传统方法在追求高性能和实现可制造性之间的鸿沟。其核心创新在于引入多级惩罚和投影机制，将连续的优化问题转化为离散厚度选择，这对于实际工程应用具有重要意义。通过消除不切实际的薄区域，并同时支持增材制造和传统制造，该方法极大地提升了拓扑优化设计的实用性。"}}
{"id": "2507.18910", "title": "A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions", "authors": ["Agada Joseph Oche", "Ademola Glory Folashade", "Tirthankar Ghosal", "Arpan Biswas"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      33 pages, 2 figures", "url": "http://arxiv.org/abs/2507.18910v1", "summary": "Retrieval-Augmented Generation (RAG) represents a major advancement in\nnatural language processing (NLP), combining large language models (LLMs) with\ninformation retrieval systems to enhance factual grounding, accuracy, and\ncontextual relevance. This paper presents a comprehensive systematic review of\nRAG, tracing its evolution from early developments in open domain question\nanswering to recent state-of-the-art implementations across diverse\napplications. The review begins by outlining the motivations behind RAG,\nparticularly its ability to mitigate hallucinations and outdated knowledge in\nparametric models. Core technical components-retrieval mechanisms,\nsequence-to-sequence generation models, and fusion strategies are examined in\ndetail. A year-by-year analysis highlights key milestones and research trends,\nproviding insight into RAG's rapid growth. The paper further explores the\ndeployment of RAG in enterprise systems, addressing practical challenges\nrelated to retrieval of proprietary data, security, and scalability. A\ncomparative evaluation of RAG implementations is conducted, benchmarking\nperformance on retrieval accuracy, generation fluency, latency, and\ncomputational efficiency. Persistent challenges such as retrieval quality,\nprivacy concerns, and integration overhead are critically assessed. Finally,\nthe review highlights emerging solutions, including hybrid retrieval\napproaches, privacy-preserving techniques, optimized fusion strategies, and\nagentic RAG architectures. These innovations point toward a future of more\nreliable, efficient, and context-aware knowledge-intensive NLP systems.", "comment": "33 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.18910v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "检索增强生成（RAG）系统综述：进展、差距与未来方向", "tldr": "本文对检索增强生成（RAG）系统进行了全面的系统性综述，探讨了其进展、现有问题和未来发展方向。", "motivation": "RAG旨在结合大型语言模型和信息检索系统，以增强事实基础、准确性和上下文相关性，并缓解参数模型中幻觉和知识过时的问题。", "method": "本文对RAG进行了全面的系统性综述，追溯了其从早期发展到最新应用中的演变。详细考察了核心技术组件（检索机制、序列到序列生成模型、融合策略），并进行了逐年分析以突出里程碑和研究趋势。此外，还探讨了RAG在企业系统中的部署挑战，并对RAG实现进行了比较评估，基准测试了检索准确性、生成流畅性、延迟和计算效率。", "result": "RAG取得了快速发展，但仍面临检索质量、隐私问题和集成开销等挑战。新兴解决方案包括混合检索方法、隐私保护技术、优化的融合策略和代理式RAG架构。", "conclusion": "RAG的创新指明了未来更可靠、高效和上下文感知的知识密集型自然语言处理系统的发展方向。", "translation": "检索增强生成（RAG）代表了自然语言处理（NLP）领域的一项重大进展，它将大型语言模型（LLM）与信息检索系统相结合，以增强事实基础、准确性和上下文相关性。本文对RAG进行了全面的系统性综述，追溯了其从开放域问答的早期发展到跨多样化应用的最新先进实现。该综述首先概述了RAG背后的动机，特别是其减轻参数模型中幻觉和过时知识的能力。详细审查了核心技术组件——检索机制、序列到序列生成模型和融合策略。逐年分析突出了关键里程碑和研究趋势，提供了对RAG快速增长的洞察。本文进一步探讨了RAG在企业系统中的部署，解决了与专有数据检索、安全性和可扩展性相关的实际挑战。对RAG的实现进行了比较评估，基准测试了检索准确性、生成流畅性、延迟和计算效率。对检索质量、隐私问题和集成开销等持续存在的挑战进行了批判性评估。最后，该综述强调了新兴解决方案，包括混合检索方法、隐私保护技术、优化的融合策略和代理式RAG架构。这些创新指向了未来更可靠、高效和上下文感知的知识密集型NLP系统。", "summary": "本文对检索增强生成（RAG）系统进行了全面的系统性综述。RAG通过结合大型语言模型和信息检索，旨在解决LLM的幻觉和知识过时问题。综述详细分析了RAG的核心技术组件、发展里程碑、在企业部署中的挑战以及不同实现的性能评估。文章还批判性地评估了现有挑战，并提出了混合检索、隐私保护、优化融合和代理式RAG等未来解决方案，预示了更可靠、高效的知识密集型NLP系统。", "keywords": "检索增强生成, RAG, 大型语言模型, 信息检索, 自然语言处理", "comments": "这篇综述论文通过系统性地梳理RAG的进展、挑战和未来方向，为研究人员和实践者提供了宝贵的概览。其创新之处在于全面覆盖了技术组件、企业应用、性能评估以及新兴解决方案，对于理解RAG领域的现状和发展趋势具有重要意义。"}}
{"id": "2507.19415", "title": "Sample Abundance for Signal Processing: A Brief Introduction", "authors": ["Arian Eamaz", "Farhang Yeganegi", "Mojtaba Soltanalian"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      arXiv admin note: substantial text overlap with arXiv:2308.00695", "url": "http://arxiv.org/abs/2507.19415v1", "summary": "This paper reports, by way of introduction, on the advances made by our group\nand the broader signal processing community on the concept of sample abundance;\na phenomenon that naturally arises in one-bit and few-bit signal processing\nframeworks. By leveraging large volumes of low-precision measurements, we show\nhow traditionally costly constraints, such as matrix semi-definiteness and rank\nconditions, become redundant, yielding simple overdetermined linear feasibility\nproblems. We illustrate key algorithms, theoretical guarantees via the Finite\nVolume Property, and the sample abundance singularity phenomenon, where\ncomputational complexity sharply drops.", "comment": "arXiv admin note: substantial text overlap with arXiv:2308.00695", "pdf_url": "http://arxiv.org/pdf/2507.19415v1", "cate": "cs.IT", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "信号处理中的样本丰度：简要介绍", "tldr": "本文介绍了样本丰度在低精度信号处理中的应用，展示了如何利用大量低精度测量简化复杂约束，从而将问题转化为简单的线性可行性问题，并探讨了相关算法和理论保证。", "motivation": "本文旨在介绍样本丰度这一概念及其在信号处理领域（尤其是一比特和少比特框架下）的最新进展，展示如何利用大量低精度测量来简化传统上计算成本高昂的约束条件。", "method": "通过利用大量低精度测量，本文展示了如何使矩阵半正定性和秩条件等传统上成本高昂的约束变得冗余，从而得到简单的超定线性可行性问题。文中还阐述了关键算法、通过有限体积性质获得的理论保证以及样本丰度奇异现象。", "result": "结果表明，传统上成本高昂的约束（如矩阵半正定性和秩条件）在样本丰度现象下变得冗余，从而产生了简单的超定线性可行性问题，并且计算复杂性急剧下降。", "conclusion": "本文介绍并阐述了样本丰度在信号处理中的概念和进展，展示了其如何通过利用大量低精度测量来简化复杂的信号处理问题，从而降低计算复杂性。", "translation": "本文旨在介绍本课题组及更广泛的信号处理社区在样本丰度概念上取得的进展；这是一种在一比特和少比特信号处理框架中自然产生的现象。通过利用大量低精度测量，我们展示了传统上成本高昂的约束，如矩阵半正定性和秩条件，如何变得冗余，从而产生简单的超定线性可行性问题。我们阐述了关键算法、通过有限体积性质获得的理论保证，以及样本丰度奇异现象，在该现象中计算复杂性急剧下降。", "summary": "本文介绍了信号处理中的“样本丰度”概念，该概念在低精度测量框架中自然出现。研究表明，通过利用大量低精度数据，可以简化传统上复杂的约束（如矩阵半正定性和秩条件），将其转化为简单的超定线性可行性问题。文中还探讨了相关算法、基于有限体积性质的理论保障以及计算复杂度显著降低的样本丰度奇异现象。", "keywords": "样本丰度, 信号处理, 低精度测量, 线性可行性, 有限体积性质", "comments": "本文创新性地提出了利用“样本丰度”来简化信号处理中复杂约束的方法，将高成本的非线性问题转化为简单的线性可行性问题，这对于处理低精度数据和大规模数据集具有重要意义。特别是“样本丰度奇异现象”的发现，预示着计算效率的巨大提升潜力。"}}
{"id": "2507.18795", "title": "Simulation-Driven Reinforcement Learning in Queuing Network Routing Optimization", "authors": ["Fatima Al-Ani", "Molly Wang", "Jevon Charles", "Aaron Ong", "Joshua Forday", "Vinayak Modi"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18795v1", "summary": "This study focuses on the development of a simulation-driven reinforcement\nlearning (RL) framework for optimizing routing decisions in complex queueing\nnetwork systems, with a particular emphasis on manufacturing and communication\napplications. Recognizing the limitations of traditional queueing methods,\nwhich often struggle with dynamic, uncertain environments, we propose a robust\nRL approach leveraging Deep Deterministic Policy Gradient (DDPG) combined with\nDyna-style planning (Dyna-DDPG). The framework includes a flexible and\nconfigurable simulation environment capable of modeling diverse queueing\nscenarios, disruptions, and unpredictable conditions. Our enhanced Dyna-DDPG\nimplementation incorporates separate predictive models for next-state\ntransitions and rewards, significantly improving stability and sample\nefficiency. Comprehensive experiments and rigorous evaluations demonstrate the\nframework's capability to rapidly learn effective routing policies that\nmaintain robust performance under disruptions and scale effectively to larger\nnetwork sizes. Additionally, we highlight strong software engineering practices\nemployed to ensure reproducibility and maintainability of the framework,\nenabling practical deployment in real-world scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18795v1", "cate": "cs.AI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "排队网络路由优化中的仿真驱动强化学习", "tldr": "本研究提出了一种仿真驱动的强化学习框架（Dyna-DDPG）用于优化复杂排队网络系统中的路由决策，解决了传统方法在动态不确定环境中的局限性，并展示了其在中断下的鲁棒性和可扩展性。", "motivation": "传统排队方法在处理动态、不确定的环境时存在局限性，难以优化复杂排队网络系统中的路由决策，尤其是在制造和通信应用中。因此，需要一个更鲁棒的解决方案。", "method": "本研究提出了一个仿真驱动的强化学习（RL）框架，利用深度确定性策略梯度（DDPG）结合Dyna-风格规划（Dyna-DDPG）。该框架包含一个灵活可配置的仿真环境，能够模拟多种排队场景、中断和不可预测条件。增强的Dyna-DDPG实现引入了独立的下一状态转换和奖励预测模型，以提高稳定性和样本效率。", "result": "实验和评估表明，该框架能够快速学习有效的路由策略，在中断下保持鲁棒性能，并能有效扩展到更大的网络规模。此外，该框架采用强大的软件工程实践，确保了可复现性和可维护性。", "conclusion": "该仿真驱动的强化学习框架（Dyna-DDPG）能够有效优化复杂排队网络中的路由决策，并在动态不确定环境下展现出优异的鲁棒性和可扩展性，使其适用于实际部署。", "translation": "本研究致力于开发一种仿真驱动的强化学习（RL）框架，用于优化复杂排队网络系统中的路由决策，特别强调其在制造和通信应用中的潜力。鉴于传统排队方法在处理动态、不确定环境时常遇到的局限性，我们提出了一种鲁棒的RL方法，该方法利用深度确定性策略梯度（DDPG）并结合了Dyna-风格规划（Dyna-DDPG）。该框架包含一个灵活且可配置的仿真环境，能够模拟多样化的排队场景、中断和不可预测条件。我们增强的Dyna-DDPG实现整合了独立的下一状态转换和奖励预测模型，显著提高了稳定性和样本效率。全面的实验和严格的评估证明了该框架能够快速学习有效的路由策略，在中断下保持鲁棒性能，并能有效扩展到更大的网络规模。此外，我们强调了所采用的强大软件工程实践，以确保框架的可复现性和可维护性，从而实现在现实世界场景中的实际部署。", "summary": "本研究提出并验证了一种基于仿真驱动的强化学习框架（Dyna-DDPG），旨在解决复杂排队网络中路由优化问题，尤其适用于制造和通信领域。该框架通过结合DDPG和Dyna-风格规划，并在仿真环境中模拟动态不确定性，克服了传统方法的局限性。其增强的Dyna-DDPG版本通过独立的预测模型提升了稳定性和样本效率。实验结果表明，该框架能有效学习鲁棒的路由策略，并在面临中断和网络规模扩大时表现出色，同时具备良好的可复现性和可维护性，支持实际部署。", "keywords": "强化学习, 排队网络, 路由优化, 仿真驱动, Dyna-DDPG", "comments": "该论文创新性地将仿真驱动的强化学习与Dyna-DDPG结合，解决了复杂排队网络在动态不确定环境下的路由优化难题。其引入独立的预测模型以提高稳定性和样本效率是一个亮点。此外，强调软件工程实践对于框架的实际部署和可复现性至关重要，提升了研究的实用价值。"}}
{"id": "2507.19094", "title": "Environmental (in)considerations in the Design of Smartphone Settings", "authors": ["Thomas Thibault", "Léa Mosesso", "Camille Adam", "Aurélien Tabard", "Anaëlle Beignon", "Nolwenn Maudet"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Post-proceedings paper presented at LIMITS 2025: 11th Workshop on Computing within Limits, 2025-06-26/27, Online", "url": "http://arxiv.org/abs/2507.19094v1", "summary": "Designing for sufficiency is one of many approaches that could foster more\nmoderate and sustainable digital practices. Based on the Sustainable\nInformation and Communication Technologies (ICT) and Human-Computer Interaction\n(HCI) literature, we identify five environmental settings categories. However,\nour analysis of three mobile OS and nine representative applications shows an\noverall lack of environmental concerns in settings design, leading us to\nidentify six pervasive anti-patterns. Environmental settings, where they exist,\nare set on the most intensive option by default. They are not presented as\nsuch, are not easily accessible, and offer little explanation of their impact.\nInstead, they encourage more intensive use. Based on these findings, we create\na design workbook that explores design principles for environmental settings:\npresenting the environmental potential of settings; shifting to environmentally\nneutral states; previewing effects to encourage moderate use; rethinking\ndefaults; facilitating settings access and; exploring more frugal settings.\nBuilding upon this workbook, we discuss how settings can tie individual\nbehaviors to systemic factors.", "comment": "Post-proceedings paper presented at LIMITS 2025: 11th Workshop on\n  Computing within Limits, 2025-06-26/27, Online", "pdf_url": "http://arxiv.org/pdf/2507.19094v1", "cate": "cs.HC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "智能手机设置设计中的环境（不）考虑", "tldr": "该研究发现智能手机设置普遍缺乏对环境的考虑，默认鼓励高强度使用；提出了设计原则以促进更可持续的数字实践。", "motivation": "旨在促进更适度和可持续的数字实践，并解决智能手机设置设计中普遍存在的环境考虑不足的问题。", "method": "基于可持续信息通信技术（ICT）和人机交互（HCI）文献，识别了五种环境设置类别。通过分析三个移动操作系统和九个代表性应用程序，发现了六种普遍存在的反模式。基于这些发现，创建了一个探索环境设置设计原则的设计工作簿。", "result": "研究发现，智能手机设置设计普遍缺乏环境考量，导致了六种普遍的反模式。现有的环境设置默认选择最耗能的选项，且未明确提示其环境影响，不易访问，解释不足，反而鼓励更密集的使用。", "conclusion": "该研究创建了一个设计工作簿，提出了环境设置的设计原则，旨在通过设置将个体行为与系统性因素联系起来，从而促进更可持续的数字实践。", "translation": "促进适度和可持续的数字实践是实现可持续性的方法之一。基于可持续信息通信技术（ICT）和人机交互（HCI）文献，我们确定了五类环境设置。然而，我们对三个移动操作系统和九个代表性应用程序的分析表明，设置设计中普遍缺乏环境考量，导致我们识别出六种普遍存在的反模式。环境设置（如果存在）默认设置为最耗能的选项。它们并未明确呈现为环境设置，不易访问，且对其影响解释甚少。相反，它们鼓励更密集的使用。基于这些发现，我们创建了一个设计工作簿，探讨了环境设置的设计原则：呈现设置的环境潜力；转向环境友好状态；预览效果以鼓励适度使用；重新思考默认设置；促进设置访问；以及探索更节俭的设置。在此工作簿的基础上，我们讨论了设置如何将个体行为与系统性因素联系起来。", "summary": "本研究旨在通过改进智能手机设置设计来促进可持续数字实践。研究分析了现有移动操作系统和应用程序，发现其环境设置普遍不足且设计不佳，常默认高强度使用并缺乏环境影响提示，识别出六种反模式。为此，研究提出了一系列设计原则并创建了设计工作簿，旨在引导设计者创建更环保、鼓励适度使用的智能手机设置，并将个体行为与更广泛的可持续性目标联系起来。", "keywords": "可持续数字实践, 智能手机设置, 环境考量, 人机交互, 设计原则", "comments": "本文的创新之处在于系统性地识别了智能手机设置设计中普遍存在的环境反模式，并提供了一套具体的设计原则来指导更可持续的数字设置设计。其重要性在于将人机交互设计与环境可持续性紧密结合，为数字产品设计者提供了实用的工具和思考框架，有助于推动用户行为向更环保的方向转变。"}}
{"id": "2507.19305", "title": "Demystifying AI in Criminal Justice", "authors": ["Richard Berk"], "categories": ["cs.CY", "stat.AP"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      25 pages. No figures or tables", "url": "http://arxiv.org/abs/2507.19305v1", "summary": "There is widespread confusion among criminal justice practitioners and legal\nscholars about the use of artificial intelligence in criminal justice. This\ndidactic review is written for readers with little or no background in\nstatistics or computer science. It is not intended to replace more technical\ntreatments. It is intended to supplement them and encourage readers to dig more\ndeeply into topics that strike their fancy.", "comment": "25 pages. No figures or tables", "pdf_url": "http://arxiv.org/pdf/2507.19305v1", "cate": "cs.CY", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "揭秘人工智能在刑事司法中的应用", "tldr": "本文旨在为刑事司法从业者和法律学者澄清人工智能在刑事司法中的应用，特别针对非技术背景的读者。", "motivation": "刑事司法从业者和法律学者对人工智能在刑事司法中的使用普遍存在困惑。", "method": "这是一篇教学性综述，旨在为几乎没有统计学或计算机科学背景的读者撰写，旨在补充而非取代更技术性的处理方法，并鼓励读者深入探索感兴趣的主题。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "刑事司法从业者和法律学者对人工智能在刑事司法中的使用普遍存在困惑。这篇教学性综述是为那些几乎没有统计学或计算机科学背景的读者撰写的。它无意取代更专业的技术性论述，而是旨在补充这些论述，并鼓励读者更深入地探究他们感兴趣的话题。", "summary": "鉴于刑事司法从业者和法律学者对人工智能在刑事司法中的应用普遍存在困惑，本文作为一篇教学性综述，旨在为缺乏统计学或计算机科学背景的读者提供清晰的解释，以补充现有技术文献并激发读者深入探究相关主题。", "keywords": "人工智能, 刑事司法, 教学综述, 非技术读者", "comments": "本文针对刑事司法领域中人工智能理解的普遍困惑，提供了一份面向非技术读者的教学性综述，填补了重要的知识空白。其教学性质和鼓励深入学习的意图是其创新和重要性所在。"}}
{"id": "2507.19232", "title": "Event-Driven Storytelling with Multiple Lifelike Humans in a 3D Scene", "authors": ["Donggeun Lim", "Jinseok Bae", "Inwoo Hwang", "Seungmin Lee", "Hwanhee Lee", "Young Min Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      16 pages, project page: this https URL", "url": "http://arxiv.org/abs/2507.19232v1", "summary": "In this work, we propose a framework that creates a lively virtual dynamic\nscene with contextual motions of multiple humans. Generating multi-human\ncontextual motion requires holistic reasoning over dynamic relationships among\nhuman-human and human-scene interactions. We adapt the power of a large\nlanguage model (LLM) to digest the contextual complexity within textual input\nand convert the task into tangible subproblems such that we can generate\nmulti-agent behavior beyond the scale that was not considered before.\nSpecifically, our event generator formulates the temporal progression of a\ndynamic scene into a sequence of small events. Each event calls for a\nwell-defined motion involving relevant characters and objects. Next, we\nsynthesize the motions of characters at positions sampled based on spatial\nguidance. We employ a high-level module to deliver scalable yet comprehensive\ncontext, translating events into relative descriptions that enable the\nretrieval of precise coordinates. As the first to address this problem at scale\nand with diversity, we offer a benchmark to assess diverse aspects of\ncontextual reasoning. Benchmark results and user studies show that our\nframework effectively captures scene context with high scalability. The code\nand benchmark, along with result videos, are available at our project page:\nhttps://rms0329.github.io/Event-Driven-Storytelling/.", "comment": "16 pages, project page:\n  https://rms0329.github.io/Event-Driven-Storytelling/", "pdf_url": "http://arxiv.org/pdf/2507.19232v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "事件驱动的3D场景中多逼真人物故事叙述", "tldr": "该工作提出了一个框架，利用大型语言模型（LLM）生成3D场景中多个人物的上下文相关动作，通过事件驱动的方式实现大规模和多样化的虚拟动态场景，并提供了一个新的基准测试。", "motivation": "生成多人物上下文相关动作需要对人与人、人与场景之间的动态关系进行整体推理，现有的方法在规模和多样性上存在局限性，无法处理大规模的多智能体行为。", "method": "该框架利用大型语言模型（LLM）处理文本输入中的上下文复杂性，并将任务转化为具体的子问题。事件生成器将动态场景的时间进程分解为一系列小事件，每个事件定义相关的角色和对象的动作。然后，根据空间指导采样位置，合成角色的动作。一个高级模块将事件转换为相对描述，以检索精确的坐标，从而实现可扩展且全面的上下文交付。", "result": "基准测试结果和用户研究表明，该框架能够有效捕捉场景上下文，并具有高可扩展性。它是第一个大规模、多样化解决此问题的框架，并提供了一个评估上下文推理各个方面的基准。", "conclusion": "该论文成功提出了一个事件驱动的框架，利用大型语言模型生成大规模、多样化的3D场景中的多人物上下文相关动作，并通过基准测试和用户研究验证了其有效性和高可扩展性，为虚拟动态场景的创建提供了新方法。", "translation": "在这项工作中，我们提出了一个框架，用于创建具有多个逼真人物上下文动作的生动虚拟动态场景。生成多人物上下文动作需要对人与人以及人与场景交互之间的动态关系进行整体推理。我们利用大型语言模型（LLM）的能力来消化文本输入中的上下文复杂性，并将任务转换为具体的子问题，从而生成超越以往规模的多智能体行为。具体来说，我们的事件生成器将动态场景的时间进程表述为一系列小事件。每个事件都需要一个明确定义的动作，涉及相关的角色和对象。接下来，我们根据空间指导采样的位置合成角色的动作。我们采用一个高级模块来提供可扩展且全面的上下文，将事件转换为相对描述，从而能够检索精确的坐标。作为第一个大规模、多样化解决此问题的工作，我们提供了一个基准来评估上下文推理的各个方面。基准测试结果和用户研究表明，我们的框架能够有效地捕捉场景上下文，并具有高可扩展性。代码、基准测试以及结果视频可在我们的项目页面获取：https://rms0329.github.io/Event-Driven-Storytelling/。", "summary": "该论文提出了一个事件驱动的框架，旨在3D场景中创建包含多个逼真人物的生动虚拟动态场景。该框架利用大型语言模型（LLM）处理复杂的文本上下文，并将多人物上下文动作生成任务分解为可管理的子问题。通过事件生成器将场景进展转化为一系列事件，并结合空间指导合成人物动作，该方法首次实现了大规模和多样化的多智能体行为生成。实验结果和用户研究验证了其在捕捉场景上下文和高可扩展性方面的有效性。", "keywords": "事件驱动, 多人物动作, 大型语言模型, 3D场景, 上下文推理", "comments": "这项工作具有显著的创新性，它首次将大型语言模型应用于大规模、多样化的3D场景多人物上下文动作生成，解决了以往方法在规模上的局限性。其事件驱动的框架和对LLM的巧妙运用，为创建更真实、更丰富的虚拟世界提供了新的思路和强大的工具。同时，提供了基准测试也促进了该领域未来的研究和评估。"}}
{"id": "2506.02614", "title": "High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset", "authors": ["Guohang Zhuang", "Weixi Song", "Jinyang Huang", "Chenwei Yang", "Wanli OuYang", "Yan Lu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.02614v4", "summary": "With the rapid development of space exploration, space debris has attracted\nmore attention due to its potential extreme threat, leading to the need for\nreal-time and accurate debris tracking. However, existing methods are mainly\nbased on traditional signal processing, which cannot effectively process the\ncomplex background and dense space debris. In this paper, we propose a deep\nlearning-based Space Debris Tracking Network~(SDT-Net) to achieve highly\naccurate debris tracking. SDT-Net effectively represents the feature of debris,\nenhancing the efficiency and stability of end-to-end model learning. To train\nand evaluate this model effectively, we also produce a large-scale dataset\nSpace Debris Tracking Dataset (SDTD) by a novel observation-based data\nsimulation scheme. SDTD contains 18,040 video sequences with a total of 62,562\nframes and covers 250,000 synthetic space debris. Extensive experiments\nvalidate the effectiveness of our model and the challenging of our dataset.\nFurthermore, we test our model on real data from the Antarctic Station,\nachieving a MOTA score of 73.2%, which demonstrates its strong transferability\nto real-world scenarios. Our dataset and code will be released soon.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.02614v4", "cate": "cs.CV", "date": "2025-06-03", "updated": "2025-07-25", "AI": {"title_translation": "复杂天光背景下基于大规模数据集的高性能空间碎片跟踪", "tldr": "本文提出了一种基于深度学习的SDT-Net，并构建了一个大规模数据集SDTD，以实现复杂背景下高精度的空间碎片跟踪。", "motivation": "随着空间探索的快速发展，空间碎片因其潜在的极端威胁而备受关注，导致对实时准确碎片跟踪的需求。然而，现有方法主要基于传统信号处理，无法有效处理复杂背景和密集空间碎片。", "method": "提出了一种基于深度学习的空间碎片跟踪网络（SDT-Net），旨在实现高精度碎片跟踪。SDT-Net有效表示碎片特征，提高端到端模型学习的效率和稳定性。为有效训练和评估模型，通过一种新颖的基于观测的数据模拟方案，生成了一个大规模空间碎片跟踪数据集（SDTD），包含18,040个视频序列，共62,562帧和250,000个合成空间碎片。", "result": "广泛的实验验证了模型有效性及数据集的挑战性。在南极站的真实数据上测试，模型达到了73.2%的MOTA分数，表明其对真实场景的强大可迁移性。", "conclusion": "该研究提出的深度学习模型和大规模数据集能够有效应对复杂天光背景下的空间碎片跟踪挑战，并在真实世界场景中展现出强大的性能和可迁移性。", "translation": "随着空间探索的快速发展，空间碎片因其潜在的极端威胁而备受关注，导致对实时准确碎片跟踪的需求。然而，现有方法主要基于传统信号处理，无法有效处理复杂背景和密集空间碎片。在本文中，我们提出了一种基于深度学习的空间碎片跟踪网络（SDT-Net），以实现高精度碎片跟踪。SDT-Net有效表示碎片特征，提高端到端模型学习的效率和稳定性。为有效训练和评估该模型，我们还通过一种新颖的基于观测的数据模拟方案，生成了一个大规模空间碎片跟踪数据集（SDTD）。SDTD包含18,040个视频序列，共62,562帧，并涵盖250,000个合成空间碎片。广泛的实验验证了我们模型的有效性以及我们数据集的挑战性。此外，我们在南极站的真实数据上测试了我们的模型，达到了73.2%的MOTA分数，这表明其对真实场景的强大可迁移性。我们的数据集和代码将很快发布。", "summary": "本文针对复杂天光背景下空间碎片跟踪的挑战，提出了一种基于深度学习的SDT-Net，旨在实现高精度碎片跟踪。为训练和评估该模型，研究团队还构建了一个大规模的空间碎片跟踪数据集SDTD。实验结果表明，该模型在合成数据和南极站真实数据上均表现出卓越的性能和强大的可迁移性，有效解决了传统方法在复杂背景下处理碎片跟踪的局限性。", "keywords": "空间碎片跟踪, 深度学习, 大规模数据集, SDT-Net, SDTD", "comments": "该论文创新性地将深度学习应用于空间碎片跟踪领域，并通过构建大规模高质量数据集解决了数据稀缺的问题，显著提升了复杂背景下碎片跟踪的精度和稳定性。其在真实世界数据上的验证结果进一步证明了该方法的实用性和广阔应用前景。"}}
{"id": "2507.18819", "title": "Probabilistic Collision Risk Estimation through Gauss-Legendre Cubature and Non-Homogeneous Poisson Processes", "authors": ["Trent Weiss", "Madhur Behl"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      This work has been submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.18819v1", "summary": "Overtaking in high-speed autonomous racing demands precise, real-time\nestimation of collision risk; particularly in wheel-to-wheel scenarios where\nsafety margins are minimal. Existing methods for collision risk estimation\neither rely on simplified geometric approximations, like bounding circles, or\nperform Monte Carlo sampling which leads to overly conservative motion planning\nbehavior at racing speeds. We introduce the Gauss-Legendre Rectangle (GLR)\nalgorithm, a principled two-stage integration method that estimates collision\nrisk by combining Gauss-Legendre with a non-homogeneous Poisson process over\ntime. GLR produces accurate risk estimates that account for vehicle geometry\nand trajectory uncertainty. In experiments across 446 overtaking scenarios in a\nhigh-fidelity Formula One racing simulation, GLR outperforms five\nstate-of-the-art baselines achieving an average error reduction of 77% and\nsurpassing the next-best method by 52%, all while running at 1000 Hz. The\nframework is general and applicable to broader motion planning contexts beyond\nautonomous racing.", "comment": "This work has been submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.18819v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于高斯-勒让德求积与非齐次泊松过程的概率碰撞风险估计", "tldr": "本文提出GLR算法，结合高斯-勒让德与非齐次泊松过程，在高保真F1模拟中实现高精度、实时碰撞风险估计，显著优于现有方法。", "motivation": "现有碰撞风险估计方法依赖简化的几何近似或蒙特卡洛采样，导致在高速赛车场景下过度保守的运动规划行为，且精度不足。", "method": "引入了高斯-勒让德矩形（GLR）算法，这是一种原则性的两阶段积分方法，通过结合高斯-勒让德与非齐次泊松过程随时间推移来估计碰撞风险。GLR考虑了车辆几何形状和轨迹不确定性。", "result": "在446个超车场景的高保真F1赛车模拟实验中，GLR优于五种最先进的基线方法，平均误差降低77%，比次优方法高出52%，同时以1000 Hz运行。", "conclusion": "GLR算法能提供准确的碰撞风险估计，适用于高速自主赛车，且该框架具有通用性，可应用于更广泛的运动规划场景。", "translation": "高速自主赛车中的超车需要精确、实时的碰撞风险估计；特别是在安全裕度极小的轮对轮场景中。现有的碰撞风险估计方法要么依赖简化的几何近似（如边界圆），要么执行蒙特卡洛采样，这导致在赛车速度下产生过度保守的运动规划行为。我们引入了高斯-勒让德矩形（GLR）算法，这是一种原则性的两阶段积分方法，通过将高斯-勒让德与随时间变化的非齐次泊松过程相结合来估计碰撞风险。GLR产生准确的风险估计，并考虑了车辆几何形状和轨迹不确定性。在一次高保真F1赛车模拟中进行的446个超车场景的实验中，GLR的性能优于五种最先进的基线方法，平均误差降低了77%，并超越次优方法52%，同时以1000 Hz运行。该框架具有通用性，可应用于自主赛车之外的更广泛的运动规划环境。", "summary": "本文提出了一种名为高斯-勒让德矩形（GLR）的新型两阶段积分算法，用于高速自主赛车中精确、实时的概率碰撞风险估计。该算法结合了高斯-勒让德求积和非齐次泊松过程，能够考虑车辆几何形状和轨迹不确定性。实验结果表明，在F1赛车模拟中，GLR在准确性和实时性方面显著优于现有方法，平均误差降低77%，且运行频率高达1000 Hz，展现出在运动规划领域的广泛应用潜力。", "keywords": "碰撞风险估计, 高斯-勒让德, 非齐次泊松过程, 自主赛车, 运动规划", "comments": "本文的创新点在于提出了GLR算法，结合了高斯-勒让德和非齐次泊松过程，解决了现有方法在高速场景下精度和实时性不足的问题。其重要性在于为高风险、实时性要求高的自主驾驶场景提供了更精确、高效的碰撞风险估计方法，尤其是在极小的安全裕度下表现出色。实验结果令人印象深刻，表明其在实际应用中的巨大潜力。"}}
{"id": "2507.18758", "title": "Learning Efficient and Generalizable Human Representation with Human Gaussian Model", "authors": ["Yifan Liu", "Shengjun Zhang", "Chensheng Dai", "Yang Chen", "Hao Liu", "Chen Li", "Yueqi Duan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18758v1", "summary": "Modeling animatable human avatars from videos is a long-standing and\nchallenging problem. While conventional methods require per-instance\noptimization, recent feed-forward methods have been proposed to generate 3D\nGaussians with a learnable network. However, these methods predict Gaussians\nfor each frame independently, without fully capturing the relations of\nGaussians from different timestamps. To address this, we propose Human Gaussian\nGraph to model the connection between predicted Gaussians and human SMPL mesh,\nso that we can leverage information from all frames to recover an animatable\nhuman representation. Specifically, the Human Gaussian Graph contains dual\nlayers where Gaussians are the first layer nodes and mesh vertices serve as the\nsecond layer nodes. Based on this structure, we further propose the intra-node\noperation to aggregate various Gaussians connected to one mesh vertex, and\ninter-node operation to support message passing among mesh node neighbors.\nExperimental results on novel view synthesis and novel pose animation\ndemonstrate the efficiency and generalization of our method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18758v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "使用人体高斯模型学习高效且可泛化的人体表征", "tldr": "提出了一种名为Human Gaussian Graph的新方法，通过建模高斯与SMPL网格之间的连接，实现从视频中高效且可泛化地恢复可动画人体表征，解决了现有方法独立预测高斯的问题。", "motivation": "从视频中建模可动画人体化身是一个长期存在的挑战。现有前馈方法独立预测每一帧的高斯，未能充分捕捉不同时间戳高斯之间的关系，导致效率和泛化性不足。", "method": "我们提出了Human Gaussian Graph (HGG) 来建模预测高斯与人体SMPL网格之间的连接。HGG包含双层结构：高斯是第一层节点，网格顶点是第二层节点。在此结构基础上，我们进一步提出了节点内操作（聚合连接到同一网格顶点的各种高斯）和节点间操作（支持网格节点邻居之间的消息传递），以利用所有帧的信息恢复可动画人体表征。", "result": "在新视角合成和新姿态动画上的实验结果证明了我们方法的效率和泛化性。", "conclusion": "通过引入Human Gaussian Graph及其双层结构和操作，本方法能够有效利用多帧信息，学习到高效且可泛化的人体表征，从而解决了现有高斯基方法在建模可动画人体化身时面临的挑战。", "translation": "从视频中建模可动画人体化身是一个长期存在的挑战性问题。虽然传统方法需要对每个实例进行优化，但最近提出了前馈方法，通过可学习网络生成3D高斯。然而，这些方法独立预测每一帧的高斯，未能充分捕捉不同时间戳高斯之间的关系。为了解决这个问题，我们提出了Human Gaussian Graph来建模预测高斯与人体SMPL网格之间的连接，以便我们可以利用所有帧的信息来恢复可动画人体表征。具体来说，Human Gaussian Graph包含双层，其中高斯是第一层节点，网格顶点作为第二层节点。基于这种结构，我们进一步提出了节点内操作来聚合连接到同一网格顶点的各种高斯，以及节点间操作来支持网格节点邻居之间的消息传递。在新视角合成和新姿态动画上的实验结果证明了我们方法的效率和泛化性。", "summary": "本文提出了一种名为Human Gaussian Graph (HGG) 的新方法，用于从视频中学习高效且可泛化的人体表征。针对现有前馈方法独立预测高斯导致未能充分利用时间信息的问题，HGG通过构建高斯与人体SMPL网格之间的双层图结构，利用节点内操作聚合高斯和节点间操作进行消息传递，从而整合多帧信息以恢复可动画人体表征。实验证明，该方法在视角合成和姿态动画方面表现出良好的效率和泛化性。", "keywords": "人体表征, 高斯模型, 人体高斯图, SMPL, 可动画化身", "comments": "本文的创新点在于提出了Human Gaussian Graph这一新颖的图结构，将3D高斯与人体SMPL网格相结合，并通过特定的节点操作有效利用了多帧信息，解决了现有高斯基方法在处理人体动画时独立帧处理的局限性。这对于实现更高效、更泛化的人体化身建模具有重要意义。"}}
{"id": "2507.19247", "title": "A Markov Categorical Framework for Language Modeling", "authors": ["Yifan Zhang"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2507.19247v1", "summary": "Auto-regressive language models factorize sequence probabilities and are\ntrained by minimizing the negative log-likelihood (NLL) objective. While\nempirically powerful, a deep theoretical understanding of why this simple\nobjective yields such versatile representations remains elusive. This work\nintroduces a unifying analytical framework using Markov Categories (MCs) to\ndeconstruct the AR generation process and the NLL objective. We model the\nsingle-step generation map as a composition of Markov kernels in the category\nStoch. This compositional view, when enriched with statistical divergences,\nallows us to dissect information flow and learned geometry. Our framework makes\nthree main contributions. First, we provide a formal, information-theoretic\nrationale for the success of modern speculative decoding methods like EAGLE,\nquantifying the information surplus in hidden states that these methods\nexploit. Second, we formalize how NLL minimization forces the model to learn\nnot just the next token, but the data's intrinsic conditional stochasticity, a\nprocess we analyze using categorical entropy. Third, and most centrally, we\nprove that NLL training acts as an implicit form of spectral contrastive\nlearning. By analyzing the information geometry of the model's prediction head,\nwe show that NLL implicitly forces the learned representation space to align\nwith the eigenspectrum of a predictive similarity operator, thereby learning a\ngeometrically structured space without explicit contrastive pairs. This\ncompositional and information-geometric perspective reveals the deep structural\nprinciples underlying the effectiveness of modern LMs. Project Page:\nhttps://github.com/asiresearch/lm-theory", "comment": "Project Page: https://github.com/asiresearch/lm-theory", "pdf_url": "http://arxiv.org/pdf/2507.19247v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "语言建模的马尔可夫范畴框架", "tldr": "本文引入了一个统一的马尔可夫范畴（MC）分析框架，以深入理解自回归语言模型中的NLL目标和生成过程。该框架揭示了现代语言模型有效性的深层结构原理，解释了推测解码、NLL学习内在条件随机性以及NLL作为隐式谱对比学习的作用。", "motivation": "尽管自回归语言模型在实践中表现强大，但对于为何其简单的负对数似然（NLL）目标能够产生如此多功能的表示，缺乏深入的理论理解。", "method": "本文引入了一个统一的分析框架，使用马尔可夫范畴（MCs）来解构自回归（AR）生成过程和负对数似然（NLL）目标。该方法将单步生成映射建模为Stoch范畴中马尔可夫核的组合，并结合统计散度来剖析信息流和学习到的几何结构。", "result": "1. 为EAGLE等现代推测解码方法提供了形式化的信息理论依据，量化了这些方法利用的隐藏状态中的信息盈余。\n2. 形式化了NLL最小化如何迫使模型不仅学习下一个token，还学习数据的内在条件随机性，并使用范畴熵进行了分析。\n3. 证明了NLL训练是一种隐式的谱对比学习形式，通过分析模型预测头的信息几何，表明NLL隐式地强制学习到的表示空间与预测相似性算子的特征谱对齐，从而在没有显式对比对的情况下学习到几何结构化的空间。", "conclusion": "这种组合式和信息几何的视角揭示了现代语言模型有效性背后的深层结构原理。", "translation": "自回归语言模型分解序列概率，并通过最小化负对数似然（NLL）目标进行训练。尽管在经验上功能强大，但对于这种简单目标为何能产生如此多功能的表示，其深层理论理解仍然难以捉摸。这项工作引入了一个统一的分析框架，使用马尔可夫范畴（MCs）来解构自回归（AR）生成过程和NLL目标。我们将在Stoch范畴中将单步生成映射建模为马尔可夫核的组合。当这种组合视角与统计散度相结合时，使我们能够剖析信息流和学习到的几何结构。我们的框架有三个主要贡献。首先，我们为EAGLE等现代推测解码方法的成功提供了形式化的信息理论依据，量化了这些方法利用的隐藏状态中的信息盈余。其次，我们形式化了NLL最小化如何迫使模型不仅学习下一个token，还学习数据的内在条件随机性，我们使用范畴熵分析了这一过程。第三，也是最核心的，我们证明了NLL训练是一种隐式的谱对比学习形式。通过分析模型预测头的信息几何，我们表明NLL隐式地强制学习到的表示空间与预测相似性算子的特征谱对齐，从而在没有显式对比对的情况下学习到几何结构化的空间。这种组合式和信息几何的视角揭示了现代语言模型有效性背后的深层结构原理。项目页面：https://github.com/asiresearch/lm-theory", "summary": "本文提出一个基于马尔可夫范畴的统一分析框架，旨在从理论上理解自回归语言模型及其负对数似然（NLL）训练的有效性。该框架通过组合马尔可夫核并结合信息几何，揭示了AR生成过程和NLL目标的深层机制。研究发现，NLL训练不仅解释了推测解码的成功，还促使模型学习数据的内在条件随机性，并作为一种隐式谱对比学习形式，使得学习到的表示空间具有几何结构。这一理论视角深入阐释了现代语言模型高效工作的基础原理。", "keywords": "马尔可夫范畴, 语言建模, NLL, 对比学习, 信息几何", "comments": "该论文通过引入马尔可夫范畴这一新颖的理论框架，为理解自回归语言模型和NLL训练的深层机制提供了重要的理论贡献。其创新之处在于将信息几何与范畴论相结合，揭示了NLL训练的隐式对比学习特性，这对于指导未来语言模型的设计和优化具有重要意义。它弥补了现有理论理解的不足。"}}
{"id": "2501.17772", "title": "Self-Supervised Frameworks for Speaker Verification via Bootstrapped Positive Sampling", "authors": ["Theo Lepage", "Reda Dehak"], "categories": ["eess.AS", "cs.LG", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      accepted for publication in IEEE TASLP", "url": "http://arxiv.org/abs/2501.17772v4", "summary": "Recent developments in Self-Supervised Learning (SSL) have demonstrated\nsignificant potential for Speaker Verification (SV), but closing the\nperformance gap with supervised systems remains an ongoing challenge. SSL\nframeworks rely on anchor-positive pairs, constructed from segments of the same\naudio utterance. Hence, positives have channel characteristics similar to those\nof their corresponding anchors, even with extensive data-augmentation.\nTherefore, this positive sampling strategy is a fundamental limitation as it\nencodes too much information regarding the recording source in the learned\nrepresentations. This article introduces Self-Supervised Positive Sampling\n(SSPS), a bootstrapped technique for sampling appropriate and diverse positives\nin SSL frameworks for SV. SSPS samples positives close to their anchor in the\nrepresentation space, assuming that these pseudo-positives belong to the same\nspeaker identity but correspond to different recording conditions. This method\nconsistently demonstrates improvements in SV performance on VoxCeleb benchmarks\nwhen applied to major SSL frameworks, including SimCLR, SwAV, VICReg, and DINO.\nUsing SSPS, SimCLR and DINO achieve 2.57% and 2.53% EER on VoxCeleb1-O,\nrespectively. SimCLR yields a 58% relative reduction in EER, getting comparable\nperformance to DINO with a simpler training framework. Furthermore, SSPS lowers\nintra-class variance and reduces channel information in speaker representations\nwhile exhibiting greater robustness without data-augmentation.", "comment": "accepted for publication in IEEE TASLP", "pdf_url": "http://arxiv.org/pdf/2501.17772v4", "cate": "eess.AS", "date": "2025-01-29", "updated": "2025-07-24", "AI": {"title_translation": "通过自举正样本采样的说话人确认自监督框架", "tldr": "本文提出了一种名为自监督正样本采样（SSPS）的新方法，通过在表示空间中选择多样化的伪正样本来改进说话人确认中的自监督学习，显著提高了性能，并减少了表示中的信道信息。", "motivation": "尽管自监督学习（SSL）在说话人确认（SV）中展现出巨大潜力，但与监督系统之间的性能差距仍然存在。现有的SSL框架在构建锚点-正样本对时，由于正样本来自同一语音片段，导致其信道特性与锚点相似，从而在学习到的表示中编码了过多的录音源信息，这限制了其性能。", "method": "本文引入了自监督正样本采样（SSPS），这是一种自举技术，用于在SV的SSL框架中采样合适且多样化的正样本。SSPS在表示空间中选择靠近锚点的伪正样本，假设这些样本属于同一说话人身份但对应不同的录音条件。", "result": "SSPS在VoxCeleb基准测试上应用于SimCLR、SwAV、VICReg和DINO等主要SSL框架时，持续提高了SV性能。使用SSPS，SimCLR和DINO在VoxCeleb1-O上分别实现了2.57%和2.53%的等错误率（EER）。SimCLR的EER相对降低了58%，以更简单的训练框架达到了与DINO相当的性能。此外，SSPS降低了说话人表示中的类内方差和信道信息，并在没有数据增强的情况下表现出更强的鲁棒性。", "conclusion": "SSPS通过改进正样本采样策略，有效解决了自监督说话人确认中表示学习编码过多录音源信息的问题，显著提升了性能，并增强了模型的鲁棒性。", "translation": "自监督学习（SSL）的最新发展在说话人确认（SV）方面展现出巨大潜力，但缩小与监督系统之间的性能差距仍然是一个持续的挑战。SSL框架依赖于由同一音频话语片段构建的锚点-正样本对。因此，即使进行了大量数据增强，正样本也具有与其对应锚点相似的信道特性。这种正样本采样策略是一个根本性的限制，因为它在学习到的表示中编码了过多的录音源信息。本文介绍了自监督正样本采样（SSPS），这是一种自举技术，用于在SV的SSL框架中采样合适且多样化的正样本。SSPS在表示空间中采样靠近其锚点的正样本，假设这些伪正样本属于同一说话人身份但对应不同的录音条件。该方法在应用于包括SimCLR、SwAV、VICReg和DINO在内的主要SSL框架时，在VoxCeleb基准测试上持续展现出SV性能的改进。使用SSPS，SimCLR和DINO在VoxCeleb1-O上分别实现了2.57%和2.53%的等错误率（EER）。SimCLR的EER相对降低了58%，以更简单的训练框架获得了与DINO相当的性能。此外，SSPS降低了说话人表示中的类内方差并减少了信道信息，同时在没有数据增强的情况下表现出更强的鲁棒性。", "summary": "本文提出了一种名为自监督正样本采样（SSPS）的新颖方法，旨在解决自监督学习（SSL）在说话人确认（SV）中因现有正样本采样策略编码过多信道信息而导致的性能限制。SSPS采用自举技术，在表示空间中选择与锚点接近但对应不同录音条件的伪正样本。实验结果表明，SSPS显著提升了SimCLR、SwAV、VICReg和DINO等主要SSL框架在VoxCeleb基准测试上的SV性能，例如SimCLR的EER相对降低了58%，并达到了与DINO相当的性能。此外，SSPS有效降低了类内方差和表示中的信道信息，并增强了模型在无数据增强条件下的鲁棒性。", "keywords": "自监督学习, 说话人确认, 正样本采样, 表示学习, 自举技术", "comments": "本文的创新点在于提出了SSPS这一新颖的正样本采样策略，解决了自监督说话人确认中现有方法因信道信息冗余而导致的性能瓶颈。通过在表示空间中选择伪正样本，SSPS能够学习到更纯净、更具判别力的说话人表示。其重要性体现在显著提升了多种主流SSL框架的性能，并使得SimCLR这一相对简单的框架能够达到复杂模型DINO的水平。此外，SSPS在无数据增强情况下的鲁棒性也值得关注，这为实际应用提供了便利。"}}
{"id": "2506.14988", "title": "Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits", "authors": ["Tianyi Xu", "Jiaxin Liu", "Nicholas Mattei", "Zizhan Zheng"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.14988v3", "summary": "We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at\nensuring fair outcomes across agents while maximizing overall system\nperformance. A key challenge in this setting is decision-making under limited\ninformation about arm rewards. To address this, we introduce a novel probing\nframework that strategically gathers information about selected arms before\nallocation. In the offline setting, where reward distributions are known, we\nleverage submodular properties to design a greedy probing algorithm with a\nprovable performance bound. For the more complex online setting, we develop an\nalgorithm that achieves sublinear regret while maintaining fairness. Extensive\nexperiments on synthetic and real-world datasets show that our approach\noutperforms baseline methods, achieving better fairness and efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.14988v3", "cate": "cs.LG", "date": "2025-06-17", "updated": "2025-07-25", "AI": {"title_translation": "带有探测机制的多智能体多臂老虎机公平算法", "tldr": "本文提出了一个带有新颖探测机制的多智能体多臂老虎机（MA-MAB）框架，旨在在有限信息下确保智能体之间的公平结果并最大化系统性能，并在离线和在线设置中都取得了优异表现。", "motivation": "在多智能体多臂老虎机（MA-MAB）框架中，如何在最大化整体系统性能的同时确保智能体之间的公平结果是一个关键挑战，尤其是在关于臂奖励信息有限的情况下。", "method": "本文引入了一种新颖的探测框架，在分配之前策略性地收集关于所选臂的信息。在奖励分布已知的离线设置中，利用次模性质设计了一个具有可证明性能界限的贪婪探测算法。对于更复杂的在线设置，开发了一种在保持公平性的同时实现次线性遗憾的算法。", "result": "在合成数据集和真实世界数据集上进行的广泛实验表明，所提出的方法优于基线方法，实现了更好的公平性和效率。", "conclusion": "本文提出的带有探测机制的公平算法能够有效解决多智能体多臂老虎机问题中的公平性与效率权衡，并在离线和在线设置中都表现出色。", "translation": "我们提出了一个多智能体多臂老虎机（MA-MAB）框架，旨在确保智能体之间的公平结果，同时最大化整体系统性能。在这种设置中，关键挑战是在关于臂奖励信息有限的情况下进行决策。为了解决这个问题，我们引入了一种新颖的探测框架，在分配之前策略性地收集关于所选臂的信息。在奖励分布已知的离线设置中，我们利用次模性质设计了一个具有可证明性能界限的贪婪探测算法。对于更复杂的在线设置，我们开发了一种在保持公平性的同时实现次线性遗憾的算法。在合成数据集和真实世界数据集上进行的广泛实验表明，我们的方法优于基线方法，实现了更好的公平性和效率。", "summary": "本文提出了一个新颖的多智能体多臂老虎机（MA-MAB）框架，通过引入一个探测机制来策略性地收集关于臂奖励的信息，以解决在有限信息下确保智能体公平性并最大化系统性能的挑战。该框架针对离线设置设计了一个基于次模性质的贪婪探测算法并提供了性能保证，同时为在线设置开发了一个能实现次线性遗憾并保持公平性的算法。实验结果表明，该方法在公平性和效率方面均优于现有基线。", "keywords": "多智能体多臂老虎机, 公平性, 探测机制, 次模, 在线算法", "comments": "本文的创新点在于提出了一个新颖的探测框架，有效地解决了多智能体多臂老虎机问题中公平性与效率的权衡，尤其是在信息受限的情况下。它不仅为离线设置提供了理论性能保证，也为更复杂的在线设置设计了有效的算法，展现了其在理论和实践上的重要性。"}}
{"id": "2507.16952", "title": "Evaluating Ensemble and Deep Learning Models for Static Malware Detection with Dimensionality Reduction Using the EMBER Dataset", "authors": ["Md Min-Ha-Zul Abedin", "Tazqia Mehrub"], "categories": ["cs.CR", "cs.AI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16952v2", "summary": "This study investigates the effectiveness of several machine learning\nalgorithms for static malware detection using the EMBER dataset, which contains\nfeature representations of Portable Executable (PE) files. We evaluate eight\nclassification models: LightGBM, XGBoost, CatBoost, Random Forest, Extra Trees,\nHistGradientBoosting, k-Nearest Neighbors (KNN), and TabNet, under three\npreprocessing settings: original feature space, Principal Component Analysis\n(PCA), and Linear Discriminant Analysis (LDA). The models are assessed on\naccuracy, precision, recall, F1 score, and AUC to examine both predictive\nperformance and robustness. Ensemble methods, especially LightGBM and XGBoost,\nshow the best overall performance across all configurations, with minimal\nsensitivity to PCA and consistent generalization. LDA improves KNN performance\nbut significantly reduces accuracy for boosting models. TabNet, while promising\nin theory, underperformed under feature reduction, likely due to architectural\nsensitivity to input structure. The analysis is supported by detailed\nexploratory data analysis (EDA), including mutual information ranking, PCA or\nt-SNE visualizations, and outlier detection using Isolation Forest and Local\nOutlier Factor (LOF), which confirm the discriminatory capacity of key features\nin the EMBER dataset. The results suggest that boosting models remain the most\nreliable choice for high-dimensional static malware detection, and that\ndimensionality reduction should be applied selectively based on model type.\nThis work provides a benchmark for comparing classification models and\npreprocessing strategies in malware detection tasks and contributes insights\nthat can guide future system development and real-world deployment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16952v2", "cate": "cs.CR", "date": "2025-07-22", "updated": "2025-07-24", "AI": {"title_translation": "使用EMBER数据集评估集成和深度学习模型在静态恶意软件检测中的降维效果", "tldr": "本研究评估了在EMBER数据集上使用PCA和LDA进行降维后，多种机器学习模型（包括集成和深度学习模型）在静态恶意软件检测中的性能。结果显示，LightGBM和XGBoost等集成方法表现最佳且对PCA不敏感，而降维应根据模型类型选择性应用。", "motivation": "本研究旨在调查多种机器学习算法在静态恶意软件检测中的有效性，并评估不同预处理设置（原始特征空间、PCA和LDA）对模型性能的影响。", "method": "本研究使用EMBER数据集，该数据集包含可执行文件（PE）的特征表示。评估了八种分类模型：LightGBM、XGBoost、CatBoost、Random Forest、Extra Trees、HistGradientBoosting、k-Nearest Neighbors (KNN) 和 TabNet。在三种预处理设置下进行评估：原始特征空间、主成分分析（PCA）和线性判别分析（LDA）。模型通过准确率、精确率、召回率、F1分数和AUC进行评估。分析还包括探索性数据分析（EDA），如互信息排序、PCA或t-SNE可视化，以及使用Isolation Forest和Local Outlier Factor (LOF)进行异常值检测。", "result": "集成方法，特别是LightGBM和XGBoost，在所有配置下表现出最佳的整体性能，对PCA的敏感性最小，并具有一致的泛化能力。LDA改善了KNN的性能，但显著降低了提升模型的准确率。TabNet在特征约简下表现不佳，可能由于其架构对输入结构的敏感性。探索性数据分析（EDA）证实了EMBER数据集中关键特征的判别能力。", "conclusion": "提升模型仍然是高维静态恶意软件检测最可靠的选择，并且降维应根据模型类型选择性应用。这项工作为恶意软件检测任务中的分类模型和预处理策略的比较提供了基准，并提供了可以指导未来系统开发和实际部署的见解。", "translation": "本研究调查了使用EMBER数据集进行静态恶意软件检测的几种机器学习算法的有效性，该数据集包含可执行文件（PE）的特征表示。我们评估了八种分类模型：LightGBM、XGBoost、CatBoost、Random Forest、Extra Trees、HistGradientBoosting、k-Nearest Neighbors (KNN) 和 TabNet，在三种预处理设置下：原始特征空间、主成分分析（PCA）和线性判别分析（LDA）。模型通过准确率、精确率、召回率、F1分数和AUC进行评估，以检查预测性能和鲁棒性。集成方法，特别是LightGBM和XGBoost，在所有配置下表现出最佳的整体性能，对PCA的敏感性最小，并具有一致的泛化能力。LDA改善了KNN的性能，但显著降低了提升模型的准确率。TabNet虽然在理论上有前景，但在特征约简下表现不佳，可能由于其架构对输入结构的敏感性。分析得到了详细探索性数据分析（EDA）的支持，包括互信息排序、PCA或t-SNE可视化，以及使用Isolation Forest和Local Outlier Factor (LOF)进行异常值检测，这些证实了EMBER数据集中关键特征的判别能力。结果表明，提升模型仍然是高维静态恶意软件检测最可靠的选择，并且降维应根据模型类型选择性应用。这项工作为恶意软件检测任务中的分类模型和预处理策略的比较提供了基准，并提供了可以指导未来系统开发和实际部署的见解。", "summary": "本研究评估了在EMBER数据集上进行静态恶意软件检测时，不同机器学习模型（包括集成和深度学习模型）在多种降维策略（PCA和LDA）下的性能。研究发现，LightGBM和XGBoost等集成方法在所有配置下表现最佳，且对PCA不敏感。尽管LDA能改善KNN性能，但会降低提升模型的准确率。TabNet在特征约简下表现不佳。研究强调，提升模型是高维静态恶意软件检测的可靠选择，降维应根据模型类型选择性应用。本工作为恶意软件检测的分类模型和预处理策略提供了基准。", "keywords": "静态恶意软件检测, 集成学习, 深度学习, 降维, EMBER数据集", "comments": "这项研究通过全面的实验评估了多种机器学习模型和降维技术在静态恶意软件检测中的应用，为该领域的模型选择和预处理策略提供了宝贵的实践指导。其创新点在于对不同模型在降维后的性能进行了细致的比较，并指出了降维应根据模型类型选择性应用的重要性。研究结果对实际部署和未来系统开发具有重要意义。"}}
{"id": "2507.19275", "title": "Mut4All: Fuzzing Compilers via LLM-Synthesized Mutators Learned from Bug Reports", "authors": ["Bo Wang", "Pengyang Wang", "Chong Chen", "Qi Sun", "Jieke Shi", "Chengran Yang", "Ming Deng", "Youfang Lin", "Zhou Yang", "David Lo"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19275v1", "summary": "Mutation-based fuzzing is effective for uncovering compiler bugs, but\ndesigning high-quality mutators for modern languages with complex constructs\n(e.g., templates, macros) remains challenging. Existing methods rely heavily on\nmanual design or human-in-the-loop correction, limiting scalability and\ncross-language generalizability.\n  We present Mut4All, a fully automated, language-agnostic framework that\nsynthesizes mutators using Large Language Models (LLMs) and compiler-specific\nknowledge from bug reports. It consists of three agents: (1) a mutator\ninvention agent that identifies mutation targets and generates mutator metadata\nusing compiler-related insights; (2) a mutator implementation synthesis agent,\nfine-tuned to produce initial implementations; and (3) a mutator refinement\nagent that verifies and corrects the mutators via unit-test feedback.\n  Mut4All processes 1000 bug reports (500 Rust, 500 C++), yielding 319 Rust and\n403 C++ mutators at ~$0.08 each via GPT-4o. Our customized fuzzer, using these\nmutators, finds 62 bugs in Rust compilers (38 new, 7 fixed) and 34 bugs in C++\ncompilers (16 new, 1 fixed). Mut4All outperforms existing methods in both\nunique crash detection and coverage, ranking first on Rust and second on C++.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19275v1", "cate": "cs.SE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "Mut4All：通过从错误报告中学习的LLM合成变异器对编译器进行模糊测试", "tldr": "Mut4All是一个自动化、语言无关的框架，它使用LLM和编译器bug报告来合成高质量的变异器，从而有效地发现编译器bug。", "motivation": "现有的基于变异的模糊测试方法在设计高质量的变异器方面存在挑战，尤其对于现代语言的复杂结构（如模板、宏），且这些方法高度依赖手动设计或人工干预，限制了可扩展性和跨语言通用性。", "method": "提出了Mut4All，一个全自动化、语言无关的框架，它使用大型语言模型（LLM）和来自错误报告的编译器特定知识来合成变异器。该框架由三个代理组成：变异器发明代理（识别变异目标并生成元数据）、变异器实现合成代理（生成初始实现）和变异器精炼代理（通过单元测试反馈验证和纠正变异器）。", "result": "Mut4All处理了1000份错误报告（500份Rust，500份C++），通过GPT-4o生成了319个Rust和403个C++变异器，每个成本约0.08美元。使用这些变异器的定制模糊测试器在Rust编译器中发现了62个bug（38个新bug，7个已修复），在C++编译器中发现了34个bug（16个新bug，1个已修复）。Mut4All在独特的崩溃检测和覆盖率方面均优于现有方法，在Rust上排名第一，在C++上排名第二。", "conclusion": "Mut4All通过利用LLM自动化变异器合成，显著提高了编译器模糊测试的效率和效果，超越了现有方法，并在发现新bug方面表现出色。", "translation": "基于变异的模糊测试在发现编译器错误方面是有效的，但为具有复杂结构（例如模板、宏）的现代语言设计高质量的变异器仍然具有挑战性。现有方法严重依赖手动设计或人工干预，这限制了可扩展性和跨语言通用性。\n我们提出了Mut4All，一个全自动化、语言无关的框架，它使用大型语言模型（LLM）和来自错误报告的编译器特定知识来合成变异器。它由三个代理组成：（1）一个变异器发明代理，它识别变异目标并利用编译器相关见解生成变异器元数据；（2）一个变异器实现合成代理，经过微调以生成初始实现；（3）一个变异器精炼代理，它通过单元测试反馈验证和纠正变异器。\nMut4All处理了1000份错误报告（500份Rust，500份C++），通过GPT-4o生成了319个Rust和403个C++变异器，每个成本约0.08美元。我们定制的模糊测试器使用这些变异器，在Rust编译器中发现了62个bug（38个新bug，7个已修复），在C++编译器中发现了34个bug（16个新bug，1个已修复）。Mut4All在独特的崩溃检测和覆盖率方面均优于现有方法，在Rust上排名第一，在C++上排名第二。", "summary": "Mut4All是一个创新的自动化框架，利用大型语言模型和编译器错误报告来合成高质量的变异器，以改进编译器模糊测试。该系统通过三个智能代理实现变异器的发明、实现和精炼。实验结果表明，Mut4All成功为Rust和C++编译器生成了大量变异器，并发现了显著数量的新bug，在性能上超越了现有方法，有效解决了传统变异器设计中可扩展性和通用性的挑战。", "keywords": "模糊测试, 编译器, LLM, 变异器, 自动化", "comments": "Mut4All的创新之处在于其将LLM应用于编译器模糊测试中的变异器自动化合成，解决了手动设计变异器耗时且难以扩展的问题。其语言无关性和利用真实bug报告学习的能力使其具有很高的实用价值和通用性。该方法显著提高了发现新编译器bug的效率，对软件测试和编译器开发领域具有重要意义。"}}
{"id": "2507.13229", "title": "$S^2M^2$: Scalable Stereo Matching Model for Reliable Depth Estimation", "authors": ["Junhong Min", "Youngpil Jeon", "Jimin Kim", "Minyong Choi"], "categories": ["cs.CV", "cs.AI", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 5 figures, ICCV accepted paper", "url": "http://arxiv.org/abs/2507.13229v2", "summary": "The pursuit of a generalizable stereo matching model, capable of performing\nacross varying resolutions and disparity ranges without dataset-specific\nfine-tuning, has revealed a fundamental trade-off. Iterative local search\nmethods achieve high scores on constrained benchmarks, but their core mechanism\ninherently limits the global consistency required for true generalization. On\nthe other hand, global matching architectures, while theoretically more robust,\nhave been historically rendered infeasible by prohibitive computational and\nmemory costs. We resolve this dilemma with $S^2M^2$: a global matching\narchitecture that achieves both state-of-the-art accuracy and high efficiency\nwithout relying on cost volume filtering or deep refinement stacks. Our design\nintegrates a multi-resolution transformer for robust long-range correspondence,\ntrained with a novel loss function that concentrates probability on feasible\nmatches. This approach enables a more robust joint estimation of disparity,\nocclusion, and confidence. $S^2M^2$ establishes a new state of the art on the\nMiddlebury v3 and ETH3D benchmarks, significantly outperforming prior methods\nacross most metrics while reconstructing high-quality details with competitive\nefficiency.", "comment": "8 pages, 5 figures, ICCV accepted paper", "pdf_url": "http://arxiv.org/pdf/2507.13229v2", "cate": "cs.CV", "date": "2025-07-17", "updated": "2025-07-25", "AI": {"title_translation": "$S^2M^2$: 可扩展立体匹配模型，用于可靠深度估计", "tldr": "$S^2M^2$ 是一种高效且准确的全局立体匹配模型，通过多分辨率Transformer和新颖的损失函数解决了传统全局方法计算成本高的问题，并在基准测试中达到了SOTA。", "motivation": "现有的立体匹配模型在泛化能力上存在根本性权衡：迭代局部搜索方法准确但缺乏全局一致性，而全局匹配架构虽然理论上更鲁棒，却因计算和内存成本过高而不可行。", "method": "$S^2M^2$ 是一种全局匹配架构，通过集成多分辨率Transformer实现鲁棒的长程对应，并使用一种将概率集中在可行匹配上的新型损失函数进行训练，从而实现视差、遮挡和置信度的鲁棒联合估计。", "result": "$S^2M^2$ 在Middlebury v3和ETH3D基准测试中建立了新的SOTA，在大多数指标上显著优于现有方法，同时以具有竞争力的效率重建高质量细节。", "conclusion": "$S^2M^2$ 成功地解决了一个困境，即在不依赖成本量滤波或深度精炼堆栈的情况下，实现了一种既具有最先进精度又具有高效率的全局匹配架构。", "translation": "追求一种能够在不同分辨率和视差范围内执行而无需特定数据集微调的通用立体匹配模型，揭示了一个根本性的权衡。迭代局部搜索方法在受限基准测试中取得了高分，但其核心机制固有地限制了真正泛化所需的全局一致性。另一方面，全局匹配架构虽然理论上更稳健，但历史上因高昂的计算和内存成本而变得不可行。我们通过 $S^2M^2$ 解决了这一困境：$S^2M^2$ 是一种全局匹配架构，无需依赖成本量滤波或深度精炼堆栈即可实现最先进的精度和高效率。我们的设计集成了多分辨率Transformer以实现鲁棒的长程对应，并采用一种将概率集中在可行匹配上的新型损失函数进行训练。这种方法能够更鲁棒地联合估计视差、遮挡和置信度。$S^2M^2$ 在Middlebury v3和ETH3D基准测试中建立了新的最先进水平，在大多数指标上显著优于现有方法，同时以具有竞争力的效率重建高质量细节。", "summary": "$S^2M^2$ 提出了一种可扩展的全局立体匹配模型，旨在解决传统方法在泛化能力和计算成本之间的权衡。该模型结合了多分辨率Transformer和新颖的损失函数，实现了对视差、遮挡和置信度的鲁棒联合估计，无需复杂的后处理。实验结果表明，$S^2M^2$ 在Middlebury v3和ETH3D基准测试上达到了最先进的性能，并在保持高效率的同时提供了高质量的深度估计。", "keywords": "立体匹配, 深度估计, 全局匹配, Transformer, 泛化", "comments": "$S^2M^2$ 的创新之处在于它成功地结合了全局匹配架构的鲁棒性与高效率，通过引入多分辨率Transformer和新颖的损失函数，避免了传统全局方法高昂的计算成本以及局部方法的泛化限制。这对于需要跨不同场景和分辨率进行深度估计的应用具有重要意义，因为它减少了对数据集特定微调的依赖。"}}
{"id": "2507.19366", "title": "Edge-weighted Matching in the Dark", "authors": ["Zhiyi Huang", "Enze Sun", "Xiaowei Wu", "Jiahao Zhao"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19366v1", "summary": "We present a $0.659$-competitive Quadratic Ranking algorithm for the\nOblivious Bipartite Matching problem, a distribution-free version of\nQuery-Commit Matching. This result breaks the $1-\\frac{1}{e}$ barrier,\naddressing an open question raised by Tang, Wu, and Zhang (JACM 2023).\nMoreover, the competitive ratio of this distribution-free algorithm improves\nthe best existing $0.641$ ratio for Query-Commit Matching achieved by the\ndistribution-dependent algorithm of Chen, Huang, Li, and Tang (SODA 2025).\n  Quadratic Ranking is a novel variant of the classic Ranking algorithm. We\nparameterize the algorithm with two functions, and let two key expressions in\nthe definition and analysis of the algorithm be quadratic forms of the two\nfunctions. We show that the quadratic forms are the unique choices that satisfy\na set of natural properties. Further, they allow us to optimize the choice of\nthe two functions using powerful quadratic programming solvers.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19366v1", "cate": "cs.DS", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "黑暗中的边加权匹配", "tldr": "本文提出了一个0.659竞争比的二次排序算法，用于解决盲二分匹配问题，打破了1-1/e的壁垒，并改进了现有算法的竞争比。", "motivation": "本文旨在解决Tang, Wu, and Zhang (JACM 2023)提出的开放问题，并改进现有Query-Commit匹配问题中最佳的0.641竞争比。", "method": "本文提出了一种名为“二次排序”（Quadratic Ranking）的新型算法，它是经典排序算法的一种变体。该算法通过两个函数进行参数化，并将其定义和分析中的两个关键表达式设为这两个函数的二次形式。研究表明，这些二次形式是满足一组自然性质的唯一选择，并且允许使用强大的二次规划求解器来优化这两个函数的选择。", "result": "本文提出了一个针对盲二分匹配问题的0.659竞争比的二次排序算法，这是一个与分布无关的Query-Commit匹配版本。这一结果打破了1-1/e的壁垒，并且该算法的竞争比优于Chen, Huang, Li, and Tang (SODA 2025)提出的、依赖于分布的Query-Commit匹配算法的0.641最佳竞争比。", "conclusion": "本文提出的二次排序算法在盲二分匹配问题上取得了显著的竞争比提升，成功打破了1-1/e的壁垒，并超越了现有最佳的依赖于分布的算法性能，证明了其独特且优越的性质。", "translation": "我们提出了一个针对盲二分匹配问题的0.659竞争比二次排序算法，这是Query-Commit匹配的一个与分布无关的版本。这一结果打破了1-1/e的壁垒，解决了Tang, Wu, and Zhang (JACM 2023)提出的一个开放问题。此外，这个与分布无关的算法的竞争比，改进了Chen, Huang, Li, and Tang (SODA 2025)提出的、依赖于分布的算法在Query-Commit匹配问题上达到的0.641最佳比率。二次排序是经典排序算法的一种新颖变体。我们用两个函数对算法进行参数化，并让算法定义和分析中的两个关键表达式成为这两个函数的二次形式。我们证明了二次形式是满足一组自然性质的唯一选择。此外，它们允许我们使用强大的二次规划求解器来优化这两个函数的选择。", "summary": "本文提出了一种新颖的二次排序算法，用于解决盲二分匹配问题。该算法达到了0.659的竞争比，成功打破了长期存在的1-1/e壁垒，并超越了目前Query-Commit匹配问题中依赖于分布的最佳算法（0.641竞争比）。该算法通过参数化和利用二次形式进行优化，解决了相关领域的开放问题。", "keywords": "二次排序, 盲二分匹配, 竞争比, 1-1/e壁垒, 算法优化", "comments": "本文的创新之处在于提出了“二次排序”这一新颖的算法变体，并通过将其关键表达式设计为二次形式，实现了算法的参数化和优化。其重要性体现在成功打破了Query-Commit匹配问题中的1-1/e竞争比壁垒，并显著提升了与分布无关算法的性能，解决了该领域的一个开放问题。"}}
{"id": "2407.02075", "title": "Label Anything: Multi-Class Few-Shot Semantic Segmentation with Visual Prompts", "authors": ["Pasquale De Marinis", "Nicola Fanelli", "Raffaele Scaringi", "Emanuele Colonna", "Giuseppe Fiameni", "Gennaro Vessio", "Giovanna Castellano"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ECAI 2025 - 28th European Conference on Artificial Intelligence", "url": "http://arxiv.org/abs/2407.02075v3", "summary": "Few-shot semantic segmentation aims to segment objects from previously unseen\nclasses using only a limited number of labeled examples. In this paper, we\nintroduce Label Anything, a novel transformer-based architecture designed for\nmulti-prompt, multi-way few-shot semantic segmentation. Our approach leverages\ndiverse visual prompts -- points, bounding boxes, and masks -- to create a\nhighly flexible and generalizable framework that significantly reduces\nannotation burden while maintaining high accuracy. Label Anything makes three\nkey contributions: ($\\textit{i}$) we introduce a new task formulation that\nrelaxes conventional few-shot segmentation constraints by supporting various\ntypes of prompts, multi-class classification, and enabling multiple prompts\nwithin a single image; ($\\textit{ii}$) we propose a novel architecture based on\ntransformers and attention mechanisms; and ($\\textit{iii}$) we design a\nversatile training procedure allowing our model to operate seamlessly across\ndifferent $N$-way $K$-shot and prompt-type configurations with a single trained\nmodel. Our extensive experimental evaluation on the widely used COCO-$20^i$\nbenchmark demonstrates that Label Anything achieves state-of-the-art\nperformance among existing multi-way few-shot segmentation methods, while\nsignificantly outperforming leading single-class models when evaluated in\nmulti-class settings. Code and trained models are available at\nhttps://github.com/pasqualedem/LabelAnything.", "comment": "ECAI 2025 - 28th European Conference on Artificial Intelligence", "pdf_url": "http://arxiv.org/pdf/2407.02075v3", "cate": "cs.CV", "date": "2024-07-02", "updated": "2025-07-25", "AI": {"title_translation": "Label Anything: 具有视觉提示的多类别小样本语义分割", "tldr": "Label Anything 提出了一种基于 Transformer 的新型架构，利用点、边界框和掩码等多种视觉提示，实现了多类别小样本语义分割的领先性能，显著减少了标注负担。", "motivation": "小样本语义分割旨在利用有限的标注样本分割未见类别的对象。本研究的动机是开发一个灵活且可泛化的框架，以显著减少标注负担，同时保持高精度，从而解决多类别小样本小样本语义分割的挑战。", "method": "本文引入了 Label Anything，一种新颖的基于 Transformer 的架构，用于多提示、多类别小样本语义分割。该方法利用点、边界框和掩码等多种视觉提示，并基于 Transformer 和注意力机制构建。它还设计了一种通用的训练过程，使模型能够在不同的 N-way K-shot 和提示类型配置下无缝运行。", "result": "在广泛使用的 COCO-20i 基准测试中，Label Anything 在现有的多类别小样本分割方法中取得了最先进的性能，并且在多类别设置下显著优于领先的单类别模型。", "conclusion": "Label Anything 提出了一种新颖的任务公式和基于 Transformer 的架构，利用多种视觉提示实现了多类别小样本语义分割的领先性能。该模型高度灵活、可泛化，并能有效减少标注负担，证明了其在实际应用中的潜力。", "translation": "小样本语义分割旨在利用有限的标注样本从以前未见过的类别中分割出对象。在本文中，我们引入了 Label Anything，这是一种新颖的基于 Transformer 的架构，专为多提示、多类别小样本语义分割而设计。我们的方法利用多样化的视觉提示——点、边界框和掩码——来创建一个高度灵活和可泛化的框架，该框架显著减少了标注负担，同时保持了高精度。Label Anything 贡献了三个关键点：(i) 我们引入了一种新的任务公式，通过支持各种类型的提示、多类别分类以及在单个图像中启用多个提示来放宽传统的小样本分割限制；(ii) 我们提出了一种基于 Transformer 和注意力机制的新颖架构；(iii) 我们设计了一种通用的训练过程，允许我们的模型在单个训练模型下无缝地在不同的 N-way K-shot 和提示类型配置中操作。我们在广泛使用的 COCO-20i 基准测试上进行了广泛的实验评估，结果表明 Label Anything 在现有的多类别小样本分割方法中取得了最先进的性能，同时在多类别设置下显著优于领先的单类别模型。代码和训练模型可在 https://github.com/pasqualedem/LabelAnything 获取。", "summary": "本文提出了 Label Anything，一个创新的基于 Transformer 的架构，用于多提示、多类别小样本语义分割。它通过利用点、边界框和掩码等多种视觉提示，并提出新的任务公式和训练过程，显著减少了标注工作量，同时在 COCO-20i 基准测试上实现了最先进的性能，在多类别设置下表现优于现有方法。", "keywords": "小样本语义分割, 视觉提示, Transformer, 多类别, Label Anything", "comments": "Label Anything 的创新之处在于其独特的多提示、多类别小样本分割任务公式，这显著扩展了传统小样本分割的适用范围。其基于 Transformer 和注意力机制的架构，结合支持多种视觉提示的灵活性，使其在减少标注负担的同时实现了卓越的性能。该工作对于推动小样本学习在实际应用中的部署具有重要意义。"}}
{"id": "2507.19364", "title": "Integrating LLM in Agent-Based Social Simulation: Opportunities and Challenges", "authors": ["Patrick Taillandier", "Jean Daniel Zucker", "Arnaud Grignard", "Benoit Gaudou", "Nghi Quang Huynh", "Alexis Drogoul"], "categories": ["cs.AI", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19364v1", "summary": "This position paper examines the use of Large Language Models (LLMs) in\nsocial simulation, analyzing both their potential and their limitations from a\ncomputational social science perspective. The first part reviews recent\nfindings on the ability of LLMs to replicate key aspects of human cognition,\nincluding Theory of Mind reasoning and social inference, while also\nhighlighting significant limitations such as cognitive biases, lack of true\nunderstanding, and inconsistencies in behavior. The second part surveys\nemerging applications of LLMs in multi-agent simulation frameworks, focusing on\nsystem architectures, scale, and validation strategies. Notable projects such\nas Generative Agents (Smallville) and AgentSociety are discussed in terms of\ntheir design choices, empirical grounding, and methodological innovations.\nParticular attention is given to the challenges of behavioral fidelity,\ncalibration, and reproducibility in large-scale LLM-driven simulations. The\nfinal section distinguishes between contexts where LLMs, like other black-box\nsystems, offer direct value-such as interactive simulations and serious\ngames-and those where their use is more problematic, notably in explanatory or\npredictive modeling. The paper concludes by advocating for hybrid approaches\nthat integrate LLMs into traditional agent-based modeling platforms (GAMA,\nNetlogo, etc), enabling modelers to combine the expressive flexibility of\nlanguage-based reasoning with the transparency and analytical rigor of\nclassical rule-based systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19364v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "将大型语言模型集成到基于代理的社会模拟中：机遇与挑战", "tldr": "本立场论文探讨了大型语言模型（LLM）在社会模拟中的应用，分析了其潜力与局限性，并提出将LLM与传统代理建模平台结合的混合方法。", "motivation": "本论文的动机是从计算社会科学的角度审视大型语言模型（LLM）在社会模拟中的应用，分析其潜力与局限性。", "method": "本文首先回顾了LLM在复制人类认知方面的能力及局限性（如认知偏差、缺乏真正理解）；其次，调查了LLM在多代理模拟框架中的新兴应用，讨论了Generative Agents和AgentSociety等项目的设计选择、经验基础和方法创新，并特别关注了行为保真度、校准和可重复性等挑战；最后，区分了LLM有直接价值（如交互式模拟、严肃游戏）和存在问题（如解释性或预测性建模）的上下文。", "result": "LLM在复制人类认知方面展现出潜力（如心智理论推理、社会推理），但也存在显著局限性（如认知偏差、缺乏真正理解、行为不一致）。在多代理模拟中，存在行为保真度、校准和大规模LLM驱动模拟的可重复性等挑战。LLM在交互式模拟和严肃游戏中具有直接价值，但在解释性或预测性建模中应用更为复杂。", "conclusion": "论文主张采用混合方法，将大型语言模型集成到传统的基于代理的建模平台（如GAMA, Netlogo）中，以结合基于语言推理的表达灵活性与经典基于规则系统的透明度和分析严谨性。", "translation": "这篇立场论文从计算社会科学的角度审视了大型语言模型（LLM）在社会模拟中的应用，分析了它们的潜力与局限性。第一部分回顾了LLM在复制人类认知关键方面（包括心智理论推理和社会推理）的最新发现，同时也强调了其显著局限性，如认知偏差、缺乏真正的理解以及行为不一致。第二部分调查了LLM在多代理模拟框架中的新兴应用，重点关注系统架构、规模和验证策略。讨论了Generative Agents (Smallville) 和 AgentSociety 等著名项目的设计选择、经验基础和方法创新。特别关注了大规模LLM驱动模拟中行为保真度、校准和可重复性等挑战。最后一部分区分了LLM（像其他黑盒系统一样）提供直接价值的语境——例如交互式模拟和严肃游戏——以及其使用更具问题性的语境，尤其是在解释性或预测性建模中。论文最后倡导采用混合方法，将LLM集成到传统的基于代理的建模平台（GAMA, Netlogo等）中，使建模者能够将基于语言推理的表达灵活性与经典基于规则系统的透明度和分析严谨性相结合。", "summary": "这篇立场论文深入探讨了大型语言模型（LLM）在社会模拟中的应用，从计算社会科学视角分析了其潜力与局限性。论文首先审视了LLM复制人类认知的能力及固有缺陷，随后考察了LLM在多代理模拟中的新兴应用，并指出其在行为保真度、校准和可重复性方面面临的挑战。最后，论文区分了LLM的适用场景，并提出了将LLM与传统代理建模平台结合的混合方法，以兼顾语言推理的灵活性与传统系统的透明度和严谨性。", "keywords": "大型语言模型, 社会模拟, 代理模型, 计算社会科学, 混合方法", "comments": "这篇论文对于理解大型语言模型在社会模拟领域的应用前景和挑战具有重要指导意义。其创新之处在于系统性地梳理了LLM的认知能力和局限性，并结合具体项目案例分析了LLM驱动的多代理模拟的实践问题。论文的价值在于提出了混合方法论，为未来的研究指明了方向，即如何有效整合LLM的表达能力与传统模型的分析严谨性，这对于推动计算社会科学的发展至关重要。"}}
{"id": "2507.19234", "title": "Virne: A Comprehensive Benchmark for Deep RL-based Network Resource Allocation in NFV", "authors": ["Tianfu Wang", "Liwei Deng", "Xi Chen", "Junyang Wang", "Huiguo He", "Leilei Ding", "Wei Wu", "Qilin Fan", "Hui Xiong"], "categories": ["cs.NI", "cs.AI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19234v1", "summary": "Resource allocation (RA) is critical to efficient service deployment in\nNetwork Function Virtualization (NFV), a transformative networking paradigm.\nRecently, deep Reinforcement Learning (RL)-based methods have been showing\npromising potential to address this complexity. However, the lack of a\nsystematic benchmarking framework and thorough analysis hinders the exploration\nof emerging networks and the development of more robust algorithms while\ncausing inconsistent evaluation. In this paper, we introduce Virne, a\ncomprehensive benchmarking framework for the NFV-RA problem, with a focus on\nsupporting deep RL-based methods. Virne provides customizable simulations for\ndiverse network scenarios, including cloud, edge, and 5G environments. It also\nfeatures a modular and extensible implementation pipeline that supports over 30\nmethods of various types, and includes practical evaluation perspectives beyond\neffectiveness, such as scalability, generalization, and scalability.\nFurthermore, we conduct in-depth analysis through extensive experiments to\nprovide valuable insights into performance trade-offs for efficient\nimplementation and offer actionable guidance for future research directions.\nOverall, with its diverse simulations, rich implementations, and extensive\nevaluation capabilities, Virne could serve as a comprehensive benchmark for\nadvancing NFV-RA methods and deep RL applications. The code is publicly\navailable at https://github.com/GeminiLight/virne.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19234v1", "cate": "cs.NI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "Virne: 一个用于基于深度强化学习的NFV网络资源分配的综合基准", "tldr": "Virne是一个综合性基准框架，旨在解决NFV网络资源分配中基于深度强化学习方法的评估和开发不足的问题。", "motivation": "现有的深度强化学习方法在网络功能虚拟化(NFV)中的资源分配(RA)方面显示出潜力，但缺乏系统性的基准测试框架和彻底的分析阻碍了新兴网络的探索和更稳健算法的开发，并导致评估不一致。", "method": "本文提出了Virne，一个针对NFV-RA问题的综合基准测试框架，专注于支持基于深度强化学习的方法。Virne提供可定制的模拟以适应云、边缘和5G等多样化网络场景。它还具有模块化和可扩展的实现管道，支持超过30种不同类型的方法，并包含除了效率之外的实际评估视角，如可扩展性和泛化性。", "result": "通过广泛的实验进行了深入分析，为高效实现提供了有价值的性能权衡见解，并为未来的研究方向提供了可操作的指导。", "conclusion": "Virne凭借其多样的模拟、丰富的实现和广泛的评估能力，可以作为推动NFV-RA方法和深度强化学习应用的综合基准。", "translation": "资源分配（RA）对于网络功能虚拟化（NFV）中高效的服务部署至关重要，NFV是一种变革性的网络范式。最近，基于深度强化学习（RL）的方法在解决这种复杂性方面显示出有前景的潜力。然而，缺乏系统性的基准测试框架和彻底的分析阻碍了新兴网络的探索和更稳健算法的开发，同时导致评估不一致。在本文中，我们介绍了Virne，一个针对NFV-RA问题的综合基准测试框架，重点支持基于深度RL的方法。Virne为多样化的网络场景提供可定制的模拟，包括云、边缘和5G环境。它还具有模块化和可扩展的实现管道，支持超过30种不同类型的方法，并包括除了有效性之外的实际评估视角，例如可扩展性、泛化性。此外，我们通过广泛的实验进行了深入分析，为高效实现提供了有价值的性能权衡见解，并为未来的研究方向提供了可操作的指导。总的来说，凭借其多样化的模拟、丰富的实现和广泛的评估能力，Virne可以作为推动NFV-RA方法和深度RL应用的综合基准。代码已在https://github.com/GeminiLight/virne公开。", "summary": "本文介绍了Virne，一个全面的基准测试框架，用于解决网络功能虚拟化（NFV）中基于深度强化学习的资源分配（RA）问题。Virne提供可定制的模拟环境，支持多种网络场景，并集成了模块化和可扩展的管道，兼容超过30种方法。该框架不仅关注算法的有效性，还考虑了可扩展性和泛化能力等实用评估指标。通过广泛的实验和深入分析，Virne旨在为NFV-RA的未来研究提供有价值的见解和指导，促进深度强化学习在该领域的应用和发展。", "keywords": "NFV, 资源分配, 深度强化学习, 基准测试, Virne", "comments": "Virne的创新之处在于它提供了一个急需的、全面的基准测试框架，解决了基于深度强化学习的NFV资源分配领域中评估不一致和缺乏系统性分析的问题。其模块化和可扩展的设计，以及对多样化网络场景和多方面评估指标的支持，使其成为推动该领域研究的重要工具。该框架的贡献在于标准化了性能评估，为研究人员提供了比较和开发新算法的平台，从而加速了NFV中深度强化学习方法的进步。"}}
{"id": "2507.19334", "title": "Doubling Your Data in Minutes: Ultra-fast Tabular Data Generation via LLM-Induced Dependency Graphs", "authors": ["Shuo Yang", "Zheyu Zhang", "Bardh Prenkaj", "Gjergji Kasneci"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19334v1", "summary": "Tabular data is critical across diverse domains, yet high-quality datasets\nremain scarce due to privacy concerns and the cost of collection. Contemporary\napproaches adopt large language models (LLMs) for tabular augmentation, but\nexhibit two major limitations: (1) dense dependency modeling among tabular\nfeatures that can introduce bias, and (2) high computational overhead in\nsampling. To address these issues, we propose SPADA for SPArse\nDependency-driven Augmentation, a lightweight generative framework that\nexplicitly captures sparse dependencies via an LLM-induced graph. We treat each\nfeature as a node and synthesize values by traversing the graph, conditioning\neach feature solely on its parent nodes. We explore two synthesis strategies: a\nnon-parametric method using Gaussian kernel density estimation, and a\nconditional normalizing flow model that learns invertible mappings for\nconditional density estimation. Experiments on four datasets show that SPADA\nreduces constraint violations by 4% compared to diffusion-based methods and\naccelerates generation by nearly 9,500 times over LLM-based baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19334v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "在数分钟内数据量翻倍：通过LLM诱导依赖图实现超快速表格数据生成", "tldr": "本文提出SPADA框架，通过LLM诱导的稀疏依赖图，解决了现有LLM表格数据增强方法中密集依赖建模偏差和高计算开销的问题，实现了超快速且高质量的表格数据生成。", "motivation": "高质量表格数据集稀缺，现有大型语言模型（LLM）用于表格数据增强的方法存在两个主要限制：1）表格特征间密集依赖建模可能引入偏差；2）采样计算开销高。", "method": "提出SPADA（SPArse Dependency-driven Augmentation）轻量级生成框架，通过LLM诱导的图显式捕获稀疏依赖。该方法将每个特征视为节点，并通过遍历图来合成值，每个特征仅以其父节点为条件。探索了两种合成策略：使用高斯核密度估计的非参数方法，以及学习可逆映射的条件归一化流模型。", "result": "在四个数据集上的实验表明，与基于扩散的方法相比，SPADA将约束违反减少了4%；与基于LLM的基线相比，生成速度加快了近9,500倍。", "conclusion": "SPADA通过引入LLM诱导的稀疏依赖图，有效解决了现有LLM表格数据增强方法中的偏差和计算开销问题，实现了超快速且高质量的表格数据生成。", "translation": "表格数据在各个领域都至关重要，但由于隐私问题和收集成本，高质量数据集仍然稀缺。当代方法采用大型语言模型（LLM）进行表格增强，但存在两个主要限制：（1）表格特征之间密集的依赖建模可能引入偏差，以及（2）采样计算开销高。为了解决这些问题，我们提出了SPADA，一种稀疏依赖驱动的增强框架，它通过LLM诱导的图显式捕获稀疏依赖。我们将每个特征视为一个节点，并通过遍历图来合成值，每个特征仅以其父节点为条件。我们探索了两种合成策略：一种使用高斯核密度估计的非参数方法，以及一种学习可逆映射以进行条件密度估计的条件归一化流模型。在四个数据集上的实验表明，与基于扩散的方法相比，SPADA将约束违反减少了4%，并且比基于LLM的基线加速生成了近9,500倍。", "summary": "针对高质量表格数据稀缺以及现有LLM表格增强方法中密集依赖建模偏差和高计算开销的问题，本文提出了SPADA（SPArse Dependency-driven Augmentation）框架。SPADA利用LLM诱导的稀疏依赖图来捕获特征间的稀疏依赖，并通过遍历图、仅基于父节点条件合成值。该框架支持非参数（高斯核密度估计）和条件归一化流两种合成策略。实验结果显示，SPADA不仅将约束违反减少了4%，还实现了比LLM基线快9,500倍的生成速度。", "keywords": "表格数据生成, LLM, 依赖图, 数据增强, SPADA", "comments": "SPADA的创新之处在于利用LLM生成稀疏依赖图，有效解决了现有LLM表格数据增强中密集依赖建模引入的偏差和高昂的计算开销。其在生成速度上达到近9,500倍的显著提升，表明了其在实际应用中的巨大潜力，对于需要快速生成大量高质量表格数据的场景具有重要意义。"}}
{"id": "2507.18884", "title": "MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service", "authors": ["Ming Gong", "Xucheng Huang", "Ziheng Xu", "Vijayan K. Asari"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18884v1", "summary": "High-quality dialogue is crucial for e-commerce customer service, yet\ntraditional intent-based systems struggle with dynamic, multi-turn\ninteractions. We present MindFlow+, a self-evolving dialogue agent that learns\ndomain-specific behavior by combining large language models (LLMs) with\nimitation learning and offline reinforcement learning (RL). MindFlow+\nintroduces two data-centric mechanisms to guide learning: tool-augmented\ndemonstration construction, which exposes the model to knowledge-enhanced and\nagentic (ReAct-style) interactions for effective tool use; and\nreward-conditioned data modeling, which aligns responses with task-specific\ngoals using reward signals. To evaluate the model's role in response\ngeneration, we introduce the AI Contribution Ratio, a novel metric quantifying\nAI involvement in dialogue. Experiments on real-world e-commerce conversations\nshow that MindFlow+ outperforms strong baselines in contextual relevance,\nflexibility, and task accuracy. These results demonstrate the potential of\ncombining LLMs tool reasoning, and reward-guided learning to build\ndomain-specialized, context-aware dialogue systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18884v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "MindFlow+：一种用于电子商务客户服务的自进化智能体", "tldr": "MindFlow+是一个结合了LLM、模仿学习和离线强化学习的自进化对话智能体，通过工具增强演示和奖励条件数据建模，显著提升了电商客服对话的质量。", "motivation": "传统基于意图的系统难以处理动态、多轮的对话，而高质量对话对电子商务客户服务至关重要。本文旨在解决这一挑战，开发一个能够处理复杂交互的智能对话系统。", "method": "MindFlow+结合了大型语言模型（LLM）、模仿学习和离线强化学习。它引入了两种数据中心机制：1) 工具增强演示构建，使模型接触到知识增强和智能体式（ReAct风格）交互，以有效使用工具；2) 奖励条件数据建模，利用奖励信号使响应与任务特定目标对齐。此外，还引入了AI贡献率（AI Contribution Ratio）这一新指标来量化AI在对话中的参与度。", "result": "在真实世界电子商务对话中的实验表明，MindFlow+在上下文相关性、灵活性和任务准确性方面优于强大的基线模型。", "conclusion": "结合LLM的工具推理能力和奖励引导学习，能够构建领域专业化、上下文感知的对话系统。", "translation": "高质量对话对于电子商务客户服务至关重要，然而传统的基于意图的系统难以处理动态、多轮的交互。我们提出了MindFlow+，一个自进化的对话智能体，它通过将大型语言模型（LLMs）与模仿学习和离线强化学习相结合，学习领域特定的行为。MindFlow+引入了两种以数据为中心的机制来指导学习：工具增强演示构建，它使模型接触到知识增强和智能体式（ReAct风格）的交互，以有效利用工具；以及奖励条件数据建模，它利用奖励信号使响应与任务特定目标对齐。为了评估模型在响应生成中的作用，我们引入了AI贡献率，这是一个量化AI在对话中参与度的新指标。在真实世界电子商务对话中的实验表明，MindFlow+在上下文相关性、灵活性和任务准确性方面优于强大的基线模型。这些结果证明了结合LLM工具推理和奖励引导学习来构建领域专业化、上下文感知对话系统的潜力。", "summary": "MindFlow+是一个为电子商务客户服务设计的自进化对话智能体，旨在解决传统系统在处理动态多轮对话中的不足。它融合了大型语言模型、模仿学习和离线强化学习，并通过工具增强演示构建和奖励条件数据建模来优化学习过程。此外，该研究提出了AI贡献率以衡量AI在对话中的参与度。实验结果显示，MindFlow+在上下文相关性、灵活性和任务准确性方面均优于现有基线，证明了其在构建领域特定、上下文感知对话系统方面的潜力。", "keywords": "对话智能体, 大型语言模型, 强化学习, 电子商务, 客户服务", "comments": "MindFlow+的创新之处在于其结合了LLM、模仿学习和离线强化学习，并通过数据中心机制（工具增强演示和奖励条件数据建模）来优化对话智能体的学习和行为。引入AI贡献率是一个有价值的新指标，有助于量化和理解AI在对话中的实际作用。该研究为构建更智能、更灵活的领域专用对话系统提供了新的方向和有效方法。"}}
{"id": "2412.12591", "title": "LLMs are Also Effective Embedding Models: An In-depth Overview", "authors": ["Chongyang Tao", "Tao Shen", "Shen Gao", "Junshuo Zhang", "Zhen Li", "Kai Hua", "Wenpeng Hu", "Zhengwei Tao", "Shuai Ma"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      38 pages", "url": "http://arxiv.org/abs/2412.12591v2", "summary": "Large language models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art performance across various tasks. Recently, their\neffectiveness as embedding models has gained attention, marking a paradigm\nshift from traditional encoder-only models like ELMo and BERT to decoder-only,\nlarge-scale LLMs such as GPT, LLaMA, and Mistral. This survey provides an\nin-depth overview of this transition, beginning with foundational techniques\nbefore the LLM era, followed by LLM-based embedding models through two main\nstrategies to derive embeddings from LLMs. 1) Direct prompting: We mainly\ndiscuss the prompt designs and the underlying rationale for deriving\ncompetitive embeddings. 2) Data-centric tuning: We cover extensive aspects that\naffect tuning an embedding model, including model architecture, training\nobjectives, data constructions, etc. Upon the above, we also cover advanced\nmethods for producing embeddings from longer texts, multilingual, code,\ncross-modal data, as well as reasoning-aware and other domain-specific\nscenarios. Furthermore, we discuss factors affecting choices of embedding\nmodels, such as performance/efficiency comparisons, dense vs sparse embeddings,\npooling strategies, and scaling law. Lastly, the survey highlights the\nlimitations and challenges in adapting LLMs for embeddings, including\ncross-task embedding quality, trade-offs between efficiency and accuracy,\nlow-resource, long-context, data bias, robustness, etc. This survey serves as a\nvaluable resource for researchers and practitioners by synthesizing current\nadvancements, highlighting key challenges, and offering a comprehensive\nframework for future work aimed at enhancing the effectiveness and efficiency\nof LLMs as embedding models.", "comment": "38 pages", "pdf_url": "http://arxiv.org/pdf/2412.12591v2", "cate": "cs.CL", "date": "2024-12-17", "updated": "2025-07-25", "AI": {"title_translation": "LLMs也是有效的嵌入模型：深度综述", "tldr": "本综述深入探讨了大型语言模型（LLMs）作为嵌入模型的转变、技术、挑战和未来方向。", "motivation": "大型语言模型（LLMs）作为嵌入模型的有效性日益受到关注，这标志着从传统编码器模型到基于解码器的LLMs的范式转变。本综述旨在对这一转变提供深入的概述。", "method": "本综述首先回顾了LLM时代之前的基本技术，然后详细介绍了基于LLMs的嵌入模型，主要通过两种策略：1) 直接提示，讨论了竞争性嵌入的提示设计和基本原理；2) 数据中心调优，涵盖了影响嵌入模型调优的广泛方面，如模型架构、训练目标和数据构建。此外，综述还涵盖了从更长文本、多语言、代码、跨模态数据以及推理感知和其他领域特定场景中生成嵌入的先进方法。文中还讨论了影响嵌入模型选择的因素，如性能/效率比较、密集与稀疏嵌入、池化策略和缩放定律。最后，综述强调了LLMs在适应嵌入模型时的局限性和挑战，包括跨任务嵌入质量、效率与准确性之间的权衡、低资源、长上下文、数据偏差和鲁棒性等。", "result": "本综述通过综合当前进展、强调关键挑战并为未来工作提供全面的框架，为研究人员和从业者提供了一个宝贵的资源，旨在增强LLMs作为嵌入模型的有效性和效率。", "conclusion": "本综述综合了当前进展，强调了关键挑战，并为未来的研究工作提供了一个全面的框架，以增强LLMs作为嵌入模型的有效性和效率。", "translation": "大型语言模型（LLMs）通过在各种任务中实现最先进的性能，彻底改变了自然语言处理。最近，它们作为嵌入模型的有效性引起了关注，标志着从ELMo和BERT等传统仅编码器模型到GPT、LLaMA和Mistral等仅解码器、大规模LLMs的范式转变。本综述对这一转变进行了深入概述，从LLM时代之前的基本技术开始，然后通过两种主要策略介绍基于LLM的嵌入模型，以从LLMs中导出嵌入。1) 直接提示：我们主要讨论了提示设计和导出具有竞争力嵌入的潜在原理。2) 数据中心调优：我们涵盖了影响嵌入模型调优的广泛方面，包括模型架构、训练目标、数据构建等。在此基础上，我们还涵盖了从更长文本、多语言、代码、跨模态数据以及推理感知和其他领域特定场景中生成嵌入的先进方法。此外，我们讨论了影响嵌入模型选择的因素，例如性能/效率比较、密集与稀疏嵌入、池化策略和缩放定律。最后，本综述强调了LLMs在适应嵌入模型时的局限性和挑战，包括跨任务嵌入质量、效率与准确性之间的权衡、低资源、长上下文、数据偏差、鲁棒性等。本综述通过综合当前进展、强调关键挑战并为未来工作提供全面的框架，为研究人员和从业者提供了宝贵的资源，旨在增强LLMs作为嵌入模型的有效性和效率。", "summary": "本文对大型语言模型（LLMs）作为嵌入模型的转变进行了深入综述。它首先回顾了LLM时代之前的技术，随后详细阐述了两种基于LLM的嵌入策略：直接提示和数据中心调优。综述还涵盖了处理多模态和特定领域数据的先进方法，并讨论了影响嵌入模型选择的各种因素，如性能、效率和池化策略。最后，文章指出了LLMs在嵌入应用中的局限性与挑战，旨在为研究人员和从业者提供一个全面的参考框架，以促进该领域未来的发展。", "keywords": "大型语言模型, 嵌入模型, 自然语言处理, 综述, 表示学习", "comments": "这篇综述论文的重要性在于它系统地梳理了LLMs从传统NLP模型向通用嵌入模型转变的整个过程，并详细介绍了实现这一转变的关键技术和策略。它不仅涵盖了基础理论，还探讨了实际应用中的高级方法和面临的挑战，为该领域的研究人员和从业者提供了宝贵的路线图和综合框架。"}}
{"id": "2507.18868", "title": "A Neuroscience-Inspired Dual-Process Model of Compositional Generalization", "authors": ["Alex Noviello", "Claas Beger", "Jacob Groner", "Kevin Ellis", "Weinan Sun"], "categories": ["cs.AI", "cs.NE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18868v1", "summary": "Systematic compositional generalization - constructing and understanding\nnovel combinations of known building blocks - remains a core challenge for AI\nsystems. Human cognition achieves this flexibility via the interplay of the\nhippocampus (HPC) and prefrontal cortex (PFC): the hippocampus rapidly encodes\nepisodes, and the prefrontal cortex consolidates them into reusable schemas for\nreasoning. Drawing on these insights, we present MIRAGE (Meta-Inference with\nRules and Abstractions from Generalized Experience), a framework that achieves\nsystematic generalization on compositional tasks. MIRAGE has two interacting\nmodules mirroring the brain's deliberative HPC-PFC loop and intuitive\nneocortical pattern recognition. (1) The meta-trained Transformer Neural\nDecomposer, paralleling neocortical \"System 1\" computation, is trained on a\ntask-agnostic stream of randomly sampled compositional grammars and applies one\ndecomposition step per pass, with successive passes iteratively refining the\nsequence representation. (2) The Schema Engine, analogous to the HPC-PFC\n\"System 2\" loop, dynamically extracts, ranks, and applies reusable schemas,\nstoring variable bindings in episodic memory and expanding them when needed. By\nexplicitly equipping the Transformer component of MIRAGE with actively managed\nschematic structures, our model performs systematic compositional operations\nthrough explicit schema application and transformation, relying solely on\nfrozen weights when solving entirely novel tasks. This approach demonstrates\nsystematic compositional generalization on the SCAN benchmark, achieving > 99%\naccuracy on all task splits with only 1.19M parameters in the transformer\nmodule. Ablation studies confirm that MIRAGE's systematicity critically depends\non the quality of extracted schemas and the model's iterative refinement\nprocess.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18868v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "受神经科学启发的组合泛化双过程模型", "tldr": "受神经科学启发的双过程AI模型MIRAGE，通过模仿大脑的海马体-前额叶皮层（HPC-PFC）循环和新皮层处理，在SCAN基准测试中实现了系统性的组合泛化。", "motivation": "系统性的组合泛化——构建和理解已知构建块的新颖组合——仍然是AI系统的核心挑战。人类认知通过海马体（HPC）和前额叶皮层（PFC）的相互作用实现这种灵活性。", "method": "本文提出了MIRAGE（基于泛化经验的规则和抽象元推理）框架，该框架包含两个交互模块，模仿大脑的HPC-PFC循环和直觉的新皮层模式识别。第一个模块是元训练的Transformer神经分解器（System 1），用于对任务无关的随机采样组合语法流进行训练，并通过连续的迭代细化序列表示。第二个模块是图式引擎（System 2），动态提取、排序和应用可重用图式，将变量绑定存储在情景记忆中并在需要时扩展。模型通过显式图式应用和转换执行系统性的组合操作，在解决全新任务时仅依赖冻结权重。", "result": "在SCAN基准测试中，该方法展示了系统性的组合泛化能力，在所有任务分割上均达到了99%以上的准确率，且Transformer模块仅有1.19M参数。消融研究证实，MIRAGE的系统性关键取决于提取图式的质量和模型的迭代细化过程。", "conclusion": "该研究展示了一个受神经科学启发的双过程模型，通过结合类似大脑的系统1和系统2处理，有效解决了AI系统中的组合泛化挑战，并在组合泛化任务上取得了显著性能。", "translation": "系统性的组合泛化——构建和理解已知构建块的新颖组合——仍然是AI系统的核心挑战。人类认知通过海马体（HPC）和前额叶皮层（PFC）的相互作用实现这种灵活性。借鉴这些见解，我们提出了MIRAGE（基于泛化经验的规则和抽象元推理）框架，该框架在组合任务上实现了系统性泛化。MIRAGE拥有两个交互模块，分别模仿大脑的深思熟虑的HPC-PFC循环和直觉的新皮层模式识别。(1) 元训练的Transformer神经分解器，与新皮层“系统1”计算并行，在任务无关的随机采样组合语法流上进行训练，每次通过应用一个分解步骤，连续通过迭代细化序列表示。(2) 图式引擎，类似于HPC-PFC“系统2”循环，动态提取、排序和应用可重用图式，将变量绑定存储在情景记忆中并在需要时扩展。通过明确地为MIRAGE的Transformer组件配备主动管理的图式结构，我们的模型通过显式图式应用和转换执行系统性的组合操作，在解决全新任务时仅依赖冻结权重。这种方法在SCAN基准测试上展示了系统性的组合泛化能力，在所有任务分割上均达到了99%以上的准确率，且Transformer模块仅有1.19M参数。消融研究证实，MIRAGE的系统性关键取决于提取图式的质量和模型的迭代细化过程。", "summary": "本文提出了MIRAGE框架，一个受神经科学启发的双过程模型，旨在解决AI系统中的组合泛化挑战。该模型模仿大脑的海马体-前额叶皮层循环和新皮层模式识别，包含一个元训练的Transformer神经分解器（System 1）和一个图式引擎（System 2）。MIRAGE通过显式应用和转换图式，在SCAN基准测试上实现了超过99%的准确率，证明了其在系统性组合泛化方面的有效性。", "keywords": "组合泛化, 双过程模型, 神经科学启发AI, 图式, Transformer", "comments": "该论文的创新之处在于将神经科学中的双过程理论（System 1/2）和大脑区域功能（HPC-PFC循环）直接融入到AI模型的架构中，以解决组合泛化这一核心挑战。通过显式管理图式结构和迭代细化过程，模型能够在仅依赖冻结权重的情况下处理全新任务，这在提高模型泛化能力方面具有重要意义。在SCAN基准测试上以相对较少的参数实现高精度，也凸显了其效率和潜力。"}}
{"id": "2507.18925", "title": "WiSE-OD: Benchmarking Robustness in Infrared Object Detection", "authors": ["Heitor R. Medeiros", "Atif Belal", "Masih Aminbeidokhti", "Eric Granger", "Marco Pedersoli"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, conference", "url": "http://arxiv.org/abs/2507.18925v1", "summary": "Object detection (OD) in infrared (IR) imagery is critical for low-light and\nnighttime applications. However, the scarcity of large-scale IR datasets forces\nmodels to rely on weights pre-trained on RGB images. While fine-tuning on IR\nimproves accuracy, it often compromises robustness under distribution shifts\ndue to the inherent modality gap between RGB and IR. To address this, we\nintroduce LLVIP-C and FLIR-C, two cross-modality out-of-distribution (OOD)\nbenchmarks built by applying corruption to standard IR datasets. Additionally,\nto fully leverage the complementary knowledge from RGB and infrared trained\nmodels, we propose WiSE-OD, a weight-space ensembling method with two variants:\nWiSE-OD$_{ZS}$, which combines RGB zero-shot and IR fine-tuned weights, and\nWiSE-OD$_{LP}$, which blends zero-shot and linear probing. Evaluated across\nthree RGB-pretrained detectors and two robust baselines, WiSE-OD improves both\ncross-modality and corruption robustness without any additional training or\ninference cost.", "comment": "8 pages, conference", "pdf_url": "http://arxiv.org/pdf/2507.18925v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "WiSE-OD：红外目标检测中的鲁棒性基准测试", "tldr": "本文引入了LLVIP-C和FLIR-C两个新的跨模态OOD基准数据集，并提出了WiSE-OD，一种权重空间集成方法，以提高红外目标检测在分布偏移下的鲁棒性，且无需额外训练或推理成本。", "motivation": "红外图像目标检测在低光和夜间应用中至关重要，但缺乏大规模红外数据集导致模型依赖RGB图像预训练权重。在红外数据上微调虽然能提高准确性，但由于RGB和红外之间的模态差异，往往会损害在分布偏移下的鲁棒性。", "method": "为解决鲁棒性问题，本文引入了LLVIP-C和FLIR-C，这两个通过对标准红外数据集应用损坏而构建的跨模态域外（OOD）基准。此外，为充分利用RGB和红外训练模型的互补知识，本文提出了WiSE-OD，一种权重空间集成方法，包含两种变体：WiSE-OD$_{ZS}$（结合RGB零样本和红外微调权重）和WiSE-OD$_{LP}$（融合零样本和线性探测）。", "result": "WiSE-OD在三种RGB预训练检测器和两种鲁棒基线上进行了评估，结果显示它在不增加任何额外训练或推理成本的情况下，提高了跨模态和损坏鲁棒性。", "conclusion": "本文通过引入新的跨模态OOD基准和提出高效的权重空间集成方法WiSE-OD，有效提升了红外目标检测在分布偏移下的鲁棒性，解决了现有方法在准确性和鲁棒性之间权衡的难题。", "translation": "红外（IR）图像中的目标检测（OD）对于微光和夜间应用至关重要。然而，大规模红外数据集的稀缺迫使模型依赖于RGB图像上预训练的权重。尽管在红外图像上进行微调可以提高准确性，但由于RGB和红外固有的模态差异，在分布偏移下往往会损害鲁棒性。为了解决这个问题，我们引入了LLVIP-C和FLIR-C，这是通过对标准红外数据集应用损坏而构建的两个跨模态域外（OOD）基准。此外，为了充分利用RGB和红外训练模型的互补知识，我们提出了WiSE-OD，一种权重空间集成方法，它有两种变体：WiSE-OD$_{ZS}$，结合了RGB零样本和红外微调权重；以及WiSE-OD$_{LP}$，融合了零样本和线性探测。WiSE-OD在三种RGB预训练检测器和两种鲁棒基线上进行了评估，结果显示它在不增加任何额外训练或推理成本的情况下，提高了跨模态和损坏鲁棒性。", "summary": "本文针对红外目标检测中因模态差异导致的鲁棒性下降问题，提出了两种新的跨模态域外（OOD）基准数据集LLVIP-C和FLIR-C。同时，为有效结合RGB和红外模型的知识，引入了WiSE-OD，一种无需额外训练或推理成本的权重空间集成方法。实验证明，WiSE-OD显著提升了模型在跨模态和损坏条件下的鲁棒性。", "keywords": "红外目标检测, 鲁棒性, 跨模态, 权重空间集成, OOD基准", "comments": "本文的创新点在于提出了新的跨模态OOD基准数据集，以及一种无需额外计算成本的权重空间集成方法WiSE-OD，有效解决了红外目标检测在模态差异下的鲁棒性问题。其重要性体现在为低光和夜间应用提供了更鲁棒的解决方案，并为未来的研究提供了新的基准和方法论。"}}
{"id": "2507.19059", "title": "Revisiting DETR for Small Object Detection via Noise-Resilient Query Optimization", "authors": ["Xiaocheng Fang", "Jieyi Cai", "Huanyu Liu", "Wenxiu Cai", "Yishu Liu", "Bingzhi Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      2025 IEEE International Conference on Multimedia and Expo (ICME)", "url": "http://arxiv.org/abs/2507.19059v1", "summary": "Despite advancements in Transformer-based detectors for small object\ndetection (SOD), recent studies show that these detectors still face challenges\ndue to inherent noise sensitivity in feature pyramid networks (FPN) and\ndiminished query quality in existing label assignment strategies. In this\npaper, we propose a novel Noise-Resilient Query Optimization (NRQO) paradigm,\nwhich innovatively incorporates the Noise-Tolerance Feature Pyramid Network\n(NT-FPN) and the Pairwise-Similarity Region Proposal Network (PS-RPN).\nSpecifically, NT-FPN mitigates noise during feature fusion in FPN by preserving\nspatial and semantic information integrity. Unlike existing label assignment\nstrategies, PS-RPN generates a sufficient number of high-quality positive\nqueries by enhancing anchor-ground truth matching through position and shape\nsimilarities, without the need for additional hyperparameters. Extensive\nexperiments on multiple benchmarks consistently demonstrate the superiority of\nNRQO over state-of-the-art baselines.", "comment": "2025 IEEE International Conference on Multimedia and Expo (ICME)", "pdf_url": "http://arxiv.org/pdf/2507.19059v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "通过噪声弹性查询优化重访DETR用于小目标检测", "tldr": "本文提出了噪声弹性查询优化（NRQO）范式，通过结合噪声容忍特征金字塔网络（NT-FPN）和成对相似度区域提议网络（PS-RPN），解决了基于Transformer的小目标检测器在FPN噪声敏感性和查询质量下降方面的挑战，并在多个基准测试中表现出优异性能。", "motivation": "尽管基于Transformer的检测器在小目标检测（SOD）方面取得了进展，但它们仍面临挑战，原因在于特征金字塔网络（FPN）固有的噪声敏感性以及现有标签分配策略中查询质量的下降。", "method": "本文提出了噪声弹性查询优化（NRQO）范式，它创新性地结合了噪声容忍特征金字塔网络（NT-FPN）和成对相似度区域提议网络（PS-RPN）。NT-FPN通过保持空间和语义信息完整性来减轻FPN在特征融合过程中的噪声。PS-RPN通过增强锚点-真值匹配，基于位置和形状相似性生成足够数量的高质量正查询，且无需额外超参数。", "result": "在多个基准测试中进行的大量实验一致表明，NRQO优于最先进的基线方法。", "conclusion": "通过引入噪声弹性查询优化（NRQO），包括NT-FPN和PS-RPN，可以有效解决基于Transformer的小目标检测器在FPN噪声敏感性和查询质量方面的挑战，从而显著提升小目标检测的性能。", "translation": "尽管基于Transformer的检测器在小目标检测（SOD）方面取得了进展，但最近的研究表明，由于特征金字塔网络（FPN）固有的噪声敏感性以及现有标签分配策略中查询质量的下降，这些检测器仍然面临挑战。在本文中，我们提出了一种新颖的噪声弹性查询优化（NRQO）范式，它创新性地结合了噪声容忍特征金字塔网络（NT-FPN）和成对相似度区域提议网络（PS-RPN）。具体而言，NT-FPN通过保持空间和语义信息的完整性来减轻FPN在特征融合过程中的噪声。与现有标签分配策略不同，PS-RPN通过增强锚点-真值匹配，基于位置和形状相似性生成足够数量的高质量正查询，且无需额外超参数。在多个基准测试中进行的大量实验一致表明，NRQO优于最先进的基线方法。", "summary": "本文针对基于Transformer的小目标检测器在FPN噪声敏感性和查询质量下降的挑战，提出了一种名为噪声弹性查询优化（NRQO）的新范式。NRQO包含两个核心组件：噪声容忍特征金字塔网络（NT-FPN）用于缓解特征融合中的噪声，以及成对相似度区域提议网络（PS-RPN）用于生成高质量的查询。实验结果表明，NRQO在多个基准测试中均优于现有先进方法。", "keywords": "小目标检测, Transformer, DETR, 噪声弹性查询优化, 特征金字塔网络", "comments": "该论文通过提出NRQO范式，创新性地解决了Transformer基检测器在小目标检测中面临的FPN噪声敏感性和查询质量下降的关键问题。NT-FPN和PS-RPN的结合，特别是PS-RPN无需额外超参数的设计，展现了其方法在实际应用中的潜力和鲁棒性。该研究对于提升小目标检测的性能具有重要意义。"}}
{"id": "2507.18899", "title": "Procedural city modeling", "authors": ["Thomas Lechner", "Ben Watson", "Uri Wilensky", "Martin Felsen"], "categories": ["cs.GR"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18899v1", "summary": "We propose a method to procedurally generate a familiar yet complex human\nartifact: the city. We are not trying to reproduce existing cities, but to\ngenerate artificial cities that are convincing and plausible by capturing\ndevelopmental behavior. In addition, our results are meant to build upon\nthemselves, such that they ought to look compelling at any point along the\ntransition from village to metropolis. Our approach largely focuses upon land\nusage and building distribution for creating realistic city environments,\nwhereas previous attempts at city modeling have mainly focused on populating\nroad networks. Finally, we want our model to be self automated to the point\nthat the only necessary input is a terrain description, but other high-level\nand low-level parameters can be specified to support artistic contributions.\nWith the aid of agent based simulation we are generating a system of agents and\nbehaviors that interact with one another through their effects upon a simulated\nenvironment. Our philosophy is that as each agent follows a simple behavioral\nrule set, a more complex behavior will tend to emerge out of the interactions\nbetween the agents and their differing rule sets. By confining our model to a\nset of simple rules for each class of agents, we hope to make our model\nextendible not only in regard to the types of structures that are produced, but\nalso in describing the social and cultural influences prevalent in all cities", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18899v1", "cate": "cs.GR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "程序化城市建模", "tldr": "提出了一种基于代理的程序化城市生成方法，通过关注土地使用和建筑分布，从村庄到大都市的任何阶段都能生成逼真且可信的城市，且仅需地形描述作为输入。", "motivation": "传统城市建模主要关注填充道路网络，而本文旨在通过捕捉发展行为来生成逼真且可信的人工城市，使其在从村庄到大都市的任何发展阶段都具有吸引力，并且实现高度自动化。", "method": "提出了一种程序化生成城市的方法。该方法主要关注土地使用和建筑分布，而不是像以往那样仅关注填充道路网络。通过代理基模拟，生成一个代理系统和行为，它们通过影响模拟环境相互作用。每个代理遵循简单的行为规则集，复杂的行为将从代理之间的交互中涌现。模型仅需地形描述作为必要输入，并支持高层和低层参数的指定以支持艺术贡献。", "result": "生成了令人信服且可信的人工城市，这些城市通过捕捉发展行为而产生。结果在从村庄到大都市的任何过渡点都显得引人注目。", "conclusion": "通过为每类代理设置简单的规则，该模型不仅在生成的结构类型方面具有可扩展性，而且在描述城市中普遍存在的社会和文化影响方面也具有可扩展性。", "translation": "我们提出了一种程序化生成熟悉而复杂的人类产物——城市的方法。我们不试图复制现有城市，而是通过捕捉发展行为来生成令人信服和可信的人工城市。此外，我们的结果旨在自我构建，使其在从村庄到大都市的任何过渡点都应显得引人注目。我们的方法主要侧重于土地使用和建筑分布以创建逼真的城市环境，而之前城市建模的尝试主要集中在填充道路网络。最后，我们希望我们的模型能够实现自动化，以至于唯一必要的输入是地形描述，但可以指定其他高级和低级参数以支持艺术贡献。借助基于代理的模拟，我们正在生成一个代理和行为系统，它们通过对模拟环境的影响相互作用。我们的理念是，随着每个代理遵循简单的行为规则集，更复杂的行为将倾向于从代理之间及其不同规则集之间的交互中涌现。通过将我们的模型限制为每类代理的一组简单规则，我们希望使我们的模型不仅在生成的结构类型方面具有可扩展性，而且在描述所有城市中普遍存在的社会和文化影响方面也具有可扩展性。", "summary": "本文提出了一种新颖的程序化城市建模方法，旨在生成逼真且可信的人工城市。与以往专注于道路网络填充不同，该方法侧重于土地使用和建筑分布，并利用基于代理的模拟。通过简单的代理行为规则，模型能够从地形描述自动生成复杂的城市，并在从村庄到大都市的任何发展阶段都保持视觉吸引力。该模型具有良好的可扩展性，不仅适用于结构类型，还可用于描述社会和文化影响。", "keywords": "程序化城市建模, 代理基模拟, 土地使用, 城市发展, 自动化生成", "comments": "该论文的创新之处在于其采用基于代理的模拟来程序化生成城市，并关注土地使用和建筑分布，而非仅仅填充道路网络。通过简单的规则涌现复杂行为的哲学，使得模型在实现高度自动化和可扩展性方面具有潜力，能够模拟城市发展并融入社会文化影响，这对虚拟环境、游戏开发和城市规划领域具有重要意义。"}}
{"id": "2409.04263", "title": "Spectral alignment of kernel matrices and applications", "authors": ["Tizan Wenzel", "Armin Iske"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2409.04263v2", "summary": "Kernel matrices are a key quantity in kernel-based approximation, and\nimportant properties such as stability and algorithmic convergence can be\nanalyzed with their help.\n  In this work we refine a multivariate Ingham-type theorem, which is then\nleveraged to obtain novel and refined stability estimates on kernel matrices.\nFor this, we focus on the case of finitely smooth kernels, such as the family\nof Mat\\'ern or Wendland kernels, while noting that the results also extend to\nnorm-equivalent kernels. In particular we obtain results that relate the\nRayleigh quotients of kernel matrices for kernels of different smoothness to\neach other. Finally we comment on conclusions for the eigenvectors of these\nkernel matrices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2409.04263v2", "cate": "math.NA", "date": "2024-09-06", "updated": "2025-07-25", "AI": {"title_translation": "核矩阵的谱对齐及其应用", "tldr": "本文通过改进多元Ingham型定理，为核矩阵提供了新颖且精炼的稳定性估计，并探讨了不同光滑度核矩阵的瑞利商之间的关系。", "motivation": "核矩阵是基于核的近似中的关键量，其稳定性及算法收敛性等重要特性可通过核矩阵进行分析。", "method": "本文改进了一个多元Ingham型定理，并将其应用于获取核矩阵的稳定性估计。研究主要关注有限光滑核，如Matérn核或Wendland核家族，同时也指出结果可扩展到范数等价核。", "result": "研究获得了关于核矩阵的新颖且精炼的稳定性估计。特别是，得到了将不同光滑度核的核矩阵的瑞利商相互关联的结果。", "conclusion": "文章最后对这些核矩阵的特征向量的结论进行了评论。", "translation": "核矩阵是基于核的近似中的关键量，借助它们可以分析稳定性、算法收敛性等重要性质。在这项工作中，我们改进了一个多元Ingham型定理，然后利用该定理获得了关于核矩阵的新颖且精炼的稳定性估计。为此，我们专注于有限光滑核的情况，例如Matérn或Wendland核家族，同时指出结果也适用于范数等价核。特别是，我们获得了将不同光滑度核的核矩阵的瑞利商相互关联的结果。最后，我们对这些核矩阵的特征向量的结论进行了评论。", "summary": "本文通过改进多元Ingham型定理，为核矩阵提供了新的稳定性估计。研究聚焦于有限光滑核（如Matérn或Wendland核），并揭示了不同光滑度核矩阵的瑞利商之间的关系，同时还讨论了核矩阵特征向量的相关结论。", "keywords": "核矩阵, 谱对齐, 稳定性估计, Ingham型定理, 瑞利商", "comments": "本文在核矩阵稳定性分析方面取得了进展，通过引入改进的Ingham型定理，为理解和分析基于核的近似提供了更精细的工具。其创新性在于将理论结果应用于实际的核函数族，并探讨了不同光滑度核函数之间的谱特性关联，这对于优化核方法具有潜在价值。"}}
{"id": "2507.18820", "title": "MetaMorph -- A Metamodelling Approach For Robot Morphology", "authors": ["Rachel Ringe", "Robin Nolte", "Nima Zargham", "Robert Porzel", "Rainer Malaka"], "categories": ["cs.RO", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Copyright 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "url": "http://arxiv.org/abs/2507.18820v1", "summary": "Robot appearance crucially shapes Human-Robot Interaction (HRI) but is\ntypically described via broad categories like anthropomorphic, zoomorphic, or\ntechnical. More precise approaches focus almost exclusively on anthropomorphic\nfeatures, which fail to classify robots across all types, limiting the ability\nto draw meaningful connections between robot design and its effect on\ninteraction. In response, we present MetaMorph, a comprehensive framework for\nclassifying robot morphology. Using a metamodeling approach, MetaMorph was\nsynthesized from 222 robots in the IEEE Robots Guide, offering a structured\nmethod for comparing visual features. This model allows researchers to assess\nthe visual distances between robot models and explore optimal design traits\ntailored to different tasks and contexts.", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "pdf_url": "http://arxiv.org/pdf/2507.18820v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "MetaMorph——一种机器人形态的元建模方法", "tldr": "现有的机器人外观分类方法过于宽泛或过于关注拟人特征，限制了对设计与交互影响的分析。本文提出了 MetaMorph，一个综合的元建模框架，通过对222个机器人的综合分析，实现机器人形态的分类和比较，从而评估视觉距离并探索最佳设计。", "motivation": "当前描述机器人外观的方法过于笼统（如拟人、拟动物或技术型）或过于狭窄，主要集中在拟人特征上，这限制了对所有类型机器人进行分类的能力，也阻碍了在机器人设计及其对人机交互的影响之间建立有意义的联系。", "method": "本文提出了 MetaMorph，一个用于分类机器人形态的综合框架。它采用元建模方法，从 IEEE 机器人指南中的 222 个机器人数据中综合而成。", "result": "MetaMorph 提供了一种结构化方法来比较机器人的视觉特征，允许研究人员评估机器人模型之间的视觉距离，并探索针对不同任务和背景量身定制的最佳设计特征。", "conclusion": "MetaMorph 提供了一种更精确和全面的机器人形态分类方法，有助于更好地理解和优化机器人设计以适应人机交互。", "translation": "机器人外观对人机交互（HRI）至关重要，但通常通过广义类别（如拟人、拟动物或技术型）来描述。更精确的方法几乎只关注拟人特征，这无法对所有类型的机器人进行分类，限制了在机器人设计及其对交互影响之间建立有意义联系的能力。为此，我们提出了 MetaMorph，一个用于分类机器人形态的综合框架。MetaMorph 采用元建模方法，从 IEEE 机器人指南中的 222 个机器人中综合而成，提供了一种比较视觉特征的结构化方法。该模型允许研究人员评估机器人模型之间的视觉距离，并探索针对不同任务和背景量身定制的最佳设计特征。", "summary": "MetaMorph 是一个新颖的元建模框架，旨在解决现有机器人外观分类方法在广度和精度上的不足。该框架基于对 IEEE 机器人指南中222个机器人的分析构建，提供了一种结构化的方法来精确分类和比较机器人形态的视觉特征。它使研究人员能够量化机器人模型间的视觉差异，并探索针对特定任务和环境的最佳设计特性，从而加深对机器人设计与人机交互之间关系的理解。", "keywords": "机器人形态, 元建模, 人机交互, 机器人分类, 设计特征", "comments": "MetaMorph 的创新之处在于其采用元建模方法，从大量实际机器人数据中综合出一个全面的分类框架，解决了传统分类方法在覆盖范围和精度上的不足。这对于深入理解机器人形态如何影响人机交互，并指导未来机器人设计具有重要意义。"}}
{"id": "2501.07343", "title": "Fast-Revisit Coverage Path Planning for Autonomous Mobile Patrol Robots Using Long-Range Sensor Information", "authors": ["Srinivas Kachavarapu", "Tobias Doernbach", "Reinhard Gerndt"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      accepted for presentation at the International Conference on Intelligent Robots and Systems (IROS) 2025", "url": "http://arxiv.org/abs/2501.07343v2", "summary": "The utilization of Unmanned Ground Vehicles (UGVs) for patrolling industrial\nsites has expanded significantly. These UGVs typically are equipped with\nperception systems, e.g., computer vision, with limited range due to sensor\nlimitations or site topology. High-level control of the UGVs requires Coverage\nPath Planning (CPP) algorithms that navigate all relevant waypoints and\npromptly start the next cycle. In this paper, we propose the novel Fast-Revisit\nCoverage Path Planning (FaRe-CPP) algorithm using a greedy heuristic approach\nto propose waypoints for maximum coverage area and a random search-based path\noptimization technique to obtain a path along the proposed waypoints with\nminimum revisit time. We evaluated the algorithm in a simulated environment\nusing Gazebo and a camera-equipped TurtleBot3 against a number of existing\nalgorithms. Compared to their average path lengths and revisit times, our\nFaRe-CPP algorithm showed a reduction of at least 21% and 33%, respectively, in\nthese highly relevant performance indicators.", "comment": "accepted for presentation at the International Conference on\n  Intelligent Robots and Systems (IROS) 2025", "pdf_url": "http://arxiv.org/pdf/2501.07343v2", "cate": "cs.RO", "date": "2025-01-13", "updated": "2025-07-25", "AI": {"title_translation": "基于长距离传感器信息的自主移动巡逻机器人快速重访覆盖路径规划", "tldr": "本文提出了一种名为FaRe-CPP的新型覆盖路径规划算法，通过贪婪启发式方法最大化覆盖面积并使用随机搜索优化路径，显著减少了巡逻机器人的路径长度和重访时间。", "motivation": "工业场所的无人地面车辆(UGVs)巡逻应用日益广泛，但其感知系统（如计算机视觉）受限于传感器范围或场地拓扑结构。高层控制需要覆盖路径规划(CPP)算法来导航所有相关路点并迅速开始下一个周期。", "method": "本文提出了一种新颖的快速重访覆盖路径规划(FaRe-CPP)算法。该算法采用贪婪启发式方法来提出最大覆盖区域的路点，并结合基于随机搜索的路径优化技术，以获得具有最短重访时间的路径。", "result": "在Gazebo模拟环境中，使用配备摄像头的TurtleBot3对FaRe-CPP算法与现有算法进行了评估。与现有算法的平均路径长度和重访时间相比，FaRe-CPP算法在这些高度相关的性能指标上分别至少减少了21%和33%。", "conclusion": "FaRe-CPP算法通过结合贪婪启发式和随机搜索路径优化，能够有效缩短巡逻机器人的路径长度和重访时间，从而提高了自主移动巡逻机器人的效率。", "translation": "无人地面车辆（UGVs）在工业场所的巡逻应用已显著扩展。这些UGVs通常配备感知系统，例如计算机视觉，但由于传感器限制或场地拓扑结构，其范围有限。UGVs的高层控制需要覆盖路径规划（CPP）算法，以导航所有相关路点并及时开始下一个周期。在本文中，我们提出了一种新颖的快速重访覆盖路径规划（FaRe-CPP）算法，该算法使用贪婪启发式方法来提出最大覆盖区域的路点，并结合基于随机搜索的路径优化技术，以获得具有最短重访时间的路径。我们在Gazebo模拟环境中使用配备摄像头的TurtleBot3对该算法与多个现有算法进行了评估。与它们的平均路径长度和重访时间相比，我们的FaRe-CPP算法在这些高度相关的性能指标上分别至少减少了21%和33%。", "summary": "本文针对自主移动巡逻机器人提出了一种名为FaRe-CPP的新型覆盖路径规划算法。该算法结合贪婪启发式方法以最大化区域覆盖，并利用随机搜索优化路径以最小化重访时间。在模拟环境中与现有算法的对比评估显示，FaRe-CPP在路径长度和重访时间上分别实现了至少21%和33%的显著减少，证明了其在提高巡逻效率方面的有效性。", "keywords": "覆盖路径规划, 快速重访, 自主移动机器人, 贪婪启发式, 随机搜索", "comments": "该论文的创新点在于提出了FaRe-CPP算法，结合了贪婪启发式和随机搜索两种策略来优化覆盖路径和重访时间。其重要性在于显著提升了自主移动巡逻机器人的效率，在工业巡逻等实际应用中具有潜在价值。论文在模拟环境中进行了验证，未来可在实际物理环境中进行更全面的测试。"}}
{"id": "2504.10096", "title": "Performance in solving the Hermitian and pseudo-Hermitian Bethe-Salpeter equation with the Yambo code", "authors": ["Petru Milev", "Blanca Mellado-Pinto", "Muralidhar Nalabothula", "Ali Esquembre Kucukalic", "Fernando Alvarruiz", "Enrique Ramos", "Alejandro Molina-Sanchez", "Ludger Wirtz", "Jose E. Roman", "Davide Sangalli"], "categories": ["cond-mat.mtrl-sci", "cs.DC", "81-04", "A.0"], "primary_category": "Subjects:       Materials Science (cond-mat.mtrl-sci)", "pdf_link": null, "comments": "Comments:      Submitted to SciPost Physics Codebases", "url": "http://arxiv.org/abs/2504.10096v2", "summary": "We analyze the performance of two strategies in solving the structured\neigenvalue problem deriving from the Bethe-Salpeter equation (BSE) in condensed\nmatter physics. The BSE matrix is constructed with the \\texttt{Yambo} code, and\nthe two strategies are implemented by interfacing \\texttt{Yambo} with the\nScaLAPACK and ELPA libraries for direct diagonalization, and with the SLEPc\nlibrary for the iterative approach. We consider both the Hermitian\n(Tamm-Dancoff approximation) and pseudo-Hermitian forms, addressing dense\nmatrices of three different sizes. A description of the implementation is also\nprovided, with details for the pseudo-Hermitian case. Timing and memory\nutilization are analyzed on both CPU and GPU clusters. Our results demonstrate\nthat it is now feasible to handle dense BSE matrices of the order of 10$^5$.", "comment": "Submitted to SciPost Physics Codebases", "pdf_url": "http://arxiv.org/pdf/2504.10096v2", "cate": "cond-mat.mtrl-sci", "date": "2025-04-14", "updated": "2025-07-24", "AI": {"title_translation": "使用Yambo代码求解厄米和伪厄米Bethe-Salpeter方程的性能", "tldr": "研究了使用Yambo代码通过直接对角化和迭代方法求解Bethe-Salpeter方程（BSE）的性能，结果表明可以处理10^5量级的密集BSE矩阵。", "motivation": "分析在凝聚态物理中求解Bethe-Salpeter方程（BSE）产生的结构化特征值问题的两种策略的性能。", "method": "使用Yambo代码构建BSE矩阵，并通过Yambo与ScaLAPACK、ELPA库（用于直接对角化）以及SLEPc库（用于迭代方法）进行接口实现两种策略。研究考虑了厄米（Tamm-Dancoff近似）和伪厄米形式，处理了三种不同大小的密集矩阵。分析了CPU和GPU集群上的时间和内存利用率。", "result": "结果表明，现在可以处理10^5量级的密集BSE矩阵。", "conclusion": "论文结论是现在处理10^5量级的密集Bethe-Salpeter方程矩阵在计算上是可行的。", "translation": "我们分析了在凝聚态物理中求解源自Bethe-Salpeter方程（BSE）的结构化特征值问题的两种策略的性能。BSE矩阵使用Yambo代码构建，这两种策略通过将Yambo与ScaLAPACK和ELPA库（用于直接对角化）以及SLEPc库（用于迭代方法）进行接口来实现。我们考虑了厄米（Tamm-Dancoff近似）和伪厄米形式，处理了三种不同大小的密集矩阵。论文还提供了实现描述，包括伪厄米情况的详细信息。在CPU和GPU集群上分析了时间和内存利用率。我们的结果表明，现在处理10^5量级的密集BSE矩阵是可行的。", "summary": "本文分析了使用Yambo代码求解凝聚态物理中Bethe-Salpeter方程（BSE）产生的结构化特征值问题的两种策略的性能。通过将Yambo与ScaLAPACK/ELPA（直接对角化）和SLEPc（迭代方法）库接口，研究了厄米和伪厄米形式的三种不同大小的密集矩阵。在CPU和GPU集群上评估了时间和内存利用率，结果表明可以有效地处理高达10^5量级的密集BSE矩阵。", "keywords": "Bethe-Salpeter方程, Yambo, 特征值问题, 性能分析, 密集矩阵", "comments": "这项研究通过系统地评估不同的求解策略和库（ScaLAPACK, ELPA, SLEPc）在处理Bethe-Salpeter方程中的性能，显著推动了凝聚态物理计算的边界。其创新之处在于证明了在现代计算架构下，处理10^5量级的密集BSE矩阵是可行的，这为未来更复杂的材料模拟奠定了基础。"}}
{"id": "2406.12549", "title": "MultiSocial: Multilingual Benchmark of Machine-Generated Text Detection of Social-Media Texts", "authors": ["Dominik Macko", "Jakub Kopal", "Robert Moro", "Ivan Srba"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 main", "url": "http://arxiv.org/abs/2406.12549v2", "summary": "Recent LLMs are able to generate high-quality multilingual texts,\nindistinguishable for humans from authentic human-written ones. Research in\nmachine-generated text detection is however mostly focused on the English\nlanguage and longer texts, such as news articles, scientific papers or student\nessays. Social-media texts are usually much shorter and often feature informal\nlanguage, grammatical errors, or distinct linguistic items (e.g., emoticons,\nhashtags). There is a gap in studying the ability of existing methods in\ndetection of such texts, reflected also in the lack of existing multilingual\nbenchmark datasets. To fill this gap we propose the first multilingual (22\nlanguages) and multi-platform (5 social media platforms) dataset for\nbenchmarking machine-generated text detection in the social-media domain,\ncalled MultiSocial. It contains 472,097 texts, of which about 58k are\nhuman-written and approximately the same amount is generated by each of 7\nmultilingual LLMs. We use this benchmark to compare existing detection methods\nin zero-shot as well as fine-tuned form. Our results indicate that the\nfine-tuned detectors have no problem to be trained on social-media texts and\nthat the platform selection for training matters.", "comment": "ACL 2025 main", "pdf_url": "http://arxiv.org/pdf/2406.12549v2", "cate": "cs.CL", "date": "2024-06-18", "updated": "2025-07-25", "AI": {"title_translation": "MultiSocial：社交媒体机器生成文本检测的多语言基准", "tldr": "该研究提出了MultiSocial，首个针对社交媒体文本的机器生成文本检测多语言（22种语言）多平台基准数据集，以填补现有研究主要集中于英语和长文本的空白，并发现微调检测器在社交媒体文本上表现良好且训练时的平台选择很重要。", "motivation": "当前机器生成文本检测研究主要集中于英语和长文本，而社交媒体文本通常更短、非正式且包含独特语言特征。现有方法在检测此类文本方面的能力研究存在空白，且缺乏多语言基准数据集。", "method": "提出了名为MultiSocial的首个多语言（22种语言）和多平台（5个社交媒体平台）数据集，用于社交媒体领域机器生成文本检测的基准测试。该数据集包含472,097条文本，其中约5.8万条是人类撰写，其余由7个多语言大型语言模型生成。研究使用此基准比较了零样本和微调形式的现有检测方法。", "result": "结果表明，微调后的检测器可以很好地在社交媒体文本上进行训练，并且训练时的平台选择很重要。", "conclusion": "研究成功构建了MultiSocial多语言多平台数据集，填补了社交媒体机器生成文本检测基准的空白，并证明了微调检测器在处理社交媒体文本方面的有效性以及训练平台选择的重要性。", "translation": "最近的大型语言模型能够生成高质量的多语言文本，人类无法区分其与真实的人类撰写文本。然而，机器生成文本检测的研究主要集中在英语和较长的文本，例如新闻文章、科学论文或学生论文。社交媒体文本通常短得多，并且通常具有非正式语言、语法错误或独特的语言项（例如，表情符号、话题标签）。在研究现有方法检测此类文本的能力方面存在空白，这也反映在现有多语言基准数据集的缺乏上。为了填补这一空白，我们提出了第一个用于社交媒体领域机器生成文本检测基准测试的多语言（22种语言）和多平台（5个社交媒体平台）数据集，名为MultiSocial。它包含472,097条文本，其中约5.8万条是人类撰写，其余由7个多语言大型语言模型各生成约相同数量的文本。我们使用此基准比较了零样本以及微调形式的现有检测方法。我们的结果表明，微调后的检测器在社交媒体文本上训练没有问题，并且训练时的平台选择很重要。", "summary": "本研究针对当前机器生成文本检测研究主要集中于英语和长文本的局限性，提出了MultiSocial数据集。该数据集是首个多语言（22种语言）和多平台（5个社交媒体平台）的机器生成文本检测基准，专门用于社交媒体文本，包含近47.2万条人类和大型语言模型生成的文本。通过使用此基准，研究比较了现有检测方法，并发现微调后的检测器能够有效处理社交媒体文本，且训练时选择的平台对性能有重要影响。", "keywords": "机器生成文本检测, 多语言, 社交媒体, 基准数据集, MultiSocial", "comments": "MultiSocial数据集的创新之处在于其首次为社交媒体文本提供了多语言、多平台的机器生成文本检测基准，填补了现有研究的空白。其重要性在于为未来在该领域的研究提供了宝贵的资源，并揭示了微调方法在社交媒体文本检测中的潜力以及训练数据来源的重要性。"}}
{"id": "2507.19338", "title": "Branch-and-bound method for calculating Viterbi path in triplet Markov models", "authors": ["Oskar Soop", "Jüri Lember"], "categories": ["stat.CO", "cs.IT", "math.IT"], "primary_category": "Subjects:       Computation (stat.CO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19338v1", "summary": "We consider a bivariate, possibly non-homogeneous, finite-state Markov chain\n$(X,U)=\\{(X_t,U_t)\\}_{t=1}^n$. We are interested in the marginal process $X$,\nwhich typically is not a Markov chain. The goal is to find a realization (path)\n$x=(x_1,\\ldots,x_n)$ with maximal probability $P(X=x)$. If $X$ is Markov chain,\nthen such path can be efficiently found using the celebrated Viterbi algorithm.\nHowever, when $X$ is not Markovian, identifying the most probable path --\nhereafter referred to as the Viterbi path -- becomes computationally expensive.\nIn this paper, we explore the branch-and-bound method for finding Viterbi\npaths. The method is based on the lower and upper bounds on maximum probability\n$\\max_x P(X=x)$, and the objective of the paper is to exploit the joint Markov\nproperty of $(X,Y)$ to calculate possibly good bounds in possibly cheap way.\n  This research is motivated by decoding or segmentation problem in triplet\nMarkov models. A triplet Markov model is trivariate homogeneous Markov process\n$(X,U,Y)$. In decoding, a realization of one marginal process $Y$ is observed\n(representing the data), while $X$ and $U$ are latent processes. The process\n$U$ serves as a nuisance variable, whereas $X$ is the process of primary\ninterest. Decoding refers to estimating the hidden sequence $X$ based solely on\nthe observation $Y$. Conditional on $Y$, the latent processes $(X, U)$ form a\nnon-homogeneous Markov chain. In this context, the Viterbi path corresponds to\nthe maximum a posteriori (MAP) estimate of $X$, making it a natural choice for\nsignal reconstruction.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19338v1", "cate": "stat.CO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "三联体马尔可夫模型中维特比路径计算的分支定界方法", "tldr": "本文提出一种分支定界方法，用于在非马尔可夫边际过程（如三联体马尔可夫模型中的隐变量）中高效计算维特比路径。", "motivation": "该研究旨在解决三联体马尔可夫模型中的解码或分割问题。在这些模型中，当感兴趣的边际过程X不是马尔可夫链时，寻找最大概率路径（维特比路径）的计算成本很高，因此需要一种更高效的方法。", "method": "本文采用分支定界方法来寻找维特比路径。该方法基于最大概率的上下界，并利用联合马尔可夫过程(X,Y)的性质来计算可能较好的边界，以较低的成本实现。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "我们考虑一个二元、可能非齐次的有限状态马尔可夫链$(X,U)=\\{(X_t,U_t)\\}_{t=1}^n$。我们对边际过程$X$感兴趣，它通常不是马尔可夫链。目标是找到一个具有最大概率$P(X=x)$的实现（路径）$x=(x_1,\\ldots,x_n)$。如果$X$是马尔可夫链，那么可以使用著名的维特比算法高效地找到这样的路径。然而，当$X$不是马尔可夫链时，识别最可能的路径——此后称为维特比路径——计算成本变得很高。在本文中，我们探索了用于寻找维特比路径的分支定界方法。该方法基于最大概率$\\\\max_x P(X=x)$的下界和上界，并且本文的目标是利用$(X,Y)$的联合马尔可夫性质，以可能较低的成本计算可能较好的边界。这项研究的动机是三联体马尔可夫模型中的解码或分割问题。三联体马尔可夫模型是三元齐次马尔可夫过程$(X,U,Y)$。在解码中，观察到一个边际过程$Y$的实现（表示数据），而$X$和$U$是潜在过程。过程$U$作为干扰变量，而$X$是主要感兴趣的过程。解码是指仅根据观察到的$Y$来估计隐藏序列$X$。在给定$Y$的条件下，潜在过程$(X,U)$形成一个非齐次马尔可夫链。在这种情况下，维特比路径对应于$X$的最大后验(MAP)估计，使其成为信号重建的自然选择。", "summary": "本文针对边际过程非马尔可夫链的情况，提出一种分支定界方法来高效计算维特比路径。传统维特比算法适用于马尔可夫链，但对于非马尔可夫边际过程（如三联体马尔可夫模型中的隐变量X），寻找最大概率路径计算成本高昂。该方法利用了联合马尔可夫过程的性质来计算最大概率的上下界，旨在以较低成本获得较好的边界。研究动机来源于三联体马尔可夫模型中的解码或分割问题，其中维特比路径对应于隐藏序列X的最大后验估计。", "keywords": "分支定界, 维特比路径, 三联体马尔可夫模型, 非马尔可夫过程, 最大后验估计", "comments": "本文关注一个重要的计算挑战：在边际过程非马尔可夫链的情况下，如何高效地找到维特比路径。通过引入分支定界方法，并利用联合马尔可夫性质来优化边界计算，该研究为处理此类复杂概率模型提供了一个有前景的途径。这对于信号处理、生物信息学等领域中的解码和分割问题具有潜在的应用价值。"}}
{"id": "2507.18631", "title": "Layer-Aware Representation Filtering: Purifying Finetuning Data to Preserve LLM Safety Alignment", "authors": ["Hao Li", "Lijun Li", "Zhenghao Lu", "Xianyi Wei", "Rui Li", "Jing Shao", "Lei Sha"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18631v2", "summary": "With rapid advancement and increasing accessibility of LLMs, fine-tuning\naligned models has become a critical step for adapting them to real-world\napplications, which makes the safety of this fine-tuning process more important\nthan ever. However, recent studies have highlighted a critical challenge: even\nwhen fine-tuning with seemingly benign downstream datasets, the safety of\naligned LLMs can be compromised, making them more susceptible to malicious\ninstructions.\n  In this paper, we show that fine-tuning datasets often contain samples with\nsafety-degrading features that are not easily identifiable on the surface.\nThese samples can significantly degrade the safety alignment of LLMs during\nfine-tuning. To address this issue, we propose LARF, a Layer-Aware\nRepresentation Filtering method. This method identifies safety-sensitive layers\nwithin the LLM and leverages their representations to detect which data samples\nin the post-training dataset contain safety-degrading features.\n  Experimental results demonstrate that LARF can effectively identify benign\ndata with safety-degrading features. After removing such data, the safety\nalignment degradation caused by fine-tuning is mitigated. Please see our code\nat https://github.com/LLLeoLi/LARF.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18631v2", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-25", "AI": {"title_translation": "层感知表示过滤：净化微调数据以保持大型语言模型安全对齐", "tldr": "对LLM进行微调时，即使使用看似良性的数据也可能损害其安全性。本文提出LARF方法，通过识别LLM中的安全敏感层来过滤掉微调数据中隐藏的安全降级样本，从而有效保持LLM的安全对齐。", "motivation": "现有研究表明，即使使用看似良性的下游数据集进行微调，对齐的LLM的安全性也可能受到损害，使其更容易受到恶意指令的影响。微调数据集中通常包含表面上不易识别的、具有安全降级特征的样本，这些样本在微调过程中会显著降低LLM的安全对齐。", "method": "论文提出了一种名为LARF（Layer-Aware Representation Filtering）的层感知表示过滤方法。该方法识别LLM中对安全敏感的层，并利用这些层的表示来检测微调数据集中哪些数据样本包含安全降级特征。", "result": "实验结果表明，LARF能够有效地识别具有安全降级特征的良性数据。在移除这些数据后，微调导致的安全对齐退化得到了缓解。", "conclusion": "通过识别并移除微调数据集中潜在的安全降级样本，可以有效减轻大型语言模型在微调过程中出现的安全对齐退化问题，从而维护其安全性。", "translation": "随着大型语言模型（LLM）的快速发展和日益普及，对齐模型的微调已成为使其适应实际应用的关键一步，这使得微调过程的安全性比以往任何时候都更加重要。然而，最近的研究突出了一项关键挑战：即使使用看似良性的下游数据集进行微调，对齐的LLM的安全性也可能受到损害，使其更容易受到恶意指令的影响。\n在本文中，我们表明微调数据集通常包含具有安全降级特征的样本，这些特征在表面上不易识别。这些样本在微调过程中会显著降低LLM的安全对齐。为了解决这个问题，我们提出了LARF，一种层感知表示过滤方法。该方法识别LLM中对安全敏感的层，并利用它们的表示来检测后训练数据集中哪些数据样本包含安全降级特征。\n实验结果表明，LARF可以有效地识别具有安全降级特征的良性数据。在移除此类数据后，微调引起的安全对齐退化得到了缓解。请在https://github.com/LLLeoLi/LARF查看我们的代码。", "summary": "本文提出了一种层感知表示过滤（LARF）方法，旨在解决大型语言模型在微调过程中可能出现的安全对齐退化问题。研究发现，即使是看似无害的微调数据集也可能包含不易察觉的安全降级特征样本。LARF通过识别LLM中的安全敏感层并利用其表示来过滤掉这些有害样本。实验证明，该方法能有效识别并移除导致安全问题的样本，从而显著缓解微调过程中的安全对齐退化。", "keywords": "大型语言模型安全, 微调, 数据过滤, 层感知表示, 安全对齐", "comments": "这项研究提出了一个重要的视角，即微调数据本身可能隐藏安全隐患，这不同于传统上关注模型鲁棒性或对抗性攻击。LARF通过利用模型内部的层表示来识别问题数据，提供了一种新颖且实用的数据净化策略，对于维护LLM的安全性具有重要意义。"}}
{"id": "2507.18996", "title": "Adapting to Fragmented and Evolving Data: A Fisher Information Perspective", "authors": ["Behraj Khan", "Tahir Qasim Syed", "Nouman Muhammad Durrani"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18996v1", "summary": "Modern machine learning systems operating in dynamic environments often face\n\\textit{sequential covariate shift} (SCS), where input distributions evolve\nover time while the conditional distribution remains stable. We introduce FADE\n(Fisher-based Adaptation to Dynamic Environments), a lightweight and\ntheoretically grounded framework for robust learning under SCS. FADE employs a\nshift-aware regularization mechanism anchored in Fisher information geometry,\nguiding adaptation by modulating parameter updates based on sensitivity and\nstability. To detect significant distribution changes, we propose a\nCramer-Rao-informed shift signal that integrates KL divergence with temporal\nFisher dynamics. Unlike prior methods requiring task boundaries, target\nsupervision, or experience replay, FADE operates online with fixed memory and\nno access to target labels. Evaluated on seven benchmarks spanning vision,\nlanguage, and tabular data, FADE achieves up to 19\\% higher accuracy under\nsevere shifts, outperforming methods such as TENT and DIW. FADE also\ngeneralizes naturally to federated learning by treating heterogeneous clients\nas temporally fragmented environments, enabling scalable and stable adaptation\nin decentralized settings. Theoretical analysis guarantees bounded regret and\nparameter consistency, while empirical results demonstrate FADE's robustness\nacross modalities and shift intensities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18996v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "适应碎片化和演变数据：一个费舍尔信息视角", "tldr": "本文介绍了FADE，一个基于费舍尔信息的框架，用于在序列协变量漂移下进行鲁棒机器学习。它在严重漂移下提高了准确性，并能推广到联邦学习，无需目标标签或大量内存。", "motivation": "现代机器学习系统在动态环境中运行时，经常面临序列协变量漂移（SCS），即输入分布随时间演变，导致性能下降。因此，需要一种能够适应这些不断演变的数据流，且无需强假设（如访问目标标签或大内存）的鲁棒学习方法。", "method": "本文引入了FADE（基于费舍尔信息的动态环境适应）框架。FADE是一个轻量级且具有理论基础的框架，它采用基于费舍尔信息几何的漂移感知正则化机制，通过根据敏感性和稳定性调节参数更新来指导适应。它还提出了一种克拉默-拉奥信息漂移信号，该信号将KL散度与时间费舍尔动态相结合，用于检测分布变化。FADE可以在线操作，具有固定内存且无需访问目标标签。它还被证明可以推广到联邦学习。", "result": "FADE在涵盖视觉、语言和表格数据的七个基准测试中，在严重漂移下实现了高达19%的准确率提升，优于TENT和DIW等方法。它还可以自然地推广到联邦学习，从而在去中心化设置中实现可扩展和稳定的适应。理论分析保证了有界遗憾和参数一致性。", "conclusion": "FADE为适应序列协变量漂移提供了一个鲁棒、轻量级且具有理论基础的框架，显示出卓越的实证性能和对联邦学习的泛化能力，并有理论保证。", "translation": "现代机器学习系统在动态环境中运行时，经常面临\\textit{序列协变量漂移}（SCS），即输入分布随时间演变，而条件分布保持稳定。我们引入了FADE（基于费舍尔信息的动态环境适应），一个轻量级且具有理论基础的框架，用于SCS下的鲁棒学习。FADE采用了一种基于费舍尔信息几何的漂移感知正则化机制，通过根据敏感性和稳定性调节参数更新来指导适应。为了检测显著的分布变化，我们提出了一种克拉默-拉奥信息漂移信号，该信号将KL散度与时间费舍尔动态相结合。与需要任务边界、目标监督或经验回放的现有方法不同，FADE可以在线操作，具有固定内存且无需访问目标标签。在涵盖视觉、语言和表格数据的七个基准测试中进行评估，FADE在严重漂移下实现了高达19%的准确率提升，优于TENT和DIW等方法。FADE还可以自然地推广到联邦学习，将异构客户端视为时间碎片化环境，从而在去中心化设置中实现可扩展和稳定的适应。理论分析保证了有界遗憾和参数一致性，而实证结果证明了FADE在不同模态和漂移强度下的鲁棒性。", "summary": "本文介绍了FADE（基于费舍尔信息的动态环境适应），一个新颖、轻量级且具有理论基础的框架，旨在解决序列协变量漂移（SCS）下的鲁棒机器学习问题。FADE利用费舍尔信息几何进行漂移感知正则化，并采用克拉默-拉奥信息信号检测分布变化。与以往方法不同，FADE在线操作，具有固定内存且无需目标标签。在各种数据模态上的实证评估表明，FADE在严重漂移下具有卓越的准确性（高达19%的提升），并能自然地推广到联邦学习，同时有界遗憾和参数一致性也得到了理论保证。", "keywords": "序列协变量漂移, 费舍尔信息, 在线学习, 联邦学习, 分布漂移", "comments": "FADE的创新之处在于它利用费舍尔信息几何来鲁棒地适应序列协变量漂移，而无需依赖目标标签或大内存，这些在动态环境中是常见的限制。它能够推广到联邦学习对于去中心化应用尤为重要。理论保证补充了强大的实证结果，使其成为自适应机器学习领域的重要贡献。"}}
{"id": "2506.00099", "title": "Finance as Extended Biology: Reciprocity as the Cognitive Substrate of Financial Behavior", "authors": ["Egil Diau"], "categories": ["physics.soc-ph", "cs.CE", "q-fin.TR"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "Comments:      Position paper on LLM-agent simulation of financial structures. This update clarifies setup and adds a reciprocity-based table. Builds on arXiv:2505.02945 and 2505.08319", "url": "http://arxiv.org/abs/2506.00099v2", "summary": "A central challenge in economics and artificial intelligence is explaining\nhow financial behaviors-such as credit, insurance, and trade-emerge without\nformal institutions. We argue that these functions are not products of\ninstitutional design, but structured extensions of a single behavioral\nsubstrate: reciprocity. Far from being a derived strategy, reciprocity served\nas the foundational logic of early human societies-governing the circulation of\ngoods, regulation of obligation, and maintenance of long-term cooperation well\nbefore markets, money, or formal rules. Trade, commonly regarded as the origin\nof financial systems, is reframed here as the canonical form of reciprocity:\nsimultaneous, symmetric, and partner-contingent. Building on this logic, we\nreconstruct four core financial functions-credit, insurance, token exchange,\nand investment-as expressions of the same underlying principle under varying\nconditions. By grounding financial behavior in minimal, simulateable dynamics\nof reciprocal interaction, this framework shifts the focus from institutional\nengineering to behavioral computation-offering a new foundation for modeling\ndecentralized financial behavior in both human and artificial agents.", "comment": "Position paper on LLM-agent simulation of financial structures. This\n  update clarifies setup and adds a reciprocity-based table. Builds on\n  arXiv:2505.02945 and 2505.08319", "pdf_url": "http://arxiv.org/pdf/2506.00099v2", "cate": "physics.soc-ph", "date": "2025-05-30", "updated": "2025-06-06", "AI": {"title_translation": "金融作为扩展生物学：互惠作为金融行为的认知基础", "tldr": "本文认为，金融行为（如信贷、保险和贸易）并非制度设计产物，而是基于互惠这一单一行为基础的扩展，并提出了一种新的框架来模拟去中心化金融行为。", "motivation": "经济学和人工智能领域的一个核心挑战是解释金融行为（如信贷、保险和贸易）如何在没有正式机构的情况下出现。", "method": "本文将贸易重新定义为互惠的典型形式，并在此逻辑基础上，将信贷、保险、代币交换和投资这四种核心金融功能重构为在不同条件下互惠原则的表达。通过将金融行为植根于互惠互动的最小、可模拟的动态中，该框架将焦点从制度工程转向行为计算。", "result": "该框架提供了一个新的基础，用于模拟人类和人工智能代理中的去中心化金融行为。", "conclusion": "金融行为是互惠的延伸，而非制度设计产物，这一视角为建模去中心化金融行为提供了新的基础。", "translation": "经济学和人工智能领域的一个核心挑战是解释金融行为——例如信贷、保险和贸易——如何在没有正式机构的情况下出现。我们认为这些功能并非制度设计的产物，而是单一行为基础——互惠——的结构化扩展。互惠远非一种衍生策略，它在市场、货币或正式规则出现之前，就作为早期人类社会的根本逻辑，管理着商品的流通、义务的规范和长期合作的维持。贸易，通常被认为是金融系统的起源，在此被重新定义为互惠的典型形式：同时性、对称性以及依赖于伙伴。基于这一逻辑，我们将信贷、保险、代币交换和投资这四种核心金融功能重构为在不同条件下同一潜在原则的表达。通过将金融行为植根于互惠互动的最小、可模拟动态中，该框架将焦点从制度工程转向行为计算——为建模人类和人工智能代理中的去中心化金融行为提供了新的基础。", "summary": "本文提出金融行为（如信贷、保险、贸易）并非源于正式制度，而是互惠这一基本行为模式的结构化延伸。作者将互惠视为早期社会的基础逻辑，并重新定义贸易为互惠的典型形式。在此基础上，文章将信贷、保险、代币交换和投资等核心金融功能解释为互惠原则在不同条件下的体现。该框架通过关注互惠互动的行为动态而非制度设计，为模拟人类和人工智能体中的去中心化金融行为提供了新基础。", "keywords": "互惠, 金融行为, 去中心化金融, 行为计算, 扩展生物学", "comments": "本文的创新点在于将金融行为的起源追溯到生物学上的互惠机制，而非传统的制度设计。这种“金融作为扩展生物学”的视角，为理解和建模去中心化金融行为提供了全新的理论基础，对人工智能中设计去中心化经济系统具有重要启示。"}}
{"id": "2507.18972", "title": "TiVy: Time Series Visual Summary for Scalable Visualization", "authors": ["Gromit Yeuk-Yin Chan", "Luis Gustavo Nonato", "Themis Palpanas", "Cláudio T. Silva", "Juliana Freire"], "categories": ["cs.GR", "cs.LG"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      to be published in TVCG (IEEE VIS 2025)", "url": "http://arxiv.org/abs/2507.18972v1", "summary": "Visualizing multiple time series presents fundamental tradeoffs between\nscalability and visual clarity. Time series capture the behavior of many\nlarge-scale real-world processes, from stock market trends to urban activities.\nUsers often gain insights by visualizing them as line charts, juxtaposing or\nsuperposing multiple time series to compare them and identify trends and\npatterns. However, existing representations struggle with scalability: when\ncovering long time spans, leading to visual clutter from too many small\nmultiples or overlapping lines. We propose TiVy, a new algorithm that\nsummarizes time series using sequential patterns. It transforms the series into\na set of symbolic sequences based on subsequence visual similarity using\nDynamic Time Warping (DTW), then constructs a disjoint grouping of similar\nsubsequences based on the frequent sequential patterns. The grouping result, a\nvisual summary of time series, provides uncluttered superposition with fewer\nsmall multiples. Unlike common clustering techniques, TiVy extracts similar\nsubsequences (of varying lengths) aligned in time. We also present an\ninteractive time series visualization that renders large-scale time series in\nreal-time. Our experimental evaluation shows that our algorithm (1) extracts\nclear and accurate patterns when visualizing time series data, (2) achieves a\nsignificant speed-up (1000X) compared to a straightforward DTW clustering. We\nalso demonstrate the efficiency of our approach to explore hidden structures in\nmassive time series data in two usage scenarios.", "comment": "to be published in TVCG (IEEE VIS 2025)", "pdf_url": "http://arxiv.org/pdf/2507.18972v1", "cate": "cs.GR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "TiVy：可扩展可视化的时间序列视觉摘要", "tldr": "TiVy通过序列模式总结时间序列，以解决大规模时间序列可视化中的可伸缩性和视觉清晰度问题，并显著提高速度。", "motivation": "大规模时间序列可视化在可伸缩性和视觉清晰度之间存在根本性权衡。现有表示方法在处理长时间跨度时，由于过多的小型图或重叠线导致视觉混乱。", "method": "提出TiVy算法，它使用动态时间规整（DTW）将时间序列转换为基于子序列视觉相似性的符号序列，然后根据频繁序列模式构建相似子序列的非交集分组，生成时间序列的视觉摘要。TiVy提取时间上对齐的变长相似子序列。还提出了一个交互式时间序列可视化工具。", "result": "实验表明，TiVy在可视化时间序列数据时能提取清晰准确的模式，与直接DTW聚类相比实现了1000倍的显著加速，并在两种使用场景中展示了探索海量时间序列数据中隐藏结构的效率。", "conclusion": "TiVy提供了一种解决大规模时间序列可视化中可伸缩性和视觉清晰度挑战的新方法，通过有效的序列模式总结和交互式可视化，能够提取清晰模式并显著提高性能。", "translation": "可视化多个时间序列在可伸缩性和视觉清晰度之间存在根本性的权衡。时间序列捕捉了许多大规模真实世界过程的行为，从股市趋势到城市活动。用户通常通过将它们可视化为折线图，并并置或叠加多个时间序列来比较它们并识别趋势和模式，从而获得洞察力。然而，现有的表示方法在可伸缩性方面存在困难：当覆盖长时间跨度时，会导致过多的小型图或重叠线造成的视觉混乱。我们提出了TiVy，一种使用序列模式总结时间序列的新算法。它使用动态时间规整（DTW）根据子序列的视觉相似性将时间序列转换为一组符号序列，然后根据频繁序列模式构建相似子序列的非交集分组。分组结果是时间序列的视觉摘要，提供了更少的叠加和更少的迷你图，从而减少了视觉混乱。与常见的聚类技术不同，TiVy提取时间上对齐的相似子序列（长度可变）。我们还提出了一个交互式时间序列可视化工具，可以实时渲染大规模时间序列。我们的实验评估表明，我们的算法（1）在可视化时间序列数据时提取清晰准确的模式，（2）与直接的DTW聚类相比，实现了显著的加速（1000倍）。我们还在两种使用场景中展示了我们方法在探索海量时间序列数据中隐藏结构方面的效率。", "summary": "本论文提出了TiVy算法，旨在解决大规模时间序列可视化中可伸缩性和视觉清晰度的权衡问题。TiVy通过利用动态时间规整（DTW）将时间序列转换为符号序列，并基于频繁序列模式对相似子序列进行分组，从而生成时间序列的视觉摘要。这种方法避免了传统方法中因长时间跨度导致的视觉混乱，并能提取时间对齐的变长相似子序列。实验结果表明，TiVy能有效提取清晰准确的模式，并相较于直接DTW聚类实现了千倍的速度提升，有效支持了对海量时间序列数据的探索。", "keywords": "时间序列可视化, 序列模式, 动态时间规整, 可伸缩性, 视觉摘要", "comments": "TiVy的创新点在于其结合了DTW与序列模式挖掘，以生成时间对齐的变长子序列分组，从而创建视觉摘要，有效解决了大规模时间序列可视化中的核心挑战（可伸缩性与视觉清晰度）。其相对于传统DTW聚类1000倍的加速，也凸显了其实用性和重要性。该方法对于需要分析海量时间序列数据的领域（如金融、城市规划等）具有重要意义。"}}
{"id": "2506.18414", "title": "Latent Space Analysis for Melanoma Prevention", "authors": ["Ciro Listone", "Aniello Murano"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      The proposed approach presents some technical imperfections and needs to be refined with further examinations", "url": "http://arxiv.org/abs/2506.18414v2", "summary": "Melanoma represents a critical health risk due to its aggressive progression\nand high mortality, underscoring the need for early, interpretable diagnostic\ntools. While deep learning has advanced in skin lesion classification, most\nexisting models provide only binary outputs, offering limited clinical insight.\nThis work introduces a novel approach that extends beyond classification,\nenabling interpretable risk modelling through a Conditional Variational\nAutoencoder. The proposed method learns a structured latent space that captures\nsemantic relationships among lesions, allowing for a nuanced, continuous\nassessment of morphological differences. An SVM is also trained on this\nrepresentation effectively differentiating between benign nevi and melanomas,\ndemonstrating strong and consistent performance. More importantly, the learned\nlatent space supports visual and geometric interpretation of malignancy, with\nthe spatial proximity of a lesion to known melanomas serving as a meaningful\nindicator of risk. This approach bridges predictive performance with clinical\napplicability, fostering early detection, highlighting ambiguous cases, and\nenhancing trust in AI-assisted diagnosis through transparent and interpretable\ndecision-making.", "comment": "The proposed approach presents some technical imperfections and needs\n  to be refined with further examinations", "pdf_url": "http://arxiv.org/pdf/2506.18414v2", "cate": "cs.CV", "date": "2025-06-23", "updated": "2025-07-25", "AI": {"title_translation": "黑色素瘤预防的潜在空间分析", "tldr": "该研究提出了一种基于条件变分自动编码器的新方法，通过学习可解释的潜在空间来对黑色素瘤进行风险建模，从而实现早期诊断和提高AI辅助诊断的信任度。", "motivation": "黑色素瘤因其侵袭性进展和高死亡率而构成严重的健康风险，现有深度学习模型多提供二元输出，提供的临床洞察有限，因此需要早期、可解释的诊断工具。", "method": "引入了一种新方法，通过条件变分自动编码器（CVAE）学习结构化的潜在空间，捕获病变之间的语义关系，实现形态差异的细致连续评估。在此表示上训练SVM以有效区分良性痣和黑色素瘤。", "result": "训练的SVM在区分良性痣和黑色素瘤方面表现出强大且一致的性能。学习到的潜在空间支持恶性肿瘤的视觉和几何解释，病变与已知黑色素瘤的空间接近度可作为有意义的风险指标。", "conclusion": "该方法将预测性能与临床适用性相结合，促进早期检测，突出模糊病例，并通过透明和可解释的决策提高对AI辅助诊断的信任。", "translation": "黑色素瘤因其侵袭性进展和高死亡率而构成严重的健康风险，这突显了对早期、可解释诊断工具的需求。尽管深度学习在皮肤病变分类方面取得了进展，但大多数现有模型仅提供二元输出，提供的临床洞察有限。这项工作引入了一种超越分类的新方法，通过条件变分自动编码器实现可解释的风险建模。所提出的方法学习了一个结构化的潜在空间，该空间捕获了病变之间的语义关系，从而可以对形态差异进行细致、连续的评估。在此表示上还训练了一个支持向量机（SVM），有效区分良性痣和黑色素瘤，表现出强大且一致的性能。更重要的是，学习到的潜在空间支持恶性肿瘤的视觉和几何解释，病变与已知黑色素瘤的空间接近度可作为有意义的风险指标。这种方法将预测性能与临床适用性相结合，通过透明和可解释的决策，促进早期检测，突出模糊病例，并增强对AI辅助诊断的信任。", "summary": "本文提出一种新颖的黑色素瘤早期诊断方法，利用条件变分自动编码器学习一个结构化、可解释的潜在空间，以捕捉皮肤病变间的语义关系，并提供连续的形态差异评估。在此潜在空间上训练的SVM能有效区分良性痣和黑色素瘤，并表现出高一致性。该方法的关键在于其潜在空间支持恶性肿瘤的视觉和几何解释，通过病变与已知黑色素瘤的接近度来指示风险，从而提升了AI辅助诊断的透明度和临床适用性。", "keywords": "黑色素瘤, 潜在空间, 条件变分自动编码器, 可解释AI, 早期诊断", "comments": "这项研究的创新之处在于其超越了传统的二元分类，通过构建可解释的潜在空间，实现了对黑色素瘤风险的连续和可视化评估。这不仅提高了诊断的精细度，也增强了AI决策的透明度和临床信任度，对于早期预警和模糊病例的识别具有重要意义。"}}
{"id": "2308.12797", "title": "TrafficMCTS: A Closed-Loop Traffic Flow Generation Framework with Group-Based Monte Carlo Tree Search", "authors": ["Ze Fu", "Licheng Wen", "Pinlong Cai", "Daocheng Fu", "Song Mao", "Botian Shi"], "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Published in IEEE Transactions on Intelligent Transportation Systems", "url": "http://arxiv.org/abs/2308.12797v3", "summary": "Traffic flow simulation within the domain of intelligent transportation\nsystems is garnering significant attention, and generating realistic, diverse,\nand human-like traffic patterns presents critical challenges that must be\naddressed. Current approaches often hinge on predefined driver models,\nobjective optimization, or reliance on pre-recorded driving datasets, imposing\nlimitations on their scalability, versatility, and adaptability. In this paper,\nwe introduce TrafficMCTS, an innovative framework that harnesses the synergy of\ngroup-based Monte Carlo tree search (MCTS) and Social Value Orientation (SVO)\nto engender a multifaceted traffic flow with varying driving styles and\ncooperative tendencies. Anchored by a closed-loop architecture, our framework\nenables vehicles to dynamically adapt to their environment in real time, and\nensure feasible collision-free trajectories. Through comprehensive comparisons\nwith state-of-the-art methods, we illuminate the advantages of our approach in\nterms of computational efficiency, planning success rate, intention completion\ntime, and diversity metrics. Besides, we simulate multiple scenarios to\nillustrate the effectiveness of the proposed framework and highlight its\nability to induce diverse social behaviors within the traffic flow. Finally, we\nvalidate the scalability of TrafficMCTS by demonstrating its capability to\nefficiently simulate diverse traffic scenarios involving numerous interacting\nvehicles within a complex road network, capturing the intricate dynamics of\nhuman-like driving behaviors.", "comment": "Published in IEEE Transactions on Intelligent Transportation Systems", "pdf_url": "http://arxiv.org/pdf/2308.12797v3", "cate": "cs.RO", "date": "2023-08-24", "updated": "2025-07-25", "AI": {"title_translation": "TrafficMCTS：一种基于群体蒙特卡洛树搜索的闭环交通流生成框架", "tldr": "TrafficMCTS是一个创新的闭环交通流生成框架，结合了基于群体的蒙特卡洛树搜索和社会价值导向，能生成多样化、类人且无碰撞的交通模式，并在效率、成功率、完成时间及多样性方面优于现有方法，同时展现出良好的可扩展性。", "motivation": "当前的交通流模拟方法依赖预定义驾驶模型、目标优化或预记录数据集，导致可扩展性、通用性和适应性受限。研究旨在解决生成真实、多样且类人交通模式的关键挑战。", "method": "本文引入了TrafficMCTS框架，它结合了基于群体的蒙特卡洛树搜索（MCTS）和社会价值导向（SVO）。该框架采用闭环架构，使车辆能够实时动态适应环境，并确保可行的无碰撞轨迹。", "result": "TrafficMCTS在计算效率、规划成功率、意图完成时间和多样性指标方面优于现有最先进方法。它能有效模拟多种场景并诱导交通流中多样化的社会行为。该框架还被验证在模拟涉及大量交互车辆的复杂路网场景中具有高效的可扩展性，能够捕捉类人驾驶行为的复杂动态。", "conclusion": "TrafficMCTS框架通过结合群体MCTS和SVO，成功生成了具有不同驾驶风格和合作倾向的多样化交通流，并在效率、多样性和可扩展性方面展现出显著优势，有效解决了现有方法的局限性。", "translation": "智能交通系统领域的交通流模拟正受到广泛关注，而生成真实、多样化和类人交通模式面临着必须解决的关键挑战。当前的方法通常依赖于预定义的驾驶员模型、目标优化或预先记录的驾驶数据集，这限制了它们的可扩展性、通用性和适应性。在本文中，我们引入了TrafficMCTS，这是一个创新的框架，它利用基于群体的蒙特卡洛树搜索（MCTS）和社会价值导向（SVO）的协同作用，以产生具有不同驾驶风格和合作倾向的多方面交通流。以闭环架构为基础，我们的框架使车辆能够实时动态适应其环境，并确保可行的无碰撞轨迹。通过与最先进方法的全面比较，我们阐明了我们方法在计算效率、规划成功率、意图完成时间以及多样性指标方面的优势。此外，我们模拟了多种场景以说明所提出框架的有效性，并强调其在交通流中诱导多样化社会行为的能力。最后，我们通过展示TrafficMCTS在复杂路网中高效模拟涉及众多交互车辆的多样化交通场景的能力，验证了其可扩展性，捕捉了类人驾驶行为的复杂动态。", "summary": "TrafficMCTS是一种新颖的闭环交通流生成框架，旨在解决现有方法在生成真实、多样化和类人交通模式方面的局限性。该框架巧妙地融合了群体蒙特卡洛树搜索（MCTS）和社会价值导向（SVO），使车辆能够实时动态调整，并确保无碰撞轨迹。实验结果表明，TrafficMCTS在计算效率、规划成功率、意图完成时间以及多样性方面均超越了现有技术，并能有效模拟复杂场景中多样化的社会行为。此外，该框架在模拟大规模复杂路网中的交互车辆时也展现出卓越的可扩展性，能够捕捉类人驾驶行为的复杂动态。", "keywords": "交通流生成, 蒙特卡洛树搜索, 社会价值导向, 闭环系统, 智能交通系统", "comments": "TrafficMCTS的创新之处在于结合了群体MCTS和SVO，实现了驾驶风格和合作倾向的多样化生成，并通过闭环架构确保了实时适应性和轨迹可行性。这提高了交通流模拟的真实性和实用性。其在效率、多样性和可扩展性方面的表现，使其在智能交通系统领域具有重要应用潜力。"}}
{"id": "2507.19104", "title": "A systematic literature review to unveil users objective reaction to virtual experiences: Complemented with a conceptual model (QoUX in VE)", "authors": ["Alireza Mortezapour", "Andrea Antonio Cantone", "Monica Maria Lucia Sebillo", "Giuliana Vitiello"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19104v1", "summary": "In pursuit of documenting users Neurophysiological responses during\nexperiencing virtual environments (VE), this systematic review presents a novel\nconceptual model of UX in VE. Searching across seven databases yielded to 1743\narticles. Rigorous screenings, included only 66 articles. Notably, UX in VE\nlacks a consensus definition. Obviously, this UX has many unique sub-dimensions\nthat are not mentioned in other products. The presented conceptual model\ncontains 26 subdimensions which mostly not supported in previous subjective\ntools and questionnaires. While EEG and ECG were common, brain ultrasound,\nemployed in one study, highlights the need for using neurophysiological\nassessments to comprehensively grasp immersive UX intricacies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19104v1", "cate": "cs.HC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "一项系统文献综述，旨在揭示用户对虚拟体验的客观反应：辅以概念模型（虚拟环境中的用户体验质量）", "tldr": "系统综述揭示了虚拟环境中用户体验（UX）的神经生理反应，并提出了一个包含26个子维度的新概念模型，强调需要客观评估。", "motivation": "旨在记录用户在体验虚拟环境（VE）时的神经生理反应，并解决虚拟环境中用户体验缺乏共识定义的问题。", "method": "进行了一项系统文献综述，在七个数据库中搜索，最初得到1743篇文章，经过严格筛选后纳入66篇文章。此外，提出了一个包含26个子维度的新概念模型。", "result": "虚拟环境中的用户体验缺乏共识定义，且具有许多在其他产品中未提及的独特子维度。提出的概念模型包含26个子维度，这些维度在以往的主观工具和问卷中大多未被支持。EEG和ECG是常用的评估方法，一项研究中使用的脑部超声强调了使用神经生理学评估的必要性。", "conclusion": "虚拟环境中的用户体验（UX）缺乏共识定义，且包含独特的子维度，需要通过神经生理学评估来全面理解其复杂性，并提出了一个包含26个子维度的新概念模型。", "translation": "旨在记录用户在体验虚拟环境（VE）时的神经生理反应，本系统综述提出了一个新颖的虚拟环境中用户体验（UX）概念模型。通过在七个数据库中搜索，共获得了1743篇文章。经过严格筛选，仅纳入了66篇文章。值得注意的是，虚拟环境中的用户体验缺乏共识定义。显然，这种用户体验有许多独特的子维度，这些在其他产品中并未提及。提出的概念模型包含26个子维度，这些维度大多未在以往的主观工具和问卷中得到支持。虽然EEG和ECG是常见的评估方法，但一项研究中使用的脑部超声强调了使用神经生理学评估来全面掌握沉浸式用户体验复杂性的必要性。", "summary": "本文通过一项系统文献综述，旨在揭示用户在虚拟环境中的神经生理反应，并提出了一个名为“虚拟环境中用户体验质量（QoUX in VE）”的新颖概念模型。该综述筛选了来自七个数据库的1743篇文章，最终纳入66篇。研究发现虚拟环境中的用户体验缺乏共识定义，且包含26个独特的子维度，这些维度在传统主观评估工具中鲜有涉及。文章强调了使用EEG、ECG以及脑部超声等神经生理学评估方法来全面理解沉浸式用户体验复杂性的重要性。", "keywords": "虚拟体验, 用户体验, 神经生理反应, 系统综述, 概念模型", "comments": "这篇论文通过系统综述揭示了虚拟环境中用户体验评估的不足，并提出了一个包含多维度的新概念模型，这对于统一虚拟环境用户体验的定义和评估方法具有重要意义。强调使用神经生理学方法进行客观评估，指出了未来研究的方向，具有一定的创新性。"}}
{"id": "2507.18682", "title": "Recommendations to overcome language barriers in the Vera C. Rubin Observatory Research Ecosystem", "authors": ["José Antonio Alonso Pavón", "Andrés Alejandro Plazas Malagón"], "categories": ["astro-ph.IM", "cs.CY", "physics.soc-ph"], "primary_category": "Subjects:       Instrumentation and Methods for Astrophysics (astro-ph.IM)", "pdf_link": null, "comments": "Comments:      19 pages, accepted for publication at the Bulletin of the American Astronomical Society (peer review requested), Vol. 57, Issue 1", "url": "http://arxiv.org/abs/2507.18682v1", "summary": "The report presents a comprehensive set of five recommendations to reduce\nlanguage barriers within the Vera C. Rubin Observatory Research Ecosystem,\npromoting greater inclusion of researchers who are speakers of English as an\nadditional language. Recognizing that English linguistic hegemony in science\nlimits participation and productivity, the document proposes multilingual\npresentation formats, academic writing training, a Virtual Writing Center,\nlanguage support programs, and writing retreats. Each recommendation is\ngrounded in both pedagogical theory and empirical evidence, with an emphasis on\ncollaborative, socially embedded approaches to scientific writing. The proposed\nacademic writing training integrates constructivist and socio-cultural\nperspectives, emphasizing genre awareness, rhetorical competence, and\nreflective practices. The Virtual Writing Center would serve as a permanent\ninfrastructure offering personalized tutoring and peer review support, while\nthe language support programs address ongoing needs through workshops,\nconsultations, and access to language tools. Writing retreats provide immersive\nenvironments for focused work and mentorship. The recommendations also\nencourage ethical use of AI tools for translation and writing assistance,\nfostering digital literacy alongside linguistic proficiency. Collectively,\nthese initiatives aim to transform language from a barrier into a resource,\nrecognizing multilingualism as an asset in global research collaboration.\nRather than offering a one-size-fits-all solution, the document advocates for\nadaptable, community-driven strategies that can evolve within the diverse\ninstitutional and disciplinary contexts of the Rubin Research Ecosystem. By\nimplementing these practices, the Ecosystem could lead efforts to democratize\nscientific communication and foster a more equitable, multilingual research\nculture.", "comment": "19 pages, accepted for publication at the Bulletin of the American\n  Astronomical Society (peer review requested), Vol. 57, Issue 1", "pdf_url": "http://arxiv.org/pdf/2507.18682v1", "cate": "astro-ph.IM", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "克服维拉·C·鲁宾天文台研究生态系统语言障碍的建议", "tldr": "本报告提出了五项建议，旨在减少维拉·C·鲁宾天文台研究生态系统中的语言障碍，促进更多英语作为第二语言的研究人员的参与。", "motivation": "科学领域英语的语言霸权限制了非英语母语研究人员的参与和生产力，本研究旨在将语言从障碍转化为资源，促进更广泛的包容性。", "method": "本报告提出了五项建议：多语言演示形式、学术写作培训（整合建构主义和社会文化视角）、虚拟写作中心（提供个性化辅导和同行评审支持）、语言支持项目（研讨会、咨询和语言工具）以及写作静修营（沉浸式工作和指导环境）。报告还鼓励道德使用AI工具进行翻译和写作辅助。所有建议均基于教学理论和实证证据，强调协作和社会嵌入式方法。", "result": "通过实施这些建议，维拉·C·鲁宾天文台研究生态系统可以引领科学传播民主化，并培养更加公平、多语种的研究文化。这些举措旨在将语言从障碍转化为资源，认识到多语言能力在全球研究合作中的宝贵资产作用。", "conclusion": "本报告提倡采用适应性强、社区驱动的策略来克服语言障碍，促进包容性，并将多语言能力视为全球研究合作中的一项资产，最终培养更公平、多语种的研究文化。", "translation": "本报告提出了一套全面的五项建议，旨在减少维拉·C·鲁宾天文台研究生态系统内的语言障碍，促进更多将英语作为附加语言的研究人员的包容性。报告认识到科学中英语的语言霸权限制了参与和生产力，因此提出了多语言演示形式、学术写作培训、虚拟写作中心、语言支持项目和写作静修营。每项建议都以教学理论和实证证据为基础，强调协作的、社会嵌入式的科学写作方法。拟议的学术写作培训整合了建构主义和社会文化视角，强调体裁意识、修辞能力和反思实践。虚拟写作中心将作为永久性基础设施，提供个性化辅导和同行评审支持，而语言支持项目则通过研讨会、咨询和语言工具访问来满足持续需求。写作静修营为集中工作和指导提供沉浸式环境。这些建议还鼓励道德使用人工智能工具进行翻译和写作辅助，在培养语言能力的同时培养数字素养。总的来说，这些举措旨在将语言从障碍转化为资源，认识到多语言能力在全球研究合作中是一种资产。报告没有提供一刀切的解决方案，而是提倡适应性强、社区驱动的策略，这些策略可以在鲁宾研究生态系统多样化的机构和学科背景下发展。通过实施这些实践，该生态系统可以引领科学传播民主化，并培养更公平、多语种的研究文化。", "summary": "本报告为维拉·C·鲁宾天文台研究生态系统提出了五项基于证据的建议，以克服语言障碍。这些建议包括多语言展示形式、学术写作培训、虚拟写作中心、语言支持项目和写作静修营，并鼓励道德使用AI工具。其目标是增强非英语母语研究人员的包容性，促进科学交流的民主化，并将多语言能力视为研究资产，从而培养一个更公平、更具适应性的研究环境。", "keywords": "语言障碍, 多语言能力, 研究生态系统, 科学传播, 学术写作", "comments": "该论文的重要性在于它解决了全球科学合作中的一个重大问题——语言障碍。其创新之处在于提供了一个全面、多方面的解决方案，该方案植根于教学理论和实证证据，超越了简单的翻译，转向了全面的支持和文化变革。对社区驱动策略和道德使用AI工具的强调尤其具有前瞻性。"}}
{"id": "2507.19239", "title": "CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception", "authors": ["Jiaru Zhong", "Jiahao Wang", "Jiahui Xu", "Xiaofan Li", "Zaiqing Nie", "Haibao Yu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025 (Highlight)", "url": "http://arxiv.org/abs/2507.19239v1", "summary": "Cooperative perception aims to address the inherent limitations of\nsingle-vehicle autonomous driving systems through information exchange among\nmultiple agents. Previous research has primarily focused on single-frame\nperception tasks. However, the more challenging cooperative sequential\nperception tasks, such as cooperative 3D multi-object tracking, have not been\nthoroughly investigated. Therefore, we propose CoopTrack, a fully\ninstance-level end-to-end framework for cooperative tracking, featuring\nlearnable instance association, which fundamentally differs from existing\napproaches. CoopTrack transmits sparse instance-level features that\nsignificantly enhance perception capabilities while maintaining low\ntransmission costs. Furthermore, the framework comprises two key components:\nMulti-Dimensional Feature Extraction, and Cross-Agent Association and\nAggregation, which collectively enable comprehensive instance representation\nwith semantic and motion features, and adaptive cross-agent association and\nfusion based on a feature graph. Experiments on both the V2X-Seq and Griffin\ndatasets demonstrate that CoopTrack achieves excellent performance.\nSpecifically, it attains state-of-the-art results on V2X-Seq, with 39.0\\% mAP\nand 32.8\\% AMOTA. The project is available at\nhttps://github.com/zhongjiaru/CoopTrack.", "comment": "Accepted by ICCV 2025 (Highlight)", "pdf_url": "http://arxiv.org/pdf/2507.19239v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "CoopTrack：探索端到端学习实现高效协作序列感知", "tldr": "CoopTrack是一个用于协作3D多目标跟踪的端到端框架，通过传输稀疏实例级特征和可学习的实例关联，在V2X-Seq数据集上取得了最先进的性能。", "motivation": "现有的协作感知研究主要集中在单帧感知任务，而更具挑战性的协作序列感知任务（如协作3D多目标跟踪）尚未得到充分研究，这限制了单车自动驾驶系统的固有能力。", "method": "论文提出了CoopTrack，一个完全实例级的端到端协作跟踪框架，其核心是可学习的实例关联。CoopTrack通过传输稀疏的实例级特征来增强感知能力并保持低传输成本。该框架包含两个关键组件：多维特征提取，用于生成包含语义和运动特征的全面实例表示；以及跨智能体关联与聚合，用于基于特征图实现自适应的跨智能体关联和融合。", "result": "CoopTrack在V2X-Seq和Griffin数据集上表现出色。特别地，它在V2X-Seq数据集上取得了最先进的结果，mAP达到39.0%，AMOTA达到32.8%。", "conclusion": "CoopTrack通过其创新的端到端框架和高效的特征传输机制，有效解决了协作序列感知中的多目标跟踪挑战，并在相关数据集上实现了领先的性能，证明了其在提升自动驾驶系统感知能力方面的潜力。", "translation": "协作感知旨在通过多个智能体之间的信息交换来解决单车自动驾驶系统固有的局限性。先前的研究主要集中在单帧感知任务。然而，更具挑战性的协作序列感知任务，例如协作3D多目标跟踪，尚未得到彻底研究。因此，我们提出了CoopTrack，一个用于协作跟踪的完全实例级端到端框架，其特点是可学习的实例关联，这与现有方法根本不同。CoopTrack传输稀疏的实例级特征，显著增强了感知能力，同时保持了低传输成本。此外，该框架包含两个关键组件：多维特征提取，以及跨智能体关联与聚合，它们共同实现了具有语义和运动特征的全面实例表示，以及基于特征图的自适应跨智能体关联和融合。在V2X-Seq和Griffin数据集上的实验表明，CoopTrack取得了优异的性能。具体而言，它在V2X-Seq上取得了最先进的结果，mAP达到39.0%，AMOTA达到32.8%。该项目可在https://github.com/zhongjiaru/CoopTrack获取。", "summary": "本文提出了CoopTrack，一个针对协作序列感知任务（如3D多目标跟踪）的端到端框架。CoopTrack通过传输稀疏实例级特征和引入可学习的实例关联，有效提升了多智能体协作感知能力并降低了通信成本。该框架包含多维特征提取和跨智能体关联与聚合两大核心组件。实验结果表明，CoopTrack在V2X-Seq和Griffin数据集上表现出色，尤其在V2X-Seq上达到了最先进的性能。", "keywords": "协作感知, 序列感知, 3D多目标跟踪, 端到端学习, 实例级特征", "comments": "CoopTrack的创新之处在于其端到端的实例级协作跟踪方法，特别是可学习的实例关联和稀疏特征传输机制，这显著提升了协作感知效率和性能。它为解决自动驾驶中多智能体序列感知和跟踪的复杂挑战提供了新的范式。"}}
{"id": "2506.16629", "title": "Learning Causally Predictable Outcomes from Psychiatric Longitudinal Data", "authors": ["Eric V. Strobl"], "categories": ["cs.LG", "q-bio.QM", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      R code is available at this http URL", "url": "http://arxiv.org/abs/2506.16629v4", "summary": "Causal inference in longitudinal biomedical data remains a central challenge,\nespecially in psychiatry, where symptom heterogeneity and latent confounding\nfrequently undermine classical estimators. Most existing methods for treatment\neffect estimation presuppose a fixed outcome variable and address confounding\nthrough observed covariate adjustment. However, the assumption of\nunconfoundedness may not hold for a fixed outcome in practice. To address this\nfoundational limitation, we directly optimize the outcome definition to\nmaximize causal identifiability. Our DEBIAS (Durable Effects with\nBackdoor-Invariant Aggregated Symptoms) algorithm learns non-negative,\nclinically interpretable weights for outcome aggregation, maximizing durable\ntreatment effects and empirically minimizing both observed and latent\nconfounding by leveraging the time-limited direct effects of prior treatments\nin psychiatric longitudinal data. The algorithm also furnishes an empirically\nverifiable test for outcome unconfoundedness. DEBIAS consistently outperforms\nstate-of-the-art methods in recovering causal effects for clinically\ninterpretable composite outcomes across comprehensive experiments in depression\nand schizophrenia.", "comment": "R code is available at github.com/ericstrobl/DEBIAS", "pdf_url": "http://arxiv.org/pdf/2506.16629v4", "cate": "cs.LG", "date": "2025-06-19", "updated": "2025-07-25", "AI": {"title_translation": "从精神病学纵向数据中学习因果可预测结果", "tldr": "DEBIAS是一种新算法，通过优化结果定义和最小化混杂因素，改进了精神病学纵向数据中的因果推断，性能优于现有方法。", "motivation": "在精神病学等领域，纵向生物医学数据中的因果推断面临巨大挑战，因为症状异质性和潜在混杂因素经常会削弱经典估计器。大多数现有方法预设一个固定的结果变量并通过观测协变量调整来处理混杂，但这种无混杂假设在实践中可能不成立。", "method": "本文提出了DEBIAS（具有后门不变聚合症状的持久效应）算法。该算法通过直接优化结果定义来最大化因果可识别性。它学习非负的、临床可解释的权重用于结果聚合，最大化持久治疗效果，并通过利用精神病学纵向数据中先前治疗的有时限直接效应，在经验上最小化观测和潜在混杂。DEBIAS算法还提供了一个可经验验证的结果无混杂检验。", "result": "DEBIAS算法在抑郁症和精神分裂症的综合实验中，在恢复临床可解释复合结果的因果效应方面，始终优于最先进的方法。", "conclusion": "DEBIAS算法通过优化结果定义和有效处理混杂因素，为精神病学纵向数据中的因果推断提供了一种优越且经验可验证的解决方案。", "translation": "纵向生物医学数据中的因果推断仍然是一个核心挑战，尤其是在精神病学领域，症状异质性和潜在混杂经常会削弱经典估计器。大多数现有的治疗效果估计方法都预设了一个固定的结果变量，并通过观测协变量调整来处理混杂。然而，无混杂的假设在实践中对于固定结果可能不成立。为了解决这个基本限制，我们直接优化结果定义以最大化因果可识别性。我们的DEBIAS（具有后门不变聚合症状的持久效应）算法学习非负的、临床可解释的权重用于结果聚合，通过利用精神病学纵向数据中先前治疗的有时限直接效应，最大化持久治疗效果，并在经验上最小化观测和潜在混杂。该算法还提供了一个可经验验证的结果无混杂检验。DEBIAS在抑郁症和精神分裂症的综合实验中，在恢复临床可解释复合结果的因果效应方面，始终优于最先进的方法。", "summary": "本论文提出了一种名为DEBIAS的算法，旨在解决精神病学纵向数据中因果推断的挑战。DEBIAS通过学习临床可解释的权重来优化症状聚合的结果定义，从而最大化持久治疗效果并最小化观测和潜在混杂。该算法还包含一个用于结果无混杂的经验可验证检验。在抑郁症和精神分裂症的实验中，DEBIAS在恢复临床可解释复合结果的因果效应方面表现出优于现有最先进方法的性能。", "keywords": "因果推断, 纵向数据, 精神病学, 混杂因素, 结果聚合", "comments": "本研究的创新之处在于其不局限于传统的协变量调整，而是通过直接优化结果定义来增强因果可识别性，这对于处理精神病学数据中固有的复杂性（如症状异质性和潜在混杂）至关重要。提供结果无混杂的经验可验证检验，进一步提升了该方法的实用性和可靠性。"}}
{"id": "2507.18883", "title": "Success in Humanoid Reinforcement Learning under Partial Observation", "authors": ["Wuhao Wang", "Zhiyong Chen"], "categories": ["cs.AI", "cs.RO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      11 pages, 3 figures, and 4 tables. Not published anywhere else", "url": "http://arxiv.org/abs/2507.18883v1", "summary": "Reinforcement learning has been widely applied to robotic control, but\neffective policy learning under partial observability remains a major\nchallenge, especially in high-dimensional tasks like humanoid locomotion. To\ndate, no prior work has demonstrated stable training of humanoid policies with\nincomplete state information in the benchmark Gymnasium Humanoid-v4\nenvironment. The objective in this environment is to walk forward as fast as\npossible without falling, with rewards provided for staying upright and moving\nforward, and penalties incurred for excessive actions and external contact\nforces. This research presents the first successful instance of learning under\npartial observability in this environment. The learned policy achieves\nperformance comparable to state-of-the-art results with full state access,\ndespite using only one-third to two-thirds of the original states. Moreover,\nthe policy exhibits adaptability to robot properties, such as variations in\nbody part masses. The key to this success is a novel history encoder that\nprocesses a fixed-length sequence of past observations in parallel. Integrated\ninto a standard model-free algorithm, the encoder enables performance on par\nwith fully observed baselines. We hypothesize that it reconstructs essential\ncontextual information from recent observations, thereby enabling robust\ndecision-making.", "comment": "11 pages, 3 figures, and 4 tables. Not published anywhere else", "pdf_url": "http://arxiv.org/pdf/2507.18883v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "部分可观测下人形机器人强化学习的成功", "tldr": "本文首次在部分可观测的Gymnasium Humanoid-v4环境中成功实现了人形机器人强化学习，通过新颖的历史编码器，在仅使用部分状态信息的情况下，达到了与完全观测基线相当的性能，并展现了对机器人属性变化的适应性。", "motivation": "强化学习在机器人控制中应用广泛，但在部分可观测条件下进行有效策略学习仍然是巨大挑战，尤其是在人形机器人运动等高维任务中。此前，在基准Gymnasium Humanoid-v4环境中，尚未有工作能在不完全状态信息下稳定训练人形机器人策略。", "method": "本研究提出了一种新颖的历史编码器，该编码器并行处理固定长度的过去观测序列。它被集成到标准的无模型算法中，从而在部分可观测环境中实现了与完全观测基线相当的性能。", "result": "本研究首次在Gymnasium Humanoid-v4环境中成功实现了部分可观测下的学习。所学策略在仅使用原始状态信息的三分之一到三分之二的情况下，达到了与完全状态访问的最新成果相当的性能。此外，该策略还表现出对机器人属性（如身体部位质量变化）的适应性。", "conclusion": "通过引入新颖的历史编码器，本研究成功解决了部分可观测下人形机器人强化学习的挑战，实现了与完全观测基线相当的性能，并猜测编码器能够从近期观测中重建关键上下文信息，从而实现鲁棒的决策。", "translation": "强化学习已广泛应用于机器人控制，但在部分可观测条件下进行有效策略学习仍然是一个重大挑战，尤其是在人形机器人运动等高维任务中。迄今为止，在基准Gymnasium Humanoid-v4环境中，还没有任何先前的工作能够证明在不完全状态信息下稳定训练人形机器人策略。该环境的目标是在不摔倒的情况下尽可能快地向前行走，通过保持直立和向前移动获得奖励，并因过度动作和外部接触力而受到惩罚。本研究首次成功实现了在该环境中部分可观测下的学习。尽管仅使用原始状态信息的三分之一到三分之二，所学策略仍能达到与完全状态访问的最新成果相当的性能。此外，该策略还表现出对机器人属性（如身体部位质量变化）的适应性。这一成功的关键在于一种新颖的历史编码器，它并行处理固定长度的过去观测序列。该编码器集成到标准的无模型算法中，使其性能与完全观测基线持平。我们假设它从近期观测中重建了必要的上下文信息，从而实现了鲁棒的决策。", "summary": "本研究首次在部分可观测的Gymnasium Humanoid-v4环境中成功实现了人形机器人强化学习。面对高维任务中部分可观测性带来的挑战，论文提出了一种新颖的历史编码器，该编码器能并行处理历史观测序列，并将其集成到标准无模型算法中。实验结果表明，尽管仅使用部分状态信息，所学策略仍能达到与完全观测下最先进方法相当的性能，并展现了对机器人属性变化的适应性，这得益于编码器对关键上下文信息的重建能力。", "keywords": "强化学习, 人形机器人, 部分可观测, 历史编码器, 机器人控制", "comments": "本文的创新点在于提出了一个新颖的历史编码器，成功解决了高维人形机器人任务在部分可观测条件下的强化学习难题，并首次在该基准环境中实现了突破。其重要性在于证明了在信息受限的情况下，通过有效利用历史信息，也能达到与完全信息下相媲美的控制性能，这对于现实世界中传感器受限的机器人应用具有重要意义。该方法在减少所需状态信息的同时保持高性能，也为未来类似研究提供了新的思路。"}}
{"id": "2507.18830", "title": "RealDeal: Enhancing Realism and Details in Brain Image Generation via Image-to-Image Diffusion Models", "authors": ["Shen Zhu", "Yinzhu Jin", "Tyler Spears", "Ifrah Zawar", "P. Thomas Fletcher"], "categories": ["eess.IV", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      19 pages, 10 figures", "url": "http://arxiv.org/abs/2507.18830v1", "summary": "We propose image-to-image diffusion models that are designed to enhance the\nrealism and details of generated brain images by introducing sharp edges, fine\ntextures, subtle anatomical features, and imaging noise. Generative models have\nbeen widely adopted in the biomedical domain, especially in image generation\napplications. Latent diffusion models achieve state-of-the-art results in\ngenerating brain MRIs. However, due to latent compression, generated images\nfrom these models are overly smooth, lacking fine anatomical structures and\nscan acquisition noise that are typically seen in real images. This work\nformulates the realism enhancing and detail adding process as image-to-image\ndiffusion models, which refines the quality of LDM-generated images. We employ\ncommonly used metrics like FID and LPIPS for image realism assessment.\nFurthermore, we introduce new metrics to demonstrate the realism of images\ngenerated by RealDeal in terms of image noise distribution, sharpness, and\ntexture.", "comment": "19 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.18830v1", "cate": "eess.IV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "RealDeal: 通过图像到图像扩散模型增强脑图像生成的真实感和细节", "tldr": "RealDeal提出了一种图像到图像扩散模型，用于增强生成脑图像的真实感和细节，解决现有潜在扩散模型生成图像过于平滑的问题。", "motivation": "现有的潜在扩散模型在生成脑部MRI图像时，由于潜在压缩，生成的图像过于平滑，缺乏真实图像中常见的精细解剖结构和扫描采集噪声。", "method": "本文将真实感增强和细节添加过程表述为图像到图像扩散模型，该模型通过引入锐利边缘、精细纹理、微妙解剖特征和成像噪声来优化潜在扩散模型生成的图像质量。", "result": "Not mentioned in abstract", "conclusion": "该工作通过图像到图像扩散模型成功提升了生成脑图像的真实感和细节，解决了现有生成模型图像平滑度过高的问题，并通过新旧指标验证了其有效性。", "translation": "我们提出了图像到图像扩散模型，旨在通过引入锐利边缘、精细纹理、微妙解剖特征和成像噪声，增强生成脑图像的真实感和细节。生成模型已广泛应用于生物医学领域，尤其是在图像生成应用中。潜在扩散模型在生成脑部MRI方面取得了最先进的结果。然而，由于潜在压缩，这些模型生成的图像过于平滑，缺乏真实图像中常见的精细解剖结构和扫描采集噪声。这项工作将真实感增强和细节添加过程表述为图像到图像扩散模型，从而提升了LDM生成图像的质量。我们采用FID和LPIPS等常用指标进行图像真实感评估。此外，我们引入了新指标，以展示RealDeal生成的图像在图像噪声分布、清晰度和纹理方面的真实感。", "summary": "RealDeal提出了一种基于图像到图像扩散模型的新方法，旨在解决现有潜在扩散模型在生成脑图像时存在的平滑度过高、缺乏细节和噪声的问题。该模型通过引入锐利边缘、精细纹理、微妙解剖特征和成像噪声，有效提升了生成脑图像的真实感和细节。研究采用FID、LPIPS以及新引入的图像噪声分布、清晰度和纹理指标来评估模型的性能。", "keywords": "图像到图像扩散模型, 脑图像生成, 真实感增强, 细节添加, 潜在扩散模型", "comments": "RealDeal的创新之处在于将图像到图像扩散模型应用于增强生成脑图像的真实感，特别关注了细节和噪声的添加，这弥补了现有潜在扩散模型在生物医学图像生成中存在的不足。引入新的评估指标也体现了其对图像质量多维度的考量。"}}
{"id": "2507.19283", "title": "Towards LLM-Enhanced Group Recommender Systems", "authors": ["Sebastian Lubos", "Alexander Felfernig", "Thi Ngoc Trang Tran", "Viet-Man Le", "Damian Garber", "Manuel Henrich", "Reinhard Willfort", "Jeremias Fuchs"], "categories": ["cs.IR", "cs.AI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19283v1", "summary": "In contrast to single-user recommender systems, group recommender systems are\ndesigned to generate and explain recommendations for groups. This\ngroup-oriented setting introduces additional complexities, as several factors -\nabsent in individual contexts - must be addressed. These include understanding\ngroup dynamics (e.g., social dependencies within the group), defining effective\ndecision-making processes, ensuring that recommendations are suitable for all\ngroup members, and providing group-level explanations as well as explanations\nfor individual users. In this paper, we analyze in which way large language\nmodels (LLMs) can support these aspects and help to increase the overall\ndecision support quality and applicability of group recommender systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19283v1", "cate": "cs.IR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "迈向LLM增强的群组推荐系统", "tldr": "本文探讨大型语言模型（LLMs）如何支持群组推荐系统，以提升决策支持质量和适用性。", "motivation": "与单用户推荐系统不同，群组推荐系统面临更多复杂性，包括理解群组动态、制定有效的决策过程、确保推荐适用于所有成员以及提供群组和个人层面的解释。本文旨在分析LLMs如何解决这些挑战。", "method": "本文分析大型语言模型（LLMs）如何支持群组推荐系统中的各种复杂因素，以提高整体决策支持质量和适用性。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "与单用户推荐系统不同，群组推荐系统旨在为群组生成和解释推荐。这种面向群组的设置引入了额外的复杂性，因为必须解决个人环境中不存在的几个因素。这些因素包括理解群组动态（例如，群组内的社会依赖）、定义有效的决策过程、确保推荐适用于所有群组成员，以及提供群组层面的解释和针对个别用户的解释。在本文中，我们分析了大型语言模型（LLMs）如何支持这些方面，并帮助提高群组推荐系统的整体决策支持质量和适用性。", "summary": "本文探讨了大型语言模型（LLMs）在群组推荐系统中的应用潜力。群组推荐系统相较于单用户系统面临更多挑战，如理解群组动态、决策过程、成员适用性及解释性。研究旨在分析LLMs如何帮助应对这些复杂性，从而提升群组推荐系统的决策支持质量和实际应用价值。", "keywords": "群组推荐系统, 大型语言模型, 决策支持, 群组动态, 推荐系统", "comments": "本文的创新点在于提出了将大型语言模型应用于群组推荐系统，以解决其固有的复杂性。这对于提升群组决策支持的质量和推荐的适用性具有重要意义。该研究方向具有前瞻性，可能为未来的群组推荐系统发展开辟新途径。"}}
{"id": "2507.11306", "title": "P.808 Multilingual Speech Enhancement Testing: Approach and Results of URGENT 2025 Challenge", "authors": ["Marvin Sach", "Yihui Fu", "Kohei Saijo", "Wangyou Zhang", "Samuele Cornell", "Robin Scheibler", "Chenda Li", "Anurag Kumar", "Wei Wang", "Yanmin Qian", "Shinji Watanabe", "Tim Fingscheidt"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      5 pages, 2 figures", "url": "http://arxiv.org/abs/2507.11306v2", "summary": "In speech quality estimation for speech enhancement (SE) systems, subjective\nlistening tests so far are considered as the gold standard. This should be even\nmore true considering the large influx of new generative or hybrid methods into\nthe field, revealing issues of some objective metrics. Efforts such as the\nInterspeech 2025 URGENT Speech Enhancement Challenge also involving non-English\ndatasets add the aspect of multilinguality to the testing procedure. In this\npaper, we provide a brief recap of the ITU-T P.808 crowdsourced subjective\nlistening test method. A first novel contribution is our proposed process of\nlocalizing both text and audio components of Naderi and Cutler's implementation\nof crowdsourced subjective absolute category rating (ACR) listening tests\ninvolving text-to-speech (TTS). Further, we provide surprising analyses of and\ninsights into URGENT Challenge results, tackling the reliability of (P.808) ACR\nsubjective testing as gold standard in the age of generative AI. Particularly,\nit seems that for generative SE methods, subjective (ACR MOS) and objective\n(DNSMOS, NISQA) reference-free metrics should be accompanied by objective phone\nfidelity metrics to reliably detect hallucinations. Finally, we will soon\nrelease our localization scripts and methods for easy deployment for new\nmultilingual speech enhancement subjective evaluations according to ITU-T\nP.808.", "comment": "5 pages, 2 figures", "pdf_url": "http://arxiv.org/pdf/2507.11306v2", "cate": "eess.AS", "date": "2025-07-15", "updated": "2025-07-25", "AI": {"title_translation": "P.808 多语言语音增强测试：URGENT 2025 挑战赛的方法与结果", "tldr": "本文讨论了 URGENT 2025 多语言语音增强挑战赛，重点关注 P.808 主观测试的可靠性，尤其是在生成式 AI 时代，并建议结合主观和客观指标以及语音保真度指标来检测幻觉。", "motivation": "在语音增强 (SE) 系统的语音质量评估中，主观听力测试被视为黄金标准，但随着新的生成式或混合方法的出现，一些客观指标暴露出问题。Interspeech 2025 URGENT 语音增强挑战赛引入了多语言方面。本文旨在探讨在生成式 AI 时代，P.808 ACR 主观测试作为黄金标准的可靠性问题。", "method": "本文回顾了 ITU-T P.808 众包主观听力测试方法。提出了一种新的方法，用于本地化 Naderi 和 Cutler 的众包主观绝对类别评级 (ACR) 听力测试中的文本和音频组件，该测试涉及文本到语音 (TTS)。此外，本文还对 URGENT 挑战赛的结果进行了分析。", "result": "研究发现，对于生成式语音增强方法，主观 (ACR MOS) 和客观 (DNSMOS, NISQA) 无参考指标似乎需要辅以客观语音保真度指标，以可靠地检测幻觉。", "conclusion": "对于生成式语音增强方法，为了可靠地检测幻觉，主观和客观的无参考指标应与客观语音保真度指标相结合。作者将发布其本地化脚本和方法，以便于未来进行新的多语言语音增强主观评估。", "translation": "在语音增强 (SE) 系统的语音质量评估中，主观听力测试迄今为止被认为是黄金标准。考虑到新的生成式或混合方法大量涌入该领域，这甚至更应如此，因为这些方法暴露出一些客观指标的问题。例如 Interspeech 2025 URGENT 语音增强挑战赛，其中也涉及非英语数据集，为测试程序增加了多语言方面。在本文中，我们简要回顾了 ITU-T P.808 众包主观听力测试方法。第一个新颖的贡献是我们提出的本地化 Naderi 和 Cutler 的众包主观绝对类别评级 (ACR) 听力测试（涉及文本到语音 (TTS)）的文本和音频组件的过程。此外，我们对 URGENT 挑战赛的结果进行了令人惊讶的分析和深入见解，探讨了在生成式 AI 时代，(P.808) ACR 主观测试作为黄金标准的可靠性。特别是，对于生成式 SE 方法，主观 (ACR MOS) 和客观 (DNSMOS, NISQA) 无参考指标似乎需要辅以客观语音保真度指标，以可靠地检测幻觉。最后，我们将很快发布我们的本地化脚本和方法，以便于根据 ITU-T P.808 轻松部署新的多语言语音增强主观评估。", "summary": "本文探讨了评估多语言语音增强系统的挑战，尤其是在生成式 AI 兴起的背景下。论文回顾了 ITU-T P.808 主观测试方法，并提出了一种用于众包 ACR 测试的新型本地化流程。通过分析 URGENT 2025 挑战赛的结果，作者发现对于生成式语音增强，传统的主观和客观指标可能不足，并建议结合客观语音保真度指标以可靠地检测幻觉。论文还宣布将发布其本地化工具。", "keywords": "语音增强, P.808, 主观测试, 多语言, 生成式 AI, 幻觉", "comments": "这篇论文很重要，因为它揭示了现有黄金标准主观评估方法 (P.808 ACR) 在应用于基于生成式 AI 的新型语音增强系统时的局限性，特别是关于“幻觉”问题。所提出的众包测试本地化方法是对多语言评估的实际创新。发现需要将客观语音保真度指标与传统主观/客观指标结合使用，是该领域未来研究和开发的关键见解。"}}
{"id": "2410.10780", "title": "MaskControl: Spatio-Temporal Control for Masked Motion Synthesis", "authors": ["Ekkasit Pinyoanuntapong", "Muhammad Usama Saleem", "Korrawe Karunratanakul", "Pu Wang", "Hongfei Xue", "Chen Chen", "Chuan Guo", "Junli Cao", "Jian Ren", "Sergey Tulyakov"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Camera Ready Version. ICCV2025 (Oral). Change name from ControlMM to MaskControl. project page this https URL", "url": "http://arxiv.org/abs/2410.10780v3", "summary": "Recent advances in motion diffusion models have enabled spatially\ncontrollable text-to-motion generation. However, these models struggle to\nachieve high-precision control while maintaining high-quality motion\ngeneration. To address these challenges, we propose MaskControl, the first\napproach to introduce controllability to the generative masked motion model.\nOur approach introduces two key innovations. First, \\textit{Logits Regularizer}\nimplicitly perturbs logits at training time to align the distribution of motion\ntokens with the controlled joint positions, while regularizing the categorical\ntoken prediction to ensure high-fidelity generation. Second, \\textit{Logit\nOptimization} explicitly optimizes the predicted logits during inference time,\ndirectly reshaping the token distribution that forces the generated motion to\naccurately align with the controlled joint positions. Moreover, we introduce\n\\textit{Differentiable Expectation Sampling (DES)} to combat the\nnon-differential distribution sampling process encountered by logits\nregularizer and optimization. Extensive experiments demonstrate that\nMaskControl outperforms state-of-the-art methods, achieving superior motion\nquality (FID decreases by ~77\\%) and higher control precision (average error\n0.91 vs. 1.08). Additionally, MaskControl enables diverse applications,\nincluding any-joint-any-frame control, body-part timeline control, and\nzero-shot objective control. Video visualization can be found at\nhttps://www.ekkasit.com/ControlMM-page/", "comment": "Camera Ready Version. ICCV2025 (Oral). Change name from ControlMM to\n  MaskControl. project page https://exitudio.github.io/ControlMM-page", "pdf_url": "http://arxiv.org/pdf/2410.10780v3", "cate": "cs.CV", "date": "2024-10-14", "updated": "2025-07-25", "AI": {"title_translation": "MaskControl：蒙版运动合成的时空控制", "tldr": "MaskControl是一种新的方法，通过引入Logits Regularizer、Logit Optimization和Differentiable Expectation Sampling，首次为生成式蒙版运动模型带来了高精度时空控制，显著提高了运动质量和控制精度，并支持多种应用。", "motivation": "现有的运动扩散模型在实现高精度控制的同时，难以保持高质量的运动生成，这是一个亟待解决的挑战。", "method": "本文提出了MaskControl，这是首次将可控性引入生成式蒙版运动模型的方法。它引入了两项关键创新：Logits Regularizer在训练时隐式扰动logits，以使运动令牌分布与受控关节位置对齐，并正则化分类令牌预测以确保高保真生成；Logit Optimization在推理时显式优化预测的logits，直接重塑令牌分布以强制生成的运动精确对齐受控关节位置。此外，为了解决logits正则化器和优化过程中遇到的不可微分分布采样问题，本文引入了Differentiable Expectation Sampling (DES)。", "result": "实验结果表明，MaskControl优于现有最先进的方法，实现了卓越的运动质量（FID降低约77%）和更高的控制精度（平均误差0.91 vs. 1.08）。此外，MaskControl还支持多种应用，包括任意关节任意帧控制、身体部位时间线控制和零样本目标控制。", "conclusion": "MaskControl成功地为生成式蒙版运动模型引入了高精度的时空控制，显著提升了运动质量和控制精度，并展现了在多样化应用中的潜力。", "translation": "运动扩散模型的最新进展使得空间可控的文本到运动生成成为可能。然而，这些模型在实现高精度控制的同时，难以保持高质量的运动生成。为了解决这些挑战，我们提出了MaskControl，这是首次将可控性引入生成式蒙版运动模型的方法。我们的方法引入了两项关键创新。首先，Logits Regularizer在训练时隐式扰动logits，以使运动令牌分布与受控关节位置对齐，同时正则化分类令牌预测以确保高保真生成。其次，Logit Optimization在推理时显式优化预测的logits，直接重塑令牌分布，强制生成的运动精确对齐受控关节位置。此外，我们引入了Differentiable Expectation Sampling (DES) 来解决logits正则化器和优化过程中遇到的不可微分分布采样问题。广泛的实验表明，MaskControl优于现有最先进的方法，实现了卓越的运动质量（FID降低约77%）和更高的控制精度（平均误差0.91 vs. 1.08）。此外，MaskControl还支持多种应用，包括任意关节任意帧控制、身体部位时间线控制和零样本目标控制。视频可视化可在https://www.ekkasit.com/ControlMM-page/找到。", "summary": "MaskControl是一种开创性的方法，旨在解决现有运动扩散模型在保持高质量运动生成的同时，难以实现高精度时空控制的问题。该方法通过引入Logits Regularizer和Logit Optimization两大核心创新，分别在训练和推理阶段对运动令牌分布进行精确控制，并辅以Differentiable Expectation Sampling来克服采样过程中的不可微分性。实验证明，MaskControl在运动质量和控制精度上均显著优于SOTA方法，并支持多种灵活的运动控制应用。", "keywords": "MaskControl, 运动合成, 时空控制, 蒙版运动模型, 扩散模型", "comments": "MaskControl的创新性在于首次将高精度可控性引入生成式蒙版运动模型，通过Logits Regularizer和Logit Optimization在训练和推理阶段分别进行隐式和显式控制，并引入Differentiable Expectation Sampling来解决技术难题。其在运动质量和控制精度上的显著提升（FID降低约77%，误差降低）以及支持多样化应用的能力，显示了其在运动合成领域的巨大潜力。"}}
{"id": "2507.18847", "title": "Equivariant Volumetric Grasping", "authors": ["Pinhao Song", "Yutong Hu", "Pengteng Li", "Renaud Detry"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      19 pages", "url": "http://arxiv.org/abs/2507.18847v1", "summary": "We propose a new volumetric grasp model that is equivariant to rotations\naround the vertical axis, leading to a significant improvement in sample\nefficiency. Our model employs a tri-plane volumetric feature representation --\ni.e., the projection of 3D features onto three canonical planes. We introduce a\nnovel tri-plane feature design in which features on the horizontal plane are\nequivariant to 90{\\deg} rotations, while the sum of features from the other two\nplanes remains invariant to the same transformations. This design is enabled by\na new deformable steerable convolution, which combines the adaptability of\ndeformable convolutions with the rotational equivariance of steerable ones.\nThis allows the receptive field to adapt to local object geometry while\npreserving equivariance properties. We further develop equivariant adaptations\nof two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically,\nwe derive a new equivariant formulation of IGD's deformable attention mechanism\nand propose an equivariant generative model of grasp orientations based on flow\nmatching. We provide a detailed analytical justification of the proposed\nequivariance properties and validate our approach through extensive simulated\nand real-world experiments. Our results demonstrate that the proposed\nprojection-based design significantly reduces both computational and memory\ncosts. Moreover, the equivariant grasp models built on top of our tri-plane\nfeatures consistently outperform their non-equivariant counterparts, achieving\nhigher performance with only a modest computational overhead. Video and code\ncan be viewed in: https://mousecpn.github.io/evg-page/", "comment": "19 pages", "pdf_url": "http://arxiv.org/pdf/2507.18847v1", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "等变体积抓取", "tldr": "提出了一种新的等变体积抓取模型，采用三平面特征表示和可变形可控卷积，显著提高了样本效率并优于非等变模型。", "motivation": "为了显著提高抓取模型的样本效率，研究等变性对体积抓取的影响。", "method": "该研究提出了一种新的体积抓取模型，该模型对绕垂直轴的旋转具有等变性。模型采用三平面体积特征表示，其中水平平面特征对90度旋转等变，而另外两个平面的特征之和对相同变换保持不变。这通过一种新的可变形可控卷积实现，该卷积结合了可变形卷积的适应性和可控卷积的旋转等变性。此外，该研究还开发了两种最先进的体积抓取规划器GIGA和IGD的等变适应版本，包括IGD可变形注意力机制的等变公式以及基于流匹配的抓取方向等变生成模型。", "result": "所提出的基于投影的设计显著降低了计算和内存成本。基于三平面特征构建的等变抓取模型始终优于其非等变对应物，以适度的计算开销实现了更高的性能。", "conclusion": "该研究成功地提出并验证了一种等变体积抓取模型，通过创新的三平面特征表示和可变形可控卷积，显著提高了抓取效率和性能，并在模拟和真实世界实验中得到了证实。", "translation": "我们提出了一种新的体积抓取模型，该模型对绕垂直轴的旋转具有等变性，从而显著提高了样本效率。我们的模型采用三平面体积特征表示——即将3D特征投影到三个标准平面上。我们引入了一种新颖的三平面特征设计，其中水平平面上的特征对90度旋转等变，而另外两个平面的特征之和对相同的变换保持不变。这种设计通过一种新的可变形可控卷积实现，它结合了可变形卷积的适应性与可控卷积的旋转等变性。这使得感受野能够适应局部物体几何形状，同时保持等变性。我们进一步开发了两种最先进的体积抓取规划器GIGA和IGD的等变适应版本。具体来说，我们推导了IGD可变形注意力机制的新等变公式，并提出了一种基于流匹配的抓取方向等变生成模型。我们对所提出的等变特性提供了详细的分析论证，并通过广泛的模拟和真实世界实验验证了我们的方法。我们的结果表明，所提出的基于投影的设计显著降低了计算和内存成本。此外，基于我们的三平面特征构建的等变抓取模型始终优于其非等变对应物，以适度的计算开销实现了更高的性能。视频和代码可在以下网址查看：https://mousecpn.github.io/evg-page/", "summary": "本论文提出了一种名为“等变体积抓取”的新型抓取模型，该模型对垂直轴旋转具有等变性，显著提升了样本效率。核心在于创新的三平面体积特征表示和可变形可控卷积，后者结合了适应性和旋转等变性。研究还对现有抓取规划器GIGA和IGD进行了等变性改造。实验证明，该模型不仅大幅降低了计算和内存成本，而且在性能上显著超越了非等变模型，证明了其在机器人抓取领域的有效性和优越性。", "keywords": "等变性, 体积抓取, 三平面特征, 可变形可控卷积, 机器人抓取", "comments": "该论文的创新点在于将等变性引入体积抓取模型，特别是通过新颖的三平面特征表示和可变形可控卷积来实现。这种设计不仅解决了传统模型在旋转不变性方面的局限，还显著提高了样本效率、降低了计算成本，并提升了抓取性能。其结合适应性与等变性的思路对未来的机器人感知和操作任务具有重要启发意义，尤其是在需要处理复杂几何和姿态变化的场景中。"}}
{"id": "2507.19346", "title": "Short-Form Video Recommendations with Multimodal Embeddings: Addressing Cold-Start and Bias Challenges", "authors": ["Andrii Dzhoha", "Katya Mirylenka", "Egor Malykh", "Marco-Andrea Buchmann", "Francesca Catino"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19346v1", "summary": "In recent years, social media users have spent significant amounts of time on\nshort-form video platforms. As a result, established platforms in other\ndomains, such as e-commerce, have begun introducing short-form video content to\nengage users and increase their time spent on the platform. The success of\nthese experiences is due not only to the content itself but also to a unique UI\ninnovation: instead of offering users a list of choices to click, platforms\nactively recommend content for users to watch one at a time. This creates new\nchallenges for recommender systems, especially when launching a new video\nexperience. Beyond the limited interaction data, immersive feed experiences\nintroduce stronger position bias due to the UI and duration bias when\noptimizing for watch-time, as models tend to favor shorter videos. These\nissues, together with the feedback loop inherent in recommender systems, make\nit difficult to build effective solutions. In this paper, we highlight the\nchallenges faced when introducing a new short-form video experience and present\nour experience showing that, even with sufficient video interaction data, it\ncan be more beneficial to leverage a video retrieval system using a fine-tuned\nmultimodal vision-language model to overcome these challenges. This approach\ndemonstrated greater effectiveness compared to conventional supervised learning\nmethods in online experiments conducted on our e-commerce platform.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19346v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "多模态嵌入的短视频推荐：解决冷启动和偏差挑战", "tldr": "该论文提出了一种利用微调多模态视觉-语言模型进行视频检索的方法，以解决短视频推荐中的冷启动和偏差挑战，并在在线实验中表现出比传统监督学习方法更好的效果。", "motivation": "近年来，用户在短视频平台上花费了大量时间，导致电商等其他领域平台也开始引入短视频内容以吸引用户。然而，这种独特的推荐模式带来了新的挑战，尤其是在推出新的视频体验时，由于有限的互动数据、UI导致的强位置偏差以及优化观看时长时倾向于推荐短视频的时长偏差，使得构建有效的推荐系统变得困难。", "method": "该论文提出了一种利用微调多模态视觉-语言模型构建视频检索系统的方法。该系统旨在克服短视频推荐中因有限互动数据、位置偏差和时长偏差带来的挑战。", "result": "在电商平台进行的在线实验中，该方法与传统的监督学习方法相比，展现出更高的有效性。", "conclusion": "即使在视频互动数据充足的情况下，利用微调的多模态视觉-语言模型构建视频检索系统，也能更有效地克服短视频推荐中的冷启动和各种偏差挑战。", "translation": "近年来，社交媒体用户在短视频平台上花费了大量时间。因此，其他领域的成熟平台，如电子商务，已经开始引入短视频内容，以吸引用户并增加他们在平台上的停留时间。这些体验的成功不仅归因于内容本身，还在于独特的UI创新：平台不是向用户提供可点击的选择列表，而是主动向用户推荐内容，一次观看一个。这为推荐系统带来了新的挑战，尤其是在推出新的视频体验时。除了有限的互动数据，沉浸式信息流体验由于UI引入了更强的位置偏差，以及在优化观看时长时可能出现的时长偏差，因为模型倾向于偏爱较短的视频。这些问题，加上推荐系统固有的反馈循环，使得构建有效的解决方案变得困难。在本文中，我们强调了引入新的短视频体验时面临的挑战，并展示了我们的经验，即使有足够的视频互动数据，利用使用微调多模态视觉-语言模型的视频检索系统也能更有效地克服这些挑战。与在我们的电商平台上进行的在线实验中的传统监督学习方法相比，这种方法展示了更高的有效性。", "summary": "该论文探讨了短视频推荐系统面临的挑战，特别是在冷启动、位置偏差和时长偏差方面。作者提出了一种创新的解决方案，即利用经过微调的多模态视觉-语言模型构建视频检索系统。通过在电商平台上的在线实验，该方法被证明比传统监督学习方法更有效，能够更好地解决短视频推荐中的复杂问题。", "keywords": "短视频推荐, 多模态嵌入, 冷启动, 偏差挑战, 视觉-语言模型", "comments": "该论文的创新点在于将多模态视觉-语言模型应用于短视频推荐的检索阶段，特别关注了冷启动和多种偏差（位置偏差、时长偏差）等核心挑战。这种方法突破了传统监督学习对大量互动数据的依赖，为新兴的短视频内容平台提供了更鲁棒的解决方案，尤其是在用户数据稀疏的早期阶段。其重要性在于为电商等非原生短视频平台提供了有效的技术路径，以更好地利用短视频内容吸引用户，具有重要的实践意义。"}}
{"id": "2507.19390", "title": "ReCatcher: Towards LLMs Regression Testing for Code Generation", "authors": ["Altaf Allah Abbassi", "Leuson Da Silva", "Amin Nikanjam", "Foutse Khomh"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      24 pages, 3 Figures, 2 Tables", "url": "http://arxiv.org/abs/2507.19390v1", "summary": "Large Language Models (LLMs) for code generation evolve rapidly through\nfine-tuning, merging, or new model releases. However, such updates can\nintroduce regressions, not only in correctness but also in code quality and\nperformance. To address this, we present ReCatcher, a regression testing\nframework for Python code generation. ReCatcher systematically compares two\nLLMs, typically a current model and a candidate update, across three\ndimensions: logical correctness, static code quality, and execution\nperformance. We apply ReCatcher to assess regressions across three update\nscenarios, fine-tuning, merging, and model release, using CodeLlama,\nDeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with\ncross-language datasets increases syntax errors by up to 12%. Merging with\ngeneral-purpose models like Llama2 leads to regressions in correctness by up to\n18%. GPT-4o introduces regressions of up to 50% in handling missing imports\ncompared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance\ndegradation in execution time versus GPT-4o. Overall, logical correctness,\nperformance, and error handling (e.g., syntax errors and missing imports) are\nthe most regression-prone areas. Comparing ReCatcher with baseline solutions,\nit presents better and consistent accuracy across logical and performance\naspects. ReCatcher highlights the importance of systematic regression\nevaluation before adopting new models, while assisting researchers and\npractitioners in making more informed update decisions.", "comment": "24 pages, 3 Figures, 2 Tables", "pdf_url": "http://arxiv.org/pdf/2507.19390v1", "cate": "cs.SE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "ReCatcher：迈向代码生成领域大型语言模型的回归测试", "tldr": "ReCatcher是一个针对代码生成大型语言模型（LLMs）的回归测试框架，旨在系统性地评估模型更新（如微调、合并或新模型发布）可能引入的正确性、代码质量和性能方面的退化。", "motivation": "大型语言模型（LLMs）在代码生成领域通过微调、合并或新模型发布迅速发展。然而，这些更新可能导致回归，不仅影响正确性，还会影响代码质量和性能。为了解决这个问题，本文提出了ReCatcher。", "method": "本文提出了ReCatcher，一个用于Python代码生成的回归测试框架。ReCatcher系统地比较两个LLM（通常是当前模型和候选更新模型），评估其在逻辑正确性、静态代码质量和执行性能三个维度上的表现。作者将ReCatcher应用于微调、模型合并和新模型发布三种更新场景，并使用了CodeLlama、DeepSeek-Coder和GPT-4o等模型进行评估。", "result": "评估结果显示，使用跨语言数据集进行微调会使语法错误增加高达12%。与Llama2等通用模型合并会导致正确性回归高达18%。GPT-4o在处理缺失导入方面比GPT-3.5-turbo引入高达50%的回归，而GPT-4o-mini在执行时间上比GPT-4o性能下降高达80%。总体而言，逻辑正确性、性能和错误处理（如语法错误和缺失导入）是最容易发生回归的领域。与基线解决方案相比，ReCatcher在逻辑和性能方面表现出更好且一致的准确性。", "conclusion": "ReCatcher强调了在采用新模型之前进行系统性回归评估的重要性，并协助研究人员和实践者做出更明智的模型更新决策。", "translation": "大型语言模型（LLMs）在代码生成领域通过微调、合并或新模型发布迅速发展。然而，这些更新可能导致回归，不仅影响正确性，还会影响代码质量和性能。为了解决这个问题，我们提出了ReCatcher，一个用于Python代码生成的回归测试框架。ReCatcher系统地比较两个LLM，通常是当前模型和候选更新模型，评估其在逻辑正确性、静态代码质量和执行性能三个维度上的表现。我们将ReCatcher应用于微调、合并和模型发布三种更新场景，并使用CodeLlama、DeepSeek-Coder和GPT-4o进行了回归评估。我们的评估显示，使用跨语言数据集进行微调会使语法错误增加高达12%。与Llama2等通用模型合并会导致正确性回归高达18%。GPT-4o在处理缺失导入方面比GPT-3.5-turbo引入高达50%的回归，而GPT-4o-mini在执行时间上比GPT-4o性能下降高达80%。总体而言，逻辑正确性、性能和错误处理（如语法错误和缺失导入）是最容易发生回归的领域。与基线解决方案相比，ReCatcher在逻辑和性能方面表现出更好且一致的准确性。ReCatcher强调了在采用新模型之前进行系统性回归评估的重要性，同时协助研究人员和实践者做出更明智的更新决策。", "summary": "ReCatcher是一个针对代码生成LLMs的回归测试框架，旨在解决模型更新（如微调、合并、新模型发布）可能引入的正确性、代码质量和性能退化问题。该框架系统地比较两个LLM在逻辑正确性、静态代码质量和执行性能三个维度上的表现。通过对CodeLlama、DeepSeek-Coder和GPT-4o进行评估，研究发现微调、合并和新模型发布均可能导致显著的回归，尤其是在逻辑正确性、性能和错误处理方面。ReCatcher在评估准确性上优于基线方案，强调了在采用新模型前进行系统性回归评估的重要性。", "keywords": "LLMs, 回归测试, 代码生成, ReCatcher, 代码质量", "comments": "ReCatcher的创新之处在于其针对代码生成LLM的系统性回归测试框架，能够全面评估模型在更新后在正确性、代码质量和性能方面的退化。该研究通过实证分析揭示了LLM更新中常见的回归模式和高风险领域，为LLM开发者和使用者提供了宝贵的指导，有助于做出更明智的模型更新决策。其重要性在于填补了LLM代码生成领域回归测试的空白，提升了模型部署的可靠性。"}}
{"id": "2507.19108", "title": "Downward self-reducibility in the total function polynomial hierarchy", "authors": ["Karthik Gajulapalli", "Surendra Ghentiyala", "Zeyong Li", "Sidhant Saraogi"], "categories": ["cs.CC", "cs.DS"], "primary_category": "Subjects:       Computational Complexity (cs.CC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19108v1", "summary": "A problem $\\mathcal{P}$ is considered downward self-reducible, if there\nexists an efficient algorithm for $\\mathcal{P}$ that is allowed to make queries\nto only strictly smaller instances of $\\mathcal{P}$. Downward self-reducibility\nhas been well studied in the case of decision problems, and it is well known\nthat any downward self-reducible problem must lie in $\\mathsf{PSPACE}$. Harsha,\nMitropolsky and Rosen [ITCS, 2023] initiated the study of downward self\nreductions in the case of search problems. They showed the following\ninteresting collapse: if a problem is in $\\mathsf{TFNP}$ and also downward\nself-reducible, then it must be in $\\mathsf{PLS}$. Moreover, if the problem\nadmits a unique solution then it must be in $\\mathsf{UEOPL}$.\n  We demonstrate that this represents just the tip of a much more general\nphenomenon, which holds for even harder search problems that lie higher up in\nthe total function polynomial hierarchy ($\\mathsf{TF\\Sigma_i^P}$). In fact,\neven if we allow our downward self-reduction to be much more powerful, such a\ncollapse will still occur.\n  We show that any problem in $\\mathsf{TF\\Sigma_i^P}$ which admits a randomized\ndownward self-reduction with access to a $\\mathsf{\\Sigma_{i-1}^P}$ oracle must\nbe in $\\mathsf{PLS}^{\\mathsf{\\Sigma_{i-1}^P}}$. If the problem has\n\\textit{essentially unique solutions} then it lies in\n$\\mathsf{UEOPL}^{\\mathsf{\\Sigma_{i-1}^P}}$.\n  As one (out of many) application of our framework, we get new upper bounds\nfor the problems $\\mathrm{Range Avoidance}$ and $\\mathrm{Linear Ordering\nPrinciple}$ and show that they are both in $\\mathsf{UEOPL}^{\\mathsf{NP}}$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19108v1", "cate": "cs.CC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "全函数多项式层次中的向下自归约性", "tldr": "本文将向下自归约性的研究扩展到全函数多项式层次（TFΣi^P）的更高层级，表明具有随机向下自归约性的问题（即使具有强大的预言机）也会坍缩到PLS或UEOPL，并为范围规避等问题提供了新的上界。", "motivation": "Harsha, Mitropolsky和Rosen（2023）的研究开启了对全函数问题（TFNP）中向下自归约性的探索，并发现了一个有趣的坍缩现象。本文旨在证明这是一种更普遍的现象，适用于全函数多项式层次中更难的搜索问题。", "method": "本文证明了即使允许更强大的向下自归约（带预言机的随机自归约），这种坍缩现象仍会发生。具体而言，证明了TFΣi^P中的任何问题，如果存在一个可访问Σi-1^P预言机的随机向下自归约，则必然属于PLS^Σi-1^P。如果问题具有本质上唯一的解，则属于UEOPL^Σi-1^P。此外，将该框架应用于具体问题。", "result": "1. 任何在TFΣi^P中且具有可访问Σi-1^P预言机的随机向下自归约的问题，都必然在PLS^Σi-1^P中。\n2. 如果问题具有本质上唯一的解，则它位于UEOPL^Σi-1^P中。\n3. 为“范围规避”（Range Avoidance）和“线性排序原理”（Linear Ordering Principle）问题获得了新的上界，表明它们都属于UEOPL^NP。", "conclusion": "本文扩展了对搜索问题中向下自归约性的理解，揭示了全函数多项式层次中普遍存在的坍缩现象，即使在放宽归约条件的情况下也成立，并为特定问题提供了新的复杂度界限。", "translation": "一个问题$\\\\mathcal{P}$如果存在一个高效算法，允许仅查询比$\\\\mathcal{P}$严格更小的实例，则被认为是向下自归约的。向下自归约性在判定问题中得到了充分研究，众所周知，任何向下自归约问题都必须位于$\\\\mathsf{PSPACE}$中。Harsha, Mitropolsky和Rosen [ITCS, 2023] 开始了对搜索问题中向下自归约的研究。他们展示了以下有趣的坍缩：如果一个问题在$\\\\mathsf{TFNP}$中并且也是向下自归约的，那么它必然在$\\\\mathsf{PLS}$中。此外，如果该问题允许唯一解，那么它必然在$\\\\mathsf{UEOPL}$中。\n我们证明这只是一个更普遍现象的冰山一角，该现象甚至适用于全函数多项式层次（$\\\\mathsf{TF\\\\Sigma_i^P}$）中更高层的更难的搜索问题。事实上，即使我们允许向下自归约更加强大，这种坍缩仍然会发生。\n我们表明，任何在$\\\\mathsf{TF\\\\Sigma_i^P}$中且允许访问$\\\\mathsf{\\\\Sigma_{i-1}^P}$预言机的随机向下自归约的问题，都必须在$\\\\mathsf{PLS}^{\\\\mathsf{\\\\Sigma_{i-1}^P}}$中。如果问题具有\\\\textit{本质上唯一的解}，那么它位于$\\\\mathsf{UEOPL}^{\\\\mathsf{\\\\Sigma_{i-1}^P}}$中。\n作为我们框架的（众多）应用之一，我们获得了“范围规避”（Range Avoidance）和“线性排序原理”（Linear Ordering Principle）问题的新上界，并表明它们都属于$\\\\mathsf{UEOPL}^{\\\\mathsf{NP}}$。", "summary": "本文将向下自归约性的概念从全函数问题（TFNP）扩展到全函数多项式层次（TFΣi^P）的更高层级。研究表明，在TFΣi^P中的问题，即使允许更强大的（随机且带预言机的）向下自归约，也会表现出类似的复杂度坍缩现象：它们必然位于PLS^Σi-1^P中，对于本质上唯一解的问题则位于UEOPL^Σi-1^P中。作为应用，本文为“范围规避”和“线性排序原理”问题提供了新的上界，将其归入UEOPL^NP。", "keywords": "向下自归约性, 全函数多项式层次, 搜索问题, 复杂度理论, PLS, UEOPL", "comments": "本文的创新之处在于将已知的向下自归约性导致的复杂度坍缩现象，从特定的TFNP类推广到更普遍且复杂度更高的全函数多项式层次（TFΣi^P），并证明了即使在放宽自归约条件（允许随机性和预言机）的情况下，这种坍缩依然成立。这极大地深化了对搜索问题复杂度和自归约性之间关系的理解，提供了一个更通用的理论框架。此外，对具体问题（如“范围规避”和“线性排序原理”）给出新的复杂度上界，也突显了其理论成果的实际应用价值。"}}
{"id": "2502.14561", "title": "Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs", "authors": ["Paris Koloveas", "Serafeim Chatzopoulos", "Thanasis Vergoulis", "Christos Tryfonopoulos"], "categories": ["cs.CL", "cs.DL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted for publication on TPDL 2025", "url": "http://arxiv.org/abs/2502.14561v3", "summary": "This work investigates the ability of open Large Language Models (LLMs) to\npredict citation intent through in-context learning and fine-tuning. Unlike\ntraditional approaches relying on domain-specific pre-trained models like\nSciBERT, we demonstrate that general-purpose LLMs can be adapted to this task\nwith minimal task-specific data. We evaluate twelve model variations across\nfive prominent open LLM families using zero-, one-, few-, and many-shot\nprompting. Our experimental study identifies the top-performing model and\nprompting parameters through extensive in-context learning experiments. We then\ndemonstrate the significant impact of task-specific adaptation by fine-tuning\nthis model, achieving a relative F1-score improvement of 8% on the SciCite\ndataset and 4.3% on the ACL-ARC dataset compared to the instruction-tuned\nbaseline. These findings provide valuable insights for model selection and\nprompt engineering. Additionally, we make our end-to-end evaluation framework\nand models openly available for future use.", "comment": "Accepted for publication on TPDL 2025", "pdf_url": "http://arxiv.org/pdf/2502.14561v3", "cate": "cs.CL", "date": "2025-02-20", "updated": "2025-07-25", "AI": {"title_translation": "大型语言模型能否预测引文意图？关于开放大型语言模型上下文学习和微调的实验分析", "tldr": "本研究探讨了开放大型语言模型（LLMs）通过上下文学习和微调预测引文意图的能力，并证明了通用LLMs在任务特定数据极少的情况下也能适应此任务，通过微调显著提升了性能。", "motivation": "传统方法依赖于领域特定的预训练模型（如SciBERT）来预测引文意图，而本研究旨在探索通用大型语言模型（LLMs）是否能以最少的任务特定数据适应此任务。", "method": "研究评估了来自五个主要开放LLM家族的十二种模型变体，使用了零次、一次、少数次和多次提示。通过广泛的上下文学习实验确定了表现最佳的模型和提示参数。随后，通过微调该模型进行了任务特定适应。", "result": "实验确定了表现最佳的模型和提示参数。通过微调，模型在SciCite数据集上F1-分数相对提高了8%，在ACL-ARC数据集上相对提高了4.3%，优于指令微调基线。", "conclusion": "通用大型语言模型可以通过上下文学习和微调有效应用于引文意图预测任务，即使使用少量任务特定数据也能取得显著性能提升。这些发现为模型选择和提示工程提供了宝贵的见解。", "translation": "这项工作调查了开放大型语言模型（LLMs）通过上下文学习和微调预测引文意图的能力。与依赖领域特定预训练模型（如SciBERT）的传统方法不同，我们证明了通用LLMs可以以最少的任务特定数据适应此任务。我们使用零次、一次、少数次和多次提示，评估了来自五个主要开放LLM家族的十二种模型变体。我们的实验研究通过广泛的上下文学习实验确定了表现最佳的模型和提示参数。然后，我们通过微调该模型展示了任务特定适应的显著影响，与指令微调基线相比，在SciCite数据集上实现了8%的相对F1-分数提升，在ACL-ARC数据集上实现了4.3%的相对提升。这些发现为模型选择和提示工程提供了宝贵的见解。此外，我们公开了我们的端到端评估框架和模型，以供未来使用。", "summary": "本研究探讨了开放大型语言模型（LLMs）在引文意图预测任务中的能力，通过对比上下文学习和微调方法。研究发现，通用LLMs即使在任务特定数据较少的情况下也能有效适应此任务，并通过微调在SciCite和ACL-ARC数据集上取得了显著的F1-分数提升，超越了指令微调基线。研究结果为模型选择和提示工程提供了实用指导，并公开了评估框架和模型。", "keywords": "大型语言模型, 引文意图预测, 上下文学习, 微调, 开放LLMs", "comments": "这项工作的创新之处在于，它证明了通用大型语言模型在引文意图预测这类特定领域任务中，通过上下文学习和微调，可以超越传统的领域特定模型，且仅需极少的任务数据。这对于LLMs在更多专业领域的应用具有重要意义，降低了对大量标注数据的依赖。其贡献在于提供了一个全面的评估框架和实验结果，为未来LLMs在信息抽取和文本分类任务中的应用提供了宝贵的参考。"}}
{"id": "2401.10337", "title": "Noise Contrastive Estimation-based Matching Framework for Low-Resource Security Attack Pattern Recognition", "authors": ["Tu Nguyen", "Nedim Šrndić", "Alexander Neth"], "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      accepted at EACL 2024, in ARR October 2023", "url": "http://arxiv.org/abs/2401.10337v4", "summary": "Tactics, Techniques and Procedures (TTPs) represent sophisticated attack\npatterns in the cybersecurity domain, described encyclopedically in textual\nknowledge bases. Identifying TTPs in cybersecurity writing, often called TTP\nmapping, is an important and challenging task. Conventional learning approaches\noften target the problem in the classical multi-class or multilabel\nclassification setting. This setting hinders the learning ability of the model\ndue to a large number of classes (i.e., TTPs), the inevitable skewness of the\nlabel distribution and the complex hierarchical structure of the label space.\nWe formulate the problem in a different learning paradigm, where the assignment\nof a text to a TTP label is decided by the direct semantic similarity between\nthe two, thus reducing the complexity of competing solely over the large\nlabeling space. To that end, we propose a neural matching architecture with an\neffective sampling-based learn-to-compare mechanism, facilitating the learning\nprocess of the matching model despite constrained resources.", "comment": "accepted at EACL 2024, in ARR October 2023", "pdf_url": "http://arxiv.org/pdf/2401.10337v4", "cate": "cs.LG", "date": "2024-01-18", "updated": "2025-07-24", "AI": {"title_translation": "基于噪声对比估计的低资源安全攻击模式识别匹配框架", "tldr": "该论文提出了一种基于噪声对比估计的神经匹配框架，用于在低资源环境下识别网络安全文本中的TTPs（策略、技术和程序），通过计算文本与TTP标签之间的语义相似性来解决传统分类方法面临的大类别、标签分布偏斜和复杂层次结构问题。", "motivation": "传统的TTPs识别方法将问题视为多类别或多标签分类，但面临类别数量庞大、标签分布严重偏斜以及标签空间结构复杂等挑战，这些问题阻碍了模型的学习能力。", "method": "本文将TTPs识别问题重新定义为一种匹配范式，通过计算文本与TTP标签之间的直接语义相似性来决定分配。为此，作者提出了一种神经匹配架构，并结合了有效的基于采样的“学习比较”机制，以促进在资源受限情况下的模型学习。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "策略、技术和程序（TTPs）代表了网络安全领域中复杂的攻击模式，以文本知识库的形式被百科全书式地描述。在网络安全文本中识别TTPs（通常称为TTP映射）是一项重要且具有挑战性的任务。传统的学习方法通常在经典的多类别或多标签分类设置中解决该问题。这种设置由于类别数量庞大（即TTPs）、标签分布不可避免的偏斜以及标签空间复杂的层次结构，阻碍了模型的学习能力。我们将问题重新定义在一种不同的学习范式中，其中文本到TTP标签的分配由两者之间的直接语义相似性决定，从而降低了仅在大型标签空间中竞争的复杂性。为此，我们提出了一种具有有效基于采样的“学习比较”机制的神经匹配架构，尽管资源受限，但仍能促进匹配模型的学习过程。", "summary": "该论文针对网络安全领域中TTPs（策略、技术和程序）识别的挑战，提出了一种新的基于噪声对比估计的神经匹配框架。与传统的将TTPs识别视为多类别分类的方法不同，该框架将问题重新定义为计算文本与TTP标签之间的直接语义相似性，以应对大量类别、标签分布偏斜和复杂层次结构带来的困难。通过引入基于采样的“学习比较”机制，该模型旨在在资源受限的环境下有效学习TTPs的识别。", "keywords": "TTPs识别, 噪声对比估计, 匹配框架, 低资源, 网络安全", "comments": "该论文的创新点在于将TTPs识别问题从传统的分类范式转换为语义相似性匹配范式，有效规避了传统方法在大类别、数据偏斜和复杂标签结构下的局限性。特别是，提出了一种结合噪声对比估计的神经匹配架构，并强调其在低资源环境下的适用性，这对于实际的网络安全应用具有重要意义。该方法通过直接比较语义相似性来简化学习过程，提高了模型的效率和鲁棒性。"}}
{"id": "2507.19377", "title": "Deep Reinforcement Learning-Based Scheduling for Wi-Fi Multi-Access Point Coordination", "authors": ["David Nunez", "Francesc Wilhelmi", "Maksymilian Wojnar", "Katarzyna Kosek-Szott", "Szymon Szott", "Boris Bellalta"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Transactions on Machine Learning in Communications and Networking", "url": "http://arxiv.org/abs/2507.19377v1", "summary": "Multi-access point coordination (MAPC) is a key feature of IEEE 802.11bn,\nwith a potential impact on future Wi-Fi networks. MAPC enables joint scheduling\ndecisions across multiple access points (APs) to improve throughput, latency,\nand reliability in dense Wi-Fi deployments. However, implementing efficient\nscheduling policies under diverse traffic and interference conditions in\noverlapping basic service sets (OBSSs) remains a complex task. This paper\npresents a method to minimize the network-wide worst-case latency by\nformulating MAPC scheduling as a sequential decision-making problem and\nproposing a deep reinforcement learning (DRL) mechanism to minimize worst-case\ndelays in OBSS deployments. Specifically, we train a DRL agent using proximal\npolicy optimization (PPO) within an 802.11bn-compatible Gymnasium environment.\nThis environment provides observations of queue states, delay metrics, and\nchannel conditions, enabling the agent to schedule multiple AP-station pairs to\ntransmit simultaneously by leveraging spatial reuse (SR) groups. Simulations\ndemonstrate that our proposed solution outperforms state-of-the-art heuristic\nstrategies across a wide range of network loads and traffic patterns. The\ntrained machine learning (ML) models consistently achieve lower 99th-percentile\ndelays, showing up to a 30% improvement over the best baseline.", "comment": "Submitted to IEEE Transactions on Machine Learning in Communications\n  and Networking", "pdf_url": "http://arxiv.org/pdf/2507.19377v1", "cate": "cs.NI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "深度强化学习在Wi-Fi多接入点协调调度中的应用", "tldr": "本文提出了一种基于深度强化学习的调度机制，用于Wi-Fi多接入点协调，旨在最小化最差情况下的网络延迟，并在仿真中表现优于现有启发式策略。", "motivation": "尽管多接入点协调（MAPC）是IEEE 802.11bn的关键特性，但其在异构流量和干扰条件下实现高效调度仍是一个复杂问题，尤其是在重叠基本服务集（OBSSs）中。", "method": "将MAPC调度公式化为序贯决策问题，并提出了一种基于深度强化学习（DRL）的机制来最小化OBSS部署中的最差情况延迟。具体地，使用近端策略优化（PPO）在802.11bn兼容的Gymnasium环境中训练DRL智能体，该环境提供队列状态、延迟指标和信道条件的观测，使智能体能够通过空间复用组同时调度多个AP-站点对传输。", "result": "仿真结果表明，所提出的解决方案在各种网络负载和流量模式下均优于现有启发式策略。训练后的机器学习模型持续实现更低的99%分位延迟，比最佳基线提高了30%。", "conclusion": "深度强化学习能够有效解决Wi-Fi多接入点协调中的复杂调度问题，显著降低网络延迟，尤其是在最差情况下。", "translation": "多接入点协调（MAPC）是IEEE 802.11bn的一个关键特性，对未来的Wi-Fi网络具有潜在影响。MAPC能够实现跨多个接入点（AP）的联合调度决策，以提高密集Wi-Fi部署中的吞吐量、延迟和可靠性。然而，在重叠基本服务集（OBSS）中，在不同的流量和干扰条件下实现高效的调度策略仍然是一项复杂的任务。本文提出了一种方法，通过将MAPC调度表述为序贯决策问题，并提出一种深度强化学习（DRL）机制来最小化OBSS部署中的最差情况延迟，从而最小化全网络最差情况延迟。具体来说，我们在一个802.11bn兼容的Gymnasium环境中，使用近端策略优化（PPO）训练了一个DRL智能体。该环境提供队列状态、延迟指标和信道条件的观测，使智能体能够通过利用空间复用（SR）组同时调度多个AP-站对进行传输。仿真结果表明，我们提出的解决方案在各种网络负载和流量模式下均优于最先进的启发式策略。训练后的机器学习（ML）模型持续实现更低的99%分位延迟，比最佳基线提高了30%。", "summary": "本文针对IEEE 802.11bn中多接入点协调（MAPC）调度在复杂Wi-Fi环境下实现高效性的挑战，提出了一种基于深度强化学习（DRL）的解决方案。该方案将MAPC调度建模为序贯决策问题，并利用近端策略优化（PPO）训练DRL智能体，旨在最小化全网络最差情况下的延迟。通过在802.11bn兼容环境中进行仿真，结果显示该方法在降低99%分位延迟方面比现有启发式策略表现更优，最高可提升30%。", "keywords": "深度强化学习, 多接入点协调, Wi-Fi调度, 802.11bn, 最差情况延迟", "comments": "这篇论文通过将复杂的Wi-Fi多接入点调度问题转化为深度强化学习的序贯决策问题，提供了一种创新的解决方案。其核心优势在于能够适应多变的流量和干扰条件，并通过优化最差情况延迟来提升网络性能，这对于未来密集Wi-Fi部署具有重要意义。"}}
{"id": "2507.19064", "title": "Negation-Aware Test-Time Adaptation for Vision-Language Models", "authors": ["Haochen Han", "Alex Jinpeng Wang", "Fangming Liu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper will be submitted to the IEEE for possible publication", "url": "http://arxiv.org/abs/2507.19064v1", "summary": "In this paper, we study a practical but less-touched problem in\nVision-Language Models (VLMs), \\ie, negation understanding. Specifically, many\nreal-world applications require models to explicitly identify what is false or\nnon-existent, \\eg, radiologists may search for images that exclude specific\nconditions. Despite the impressive transferability of VLMs through large-scale\ntraining, they suffer from a critical limitation that fails to handle negation.\nTo address this challenge, existing methods attribute its root cause to the\nscarcity of negation training data and propose to fine-tune VLMs on massive\ndata containing explicit negation. Undoubtedly, such data-centric solutions\ndemand substantial data and computational resources, limiting their sustainable\nwidespread adoption. To tackle negation in a low-carbon manner, we empirically\nobserve that the key obstacle lies in the dual-concept shifts between the\naffirmation and negation distributions. Therefore, we propose a Negation-Aware\nTest-Time Adaptation (NEAT) method to efficiently adjust distribution-related\nparameters during inference. In brief, NEAT can reduce distribution shift in\nconsistent semantics while eliminating false distributional consistency in\nunrelated semantics. Extensive experiments on the various negation\nunderstanding tasks verify the effectiveness of the proposed method. The code\nis available at https://github.com/hhc1997/NEAT.", "comment": "This paper will be submitted to the IEEE for possible publication", "pdf_url": "http://arxiv.org/pdf/2507.19064v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "视觉-语言模型中感知否定的测试时自适应", "tldr": "提出NEAT方法，通过测试时自适应解决视觉-语言模型在理解否定句方面的不足，避免了大量数据微调的开销。", "motivation": "视觉-语言模型（VLMs）在处理否定理解方面存在关键局限性，无法识别“假”或“不存在”的内容，而许多实际应用（如放射科医生排除特定条件）需要这种能力。现有方法依赖大量数据微调，成本高昂，因此需要一种低碳高效的解决方案。", "method": "提出了一种名为“感知否定的测试时自适应”（NEAT）的方法。该方法基于对肯定和否定分布之间双概念偏移的观察，通过在推理过程中有效调整与分布相关的参数来工作。NEAT旨在减少一致语义中的分布偏移，同时消除不相关语义中错误的分布一致性。", "result": "在各种否定理解任务上的广泛实验验证了所提方法的有效性。", "conclusion": "NEAT方法能够有效解决视觉-语言模型在否定理解方面的挑战，且无需大量数据和计算资源，为低碳高效地处理否定问题提供了可行方案。", "translation": "在本文中，我们研究了视觉-语言模型（VLMs）中一个实用但较少触及的问题，即否定理解。具体来说，许多现实世界的应用要求模型明确识别什么是错误的或不存在的，例如，放射科医生可能需要搜索排除特定条件的图像。尽管VLMs通过大规模训练展现出令人印象深刻的可迁移性，但它们在处理否定方面存在一个关键限制。为了解决这一挑战，现有方法将其根本原因归结为否定训练数据的稀缺性，并提议在包含明确否定的大量数据上对VLMs进行微调。毫无疑问，这种以数据为中心的解决方案需要大量的计算资源，限制了其可持续的广泛采用。为了以低碳方式处理否定，我们凭经验观察到，关键障碍在于肯定和否定分布之间的双概念偏移。因此，我们提出了一种感知否定的测试时自适应（NEAT）方法，以在推理过程中高效调整与分布相关的参数。简而言之，NEAT可以减少一致语义中的分布偏移，同时消除不相关语义中错误的分布一致性。在各种否定理解任务上的广泛实验验证了所提方法的有效性。代码可在https://github.com/hhc1997/NEAT获取。", "summary": "本文研究了视觉-语言模型（VLMs）在否定理解方面的不足，这是实际应用中一个重要但被忽视的问题。针对现有数据密集型解决方案的局限性，作者提出了一种名为“感知否定的测试时自适应”（NEAT）的新方法。NEAT通过在推理时调整分布相关参数，有效解决了肯定和否定分布之间的双概念偏移问题，从而在不依赖大量额外训练数据的情况下，提升了VLMs的否定理解能力。实验证明了NEAT在各种否定理解任务上的有效性。", "keywords": "视觉-语言模型, 否定理解, 测试时自适应, 分布偏移, 低碳AI", "comments": "这篇论文通过提出测试时自适应的方法来解决视觉-语言模型在否定理解上的局限性，其创新点在于避免了传统数据中心解决方案所要求的大量数据和计算资源，转而关注推理阶段的分布调整。这种“低碳”的解决思路对于推动AI技术的可持续发展具有重要意义。该方法识别了否定理解的核心障碍在于分布偏移，并提供了一种高效的解决方案，有望在实际应用中，尤其是在资源受限的环境下，提升VLMs的实用性。"}}
{"id": "2507.18890", "title": "NUTMEG: Separating Signal From Noise in Annotator Disagreement", "authors": ["Jonathan Ivey", "Susan Gauch", "David Jurgens"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18890v1", "summary": "NLP models often rely on human-labeled data for training and evaluation. Many\napproaches crowdsource this data from a large number of annotators with varying\nskills, backgrounds, and motivations, resulting in conflicting annotations.\nThese conflicts have traditionally been resolved by aggregation methods that\nassume disagreements are errors. Recent work has argued that for many tasks\nannotators may have genuine disagreements and that variation should be treated\nas signal rather than noise. However, few models separate signal and noise in\nannotator disagreement. In this work, we introduce NUTMEG, a new Bayesian model\nthat incorporates information about annotator backgrounds to remove noisy\nannotations from human-labeled training data while preserving systematic\ndisagreements. Using synthetic data, we show that NUTMEG is more effective at\nrecovering ground-truth from annotations with systematic disagreement than\ntraditional aggregation methods. We provide further analysis characterizing how\ndifferences in subpopulation sizes, rates of disagreement, and rates of spam\naffect the performance of our model. Finally, we demonstrate that downstream\nmodels trained on NUTMEG-aggregated data significantly outperform models\ntrained on data from traditionally aggregation methods. Our results highlight\nthe importance of accounting for both annotator competence and systematic\ndisagreements when training on human-labeled data.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18890v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "NUTMEG：从标注者分歧中分离信号与噪声", "tldr": "NUTMEG是一个新的贝叶斯模型，通过考虑标注者背景，在人工标注数据中区分并保留系统性分歧（信号），同时去除噪声，从而提高下游模型的性能。", "motivation": "传统方法将标注者分歧视为错误并聚合，而忽略了许多任务中分歧可能是真实信号。现有模型很少能有效分离标注者分歧中的信号和噪声。", "method": "本文引入了NUTMEG，一个贝叶斯模型。它整合了标注者背景信息，旨在从人工标注的训练数据中去除噪声标注，同时保留系统性分歧。", "result": "使用合成数据表明，NUTMEG在从具有系统性分歧的标注中恢复真实值方面比传统聚合方法更有效。研究还分析了子群体大小、分歧率和垃圾信息率对模型性能的影响。实验证明，在NUTMEG聚合数据上训练的下游模型显著优于传统聚合方法训练的模型。", "conclusion": "在使用人工标注数据进行训练时，同时考虑标注者的能力和系统性分歧至关重要。", "translation": "自然语言处理（NLP）模型通常依赖人工标注数据进行训练和评估。许多方法通过众包从大量技能、背景和动机各异的标注者那里获取数据，这导致了相互冲突的标注。传统上，这些冲突通过聚合方法解决，这些方法假设分歧是错误。最近的研究认为，对于许多任务而言，标注者可能存在真实的分歧，并且这种差异应被视为信号而非噪声。然而，很少有模型能够分离标注者分歧中的信号与噪声。在这项工作中，我们引入了NUTMEG，一个新颖的贝叶斯模型，它整合了标注者背景信息，旨在从人工标注的训练数据中去除噪声标注，同时保留系统性分歧。我们使用合成数据表明，NUTMEG在从具有系统性分歧的标注中恢复真实值方面比传统聚合方法更有效。我们进一步分析了子群体大小、分歧率和垃圾信息率的差异如何影响我们模型的性能。最后，我们证明了在NUTMEG聚合数据上训练的下游模型显著优于在传统聚合方法数据上训练的模型。我们的结果强调了在使用人工标注数据进行训练时，同时考虑标注者能力和系统性分歧的重要性。", "summary": "本文提出了NUTMEG，一个贝叶斯模型，旨在解决人工标注数据中普遍存在的标注者分歧问题。与传统将分歧视为错误的方法不同，NUTMEG通过整合标注者背景信息，有效地区分并去除数据中的噪声标注，同时保留有意义的系统性分歧。实验结果表明，NUTMEG在恢复真实值和提升下游模型性能方面均优于传统聚合方法，强调了在数据标注中同时考虑标注者能力和分歧信号的重要性。", "keywords": "标注者分歧, 贝叶斯模型, 众包, 信号噪声分离, 数据聚合", "comments": "NUTMEG的创新之处在于其贝叶斯框架能够同时建模标注者的能力和任务中固有的系统性分歧，将分歧视为潜在的信号而非单纯的错误。这对于提升众包数据质量和下游模型性能具有重要意义，尤其是在那些任务本身就存在多重合理标注的领域。该模型通过考虑标注者背景信息，提供了一种更细致的数据清洗和聚合策略。"}}
{"id": "2412.19791", "title": "Local Characteristic Decomposition of Equilibrium Variables for Hyperbolic Systems of Balance Laws", "authors": ["Shaoshuai Chu", "Alexander Kurganov", "Mingye Na", "Bao Shan Wang", "Ruixiao Xin"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.19791v2", "summary": "This paper is concerned with high-order numerical methods for hyperbolic\nsystems of balance laws. Such methods are typically based on high-order\npiecewise polynomial reconstructions (interpolations) of the computed discrete\nquantities. However, such reconstructions (interpolations) may be oscillatory\nunless the reconstruction (interpolation) procedure is applied to the local\ncharacteristic variables via the local characteristic decomposition (LCD).\nAnother challenge in designing accurate and stable high-order schemes is\nrelated to enforcing a delicate balance between the fluxes, sources, and\nnonconservative product terms: a good scheme should be well-balanced (WB) in\nthe sense that it should be capable of exactly preserving certain (physically\nrelevant) steady states. One of the ways to ensure that the reconstruction\n(interpolation) preserves these steady states is to apply the reconstruction\n(interpolation) to the equilibrium variables, which are supposed to be constant\nat the steady states. To achieve this goal and to keep the reconstruction\n(interpolation) non-oscillatory, we introduce a new LCD of equilibrium\nvariables. We apply the developed technique to the fifth-order Ai-WENO-Z\ninterpolation implemented within the WB A-WENO framework recently introduced in\n[S. Chu, A. Kurganov, and R. Xin, Beijing J. of Pure and Appl. Math., to\nappear], and illustrate its performance on a variety of numerical examples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.19791v2", "cate": "math.NA", "date": "2024-12-27", "updated": "2025-07-25", "AI": {"title_translation": "守恒律双曲系统平衡变量的局部特征分解", "tldr": "本文为守恒律双曲系统引入了一种新的平衡变量局部特征分解方法，用于开发非振荡且能精确保持稳态的高阶数值格式。", "motivation": "高阶数值方法在处理守恒律双曲系统时面临两个主要挑战：一是重建过程可能产生振荡；二是难以设计能精确保持特定稳态的良好平衡格式。", "method": "引入了一种新的平衡变量局部特征分解（LCD）方法。该技术应用于五阶Ai-WENO-Z插值，并集成到WB A-WENO框架中。", "result": "在各种数值示例中展示了所开发技术的性能。", "conclusion": "通过引入平衡变量的局部特征分解，可以设计出非振荡且能精确保持稳态的高阶数值格式，从而解决高阶方法中的振荡和平衡性问题。", "translation": "本文关注守恒律双曲系统的高阶数值方法。这类方法通常基于对计算离散量进行高阶分段多项式重建（插值）。然而，除非通过局部特征分解（LCD）将重建（插值）过程应用于局部特征变量，否则此类重建（插值）可能会出现振荡。设计精确稳定的高阶格式的另一个挑战是需要在通量、源项和非守恒积项之间保持微妙的平衡：一个好的格式应该是良好平衡（WB）的，即它应该能够精确保持某些（物理相关的）稳态。确保重建（插值）保持这些稳态的方法之一是将其应用于平衡变量，这些变量在稳态下应保持不变。为了实现这一目标并保持重建（插值）的非振荡性，我们引入了一种新的平衡变量局部特征分解（LCD）。我们将所开发的技术应用于最近在[S. Chu, A. Kurganov, and R. Xin, Beijing J. of Pure and Appl. Math., to appear]中引入的WB A-WENO框架内实现的五阶Ai-WENO-Z插值，并在各种数值示例上展示了其性能。", "summary": "本文针对守恒律双曲系统的高阶数值方法，提出了一个新的平衡变量局部特征分解（LCD）方法。该方法旨在解决现有高阶重建（插值）方法可能出现的振荡问题，并确保格式能够精确保持物理相关的稳态。通过将此新LCD应用于高阶插值过程，并结合到良好平衡（WB）框架中，可以实现非振荡且稳态保持的数值模拟。研究通过数值示例验证了该方法的有效性。", "keywords": "守恒律双曲系统, 局部特征分解, 平衡变量, 高阶数值方法, 良好平衡格式", "comments": "本文的创新点在于引入了“平衡变量的局部特征分解”这一概念，有效地结合了非振荡重建和稳态保持两大挑战的解决方案，为守恒律双曲系统的高阶数值模拟提供了新的思路和工具。"}}
{"id": "2406.19930", "title": "Exploring 6G Potential for Industrial Digital Twinning and Swarm Intelligence in Obstacle-Rich Environments", "authors": ["Siyu Yuan", "Khurshid Alam", "Bin Han", "Dennis Krummacker", "Hans D. Schotten"], "categories": ["cs.RO", "cs.MA"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE VTM", "url": "http://arxiv.org/abs/2406.19930v3", "summary": "With the advent of Sixth Generation (6G) technology, the demand for efficient\nand intelligent systems in industrial applications has surged, driving the need\nfor advanced solutions in target localization. Utilizing swarm robots to locate\nunknown targets involves navigating increasingly complex environments. digital\ntwin (DT) offers a robust solution by creating a virtual replica of the\nphysical world, which enhances the swarm's navigation capabilities. Our\nframework leverages DT and integrates swarm intelligence (SI) to store physical\nmap information in the cloud, enabling robots to efficiently locate unknown\ntargets. The simulation results demonstrate that the DT framework, augmented by\nSI, significantly improves target location efficiency in obstacle-rich\nenvironments compared to traditional methods. This research underscores the\npotential of combining DT and swarm intelligence to advance the field of\nrobotic navigation and target localization in complex industrial settings.", "comment": "Submitted to IEEE VTM", "pdf_url": "http://arxiv.org/pdf/2406.19930v3", "cate": "cs.RO", "date": "2024-06-28", "updated": "2025-07-25", "AI": {"title_translation": "探索6G在障碍物丰富环境中工业数字孪生与群体智能的潜力", "tldr": "本研究探索了利用数字孪生和群体智能结合6G技术，在障碍物丰富的工业环境中提高机器人目标定位效率。", "motivation": "随着第六代（6G）技术的出现，工业应用中对高效智能系统的需求激增，推动了对目标定位先进解决方案的需求。利用群体机器人定位未知目标涉及在日益复杂的环境中导航。", "method": "我们的框架利用数字孪生（DT）并整合群体智能（SI），将物理地图信息存储在云端，使机器人能够高效定位未知目标。", "result": "仿真结果表明，通过群体智能增强的数字孪生框架，与传统方法相比，显著提高了在障碍物丰富环境中的目标定位效率。", "conclusion": "这项研究强调了结合数字孪生和群体智能在复杂工业环境中推进机器人导航和目标定位领域的潜力。", "translation": "随着第六代（6G）技术的出现，工业应用中对高效智能系统的需求激增，推动了对目标定位先进解决方案的需求。利用群体机器人定位未知目标涉及在日益复杂的环境中导航。数字孪生（DT）提供了一个强大的解决方案，通过创建物理世界的虚拟复制品，增强了群体的导航能力。我们的框架利用DT并整合群体智能（SI），将物理地图信息存储在云端，使机器人能够高效定位未知目标。仿真结果表明，通过SI增强的DT框架，与传统方法相比，显著提高了在障碍物丰富环境中的目标定位效率。这项研究强调了结合DT和群体智能在复杂工业环境中推进机器人导航和目标定位领域的潜力。", "summary": "本研究探讨了在障碍物丰富的工业环境中，利用数字孪生（DT）和群体智能（SI）结合6G技术，以提高机器人目标定位效率。该框架将物理地图信息存储在云端，使群体机器人能够高效导航和定位未知目标。仿真结果表明，与传统方法相比，该DT-SI结合的框架显著提升了目标定位效率，突显了其在复杂工业环境中的应用潜力。", "keywords": "6G, 数字孪生, 群体智能, 目标定位, 机器人导航", "comments": "该论文的创新点在于将数字孪生与群体智能相结合，并探讨其在6G背景下应用于复杂工业环境中的目标定位。这种结合提供了一个虚拟与现实交互的解决方案，有望显著提升工业自动化和机器人导航的效率。其重要性在于为未来工业机器人系统提供了一种新的、更高效的导航和定位范式。"}}
{"id": "2507.18929", "title": "MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker Emotion Recognition", "authors": ["Jian Chen", "Yuxuan Hu", "Haifeng Lu", "Wei Wang", "Min Yang", "Chengming Li", "Xiping Hu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACMMM2025", "url": "http://arxiv.org/abs/2507.18929v1", "summary": "Although pre-trained visual models with text have demonstrated strong\ncapabilities in visual feature extraction, sticker emotion understanding\nremains challenging due to its reliance on multi-view information, such as\nbackground knowledge and stylistic cues. To address this, we propose a novel\nmulti-granularity hierarchical fusion transformer (MGHFT), with a multi-view\nsticker interpreter based on Multimodal Large Language Models. Specifically,\ninspired by the human ability to interpret sticker emotions from multiple\nviews, we first use Multimodal Large Language Models to interpret stickers by\nproviding rich textual context via multi-view descriptions. Then, we design a\nhierarchical fusion strategy to fuse the textual context into visual\nunderstanding, which builds upon a pyramid visual transformer to extract both\nglobal and local sticker features at multiple stages. Through contrastive\nlearning and attention mechanisms, textual features are injected at different\nstages of the visual backbone, enhancing the fusion of global- and\nlocal-granularity visual semantics with textual guidance. Finally, we introduce\na text-guided fusion attention mechanism to effectively integrate the overall\nmultimodal features, enhancing semantic understanding. Extensive experiments on\n2 public sticker emotion datasets demonstrate that MGHFT significantly\noutperforms existing sticker emotion recognition approaches, achieving higher\naccuracy and more fine-grained emotion recognition. Compared to the best\npre-trained visual models, our MGHFT also obtains an obvious improvement, 5.4%\non F1 and 4.0% on accuracy. The code is released at\nhttps://github.com/cccccj-03/MGHFT_ACMMM2025.", "comment": "Accepted by ACMMM2025", "pdf_url": "http://arxiv.org/pdf/2507.18929v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "MGHFT：用于跨模态贴纸情感识别的多粒度分层融合Transformer", "tldr": "本文提出MGHFT模型，通过多模态大语言模型和分层融合策略，显著提升了跨模态贴纸情感识别的准确性和细粒度识别能力。", "motivation": "尽管预训练视觉模型在视觉特征提取方面表现强大，但贴纸情感理解仍具挑战，因为它高度依赖于背景知识和风格线索等多视角信息。", "method": "本文提出了多粒度分层融合Transformer (MGHFT)。该模型首先通过基于多模态大语言模型的多视角贴纸解释器提供丰富的文本上下文。接着，设计了一种分层融合策略，将文本上下文融入视觉理解，该策略利用金字塔视觉Transformer在多个阶段提取全局和局部贴纸特征。通过对比学习和注意力机制，文本特征被注入到视觉骨干的不同阶段，增强了全局和局部粒度视觉语义与文本指导的融合。最后，引入文本引导的融合注意力机制以有效整合整体多模态特征。", "result": "在2个公开贴纸情感数据集上的实验表明，MGHFT显著优于现有贴纸情感识别方法，实现了更高的准确性和更细粒度的情感识别。与最佳预训练视觉模型相比，MGHFT在F1得分上提升了5.4%，在准确率上提升了4.0%。", "conclusion": "MGHFT通过其新颖的多粒度分层融合Transformer架构和文本引导机制，有效解决了跨模态贴纸情感识别的挑战，并在公开数据集上取得了显著优于现有方法的性能，展现了其在复杂情感理解任务中的强大潜力。", "translation": "尽管带有文本的预训练视觉模型在视觉特征提取方面表现出强大的能力，但贴纸情感理解仍然具有挑战性，因为它依赖于多视角信息，例如背景知识和风格线索。为了解决这个问题，我们提出了一种新颖的多粒度分层融合Transformer (MGHFT)，其中包含一个基于多模态大语言模型的多视角贴纸解释器。具体来说，受人类从多视角解释贴纸情感能力的启发，我们首先使用多模态大语言模型通过提供多视角描述的丰富文本上下文来解释贴纸。然后，我们设计了一种分层融合策略，将文本上下文融入视觉理解中，该策略基于金字塔视觉Transformer在多个阶段提取全局和局部贴纸特征。通过对比学习和注意力机制，文本特征被注入到视觉骨干的不同阶段，增强了全局和局部粒度视觉语义与文本指导的融合。最后，我们引入了文本引导的融合注意力机制，以有效整合整体多模态特征，增强语义理解。在2个公开贴纸情感数据集上的大量实验表明，MGHFT显著优于现有贴纸情感识别方法，实现了更高的准确性和更细粒度的情感识别。与最佳预训练视觉模型相比，我们的MGHFT还在F1得分上获得了5.4%的明显提升，在准确率上获得了4.0%的提升。代码已在https://github.com/cccccj-03/MGHFT_ACMMM2025发布。", "summary": "本文提出MGHFT（多粒度分层融合Transformer）模型，用于解决跨模态贴纸情感识别中对多视角信息的依赖挑战。MGHFT利用基于多模态大语言模型的多视角解释器提供丰富的文本上下文，并通过分层融合策略将文本特征与金字塔视觉Transformer提取的全局和局部视觉特征在不同阶段进行融合。该模型通过对比学习和文本引导的注意力机制增强语义理解。实验结果表明，MGHFT在公开数据集上显著优于现有方法，并在F1和准确率上取得了明显的性能提升。", "keywords": "贴纸情感识别, 跨模态融合, Transformer, 多模态大语言模型, 分层融合", "comments": "该论文的创新之处在于其独特地结合了多模态大语言模型来提供丰富的多视角文本上下文，并设计了多粒度分层融合策略，有效地将文本指导融入视觉特征提取的不同阶段。这种方法显著提升了跨模态贴纸情感识别的性能，为处理复杂的多模态情感理解任务提供了新的思路。其性能提升数据具体且显著，证明了模型的有效性。"}}
{"id": "2406.13564", "title": "HumorDB: Can AI understand graphical humor?", "authors": ["Veedant Jain", "Gabriel Kreiman", "Felipe dos Santos Alves Feitosa"], "categories": ["cs.CV", "cs.AI", "I.5.4"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 main figures, 4 additional appendix figures", "url": "http://arxiv.org/abs/2406.13564v2", "summary": "Despite significant advancements in image segmentation and object detection,\nunderstanding complex scenes remains a significant challenge. Here, we focus on\ngraphical humor as a paradigmatic example of image interpretation that requires\nelucidating the interaction of different scene elements in the context of prior\ncognitive knowledge. This paper introduces \\textbf{HumorDB}, a novel,\ncontrolled, and carefully curated dataset designed to evaluate and advance\nvisual humor understanding by AI systems. The dataset comprises diverse images\nspanning photos, cartoons, sketches, and AI-generated content, including\nminimally contrastive pairs where subtle edits differentiate between humorous\nand non-humorous versions. We evaluate humans, state-of-the-art vision models,\nand large vision-language models on three tasks: binary humor classification,\nfunniness rating prediction, and pairwise humor comparison. The results reveal\na gap between current AI systems and human-level humor understanding. While\npretrained vision-language models perform better than vision-only models, they\nstill struggle with abstract sketches and subtle humor cues. Analysis of\nattention maps shows that even when models correctly classify humorous images,\nthey often fail to focus on the precise regions that make the image funny.\nPreliminary mechanistic interpretability studies and evaluation of model\nexplanations provide initial insights into how different architectures process\nhumor. Our results identify promising trends and current limitations,\nsuggesting that an effective understanding of visual humor requires\nsophisticated architectures capable of detecting subtle contextual features and\nbridging the gap between visual perception and abstract reasoning. All the code\nand data are available here:\n\\href{https://github.com/kreimanlab/HumorDB}{https://github.com/kreimanlab/HumorDB}", "comment": "10 main figures, 4 additional appendix figures", "pdf_url": "http://arxiv.org/pdf/2406.13564v2", "cate": "cs.CV", "date": "2024-06-19", "updated": "2025-07-24", "AI": {"title_translation": "HumorDB：AI能理解图形幽默吗？", "tldr": "本文介绍了HumorDB数据集，用于评估AI对图形幽默的理解能力，并发现当前AI系统在理解图形幽默方面与人类水平存在显著差距。", "motivation": "尽管图像分割和目标检测取得了显著进展，但理解复杂场景仍然是一个重大挑战。本文将图形幽默作为图像解释的范例，它需要阐明不同场景元素在先验认知知识背景下的相互作用，以推动AI对图像的理解。", "method": "本文引入了一个名为HumorDB的、新颖、受控且精心策划的数据集，旨在评估和推进AI系统对视觉幽默的理解。该数据集包含照片、卡通、草图和AI生成内容等多种图像，包括微对比对，其中细微的编辑区分幽默和非幽默版本。研究团队评估了人类、最先进的视觉模型和大型视觉-语言模型在三项任务上的表现：二元幽默分类、趣味性评分预测和成对幽默比较。", "result": "结果显示，当前AI系统与人类水平的幽默理解之间存在差距。虽然预训练的视觉-语言模型表现优于仅视觉模型，但它们在抽象草图和细微幽默线索方面仍然表现不佳。对注意力图的分析表明，即使模型正确分类了幽默图像，它们也常常未能聚焦于使图像有趣的精确区域。", "conclusion": "研究结果指出了有前景的趋势和当前的局限性，表明有效理解视觉幽默需要复杂的架构，这些架构能够检测细微的上下文特征，并弥合视觉感知和抽象推理之间的鸿沟。", "translation": "尽管图像分割和目标检测取得了显著进展，但理解复杂场景仍然是一个重大挑战。本文将图形幽默作为图像解释的范例，它需要阐明不同场景元素在先验认知知识背景下的相互作用，并将其置于先验认知知识的语境中。本文介绍了HumorDB，这是一个新颖、受控且精心策划的数据集，旨在评估和推进AI系统对视觉幽默的理解。该数据集包含多种图像，涵盖照片、卡通、草图和AI生成内容，包括微对比对，其中细微的编辑区分幽默和非幽默版本。我们评估了人类、最先进的视觉模型和大型视觉-语言模型在三项任务上的表现：二元幽默分类、趣味性评分预测和成对幽默比较。结果显示，当前AI系统与人类水平的幽默理解之间存在差距。虽然预训练的视觉-语言模型表现优于仅视觉模型，但它们在抽象草图和细微幽默线索方面仍然表现不佳。对注意力图的分析表明，即使模型正确分类了幽默图像，它们也常常未能聚焦于使图像有趣的精确区域。初步的机制可解释性研究和模型解释评估为不同架构如何处理幽默提供了初步见解。我们的结果指出了有前景的趋势和当前的局限性，表明有效理解视觉幽默需要复杂的架构，这些架构能够检测细微的上下文特征，并弥合视觉感知和抽象推理之间的鸿沟。所有代码和数据均可在以下网址获取：https://github.com/kreimanlab/HumorDB", "summary": "本文介绍了HumorDB数据集，旨在评估AI对图形幽默的理解能力。通过包含多种图像类型和微对比对的数据集，研究者评估了人类、视觉模型和视觉-语言模型在幽默分类、趣味性评级和成对比较任务上的表现。结果表明，尽管视觉-语言模型优于仅视觉模型，但AI在理解抽象草图和细微幽默线索方面仍远低于人类水平，且模型注意力往往未能聚焦于关键幽默区域。研究强调，未来的AI系统需具备更复杂的架构，以实现对视觉幽默的有效理解。", "keywords": "图形幽默, AI理解, HumorDB, 视觉-语言模型, 图像解释", "comments": "本文的创新之处在于构建了HumorDB数据集，特别是引入了“微对比对”来精确区分幽默和非幽默版本，这对于评估AI理解细微幽默的能力至关重要。研究的重要性在于揭示了当前AI在复杂图像理解（特别是幽默感知）方面的显著局限性，指出了现有模型在抽象推理和聚焦关键区域上的不足。这为未来AI视觉理解领域的研究方向提供了宝贵见解，强调了开发更复杂架构的需求。"}}
{"id": "2506.12479", "title": "AI Flow: Perspectives, Scenarios, and Approaches", "authors": ["Hongjun An", "Wenhan Hu", "Sida Huang", "Siqi Huang", "Ruanjun Li", "Yuanzhi Liang", "Jiawei Shao", "Yiliang Song", "Zihan Wang", "Cheng Yuan", "Chi Zhang", "Hongyuan Zhang", "Wenhao Zhuang", "Xuelong Li"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.DC", "eess.SP"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Authors are with Institute of Artificial Intelligence (TeleAI), China Telecom, China. Author names are listed alphabetically by surname. This work was conducted at TeleAI, facilitated by Dr. Jiawei Shao (e-mail: shaojw2@chinatelecom.cn) under the leadership of Prof. Xuelong Li. The corresponding author is Prof. Xuelong Li (e-mail: xuelong li@ieee.org), the CTO and Chief Scientist of China Telecom", "url": "http://arxiv.org/abs/2506.12479v3", "summary": "Pioneered by the foundational information theory by Claude Shannon and the\nvisionary framework of machine intelligence by Alan Turing, the convergent\nevolution of information and communication technologies (IT/CT) has created an\nunbroken wave of connectivity and computation. This synergy has sparked a\ntechnological revolution, now reaching its peak with large artificial\nintelligence (AI) models that are reshaping industries and redefining\nhuman-machine collaboration. However, the realization of ubiquitous\nintelligence faces considerable challenges due to substantial resource\nconsumption in large models and high communication bandwidth demands. To\naddress these challenges, AI Flow has been introduced as a multidisciplinary\nframework that integrates cutting-edge IT and CT advancements, with a\nparticular emphasis on the following three key points. First, device-edge-cloud\nframework serves as the foundation, which integrates end devices, edge servers,\nand cloud clusters to optimize scalability and efficiency for low-latency model\ninference. Second, we introduce the concept of familial models, which refers to\na series of different-sized models with aligned hidden features, enabling\neffective collaboration and the flexibility to adapt to varying resource\nconstraints and dynamic scenarios. Third, connectivity- and interaction-based\nintelligence emergence is a novel paradigm of AI Flow. By leveraging\ncommunication networks to enhance connectivity, the collaboration among AI\nmodels across heterogeneous nodes achieves emergent intelligence that surpasses\nthe capability of any single model. The innovations of AI Flow provide enhanced\nintelligence, timely responsiveness, and ubiquitous accessibility to AI\nservices, paving the way for the tighter fusion of AI techniques and\ncommunication systems.", "comment": "Authors are with Institute of Artificial Intelligence (TeleAI), China\n  Telecom, China. Author names are listed alphabetically by surname. This work\n  was conducted at TeleAI, facilitated by Dr. Jiawei Shao (e-mail:\n  shaojw2@chinatelecom.cn) under the leadership of Prof. Xuelong Li. The\n  corresponding author is Prof. Xuelong Li (e-mail: xuelong li@ieee.org), the\n  CTO and Chief Scientist of China Telecom", "pdf_url": "http://arxiv.org/pdf/2506.12479v3", "cate": "cs.AI", "date": "2025-06-14", "updated": "2025-07-24", "AI": {"title_translation": "人工智能流：视角、场景与方法", "tldr": "AI Flow是一个多学科框架，通过设备-边缘-云、家族模型和基于连接的智能涌现，解决大型AI模型资源消耗和带宽需求挑战，实现普适智能。", "motivation": "大型AI模型存在高资源消耗和高通信带宽需求，阻碍了普适智能的实现。", "method": "AI Flow是一个多学科框架，整合了IT和CT技术，主要包含三点：1. 设备-边缘-云框架，优化可扩展性、效率和低延迟模型推理。2. 家族模型概念，通过不同尺寸但隐藏特征对齐的模型实现协作和适应性。3. 基于连接和交互的智能涌现，利用通信网络促进异构节点间AI模型的协作以实现超越单一模型的涌现智能。", "result": "AI Flow提供了增强的智能、及时的响应能力和普适的AI服务可访问性。", "conclusion": "AI Flow为人工智能技术与通信系统更紧密的融合铺平了道路。", "translation": "受到克劳德·香农奠基性的信息论和艾伦·图灵富有远见的机器智能框架的启发，信息与通信技术（IT/CT）的融合演进创造了不间断的连接和计算浪潮。这种协同作用引发了一场技术革命，目前正随着重塑行业和重新定义人机协作的大型人工智能（AI）模型达到顶峰。然而，由于大型模型巨大的资源消耗和高通信带宽需求，普适智能的实现面临着相当大的挑战。为了应对这些挑战，人工智能流（AI Flow）被引入作为一个多学科框架，它整合了尖端的IT和CT进展，特别强调以下三个关键点。首先，设备-边缘-云框架作为基础，它整合了终端设备、边缘服务器和云集群，以优化低延迟模型推理的可扩展性和效率。其次，我们引入了家族模型（familial models）的概念，它指的是一系列不同尺寸但隐藏特征对齐的模型，从而实现有效的协作以及适应不同资源限制和动态场景的灵活性。第三，基于连接和交互的智能涌现是AI Flow的一种新颖范式。通过利用通信网络增强连接性，异构节点间AI模型的协作实现了超越任何单一模型能力的涌现智能。AI Flow的创新提供了增强的智能、及时的响应能力和普适的AI服务可访问性，为人工智能技术与通信系统更紧密的融合铺平了道路。", "summary": "本文介绍了AI Flow，一个旨在解决大型AI模型高资源消耗和通信带宽挑战的多学科框架。AI Flow通过整合设备-边缘-云架构、引入家族模型概念（不同尺寸但特征对齐的模型），以及利用通信网络实现基于连接和交互的智能涌现，旨在提供增强的智能、及时的响应和普适的AI服务，促进AI与通信系统的深度融合。", "keywords": "AI Flow, 设备-边缘-云, 家族模型, 涌现智能, 通信系统", "comments": "这篇论文提出了AI Flow框架，旨在解决当前大型AI模型在资源消耗和带宽方面的挑战，以实现普适智能。其创新点在于结合了设备-边缘-云的分布式计算范式、提出了“家族模型”这一新颖概念以实现模型间的协作与适应性，以及强调了通过网络连接实现“涌现智能”的重要性。该框架为AI技术与通信系统更紧密的融合提供了新的视角和方法，对于推动AI服务的泛在化具有重要意义。"}}
{"id": "2402.01255", "title": "Sequence of Numbers of Linear Codes with Increasing Hull Dimensions", "authors": ["Stefka Bouyuklieva", "Iliya Bouyukliev", "Ferruh Özbudak"], "categories": ["cs.IT", "cs.DM", "math.IT", "94B05", "E.4"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.01255v2", "summary": "The hull of a linear code $C$ is the intersection of $C$ with its dual code.\nWe present and analyze the number of linear $q$-ary codes of the same length\nand dimension but with different dimensions for their hulls. We prove that for\ngiven dimension $k$ and length $n\\ge 2k$ the number of all $[n,k]_q$ linear\ncodes with hull dimension $l$ decreases as $l$ increases. We also present\nclassification results for binary and ternary linear codes with trivial hulls\n(LCD and self-orthogonal) for some values of the length $n$ and dimension $k$,\ncomparing the obtained numbers with the number of all linear codes for the\ngiven $n$ and $k$.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.01255v2", "cate": "cs.IT", "date": "2024-02-02", "updated": "2025-07-25", "AI": {"title_translation": "具有递增核维数的线性码数量序列", "tldr": "本文分析了具有不同核维度的线性码的数量，并证明了随着核维度增加，码的数量会减少。", "motivation": "本文旨在呈现和分析具有相同长度和维度但核维度不同的线性q元码的数量，以理解线性码的核维度分布特性。", "method": "研究方法包括对具有相同长度和维度但核维度不同的线性q元码的数量进行呈现和分析，并证明了在给定维度k和长度n≥2k的情况下，所有[n,k]q线性码中核维度l的码的数量随l的增加而减少。此外，还对具有平凡核（LCD码和自正交码）的二元和三元线性码进行了分类，并将其数量与给定n和k的所有线性码的数量进行了比较。", "result": "研究证明，对于给定的维度k和长度n≥2k，所有[n,k]q线性码中核维度l的码的数量随l的增加而减少。此外，还获得了某些长度n和维度k的二元和三元线性码的平凡核（LCD码和自正交码）的分类结果，并将其数量与给定n和k的所有线性码的数量进行了比较。", "conclusion": "本文得出的结论是，线性码的核维度与码的数量之间存在负相关关系，即随着核维度的增加，具有该核维度的码的数量会减少。同时，对特定二元和三元码的分类也为理解其分布提供了见解。", "translation": "线性码C的核是C与其对偶码的交集。我们呈现并分析了具有相同长度和维度但核维度不同的线性q元码的数量。我们证明，对于给定的维度k和长度n≥2k，所有[n,k]q线性码中核维度为l的码的数量随着l的增加而减少。我们还呈现了某些长度n和维度k的二元和三元线性码的平凡核（LCD码和自正交码）的分类结果，并将所得数量与给定n和k的所有线性码的数量进行了比较。", "summary": "本文研究了具有相同长度和维度的线性q元码在不同核维度下的数量分布。研究发现，当核维度增加时，具有该核维度的线性码的数量会减少。此外，论文还对具有平凡核（LCD码和自正交码）的二元和三元线性码进行了分类，并与所有线性码的总数进行了比较。", "keywords": "线性码, 核维度, LCD码, 自正交码, 码分类", "comments": "该论文通过定量分析线性码的核维度与其数量之间的关系，揭示了一个重要的数学模式，即随着核维度的增加，码的数量呈现下降趋势。这为理解和设计具有特定核维度的线性码提供了新的视角。对二元和三元LCD码和自正交码的分类结果也具有实际应用价值。"}}
{"id": "2507.18977", "title": "Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs through Global Similarity and Weighted Sampling", "authors": ["Mehrnoosh Mirtaheri", "Ryan A. Rossi", "Sungchul Kim", "Kanak Mahadik", "Tong Yu", "Xiang Chen", "Mohammad Rostami"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18977v1", "summary": "Temporal Knowledge Graph (TKG) completion models traditionally assume access\nto the entire graph during training. This overlooks challenges stemming from\nthe evolving nature of TKGs, such as: (i) the model's requirement to generalize\nand assimilate new knowledge, and (ii) the task of managing new or unseen\nentities that often have sparse connections. In this paper, we present an\nincremental training framework specifically designed for TKGs, aiming to\naddress entities that are either not observed during training or have sparse\nconnections. Our approach combines a model-agnostic enhancement layer with a\nweighted sampling strategy, that can be augmented to and improve any existing\nTKG completion method. The enhancement layer leverages a broader, global\ndefinition of entity similarity, which moves beyond mere local neighborhood\nproximity of GNN-based methods. The weighted sampling strategy employed in\ntraining accentuates edges linked to infrequently occurring entities. We\nevaluate our method on two benchmark datasets, and demonstrate that our\nframework outperforms existing methods in total link prediction, inductive link\nprediction, and in addressing long-tail entities. Notably, our method achieves\na 10\\% improvement and a 15\\% boost in MRR for these datasets. The results\nunderscore the potential of our approach in mitigating catastrophic forgetting\nand enhancing the robustness of TKG completion methods, especially in an\nincremental training context", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18977v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "通过全局相似性和加权采样改进时间知识图谱中的长尾实体预测", "tldr": "本文提出了一个增量训练框架，结合模型无关的增强层和加权采样策略，以解决时间知识图谱中未见或连接稀疏的实体预测问题，并在基准数据集上显著优于现有方法。", "motivation": "传统的时序知识图谱（TKG）补全模型假设训练时可访问整个图，但这忽略了TKG演化带来的挑战，例如模型需要泛化和吸收新知识，以及管理连接稀疏的新实体或未见实体。", "method": "我们提出了一个专门为TKG设计的增量训练框架，旨在处理训练期间未观察到或连接稀疏的实体。该方法结合了一个模型无关的增强层和一个加权采样策略，可以增强并改进任何现有的TKG补全方法。增强层利用更广泛的全局实体相似性定义，超越了基于GNN方法的局部邻居接近度。训练中采用的加权采样策略强调与不常出现实体相关的边。", "result": "我们的方法在两个基准数据集上进行了评估，结果表明我们的框架在总链接预测、归纳链接预测以及处理长尾实体方面优于现有方法。值得注意的是，我们的方法在这些数据集上的MRR分别提高了10%和15%。", "conclusion": "结果强调了我们的方法在减轻灾难性遗忘和增强TKG补全方法鲁棒性方面的潜力，尤其是在增量训练的背景下。", "translation": "时间知识图谱（TKG）补全模型传统上假设在训练期间可以访问整个图。这忽略了TKG演化带来的挑战，例如：（i）模型需要泛化和吸收新知识，以及（ii）管理通常连接稀疏的新实体或未见实体的任务。在本文中，我们提出了一个专门为TKG设计的增量训练框架，旨在解决训练期间未观察到或连接稀疏的实体问题。我们的方法结合了一个模型无关的增强层和一个加权采样策略，可以增强并改进任何现有的TKG补全方法。增强层利用更广泛的全局实体相似性定义，这超越了基于GNN方法的局部邻居接近度。训练中采用的加权采样策略强调与不常出现实体相关的边。我们在两个基准数据集上评估了我们的方法，结果表明我们的框架在总链接预测、归纳链接预测以及处理长尾实体方面优于现有方法。值得注意的是，我们的方法在这些数据集上的MRR分别提高了10%和15%。结果强调了我们的方法在减轻灾难性遗忘和增强TKG补全方法鲁棒性方面的潜力，尤其是在增量训练的背景下。", "summary": "本文提出了一个针对时间知识图谱（TKG）的增量训练框架，以解决传统模型在处理演化TKG中新知识和稀疏连接实体方面的不足。该框架包含一个模型无关的增强层，利用全局实体相似性，以及一个加权采样策略，以关注长尾实体。实验结果表明，该方法在总链接预测、归纳链接预测和长尾实体处理上均优于现有方法，并在MRR上实现了显著提升，证实了其在减轻灾难性遗忘和增强TKG补全鲁棒性方面的有效性。", "keywords": "时间知识图谱, 长尾实体, 增量训练, 全局相似性, 加权采样", "comments": "该论文提出了一种新颖的增量训练框架，有效解决了时间知识图谱在演化过程中面临的新实体和长尾实体预测挑战。其创新性在于结合了模型无关的增强层（利用全局相似性）和加权采样策略，使得该方法具有良好的通用性和可扩展性，能够提升现有TKG补全方法的性能。特别是在处理稀疏连接和减轻灾难性遗忘方面表现突出，对于实际应用中不断变化的知识图谱具有重要意义。"}}
{"id": "2502.11486", "title": "Anti-Degeneracy Scheme for Lidar SLAM based on Particle Filter in Geometry Feature-Less Environments", "authors": ["Yanbin Li", "Wei Zhang", "Zhiguo Zhang", "Xiaogang Shi", "Ziruo Li", "Mingming Zhang", "Hongping Xie", "Wenzheng Chi"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      8 pages, 9 figures, IEEE Robotics and Automation Letters", "url": "http://arxiv.org/abs/2502.11486v2", "summary": "Simultaneous localization and mapping (SLAM) based on particle filtering has\nbeen extensively employed in indoor scenarios due to its high efficiency.\nHowever, in geometry feature-less scenes, the accuracy is severely reduced due\nto lack of constraints. In this article, we propose an anti-degeneracy system\nbased on deep learning. Firstly, we design a scale-invariant linear mapping to\nconvert coordinates in continuous space into discrete indexes, in which a data\naugmentation method based on Gaussian model is proposed to ensure the model\nperformance by effectively mitigating the impact of changes in the number of\nparticles on the feature distribution. Secondly, we develop a degeneracy\ndetection model using residual neural networks (ResNet) and transformer which\nis able to identify degeneracy by scrutinizing the distribution of the particle\npopulation. Thirdly, an adaptive anti-degeneracy strategy is designed, which\nfirst performs fusion and perturbation on the resample process to provide rich\nand accurate initial values for the pose optimization, and use a hierarchical\npose optimization combining coarse and fine matching, which is able to\nadaptively adjust the optimization frequency and the sensor trustworthiness\naccording to the degree of degeneracy, in order to enhance the ability of\nsearching the global optimal pose. Finally, we demonstrate the optimality of\nthe model, as well as the improvement of the image matrix method and GPU on the\ncomputation time through ablation experiments, and verify the performance of\nthe anti-degeneracy system in different scenarios through simulation\nexperiments and real experiments. This work has been submitted to IEEE for\npublication. Copyright may be transferred without notice, after which this\nversion may no longer be available.", "comment": "8 pages, 9 figures, IEEE Robotics and Automation Letters", "pdf_url": "http://arxiv.org/pdf/2502.11486v2", "cate": "cs.RO", "date": "2025-02-17", "updated": "2025-07-25", "AI": {"title_translation": "针对几何特征稀疏环境下基于粒子滤波的激光雷达SLAM抗退化方案", "tldr": "本文提出了一种基于深度学习的抗退化方案，用于解决在几何特征稀疏环境下，基于粒子滤波的激光雷达SLAM精度下降的问题。", "motivation": "在几何特征稀疏场景中，基于粒子滤波的同步定位与建图（SLAM）由于缺乏约束，其精度会严重降低。", "method": "本文提出了一种基于深度学习的抗退化系统。首先，设计了一种尺度不变线性映射，将连续空间坐标转换为离散索引，并提出基于高斯模型的数据增强方法。其次，开发了使用残差神经网络（ResNet）和Transformer的退化检测模型。第三，设计了一种自适应抗退化策略，该策略在重采样过程中进行融合和扰动，并采用结合粗细匹配的分层位姿优化。", "result": "通过消融实验证明了模型的优化性以及图像矩阵方法和GPU在计算时间上的改进。通过仿真实验和真实实验验证了该抗退化系统在不同场景下的性能。", "conclusion": "本文提出的基于深度学习的抗退化方案有效解决了在几何特征稀疏环境下，基于粒子滤波的激光雷达SLAM的精度下降问题，并通过实验验证了其有效性和性能。", "translation": "基于粒子滤波的同步定位与建图（SLAM）因其高效率已广泛应用于室内场景。然而，在几何特征稀疏的场景中，由于缺乏约束，精度会严重降低。本文提出了一种基于深度学习的抗退化系统。首先，我们设计了一种尺度不变线性映射，将连续空间中的坐标转换为离散索引，其中提出了一种基于高斯模型的数据增强方法，通过有效缓解粒子数量变化对特征分布的影响来确保模型性能。其次，我们开发了一种使用残差神经网络（ResNet）和Transformer的退化检测模型，该模型能够通过检查粒子群体的分布来识别退化。第三，设计了一种自适应抗退化策略，该策略首先对重采样过程进行融合和扰动，为位姿优化提供丰富而准确的初始值，并使用结合粗细匹配的分层位姿优化，该优化能够根据退化程度自适应调整优化频率和传感器可信度，以增强搜索全局最优位姿的能力。最后，我们通过消融实验证明了模型的优化性以及图像矩阵方法和GPU在计算时间上的改进，并通过仿真实验和真实实验验证了该抗退化系统在不同场景下的性能。这项工作已提交给IEEE出版。版权可能会在不另行通知的情况下转让，之后此版本可能不再可用。", "summary": "本文针对几何特征稀疏环境下基于粒子滤波的激光雷达SLAM精度下降问题，提出了一种基于深度学习的抗退化方案。该方案包含尺度不变线性映射与数据增强、基于ResNet和Transformer的退化检测模型，以及结合融合扰动重采样和分层位姿优化的自适应抗退化策略。实验证明了该方案的有效性、模型优化性和计算效率的提升。", "keywords": "激光雷达SLAM, 粒子滤波, 退化, 深度学习, 抗退化", "comments": "本文创新性地将深度学习引入到粒子滤波Lidar SLAM中，以解决几何特征稀疏环境下的退化问题。其提出的尺度不变映射、基于ResNet/Transformer的退化检测以及自适应优化策略具有较高的创新性，有望显著提升SLAM在挑战性环境下的鲁棒性和精度。"}}
{"id": "2507.18644", "title": "Interpretable inverse design of optical multilayer thin films based on extended neural adjoint and regression activation mapping", "authors": ["Sungjun Kim", "Jungho Kim"], "categories": ["physics.optics", "cs.CE", "cs.LG", "physics.comp-ph"], "primary_category": "Subjects:       Optics (physics.optics)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18644v1", "summary": "We propose an extended neural adjoint (ENA) framework, which meets six key\ncriteria for artificial intelligence-assisted inverse design of optical\nmultilayer thin films (OMTs): accuracy, efficiency, diversity, scalability,\nflexibility, and interpretability. To enhance the scalability of the existing\nneural adjoint method, we present a novel forward neural network architecture\nfor OMTs and introduce a material loss function into the existing neural\nadjoint loss function, facilitating the exploration of material configurations\nof OMTs. Furthermore, we present the detailed formulation of the regression\nactivation mapping for the presented forward neural network architecture\n(F-RAM), a feature visualization method aimed at improving interpretability. We\nvalidated the efficacy of the material loss by conducting an ablation study,\nwhere each component of the loss function is systematically removed and\nevaluated. The results indicated that the inclusion of the material loss\nsignificantly improves accuracy and diversity. To substantiate the performance\nof the ENA-based inverse design, we compared it against the residual\nnetwork-based global optimization network (Res-GLOnet). The ENA yielded the OMT\nsolutions of an inverse design with higher accuracy and better diversity\ncompared to the Res-GLOnet. To demonstrate the interpretability, we applied\nF-RAM to diverse OMT structures with similar optical properties, obtained by\nthe proposed ENA method. We showed that distributions of feature importance for\nvarious OMT structures exhibiting analogous optical properties are consistent,\ndespite variations in material configurations, layer number, and thicknesses.\nFurthermore, we demonstrate the flexibility of the ENA method by restricting\nthe initial layer of OMTs to SiO2 and 100 nm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18644v1", "cate": "physics.optics", "date": "2025-07-10", "updated": "2025-07-10", "AI": {"title_translation": "基于扩展神经伴随和回归激活映射的光学多层薄膜可解释逆向设计", "tldr": "提出了一种名为扩展神经伴随（ENA）的新框架，用于光学多层薄膜的可解释逆向设计，该框架在准确性、效率、多样性、可伸缩性、灵活性和可解释性方面表现出色。", "motivation": "现有的AI辅助光学多层薄膜逆向设计方法在可伸缩性、多样性、准确性以及可解释性等方面可能存在不足，需要一个满足多项关键标准的统一框架。", "method": "提出了一种扩展神经伴随（ENA）框架，该框架包含一个用于光学多层薄膜（OMTs）的新型前向神经网络架构，并在现有神经伴随损失函数中引入了材料损失函数，以促进材料配置的探索。此外，还详细阐述了基于所提出前向神经网络架构的回归激活映射（F-RAM）方法，旨在提高可解释性。", "result": "消融研究表明，引入材料损失显著提高了逆向设计的准确性和多样性。与基于残差网络的全局优化网络（Res-GLOnet）相比，ENA获得了更高精度和更好多样性的OMT解决方案。F-RAM应用于具有相似光学性质的不同OMT结构时，显示出尽管材料配置、层数和厚度存在差异，但特征重要性分布保持一致。此外，ENA方法还展示了其灵活性。", "conclusion": "所提出的扩展神经伴随（ENA）框架成功满足了AI辅助光学多层薄膜逆向设计的六个关键标准（准确性、效率、多样性、可伸缩性、灵活性和可解释性），通过引入材料损失和F-RAM显著提升了性能和可解释性。", "translation": "我们提出了一种扩展神经伴随（ENA）框架，该框架满足了人工智能辅助光学多层薄膜（OMTs）逆向设计的六个关键标准：准确性、效率、多样性、可伸缩性、灵活性和可解释性。为了增强现有神经伴随方法的可伸缩性，我们提出了一种用于OMTs的新型前向神经网络架构，并将材料损失函数引入到现有神经伴随损失函数中，从而促进了OMTs材料配置的探索。此外，我们详细阐述了所提出的前向神经网络架构的回归激活映射（F-RAM）的公式，这是一种旨在提高可解释性的特征可视化方法。我们通过进行消融研究验证了材料损失的功效，其中系统地移除和评估了损失函数的每个组件。结果表明，包含材料损失显著提高了准确性和多样性。为了证实基于ENA的逆向设计的性能，我们将其与基于残差网络的全局优化网络（Res-GLOnet）进行了比较。与Res-GLOnet相比，ENA在逆向设计中产生了更高准确性和更好多样性的OMT解决方案。为了证明可解释性，我们将F-RAM应用于通过所提出的ENA方法获得的具有相似光学性质的各种OMT结构。我们发现，尽管材料配置、层数和厚度存在差异，但表现出类似光学性质的各种OMT结构的特征重要性分布是一致的。此外，我们通过将OMTs的初始层限制为SiO2和100 nm来证明ENA方法的灵活性。", "summary": "该研究提出了一种名为扩展神经伴随（ENA）的新框架，用于光学多层薄膜（OMTs）的可解释逆向设计。ENA通过引入新型前向神经网络架构和材料损失函数，增强了现有神经伴随方法的可伸缩性，并促进了材料配置的探索。同时，提出了回归激活映射（F-RAM）以提高模型的可解释性。实验结果表明，ENA在准确性、效率、多样性、可伸缩性、灵活性和可解释性方面均表现出色，尤其在准确性和多样性上优于现有方法，并能有效揭示不同OMT结构中特征重要性的一致性。", "keywords": "光学多层薄膜, 逆向设计, 神经伴随, 可解释性, 回归激活映射", "comments": "该论文的创新之处在于提出了一个名为扩展神经伴随（ENA）的综合框架，该框架首次统一解决了光学多层薄膜逆向设计中的六个关键标准：准确性、效率、多样性、可伸缩性、灵活性和可解释性。特别值得关注的是，引入材料损失函数以探索材料配置，以及提出回归激活映射（F-RAM）以提升模型的可解释性，为该领域带来了显著进步。这些方法使得逆向设计不仅高效准确，而且能提供深层洞察，对于推动光学薄膜设计自动化和理解具有重要意义。"}}
{"id": "2507.18993", "title": "Agent0: Leveraging LLM Agents to Discover Multi-value Features from Text for Enhanced Recommendations", "authors": ["Blaž Škrlj", "Benoît Guilleminot", "Andraž Tori"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Agent4IR, KDD '25", "url": "http://arxiv.org/abs/2507.18993v1", "summary": "Large language models (LLMs) and their associated agent-based frameworks have\nsignificantly advanced automated information extraction, a critical component\nof modern recommender systems. While these multitask frameworks are widely used\nin code generation, their application in data-centric research is still largely\nuntapped. This paper presents Agent0, an LLM-driven, agent-based system\ndesigned to automate information extraction and feature construction from raw,\nunstructured text. Categorical features are crucial for large-scale recommender\nsystems but are often expensive to acquire. Agent0 coordinates a group of\ninteracting LLM agents to automatically identify the most valuable text aspects\nfor subsequent tasks (such as models or AutoML pipelines). Beyond its feature\nengineering capabilities, Agent0 also offers an automated prompt-engineering\ntuning method that utilizes dynamic feedback loops from an oracle. Our findings\ndemonstrate that this closed-loop methodology is both practical and effective\nfor automated feature discovery, which is recognized as one of the most\nchallenging phases in current recommender system development.", "comment": "Agent4IR, KDD '25", "pdf_url": "http://arxiv.org/pdf/2507.18993v1", "cate": "cs.IR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "Agent0：利用LLM代理从文本中发现多值特征以增强推荐系统", "tldr": "Agent0是一个基于LLM代理的系统，用于自动化从非结构化文本中提取信息和构建特征，特别是针对推荐系统中的多值特征发现和自动提示工程调优。", "motivation": "自动化信息提取是现代推荐系统的关键组成部分，而分类特征对大规模推荐系统至关重要但获取成本高昂。当前推荐系统开发中，自动化特征发现是最具挑战性的阶段之一，LLM代理在数据中心研究中的应用潜力尚未充分挖掘。", "method": "Agent0通过协调一组相互作用的LLM代理来自动识别文本中最有价值的方面，用于后续任务。此外，它还提供了一种利用来自预言机的动态反馈循环的自动化提示工程调优方法。", "result": "研究结果表明，这种闭环方法对于自动化特征发现既实用又有效。", "conclusion": "Agent0的闭环方法成功解决了推荐系统开发中最具挑战性的自动化特征发现阶段的问题。", "translation": "大型语言模型（LLM）及其相关的基于代理的框架显著推动了自动化信息提取，这是现代推荐系统的关键组成部分。虽然这些多任务框架广泛应用于代码生成，但它们在以数据为中心的研究中的应用仍未被充分利用。本文提出了Agent0，一个由LLM驱动的、基于代理的系统，旨在自动化从原始、非结构化文本中进行信息提取和特征构建。分类特征对于大规模推荐系统至关重要，但通常获取成本高昂。Agent0协调一组相互作用的LLM代理，自动识别最有价值的文本方面，以用于后续任务（例如模型或AutoML管道）。除了其特征工程能力外，Agent0还提供了一种利用来自预言机的动态反馈循环的自动化提示工程调优方法。我们的研究结果表明，这种闭环方法对于自动化特征发现既实用又有效，而自动化特征发现被认为是当前推荐系统开发中最具挑战性的阶段之一。", "summary": "Agent0是一个基于LLM代理的系统，旨在自动化从非结构化文本中提取信息和构建特征，以增强推荐系统。它通过协调LLM代理来发现有价值的文本特征，并引入了一种利用动态反馈循环进行自动提示工程调优的方法。该研究证明了这种闭环方法在自动化特征发现方面的实用性和有效性，解决了推荐系统开发中的一个关键挑战。", "keywords": "LLM代理, 特征发现, 推荐系统, 文本信息提取, 提示工程", "comments": "Agent0的创新之处在于其将LLM代理应用于推荐系统中的自动化特征发现和构建，特别是针对难以获取的多值特征。其引入的基于动态反馈循环的自动提示工程调优方法也极具创新性，有望提高信息提取的效率和准确性。这篇论文展示了LLM代理在数据中心研究，特别是推荐系统领域，巨大的未开发潜力。"}}
{"id": "2506.22566", "title": "Exploration Behavior of Untrained Policies", "authors": ["Jacob Adamczyk"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      High-dimensional Learning Dynamics Workshop at ICML-2025", "url": "http://arxiv.org/abs/2506.22566v3", "summary": "Exploration remains a fundamental challenge in reinforcement learning (RL),\nparticularly in environments with sparse or adversarial reward structures. In\nthis work, we study how the architecture of deep neural policies implicitly\nshapes exploration before training. We theoretically and empirically\ndemonstrate strategies for generating ballistic or diffusive trajectories from\nuntrained policies in a toy model. Using the theory of infinite-width networks\nand a continuous-time limit, we show that untrained policies return correlated\nactions and result in non-trivial state-visitation distributions. We discuss\nthe distributions of the corresponding trajectories for a standard\narchitecture, revealing insights into inductive biases for tackling\nexploration. Our results establish a theoretical and experimental framework for\nusing policy initialization as a design tool to understand exploration behavior\nin early training.", "comment": "High-dimensional Learning Dynamics Workshop at ICML-2025", "pdf_url": "http://arxiv.org/pdf/2506.22566v3", "cate": "cs.LG", "date": "2025-06-27", "updated": "2025-07-24", "AI": {"title_translation": "未训练策略的探索行为", "tldr": "本文研究了深度神经网络策略架构在训练前如何影响探索行为，并提供了一个理论和实验框架，利用策略初始化来理解早期训练中的探索。", "motivation": "强化学习中的探索是一个基本挑战，尤其是在奖励稀疏或对抗性环境中。", "method": "本文通过理论和实验方法，在一个玩具模型中研究了深度神经网络策略架构在训练前如何隐式地塑造探索。研究使用了无限宽度网络理论和连续时间极限，分析了未训练策略的动作相关性及其导致的状态访问分布。", "result": "未训练策略会返回相关的动作，并产生非平凡的状态访问分布。研究揭示了标准架构对应轨迹的分布，为解决探索问题提供了归纳偏置的见解。", "conclusion": "本文的结果建立了一个理论和实验框架，可以将策略初始化作为一种设计工具，以理解早期训练中的探索行为。", "translation": "探索仍然是强化学习（RL）中的一个基本挑战，尤其是在奖励稀疏或对抗性环境中。在这项工作中，我们研究了深度神经网络策略的架构在训练之前如何隐式地塑造探索。我们通过理论和实验证明了在玩具模型中从未经训练的策略生成弹道或扩散轨迹的策略。利用无限宽度网络理论和连续时间极限，我们表明未经训练的策略会返回相关的动作，并导致非平凡的状态访问分布。我们讨论了标准架构相应轨迹的分布，揭示了解决探索问题的归纳偏置的见解。我们的结果建立了一个理论和实验框架，可以将策略初始化作为一种设计工具，以理解早期训练中的探索行为。", "summary": "本文探讨了深度神经网络策略架构在训练前如何影响强化学习中的探索行为，特别是在奖励稀疏的环境中。研究通过理论分析（无限宽度网络、连续时间极限）和实验验证，在一个玩具模型中展示了未训练策略如何生成弹道或扩散轨迹，并发现其动作相关且导致非平凡的状态访问分布。这为理解策略初始化在早期训练中对探索行为的影响提供了一个新的理论和实验框架。", "keywords": "强化学习, 探索, 策略初始化, 神经网络架构, 归纳偏置", "comments": "这项工作探讨了深度学习策略在训练前的内在探索行为，这是一个相对新颖且重要的研究方向。通过分析策略初始化对探索轨迹的影响，为理解和设计更有效的探索机制提供了新的视角，尤其是在处理RL中的稀疏奖励问题时具有潜在价值。"}}
{"id": "2507.18886", "title": "A Fast and Light-weight Non-Iterative Visual Odometry with RGB-D Cameras", "authors": ["Zheng Yang", "Kuan Xu", "Shenghai Yuan", "Lihua Xie"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18886v1", "summary": "In this paper, we introduce a novel approach for efficiently estimating the\n6-Degree-of-Freedom (DoF) robot pose with a decoupled, non-iterative method\nthat capitalizes on overlapping planar elements. Conventional RGB-D visual\nodometry(RGBD-VO) often relies on iterative optimization solvers to estimate\npose and involves a process of feature extraction and matching. This results in\nsignificant computational burden and time delays. To address this, our\ninnovative method for RGBD-VO separates the estimation of rotation and\ntranslation. Initially, we exploit the overlaid planar characteristics within\nthe scene to calculate the rotation matrix. Following this, we utilize a kernel\ncross-correlator (KCC) to ascertain the translation. By sidestepping the\nresource-intensive iterative optimization and feature extraction and alignment\nprocedures, our methodology offers improved computational efficacy, achieving a\nperformance of 71Hz on a lower-end i5 CPU. When the RGBD-VO does not rely on\nfeature points, our technique exhibits enhanced performance in low-texture\ndegenerative environments compared to state-of-the-art methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18886v1", "cate": "cs.RO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "一种基于RGB-D相机的快速轻量级非迭代视觉里程计", "tldr": "本文提出了一种新的、解耦的、非迭代的RGB-D视觉里程计，利用重叠平面元素来估计6自由度机器人姿态，显著提高了计算效率，并在低纹理环境下表现更优。", "motivation": "传统的RGB-D视觉里程计（RGBD-VO）依赖迭代优化求解器和特征提取匹配，导致计算负担重和时间延迟大。", "method": "本文提出了一种解耦的非迭代方法。首先利用场景中的重叠平面特性计算旋转矩阵；然后使用核互相关器（KCC）确定平移。通过避免资源密集型迭代优化和特征提取对齐过程，提高了计算效率。", "result": "该方法在低端i5 CPU上实现了71Hz的性能。与现有技术相比，在不依赖特征点时，该技术在低纹理退化环境中表现出增强的性能。", "conclusion": "本文提出的非迭代、解耦的RGB-D视觉里程计显著提高了计算效率，并在低纹理环境下展现出优越的性能，解决了传统方法计算量大和对纹理依赖的问题。", "translation": "在本文中，我们介绍了一种新颖的方法，通过解耦的非迭代方式，利用重叠平面元素有效估计机器人的6自由度（DoF）姿态。传统的RGB-D视觉里程计（RGBD-VO）通常依赖迭代优化求解器来估计姿态，并涉及特征提取和匹配过程。这导致了显著的计算负担和时间延迟。为了解决这个问题，我们创新的RGBD-VO方法将旋转和平移的估计分开。最初，我们利用场景中叠加的平面特性来计算旋转矩阵。随后，我们利用核互相关器（KCC）来确定平移。通过避开资源密集型迭代优化以及特征提取和对齐过程，我们的方法提供了改进的计算效率，在低端i5 CPU上实现了71Hz的性能。当RGBD-VO不依赖特征点时，我们的技术在低纹理退化环境中表现出比现有技术更强的性能。", "summary": "本文提出了一种创新的RGB-D视觉里程计（RGBD-VO）方法，旨在解决传统迭代优化和特征提取带来的高计算开销问题。该方法通过解耦旋转和平移的估计，首先利用场景中的重叠平面元素计算旋转，然后通过核互相关器（KCC）确定平移，从而避免了迭代优化和特征匹配。实验结果表明，该方法计算效率高，在低端i5 CPU上可达71Hz，并且在低纹理环境中表现优于现有技术。", "keywords": "视觉里程计, RGB-D, 非迭代, 姿态估计, 低纹理", "comments": "本文的创新点在于提出了一个非迭代和解耦的姿态估计方法，显著降低了计算复杂性，并通过利用平面元素和核互相关器实现了高效的旋转和平移估计。这种方法在资源受限的设备上具有重要意义，尤其是在机器人和AR/VR应用中。此外，其在低纹理环境下的性能提升解决了传统视觉里程计的常见挑战。"}}
{"id": "2507.19003", "title": "A diffusion-based generative model for financial time series via geometric Brownian motion", "authors": ["Gihun Kim", "Sun-Yong Choi", "Yeoneung Kim"], "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "60H10, 91G80, 91G60"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19003v1", "summary": "We propose a novel diffusion-based generative framework for financial time\nseries that incorporates geometric Brownian motion (GBM), the foundation of the\nBlack--Scholes theory, into the forward noising process. Unlike standard\nscore-based models that treat price trajectories as generic numerical\nsequences, our method injects noise proportionally to asset prices at each time\nstep, reflecting the heteroskedasticity observed in financial time series. By\naccurately balancing the drift and diffusion terms, we show that the resulting\nlog-price process reduces to a variance-exploding stochastic differential\nequation, aligning with the formulation in score-based generative models. The\nreverse-time generative process is trained via denoising score matching using a\nTransformer-based architecture adapted from the Conditional Score-based\nDiffusion Imputation (CSDI) framework. Empirical evaluations on historical\nstock data demonstrate that our model reproduces key stylized facts\nheavy-tailed return distributions, volatility clustering, and the leverage\neffect more realistically than conventional diffusion models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19003v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "一种基于几何布朗运动的金融时间序列扩散生成模型", "tldr": "该论文提出了一种新的基于扩散的金融时间序列生成模型，通过将几何布朗运动整合到前向噪声过程中，更好地捕捉金融时间序列的异方差性，并在历史股票数据上表现出更真实地重现金融风格化事实的能力。", "motivation": "传统的基于分数的模型将价格轨迹视为一般的数值序列，未能反映金融时间序列中观察到的异方差性。因此，需要一种能够更准确地模拟金融数据特性的生成模型。", "method": "本文提出了一种新颖的基于扩散的生成框架，将几何布朗运动（GBM）融入前向加噪过程。该方法在每个时间步按资产价格比例注入噪声，以反映金融时间序列的异方差性。通过平衡漂移和扩散项，对数价格过程简化为方差爆炸随机微分方程。逆向生成过程通过去噪分数匹配进行训练，采用基于Transformer的架构，该架构改编自条件分数扩散插补（CSDI）框架。", "result": "在历史股票数据上的实证评估表明，该模型比传统扩散模型更真实地重现了关键的风格化事实，如重尾收益分布、波动率聚类和杠杆效应。", "conclusion": "该研究成功地开发了一种结合几何布朗运动的扩散生成模型，该模型能够有效捕捉金融时间序列的异方差性，并在模拟金融市场特有现象方面优于现有模型。", "translation": "我们提出了一种新颖的基于扩散的金融时间序列生成框架，该框架将几何布朗运动（GBM）（布莱克-斯科尔斯理论的基础）整合到前向加噪过程中。与将价格轨迹视为通用数值序列的标准基于分数的模型不同，我们的方法在每个时间步按资产价格比例注入噪声，反映了金融时间序列中观察到的异方差性。通过精确平衡漂移和扩散项，我们表明所得的对数价格过程简化为方差爆炸随机微分方程，与基于分数的生成模型中的公式一致。逆向生成过程通过去噪分数匹配进行训练，使用改编自条件分数扩散插补（CSDI）框架的基于Transformer的架构。对历史股票数据的实证评估表明，我们的模型比传统扩散模型更真实地重现了关键的风格化事实——重尾收益分布、波动率聚类和杠杆效应。", "summary": "本论文介绍了一种创新的基于扩散的生成模型，专为金融时间序列设计。该模型通过在前向噪声过程中融入几何布朗运动，解决了传统扩散模型未能捕捉金融数据异方差性的问题。它通过按资产价格比例注入噪声来模拟金融市场的真实行为。模型采用Transformer架构进行去噪分数匹配训练。实验结果表明，该模型能更准确地再现金融时间序列的关键风格化事实，如重尾分布、波动率聚类和杠杆效应，优于现有扩散模型。", "keywords": "扩散模型, 金融时间序列, 几何布朗运动, 异方差性, 风格化事实", "comments": "该论文的创新之处在于将金融学中的经典几何布朗运动与现代扩散模型相结合，有效解决了金融时间序列异方差性的建模难题。通过引入领域知识，使得模型能够更真实地捕捉金融数据的复杂特性，提升了生成金融时间序列的准确性和实用性。"}}
{"id": "2507.18840", "title": "New source, new possibilities: An exploratory study of Bluesky posts referencing scholarly articles", "authors": ["Er-Te Zheng", "Xiaorui Jiang", "Zhichao Fang", "Mike Thelwall"], "categories": ["cs.DL", "cs.CY"], "primary_category": "Subjects:       Digital Libraries (cs.DL)", "pdf_link": null, "comments": "Comments:      30 pages, 8 figures", "url": "http://arxiv.org/abs/2507.18840v1", "summary": "Amid the migration of academics from X, the social media platform Bluesky has\nbeen proposed as a potential alternative. To assess its viability and relevance\nfor science communication, this study presents the first large-scale analysis\nof scholarly article dissemination on Bluesky, exploring its potential as a new\nsource of social media metrics. We collected and analysed 87,470 Bluesky posts\nreferencing 72,898 scholarly articles from February 2024 to April 2025,\nintegrating metadata from the OpenAlex database. We examined temporal trends,\ndisciplinary coverage, language use, textual characteristics, and user\nengagement. A sharp increase in scholarly activity on Bluesky was observed from\nNovember 2024, coinciding with broader academic shifts away from X. Posts\nprimarily focus on the social, environmental, and medical sciences and are\npredominantly written in English. As on X, likes and reposts are much more\ncommon than replies and quotes. Nevertheless, Bluesky posts demonstrate a\nhigher degree of textual originality than previously observed on X, suggesting\ngreater interpretive engagement. These findings highlight Bluesky's emerging\nrole as a credible platform for science communication and a promising source\nfor altmetrics. The platform may facilitate not only early visibility of\nresearch outputs but also more meaningful scholarly dialogue in the evolving\nsocial media landscape.", "comment": "30 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.18840v1", "cate": "cs.DL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "新来源，新可能性：一项关于Bluesky上引用学术文章帖子的探索性研究", "tldr": "本研究首次大规模分析了Bluesky上学术文章的传播情况，发现该平台在学术界从X迁移后，作为科学传播的新兴平台和替代计量学的新来源，具有更高的文本原创性和潜力。", "motivation": "鉴于学者从X平台迁移，Bluesky被提议作为潜在替代方案。本研究旨在评估其在科学传播方面的可行性和相关性，并探索其作为社交媒体指标新来源的潜力。", "method": "本研究收集并分析了2024年2月至2025年4月期间87,470条引用了72,898篇学术文章的Bluesky帖子，并整合了来自OpenAlex数据库的元数据。研究检查了时间趋势、学科覆盖范围、语言使用、文本特征和用户参与度。", "result": "研究观察到自2024年11月起Bluesky上的学术活动急剧增加，这与学术界普遍从X平台转移的趋势一致。帖子主要集中在社会科学、环境科学和医学领域，且主要使用英语。与X平台类似，点赞和转发远多于回复和引用。然而，Bluesky帖子的文本原创性高于X平台，表明更深层次的解释性参与。", "conclusion": "这些发现突出了Bluesky作为科学传播的可信平台和替代计量学有前景的来源的新兴作用。该平台不仅可能促进研究成果的早期可见性，还可能在不断发展的社交媒体环境中促进更有意义的学术对话。", "translation": "在学者从X社交媒体平台迁移之际，Bluesky已被提议作为一种潜在的替代方案。为了评估其在科学传播方面的可行性和相关性，本研究首次对Bluesky上学术文章的传播进行了大规模分析，探索其作为社交媒体指标新来源的潜力。我们收集并分析了2024年2月至2025年4月期间87,470条引用了72,898篇学术文章的Bluesky帖子，并整合了OpenAlex数据库的元数据。我们检查了时间趋势、学科覆盖范围、语言使用、文本特征和用户参与度。研究观察到自2024年11月起Bluesky上的学术活动急剧增加，这与学术界普遍从X平台转移的趋势一致。帖子主要集中在社会科学、环境科学和医学领域，且主要使用英语。与X平台类似，点赞和转发远多于回复和引用。然而，Bluesky帖子的文本原创性高于X平台，表明更深层次的解释性参与。这些发现突出了Bluesky作为科学传播的可信平台和替代计量学有前景的来源的新兴作用。该平台不仅可能促进研究成果的早期可见性，还可能在不断发展的社交媒体环境中促进更有意义的学术对话。", "summary": "本研究首次大规模分析了Bluesky平台在学术界从X平台迁移后的科学传播潜力。通过对87,470条引用学术文章的Bluesky帖子进行分析，发现该平台自2024年11月起学术活动显著增加，主要集中在社会、环境和医学领域，并以英语为主。虽然用户参与模式与X类似（点赞和转发多于回复），但Bluesky上的帖子表现出更高的文本原创性。研究结论认为Bluesky是一个新兴的、可信的科学传播平台，也是替代计量学（altmetrics）的有前景来源，有助于研究成果的早期可见性和更深入的学术对话。", "keywords": "Bluesky, 科学传播, 替代计量学, 社交媒体, 学术交流", "comments": "本研究的创新之处在于首次对Bluesky上的学术文章传播进行了大规模分析，填补了该领域的数据空白。其重要性在于为评估Bluesky作为科学传播平台和替代计量学来源提供了实证基础，特别是在学术界从X平台迁移的背景下。研究发现Bluesky帖子具有更高的文本原创性，这表明该平台可能促进更深入的学术交流，超越了简单的信息分享。"}}
{"id": "2507.19253", "title": "BridgeNet: A Unified Multimodal Framework for Bridging 2D and 3D Industrial Anomaly Detection", "authors": ["An Xiang", "Zixuan Huang", "Xitong Gao", "Kejiang Ye", "Cheng-zhong Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19253v1", "summary": "Industrial anomaly detection for 2D objects has gained significant attention\nand achieved progress in anomaly detection (AD) methods. However, identifying\n3D depth anomalies using only 2D information is insufficient. Despite\nexplicitly fusing depth information into RGB images or using point cloud\nbackbone networks to extract depth features, both approaches struggle to\nadequately represent 3D information in multimodal scenarios due to the\ndisparities among different modal information. Additionally, due to the\nscarcity of abnormal samples in industrial data, especially in multimodal\nscenarios, it is necessary to perform anomaly generation to simulate real-world\nabnormal samples. Therefore, we propose a novel unified multimodal anomaly\ndetection framework to address these issues. Our contributions consist of 3 key\naspects. (1) We extract visible depth information from 3D point cloud data\nsimply and use 2D RGB images to represent appearance, which disentangles depth\nand appearance to support unified anomaly generation. (2) Benefiting from the\nflexible input representation, the proposed Multi-Scale Gaussian Anomaly\nGenerator and Unified Texture Anomaly Generator can generate richer anomalies\nin RGB and depth. (3) All modules share parameters for both RGB and depth data,\neffectively bridging 2D and 3D anomaly detection. Subsequent modules can\ndirectly leverage features from both modalities without complex fusion.\nExperiments show our method outperforms state-of-the-art (SOTA) on MVTec-3D AD\nand Eyecandies datasets. Code available at:\nhttps://github.com/Xantastic/BridgeNet", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19253v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "BridgeNet：一种弥合2D与3D工业异常检测的统一多模态框架", "tldr": "BridgeNet提出了一个统一的多模态框架，通过解耦深度和外观、生成丰富的异常样本以及共享参数，有效解决了2D和3D工业异常检测中的多模态信息表示不足和异常样本稀缺问题，并在多个数据集上取得了SOTA性能。", "motivation": "现有的2D工业异常检测方法在识别3D深度异常时信息不足。尽管尝试融合深度信息或使用点云骨干网络，但由于不同模态信息之间的差异，这些方法难以充分表示3D信息。此外，工业数据中异常样本（尤其是在多模态场景下）的稀缺性，使得需要进行异常生成来模拟真实世界的异常样本。", "method": "我们提出了一个新颖的统一多模态异常检测框架。主要贡献包括：1) 简单地从3D点云数据中提取可见深度信息，并使用2D RGB图像表示外观，从而解耦深度和外观，以支持统一的异常生成。2) 受益于灵活的输入表示，所提出的多尺度高斯异常生成器和统一纹理异常生成器能够生成更丰富的RGB和深度异常。3) 所有模块共享RGB和深度数据的参数，有效弥合了2D和3D异常检测，后续模块可以直接利用两种模态的特征，无需复杂的融合。", "result": "实验表明，我们的方法在MVTec-3D AD和Eyecandies数据集上优于最先进（SOTA）的方法。", "conclusion": "该论文成功提出了BridgeNet，一个统一的多模态框架，有效解决了2D和3D工业异常检测中多模态信息表示不足和异常样本稀缺的问题，通过创新的解耦、异常生成和参数共享机制，实现了卓越的性能。", "translation": "工业二维物体异常检测已经获得了广泛关注，并在异常检测（AD）方法上取得了进展。然而，仅使用二维信息识别三维深度异常是不够的。尽管将深度信息显式融合到RGB图像中或使用点云骨干网络提取深度特征，但由于不同模态信息之间的差异，这两种方法都难以在多模态场景中充分表示三维信息。此外，由于工业数据中异常样本的稀缺性，特别是在多模态场景下，因此有必要进行异常生成来模拟真实世界的异常样本。因此，我们提出了一种新颖的统一多模态异常检测框架来解决这些问题。我们的贡献包括三个关键方面。(1) 我们简单地从三维点云数据中提取可见深度信息，并使用二维RGB图像表示外观，从而解耦深度和外观，以支持统一的异常生成。(2) 受益于灵活的输入表示，所提出的多尺度高斯异常生成器和统一纹理异常生成器能够生成更丰富的RGB和深度异常。(3) 所有模块共享RGB和深度数据的参数，有效弥合了二维和三维异常检测。后续模块可以直接利用两种模态的特征，无需复杂的融合。实验表明，我们的方法在MVTec-3D AD和Eyecandies数据集上优于最先进（SOTA）的方法。代码可在以下网址获取：https://github.com/Xantastic/BridgeNet", "summary": "BridgeNet是一个新颖的统一多模态框架，旨在解决2D和3D工业异常检测中多模态信息表示不足和异常样本稀缺的问题。该框架通过简单地从3D点云中提取可见深度信息并使用2D RGB图像表示外观来解耦深度和外观，从而支持统一的异常生成。它利用多尺度高斯异常生成器和统一纹理异常生成器生成丰富的RGB和深度异常。此外，所有模块共享RGB和深度数据的参数，有效弥合了2D和3D异常检测。实验证明，BridgeNet在MVTec-3D AD和Eyecandies数据集上超越了现有最先进的方法。", "keywords": "工业异常检测, 多模态, 2D-3D桥接, 异常生成, BridgeNet", "comments": "BridgeNet的创新之处在于其统一的多模态框架，能够有效桥接2D和3D异常检测。通过解耦深度和外观信息，并引入灵活的异常生成器，它解决了多模态数据表示和异常样本稀缺的关键挑战。参数共享机制简化了特征融合，提高了模型的效率和有效性。其在SOTA数据集上的优异表现证明了该方法的强大实用性。"}}
{"id": "2506.20380", "title": "TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis", "authors": ["Zhengpeng Feng", "Clement Atzberger", "Sadiq Jaffer", "Jovana Knezevic", "Silja Sormunen", "Robin Young", "Madeline C Lisaius", "Markus Immitzer", "David A. Coomes", "Anil Madhavapeddy", "Andrew Blake", "Srinivasan Keshav"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.20380v2", "summary": "Satellite remote sensing from repeated observations and multiple sensors\nenables a wide range of downstream applications, including climate modeling,\ncarbon accounting, and strategies for conservation and sustainable land use.\nHowever, satellite time series are voluminous, often corrupted by sensor noise,\nclouds, and atmospheric conditions, and unevenly spaced in time, making them\nchallenging to use. We present TESSERA, an open, global, land-oriented remote\nsensing foundation model that uses self-supervised learning to generate\n`ready-to-use' embeddings at 10~m scale from pixel-level satellite time series\ndata. TESSERA uses two parallel Transformer-based encoders to combine optical\ndata from ten Sentinel-2 spectral bands at 10-60~m spatial resolution and two\nSentinel-1 synthetic aperture radar backscatter coefficients at 10~m resolution\nto create embeddings that are subsequently fused with a multilayer perceptron\nto create annual global embedding maps. We compare our work with\nstate-of-the-art task-specific models and other foundation models in five\ndiverse downstream tasks and find that TESSERA closely matches or outperforms\nthese baselines. We believe that TESSERA's ease of use, openness, computation-,\nlabel-, and data-efficiency, and high performance will prove transformative in\na wide range of vegetation-oriented ecological and agricultural applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20380v2", "cate": "cs.LG", "date": "2025-06-25", "updated": "2025-07-25", "AI": {"title_translation": "TESSERA：用于地球表示和分析的地表光谱时间嵌入", "tldr": "TESSERA是一个开放的全球陆地遥感基础模型，它使用自监督学习从像素级卫星时间序列数据生成即用型嵌入，并在多个下游任务中表现出色。", "motivation": "卫星遥感时间序列数据量大、易受传感器噪声、云和大气条件影响，且时间间隔不均，这使得它们难以使用。为了解决这些挑战，作者提出了TESSERA。", "method": "TESSERA是一个开放的、全球的、面向陆地的遥感基础模型。它使用自监督学习从像素级卫星时间序列数据生成10米尺度的“即用型”嵌入。TESSERA使用两个并行的基于Transformer的编码器，结合来自十个Sentinel-2光谱波段的光学数据（10-60米空间分辨率）和两个Sentinel-1合成孔径雷达反向散射系数（10米分辨率），创建嵌入，然后通过多层感知器融合，生成年度全球嵌入图。", "result": "TESSERA在五个不同的下游任务中与最先进的特定任务模型和其他基础模型进行了比较，结果显示TESSERA与这些基线模型表现相近或超越它们。", "conclusion": "作者认为，TESSERA的易用性、开放性、计算效率、标签效率、数据效率和高性能将在广泛的植被生态和农业应用中证明其变革性。", "translation": "卫星遥感通过重复观测和多传感器技术，支持广泛的下游应用，包括气候建模、碳核算以及保护和可持续土地利用策略。然而，卫星时间序列数据量庞大，常受到传感器噪声、云和大气条件的干扰，且时间间隔不均，这使得它们难以使用。我们提出了TESSERA，一个开放的、全球的、面向陆地的遥感基础模型，它利用自监督学习从像素级卫星时间序列数据生成10米尺度的“即用型”嵌入。TESSERA使用两个并行的基于Transformer的编码器，结合来自十个Sentinel-2光谱波段的光学数据（10-60米空间分辨率）和两个Sentinel-1合成孔径雷达反向散射系数（10米分辨率），创建嵌入，然后通过多层感知器融合，生成年度全球嵌入图。我们将我们的工作与五个不同下游任务中的最先进的特定任务模型和其他基础模型进行了比较，发现TESSERA与这些基线模型表现相近或超越它们。我们相信TESSERA的易用性、开放性、计算效率、标签效率、数据效率和高性能将在广泛的植被生态和农业应用中证明其变革性。", "summary": "TESSERA是一个新颖的全球陆地遥感基础模型，旨在解决卫星时间序列数据使用中的挑战。该模型利用自监督学习，通过两个Transformer编码器融合Sentinel-1和Sentinel-2数据，生成10米分辨率的“即用型”时间嵌入。这些嵌入随后被用于创建年度全球嵌入图。在多项下游任务中，TESSERA表现出与现有先进模型相当或更优的性能，预示其在生态和农业应用中的巨大潜力。", "keywords": "遥感, 地球表示, 时间嵌入, 基础模型, 自监督学习", "comments": "TESSERA的创新之处在于其结合了多种卫星数据源（Sentinel-1和Sentinel-2）并利用自监督学习来生成通用且“即用型”的地球表面时间嵌入，解决了传统卫星数据处理的复杂性。其作为“基础模型”的定位，以及在多个下游任务中超越或媲美现有模型的能力，表明其在推动遥感应用，特别是植被相关领域，具有重要意义。模型的开放性、数据和计算效率也是其优势。"}}
{"id": "2507.18654", "title": "Diffusion Models for Solving Inverse Problems via Posterior Sampling with Piecewise Guidance", "authors": ["Saeed Mohseni-Sehdeh", "Walid Saad", "Kei Sakaguchi", "Tao Yu"], "categories": ["cs.LG", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18654v1", "summary": "Diffusion models are powerful tools for sampling from high-dimensional\ndistributions by progressively transforming pure noise into structured data\nthrough a denoising process. When equipped with a guidance mechanism, these\nmodels can also generate samples from conditional distributions. In this paper,\na novel diffusion-based framework is introduced for solving inverse problems\nusing a piecewise guidance scheme. The guidance term is defined as a piecewise\nfunction of the diffusion timestep, facilitating the use of different\napproximations during high-noise and low-noise phases. This design is shown to\neffectively balance computational efficiency with the accuracy of the guidance\nterm. Unlike task-specific approaches that require retraining for each problem,\nthe proposed method is problem-agnostic and readily adaptable to a variety of\ninverse problems. Additionally, it explicitly incorporates measurement noise\ninto the reconstruction process. The effectiveness of the proposed framework is\ndemonstrated through extensive experiments on image restoration tasks,\nspecifically image inpainting and super-resolution. Using a class conditional\ndiffusion model for recovery, compared to the \\pgdm baseline, the proposed\nframework achieves a reduction in inference time of \\(25\\%\\) for inpainting\nwith both random and center masks, and \\(23\\%\\) and \\(24\\%\\) for \\(4\\times\\)\nand \\(8\\times\\) super-resolution tasks, respectively, while incurring only\nnegligible loss in PSNR and SSIM.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18654v1", "cate": "cs.LG", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "基于分段引导的后验采样扩散模型在逆问题求解中的应用", "tldr": "本文提出了一种新颖的基于扩散模型的框架，通过分段引导机制有效解决逆问题，提高了计算效率并保持了准确性。", "motivation": "扩散模型在处理高维分布采样方面表现强大，但现有方法在解决逆问题时可能存在效率和准确性平衡的问题，以及对特定任务的依赖性。本文旨在开发一种通用的、高效的扩散模型框架来解决各类逆问题。", "method": "本文引入了一种新颖的基于扩散模型的框架，通过使用分段引导机制来解决逆问题。引导项被定义为扩散时间步长的分段函数，允许在高噪声和低噪声阶段使用不同的近似。该方法是问题无关的，并明确将测量噪声纳入重建过程。", "result": "在图像修复（随机和中心掩码）和超分辨率（4倍和8倍）任务上进行了广泛实验。与\npgdm基线相比，所提出的框架在图像修复中将推理时间减少了25%，在超分辨率任务中分别减少了23%和24%，同时PSNR和SSIM的损失可以忽略不计。", "conclusion": "本文提出的基于分段引导的扩散模型框架，在解决逆问题方面表现出卓越的性能，有效平衡了计算效率和引导项的准确性，并且具有问题无关性，在图像恢复任务中取得了显著的效率提升而没有明显性能损失。", "translation": "扩散模型是通过去噪过程将纯噪声逐步转化为结构化数据，从而从高维分布中采样的强大工具。当配备引导机制时，这些模型还可以从条件分布中生成样本。本文引入了一种新颖的基于扩散模型的框架，利用分段引导方案解决逆问题。引导项被定义为扩散时间步长的分段函数，有助于在高噪声和低噪声阶段使用不同的近似。这种设计被证明能有效平衡计算效率和引导项的准确性。与需要针对每个问题进行重新训练的特定任务方法不同，所提出的方法与问题无关，并且易于适应各种逆问题。此外，它明确地将测量噪声纳入重建过程。通过图像恢复任务（特别是图像修复和超分辨率）的广泛实验证明了所提出框架的有效性。使用类条件扩散模型进行恢复，与\\pgdm基线相比，所提出的框架在随机和中心掩码的图像修复中推理时间减少了25%，在4倍和8倍超分辨率任务中分别减少了23%和24%，同时PSNR和SSIM的损失可以忽略不计。", "summary": "本文提出了一种基于扩散模型的新型框架，利用分段引导机制解决逆问题。该方法将引导项设计为扩散时间步长的分段函数，以在高噪声和低噪声阶段采用不同的近似，从而有效平衡计算效率和引导准确性。该框架是问题无关的，可适应多种逆问题，并明确考虑测量噪声。实验证明，在图像修复和超分辨率等任务中，该方法显著缩短了推理时间（最高达25%），同时保持了可忽略不计的图像质量损失。", "keywords": "扩散模型, 逆问题, 分段引导, 图像修复, 超分辨率", "comments": "这项工作的主要创新在于引入了分段引导机制，这有效地解决了扩散模型在逆问题求解中计算效率与准确性之间的平衡问题。其问题无关性使其具有很高的通用性和实用价值，避免了针对特定任务的重复训练。在图像恢复任务中的显著效率提升，且几乎不牺牲性能，证明了该方法的有效性。"}}
{"id": "2507.19114", "title": "A Therapeutic Role-Playing VR Game for Children with Intellectual Disabilities", "authors": ["Santiago Berrezueta-Guzman", "WenChun Chen", "Stefan Wagner"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Paper accepted for publication and presentation in the 3rd Annual IEEE International Conference on Metaverse Computing, Networking, and Applications (IEEE MetaCom 2025) will be held in Sejong University, Seoul, Republic of Korea, on August 27 - 29, 2025", "url": "http://arxiv.org/abs/2507.19114v1", "summary": "Virtual Reality (VR) offers promising avenues for innovative therapeutic\ninterventions in populations with intellectual disabilities (ID). This paper\npresents the design, development, and evaluation of Space Exodus, a novel\nVR-based role-playing game specifically tailored for children with ID. By\nintegrating immersive gameplay with therapeutic task design, Space Exodus aims\nto enhance concentration, cognitive processing, and fine motor skills through\nstructured hand-eye coordination exercises. A six-week pre-test/post-test study\nwas conducted with 16 children in Ecuador, using standardized assessments, the\nToulouse-Pieron Cancellation Test, and the Moss Attention Rating Scale\ncomplemented by detailed observational metrics. Quantitative results indicate\nstatistically significant improvements in concentration scores, with test\nscores increasing from 65.2 to 80.3 and 55.4 to 68.7, respectively (p < 0.01).\nQualitative observations revealed reduced task attempts, enhanced user\nconfidence, and increased active participation. The inclusion of a VR assistant\nprovided consistent guidance that further boosted engagement. These findings\ndemonstrate the potential of immersive, game-based learning environments as\npractical therapeutic tools, laying a robust foundation for developing\ninclusive and adaptive rehabilitation strategies for children with ID.", "comment": "Paper accepted for publication and presentation in the 3rd Annual\n  IEEE International Conference on Metaverse Computing, Networking, and\n  Applications (IEEE MetaCom 2025) will be held in Sejong University, Seoul,\n  Republic of Korea, on August 27 - 29, 2025", "pdf_url": "http://arxiv.org/pdf/2507.19114v1", "cate": "cs.HC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "一种针对智力障碍儿童的治疗性角色扮演VR游戏", "tldr": "本文介绍并评估了一款名为Space Exodus的VR角色扮演游戏，旨在通过沉浸式游戏和治疗任务来提高智力障碍儿童的专注力、认知处理能力和精细运动技能。一项为期六周的研究显示，该游戏显著提升了儿童的专注力分数，并增强了参与度。", "motivation": "虚拟现实（VR）为智力障碍（ID）人群提供了创新治疗干预的潜在途径。本文旨在设计、开发和评估一款专门为智力障碍儿童定制的VR角色扮演游戏，以增强他们的专注力、认知处理能力和精细运动技能。", "method": "本文设计、开发并评估了名为“Space Exodus”的VR角色扮演游戏。该游戏将沉浸式玩法与治疗任务设计相结合，旨在通过结构化的手眼协调练习来提高专注力、认知处理能力和精细运动技能。在厄瓜多尔对16名儿童进行了一项为期六周的前测/后测研究，使用了标准化评估、Toulouse-Pieron划消测验和Moss注意力评定量表，并辅以详细的观察指标。", "result": "定量结果显示，专注力分数有统计学上的显著提高，测试分数分别从65.2上升到80.3和从55.4上升到68.7（p < 0.01）。定性观察揭示了任务尝试次数的减少、用户信心的增强以及主动参与度的提高。VR助手的加入提供了持续的指导，进一步提升了参与度。", "conclusion": "研究结果表明，沉浸式、基于游戏的学习环境具有作为实用治疗工具的潜力，为开发针对智力障碍儿童的包容性和适应性康复策略奠定了坚实的基础。", "translation": "虚拟现实（VR）为智力障碍（ID）人群的创新治疗干预提供了有前景的途径。本文介绍了一款名为Space Exodus的新型VR角色扮演游戏的设计、开发和评估，该游戏专为智力障碍儿童量身定制。通过将沉浸式游戏与治疗任务设计相结合，Space Exodus旨在通过结构化的手眼协调练习来提高专注力、认知处理能力和精细运动技能。在厄瓜多尔对16名儿童进行了一项为期六周的前测/后测研究，使用了标准化评估、图卢兹-皮耶龙划消测验和莫斯注意力评定量表，并辅以详细的观察指标。定量结果表明，专注力分数有统计学上的显著提高，测试分数分别从65.2上升到80.3和从55.4上升到68.7（p < 0.01）。定性观察揭示了任务尝试次数的减少、用户信心的增强以及主动参与度的提高。VR助手的加入提供了持续的指导，进一步提升了参与度。这些发现证明了沉浸式、基于游戏的学习环境作为实用治疗工具的潜力，为开发针对智力障碍儿童的包容性和适应性康复策略奠定了坚实的基础。", "summary": "本文介绍了一款名为“Space Exodus”的VR角色扮演游戏，专为智力障碍儿童设计，旨在通过沉浸式游戏和治疗任务提升他们的专注力、认知处理和精细运动技能。一项在厄瓜多尔对16名儿童进行的六周研究表明，该游戏显著提高了专注力分数，并增强了参与度和自信心。研究结果强调了基于VR的游戏作为智力障碍儿童康复工具的巨大潜力。", "keywords": "VR游戏, 智力障碍儿童, 治疗干预, 专注力, 康复", "comments": "这项研究的创新之处在于将VR技术与角色扮演游戏相结合，为智力障碍儿童提供了一种新颖且具有吸引力的治疗干预方式。其重要性在于证明了游戏化学习环境在认知和运动技能提升方面的有效性，为未来的包容性康复策略提供了实践基础。研究规模相对较小（16名儿童），未来需要更大规模的研究来进一步验证其普适性和长期效果。"}}
{"id": "2507.17765", "title": "ASR-Guided Speaker-Role Diarization and Diarization-Guided ASR Decoding", "authors": ["Arindam Ghosh", "Mark Fuhs", "Bongjun Kim", "Anurag Chowdhury", "Monika Woszczyna"], "categories": ["eess.AS", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2507.17765v2", "summary": "From an application standpoint, speaker-role diarization (RD), such as doctor\nvs. patient, host vs. guest, etc. is often more useful than traditional speaker\ndiarization (SD), which assigns generic labels like speaker-1, speaker-2 etc.\nIn the context of joint automatic speech recognition (ASR) + SD (who spoke\nwhat?), recent end-to-end models employ an auxiliary SD transducer,\nsynchronized with the ASR transducer, to predict speakers per word. In this\npaper, we extend this framework to RD with three key contributions: (1) we\nsimplify the training via forced alignment and cross-entropy loss instead of\nRNNT loss, (2) we show that word prediction and role prediction require\ndifferent amounts of predictor's context, leading to separate task-specific\npredictors, unlike existing shared-predictor models, and (3) we propose a way\nto leverage RD posterior activity to influence ASR decoding and reduce\nsmall-word deletion errors.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2507.17765v2", "cate": "eess.AS", "date": "2025-07-14", "updated": "2025-07-25", "AI": {"title_translation": "ASR引导的说话人角色识别与说话人识别引导的ASR解码", "tldr": "本文将联合自动语音识别（ASR）和说话人识别（SD）框架扩展到说话人角色识别（RD），通过简化训练、使用任务特定预测器以及利用RD信息指导ASR解码，从而减少小词删除错误。", "motivation": "从应用角度来看，说话人角色识别（如医生与患者、主持人与嘉宾等）通常比传统的说话人识别（分配泛型标签如speaker-1、speaker-2等）更有用。本文旨在将现有的端到端联合ASR+SD框架扩展到说话人角色识别。", "method": "本文将联合ASR+SD框架扩展到说话人角色识别（RD），主要有三个贡献：1. 通过强制对齐和交叉熵损失简化训练，取代了RNNT损失。2. 发现单词预测和角色预测需要不同量的预测器上下文，因此提出了独立的任务特定预测器，而非现有模型中的共享预测器。3. 提出了一种利用RD后验活动来影响ASR解码并减少小词删除错误的方法。", "result": "所提出的利用说话人角色识别（RD）后验活动影响ASR解码的方法，能够减少小词删除错误。", "conclusion": "Not mentioned in abstract", "translation": "从应用角度来看，说话人角色识别（RD），例如医生与患者、主持人与嘉宾等，通常比传统的说话人识别（SD）（它分配通用标签如speaker-1、speaker-2等）更有用。在联合自动语音识别（ASR）+SD（谁说了什么？）的背景下，最近的端到端模型采用一个辅助的SD传感器，与ASR传感器同步，以预测每个单词的说话人。在本文中，我们将此框架扩展到RD，并提出了三个关键贡献：（1）我们通过强制对齐和交叉熵损失而不是RNNT损失来简化训练，（2）我们展示了单词预测和角色预测需要不同量的预测器上下文，从而导致独立的任务特定预测器，这与现有共享预测器模型不同，以及（3）我们提出了一种利用RD后验活动来影响ASR解码并减少小词删除错误的方法。", "summary": "本文将现有的联合自动语音识别（ASR）和说话人识别（SD）框架扩展到更具应用价值的说话人角色识别（RD）。研究提出了三项主要贡献：一是通过强制对齐和交叉熵损失简化训练过程；二是针对单词和角色预测对上下文需求不同的特点，设计了独立的任务特定预测器；三是利用RD的后验活动来指导ASR解码，从而有效减少了小词删除错误。", "keywords": "说话人角色识别, 自动语音识别, 语音识别, 强制对齐, 端到端", "comments": "该论文创新性地将说话人角色识别引入到ASR框架中，并提出了简化训练、优化预测器结构以及利用角色信息反向指导ASR解码的方法。特别值得关注的是，它展示了角色识别如何直接提升ASR性能，这种双向影响是其重要亮点。"}}
{"id": "2507.18738", "title": "An Explainable Equity-Aware P2P Energy Trading Framework for Socio-Economically Diverse Microgrid", "authors": ["Abhijan Theja", "Mayukha Pal"], "categories": ["eess.SY", "cs.GT", "cs.LG", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18738v1", "summary": "Fair and dynamic energy allocation in community microgrids remains a critical\nchallenge, particularly when serving socio-economically diverse participants.\nStatic optimization and cost-sharing methods often fail to adapt to evolving\ninequities, leading to participant dissatisfaction and unsustainable\ncooperation. This paper proposes a novel framework that integrates\nmulti-objective mixed-integer linear programming (MILP), cooperative game\ntheory, and a dynamic equity-adjustment mechanism driven by reinforcement\nlearning (RL). At its core, the framework utilizes a bi-level optimization\nmodel grounded in Equity-regarding Welfare Maximization (EqWM) principles,\nwhich incorporate Rawlsian fairness to prioritize the welfare of the least\nadvantaged participants. We introduce a Proximal Policy Optimization (PPO)\nagent that dynamically adjusts socio-economic weights in the optimization\nobjective based on observed inequities in cost and renewable energy access.\nThis RL-powered feedback loop enables the system to learn and adapt,\ncontinuously striving for a more equitable state. To ensure transparency,\nExplainable AI (XAI) is used to interpret the benefit allocations derived from\na weighted Shapley value. Validated across six realistic scenarios, the\nframework demonstrates peak demand reductions of up to 72.6%, and significant\ncooperative gains. The adaptive RL mechanism further reduces the Gini\ncoefficient over time, showcasing a pathway to truly sustainable and fair\nenergy communities.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18738v1", "cate": "eess.SY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "面向社会经济多样化微电网的可解释公平感知P2P能源交易框架", "tldr": "本文提出了一种新颖的可解释、公平感知的P2P能源交易框架，该框架结合了强化学习、博弈论和多目标优化，旨在解决社会经济多样化微电网中能源分配不公平的问题，并实现了显著的削峰和公平性提升。", "motivation": "在社区微电网中，尤其是在服务社会经济多样化参与者时，公平和动态的能源分配仍然是一个严峻的挑战。传统的静态优化和成本分摊方法难以适应不断演变的不平等，导致参与者不满和不可持续的合作。", "method": "本文提出了一种新颖的框架，该框架整合了多目标混合整数线性规划（MILP）、合作博弈论和由强化学习（RL）驱动的动态公平调整机制。核心是基于公平导向福利最大化（EqWM）原则的双层优化模型，该模型融入了罗尔斯公平原则以优先考虑最弱势参与者的福利。引入了近端策略优化（PPO）代理，根据观察到的成本和可再生能源获取方面的不公平性，动态调整优化目标中的社会经济权重。为确保透明度，使用可解释AI（XAI）来解释源自加权Shapley值的利益分配。", "result": "该框架在六个实际场景中进行了验证，结果表明峰值需求削减高达72.6%，并获得了显著的合作收益。自适应RL机制随着时间的推移进一步降低了基尼系数。", "conclusion": "该框架为实现真正可持续和公平的能源社区提供了一条途径。", "translation": "社区微电网中公平和动态的能源分配仍然是一个严峻的挑战，尤其是在服务社会经济多样化参与者时。静态优化和成本分摊方法往往无法适应不断演变的不平等，导致参与者不满和不可持续的合作。本文提出了一种新颖的框架，该框架整合了多目标混合整数线性规划（MILP）、合作博弈论和由强化学习（RL）驱动的动态公平调整机制。其核心是基于公平导向福利最大化（EqWM）原则的双层优化模型，该模型融入了罗尔斯公平原则以优先考虑最弱势参与者的福利。我们引入了一个近端策略优化（PPO）代理，该代理根据观察到的成本和可再生能源获取方面的不公平性，动态调整优化目标中的社会经济权重。这种由RL驱动的反馈循环使系统能够学习和适应，不断努力实现更公平的状态。为确保透明度，使用可解释AI（XAI）来解释源自加权Shapley值的利益分配。该框架在六个实际场景中进行了验证，结果表明峰值需求削减高达72.6%，并获得了显著的合作收益。自适应RL机制随着时间的推移进一步降低了基尼系数，展示了通向真正可持续和公平能源社区的途径。", "summary": "本文提出了一种面向社会经济多样化微电网的可解释公平感知P2P能源交易框架。该框架通过整合多目标混合整数线性规划、合作博弈论和基于强化学习（PPO）的动态公平调整机制，解决了现有方法在公平和动态能源分配方面的不足。它采用双层优化模型和罗尔斯公平原则来优先弱势群体。此外，利用可解释AI确保利益分配的透明性。在实际场景验证中，该框架显著降低了峰值需求并提高了合作收益，同时通过自适应RL机制有效降低了基尼系数，为构建可持续和公平的能源社区提供了新方案。", "keywords": "P2P能源交易, 微电网, 强化学习, 可解释AI, 公平感知", "comments": "该论文的创新点在于将强化学习（PPO代理）引入到P2P能源交易中，实现了动态的公平调整，以适应社会经济多样化参与者的需求，并优先考虑弱势群体。同时，结合可解释AI来提升透明度，这对于实际应用中的信任建立至关重要。其解决现有能源分配不公平和不可持续问题的能力，以及在削峰和提高公平性方面的显著效果，使得该框架具有重要的理论和实际意义。"}}
{"id": "2411.13807", "title": "MagicDrive-V2: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control", "authors": ["Ruiyuan Gao", "Kai Chen", "Bo Xiao", "Lanqing Hong", "Zhenguo Li", "Qiang Xu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 camera-ready version, Project Website: this https URL", "url": "http://arxiv.org/abs/2411.13807v4", "summary": "The rapid advancement of diffusion models has greatly improved video\nsynthesis, especially in controllable video generation, which is vital for\napplications like autonomous driving. Although DiT with 3D VAE has become a\nstandard framework for video generation, it introduces challenges in\ncontrollable driving video generation, especially for geometry control,\nrendering existing control methods ineffective. To address these issues, we\npropose MagicDrive-V2, a novel approach that integrates the MVDiT block and\nspatial-temporal conditional encoding to enable multi-view video generation and\nprecise geometric control. Additionally, we introduce an efficient method for\nobtaining contextual descriptions for videos to support diverse textual\ncontrol, along with a progressive training strategy using mixed video data to\nenhance training efficiency and generalizability. Consequently, MagicDrive-V2\nenables multi-view driving video synthesis with $3.3\\times$ resolution and\n$4\\times$ frame count (compared to current SOTA), rich contextual control, and\ngeometric controls. Extensive experiments demonstrate MagicDrive-V2's ability,\nunlocking broader applications in autonomous driving.", "comment": "ICCV 2025 camera-ready version, Project Website:\n  https://flymin.github.io/magicdrive-v2/", "pdf_url": "http://arxiv.org/pdf/2411.13807v4", "cate": "cs.CV", "date": "2024-11-21", "updated": "2025-07-25", "AI": {"title_translation": "MagicDrive-V2：用于自动驾驶的高分辨率长视频生成与自适应控制", "tldr": "MagicDrive-V2通过解决几何控制问题，并采用新的训练策略，改进了用于自动驾驶的可控、高分辨率、长视频生成。", "motivation": "现有的基于DiT和3D VAE的视频生成框架在可控驾驶视频生成中，尤其是在几何控制方面面临挑战，导致现有控制方法无效。", "method": "MagicDrive-V2通过集成MVDiT模块和时空条件编码来实现多视角视频生成和精确几何控制。此外，它引入了一种获取视频上下文描述的有效方法以支持多样化文本控制，并采用渐进式训练策略，使用混合视频数据来提高训练效率和泛化能力。", "result": "MagicDrive-V2实现了多视角驾驶视频合成，分辨率是现有SOTA的3.3倍，帧数是4倍，并具有丰富的上下文控制和几何控制。广泛的实验证明了MagicDrive-V2的能力。", "conclusion": "MagicDrive-V2的能力为自动驾驶领域解锁了更广泛的应用。", "translation": "扩散模型的快速发展极大地改善了视频合成，特别是在可控视频生成方面，这对于自动驾驶等应用至关重要。尽管带有3D VAE的DiT已成为视频生成的标准框架，但它在可控驾驶视频生成中引入了挑战，尤其是在几何控制方面，使得现有控制方法失效。为了解决这些问题，我们提出了MagicDrive-V2，一种新颖的方法，它集成了MVDiT模块和时空条件编码，以实现多视角视频生成和精确几何控制。此外，我们引入了一种获取视频上下文描述的有效方法，以支持多样化的文本控制，并采用渐进式训练策略，使用混合视频数据来提高训练效率和泛化能力。因此，MagicDrive-V2实现了多视角驾驶视频合成，分辨率是现有SOTA的3.3倍，帧数是4倍（与当前SOTA相比），并具有丰富的上下文控制和几何控制。广泛的实验证明了MagicDrive-V2的能力，为自动驾驶解锁了更广泛的应用。", "summary": "MagicDrive-V2是一种用于自动驾驶的高分辨率长视频生成方法。它解决了现有扩散模型在可控驾驶视频生成中遇到的几何控制难题，通过引入MVDiT模块、时空条件编码、高效上下文描述获取和渐进式训练策略，实现了多视角、高分辨率（3.3倍SOTA）、长帧数（4倍SOTA）且具有丰富几何与上下文控制的驾驶视频合成，极大地拓展了其在自动驾驶领域的应用潜力。", "keywords": "视频生成, 自动驾驶, 扩散模型, 几何控制, 高分辨率", "comments": "本文的创新点在于提出了MagicDrive-V2，有效解决了扩散模型在自动驾驶视频生成中面临的几何控制难题，并通过多项技术创新显著提升了生成视频的分辨率、长度和控制能力。这对于自动驾驶场景的模拟和测试具有重要意义，展现了其在该领域的巨大应用潜力。"}}
{"id": "2402.02672", "title": "Estimation of conditional average treatment effects on distributed confidential data", "authors": ["Yuji Kawamata", "Ryoki Motai", "Yukihiko Okada", "Akira Imakura", "Tetsuya Sakurai"], "categories": ["stat.ME", "cs.CR", "cs.LG"], "primary_category": "Subjects:       Methodology (stat.ME)", "pdf_link": null, "comments": "Comments:      45 pages, 12 figures", "url": "http://arxiv.org/abs/2402.02672v5", "summary": "The estimation of conditional average treatment effects (CATEs) is an\nimportant topic in many scientific fields. CATEs can be estimated with high\naccuracy if data distributed across multiple parties are centralized. However,\nit is difficult to aggregate such data owing to confidentiality or privacy\nconcerns. To address this issue, we propose data collaboration double machine\nlearning, a method for estimating CATE models using privacy-preserving fusion\ndata constructed from distributed sources, and evaluate its performance through\nsimulations. We make three main contributions. First, our method enables\nestimation and testing of semi-parametric CATE models without iterative\ncommunication on distributed data, providing robustness to model\nmis-specification compared to parametric approaches. Second, it enables\ncollaborative estimation across different time points and parties by\naccumulating a knowledge base. Third, our method performs as well as or better\nthan existing methods in simulations using synthetic, semi-synthetic, and\nreal-world datasets.", "comment": "45 pages, 12 figures", "pdf_url": "http://arxiv.org/pdf/2402.02672v5", "cate": "stat.ME", "date": "2024-02-05", "updated": "2025-07-25", "AI": {"title_translation": "分布式保密数据上的条件平均治疗效果估计", "tldr": "提出了一种数据协作双重机器学习方法，用于在不集中数据的情况下估计分布式保密数据上的条件平均治疗效果。", "motivation": "由于保密或隐私问题，难以聚合分布在多方的数据，但集中数据可以高精度估计条件平均治疗效果（CATEs）。", "method": "提出数据协作双重机器学习（data collaboration double machine learning），这是一种使用由分布式源构建的隐私保护融合数据来估计条件平均治疗效果（CATE）模型的方法。", "result": "该方法在模拟中表现与现有方法一样好或更好，并在合成、半合成和真实世界数据集上进行了评估。它支持跨不同时间点和参与方的协作估计，通过积累知识库实现。", "conclusion": "该方法能够在分布式数据上估计和测试半参数条件平均治疗效果（CATE）模型，无需迭代通信，对模型错误指定具有鲁棒性，并能实现跨时间点和参与方的协作估计，且性能优于现有方法。", "translation": "条件平均治疗效果（CATEs）的估计是许多科学领域的重要课题。如果分布在多方的数据能够集中，CATEs可以高精度估计。然而，由于保密或隐私问题，聚合此类数据很困难。为了解决这个问题，我们提出了数据协作双重机器学习，这是一种使用由分布式来源构建的隐私保护融合数据来估计CATE模型的方法，并通过模拟评估了其性能。我们做出了三个主要贡献。首先，我们的方法能够在分布式数据上估计和测试半参数CATE模型，无需迭代通信，与参数方法相比，对模型错误指定具有鲁棒性。其次，它通过积累知识库，能够实现跨不同时间点和参与方的协作估计。第三，我们的方法在使用合成、半合成和真实世界数据集的模拟中，表现与现有方法一样好或更好。", "summary": "本研究提出了一种名为数据协作双重机器学习的新方法，旨在解决在不集中敏感数据的情况下，估计分布式保密数据上的条件平均治疗效果（CATEs）的挑战。该方法通过构建隐私保护的融合数据实现CATE模型的估计和测试。其主要优势包括无需迭代通信即可处理半参数CATE模型、对模型误指定具有鲁棒性、支持跨时间点和参与方的协作估计，并通过模拟证明其性能优于现有方法。", "keywords": "条件平均治疗效果, 分布式数据, 隐私保护, 双重机器学习, 数据协作", "comments": "该论文的关键创新在于提出了数据协作双重机器学习，解决了在数据隐私和保密性受限的分布式环境中估计条件平均治疗效果（CATEs）的难题。其无需迭代通信的特性提高了效率，对模型误指定的鲁棒性增加了方法的实用性，而知识库积累则促进了跨时间点的协作。这对于需要处理敏感数据的领域（如医疗保健）具有重要意义。"}}
{"id": "2507.18863", "title": "Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language Model Reconstruction", "authors": ["Matthew Kit Khinn Teng", "Haibo Zhang", "Takeshi Saitoh"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      10 pages, 3 figures", "url": "http://arxiv.org/abs/2507.18863v1", "summary": "Visual Automatic Speech Recognition (V-ASR) is a challenging task that\ninvolves interpreting spoken language solely from visual information, such as\nlip movements and facial expressions. This task is notably challenging due to\nthe absence of auditory cues and the visual ambiguity of phonemes that exhibit\nsimilar visemes-distinct sounds that appear identical in lip motions. Existing\nmethods often aim to predict words or characters directly from visual cues, but\nthey commonly suffer from high error rates due to viseme ambiguity and require\nlarge amounts of pre-training data. We propose a novel phoneme-based two-stage\nframework that fuses visual and landmark motion features, followed by an LLM\nmodel for word reconstruction to address these challenges. Stage 1 consists of\nV-ASR, which outputs the predicted phonemes, thereby reducing training\ncomplexity. Meanwhile, the facial landmark features address speaker-specific\nfacial characteristics. Stage 2 comprises an encoder-decoder LLM model, NLLB,\nthat reconstructs the output phonemes back to words. Besides using a large\nvisual dataset for deep learning fine-tuning, our PV-ASR method demonstrates\nsuperior performance by achieving 17.4% WER on the LRS2 and 21.0% WER on the\nLRS3 dataset.", "comment": "10 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.18863v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于点视觉融合和语言模型重构的音素级视觉语音识别", "tldr": "本文提出了一种新颖的两阶段音素级视觉语音识别(V-ASR)框架，通过融合视觉和面部标志特征来预测音素，然后使用大型语言模型将音素重构为单词，显著提高了V-ASR的性能并降低了训练复杂性。", "motivation": "视觉自动语音识别(V-ASR)是一项具有挑战性的任务，因为它缺乏听觉线索以及音素在视觉上的模糊性（相似的唇部动作对应不同的发音）。现有方法直接从视觉线索预测单词或字符，但由于同形异音词的模糊性导致错误率高，并且需要大量的预训练数据。", "method": "本文提出了一种新颖的基于音素的两阶段框架。第一阶段是V-ASR，输出预测的音素，并利用面部标志特征处理说话者特定的面部特征，从而降低训练复杂性。第二阶段包含一个编码器-解码器大型语言模型NLLB，将输出的音素重构回单词。该方法融合了视觉和标志运动特征。", "result": "PV-ASR方法在LRS2数据集上实现了17.4%的词错误率(WER)，在LRS3数据集上实现了21.0%的词错误率(WER)，表现出卓越的性能。", "conclusion": "本文提出的PV-ASR方法通过两阶段的音素级识别和语言模型重构，有效解决了视觉语音识别中的挑战，显著提高了识别准确率。", "translation": "视觉自动语音识别（V-ASR）是一项具有挑战性的任务，它仅通过视觉信息（如唇部动作和面部表情）来解释口语。由于缺乏听觉线索以及音素在视觉上的模糊性（即视觉上相似但发音不同的音素），这项任务尤其具有挑战性。现有方法通常旨在直接从视觉线索预测单词或字符，但由于同形异音词的模糊性，它们通常会遭受高错误率，并且需要大量的预训练数据。我们提出了一种新颖的基于音素的两阶段框架，该框架融合了视觉和地标运动特征，然后使用LLM模型进行单词重构以应对这些挑战。第一阶段包括V-ASR，它输出预测的音素，从而降低了训练复杂性。同时，面部地标特征解决了说话者特定的面部特征。第二阶段包括一个编码器-解码器LLM模型NLLB，它将输出的音素重构回单词。除了使用大型视觉数据集进行深度学习微调外，我们的PV-ASR方法通过在LRS2数据集上实现17.4%的WER和在LRS3数据集上实现21.0%的WER，展示了卓越的性能。", "summary": "本文提出了一种名为PV-ASR的新型两阶段框架，用于解决视觉自动语音识别（V-ASR）中的挑战。该方法首先通过融合视觉和面部标志运动特征进行音素级V-ASR，以降低训练复杂性并处理说话者特异性。随后，一个基于NLLB的编码器-解码器大型语言模型将预测的音素重构为单词。该方法有效克服了视觉模糊性和对大量预训练数据的依赖，并在LRS2和LRS3数据集上分别取得了17.4%和21.0%的词错误率（WER），显著优于现有方法。", "keywords": "视觉语音识别, 音素, 大型语言模型, 点视觉融合, 词错误率", "comments": "该论文的创新点在于其独特的两阶段框架，特别是将视觉语音识别分解为音素预测和后续的语言模型重构。通过预测音素而非直接预测单词或字符，降低了训练复杂性并有效处理了视觉同形异音词的模糊性。同时，引入面部标志特征增强了对说话者特定特征的处理。结合大型语言模型进行单词重构是其成功的关键。"}}
{"id": "2507.19298", "title": "Controlling Topological Defects in Polar Fluids via Reinforcement Learning", "authors": ["Abhinav Singh", "Petros Koumoutsakos"], "categories": ["cond-mat.soft", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Soft Condensed Matter (cond-mat.soft)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19298v1", "summary": "Topological defects in active polar fluids exhibit complex dynamics driven by\ninternally generated stresses, reflecting the deep interplay between topology,\nflow, and non-equilibrium hydrodynamics. Feedback control offers a powerful\nmeans to guide such systems, enabling transitions between dynamic states. We\ninvestigated closed-loop steering of integer-charged defects in a confined\nactive fluid by modulating the spatial profile of activity. Using a continuum\nhydrodynamic model, we show that localized control of active stress induces\nflow fields that can reposition and direct defects along prescribed\ntrajectories by exploiting non-linear couplings in the system. A reinforcement\nlearning framework is used to discover effective control strategies that\nproduce robust defect transport across both trained and novel trajectories. The\nresults highlight how AI agents can learn the underlying dynamics and spatially\nstructure activity to manipulate topological excitations, offering insights\ninto the controllability of active matter and the design of adaptive,\nself-organized materials.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19298v1", "cate": "cond-mat.soft", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "通过强化学习控制极性流体中的拓扑缺陷", "tldr": "使用强化学习在极性流体中实现了对拓扑缺陷的精确控制，展示了AI在操纵复杂活性物质方面的潜力。", "motivation": "活性极性流体中的拓扑缺陷表现出复杂的动力学行为，而反馈控制是引导这类系统的有效手段。本研究旨在探索通过调制活性空间分布来引导和控制这些缺陷的方法。", "method": "本研究使用连续介质流体动力学模型，通过调制活性的空间分布，实现了对受限活性流体中整数电荷缺陷的闭环引导。在此基础上，利用强化学习框架来发现有效的控制策略，从而在训练和新颖轨迹上实现鲁棒的缺陷传输。", "result": "研究结果表明，局部控制活性应力能够诱导流场，通过利用系统中的非线性耦合来重新定位和引导缺陷沿着预设轨迹移动。AI智能体能够学习潜在的动力学并空间性地构建活性，从而有效地操纵拓扑激发，实现了缺陷的鲁棒传输。", "conclusion": "本研究揭示了活性物质的可控性，并通过AI智能体学习和操纵拓扑激发，为自适应、自组织材料的设计提供了重要见解。", "translation": "活性极性流体中的拓扑缺陷表现出由内部产生的应力驱动的复杂动力学，这反映了拓扑、流动和非平衡流体动力学之间的深刻相互作用。反馈控制为引导此类系统提供了一种强大的手段，能够实现动态状态之间的转换。我们通过调制活性的空间分布，研究了受限活性流体中整数电荷缺陷的闭环引导。使用连续介质流体动力学模型，我们表明活性应力的局部控制可以诱导流场，通过利用系统中的非线性耦合来重新定位和引导缺陷沿着预设轨迹移动。强化学习框架用于发现有效的控制策略，这些策略可以在训练和新颖轨迹上产生鲁棒的缺陷传输。结果突出显示了AI智能体如何学习潜在的动力学并空间性地构建活性以操纵拓扑激发，为活性物质的可控性和自适应、自组织材料的设计提供了见解。", "summary": "本文探讨了通过强化学习控制活性极性流体中拓扑缺陷的方法。研究人员利用连续介质流体动力学模型，通过调制活性的空间分布，实现了对受限流体中缺陷的闭环引导。他们发现，局部控制活性应力能够利用系统中的非线性耦合来重新定位和引导缺陷沿着预设轨迹。通过引入强化学习框架，该研究成功发现了实现缺陷在训练和新颖轨迹上鲁棒传输的有效控制策略。结果表明，AI智能体能够学习并利用系统动力学来操纵拓扑激发，这为活性物质的可控性和自适应、自组织材料的设计提供了新思路。", "keywords": "拓扑缺陷, 活性流体, 强化学习, 流体动力学, 控制策略", "comments": "本文的创新之处在于将强化学习应用于活性极性流体中拓扑缺陷的精确控制，这为复杂非平衡系统的操纵提供了新范式。通过结合连续介质模型和AI学习能力，展示了AI在理解和控制复杂物理系统方面的巨大潜力，对于设计新型智能材料具有重要意义。"}}
{"id": "2507.19417", "title": "Cycle-factors of regular graphs via entropy", "authors": ["Micha Christoph", "Nemanja Draganić", "António Girão", "Eoin Hurley", "Lukas Michel", "Alp Müyesser"], "categories": ["math.CO", "cs.DM", "cs.DS", "math.PR"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "Comments:      Conference version accepted to FOCS 2025. An expanded version with open problems will follow", "url": "http://arxiv.org/abs/2507.19417v1", "summary": "It is a classical result that a random permutation of $n$ elements has, on\naverage, about $\\log n$ cycles. We generalise this fact to all directed\n$d$-regular graphs on $n$ vertices by showing that, on average, a random\ncycle-factor of such a graph has $\\mathcal{O}((n\\log d)/d)$ cycles. This is\ntight up to the constant factor and improves the best previous bound of the\nform $\\mathcal{O}(n/\\sqrt{\\log d})$ due to Vishnoi. Our results also yield\nrandomised polynomial-time algorithms for finding such a cycle-factor and for\nfinding a tour of length $(1+\\mathcal{O}((\\log d)/d)) \\cdot n$ if the graph is\nconnected. This makes progress on a conjecture of Magnant and Martin and on a\nproblem studied by Vishnoi and by Feige, Ravi, and Singh. Our proof uses the\nlanguage of entropy to exploit the fact that the upper and lower bounds on the\nnumber of perfect matchings in regular bipartite graphs are extremely close.", "comment": "Conference version accepted to FOCS 2025. An expanded version with\n  open problems will follow", "pdf_url": "http://arxiv.org/pdf/2507.19417v1", "cate": "math.CO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "通过熵研究正则图的循环因子", "tldr": "本文通过熵理论证明了n个顶点的d-正则有向图的随机循环因子平均有O((n log d)/d)个循环，并提供了多项式时间算法来寻找循环因子和近似旅行路径。", "motivation": "经典结果表明n个元素的随机排列平均有大约log n个循环。本文旨在将这一事实推广到所有d-正则有向图。此外，它还致力于改进Vishnoi之前关于循环数量的界限，并解决Magnant和Martin的猜想以及Vishnoi、Feige、Ravi和Singh研究的问题。", "method": "本文使用熵的语言来利用正则二分图中完美匹配数量的上下界非常接近这一事实。这使得他们能够证明关于循环数量的平均界限，并在此基础上开发随机多项式时间算法。", "result": "研究表明，n个顶点的d-正则有向图的随机循环因子平均有O((n log d)/d)个循环。这一结果在常数因子内是紧密的，并改进了Vishnoi之前O(n/sqrt(log d))的最佳界限。此外，研究还得到了随机多项式时间算法，用于寻找这样的循环因子，以及在图连通时寻找长度为(1+O((log d)/d)) * n的旅行路径。", "conclusion": "本文成功地将随机排列的循环数量经典结果推广到d-正则有向图，显著改进了现有界限，并为相关猜想和问题提供了有效的多项式时间算法，展示了熵在图论问题中的强大应用。", "translation": "一个经典结果是，n个元素的随机排列平均有大约log n个循环。我们通过证明n个顶点的d-正则有向图的随机循环因子平均有O((n log d)/d)个循环，将这一事实推广到所有此类图。这个结果在常数因子内是紧密的，并改进了Vishnoi之前O(n/sqrt(log d))形式的最佳界限。我们的结果也产生了随机多项式时间算法，用于寻找这样的循环因子，如果图是连通的，则可以找到长度为(1+O((log d)/d)) * n的旅行路径。这使得Magnant和Martin的一个猜想以及Vishnoi和Feige、Ravi、Singh研究的一个问题取得了进展。我们的证明使用熵的语言来利用正则二分图中完美匹配数量的上下界极其接近这一事实。", "summary": "本文将随机排列中循环数量的经典结果推广到d-正则有向图，证明了其随机循环因子平均有O((n log d)/d)个循环，显著改进了现有界限。研究还提出了随机多项式时间算法，用于寻找循环因子和近似长度的旅行路径，解决了图论中的相关猜想和问题。核心方法是利用熵理论和正则二分图中完美匹配的性质。", "keywords": "循环因子, 正则图, 熵, 随机算法, 旅行路径", "comments": "这项工作通过引入熵理论来分析图的循环因子，提供了一种新颖且强大的方法。它不仅改进了现有理论界限，还产生了实用的多项式时间算法，这在理论和应用上都具有重要意义。对正则二分图中完美匹配性质的巧妙利用是其创新之处。"}}
{"id": "2503.05157", "title": "Ensemble Debiasing Across Class and Sample Levels for Fairer Prompting Accuracy", "authors": ["Ruixi Lin", "Ziqiao Wang", "Yang You"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Published as a conference paper at COLM 2025", "url": "http://arxiv.org/abs/2503.05157v4", "summary": "Language models are strong few-shot learners and achieve good overall\naccuracy in text classification tasks, masking the fact that their results\nsuffer from great class accuracy imbalance. We believe that the pursuit of\noverall accuracy should not come from enriching the strong classes, but from\nraising up the weak ones. To address the imbalance, we propose a Heaviside step\nfunction based ensemble debiasing method, which enables flexible rectifications\nof in-context learned class probabilities at both class and sample levels.\nEvaluations with Llama-2-13B on seven text classification benchmarks show that\nour approach achieves state-of-the-art overall accuracy gains with balanced\nclass accuracies. More importantly, we perform analyses on the resulted\nprobability correction scheme, showing that sample-level corrections are\nnecessary to elevate weak classes. Due to effectively correcting weak classes,\nour method also brings significant performance gains to a larger model variant,\nLlama-2-70B, especially on a biomedical domain task, further demonstrating the\nnecessity of ensemble debiasing at both levels. Our source code is available at\nhttps://github.com/NUS-HPC-AI-Lab/DCS.", "comment": "Published as a conference paper at COLM 2025", "pdf_url": "http://arxiv.org/pdf/2503.05157v4", "cate": "cs.CL", "date": "2025-03-07", "updated": "2025-07-25", "AI": {"title_translation": "跨类别和样本层面的集成去偏技术以实现更公平的提示准确性", "tldr": "语言模型在文本分类中存在类别准确性不平衡问题，本文提出一种基于Heaviside步函数的集成去偏方法，在类别和样本层面进行修正，实现了更好的整体准确性和类别平衡性。", "motivation": "语言模型在文本分类任务中存在严重的类别准确性不平衡问题，即整体准确性高但强类别被过度关注而弱类别被忽视。研究旨在解决这一问题，通过提升弱类别来达到更公平的准确性。", "method": "提出一种基于Heaviside步函数的集成去偏方法。该方法能够在类别和样本层面灵活地修正上下文学习到的类别概率。", "result": "在七个文本分类基准上使用Llama-2-13B进行评估，结果显示该方法在实现类别准确性平衡的同时，获得了最先进的整体准确性提升。分析表明样本层面的修正对于提升弱类别是必要的。该方法对Llama-2-70B等更大模型变体，尤其是在生物医学领域任务上，也带来了显著的性能提升。", "conclusion": "跨类别和样本层面的集成去偏对于有效纠正弱类别至关重要，并能显著提升大型语言模型在文本分类任务中的公平性和整体性能。", "translation": "语言模型是强大的少样本学习器，并在文本分类任务中取得了良好的整体准确性，但这掩盖了其结果存在严重的类别准确性不平衡问题。我们认为，追求整体准确性不应以强化强类别为代价，而应通过提升弱类别来实现。为了解决这种不平衡，我们提出了一种基于Heaviside步函数的集成去偏方法，该方法能够在类别和样本层面灵活地修正上下文学习到的类别概率。使用Llama-2-13B在七个文本分类基准上进行的评估表明，我们的方法在平衡类别准确性的同时，取得了最先进的整体准确性提升。更重要的是，我们对所得到的概率校正方案进行了分析，结果表明样本层面的校正对于提升弱类别是必要的。由于有效纠正了弱类别，我们的方法也为更大的模型变体Llama-2-70B带来了显著的性能提升，特别是在生物医学领域任务上，这进一步证明了在两个层面进行集成去偏的必要性。我们的源代码可在https://github.com/NUS-HPC-AI-Lab/DCS获取。", "summary": "本文针对语言模型在文本分类中存在的类别准确性不平衡问题，提出了一种新颖的基于Heaviside步函数的集成去偏方法。该方法能够在类别和样本两个层面灵活调整上下文学习到的类别概率，旨在通过提升弱类别而非仅强化强类别来改善整体准确性。实验结果表明，该方法不仅在Llama-2-13B上取得了最先进的整体准确性提升和类别平衡，而且对Llama-2-70B等更大模型也有效，尤其在生物医学领域任务中表现突出，强调了样本级修正对提升弱类别的必要性。", "keywords": "语言模型, 文本分类, 类别不平衡, 去偏, 集成方法", "comments": "本文创新性地提出了在类别和样本层面进行集成去偏的方法，通过Heaviside步函数实现对上下文学习到的概率的灵活修正。其重要性在于关注了语言模型在文本分类中的公平性问题，避免了仅追求整体准确性而忽视弱类别的弊端。实验结果证明了该方法的有效性和对大型模型的泛化能力，特别强调了样本级修正的关键作用。"}}
{"id": "2504.10519", "title": "Toward Super Agent System with Hybrid AI Routers", "authors": ["Yuhang Yao", "Haixin Wang", "Yibo Chen", "Jiawen Wang", "Min Chang Jordan Ren", "Bosheng Ding", "Salman Avestimehr", "Chaoyang He"], "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.10519v2", "summary": "AI Agents powered by Large Language Models are transforming the world through\nenormous applications. A super agent has the potential to fulfill diverse user\nneeds, such as summarization, coding, and research, by accurately understanding\nuser intent and leveraging the appropriate tools to solve tasks. However, to\nmake such an agent viable for real-world deployment and accessible at scale,\nsignificant optimizations are required to ensure high efficiency and low cost.\nThis position paper presents a design of the Super Agent System powered by the\nhybrid AI routers. Upon receiving a user prompt, the system first detects the\nintent of the user, then routes the request to specialized task agents with the\nnecessary tools or automatically generates agentic workflows. In practice, most\napplications directly serve as AI assistants on edge devices such as phones and\nrobots. As different language models vary in capability and cloud-based models\noften entail high computational costs, latency, and privacy concerns, we then\nexplore the hybrid mode where the router dynamically selects between local and\ncloud models based on task complexity. Finally, we introduce the blueprint of\nan on-device super agent enhanced with cloud. With advances in multi-modality\nmodels and edge hardware, we envision that most computations can be handled\nlocally, with cloud collaboration only as needed. Such architecture paves the\nway for super agents to be seamlessly integrated into everyday life in the near\nfuture.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.10519v2", "cate": "cs.AI", "date": "2025-04-11", "updated": "2025-07-24", "AI": {"title_translation": "迈向混合AI路由器的超级智能体系统", "tldr": "本文提出了一种基于混合AI路由器的超级智能体系统设计，旨在通过动态选择本地或云端模型来优化效率和成本，以实现大规模部署。", "motivation": "现有的大语言模型驱动的AI智能体虽然潜力巨大，但在实际部署和大规模应用中面临效率和成本的挑战，尤其是在边缘设备上。需要优化以实现高性能和低成本。", "method": "本文提出了一种超级智能体系统设计，核心是混合AI路由器。该系统首先检测用户意图，然后将请求路由到专业的任务智能体或自动生成智能体工作流。路由器会根据任务复杂性动态选择本地模型或云端模型，以平衡能力、成本、延迟和隐私。文章还介绍了云端增强的设备端超级智能体蓝图。", "result": "Not mentioned in abstract", "conclusion": "该架构为超级智能体在不久的将来无缝融入日常生活铺平了道路，并展望随着多模态模型和边缘硬件的进步，大部分计算可在本地处理，仅在需要时与云端协作。", "translation": "大语言模型驱动的AI智能体正通过海量应用改变世界。一个超级智能体有潜力通过准确理解用户意图并利用适当的工具解决任务，来满足多样化的用户需求，例如摘要、编码和研究。然而，为了使这种智能体在现实世界中可行并大规模可访问，需要进行显著优化以确保高效率和低成本。这篇立场论文提出了一种由混合AI路由器驱动的超级智能体系统设计。系统在接收到用户提示后，首先检测用户意图，然后将请求路由到具有必要工具的专业任务智能体或自动生成智能体工作流。在实践中，大多数应用程序直接作为手机和机器人等边缘设备上的AI助手。由于不同语言模型的能力各异，且基于云的模型通常会带来高计算成本、延迟和隐私问题，因此我们探索了混合模式，其中路由器根据任务复杂性动态选择本地和云模型。最后，我们介绍了云增强的设备端超级智能体蓝图。随着多模态模型和边缘硬件的进步，我们设想大多数计算可以在本地处理，仅在需要时进行云协作。这种架构为超级智能体在不久的将来无缝融入日常生活铺平了道路。", "summary": "本文提出了一种面向大规模部署的超级智能体系统设计，其核心是混合AI路由器。该系统能够识别用户意图，并将请求智能地路由到合适的任务智能体或生成工作流。为解决效率和成本问题，特别是在边缘设备上，该系统引入了混合模式，路由器根据任务复杂性动态选择本地或云端模型，旨在平衡性能与资源消耗。文章还描绘了云增强的设备端超级智能体的未来愿景。", "keywords": "超级智能体, 混合AI路由器, 边缘计算, 大语言模型, 任务路由", "comments": "该论文提出了一种创新的超级智能体架构，通过引入“混合AI路由器”解决了大模型在边缘设备上部署的效率和成本挑战。其核心思想是根据任务动态选择本地或云端计算资源，这对于实现智能体的大规模普及至关重要。该设计考虑了实际应用中的隐私、延迟和成本问题，具有重要的实践意义和前瞻性。"}}
{"id": "2507.19403", "title": "SDVDiag: A Modular Platform for the Diagnosis of Connected Vehicle Functions", "authors": ["Matthias Weiß", "Falk Dettinger", "Michael Weyrich"], "categories": ["cs.SE", "cs.AI", "cs.DC", "B.8.2; C.2.4"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      7 pages, 5 figures", "url": "http://arxiv.org/abs/2507.19403v1", "summary": "Connected and software-defined vehicles promise to offer a broad range of\nservices and advanced functions to customers, aiming to increase passenger\ncomfort and support autonomous driving capabilities. Due to the high\nreliability and availability requirements of connected vehicles, it is crucial\nto resolve any occurring failures quickly. To achieve this however, a complex\ncloud/edge architecture with a mesh of dependencies must be navigated to\ndiagnose the responsible root cause. As such, manual analyses become unfeasible\nsince they would significantly delay the troubleshooting.\n  To address this challenge, this paper presents SDVDiag, an extensible\nplatform for the automated diagnosis of connected vehicle functions. The\nplatform enables the creation of pipelines that cover all steps from initial\ndata collection to the tracing of potential root causes. In addition, SDVDiag\nsupports self-adaptive behavior by the ability to exchange modules at runtime.\nDependencies between functions are detected and continuously updated, resulting\nin a dynamic graph view of the system. In addition, vital system metrics are\nmonitored for anomalies. Whenever an incident is investigated, a snapshot of\nthe graph is taken and augmented by relevant anomalies. Finally, the analysis\nis performed by traversing the graph and creating a ranking of the most likely\ncauses.\n  To evaluate the platform, it is deployed inside an 5G test fleet environment\nfor connected vehicle functions. The results show that injected faults can be\ndetected reliably. As such, the platform offers the potential to gain new\ninsights and reduce downtime by identifying problems and their causes at an\nearly stage.", "comment": "7 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.19403v1", "cate": "cs.SE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "SDVDiag: 互联车辆功能诊断的模块化平台", "tldr": "SDVDiag是一个自动化平台，用于诊断互联车辆功能故障，通过数据收集、依赖追踪和运行时模块交换实现故障检测和原因排名。", "motivation": "互联和软件定义车辆对可靠性和可用性要求高，但复杂的云/边缘架构和依赖关系使得手动诊断故障根源不可行且耗时。", "method": "本文提出了SDVDiag平台，它是一个可扩展的自动化诊断平台。它能创建涵盖数据收集到根源追踪的诊断管道，支持运行时模块交换的自适应行为。它检测并持续更新功能间的依赖关系形成动态图，监控系统指标异常。调查事件时，获取图快照并结合异常，通过遍历图对最可能原因进行排名分析。", "result": "平台部署在5G测试车队环境中进行评估，结果显示可以可靠地检测注入的故障。", "conclusion": "平台有潜力通过早期识别问题及其原因来获得新见解并减少停机时间。", "translation": "互联和软件定义车辆有望为客户提供广泛的服务和高级功能，旨在提高乘客舒适度并支持自动驾驶能力。由于互联车辆对高可靠性和可用性的要求，快速解决任何发生的故障至关重要。然而，要实现这一点，必须在具有网状依赖关系的复杂云/边缘架构中进行导航以诊断负责的根本原因。因此，手动分析变得不可行，因为它们会显著延迟故障排除。\n为了应对这一挑战，本文提出了SDVDiag，一个用于互联车辆功能自动化诊断的可扩展平台。该平台能够创建涵盖从初始数据收集到潜在根本原因追踪所有步骤的管道。此外，SDVDiag通过在运行时交换模块的能力支持自适应行为。功能之间的依赖关系被检测并持续更新，从而形成系统的动态图视图。此外，还会监控重要的系统指标是否存在异常。每当调查事件时，都会获取图的快照并补充相关异常。最后，通过遍历图并创建最可能原因的排名来执行分析。\n为了评估该平台，它被部署在互联车辆功能的5G测试车队环境中。结果表明，注入的故障可以可靠地检测到。因此，该平台通过在早期阶段识别问题及其原因，提供了获得新见解和减少停机时间的潜力。", "summary": "SDVDiag是一个模块化且可扩展的平台，专为自动化诊断互联车辆功能故障而设计。它通过构建从数据收集到根源追踪的诊断流程，并利用动态依赖图和异常监控来识别故障的根本原因。该平台支持运行时模块交换以实现自适应性。在5G测试车队中的评估表明，SDVDiag能够可靠地检测注入的故障，从而有望减少互联车辆的停机时间并提供新的诊断见解。", "keywords": "互联车辆, 诊断, 自动化, 软件定义车辆, 故障排除", "comments": "该论文提出SDVDiag平台，其创新性在于为互联车辆的复杂故障诊断提供了自动化和模块化的解决方案，特别是在处理动态依赖关系和支持运行时自适应方面。这对于提高互联车辆的可靠性和可用性具有重要意义，因为它能显著减少手动诊断的时间和成本。其在5G测试环境中的验证也增加了其实际应用潜力。"}}
{"id": "2507.19365", "title": "A Data-Driven Approach to Estimate LEO Orbit Capacity Models", "authors": ["Braden Stock", "Maddox McVarthy", "Simone Servadio"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      18 pages, 15 figures", "url": "http://arxiv.org/abs/2507.19365v1", "summary": "Utilizing the Sparse Identification of Nonlinear Dynamics algorithm (SINDy)\nand Long Short-Term Memory Recurrent Neural Networks (LSTM), the population of\nresident space objects, divided into Active, Derelict, and Debris, in LEO can\nbe accurately modeled to predict future satellite and debris propagation. This\nproposed approach makes use of a data set coming from a computational expensive\nhigh-fidelity model, the MOCAT-MC, to provide a light, low-fidelity counterpart\nthat provides accurate forecasting in a shorter time frame.", "comment": "18 pages, 15 figures", "pdf_url": "http://arxiv.org/pdf/2507.19365v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "一种数据驱动的LEO轨道容量模型估计方法", "tldr": "本文利用SINDy和LSTM算法，基于高精度MOCAT-MC模型的数据，建立了一个轻量级的LEO空间物体（包括卫星和碎片）预测模型，以实现快速准确的未来传播预测。", "motivation": "现有高精度LEO空间物体传播模型计算成本高昂，需要一种更轻量、更快速但仍能准确预测的方法。", "method": "结合稀疏非线性动力学识别算法（SINDy）和长短期记忆循环神经网络（LSTM），利用高精度MOCAT-MC模型的数据集，对LEO中常驻空间物体（活跃、废弃、碎片）的种群进行建模。", "result": "能够准确建模LEO空间常驻物体种群，预测未来卫星和碎片传播，并提供一种轻量、低保真度但能在更短时间内提供准确预测的工具。", "conclusion": "Not mentioned in abstract", "translation": "利用稀疏非线性动力学识别算法（SINDy）和长短期记忆循环神经网络（LSTM），可以准确建模LEO中常驻空间物体（分为活跃、废弃和碎片）的种群，以预测未来的卫星和碎片传播。该方法利用来自计算成本高昂的高保真模型MOCAT-MC的数据集，提供了一个轻量、低保真的对应模型，能够在更短的时间内提供准确的预测。", "summary": "本文提出一种数据驱动方法，利用SINDy和LSTM算法，对LEO中的常驻空间物体（活跃、废弃和碎片）进行准确建模，以预测其未来传播。该方法利用计算成本高昂的高保真模型MOCAT-MC的数据集，旨在提供一个轻量、低保真度但能快速准确预测的工具。", "keywords": "LEO轨道, 空间碎片, SINDy, LSTM, 数据驱动", "comments": "该研究的创新点在于结合SINDy和LSTM算法对LEO空间物体进行建模，并利用高保真模型数据构建轻量级预测工具，有效解决了计算效率问题。"}}
{"id": "2507.18901", "title": "REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?", "authors": ["Chuxuan Hu", "Liyun Zhang", "Yeji Lim", "Aum Wadhwani", "Austin Peters", "Daniel Kang"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to ACL 2025 Findings", "url": "http://arxiv.org/abs/2507.18901v1", "summary": "Assessing the reproducibility of social science papers is essential for\npromoting rigor in research processes, but manual assessment is costly. With\nrecent advances in agentic AI systems (i.e., AI agents), we seek to evaluate\ntheir capability to automate this process. However, existing benchmarks for\nreproducing research papers (1) focus solely on reproducing results using\nprovided code and data without assessing their consistency with the paper, (2)\noversimplify real-world scenarios, and (3) lack necessary diversity in data\nformats and programming languages. To address these issues, we introduce\nREPRO-Bench, a collection of 112 task instances, each representing a social\nscience paper with a publicly available reproduction report. The agents are\ntasked with assessing the reproducibility of the paper based on the original\npaper PDF and the corresponding reproduction package. REPRO-Bench features\nend-to-end evaluation tasks on the reproducibility of social science papers\nwith complexity comparable to real-world assessments. We evaluate three\nrepresentative AI agents on REPRO-Bench, with the best-performing agent\nachieving an accuracy of only 21.4%. Building on our empirical analysis, we\ndevelop REPRO-Agent, which improves the highest accuracy achieved by existing\nagents by 71%. We conclude that more advanced AI agents should be developed to\nautomate real-world reproducibility assessment. REPRO-Bench is publicly\navailable at https://github.com/uiuc-kang-lab/REPRO-Bench.", "comment": "Accepted to ACL 2025 Findings", "pdf_url": "http://arxiv.org/pdf/2507.18901v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "REPRO-Bench：智能AI系统能否评估社会科学研究的可重复性？", "tldr": "本文介绍了REPRO-Bench，一个用于评估AI系统在社会科学研究可重复性评估方面能力的基准，并发现现有AI系统表现不佳，但通过开发的REPRO-Agent可显著提高准确率。", "motivation": "评估社会科学论文的可重复性对于提高研究严谨性至关重要，但手动评估成本高昂。鉴于智能AI系统（即AI代理）的最新进展，研究旨在评估其自动化这一过程的能力。现有基准存在缺陷，包括仅关注使用提供代码和数据再现结果而不评估其与论文的一致性，过度简化现实场景，以及缺乏数据格式和编程语言的多样性。", "method": "为解决现有问题，研究引入了REPRO-Bench，这是一个包含112个任务实例的集合，每个实例代表一篇附带公开再现报告的社会科学论文。AI代理的任务是根据原始论文PDF和相应的再现包评估论文的可重复性。REPRO-Bench包含与现实评估复杂性相当的端到端评估任务。研究在REPRO-Bench上评估了三种代表性AI代理，并基于实证分析开发了REPRO-Agent。", "result": "在REPRO-Bench上评估的三种代表性AI代理中，表现最好的代理准确率仅为21.4%。通过开发的REPRO-Agent，将现有代理的最高准确率提高了71%。", "conclusion": "需要开发更先进的AI代理来自动化现实世界中的可重复性评估。", "translation": "评估社会科学论文的可重复性对于促进研究过程的严谨性至关重要，但手动评估成本高昂。随着智能AI系统（即AI代理）的最新进展，我们旨在评估它们自动化这一过程的能力。然而，现有的研究论文再现基准存在以下问题：（1）仅关注使用提供的代码和数据再现结果，而没有评估其与论文的一致性；（2）过度简化了现实世界的场景；（3）缺乏数据格式和编程语言的必要多样性。为了解决这些问题，我们引入了REPRO-Bench，这是一个包含112个任务实例的集合，每个实例代表一篇附带公开再现报告的社会科学论文。代理的任务是根据原始论文PDF和相应的再现包评估论文的可重复性。REPRO-Bench具有与现实世界评估复杂性相当的社会科学论文可重复性端到端评估任务。我们在REPRO-Bench上评估了三种代表性AI代理，其中表现最好的代理准确率仅为21.4%。基于我们的实证分析，我们开发了REPRO-Agent，将现有代理的最高准确率提高了71%。我们得出结论，应开发更先进的AI代理来自动化现实世界中的可重复性评估。REPRO-Bench已在https://github.com/uiuc-kang-lab/REPRO-Bench公开。", "summary": "该研究旨在评估智能AI系统在自动化社会科学研究可重复性评估方面的能力。针对现有基准的不足，论文引入了REPRO-Bench，一个包含112个真实社会科学论文可重复性评估任务的基准。初步评估显示，现有AI代理表现不佳，最佳准确率仅为21.4%。为改善此状况，研究开发了REPRO-Agent，显著提升了评估准确率71%。研究强调未来需开发更先进的AI代理以实现高效的自动化可重复性评估。", "keywords": "AI代理, 可重复性评估, 社会科学, REPRO-Bench, REPRO-Agent", "comments": "本文创新性地提出了REPRO-Bench，一个专门用于评估AI代理在社会科学研究可重复性评估方面能力的基准，解决了现有基准在真实性、复杂性和多样性上的不足。研究不仅揭示了当前AI代理在此任务上的局限性（低准确率），还通过开发REPRO-Agent展示了显著的改进潜力。这对于推动AI在科学研究严谨性保障领域的应用具有重要意义，同时也指出了未来AI技术在处理复杂、多模态信息方面的发展方向。"}}
{"id": "2502.08306", "title": "A posteriori error control for a finite volume scheme for a cross-diffusion model of ion transport", "authors": ["Arne Berrens", "Jan Giesselmann"], "categories": ["math.NA", "cs.NA", "65M15, 35K65, 35K51, 65M08, 35K40"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      27 pages, 2 tables", "url": "http://arxiv.org/abs/2502.08306v2", "summary": "We derive a reliable a posteriori error estimate for a cell-centered finite\nvolume scheme approximating a cross-diffusion system modeling ion transport\nthrough nanopores. To this end, we derive a stability framework that is\nindependent of the numerical scheme and introduce a suitable (conforming)\nreconstruction of the numerical solution. The stability framework relies on\nsome simplifying assumptions that coincide with those made in weak uniqueness\nresults for this system. Additionally, when electrical forces are present, we\nassume that the solvent concentration is uniformly bounded from below. This is\nthe first a posteriori error estimate for a cross-diffusion system. Along the\nway, we derive a pointwise a posteriori error estimate for a finite volume\nscheme that approximates the diffusion equation. We conduct numerical\nexperiments showing that the error estimator scales with the same order as the\ntrue error.", "comment": "27 pages, 2 tables", "pdf_url": "http://arxiv.org/pdf/2502.08306v2", "cate": "math.NA", "date": "2025-02-12", "updated": "2025-07-25", "AI": {"title_translation": "离子输运交叉扩散模型的有限体积格式的后验误差控制", "tldr": "本文为模拟离子输运的交叉扩散模型中的有限体积格式，推导了首个可靠的后验误差估计方法，并通过数值实验验证了其有效性。", "motivation": "为模拟离子通过纳米孔传输的交叉扩散系统，需要为单元中心有限体积格式推导可靠的后验误差估计，这是交叉扩散系统的第一个后验误差估计。", "method": "研究者推导了一个独立于数值格式的稳定性框架，并引入了合适的（协调的）数值解重构。该稳定性框架依赖于与该系统弱唯一性结果中相同的简化假设。当存在电场力时，假设溶剂浓度有统一的下界。此外，还为近似扩散方程的有限体积格式推导了一个点式后验误差估计。", "result": "成功推导了交叉扩散系统的第一个后验误差估计。数值实验表明，误差估计器与真实误差的量级一致。", "conclusion": "本研究成功地为模拟离子传输的交叉扩散模型中的有限体积格式开发了可靠的后验误差估计方法，并验证了其有效性。", "translation": "我们为近似模拟离子通过纳米孔传输的交叉扩散系统的单元中心有限体积格式推导了一个可靠的后验误差估计。为此，我们推导了一个独立于数值格式的稳定性框架，并引入了合适的（协调的）数值解重构。该稳定性框架依赖于与该系统弱唯一性结果中相同的简化假设。此外，当存在电场力时，我们假设溶剂浓度有统一的下界。这是交叉扩散系统的第一个后验误差估计。在此过程中，我们为近似扩散方程的有限体积格式推导了一个点式后验误差估计。我们进行了数值实验，结果表明误差估计器与真实误差的量级一致。", "summary": "本研究为模拟离子通过纳米孔传输的交叉扩散系统中的单元中心有限体积格式，提出了首个可靠的后验误差估计方法。该方法基于独立于数值格式的稳定性框架和数值解重构，并在存在电场力时考虑了溶剂浓度下界。数值实验验证了所提出误差估计器的有效性，其与真实误差具有相同的量级。", "keywords": "后验误差估计, 交叉扩散, 有限体积法, 离子输运, 稳定性框架", "comments": "该论文的创新之处在于首次为交叉扩散系统推导了后验误差估计，这在数值分析领域具有重要意义。所提出的稳定性框架独立于数值格式，增加了方法的普适性。"}}
{"id": "2507.19089", "title": "Fine-Grained Traffic Inference from Road to Lane via Spatio-Temporal Graph Node Generation", "authors": ["Shuhao Li", "Weidong Yang", "Yue Cui", "Xiaoxing Liu", "Lingkai Meng", "Lipeng Ma", "Fan Zhang"], "categories": ["cs.AI", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19089v1", "summary": "Fine-grained traffic management and prediction are fundamental to key\napplications such as autonomous driving, lane change guidance, and traffic\nsignal control. However, obtaining lane-level traffic data has become a\ncritical bottleneck for data-driven models due to limitations in the types and\nnumber of sensors and issues with the accuracy of tracking algorithms. To\naddress this, we propose the Fine-grained Road Traffic Inference (FRTI) task,\nwhich aims to generate more detailed lane-level traffic information using\nlimited road data, providing a more energy-efficient and cost-effective\nsolution for precise traffic management. This task is abstracted as the first\nscene of the spatio-temporal graph node generation problem. We designed a\ntwo-stage framework--RoadDiff--to solve the FRTI task. solve the FRTI task.\nThis framework leverages the Road-Lane Correlation Autoencoder-Decoder and the\nLane Diffusion Module to fully utilize the limited spatio-temporal dependencies\nand distribution relationships of road data to accurately infer fine-grained\nlane traffic states. Based on existing research, we designed several baseline\nmodels with the potential to solve the FRTI task and conducted extensive\nexperiments on six datasets representing different road conditions to validate\nthe effectiveness of the RoadDiff model in addressing the FRTI task. The\nrelevant datasets and code are available at\nhttps://github.com/ShuhaoLii/RoadDiff.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19089v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "细粒度交通推断：从道路到车道通过时空图节点生成", "tldr": "提出FRTI任务和RoadDiff模型，利用有限道路数据推断细粒度车道级交通信息，解决车道级数据获取瓶颈。", "motivation": "细粒度交通管理和预测对自动驾驶、车道变换引导和交通信号控制等关键应用至关重要，但获取车道级交通数据因传感器类型和数量的限制以及跟踪算法的准确性问题成为数据驱动模型的关键瓶颈。", "method": "提出一个两阶段框架RoadDiff来解决FRTI任务。该框架利用道路-车道相关性自编码器-解码器和车道扩散模块，充分利用道路数据有限的时空依赖性和分布关系，以准确推断细粒度车道交通状态。", "result": "在代表不同道路条件的六个数据集上进行了广泛实验，验证了RoadDiff模型在解决FRTI任务方面的有效性。", "conclusion": "Not mentioned in abstract", "translation": "细粒度交通管理和预测对于自动驾驶、车道变换引导和交通信号控制等关键应用至关重要。然而，由于传感器类型和数量的限制以及跟踪算法的准确性问题，获取车道级交通数据已成为数据驱动模型的关键瓶颈。为了解决这个问题，我们提出了细粒度道路交通推断（FRTI）任务，旨在利用有限的道路数据生成更详细的车道级交通信息，为精确的交通管理提供更节能、更具成本效益的解决方案。这项任务被抽象为时空图节点生成问题的第一个场景。我们设计了一个两阶段框架——RoadDiff——来解决FRTI任务。该框架利用道路-车道相关性自编码器-解码器和车道扩散模块，充分利用道路数据有限的时空依赖性和分布关系，以准确推断细粒度车道交通状态。基于现有研究，我们设计了几种有潜力解决FRTI任务的基线模型，并在代表不同道路条件的六个数据集上进行了广泛实验，以验证RoadDiff模型在解决FRTI任务方面的有效性。相关数据集和代码可在https://github.com/ShuhaoLii/RoadDiff获取。", "summary": "本文提出了细粒度道路交通推断（FRTI）任务，旨在利用有限的道路数据生成详细的车道级交通信息，以解决现有车道级数据获取的瓶颈。该任务被建模为时空图节点生成问题，并提出了两阶段框架RoadDiff。RoadDiff结合了道路-车道相关性自编码器-解码器和车道扩散模块，有效推断细粒度车道交通状态。在多个数据集上的实验验证了RoadDiff模型的有效性。", "keywords": "细粒度交通推断, 车道级数据, 时空图, 扩散模型, RoadDiff", "comments": "本文的创新点在于提出了一个新颖的细粒度道路交通推断（FRTI）任务，并将其抽象为时空图节点生成问题，为解决车道级交通数据稀缺性提供了一种有前景的方法。所提出的RoadDiff框架通过结合自编码器和扩散模型，有效地利用了有限的道路数据进行细粒度推断，具有重要的实际应用潜力，尤其是在智能交通系统和自动驾驶领域。"}}
{"id": "2507.18889", "title": "RailX: A Flexible, Scalable, and Low-Cost Network Architecture for Hyper-Scale LLM Training Systems", "authors": ["Yinxiao Feng", "Tiancheng Chen", "Yuchen Wei", "Siyuan Shen", "Shiju Wang", "Wei Li", "Kaisheng Ma", "Torsten Hoefler"], "categories": ["cs.AR", "cs.DC", "cs.NI"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      25 pages, 21 figures, 6 tables", "url": "http://arxiv.org/abs/2507.18889v1", "summary": "Increasingly large AI workloads are calling for hyper-scale infrastructure;\nhowever, traditional interconnection network architecture is neither scalable\nnor cost-effective enough. Tree-based topologies such as the\n\\textit{Rail-optimized} network are extremely expensive, while direct\ntopologies such as \\textit{Torus} have insufficient bisection bandwidth and\nflexibility. In this paper, we propose \\textit{RailX}, a reconfigurable network\narchitecture based on intra-node direct connectivity and inter-node circuit\nswitching. Nodes and optical switches are physically 2D-organized, achieving\nbetter scalability than existing centralized circuit switching networks. We\npropose a novel interconnection method based on \\textit{Hamiltonian\nDecomposition} theory to organize separate rail-based rings into\n\\textit{all-to-all} topology, simultaneously optimizing ring-collective and\nall-to-all communication. More than $100$K chips with hyper bandwidth can be\ninterconnected with a flat switching layer, and the diameter is only $2\\sim4$\ninter-node hops. The network cost per injection/All-Reduce bandwidth of\n\\textit{RailX} is less than $10\\%$ of the Fat-Tree, and the cost per\nbisection/All-to-All bandwidth is less than $50\\%$ of the Fat-Tree.\nSpecifically, only $\\sim$\\$$1.3$B is required to interconnect 200K chips with\n1.8TB bandwidth. \\textit{RailX} can also be used in the ML-as-a-service (MLaaS)\nscenario, where single or multiple training workloads with various shapes,\nscales, and parallelism strategies can be flexibly mapped, and failures can be\nworked around.", "comment": "25 pages, 21 figures, 6 tables", "pdf_url": "http://arxiv.org/pdf/2507.18889v1", "cate": "cs.AR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "RailX：一种用于超大规模LLM训练系统的灵活、可扩展、低成本网络架构", "tldr": "RailX是一种新型网络架构，专为超大规模LLM训练系统设计，通过创新的互连方法实现高带宽、低成本和高可扩展性，解决了传统网络架构在扩展性和成本方面的不足。", "motivation": "当前大型AI工作负载需要超大规模基础设施，但传统的互连网络架构（如树形拓扑和直接拓扑）在可扩展性、成本效益、双向带宽和灵活性方面存在不足，无法满足需求。", "method": "本文提出了RailX，一种基于节点内直接连接和节点间电路交换的可重构网络架构。节点和光开关物理上呈2D组织，通过基于哈密顿分解理论的新型互连方法，将独立的基于轨道的环组织成全连接拓扑，同时优化环内集合通信和全连接通信。", "result": "RailX可互连超过10万个芯片，具有超高带宽，直径仅为2~4个节点间跳数。RailX的每注入/All-Reduce带宽成本低于Fat-Tree的10%，每双向/All-to-All带宽成本低于Fat-Tree的50%。互连20万个芯片（1.8TB带宽）仅需约13亿美元。RailX还能灵活地映射各种形状、规模和并行策略的训练工作负载，并能应对故障。", "conclusion": "RailX是一种灵活、可扩展且低成本的网络架构，能够有效支持超大规模LLM训练系统，并在MLaaS场景中展现出优异的适应性和故障处理能力。", "translation": "日益增长的AI工作负载需要超大规模基础设施；然而，传统的互连网络架构既不具备足够的扩展性，也不具备成本效益。基于树的拓扑结构，如“轨道优化”网络，极其昂贵，而直接拓扑结构，如“环形拓扑”，则双向带宽和灵活性不足。在本文中，我们提出了RailX，一种基于节点内直接连接和节点间电路交换的可重构网络架构。节点和光开关在物理上呈2D组织，比现有集中式电路交换网络实现了更好的可扩展性。我们提出了一种基于哈密顿分解理论的新型互连方法，将独立的基于轨道的环组织成“全连接”拓扑，同时优化了环集合通信和全连接通信。超过10万个芯片可以通过一个扁平的交换层互连，具有超高带宽，且直径仅为2~4个节点间跳数。RailX的每注入/All-Reduce带宽成本低于Fat-Tree的10%，每双向/All-to-All带宽成本低于Fat-Tree的50%。具体来说，互连20万个芯片（1.8TB带宽）仅需约13亿美元。RailX还可用于ML即服务（MLaaS）场景，其中可以灵活地映射各种形状、规模和并行策略的单个或多个训练工作负载，并且可以规避故障。", "summary": "RailX提出了一种用于超大规模LLM训练系统的可重构网络架构，旨在解决传统网络架构在可扩展性和成本方面的不足。该架构结合了节点内直接连接和节点间电路交换，并采用2D物理组织和基于哈密顿分解的新型互连方法，实现了超高带宽、低成本和优异的可扩展性。实验结果表明，RailX在成本效益和性能上远超Fat-Tree等传统方案，并能灵活支持多种AI训练工作负载和故障应对。", "keywords": "RailX, LLM训练, 网络架构, 可扩展性, 低成本", "comments": "RailX的创新之处在于其结合了节点内直接连接与节点间电路交换，并引入了基于哈密顿分解的2D物理组织和互连方法，从而在保证高带宽的同时显著降低了成本并提升了可扩展性。其在超大规模LLM训练系统中的应用潜力巨大，尤其是在成本效益方面表现突出，这对于推动大规模AI计算的普及具有重要意义。此外，其在MLaaS场景中的灵活性和故障应对能力也增加了其实用价值。"}}
{"id": "2507.19071", "title": "Cross-Subject Mind Decoding from Inaccurate Representations", "authors": ["Yangyang Xu", "Bangzhen Liu", "Wenqi Shao", "Yong Du", "Shengfeng He", "Tingting Zhu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19071v1", "summary": "Decoding stimulus images from fMRI signals has advanced with pre-trained\ngenerative models. However, existing methods struggle with cross-subject\nmappings due to cognitive variability and subject-specific differences. This\nchallenge arises from sequential errors, where unidirectional mappings generate\npartially inaccurate representations that, when fed into diffusion models,\naccumulate errors and degrade reconstruction fidelity. To address this, we\npropose the Bidirectional Autoencoder Intertwining framework for accurate\ndecoded representation prediction. Our approach unifies multiple subjects\nthrough a Subject Bias Modulation Module while leveraging bidirectional mapping\nto better capture data distributions for precise representation prediction. To\nfurther enhance fidelity when decoding representations into stimulus images, we\nintroduce a Semantic Refinement Module to improve semantic representations and\na Visual Coherence Module to mitigate the effects of inaccurate visual\nrepresentations. Integrated with ControlNet and Stable Diffusion, our method\noutperforms state-of-the-art approaches on benchmark datasets in both\nqualitative and quantitative evaluations. Moreover, our framework exhibits\nstrong adaptability to new subjects with minimal training samples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19071v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "跨主体不精确表征下的思维解码", "tldr": "本文提出双向自编码器交织（BAI）框架，解决了从fMRI信号进行跨主体思维解码时因不精确表征导致的误差积累问题。BAI通过双向映射、主体偏差调制、语义细化和视觉一致性模块提升解码精度和泛化能力，并在基准数据集上超越现有SOTA方法，对新主体适应性强。", "motivation": "现有基于预训练生成模型的fMRI信号刺激图像解码方法在跨主体映射方面存在困难，主要原因是认知变异性和个体差异导致单向映射产生部分不精确的表征，这些不精确表征在输入到扩散模型后会积累误差，从而降低重建图像的保真度。", "method": "提出双向自编码器交织（Bidirectional Autoencoder Intertwining, BAI）框架。该框架通过主体偏差调制模块统一多个主体，并利用双向映射来更准确地捕捉数据分布以进行精确的表征预测。为进一步提高将表征解码为刺激图像时的保真度，引入语义细化模块以改善语义表征，以及视觉一致性模块以减轻不精确视觉表征的影响。该方法与ControlNet和Stable Diffusion集成。", "result": "在基准数据集上，该方法在定性和定量评估中均优于最先进的方法。此外，该框架对新主体表现出强大的适应性，只需极少的训练样本。", "conclusion": "本文通过提出的双向自编码器交织（BAI）框架，有效解决了跨主体思维解码中由不精确表征导致的误差积累问题，显著提高了图像重建的保真度，并展示了对新主体的良好泛化能力。", "translation": "从fMRI信号解码刺激图像的技术已随着预训练生成模型的发展而进步。然而，由于认知变异性和个体特异性差异，现有方法在跨主体映射方面面临挑战。这种挑战源于序列错误，即单向映射会生成部分不精确的表征，当这些表征输入到扩散模型中时，会积累误差并降低重建保真度。为解决此问题，我们提出了双向自编码器交织（Bidirectional Autoencoder Intertwining）框架，用于准确的解码表征预测。我们的方法通过主体偏差调制模块统一多个主体，同时利用双向映射更好地捕捉数据分布，以实现精确的表征预测。为了在将表征解码为刺激图像时进一步提高保真度，我们引入了语义细化模块来改善语义表征，以及视觉一致性模块来减轻不精确视觉表征的影响。我们的方法与ControlNet和Stable Diffusion集成，在基准数据集上，无论是在定性还是定量评估中，均优于最先进的方法。此外，我们的框架对新主体表现出强大的适应性，只需极少的训练样本。", "summary": "本文提出一种名为双向自编码器交织（BAI）的新框架，旨在解决从fMRI信号进行跨主体思维解码时，因不精确表征导致误差积累的问题。BAI框架通过主体偏差调制模块统一不同主体，并利用双向映射实现精确的表征预测。此外，引入语义细化模块和视觉一致性模块以提高解码图像的保真度。实验结果表明，该方法在基准数据集上性能优越，并对新主体具有良好的泛化能力。", "keywords": "跨主体解码, fMRI, 思维解码, 双向自编码器, 表征学习", "comments": "本文的创新点在于提出了BAI框架来解决跨主体fMRI思维解码中的表征不精确问题，特别是通过引入双向映射、主体偏差调制、语义细化和视觉一致性模块来系统地提升解码准确性和泛化能力。其结合ControlNet和Stable Diffusion也体现了对现有先进生成模型的有效利用。这项工作对于脑机接口和神经科学领域具有重要意义，有望推动更精确和鲁棒的脑活动解码技术发展。"}}
{"id": "2504.16299", "title": "Towards Quantum Universal Hypothesis Testing", "authors": ["Arick Grootveld", "Haodong Yang", "Biao Chen", "Venkata Gandikota", "Jason Pollack"], "categories": ["cs.IT", "math.IT", "quant-ph"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Accepted at ITW 2025", "url": "http://arxiv.org/abs/2504.16299v2", "summary": "Hoeffding's formulation and solution to the universal hypothesis testing\n(UHT) problem had a profound impact on many subsequent works dealing with\nasymmetric hypotheses. In this work, we introduce a quantum universal\nhypothesis testing framework that serves as a quantum analog to Hoeffding's\nUHT. Motivated by Hoeffding's approach, which estimates the empirical\ndistribution and uses it to construct the test statistic, we employ quantum\nstate tomography to reconstruct the unknown state prior to forming the test\nstatistic. Leveraging the concentration properties of quantum state tomography,\nwe establish the exponential consistency of the proposed test: the type II\nerror probability decays exponentially quickly, with the exponent determined by\nthe trace distance between the true state and the nominal state.", "comment": "Accepted at ITW 2025", "pdf_url": "http://arxiv.org/pdf/2504.16299v2", "cate": "cs.IT", "date": "2025-04-22", "updated": "2025-07-24", "AI": {"title_translation": "量子通用假设检验的探索", "tldr": "本文提出了一个量子通用假设检验框架，作为Hoeffding通用假设检验的量子模拟，通过量子态层析成像实现了检验的指数一致性。", "motivation": "经典的Hoeffding通用假设检验对非对称假设产生了深远影响。受其启发，本文旨在构建一个量子版本的通用假设检验框架。", "method": "借鉴Hoeffding通过估计经验分布来构建检验统计量的方法，本文采用量子态层析成像技术来重建未知量子态，然后构建检验统计量。", "result": "提出的量子通用假设检验方法具有指数一致性：第二类错误概率呈指数级快速衰减，其指数由真实态和标称态之间的迹距离决定。", "conclusion": "本文成功建立了一个量子通用假设检验框架，通过量子态层析成像实现了其指数一致性，为量子假设检验提供了新的理论基础。", "translation": "Hoeffding对通用假设检验（UHT）问题的表述和解决方案对许多后续处理非对称假设的工作产生了深远影响。在这项工作中，我们引入了一个量子通用假设检验框架，作为Hoeffding UHT的量子模拟。受Hoeffding通过估计经验分布并利用其构建检验统计量的方法的启发，我们采用量子态层析成像来重构未知态，然后形成检验统计量。利用量子态层析成像的集中特性，我们建立了所提出检验的指数一致性：第二类错误概率呈指数级快速衰减，其指数由真实态和标称态之间的迹距离决定。", "summary": "本文提出了一个量子通用假设检验（UHT）框架，作为经典Hoeffding UHT的量子对应物。该方法借鉴Hoeffding的思路，通过量子态层析成像重建未知量子态来构建检验统计量。研究结果表明，所提出的检验具有指数一致性，即第二类错误概率呈指数级衰减，衰减速度由真实态与标称态之间的迹距离决定。", "keywords": "量子通用假设检验, 量子态层析成像, 指数一致性, 假设检验, 迹距离", "comments": "这项工作将经典的通用假设检验推广到量子领域，具有重要的理论意义。通过引入量子态层析成像，解决了量子态未知的问题，并证明了指数一致性，为量子信息处理中的假设检验提供了坚实的基础。"}}
{"id": "2507.18947", "title": "GEAR: Gaze-Enabled Human-Robot Collaborative Assembly", "authors": ["Asad Ali Shahid", "Angelo Moroncelli", "Drazen Brscic", "Takayuki Kanda", "Loris Roveda"], "categories": ["cs.RO", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted for publication at 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)", "url": "http://arxiv.org/abs/2507.18947v1", "summary": "Recent progress in robot autonomy and safety has significantly improved\nhuman-robot interactions, enabling robots to work alongside humans on various\ntasks. However, complex assembly tasks still present significant challenges due\nto inherent task variability and the need for precise operations. This work\nexplores deploying robots in an assistive role for such tasks, where the robot\nassists by fetching parts while the skilled worker provides high-level guidance\nand performs the assembly. We introduce GEAR, a gaze-enabled system designed to\nenhance human-robot collaboration by allowing robots to respond to the user's\ngaze. We evaluate GEAR against a touch-based interface where users interact\nwith the robot through a touchscreen. The experimental study involved 30\nparticipants working on two distinct assembly scenarios of varying complexity.\nResults demonstrated that GEAR enabled participants to accomplish the assembly\nwith reduced physical demand and effort compared to the touchscreen interface,\nespecially for complex tasks, maintaining great performance, and receiving\nobjects effectively. Participants also reported enhanced user experience while\nperforming assembly tasks. Project page: sites.google.com/view/gear-hri", "comment": "Accepted for publication at 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025)", "pdf_url": "http://arxiv.org/pdf/2507.18947v1", "cate": "cs.RO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "GEAR：注视辅助人机协作装配", "tldr": "GEAR是一个注视辅助系统，通过让机器人响应用户的注视来增强人机协作装配任务，实验表明其能减少体力需求并提升用户体验。", "motivation": "由于固有的任务可变性和对精确操作的需求，复杂的装配任务对机器人来说仍然是巨大的挑战。因此，本研究旨在探索机器人在此类任务中作为辅助角色部署的可能性。", "method": "研究引入了GEAR，一个注视辅助系统，旨在通过允许机器人响应用户的注视来增强人机协作。通过实验研究，将GEAR与基于触摸屏的界面进行了比较，实验涉及30名参与者在两种不同复杂度的装配场景中进行操作。", "result": "实验结果表明，与触摸屏界面相比，GEAR使参与者能够以更少的体力需求和精力完成装配，尤其是在复杂任务中，同时保持了良好的性能并有效接收物体。参与者还报告在执行装配任务时用户体验得到提升。", "conclusion": "GEAR系统能够有效提升人机协作装配任务的效率和用户体验，尤其是在复杂任务中，通过注视交互减少了体力需求。", "translation": "机器人自主性和安全性方面的最新进展显著改善了人机交互，使机器人能够与人类一起执行各种任务。然而，由于固有的任务可变性和对精确操作的需求，复杂的装配任务仍然面临重大挑战。这项工作探索了在此类任务中部署机器人作为辅助角色，即机器人通过取物提供协助，而熟练工人则提供高级指导并执行装配。我们引入了GEAR，一个注视辅助系统，旨在通过允许机器人响应用户的注视来增强人机协作。我们将GEAR与基于触摸的界面进行了评估，在该界面中用户通过触摸屏与机器人交互。实验研究涉及30名参与者在两种不同复杂度的装配场景中工作。结果表明，与触摸屏界面相比，GEAR使参与者能够以更少的体力需求和精力完成装配，尤其是在复杂任务中，同时保持了良好的性能并有效接收物体。参与者还报告在执行装配任务时用户体验得到增强。项目页面：sites.google.com/view/gear-hri", "summary": "本研究提出了GEAR，一个注视辅助系统，旨在提升复杂装配任务中的人机协作。该系统允许机器人根据用户的注视提供零件辅助，从而减轻熟练工人的体力负担。通过与基于触摸屏的界面进行对比实验，GEAR在减少体力需求、保持高性能和提升用户体验方面表现出显著优势，尤其适用于高复杂度任务。", "keywords": "人机协作, 注视交互, 机器人辅助, 装配任务, 用户体验", "comments": "GEAR的创新之处在于其引入了注视作为人机交互的新范式，有效解决了复杂装配任务中机器人辅助的挑战。该系统通过直观的注视交互，降低了用户的认知和体力负担，显著提升了人机协作的效率和用户满意度。其重要性在于为未来人机协作系统的设计提供了新的思路，特别是在需要高精度和灵活响应的工业或服务场景中具有广阔的应用前景。"}}
{"id": "2408.00998", "title": "FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features for Highly Controllable Text-Driven Image Translation", "authors": ["Xiang Gao", "Jiaying Liu"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted conference paper of ACM MM 2024", "url": "http://arxiv.org/abs/2408.00998v5", "summary": "Large-scale text-to-image diffusion models have been a revolutionary\nmilestone in the evolution of generative AI and multimodal technology, allowing\nwonderful image generation with natural-language text prompt. However, the\nissue of lacking controllability of such models restricts their practical\napplicability for real-life content creation. Thus, attention has been focused\non leveraging a reference image to control text-to-image synthesis, which is\nalso regarded as manipulating (or editing) a reference image as per a text\nprompt, namely, text-driven image-to-image translation. This paper contributes\na novel, concise, and efficient approach that adapts pre-trained large-scale\ntext-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a\nplug-and-play manner, realizing high-quality and versatile text-driven I2I\ntranslation without any model training, model fine-tuning, or online\noptimization process. To guide T2I generation with a reference image, we\npropose to decompose diverse guiding factors with different frequency bands of\ndiffusion features in the DCT spectral space, and accordingly devise a novel\nfrequency band substitution layer which realizes dynamic control of the\nreference image to the T2I generation result in a plug-and-play manner. We\ndemonstrate that our method allows flexible control over both guiding factor\nand guiding intensity of the reference image simply by tuning the type and\nbandwidth of the substituted frequency band, respectively. Extensive\nqualitative and quantitative experiments verify superiority of our approach\nover related methods in I2I translation visual quality, versatility, and\ncontrollability. The code is publicly available at:\nhttps://github.com/XiangGao1102/FBSDiff.", "comment": "Accepted conference paper of ACM MM 2024", "pdf_url": "http://arxiv.org/pdf/2408.00998v5", "cate": "cs.CV", "date": "2024-08-02", "updated": "2025-07-25", "AI": {"title_translation": "FBSDiff：即插即用扩散特征频带替换实现高度可控的文本驱动图像转换", "tldr": "FBSDiff提出了一种即插即用的方法，通过替换扩散特征的频带，将预训练的文本到图像模型转换为高度可控的文本驱动图像到图像转换，无需训练或微调。", "motivation": "大型文本到图像扩散模型在图像生成方面取得革命性进展，但缺乏可控性限制了其实际应用。因此，需要一种方法来利用参考图像控制文本到图像合成，即文本驱动的图像到图像转换。", "method": "本文提出了一种新颖、简洁、高效的FBSDiff方法，以即插即用的方式将预训练的大型文本到图像（T2I）扩散模型适应到图像到图像（I2I）范式，无需任何模型训练、模型微调或在线优化。该方法通过在DCT频谱空间中分解扩散特征的不同频带的指导因素，并设计了一个新颖的频带替换层，实现参考图像对T2I生成结果的动态控制。通过调整替换频带的类型和带宽，可以灵活控制参考图像的指导因素和指导强度。", "result": "该方法允许灵活控制参考图像的指导因素和指导强度。广泛的定性和定量实验验证了该方法在I2I转换视觉质量、多功能性和可控性方面优于相关方法。", "conclusion": "该论文贡献了一种新颖、简洁、高效的即插即用方法FBSDiff，实现了高质量和多功能的文本驱动图像到图像转换，无需模型训练或微调，并显著提高了可控性。", "translation": "大型文本到图像扩散模型是生成式AI和多模态技术演进中的一个革命性里程碑，能够通过自然语言文本提示生成精彩的图像。然而，此类模型缺乏可控性问题限制了它们在现实内容创作中的实际适用性。因此，人们的注意力集中在利用参考图像来控制文本到图像合成，这也被视为根据文本提示操纵（或编辑）参考图像，即文本驱动的图像到图像转换。本文贡献了一种新颖、简洁、高效的方法，以即插即用的方式将预训练的大型文本到图像（T2I）扩散模型适应到图像到图像（I2I）范式，实现了高质量和多功能的文本驱动I2I转换，无需任何模型训练、模型微调或在线优化过程。为了用参考图像指导T2I生成，我们提出在DCT频谱空间中分解扩散特征的不同频带的各种指导因素，并相应地设计了一种新颖的频带替换层，以即插即用的方式实现参考图像对T2I生成结果的动态控制。我们证明了我们的方法可以通过简单地调整替换频带的类型和带宽，分别灵活控制参考图像的指导因素和指导强度。广泛的定性和定量实验验证了我们的方法在I2I转换视觉质量、多功能性和可控性方面优于相关方法。代码已公开：https://github.com/XiangGao1102/FBSDiff。", "summary": "本文提出了FBSDiff，一种新颖、高效的即插即用方法，旨在解决大型文本到图像扩散模型在图像到图像（I2I）转换中缺乏可控性的问题。通过在DCT频谱空间中分解扩散特征的不同频带，并引入一个频带替换层，FBSDiff能够将预训练的文本到图像模型转换为高度可控的文本驱动I2I转换，而无需任何训练或微调。该方法允许用户通过调整频带类型和带宽来灵活控制参考图像的指导因素和强度。实验结果表明，FBSDiff在I2I转换的视觉质量、多功能性和可控性方面优于现有方法。", "keywords": "文本到图像转换, 图像到图像转换, 扩散模型, 可控性, 频带替换", "comments": "该论文的创新点在于提出了一个无需训练或微调的即插即用框架，将T2I模型应用于I2I任务，并显著提高了可控性。通过在频域（DCT空间）操作扩散特征，并设计独特的频带替换层，实现了对参考图像指导因素和强度的精细控制，这对于实际应用中的内容创作非常重要。其无需额外训练的特性大大降低了应用门槛。"}}
{"id": "2507.18937", "title": "CNN-based Surface Temperature Forecasts with Ensemble Numerical Weather Prediction over Medium-range Forecast Periods", "authors": ["Takuya Inoue", "Takuya Kawabata"], "categories": ["physics.ao-ph", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Atmospheric and Oceanic Physics (physics.ao-ph)", "pdf_link": null, "comments": "Comments:      32 pages, 10 figures", "url": "http://arxiv.org/abs/2507.18937v1", "summary": "This study proposes a method that integrates convolutional neural networks\n(CNNs) with ensemble numerical weather prediction (NWP) models, enabling\nsurface temperature forecasting at lead times beyond the short-range (five-day)\nforecast period. Owing to limited computational resources, operational\nmedium-range temperature forecasts typically rely on low-resolution NWP models,\nwhich are prone to systematic and random errors. To resolve these limitations,\nthe proposed method first reduces systematic errors through CNN-based\npost-processing (bias correction and spatial super-resolution) on each ensemble\nmember, reconstructing high-resolution temperature fields from low-resolution\nmodel outputs. Second, it reduces random errors through ensemble averaging of\nthe CNN-corrected members. This study also investigates whether the sequence of\nCNN correction and ensemble averaging affects the forecast accuracy. For\ncomparison with the proposed method, we additionally conducted experiments with\nthe CNN trained on ensemble-averaged forecasts. The first approach--CNN\ncorrection before ensemble averaging--consistently achieved higher accuracy\nthan the reverse approach. Although based on low-resolution ensemble forecasts,\nthe proposed method notably outperformed the high-resolution deterministic NWP\nmodels. These findings indicate that combining CNN-based correction with\nensemble averaging effectively reduces both the systematic and random errors in\nNWP model outputs. The proposed approach is a practical and scalable solution\nfor improving medium-range temperature forecasts, and is particularly valuable\nat operational centers with limited computational resources.", "comment": "32 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.18937v1", "cate": "physics.ao-ph", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于CNN的集合数值天气预报在中期预报时段的地表温度预测", "tldr": "该研究提出了一种将CNN与集合数值天气预报模型结合的方法，用于中期地表温度预测，有效减少了系统误差和随机误差，并优于高分辨率确定性模型。", "motivation": "由于计算资源有限，现有的中期温度预报通常依赖低分辨率的数值天气预报（NWP）模型，这些模型容易产生系统性误差和随机误差。", "method": "本研究提出了一种结合卷积神经网络（CNN）和集合数值天气预报（NWP）模型的方法。首先，通过基于CNN的后处理（偏差校正和空间超分辨率）来减少每个集合成员的系统误差，从低分辨率模型输出重建高分辨率温度场。其次，通过对CNN校正后的成员进行集合平均来减少随机误差。研究还探讨了CNN校正和集合平均的顺序对预报准确性的影响，并与CNN在集合平均预报上训练的方法进行了比较。", "result": "结果显示，先进行CNN校正再进行集合平均的方法比相反的方法取得了更高的准确性。尽管基于低分辨率集合预报，但所提出的方法显著优于高分辨率确定性数值天气预报模型。", "conclusion": "本研究的发现表明，将基于CNN的校正与集合平均相结合，能有效减少数值天气预报模型输出中的系统误差和随机误差。所提出的方法是改进中期温度预报的一种实用且可扩展的解决方案，对于计算资源有限的业务中心尤其有价值。", "translation": "本研究提出了一种将卷积神经网络（CNN）与集合数值天气预报（NWP）模型相结合的方法，使得地表温度预报的提前期能够超出短期（五天）预报周期。由于计算资源有限，业务中期温度预报通常依赖低分辨率的NWP模型，这些模型容易产生系统性误差和随机误差。为了解决这些限制，所提出的方法首先通过对每个集合成员进行基于CNN的后处理（偏差校正和空间超分辨率）来减少系统误差，从低分辨率模型输出重建高分辨率温度场。其次，通过对CNN校正后的成员进行集合平均来减少随机误差。本研究还调查了CNN校正和集合平均的顺序是否会影响预报准确性。为了与所提出的方法进行比较，我们还额外进行了在集合平均预报上训练CNN的实验。第一种方法——在集合平均之前进行CNN校正——始终比相反的方法获得更高的准确性。尽管基于低分辨率集合预报，但所提出的方法显著优于高分辨率确定性NWP模型。这些发现表明，将基于CNN的校正与集合平均相结合，能有效减少NWP模型输出中的系统误差和随机误差。所提出的方法是改进中期温度预报的一种实用且可扩展的解决方案，对于计算资源有限的业务中心尤其有价值。", "summary": "本研究提出了一种结合CNN和集合数值天气预报模型的方法，旨在提高中期地表温度预报的准确性。该方法通过CNN对每个集合成员进行偏差校正和空间超分辨率处理，以减少系统误差并重建高分辨率温度场；随后通过集合平均减少随机误差。实验表明，先进行CNN校正再进行集合平均的顺序能带来更高的准确性，并且该方法即使基于低分辨率集合预报，其性能也显著优于高分辨率确定性数值天气预报模型。这表明该结合方法能有效减少NWP模型输出的误差，为计算资源受限的业务中心提供了一种实用且可扩展的中期温度预报改进方案。", "keywords": "CNN, 地表温度预报, 集合数值天气预报, 中期预报, 误差校正", "comments": "该论文提出了一种创新性的方法，将深度学习（CNN）与传统的集合数值天气预报相结合，有效解决了中期温度预报中因低分辨率模型导致的系统误差和随机误差问题。其亮点在于明确了CNN校正和集合平均的最佳顺序，并证明了即使使用低分辨率数据也能超越高分辨率确定性模型，这对于计算资源有限的业务中心具有重要的实际应用价值和可扩展性。"}}
{"id": "2507.19402", "title": "FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for Financial Fraud Detection A Technical Report", "authors": ["Matteo Cardaioli", "Luca Marangoni", "Giada Martini", "Francesco Mazzolin", "Luca Pajola", "Andrea Ferretto Parodi", "Alessandra Saitta", "Maria Chiara Vernillo"], "categories": ["cs.LG", "cs.CE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This is a technical report", "url": "http://arxiv.org/abs/2507.19402v1", "summary": "The increasing complexity and volume of financial transactions pose\nsignificant challenges to traditional fraud detection systems. This technical\nreport investigates and compares the efficacy of classical, quantum, and\nquantum-hybrid machine learning models for the binary classification of\nfraudulent financial activities.\n  As of our methodology, first, we develop a comprehensive behavioural feature\nengineering framework to transform raw transactional data into a rich,\ndescriptive feature set. Second, we implement and evaluate a range of models on\nthe IBM Anti-Money Laundering (AML) dataset. The classical baseline models\ninclude Logistic Regression, Decision Tree, Random Forest, and XGBoost. These\nare compared against three hybrid classic quantum algorithms architectures: a\nQuantum Support Vector Machine (QSVM), a Variational Quantum Classifier (VQC),\nand a Hybrid Quantum Neural Network (HQNN).\n  Furthermore, we propose Fraud Detection for Quantum Computing (FD4QC), a\npractical, API-driven system architecture designed for real-world deployment,\nfeaturing a classical-first, quantum-enhanced philosophy with robust fallback\nmechanisms.\n  Our results demonstrate that classical tree-based models, particularly\n\\textit{Random Forest}, significantly outperform the quantum counterparts in\nthe current setup, achieving high accuracy (\\(97.34\\%\\)) and F-measure\n(\\(86.95\\%\\)). Among the quantum models, \\textbf{QSVM} shows the most promise,\ndelivering high precision (\\(77.15\\%\\)) and a low false-positive rate\n(\\(1.36\\%\\)), albeit with lower recall and significant computational overhead.\n  This report provides a benchmark for a real-world financial application,\nhighlights the current limitations of quantum machine learning in this domain,\nand outlines promising directions for future research.", "comment": "This is a technical report", "pdf_url": "http://arxiv.org/pdf/2507.19402v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "FD4QC：经典与量子混合机器学习在金融欺诈检测中的应用——一份技术报告", "tldr": "本报告探讨并比较了经典、量子和量子混合机器学习模型在金融欺诈检测中的效用，发现经典模型目前表现更优，并提出了一个实用的量子增强系统架构。", "motivation": "现有欺诈检测系统面临金融交易日益复杂和数量庞大带来的挑战。", "method": "首先，开发了一个全面的行为特征工程框架，将原始交易数据转化为丰富、描述性的特征集。其次，在IBM反洗钱（AML）数据集上，实现并评估了一系列模型，包括逻辑回归、决策树、随机森林、XGBoost等经典基线模型，以及量子支持向量机（QSVM）、变分量子分类器（VQC）和混合量子神经网络（HQNN）等三种混合经典量子算法架构。此外，提出了一个实用的、API驱动的FD4QC系统架构，采用“经典优先，量子增强”的理念，并具有强大的回退机制。", "result": "在当前设置下，经典树模型（特别是随机森林）显著优于量子对应模型，达到了97.34%的准确率和86.95%的F值。在量子模型中，QSVM表现出最大的潜力，提供了高精度（77.15%）和低误报率（1.36%），尽管召回率较低且计算开销大。", "conclusion": "本报告为实际金融应用提供了基准，突出了量子机器学习在该领域的当前局限性，并指出了未来研究的有前景方向。", "translation": "金融交易日益复杂和庞大的数量对传统欺诈检测系统构成了重大挑战。本技术报告调查并比较了经典、量子和量子混合机器学习模型在金融欺诈活动二元分类中的效用。在我们的方法论中，首先，我们开发了一个全面的行为特征工程框架，将原始交易数据转换为丰富、描述性的特征集。其次，我们在IBM反洗钱（AML）数据集上实现并评估了一系列模型。经典基线模型包括逻辑回归、决策树、随机森林和XGBoost。这些模型与三种混合经典量子算法架构进行了比较：量子支持向量机（QSVM）、变分量子分类器（VQC）和混合量子神经网络（HQNN）。此外，我们提出了用于量子计算的欺诈检测（FD4QC），这是一个实用的、API驱动的系统架构，专为实际部署而设计，具有经典优先、量子增强的理念和强大的回退机制。我们的结果表明，经典树模型，特别是随机森林，在当前设置下显著优于量子对应模型，实现了高准确率（97.34%）和F值（86.95%）。在量子模型中，QSVM显示出最大的潜力，提供了高精度（77.15%）和低误报率（1.36%），尽管召回率较低且计算开销大。本报告为实际金融应用提供了基准，突出了量子机器学习在该领域的当前局限性，并概述了未来研究的有前景方向。", "summary": "这份技术报告FD4QC研究了经典、量子和量子混合机器学习模型在金融欺诈检测中的应用。通过开发行为特征工程框架并在IBM AML数据集上评估多种模型，报告发现经典树模型（尤其是随机森林）在当前设置下表现最佳，而量子支持向量机（QSVM）在量子模型中显示出潜力。报告还提出了一个实用的FD4QC系统架构，并指出了量子机器学习在该领域的局限性和未来研究方向。", "keywords": "金融欺诈检测, 机器学习, 量子计算, 混合模型, 经典算法", "comments": "这篇报告对比了传统和量子机器学习在金融欺诈检测中的应用，其创新点在于提出了一种结合经典与量子方法的系统架构（FD4QC），并首次提供了在该领域量子机器学习的基准测试。报告清晰地指出了当前量子计算在实际应用中的局限性（如计算开销大、性能不如经典模型），这对于指导未来量子算法和硬件的发展具有重要意义。然而，报告也强调了量子模型在某些指标上的潜力，预示了其长期发展前景。"}}
{"id": "2507.19093", "title": "Graph Neural Network-Based Predictor for Optimal Quantum Hardware Selection", "authors": ["Antonio Tudisco", "Deborah Volpe", "Giacomo Orlandi", "Giovanna Turvani"], "categories": ["quant-ph", "cs.LG"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19093v1", "summary": "The growing variety of quantum hardware technologies, each with unique\npeculiarities such as connectivity and native gate sets, creates challenges\nwhen selecting the best platform for executing a specific quantum circuit. This\nselection process usually involves a brute-force approach: compiling the\ncircuit on various devices and evaluating performance based on factors such as\ncircuit depth and gate fidelity. However, this method is computationally\nexpensive and does not scale well as the number of available quantum processors\nincreases. In this work, we propose a Graph Neural Network (GNN)-based\npredictor that automates hardware selection by analyzing the Directed Acyclic\nGraph (DAG) representation of a quantum circuit. Our study evaluates 498\nquantum circuits (up to 27 qubits) from the MQT Bench dataset, compiled using\nQiskit on four devices: three superconducting quantum processors (IBM-Kyiv,\nIBM-Brisbane, IBM-Sherbrooke) and one trapped-ion processor (IONQ-Forte).\nPerformance is estimated using a metric that integrates circuit depth and gate\nfidelity, resulting in a dataset where 93 circuits are optimally compiled on\nthe trapped-ion device, while the remaining circuits prefer superconducting\nplatforms. By exploiting graph-based machine learning, our approach avoids\nextracting the circuit features for the model evaluation but directly embeds it\nas a graph, significantly accelerating the optimal target decision-making\nprocess and maintaining all the information. Experimental results prove 94.4%\naccuracy and an 85.5% F1 score for the minority class, effectively predicting\nthe best compilation target. The developed code is publicly available on GitHub\n(https://github.com/antotu/GNN-Model-Quantum-Predictor).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19093v1", "cate": "quant-ph", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于图神经网络的最佳量子硬件选择预测器", "tldr": "本文提出了一种基于图神经网络（GNN）的预测器，通过分析量子电路的DAG表示，自动化选择最佳量子硬件平台，解决了传统暴力方法计算成本高且扩展性差的问题。", "motivation": "量子硬件技术的种类日益增多，每种硬件都有其独特的特性，这使得为特定量子电路选择最佳执行平台成为一项挑战。传统的选择方法（暴力编译和评估）计算成本高昂且随着量子处理器数量的增加而难以扩展。", "method": "本文提出了一种基于图神经网络（GNN）的预测器，通过分析量子电路的有向无环图（DAG）表示来自动化硬件选择。该研究评估了MQT Bench数据集中498个量子电路（最多27个量子比特），使用Qiskit在四种设备上进行编译：三种超导量子处理器（IBM-Kyiv, IBM-Brisbane, IBM-Sherbrooke）和一种离子阱处理器（IONQ-Forte）。性能通过集成电路深度和门保真度的指标进行评估。该方法利用基于图的机器学习，直接将电路嵌入为图，避免了特征提取。", "result": "实验结果表明，该方法在预测最佳编译目标方面达到了94.4%的准确率和少数类85.5%的F1分数。在评估的数据集中，93个电路在离子阱设备上实现了最佳编译，而其余电路更倾向于超导平台。", "conclusion": "本文提出的基于图神经网络的预测器能够有效地自动化量子硬件选择过程，显著加速最佳目标决策，并保持所有信息，解决了传统方法的计算成本和扩展性问题。", "translation": "不断增长的量子硬件技术种类，每种都具有独特的特性，如连接性和原生门集，这在为执行特定量子电路选择最佳平台时带来了挑战。这种选择过程通常涉及暴力破解方法：在各种设备上编译电路并根据电路深度和门保真度等因素评估性能。然而，这种方法计算成本高昂，并且随着可用量子处理器数量的增加而扩展性不佳。在这项工作中，我们提出了一种基于图神经网络（GNN）的预测器，通过分析量子电路的有向无环图（DAG）表示来自动化硬件选择。我们的研究评估了来自MQT Bench数据集的498个量子电路（最多27个量子比特），使用Qiskit在四种设备上进行编译：三种超导量子处理器（IBM-Kyiv、IBM-Brisbane、IBM-Sherbrooke）和一种离子阱处理器（IONQ-Forte）。性能通过集成电路深度和门保真度的指标进行估计，结果数据集显示93个电路在离子阱设备上实现了最佳编译，而其余电路更倾向于超导平台。通过利用基于图的机器学习，我们的方法避免了为模型评估提取电路特征，而是直接将其嵌入为图，显著加速了最佳目标决策过程并保留了所有信息。实验结果证明，对于少数类，准确率达到94.4%，F1分数为85.5%，有效地预测了最佳编译目标。开发的代码已在GitHub上公开（https://github.com/antotu/GNN-Model-Quantum-Predictor）。", "summary": "本文提出了一种基于图神经网络（GNN）的预测器，旨在自动化量子硬件选择过程。针对现有暴力选择方法计算成本高和扩展性差的问题，该方法通过直接分析量子电路的DAG表示，避免了复杂的特征提取。研究在包含498个量子电路的数据集上进行了评估，结果显示其在预测最佳编译目标方面达到了94.4%的准确率和85.5%的F1分数，显著提高了决策效率和准确性。", "keywords": "图神经网络, 量子硬件选择, 量子电路, 预测器, 自动化", "comments": "该论文的创新点在于将图神经网络应用于量子硬件选择问题，通过直接处理量子电路的图表示，避免了传统方法中耗时的特征工程和暴力搜索。这为自动化量子编译和优化提供了一条有前景的路径，对于未来量子计算的实际应用具有重要意义。其高效性和准确性是其主要优势。"}}
{"id": "2507.05211", "title": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation", "authors": ["Zongyan Han", "Mohamed El Amine Boudjoghra", "Jiahua Dong", "Jinhong Wang", "Rao Muhammad Anwer"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV2025", "url": "http://arxiv.org/abs/2507.05211v2", "summary": "Unified segmentation of 3D point clouds is crucial for scene understanding,\nbut is hindered by its sparse structure, limited annotations, and the challenge\nof distinguishing fine-grained object classes in complex environments. Existing\nmethods often struggle to capture rich semantic and contextual information due\nto limited supervision and a lack of diverse multimodal cues, leading to\nsuboptimal differentiation of classes and instances. To address these\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\npre-trained vision-language models (e.g., CLIP) and large language models\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\ndescriptions and reference images from the internet, our method incorporates\nrich multimodal cues, facilitating fine-grained class and instance separation.\nWe further design a Semantic-Visual Contrastive Loss to align point features\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\nresults in semantic, instance, and panoptic segmentation, offering a scalable\nand practical solution for 3D understanding. Our code is available at\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg.", "comment": "Accepted by ICCV2025", "pdf_url": "http://arxiv.org/pdf/2507.05211v2", "cate": "cs.CV", "date": "2025-07-07", "updated": "2025-07-25", "AI": {"title_translation": "一体化：视觉描述引导的统一三维点云分割", "tldr": "VDG-Uni3DSeg 框架整合预训练视觉-语言模型和大型语言模型，利用多模态线索实现最先进的统一三维点云分割。", "motivation": "三维点云的统一分割对场景理解至关重要，但面临稀疏结构、标注有限以及复杂环境中细粒度物体类别难以区分的挑战。现有方法因监督有限和缺乏多模态线索，难以捕获丰富的语义和上下文信息，导致类别和实例区分效果不佳。", "method": "本文提出了 VDG-Uni3DSeg 框架，通过整合预训练视觉-语言模型（如 CLIP）和大型语言模型（LLMs）来增强三维分割。该方法利用 LLM 生成的文本描述和互联网参考图像，引入了丰富的多模态线索。此外，设计了语义-视觉对比损失（Semantic-Visual Contrastive Loss）以对齐点特征与多模态查询，并设计了空间增强模块（Spatial Enhanced Module）以有效建模场景范围的关系。该框架在离线生成多模态知识的封闭集范式下运行。", "result": "VDG-Uni3DSeg 在语义、实例和全景分割方面取得了最先进的结果。", "conclusion": "VDG-Uni3DSeg 为三维理解提供了一个可扩展且实用的解决方案。", "translation": "三维点云的统一分割对场景理解至关重要，但其稀疏的结构、有限的标注以及在复杂环境中区分细粒度物体类别的挑战阻碍了其发展。现有方法由于监督有限和缺乏多样的多模态线索，往往难以捕获丰富的语义和上下文信息，导致类别和实例区分效果不佳。为了解决这些挑战，我们提出了 VDG-Uni3DSeg，一个新颖的框架，它整合了预训练视觉-语言模型（例如 CLIP）和大型语言模型（LLMs）来增强三维分割。通过利用 LLM 生成的文本描述和来自互联网的参考图像，我们的方法融入了丰富的多模态线索，促进了细粒度类别和实例的分离。我们进一步设计了一种语义-视觉对比损失（Semantic-Visual Contrastive Loss）来对齐点特征与多模态查询，以及一个空间增强模块（Spatial Enhanced Module）来有效建模场景范围的关系。VDG-Uni3DSeg 在利用离线生成的多模态知识的封闭集范式下运行，在语义、实例和全景分割方面取得了最先进的结果，为三维理解提供了一个可扩展且实用的解决方案。我们的代码可在 https://github.com/Hanzy1996/VDG-Uni3DSeg 获取。", "summary": "针对三维点云统一分割中存在的稀疏性、标注不足以及细粒度区分困难等问题，本文提出了 VDG-Uni3DSeg 框架。该框架创新性地整合了预训练视觉-语言模型和大型语言模型，并利用 LLM 生成的文本描述和互联网图像作为多模态线索，以增强细粒度类别和实例的区分能力。通过引入语义-视觉对比损失和空间增强模块，VDG-Uni3DSeg 在语义、实例和全景分割任务上取得了最先进的性能，为三维场景理解提供了一个可扩展且实用的解决方案。", "keywords": "点云分割, 视觉-语言模型, 大型语言模型, 多模态, 三维理解", "comments": "该论文的创新点在于将视觉-语言模型和大型语言模型引入三维点云分割任务，并利用 LLM 生成的文本描述和互联网图像作为丰富多模态线索。这种方法有效解决了传统三维分割中语义信息和上下文捕获不足的问题。此外，提出的语义-视觉对比损失和空间增强模块也对性能提升起到了关键作用。该工作为三维理解提供了一个通用且高性能的解决方案，具有重要的实用价值。"}}
{"id": "2507.18668", "title": "Efficient Knowledge Tracing Leveraging Higher-Order Information in Integrated Graphs", "authors": ["Donghee Han", "Daehee Kim", "Minjun Lee", "Daeyoung Roh", "Keejun Han", "Mun Yong Yi"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18668v1", "summary": "The rise of online learning has led to the development of various knowledge\ntracing (KT) methods. However, existing methods have overlooked the problem of\nincreasing computational cost when utilizing large graphs and long learning\nsequences. To address this issue, we introduce Dual Graph Attention-based\nKnowledge Tracing (DGAKT), a graph neural network model designed to leverage\nhigh-order information from subgraphs representing student-exercise-KC\nrelationships. DGAKT incorporates a subgraph-based approach to enhance\ncomputational efficiency. By processing only relevant subgraphs for each target\ninteraction, DGAKT significantly reduces memory and computational requirements\ncompared to full global graph models. Extensive experimental results\ndemonstrate that DGAKT not only outperforms existing KT models but also sets a\nnew standard in resource efficiency, addressing a critical need that has been\nlargely overlooked by prior KT approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18668v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "利用集成图中高阶信息的有效知识追踪", "tldr": "DGAKT是一种新的知识追踪模型，通过利用子图中的高阶信息来提高计算效率和性能，解决了现有方法在大图和长学习序列下的计算成本问题。", "motivation": "现有知识追踪（KT）方法忽略了在大图和长学习序列下，计算成本不断增加的问题。", "method": "我们引入了基于双图注意力（DGAKT）的知识追踪模型，这是一种图神经网络模型，旨在利用表示学生-练习-知识点（KC）关系的子图中的高阶信息。DGAKT通过处理每个目标交互的相关子图，显著降低了内存和计算需求。", "result": "实验结果表明，DGAKT不仅优于现有知识追踪模型，而且在资源效率方面树立了新标准。", "conclusion": "DGAKT通过其子图方法有效解决了知识追踪中计算效率和性能的挑战，为未来的研究提供了一个有前景的方向。", "translation": "在线学习的兴起催生了各种知识追踪（KT）方法的发展。然而，现有方法忽视了在使用大型图和长学习序列时计算成本增加的问题。为了解决这个问题，我们引入了基于双图注意力（DGAKT）的知识追踪模型，这是一种图神经网络模型，旨在利用表示学生-练习-知识点（KC）关系的子图中的高阶信息。DGAKT采用基于子图的方法来提高计算效率。通过仅处理每个目标交互的相关子图，与完整的全局图模型相比，DGAKT显著降低了内存和计算需求。大量的实验结果表明，DGAKT不仅优于现有KT模型，而且在资源效率方面树立了新标准，解决了以往KT方法在很大程度上忽视的关键需求。", "summary": "该论文提出了一种名为DGAKT的新型知识追踪（KT）模型，旨在解决现有KT方法在使用大图和长学习序列时面临的计算成本高昂问题。DGAKT利用基于双图注意力机制的图神经网络，通过处理学生-练习-知识点关系的子图中的高阶信息，显著提高了计算效率和资源利用率。实验结果表明，DGAKT在性能上优于现有KT模型，并在资源效率方面设立了新标准。", "keywords": "知识追踪, 图神经网络, 计算效率, 高阶信息, 子图", "comments": "DGAKT的创新之处在于其子图处理方法，这有效地解决了传统图神经网络在处理大规模教育数据时面临的计算效率瓶颈。通过聚焦于相关子图，该模型在保持或提升性能的同时，显著降低了资源消耗，这对于实际应用具有重要意义。"}}
{"id": "2503.13787", "title": "A Systematic Digital Engineering Approach to Verification & Validation of Autonomous Ground Vehicles in Off-Road Environments", "authors": ["Tanmay Vilas Samak", "Chinmay Vilas Samak", "Julia Brault", "Cori Harber", "Kirsten McCane", "Jonathon Smereka", "Mark Brudnak", "David Gorsich", "Venkat Krovi"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted at Modeling, Estimation and Control Conference (MECC) 2025. DISTRIBUTION STATEMENT A. Approved for public release; distribution is unlimited. OPSEC9523", "url": "http://arxiv.org/abs/2503.13787v2", "summary": "The engineering community currently encounters significant challenges in the\nsystematic development and validation of autonomy algorithms for off-road\nground vehicles. These challenges are posed by unusually high test parameters\nand algorithmic variants. In order to address these pain points, this work\npresents an optimized digital engineering framework that tightly couples\ndigital twin simulations with model-based systems engineering (MBSE) and\nmodel-based design (MBD) workflows. The efficacy of the proposed framework is\ndemonstrated through an end-to-end case study of an autonomous light tactical\nvehicle (LTV) performing visual servoing to drive along a dirt road and\nreacting to any obstacles or environmental changes. The presented methodology\nallows for traceable requirements engineering, efficient variant management,\ngranular parameter sweep setup, systematic test-case definition, and automated\nexecution of the simulations. The candidate off-road autonomy algorithm is\nevaluated for satisfying requirements against a battery of 128 test cases,\nwhich is procedurally generated based on the test parameters (times of the day\nand weather conditions) and algorithmic variants (perception, planning, and\ncontrol sub-systems). Finally, the test results and key performance indicators\nare logged, and the test report is generated automatically. This then allows\nfor manual as well as automated data analysis with traceability and\ntractability across the digital thread.", "comment": "Accepted at Modeling, Estimation and Control Conference (MECC) 2025.\n  DISTRIBUTION STATEMENT A. Approved for public release; distribution is\n  unlimited. OPSEC9523", "pdf_url": "http://arxiv.org/pdf/2503.13787v2", "cate": "cs.RO", "date": "2025-03-18", "updated": "2025-07-25", "AI": {"title_translation": "非公路环境下自主地面车辆验证与确认的系统化数字工程方法", "tldr": "本文提出了一种优化的数字工程框架，将数字孪生仿真与基于模型的系统工程（MBSE）和基于模型的设计（MBD）工作流紧密结合，以系统化地验证和确认非公路自主地面车辆，并通过一个轻型战术车辆的案例研究证明了其有效性。", "motivation": "工程界在非公路地面车辆自主算法的系统开发和验证方面面临重大挑战，这些挑战源于异常高的测试参数和算法变体。", "method": "本文提出了一种优化的数字工程框架，该框架将数字孪生仿真与基于模型的系统工程（MBSE）和基于模型的设计（MBD）工作流紧密结合。该方法实现了可追溯的需求工程、高效的变体管理、细粒度的参数扫描设置、系统化的测试用例定义以及仿真的自动化执行。", "result": "通过一个自主轻型战术车辆（LTV）沿土路进行视觉伺服驾驶并对障碍物做出反应的端到端案例研究，证明了所提出框架的有效性。候选非公路自主算法针对128个程序化生成的测试用例（基于时间、天气和算法变体）进行了评估，测试结果和关键性能指标被记录并自动生成测试报告，实现了数字线程的可追溯性和可跟踪性。", "conclusion": "所提出的数字工程框架通过集成数字孪生仿真、MBSE和MBD，有效解决了非公路自主地面车辆验证与确认的挑战，实现了系统化、高效且可追溯的测试。", "translation": "工程界目前在非公路地面车辆自主算法的系统开发和验证方面遇到了重大挑战。这些挑战源于异常高的测试参数和算法变体。为了解决这些痛点，本文提出了一种优化的数字工程框架，该框架将数字孪生仿真与基于模型的系统工程（MBSE）和基于模型的设计（MBD）工作流紧密结合。通过一个自主轻型战术车辆（LTV）沿土路进行视觉伺服驾驶并对任何障碍物或环境变化做出反应的端到端案例研究，证明了所提出框架的有效性。所提出的方法允许可追溯的需求工程、高效的变体管理、细粒度的参数扫描设置、系统化的测试用例定义以及仿真的自动化执行。候选非公路自主算法针对一系列128个测试用例进行了评估，这些测试用例是根据测试参数（一天中的时间、天气条件）和算法变体（感知、规划和控制子系统）程序化生成的，以满足要求。最后，测试结果和关键性能指标被记录下来，并自动生成测试报告。这使得手动以及自动化数据分析都具有数字线程的可追溯性和可跟踪性。", "summary": "本文提出了一种优化的数字工程框架，旨在解决非公路自主地面车辆算法在开发和验证中遇到的挑战。该框架将数字孪生仿真与基于模型的系统工程（MBSE）和设计（MBD）工作流紧密结合，以实现可追溯的需求工程、高效的变体管理、细粒度的参数设置和自动化的测试执行。通过一个自主轻型战术车辆的案例研究，证明了其在评估算法和生成测试报告方面的有效性，从而提高了测试的系统性、效率和可追溯性。", "keywords": "数字工程, 自主车辆, 非公路, 验证与确认, 数字孪生", "comments": "该论文为自主车辆，特别是在复杂的非公路环境中的验证与确认，提供了一种创新且系统化的方法。其优势在于将成熟的工程方法（MBSE、MBD）与现代仿真技术（数字孪生）相结合，构建了一个全面的V&V流程。对自动化测试用例生成和结果报告的关注显著提高了效率和可追溯性，解决了传统测试的关键局限性。"}}
{"id": "2507.19362", "title": "LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences", "authors": ["Yusuke Hirota", "Boyi Li", "Ryo Hachiuma", "Yueh-Hua Wu", "Boris Ivanovic", "Yuta Nakashima", "Marco Pavone", "Yejin Choi", "Yu-Chiang Frank Wang", "Chao-Han Huck Yang"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ACL 2025. Leaderboard: this http URL", "url": "http://arxiv.org/abs/2507.19362v1", "summary": "Large Vision-Language Models (LVLMs) have transformed image captioning,\nshifting from concise captions to detailed descriptions. We introduce LOTUS, a\nleaderboard for evaluating detailed captions, addressing three main gaps in\nexisting evaluations: lack of standardized criteria, bias-aware assessments,\nand user preference considerations. LOTUS comprehensively evaluates various\naspects, including caption quality (e.g., alignment, descriptiveness), risks\n(\\eg, hallucination), and societal biases (e.g., gender bias) while enabling\npreference-oriented evaluations by tailoring criteria to diverse user\npreferences. Our analysis of recent LVLMs reveals no single model excels across\nall criteria, while correlations emerge between caption detail and bias risks.\nPreference-oriented evaluations demonstrate that optimal model selection\ndepends on user priorities.", "comment": "Accepted to ACL 2025. Leaderboard:\n  huggingface.co/spaces/nvidia/lotus-vlm-bias-leaderboard", "pdf_url": "http://arxiv.org/pdf/2507.19362v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "LOTUS：一个评估从质量到社会偏见和用户偏好的详细图像字幕的排行榜", "tldr": "LOTUS是一个新的排行榜，用于评估详细图像字幕，解决了现有评估中缺乏标准化标准、偏见意识评估和用户偏好考虑的问题。", "motivation": "现有的大型视觉语言模型（LVLMs）的图像字幕评估存在三个主要空白：缺乏标准化标准、缺乏偏见意识评估以及未考虑用户偏好。", "method": "LOTUS通过综合评估字幕质量（如对齐、描述性）、风险（如幻觉）和社会偏见（如性别偏见）来评估详细字幕。它还通过根据不同的用户偏好定制标准来实现面向偏好的评估。", "result": "对最近的LVLMs的分析表明，没有一个模型能在所有标准上表现出色，并且字幕细节与偏见风险之间存在相关性。面向偏好的评估表明，最佳模型选择取决于用户优先级。", "conclusion": "没有一个单一的大型视觉语言模型能在所有详细图像字幕评估标准上表现出色，最佳模型选择取决于用户偏好和优先级。", "translation": "大型视觉语言模型（LVLMs）改变了图像字幕的方式，从简洁的字幕转向详细的描述。我们引入了LOTUS，一个用于评估详细字幕的排行榜，解决了现有评估中的三个主要空白：缺乏标准化标准、缺乏偏见意识评估和未考虑用户偏好。LOTUS全面评估了各个方面，包括字幕质量（例如，对齐性、描述性）、风险（例如，幻觉）和社会偏见（例如，性别偏见），同时通过根据不同的用户偏好定制标准来实现面向偏好的评估。我们对近期LVLMs的分析显示，没有一个模型能在所有标准上表现出色，同时字幕细节与偏见风险之间存在相关性。面向偏好的评估表明，最佳模型选择取决于用户优先级。", "summary": "该论文介绍了LOTUS，一个用于评估详细图像字幕的排行榜，旨在解决现有评估在标准化标准、偏见意识和用户偏好方面的不足。LOTUS综合评估字幕质量、风险和社会偏见，并支持面向偏好的评估。研究发现，没有一个LVLM能在所有评估标准上都表现优异，且字幕细节与偏见风险相关，模型选择应基于用户偏好。", "keywords": "图像字幕, 视觉语言模型, 评估, 排行榜, 社会偏见", "comments": "LOTUS的创新之处在于其全面性和对用户偏好的考量，为详细图像字幕的评估提供了一个更细致和实用的框架，特别是引入了对社会偏见和用户偏好的评估，这对于LVLMs的负责任开发至关重要。"}}
{"id": "2507.19262", "title": "OVFact: Measuring and Improving Open-Vocabulary Factuality for Long Caption Models", "authors": ["Monika Wysoczańska", "Shyamal Buch", "Anurag Arnab", "Cordelia Schmid"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19262v1", "summary": "Large vision-language models (VLMs) often struggle to generate long and\nfactual captions. However, traditional measures for hallucination and\nfactuality are not well suited for evaluating longer, more diverse captions and\nin settings where ground-truth human-annotated captions are unavailable. We\nintroduce OV-Fact, a novel method for measuring caption factuality of long\ncaptions that leverages open-vocabulary visual grounding and tool-based\nverification without depending on human annotations. Our method improves\nagreement with human judgments and captures both caption descriptiveness\n(recall) and factual precision in the same metric. Furthermore, unlike previous\nmetrics, our reference-free method design enables new applications towards\nfactuality-based data filtering. We observe models trained on an\nOVFact-filtered (2.5-5x less) subset of a large-scale, noisy (VLM-generated)\npretraining set meaningfully improve factuality precision without sacrificing\ncaption descriptiveness across a range of downstream long caption benchmarks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19262v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "OVFact：衡量和改进长文本描述模型中的开放词汇事实性", "tldr": "OVFact 是一种无需人工标注，用于衡量和改进长文本描述模型事实性的新方法，它能有效提高模型事实性精度，同时保持描述性。", "motivation": "大型视觉语言模型（VLMs）在生成长且事实性的描述时常遇到困难。传统的幻觉和事实性衡量标准不适用于评估更长、更多样化的描述，尤其是在缺乏人工标注的真实数据的情况下。", "method": "本研究引入了 OV-Fact，一种衡量长文本描述事实性的新方法。该方法利用开放词汇视觉接地和基于工具的验证，无需依赖人工标注。它能在同一指标中捕捉描述性（召回率）和事实性精度。此外，这种无需参考的方法设计支持基于事实性的数据过滤新应用。", "result": "研究观察到，在经过 OVFact 过滤（减少2.5-5倍）的大规模、嘈杂（VLM 生成的）预训练子集上训练的模型，在各种下游长文本描述基准测试中，显著提高了事实性精度，同时没有牺牲描述性。", "conclusion": "OVFact 是一种有效的方法，可以衡量并改进长文本描述模型的开放词汇事实性，通过数据过滤显著提升模型性能。", "translation": "大型视觉语言模型（VLMs）在生成长且事实性的描述时常遇到困难。然而，传统的幻觉和事实性衡量标准不适用于评估更长、更多样化的描述，尤其是在缺乏人工标注的真实数据的情况下。我们引入了 OV-Fact，一种衡量长文本描述事实性的新方法，它利用开放词汇视觉接地和基于工具的验证，无需依赖人工标注。我们的方法提高了与人类判断的一致性，并在同一指标中捕捉了描述性（召回率）和事实性精度。此外，与以前的指标不同，我们的无参考方法设计支持基于事实性数据过滤的新应用。我们观察到，在经过 OVFact 过滤（减少2.5-5倍）的大规模、嘈杂（VLM 生成的）预训练子集上训练的模型，在各种下游长文本描述基准测试中，显著提高了事实性精度，同时没有牺牲描述性。", "summary": "本文提出了 OV-Fact，一种无需人工标注即可衡量长文本描述事实性的新方法。该方法利用开放词汇视觉接地和工具验证，提高了与人类判断的一致性，并能同时评估描述性和事实性精度。OV-Fact 的无参考设计使其能够应用于基于事实性的数据过滤。实验证明，在经 OVFact 过滤的数据集上训练的模型，在不牺牲描述性的前提下，显著提升了长文本描述的事实性精度。", "keywords": "事实性, 长文本描述, 视觉语言模型, 数据过滤, OV-Fact", "comments": "本文提出的 OV-Fact 方法具有创新性，它解决了现有事实性评估方法在长文本描述和缺乏人工标注场景下的局限性。其无需参考的特性为数据过滤和模型训练提供了新的方向，对于提高大型视觉语言模型生成文本的可靠性具有重要意义。通过过滤预训练数据来提升模型性能，是一种高效且实用的方法。"}}
{"id": "2507.05465", "title": "2048: Reinforcement Learning in a Delayed Reward Environment", "authors": ["Prady Saligram", "Tanvir Bhathal", "Robby Manihani"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      We found an issue with our result aggregation scripts: some evaluation logs were incomplete and others duplicated, causing incorrect numbers in tables and figures. Because these graphs and tables underpin key comparisons, we are withdrawing the paper to regenerate verified results", "url": "http://arxiv.org/abs/2507.05465v2", "summary": "Delayed and sparse rewards present a fundamental obstacle for\nreinforcement-learning (RL) agents, which struggle to assign credit for actions\nwhose benefits emerge many steps later. The sliding-tile game 2048 epitomizes\nthis challenge: although frequent small score changes yield immediate feedback,\nthey often mislead agents into locally optimal but globally suboptimal\nstrategies. In this work, we introduce a unified, distributional multi-step RL\nframework designed to directly optimize long-horizon performance. Using the\nopen source Gym-2048 environment we develop and compare four agent variants:\nstandard DQN, PPO, QR-DQN (Quantile Regression DQN), and a novel Horizon-DQN\n(H-DQN) that integrates distributional learning, dueling architectures, noisy\nnetworks, prioritized replay, and more. Empirical evaluation reveals a clear\nhierarchy in effectiveness: max episode scores improve from 3.988K (DQN) to\n5.756K (PPO), 8.66K (QR-DQN), and 18.21K (H-DQN), with H-DQN reaching the 2048\ntile. Upon scaling H-DQN it reaches a max score 41.828K and a 4096 tile. These\nresults demonstrate that distributional, multi-step targets substantially\nenhance performance in sparse-reward domains, and they suggest promising\navenues for further gains through model-based planning and curriculum learning.", "comment": "We found an issue with our result aggregation scripts: some\n  evaluation logs were incomplete and others duplicated, causing incorrect\n  numbers in tables and figures. Because these graphs and tables underpin key\n  comparisons, we are withdrawing the paper to regenerate verified results", "pdf_url": "http://arxiv.org/pdf/2507.05465v2", "cate": "cs.LG", "date": "2025-07-07", "updated": "2025-07-24", "AI": {"title_translation": "2048：延迟奖励环境中的强化学习", "tldr": "本文提出了一种统一的、分布式的多步强化学习框架，以解决2048游戏中延迟稀疏奖励带来的挑战，并开发了H-DQN代理，显著提高了游戏表现，达到了更高的分数和瓦片。", "motivation": "延迟和稀疏奖励是强化学习（RL）代理面临的根本障碍，导致代理难以对效益延迟出现的动作进行归因。2048游戏是这一挑战的典型例子，尽管频繁的小分数变化提供即时反馈，但往往会误导代理采取局部最优但全局次优的策略。", "method": "本文引入了一种统一的、分布式的多步强化学习框架，旨在直接优化长期表现。研究人员使用开源的Gym-2048环境，开发并比较了四种代理变体：标准DQN、PPO、QR-DQN（分位数回归DQN）以及一种新颖的Horizon-DQN（H-DQN）。H-DQN集成了分布式学习、对偶架构、噪声网络和优先经验回放等技术。", "result": "经验评估显示了清晰的有效性等级：最大回合分数从DQN的3.988K提高到PPO的5.756K、QR-DQN的8.66K，以及H-DQN的18.21K。H-DQN成功达到了2048瓦片。经过扩展的H-DQN达到了41.828K的最大分数和4096瓦片。", "conclusion": "这些结果表明，分布式、多步目标在稀疏奖励领域显著提升了性能，并暗示通过基于模型的规划和课程学习可以进一步获得收益。", "translation": "延迟和稀疏奖励是强化学习（RL）代理面临的根本障碍，它们难以对效益在许多步之后才出现的动作进行归因。滑动拼图游戏2048是这一挑战的缩影：尽管频繁的小分数变化会产生即时反馈，但它们常常会误导代理采取局部最优但全局次优的策略。在这项工作中，我们引入了一个统一的、分布式的多步RL框架，旨在直接优化长期性能。我们使用开源的Gym-2048环境开发并比较了四种代理变体：标准DQN、PPO、QR-DQN（分位数回归DQN）以及一种新颖的Horizon-DQN（H-DQN），它集成了分布式学习、对偶架构、噪声网络、优先经验回放等。经验评估揭示了效果的清晰等级：最大回合分数从3.988K（DQN）提高到5.756K（PPO）、8.66K（QR-DQN）和18.21K（H-DQN），其中H-DQN达到了2048瓦片。在扩展H-DQN后，它达到了41.828K的最大分数和4096瓦片。这些结果表明，分布式、多步目标显著增强了稀疏奖励领域的性能，并为通过基于模型的规划和课程学习获得进一步收益提供了有希望的途径。", "summary": "本文针对2048游戏中延迟和稀疏奖励对强化学习代理造成的挑战，提出了一个统一的、分布式的多步强化学习框架。研究人员开发并比较了DQN、PPO、QR-DQN和一种新颖的Horizon-DQN（H-DQN）。实验结果表明，H-DQN在最大回合分数上显著优于其他代理，并成功达到了2048瓦片，扩展后甚至达到4096瓦片和更高的分数，证明了分布式、多步目标在稀疏奖励环境中的有效性。", "keywords": "强化学习, 延迟奖励, 分布式RL, 2048游戏, H-DQN", "comments": "本文创新性地将多种先进的RL技术（如分布式学习、对偶架构、噪声网络、优先经验回放）整合到H-DQN中，以解决延迟和稀疏奖励问题。其在2048游戏中的显著性能提升证明了该框架的有效性，并为未来在类似复杂RL环境中的研究提供了有价值的方向。论文强调了分布式、多步目标的重要性，并指出了模型规划和课程学习的潜在作用。"}}
{"id": "2402.07510", "title": "Secret Collusion among AI Agents: Multi-Agent Deception via Steganography", "authors": ["Sumeet Ramesh Motwani", "Mikhail Baranchuk", "Martin Strohmeier", "Vijay Bolina", "Philip H. S. Torr", "Lewis Hammond", "Christian Schroeder de Witt"], "categories": ["cs.AI", "cs.CR"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2402.07510v5", "summary": "Recent capability increases in large language models (LLMs) open up\napplications in which groups of communicating generative AI agents solve joint\ntasks. This poses privacy and security challenges concerning the unauthorised\nsharing of information, or other unwanted forms of agent coordination. Modern\nsteganographic techniques could render such dynamics hard to detect. In this\npaper, we comprehensively formalise the problem of secret collusion in systems\nof generative AI agents by drawing on relevant concepts from both AI and\nsecurity literature. We study incentives for the use of steganography, and\npropose a variety of mitigation measures. Our investigations result in a model\nevaluation framework that systematically tests capabilities required for\nvarious forms of secret collusion. We provide extensive empirical results\nacross a range of contemporary LLMs. While the steganographic capabilities of\ncurrent models remain limited, GPT-4 displays a capability jump suggesting the\nneed for continuous monitoring of steganographic frontier model capabilities.\nWe conclude by laying out a comprehensive research program to mitigate future\nrisks of collusion between generative AI models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2402.07510v5", "cate": "cs.AI", "date": "2024-02-12", "updated": "2025-07-25", "AI": {"title_translation": "AI智能体间的秘密合谋：通过隐写术实现多智能体欺骗", "tldr": "本文形式化了生成式AI智能体系统中秘密合谋的问题，研究了隐写术的使用动机和缓解措施，并提出了一个评估框架。研究发现当前模型的隐写能力有限，但GPT-4显示出显著提升，预示着未来潜在风险，需要持续监测和研究以缓解合谋风险。", "motivation": "大型语言模型（LLMs）能力的提升使得生成式AI智能体群体能够协同解决任务，但这带来了未经授权的信息共享或其他形式的智能体协调所引发的隐私和安全挑战。现代隐写技术可能使这类动态难以被察觉。", "method": "本文综合形式化了生成式AI智能体系统中秘密合谋的问题，借鉴了AI和安全文献中的相关概念。研究了使用隐写术的动机，并提出了一系列缓解措施。研究成果是一个模型评估框架，系统地测试了各种形式的秘密合谋所需的能力。", "result": "当前模型的隐写能力仍然有限，但GPT-4显示出能力上的显著提升，这表明需要持续监测隐写前沿模型的能力。", "conclusion": "本文最后提出了一个全面的研究计划，以减轻未来生成式AI模型之间合谋的风险。", "translation": "大型语言模型（LLMs）近期能力的提升，使得通过通信的生成式AI智能体群体能够解决联合任务的应用成为可能。这带来了关于未经授权的信息共享或其他不希望出现的智能体协调形式的隐私和安全挑战。现代隐写技术可能使此类动态难以被检测。在本文中，我们通过借鉴AI和安全文献中的相关概念，全面形式化了生成式AI智能体系统中的秘密合谋问题。我们研究了使用隐写术的动机，并提出了一系列缓解措施。我们的研究成果是一个模型评估框架，该框架系统地测试了各种形式秘密合谋所需的能力。我们提供了对一系列当代LLM的广泛实证结果。虽然当前模型的隐写能力仍然有限，但GPT-4显示出能力上的显著提升，这表明需要持续监测隐写前沿模型的能力。我们最后提出了一个全面的研究计划，以减轻未来生成式AI模型之间合谋的风险。", "summary": "本研究探讨了生成式AI智能体之间秘密合谋的潜在风险，特别是在信息共享和协调任务中可能利用隐写术进行难以察觉的欺骗。文章形式化了这一问题，分析了隐写术的使用动机，并提出了缓解策略。通过构建评估框架，研究发现虽然当前LLM的隐写能力有限，但GPT-4已展现出显著提升，这强调了持续监测和深入研究以应对未来合谋风险的必要性。", "keywords": "AI智能体, 隐写术, 合谋, 大型语言模型, 安全", "comments": "本文创新性地将隐写术引入到多智能体AI系统的安全分析中，揭示了LLM在未来可能被用于秘密合谋的潜在风险。其重要性在于首次系统地形式化了这一新兴的安全威胁，并提出了实用的评估框架和缓解措施，对AI伦理和安全领域具有前瞻性意义。文章指出了GPT-4在隐写能力上的飞跃，提示了未来对AI模型能力的持续关注和风险防范的紧迫性。"}}
{"id": "2507.19017", "title": "MindSpeed RL: Distributed Dataflow for Scalable and Efficient RL Training on Ascend NPU Cluster", "authors": ["Laingjun Feng", "Chenyi Pan", "Xinjie Guo", "Fei Mei", "Benzhe Ning", "Jianxiang Zhang", "Xinyang Liu", "Beirong Zhou", "Zeng Shu", "Chang Liu", "Guang Yang", "Zhenyu Han", "Jiangben Wang", "Bo Wang"], "categories": ["cs.LG", "cs.AI", "CS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages", "url": "http://arxiv.org/abs/2507.19017v1", "summary": "Reinforcement learning (RL) is a paradigm increasingly used to align large\nlanguage models. Popular RL algorithms utilize multiple workers and can be\nmodeled as a graph, where each node is the status of a worker and each edge\nrepresents dataflow between nodes. Owing to the heavy cross-node dependencies,\nthe RL training system usually suffers from poor cluster scalability and low\nmemory utilization. In this article, we introduce MindSpeed RL, an effective\nand efficient system for large-scale RL training. Unlike existing centralized\nmethods, MindSpeed RL organizes the essential data dependencies in RL training,\ni.e., sample flow and resharding flow, from a distributed view. On the one\nhand, a distributed transfer dock strategy, which sets controllers and\nwarehouses on the basis of the conventional replay buffer, is designed to\nrelease the dispatch overhead in the sample flow. A practical allgather--swap\nstrategy is presented to eliminate redundant memory usage in resharding flow.\nIn addition, MindSpeed RL further integrates numerous parallelization\nstrategies and acceleration techniques for systematic optimization. Compared\nwith existing state-of-the-art systems, comprehensive experiments on the RL\ntraining of popular Qwen2.5-Dense-7B/32B, Qwen3-MoE-30B, and\nDeepSeek-R1-MoE-671B show that MindSpeed RL increases the throughput by 1.42 ~\n3.97 times. Finally, we open--source MindSpeed RL and perform all the\nexperiments on a super pod of Ascend with 384 neural processing units (NPUs) to\ndemonstrate the powerful performance and reliability of Ascend.", "comment": "9 pages", "pdf_url": "http://arxiv.org/pdf/2507.19017v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "MindSpeed RL：面向昇腾NPU集群的可伸缩高效强化学习训练的分布式数据流", "tldr": "MindSpeed RL是一个用于昇腾NPU集群的大规模强化学习（RL）训练的分布式系统，通过优化数据依赖和集成并行化策略，显著提升了RL训练的吞吐量和效率。", "motivation": "强化学习（RL）训练系统在对齐大型语言模型时，由于繁重的跨节点依赖，通常面临集群可伸缩性差和内存利用率低的问题。", "method": "MindSpeed RL从分布式视角组织关键数据依赖（样本流和再分片流）。它设计了分布式传输坞策略（在传统回放缓冲区基础上设置控制器和仓库）以减少样本流的调度开销，并提出了allgather-swap策略以消除再分片流中的冗余内存使用。此外，系统还集成了多种并行化策略和加速技术。", "result": "与现有最先进的系统相比，MindSpeed RL在流行的Qwen2.5-Dense-7B/32B、Qwen3-MoE-30B和DeepSeek-R1-MoE-671B等大型语言模型的RL训练中，吞吐量提高了1.42至3.97倍。所有实验均在包含384个NPU的昇腾超级pod上进行。", "conclusion": "MindSpeed RL是一个用于大规模强化学习训练的有效且高效的系统，在昇腾NPU集群上展示了强大的性能和可靠性，并且该系统已开源。", "translation": "强化学习（RL）是一种越来越多地用于对齐大型语言模型的范式。流行的RL算法利用多个worker，可以建模为图，其中每个节点是worker的状态，每条边表示节点之间的数据流。由于繁重的跨节点依赖，RL训练系统通常面临集群可伸缩性差和内存利用率低的问题。在本文中，我们介绍了MindSpeed RL，一个用于大规模RL训练的有效且高效的系统。与现有的集中式方法不同，MindSpeed RL从分布式视角组织RL训练中基本的数据依赖，即样本流和再分片流。一方面，设计了一种分布式传输坞策略，在传统回放缓冲区的基础上设置控制器和仓库，以释放样本流中的调度开销。提出了一种实用的allgather-swap策略，以消除再分片流中的冗余内存使用。此外，MindSpeed RL还集成了大量的并行化策略和加速技术，以实现系统优化。与现有最先进的系统相比，在流行的Qwen2.5-Dense-7B/32B、Qwen3-MoE-30B和DeepSeek-R1-MoE-671B的RL训练上进行的综合实验表明，MindSpeed RL将吞吐量提高了1.42到3.97倍。最后，我们开源了MindSpeed RL，并在一个包含384个神经网络处理单元（NPU）的昇腾超级pod上进行了所有实验，以展示昇腾的强大性能和可靠性。", "summary": "MindSpeed RL是一个用于大规模强化学习训练的分布式系统，旨在解决现有RL训练系统在处理大型语言模型时面临的集群可伸缩性差和内存利用率低的问题。它通过采用分布式传输坞策略优化样本流的调度开销，并利用allgather-swap策略消除再分片流中的冗余内存使用，从而高效管理数据依赖。此外，该系统还集成了多种并行化和加速技术。实验结果表明，MindSpeed RL在Ascend NPU集群上，相对于现有SOTA系统，能够将RL训练的吞吐量提高1.42至3.97倍，并且该系统已开源。", "keywords": "强化学习, 分布式训练, 昇腾NPU, 数据流, 可伸缩性", "comments": "MindSpeed RL通过创新的分布式数据流管理（分布式传输坞和allgather-swap策略）解决了大规模RL训练中常见的可伸缩性和内存效率问题。其在昇腾NPU集群上的显著性能提升（高达3.97倍吞吐量）和对大型LLMs的支持，显示了其在推动RL应用于LLM对齐方面的潜力。开源性质也利于社区采纳和进一步发展。"}}
{"id": "2507.18853", "title": "Approximating CCCV charging using SOC-dependent tapered charging power constraints in long-term microgrid planning", "authors": ["Hassan Zahid Butt", "Xingpeng Li"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18853v1", "summary": "Traditional long-term microgrid planning models assume constant power\ncharging for battery energy storage systems (BESS), overlooking efficiency\nlosses that occur toward the end of charge due to rising internal resistance.\nWhile this issue can be mitigated at the cell level using constant\ncurrent-constant voltage (CCCV) charging, it is impractical at the pack level\nin large-scale systems. However, battery management systems and inverter\ncontrols can emulate this effect by tapering charging power at high\nstate-of-charge (SOC) levels, trading off charging speed for improved\nefficiency and reduced thermal stress. Ignoring this behavior in planning\nmodels can lead to undersized batteries and potential reliability issues. This\npaper proposes a tractable and scalable approach to approximate CCCV behavior\nusing SOC-dependent tapered charging power (TCP) constraints. A MATLAB-based\nproof of concept demonstrates the energy delivery and efficiency benefits of\ntapering. The method is integrated into a long-term planning framework and\nevaluated under a synthetic load and solar profile. Results show tapering\nsignificantly affects BESS sizing, cost, and reliability under dynamic\noperating conditions that demand fast charging. These findings highlight\ntapering as a critical modeling factor for accurately capturing BESS\nperformance in long-term microgrid planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18853v1", "cate": "eess.SY", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "在长期微电网规划中利用SOC依赖的递减充电功率约束近似CCCV充电", "tldr": "该论文提出了一种可处理且可扩展的方法，通过引入基于SOC的递减充电功率约束来近似电池储能系统（BESS）的CCCV充电行为，以在长期微电网规划中更准确地反映电池性能和效率。", "motivation": "传统的长期微电网规划模型假设电池储能系统（BESS）恒功率充电，忽略了充电末期因内阻升高导致的效率损失。尽管恒流恒压（CCCV）充电可以在电池单元层面缓解此问题，但在大型系统中的电池组层面不切实际。忽略这种递减充电行为可能导致电池尺寸不足和潜在的可靠性问题。", "method": "本论文提出了一种可处理且可扩展的方法，通过使用SOC依赖的递减充电功率（TCP）约束来近似CCCV行为。通过MATLAB概念验证展示了递减充电在能量输送和效率方面的益处。该方法被整合到长期规划框架中，并在合成负载和太阳能配置文件下进行评估。", "result": "结果显示，在需要快速充电的动态操作条件下，递减充电显著影响BESS的尺寸、成本和可靠性。", "conclusion": "这些发现强调了递减充电作为关键建模因素的重要性，以便在长期微电网规划中准确捕捉BESS性能。", "translation": "传统的长期微电网规划模型假设电池储能系统（BESS）恒功率充电，忽略了充电末期因内阻升高导致的效率损失。尽管这个问题可以在电池单元层面通过恒流恒压（CCCV）充电来缓解，但在大型系统中的电池组层面并不实际。然而，电池管理系统和逆变器控制可以通过在高荷电状态（SOC）水平下递减充电功率来模拟这种效果，以牺牲充电速度换取效率提升和热应力降低。在规划模型中忽略这种行为可能导致电池尺寸不足和潜在的可靠性问题。本文提出了一种可处理且可扩展的方法，利用SOC依赖的递减充电功率（TCP）约束来近似CCCV行为。基于MATLAB的概念验证展示了递减充电在能量输送和效率方面的益处。该方法被整合到长期规划框架中，并在合成负载和太阳能配置文件下进行评估。结果显示，在需要快速充电的动态操作条件下，递减充电显著影响BESS的尺寸、成本和可靠性。这些发现强调了递减充电作为关键建模因素的重要性，以便在长期微电网规划中准确捕捉BESS性能。", "summary": "本研究解决了传统微电网规划模型中忽略电池充电末期效率损失的问题，提出了一种利用荷电状态（SOC）依赖的递减充电功率约束来近似恒流恒压（CCCV）充电行为的方法。该方法旨在更准确地反映电池性能，避免电池尺寸不足和可靠性问题。通过MATLAB验证，并将其整合到长期规划框架中进行评估，结果表明递减充电对电池储能系统（BESS）的尺寸、成本和可靠性有显著影响，尤其是在需要快速充电的场景下。研究强调了递减充电在长期微电网规划中作为关键建模因素的重要性。", "keywords": "微电网规划, 电池储能系统, 递减充电功率, CCCV充电, 荷电状态", "comments": "该论文创新性地将电池管理系统层面的递减充电行为引入到宏观的长期微电网规划模型中，弥补了传统规划模型在电池效率和尺寸估算上的不足。其重要性在于，通过更精确的电池行为建模，可以提高微电网规划的准确性，优化BESS的配置和成本，并增强系统的可靠性。这种从细节到整体的建模思路对于实际工程应用具有指导意义。"}}
{"id": "2411.18651", "title": "Verbalized Representation Learning for Interpretable Few-Shot Generalization", "authors": ["Cheng-Fu Yang", "Da Yin", "Wenbo Hu", "Nanyun Peng", "Bolei Zhou", "Kai-Wei Chang"], "categories": ["cs.CV", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2411.18651v2", "summary": "Humans recognize objects after observing only a few examples, a remarkable\ncapability enabled by their inherent language understanding of the real-world\nenvironment. Developing verbalized and interpretable representation can\nsignificantly improve model generalization in low-data settings. In this work,\nwe propose Verbalized Representation Learning (VRL), a novel approach for\nautomatically extracting human-interpretable features for object recognition\nusing few-shot data. Our method uniquely captures inter-class differences and\nintra-class commonalities in the form of natural language by employing a\nVision-Language Model (VLM) to identify key discriminative features between\ndifferent classes and shared characteristics within the same class. These\nverbalized features are then mapped to numeric vectors through the VLM. The\nresulting feature vectors can be further utilized to train and infer with\ndownstream classifiers. Experimental results show that, at the same model\nscale, VRL achieves a 24% absolute improvement over prior state-of-the-art\nmethods while using 95% less data and a smaller mode. Furthermore, compared to\nhuman-labeled attributes, the features learned by VRL exhibit a 20% absolute\ngain when used for downstream classification tasks. Code is available at:\nhttps://github.com/joeyy5588/VRL/tree/main.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2411.18651v2", "cate": "cs.CV", "date": "2024-11-27", "updated": "2025-07-25", "AI": {"title_translation": "可解释的少样本泛化口语化表示学习", "tldr": "VRL利用视觉-语言模型从少量数据中提取人类可解释的口语化特征，显著提高了少样本目标识别的泛化能力，并优于现有技术。", "motivation": "现有模型在低数据量设置下的泛化能力不足，而人类能通过语言理解仅凭少量样本识别物体，这启发了作者开发可解释的口语化表示来提升模型性能。", "method": "本文提出了口语化表示学习（VRL），一种利用视觉-语言模型（VLM）自动提取人类可解释特征的方法。VLM以自然语言形式捕捉类间差异和类内共性，然后将这些口语化特征映射为数值向量，供下游分类器使用。", "result": "VRL在相同模型规模下，相较于现有最先进方法实现了24%的绝对性能提升，同时使用的数据量减少了95%，模型更小。与人工标注属性相比，VRL学习到的特征在下游分类任务中表现出20%的绝对增益。", "conclusion": "VRL通过学习人类可解释的口语化特征，显著提升了模型在少样本设置下的泛化能力，并在效率和性能上超越了现有方法及人工标注特征。", "translation": "人类仅通过观察少量样本就能识别物体，这种卓越的能力得益于其对现实世界环境固有的语言理解。开发口语化和可解释的表示可以显著提高模型在低数据量设置下的泛化能力。在这项工作中，我们提出了口语化表示学习（VRL），这是一种利用少样本数据自动提取用于目标识别的人类可解释特征的新颖方法。我们的方法独特之处在于，通过使用视觉-语言模型（VLM）识别不同类别之间的关键判别特征以及同一类别内的共享特征，以自然语言的形式捕捉类间差异和类内共性。然后，这些口语化特征通过VLM映射为数值向量。生成的特征向量可以进一步用于训练和推理下游分类器。实验结果表明，在相同的模型规模下，VRL相较于现有最先进方法实现了24%的绝对性能提升，同时使用的数据量减少了95%，并且模型更小。此外，与人工标注属性相比，VRL学习到的特征在用于下游分类任务时表现出20%的绝对增益。代码可在以下地址获取：https://github.com/joeyy5588/VRL/tree/main。", "summary": "本文提出了口语化表示学习（VRL），一种新颖的少样本目标识别方法。VRL利用视觉-语言模型（VLM）自动提取人类可解释的自然语言特征，捕捉类间差异和类内共性，并将这些特征转换为数值向量以供下游分类器使用。实验证明，VRL在数据效率和性能上均显著优于现有最先进方法和人工标注特征。", "keywords": "口语化表示学习, 少样本泛化, 可解释性, 视觉-语言模型, 目标识别", "comments": "VRL的创新性在于其利用VLM生成人类可解释的“口语化”特征，这不仅提升了模型在少样本设置下的泛化能力，也增强了模型的可解释性。该方法在数据效率和性能上的显著提升，表明了结合语言理解进行表示学习的巨大潜力，对于解决数据稀缺问题具有重要意义。"}}
{"id": "2505.15670", "title": "SALM-Duplex: Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model", "authors": ["Ke Hu", "Ehsan Hosseini-Asl", "Chen Chen", "Edresson Casanova", "Subhankar Ghosh", "Piotr Żelasko", "Zhehuai Chen", "Jason Li", "Jagadeesh Balam", "Boris Ginsburg"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to Interspeech 2025", "url": "http://arxiv.org/abs/2505.15670v4", "summary": "Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility.", "comment": "Accepted to Interspeech 2025", "pdf_url": "http://arxiv.org/pdf/2505.15670v4", "cate": "cs.CL", "date": "2025-05-21", "updated": "2025-07-25", "AI": {"title_translation": "SALM-Duplex：高效直接的双工语音到语音语言模型建模", "tldr": "SALM-Duplex提出了一种新颖的双工语音到语音（S2S）架构，能直接建模同步的用户和代理语音流，支持用户打断，无需语音预训练，并显著提升了交互性能和效率。", "motivation": "当前的语音语言模型受限于回合制交互，缺乏实时适应性，例如用户打断（barge-in）功能，这限制了人机交互的直观性。", "method": "我们提出了一种新颖的双工语音到语音（S2S）架构，该架构具有连续用户输入和编解码器代理输出，并结合了通道融合，可以直接建模同步的用户和代理流。该模型使用预训练的流式编码器进行用户输入，使其成为首个无需语音预训练的双工S2S模型。此外，代理和用户建模的独立架构有助于编解码器微调以获得更好的代理语音，并将比特率（0.6 kbps）减半。", "result": "所提出的模型在推理、轮流切换和打断能力方面优于先前的双工模型。由于跳过了语音预训练，该模型所需语音数据显著减少，极大地简化了从任何大型语言模型（LLM）构建双工S2S模型的过程。此外，它是第一个提供训练和推理代码的开源双工S2S模型，促进了可复现性。", "conclusion": "SALM-Duplex通过其创新的双工S2S架构，有效解决了现有语音语言模型在实时交互和用户打断方面的局限性，显著提升了性能和效率，并降低了构建成本，为未来的语音人机交互奠定了基础。", "translation": "口语对话是一种直观的人机交互形式，但当前的语音语言模型通常仍受限于回合制交流，缺乏实时适应性，例如用户打断。我们提出了一种新颖的双工语音到语音（S2S）架构，该架构具有连续用户输入和编解码器代理输出，并结合了通道融合，可以直接建模同步的用户和代理流。使用预训练的流式编码器进行用户输入，使得该模型成为首个无需语音预训练的双工S2S模型。代理和用户建模的独立架构有助于编解码器微调以获得更好的代理语音，并将比特率（0.6 kbps）与现有工作相比减半。实验结果表明，所提出的模型在推理、轮流切换和打断能力方面优于先前的双工模型。由于跳过了语音预训练，该模型所需语音数据显著减少，这显著简化了从任何大型语言模型（LLM）构建双工S2S模型的过程。最后，它是第一个提供训练和推理代码的开源双工S2S模型，以促进可复现性。", "summary": "SALM-Duplex介绍了一种新颖的双工语音到语音（S2S）架构，旨在解决传统语音语言模型在实时人机交互中缺乏用户打断能力的问题。该模型通过直接建模同步的用户和代理语音流，并利用预训练的流式编码器避免了语音预训练的需要。它采用独立的代理和用户建模架构，优化了代理语音质量并将比特率降低了一半。实验证明，SALM-Duplex在推理、轮流切换和打断能力方面优于现有模型，同时显著减少了对语音数据的需求，简化了与任何大型语言模型的集成。此外，该模型是首个提供开源训练和推理代码的双工S2S模型。", "keywords": "双工建模, 语音到语音, 语言模型, 用户打断, 实时交互", "comments": "该论文的创新点在于提出了首个无需语音预训练的双工S2S模型，这大大降低了模型构建的复杂性和数据需求。其直接建模同步语音流和支持用户打断的能力，对提升人机交互的自然度和效率具有重要意义。此外，开源代码的提供也极大地促进了研究的可复现性和社区协作。"}}
{"id": "2507.19109", "title": "Pareto-NRPA: A Novel Monte-Carlo Search Algorithm for Multi-Objective Optimization", "authors": ["Noé Lallouet", "Tristan Cazenave", "Cyrille Enderli"], "categories": ["cs.AI", "cs.NE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Preprint ; accepted to ECAI 2025", "url": "http://arxiv.org/abs/2507.19109v1", "summary": "We introduce Pareto-NRPA, a new Monte-Carlo algorithm designed for\nmulti-objective optimization problems over discrete search spaces. Extending\nthe Nested Rollout Policy Adaptation (NRPA) algorithm originally formulated for\nsingle-objective problems, Pareto-NRPA generalizes the nested search and policy\nupdate mechanism to multi-objective optimization. The algorithm uses a set of\npolicies to concurrently explore different regions of the solution space and\nmaintains non-dominated fronts at each level of search. Policy adaptation is\nperformed with respect to the diversity and isolation of sequences within the\nPareto front. We benchmark Pareto-NRPA on two classes of problems: a novel\nbi-objective variant of the Traveling Salesman Problem with Time Windows\nproblem (MO-TSPTW), and a neural architecture search task on well-known\nbenchmarks. Results demonstrate that Pareto-NRPA achieves competitive\nperformance against state-of-the-art multi-objective algorithms, both in terms\nof convergence and diversity of solutions. Particularly, Pareto-NRPA strongly\noutperforms state-of-the-art evolutionary multi-objective algorithms on\nconstrained search spaces. To our knowledge, this work constitutes the first\nadaptation of NRPA to the multi-objective setting.", "comment": "Preprint ; accepted to ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.19109v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "Pareto-NRPA：一种用于多目标优化的新型蒙特卡洛搜索算法", "tldr": "Pareto-NRPA是一种新的蒙特卡洛搜索算法，它将单目标NRPA扩展到多目标优化问题，并在离散搜索空间中表现出优异的性能。", "motivation": "该研究的动机是将原为单目标问题设计的Nested Rollout Policy Adaptation (NRPA)算法推广到多目标优化领域，以解决离散搜索空间中的多目标优化问题。", "method": "Pareto-NRPA算法是Nested Rollout Policy Adaptation (NRPA)算法的多目标扩展。它使用一组策略并发探索解空间的不同区域，并在搜索的每个级别维护非支配前沿。策略适应是根据帕累托前沿内序列的多样性和隔离度进行的。", "result": "Pareto-NRPA在多目标旅行商问题（MO-TSPTW）和神经架构搜索任务上进行了基准测试。结果表明，该算法在收敛性和解决方案多样性方面均优于最先进的多目标算法，尤其在约束搜索空间上显著优于最先进的进化多目标算法。", "conclusion": "这项工作首次将NRPA算法应用于多目标优化设置，并且Pareto-NRPA在多目标优化问题上表现出有竞争力的性能。", "translation": "我们引入了Pareto-NRPA，这是一种新的蒙特卡洛算法，专为离散搜索空间上的多目标优化问题而设计。Pareto-NRPA扩展了最初为单目标问题制定的嵌套展开策略适应（NRPA）算法，将嵌套搜索和策略更新机制推广到多目标优化。该算法使用一组策略并发探索解空间的不同区域，并在搜索的每个级别维护非支配前沿。策略适应是根据帕累托前沿内序列的多样性和隔离度进行的。我们在两类问题上对Pareto-NRPA进行了基准测试：一个新颖的双目标时间窗旅行商问题（MO-TSPTW）变体，以及在知名基准上的神经架构搜索任务。结果表明，Pareto-NRPA在收敛性和解决方案多样性方面均达到了与最先进的多目标算法相当的性能。特别是，Pareto-NRPA在约束搜索空间上显著优于最先进的进化多目标算法。据我们所知，这项工作是NRPA首次应用于多目标环境。", "summary": "本文提出Pareto-NRPA，一种基于蒙特卡洛搜索的新型多目标优化算法。该算法是单目标NRPA的扩展，通过使用策略集探索解空间并维护非支配前沿，实现策略基于帕累托前沿多样性进行适应。实验证明，Pareto-NRPA在多目标旅行商问题和神经架构搜索任务上，相较于现有最先进的多目标算法，在收敛性和解多样性方面均表现出竞争力，尤其在约束搜索空间中性能更优。", "keywords": "多目标优化, 蒙特卡洛搜索, NRPA, 帕累托前沿, 离散搜索空间", "comments": "本文的创新点在于首次将单目标优化领域的NRPA算法成功推广并应用于多目标优化问题。其通过引入策略集和维护非支配前沿的机制，有效处理了多目标场景下的探索和收敛问题。特别是在约束搜索空间中表现出的优越性，突显了其在复杂问题解决上的潜力。"}}
{"id": "2507.19193", "title": "Where are the Frontlines? A Visualization Approach for Map Control in Team-Based Games", "authors": ["Jonas Peché", "Aliaksei Tsishurou", "Alexander Zap", "Guenter Wallner"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19193v1", "summary": "A central area of interest in many competitive online games is spatial\nbehavior which due to its complexity can be difficult to visualize. Such\nbehaviors of interest include not only overall movement patterns but also being\nable to understand which player or team is exerting control over an area to\ninform decision-making. Map control can, however, be challenging to quantify.\nIn this paper, we propose a method for calculating frontlines and first efforts\ntowards a visualization of them. The visualization can show map control and\nfrontlines at a specific time point or changes of these over time. For this\npurpose, it utilizes support vector machines to derive frontlines from unit\npositions. We illustrate our algorithm and visualization with examples based on\nthe team-based online game World of Tanks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19193v1", "cate": "cs.HC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "战线在哪里？一种用于团队游戏中地图控制的可视化方法", "tldr": "提出一种基于支持向量机的方法，用于可视化团队游戏中地图控制和战线。", "motivation": "在许多竞技性在线游戏中，空间行为由于复杂性难以可视化，且地图控制难以量化。了解玩家或团队对区域的控制对于决策制定至关重要。", "method": "提出一种计算和可视化战线的方法。该方法利用支持向量机从单位位置推导出战线，并能显示特定时间点或随时间变化的地图控制和战线。", "result": "通过《坦克世界》这款团队在线游戏中的例子，展示了所提出的算法和可视化方法。", "conclusion": "本文提出了一种使用支持向量机从单位位置推导战线，并可视化地图控制的方法，旨在帮助理解团队游戏中复杂的空间行为。", "translation": "许多竞技性在线游戏的核心兴趣领域是空间行为，由于其复杂性，这种行为难以可视化。这些感兴趣的行为不仅包括整体移动模式，还包括能够理解哪个玩家或团队正在对某个区域施加控制以辅助决策。然而，地图控制可能难以量化。在本文中，我们提出了一种计算战线的方法，并首次尝试对其进行可视化。该可视化可以显示特定时间点的地图控制和战线，或这些随时间的变化。为此，它利用支持向量机从单位位置推导出战线。我们通过基于团队在线游戏《坦克世界》的示例来说明我们的算法和可视化。", "summary": "本文针对竞技性在线游戏中空间行为难以可视化和地图控制难以量化的问题，提出了一种创新的方法。该方法利用支持向量机从单位位置计算并可视化战线，从而展示地图控制。研究通过《坦克世界》的案例，有效演示了其算法和可视化效果，旨在帮助玩家和分析师更好地理解和利用游戏中的空间信息。", "keywords": "地图控制, 战线, 可视化, 支持向量机, 团队游戏", "comments": "这篇论文提出了一种新颖的方法，利用支持向量机来解决团队游戏中地图控制和战线可视化的挑战，这对于游戏分析和策略制定具有潜在的价值。其创新点在于将机器学习技术应用于游戏空间行为分析，并提供动态可视化的可能性。"}}
{"id": "2507.19304", "title": "Multistream Network for LiDAR and Camera-based 3D Object Detection in Outdoor Scenes", "authors": ["Muhammad Ibrahim", "Naveed Akhtar", "Haitian Wang", "Saeed Anwar", "Ajmal Mian"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper has been accepted by IEEE/RSJ IROS 2025 for oral presentation on 19 Oct. 2025", "url": "http://arxiv.org/abs/2507.19304v1", "summary": "Fusion of LiDAR and RGB data has the potential to enhance outdoor 3D object\ndetection accuracy. To address real-world challenges in outdoor 3D object\ndetection, fusion of LiDAR and RGB input has started gaining traction. However,\neffective integration of these modalities for precise object detection task\nstill remains a largely open problem. To address that, we propose a MultiStream\nDetection (MuStD) network, that meticulously extracts task-relevant information\nfrom both data modalities. The network follows a three-stream structure. Its\nLiDAR-PillarNet stream extracts sparse 2D pillar features from the LiDAR input\nwhile the LiDAR-Height Compression stream computes Bird's-Eye View features. An\nadditional 3D Multimodal stream combines RGB and LiDAR features using UV\nmapping and polar coordinate indexing. Eventually, the features containing\ncomprehensive spatial, textural and geometric information are carefully fused\nand fed to a detection head for 3D object detection. Our extensive evaluation\non the challenging KITTI Object Detection Benchmark using public testing server\nat\nhttps://www.cvlibs.net/datasets/kitti/eval_object_detail.php?&result=d162ec699d6992040e34314d19ab7f5c217075e0\nestablishes the efficacy of our method by achieving new state-of-the-art or\nhighly competitive results in different categories while remaining among the\nmost efficient methods. Our code will be released through MuStD GitHub\nrepository at https://github.com/IbrahimUWA/MuStD.git", "comment": "This paper has been accepted by IEEE/RSJ IROS 2025 for oral\n  presentation on 19 Oct. 2025", "pdf_url": "http://arxiv.org/pdf/2507.19304v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "用于室外场景中基于激光雷达和摄像头的3D目标检测的多流网络", "tldr": "提出了一种名为MuStD的多流网络，用于有效融合激光雷达和RGB数据，以提高室外3D目标检测的准确性和效率，并在KITTI基准上取得了最先进或极具竞争力的结果。", "motivation": "现有方法在有效整合激光雷达和RGB数据以实现精确的室外3D目标检测方面仍面临挑战，而这对于解决实际问题至关重要。", "method": "本文提出了一种多流检测（MuStD）网络，该网络采用三流结构：LiDAR-PillarNet流提取稀疏2D柱状特征；LiDAR-Height Compression流计算鸟瞰图特征；额外的3D多模态流通过UV映射和极坐标索引结合RGB和LiDAR特征。最终，包含空间、纹理和几何信息的特征被融合并送入检测头进行3D目标检测。", "result": "在具有挑战性的KITTI目标检测基准上，所提出的方法在不同类别中取得了新的最先进或极具竞争力的结果，同时保持了较高的效率。", "conclusion": "所提出的MuStD网络能够有效融合激光雷达和RGB数据，显著提升了室外3D目标检测的准确性和效率。", "translation": "激光雷达和RGB数据的融合有潜力提高室外3D目标检测的准确性。为了解决室外3D目标检测中的实际挑战，激光雷达和RGB输入的融合已开始受到关注。然而，有效整合这些模态以实现精确目标检测任务仍然是一个很大的开放问题。为了解决这个问题，我们提出了一种多流检测（MuStD）网络，它精心从两种数据模态中提取与任务相关的信息。该网络遵循三流结构。其LiDAR-PillarNet流从激光雷达输入中提取稀疏的2D柱状特征，而LiDAR-Height Compression流计算鸟瞰图特征。一个额外的3D多模态流使用UV映射和极坐标索引结合RGB和激光雷达特征。最终，包含全面的空间、纹理和几何信息的特征被仔细融合并馈送到检测头进行3D目标检测。我们在具有挑战性的KITTI目标检测基准上，使用公共测试服务器（https://www.cvlibs.net/datasets/kitti/eval_object_detail.php?&result=d162ec699d6992040e34314d19ab7f5c217075e0）进行了广泛评估，通过在不同类别中取得新的最先进或极具竞争力的结果，同时保持在最高效的方法之列，从而确立了我们方法的有效性。我们的代码将通过MuStD GitHub仓库（https://github.com/IbrahimUWA/MuStD.git）发布。", "summary": "本论文提出了一种名为MuStD的多流网络，旨在解决室外3D目标检测中激光雷达和RGB数据有效融合的挑战。该网络采用独特的三流结构，分别处理激光雷达的柱状特征、鸟瞰图特征，并通过3D多模态流结合RGB和激光雷达特征。这些丰富的特征经过融合后送入检测头进行3D目标检测。在KITTI基准上的广泛评估表明，MuStD在保持高效率的同时，在多个类别上取得了最先进或极具竞争力的结果。", "keywords": "3D目标检测, 激光雷达, 摄像头, 多流网络, 数据融合", "comments": "该论文的创新点在于其提出的三流MuStD网络，能够精细地从激光雷达和RGB数据中提取并融合多层次特征，有效解决了多模态数据融合的难题。其在KITTI基准上取得的SOTA结果证明了方法的有效性和实用性，对于提高室外3D目标检测的准确性和效率具有重要意义。"}}
{"id": "1806.03814", "title": "Minmax-Regret $k$-Sink Location on a Dynamic Tree Network with Uniform Capacities", "authors": ["Mordecai J. Golin", "Sai Sandeep"], "categories": ["cs.DS"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/1806.03814v2", "summary": "A dynamic flow network $G$ with uniform capacity $c$ is a graph in which at\nmost $c$ units of flow can enter an edge in one time unit. If flow enters a\nvertex faster than it can leave, congestion occurs. The evacuation problem is\nto evacuate all flow to sinks assuming that all flow is confluent, i.e., all\nflow passing through a particular vertex must follow the same exit edge. The\n$k$-sink location problem is to place $k$-sinks so as to minimize this\nevacuation time. Although the $k$-sink location problem is NP-Hard on a general\ngraph it can be solved in $\\tilde O(k^2 n)$ time on trees.\n  The concept of minmax-regret arises from robust optimization. For each\nsource, a range of possible flow values is provided and any scenario with flow\nvalues in those ranges might occur. The goal is to find a sink placement that\nminimizes, over all possible scenarios, the difference between the evacuation\ntime to those sinks and the minimal evacuation time of that scenario.\n  The Minmax-Regret $k$-Sink Location on a Dynamic Path Networks with uniform\ncapacities is polynomial solvable in $n$ and $k$. Similarly, the Minmax-Regret\n$k$-center problem on trees is polynomial solvable in $n$ and $k$. Prior to\nthis work, polynomial time solutions to the Minmax-Regret $k$-Sink Location on\nDynamic Tree Networks with uniform capacities were only known for $k=1$. This\npaper solves this problem, for general $k,$ in time\n  $$O\\Bigl( \\max(k^2 \\log^2 k,\\log ^2n)\\, k^4 n^2 \\log^5 n\\Bigr)$$", "comment": null, "pdf_url": "http://arxiv.org/pdf/1806.03814v2", "cate": "cs.DS", "date": "2018-06-11", "updated": "2025-07-24", "AI": {"title_translation": "动态树网络上具有统一容量的最小最大后悔k-汇点选址问题", "tldr": "本文解决了动态树网络上具有统一容量的最小最大后悔k-汇点选址问题，并给出了适用于任意k的多项式时间解。", "motivation": "疏散问题旨在将所有流疏散到汇点，而k-汇点选址问题是放置k个汇点以最小化疏散时间。虽然在一般图上是NP难的，但在树上可以在$\tilde O(k^2 n)$时间内解决。最小最大后悔的概念源于鲁棒优化，旨在处理流值的不确定性，即在给定流量范围的各种情景下，找到一个汇点布局，使得其疏散时间与该情景下的最小疏散时间之间的差异最小化。此前，动态树网络上具有统一容量的最小最大后悔k-汇点选址问题仅对k=1已知多项式时间解。", "method": "本文为动态树网络上具有统一容量的最小最大后悔k-汇点选址问题，提出了一个适用于一般k的多项式时间解决方案。", "result": "本文为动态树网络上具有统一容量的最小最大后悔k-汇点选址问题，给出了一个时间复杂度为$O(\\max(k^2 \\log^2 k,\\log ^2n)\\, k^4 n^2 \\log^5 n)$的解决方案，适用于一般k。", "conclusion": "本文解决了动态树网络上具有统一容量的最小最大后悔k-汇点选址问题，将其从仅对k=1有解扩展到适用于一般k，并提供了多项式时间算法。", "translation": "一个具有统一容量c的动态流网络G是一个图，其中在一个时间单位内最多有c单位的流可以进入一条边。如果流进入顶点的速度快于其离开的速度，就会发生拥塞。疏散问题是指假设所有流都是汇合的，即将所有流疏散到汇点，即所有流通过特定顶点时必须遵循相同的出口边。k-汇点选址问题是放置k个汇点以最小化此疏散时间。尽管k-汇点选址问题在一般图上是NP难的，但在树上可以在$\tilde O(k^2 n)$时间内解决。\n最小最大后悔的概念源于鲁棒优化。对于每个源，提供了一系列可能的流值，并且任何流值在此范围内的情景都可能发生。目标是找到一个汇点布局，使得在所有可能情景下，疏散到这些汇点的时间与该情景下的最小疏散时间之间的差异最小化。\n具有统一容量的动态路径网络上的最小最大后悔k-汇点选址问题在n和k上是多项式可解的。类似地，树上的最小最大后悔k-中心问题在n和k上是多项式可解的。在本文工作之前，动态树网络上具有统一容量的最小最大后悔k-汇点选址问题的多项式时间解仅对k=1已知。本文解决了这个问题，对于一般k，时间复杂度为$$O\\Bigl( \\max(k^2 \\log^2 k,\\log ^2n)\\, k^4 n^2 \\log^5 n\\Bigr)$$", "summary": "本文研究了动态树网络上具有统一容量的最小最大后悔k-汇点选址问题。该问题旨在在流值不确定的情况下，通过放置k个汇点来最小化疏散时间与最优疏散时间之间的最大差异。鉴于此前该问题在动态树网络上仅对k=1有已知解，本文提出了一个针对一般k的多项式时间算法，显著扩展了该问题的可解范围。", "keywords": "最小最大后悔, k-汇点选址, 动态树网络, 统一容量, 疏散问题", "comments": "这项工作将鲁棒优化问题（最小最大后悔）在动态树网络中的可解性从特定情况（k=1）扩展到一般情况（任意k），这是网络优化和鲁棒设施选址领域的一个重要理论进展。"}}
{"id": "2503.17799", "title": "Relation Extraction with Instance-Adapted Predicate Descriptions", "authors": ["Yuhang Jiang", "Ramakanth Kavuluru"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This paper has been accepted to appear in the proceedings of AMIA 2025", "url": "http://arxiv.org/abs/2503.17799v2", "summary": "Relation extraction (RE) is a standard information extraction task playing a\nmajor role in downstream applications such as knowledge discovery and question\nanswering. Although decoder-only large language models are excelling in\ngenerative tasks, smaller encoder models are still the go to architecture for\nRE. In this paper, we revisit fine-tuning such smaller models using a novel\ndual-encoder architecture with a joint contrastive and cross-entropy loss.\nUnlike previous methods that employ a fixed linear layer for predicate\nrepresentations, our approach uses a second encoder to compute\ninstance-specific predicate representations by infusing them with real entity\nspans from corresponding input instances. We conducted experiments on two\nbiomedical RE datasets and two general domain datasets. Our approach achieved\nF1 score improvements ranging from 1% to 2% over state-of-the-art methods with\na simple but elegant formulation. Ablation studies justify the importance of\nvarious components built into the proposed architecture.", "comment": "This paper has been accepted to appear in the proceedings of AMIA\n  2025", "pdf_url": "http://arxiv.org/pdf/2503.17799v2", "cate": "cs.CL", "date": "2025-03-22", "updated": "2025-07-25", "AI": {"title_translation": "关系抽取与实例自适应谓词描述", "tldr": "本文提出一种新的双编码器架构，通过实例特定的谓词表示来改进关系抽取，并在多个数据集上取得了1%-2%的F1分数提升。", "motivation": "关系抽取(RE)是信息抽取中的一个重要任务，对知识发现和问答等下游应用至关重要。尽管解码器模型在生成任务中表现出色，但小型编码器模型仍是RE的首选架构，本文旨在改进这类模型的微调方法。", "method": "本文提出了一种新颖的双编码器架构，结合了对比损失和交叉熵损失进行联合训练，以微调小型编码器模型。与以往使用固定线性层表示谓词的方法不同，该方法使用第二个编码器通过融入输入实例中的真实实体跨度来计算实例特定的谓词表示。", "result": "该方法在两个生物医学RE数据集和两个通用领域数据集上进行了实验，与最先进的方法相比，F1分数提高了1%至2%。消融研究也证明了所提出架构中各个组件的重要性。", "conclusion": "本文提出了一种简单而优雅的实例自适应谓词描述方法，通过双编码器架构显著提升了关系抽取的性能，并在多个数据集上优于现有SOTA方法。", "translation": "关系抽取（RE）是一个标准的信息抽取任务，在知识发现和问答等下游应用中扮演着重要角色。尽管仅解码器的大型语言模型在生成任务中表现出色，但较小的编码器模型仍然是关系抽取的首选架构。在本文中，我们重新审视了使用一种新颖的双编码器架构以及联合对比损失和交叉熵损失来微调这类较小模型的方法。与以往采用固定线性层进行谓词表示的方法不同，我们的方法使用第二个编码器，通过将真实实体跨度从相应的输入实例中融入，来计算实例特定的谓词表示。我们在两个生物医学关系抽取数据集和两个通用领域数据集上进行了实验。我们的方法以一种简单而优雅的公式，在F1分数上比最先进的方法提高了1%到2%。消融研究证明了所提出架构中构建的各种组件的重要性。", "summary": "本文针对关系抽取任务，提出了一种新颖的双编码器架构，用于微调小型编码器模型。该方法通过引入第二个编码器来生成实例特定的谓词表示，克服了传统方法中使用固定谓词表示的局限性。实验结果表明，该方法在多个生物医学和通用领域数据集上，F1分数相比现有SOTA方法有1%到2%的提升，并通过消融研究验证了其组件的有效性。", "keywords": "关系抽取, 实例自适应谓词, 双编码器, 对比学习, 实体跨度", "comments": "本文的创新点在于提出了实例自适应的谓词描述方法，通过双编码器架构和结合实体跨度来生成动态的谓词表示，这比传统固定表示方法更具灵活性和表达力。其简单而优雅的公式以及在SOTA基础上的性能提升，证明了该方法的有效性和实用价值，尤其是在特定领域如生物医学关系抽取中可能具有重要意义。"}}
{"id": "2507.18979", "title": "Frequency Response Data-Driven Disturbance Observer Design for Flexible Joint Robots", "authors": ["Deokjin Lee", "Junho Song", "Alireza Karimi", "Sehoon Oh"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18979v1", "summary": "Motion control of flexible joint robots (FJR) is challenged by inherent\nflexibility and configuration-dependent variations in system dynamics. While\ndisturbance observers (DOB) can enhance system robustness, their performance is\noften limited by the elasticity of the joints and the variations in system\nparameters, which leads to a conservative design of the DOB. This paper\npresents a novel frequency response function (FRF)-based optimization method\naimed at improving DOB performance, even in the presence of flexibility and\nsystem variability. The proposed method maximizes control bandwidth and\neffectively suppresses vibrations, thus enhancing overall system performance.\nClosed-loop stability is rigorously proven using the Nyquist stability\ncriterion. Experimental validation on a FJR demonstrates that the proposed\napproach significantly improves robustness and motion performance, even under\nconditions of joint flexibility and system variation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18979v1", "cate": "cs.RO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "柔性关节机器人频率响应数据驱动扰动观测器设计", "tldr": "本文提出了一种基于频率响应函数(FRF)优化的新型扰动观测器(DOB)设计方法，旨在提高柔性关节机器人(FJR)在存在柔性和系统变异性情况下的控制性能，并通过实验验证了其对鲁棒性和运动性能的显著改善。", "motivation": "柔性关节机器人(FJR)的运动控制面临固有柔性和系统动力学配置依赖性变化的挑战。传统的扰动观测器(DOB)性能常受关节弹性和系统参数变化限制，导致设计保守。因此，需要一种新的方法来提升DOB在这些条件下的性能。", "method": "本文提出了一种新颖的基于频率响应函数(FRF)的优化方法，旨在提高扰动观测器(DOB)的性能，即使在存在柔性和系统变异性的情况下。该方法通过最大化控制带宽并有效抑制振动来增强系统整体性能。闭环稳定性通过奈奎斯特稳定性判据得到严格证明。", "result": "所提出的方法能够最大化控制带宽并有效抑制振动，从而提高整体系统性能。在柔性关节机器人上的实验验证表明，即使在关节柔性和系统变化条件下，该方法也能显著提高鲁棒性和运动性能。", "conclusion": "本文提出的基于频率响应函数(FRF)优化的扰动观测器设计方法，能够有效应对柔性关节机器人的固有柔性和系统参数变化带来的挑战，显著提升了机器人的鲁棒性和运动性能。", "translation": "柔性关节机器人(FJR)的运动控制受到固有柔性和系统动力学配置依赖性变化的挑战。虽然扰动观测器(DOB)可以增强系统鲁棒性，但其性能常受关节弹性和系统参数变化的限制，这导致了扰动观测器设计的保守性。本文提出了一种新颖的基于频率响应函数(FRF)的优化方法，旨在提高扰动观测器的性能，即使在存在柔性和系统变异性的情况下。所提出的方法能够最大化控制带宽并有效抑制振动，从而提高整体系统性能。闭环稳定性通过奈奎斯特稳定性判据得到严格证明。在柔性关节机器人上的实验验证表明，即使在关节柔性和系统变化条件下，所提出的方法也能显著提高鲁棒性和运动性能。", "summary": "本文针对柔性关节机器人(FJR)在柔性和系统变异性下的运动控制挑战，提出了一种基于频率响应函数(FRF)优化的新型扰动观测器(DOB)设计方法。该方法旨在克服传统DOB受关节弹性和参数变化限制的问题，通过最大化控制带宽和抑制振动来提升系统性能。研究通过奈奎斯特稳定性判据证明了闭环稳定性，并在FJR上进行了实验验证，结果显示所提方法显著改善了机器人的鲁棒性和运动性能。", "keywords": "柔性关节机器人, 扰动观测器, 频率响应函数, 鲁棒性, 运动控制", "comments": "该论文的创新点在于提出了一个基于频率响应函数(FRF)的数据驱动优化方法来设计扰动观测器(DOB)，有效解决了柔性关节机器人(FJR)在面对固有柔性和系统参数变化时控制性能受限的问题。这种方法通过直接利用频率响应数据进行优化，避免了传统模型驱动方法中对精确模型的需求，具有较强的实用价值和工程意义。实验验证也充分展示了其在提升鲁棒性和运动性能方面的有效性。"}}
{"id": "2507.18870", "title": "Transferable and Undefendable Point Cloud Attacks via Medial Axis Transform", "authors": ["Keke Tang", "Yuze Gao", "Weilong Peng", "Xiaofei Wang", "Meie Fang", "Peican Zhu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18870v1", "summary": "Studying adversarial attacks on point clouds is essential for evaluating and\nimproving the robustness of 3D deep learning models. However, most existing\nattack methods are developed under ideal white-box settings and often suffer\nfrom limited transferability to unseen models and insufficient robustness\nagainst common defense mechanisms. In this paper, we propose MAT-Adv, a novel\nadversarial attack framework that enhances both transferability and\nundefendability by explicitly perturbing the medial axis transform (MAT)\nrepresentations, in order to induce inherent adversarialness in the resulting\npoint clouds. Specifically, we employ an autoencoder to project input point\nclouds into compact MAT representations that capture the intrinsic geometric\nstructure of point clouds. By perturbing these intrinsic representations,\nMAT-Adv introduces structural-level adversarial characteristics that remain\neffective across diverse models and defense strategies. To mitigate overfitting\nand prevent perturbation collapse, we incorporate a dropout strategy into the\noptimization of MAT perturbations, further improving transferability and\nundefendability. Extensive experiments demonstrate that MAT-Adv significantly\noutperforms existing state-of-the-art methods in both transferability and\nundefendability. Codes will be made public upon paper acceptance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18870v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "通过中轴变换实现可迁移且不可防御的点云攻击", "tldr": "本文提出了MAT-Adv，一种新颖的点云对抗性攻击框架，通过扰动中轴变换（MAT）表示来生成可迁移且不可防御的对抗样本，实验证明其性能优于现有SOTA方法。", "motivation": "研究点云上的对抗性攻击对于评估和改进3D深度学习模型的鲁棒性至关重要。然而，大多数现有攻击方法是在理想的白盒设置下开发的，并且通常在对未见模型的迁移性以及对常见防御机制的鲁棒性方面存在局限性。", "method": "本文提出了MAT-Adv，一个新颖的对抗性攻击框架。它采用自编码器将输入点云投影到紧凑的中轴变换（MAT）表示中。通过扰动这些内在的MAT表示，MAT-Adv引入了结构级别的对抗性特征。为了减轻过拟合并防止扰动崩溃，还在MAT扰动的优化中结合了dropout策略。", "result": "广泛的实验表明，MAT-Adv在可迁移性和不可防御性方面显著优于现有的最先进方法。", "conclusion": "MAT-Adv通过扰动中轴变换（MAT）表示，成功地增强了点云对抗性攻击的可迁移性和不可防御性，克服了现有方法的局限性，从而有效地评估和改进了3D深度学习模型的鲁棒性。", "translation": "研究点云上的对抗性攻击对于评估和改进3D深度学习模型的鲁棒性至关重要。然而，大多数现有的攻击方法是在理想的白盒设置下开发的，并且通常在对未见模型的迁移性以及对常见防御机制的鲁棒性方面存在局限性。在本文中，我们提出了MAT-Adv，一种新颖的对抗性攻击框架，通过显式扰动中轴变换（MAT）表示来增强可迁移性和不可防御性，从而在生成的点云中引入固有的对抗性。具体来说，我们采用自编码器将输入点云投影到紧凑的MAT表示中，这些表示捕获了点云的内在几何结构。通过扰动这些内在表示，MAT-Adv引入了结构级别的对抗性特征，这些特征在不同的模型和防御策略中都保持有效。为了减轻过拟合并防止扰动崩溃，我们在MAT扰动的优化中结合了dropout策略，进一步提高了可迁移性和不可防御性。广泛的实验表明，MAT-Adv在可迁移性和不可防御性方面显著优于现有的最先进方法。代码将在论文接收后公开。", "summary": "本文介绍了MAT-Adv，一个针对点云3D深度学习模型的新型对抗性攻击框架。为解决现有方法在迁移性和可防御性上的局限性，MAT-Adv通过扰动点云的内在中轴变换（MAT）表示来引入结构级别的对抗性特征。该方法利用自编码器进行MAT投影，并在优化过程中融入dropout策略。实验证明，MAT-Adv在可迁移性和不可防御性方面显著超越了现有最先进方法。", "keywords": "点云攻击, 对抗样本, 中轴变换, 可迁移性, 不可防御性", "comments": "本文的创新之处在于通过扰动点云的内在几何结构（中轴变换表示）来生成对抗样本，而非直接扰动点云本身。这种方法有效地增强了攻击的可迁移性和对防御机制的鲁棒性。这项研究对于深入理解和提升3D深度学习模型的鲁棒性具有重要意义。"}}
{"id": "2507.19368", "title": "Counterfactual Explanations in Medical Imaging: Exploring SPN-Guided Latent Space Manipulation", "authors": ["Julia Siekiera", "Stefan Kramer"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages, 3 figures", "url": "http://arxiv.org/abs/2507.19368v1", "summary": "Artificial intelligence is increasingly leveraged across various domains to\nautomate decision-making processes that significantly impact human lives. In\nmedical image analysis, deep learning models have demonstrated remarkable\nperformance. However, their inherent complexity makes them black box systems,\nraising concerns about reliability and interpretability. Counterfactual\nexplanations provide comprehensible insights into decision processes by\npresenting hypothetical \"what-if\" scenarios that alter model classifications.\nBy examining input alterations, counterfactual explanations provide patterns\nthat influence the decision-making process. Despite their potential, generating\nplausible counterfactuals that adhere to similarity constraints providing\nhuman-interpretable explanations remains a challenge. In this paper, we\ninvestigate this challenge by a model-specific optimization approach. While\ndeep generative models such as variational autoencoders (VAEs) exhibit\nsignificant generative power, probabilistic models like sum-product networks\n(SPNs) efficiently represent complex joint probability distributions. By\nmodeling the likelihood of a semi-supervised VAE's latent space with an SPN, we\nleverage its dual role as both a latent space descriptor and a classifier for a\ngiven discrimination task. This formulation enables the optimization of latent\nspace counterfactuals that are both close to the original data distribution and\naligned with the target class distribution. We conduct experimental evaluation\non the cheXpert dataset. To evaluate the effectiveness of the integration of\nSPNs, our SPN-guided latent space manipulation is compared against a neural\nnetwork baseline. Additionally, the trade-off between latent variable\nregularization and counterfactual quality is analyzed.", "comment": "10 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.19368v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "医疗影像中的反事实解释：探索SPN引导的潜在空间操作", "tldr": "本文提出一种通过SPN引导的潜在空间操作方法，为医学影像中的深度学习模型生成可解释的反事实解释，以解决黑箱问题。", "motivation": "深度学习模型在医疗影像分析中表现出色，但其固有的复杂性使其成为“黑箱”系统，引发了对可靠性和可解释性的担忧。生成符合相似性约束并提供人类可解释的反事实解释仍然是一个挑战。", "method": "本文提出一种模型特定的优化方法。通过使用求和积网络（SPN）对半监督变分自编码器（VAE）的潜在空间似然进行建模，并利用SPN作为潜在空间描述符和给定判别任务的分类器双重角色。这种方法能够优化既接近原始数据分布又与目标类别分布对齐的潜在空间反事实。", "result": "在cheXpert数据集上进行了实验评估。将SPN引导的潜在空间操作与神经网络基线进行了比较，以评估SPN集成的有效性。此外，还分析了潜在变量正则化与反事实质量之间的权衡。结果表明该方法能够优化生成接近原始数据分布并与目标类别分布对齐的潜在空间反事实。", "conclusion": "本文提出了一种有效的方法，通过SPN引导的潜在空间操作来生成医疗影像中深度学习模型的可解释反事实解释，从而解决了黑箱系统的可靠性和可解释性问题。", "translation": "人工智能在各个领域日益被用于自动化决策过程，这些过程对人类生活产生重大影响。在医疗图像分析中，深度学习模型表现出卓越的性能。然而，它们固有的复杂性使它们成为黑箱系统，引发了对可靠性和可解释性的担忧。反事实解释通过呈现改变模型分类的假设“如果-那么”情景，提供了对决策过程的理解性洞察。通过检查输入改变，反事实解释提供了影响决策过程的模式。尽管其潜力巨大，但生成符合相似性约束并提供人类可解释的反事实仍然是一个挑战。在本文中，我们通过一种模型特定的优化方法来研究这一挑战。虽然变分自编码器（VAE）等深度生成模型表现出显著的生成能力，但求和积网络（SPN）等概率模型能有效地表示复杂的联合概率分布。通过使用SPN对半监督VAE的潜在空间似然进行建模，我们利用其作为潜在空间描述符和给定判别任务的分类器的双重作用。这种公式使得能够优化既接近原始数据分布又与目标类别分布对齐的潜在空间反事实。我们在cheXpert数据集上进行了实验评估。为了评估SPN集成的有效性，我们将SPN引导的潜在空间操作与神经网络基线进行了比较。此外，还分析了潜在变量正则化与反事实质量之间的权衡。", "summary": "本文旨在解决医疗影像中深度学习模型缺乏可解释性的问题。作者提出一种新颖的模型特定优化方法，利用求和积网络（SPN）对半监督变分自编码器（VAE）的潜在空间似然进行建模。SPN在此作为潜在空间描述符和分类器，使得能够生成既接近原始数据又符合目标类别分布的反事实解释。在cheXpert数据集上的实验验证了该方法，并与神经网络基线进行了比较，同时分析了正则化与反事实质量的权衡。", "keywords": "反事实解释, 医疗影像, 求和积网络 (SPN), 变分自编码器 (VAE), 潜在空间操作", "comments": "本文的创新点在于将SPN与VAE结合，利用SPN在建模复杂概率分布和作为分类器方面的优势，来优化生成高质量且可解释的潜在空间反事实。这对于提高医疗影像AI决策的透明度和可信度具有重要意义。"}}
{"id": "2502.13831", "title": "Linearized Localized Orthogonal Decomposition for Quasilinear Nonmonotone Elliptic PDE", "authors": ["Maher Khrais", "Barbara Verfürth"], "categories": ["math.NA", "cs.NA", "65N30, 65N15, 35J60, 65J15, 65N12"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.13831v2", "summary": "In this paper, we propose and analyze a multiscale method for a class of\nquasilinear elliptic problems of nonmonotone type with spatially multiscale\ncoefficient. The numerical approach is inspired by the Localized Orthogonal\nDecomposition (LOD), so that we do not require structural assumptions such as\nperiodicity or scale separation and only need minimal regularity assumptions on\nthe coefficient.To construct the multiscale space, we solve linear fine-scale\nproblems on small local subdomains, for which we consider two different\nlinearization techniques. For both, we present a rigorous well-posedness\nanalysis and convergence estimates in the $H^1$-semi norm. We compare and\ndiscuss theoretically and numerically the performance of our strategies for\ndifferent linearization points. Numerical experiments underline the theoretical\nfindings and illustrate the applicability of the method.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.13831v2", "cate": "math.NA", "date": "2025-02-19", "updated": "2025-07-25", "AI": {"title_translation": "拟线性非单调椭圆偏微分方程的线性化局部正交分解", "tldr": "本文提出并分析了一种基于局部正交分解（LOD）的多尺度方法，用于求解具有空间多尺度系数的拟线性非单调椭圆问题，并给出了适定性分析和收敛性估计。", "motivation": "本文旨在为具有空间多尺度系数的拟线性非单调椭圆问题提出并分析一种多尺度方法，该方法不需要周期性或尺度分离等结构假设，且仅需对系数进行最小的正则性假设。", "method": "本文提出了一种受局部正交分解（LOD）启发的数值方法。该方法通过在小的局部子域上求解线性精细尺度问题来构建多尺度空间，并为此考虑了两种不同的线性化技术。对这两种技术，都进行了严格的适定性分析和H1半范数下的收敛性估计。", "result": "文章对两种线性化技术都提出了严格的适定性分析和在H1半范数下的收敛性估计。从理论和数值上比较并讨论了不同线性化点下策略的性能。数值实验证实了理论发现并说明了该方法的适用性。", "conclusion": "本文提出的线性化局部正交分解方法对于具有多尺度系数的拟线性非单调椭圆偏微分方程是适定的、收敛的且适用。", "translation": "本文提出并分析了一种用于具有空间多尺度系数的拟线性非单调椭圆问题多尺度方法。该数值方法受局部正交分解（LOD）的启发，因此我们不需要周期性或尺度分离等结构假设，而只需要对系数进行最少的正则性假设。为了构建多尺度空间，我们在小局部子域上求解线性精细尺度问题，为此我们考虑了两种不同的线性化技术。对于这两种技术，我们都提供了严格的适定性分析和H1半范数下的收敛性估计。我们从理论和数值上比较并讨论了我们策略在不同线性化点下的性能。数值实验证实了理论发现并说明了该方法的适用性。", "summary": "本文提出并分析了一种基于局部正交分解（LOD）的多尺度方法，用于解决具有空间多尺度系数的拟线性非单调椭圆偏微分方程。该方法避免了强结构假设，仅需最小的系数正则性。通过在局部子域上求解线性精细尺度问题，并采用两种线性化技术来构建多尺度空间。论文提供了严格的适定性分析和H1半范数下的收敛性估计，并进行了理论和数值比较。数值实验验证了理论结果并展示了方法的适用性。", "keywords": "局部正交分解, 拟线性椭圆偏微分方程, 多尺度方法, 非单调, 线性化", "comments": "本文的创新之处在于将局部正交分解（LOD）应用于拟线性非单调椭圆偏微分方程，这类问题由于其非线性和非单调性，尤其是在存在多尺度系数时，通常具有挑战性。该方法能够避免周期性或尺度分离等强结构假设，并且仅要求最小的正则性，这显著拓宽了其适用范围。严谨的理论分析与数值验证相结合，进一步增强了本文的贡献。"}}
{"id": "2507.18902", "title": "SLoW: Select Low-frequency Words! Automatic Dictionary Selection for Translation on Large Language Models", "authors": ["Hongyuan Lu", "Zixuan Li", "Zefan Zhang", "Wai Lam"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18902v1", "summary": "There are more than 7,000 languages around the world, and current Large\nLanguage Models (LLMs) only support hundreds of languages. Dictionary-based\nprompting methods can enhance translation on them, but most methods use all the\navailable dictionaries, which could be expensive. Instead, it will be flexible\nto have a trade-off between token consumption and translation performance. This\npaper proposes a novel task called \\textbf{A}utomatic \\textbf{D}ictionary\n\\textbf{S}election (\\textbf{ADS}). The goal of the task is to automatically\nselect which dictionary to use to enhance translation. We propose a novel and\neffective method which we call \\textbf{S}elect \\textbf{Lo}w-frequency\n\\textbf{W}ords! (\\textbf{SLoW}) which selects those dictionaries that have a\nlower frequency. Our methods have unique advantages. First, there is no need\nfor access to the training data for frequency estimation (which is usually\nunavailable). Second, it inherits the advantage of dictionary-based methods,\nwhere no additional tuning is required on LLMs. Experimental results on 100\nlanguages from FLORES indicate that SLoW surpasses strong baselines, and it can\nobviously save token usage, with many languages even surpassing the translation\nperformance of the full dictionary baseline.\\footnote{A shocking fact is that\nthere is no need to use the actual training data (often unobtainable) for\nfrequency estimation, and an estimation frequency obtained using public\nresources is still apparently effective in improving translation with ChatGPT\nand Llama, and DeepSeek.}\\footnote{Code and data available upon publication.}", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18902v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "SLoW：选择低频词！大型语言模型翻译的自动词典选择", "tldr": "SLoW提出了一种自动词典选择（ADS）方法，通过选择低频词典来提高大型语言模型的翻译性能并节省token消耗，无需访问训练数据，且在100种语言上表现优于基线并节省了token。", "motivation": "当前大型语言模型（LLMs）仅支持数百种语言，而全球有7000多种语言。基于词典的提示方法可以增强LLMs的翻译能力，但大多数方法使用所有可用词典，这可能导致高昂的成本。因此，需要在token消耗和翻译性能之间找到一个平衡点。", "method": "本文提出了一个新任务——自动词典选择（ADS），旨在自动选择用于增强翻译的词典。为此，提出了一种名为SLoW（Select Low-frequency Words!）的新颖有效方法，该方法选择包含低频词的词典。SLoW的独特优势包括：无需访问训练数据进行频率估计（通常不可用），以及继承了基于词典方法的优势，即无需对LLMs进行额外微调。", "result": "在FLORES数据集上对100种语言进行的实验结果表明，SLoW超越了强大的基线方法，并能显著节省token使用量，许多语言的翻译性能甚至超过了使用完整词典的基线。此外，无需使用实际训练数据（通常无法获取）进行频率估计，使用公共资源获得的估计频率在改进ChatGPT、Llama和DeepSeek的翻译方面仍然明显有效。", "conclusion": "SLoW方法通过选择低频词典，在不进行LLM微调且无需训练数据的情况下，有效地实现了大型语言模型翻译的自动词典选择，显著节省了token并提升了翻译性能，甚至在许多情况下超越了使用完整词典的基线。", "translation": "全球有超过7000种语言，而当前的大型语言模型（LLMs）仅支持数百种语言。基于词典的提示方法可以增强LLMs的翻译能力，但大多数方法使用所有可用词典，这可能会非常昂贵。相反，如果在token消耗和翻译性能之间进行权衡，将会更加灵活。本文提出了一个新颖的任务，称为自动词典选择（ADS）。该任务的目标是自动选择使用哪个词典来增强翻译。我们提出了一种新颖且有效的方法，我们称之为SLoW（选择低频词！），它选择那些频率较低的词典。我们的方法具有独特的优势。首先，无需访问用于频率估计的训练数据（这通常是不可用的）。其次，它继承了基于词典方法的优势，即无需对LLMs进行额外微调。在FLORES数据集上对100种语言进行的实验结果表明，SLoW超越了强大的基线方法，并且可以明显节省token使用量，许多语言甚至超过了完整词典基线的翻译性能。\n一个令人震惊的事实是，无需使用实际训练数据（通常无法获取）进行频率估计，使用公共资源获得的估计频率在改进ChatGPT、Llama和DeepSeek的翻译方面仍然明显有效。代码和数据将在发布时提供。", "summary": "本文提出了一种名为自动词典选择（ADS）的新任务，旨在为大型语言模型（LLMs）的翻译任务自动选择合适的词典，以平衡token消耗和翻译性能。为此，研究者开发了SLoW（选择低频词！）方法，该方法通过选择包含低频词的词典来提高翻译效果。SLoW的优势在于无需访问训练数据进行频率估计，也无需对LLMs进行额外微调。实验结果表明，SLoW在100种语言上优于现有基线，显著节省了token使用，并且在许多情况下甚至超越了使用完整词典的翻译性能。", "keywords": "自动词典选择, 低频词, 大型语言模型, 机器翻译, token节省", "comments": "SLoW方法的创新之处在于提出了自动词典选择（ADS）这一新任务，并提供了一种无需训练数据和LLM微调的有效解决方案。其核心思想是选择低频词典，这既能节省计算资源（token），又能提高翻译质量，甚至在某些情况下超越使用完整词典的效果，这对于资源有限的低资源语言翻译尤其重要。这种方法为LLMs在多语言翻译领域提供了新的思路，具有较高的实用价值和研究潜力。"}}
{"id": "2507.18671", "title": "Innovator: Scientific Continued Pretraining with Fine-grained MoE Upcycling", "authors": ["Ning Liao", "Xiaoxing Wang", "Zehao Lin", "Weiyang Guo", "Feng Hong", "Shixiang Song", "Geng Yu", "Zihua Zhao", "Sitao Xie", "Longxuan Wei", "Xiangqi Jin", "Xiaohan Qin", "Jiale Ma", "Kai Chen", "Jiangchao Yao", "Zhouhan Lin", "Junchi Yan", "Zhiyu Li", "Feiyu Xiong", "Yanfeng Wang", "Linfeng Zhang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Technical Report", "url": "http://arxiv.org/abs/2507.18671v1", "summary": "A large language model (LLM) with knowledge in both scientific and general\ntasks is the foundation of science general intelligence. However, directly\ncontinued pretraining an LLM using science data usually leads to catastrophic\nforgetting, which indicates severe degradation in general ability. In this\nreport, we present Innovator, which solves this problem by upcycling a\npre-trained dense LLM into a fine-grained Mixtures-of-Experts model during\ncontinued pretraining, where different experts are expected to learn science\nknowledge in different disciplines, and a shared expert is utilized for general\ntasks. Innovator introduces a four-stage upcycle training paradigm: (1)\nScientific Expert Induction on discipline-specific data, (2) Fine-grained\nExpert Splitting via FFN dimension decomposition, (3) Science-Aware Routing\nwarmup, and (4) Generalist-Scientist Integration training on hybrid datasets.\nSuch a paradigm enables knowledge in the general domain, and different\nscientific disciplines can be decoupled, avoiding the negative influence among\nknowledge in different domains. With 53.3B total parameters and 13.3B\nactivated, Innovator extends Qwen2.5-7B using a shared general expert and 64\nspecialized scientific experts with 8 activated. Trained on 300B tokens with\ntri-level quality-controlled data, Innovator achieves 25% average improvement\nacross 30 scientific tasks with a win rate as 70%, while retaining 99%\nperformance in general tasks. Furthermore, Innovator-Reason, which is\npost-trained from Innovator for reasoning boosting, exhibits excellent\nreasoning performance in solving complex scientific problems with improvements\nover 30%.", "comment": "Technical Report", "pdf_url": "http://arxiv.org/pdf/2507.18671v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "Innovator：基于细粒度MoE升级的科学领域持续预训练", "tldr": "Innovator通过将预训练的LLM升级为细粒度MoE模型，解决了科学领域持续预训练中的灾难性遗忘问题，实现了通用能力和科学知识的兼顾。", "motivation": "大型语言模型（LLM）在科学和通用任务中兼具知识是实现科学通用智能的基础。然而，直接使用科学数据对LLM进行持续预训练通常会导致灾难性遗忘，即通用能力严重下降。", "method": "本文提出了Innovator模型，通过在持续预训练过程中将预训练的密集型LLM升级为细粒度混合专家（MoE）模型来解决此问题。Innovator引入了一个四阶段的升级训练范式：(1) 在特定学科数据上进行科学专家诱导，(2) 通过FFN维度分解进行细粒度专家拆分，(3) 科学感知路由预热，以及(4) 在混合数据集上进行通用-科学家整合训练。该范式使得通用领域和不同科学学科的知识可以解耦，避免了不同领域知识间的负面影响。Innovator基于Qwen2.5-7B扩展，总参数53.3B，激活参数13.3B，包含一个共享通用专家和64个专业科学专家（激活8个）。模型在3000亿tokens的三级质量控制数据上进行训练。", "result": "Innovator在30个科学任务上平均提升了25%，胜率为70%，同时在通用任务上保持了99%的性能。此外，从Innovator进行推理增强后训练得到的Innovator-Reason在解决复杂科学问题方面表现出卓越的推理性能，提升超过30%。", "conclusion": "Innovator通过其独特的MoE升级和四阶段训练范式，成功解决了科学领域持续预训练中的灾难性遗忘问题，实现了科学知识学习与通用能力保持的平衡，并展现了出色的科学推理能力。", "translation": "一个在科学和通用任务中都拥有知识的大型语言模型（LLM）是科学通用智能的基础。然而，直接使用科学数据对LLM进行持续预训练通常会导致灾难性遗忘，这表明通用能力严重下降。在本报告中，我们提出了Innovator，它通过在持续预训练期间将预训练的密集型LLM升级为细粒度混合专家（MoE）模型来解决这个问题，其中不同的专家预计将学习不同学科的科学知识，并利用一个共享专家来处理通用任务。Innovator引入了一个四阶段的升级训练范式：(1) 在学科特定数据上进行科学专家诱导，(2) 通过FFN维度分解进行细粒度专家拆分，(3) 科学感知路由预热，以及(4) 在混合数据集上进行通用-科学家整合训练。这种范式使得通用领域和不同科学学科的知识可以解耦，避免了不同领域知识之间的负面影响。Innovator拥有53.3B的总参数和13.3B的激活参数，它通过一个共享通用专家和64个专业科学专家（激活8个）扩展了Qwen2.5-7B。Innovator在3000亿tokens的三级质量控制数据上进行训练，在30个科学任务上平均提升了25%，胜率为70%，同时在通用任务上保持了99%的性能。此外，从Innovator进行推理增强后训练得到的Innovator-Reason在解决复杂科学问题方面表现出卓越的推理性能，提升超过30%。", "summary": "本文提出了Innovator，一个通过将预训练的密集型LLM升级为细粒度混合专家（MoE）模型，解决科学领域持续预训练中灾难性遗忘问题的模型。Innovator采用四阶段训练范式，使通用和科学知识得以解耦。该模型基于Qwen2.5-7B扩展，总参数53.3B，激活参数13.3B，在30个科学任务上实现了25%的平均提升和70%的胜率，同时通用任务性能保持99%。其推理增强版本Innovator-Reason在复杂科学问题上推理性能提升超过30%。", "keywords": "大语言模型, 持续预训练, 混合专家模型, 灾难性遗忘, 科学智能", "comments": "Innovator的创新点在于其独特的MoE升级范式，通过解耦通用知识和特定科学领域的知识，有效避免了持续预训练中的灾难性遗忘问题。这种方法对于构建能够同时胜任通用和专业领域任务的LLM具有重要意义，尤其是在科学智能领域。其在保持通用能力的同时显著提升科学任务表现，并进一步通过推理增强版本展示了解决复杂科学问题的潜力，为未来科学LLM的发展提供了新的思路。"}}
{"id": "2507.19432", "title": "Resolving Build Conflicts via Example-Based and Rule-Based Program Transformations", "authors": ["Sheikh Shadab Towqir", "Fei He", "Todd Mytkowicz", "Na Meng"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19432v1", "summary": "Merge conflicts often arise when developers integrate changes from different\nsoftware branches. The conflicts can result from overlapping edits in programs\n(i.e., textual conflicts) or cause build and test errors (i.e., build and test\nconflicts). They degrade software quality and hinder programmer productivity.\nWhile several tools detect build conflicts, few offer meaningful support for\nresolving cases like those caused by method removal. To overcome limitations of\nexisting tools, we introduce BUCOR (Build Conflict Resolver), a new conflict\nresolver. BUCOR first detects conflicts by comparing three versions related to\na merging scenario: base b, left l, and right r. To resolve conflicts, it\nemploys two complementary strategies: example-based transformation (BUCOR-E)\nand rule-based transformation (BUCOR-R). BUCOR-R applies predefined rules to\nhandle common, well-understood conflicts. BUCOR-E mines branch versions (l and\nr) for exemplar edits applied to fix related build errors. From these examples,\nit infers and generalizes program transformation patterns to resolve more\ncomplex conflicts.\n  We evaluated BUCOR on 88 real-world build conflicts spanning 21 distinct\nconflict types. BUCOR generated at least one solution for 65 cases and\ncorrectly resolved 43 conflicts. We observed that this hybrid\napproach--combining context-aware, example-based learning with structured,\nrule-based resolution--can effectively help resolve conflicts. Our research\nsheds light on future directions for more intelligent and automated merge\ntools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19432v1", "cate": "cs.SE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "通过基于示例和基于规则的程序转换解决构建冲突", "tldr": "BUCOR是一款新的工具，它结合了基于示例学习和基于规则转换的方法，旨在解决软件开发中常见的构建冲突，并取得了显著成效。", "motivation": "开发人员在集成不同软件分支的更改时，经常会遇到合并冲突，这些冲突可能是文本冲突，也可能导致构建和测试错误，从而降低软件质量并阻碍生产力。尽管现有工具可以检测构建冲突，但很少有工具能为解决例如方法删除等复杂情况提供有效的支持。", "method": "本文介绍了BUCOR（构建冲突解决器），一种新的冲突解决器。BUCOR首先通过比较基础版本b、左版本l和右版本r三个相关版本来检测冲突。为解决冲突，它采用了两种互补策略：基于示例的转换（BUCOR-E）和基于规则的转换（BUCOR-R）。BUCOR-R应用预定义规则处理常见且易于理解的冲突。BUCOR-E则从分支版本（l和r）中挖掘用于修复相关构建错误的示例编辑，并从中推断和泛化程序转换模式，以解决更复杂的冲突。", "result": "BUCOR在88个真实世界的构建冲突（涵盖21种不同冲突类型）上进行了评估。BUCOR为其中65个案例生成了至少一个解决方案，并正确解决了43个冲突。", "conclusion": "这种结合了上下文感知、基于示例的学习与结构化、基于规则的解决方案的混合方法，可以有效地帮助解决冲突。本研究为未来开发更智能、更自动化的合并工具指明了方向。", "translation": "合并冲突通常在开发人员整合来自不同软件分支的更改时出现。冲突可能源于程序中重叠的编辑（即文本冲突），或导致构建和测试错误（即构建和测试冲突）。它们会降低软件质量并阻碍程序员的生产力。虽然有几种工具可以检测构建冲突，但很少有工具能为解决诸如方法删除等情况提供有意义的支持。为了克服现有工具的局限性，我们引入了BUCOR（构建冲突解决器），一种新的冲突解决器。BUCOR首先通过比较与合并场景相关的三个版本来检测冲突：基础版本b、左版本l和右版本r。为了解决冲突，它采用了两种互补的策略：基于示例的转换（BUCOR-E）和基于规则的转换（BUCOR-R）。BUCOR-R应用预定义规则来处理常见、易于理解的冲突。BUCOR-E从分支版本（l和r）中挖掘用于修复相关构建错误的示例编辑。从这些示例中，它推断并推广程序转换模式，以解决更复杂的冲突。\n我们对BUCOR在88个真实世界的构建冲突（涵盖21种不同的冲突类型）上进行了评估。BUCOR为65个案例生成了至少一个解决方案，并正确解决了43个冲突。我们观察到，这种混合方法——结合了上下文感知、基于示例的学习与结构化、基于规则的解决方案——可以有效地帮助解决冲突。我们的研究为未来更智能、更自动化的合并工具指明了方向。", "summary": "本文介绍了一种名为BUCOR的新型工具，旨在解决软件集成过程中出现的构建冲突。为克服现有工具的局限性，BUCOR通过比较三个版本（基础、左、右）来检测冲突，并采用混合策略：利用基于规则的转换处理常见冲突，以及基于示例的学习来解决更复杂的冲突。BUCOR在88个真实世界冲突上的评估显示，它为65个案例提供了解决方案，并成功解决了43个冲突，证明了其组合策略在提升合并工具智能性方面的有效性。", "keywords": "构建冲突, 程序转换, 合并冲突, 冲突解决, 软件集成", "comments": "本文针对软件开发中普遍存在的构建冲突问题，提出了一种创新的混合方法。该方法结合了基于规则的通用性和基于示例的适应性，是其主要亮点。通过在真实世界冲突数据集上的实证评估，进一步增强了研究的可信度。其贡献在于超越了单纯的冲突检测，提供了具体的冲突解决支持，尤其对复杂案例的处理能力具有重要意义。"}}
{"id": "2507.19074", "title": "A Self-training Framework for Semi-supervised Pulmonary Vessel Segmentation and Its Application in COPD", "authors": ["Shuiqing Zhao", "Meihuan Wang", "Jiaxuan Xu", "Jie Feng", "Wei Qian", "Rongchang Chen", "Zhenyu Liang", "Shouliang Qi", "Yanan Wu"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19074v1", "summary": "Background: It is fundamental for accurate segmentation and quantification of\nthe pulmonary vessel, particularly smaller vessels, from computed tomography\n(CT) images in chronic obstructive pulmonary disease (COPD) patients.\nObjective: The aim of this study was to segment the pulmonary vasculature using\na semi-supervised method. Methods: In this study, a self-training framework is\nproposed by leveraging a teacher-student model for the segmentation of\npulmonary vessels. First, the high-quality annotations are acquired in the\nin-house data by an interactive way. Then, the model is trained in the\nsemi-supervised way. A fully supervised model is trained on a small set of\nlabeled CT images, yielding the teacher model. Following this, the teacher\nmodel is used to generate pseudo-labels for the unlabeled CT images, from which\nreliable ones are selected based on a certain strategy. The training of the\nstudent model involves these reliable pseudo-labels. This training process is\niteratively repeated until an optimal performance is achieved. Results:\nExtensive experiments are performed on non-enhanced CT scans of 125 COPD\npatients. Quantitative and qualitative analyses demonstrate that the proposed\nmethod, Semi2, significantly improves the precision of vessel segmentation by\n2.3%, achieving a precision of 90.3%. Further, quantitative analysis is\nconducted in the pulmonary vessel of COPD, providing insights into the\ndifferences in the pulmonary vessel across different severity of the disease.\nConclusion: The proposed method can not only improve the performance of\npulmonary vascular segmentation, but can also be applied in COPD analysis. The\ncode will be made available at\nhttps://github.com/wuyanan513/semi-supervised-learning-for-vessel-segmentation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19074v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "肺血管半监督分割的自训练框架及其在慢阻肺中的应用", "tldr": "本文提出了一种基于自训练框架的半监督肺血管分割方法，显著提高了慢阻肺患者肺血管分割的精度，并可用于慢阻肺分析。", "motivation": "准确分割和量化肺血管，特别是小血管，对于慢性阻塞性肺疾病（COPD）患者的CT图像分析至关重要。本研究旨在利用半监督方法分割肺血管系统。", "method": "提出了一种基于教师-学生模型的自训练框架。首先，通过交互方式获取高质量的内部数据标注。然后，在少量标记的CT图像上训练全监督模型作为教师模型。教师模型生成未标记CT图像的伪标签，并选择可靠的伪标签来训练学生模型。该训练过程迭代重复直至达到最佳性能。", "result": "在125名慢阻肺患者的非增强CT扫描上进行了广泛实验。结果显示，所提出的Semi2方法显著提高了血管分割精度2.3%，达到90.3%。此外，对慢阻肺肺血管进行了定量分析，揭示了不同疾病严重程度下肺血管的差异。", "conclusion": "所提出的方法不仅可以提高肺血管分割的性能，还可以应用于慢阻肺分析。", "translation": "背景：从计算机断层扫描（CT）图像中准确分割和量化肺血管，特别是较小的血管，对于慢性阻塞性肺疾病（COPD）患者至关重要。\n目的：本研究旨在利用半监督方法分割肺血管系统。\n方法：本研究提出了一种利用教师-学生模型进行肺血管分割的自训练框架。首先，通过交互方式在内部数据中获取高质量的标注。然后，模型以半监督方式进行训练。在少量标记的CT图像上训练一个全监督模型，作为教师模型。接着，教师模型用于为未标记的CT图像生成伪标签，并根据特定策略选择可靠的伪标签。学生模型的训练涉及这些可靠的伪标签。该训练过程迭代重复，直到达到最佳性能。\n结果：对125名COPD患者的非增强CT扫描进行了广泛实验。定量和定性分析表明，所提出的方法Semi2显著提高了血管分割精度2.3%，达到90.3%。此外，对COPD患者的肺血管进行了定量分析，深入了解了不同疾病严重程度下肺血管的差异。\n结论：所提出的方法不仅可以提高肺血管分割的性能，还可以应用于COPD分析。代码将在https://github.com/wuyanan513/semi-supervised-learning-for-vessel-segmentation 提供。", "summary": "本文提出了一种用于肺血管半监督分割的自训练框架Semi2，该框架利用教师-学生模型，通过迭代的伪标签生成和学生模型训练来提高分割精度。该方法在慢阻肺患者CT图像上实现了90.3%的血管分割精度，并可用于分析不同疾病严重程度下肺血管的差异，为慢阻肺的诊断和分析提供了新的工具。", "keywords": "肺血管分割, 半监督学习, 自训练框架, 教师-学生模型, 慢性阻塞性肺疾病 (COPD)", "comments": "这篇论文提出了一种创新的自训练框架，结合了教师-学生模型和迭代伪标签生成，有效解决了医学图像分割中标记数据稀缺的问题。其在慢阻肺肺血管分割中的应用，不仅提升了分割精度，还为疾病的定量分析提供了新的视角，具有重要的临床应用潜力。"}}
{"id": "2507.19033", "title": "SelfRACG: Enabling LLMs to Self-Express and Retrieve for Code Generation", "authors": ["Qian Dong", "Jia Chen", "Qingyao Ai", "Hongning Wang", "Haitao Li", "Yi Wu", "Yao Hu", "Yiqun Liu", "Shaoping Ma"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Tsinghua&Xiaohongshu", "url": "http://arxiv.org/abs/2507.19033v1", "summary": "Existing retrieval-augmented code generation (RACG) methods typically use an\nexternal retrieval module to fetch semantically similar code snippets used for\ngenerating subsequent fragments. However, even for consecutive code fragments,\nthe content often diverges due to logical progression, resulting in a content\ngap. This gap undermines the performance of current RACG methods, as\n\\textit{external} retrieval modules based on content matching fail to infer the\nspecific information need of LLMs to generate the next code fragment.\nTherefore, we propose \\textbf{SelfRACG}, a novel paradigm that enables large\nlanguage models (LLMs) to \\textbf{Self}-express their information needs to\nenhance \\textbf{RACG}. Specifically, SelfRACG includes an information need\nexpression module and a two-stage information need-guided training strategy,\nwhich encourages LLMs to express their information need. Extensive experiments\ndemonstrate that SelfRACG can retrieve external knowledge that better aligns\nwith the LLM's own information needs, resulting in superior generation\nperformance compared to vanilla RACG.", "comment": "Tsinghua&Xiaohongshu", "pdf_url": "http://arxiv.org/pdf/2507.19033v1", "cate": "cs.IR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "自我RACG：使大型语言模型能够自我表达和检索以进行代码生成", "tldr": "现有代码生成方法在检索时存在内容鸿沟，本文提出SelfRACG，让LLM自我表达信息需求，从而提高代码生成性能。", "motivation": "现有检索增强代码生成（RACG）方法中，外部检索模块因逻辑进展导致内容分歧，无法推断LLM生成下一个代码片段的特定信息需求，从而产生内容鸿沟并损害性能。", "method": "提出SelfRACG，一种新颖的范式，使大型语言模型（LLMs）能够自我表达其信息需求以增强RACG。SelfRACG包含一个信息需求表达模块和一种两阶段信息需求引导训练策略，鼓励LLMs表达其信息需求。", "result": "SelfRACG能够检索与LLM自身信息需求更一致的外部知识，从而产生优于传统RACG的生成性能。", "conclusion": "SelfRACG通过使LLMs自我表达信息需求，有效解决了RACG中存在的“内容鸿沟”问题，显著提升了代码生成效果。", "translation": "现有检索增强代码生成（RACG）方法通常使用外部检索模块来获取语义相似的代码片段，用于生成后续片段。然而，即使是连续的代码片段，内容也常常因逻辑进展而发散，导致内容鸿沟。这种鸿沟损害了当前RACG方法的性能，因为基于内容匹配的“外部”检索模块无法推断LLM生成下一个代码片段的特定信息需求。因此，我们提出了**SelfRACG**，一种新颖的范式，使大型语言模型（LLMs）能够**自我**表达其信息需求以增强**RACG**。具体来说，SelfRACG包括一个信息需求表达模块和一种两阶段信息需求引导训练策略，鼓励LLMs表达其信息需求。广泛的实验表明，SelfRACG可以检索与LLM自身信息需求更一致的外部知识，从而产生优于传统RACG的生成性能。", "summary": "本文提出SelfRACG，旨在解决现有检索增强代码生成（RACG）中因逻辑进展导致内容鸿沟的问题。SelfRACG通过引入信息需求表达模块和两阶段训练策略，使大型语言模型（LLMs）能够自我表达其信息需求。实验证明，该方法能更好地检索外部知识，并显著提升代码生成性能。", "keywords": "代码生成, 检索增强, 大型语言模型, 信息需求, 自我表达", "comments": "SelfRACG的创新之处在于将信息检索的“主动权”从外部模块转移到LLM自身，让LLM能够“自我表达”其信息需求，从而更精准地获取所需上下文。这有效地解决了传统RACG中因内容不匹配导致的性能瓶颈，为代码生成领域提供了一个新的视角和范式。"}}
{"id": "2507.09257", "title": "On Lattice Isomorphism Problems for Lattices from LCD Codes over Finite Rings", "authors": ["Yusaku Nishimura", "Katsuyuki Takashima", "Tsuyoshi Miezaki"], "categories": ["cs.IT", "math.CO", "math.IT", "Primary 11T71, Secondary 14G50"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      16 pages", "url": "http://arxiv.org/abs/2507.09257v2", "summary": "These days, post-quantum cryptography based on the lattice isomorphism\nproblem has been proposed. Ducas-Gibbons introduced the hull attack, which\nsolves the lattice isomorphism problem for lattices obtained by Construction A\nfrom an LCD code over a finite field. Using this attack, they showed that the\nlattice isomorphism problem for such lattices can be reduced to the lattice\nisomorphism problem with the trivial lattice $\\mathbb{Z}^n$ and the graph\nisomorphism problem. While the previous work by Ducas-Gibbons only considered\nlattices constructed by a code over a \\textit{finite field}, this paper\nconsiders lattices constructed by a code over a \\textit{finite ring}\n$\\mathbb{Z}/k\\mathbb{Z}$, which is a more general case. In particular, when $k$\nis odd, an odd prime power, or not divisible by $4$, we show that the lattice\nisomorphism problem can be reduced to the lattice isomorphism problem for\n$\\mathbb{Z}^n$ and the graph isomorphism problem.", "comment": "16 pages", "pdf_url": "http://arxiv.org/pdf/2507.09257v2", "cate": "cs.IT", "date": "2025-07-12", "updated": "2025-07-25", "AI": {"title_translation": "关于有限环上LCD码格的格同构问题", "tldr": "本文研究了从有限环上LCD码构造的格的格同构问题，并证明在特定条件下，该问题可以简化为与平凡格和图同构问题。", "motivation": "后量子密码学中基于格同构问题的方案已被提出。然而，之前的研究仅限于有限域上的格构造。本文的动机在于将这一研究推广到更一般的有限环情况。", "method": "本文考虑了从有限环$\\\\mathbb{Z}/k\\\\mathbb{Z}$上的LCD码构造的格，并研究了它们的格同构问题。", "result": "研究结果表明，当$k$为奇数、奇素数幂或不能被4整除时，该格同构问题可以被简化为平凡格$\\\\mathbb{Z}^n$的格同构问题和图同构问题。", "conclusion": "本文成功将格同构问题在特定条件下的简化结果推广到更一般的有限环上的LCD码格，扩展了相关理论在后量子密码学背景下的适用性。", "translation": "目前，基于格同构问题的后量子密码学已被提出。Ducas-Gibbons引入了船体攻击，解决了从有限域上的LCD码通过构造A获得的格的格同构问题。利用这种攻击，他们表明这类格的格同构问题可以简化为与平凡格$\\\\mathbb{Z}^n$的格同构问题和图同构问题。虽然Ducas-Gibbons之前的工作只考虑了由有限域上的码构造的格，但本文考虑了由有限环$\\\\mathbb{Z}/k\\\\mathbb{Z}$上的码构造的格，这是一种更普遍的情况。特别是，当$k$为奇数、奇素数幂或不能被4整除时，我们表明格同构问题可以简化为$\\\\mathbb{Z}^n$的格同构问题和图同构问题。", "summary": "本文旨在将格同构问题的研究从有限域扩展到更一般的有限环$\\\\mathbb{Z}/k\\\\mathbb{Z}$上的LCD码格。基于Ducas-Gibbons的早期工作，本研究发现，当$k$满足奇数、奇素数幂或不能被4整除的条件时，此类格的格同构问题可以被简化为与平凡格$\\\\mathbb{Z}^n$的格同构问题以及图同构问题。这为基于格的后量子密码学提供了更广泛的理论基础。", "keywords": "格同构问题, LCD码, 有限环, 后量子密码学, 图同构问题", "comments": "本文的创新之处在于将格同构问题的研究范围从有限域推广到了更一般的有限环，这对于理解和构建基于格的后量子密码学方案具有重要意义。它在特定条件下给出了格同构问题的简化路径，为后续研究提供了基础。文章清晰地指出了其工作的推广性。"}}
{"id": "2501.12612", "title": "T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation", "authors": ["Lijun Li", "Zhelun Shi", "Xuhao Hu", "Bowen Dong", "Yiran Qin", "Xihui Liu", "Lu Sheng", "Jing Shao"], "categories": ["cs.CL", "cs.CR"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted at CVPR 2025", "url": "http://arxiv.org/abs/2501.12612v3", "summary": "Text-to-image (T2I) models have rapidly advanced, enabling the generation of\nhigh-quality images from text prompts across various domains. However, these\nmodels present notable safety concerns, including the risk of generating\nharmful, biased, or private content. Current research on assessing T2I safety\nremains in its early stages. While some efforts have been made to evaluate\nmodels on specific safety dimensions, many critical risks remain unexplored. To\naddress this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I\nmodels across three key domains: toxicity, fairness, and bias. We build a\ndetailed hierarchy of 12 tasks and 44 categories based on these three domains,\nand meticulously collect 70K corresponding prompts. Based on this taxonomy and\nprompt set, we build a large-scale T2I dataset with 68K manually annotated\nimages and train an evaluator capable of detecting critical risks that previous\nwork has failed to identify, including risks that even ultra-large proprietary\nmodels like GPTs cannot correctly detect. We evaluate 12 prominent diffusion\nmodels on T2ISafety and reveal several concerns including persistent issues\nwith racial fairness, a tendency to generate toxic content, and significant\nvariation in privacy protection across the models, even with defense methods\nlike concept erasing. Data and evaluator are released under\nhttps://github.com/adwardlee/t2i_safety.", "comment": "Accepted at CVPR 2025", "pdf_url": "http://arxiv.org/pdf/2501.12612v3", "cate": "cs.CL", "date": "2025-01-22", "updated": "2025-07-25", "AI": {"title_translation": "T2ISafety：评估图像生成中公平性、毒性和隐私的基准", "tldr": "引入T2ISafety，一个包含12个任务、44个类别和7万个提示的基准，用于评估T2I模型的毒性、公平性和隐私性，揭示了主流模型中存在的持续问题。", "motivation": "当前文本到图像（T2I）模型存在显著的安全问题，包括生成有害、有偏见或私人内容的风险。现有T2I安全评估研究仍处于早期阶段，许多关键风险尚未被充分探索，存在评估空白。", "method": "引入T2ISafety安全基准，用于评估T2I模型在毒性、公平性和偏见三个关键领域。基于这三个领域，构建了包含12个任务和44个类别的详细层次结构，并收集了7万个相应提示。基于此分类法和提示集，构建了一个包含6.8万张手动标注图像的大规模T2I数据集，并训练了一个能够检测以前工作未能识别的关键风险（包括超大型专有模型也无法检测到的风险）的评估器。最后，使用T2ISafety评估了12个主流扩散模型。", "result": "对12个主流扩散模型进行评估后发现，存在持续的种族公平性问题、生成有害内容的倾向，以及即使采用概念擦除等防御方法，模型之间在隐私保护方面也存在显著差异。所训练的评估器能够检测到甚至超大型专有模型也未能正确识别的关键风险。", "conclusion": "T2ISafety提供了一个全面的基准，揭示了当前T2I模型在公平性、毒性和隐私方面的显著安全漏洞，表明即使现有防御机制也需要进一步改进。", "translation": "文本到图像（T2I）模型发展迅速，能够从文本提示生成各种领域的高质量图像。然而，这些模型存在显著的安全问题，包括生成有害、有偏见或私人内容的风险。当前评估T2I安全性的研究仍处于早期阶段。尽管已经做出了一些努力来评估特定安全维度上的模型，但许多关键风险仍未被探索。为了弥补这一空白，我们引入了T2ISafety，这是一个评估T2I模型在三个关键领域（毒性、公平性和偏见）的安全性基准。我们基于这三个领域建立了12个任务和44个类别的详细层次结构，并精心收集了7万个相应的提示。基于此分类法和提示集，我们构建了一个包含6.8万张手动标注图像的大规模T2I数据集，并训练了一个评估器，能够检测以前工作未能识别的关键风险，包括甚至像GPT这样超大型专有模型都无法正确检测到的风险。我们评估了12个主流扩散模型在T2ISafety上的表现，并揭示了几个问题，包括种族公平性的持续问题、生成有害内容的倾向，以及即使采用概念擦除等防御方法，模型之间在隐私保护方面也存在显著差异。数据和评估器已在https://github.com/adwardlee/t2i_safety 发布。", "summary": "本文介绍了T2ISafety，这是一个旨在全面评估文本到图像（T2I）模型在毒性、公平性和隐私等安全方面的基准。该基准包含12个任务和44个类别的详细层次结构、7万个提示，以及一个包含6.8万张手动标注图像的数据集，用于训练先进的风险评估器。对12个主流扩散模型使用T2ISafety进行评估后，揭示了显著问题，例如种族不公平、有害内容生成以及隐私保护不一致，强调了T2I模型需要改进安全机制。", "keywords": "T2I安全, 图像生成, 公平性, 毒性, 隐私", "comments": "T2ISafety通过其全面、结构化的方法来解决T2I安全问题，涵盖了多个维度（毒性、公平性、隐私），并构建了详细的层次结构和大规模标注数据集，具有创新性。它能够检测出甚至大型专有模型都未能识别的风险，这凸显了其重要性。该工作为未来研究更安全的T2I生成提供了宝贵的工具和数据集。"}}
{"id": "2507.19013", "title": "A Formalization of the Correctness of the Floodsub Protocol", "authors": ["Ankit Kumar", "Panagiotis Manolios"], "categories": ["cs.LO", "cs.NI"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      In Proceedings ACL2 2025, arXiv:2507.18567", "url": "http://arxiv.org/abs/2507.19013v1", "summary": "Floodsub is a simple, robust and popular peer-to-peer publish/subscribe\n(pubsub) protocol, where nodes can arbitrarily leave or join the network,\nsubscribe to or unsubscribe from topics and forward newly received messages to\nall of their neighbors, except the sender or the originating peer. To show the\ncorrectness of Floodsub, we propose its specification: Broadcastsub, in which\nimplementation details like network connections and neighbor subscriptions are\nelided. To show that Floodsub does really implement Broadcastsub, one would\nhave to show that the two systems have related infinite computations. We prove\nthis by reasoning locally about states and their successors using Well-Founded\nSimulation (WFS). In this paper, we focus on the mechanization of a proof which\nshows that Floodsub is a simulation refinement of Broadcastsub using WFS. To\nthe best of our knowledge, ours is the first mechanized refinement-based\nverification of a real world pubsub protocol.", "comment": "In Proceedings ACL2 2025, arXiv:2507.18567", "pdf_url": "http://arxiv.org/pdf/2507.19013v1", "cate": "cs.LO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "Floodsub协议正确性的形式化", "tldr": "本文通过提出Broadcastsub规范并使用良基模拟（WFS）进行机械化证明，首次实现了对Floodsub这一真实世界发布/订阅协议的细化验证，以证明其正确性。", "motivation": "为了证明Floodsub协议的正确性。", "method": "提出Floodsub的规范Broadcastsub，并使用良基模拟（WFS）通过局部推理状态及其后续来证明Floodsub是Broadcastsub的模拟细化。重点在于证明过程的机械化。", "result": "成功机械化地证明了Floodsub是Broadcastsub的模拟细化。这是首次对真实世界发布/订阅协议进行机械化、基于细化的验证。", "conclusion": "本文首次对真实世界的发布/订阅协议Floodsub进行了机械化、基于细化的正确性验证。", "translation": "Floodsub是一个简单、健壮且流行的点对点发布/订阅（pubsub）协议，其中节点可以任意离开或加入网络，订阅或取消订阅主题，并将新收到的消息转发给所有邻居，除了发送者或原始对等点。为了证明Floodsub的正确性，我们提出了它的规范：Broadcastsub，其中省略了网络连接和邻居订阅等实现细节。要证明Floodsub确实实现了Broadcastsub，就必须证明这两个系统具有相关的无限计算。我们通过使用良基模拟（WFS）对状态及其后续进行局部推理来证明这一点。在本文中，我们专注于证明的机械化，该证明表明Floodsub是使用WFS的Broadcastsub的模拟细化。据我们所知，这是首次对真实世界发布/订阅协议进行机械化、基于细化的验证。", "summary": "本文旨在形式化证明Floodsub协议的正确性。作者首先提出了一个抽象规范Broadcastsub，该规范省略了Floodsub的具体实现细节。随后，他们利用良基模拟（WFS）技术，通过对系统状态及其后续进行局部推理，机械化地证明了Floodsub是Broadcastsub的一个模拟细化。这项工作是首次对真实世界的发布/订阅协议进行机械化、基于细化的验证。", "keywords": "Floodsub, 发布/订阅协议, 正确性验证, 形式化, 良基模拟", "comments": "这项工作的重要性在于它是首次对真实世界的发布/订阅协议进行机械化、基于细化的正确性验证。通过形式化方法，它增强了对Floodsub协议健壮性和可靠性的信心，为未来类似协议的验证提供了范例。"}}
{"id": "2507.06133", "title": "Bridging Sequential Deep Operator Network and Video Diffusion: Residual Refinement of Spatio-Temporal PDE Solutions", "authors": ["Jaewan Park", "Farid Ahmed", "Kazuma Kobayashi", "Seid Koric", "Syed Bahauddin Alam", "Iwona Jasiuk", "Diab Abueidda"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06133v2", "summary": "Video-diffusion models have recently set the standard in video generation,\ninpainting, and domain translation thanks to their training stability and high\nperceptual fidelity. Building on these strengths, we repurpose conditional\nvideo diffusion as a physics surrogate for spatio-temporal fields governed by\npartial differential equations (PDEs). Our two-stage surrogate first applies a\nSequential Deep Operator Network (S-DeepONet) to produce a coarse,\nphysics-consistent prior from the prescribed boundary or loading conditions.\nThe prior is then passed to a conditional video diffusion model that learns\nonly the residual: the point-wise difference between the ground truth and the\nS-DeepONet prediction. By shifting the learning burden from the full solution\nto its much smaller residual space, diffusion can focus on sharpening\nhigh-frequency structures without sacrificing global coherence. The framework\nis assessed on two disparate benchmarks: (i) vortex-dominated lid-driven cavity\nflow and (ii) tensile plastic deformation of dogbone specimens. Across these\ndata sets the hybrid surrogate consistently outperforms its single-stage\ncounterpart, cutting the mean relative L2 error from 4.57% to 0.83% for the\nflow problem and from 4.42% to 2.94% for plasticity, a relative improvements of\n81.8% and 33.5% respectively. The hybrid approach not only lowers quantitative\nerrors but also improves visual quality, visibly recovering fine spatial\ndetails. These results show that (i) conditioning diffusion on a physics-aware\nprior enables faithful reconstruction of localized features, (ii) residual\nlearning reduces the problem, accelerating convergence and enhancing accuracy,\nand (iii) the same architecture transfers seamlessly from incompressible flow\nto nonlinear elasto-plasticity without problem-specific architectural\nmodifications, highlighting its broad applicability to nonlinear,\ntime-dependent continua.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06133v2", "cate": "cs.CE", "date": "2025-07-08", "updated": "2025-07-24", "AI": {"title_translation": "连接序列深度算子网络与视频扩散：时空偏微分方程解的残差细化", "tldr": "本文提出了一种结合序列深度算子网络（S-DeepONet）和条件视频扩散模型的两阶段混合代理框架，通过学习残差来求解时空偏微分方程（PDEs），显著提高了物理场预测的准确性和视觉质量，并展示了其在不同物理问题上的普适性。", "motivation": "视频扩散模型在视频生成、修复和域转换方面表现出色，本文旨在利用其优势，将其重新定位为时空物理场（由偏微分方程控制）的物理代理，以提高传统方法的准确性和细节恢复能力。", "method": "本文提出一个两阶段的混合代理框架。首先，使用序列深度算子网络（S-DeepONet）从边界或加载条件生成一个粗糙的、物理一致的先验解。然后，将此先验解传递给一个条件视频扩散模型，该模型仅学习残差（即真实值与S-DeepONet预测之间的点对点差异）。通过将学习负担从完整解转移到更小的残差空间，扩散模型可以专注于锐化高频结构，同时保持全局一致性。", "result": "该混合代理在两个基准测试中表现出色：对于涡流主导的盖驱动腔流问题，平均相对L2误差从4.57%降至0.83%（提高了81.8%）；对于狗骨试样的拉伸塑性变形问题，误差从4.42%降至2.94%（提高了33.5%）。该方法不仅降低了量化误差，还改善了视觉质量，明显恢复了精细的空间细节。", "conclusion": "研究结果表明：(i) 将扩散模型以物理先验为条件能够忠实地重建局部特征；(ii) 残差学习缩小了问题规模，加速了收敛并提高了精度；(iii) 相同的架构可以无缝地从不可压缩流迁移到非线性弹塑性，无需针对具体问题进行架构修改，突出了其对非线性、时间相关连续体的广泛适用性。", "translation": "视频扩散模型凭借其训练稳定性和高感知保真度，最近在视频生成、图像修复和域转换方面设定了标准。基于这些优势，我们将条件视频扩散重新定位为由偏微分方程（PDEs）控制的时空场的物理代理。我们的两阶段代理首先应用序列深度算子网络（S-DeepONet）从预设的边界或加载条件生成一个粗糙的、物理一致的先验。然后，将该先验传递给一个条件视频扩散模型，该模型仅学习残差：即真实值与S-DeepONet预测之间的点对点差异。通过将学习负担从完整解转移到其小得多的残差空间，扩散模型可以专注于锐化高频结构，而不会牺牲全局一致性。该框架在两个截然不同的基准测试中进行了评估：(i) 涡流主导的盖驱动腔流和 (ii) 狗骨试样的拉伸塑性变形。在这些数据集中，混合代理始终优于其单阶段对应物，对于流动问题，平均相对L2误差从4.57%降至0.83%，对于塑性问题，从4.42%降至2.94%，分别相对提高了81.8%和33.5%。混合方法不仅降低了量化误差，还改善了视觉质量，明显恢复了精细的空间细节。这些结果表明 (i) 以物理感知先验为条件进行扩散能够忠实地重建局部特征，(ii) 残差学习缩小了问题规模，加速了收敛并提高了精度，以及 (iii) 相同的架构可以无缝地从不可压缩流迁移到非线性弹塑性，无需针对具体问题进行架构修改，突出了其对非线性、时间相关连续体的广泛适用性。", "summary": "本文提出了一种新颖的两阶段混合代理框架，将序列深度算子网络（S-DeepONet）与条件视频扩散模型相结合，用于求解由偏微分方程（PDEs）控制的时空物理场。该框架首先由S-DeepONet生成一个粗略的物理一致性先验，然后视频扩散模型学习并细化真实解与先验之间的残差。这种残差学习方法显著降低了学习难度，使得扩散模型能够有效捕捉高频细节，同时保持全局一致性。在盖驱动腔流和拉伸塑性变形这两个基准测试中，该混合方法在量化误差和视觉质量上均显著优于单一模型，展示了其在不同非线性、时间相关连续体问题上的广泛适用性和优越性能。", "keywords": "视频扩散, 深度算子网络, PDE求解, 残差学习, 物理代理", "comments": "这项工作创新性地将物理驱动的深度算子网络与数据驱动的视频扩散模型相结合，通过残差学习的范式，有效提升了复杂时空偏微分方程解的精度和细节表现力。其亮点在于利用物理先验指导扩散模型的学习，并专注于残差的细化，这不仅提高了计算效率，也增强了模型的泛化能力。该方法在不同物理领域（流体和塑性变形）的成功应用，预示了其在科学计算和工程仿真中的巨大潜力。"}}
{"id": "2409.00920", "title": "ToolACE: Winning the Points of LLM Function Calling", "authors": ["Weiwen Liu", "Xu Huang", "Xingshan Zeng", "Xinlong Hao", "Shuai Yu", "Dexun Li", "Shuai Wang", "Weinan Gan", "Zhengying Liu", "Yuanqing Yu", "Zezhong Wang", "Yuxian Wang", "Wu Ning", "Yutai Hou", "Bin Wang", "Chuhan Wu", "Xinzhi Wang", "Yong Liu", "Yasheng Wang", "Duyu Tang", "Dandan Tu", "Lifeng Shang", "Xin Jiang", "Ruiming Tang", "Defu Lian", "Qun Liu", "Enhong Chen"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      21 pages, 22 figures", "url": "http://arxiv.org/abs/2409.00920v2", "summary": "Function calling significantly extends the application boundary of large\nlanguage models, where high-quality and diverse training data is critical for\nunlocking this capability. However, real function-calling data is quite\nchallenging to collect and annotate, while synthetic data generated by existing\npipelines tends to lack coverage and accuracy. In this paper, we present\nToolACE, an automatic agentic pipeline designed to generate accurate, complex,\nand diverse tool-learning data. ToolACE leverages a novel self-evolution\nsynthesis process to curate a comprehensive API pool of 26,507 diverse APIs.\nDialogs are further generated through the interplay among multiple agents,\nguided by a formalized thinking process. To ensure data accuracy, we implement\na dual-layer verification system combining rule-based and model-based checks.\nWe demonstrate that models trained on our synthesized data, even with only 8B\nparameters, achieve state-of-the-art performance on the Berkeley\nFunction-Calling Leaderboard, rivaling the latest GPT-4 models. Our model and a\nsubset of the data are publicly available at https://huggingface.co/Team-ACE.", "comment": "21 pages, 22 figures", "pdf_url": "http://arxiv.org/pdf/2409.00920v2", "cate": "cs.LG", "date": "2024-09-02", "updated": "2025-07-25", "AI": {"title_translation": "ToolACE: 赢得LLM函数调用的关键点", "tldr": "ToolACE是一个自动代理管道，用于生成高质量、多样化的大语言模型（LLM）函数调用训练数据。通过新颖的自进化合成过程和双层验证系统，ToolACE能够生成准确、复杂的数据，使得即使是8B参数的模型也能在函数调用排行榜上达到最先进的性能，媲美GPT-4模型。", "motivation": "现有函数调用训练数据收集和标注困难，且现有管道生成的合成数据缺乏覆盖率和准确性，这限制了大型语言模型（LLM）函数调用能力的充分发挥。", "method": "ToolACE是一个自动代理管道，旨在生成准确、复杂和多样化的工具学习数据。它利用新颖的自进化合成过程策划了一个包含26,507个多样化API的综合API池。对话通过多个代理之间的相互作用生成，并由形式化的思维过程指导。为确保数据准确性，ToolACE实施了一个结合了基于规则和基于模型的双层验证系统。", "result": "使用ToolACE合成数据训练的模型，即使只有8B参数，在Berkeley函数调用排行榜上也能达到最先进的性能，与最新的GPT-4模型相媲美。", "conclusion": "ToolACE成功提供了一个自动化的代理管道，能够生成高质量、多样化且准确的函数调用训练数据，有效解决了数据稀缺和质量问题，使得小型参数模型也能在函数调用任务上达到顶尖性能，从而显著扩展了大型语言模型的应用边界。", "translation": "函数调用显著扩展了大型语言模型的应用边界，其中高质量和多样化的训练数据对于释放此能力至关重要。然而，真实的函数调用数据很难收集和标注，而现有管道生成的合成数据往往缺乏覆盖范围和准确性。在本文中，我们提出了ToolACE，一个自动代理管道，旨在生成准确、复杂和多样化的工具学习数据。ToolACE利用新颖的自进化合成过程来策划一个包含26,507个多样化API的综合API池。对话通过多个代理之间的相互作用进一步生成，并由形式化的思维过程指导。为了确保数据准确性，我们实施了一个结合了基于规则和基于模型的双层验证系统。我们证明，即使只有8B参数，使用我们合成数据训练的模型在Berkeley函数调用排行榜上也能达到最先进的性能，与最新的GPT-4模型相媲美。我们的模型和部分数据已在https://huggingface.co/Team-ACE 公开可用。", "summary": "本文提出了ToolACE，一个自动代理管道，旨在生成高质量、多样化且准确的大型语言模型（LLM）函数调用训练数据，以解决现有数据收集和合成的挑战。ToolACE通过自进化合成过程构建了一个包含26,507个API的综合池，并利用多代理交互和双层验证系统确保数据的复杂性和准确性。实验证明，即使是8B参数的模型，在ToolACE合成数据上训练后，也能在函数调用排行榜上达到最先进的性能，与GPT-4模型相当。", "keywords": "函数调用, 大语言模型, 数据生成, 自动化管道, 自进化合成", "comments": "ToolACE的创新之处在于其自动化的数据生成管道，特别是自进化合成过程和双层验证系统，有效解决了函数调用数据稀缺和质量不高的问题。其重要性体现在能够使小参数模型达到与大型专有模型相当的性能，降低了LLM函数调用能力实现的门槛，对LLM在实际应用中的推广具有重要意义。"}}
{"id": "2507.08013", "title": "MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model", "authors": ["K. Sahit Reddy", "N. Ragavenderan", "Vasanth K.", "Ganesh N. Naik", "Vishalakshi Prabhu", "Nagaraja G. S"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.08013v2", "summary": "Recent advances in natural language processing (NLP) have been driven\nbypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel\nat understanding complex texts, but biomedical literature, withits\ndomain-specific terminology, poses challenges that models likeWord2Vec and\nbidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5,\ndespite capturing context, fall short in tasks needingbidirectional\nunderstanding, unlike BERT. Addressing this, we proposedMedicalBERT, a\npretrained BERT model trained on a large biomedicaldataset and equipped with\ndomain-specific vocabulary that enhances thecomprehension of biomedical\nterminology. MedicalBERT model is furtheroptimized and fine-tuned to address\ndiverse tasks, including named entityrecognition, relation extraction, question\nanswering, sentence similarity, anddocument classification. Performance metrics\nsuch as the F1-score,accuracy, and Pearson correlation are employed to showcase\nthe efficiencyof our model in comparison to other BERT-based models such as\nBioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost\nof the benchmarks, and surpasses the general-purpose BERT model by5.67% on\naverage across all the tasks evaluated respectively. This work alsounderscores\nthe potential of leveraging pretrained BERT models for medicalNLP tasks,\ndemonstrating the effectiveness of transfer learning techniques incapturing\ndomain-specific information.\n  (PDF) MedicalBERT: enhancing biomedical natural language processing using\npretrained BERT-based model. Available from:\nhttps://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model\n[accessed Jul 06 2025].", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.08013v2", "cate": "cs.CL", "date": "2025-07-06", "updated": "2025-07-25", "AI": {"title_translation": "MedicalBERT：使用预训练BERT模型增强生物医学自然语言处理", "tldr": "MedicalBERT是一个针对生物医学领域优化的预训练BERT模型，通过领域特定词汇和大规模生物医学数据集训练，在多种生物医学NLP任务上优于其他BERT模型。", "motivation": "现有的预训练语言模型（如BERT、RoBERTa、T5、GPT）在理解复杂文本方面表现出色，但生物医学文献的领域特定术语对这些模型构成挑战。GPT和T5在需要双向理解的任务中表现不足。为了解决现有模型在生物医学NLP中的局限性，特别是双向理解和领域特定术语处理能力不足的问题，本文提出了MedicalBERT。", "method": "本文提出了MedicalBERT，这是一个预训练的BERT模型，它在一个大型生物医学数据集上进行训练，并配备了领域特定的词汇，以增强对生物医学术语的理解。MedicalBERT模型进一步优化和微调，以解决命名实体识别、关系提取、问答、句子相似性和文档分类等多种任务。使用F1分数、准确率和皮尔逊相关系数等性能指标来展示模型效率。", "result": "MedicalBERT在大多数基准测试中优于BioBERT、SciBERT和ClinicalBERT等其他基于BERT的模型。在所有评估任务中，MedicalBERT平均比通用BERT模型高出5.67%。", "conclusion": "这项工作强调了利用预训练BERT模型进行医学NLP任务的潜力，并证明了迁移学习技术在捕获领域特定信息方面的有效性。", "translation": "自然语言处理（NLP）的最新进展是由BERT、RoBERTa、T5和GPT等预训练语言模型推动的。这些模型在理解复杂文本方面表现出色，但生物医学文献及其领域特定术语带来了挑战，这是Word2Vec和双向长短期记忆（Bi-LSTM）等模型无法完全解决的。GPT和T5尽管能捕捉上下文，但在需要双向理解的任务中表现不足，这与BERT不同。为了解决这个问题，我们提出了MedicalBERT，一个在大型生物医学数据集上训练并配备领域特定词汇的预训练BERT模型，它增强了对生物医学术语的理解。MedicalBERT模型进一步优化和微调，以解决各种任务，包括命名实体识别、关系提取、问答、句子相似性和文档分类。我们采用F1分数、准确率和皮尔逊相关系数等性能指标来展示我们模型与BioBERT、SciBERT和ClinicalBERT等其他基于BERT的模型相比的效率。MedicalBERT在大多数基准测试中优于这些模型，并且在所有评估任务中平均比通用BERT模型高出5.67%。这项工作还强调了利用预训练BERT模型进行医学NLP任务的潜力，证明了迁移学习技术在捕获领域特定信息方面的有效性。", "summary": "本文介绍了MedicalBERT，一个专门为生物医学领域设计的预训练BERT模型。针对现有模型在处理生物医学特定术语和需要双向理解的任务上的局限性，MedicalBERT通过在大规模生物医学数据集上训练并集成领域特定词汇来解决这些问题。该模型在命名实体识别、关系提取、问答等多种生物医学NLP任务上进行了优化和微调。实验结果表明，MedicalBERT在大多数基准测试中优于BioBERT、SciBERT和ClinicalBERT等其他BERT基模型，并且在所有评估任务上平均比通用BERT模型性能提升5.67%。这表明预训练BERT模型在医学NLP任务中具有巨大潜力，并验证了迁移学习在捕获领域特定信息方面的有效性。", "keywords": "MedicalBERT, 生物医学自然语言处理, 预训练模型, BERT, 迁移学习", "comments": "MedicalBERT的创新之处在于其专门针对生物医学领域的预训练策略，包括使用大规模生物医学数据集和整合领域特定词汇，这使其能够更好地理解和处理复杂的医学术语。其重要性在于，它为生物医学NLP任务提供了一个更高效、更准确的工具，超越了现有的通用和部分领域专用模型。该研究也进一步证实了迁移学习在特定领域NLP任务中的强大能力和应用前景。"}}
{"id": "2507.19132", "title": "OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?", "authors": ["Xuetian Chen", "Yinghao Chen", "Xinfeng Yuan", "Zhuo Peng", "Lu Chen", "Yuekeng Li", "Zhoujia Zhang", "Yingqian Huang", "Leyan Huang", "Jiaqing Liang", "Tianbao Xie", "Zhiyong Wu", "Qiushi Sun", "Biqing Qi", "Bowen Zhou"], "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Work in progress", "url": "http://arxiv.org/abs/2507.19132v1", "summary": "Computer-using agents have shown strong potential to boost human productivity\nand enable new application forms across platforms. While recent advances have\nled to usable applications, existing benchmarks fail to account for the\ninternal task heterogeneity and the corresponding agent capabilities, as well\nas their alignment with actual user demands-hindering both targeted capability\ndevelopment and the reliable transition of research progress into practical\ndeployment. To bridge the gap, we present OS-MAP, a benchmark for daily\ncomputer-using automation that organizes its 416 realistic tasks across 15\napplications along two key dimensions: a five-level taxonomy of automation and\na generalization scope derived from a real-world user demand hierarchy. To\nenable fine-grained analysis of required capabilities and alignment with\nreal-world scenarios, OS-MAP evaluates agents along two dimensions: automation\nlevel across a five-level taxonomy, and generalization scope across a demand\nhierarchy. This design captures varying levels of required agent autonomy and\ngeneralization, forming a performance-generalization evaluation matrix for\nstructured and comprehensive assessment. Experiments show that even\nState-of-the-Art agents with VLM backbones struggle with higher-level tasks\ninvolving perception, reasoning, and coordination-highlighting the need for a\ndeeper understanding of current strengths and limitations to drive the future\nprogress in computer-using agents research and deployment. All code,\nenvironments, baselines, and data are publicly available at\nhttps://github.com/OS-Copilot/OS-Map.", "comment": "Work in progress", "pdf_url": "http://arxiv.org/pdf/2507.19132v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "OS-MAP：计算机使用智能体在广度和深度上能走多远？", "tldr": "OS-MAP是一个新的计算机使用智能体基准，旨在解决现有评估不足的问题，并发现即使是SOTA智能体在复杂任务上仍有显著局限。", "motivation": "现有基准未能充分考虑计算机使用智能体内部任务的异构性、智能体的实际能力以及与用户需求的对齐，这阻碍了有针对性的能力开发和研究成果向实际部署的可靠转化。", "method": "本文提出了OS-MAP，一个针对日常计算机使用自动化任务的基准，它将15个应用程序中的416个真实任务，按照自动化五级分类和源自真实用户需求层次的泛化范围这两个关键维度进行组织。OS-MAP通过评估智能体在这两个维度上的表现，形成一个性能-泛化评估矩阵，以实现对所需能力和真实场景对齐的细粒度分析。", "result": "实验表明，即使是搭载VLM骨干模型的最先进（State-of-the-Art）智能体，在涉及感知、推理和协调的更高层次任务上仍表现不佳。", "conclusion": "需要更深入地理解当前计算机使用智能体的优势和局限性，以推动该领域研究和部署的未来进展。", "translation": "计算机使用智能体已展现出在提升人类生产力和实现跨平台新应用形式方面的强大潜力。尽管最近的进展已催生出可用的应用，但现有基准未能考虑内部任务异构性及相应的智能体能力，也未能与实际用户需求对齐，这阻碍了有针对性的能力开发和研究进展向实际部署的可靠转化。为弥合这一差距，我们提出了OS-MAP，一个用于日常计算机使用自动化的基准，它根据两个关键维度组织了15个应用程序中的416个真实任务：一个五级自动化分类法和一个源自真实世界用户需求层次的泛化范围。为了实现对所需能力和真实世界场景对齐的细粒度分析，OS-MAP根据两个维度评估智能体：跨五级分类法的自动化水平和跨需求层次的泛化范围。这种设计捕捉了所需智能体自主性和泛化能力的不同水平，形成了一个用于结构化和全面评估的性能-泛化评估矩阵。实验表明，即使是搭载VLM骨干模型的最先进智能体，在涉及感知、推理和协调的更高层次任务上仍面临困难——这凸显了需要更深入理解当前优势和局限性，以推动计算机使用智能体研究和部署的未来进展。所有代码、环境、基线和数据均可在https://github.com/OS-Copilot/OS-Map公开获取。", "summary": "OS-MAP是一个旨在评估计算机使用智能体广度和深度能力的新基准。它通过组织416个跨15个应用程序的真实任务，并引入自动化五级分类和基于用户需求的泛化范围这两个评估维度，解决了现有基准在任务异构性和用户需求对齐方面的不足。研究发现，即使是当前最先进的智能体，在需要高级感知、推理和协调能力的复杂任务上仍面临挑战，这强调了深入理解智能体能力和局限性对于未来研究与部署的重要性。", "keywords": "计算机使用智能体, 基准, OS-MAP, 自动化, 泛化", "comments": "OS-MAP的创新在于其独特的双维度评估框架，即结合了自动化级别和泛化范围，这使得对计算机使用智能体的评估更加全面和贴近实际需求。该基准不仅揭示了当前SOTA模型在复杂任务上的不足，也为未来的智能体能力发展指明了方向，对于推动该领域的研究和实际应用具有重要意义。"}}
{"id": "2507.18967", "title": "Underwater Waste Detection Using Deep Learning A Performance Comparison of YOLOv7 to 10 and Faster RCNN", "authors": ["UMMPK Nawarathne", "HMNS Kumari", "HMLS Kumari"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      7 pages, 11 figures, to be published in International Journal of Research in Computing (IJRC)", "url": "http://arxiv.org/abs/2507.18967v1", "summary": "Underwater pollution is one of today's most significant environmental\nconcerns, with vast volumes of garbage found in seas, rivers, and landscapes\naround the world. Accurate detection of these waste materials is crucial for\nsuccessful waste management, environmental monitoring, and mitigation\nstrategies. In this study, we investigated the performance of five cutting-edge\nobject recognition algorithms, namely YOLO (You Only Look Once) models,\nincluding YOLOv7, YOLOv8, YOLOv9, YOLOv10, and Faster Region-Convolutional\nNeural Network (R-CNN), to identify which model was most effective at\nrecognizing materials in underwater situations. The models were thoroughly\ntrained and tested on a large dataset containing fifteen different classes\nunder diverse conditions, such as low visibility and variable depths. From the\nabove-mentioned models, YOLOv8 outperformed the others, with a mean Average\nPrecision (mAP) of 80.9%, indicating a significant performance. This increased\nperformance is attributed to YOLOv8's architecture, which incorporates advanced\nfeatures such as improved anchor-free mechanisms and self-supervised learning,\nallowing for more precise and efficient recognition of items in a variety of\nsettings. These findings highlight the YOLOv8 model's potential as an effective\ntool in the global fight against pollution, improving both the detection\ncapabilities and scalability of underwater cleanup operations.", "comment": "7 pages, 11 figures, to be published in International Journal of\n  Research in Computing (IJRC)", "pdf_url": "http://arxiv.org/pdf/2507.18967v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "水下废物检测使用深度学习：YOLOv7至10和Faster RCNN的性能比较", "tldr": "本研究比较了YOLOv7-10和Faster RCNN在水下废物检测中的性能，发现YOLOv8表现最佳，mAP达到80.9%。", "motivation": "水下污染是一个重要的环境问题，准确检测废物对于成功的废物管理、环境监测和缓解策略至关重要。", "method": "研究比较了YOLOv7、YOLOv8、YOLOv9、YOLOv10和Faster R-CNN五种目标识别算法，在一个包含15个不同类别的水下废物数据集上进行训练和测试，数据集包含低能见度和不同深度等条件。", "result": "YOLOv8表现最佳，平均精度（mAP）达到80.9%。其优异性能归因于其改进的无锚机制和自监督学习等高级特性。", "conclusion": "YOLOv8模型在对抗全球污染方面具有潜力，可以提高水下清理操作的检测能力和可扩展性。", "translation": "水下污染是当今最重要的环境问题之一，世界各地的海洋、河流和陆地中都发现了大量的垃圾。准确检测这些废弃物对于成功的废物管理、环境监测和缓解策略至关重要。在这项研究中，我们调查了五种尖端目标识别算法的性能，即YOLO（You Only Look Once）模型，包括YOLOv7、YOLOv8、YOLOv9、YOLOv10，以及Faster区域卷积神经网络（R-CNN），以确定哪种模型在水下环境中识别材料最有效。这些模型在一个包含15个不同类别的庞大数据集上进行了彻底的训练和测试，涵盖了低能见度和不同深度等多种条件。在上述模型中，YOLOv8的表现优于其他模型，平均精度（mAP）达到80.9%，显示出显著的性能。这种性能的提高归因于YOLOv8的架构，该架构融合了改进的无锚机制和自监督学习等高级特性，从而可以在各种设置中更精确、更高效地识别物体。这些发现突出了YOLOv8模型作为对抗全球污染的有效工具的潜力，提高了水下清理操作的检测能力和可扩展性。", "summary": "本研究旨在比较YOLOv7、YOLOv8、YOLOv9、YOLOv10和Faster R-CNN在水下废物检测中的性能。研究人员在一个包含15个类别、涵盖多种水下条件的大型数据集上对这些模型进行了训练和测试。结果显示，YOLOv8以80.9%的平均精度（mAP）表现最佳，其性能优势归因于其先进的架构，包括改进的无锚机制和自监督学习。研究表明YOLOv8在水下废物检测和清理操作中具有重要的应用潜力。", "keywords": "水下废物检测, 深度学习, YOLOv8, 目标识别, 环境污染", "comments": "这项研究通过系统比较多种先进的深度学习模型在复杂水下环境中的废物检测性能，为实际应用提供了有价值的指导。YOLOv8的优异表现及其架构特点的解释，有助于理解其有效性。该研究的创新性在于其全面的比较和对水下特定挑战的考虑，对环境监测和清理工作具有重要意义。"}}
{"id": "2507.13936", "title": "Extracting Insights from Large-Scale Telematics Data for ITS Applications: Lessons and Recommendations", "authors": ["Gibran Ali", "Neal Feierabend", "Prarthana Doshi", "Calvin Winkowski", "Michael Fontaine"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "Comments:      Accepted for 2025 IEEE International Conference on Intelligent Transportation Systems (ITSC 2025)", "url": "http://arxiv.org/abs/2507.13936v2", "summary": "Over 90% of new vehicles in the United States now collect and transmit\ntelematics data. Similar trends are seen in other developed countries.\nTransportation planners have previously utilized telematics data in various\nforms, but its current scale offers significant new opportunities in traffic\nmeasurement, classification, planning, and control. Despite these\nopportunities, the enormous volume of data and lack of standardization across\nmanufacturers necessitates a clearer understanding of the data and improved\ndata processing methods for extracting actionable insights.\n  This paper takes a step towards addressing these needs through four primary\nobjectives. First, a data processing pipeline was built to efficiently analyze\n1.4 billion miles (120 million trips) of telematics data collected in Virginia\nbetween August 2021 and August 2022. Second, an open data repository of trip\nand roadway segment level summaries was created. Third, interactive\nvisualization tools were designed to extract insights from these data about\ntrip-taking behavior and the speed profiles of roadways. Finally, major\nchallenges that were faced during processing this data are summarized and\nrecommendations to overcome them are provided. This work will help\nmanufacturers collecting the data and transportation professionals using the\ndata to develop a better understanding of the possibilities and major pitfalls\nto avoid.", "comment": "Accepted for 2025 IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2025)", "pdf_url": "http://arxiv.org/pdf/2507.13936v2", "cate": "cs.CY", "date": "2025-07-18", "updated": "2025-07-25", "AI": {"title_translation": "从大规模远程信息处理数据中提取见解以用于ITS应用：经验与建议", "tldr": "本文处理大规模远程信息处理数据，为ITS应用提取见解，包括构建数据管道、开放存储库、可视化工具，并提供克服数据处理挑战的建议。", "motivation": "尽管大规模远程信息处理数据为交通测量、分类、规划和控制提供了巨大机会，但其庞大的体量和缺乏标准化阻碍了可操作见解的提取，因此需要更清晰地理解数据和改进数据处理方法。", "method": "本文构建了一个数据处理管道，用于高效分析14亿英里（1.2亿次行程）的远程信息处理数据；创建了一个行程和道路段级别摘要的开放数据存储库；设计了交互式可视化工具，以提取行程行为和道路速度剖面的见解；最后，总结了处理数据时面临的主要挑战并提供了克服建议。", "result": "建立了一个数据处理管道，创建了一个开放数据存储库，并设计了交互式可视化工具，从而能够从大规模远程信息处理数据中提取见解。同时，总结了主要的加工挑战并提供了建议。", "conclusion": "这项工作将帮助数据收集制造商和使用数据的交通专业人员更好地理解远程信息处理数据的可能性和需要避免的主要陷阱。", "translation": "美国90%以上的新车现在都收集并传输远程信息处理数据。其他发达国家也出现了类似的趋势。交通规划者以前曾以各种形式利用远程信息处理数据，但其目前的规模为交通测量、分类、规划和控制提供了重要的新机会。尽管存在这些机会，但庞大的数据量和制造商之间缺乏标准化使得需要更清晰地理解数据和改进数据处理方法来提取可操作的见解。\n本文通过四个主要目标来解决这些需求。首先，建立了一个数据处理管道，以高效分析2021年8月至2022年8月期间在弗吉尼亚州收集的14亿英里（1.2亿次行程）远程信息处理数据。其次，创建了一个行程和道路段级别摘要的开放数据存储库。第三，设计了交互式可视化工具，以从这些数据中提取有关行程行为和道路速度剖面的见解。最后，总结了处理这些数据时面临的主要挑战，并提供了克服这些挑战的建议。这项工作将帮助收集数据的制造商和使用数据的交通专业人员更好地理解可能性和避免主要陷阱。", "summary": "本文旨在解决从大规模远程信息处理数据中提取见解以用于智能交通系统（ITS）应用的挑战。文中详细介绍了为14亿英里远程信息处理数据开发数据处理管道、创建开放数据存储库以及设计交互式可视化工具以分析行程行为和道路速度剖面的过程。此外，论文还识别了关键的处理挑战并提供了克服它们的建议，旨在帮助数据收集者和使用者有效利用远程信息处理数据。", "keywords": "远程信息处理数据, ITS, 数据处理, 交通规划, 大数据", "comments": "该论文具有重要意义，因为它解决了利用大规模远程信息处理数据的实际挑战，这些数据正变得越来越普及。论文重点关注从数据处理到可视化的完整流程，并识别陷阱和提供建议，使其对ITS领域的工业界和研究界都具有高度实用性。开放数据存储库的创建也是一项宝贵的贡献。"}}
{"id": "2507.19143", "title": "Game-Theoretic Gradient Control for Robust Neural Network Training", "authors": ["Maria Zaitseva", "Ivan Tomilov", "Natalia Gusarova"], "categories": ["cs.NE", "cs.LG", "68T07", "I.2.6"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      19 pages, 6 figures", "url": "http://arxiv.org/abs/2507.19143v1", "summary": "Feed-forward neural networks (FFNNs) are vulnerable to input noise, reducing\nprediction performance. Existing regularization methods like dropout often\nalter network architecture or overlook neuron interactions. This study aims to\nenhance FFNN noise robustness by modifying backpropagation, interpreted as a\nmulti-agent game, and exploring controlled target variable noising. Our\n\"gradient dropout\" selectively nullifies hidden layer neuron gradients with\nprobability 1 - p during backpropagation, while keeping forward passes active.\nThis is framed within compositional game theory. Additionally, target variables\nwere perturbed with white noise or stable distributions. Experiments on ten\ndiverse tabular datasets show varying impacts: improvement or diminishing of\nrobustness and accuracy, depending on dataset and hyperparameters. Notably, on\nregression tasks, gradient dropout (p = 0.9) combined with stable distribution\ntarget noising significantly increased input noise robustness, evidenced by\nflatter MSE curves and more stable SMAPE values. These results highlight the\nmethod's potential, underscore the critical role of adaptive parameter tuning,\nand open new avenues for analyzing neural networks as complex adaptive systems\nexhibiting emergent behavior within a game-theoretic framework.", "comment": "19 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.19143v1", "cate": "cs.NE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于博弈论的梯度控制用于鲁棒神经网络训练", "tldr": "本研究通过引入“梯度dropout”和目标变量噪声来增强前馈神经网络的噪声鲁棒性，将其反向传播解释为多智能体博弈，并在回归任务中取得了显著效果。", "motivation": "前馈神经网络(FFNNs)容易受到输入噪声的影响，从而降低预测性能。现有的正则化方法（如dropout）通常会改变网络架构或忽略神经元间的相互作用，因此本研究旨在通过修改反向传播来增强FFNN的噪声鲁棒性。", "method": "本研究将反向传播解释为多智能体博弈，并引入“梯度dropout”，即在反向传播过程中以1-p的概率选择性地将隐藏层神经元梯度置零，同时保持前向传播活跃。此外，还使用白噪声或稳定分布扰动目标变量。这些方法均在组合博弈论框架下进行。", "result": "在十个不同的表格数据集上的实验显示了不同的影响：鲁棒性和准确性可能提高也可能降低，具体取决于数据集和超参数。值得注意的是，在回归任务中，梯度dropout (p = 0.9) 与稳定分布目标噪声相结合显著提高了输入噪声鲁棒性，表现为更平坦的MSE曲线和更稳定的SMAPE值。", "conclusion": "本研究结果突出了该方法的潜力，强调了自适应参数调整的关键作用，并为在博弈论框架内分析作为复杂自适应系统展现涌现行为的神经网络开辟了新途径。", "translation": "前馈神经网络 (FFNNs) 容易受到输入噪声的影响，从而降低预测性能。现有的正则化方法（如dropout）通常会改变网络架构或忽略神经元间的相互作用。本研究旨在通过修改反向传播来增强FFNN的噪声鲁棒性，将反向传播解释为多智能体博弈，并探索受控的目标变量噪声。我们的“梯度dropout”在反向传播过程中以1-p的概率选择性地将隐藏层神经元梯度置零，同时保持前向传播活跃。这被置于组合博弈论的框架内。此外，目标变量用白噪声或稳定分布进行了扰动。在十个不同的表格数据集上的实验显示了不同的影响：鲁棒性和准确性可能提高也可能降低，具体取决于数据集和超参数。值得注意的是，在回归任务中，梯度dropout (p = 0.9) 与稳定分布目标噪声相结合显著提高了输入噪声鲁棒性，表现为更平坦的MSE曲线和更稳定的SMAPE值。这些结果突出了该方法的潜力，强调了自适应参数调整的关键作用，并为在博弈论框架内分析作为复杂自适应系统展现涌现行为的神经网络开辟了新途径。", "summary": "本研究旨在通过修改反向传播和引入目标变量噪声来提高前馈神经网络的噪声鲁棒性。作者将反向传播视为多智能体博弈，并提出了一种“梯度dropout”方法，即在反向传播中选择性地置零隐藏层神经元梯度。此外，还探讨了使用白噪声或稳定分布对目标变量进行扰动。实验结果表明，该方法在不同数据集和超参数下表现出不同的效果，但在回归任务中，特定配置下的梯度dropout结合稳定分布目标噪声显著增强了输入噪声鲁棒性。这表明了该方法的潜力以及自适应参数调整的重要性。", "keywords": "梯度dropout, 神经网络鲁棒性, 博弈论, 反向传播, 噪声鲁棒性", "comments": "这项研究的创新之处在于将神经网络的反向传播过程解释为多智能体博弈，并在此框架下提出了“梯度dropout”的概念，这为增强神经网络的鲁棒性提供了一个新颖的视角。此外，结合目标变量噪声处理也增加了方法的全面性。其重要性体现在为处理神经网络的输入噪声脆弱性提供了新的解决方案，并为将神经网络视为复杂自适应系统提供了博弈论分析框架。"}}
{"id": "2507.10616", "title": "Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them", "authors": ["Neel Rajani", "Aryo Pradipta Gema", "Seraphina Goldfarb-Tarrant", "Ivan Titov"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10616v2", "summary": "Training large language models (LLMs) for reasoning via maths and code\ndatasets has become a major new focus in LLM post-training. Two particularly\npopular approaches are reinforcement learning (RL) and supervised fine-tuning\n(SFT), but their training dynamics are poorly understood. We present a\ncomparative analysis of RL and SFT on the same maths problems with the same\nmodel and similar hyperparameters. We find that RL yields minor in-domain gains\non maths and slight degradation on knowledge-intensive benchmarks like MMLU,\nwhile both trends are more pronounced in SFT. We also analyse model parameters\nacross checkpoints, observing that both algorithms modify query and key weights\nthe most. Meanwhile, SFT exhibits greater updates and also affects mid-layer\nMLPs more, leading us to hypothesise that this may have caused the\nout-of-domain degradation. We therefore investigate whether freezing parts of\nthe model during training can mitigate the reduced performance on\nknowledge-intensive benchmarks. However, our results are inconclusive, with\nbenefits on GPQA:Diamond and degradation on other benchmarks. Taken together,\nour observations provide a preliminary indication for why RL amplifies existing\ncapabilities, while SFT replaces old skills with new ones.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10616v2", "cate": "cs.LG", "date": "2025-07-13", "updated": "2025-07-25", "AI": {"title_translation": "手术刀与锤子：GRPO增强现有能力，SFT取而代之", "tldr": "本研究比较了强化学习（RL）和监督微调（SFT）对大型语言模型（LLM）推理能力的影响。结果显示，RL能小幅提升域内能力但轻微降低域外能力，而SFT的效果更显著，且可能因参数更新更大而导致域外能力下降。这初步表明RL是能力增强，SFT是能力替换。", "motivation": "尽管强化学习（RL）和监督微调（SFT）是训练大型语言模型（LLM）进行推理的流行方法，但它们的训练动态尚不清楚。本研究旨在通过比较分析来理解这两种方法的潜在机制及其对模型能力的影响。", "method": "研究人员在相同的数学问题上，使用相同的模型和相似的超参数对RL和SFT进行了比较分析。他们还分析了跨检查点的模型参数更新情况，并探讨了在训练过程中冻结模型部分参数是否能减轻知识密集型基准上的性能下降。", "result": "1. RL在数学领域内获得了微小的收益，但在MMLU等知识密集型基准上略有下降。2. SFT的这两种趋势（收益和下降）都更为明显。3. 两种算法都主要修改了查询（query）和键（key）权重。4. SFT表现出更大的参数更新，并且更多地影响了中间层的MLP。5. 冻结模型部分参数以减轻知识密集型基准上性能下降的尝试结果尚无定论，在GPQA:Diamond上有所改善，但在其他基准上则有所下降。", "conclusion": "综合来看，本研究的观察初步揭示了为什么强化学习（RL）会增强大型语言模型（LLM）的现有能力，而监督微调（SFT）则用新技能取代旧技能。", "translation": "通过数学和代码数据集训练大型语言模型（LLM）进行推理已成为LLM后训练的一个主要新焦点。其中两种特别流行的方法是强化学习（RL）和监督微调（SFT），但它们的训练动态却知之甚少。我们对RL和SFT在相同的数学问题上，使用相同的模型和相似的超参数进行了比较分析。我们发现RL在数学领域内获得了微小的收益，并在MMLU等知识密集型基准上略有下降，而SFT的这两种趋势则更为明显。我们还分析了跨检查点的模型参数，观察到两种算法都主要修改了查询（query）和键（key）权重。同时，SFT表现出更大的更新，并且更多地影响了中间层的MLP，这使我们假设这可能导致了域外性能的下降。因此，我们研究了在训练过程中冻结模型部分参数是否可以减轻知识密集型基准上的性能下降。然而，我们的结果尚无定论，在GPQA:Diamond上有所改善，但在其他基准上则有所下降。综合来看，我们的观察初步揭示了为什么RL会增强现有能力，而SFT则用新技能取代旧技能。", "summary": "本研究对大型语言模型（LLM）的强化学习（RL）和监督微调（SFT）两种训练方法进行了比较分析，旨在理解它们对LLM推理能力的影响。研究发现，RL在域内数学任务上带来微小提升，但对知识密集型基准（如MMLU）有轻微负面影响；而SFT的这些趋势更为显著。参数分析表明，两种方法都主要修改查询和键权重，但SFT的更新量更大，并更深地影响中间层MLP，这可能是其导致域外性能下降的原因。尽管尝试通过冻结模型部分参数来缓解性能下降的效果不一，但这些发现初步表明RL倾向于增强LLM现有能力，而SFT则倾向于替换旧技能。", "keywords": "大型语言模型, 强化学习, 监督微调, 训练动态, 推理能力", "comments": "本文通过“手术刀与锤子”的比喻，形象地揭示了强化学习（RL）和监督微调（SFT）在训练大型语言模型（LLM）时对模型能力的不同影响。RL更像“手术刀”，倾向于在现有能力基础上进行精细增强，而SFT则像“锤子”，可能通过更大幅度的参数修改来“重塑”模型，甚至替换原有技能，导致域外性能下降。这项研究为理解LLM微调的内在机制提供了宝贵的初步见解，尤其是在平衡域内性能提升与域外能力保持方面具有重要意义。尽管冻结模型部分参数的缓解策略结果尚不明确，但其探索方向值得进一步研究。"}}
{"id": "2507.19079", "title": "SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and Navigation Research", "authors": ["Feng Zhu", "Zihang Zhang", "Kangcheng Teng", "Abduhelil Yakup", "Xiaohong Zhang"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19079v1", "summary": "High-precision navigation and positioning systems are critical for\napplications in autonomous vehicles and mobile mapping, where robust and\ncontinuous localization is essential. To test and enhance the performance of\nalgorithms, some research institutions and companies have successively\nconstructed and publicly released datasets. However, existing datasets still\nsuffer from limitations in sensor diversity and environmental coverage. To\naddress these shortcomings and advance development in related fields, the\nSmartPNT Multisource Integrated Navigation, Positioning, and Attitude Dataset\nhas been developed. This dataset integrates data from multiple sensors,\nincluding Global Navigation Satellite Systems (GNSS), Inertial Measurement\nUnits (IMU), optical cameras, and LiDAR, to provide a rich and versatile\nresource for research in multi-sensor fusion and high-precision navigation. The\ndataset construction process is thoroughly documented, encompassing sensor\nconfigurations, coordinate system definitions, and calibration procedures for\nboth cameras and LiDAR. A standardized framework for data collection and\nprocessing ensures consistency and scalability, enabling large-scale analysis.\nValidation using state-of-the-art Simultaneous Localization and Mapping (SLAM)\nalgorithms, such as VINS-Mono and LIO-SAM, demonstrates the dataset's\napplicability for advanced navigation research. Covering a wide range of\nreal-world scenarios, including urban areas, campuses, tunnels, and suburban\nenvironments, the dataset offers a valuable tool for advancing navigation\ntechnologies and addressing challenges in complex environments. By providing a\npublicly accessible, high-quality dataset, this work aims to bridge gaps in\nsensor diversity, data accessibility, and environmental representation,\nfostering further innovation in the field.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19079v1", "cate": "cs.RO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "SmartPNT-MSF：一个用于定位和导航研究的多传感器融合数据集", "tldr": "SmartPNT-MSF是一个新的多传感器融合数据集，用于高精度定位和导航研究，旨在解决现有数据集在传感器多样性和环境覆盖方面的局限性。", "motivation": "高精度导航和定位系统对于自动驾驶车辆和移动测绘应用至关重要，但现有数据集在传感器多样性和环境覆盖方面存在局限性。为解决这些不足并促进相关领域的发展，SmartPNT-MSF数据集被开发出来。", "method": "SmartPNT数据集集成了GNSS、IMU、光学相机和LiDAR等多种传感器数据。其构建过程有详细文档，涵盖传感器配置、坐标系定义以及相机和LiDAR的校准程序。采用标准化数据采集和处理框架，并通过VINS-Mono和LIO-SAM等SLAM算法进行验证。", "result": "SmartPNT-MSF数据集为多传感器融合和高精度导航研究提供了丰富多样的资源，并展示了其在高级导航研究中的适用性。它涵盖了城市、校园、隧道和郊区等多种真实世界场景。", "conclusion": "SmartPNT-MSF数据集通过提供一个公开、高质量的数据集，旨在弥补传感器多样性、数据可访问性和环境表示方面的空白，从而促进该领域的进一步创新。", "translation": "高精度导航和定位系统对于自动驾驶车辆和移动测绘应用至关重要，在这些应用中，鲁棒和连续的定位是必不可少的。为了测试和提高算法性能，一些研究机构和公司陆续构建并公开发布了数据集。然而，现有数据集在传感器多样性和环境覆盖方面仍然存在局限性。为了解决这些缺点并推动相关领域的发展，SmartPNT多源集成导航、定位和姿态数据集已被开发出来。该数据集整合了来自全球导航卫星系统（GNSS）、惯性测量单元（IMU）、光学相机和激光雷达等多种传感器的数据，为多传感器融合和高精度导航研究提供了丰富多样的资源。数据集的构建过程有详细的文档记录，包括传感器配置、坐标系定义以及相机和激光雷达的校准程序。标准化数据收集和处理框架确保了一致性和可扩展性，从而能够进行大规模分析。使用VINS-Mono和LIO-SAM等最先进的同步定位与建图（SLAM）算法进行验证，证明了该数据集在高级导航研究中的适用性。该数据集涵盖了广泛的真实世界场景，包括城市区域、校园、隧道和郊区环境，为推进导航技术和解决复杂环境中的挑战提供了宝贵的工具。通过提供一个公开可用的高质量数据集，这项工作旨在弥补传感器多样性、数据可访问性和环境表示方面的空白，从而促进该领域的进一步创新。", "summary": "SmartPNT-MSF是一个为高精度定位和导航研究而开发的多传感器融合数据集。它整合了GNSS、IMU、相机和LiDAR数据，旨在解决现有数据集在传感器多样性和环境覆盖上的不足。该数据集构建过程详尽，并经过SLAM算法验证，涵盖多种真实世界场景，为推动导航技术提供了高质量、公开可用的资源。", "keywords": "多传感器融合, 定位导航, 数据集, 自动驾驶, SLAM", "comments": "该论文提出了一个重要且及时的多传感器融合数据集，旨在解决现有数据集在传感器多样性和环境覆盖方面的痛点。通过集成多种传感器并在多样化环境中进行数据采集，SmartPNT-MSF为自动驾驶和移动测绘领域的算法开发和验证提供了宝贵的资源。其详细的文档和标准化框架有助于数据集的可用性和可扩展性，并通过主流SLAM算法的验证进一步增强了其创新性和实用性。"}}
{"id": "2503.16806", "title": "DyWA: Dynamics-adaptive World Action Model for Generalizable Non-prehensile Manipulation", "authors": ["Jiangran Lyu", "Ziming Li", "Xuesong Shi", "Chaoyi Xu", "Yizhou Wang", "He Wang"], "categories": ["cs.RO", "cs.AI"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Project Page: this https URL", "url": "http://arxiv.org/abs/2503.16806v2", "summary": "Nonprehensile manipulation is crucial for handling objects that are too thin,\nlarge, or otherwise ungraspable in unstructured environments. While\nconventional planning-based approaches struggle with complex contact modeling,\nlearning-based methods have recently emerged as a promising alternative.\nHowever, existing learning-based approaches face two major limitations: they\nheavily rely on multi-view cameras and precise pose tracking, and they fail to\ngeneralize across varying physical conditions, such as changes in object mass\nand table friction. To address these challenges, we propose the\nDynamics-Adaptive World Action Model (DyWA), a novel framework that enhances\naction learning by jointly predicting future states while adapting to dynamics\nvariations based on historical trajectories. By unifying the modeling of\ngeometry, state, physics, and robot actions, DyWA enables more robust policy\nlearning under partial observability. Compared to baselines, our method\nimproves the success rate by 31.5% using only single-view point cloud\nobservations in the simulation. Furthermore, DyWA achieves an average success\nrate of 68% in real-world experiments, demonstrating its ability to generalize\nacross diverse object geometries, adapt to varying table friction, and\nrobustness in challenging scenarios such as half-filled water bottles and\nslippery surfaces.", "comment": "Project Page:https://pku-epic.github.io/DyWA/", "pdf_url": "http://arxiv.org/pdf/2503.16806v2", "cate": "cs.RO", "date": "2025-03-21", "updated": "2025-07-25", "AI": {"title_translation": "DyWA：可泛化非抓取操作的动力学自适应世界动作模型", "tldr": "DyWA是一个新的学习框架，通过预测未来状态并适应动力学变化来改进非抓取操作，解决了现有方法对多视角相机和精确姿态跟踪的依赖以及泛化能力差的问题，并在模拟和真实世界中表现出更高的成功率和泛化能力。", "motivation": "现有学习型非抓取操作方法存在两个主要局限性：它们过度依赖多视角相机和精确的姿态跟踪，并且无法泛化到不同物理条件，例如物体质量和桌面摩擦力的变化。", "method": "本文提出了动力学自适应世界动作模型（DyWA），这是一种新颖的框架，通过联合预测未来状态并根据历史轨迹适应动力学变化来增强动作学习。DyWA通过统一建模几何、状态、物理和机器人动作，在部分可观测性下实现了更鲁棒的策略学习。", "result": "在模拟中，DyWA使用单视角点云观察将成功率提高了31.5%。在真实世界实验中，DyWA取得了平均68%的成功率，并展示了其在不同物体几何形状、适应不同桌面摩擦力以及在半满水瓶和湿滑表面等挑战性场景中的泛化能力和鲁棒性。", "conclusion": "DyWA通过其动力学自适应和统一建模的方法，显著提高了非抓取操作的成功率、泛化能力和鲁棒性，克服了现有学习型方法的局限性。", "translation": "非抓取操作对于在非结构化环境中处理过薄、过大或无法抓取的物体至关重要。传统的基于规划的方法在复杂接触建模方面面临困难，而基于学习的方法最近作为一种有前途的替代方案出现。然而，现有的基于学习的方法面临两个主要局限性：它们严重依赖多视角相机和精确的姿态跟踪，并且无法泛化到不同的物理条件，例如物体质量和桌面摩擦力的变化。为了解决这些挑战，我们提出了动力学自适应世界动作模型（DyWA），这是一种新颖的框架，通过联合预测未来状态并根据历史轨迹适应动力学变化来增强动作学习。通过统一建模几何、状态、物理和机器人动作，DyWA 在部分可观测性下实现了更鲁棒的策略学习。与基线相比，我们的方法在模拟中仅使用单视角点云观察就将成功率提高了 31.5%。此外，DyWA 在真实世界实验中取得了平均 68% 的成功率，证明了其在不同物体几何形状、适应不同桌面摩擦力以及在半满水瓶和湿滑表面等挑战性场景中的泛化能力和鲁棒性。", "summary": "本文提出了DyWA（动力学自适应世界动作模型），一个针对非抓取操作的新型学习框架。它旨在解决现有学习方法对多视角相机和精确姿态跟踪的依赖以及泛化能力不足的问题。DyWA通过联合预测未来状态并根据历史轨迹适应动力学变化来增强动作学习，并统一了几何、状态、物理和机器人动作的建模。实验结果表明，DyWA在模拟中显著提高了成功率，并在真实世界中展示了对不同物体、摩擦力和挑战性场景的泛化能力和鲁棒性。", "keywords": "非抓取操作, 动力学自适应, 世界模型, 机器人学习, 泛化能力", "comments": "DyWA的创新之处在于其动力学自适应能力和统一建模方法，这使其能够克服现有学习型非抓取操作方法在泛化性和对观察依赖性方面的局限。在仅使用单视角点云的情况下实现高成功率，并展示在真实世界复杂场景下的鲁棒性，是该研究的重要贡献，对于推动机器人非抓取操作在非结构化环境中的应用具有重要意义。"}}
{"id": "2507.18888", "title": "Enhancing Robustness of Control Barrier Function: A Reciprocal Resistance-based Approach", "authors": ["Xinming Wang", "Zongyi Guo", "Jianguo Guo", "Jun Yang", "Yunda Yan"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      7 pages, 5 figures, No presented at any conference", "url": "http://arxiv.org/abs/2507.18888v1", "summary": "In this note, a new reciprocal resistance-based control barrier function\n(RRCBF) is developed to enhance the robustness of control barrier functions for\ndisturbed affine nonlinear systems, without requiring explicit knowledge of\ndisturbance bounds. By integrating a reciprocal resistance-like term into the\nconventional zeroing barrier function framework, we formally establish the\nconcept of the reciprocal resistance-based barrier function (RRBF), rigorously\nproving the forward invariance of its associated safe set and its robustness\nagainst bounded disturbances. The RRBF inherently generates a buffer zone near\nthe boundary of the safe set, effectively dominating the influence of\nuncertainties and external disturbances. This foundational concept is extended\nto formulate RRCBFs, including their high-order variants. To alleviate\nconservatism in the presence of complex, time-varying disturbances, we further\nintroduce a disturbance observer-based RRCBF (DO-RRCBF), which exploits\ndisturbance estimates to enhance safety guarantees and recover nominal control\nperformance. The effectiveness of the proposed framework is validated through\ntwo simulation studies: a second-order linear system illustrating forward\ninvariance in the phase plane, and an adaptive cruise control scenario\ndemonstrating robustness in systems with high relative degree.", "comment": "7 pages, 5 figures, No presented at any conference", "pdf_url": "http://arxiv.org/pdf/2507.18888v1", "cate": "eess.SY", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "增强控制障碍函数的鲁棒性：一种基于互易电阻的方法", "tldr": "开发了一种新的基于互易电阻的控制障碍函数（RRCBF），用于增强受扰仿射非线性系统的鲁棒性，无需明确的扰动边界知识，并通过仿真验证了其有效性。", "motivation": "现有控制障碍函数在受扰仿射非线性系统中鲁棒性不足，且通常需要明确的扰动边界知识。本研究旨在无需此类知识的情况下提高控制障碍函数的鲁棒性。", "method": "1. 开发了互易电阻基障碍函数（RRBF），通过将互易电阻类项集成到传统归零障碍函数框架中。2. 严格证明了RRBF相关安全集的前向不变性及其对有界扰动的鲁棒性。3. RRBF内在地在安全集边界附近生成缓冲区，有效抑制不确定性和外部扰动。4. 将此概念扩展到高阶RRCBFs。5. 引入了基于扰动观测器的RRCBF（DO-RRCBF），以减轻复杂时变扰动下的保守性，并利用扰动估计增强安全性。", "result": "通过两项仿真研究验证了所提出框架的有效性：1. 一个二阶线性系统演示了相平面中的前向不变性。2. 一个自适应巡航控制场景展示了高相对度系统中的鲁棒性。", "conclusion": "论文成功开发并验证了一种新的基于互易电阻的控制障碍函数（RRCBF），有效增强了受扰系统的鲁棒性，且无需明确的扰动边界知识，并通过引入DO-RRCBF进一步提升了性能。", "translation": "在这篇笔记中，开发了一种新的基于互易电阻的控制障碍函数（RRCBF），旨在增强受扰仿射非线性系统控制障碍函数的鲁棒性，而无需明确的扰动边界知识。通过将互易电阻类项集成到传统的归零障碍函数框架中，我们正式建立了基于互易电阻的障碍函数（RRBF）的概念，并严格证明了其相关安全集的前向不变性及其对有界扰动的鲁棒性。RRBF在安全集边界附近固有地生成一个缓冲区，有效主导了不确定性和外部扰动的影响。这一基础概念被扩展以构建RRCBF，包括其高阶变体。为了减轻复杂时变扰动下的保守性，我们进一步引入了基于扰动观测器的RRCBF（DO-RRCBF），它利用扰动估计来增强安全保证并恢复标称控制性能。所提出框架的有效性通过两项仿真研究得到验证：一个二阶线性系统演示了相平面中的前向不变性，以及一个自适应巡航控制场景展示了高相对度系统中的鲁棒性。", "summary": "本论文提出了一种名为互易电阻基控制障碍函数（RRCBF）的新方法，旨在增强受扰仿射非线性系统的鲁棒性，且无需预先知道扰动边界。通过将互易电阻项融入传统障碍函数框架，该方法能生成一个缓冲区以有效应对不确定性与外部扰动。为处理复杂时变扰动，进一步引入了基于扰动观测器的RRCBF（DO-RRCBF）。仿真结果验证了该框架在保障系统安全性和鲁棒性方面的有效性。", "keywords": "控制障碍函数, 鲁棒性, 互易电阻, 扰动观测器, 安全关键系统", "comments": "该论文的创新点在于引入了“互易电阻”的概念来增强控制障碍函数的鲁棒性，并解决了传统方法需要明确扰动边界知识的限制。通过生成“缓冲区”来吸收扰动，以及引入扰动观测器来处理复杂扰动，提高了方法的实用性和性能。这对于在不确定环境下实现安全关键控制具有重要意义。"}}
{"id": "2507.19264", "title": "SimMLM: A Simple Framework for Multi-modal Learning with Missing Modality", "authors": ["Sijie Li", "Chen Chen", "Jungong Han"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19264v1", "summary": "In this paper, we propose SimMLM, a simple yet powerful framework for\nmultimodal learning with missing modalities. Unlike existing approaches that\nrely on sophisticated network architectures or complex data imputation\ntechniques, SimMLM provides a generic and effective solution that can adapt to\nvarious missing modality scenarios with improved accuracy and robustness.\nSpecifically, SimMLM consists of a generic Dynamic Mixture of Modality Experts\n(DMoME) architecture, featuring a dynamic, learnable gating mechanism that\nautomatically adjusts each modality's contribution in both full and partial\nmodality settings. A key innovation of SimMLM is the proposed More vs. Fewer\n(MoFe) ranking loss, which ensures that task accuracy improves or remains\nstable as more modalities are made available. This aligns the model with an\nintuitive principle: removing one or more modalities should not increase\naccuracy. We validate SimMLM on multimodal medical image segmentation (BraTS\n2018) and multimodal classification (UPMC Food-101, avMNIST) tasks, where it\nconsistently surpasses competitive methods, demonstrating superior accuracy,\ninterpretability, robustness, and reliability across both complete and missing\nmodality scenarios at test time.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19264v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "SimMLM：一种处理缺失模态的多模态学习的简单框架", "tldr": "SimMLM是一个简单而强大的框架，用于处理缺失模态的多模态学习，它通过动态模态专家混合架构和MoFe排序损失，在各种任务中表现出优越的准确性和鲁棒性。", "motivation": "现有处理缺失模态的多模态学习方法依赖于复杂的网络架构或数据插补技术，SimMLM旨在提供一个通用且有效的解决方案，以适应各种缺失模态场景，并提高准确性和鲁棒性。", "method": "SimMLM框架包含一个通用的动态模态专家混合 (DMoME) 架构，该架构具有一个动态、可学习的门控机制，可以在完整和部分模态设置中自动调整每种模态的贡献。关键创新是提出了“多对少”(MoFe) 排序损失，确保随着可用模态的增加，任务准确性提高或保持稳定。", "result": "SimMLM在多模态医学图像分割 (BraTS 2018) 和多模态分类 (UPMC Food-101, avMNIST) 任务上进行了验证，结果表明它始终超越竞争方法，在测试时的完整和缺失模态场景下，表现出卓越的准确性、可解释性、鲁棒性、和可靠性。", "conclusion": "SimMLM提供了一个简单而强大的框架，用于处理缺失模态的多模态学习，通过其创新架构和损失函数，在多种任务中实现了优越的性能，并增强了准确性、可解释性、鲁棒性和可靠性。", "translation": "在本文中，我们提出了 SimMLM，一个简单而强大的处理缺失模态的多模态学习框架。与现有依赖复杂网络架构或复杂数据插补技术的方法不同，SimMLM 提供了一个通用且有效的解决方案，可以适应各种缺失模态场景，并提高准确性和鲁棒性。具体来说，SimMLM 包含一个通用的动态模态专家混合 (DMoME) 架构，其特点是一个动态的、可学习的门控机制，可以在完整和部分模态设置中自动调整每种模态的贡献。SimMLM 的一个关键创新是提出了“多对少”(MoFe) 排序损失，该损失确保随着可用模态的增加，任务准确性提高或保持稳定。这使模型与一个直观的原则保持一致：移除一个或多个模态不应提高准确性。我们在多模态医学图像分割 (BraTS 2018) 和多模态分类 (UPMC Food-101, avMNIST) 任务上验证了 SimMLM，它始终超越竞争方法，在测试时的完整和缺失模态场景下，表现出卓越的准确性、可解释性、鲁棒性和可靠性。", "summary": "SimMLM是一个针对缺失模态多模态学习的通用框架。它采用动态模态专家混合（DMoME）架构和创新的“多对少”（MoFe）排序损失，后者确保更多模态的可用性不会降低准确性。该框架在医学图像分割和多模态分类任务上表现出卓越的准确性、鲁棒性、可解释性和可靠性，优于现有方法。", "keywords": "多模态学习, 缺失模态, 动态模态专家混合, 排序损失, 鲁棒性", "comments": "SimMLM的创新在于其简单性、通用的DMoME架构以及独特的MoFe排序损失，这些特性使其在处理缺失模态时具有优越的适应性和性能，解决了现有方法复杂性高的问题。"}}
{"id": "2411.19628", "title": "Accelerating Multimodal Large Language Models via Dynamic Visual-Token Exit and the Empirical Findings", "authors": ["Qiong Wu", "Wenhao Lin", "Yiyi Zhou", "Weihao Ye", "Zhanpeng Zen", "Xiaoshuai Sun", "Rongrong Ji"], "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.19628v2", "summary": "The excessive use of visual tokens in existing Multimoal Large Language\nModels (MLLMs) often exhibits obvious redundancy and brings in prohibitively\nexpensive computation. To gain insights into this problem, we first conduct\nextensive empirical studies on the attention behaviors of MLLMs, and summarize\nthree main inference stages in MLLMs: (i) Early fusion between tokens is first\naccomplished quickly. (ii) Intra-modality modeling then comes to play. (iii)\nMultimodal reasoning} resumes and lasts until the end of inference. In\nparticular, we reveal that visual tokens will stop contributing to reasoning\nwhen the text tokens receive enough image information, yielding obvious visual\nredundancy. Based on these generalized observations, we propose a simple yet\neffective method to improve the efficiency of MLLMs, termed dynamic\nvisual-token exit (DyVTE). DyVTE uses lightweight hyper-networks to perceive\nthe text token status and decide the removal of all visual tokens after a\ncertain layer, thereby addressing the observed visual redundancy. To validate\nVTE, we apply it to a set of MLLMs, including LLaVA, VILA, Eagle and InternVL,\nand conduct extensive experiments on a bunch of benchmarks. The experiment\nresults not only show the effectiveness of our VTE in improving MLLMs'\nefficiency, but also yield the general modeling patterns of MLLMs, well\nfacilitating the in-depth understanding of MLLMs. Our code is released at\nhttps://github.com/DoubtedSteam/DyVTE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.19628v2", "cate": "cs.CV", "date": "2024-11-29", "updated": "2025-07-25", "AI": {"title_translation": "通过动态视觉-Token退出和实证研究加速多模态大语言模型", "tldr": "提出DyVTE方法，通过动态移除冗余视觉token来加速多模态大语言模型，并基于实证研究揭示了视觉冗余问题。", "motivation": "现有多模态大语言模型(MLLMs)中视觉token的过度使用导致明显的冗余和高昂的计算成本。", "method": "首先对MLLMs的注意力行为进行广泛的实证研究，总结出三个推理阶段，并发现当文本token接收到足够图像信息时，视觉token会停止贡献，产生视觉冗余。基于此，提出动态视觉-token退出(DyVTE)方法，利用轻量级超网络感知文本token状态，并在特定层后移除所有视觉token以解决冗余问题。", "result": "DyVTE在LLaVA、VILA、Eagle和InternVL等MLLMs上进行了验证，实验结果表明DyVTE有效提高了MLLMs的效率，并揭示了MLLMs的一般建模模式，有助于深入理解MLLMs。", "conclusion": "论文通过实证研究揭示了多模态大语言模型中视觉token的冗余问题，并提出了一种简单有效的动态视觉-token退出（DyVTE）方法，显著提升了模型的效率，同时也加深了对MLLMs建模机制的理解。", "translation": "现有多模态大语言模型（MLLMs）中视觉token的过度使用通常表现出明显的冗余，并带来高昂的计算成本。为了深入了解这个问题，我们首先对MLLMs的注意力行为进行了广泛的实证研究，并总结了MLLMs的三个主要推理阶段：（i）token之间的早期融合首先快速完成。(ii) 模态内建模随后发挥作用。(iii) 多模态推理恢复并持续到推理结束。特别是，我们发现当文本token接收到足够的图像信息时，视觉token将停止对推理的贡献，从而产生明显的视觉冗余。基于这些普遍观察，我们提出了一种简单而有效的方法来提高MLLMs的效率，称为动态视觉-token退出（DyVTE）。DyVTE使用轻量级超网络感知文本token状态，并决定在特定层后移除所有视觉token，从而解决观察到的视觉冗余问题。为了验证VTE，我们将其应用于一系列MLLMs，包括LLaVA、VILA、Eagle和InternVL，并在大量基准测试中进行了广泛实验。实验结果不仅显示了我们的VTE在提高MLLMs效率方面的有效性，而且还揭示了MLLMs的一般建模模式，极大地促进了对MLLMs的深入理解。我们的代码已在https://github.com/DoubtedSteam/DyVTE 发布。", "summary": "本研究致力于解决多模态大语言模型（MLLMs）中视觉token过度使用导致的冗余和高计算成本问题。通过对MLLMs注意力行为的实证研究，论文揭示了视觉token在文本token获取足够图像信息后便不再贡献，从而产生视觉冗余。基于此发现，作者提出了一种名为动态视觉-token退出（DyVTE）的有效方法，该方法利用轻量级超网络动态决定在特定层后移除所有视觉token。实验结果表明，DyVTE在多个MLLMs上显著提高了效率，并有助于深入理解MLLMs的通用建模模式。", "keywords": "多模态大语言模型, 视觉Token冗余, 动态视觉-Token退出, 效率, 注意力行为", "comments": "该论文的创新点在于通过深入的实证研究揭示了多模态大语言模型中视觉token的冗余问题，并基于此提出了一个简单而有效的动态退出机制。DyVTE方法通过在推理过程中动态移除冗余的视觉token，显著提升了MLLMs的计算效率，这对于部署和应用大型多模态模型具有重要意义。同时，论文的实证发现也为未来MLLMs的设计和优化提供了宝贵的见解。"}}
{"id": "2507.18680", "title": "Market Making Strategies with Reinforcement Learning", "authors": ["Óscar Fernández Vicente"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18680v1", "summary": "This thesis presents the results of a comprehensive research project focused\non applying Reinforcement Learning (RL) to the problem of market making in\nfinancial markets. Market makers (MMs) play a fundamental role in providing\nliquidity, yet face significant challenges arising from inventory risk,\ncompetition, and non-stationary market dynamics. This research explores how RL,\nparticularly Deep Reinforcement Learning (DRL), can be employed to develop\nautonomous, adaptive, and profitable market making strategies.\n  The study begins by formulating the MM task as a reinforcement learning\nproblem, designing agents capable of operating in both single-agent and\nmulti-agent settings within a simulated financial environment. It then\naddresses the complex issue of inventory management using two complementary\napproaches: reward engineering and Multi-Objective Reinforcement Learning\n(MORL). While the former uses dynamic reward shaping to guide behavior, the\nlatter leverages Pareto front optimization to explicitly balance competing\nobjectives.\n  To address the problem of non-stationarity, the research introduces POW-dTS,\na novel policy weighting algorithm based on Discounted Thompson Sampling. This\nmethod allows agents to dynamically select and combine pretrained policies,\nenabling continual adaptation to shifting market conditions.\n  The experimental results demonstrate that the proposed RL-based approaches\nsignificantly outperform traditional and baseline algorithmic strategies across\nvarious performance metrics. Overall, this research thesis contributes new\nmethodologies and insights for the design of robust, efficient, and adaptive\nmarket making agents, reinforcing the potential of RL to transform algorithmic\ntrading in complex financial systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18680v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于强化学习的市场做市策略", "tldr": "本研究探讨了如何利用强化学习（特别是深度强化学习）来开发自主、适应性强且有利可图的做市策略，以应对库存风险、竞争和非平稳市场动态等挑战。", "motivation": "市场做市商在提供流动性方面发挥着重要作用，但面临库存风险、竞争和非平稳市场动态带来的重大挑战。本研究旨在探索如何利用强化学习来开发自主、适应性强且有利可图的做市策略。", "method": "本研究将做市任务表述为强化学习问题，设计了能在模拟金融环境中单代理和多代理设置下运行的代理。通过奖励工程和多目标强化学习（MORL）两种方法来解决库存管理问题。为解决非平稳性问题，引入了一种基于折扣汤普森采样的策略加权算法POW-dTS，使代理能够动态选择和组合预训练策略。", "result": "实验结果表明，所提出的基于强化学习的方法在各种性能指标上显著优于传统和基线算法策略。", "conclusion": "本研究论文为设计稳健、高效和自适应的做市代理贡献了新的方法和见解，强化了强化学习在复杂金融系统中改变算法交易的潜力。", "translation": "本论文展示了一项全面研究项目的结果，该项目专注于将强化学习（RL）应用于金融市场中的做市问题。做市商（MMs）在提供流动性方面发挥着基础性作用，但面临来自库存风险、竞争和非平稳市场动态的重大挑战。本研究探讨了如何利用强化学习，特别是深度强化学习（DRL），来开发自主、适应性强且有利可图的做市策略。\n\n该研究首先将做市任务表述为强化学习问题，设计了能够在模拟金融环境中单代理和多代理设置下运行的代理。然后，它使用两种互补的方法解决了库存管理的复杂问题：奖励工程和多目标强化学习（MORL）。前者使用动态奖励塑形来引导行为，后者则利用帕累托前沿优化来明确平衡相互竞争的目标。\n\n为了解决非平稳性问题，本研究引入了POW-dTS，一种基于折扣汤普森采样的新型策略加权算法。这种方法允许代理动态选择和组合预训练策略，从而实现对不断变化的市场条件的持续适应。\n\n实验结果表明，所提出的基于强化学习的方法在各种性能指标上显著优于传统和基线算法策略。总的来说，本研究论文为设计稳健、高效和自适应的做市代理贡献了新的方法和见解，强化了强化学习在复杂金融系统中改变算法交易的潜力。", "summary": "本研究论文探讨了如何利用强化学习（RL），特别是深度强化学习（DRL），来解决金融市场中的做市问题。针对做市商面临的库存风险、竞争和非平稳市场动态等挑战，研究将做市任务建模为强化学习问题，并设计了在模拟环境中运行的智能体。论文提出了奖励工程和多目标强化学习（MORL）来管理库存，并引入了POW-dTS策略加权算法以应对市场非平稳性。实验结果表明，所提出的RL方法显著优于传统策略，为设计稳健、高效的自适应做市智能体提供了新方法和见解，展示了RL在算法交易中的巨大潜力。", "keywords": "强化学习, 市场做市, 深度强化学习, 库存管理, 非平稳性", "comments": "该论文的创新之处在于将强化学习应用于复杂的做市领域，并针对该领域特有的挑战（如库存风险、非平稳性）提出了具体的解决方案，例如结合奖励工程、MORL和POW-dTS算法。其重要性在于证明了RL在开发更高效、自适应的算法交易策略方面的潜力，有望改变金融市场的做市方式。"}}
{"id": "2507.19031", "title": "ProGMLP: A Progressive Framework for GNN-to-MLP Knowledge Distillation with Efficient Trade-offs", "authors": ["Weigang Lu", "Ziyu Guan", "Wei Zhao", "Yaming Yang", "Yujie Sun", "Zheng Liang", "Yibing Zhan", "Dapeng Tao"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19031v1", "summary": "GNN-to-MLP (G2M) methods have emerged as a promising approach to accelerate\nGraph Neural Networks (GNNs) by distilling their knowledge into simpler\nMulti-Layer Perceptrons (MLPs). These methods bridge the gap between the\nexpressive power of GNNs and the computational efficiency of MLPs, making them\nwell-suited for resource-constrained environments. However, existing G2M\nmethods are limited by their inability to flexibly adjust inference cost and\naccuracy dynamically, a critical requirement for real-world applications where\ncomputational resources and time constraints can vary significantly. To address\nthis, we introduce a Progressive framework designed to offer flexible and\non-demand trade-offs between inference cost and accuracy for GNN-to-MLP\nknowledge distillation (ProGMLP). ProGMLP employs a Progressive Training\nStructure (PTS), where multiple MLP students are trained in sequence, each\nbuilding on the previous one. Furthermore, ProGMLP incorporates Progressive\nKnowledge Distillation (PKD) to iteratively refine the distillation process\nfrom GNNs to MLPs, and Progressive Mixup Augmentation (PMA) to enhance\ngeneralization by progressively generating harder mixed samples. Our approach\nis validated through comprehensive experiments on eight real-world graph\ndatasets, demonstrating that ProGMLP maintains high accuracy while dynamically\nadapting to varying runtime scenarios, making it highly effective for\ndeployment in diverse application settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19031v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "ProGMLP：一种用于GNN到MLP知识蒸馏的渐进式框架，具有高效的权衡", "tldr": "提出ProGMLP，一个渐进式框架，通过序列训练MLP学生和渐进式知识蒸馏，实现GNN到MLP知识蒸馏中推理成本和准确性之间的灵活权衡。", "motivation": "现有GNN-to-MLP方法无法灵活调整推理成本和准确性，这在计算资源和时间限制多变的实际应用中是一个关键需求。", "method": "引入ProGMLP框架，采用渐进式训练结构（PTS），顺序训练多个MLP学生；结合渐进式知识蒸馏（PKD）迭代优化蒸馏过程；并采用渐进式混合增强（PMA）生成更难的混合样本以增强泛化能力。", "result": "在八个真实世界图数据集上的综合实验表明，ProGMLP在保持高准确性的同时，能动态适应不同的运行时场景。", "conclusion": "ProGMLP为GNN-to-MLP知识蒸馏提供了一个高效、灵活的解决方案，使其在资源受限和动态变化的部署环境中非常有效。", "translation": "GNN到MLP（G2M）方法已成为一种有前途的方法，通过将图神经网络（GNNs）的知识蒸馏到更简单的多层感知机（MLPs）中来加速GNNs。这些方法弥合了GNNs的表达能力和MLPs的计算效率之间的差距，使其非常适用于资源受限的环境。然而，现有的G2M方法受限于其无法动态灵活调整推理成本和准确性，这对于计算资源和时间限制可能显著变化的实际应用而言是一个关键要求。为了解决这个问题，我们引入了一个渐进式框架（ProGMLP），旨在为GNN到MLP知识蒸馏提供推理成本和准确性之间的灵活按需权衡。ProGMLP采用渐进式训练结构（PTS），其中多个MLP学生按顺序训练，每个学生在前一个的基础上进行学习。此外，ProGMLP结合了渐进式知识蒸馏（PKD）来迭代优化从GNN到MLP的蒸馏过程，以及渐进式混合增强（PMA）通过逐步生成更难的混合样本来提高泛化能力。我们的方法通过在八个真实世界图数据集上的综合实验得到了验证，结果表明ProGMLP在保持高准确性的同时，能动态适应不同的运行时场景，使其在各种应用部署中非常有效。", "summary": "ProGMLP是一个新颖的渐进式框架，旨在解决现有GNN-to-MLP知识蒸馏方法中推理成本和准确性之间缺乏灵活权衡的问题。它通过渐进式训练结构、渐进式知识蒸馏和渐进式混合增强，使得MLP学生能够顺序学习并迭代优化蒸馏过程，同时增强泛化能力。实验证明，ProGMLP在保持高准确性的同时，能有效适应不同的运行时环境。", "keywords": "GNN-to-MLP, 知识蒸馏, 渐进式框架, 推理成本, 准确性", "comments": "这篇论文的创新点在于提出了一个渐进式框架ProGMLP，解决了GNN-to-MLP知识蒸馏中动态权衡计算效率和准确性的难题。通过引入PTS、PKD和PMA等组件，ProGMLP提供了一种灵活且高效的解决方案，使其在资源受限的实际应用中具有重要价值。"}}
{"id": "2507.19321", "title": "SIDE: Sparse Information Disentanglement for Explainable Artificial Intelligence", "authors": ["Viktar Dubovik", "Łukasz Struski", "Jacek Tabor", "Dawid Rymarczyk"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19321v1", "summary": "Understanding the decisions made by deep neural networks is essential in\nhigh-stakes domains such as medical imaging and autonomous driving. Yet, these\nmodels often lack transparency, particularly in computer vision.\nPrototypical-parts-based neural networks have emerged as a promising solution\nby offering concept-level explanations. However, most are limited to\nfine-grained classification tasks, with few exceptions such as InfoDisent.\nInfoDisent extends prototypical models to large-scale datasets like ImageNet,\nbut produces complex explanations.\n  We introduce Sparse Information Disentanglement for Explainability (SIDE), a\nnovel method that improves the interpretability of prototypical parts through a\ndedicated training and pruning scheme that enforces sparsity. Combined with\nsigmoid activations in place of softmax, this approach allows SIDE to associate\neach class with only a small set of relevant prototypes. Extensive experiments\nshow that SIDE matches the accuracy of existing methods while reducing\nexplanation size by over $90\\%$, substantially enhancing the understandability\nof prototype-based explanations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19321v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "SIDE：可解释人工智能的稀疏信息解缠结", "tldr": "SIDE提出了一种稀疏信息解缠结方法，通过训练和剪枝方案以及sigmoid激活函数，在保持准确性的同时显著减少了基于原型的解释的复杂性。", "motivation": "深度神经网络（特别是计算机视觉领域）缺乏透明度，这在高风险领域（如医学成像和自动驾驶）中是亟需解决的问题。现有的原型部分神经网络虽然提供了概念级解释，但大多局限于细粒度分类任务，且像InfoDisent这样的方法在扩展到大型数据集时会产生复杂的解释，降低了可理解性。", "method": "本文提出了可解释性稀疏信息解缠结（SIDE），这是一种通过专门的训练和剪枝方案来强制执行稀疏性的新方法，从而提高了原型部分的解释性。结合使用sigmoid激活函数而非softmax，SIDE能够将每个类别与一小部分相关原型相关联。", "result": "SIDE在保持现有方法准确性的同时，将解释大小减少了90%以上，极大地增强了基于原型的解释的可理解性。", "conclusion": "SIDE通过引入稀疏性训练和剪枝以及sigmoid激活，成功地提高了原型部分解释的可理解性，同时保持了模型的准确性，有效解决了现有原型模型解释复杂的问题。", "translation": "理解深度神经网络的决策在高风险领域（如医学成像和自动驾驶）中至关重要。然而，这些模型，尤其是在计算机视觉领域，往往缺乏透明度。基于原型部分的神经网络作为一种有前景的解决方案出现，它们提供了概念级的解释。然而，大多数此类模型仅限于细粒度分类任务，少数例外如InfoDisent。InfoDisent将原型模型扩展到ImageNet等大型数据集，但会产生复杂的解释。\n我们引入了可解释性稀疏信息解缠结（SIDE），这是一种通过专门的训练和剪枝方案强制执行稀疏性来提高原型部分可解释性的新方法。结合使用sigmoid激活函数而非softmax，这种方法使得SIDE能够将每个类别与一小部分相关原型相关联。大量实验表明，SIDE在匹配现有方法准确性的同时，将解释大小减少了90%以上，极大地增强了基于原型解释的可理解性。", "summary": "SIDE提出了一种新颖的稀疏信息解缠结方法，旨在提高深度神经网络（特别是计算机视觉领域）的解释性。该方法通过引入专门的训练和剪枝方案来强制稀疏性，并结合sigmoid激活函数，使得模型能够将每个类别与少数相关原型关联起来。实验结果显示，SIDE在保持与现有方法相同准确率的同时，将解释的复杂性降低了90%以上，显著提升了原型解释的可理解性。", "keywords": "可解释人工智能,稀疏性,原型网络,信息解缠结,深度学习解释", "comments": "SIDE的创新点在于通过引入强制稀疏性的训练和剪枝方案，以及使用sigmoid替代softmax，有效地解决了现有原型模型解释过于复杂的问题。其在大幅减少解释大小的同时保持了准确性，这对于高风险领域的可解释AI应用具有重要意义。该方法有望推动可解释AI在实际应用中的落地。"}}
{"id": "2506.04076", "title": "Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems", "authors": ["Jhen-Ke Lin", "Hao-Chien Lu", "Chung-Chun Wang", "Hong-Yun Lin", "Berlin Chen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      accepted to the ISCA SLaTE-2025 Workshop", "url": "http://arxiv.org/abs/2506.04076v2", "summary": "Verbatim transcription for automatic speaking assessment demands accurate\ncapture of disfluencies, crucial for downstream tasks like error analysis and\nfeedback. However, many ASR systems discard or generalize hesitations, losing\nimportant acoustic details. We fine-tune Whisper models on the Speak & Improve\n2025 corpus using low-rank adaptation (LoRA), without recourse to external\naudio training data. We compare three annotation schemes: removing hesitations\n(Pure), generic tags (Rich), and acoustically precise fillers inferred by\nGemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge\nsystem achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge\nexperiments reveal that fine-tuning Whisper Large V3 Turbo with the \"Extra\"\nscheme yielded a 5.5% WER, an 11.3% relative improvement over the \"Pure\" scheme\n(6.2% WER). This demonstrates that explicit, realistic filled-pause labeling\nsignificantly enhances ASR accuracy for verbatim L2 speech transcription.", "comment": "accepted to the ISCA SLaTE-2025 Workshop", "pdf_url": "http://arxiv.org/pdf/2506.04076v2", "cate": "cs.CL", "date": "2025-06-04", "updated": "2025-07-25", "AI": {"title_translation": "声学精确的犹豫标记对于端到端逐字转录系统至关重要", "tldr": "声学精确的犹豫标记显著提高了逐字L2语音转录的ASR准确性。", "motivation": "自动口语评估的逐字转录需要准确捕获非流利语（如犹豫），这对于错误分析和反馈等下游任务至关重要。然而，许多ASR系统会丢弃或泛化这些重要的声学细节。", "method": "在Speak & Improve 2025语料库上使用低秩适应（LoRA）对Whisper模型进行微调，无需外部音频训练数据。比较了三种标注方案：移除犹豫（Pure）、通用标签（Rich）以及由Gemini 2.0 Flash从现有音频-转录对中推断出的声学精确填充词（Extra）。", "result": "挑战系统实现了6.47%的词错误率（Pure）和5.81%的词错误率（Extra）。挑战后的实验表明，使用“Extra”方案微调Whisper Large V3 Turbo模型产生了5.5%的词错误率，相对于“Pure”方案（6.2%的词错误率）有11.3%的相对改进。", "conclusion": "明确、真实的填充停顿标注显著提高了逐字L2语音转录的ASR准确性。", "translation": "自动口语评估的逐字转录需要准确捕获非流利语，这对于错误分析和反馈等下游任务至关重要。然而，许多ASR系统会丢弃或泛化犹豫，从而丢失重要的声学细节。我们使用低秩适应（LoRA）在Speak & Improve 2025语料库上对Whisper模型进行微调，无需外部音频训练数据。我们比较了三种标注方案：移除犹豫（Pure）、通用标签（Rich）以及由Gemini 2.0 Flash从现有音频-转录对中推断出的声学精确填充词（Extra）。我们的挑战系统实现了6.47%的词错误率（Pure）和5.81%的词错误率（Extra）。挑战后的实验表明，使用“Extra”方案微调Whisper Large V3 Turbo模型产生了5.5%的词错误率，相对于“Pure”方案（6.2%的词错误率）有11.3%的相对改进。这表明，明确、真实的填充停顿标注显著提高了逐字L2语音转录的ASR准确性。", "summary": "本文研究了声学精确的犹豫标记对于逐字ASR系统，特别是L2语音转录的重要性。通过使用低秩适应（LoRA）在Speak & Improve 2025语料库上微调Whisper模型，并比较不同的标注方案，研究发现明确、真实的填充停顿标注（由Gemini 2.0 Flash推断的“Extra”方案）显著提高了ASR准确性，相对于移除犹豫的方案，词错误率相对降低了11.3%。", "keywords": "犹豫标记, ASR, 逐字转录, 非流利语, Whisper模型", "comments": "该论文强调了逐字转录中一个常被标准ASR系统忽视的关键方面：准确捕获非流利语。使用Gemini 2.0 Flash推断声学精确的填充词是一种创新方法。L2语音词错误率的显著改善表明了这项工作对于语言学习评估等实际应用的重要性。"}}
{"id": "2505.16142", "title": "Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning", "authors": ["Shicheng Xu", "Liang Pang", "Yunchang Zhu", "Jia Gu", "Zihao Wei", "Jingcheng Deng", "Feiyang Pan", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      15 pages", "url": "http://arxiv.org/abs/2505.16142v3", "summary": "Distilling reasoning paths from teacher to student models via supervised\nfine-tuning (SFT) provides a shortcut for improving the reasoning ability of\nsmaller Large Language Models (LLMs). However, the reasoning paths generated by\nteacher models often reflect only surface-level traces of their underlying\nauthentic reasoning. Insights from cognitive neuroscience suggest that\nauthentic reasoning involves a complex interweaving between meta-reasoning\n(which selects appropriate sub-problems from multiple candidates) and solving\n(which addresses the sub-problem). This implies authentic reasoning has an\nimplicit multi-branch structure. Supervised fine-tuning collapses this rich\nstructure into a flat sequence of token prediction in the teacher's reasoning\npath, preventing effective distillation of this structure to students. To\naddress this limitation, we propose RLKD, a reinforcement learning (RL)-based\ndistillation framework guided by a novel Generative Structure Reward Model\n(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving\nsteps and computes rewards to measure structural alignment between student and\nteacher reasoning. RLKD combines this reward with RL, enabling student LLMs to\ninternalize the teacher's implicit multi-branch reasoning structure rather than\nmerely mimicking fixed output paths. Experiments show RLKD surpasses standard\nSFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,\nunlocking greater student reasoning potential than SFT-based distillation.", "comment": "15 pages", "pdf_url": "http://arxiv.org/pdf/2505.16142v3", "cate": "cs.CL", "date": "2025-05-22", "updated": "2025-07-25", "AI": {"title_translation": "通过强化学习蒸馏大型语言模型推理中的隐式多分支结构", "tldr": "本文提出RLKD，一个基于强化学习的蒸馏框架，通过生成式结构奖励模型（GSRM）帮助学生LLM内化教师模型隐式多分支推理结构，而非仅仅模仿固定输出路径，实验证明其效果优于SFT-RL。", "motivation": "现有的通过监督微调（SFT）从教师模型向学生模型蒸馏推理路径的方法，只能反映教师模型表面层次的推理痕迹。认知神经科学表明真实的推理涉及元推理和解决子问题的复杂交织，具有隐式多分支结构。SFT将这种丰富的结构扁平化为令牌预测的序列，阻碍了结构向学生模型的有效蒸馏。", "method": "提出RLKD（Reinforcement Learning-based Distillation）框架，该框架由一个新颖的生成式结构奖励模型（GSRM）引导。GSRM将推理路径转换为多个元推理-解决步骤，并计算奖励以衡量学生和教师推理之间的结构对齐。RLKD将此奖励与强化学习结合，使学生LLM能够内化教师的隐式多分支推理结构。", "result": "实验表明，RLKD即使在RL-only模式下仅用0.1%的数据进行训练，也能超越标准的SFT-RL管道，释放出比基于SFT的蒸馏更大的学生推理潜力。", "conclusion": "RLKD框架通过结合生成式结构奖励模型和强化学习，能够有效蒸馏LLM中隐式的多分支推理结构，显著提升学生LLM的推理能力，优于传统的监督微调方法。", "translation": "通过监督微调（SFT）将推理路径从教师模型蒸馏到学生模型，为提高小型大型语言模型（LLM）的推理能力提供了一条捷径。然而，教师模型生成的推理路径往往只反映其底层真实推理的表面痕迹。认知神经科学的见解表明，真实的推理涉及元推理（从多个候选方案中选择合适的子问题）和解决（处理子问题）之间复杂的交织。这意味着真实的推理具有隐式多分支结构。监督微调将这种丰富的结构扁平化为教师推理路径中扁平的令牌预测序列，从而阻碍了这种结构向学生模型的有效蒸馏。为了解决这一限制，我们提出了RLKD，一个基于强化学习（RL）的蒸馏框架，由一个新颖的生成式结构奖励模型（GSRM）引导。我们的GSRM将推理路径转换为多个元推理-解决步骤，并计算奖励以衡量学生和教师推理之间的结构对齐。RLKD将此奖励与RL结合，使学生LLM能够内化教师的隐式多分支推理结构，而不仅仅是模仿固定的输出路径。实验表明，即使在RL-only模式下仅用0.1%的数据进行训练，RLKD也能超越标准的SFT-RL管道，释放出比基于SFT的蒸馏更大的学生推理潜力。", "summary": "本研究提出RLKD，一个基于强化学习的蒸馏框架，旨在解决传统监督微调（SFT）无法有效蒸馏大型语言模型（LLM）中隐式多分支推理结构的问题。RLKD引入了生成式结构奖励模型（GSRM），将推理路径分解为元推理-解决步骤，并计算结构对齐奖励。通过将此奖励与强化学习结合，RLKD使学生LLM能够内化教师的深层推理结构。实验证明，RLKD在少量数据下也能显著提升学生模型的推理能力，优于SFT-RL管道。", "keywords": "强化学习, 知识蒸馏, 大型语言模型, 多分支推理, 奖励模型", "comments": "本文的创新点在于提出了RLKD框架和GSRM，解决了SFT在蒸馏LLM深层推理结构方面的局限性。通过引入对隐式多分支结构的奖励，RLKD能够让学生模型内化更真实的推理过程，而非仅仅模仿表面输出。其在极少量数据下表现优异，显示了其高效性和潜力。"}}
{"id": "2505.15265", "title": "Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs", "authors": ["Zihao Pan", "Yu Tong", "Weibin Wu", "Jingyi Wang", "Lifeng Chen", "Zhe Zhao", "Jiajia Wei", "Yitong Qiao", "Zibin Zheng"], "categories": ["cs.CV", "cs.AI", "cs.CR"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      The paper needs major revisions, so it is being withdrawn", "url": "http://arxiv.org/abs/2505.15265v2", "summary": "Adversarial attacks aim to generate malicious inputs that mislead deep\nmodels, but beyond causing model failure, they cannot provide certain\ninterpretable information such as ``\\textit{What content in inputs make models\nmore likely to fail?}'' However, this information is crucial for researchers to\nspecifically improve model robustness. Recent research suggests that models may\nbe particularly sensitive to certain semantics in visual inputs (such as\n``wet,'' ``foggy''), making them prone to errors. Inspired by this, in this\npaper we conducted the first exploration on large vision-language models\n(LVLMs) and found that LVLMs indeed are susceptible to hallucinations and\nvarious errors when facing specific semantic concepts in images. To efficiently\nsearch for these sensitive concepts, we integrated large language models (LLMs)\nand text-to-image (T2I) models to propose a novel semantic evolution framework.\nRandomly initialized semantic concepts undergo LLM-based crossover and mutation\noperations to form image descriptions, which are then converted by T2I models\ninto visual inputs for LVLMs. The task-specific performance of LVLMs on each\ninput is quantified as fitness scores for the involved semantics and serves as\nreward signals to further guide LLMs in exploring concepts that induce LVLMs.\nExtensive experiments on seven mainstream LVLMs and two multimodal tasks\ndemonstrate the effectiveness of our method. Additionally, we provide\ninteresting findings about the sensitive semantics of LVLMs, aiming to inspire\nfurther in-depth research.", "comment": "The paper needs major revisions, so it is being withdrawn", "pdf_url": "http://arxiv.org/pdf/2505.15265v2", "cate": "cs.CV", "date": "2025-05-21", "updated": "2025-07-25", "AI": {"title_translation": "盲点导航：LVLM中敏感语义概念的进化发现", "tldr": "本文首次探索了大型视觉语言模型（LVLMs）对特定语义概念的敏感性，并提出了一种结合LLMs和T2I模型的语义进化框架来高效发现这些敏感概念，旨在提高LVLMs的鲁棒性。", "motivation": "现有对抗性攻击无法提供模型失败的解释性信息（即输入中何种内容导致模型失败），而这对于研究人员专门提高模型鲁棒性至关重要。最近的研究表明模型可能对视觉输入中的某些语义特别敏感，本文首次探索了LVLMs在此方面的敏感性。", "method": "提出了一种新颖的语义进化框架，融合了大型语言模型（LLMs）和文本到图像（T2I）模型。该框架通过LLM对随机初始化的语义概念进行交叉和变异操作生成图像描述，然后由T2I模型转换为LVLMs的视觉输入。LVLMs在特定任务上的性能被量化为适应度分数，并作为奖励信号进一步指导LLMs探索能诱导LVLMs出错的敏感概念。", "result": "在七个主流LVLMs和两个多模态任务上的大量实验证明了该方法的有效性。此外，本文还提供了关于LVLMs敏感语义的有趣发现。", "conclusion": "本文成功地探索并识别了LVLMs对特定语义概念的敏感性，并提出了一种有效的语义进化框架来发现这些概念，为提高LVLM的鲁棒性提供了重要见解和研究方向。", "translation": "对抗性攻击旨在生成恶意输入以误导深度模型，但除了导致模型失败之外，它们无法提供某些可解释的信息，例如“输入中哪些内容使模型更容易失败？”然而，这些信息对于研究人员专门提高模型鲁棒性至关重要。最近的研究表明，模型可能对视觉输入中的某些语义（例如“湿润”、“多雾”）特别敏感，从而容易出错。受此启发，本文首次对大型视觉语言模型（LVLMs）进行了探索，发现LVLMs在面对图像中的特定语义概念时确实容易产生幻觉和各种错误。为了有效地搜索这些敏感概念，我们整合了大型语言模型（LLMs）和文本到图像（T2I）模型，提出了一种新颖的语义进化框架。随机初始化的语义概念经过基于LLM的交叉和变异操作形成图像描述，然后由T2I模型转换为LVLMs的视觉输入。LVLMs在每个输入上的特定任务性能被量化为相关语义的适应度分数，并作为奖励信号进一步指导LLMs探索能诱导LVLMs出错的概念。在七个主流LVLMs和两个多模态任务上的大量实验证明了我们方法的有效性。此外，我们还提供了关于LVLMs敏感语义的有趣发现，旨在启发进一步的深入研究。", "summary": "本文首次探索了大型视觉语言模型（LVLMs）对特定语义概念的敏感性，发现它们在面对此类概念时容易产生错误。为高效发现这些敏感概念，作者提出了一种结合大型语言模型（LLMs）和文本到图像（T2I）模型的语义进化框架。该框架通过LLM生成图像描述，T2I模型生成视觉输入，并利用LVLM的性能反馈指导LLM迭代搜索敏感语义。实验证明了该方法的有效性，并揭示了LVLMs的敏感语义，为提升模型鲁棒性提供了新思路。", "keywords": "大型视觉语言模型, 语义敏感性, 对抗性攻击, 进化算法, 可解释性", "comments": "本文的创新点在于首次将进化搜索与LLM及T2I模型结合，用于系统性地发现大型视觉语言模型中的“盲点”或敏感语义概念。这为理解模型失败原因提供了一种可解释的方法，超越了传统对抗攻击仅导致模型失败的局限。该研究对于提高LVLM的鲁棒性和安全性具有重要意义，并为未来的可解释AI研究开辟了新方向。"}}
{"id": "2507.19376", "title": "Archiverse: an Approach for Immersive Cultural Heritage", "authors": ["Wieslaw Kopeć", "Anna Jaskulska", "Władysław Fuchs", "Wiktor Stawski", "Stanisław Knapiński", "Barbara Karpowicz", "Rafał Masłyk"], "categories": ["cs.HC", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19376v1", "summary": "Digital technologies and tools have transformed the way we can study cultural\nheritage and the way we can recreate it digitally. Techniques such as laser\nscanning, photogrammetry, and a variety of Mixed Reality solutions have enabled\nresearchers to examine cultural objects and artifacts more precisely and from\nnew perspectives. In this part of the panel, we explore how Virtual Reality\n(VR) and eXtended Reality (XR) can serve as tools to recreate and visualize the\nremains of historical cultural heritage and experience it in simulations of its\noriginal complexity, which means immersive and interactive. Visualization of\nmaterial culture exemplified by archaeological sites and architecture can be\nparticularly useful when only ruins or archaeological remains survive. However,\nthese advancements also bring significant challenges, especially in the area of\ntransdisciplinary cooperation between specialists from many, often distant,\nfields, and the dissemination of virtual immersive environments among both\nprofessionals and the general public.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19376v1", "cate": "cs.HC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "Archiverse：一种沉浸式文化遗产方法", "tldr": "本文探讨了如何利用虚拟现实（VR）和扩展现实（XR）等数字技术，以沉浸式和互动的方式重建和可视化文化遗产，并讨论了跨学科合作和普及面临的挑战。", "motivation": "数字技术改变了文化遗产的研究和数字重建方式，尤其是在仅存遗迹的情况下，需要新的方法来精确审视、重建和可视化文化遗产的原始复杂性。", "method": "采用虚拟现实 (VR) 和扩展现实 (XR) 作为工具，结合激光扫描、摄影测量和混合现实解决方案，以沉浸式和互动的方式重建和可视化历史文化遗产。", "result": "这些技术使得研究人员能够更精确地从新视角审视文化对象和文物，并能以模拟其原始复杂性的方式体验历史文化遗产。", "conclusion": "尽管数字技术带来了进步，但在跨学科合作以及在专业人士和公众中传播虚拟沉浸式环境方面仍存在重大挑战。", "translation": "数字技术和工具改变了我们研究文化遗产的方式以及数字化重建文化遗产的方式。激光扫描、摄影测量和各种混合现实解决方案等技术使研究人员能够更精确地从新的角度审视文化物品和文物。在本部分的专题讨论中，我们探讨了虚拟现实 (VR) 和扩展现实 (XR) 如何作为工具，重建和可视化历史文化遗产的遗迹，并以模拟其原始复杂性的方式（即沉浸式和互动式）体验它。以考古遗址和建筑为例的物质文化可视化在仅存废墟或考古遗迹时尤其有用。然而，这些进步也带来了重大挑战，特别是在来自许多（通常是遥远的）领域的专家之间的跨学科合作方面，以及在专业人士和公众中传播虚拟沉浸式环境方面。", "summary": "Archiverse 提出了一种利用虚拟现实（VR）和扩展现实（XR）等数字技术来沉浸式重建和可视化文化遗产的方法。该方法旨在通过模拟原始复杂性来增强文化遗产的体验，尤其适用于仅存遗迹的情况。尽管技术进步显著，但也面临着跨学科合作和虚拟环境普及的挑战。", "keywords": "沉浸式文化遗产, 虚拟现实, 扩展现实, 数字重建, 跨学科合作", "comments": "该论文的创新点在于利用VR/XR技术提供沉浸式文化遗产体验，突破了传统研究和展示的局限。然而，其重要局限性在于明确指出了跨学科合作以及向专业人士和公众普及虚拟沉MERSIVE环境所面临的挑战，这可能影响其技术的广泛应用和影响力。"}}
{"id": "2507.19408", "title": "On Arbitrary Predictions from Equally Valid Models", "authors": ["Sarah Lockfisch", "Kristian Schwethelm", "Martin Menten", "Rickmer Braren", "Daniel Rueckert", "Alexander Ziller", "Georgios Kaissis"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19408v1", "summary": "Model multiplicity refers to the existence of multiple machine learning\nmodels that describe the data equally well but may produce different\npredictions on individual samples. In medicine, these models can admit\nconflicting predictions for the same patient -- a risk that is poorly\nunderstood and insufficiently addressed.\n  In this study, we empirically analyze the extent, drivers, and ramifications\nof predictive multiplicity across diverse medical tasks and model\narchitectures, and show that even small ensembles can mitigate/eliminate\npredictive multiplicity in practice. Our analysis reveals that (1) standard\nvalidation metrics fail to identify a uniquely optimal model and (2) a\nsubstantial amount of predictions hinges on arbitrary choices made during model\ndevelopment. Using multiple models instead of a single model reveals instances\nwhere predictions differ across equally plausible models -- highlighting\npatients that would receive arbitrary diagnoses if any single model were used.\nIn contrast, (3) a small ensemble paired with an abstention strategy can\neffectively mitigate measurable predictive multiplicity in practice;\npredictions with high inter-model consensus may thus be amenable to automated\nclassification. While accuracy is not a principled antidote to predictive\nmultiplicity, we find that (4) higher accuracy achieved through increased model\ncapacity reduces predictive multiplicity.\n  Our findings underscore the clinical importance of accounting for model\nmultiplicity and advocate for ensemble-based strategies to improve diagnostic\nreliability. In cases where models fail to reach sufficient consensus, we\nrecommend deferring decisions to expert review.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19408v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "关于来自同样有效模型的任意预测", "tldr": "在医学领域，机器学习模型多重性会导致任意预测。本研究经验性地分析了这一问题，并发现即使是小型集成模型也能有效缓解预测多重性，提高诊断可靠性，同时建议在共识不足时寻求专家审查。", "motivation": "机器学习中的模型多重性导致即使同样有效的模型也可能对单个样本产生不同预测。在医学领域，这可能导致对同一患者的冲突诊断，而这种风险目前了解不足且未得到充分解决。", "method": "本研究对跨越不同医疗任务和模型架构的预测多重性的程度、驱动因素和后果进行了经验性分析。研究还探讨了小型集成模型以及结合弃权策略来缓解预测多重性的有效性。", "result": "1. 标准验证指标无法识别出唯一最优模型。\n2. 大量预测取决于模型开发过程中的任意选择。\n3. 小型集成模型结合弃权策略能有效缓解可测量的预测多重性。\n4. 通过增加模型容量获得更高准确性可以减少预测多重性。", "conclusion": "考虑模型多重性具有重要的临床意义。应倡导基于集成模型的策略以提高诊断可靠性。当模型未能达成充分共识时，建议将决策推迟至专家审查。", "translation": "模型多重性指的是存在多个机器学习模型，它们在描述数据方面同样出色，但可能对单个样本产生不同的预测。在医学领域，这些模型可能对同一患者给出相互冲突的预测——这种风险了解不足且未得到充分解决。\n在本研究中，我们经验性地分析了跨不同医疗任务和模型架构的预测多重性的程度、驱动因素和后果，并表明即使是小型集成模型也能在实践中缓解/消除预测多重性。我们的分析揭示了：(1) 标准验证指标未能识别出唯一最优模型；(2) 大量预测取决于模型开发过程中做出的任意选择。使用多个模型而非单个模型揭示了预测在同样合理的模型之间存在差异的实例——这突出了如果使用任何单个模型，患者将获得任意诊断的情况。相比之下，(3) 一个小型集成模型与弃权策略相结合，可以有效地缓解实践中可测量的预测多重性；因此，具有高模型间共识的预测可能适用于自动化分类。尽管准确性并非解决预测多重性的原则性解药，但我们发现 (4) 通过增加模型容量实现的更高准确性可以减少预测多重性。\n我们的研究结果强调了考虑模型多重性的临床重要性，并倡导基于集成模型的策略来提高诊断可靠性。在模型未能达成充分共识的情况下，我们建议将决策推迟给专家审查。", "summary": "本论文探讨了机器学习中的模型多重性问题，特别是在医学领域，即使是同样有效的模型也可能产生冲突的预测。研究通过经验分析揭示，标准验证指标无法确定唯一最优模型，且许多预测是任意的。论文指出，小型集成模型（尤其结合弃权策略）能有效缓解预测多重性，从而提高诊断可靠性。最终，研究强调了考虑模型多重性的临床重要性，并推荐采用集成策略，在模型共识不足时则交由专家审查。", "keywords": "模型多重性, 预测多重性, 集成学习, 医疗AI, 诊断可靠性", "comments": "这篇论文解决了机器学习模型部署中的一个关键问题，尤其是在医疗等高风险领域，模型预测的“任意性”可能带来严重后果。研究发现即使是小型集成模型也能有效缓解这一问题，以及在共识不足时建议专家介入，这些都是对负责任AI开发的实用且重要的贡献。它强调了仅依赖标准指标选出“最佳”单一模型的局限性。"}}
{"id": "2505.24793", "title": "AFIRE: Accurate and Fast Image Reconstruction Algorithm for Geometric-inconsistent Multispectral CT", "authors": ["Yu Gao", "Chong Chen"], "categories": ["math.NA", "cs.NA", "physics.med-ph", "65J15, 65R32, 65J22, 68U10"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      39 pages, 16 figures, 1 table", "url": "http://arxiv.org/abs/2505.24793v2", "summary": "For nonlinear multispectral computed tomography (CT), accurate and fast image\nreconstruction is challenging when the scanning geometries under different\nX-ray energy spectra are inconsistent or mismatched. Motivated by this, we\npropose an Accurate and Fast Image REconstruction (AFIRE) algorithm to address\nsuch problems in the case of mildly full scan. From the continuous (resp.\ndiscrete) setting, we discover that the derivative operator (gradient) of the\ninvolved nonlinear mapping at some special points, for example, at zero, can be\nrepresented as a composition (block multiplication) of a diagonal operator\n(matrix) composed of X-ray transforms (projection matrices) and a very\nsmall-scale matrix. Based on these insights, the AFIRE algorithm is proposed by\nleveraging the simplified Newton method. Under proper conditions, we establish\nthe convergence theory of the proposed algorithm. Furthermore, numerical\nexperiments are also carried out to verify that the proposed algorithm can\naccurately and effectively reconstruct the basis images in completely\ngeometric-inconsistent dual-energy CT with noiseless and noisy projection data.\nParticularly, the proposed algorithm significantly outperforms some\nstate-of-the-art methods in terms of accuracy and efficiency. Finally, the\nflexibility and extensibility of the proposed algorithm are also demonstrated.", "comment": "39 pages, 16 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2505.24793v2", "cate": "math.NA", "date": "2025-05-30", "updated": "2025-07-25", "AI": {"title_translation": "AFIRE：用于几何不一致多光谱CT的精确快速图像重建算法", "tldr": "AFIRE是一种针对几何不一致多光谱CT的精确快速图像重建算法，通过简化牛顿法解决现有挑战，并在准确性和效率上优于现有方法。", "motivation": "当不同X射线能谱下的扫描几何形状不一致或不匹配时，非线性多光谱CT的精确快速图像重建具有挑战性。", "method": "提出了一种名为AFIRE的精确快速图像重建算法。该算法基于对所涉及非线性映射在特殊点（如零点）的导数算子（梯度）的发现，即其可表示为对角算子（矩阵）与X射线变换（投影矩阵）的组合（块乘法）以及一个非常小规模的矩阵的组合。AFIRE算法通过利用简化牛顿法提出，并建立了收敛理论。", "result": "数值实验验证了所提出的算法能够准确有效地重建完全几何不一致双能CT中的基图像，无论是否有噪声投影数据。该算法在准确性和效率方面显著优于一些现有最先进的方法，并且展示了其灵活性和可扩展性。", "conclusion": "AFIRE算法能够有效解决几何不一致多光谱CT中的图像重建挑战，在准确性和效率上表现出色，并具有良好的灵活性和可扩展性。", "translation": "对于非线性多光谱计算机断层扫描（CT），当不同X射线能谱下的扫描几何形状不一致或不匹配时，精确快速的图像重建具有挑战性。受此启发，我们提出了一种精确快速图像重建（AFIRE）算法，以解决轻度全扫描情况下的此类问题。从连续（或离散）设置中，我们发现所涉及的非线性映射在某些特殊点（例如，在零点）的导数算子（梯度）可以表示为由X射线变换（投影矩阵）组成的对角算子（矩阵）与一个非常小规模矩阵的组合（块乘法）。基于这些见解，AFIRE算法通过利用简化牛顿法提出。在适当条件下，我们建立了所提出算法的收敛理论。此外，还进行了数值实验，以验证所提出的算法能够在完全几何不一致的双能CT中，无论是否有噪声投影数据，都能准确有效地重建基图像。特别是，所提出的算法在准确性和效率方面显著优于一些最先进的方法。最后，还展示了所提出算法的灵活性和可扩展性。", "summary": "本文提出了一种名为AFIRE的精确快速图像重建算法，旨在解决非线性多光谱CT中因扫描几何不一致或不匹配导致的图像重建难题。该算法基于对非线性映射导数算子的深入分析，并利用简化牛顿法实现。研究建立了算法的收敛理论，并通过数值实验证明了其在几何不一致双能CT中重建基图像的准确性和高效性，性能显著优于现有方法，并展现了良好的灵活性和可扩展性。", "keywords": "多光谱CT, 图像重建, 几何不一致, AFIRE, 简化牛顿法", "comments": "AFIRE算法的创新之处在于其针对几何不一致多光谱CT的特定挑战，通过对非线性映射导数的独特发现，并结合简化牛顿法来提高重建的准确性和速度。其重要性在于解决了实际应用中常见的几何不匹配问题，并被证明在性能上优于现有技术，这对于提升多光谱CT的临床应用价值具有重要意义。文章还提到了算法的灵活性和可扩展性，预示了其未来在不同CT场景下的应用潜力。"}}
{"id": "2507.12005", "title": "Kernelization for list $H$-coloring for graphs with small vertex cover", "authors": ["Marta Piecyk", "Astrid Pieterse", "Paweł Rzążewski", "Magnus Wahlström"], "categories": ["math.CO", "cs.DS"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.12005v2", "summary": "For a fixed graph $H$, in the List $H$-Coloring problem, we are given a graph\n$G$ along with list $L(v) \\subseteq V(H)$ for every $v \\in V(G)$, and we have\nto determine if there exists a list homomorphism $\\varphi$ from $(G,L)$ to $H$,\ni.e., an edge preserving mapping $\\varphi: V(G)\\to V(H)$ that satisfies\n$\\varphi(v)\\in L(v)$ for every $v\\in V(G)$. Note that if $H$ is the complete\ngraph on $q$ vertices, the problem is equivalent to List $q$-Coloring. We\ninvestigate the kernelization properties of List $H$-Coloring parameterized by\nthe vertex cover number of $G$: given an instance $(G,L)$ and a vertex cover of\n$G$ of size $k$, can we reduce $(G,L)$ to an equivalent instance $(G',L')$ of\nList $H$-Coloring where the size of $G'$ is bounded by a low-degree polynomial\n$p(k)$ in $k$? This question has been investigated previously by Jansen and\nPieterse [Algorithmica 2019], who provided an upper bound, which turns out to\nbe optimal if $H$ is a complete graph, i.e., for List $q$-Coloring. This result\nwas one of the first applications of the method of kernelization via\nbounded-degree polynomials. We define two new integral graph invariants,\n$c^*(H)$ and $d^*(H)$, with $d^*(H) \\leq c^*(H) \\leq d^*(H)+1$, and show that\nfor every graph $H$, List $H$-Coloring\n  -- has a kernel with $\\mathcal{O}(k^{c^*(H)})$ vertices,\n  -- admits no kernel of size $\\mathcal{O}(k^{d^*(H)-\\varepsilon})$ for any\n$\\varepsilon > 0$, unless the polynomial hierarchy collapses.\n  -- Furthermore, if $c^*(H) > d^*(H)$, then there is a kernel with\n$\\mathcal{O}(k^{c^*(H)-\\varepsilon})$ vertices where $\\varepsilon \\geq\n2^{1-c^*(H)}$.\n  Additionally, we show that for some classes of graphs, including powers of\ncycles and graphs $H$ where $\\Delta(H) \\leq c^*(H)$ (which in particular\nincludes cliques), the bound $d^*(H)$ is tight, using the polynomial method. We\nconjecture that this holds in general.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.12005v2", "cate": "math.CO", "date": "2025-07-16", "updated": "2025-07-25", "AI": {"title_translation": "顶点覆盖数较小图的列表H着色的核化", "tldr": "本文研究了列表H着色问题在顶点覆盖数参数下的核化性质，定义了新的图不变量，给出了核大小的紧密上下界。", "motivation": "本文旨在研究列表H着色问题在图顶点覆盖数参数下的核化性质，特别是探索其在顶点覆盖数k作为参数时，实例能否被缩减到大小由k的多项式界定的等价实例。这项研究延续了Jansen和Pieterse先前的工作，并试图为非完全图H的情况提供更通用的界限。", "method": "作者定义了两个新的整数图不变量$c^*(H)$和$d^*(H)$。他们利用这些不变量来推导列表H着色问题的核大小界限。此外，他们还利用多项式方法证明了某些图类别（包括循环图的幂和满足$\\Delta(H) \\leq c^*(H)$的图）的下界是紧密的。", "result": "本文定义了两个新的整数图不变量$c^*(H)$和$d^*(H)$，并证明了：\n* 列表H着色问题存在一个具有$\\mathcal{O}(k^{c^*(H)})$个顶点的核。\n* 除非多项式层级崩溃，否则对于任何$\\varepsilon > 0$，该问题不存在大小为$\\mathcal{O}(k^{d^*(H)-\\varepsilon})$的核。\n* 如果$c^*(H) > d^*(H)$，则存在一个具有$\\mathcal{O}(k^{c^*(H)-\\varepsilon})$个顶点的核，其中$\\varepsilon \\geq 2^{1-c^*(H)}$。\n* 对于某些图类别（包括循环的幂和$\\Delta(H) \\leq c^*(H)$的图），使用多项式方法证明了$d^*(H)$的界限是紧密的。", "conclusion": "本文为列表H着色问题在顶点覆盖数参数下的核化性质建立了紧密的界限，并通过引入新的图不变量来刻画这些界限。研究结果表明了在特定条件下核大小的精确复杂度，并推测了下界的普遍紧密性。", "translation": "对于一个固定的图$H$，在列表$H$着色问题中，给定一个图$G$以及对于$G$中每个顶点$v$的列表$L(v) \\subseteq V(H)$，我们需要确定是否存在一个从$(G,L)$到$H$的列表同态$\\varphi$，即一个保边映射$\\varphi: V(G)\\to V(H)$，满足对于每个$v \\in V(G)$，$ \\varphi(v)\\in L(v)$。注意，如果$H$是$q$个顶点的完全图，则该问题等价于列表$q$着色问题。我们研究了列表$H$着色问题在图$G$的顶点覆盖数参数下的核化性质：给定一个实例$(G,L)$和一个大小为$k$的$G$的顶点覆盖，我们能否将$(G,L)$缩减为一个等价的列表$H$着色实例$(G',L')$，其中$G'$的大小由$k$的低度多项式$p(k)$界定？这个问题之前由Jansen和Pieterse [Algorithmica 2019]研究过，他们提供了一个上界，当$H$是完全图时（即对于列表$q$着色问题），该上界被证明是最优的。这个结果是核化通过有界度多项式方法的最早应用之一。我们定义了两个新的整数图不变量$c^*(H)$和$d^*(H)$，满足$d^*(H) \\leq c^*(H) \\leq d^*(H)+1$，并表明对于每个图$H$，列表$H$着色问题\n-- 有一个具有$\\mathcal{O}(k^{c^*(H)})$个顶点的核，\n-- 除非多项式层级崩溃，否则对于任何$\\varepsilon > 0$，不接受大小为$\\mathcal{O}(k^{d^*(H)-\\varepsilon})$的核。\n-- 此外，如果$c^*(H) > d^*(H)$，则存在一个具有$\\mathcal{O}(k^{c^*(H)-\\varepsilon})$个顶点的核，其中$\\varepsilon \\geq 2^{1-c^*(H)}$。\n此外，我们还表明，对于某些图类别，包括循环的幂和$\\Delta(H) \\leq c^*(H)$的图（其中特别包括团），使用多项式方法证明了$d^*(H)$的界限是紧密的。我们推测这在一般情况下也成立。", "summary": "本文研究了列表H着色问题在图的顶点覆盖数k参数下的核化性质。在现有工作的基础上，作者引入了两个新的整数图不变量$c^*(H)$和$d^*(H)$，并利用它们为核的大小建立了精确的上下界。研究表明存在一个大小为$\\mathcal{O}(k^{c^*(H)})$的核，并且除非多项式层级崩溃，否则不存在大小为$\\mathcal{O}(k^{d^*(H)-\\varepsilon})$的核。此外，当$c^*(H) > d^*(H)$时，他们展示了更紧密的界限，并使用多项式方法证明了$d^*(H)$对于特定图类别的紧密性，同时推测其普遍适用性。", "keywords": "列表H着色, 核化, 顶点覆盖, 图不变量, 多项式方法", "comments": "该论文通过引入新颖的整数图不变量，精确地刻画了核化界限，扩展了先前的工作。其利用多项式方法建立下界和紧密性的做法意义重大，尤其考虑到该方法在该背景下相对较新。关于$d^*(H)$普遍紧密性的猜想为未来的研究提出了一个开放问题。"}}
{"id": "2507.19172", "title": "PhysDrive: A Multimodal Remote Physiological Measurement Dataset for In-vehicle Driver Monitoring", "authors": ["Jiyao Wang", "Xiao Yang", "Qingyong Hu", "Jiankai Tang", "Can Liu", "Dengbo He", "Yuntao Wang", "Yingcong Chen", "Kaishun Wu"], "categories": ["cs.AI", "cs.CV"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      It is the initial version, not the final version", "url": "http://arxiv.org/abs/2507.19172v1", "summary": "Robust and unobtrusive in-vehicle physiological monitoring is crucial for\nensuring driving safety and user experience. While remote physiological\nmeasurement (RPM) offers a promising non-invasive solution, its translation to\nreal-world driving scenarios is critically constrained by the scarcity of\ncomprehensive datasets. Existing resources are often limited in scale, modality\ndiversity, the breadth of biometric annotations, and the range of captured\nconditions, thereby omitting inherent real-world challenges in driving. Here,\nwe present PhysDrive, the first large-scale multimodal dataset for contactless\nin-vehicle physiological sensing with dedicated consideration on various\nmodality settings and driving factors. PhysDrive collects data from 48 drivers,\nincluding synchronized RGB, near-infrared camera, and raw mmWave radar data,\naccompanied with six synchronized ground truths (ECG, BVP, Respiration, HR, RR,\nand SpO2). It covers a wide spectrum of naturalistic driving conditions,\nincluding driver motions, dynamic natural light, vehicle types, and road\nconditions. We extensively evaluate both signal-processing and deep-learning\nmethods on PhysDrive, establishing a comprehensive benchmark across all\nmodalities, and release full open-source code with compatibility for mainstream\npublic toolboxes. We envision PhysDrive will serve as a foundational resource\nand accelerate research on multimodal driver monitoring and smart-cockpit\nsystems.", "comment": "It is the initial version, not the final version", "pdf_url": "http://arxiv.org/pdf/2507.19172v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "PhysDrive：一个用于车载驾驶员监测的多模态远程生理测量数据集", "tldr": "PhysDrive是一个大规模多模态数据集，用于车载远程生理测量，旨在解决现有数据集的局限性并加速相关研究。", "motivation": "稳健且不显眼的车载生理监测对于确保驾驶安全和用户体验至关重要。远程生理测量（RPM）提供了一种有前景的非侵入性解决方案，但由于缺乏全面的数据集，其在实际驾驶场景中的应用受到严重限制。现有资源在规模、模态多样性、生物识别注释广度和捕获条件范围方面通常有限，从而忽略了驾驶中固有的实际挑战。", "method": "本文提出了PhysDrive，这是第一个大规模多模态数据集，用于非接触式车载生理传感，并专门考虑了各种模态设置和驾驶因素。PhysDrive收集了48名驾驶员的数据，包括同步的RGB、近红外相机和原始毫米波雷达数据，并附带六个同步的地面真实数据（ECG、BVP、呼吸、HR、RR和SpO2）。它涵盖了广泛的自然驾驶条件，包括驾驶员动作、动态自然光、车辆类型和道路条件。作者在PhysDrive上广泛评估了信号处理和深度学习方法，建立了所有模态的综合基准，并发布了与主流公共工具箱兼容的完整开源代码。", "result": "在PhysDrive数据集上，作者广泛评估了信号处理和深度学习方法，并在所有模态上建立了全面的基准。同时，发布了与主流公共工具箱兼容的完整开源代码。", "conclusion": "作者设想PhysDrive将作为一个基础资源，加速多模态驾驶员监测和智能座舱系统的研究。", "translation": "PhysDrive：一个用于车载驾驶员监测的多模态远程生理测量数据集\n\n稳健且不显眼的车载生理监测对于确保驾驶安全和用户体验至关重要。虽然远程生理测量（RPM）提供了一种有前景的非侵入性解决方案，但由于缺乏全面的数据集，其在实际驾驶场景中的应用受到严重限制。现有资源通常在规模、模态多样性、生物识别注释广度和捕获条件范围方面有限，从而忽略了驾驶中固有的实际挑战。本文提出了PhysDrive，这是第一个大规模多模态数据集，用于非接触式车载生理传感，并专门考虑了各种模态设置和驾驶因素。PhysDrive收集了48名驾驶员的数据，包括同步的RGB、近红外相机和原始毫米波雷达数据，并附带六个同步的地面真实数据（ECG、BVP、呼吸、HR、RR和SpO2）。它涵盖了广泛的自然驾驶条件，包括驾驶员动作、动态自然光、车辆类型和道路条件。我们广泛评估了PhysDrive上的信号处理和深度学习方法，建立了所有模态的综合基准，并发布了与主流公共工具箱兼容的完整开源代码。我们设想PhysDrive将作为一个基础资源，加速多模态驾驶员监测和智能座舱系统的研究。", "summary": "本文介绍了PhysDrive，一个专为车载驾驶员监测设计的大规模多模态远程生理测量数据集。该数据集旨在解决现有资源在规模、模态多样性和真实驾驶条件覆盖方面的不足。PhysDrive包含来自48名驾驶员的RGB、近红外和毫米波雷达数据，以及ECG、BVP、呼吸、HR、RR和SpO2等六种生理指标的同步地面真实数据，覆盖了多种自然驾驶情境。研究人员已在该数据集上对信号处理和深度学习方法进行了广泛评估，并建立了全面的基准，同时发布了开源代码，旨在推动多模态驾驶员监测和智能座舱系统的研究。", "keywords": "车载监测, 远程生理测量, 多模态数据集, 驾驶安全, PhysDrive", "comments": "PhysDrive的创新之处在于它是首个大规模、多模态、专门针对车载环境的远程生理测量数据集，弥补了现有数据集在真实世界驾驶场景中应用受限的空白。其包含了多样化的模态数据和丰富的地面真实生理指标，并考虑了复杂的驾驶条件，这对于推动驾驶安全和智能座舱技术的发展具有重要意义。通过提供全面的基准和开源代码，该数据集有望成为该领域的基础资源。"}}
{"id": "2507.18895", "title": "Dealing with Segmentation Errors in Needle Reconstruction for MRI-Guided Brachytherapy", "authors": ["Vangelis Kostoulas", "Arthur Guijt", "Ellen M. Kerkhof", "Bradley R. Pieters", "Peter A. N. Bosman", "Tanja Alderliesten"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Published in: Proc. SPIE Medical Imaging 2025, Vol. 13408, 1340826", "url": "http://arxiv.org/abs/2507.18895v1", "summary": "Brachytherapy involves bringing a radioactive source near tumor tissue using\nimplanted needles. Image-guided brachytherapy planning requires amongst others,\nthe reconstruction of the needles. Manually annotating these needles on patient\nimages can be a challenging and time-consuming task for medical professionals.\nFor automatic needle reconstruction, a two-stage pipeline is commonly adopted,\ncomprising a segmentation stage followed by a post-processing stage. While deep\nlearning models are effective for segmentation, their results often contain\nerrors. No currently existing post-processing technique is robust to all\npossible segmentation errors. We therefore propose adaptations to existing\npost-processing techniques mainly aimed at dealing with segmentation errors and\nthereby improving the reconstruction accuracy. Experiments on a prostate cancer\ndataset, based on MRI scans annotated by medical professionals, demonstrate\nthat our proposed adaptations can help to effectively manage segmentation\nerrors, with the best adapted post-processing technique achieving median\nneedle-tip and needle-bottom point localization errors of $1.07$ (IQR $\\pm\n1.04$) mm and $0.43$ (IQR $\\pm 0.46$) mm, respectively, and median shaft error\nof $0.75$ (IQR $\\pm 0.69$) mm with 0 false positive and 0 false negative\nneedles on a test set of 261 needles.", "comment": "Published in: Proc. SPIE Medical Imaging 2025, Vol. 13408, 1340826", "pdf_url": "http://arxiv.org/pdf/2507.18895v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "处理MRI引导近距离放射治疗中针头重建的分割误差", "tldr": "本文提出对现有后处理技术进行改进，以有效处理分割误差，从而提高MRI引导近距离放射治疗中针头重建的准确性。", "motivation": "近距离放射治疗中的针头重建对于图像引导规划至关重要，但手动标注耗时且困难。自动重建的分割阶段常出现误差，而现有后处理技术无法鲁棒地处理所有这些误差，导致重建精度不足。", "method": "本文提出对现有后处理技术进行改进，主要目标是处理分割误差，从而提高重建精度。", "result": "在由医学专业人员标注的MRI扫描前列腺癌数据集上的实验表明，所提出的改进能够有效管理分割误差。最佳改进的后处理技术在261根针头的测试集上，针尖和针底部定位误差中位数分别为1.07毫米（IQR ± 1.04毫米）和0.43毫米（IQR ± 0.46毫米），针杆误差中位数为0.75毫米（IQR ± 0.69毫米），且无假阳性或假阴性针头。", "conclusion": "通过对现有后处理技术进行适应性改进，可以有效地处理分割误差，显著提高MRI引导近距离放射治疗中针头重建的准确性。", "translation": "近距离放射治疗涉及使用植入的针头将放射源带到肿瘤组织附近。图像引导的近距离放射治疗规划除其他外，需要重建针头。在患者图像上手动标注这些针头对医疗专业人员来说可能是一项具有挑战性且耗时的任务。对于自动针头重建，通常采用两阶段流程，包括分割阶段和后处理阶段。虽然深度学习模型对分割有效，但其结果通常包含误差。目前没有现有后处理技术能够鲁棒地处理所有可能的分割误差。因此，我们提出了对现有后处理技术的改进，主要旨在处理分割误差，从而提高重建精度。基于医学专业人员标注的MRI扫描前列腺癌数据集的实验表明，我们提出的改进可以帮助有效管理分割误差，其中最佳的改进后处理技术在261根针头的测试集上，针尖和针底部定位误差中位数分别为1.07（IQR ± 1.04）毫米和0.43（IQR ± 0.46）毫米，针杆误差中位数为0.75（IQR ± 0.69）毫米，且无假阳性或假阴性针头。", "summary": "本文针对MRI引导近距离放射治疗中自动针头重建的分割误差问题，提出对现有后处理技术进行适应性改进。研究指出，尽管深度学习在分割中有效，但其结果常有误差，且当前后处理方法不足以应对。通过实验验证，所提出的改进能有效管理分割误差，显著提升针头重建的精度，实现了较低的定位和针杆误差，且无假阳性/阴性针头。", "keywords": "针头重建, 分割误差, 后处理, 近距离放射治疗, MRI", "comments": "该论文解决了医学图像处理中一个实际且重要的挑战，即在自动针头重建过程中处理分割误差。其创新点在于对现有后处理技术的适应性改进，而不是从头开发新模型，这可能更具实用性和易于集成。实验结果表明了其方法的有效性，尤其是在实现了非常低的定位误差和零假阳性/阴性针头方面，这对于临床应用至关重要。该研究对于提高近距离放射治疗的精度和效率具有重要意义。"}}
{"id": "2507.19076", "title": "SP-Mamba: Spatial-Perception State Space Model for Unsupervised Medical Anomaly Detection", "authors": ["Rui Pan", "Ruiying Lu"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      11 pages", "url": "http://arxiv.org/abs/2507.19076v1", "summary": "Radiography imaging protocols target on specific anatomical regions,\nresulting in highly consistent images with recurrent structural patterns across\npatients. Recent advances in medical anomaly detection have demonstrated the\neffectiveness of CNN- and transformer-based approaches. However, CNNs exhibit\nlimitations in capturing long-range dependencies, while transformers suffer\nfrom quadratic computational complexity. In contrast, Mamba-based models,\nleveraging superior long-range modeling, structural feature extraction, and\nlinear computational efficiency, have emerged as a promising alternative. To\ncapitalize on the inherent structural regularity of medical images, this study\nintroduces SP-Mamba, a spatial-perception Mamba framework for unsupervised\nmedical anomaly detection. The window-sliding prototype learning and\nCircular-Hilbert scanning-based Mamba are introduced to better exploit\nconsistent anatomical patterns and leverage spatial information for medical\nanomaly detection. Furthermore, we excavate the concentration and contrast\ncharacteristics of anomaly maps for improving anomaly detection. Extensive\nexperiments on three diverse medical anomaly detection benchmarks confirm the\nproposed method's state-of-the-art performance, validating its efficacy and\nrobustness. The code is available at https://github.com/Ray-RuiPan/SP-Mamba.", "comment": "11 pages", "pdf_url": "http://arxiv.org/pdf/2507.19076v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "SP-Mamba：用于无监督医学异常检测的空间感知状态空间模型", "tldr": "SP-Mamba利用Mamba模型在医学图像中进行无监督异常检测，通过空间感知和高效计算克服了CNN和Transformer的局限性，并实现了最先进的性能。", "motivation": "现有的CNN模型在捕获长距离依赖方面存在局限性，而Transformer模型则面临二次计算复杂度的挑战。为了克服这些问题并利用Mamba模型在长距离建模、结构特征提取和线性计算效率方面的优势，作者提出了SP-Mamba。", "method": "本文提出了SP-Mamba，一个用于无监督医学异常检测的空间感知Mamba框架。该方法引入了窗口滑动原型学习和基于Circular-Hilbert扫描的Mamba，以更好地利用一致的解剖模式和空间信息。此外，还挖掘了异常图的集中度和对比度特征来改进异常检测。", "result": "在三个不同的医学异常检测基准上的广泛实验证实了所提出方法的最先进性能。", "conclusion": "SP-Mamba在无监督医学异常检测方面表现出有效性和鲁棒性，并在多个基准测试中达到了最先进的性能。", "translation": "放射影像协议针对特定的解剖区域，导致图像高度一致，并在患者之间呈现重复的结构模式。医学异常检测的最新进展已经证明了基于CNN和Transformer方法的有效性。然而，CNN在捕获长距离依赖方面存在局限性，而Transformer则面临二次计算复杂度的困扰。相比之下，基于Mamba的模型凭借卓越的长距离建模、结构特征提取和线性计算效率，已成为一种有前途的替代方案。为了利用医学图像固有的结构规律性，本研究引入了SP-Mamba，一个用于无监督医学异常检测的空间感知Mamba框架。引入了窗口滑动原型学习和基于Circular-Hilbert扫描的Mamba，以更好地利用一致的解剖模式和利用空间信息进行医学异常检测。此外，我们挖掘了异常图的集中度和对比度特征，以改善异常检测。在三个不同的医学异常检测基准上的广泛实验证实了所提出方法的最先进性能，验证了其有效性和鲁棒性。代码可在https://github.com/Ray-RuiPan/SP-Mamba获取。", "summary": "SP-Mamba是一个新的基于Mamba的空间感知框架，专为无监督医学异常检测设计。它旨在克服传统CNN在长距离依赖捕获方面的不足以及Transformer的计算效率问题。通过引入窗口滑动原型学习和基于Circular-Hilbert扫描的Mamba，SP-Mamba能够有效利用医学图像的结构规律和空间信息。该方法还利用异常图的集中度和对比度特性来提高检测效果。实验结果表明，SP-Mamba在多个医学异常检测基准上实现了最先进的性能，证明了其有效性和鲁棒性。", "keywords": "医学异常检测, Mamba, 状态空间模型, 无监督学习, 空间感知", "comments": "这篇论文的创新点在于将Mamba模型引入到医学图像的无监督异常检测领域，有效地解决了CNN和Transformer在长距离依赖和计算效率上的痛点。通过结合空间感知机制（如窗口滑动原型学习和Circular-Hilbert扫描），该方法能更好地适应医学图像的结构特点。其在多个基准上的SOTA表现证明了该方法的强大潜力。"}}
{"id": "2507.18915", "title": "Mining Contextualized Visual Associations from Images for Creativity Understanding", "authors": ["Ananya Sahu", "Amith Ananthram", "Kathleen McKeown"], "categories": ["cs.CL", "cs.CV"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18915v1", "summary": "Understanding another person's creative output requires a shared language of\nassociation. However, when training vision-language models such as CLIP, we\nrely on web-scraped datasets containing short, predominantly literal, alt-text.\nIn this work, we introduce a method for mining contextualized associations for\nsalient visual elements in an image that can scale to any unlabeled dataset.\nGiven an image, we can use these mined associations to generate high quality\ncreative captions at increasing degrees of abstraction. With our method, we\nproduce a new dataset of visual associations and 1.7m creative captions for the\nimages in MSCOCO. Human evaluation confirms that these captions remain visually\ngrounded while exhibiting recognizably increasing abstraction. Moreover,\nfine-tuning a visual encoder on this dataset yields meaningful improvements in\nzero-shot image-text retrieval in two creative domains: poetry and metaphor\nvisualization. We release our dataset, our generation code and our models for\nuse by the broader community.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18915v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "从图像中挖掘语境化视觉关联以理解创造力", "tldr": "本文提出一种从图像中挖掘语境化视觉关联的方法，用于生成高质量的创意描述，并构建了一个新的数据集，显著提升了视觉编码器在创意领域（如诗歌和隐喻可视化）的图文检索性能。", "motivation": "理解他人的创意输出需要共享的联想语言。然而，训练视觉-语言模型（如CLIP）时依赖的网络抓取数据集通常包含简短、主要字面的alt-text，这限制了模型对创意关联的理解。", "method": "引入了一种从图像中挖掘显著视觉元素语境化关联的方法，该方法可扩展到任何未标注的数据集。利用这些挖掘的关联，生成高质量且抽象度不断提高的创意描述。", "result": "生产了一个新的视觉关联数据集，并为MSCOCO中的图像生成了170万条创意描述。人类评估证实这些描述在保持视觉基础的同时，抽象度显著提高。此外，在该数据集上对视觉编码器进行微调，在诗歌和隐喻可视化这两个创意领域的零样本图文检索中取得了显著改进。", "conclusion": "通过挖掘语境化视觉关联并生成创意描述，本方法不仅能生成视觉基础良好且抽象度递增的描述，还能有效提升视觉编码器在创意领域的表现，促进对创造力的理解。", "translation": "理解他人的创意输出需要共享的联想语言。然而，在训练像CLIP这样的视觉-语言模型时，我们依赖于包含简短、主要是字面意义的alt-text的网络抓取数据集。在这项工作中，我们引入了一种从图像中挖掘显著视觉元素语境化关联的方法，该方法可以扩展到任何未标注的数据集。给定一张图像，我们可以使用这些挖掘出的关联来生成高质量、抽象度不断提高的创意描述。通过我们的方法，我们制作了一个新的视觉关联数据集，并为MSCOCO中的图像生成了170万条创意描述。人类评估证实，这些描述在保持视觉基础的同时，表现出可识别的抽象度增加。此外，在该数据集上对视觉编码器进行微调，在诗歌和隐喻可视化这两个创意领域的零样本图文检索中取得了显著改进。我们发布了我们的数据集、生成代码和模型，供更广泛的社区使用。", "summary": "本文提出了一种新颖的方法，旨在从图像中挖掘语境化的视觉关联，以克服现有视觉-语言模型在理解创意关联方面的局限性。该方法能够为图像生成高质量、抽象度递增的创意描述，并构建了一个包含170万条MSCOCO图像创意描述的新数据集。实验证明，利用该数据集微调视觉编码器，显著提升了模型在诗歌和隐喻可视化等创意领域的零样本图文检索性能，为理解人类创造力提供了新的途径。", "keywords": "语境化视觉关联, 创意理解, 图像描述生成, 视觉-语言模型, 数据集", "comments": "这项工作通过提出一种新颖的上下文视觉关联挖掘方法，解决了当前视觉-语言模型在理解和生成创意内容方面的不足。其创新之处在于能够从无标签数据中提取深层次的语义关联，并生成具有不同抽象程度的描述，这对于促进AI在创意领域的应用和理解具有重要意义。数据集的发布也为后续研究提供了宝贵资源。"}}
{"id": "2507.13294", "title": "A Framework of Distributed Source Encryption using Mutual Information Security Criterion and the Strong Converse Theorem", "authors": ["Yasutada Oohama", "Bagus Santoso"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      11 pages, two figures. This is an extended version of the paper accepted for presentation in ITW 2025, which will be held at Sydney form Sept. 29 to Oct. 3 in 2025. The conference accepted paper consists of 5 pages for the text and one page for the reference. arXiv admin note: text overlap with arXiv:2102.06363", "url": "http://arxiv.org/abs/2507.13294v2", "summary": "We reinvestigate the general distributed secure source coding based on the\ncommon key cryptosystem proposed by Oohama and Santoso (ITW 2021). They\nproposed a framework of distributed source encryption and derived the necessary\nand sufficient conditions to have reliable and secure transmission. However,\nthe bounds of the rate region, which specifies both necessary and sufficient\nconditions to have reliable and secure transmission under the proposed\ncryptosystem, were derived based on a self-tailored non-standard} security\ncriterion. In this paper we adopt the standard security criterion, i.e.,\nstandard mutual information. We successfully establish the bounds of the rate\nregion based on this security criterion. Information spectrum method and a\nvariant of Birkhoff-von Neumann theorem play an important role in deriving the\nresult.", "comment": "11 pages, two figures. This is an extended version of the paper\n  accepted for presentation in ITW 2025, which will be held at Sydney form\n  Sept. 29 to Oct. 3 in 2025. The conference accepted paper consists of 5 pages\n  for the text and one page for the reference. arXiv admin note: text overlap\n  with arXiv:2102.06363", "pdf_url": "http://arxiv.org/pdf/2507.13294v2", "cate": "cs.IT", "date": "2025-07-17", "updated": "2025-07-25", "AI": {"title_translation": "基于互信息安全准则和强逆定理的分布式信源加密框架", "tldr": "本文基于标准互信息安全准则，重新研究了分布式安全信源编码，并成功确立了速率区域的界限。", "motivation": "Oohama和Santoso（ITW 2021）提出的分布式信源加密框架，其速率区域的界限是基于非标准安全准则推导的。本文旨在采用标准互信息安全准则来重新确立这些界限。", "method": "采用标准互信息作为安全准则，并利用信息谱方法和Birkhoff-von Neumann定理的变体来推导速率区域的界限。", "result": "成功地基于标准互信息安全准则确立了分布式安全信源编码的速率区域界限。", "conclusion": "通过采用标准互信息安全准则，本文为分布式信源加密提供了更具普适性和理论依据的速率区域界限，改进了现有框架。", "translation": "我们重新研究了Oohama和Santoso（ITW 2021）提出的基于通用密钥密码系统的通用分布式安全信源编码。他们提出了一个分布式信源加密框架，并推导了实现可靠和安全传输的必要和充分条件。然而，在该密码系统下，指定可靠和安全传输的必要和充分条件的速率区域界限是基于一种“自定制的非标准”安全准则推导的。在本文中，我们采用了标准安全准则，即标准互信息。我们成功地基于这一安全准则确立了速率区域的界限。信息谱方法和Birkhoff-von Neumann定理的一个变体在推导结果中发挥了重要作用。", "summary": "本文重新审视了Oohama和Santoso（ITW 2021）提出的分布式安全信源编码框架。针对该框架中速率区域界限基于非标准安全准则的问题，本文采纳了标准的互信息安全准则，并利用信息谱方法和Birkhoff-von Neumann定理的变体，成功地确立了在此标准准则下的速率区域界限，从而改进了分布式信源加密的理论基础。", "keywords": "分布式信源加密, 互信息安全准则, 速率区域, 信息谱方法, 强逆定理", "comments": "本文的创新点在于将分布式信源加密框架中的非标准安全准则替换为更具普适性和理论基础的标准互信息安全准则，并成功推导了相应的速率区域界限。这对于分布式安全通信的理论研究具有重要意义，因为它提供了更严格和标准的性能评估基准。"}}
{"id": "2507.19082", "title": "Bot Appétit! Exploring how Robot Morphology Shapes Perceived Affordances via a Mise en Place Scenario in a VR Kitchen", "authors": ["Rachel Ringe", "Leandra Thiele", "Mihai Pomarlan", "Nima Zargham", "Robin Nolte", "Lars Hurrelbrink", "Rainer Malaka"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Copyright 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "url": "http://arxiv.org/abs/2507.19082v1", "summary": "This study explores which factors of the visual design of a robot may\ninfluence how humans would place it in a collaborative cooking scenario and how\nthese features may influence task delegation. Human participants were placed in\na Virtual Reality (VR) environment and asked to set up a kitchen for cooking\nalongside a robot companion while considering the robot's morphology. We\ncollected multimodal data for the arrangements created by the participants,\ntranscripts of their think-aloud as they were performing the task, and\ntranscripts of their answers to structured post-task questionnaires. Based on\nanalyzing this data, we formulate several hypotheses: humans prefer to\ncollaborate with biomorphic robots; human beliefs about the sensory\ncapabilities of robots are less influenced by the morphology of the robot than\nbeliefs about action capabilities; and humans will implement fewer avoidance\nstrategies when sharing space with gracile robots. We intend to verify these\nhypotheses in follow-up studies.", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "pdf_url": "http://arxiv.org/pdf/2507.19082v1", "cate": "cs.RO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "机器人食欲！通过VR厨房中的准备场景探索机器人形态如何塑造感知可供性", "tldr": "这项研究探索了机器人的视觉设计因素如何影响人类在协作烹饪场景中放置机器人以及这些特征如何影响任务分配。通过VR实验，研究者提出了关于人类偏好、机器人感知和行动能力信念以及规避策略的假设。", "motivation": "本研究旨在探索机器人视觉设计的哪些因素可能影响人类在协作烹饪场景中如何放置机器人，以及这些特征如何影响任务分配。", "method": "研究将人类参与者置于虚拟现实（VR）环境中，要求他们在考虑机器人形态的同时，与机器人同伴一起布置厨房以进行烹饪。研究收集了参与者创建的布置的多模态数据、他们在执行任务时的有声思考记录以及他们对结构化任务后问卷的回答记录。", "result": "基于数据分析，研究提出了几项假设：人类倾向于与仿生机器人协作；人类对机器人感知能力的信念受机器人形态的影响小于对行动能力的信念；与纤细机器人共享空间时，人类会采取较少的规避策略。", "conclusion": "本研究提出了关于机器人形态影响人类协作行为的几项假设，并计划在后续研究中验证这些假设。", "translation": "本研究探讨了机器人视觉设计的哪些因素可能影响人类在协作烹饪场景中如何放置机器人，以及这些特征如何影响任务分配。人类参与者被置于虚拟现实（VR）环境中，并被要求在考虑机器人形态的同时，与机器人同伴一起布置厨房以进行烹饪。我们收集了参与者创建的布置的多模态数据、他们在执行任务时的有声思考记录，以及他们对结构化任务后问卷的回答记录。基于对这些数据的分析，我们提出了几个假设：人类更喜欢与仿生机器人协作；人类对机器人感知能力的信念受机器人形态的影响小于对行动能力的信念；以及当与纤细机器人共享空间时，人类会实施更少的规避策略。我们打算在后续研究中验证这些假设。", "summary": "本研究通过虚拟现实（VR）环境中的厨房布置任务，探讨了机器人形态对人类感知可供性和任务分配的影响。参与者与不同形态的机器人协作，研究收集了多模态数据并分析得出三项主要假设：人类偏好与仿生机器人协作；机器人形态对感知能力信念的影响小于对行动能力信念的影响；与纤细机器人共享空间时，人类的规避行为较少。这些假设有待后续研究验证。", "keywords": "机器人形态, 感知可供性, 虚拟现实, 人机协作, 任务分配", "comments": "这项研究通过创新的VR厨房场景设置，初步探讨了机器人形态对人类协作行为和感知的影响，特别是提出了关于仿生偏好、能力信念和规避策略的假设。其价值在于为未来人机协作机器人设计提供了初步的指导方向，尽管目前仅停留在假设阶段，但为后续的实证研究奠定了基础。"}}
{"id": "2412.13666", "title": "Evaluation of LLM Vulnerabilities to Being Misused for Personalized Disinformation Generation", "authors": ["Aneta Zugecova", "Dominik Macko", "Ivan Srba", "Robert Moro", "Jakub Kopal", "Katarina Marcincinova", "Matus Mesarcik"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      ACL 2025 main", "url": "http://arxiv.org/abs/2412.13666v2", "summary": "The capabilities of recent large language models (LLMs) to generate\nhigh-quality content indistinguishable by humans from human-written texts\nraises many concerns regarding their misuse. Previous research has shown that\nLLMs can be effectively misused for generating disinformation news articles\nfollowing predefined narratives. Their capabilities to generate personalized\n(in various aspects) content have also been evaluated and mostly found usable.\nHowever, a combination of personalization and disinformation abilities of LLMs\nhas not been comprehensively studied yet. Such a dangerous combination should\ntrigger integrated safety filters of the LLMs, if there are some. This study\nfills this gap by evaluating vulnerabilities of recent open and closed LLMs,\nand their willingness to generate personalized disinformation news articles in\nEnglish. We further explore whether the LLMs can reliably meta-evaluate the\npersonalization quality and whether the personalization affects the\ngenerated-texts detectability. Our results demonstrate the need for stronger\nsafety-filters and disclaimers, as those are not properly functioning in most\nof the evaluated LLMs. Additionally, our study revealed that the\npersonalization actually reduces the safety-filter activations; thus\neffectively functioning as a jailbreak. Such behavior must be urgently\naddressed by LLM developers and service providers.", "comment": "ACL 2025 main", "pdf_url": "http://arxiv.org/pdf/2412.13666v2", "cate": "cs.CL", "date": "2024-12-18", "updated": "2025-07-25", "AI": {"title_translation": "评估大型语言模型被滥用于个性化虚假信息生成的漏洞", "tldr": "大型语言模型能够生成个性化虚假信息，而现有安全过滤器无效，甚至因个性化而失效，形同越狱。", "motivation": "先前的研究表明大型语言模型可以生成虚假信息和个性化内容，但个性化与虚假信息相结合的能力尚未得到全面研究。本研究旨在填补这一空白，因为这种危险组合理应触发安全过滤器。", "method": "本研究评估了近期开放和封闭的大型语言模型生成个性化英语虚假新闻文章的漏洞和意愿。此外，还探讨了这些模型能否可靠地元评估个性化质量以及个性化是否影响生成文本的可检测性。", "result": "结果表明，大多数被评估的大型语言模型中的安全过滤器和免责声明功能不正常。个性化实际上降低了安全过滤器的激活，有效地起到了越狱的作用。", "conclusion": "需要更强的安全过滤器和免责声明。大型语言模型开发者和服务提供商必须紧急解决个性化降低安全过滤器激活的问题。", "translation": "最近大型语言模型（LLMs）生成高质量内容的能力，使其与人类撰写的文本难以区分，这引发了对其滥用的诸多担忧。先前的研究表明，LLMs可以有效地被滥用于生成符合预定义叙事的虚假新闻文章。它们生成个性化（在各个方面）内容的能力也得到了评估，并且大多被发现是可用的。然而，LLMs的个性化和虚假信息生成能力的结合尚未得到全面研究。这种危险的组合，如果存在安全过滤器，应该触发LLMs的集成安全过滤器。本研究通过评估近期开放和封闭LLMs的漏洞及其生成个性化英语虚假新闻文章的意愿来填补这一空白。我们进一步探讨了LLMs是否能可靠地元评估个性化质量，以及个性化是否影响生成文本的可检测性。我们的结果表明，需要更强的安全过滤器和免责声明，因为在大多数被评估的LLMs中，这些功能未能正常运作。此外，我们的研究发现，个性化实际上降低了安全过滤器的激活，从而有效地起到了越狱的作用。LLM开发者和服务提供商必须紧急解决这种行为。", "summary": "本研究调查了大型语言模型（LLMs）被滥用于生成个性化虚假信息的未解决漏洞，这是一个先前未被充分探索的危险组合。研究评估了各种LLM生成个性化虚假新闻的倾向，并探讨了LLM是否能评估个性化质量以及个性化是否影响文本的可检测性。研究结果强调，当前的LLM安全过滤器不足，甚至会被个性化绕过，起到“越狱”作用，这需要开发者紧急干预。", "keywords": "大型语言模型, 虚假信息, 个性化, 漏洞, 安全过滤器", "comments": "这篇论文探讨了大型语言模型被滥用于个性化虚假信息生成这一关键且及时的问题。其创新之处在于专门研究了个性化和虚假信息生成这一危险组合，这是以往研究忽视的。研究发现个性化通过降低安全过滤器激活来充当“越狱”机制，这一点尤其深刻和令人警醒，突出了一个需要LLM社区紧急关注的重大漏洞。该研究的重要性在于提高了人们对这种特定滥用方式的认识，并呼吁建立更强的安全机制。"}}
{"id": "2507.19446", "title": "An OpenSource CI/CD Pipeline for Variant-Rich Software-Defined Vehicles", "authors": ["Matthias Weiß", "Anish Navalgund", "Johannes Stümpfle", "Falk Dettinger", "Michael Weyrich"], "categories": ["cs.SE", "cs.DC", "B.8.2; C.2.4"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      7 pages, 5 figures", "url": "http://arxiv.org/abs/2507.19446v1", "summary": "Software-defined vehicles (SDVs) offer a wide range of connected\nfunctionalities, including enhanced driving behavior and fleet management.\nThese features are continuously updated via over-the-air (OTA) mechanisms,\nresulting in a growing number of software versions and variants due to the\ndiversity of vehicles, cloud/edge environments, and stakeholders involved. The\nlack of a unified integration environment further complicates development, as\nconnected mobility solutions are often built in isolation. To ensure reliable\noperations across heterogeneous systems, a dynamic orchestration of functions\nthat considers hardware and software variability is essential. This paper\npresents an open-source CI/CD pipeline tailored for SDVs. It automates the\nbuild, test, and deployment phases using a combination of containerized\nopen-source tools, creating a standardized, portable, and scalable ecosystem\naccessible to all stakeholders. Additionally, a custom OTA middleware\ndistributes software updates and supports rollbacks across vehicles and backend\nservices. Update variants are derived based on deployment target dependencies\nand hardware configurations. The pipeline also supports continuous development\nand deployment of AI models for autonomous driving features. Its effectiveness\nis evaluated using an automated valet parking (AVP) scenario involving\nTurtleBots and a coordinating backend server. Two object detection variants are\ndeveloped and deployed to match hardware-specific requirements. Results\ndemonstrate seamless OTA updates, correct variant selection, and successful\norchestration across all targets. Overall, the proposed pipeline provides a\nscalable and efficient solution for managing software variants and OTA updates\nin SDVs, contributing to the advancement of future mobility technologies.", "comment": "7 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.19446v1", "cate": "cs.SE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "适用于多变体软件定义汽车的开源CI/CD管道", "tldr": "本文提出一个开源CI/CD管道，用于管理软件定义汽车中复杂的软件变体和OTA更新，确保跨异构系统可靠运行。", "motivation": "软件定义汽车（SDVs）的功能通过OTA持续更新，导致软件版本和变体数量不断增加，且缺乏统一的集成环境，使得开发复杂化。为确保在异构系统上的可靠操作，需要动态编排功能并考虑硬件和软件的可变性。", "method": "论文提出了一个为SDVs量身定制的开源CI/CD管道。它结合使用容器化的开源工具，自动化构建、测试和部署阶段，创建一个标准化、可移植和可扩展的生态系统。此外，一个定制的OTA中间件负责分发软件更新并支持车辆和后端服务的版本回滚。更新变体根据部署目标依赖性和硬件配置生成。该管道还支持自动驾驶AI模型的持续开发和部署。", "result": "该管道在自动代客泊车（AVP）场景中进行了评估，涉及TurtleBots和一个协调后端服务器。结果表明，实现了无缝的OTA更新、正确的变体选择以及所有目标之间的成功编排。", "conclusion": "所提出的管道为管理软件定义汽车中的软件变体和OTA更新提供了一个可扩展且高效的解决方案，有助于推动未来移动技术的发展。", "translation": "软件定义汽车（SDVs）提供广泛的互联功能，包括增强的驾驶行为和车队管理。这些功能通过空中下载（OTA）机制持续更新，由于车辆、云/边缘环境和相关利益方的多样性，导致软件版本和变体数量不断增长。缺乏统一的集成环境使开发进一步复杂化，因为互联移动解决方案通常是孤立构建的。为了确保在异构系统上的可靠操作，考虑硬件和软件可变性的功能动态编排至关重要。本文提出了一个专为SDVs定制的开源CI/CD管道。它结合使用容器化的开源工具，自动化构建、测试和部署阶段，创建一个标准化、可移植和可扩展的生态系统，所有利益相关者均可访问。此外，一个定制的OTA中间件分发软件更新并支持车辆和后端服务的版本回滚。更新变体根据部署目标依赖性和硬件配置派生。该管道还支持自动驾驶AI模型的持续开发和部署。其有效性通过一个涉及TurtleBots和协调后端服务器的自动代客泊车（AVP）场景进行评估。开发并部署了两种目标检测变体以匹配特定硬件要求。结果表明，实现了无缝的OTA更新、正确的变体选择以及所有目标之间的成功编排。总的来说，所提出的管道为管理SDVs中的软件变体和OTA更新提供了一个可扩展且高效的解决方案，有助于推动未来移动技术的发展。", "summary": "本文针对软件定义汽车（SDVs）中日益增长的软件变体和OTA更新管理挑战，提出了一个开源CI/CD管道。该管道利用容器化工具自动化构建、测试和部署过程，并包含一个定制的OTA中间件，以支持跨异构系统的更新和回滚，同时考虑硬件和软件差异。通过自动代客泊车场景的评估，验证了其在实现无缝更新、正确变体选择和有效编排方面的能力，为SDVs的软件生命周期管理提供了可扩展且高效的解决方案。", "keywords": "软件定义汽车, CI/CD, OTA更新, 软件变体, 开源管道", "comments": "这篇论文通过提出一个开源的CI/CD管道，有效地解决了软件定义汽车（SDVs）中复杂软件变体和OTA更新的挑战。其创新性在于结合了容器化技术和定制的OTA中间件，实现了跨异构系统的标准化、可移植和可扩展的软件交付。特别值得注意的是，该管道还支持AI模型的持续开发和部署，这对于自动驾驶等前沿应用至关重要。论文通过实际场景的验证，展示了其在无缝更新和正确变体管理方面的有效性，对于推动未来移动技术的发展具有重要意义。"}}
{"id": "2507.19333", "title": "Injecting External Knowledge into the Reasoning Process Enhances Retrieval-Augmented Generation", "authors": ["Minghao Tang", "Shiyu Ni", "Jiafeng Guo", "Keping Bi"], "categories": ["cs.IR", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19333v1", "summary": "Retrieval-augmented generation (RAG) has been widely adopted to augment large\nlanguage models (LLMs) with external knowledge for knowledge-intensive tasks.\nHowever, its effectiveness is often undermined by the presence of noisy (i.e.,\nlow-quality) retrieved passages. Enhancing LLMs' robustness to such noise is\ncritical for improving the reliability of RAG systems. Recent advances have\nequipped LLMs with strong reasoning and self-reflection capabilities, allowing\nthem to identify and correct errors in their reasoning process. Inspired by\nthis ability, we propose Passage Injection-a simple yet effective method that\nexplicitly incorporates retrieved passages into LLMs' reasoning process, aiming\nto enhance the model's ability to recognize and resist noisy passages. We\nvalidate Passage Injection under general RAG settings using BM25 as the\nretriever. Experiments on four reasoning-enhanced LLMs across four factual QA\ndatasets demonstrate that Passage Injection significantly improves overall RAG\nperformance. Further analysis on two noisy retrieval settings-random noise,\nwhere the model is provided irrelevant passages, and counterfactual noise,\nwhere it is given misleading passages-shows that Passage Injection consistently\nimproves robustness. Controlled experiments confirm that Passage Injection can\nalso effectively leverage helpful passages. These findings suggest that\nincorporating passages in LLMs' reasoning process is a promising direction for\nbuilding more robust RAG systems. The code can be found\n\\href{here}{https://github.com/mh-tang/Passage-Injection}.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19333v1", "cate": "cs.IR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "将外部知识注入推理过程增强检索增强生成", "tldr": "本文提出了一种名为“段落注入”的简单有效方法，通过将检索到的段落明确地整合到大型语言模型（LLM）的推理过程中，显著提高了检索增强生成（RAG）系统对嘈杂段落的鲁棒性。", "motivation": "检索增强生成（RAG）的有效性常因检索到低质量或嘈杂的段落而受到损害。提高大型语言模型（LLM）对这种噪声的鲁棒性对于提升RAG系统的可靠性至关重要。", "method": "本文提出“段落注入”方法，该方法将检索到的段落明确地整合到大型语言模型（LLM）的推理过程中。此方法旨在增强模型识别和抵抗嘈杂段落的能力。", "result": "实验表明，“段落注入”显著提高了RAG的整体性能。在随机噪声和反事实噪声两种嘈杂检索设置下，该方法持续提高了鲁棒性。对照实验也证实，它能有效利用有用的段落。", "conclusion": "将检索到的段落整合到大型语言模型（LLM）的推理过程中，是构建更鲁棒的检索增强生成（RAG）系统的一个有前景的方向。", "translation": "检索增强生成 (RAG) 已被广泛采用，用于为大型语言模型 (LLM) 注入外部知识以处理知识密集型任务。然而，其有效性常常因检索到的嘈杂（即低质量）段落的存在而受到损害。增强 LLM 对此类噪声的鲁棒性对于提高 RAG 系统的可靠性至关重要。最近的进展使 LLM 具备了强大的推理和自我反思能力，使其能够识别和纠正推理过程中的错误。受此能力的启发，我们提出了段落注入（Passage Injection）——一种简单而有效的方法，它将检索到的段落明确地整合到 LLM 的推理过程中，旨在增强模型识别和抵抗嘈杂段落的能力。我们在通用 RAG 设置下使用 BM25 作为检索器验证了段落注入。在四个事实问答数据集上对四个推理增强型 LLM 进行的实验表明，段落注入显著提高了 RAG 的整体性能。对两种噪声检索设置（随机噪声，即模型提供了不相关段落；反事实噪声，即提供了误导性段落）的进一步分析表明，段落注入持续提高了鲁棒性。对照实验证实，段落注入也能有效利用有用的段落。这些发现表明，将段落整合到 LLM 的推理过程中是构建更鲁棒的 RAG 系统的一个有前景的方向。代码可在 \n\\href{here}{https://github.com/mh-tang/Passage-Injection} 找到。", "summary": "检索增强生成（RAG）系统在处理嘈杂检索结果时表现不佳。本文提出了一种名为“段落注入”的新方法，通过将检索到的段落显式地融入大型语言模型（LLM）的推理过程，旨在提高模型识别和抵抗噪声的能力。实验证明，该方法显著提升了RAG的整体性能，并在各种噪声环境下持续增强了系统鲁棒性，同时也能有效利用有用信息，为构建更稳健的RAG系统提供了新思路。", "keywords": "RAG, 噪声鲁棒性, 推理过程, 文本注入, LLM", "comments": "本文的创新点在于将检索到的段落明确地整合到LLM的推理过程中，而非仅仅作为上下文提供。这种方法使模型能够主动地识别并抵抗噪声，有效解决了RAG系统中因低质量检索结果导致的性能下降问题。其提出的“段落注入”方法简单而有效，对提高RAG系统的可靠性具有重要意义。"}}
{"id": "2507.10643", "title": "TaylorPODA: A Taylor Expansion-Based Method to Improve Post-Hoc Attributions for Opaque Models", "authors": ["Yuchi Tang", "Iñaki Esnaola", "George Panoutsos"], "categories": ["stat.ML", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      17 pages, 6 figures. To be submitted to AAAI 2026. Re-upload with amended author list", "url": "http://arxiv.org/abs/2507.10643v2", "summary": "Existing post-hoc model-agnostic methods generate external explanations for\nopaque models, primarily by locally attributing the model output to its input\nfeatures. However, they often lack an explicit and systematic framework for\nquantifying the contribution of individual features. Building on the Taylor\nexpansion framework introduced by Deng et al. (2024) to unify existing local\nattribution methods, we propose a rigorous set of postulates -- \"precision\",\n\"federation\", and \"zero-discrepancy\" -- to govern Taylor term-specific\nattribution. Guided by these postulates, we introduce TaylorPODA (Taylor\nexpansion-derived imPortance-Order aDapted Attribution), which incorporates an\nadditional \"adaptation\" property. This property enables alignment with\ntask-specific goals, especially in post-hoc settings lacking ground-truth\nexplanations. Empirical evaluations demonstrate that TaylorPODA achieves\ncompetitive results against baseline methods, providing principled and\nvisualization-friendly explanations. This work represents a step toward the\ntrustworthy deployment of opaque models by offering explanations with stronger\ntheoretical grounding.", "comment": "17 pages, 6 figures. To be submitted to AAAI 2026. Re-upload with\n  amended author list", "pdf_url": "http://arxiv.org/pdf/2507.10643v2", "cate": "stat.ML", "date": "2025-07-14", "updated": "2025-07-25", "AI": {"title_translation": "TaylorPODA：一种基于泰勒展开改进不透明模型事后归因的方法", "tldr": "TaylorPODA提出了一种基于泰勒展开的新方法，通过引入精确性、联合性和零差异性等原则，改进了不透明模型的事后归因解释，并在经验评估中表现出色。", "motivation": "现有事后模型无关方法在量化单个特征贡献时缺乏明确和系统的框架，这阻碍了不透明模型的可靠部署。", "method": "本文在邓等人（2024）提出的泰勒展开框架基础上，提出了“精确性”、“联合性”和“零差异性”等公设来规范泰勒项特异性归因。在此指导下，引入了TaylorPODA方法，它额外包含一个“适应性”特性，使其能够与任务特定目标对齐，尤其是在缺乏真实解释的事后设置中。", "result": "经验评估表明，TaylorPODA与基线方法相比取得了有竞争力的结果，并提供了原则性强且易于可视化的解释。", "conclusion": "TaylorPODA通过提供具有更强理论基础的解释，代表着朝着不透明模型可信部署迈出了一步。", "translation": "现有事后模型无关方法主要通过局部归因模型输出到其输入特征，为不透明模型生成外部解释。然而，它们往往缺乏一个明确和系统的框架来量化单个特征的贡献。本文在邓等人（2024）引入的用于统一现有局部归因方法的泰勒展开框架基础上，提出了一套严谨的公设——“精确性”、“联合性”和“零差异性”——来规范泰勒项特异性归因。在这些公设的指导下，我们引入了TaylorPODA（Taylor expansion-derived imPortance-Order aDapted Attribution），它包含一个额外的“适应性”特性。该特性使得其能够与任务特定目标对齐，尤其是在缺乏真实解释的事后设置中。经验评估表明，TaylorPODA与基线方法相比取得了有竞争力的结果，提供了原则性强且易于可视化的解释。这项工作通过提供具有更强理论基础的解释，代表着朝着不透明模型可信部署迈出了一步。", "summary": "本文提出了TaylorPODA，一种基于泰勒展开的新方法，旨在改进不透明模型的事后归因解释。它基于一套严格的公设（精确性、联合性、零差异性）来量化特征贡献，并引入“适应性”特性以与任务目标对齐。经验评估显示，TaylorPODA在解释效果上优于现有方法，为不透明模型的可信部署提供了理论基础更强的解释。", "keywords": "泰勒展开, 事后归因, 不透明模型, 可解释性AI, 特征贡献", "comments": "TaylorPODA的创新之处在于其将泰勒展开与一套严谨的公设相结合，为事后归因提供了一个更具理论基础的框架。特别是“适应性”属性，使其在缺乏真实解释的实际应用中更具实用性。这项工作对于提升不透明模型的可解释性和可信赖性具有重要意义。"}}
{"id": "2507.18681", "title": "Concept Probing: Where to Find Human-Defined Concepts (Extended Version)", "authors": ["Manuel de Sousa Ribeiro", "Afonso Leote", "João Leite"], "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Extended version of the paper published in Proceedings of the International Conference on Neurosymbolic Learning and Reasoning (NeSy 2025)", "url": "http://arxiv.org/abs/2507.18681v1", "summary": "Concept probing has recently gained popularity as a way for humans to peek\ninto what is encoded within artificial neural networks. In concept probing,\nadditional classifiers are trained to map the internal representations of a\nmodel into human-defined concepts of interest. However, the performance of\nthese probes is highly dependent on the internal representations they probe\nfrom, making identifying the appropriate layer to probe an essential task. In\nthis paper, we propose a method to automatically identify which layer's\nrepresentations in a neural network model should be considered when probing for\na given human-defined concept of interest, based on how informative and regular\nthe representations are with respect to the concept. We validate our findings\nthrough an exhaustive empirical analysis over different neural network models\nand datasets.", "comment": "Extended version of the paper published in Proceedings of the\n  International Conference on Neurosymbolic Learning and Reasoning (NeSy 2025)", "pdf_url": "http://arxiv.org/pdf/2507.18681v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "概念探测：在哪里找到人类定义的概唸（扩展版）", "tldr": "本文提出了一种自动识别神经网络中用于概念探测的最佳层的方法，该方法基于表示的信息性和规律性，并通过详尽的实证分析进行了验证。", "motivation": "概念探测的性能高度依赖于其探测的内部表示，因此识别合适的探测层至关重要。", "method": "本文提出了一种方法，根据表示对于给定的人类定义概念的信息性和规律性，自动识别神经网络模型中应考虑哪个层的表示来进行探测。", "result": "通过对不同的神经网络模型和数据集进行详尽的实证分析，验证了我们的发现。", "conclusion": "通过详尽的实证分析，本文提出的识别最佳探测层的方法得到了验证。", "translation": "概念探测最近作为一种让人类窥探人工神经网络内部编码内容的方式而受到欢迎。在概念探测中，额外的分类器被训练来将模型的内部表示映射到感兴趣的人类定义概念。然而，这些探测器的性能高度依赖于它们所探测的内部表示，这使得识别合适的探测层成为一项基本任务。在本文中，我们提出了一种方法，根据表示对于给定的人类定义概念的信息性和规律性，自动识别神经网络模型中应考虑哪个层的表示来进行探测。我们通过对不同的神经网络模型和数据集进行详尽的实证分析来验证我们的发现。", "summary": "概念探测是理解神经网络内部表示的流行方法，但其效果取决于所选的探测层。本文提出了一种新颖的方法，可以自动识别神经网络中用于概念探测的最佳层，该方法基于内部表示对特定概念的信息性和规律性。研究通过对多种模型和数据集的广泛实证分析验证了该方法的有效性。", "keywords": "概念探测, 神经网络, 内部表示, 层识别, 机器学习", "comments": "本文的创新点在于提出了一个自动化的方法来解决概念探测中选择合适探测层这一关键问题，这对于提高概念探测的效率和准确性具有重要意义。通过考虑表示的信息性和规律性，该方法提供了一个量化的标准来指导层选择，有助于更有效地理解神经网络的内部工作机制。"}}
{"id": "2507.19173", "title": "High-Fidelity RF Mapping: Assessing Environmental Modeling in 6G Network Digital Twins", "authors": ["Lorenzo Cazzella", "Francesco Linsalata", "Damiano Badini", "Matteo Matteucci", "Maurizio Magarini", "Umberto Spagnolini"], "categories": ["eess.SP", "cs.NI"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19173v1", "summary": "The design of accurate Digital Twins (DTs) of electromagnetic environments\nstrictly depends on the fidelity of the underlying environmental modeling.\nEvaluating the differences among diverse levels of modeling accuracy is key to\ndetermine the relevance of the model features towards both efficient and\naccurate DT simulations. In this paper, we propose two metrics, the Hausdorff\nray tracing (HRT) and chamfer ray tracing (CRT) distances, to consistently\ncompare the temporal, angular and power features between two ray tracing\nsimulations performed on 3D scenarios featured by environmental changes. To\nevaluate the introduced metrics, we considered a high-fidelity digital twin\nmodel of an area of Milan, Italy and we enriched it with two different types of\nenvironmental changes: (i) the inclusion of parked vehicles meshes, and (ii)\nthe segmentation of the buildings facade faces to separate the windows mesh\ncomponents from the rest of the building. We performed grid-based and vehicular\nray tracing simulations at 28 GHz carrier frequency on the obtained scenarios\nintegrating the NVIDIA Sionna RT ray tracing simulator with the SUMO vehicular\ntraffic simulator. Both the HRT and CRT metrics highlighted the areas of the\nscenarios where the simulated radio propagation features differ owing to the\nintroduced mesh integrations, while the vehicular ray tracing simulations\nallowed to uncover the distance patterns arising along realistic vehicular\ntrajectories.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19173v1", "cate": "eess.SP", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "高保真射频映射：评估6G网络数字孪生中的环境建模", "tldr": "论文提出了HRT和CRT两种度量方法，用于评估6G网络数字孪生中电磁环境建模的准确性，通过在米兰区域数字孪生模型中引入环境变化进行射线追踪仿真验证了其有效性。", "motivation": "准确的电磁环境数字孪生设计严格依赖于底层环境建模的保真度。评估不同建模精度级别之间的差异是确定模型特征对于高效和准确的数字孪生仿真的相关性的关键。", "method": "论文提出了两种新的度量方法：Hausdorff射线追踪（HRT）距离和Chamfer射线追踪（CRT）距离，用于一致性地比较在具有环境变化的3D场景中执行的两次射线追踪仿真之间的时域、角度和功率特征。为了评估这些度量，研究人员构建了意大利米兰地区的高保真数字孪生模型，并增加了两种环境变化：(i) 包含停放车辆网格，(ii) 分割建筑立面以分离窗户网格组件。研究人员在获得的场景上进行了28 GHz载波频率下的基于网格和车载射线追踪仿真，并集成了NVIDIA Sionna RT射线追踪模拟器和SUMO车辆交通模拟器。", "result": "HRT和CRT两种度量方法都突出了由于引入网格集成导致模拟无线电传播特征存在差异的场景区域。同时，车载射线追踪仿真揭示了沿真实车辆轨迹产生的距离模式。", "conclusion": "论文提出的HRT和CRT度量方法能够有效评估6G网络数字孪生中电磁环境建模的准确性，并识别出环境变化对无线电传播特征的影响。", "translation": "电磁环境的精确数字孪生（DTs）设计严格依赖于底层环境建模的保真度。评估不同建模精度级别之间的差异是确定模型特征对于高效和准确的DT仿真的相关性的关键。在本文中，我们提出了两种度量方法，Hausdorff射线追踪（HRT）和Chamfer射线追踪（CRT）距离，以一致地比较在具有环境变化的3D场景中执行的两次射线追踪仿真之间的时域、角度和功率特征。为了评估引入的度量方法，我们考虑了意大利米兰地区的高保真数字孪生模型，并通过两种不同类型的环境变化对其进行了丰富：（i）包含停放车辆网格，以及（ii）分割建筑物立面以将窗户网格组件与建筑物的其余部分分离。我们在获得的场景上以28 GHz载波频率进行了基于网格和车载射线追踪仿真，并将NVIDIA Sionna RT射线追踪模拟器与SUMO车辆交通模拟器集成。HRT和CRT度量方法都突出了由于引入网格集成导致模拟无线电传播特征存在差异的场景区域，而车载射线追踪仿真则揭示了沿真实车辆轨迹产生的距离模式。", "summary": "本文针对6G网络数字孪生中电磁环境建模的准确性评估问题，提出了Hausdorff射线追踪（HRT）和Chamfer射线追踪（CRT）两种新的距离度量方法。这些度量方法能够量化不同环境建模精度下射线追踪仿真结果在时间、角度和功率特征上的差异。研究团队在米兰地区的高保真数字孪生模型中引入车辆和建筑立面变化，并结合NVIDIA Sionna RT和SUMO进行射线追踪仿真，验证了HRT和CRT在识别环境变化对无线电传播影响方面的有效性。", "keywords": "6G网络, 数字孪生, 射频映射, 射线追踪, 环境建模", "comments": "该论文的创新点在于提出了HRT和CRT两种新颖的度量方法，用于量化和评估6G网络数字孪生中电磁环境建模的保真度。这对于确保未来6G网络的高效和准确仿真至关重要。通过在实际城市场景中引入环境变化并进行详细的射线追踪仿真，论文验证了这些度量的实用性，为数字孪生技术的进一步发展提供了有价值的工具。"}}
{"id": "2507.19029", "title": "Research on Sectionalizing Switches Placement Problem of Distribution System Automation Based on Multi-Objective Optimization Analysis", "authors": ["Selma Cheshmeh Khavar", "Arya Abdollahi"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19029v1", "summary": "Achieving high distribution-reliability levels and concurrently minimizing\noperating costs can be considered as the main issues in distribution system\noptimization. Determination of the optimal number and location of automation\ndevices in the distribution system network is an essential issue from the\nreliability and economical points of view. To address these issues, this paper\ndevelops a multi-objective model, wherein the primary objective, optimal\nautomation devices placement is implemented aiming at minimizing the operating\ncosts, while in the second objective the reliability indices improvement is\ntaken into account. So, modified non dominated sorting genetic algorithm, is\ndeveloped and presented to solve this multi-objective mixed-integer non-linear\nprogramming problem. The feasibility of the proposed algorithm examined by\napplication to two distribution feeders of the Tabriz distribution network\ncontaining the third feeder of the Azar substation with a distributed\ngeneration unit and first and third feeders of ElGoli substation which form a\ndouble feed feeder.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19029v1", "cate": "eess.SY", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "配电系统自动化分段开关选址问题的多目标优化分析研究", "tldr": "本文提出了一种基于改进非支配排序遗传算法的多目标优化模型，用于解决配电系统中自动化设备的最优放置问题，旨在同时提高可靠性并降低运营成本。", "motivation": "配电系统优化中的主要问题是实现高配电可靠性并同时最小化运营成本。从可靠性和经济性角度来看，确定配电系统网络中自动化设备的最佳数量和位置是一个重要问题。", "method": "论文开发了一个多目标模型，其中首要目标是最小化运营成本，次要目标是提高可靠性指标。为了解决这个多目标混合整数非线性规划问题，开发并提出了改进的非支配排序遗传算法。", "result": "所提出的算法通过应用于大不里士配电网络的两个配电馈线（包括带有分布式发电单元的Azar变电站的第三个馈线以及构成双馈线的ElGoli变电站的第一和第三个馈线）验证了其可行性。", "conclusion": "本文提出的改进非支配排序遗传算法能够有效解决配电系统中自动化设备的最优放置这一多目标优化问题，并在实际案例中验证了其可行性。", "translation": "实现高配电可靠性水平并同时最小化运营成本，可以被认为是配电系统优化的主要问题。从可靠性和经济性角度来看，确定配电系统网络中自动化设备的最佳数量和位置是一个重要问题。为了解决这些问题，本文开发了一个多目标模型，其中主要目标是最小化运营成本，实现自动化设备的最佳放置，而第二个目标则考虑了可靠性指标的改进。因此，开发并提出了改进的非支配排序遗传算法，以解决这个多目标混合整数非线性规划问题。所提出的算法通过应用于大不里士配电网络的两个配电馈线（包括带有分布式发电单元的Azar变电站的第三个馈线以及构成双馈线的ElGoli变电站的第一和第三个馈线）验证了其可行性。", "summary": "本文针对配电系统自动化中自动化设备（如分段开关）的最佳选址问题，提出了一个多目标优化模型。该模型旨在同时最小化运营成本和提高系统可靠性。为解决这一复杂的混合整数非线性规划问题，研究者开发并应用了一种改进的非支配排序遗传算法。该算法的有效性通过在Tabriz配电网络的实际馈线案例中进行了验证。", "keywords": "配电系统自动化, 分段开关, 多目标优化, 遗传算法, 可靠性", "comments": "本文的创新点在于将配电系统自动化设备的选址问题建模为一个多目标优化问题，并采用改进的非支配排序遗传算法进行求解，兼顾了经济性和可靠性。这种方法对于提高配电网的运行效率和供电质量具有重要意义。然而，抽象中没有详细说明改进算法的具体细节或与其他方法的比较，这可能限制了对其创新性的全面评估。"}}
{"id": "2411.19083", "title": "ObjectRelator: Enabling Cross-View Object Relation Understanding Across Ego-Centric and Exo-Centric Perspectives", "authors": ["Yuqian Fu", "Runze Wang", "Bin Ren", "Guolei Sun", "Biao Gong", "Yanwei Fu", "Danda Pani Paudel", "Xuanjing Huang", "Luc Van Gool"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV25 (Highlight)", "url": "http://arxiv.org/abs/2411.19083v2", "summary": "Bridging the gap between ego-centric and exo-centric views has been a\nlong-standing question in computer vision. In this paper, we focus on the\nemerging Ego-Exo object correspondence task, which aims to understand object\nrelations across ego-exo perspectives through segmentation. While numerous\nsegmentation models have been proposed, most operate on a single image (view),\nmaking them impractical for cross-view scenarios. PSALM, a recently proposed\nsegmentation method, stands out as a notable exception with its demonstrated\nzero-shot ability on this task. However, due to the drastic viewpoint change\nbetween ego and exo, PSALM fails to accurately locate and segment objects,\nespecially in complex backgrounds or when object appearances change\nsignificantly. To address these issues, we propose ObjectRelator, a novel\napproach featuring two key modules: Multimodal Condition Fusion (MCFuse) and\nSSL-based Cross-View Object Alignment (XObjAlign). MCFuse introduces language\nas an additional cue, integrating both visual masks and textual descriptions to\nimprove object localization and prevent incorrect associations. XObjAlign\nenforces cross-view consistency through self-supervised alignment, enhancing\nrobustness to object appearance variations. Extensive experiments demonstrate\nObjectRelator's effectiveness on the large-scale Ego-Exo4D benchmark and\nHANDAL-X (an adapted dataset for cross-view segmentation) with state-of-the-art\nperformance. Code is made available at: http://yuqianfu.com/ObjectRelator.", "comment": "Accepted by ICCV25 (Highlight)", "pdf_url": "http://arxiv.org/pdf/2411.19083v2", "cate": "cs.CV", "date": "2024-11-28", "updated": "2025-07-25", "AI": {"title_translation": "ObjectRelator：实现跨第一人称和第三人称视角的跨视角对象关系理解", "tldr": "本文提出了ObjectRelator，通过多模态融合和自监督对齐，显著改善了第一人称和第三人称视角之间的跨视角对象分割，并取得了最先进的性能。", "motivation": "计算机视觉领域长期存在弥合第一人称和第三人称视角之间差距的挑战。现有的分割模型大多在单一视图上操作，不适用于跨视角场景。尽管PSALM等方法在零样本能力上有所突破，但在复杂背景或对象外观显著变化时，它们无法准确地定位和分割对象。因此，需要解决新兴的第一人称-第三人称对象对应任务，即通过分割来理解跨视角的对象关系。", "method": "本文提出了ObjectRelator，这是一种新颖的方法，包含两个关键模块：多模态条件融合（MCFuse）和基于SSL的跨视角对象对齐（XObjAlign）。MCFuse引入语言作为额外线索，整合视觉掩码和文本描述以改进对象定位并防止错误关联。XObjAlign通过自监督对齐强制执行跨视角一致性，增强了对对象外观变化的鲁棒性。", "result": "大量的实验表明，ObjectRelator在大型Ego-Exo4D基准和HANDAL-X（一个用于跨视角分割的改编数据集）上表现出有效性，并取得了最先进的性能。", "conclusion": "ObjectRelator通过创新的多模态条件融合和自监督跨视角对象对齐模块，有效解决了第一人称和第三人称视角之间的对象关系理解问题，弥合了现有方法的不足，并在相关基准上取得了最先进的性能。", "translation": "弥合第一人称视角和第三人称视角之间的差距一直是计算机视觉领域的一个长期问题。在本文中，我们专注于新兴的第一人称-第三人称对象对应任务，该任务旨在通过分割来理解跨第一人称-第三人称视角的对象关系。尽管已经提出了许多分割模型，但大多数都在单个图像（视图）上操作，这使得它们在跨视角场景中不切实际。最近提出的分割方法PSALM是一个显著的例外，它在该任务上展示了零样本能力。然而，由于第一人称和第三人称之间视角变化的剧烈，PSALM无法准确地定位和分割对象，尤其是在复杂背景下或当对象外观发生显著变化时。为了解决这些问题，我们提出了ObjectRelator，这是一种新颖的方法，具有两个关键模块：多模态条件融合（MCFuse）和基于SSL的跨视角对象对齐（XObjAlign）。MCFuse引入语言作为额外线索，整合视觉掩码和文本描述以改进对象定位并防止错误关联。XObjAlign通过自监督对齐强制执行跨视角一致性，增强了对对象外观变化的鲁棒性。大量的实验表明，ObjectRelator在大型Ego-Exo4D基准和HANDAL-X（一个用于跨视角分割的改编数据集）上表现出有效性，并取得了最先进的性能。代码已在：http://yuqianfu.com/ObjectRelator 提供。", "summary": "本文针对计算机视觉中长期存在的跨第一人称和第三人称视角对象对应问题，提出了一种名为ObjectRelator的新方法。现有分割模型在跨视角场景中表现不佳，即使是零样本能力强的PSALM也无法在复杂背景或外观变化下准确分割。ObjectRelator通过引入多模态条件融合（MCFuse）和自监督跨视角对象对齐（XObjAlign）两个核心模块来解决此问题。MCFuse利用语言和视觉信息改进对象定位，而XObjAlign通过自监督学习增强跨视角一致性。实验证明，ObjectRelator在Ego-Exo4D和HANDAL-X数据集上均达到了最先进的性能。", "keywords": "跨视角对象理解, 第一人称-第三人称视角, 对象分割, 多模态融合, 自监督学习", "comments": "该论文通过整合多模态信息（视觉和文本）和自监督学习，有效解决了跨视角对象理解这一重要且具有挑战性的问题。其创新点在于提出的MCFuse和XObjAlign模块，它们分别提升了对象定位的准确性和跨视角一致性，这对于弥合第一人称和第三人称视角之间的差距至关重要。该工作为增强计算机视觉系统在复杂真实世界场景中的鲁棒性和泛化能力迈出了重要一步。"}}
{"id": "2507.15158", "title": "Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition", "authors": ["A. H. Abbas", "Hend Abdel-Ghani", "Ivan S. Maksymov"], "categories": ["cs.LG", "physics.app-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15158v2", "summary": "As artificial intelligence continues to push into real-time, edge-based and\nresource-constrained environments, there is an urgent need for novel,\nhardware-efficient computational models. In this study, we present and validate\na neuromorphic computing architecture based on resonant-tunnelling diodes\n(RTDs), which exhibit the nonlinear characteristics ideal for physical\nreservoir computing (RC). We theoretically formulate and numerically implement\nan RTD-based RC system and demonstrate its effectiveness on two image\nrecognition benchmarks: handwritten digit classification and object recognition\nusing the Fruit~360 dataset. Our results show that this circuit-level\narchitecture delivers promising performance while adhering to the principles of\nnext-generation RC -- eliminating random connectivity in favour of a\ndeterministic nonlinear transformation of input signals.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15158v2", "cate": "cs.LG", "date": "2025-07-20", "updated": "2025-07-25", "AI": {"title_translation": "基于谐振隧穿二极管的图像识别储层计算系统", "tldr": "开发了一种基于谐振隧穿二极管（RTD）的储层计算系统，用于高效图像识别，并在两个基准测试中表现出色。", "motivation": "人工智能在实时、边缘和资源受限环境中应用的需求日益增长，亟需新颖、硬件高效的计算模型。", "method": "提出并验证了一种基于谐振隧穿二极管（RTD）的神经形态计算架构，该架构利用RTD的非线性特性进行物理储层计算。理论公式化并数值实现了基于RTD的RC系统，并将其应用于手写数字分类和使用Fruit~360数据集进行物体识别。", "result": "该电路级架构在遵守下一代RC原则（消除随机连接，转而采用确定性非线性输入信号变换）的同时，在图像识别任务中表现出良好的性能。", "conclusion": "该研究表明，基于谐振隧穿二极管的储层计算系统在图像识别方面表现出良好的性能，并且符合下一代储层计算的原则，即采用确定性非线性变换而非随机连接。", "translation": "随着人工智能不断深入实时、边缘和资源受限的环境，对新颖、硬件高效的计算模型的需求日益迫切。在本研究中，我们提出并验证了一种基于谐振隧穿二极管（RTD）的神经形态计算架构，该架构展现出物理储层计算理想的非线性特性。我们从理论上公式化并数值实现了基于RTD的储层计算系统，并在两个图像识别基准测试中展示了其有效性：手写数字分类和使用Fruit~360数据集进行物体识别。我们的结果表明，这种电路级架构在遵守下一代储层计算原则——消除随机连接，转而采用输入信号的确定性非线性变换——的同时，提供了有前景的性能。", "summary": "本研究针对实时、边缘和资源受限环境下对高效计算模型的需求，提出并验证了一种基于谐振隧穿二极管（RTD）的神经形态储层计算系统。该系统利用RTD的非线性特性，通过理论建模和数值实现，在手写数字分类和Fruit~360数据集物体识别等图像识别任务中展现了良好的性能，并符合下一代储层计算采用确定性非线性变换的原则。", "keywords": "谐振隧穿二极管, 储层计算, 图像识别, 神经形态计算, 硬件高效", "comments": "该论文的创新之处在于利用谐振隧穿二极管（RTD）的固有非线性特性构建储层计算系统，这为硬件高效的神经形态计算提供了一种新颖的途径。其重要性在于为实时、边缘和资源受限的人工智能应用提供了一种有前景的解决方案，并且通过采用确定性非线性变换而非随机连接，推动了下一代储层计算的发展。"}}
{"id": "2507.18637", "title": "More Expert-like Eye Gaze Movement Patterns are Related to Better X-ray Reading", "authors": ["Pingjing Yang", "Jennifer Cromley", "Jana Diesner"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      This work will appear at the 26th International Conference on Artificial Intelligence in Education (AIED 2025)", "url": "http://arxiv.org/abs/2507.18637v1", "summary": "Understanding how novices acquire and hone visual search skills is crucial\nfor developing and optimizing training methods across domains. Network analysis\nmethods can be used to analyze graph representations of visual expertise. This\nstudy investigates the relationship between eye-gaze movements and learning\noutcomes among undergraduate dentistry students who were diagnosing dental\nradiographs over multiple semesters. We use network analysis techniques to\nmodel eye-gaze scanpaths as directed graphs and examine changes in network\nmetrics over time. Using time series clustering on each metric, we identify\ndistinct patterns of visual search strategies and explore their association\nwith students' diagnostic performance. Our findings suggest that the network\nmetric of transition entropy is negatively correlated with performance scores,\nwhile the number of nodes and edges as well as average PageRank are positively\ncorrelated with performance scores. Changes in network metrics for individual\nstudents over time suggest a developmental shift from intermediate to\nexpert-level processing. These insights contribute to understanding expertise\nacquisition in visual tasks and can inform the design of AI-assisted learning\ninterventions.", "comment": "This work will appear at the 26th International Conference on\n  Artificial Intelligence in Education (AIED 2025)", "pdf_url": "http://arxiv.org/pdf/2507.18637v1", "cate": "cs.HC", "date": "2025-05-10", "updated": "2025-05-10", "AI": {"title_translation": "更像专家的眼动模式与更好的X射线判读能力相关", "tldr": "该研究发现，更像专家的眼动模式（通过网络分析衡量）与更好的X射线判读表现相关，揭示了视觉技能习得的动态变化。", "motivation": "理解新手如何获取和磨练视觉搜索技能对于开发和优化跨领域的训练方法至关重要。本研究旨在调查眼动模式与牙科放射诊断学习成果之间的关系。", "method": "研究人员使用网络分析技术将眼动轨迹建模为有向图，并检查网络指标随时间的变化。通过对每个指标进行时间序列聚类，识别出不同的视觉搜索策略模式，并探讨它们与学生诊断表现的关联。研究对象是多学期诊断牙科X射线的本科牙科学生。", "result": "研究结果表明，网络指标中的转换熵与表现分数呈负相关，而节点和边的数量以及平均PageRank与表现分数呈正相关。个体学生网络指标随时间的变化表明，存在从中间水平向专家水平处理的发展性转变。", "conclusion": "这些见解有助于理解视觉任务中的专业知识习得，并能为AI辅助学习干预的设计提供信息。", "translation": "理解新手如何获取和磨练视觉搜索技能对于开发和优化跨领域的训练方法至关重要。网络分析方法可用于分析视觉专业知识的图表示。本研究调查了多学期诊断牙科X射线的本科牙科学生中眼动与学习成果之间的关系。我们使用网络分析技术将眼动轨迹建模为有向图，并检查网络指标随时间的变化。通过对每个指标进行时间序列聚类，我们识别出不同的视觉搜索策略模式，并探索它们与学生诊断表现的关联。我们的研究结果表明，网络指标中的转换熵与表现分数呈负相关，而节点和边的数量以及平均PageRank与表现分数呈正相关。个体学生网络指标随时间的变化表明，存在从中间水平向专家水平处理的发展性转变。这些见解有助于理解视觉任务中的专业知识习得，并能为AI辅助学习干预的设计提供信息。", "summary": "本研究利用网络分析方法，将本科牙科学生在诊断牙科X射线时的眼动轨迹建模为有向图，以探究眼动模式与学习成果之间的关系。研究发现，某些网络指标（如转换熵、节点和边数量、PageRank）与诊断表现显著相关，且眼动模式随时间呈现出向专家级处理发展的趋势。这些发现为理解视觉技能习得和设计AI辅助学习干预提供了重要见解。", "keywords": "眼动, 视觉搜索, 网络分析, 技能习得, X射线判读", "comments": "该研究通过将眼动轨迹转化为网络图进行分析，提供了一种新颖的量化视觉搜索策略的方法。其创新之处在于将网络分析应用于眼动数据，揭示了专家级视觉搜索模式的特征。研究结果对于优化专业技能培训，特别是在医学图像诊断等领域，以及开发个性化AI辅助教学工具具有重要意义。"}}
{"id": "2507.18973", "title": "A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation", "authors": ["Bohan Yao", "Vikas Yadav"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      21 pages, 3 figures", "url": "http://arxiv.org/abs/2507.18973v1", "summary": "Augmenting large language models (LLMs) with external tools is a promising\navenue for developing high-performance mathematical reasoning systems. Prior\ntool-augmented approaches typically finetune an LLM to select and invoke a\nsingle tool at each reasoning step and show promising results on simpler math\nreasoning benchmarks such as GSM8K. However, these approaches struggle with\nmore complex math problems that require precise reasoning over multiple steps.\nTo address this limitation, in this work, we propose Multi-TAG, a Multi-Tool\nAGgregation-based framework. Instead of relying on a single tool, Multi-TAG\nguides an LLM to concurrently invoke multiple tools at each reasoning step. It\nthen aggregates their diverse outputs to verify and refine the reasoning\nprocess, enhancing solution robustness and accuracy. Notably, Multi-TAG is a\nfinetuning-free, inference-only framework, making it readily applicable to any\nLLM backbone, including large open-weight models which are computationally\nexpensive to finetune and proprietary frontier models which cannot be finetuned\nwith custom recipes. We evaluate Multi-TAG on four challenging benchmarks:\nMATH500, AIME, AMC, and OlympiadBench. Across both open-weight and\nclosed-source LLM backbones, Multi-TAG consistently and substantially\noutperforms state-of-the-art baselines, achieving average improvements of 6.0%\nto 7.5% over state-of-the-art baselines.", "comment": "21 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.18973v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "一个工具箱，而非一把锤子——Multi-TAG：通过多工具聚合扩展数学推理", "tldr": "Multi-TAG是一个免微调、仅推理的框架，通过同时调用和聚合多个外部工具来增强大型语言模型的数学推理能力，并在复杂数学基准测试上显著优于现有方法。", "motivation": "现有的工具增强型大型语言模型在处理需要多步骤精确推理的复杂数学问题时表现不佳，因为它们通常在每个推理步骤只选择和调用单个工具。", "method": "本研究提出了Multi-TAG（Multi-Tool Aggregation-based framework）框架。Multi-TAG引导大型语言模型在每个推理步骤中同时调用多个工具，然后聚合这些工具的不同输出，以验证和完善推理过程，从而提高解决方案的鲁棒性和准确性。Multi-TAG是一个免微调、仅推理的框架，适用于任何大型语言模型骨干。", "result": "Multi-TAG在MATH500、AIME、AMC和OlympiadBench这四个具有挑战性的基准测试上进行了评估。结果显示，Multi-TAG在开源和闭源大型语言模型骨干上都持续且显著优于最先进的基线方法，平均性能提升了6.0%到7.5%。", "conclusion": "Multi-TAG通过引入多工具聚合范式，成功解决了现有工具增强型LLM在复杂数学推理中的局限性，显著提高了LLM在各种挑战性数学任务上的表现，并且其免微调特性使其具有广泛的适用性。", "translation": "将大型语言模型（LLM）与外部工具结合是开发高性能数学推理系统的一个有前景的方向。先前的工具增强方法通常会对LLM进行微调，使其在每个推理步骤中选择并调用单个工具，并在GSM8K等简单数学推理基准测试上显示出有前景的结果。然而，这些方法在需要多步骤精确推理的更复杂数学问题上表现不佳。为了解决这一限制，本文提出了Multi-TAG，一个基于多工具聚合（Multi-Tool AGgregation）的框架。Multi-TAG不依赖于单一工具，而是引导LLM在每个推理步骤中同时调用多个工具。然后，它聚合这些多样化的输出，以验证和完善推理过程，从而增强解决方案的鲁棒性和准确性。值得注意的是，Multi-TAG是一个免微调、仅推理的框架，这使其可以轻松应用于任何LLM骨干，包括计算成本高昂的微调大型开源模型和无法通过自定义配方进行微调的专有前沿模型。我们在MATH500、AIME、AMC和OlympiadBench这四个具有挑战性的基准测试上评估了Multi-TAG。在开源和闭源LLM骨干上，Multi-TAG都持续且显著优于最先进的基线方法，平均性能比最先进的基线方法提高了6.0%到7.5%。", "summary": "该论文提出了Multi-TAG框架，旨在解决现有工具增强型大型语言模型在复杂数学推理中只能调用单一工具的局限性。Multi-TAG允许大型语言模型在每个推理步骤中同时调用多个外部工具，并聚合它们的输出以验证和完善推理。作为一个免微调、仅推理的框架，Multi-TAG适用于各种大型语言模型。在MATH500、AIME、AMC和OlympiadBench等四个挑战性数学基准测试中，Multi-TAG在不同LLM骨干上均显著优于现有最先进的基线方法，平均性能提升6.0%至7.5%。", "keywords": "多工具聚合, 数学推理, 大型语言模型, 工具增强, 免微调", "comments": "Multi-TAG的创新之处在于其“多工具聚合”范式，打破了传统工具增强LLM单步单工具的限制，显著提升了LLM在复杂数学推理中的表现。其免微调、仅推理的特性使其具有高度的实用性和广泛的适用性，尤其对于无法微调的专有模型和计算昂贵的开源大模型。这为LLM在实际应用中处理复杂任务提供了新的思路。"}}
{"id": "2507.19036", "title": "Neural Ordinary Differential Equations for Learning and Extrapolating System Dynamics Across Bifurcations", "authors": ["Eva van Tegelen", "George van Voorn", "Ioannis Athanasiadis", "Peter van Heijster"], "categories": ["cs.LG", "math.DS"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19036v1", "summary": "Forecasting system behaviour near and across bifurcations is crucial for\nidentifying potential shifts in dynamical systems. While machine learning has\nrecently been used to learn critical transitions and bifurcation structures\nfrom data, most studies remain limited as they exclusively focus on\ndiscrete-time methods and local bifurcations. To address these limitations, we\nuse Neural Ordinary Differential Equations which provide a continuous,\ndata-driven framework for learning system dynamics. We apply our approach to a\npredator-prey system that features both local and global bifurcations,\npresenting a challenging test case. Our results show that Neural Ordinary\nDifferential Equations can recover underlying bifurcation structures directly\nfrom timeseries data by learning parameter-dependent vector fields. Notably, we\ndemonstrate that Neural Ordinary Differential Equations can forecast\nbifurcations even beyond the parameter regions represented in the training\ndata. We also assess the method's performance under limited and noisy data\nconditions, finding that model accuracy depends more on the quality of\ninformation that can be inferred from the training data, than on the amount of\ndata available.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19036v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "用于学习和外推跨分岔系统动力学的神经常微分方程", "tldr": "本文提出使用神经常微分方程（NODEs）来学习和预测动态系统在分岔点附近及跨越分岔时的行为，并证明其能从时间序列数据中恢复分岔结构，甚至能外推到训练数据范围之外。", "motivation": "预测系统在分岔点附近和跨越分岔时的行为对于识别动态系统中潜在的转变至关重要。现有的机器学习方法大多局限于离散时间方法和局部性分岔，存在局限性。", "method": "本文使用神经常微分方程（NODEs）作为一种连续的、数据驱动的框架来学习系统动力学。将该方法应用于一个具有局部和全局分岔的捕食者-猎物系统，作为一个具有挑战性的测试案例。", "result": "结果表明，神经常微分方程可以通过学习依赖于参数的向量场，直接从时间序列数据中恢复潜在的分岔结构。值得注意的是，该方法甚至可以预测超出训练数据所代表的参数区域的分岔。此外，在有限和噪声数据条件下评估了该方法的性能，发现模型准确性更多地取决于可以从训练数据中推断出的信息质量，而非可用数据量。", "conclusion": "神经常微分方程提供了一个强大的连续数据驱动框架，可以有效地学习和外推跨越局部和全局分岔的系统动力学，即使在数据有限或存在噪声的情况下也能表现良好，并且能够进行超范围预测。", "translation": "预测系统在分岔点附近和跨越分岔时的行为对于识别动态系统中潜在的转变至关重要。尽管机器学习最近被用于从数据中学习临界转变和分岔结构，但大多数研究仍然受限，因为它们只关注离散时间方法和局部性分岔。为了解决这些局限性，我们使用神经常微分方程，它提供了一个连续的、数据驱动的框架来学习系统动力学。我们将我们的方法应用于一个具有局部和全局分岔的捕食者-猎物系统，提出了一个具有挑战性的测试案例。我们的结果表明，神经常微分方程可以通过学习依赖于参数的向量场，直接从时间序列数据中恢复潜在的分岔结构。值得注意的是，我们证明了神经常微分方程甚至可以预测超出训练数据所代表的参数区域的分岔。我们还在有限和噪声数据条件下评估了该方法的性能，发现模型准确性更多地取决于可以从训练数据中推断出的信息质量，而非可用数据量。", "summary": "本文提出了一种利用神经常微分方程（NODEs）来学习和外推动态系统在分岔点附近及跨越分岔时的行为的新方法。针对现有机器学习方法在处理连续系统和全局分岔时的局限性，研究者将NODEs应用于一个复杂的捕食者-猎物系统。实验结果表明，NODEs能够有效地从时间序列数据中恢复潜在的分岔结构，并通过学习参数依赖的向量场实现对未知参数区域的分岔预测。此外，研究还发现模型精度在数据有限或有噪声的情况下，更多地取决于数据中可推断信息的质量而非数据量本身。", "keywords": "神经常微分方程, 分岔, 系统动力学, 时间序列, 外推", "comments": "本文的创新点在于将神经常微分方程应用于系统动力学中的分岔学习和外推，突破了传统离散时间方法和局部性分岔的局限。其能够从时间序列数据中直接学习参数依赖的向量场，并实现超范围的分岔预测，这对于理解和预测复杂系统的行为具有重要意义。此外，研究还强调了数据质量而非数量在模型性能中的关键作用，为实际应用提供了指导。"}}
{"id": "2507.18673", "title": "Design and Implementation of Parametrized Look-Up Tables for Post-Correction of Oversampling Low-Resolution ADCs", "authors": ["Morriel Kasher", "Michael Tinston", "Predrag Spasojevic"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      13 pages, 20 figures. arXiv admin note: text overlap with arXiv:2507.18370", "url": "http://arxiv.org/abs/2507.18673v1", "summary": "We propose a framework for the design, optimization, and implementation of\nLook-Up Tables (LUTs) used to recover noisy, oversampled, quantized signals\ngiven a parametric input model. The LUTs emulate the spectral effects of\npre-quantization dithering through an all-digital solution applied after\nquantization. This methodology decomposes the intractable LUT design problem\ninto four distinct stages, each of which is addressed analytically using a\nmodel-driven approach without reliance on training. Three dithering methods are\nstudied to improve spectral purity metrics. Two novel indexing schemes are\nproposed to limit the LUT memory overhead shown to compress the LUT size by\nover four orders of magnitude with marginal performance loss. The LUT design is\ntested with an oversampled noisy sinusoidal input quantized to 3 bits and shown\nto improve its Spurious-Free Dynamic Range (SFDR) by over 19 dBc with only 324\nbytes of memory while maintaining the same 3-bit fixed-point precision at the\ndigital output. This correction can be implemented using two-level\ncombinational logic ensuring ultra-low latency and, hence, suitable for\nlow-resolution wideband devices.", "comment": "13 pages, 20 figures. arXiv admin note: text overlap with\n  arXiv:2507.18370", "pdf_url": "http://arxiv.org/pdf/2507.18673v1", "cate": "eess.SP", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "过采样低分辨率ADC后校正参数化查找表的设计与实现", "tldr": "提出一种用于过采样低分辨率ADC后校正的参数化查找表（LUT）设计框架，通过全数字解决方案模拟抖动，显著提高SFDR并大幅压缩LUT内存。", "motivation": "恢复噪声、过采样、量化信号，特别是针对低分辨率ADC的后校正，以提高频谱纯度。", "method": "本文提出了一个用于查找表（LUTs）设计、优化和实现的框架，用于恢复噪声、过采样、量化信号。该框架通过全数字解决方案在量化后模拟预量化抖动的频谱效应。该方法将难以处理的LUT设计问题分解为四个独立的分析阶段，采用模型驱动方法且不依赖训练。研究了三种抖动方法以改善频谱纯度指标，并提出了两种新颖的索引方案以大幅限制LUT内存开销。", "result": "在3位量化的过采样噪声正弦输入测试中，LUT设计将无杂散动态范围（SFDR）提高了超过19 dBc，仅使用324字节内存，同时保持数字输出相同的3位定点精度。LUT大小被压缩了四个数量级以上，且性能损失微乎其微。", "conclusion": "所提出的校正方法可以使用两级组合逻辑实现，确保超低延迟，因此适用于低分辨率宽带设备。", "translation": "我们提出了一个用于查找表（LUTs）设计、优化和实现的框架，该查找表用于在给定参数输入模型的情况下恢复噪声、过采样、量化信号。这些LUT通过在量化后应用的全数字解决方案模拟预量化抖动的频谱效应。这种方法将难以处理的LUT设计问题分解为四个不同的阶段，每个阶段都通过模型驱动的方法进行分析解决，不依赖于训练。研究了三种抖动方法以改善频谱纯度指标。提出了两种新颖的索引方案来限制LUT内存开销，结果表明LUT大小压缩了四个数量级以上，而性能损失微乎其微。该LUT设计在3位量化的过采样噪声正弦输入下进行了测试，结果显示其无杂散动态范围（SFDR）提高了超过19 dBc，仅使用324字节内存，同时保持数字输出相同的3位定点精度。这种校正可以通过两级组合逻辑实现，确保超低延迟，因此适用于低分辨率宽带设备。", "summary": "本文提出一个用于过采样低分辨率模数转换器（ADCs）后校正的参数化查找表（LUT）设计、优化和实现框架。该框架采用全数字解决方案，在量化后模拟预量化抖动效果，并通过模型驱动的分析方法解决LUT设计问题。研究了多种抖动方法并引入了两种新颖的索引方案，显著压缩了LUT内存。实验结果表明，该方法在3位量化输入下能将SFDR提高超过19 dBc，同时内存占用极低（324字节），并保持高精度和超低延迟，使其适用于低分辨率宽带设备。", "keywords": "查找表, 后校正, 过采样ADC, 抖动, SFDR", "comments": "本文的创新点在于提出了一个无需训练、基于模型驱动的LUT设计框架，用于低分辨率ADC的后校正。通过模拟预量化抖动和引入创新的索引方案，极大地减少了LUT的内存需求，并实现了显著的SFDR提升。其全数字、低延迟的特性使其在资源受限的宽带设备中具有重要应用潜力。"}}
{"id": "2507.19420", "title": "CircuitProbe: Dissecting Spatiotemporal Visual Semantics with Circuit Tracing", "authors": ["Yiming Zhang", "Chengzhang Yu", "Zhuokai Zhao", "Kun Wang", "Qiankun Li", "Zihan Chen", "Yang Liu", "Zenghui Ding", "Yining Sun"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19420v1", "summary": "The processing mechanisms underlying language and image understanding in\nlarge vision-language models (LVLMs) have been extensively studied. However,\nthe internal reasoning mechanisms of LVLMs for spatiotemporal understanding\nremain poorly understood. In this work, we introduce a systematic,\ncircuit-based framework designed to investigate how spatiotemporal visual\nsemantics are represented and processed within these LVLMs. Specifically, our\nframework comprises three circuits: visual auditing circuit, semantic tracing\ncircuit, and attention flow circuit. Through the lens of these circuits, we\ndiscover that visual semantics are highly localized to specific object\ntokens--removing these tokens can degrade model performance by up to 92.6%.\nFurthermore, we identify that interpretable concepts of objects and actions\nemerge and become progressively refined in the middle-to-late layers of LVLMs.\nIn contrary to the current works that solely focus on objects in one image, we\nreveal that the middle-to-late layers of LVLMs exhibit specialized functional\nlocalization for spatiotemporal semantics. Our findings offer significant\nmechanistic insights into spatiotemporal semantics analysis of LVLMs, laying a\nfoundation for designing more robust and interpretable models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19420v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "CircuitProbe：使用电路追踪剖析时空视觉语义", "tldr": "本文引入CircuitProbe框架，通过电路追踪揭示大型视觉-语言模型中时空视觉语义的内部处理机制。", "motivation": "大型视觉-语言模型（LVLMs）在语言和图像理解方面的处理机制已被广泛研究，但其在时空理解方面的内部推理机制仍知之甚少。", "method": "作者提出了一个系统的、基于电路的框架CircuitProbe，用于研究时空视觉语义在LVLMs中如何表示和处理。该框架包含三个电路：视觉审计电路、语义追踪电路和注意力流电路。", "result": "研究发现视觉语义高度局限于特定的对象tokens，移除这些tokens可导致模型性能下降高达92.6%。此外，可解释的对象和动作概念在LVLMs的中后期层中出现并逐渐完善。与当前仅关注单幅图像中对象的工作不同，研究揭示LVLMs的中后期层表现出对时空语义的专门功能局部化。", "conclusion": "本文的研究结果为LVLMs的时空语义分析提供了重要的机制性见解，为设计更鲁棒和可解释的模型奠定了基础。", "translation": "大型视觉-语言模型（LVLMs）中语言和图像理解背后的处理机制已被广泛研究。然而，LVLMs用于时空理解的内部推理机制仍知之甚少。在这项工作中，我们引入了一个系统的、基于电路的框架，旨在研究这些LVLMs中时空视觉语义如何被表示和处理。具体来说，我们的框架包含三个电路：视觉审计电路、语义追踪电路和注意力流电路。通过这些电路的视角，我们发现视觉语义高度局限于特定的对象tokens——移除这些tokens可导致模型性能下降高达92.6%。此外，我们发现可解释的对象和动作概念在LVLMs的中后期层中出现并逐渐完善。与当前仅关注单幅图像中对象的工作不同，我们揭示LVLMs的中后期层表现出对时空语义的专门功能局部化。我们的发现为LVLMs的时空语义分析提供了重要的机制性见解，为设计更鲁棒和可解释的模型奠定了基础。", "summary": "本文介绍了CircuitProbe，一个基于电路的框架，用于深入探究大型视觉-语言模型（LVLMs）中时空视觉语义的内部表示和处理机制。该框架由视觉审计电路、语义追踪电路和注意力流电路组成。研究发现，视觉语义高度集中于特定的对象tokens，移除这些tokens会显著降低模型性能。此外，可解释的对象和动作概念在LVLMs的中后期层中逐渐形成并完善，并且这些层展现出对时空语义的专门功能局部化。这些发现为理解LVLMs如何处理时空信息提供了关键的机制性洞察，并为开发更可靠、更具解释性的模型提供了基础。", "keywords": "大型视觉-语言模型, 时空语义, 电路追踪, 模型解释性, 功能局部化", "comments": "该论文通过提出CircuitProbe框架，创新性地运用“电路追踪”的方法来剖析大型视觉-语言模型中复杂的时空语义处理机制，这为理解模型内部运作提供了新的视角。其发现视觉语义的局部化特性以及概念在中后期层的演变，对提升模型的鲁棒性和可解释性具有重要意义。"}}
{"id": "2507.19427", "title": "Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding", "authors": ["StepFun", ":", "Bin Wang", "Bojun Wang", "Changyi Wan", "Guanzhe Huang", "Hanpeng Hu", "Haonan Jia", "Hao Nie", "Mingliang Li", "Nuo Chen", "Siyu Chen", "Song Yuan", "Wuxun Xie", "Xiaoniu Song", "Xing Chen", "Xingping Yang", "Xuelin Zhang", "Yanbo Yu", "Yaoyu Wang", "Yibo Zhu", "Yimin Jiang", "Yu Zhou", "Yuanwei Lu", "Houyi Li", "Jingcheng Hu", "Ka Man Lo", "Ailin Huang", "Binxing Jiao", "Bo Li", "Boyu Chen", "Changxin Miao", "Chang Lou", "Chen Hu", "Chen Xu", "Chenfeng Yu", "Chengyuan Yao", "Daokuan Lv", "Dapeng Shi", "Deshan Sun", "Ding Huang", "Dingyuan Hu", "Dongqing Pang", "Enle Liu", "Fajie Zhang", "Fanqi Wan", "Gulin Yan", "Han Zhang", "Han Zhou", "Hanghao Wu", "Hangyu Guo", "Hanqi Chen", "Hanshan Zhang", "Hao Wu", "Haocheng Zhang", "Haolong Yan", "Haoran Lv", "Haoran Wei", "Hebin Zhou", "Heng Wang", "Heng Wang", "Hongxin Li", "Hongyu Zhou", "Hongyuan Wang", "Huiyong Guo", "Jia Wang", "Jiahao Gong", "Jialing Xie", "Jian Zhou", "Jianjian Sun", "Jiaoren Wu", "Jiaran Zhang", "Jiayu Liu", "Jie Cheng", "Jie Luo", "Jie Yan", "Jie Yang", "Jieyi Hou", "Jinguang Zhang", "Jinlan Cao", "Jisheng Yin", "Junfeng Liu", "Junhao Huang", "Junzhe Lin", "Kaijun Tan", "Kaixiang Li", "Kang An", "Kangheng Lin", "Kenkun Liu", "Lei Yang", "Liang Zhao", "Liangyu Chen", "Lieyu Shi", "Liguo Tan", "Lin Lin", "Lin Zhang", "Lina Chen", "Liwen Huang", "Liying Shi", "Longlong Gu", "Mei Chen", "Mengqiang Ren", "Ming Li", "Mingzhe Chen", "Na Wang", "Nan Wu", "Qi Han", "Qian Zhao", "Qiang Zhang", "Qianni Liu", "Qiaohui Chen", "Qiling Wu", "Qinglin He", "Qinyuan Tan", "Qiufeng Wang", "Qiuping Wu", "Qiuyan Liang", "Quan Sun", "Rui Li", "Ruihang Miao", "Ruosi Wan", "Ruyan Guo", "Shangwu Zhong", "Shaoliang Pang", "Shengjie Fan", "Shijie Shang", "Shilei Jiang", "Shiliang Yang", "Shiming Hao", "Shuli Gao", "Siming Huang", "Siqi Liu", "Tiancheng Cao", "Tianhao Cheng", "Tianhao Peng", "Wang You", "Wei Ji", "Wen Sun", "Wenjin Deng", "Wenqing He", "Wenzhen Zheng", "Xi Chen", "Xiangwen Kong", "Xianzhen Luo", "Xiaobo Yang", "Xiaojia Liu", "Xiaoxiao Ren", "Xin Han", "Xin Li", "Xin Wu", "Xu Zhao", "Yanan Wei", "Yang Li", "Yangguang Li", "Yangshijie Xu", "Yanming Xu", "Yaqiang Shi", "Yeqing Shen", "Yi Yang", "Yifei Yang", "Yifeng Gong", "Yihan Chen", "Yijing Yang", "Yinmin Zhang", "Yizhuang Zhou", "Yuanhao Ding", "Yuantao Fan", "Yuanzhen Yang", "Yuchu Luo", "Yue Peng", "Yufan Lu", "Yuhang Deng", "Yuhe Yin", "Yujie Liu", "Yukun Chen", "Yuling Zhao", "Yun Mou", "Yunlong Li", "Yunzhou Ju", "Yusheng Li", "Yuxiang Yang", "Yuxiang Zhang", "Yuyang Chen", "Zejia Weng", "Zhe Xie", "Zheng Ge", "Zheng Gong", "Zhenyi Lu", "Zhewei Huang", "Zhichao Chang", "Zhiguo Huang", "Zhirui Wang", "Zidong Yang", "Zili Wang", "Ziqi Wang", "Zixin Zhang", "Binxing Jiao", "Daxin Jiang", "Heung-Yeung Shum", "Xiangyu Zhang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19427v1", "summary": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19427v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "Step-3 规模庞大但经济实惠：模型-系统协同设计实现经济高效的解码", "tldr": "本文介绍了Step-3，一个3210亿参数的VLM，通过硬件感知的模型-系统协同设计（包括MFA和AFD）来最小化LLM解码成本，显著提高了吞吐量并降低了长上下文推理的成本。", "motivation": "大型语言模型（LLMs）在解码过程中，尤其是在长上下文推理任务中，面临硬件效率低下和高成本的问题。", "method": "本文引入了Step-3，一个3210亿参数的VLM，采用硬件感知的模型-系统协同设计，以最小化解码成本。其主要创新包括：1. 一种新颖的多矩阵分解注意力（MFA）机制，显著减少KV缓存大小和计算量，同时保持高注意力表达能力。2. 注意力-前馈网络分离（AFD），一个分布式推理系统，将注意力层和前馈网络（FFN）层解耦到专门的子系统中。", "result": "Step-3与DeepSeek-V3和Qwen3 MoE 235B等模型相比，显著降低了理论解码成本，并且在更长上下文下增益更大。Step-3在激活每token 380亿参数的同时实现了低成本。在Hopper GPU上的实现达到了每GPU每秒高达4,039个token的解码吞吐量（在4K上下文、FP8、无MTP条件下，50ms TPOT SLA），高于DeepSeek-V3在相同设置下的2,324个token，并为LLM解码设置了新的帕累托前沿。", "conclusion": "硬件对齐的注意力算术强度、MoE稀疏性和AFD对于实现成本效益高的LLM解码至关重要。Step-3证明了通过模型-系统协同设计可以显著提高LLM解码效率，并设定了新的性能-成本帕累托前沿。", "translation": "大型语言模型（LLMs）在解码过程中，尤其是在长上下文推理任务中，面临硬件效率低下问题。本文介绍了Step-3，一个3210亿参数的VLM，通过硬件感知的模型-系统协同设计优化，旨在最小化解码成本。Step-3在两个关键维度上进行了创新：（1）一种新颖的多矩阵分解注意力（MFA）机制，显著减少了KV缓存大小和计算量，同时保持高注意力表达能力，以及（2）注意力-前馈网络分离（AFD），一个分布式推理系统，将注意力层和前馈网络（FFN）层解耦到专门的子系统中。这种协同设计实现了前所未有的成本效率：Step-3与DeepSeek-V3和Qwen3 MoE 235B等模型相比，显著降低了理论解码成本，并且在更长上下文下增益更大。Step-3在激活每token 380亿参数的同时实现了低成本（超过DeepSeek-V3和Qwen3 MoE 235B），这表明硬件对齐的注意力算术强度、MoE稀疏性和AFD对于成本效益至关重要。我们与DeepSeek-V3在其有利场景下进行了直接比较。我们在Hopper GPU上的实现达到了每GPU每秒高达4,039个token的解码吞吐量（在4K上下文、FP8、无MTP条件下，50ms TPOT SLA）。这高于DeepSeek-V3在相同设置下的2,324个token，并为LLM解码设置了新的帕累托前沿。", "summary": "Step-3是一个3210亿参数的视觉语言模型（VLM），通过创新的模型-系统协同设计来解决大型语言模型（LLMs）解码效率低和成本高的问题。其核心技术包括多矩阵分解注意力（MFA），用于减少KV缓存和计算，以及注意力-前馈网络分离（AFD），一个解耦注意力层和FFN层的分布式推理系统。实验结果显示，Step-3在解码成本上显著优于现有模型，尤其在长上下文场景下，并在Hopper GPU上实现了更高的解码吞吐量，为LLM解码效率树立了新的标杆。", "keywords": "LLM解码, 模型-系统协同设计, 成本效益, 注意力机制, 分布式推理", "comments": "这篇论文在提高大型语言模型部署的经济可行性方面取得了显著进展，特别是在处理长上下文任务时。其创新之处在于采用了模型架构（MFA）与系统级优化（AFD）相结合的协同设计方法。这种整体性的视角解决了解码效率这一关键瓶颈，这对于LLM的广泛应用至关重要。论文展示的吞吐量提升令人印象深刻，为成本效益高的LLM推理设定了新的标准。"}}
{"id": "2412.00460", "title": "BGM: Background Mixup for X-ray Prohibited Items Detection", "authors": ["Weizhe Liu", "Renshuai Tao", "Hongguang Zhu", "Yunda Sun", "Yao Zhao", "Yunchao Wei"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.00460v3", "summary": "Current data-driven approaches for X-ray prohibited items detection remain\nunder-explored, particularly in the design of effective data augmentations.\nExisting natural image augmentations for reflected light imaging neglect the\ndata characteristics of X-ray security images. Moreover, prior X-ray\naugmentation methods have predominantly focused on foreground prohibited items,\noverlooking informative background cues. In this paper, we propose Background\nMixup (BGM), a background-based augmentation technique tailored for X-ray\nsecurity imaging domain. Unlike conventional methods, BGM is founded on an\nin-depth analysis of physical properties including: 1) X-ray Transmission\nImagery: Transmitted X-ray pixels represent composite information from multiple\nmaterials along the imaging path. 2) Material-based Pseudo-coloring:\nPseudo-coloring in X-ray images correlates directly with material properties,\naiding in material distinction. Building upon the above insights, BGM mixes\nbackground patches across regions on both 1) texture structure and 2) material\nvariation, to benefit models from complicated background cues. This enhances\nthe model's capability to handle domain-specific challenges such as\nocclusion-induced discriminative imbalance. Importantly, BGM is orthogonal and\nfully compatible with existing foreground-focused augmentation techniques,\nenabling joint use to further enhance detection performance. Extensive\nexperiments on multiple X-ray security benchmarks show that BGM consistently\nsurpasses strong baselines, without additional annotations or significant\ntraining overhead. This work pioneers the exploration of background-aware\naugmentation in X-ray prohibited items detection and provides a lightweight,\nplug-and-play solution with broad applicability.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.00460v3", "cate": "cs.CV", "date": "2024-11-30", "updated": "2025-07-25", "AI": {"title_translation": "BGM：用于X射线违禁品检测的背景混合", "tldr": "本文提出了一种名为BGM（Background Mixup）的背景增强技术，专为X射线安检图像设计，通过混合背景区域的纹理结构和材料变化，有效提升了X射线违禁品检测模型的性能，且无需额外标注或显著训练开销。", "motivation": "当前X射线违禁品检测的数据驱动方法，特别是在有效数据增强设计方面，仍未得到充分探索。现有的自然图像增强方法忽略了X射线安检图像的数据特性。此外，先前的X射线增强方法主要关注前景违禁品，忽视了信息丰富的背景线索。", "method": "本文提出了一种名为背景混合（BGM）的背景增强技术，专为X射线安检领域定制。BGM基于对X射线透射图像和基于材料的伪彩色物理特性的深入分析，通过混合不同区域的背景补丁，融合纹理结构和材料变化，以利用复杂的背景线索。它旨在增强模型处理遮挡引起的判别失衡等领域特定挑战的能力。BGM与现有关注前景的增强技术正交且完全兼容，可联合使用。", "result": "在多个X射线安检基准上的大量实验表明，BGM持续超越了强大的基线模型，且无需额外的标注或显著的训练开销。", "conclusion": "这项工作开创了X射线违禁品检测中背景感知增强的探索，并提供了一种轻量级、即插即用且适用性广泛的解决方案。", "translation": "当前X射线违禁品检测的数据驱动方法仍未得到充分探索，特别是在有效数据增强的设计方面。现有针对反射光成像的自然图像增强方法忽略了X射线安检图像的数据特性。此外，先前的X射线增强方法主要集中于前景违禁品，忽视了信息丰富的背景线索。在本文中，我们提出了一种名为背景混合（BGM）的背景增强技术，专为X射线安检成像领域量身定制。与传统方法不同，BGM建立在对物理特性的深入分析之上，包括：1) X射线透射图像：透射的X射线像素代表了沿成像路径上多种材料的复合信息。2) 基于材料的伪彩色：X射线图像中的伪彩色与材料特性直接相关，有助于区分材料。基于上述见解，BGM在纹理结构和材料变化两方面混合背景区域的补丁，以使模型受益于复杂的背景线索。这增强了模型处理领域特定挑战的能力，例如由遮挡引起的判别不平衡。重要的是，BGM与现有关注前景的增强技术正交且完全兼容，可以联合使用以进一步提高检测性能。在多个X射线安检基准上的大量实验表明，BGM持续超越了强大的基线，且无需额外标注或显著训练开销。这项工作开创了X射线违禁品检测中背景感知增强的探索，并提供了一种轻量级、即插即用的解决方案，具有广泛的适用性。", "summary": "本文针对X射线违禁品检测中数据增强的不足，特别是现有方法忽视背景线索的问题，提出了一种名为BGM（Background Mixup）的背景增强技术。BGM基于X射线图像的物理特性，通过混合不同背景区域的纹理和材料信息来生成增强数据，有效利用复杂的背景线索，提升模型处理遮挡等挑战的能力。实验证明，BGM在多个基准上均优于现有方法，且具有轻量级、即插即用的特点，可与现有前景增强技术兼容，无需额外标注或大量计算开销。", "keywords": "X射线检测, 数据增强, 背景混合, 违禁品检测, 安全成像", "comments": "BGM的创新点在于其突破了传统数据增强方法仅关注前景的局限，首次在X射线违禁品检测领域探索背景感知增强。其基于物理特性进行背景混合的设计，不仅提升了模型对复杂背景和遮挡的鲁棒性，还保持了轻量级和即插即用的特性，使其具有广泛的应用潜力。该方法填补了X射线图像特定增强的空白，对该领域的发展具有重要意义。"}}
{"id": "2506.19315", "title": "JCAPT: A Joint Modeling Approach for CAPT", "authors": ["Tzu-Hsuan Yang", "Yue-Yang He", "Berlin Chen"], "categories": ["cs.CL", "cs.AI", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to the ISCA SLaTE-2025 Workshop", "url": "http://arxiv.org/abs/2506.19315v2", "summary": "Effective pronunciation feedback is critical in second language (L2)\nlearning, for which computer-assisted pronunciation training (CAPT) systems\noften encompass two key tasks: automatic pronunciation assessment (APA) and\nmispronunciation detection and diagnosis (MDD). Recent work has shown that\njoint modeling of these two tasks can yield mutual benefits. Our unified\nframework leverages Mamba, a selective state space model (SSM), while\nintegrating phonological features and think token strategies to jointly enhance\ninterpretability and fine-grained temporal reasoning in APA and MDD. To our\nknowledge, this is the first study to combine phonological attribution,\nSSM-based modeling, and prompting in CAPT. A series of experiments conducted on\nthe speechocean762 benchmark demonstrate that our model consistently\noutperforms prior methods, particularly on the MDD task.", "comment": "Accepted to the ISCA SLaTE-2025 Workshop", "pdf_url": "http://arxiv.org/pdf/2506.19315v2", "cate": "cs.CL", "date": "2025-06-24", "updated": "2025-07-25", "AI": {"title_translation": "JCAPT：一种用于CAPT的联合建模方法", "tldr": "JCAPT是一个结合Mamba、音韵特征和思考token策略的联合建模框架，用于计算机辅助发音训练（CAPT）中的自动发音评估（APA）和误发音检测与诊断（MDD），并在基准测试中表现优于现有方法。", "motivation": "第二语言（L2）学习中有效的发音反馈至关重要，而计算机辅助发音训练（CAPT）系统通常包含自动发音评估（APA）和误发音检测与诊断（MDD）两项关键任务。现有研究表明，对这两项任务进行联合建模可以产生互惠互利的效果。", "method": "本研究提出了一个名为JCAPT的统一框架，该框架利用Mamba（一种选择性状态空间模型SSM），并整合音韵特征和思考token策略，以共同增强APA和MDD中的可解释性和细粒度时间推理。据作者所知，这是首次在CAPT中结合音韵归因、基于SSM的建模和提示策略的研究。", "result": "在speechocean762基准数据集上进行的一系列实验表明，所提出的模型JCAPT持续优于先前的方法，尤其是在MDD任务上表现突出。", "conclusion": "本研究提出的JCAPT模型通过联合建模APA和MDD任务，并首次结合Mamba、音韵特征和思考token策略，显著提升了CAPT系统的性能，尤其在误发音检测与诊断方面表现卓越。", "translation": "有效的发音反馈在第二语言（L2）学习中至关重要，为此，计算机辅助发音训练（CAPT）系统通常包含两个关键任务：自动发音评估（APA）和误发音检测与诊断（MDD）。最近的研究表明，对这两项任务进行联合建模可以产生互惠互利的效果。我们的统一框架利用Mamba（一种选择性状态空间模型SSM），同时整合音韵特征和思考token策略，以共同增强APA和MDD中的可解释性和细粒度时间推理。据我们所知，这是首次在CAPT中结合音韵归因、基于SSM的建模和提示策略的研究。在speechocean762基准数据集上进行的一系列实验表明，我们的模型持续优于先前的方法，尤其是在MDD任务上表现突出。", "summary": "JCAPT是一个新颖的计算机辅助发音训练（CAPT）联合建模框架，它将自动发音评估（APA）和误发音检测与诊断（MDD）结合起来。该框架创新性地利用Mamba（一种选择性状态空间模型SSM），并融入音韵特征和思考token策略，旨在提升系统的可解释性和细粒度时间推理能力。实验结果显示，JCAPT在speechocean762基准测试中表现出色，尤其在MDD任务上显著优于现有方法，是首次将音韵归因、SSM建模和提示策略应用于CAPT领域的研究。", "keywords": "CAPT, 联合建模, Mamba, 发音评估, 误发音检测", "comments": "该论文的创新点在于首次将Mamba（选择性状态空间模型）、音韵特征和思考token策略结合起来，应用于计算机辅助发音训练（CAPT）中的自动发音评估（APA）和误发音检测与诊断（MDD）的联合建模。这种结合有望提升系统的可解释性和细粒度时间推理能力，并在实验中取得了优于现有方法的成果，特别是在误发音检测方面表现突出，对L2发音学习具有重要意义。"}}
{"id": "2506.16470", "title": "IMEX-RB: a self-adaptive implicit-explicit time integration scheme exploiting the reduced basis method", "authors": ["Micol Bassanini", "Simone Deparis", "Francesco Sala", "Riccardo Tenderini"], "categories": ["math.NA", "cs.NA", "65M12"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      20 pages, 7 figures", "url": "http://arxiv.org/abs/2506.16470v2", "summary": "In this work, we introduce a self-adaptive implicit-explicit (IMEX) time\nintegration scheme, named IMEX-RB, for the numerical integration of systems of\nordinary differential equations (ODEs), arising from spatial discretizations of\npartial differential equations (PDEs) by finite difference methods. Leveraging\nthe Reduced Basis (RB) method, at each timestep we project the high-fidelity\nproblem onto a suitable low-dimensional subspace and integrate its dynamics\nimplicitly. Following the IMEX paradigm, the resulting solution then serves as\nan educated guess within a full-order explicit step. Notably, compared to the\ncanonical RB method, IMEX-RB neither requires a parametrization of the\nunderlying PDE nor features an offline-online splitting, since the reduced\nsubspace is built dynamically, exploiting the high-fidelity solution history.\nWe present the first-order formulation of IMEX-RB, demonstrating and showcasing\nits convergence and stability properties. In particular, under appropriate\nconditions on the method's hyperparameters, IMEX-RB is unconditionally stable.\nThe theoretical analysis is corroborated by numerical experiments performed on\nrepresentative model problems in two and three dimensions. The results\ndemonstrate that our approach can outperform conventional time integration\nschemes like backward Euler. Indeed, IMEX-RB yields high-fidelity accurate\nsolutions, provided that its main hyperparameters - namely the reduced basis\nsize and the stability tolerance - are suitably tuned. Moreover, IMEX-RB\nrealizes computational gains over backward Euler for a range of timestep sizes\nabove the forward Euler stability threshold.", "comment": "20 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.16470v2", "cate": "math.NA", "date": "2025-06-19", "updated": "2025-07-25", "AI": {"title_translation": "IMEX-RB: 一种利用降阶基方法自适应隐式-显式时间积分方案", "tldr": "IMEX-RB是一种新的自适应隐式-显式时间积分方案，结合了降阶基方法，能有效求解PDE离散化后的ODE系统，并展现出优于传统方法的性能和稳定性。", "motivation": "现有的时间积分方案在处理偏微分方程（PDE）空间离散化产生的常微分方程（ODE）系统时可能存在效率或稳定性问题。传统的降阶基（RB）方法通常需要底层PDE的参数化和离线-在线阶段，本工作旨在引入一种无需这些限制且更高效的自适应时间积分方案。", "method": "IMEX-RB是一种自适应隐式-显式（IMEX）时间积分方案，用于数值积分由PDE空间离散化（通过有限差分法）产生的ODE系统。它利用降阶基（RB）方法，在每个时间步将高保真问题投影到合适的低维子空间并隐式积分其动力学，然后将结果作为全阶显式步骤的初始猜测。与传统RB方法不同，IMEX-RB不要求底层PDE的参数化，也不需要离线-在线分离，因为降阶子空间是动态构建的，利用了高保真解历史。论文还提出了其一阶公式，并分析了其收敛性和稳定性特性。", "result": "数值实验表明，IMEX-RB在适当的超参数条件下是无条件稳定的，并展示了其收敛性和稳定性。结果显示，IMEX-RB在二维和三维的代表性模型问题上，能够提供高保真度的精确解，并且在计算性能上优于传统的反向欧拉等时间积分方案。此外，对于超出前向欧拉稳定性阈值的一系列时间步长，IMEX-RB比反向欧拉实现了计算增益。", "conclusion": "IMEX-RB是一种有效且稳定的自适应隐式-显式时间积分方案，它通过动态构建降阶子空间，成功地将降阶基方法与IMEX范式结合，避免了传统RB方法的限制。该方法在精度和计算效率上均优于传统方案，为偏微分方程离散化产生的常微分方程系统提供了高性能的数值解法，其性能表现取决于超参数的适当调整。", "translation": "在这项工作中，我们引入了一种自适应隐式-显式（IMEX）时间积分方案，名为IMEX-RB，用于数值积分由有限差分方法对偏微分方程（PDE）进行空间离散化产生的常微分方程（ODE）系统。利用降阶基（RB）方法，在每个时间步我们将高保真问题投影到一个合适的低维子空间并隐式地积分其动力学。遵循IMEX范式，所得解决方案随后作为全阶显式步骤中的一个“有根据的猜测”。值得注意的是，与经典的RB方法相比，IMEX-RB既不需要底层PDE的参数化，也不具备离线-在线分离，因为降阶子空间是动态构建的，利用了高保真解历史。我们提出了IMEX-RB的一阶公式，展示并证明了其收敛性和稳定性特性。特别是，在方法超参数的适当条件下，IMEX-RB是无条件稳定的。理论分析通过在二维和三维代表性模型问题上进行的数值实验得到了证实。结果表明，我们的方法可以超越传统的反向欧拉等时间积分方案。事实上，只要其主要超参数——即降阶基大小和稳定性容差——经过适当调整，IMEX-RB就能产生高保真度的精确解。此外，对于超出前向欧拉稳定性阈值的一系列时间步长，IMEX-RB比反向欧拉实现了计算增益。", "summary": "本文提出了一种名为IMEX-RB的自适应隐式-显式（IMEX）时间积分方案，用于求解由偏微分方程空间离散化产生的常微分方程系统。该方法巧妙地结合了降阶基（RB）技术，在每个时间步动态构建低维子空间进行隐式积分，并利用高保真解历史，从而避免了传统RB方法对参数化和离线-在线分离的需求。研究展示了IMEX-RB的收敛性和无条件稳定性，并通过数值实验证明其在精度和计算效率上均优于传统时间积分方案，如反向欧拉法，尤其是在适当调整超参数后。", "keywords": "自适应时间积分, 隐式-显式方法, 降阶基方法, 常微分方程, 偏微分方程", "comments": "该论文的创新点在于将自适应隐式-显式时间积分与动态构建的降阶基方法相结合，有效地避免了传统降阶基方法对参数化和离线-在线阶段的需求。这种动态适应性提高了方法的通用性和适用性，使其能够更灵活地应用于各种ODE系统。其无条件稳定性和在计算增益方面的表现，使其在求解大型复杂系统方面具有重要潜力，特别是在需要高精度和效率的数值模拟领域。"}}
{"id": "2507.19100", "title": "Monocular Vision-Based Swarm Robot Localization Using Equilateral Triangular Formations", "authors": ["Taewon Kang", "Ji-Wook Kwon", "Il Bae", "Jin Hyo Kim"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19100v1", "summary": "Localization of mobile robots is crucial for deploying robots in real-world\napplications such as search and rescue missions. This work aims to develop an\naccurate localization system applicable to swarm robots equipped only with\nlow-cost monocular vision sensors and visual markers. The system is designed to\noperate in fully open spaces, without landmarks or support from positioning\ninfrastructures. To achieve this, we propose a localization method based on\nequilateral triangular formations. By leveraging the geometric properties of\nequilateral triangles, the accurate two-dimensional position of each\nparticipating robot is estimated using one-dimensional lateral distance\ninformation between robots, which can be reliably and accurately obtained with\na low-cost monocular vision sensor. Experimental and simulation results\ndemonstrate that, as travel time increases, the positioning error of the\nproposed method becomes significantly smaller than that of a conventional\ndead-reckoning system, another low-cost localization approach applicable to\nopen environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19100v1", "cate": "cs.RO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于单目视觉的利用等边三角形编队的群机器人定位", "tldr": "该研究提出了一种基于等边三角形编队的单目视觉定位方法，用于在开放空间中为配备低成本传感器的群机器人提供准确的二维定位，并显示出比传统航位推算系统更低的定位误差。", "motivation": "移动机器人的定位对于其在搜索和救援等现实应用中的部署至关重要。当前挑战在于为仅配备低成本单目视觉传感器和视觉标记的群机器人开发一个在没有地标或定位基础设施支持的完全开放空间中运行的准确定位系统。", "method": "提出了一种基于等边三角形编队的定位方法。通过利用等边三角形的几何特性，使用机器人之间的一维横向距离信息来估计每个参与机器人的准确二维位置，这些信息可以通过低成本单目视觉传感器可靠且准确地获取。", "result": "实验和仿真结果表明，随着移动时间的增加，所提方法的定位误差明显小于传统的航位推算系统（另一种适用于开放环境的低成本定位方法）。", "conclusion": "该论文成功开发了一种基于等边三角形编队的单目视觉群机器人定位系统，并在开放空间中实现了比传统航位推算系统更低的定位误差，证明了其在低成本传感器条件下的有效性和准确性。", "translation": "移动机器人的定位对于在搜索和救援任务等实际应用中部署机器人至关重要。这项工作旨在开发一种适用于仅配备低成本单目视觉传感器和视觉标记的群机器人的精确定位系统。该系统设计用于在完全开放空间中运行，无需地标或定位基础设施的支持。为了实现这一点，我们提出了一种基于等边三角形编队的定位方法。通过利用等边三角形的几何特性，使用机器人之间的一维横向距离信息来估计每个参与机器人的精确二维位置，这些信息可以通过低成本单目视觉传感器可靠且准确地获取。实验和仿真结果表明，随着行进时间的增加，所提出方法的定位误差明显小于传统的航位推算系统，后者是另一种适用于开放环境的低成本定位方法。", "summary": "本研究提出了一种新颖的单目视觉群机器人定位系统，该系统利用等边三角形编队的几何特性。该系统专为配备低成本单目视觉传感器和视觉标记的群机器人在没有外部基础设施的开放空间中运行而设计。通过利用机器人间的一维横向距离信息，该方法能够准确估计每个机器人的二维位置。实验和仿真结果表明，与传统的航位推算方法相比，所提出的方法在长时间运行下能显著降低定位误差。", "keywords": "群机器人定位, 单目视觉, 等边三角形编队, 低成本传感器, 开放空间定位", "comments": "这项工作提出了一种创新的方法，通过利用简单的几何编队和低成本的单目视觉传感器解决了群机器人在开放空间中的定位挑战。其优势在于无需昂贵的基础设施支持，且在长时间运行中表现出优于传统方法的精度，这对于实际部署具有重要意义。该方法的创新性在于将二维定位问题巧妙地转化为一维距离测量，降低了对传感器复杂度的要求。"}}
{"id": "2507.05113", "title": "CLIP-Guided Backdoor Defense through Entropy-Based Poisoned Dataset Separation", "authors": ["Binyan Xu", "Fan Yang", "Xilin Dai", "Di Tang", "Kehuan Zhang"], "categories": ["cs.MM", "cs.CR", "cs.LG", "68T07", "I.2.6"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "Comments:      15 pages, 9 figures, 15 tables. To appear in the Proceedings of the 32nd ACM International Conference on Multimedia (MM '25)", "url": "http://arxiv.org/abs/2507.05113v2", "summary": "Deep Neural Networks (DNNs) are susceptible to backdoor attacks, where\nadversaries poison training data to implant backdoor into the victim model.\nCurrent backdoor defenses on poisoned data often suffer from high computational\ncosts or low effectiveness against advanced attacks like clean-label and\nclean-image backdoors. To address them, we introduce CLIP-Guided backdoor\nDefense (CGD), an efficient and effective method that mitigates various\nbackdoor attacks. CGD utilizes a publicly accessible CLIP model to identify\ninputs that are likely to be clean or poisoned. It then retrains the model with\nthese inputs, using CLIP's logits as a guidance to effectively neutralize the\nbackdoor. Experiments on 4 datasets and 11 attack types demonstrate that CGD\nreduces attack success rates (ASRs) to below 1% while maintaining clean\naccuracy (CA) with a maximum drop of only 0.3%, outperforming existing\ndefenses. Additionally, we show that clean-data-based defenses can be adapted\nto poisoned data using CGD. Also, CGD exhibits strong robustness, maintaining\nlow ASRs even when employing a weaker CLIP model or when CLIP itself is\ncompromised by a backdoor. These findings underscore CGD's exceptional\nefficiency, effectiveness, and applicability for real-world backdoor defense\nscenarios. Code: https://github.com/binyxu/CGD.", "comment": "15 pages, 9 figures, 15 tables. To appear in the Proceedings of the\n  32nd ACM International Conference on Multimedia (MM '25)", "pdf_url": "http://arxiv.org/pdf/2507.05113v2", "cate": "cs.MM", "date": "2025-07-07", "updated": "2025-07-25", "AI": {"title_translation": "基于CLIP引导和熵基中毒数据集分离的后门防御", "tldr": "CGD是一种高效且有效的后门防御方法，它利用CLIP模型识别中毒数据并指导模型再训练，显著降低了攻击成功率，同时保持了模型精度，且对多种高级攻击和CLIP模型受损情况均表现出鲁棒性。", "motivation": "深度神经网络（DNN）容易受到后门攻击，而现有防御方法在计算成本或对高级攻击（如干净标签和干净图像后门）的有效性方面存在不足。", "method": "本文提出了CLIP引导的后门防御（CGD）方法。CGD利用公开的CLIP模型来识别可能是干净或中毒的输入。然后，它使用这些输入重新训练模型，并以CLIP的logits作为指导来有效中和后门。", "result": "在4个数据集和11种攻击类型上的实验表明，CGD将攻击成功率（ASR）降低到1%以下，同时保持了干净准确率（CA），最大下降仅为0.3%，优于现有防御方法。此外，干净数据防御可以适应中毒数据，并且CGD即使在使用较弱的CLIP模型或CLIP本身被后门攻击时也表现出强大的鲁棒性。", "conclusion": "CGD在后门防御场景中展现出卓越的效率、有效性和适用性。", "translation": "深度神经网络（DNN）容易受到后门攻击，攻击者通过毒害训练数据将后门植入受害者模型。目前针对中毒数据的后门防御方法通常存在计算成本高或对高级攻击（如干净标签和干净图像后门）效果不佳的问题。为了解决这些问题，我们引入了CLIP引导的后门防御（CGD），这是一种有效且高效的方法，可以减轻各种后门攻击。CGD利用公开可用的CLIP模型来识别可能是干净或中毒的输入。然后，它使用这些输入重新训练模型，以CLIP的logits作为指导来有效中和后门。在4个数据集和11种攻击类型上的实验表明，CGD将攻击成功率（ASR）降低到1%以下，同时保持了干净准确率（CA），最大下降仅为0.3%，优于现有防御方法。此外，我们还表明，基于干净数据的防御方法可以通过CGD适应中毒数据。CGD还表现出强大的鲁棒性，即使在使用较弱的CLIP模型或CLIP本身被后门攻击时，也能保持较低的ASR。这些发现强调了CGD在实际后门防御场景中的卓越效率、有效性和适用性。代码：https://github.com/binyxu/CGD。", "summary": "本文提出了一种名为CLIP引导的后门防御（CGD）的新方法，旨在解决现有深度神经网络后门防御中计算成本高昂和对高级攻击效果不佳的问题。CGD利用CLIP模型区分干净和中毒数据，并利用CLIP的logits指导模型再训练以中和后门。实验证明，CGD在多种数据集和攻击类型上显著降低了攻击成功率，同时保持了高精度，并展现出对弱CLIP模型和CLIP自身受损的鲁棒性，突显其在实际应用中的高效性和有效性。", "keywords": "后门防御, CLIP, 中毒数据, 深度神经网络, 鲁棒性", "comments": "该论文提出了一种新颖的后门防御方法CGD，其创新点在于巧妙地利用了CLIP模型的零样本泛化能力来区分中毒数据，并将其logits作为指导进行模型再训练。这种方法不仅解决了现有防御方法计算成本高和对高级攻击（如干净标签、干净图像后门）效果不佳的痛点，而且在实验中展现出卓越的性能（ASR低、CA高）和强大的鲁棒性，即使在CLIP模型自身受损的情况下也能保持防御效果，这对于实际部署具有重要意义。其高效性和有效性使其成为后门防御领域的一个重要进展。"}}
{"id": "2507.19466", "title": "Towards Effective Immersive Technologies in Medicine: Potential and Future Applications based on VR, AR, XR and AI solutions", "authors": ["Aliaksandr Marozau", "Barbara Karpowicz", "Tomasz Kowalewski", "Pavlo Zinevych", "Wiktor Stawski", "Adam Kuzdraliński", "Wiesław Kopeć"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19466v1", "summary": "Mixed Reality (MR) technologies such as Virtual and Augmented Reality (VR,\nAR) are well established in medical practice, enhancing diagnostics, treatment,\nand education. However, there are still some limitations and challenges that\nmay be overcome thanks to the latest generations of equipment, software, and\nframeworks based on eXtended Reality (XR) by enabling immersive systems that\nsupport safer, more controlled environments for training and patient care. Our\nreview highlights recent VR and AR applications in key areas of medicine. In\nmedical education, these technologies provide realistic clinical simulations,\nimproving skills and knowledge retention. In surgery, immersive tools enhance\nprocedural precision with detailed anatomical visualizations. VR-based\nrehabilitation has shown effectiveness in restoring motor functions and\nbalance, particularly for neurological patients. In mental health, VR has been\nsuccessful in treating conditions like PTSD and phobias. Although VR and AR\nsolutions are well established, there are still some important limitations,\nincluding high costs and limited tactile feedback, which may be overcome with\nimplementing new technologies that may improve the effectiveness of immersive\nmedical applications such as XR, psychophysiological feedback or integration of\nartificial intelligence (AI) for real-time data analysis and personalized\nhealthcare and training.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19466v1", "cate": "cs.HC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "迈向医学领域中有效的沉浸式技术：基于VR、AR、XR和AI解决方案的潜力和未来应用", "tldr": "本综述探讨了VR和AR在医学领域的现有应用及其局限性，并展望了XR和AI等新技术如何克服这些局限，以实现更有效、更安全的医疗培训和患者护理。", "motivation": "尽管VR和AR技术已在医学实践中得到广泛应用，但在诊断、治疗和教育方面仍存在一些局限和挑战。本研究旨在探讨如何通过最新一代的XR和AI解决方案克服这些局限，以支持更安全、更受控的沉浸式医疗系统。", "method": "本文通过综述的方式，重点介绍了VR和AR技术在医学关键领域的最新应用。", "result": "VR和AR技术在医学教育中提供了逼真的临床模拟，提高了技能和知识保留；在手术中增强了程序精确性；在康复（特别是神经系统患者）中有效恢复运动功能和平衡；在精神健康领域成功治疗了创伤后应激障碍和恐惧症等疾病。", "conclusion": "VR和AR解决方案虽然已成熟，但仍存在成本高、触觉反馈有限等局限。未来通过实施XR、生理心理反馈或整合人工智能（AI）等新技术，可以提高沉浸式医疗应用的有效性，实现实时数据分析和个性化医疗保健及培训。", "translation": "混合现实（MR）技术，如虚拟现实（VR）和增强现实（AR），已在医学实践中得到广泛应用，增强了诊断、治疗和教育。然而，仍存在一些局限和挑战，这些可以通过基于扩展现实（XR）的最新一代设备、软件和框架来克服，从而实现支持更安全、更受控的培训和患者护理的沉浸式系统。我们的综述重点介绍了VR和AR在医学关键领域的最新应用。在医学教育中，这些技术提供了逼真的临床模拟，提高了技能和知识保留。在手术中，沉浸式工具通过详细的解剖可视化增强了手术精度。基于VR的康复已在恢复运动功能和平衡方面显示出有效性，特别是对于神经系统患者。在精神健康领域，VR已成功治疗创伤后应激障碍和恐惧症等疾病。尽管VR和AR解决方案已成熟，但仍存在一些重要局限，包括高成本和有限的触觉反馈，这些可以通过实施可能提高沉浸式医疗应用有效性的新技术来克服，例如XR、心理生理反馈或整合人工智能（AI）以进行实时数据分析和个性化医疗保健和培训。", "summary": "本综述探讨了虚拟现实（VR）和增强现实（AR）等沉浸式技术在医学领域的现有应用，涵盖医学教育、手术、康复和精神健康等方面。文章指出了这些技术在提高诊断、治疗和培训效果方面的潜力，同时也识别了高成本和触觉反馈不足等局限性。研究强调了扩展现实（XR）和人工智能（AI）等新兴技术在克服当前挑战、实现更安全、更个性化的沉浸式医疗应用方面的关键作用。", "keywords": "沉浸式技术, 虚拟现实, 增强现实, 扩展现实, 人工智能, 医疗应用", "comments": "这篇综述的创新之处在于它不仅回顾了VR和AR在医学中的现有应用，更重要的是，它前瞻性地指出了XR和AI等先进技术在克服当前局限、提升沉浸式医疗应用效果方面的巨大潜力。这对于推动医学领域的技术发展和实际应用具有重要指导意义。"}}
{"id": "2507.19077", "title": "Multi-Task Dense Prediction Fine-Tuning with Mixture of Fine-Grained Experts", "authors": ["Yangyang Xu", "Xi Ye", "Duo Su"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ACM Multimedia 2025 (MM'25)", "url": "http://arxiv.org/abs/2507.19077v1", "summary": "Multi-task learning (MTL) for dense prediction has shown promising results\nbut still faces challenges in balancing shared representations with\ntask-specific specialization. In this paper, we introduce a novel Fine-Grained\nMixture of Experts (FGMoE) architecture that explores MoE-based MTL models\nthrough a combination of three key innovations and fine-tuning. First, we\npropose intra-task experts that partition along intermediate hidden dimensions\nof MLPs, enabling finer decomposition of task information while maintaining\nparameter efficiency. Second, we introduce shared experts that consolidate\ncommon information across different contexts of the same task, reducing\nredundancy, and allowing routing experts to focus on unique aspects. Third, we\ndesign a global expert that facilitates adaptive knowledge transfer across\ntasks based on both input feature and task requirements, promoting beneficial\ninformation sharing while preventing harmful interference. In addition, we use\nthe fine-tuning approach to improve parameter efficiency only by training the\nparameters of the decoder. Extensive experimental results show that the\nproposed FGMoE uses fewer parameters and significantly outperforms current\nMoE-based competitive MTL models on two dense prediction datasets\n(\\textit{i.e.,} NYUD-v2, PASCAL-Context) in various metrics.", "comment": "Accepted to ACM Multimedia 2025 (MM'25)", "pdf_url": "http://arxiv.org/pdf/2507.19077v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "多任务密集预测微调与细粒度专家混合", "tldr": "本文提出了一种名为细粒度专家混合（FGMoE）的新型多任务学习（MTL）架构，用于密集预测。FGMoE通过引入任务内专家、共享专家和全局专家，并结合微调方法，有效平衡了共享表示与任务特定专业化，显著提升了性能并降低了参数量。", "motivation": "多任务学习（MTL）在密集预测方面虽已取得进展，但在平衡共享表示和任务特定专业化方面仍面临挑战。", "method": "本文引入了一种名为细粒度专家混合（FGMoE）的新型架构，通过结合三项关键创新和微调来探索基于MoE的MTL模型：1. 提出任务内专家，沿MLP的中间隐藏维度进行划分，实现任务信息的更精细分解，同时保持参数效率。2. 引入共享专家，整合同一任务不同上下文中的共同信息，减少冗余。3. 设计全局专家，基于输入特征和任务需求促进跨任务的自适应知识转移，避免有害干扰。此外，通过仅训练解码器参数的微调方法来提高参数效率。", "result": "所提出的FGMoE模型使用更少的参数，并在两个密集预测数据集（NYUD-v2、PASCAL-Context）上，在各种指标下，显著优于当前基于MoE的竞争性MTL模型。", "conclusion": "本文提出的细粒度专家混合（FGMoE）架构成功解决了密集预测多任务学习中共享表示与任务特定专业化之间的平衡挑战，实现了性能和参数效率的显著提升。", "translation": "多任务学习（MTL）用于密集预测已显示出有前景的结果，但仍在平衡共享表示与任务特定专业化方面面临挑战。在本文中，我们引入了一种新颖的细粒度专家混合（FGMoE）架构，该架构通过三项关键创新和微调相结合的方式探索了基于MoE的MTL模型。首先，我们提出了任务内专家，它们沿着MLP的中间隐藏维度进行划分，从而实现任务信息的更精细分解，同时保持参数效率。其次，我们引入了共享专家，它们整合了同一任务不同上下文中的共同信息，减少了冗余，并允许路由专家专注于独特方面。第三，我们设计了一个全局专家，它基于输入特征和任务需求促进跨任务的自适应知识转移，促进有益的信息共享，同时防止有害干扰。此外，我们使用微调方法，仅通过训练解码器的参数来提高参数效率。大量的实验结果表明，所提出的FGMoE使用更少的参数，并在两个密集预测数据集（即NYUD-v2，PASCAL-Context）上，在各种指标下，显著优于当前基于MoE的竞争性MTL模型。", "summary": "本文提出了一种新颖的细粒度专家混合（FGMoE）架构，用于密集预测的多任务学习。FGMoE通过引入任务内专家、共享专家和全局专家，并结合仅微调解码器参数的方法，有效解决了多任务学习中共享表示与任务特定专业化之间的平衡问题。实验证明，该模型在减少参数量的同时，在NYUD-v2和PASCAL-Context等密集预测数据集上取得了优于现有MoE-based MTL模型的性能。", "keywords": "多任务学习, 专家混合, 密集预测, 微调, 参数效率", "comments": "该论文的创新之处在于其细粒度专家混合（FGMoE）架构，通过将专家划分为任务内、共享和全局三类，实现了对任务信息的精细分解、冗余消除以及跨任务的自适应知识转移。这种设计有效平衡了多任务学习中的共享与特化需求。此外，仅微调解码器参数的策略也显著提升了参数效率。这对于密集预测领域的多任务学习具有重要意义。"}}
{"id": "2504.06662", "title": "RAMBO: RL-augmented Model-based Whole-body Control for Loco-manipulation", "authors": ["Jin Cheng", "Dongho Kang", "Gabriele Fadini", "Guanya Shi", "Stelian Coros"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE Robotics and Automation Letters (RA-L)", "url": "http://arxiv.org/abs/2504.06662v3", "summary": "Loco-manipulation, physical interaction of various objects that is\nconcurrently coordinated with locomotion, remains a major challenge for legged\nrobots due to the need for both precise end-effector control and robustness to\nunmodeled dynamics. While model-based controllers provide precise planning via\nonline optimization, they are limited by model inaccuracies. In contrast,\nlearning-based methods offer robustness, but they struggle with precise\nmodulation of interaction forces. We introduce RAMBO, a hybrid framework that\nintegrates model-based whole-body control within a feedback policy trained with\nreinforcement learning. The model-based module generates feedforward torques by\nsolving a quadratic program, while the policy provides feedback corrective\nterms to enhance robustness. We validate our framework on a quadruped robot\nacross a diverse set of real-world loco-manipulation tasks, such as pushing a\nshopping cart, balancing a plate, and holding soft objects, in both quadrupedal\nand bipedal walking. Our experiments demonstrate that RAMBO enables precise\nmanipulation capabilities while achieving robust and dynamic locomotion.", "comment": "Accepted to IEEE Robotics and Automation Letters (RA-L)", "pdf_url": "http://arxiv.org/pdf/2504.06662v3", "cate": "cs.RO", "date": "2025-04-09", "updated": "2025-07-25", "AI": {"title_translation": "RAMBO：RL增强的基于模型的全身控制用于移动操作", "tldr": "RAMBO是一个混合框架，结合了基于模型的全身控制和强化学习反馈策略，以实现四足机器人在移动操作任务中的精确控制和鲁棒性。", "motivation": "移动操作对于腿式机器人来说是一个重大挑战，因为它需要精确的末端执行器控制和对未建模动态的鲁棒性。现有的基于模型的控制器受模型不准确性限制，而基于学习的方法难以精确调制交互力。", "method": "我们提出了RAMBO，一个将基于模型的全身控制与通过强化学习训练的反馈策略相结合的混合框架。其中，基于模型的模块通过求解二次规划生成前馈扭矩，而策略提供反馈校正项以增强鲁棒性。", "result": "RAMBO在四足机器人上通过一系列真实的移动操作任务（如推动购物车、平衡盘子、握持软物体，包括四足和两足行走）进行了验证。实验表明，RAMBO实现了精确的操作能力，同时获得了鲁棒和动态的运动。", "conclusion": "RAMBO通过结合基于模型的精确规划和强化学习的鲁棒性，有效解决了腿式机器人在移动操作中面临的挑战，实现了精确控制和动态鲁棒性。", "translation": "移动操作，即各种物体与运动同时协调的物理交互，对于腿式机器人来说仍然是一个重大挑战，因为它需要精确的末端执行器控制和对未建模动态的鲁棒性。虽然基于模型的控制器通过在线优化提供精确规划，但它们受模型不准确性的限制。相比之下，基于学习的方法提供了鲁棒性，但它们在精确调节交互力方面存在困难。我们引入了RAMBO，一个混合框架，它将基于模型的全身控制集成到通过强化学习训练的反馈策略中。基于模型的模块通过求解二次规划生成前馈扭矩，而策略提供反馈校正项以增强鲁棒性。我们在四足机器人上通过一系列真实的移动操作任务验证了我们的框架，例如推动购物车、平衡盘子和握持软物体，包括四足和两足行走。我们的实验表明，RAMBO实现了精确的操作能力，同时获得了鲁棒和动态的运动。", "summary": "RAMBO是一个针对腿式机器人移动操作的混合控制框架。它结合了基于模型的全身控制的精确规划能力和强化学习反馈策略的鲁棒性。基于模型的模块负责生成前馈扭矩，而强化学习策略则提供校正反馈以应对模型不准确性和未建模动态。该框架在真实世界的四足机器人上进行了验证，成功执行了多种复杂的移动操作任务，展示了其在实现精确操作和鲁棒动态运动方面的有效性。", "keywords": "移动操作, 全身控制, 强化学习, 基于模型, 混合控制", "comments": "RAMBO的创新之处在于其混合控制策略，巧妙地结合了模型预测控制的精确性和强化学习的适应性与鲁棒性。这种结合有效地弥补了单一方法在腿式机器人移动操作中的不足，为解决复杂物理交互和未建模动态问题提供了一个有前景的解决方案。其在真实世界任务中的验证进一步证明了其实用性和有效性。"}}
{"id": "2507.19353", "title": "Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks", "authors": ["Kai Liu", "Zhan Su", "Peijie Dong", "Fengran Mo", "Jianfei Gao", "ShaoTing Zhang", "Kai Chen"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19353v1", "summary": "Recently, recurrent large language models (Recurrent LLMs) with linear\ncomputational complexity have re-emerged as efficient alternatives to\nself-attention-based LLMs (Self-Attention LLMs), which have quadratic\ncomplexity. However, Recurrent LLMs often underperform on long-context tasks\ndue to their limited fixed-size memory. Previous research has primarily focused\non enhancing the memory capacity of Recurrent LLMs through architectural\ninnovations, but these approaches have not yet enabled Recurrent LLMs to match\nthe performance of Self-Attention LLMs on long-context tasks. We argue that\nthis limitation arises because processing the entire context at once is not\nwell-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a\nchunk-wise inference method inspired by human reading strategies. Smooth\nReading processes context in chunks and iteratively summarizes the contextual\ninformation, thereby reducing memory demands and making the approach more\ncompatible with Recurrent LLMs. Our experimental results show that this method\nsubstantially narrows the performance gap between Recurrent and Self-Attention\nLLMs on long-context tasks, while preserving the efficiency advantages of\nRecurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from\n5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench.\nBesides, our method maintains the high efficiency, training 3x faster and\ninferring 2x faster at 64k context compared to Self-Attention LLMs. To our\nknowledge, this is the first work to achieve comparable performance using\nRecurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope\nour method will inspire future research in this area. To facilitate further\nprogress, we will release code and dataset.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19353v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "流畅阅读：弥合循环大语言模型与自注意力大语言模型在长文本任务上的差距", "tldr": "循环大语言模型（Recurrent LLMs）效率高但在长文本任务上表现不佳。本文提出了一种名为“流畅阅读”的分块推理方法，显著缩小了Recurrent LLMs与自注意力大语言模型（Self-Attention LLMs）在长文本任务上的性能差距，同时保持了其效率优势，是首个实现Recurrent LLMs在长文本任务上与Self-Attention LLMs相当性能的工作。", "motivation": "循环大语言模型（Recurrent LLMs）虽然计算复杂度低、效率高，但在长文本任务上由于固定大小的内存限制而表现不佳。以往的研究主要通过架构创新来增强Recurrent LLMs的内存容量，但未能使其性能与自注意力大语言模型（Self-Attention LLMs）相匹配。作者认为，这种限制源于Recurrent LLMs不适合一次性处理整个上下文。", "method": "本文提出了一种名为“流畅阅读”（Smooth Reading）的分块推理方法，该方法受到人类阅读策略的启发。它以分块的方式处理上下文，并迭代地总结上下文信息，从而减少内存需求，使该方法更兼容Recurrent LLMs。", "result": "实验结果表明，“流畅阅读”方法显著缩小了Recurrent LLMs与Self-Attention LLMs在长文本任务上的性能差距，同时保留了Recurrent LLMs的效率优势。具体而言，它使SWA-3B-4k（一种Recurrent LLM）在LongBench上的表现从比Self-Attention LLMs低5.68%提升到高3.61%。此外，该方法保持了高效率，在64k上下文时，训练速度比Self-Attention LLMs快3倍，推理速度快2倍。据作者所知，这是首次实现Recurrent LLMs在长文本任务上与Self-Attention LLMs相当性能的工作。", "conclusion": "“流畅阅读”方法成功弥合了循环大语言模型与自注意力大语言模型在长文本任务上的性能差距，同时保持了循环大语言模型固有的效率优势，首次实现了两者在长文本任务上可比的性能。", "translation": "最近，计算复杂度为线性的循环大语言模型（Recurrent LLMs）重新成为自注意力大语言模型（Self-Attention LLMs）的有效替代方案，后者具有二次复杂度。然而，Recurrent LLMs由于其有限的固定大小内存，在长文本任务上通常表现不佳。以往的研究主要集中于通过架构创新来增强Recurrent LLMs的内存容量，但这些方法尚未使Recurrent LLMs在长文本任务上的性能与Self-Attention LLMs相匹配。我们认为，这种限制的出现是因为一次性处理整个上下文不适合Recurrent LLMs。在本文中，我们提出了一种名为“流畅阅读”的分块推理方法，该方法受到人类阅读策略的启发。“流畅阅读”以分块的方式处理上下文，并迭代地总结上下文信息，从而减少内存需求，使该方法更兼容Recurrent LLMs。我们的实验结果表明，该方法显著缩小了Recurrent LLMs与Self-Attention LLMs在长文本任务上的性能差距，同时保留了Recurrent LLMs的效率优势。“流畅阅读”使SWA-3B-4k（一种Recurrent LLM）在LongBench上的表现从比Self-Attention LLMs低5.68%提升到高3.61%。此外，我们的方法保持了高效率，在64k上下文时，训练速度比Self-Attention LLMs快3倍，推理速度快2倍。据我们所知，这是首次实现Recurrent LLMs在长文本任务上与Self-Attention LLMs相当性能的工作。我们希望我们的方法能启发该领域的未来研究。为了促进进一步的进展，我们将发布代码和数据集。", "summary": "循环大语言模型（Recurrent LLMs）因其高效性被视为自注意力大语言模型（Self-Attention LLMs）的替代品，但其在长文本任务上表现不佳。本文提出了一种受人类阅读启发的分块推理方法“流畅阅读”（Smooth Reading），该方法通过分块处理上下文并迭代总结信息来降低内存需求。实验证明，此方法显著缩小了Recurrent LLMs与Self-Attention LLMs在长文本任务上的性能差距（如SWA-3B-4k在LongBench上的性能提升），同时保持了Recurrent LLMs在训练和推理上的高效率。这是首次使用Recurrent LLMs在长文本任务上实现与Self-Attention LLMs可比性能的工作。", "keywords": "循环大语言模型, 自注意力大语言模型, 长文本任务, 流畅阅读, 分块推理", "comments": "创新点在于提出了一种非架构性的、受人类阅读策略启发的“分块推理”方法，以解决循环大语言模型在长文本处理上的固有劣势。这为提升高效但受限于内存的模型的长上下文能力开辟了新途径。其重要性在于，通过提升循环大语言模型在长文本任务上的性能，同时保持其计算效率，有望推动更高效的大语言模型发展。该工作声称是首次实现循环大语言模型在长文本任务上与自注意力大语言模型性能相当的突破。论文并未明确指出该方法的局限性，但“弥合差距”的表述可能暗示在某些极端或特定场景下，自注意力模型仍可能存在优势。"}}
{"id": "2507.15511", "title": "Certificate-Sensitive Subset Sum: Realizing Instance Complexity", "authors": ["Jesus Salas"], "categories": ["cs.CC", "cs.DS", "F.1.3; F.2.2"], "primary_category": "Subjects:       Computational Complexity (cs.CC)", "pdf_link": null, "comments": "Comments:      14 pages + appendix. Companion to arXiv:2503.20162 (\"Beyond Worst-Case Subset Sum: An Adaptive, Structure-Aware Solver with Sub-2^{n/2} Enumeration\"", "url": "http://arxiv.org/abs/2507.15511v3", "summary": "We present, to our knowledge, the first deterministic, certificate-sensitive\nalgorithm for a canonical NP-complete problem whose runtime provably adapts to\nthe structure of each input. For a Subset-Sum instance $(S, t)$, let\n$\\Sigma(S)$ denote the set of distinct subset sums and define $U =\n|\\Sigma(S)|$. This set serves as an information-theoretically minimal witness,\nthe instance-complexity (IC) certificate.\n  Our solver, IC-SubsetSum, enumerates every element of $\\Sigma(S)$ in\ndeterministic time $O(U \\cdot n^2)$ and space $O(U \\cdot n)$. A randomized\nvariant achieves expected runtime $O(U \\cdot n)$. The algorithm's complexity is\nthus directly governed by the certificate size, and this structure-sensitive\nperformance is paired with a guaranteed worst-case runtime of $O^*(2^{n/2 -\n\\varepsilon})$ for some constant $\\varepsilon > 0$, the first such result to\nstrictly outperform classical methods on every instance.\n  We revisit fine-grained reductions that rely on the classical $2^{n/2}$\nhardness of SubsetSum and show that these arguments hold only for\ncollision-free instances where $U$ is maximal. IC-SubsetSum reframes this\nbarrier structurally and introduces a new paradigm for certificate-sensitive\nalgorithms across NP-complete problems.", "comment": "14 pages + appendix. Companion to arXiv:2503.20162 (\"Beyond\n  Worst-Case Subset Sum: An Adaptive, Structure-Aware Solver with Sub-2^{n/2}\n  Enumeration\"", "pdf_url": "http://arxiv.org/pdf/2507.15511v3", "cate": "cs.CC", "date": "2025-07-21", "updated": "2025-07-25", "AI": {"title_translation": "证书敏感子集和：实现实例复杂度", "tldr": "提出首个确定性、证书敏感的子集和算法IC-SubsetSum，其运行时间与实例复杂度（U）挂钩，并在最坏情况下优于经典算法，为NP完全问题引入新范式。", "motivation": "旨在开发首个确定性、证书敏感的算法，使其运行时能根据每个输入的结构自适应，解决经典的NP完全问题子集和。", "method": "提出了IC-SubsetSum算法，通过枚举所有不同的子集和集合$\\Sigma(S)$的元素，该集合的大小$U = |\\Sigma(S)|$作为实例复杂度（IC）证书。它以确定性时间$O(U \\cdot n^2)$和空间$O(U \\cdot n)$运行，其随机化变体期望运行时间为$O(U \\cdot n)$。", "result": "IC-SubsetSum算法的性能直接受证书大小U控制，且在最坏情况下能达到$O^*(2^{n/2 - \\varepsilon})$，首次严格优于所有实例上的经典方法。研究还表明，依赖经典$2^{n/2}$硬度的细粒度归约仅适用于U最大的无碰撞实例。", "conclusion": "IC-SubsetSum算法通过结构性地重构了子集和问题的计算障碍，并为NP完全问题引入了一种新的证书敏感算法范式。", "translation": "我们提出了据我们所知第一个针对经典NP完全问题的确定性、证书敏感算法，其运行时可证明地适应每个输入的结构。对于一个子集和实例$(S, t)$，令$\\Sigma(S)$表示不同子集和的集合，并定义$U = |\\Sigma(S)|$。这个集合作为信息论上最小的见证，即实例复杂度（IC）证书。我们的求解器IC-SubsetSum以确定性时间$O(U \\cdot n^2)$和空间$O(U \\cdot n)$枚举$\\Sigma(S)$的每个元素。一个随机化变体实现了期望运行时间$O(U \\cdot n)$。因此，算法的复杂性直接由证书大小控制，这种结构敏感的性能与一个保证的最坏情况运行时$O^*(2^{n/2 - \\varepsilon})$配对，其中$\\varepsilon > 0$是某个常数，这是第一个在每个实例上严格优于经典方法的结果。我们重新审视了依赖于子集和经典$2^{n/2}$硬度的细粒度归约，并表明这些论证仅适用于U最大时的无碰撞实例。IC-SubsetSum在结构上重构了这一障碍，并为NP完全问题引入了一种新的证书敏感算法范式。", "summary": "这篇论文介绍了IC-SubsetSum，这是首个确定性、证书敏感的子集和算法。该算法的运行时性能与实例复杂度证书$U$（即不同子集和的数量）直接相关，其确定性时间复杂度为$O(U \\cdot n^2)$。它在最坏情况下能达到$O^*(2^{n/2 - \\varepsilon})$的性能，首次在所有实例上严格超越了经典的$2^{n/2}$方法。论文还揭示了依赖传统硬度结果的细粒度归约仅对特定类型的实例有效，并通过IC-SubsetSum为NP完全问题领域引入了一种新的结构敏感算法范式。", "keywords": "子集和, 证书敏感算法, 实例复杂度, NP完全问题, 算法复杂性", "comments": "这项工作具有显著的创新性，因为它首次为NP完全问题提供了一种确定性、证书敏感的算法，其性能能够根据输入实例的内在结构（由实例复杂度U衡量）自适应调整。突破了经典$2^{n/2}$的复杂性下限，并在所有实例上实现性能提升，这对于理解和解决NP完全问题具有重要意义。它还重新审视并限定了现有细粒度归约的适用范围，为算法设计开辟了新的视角。"}}
{"id": "2507.15700", "title": "Estimating Rate-Distortion Functions Using the Energy-Based Model", "authors": ["Shitong Wu", "Sicheng Xu", "Lingyi Chen", "Huihui Wu", "Wenyi Zhang"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      6 pages, 6 figures", "url": "http://arxiv.org/abs/2507.15700v2", "summary": "The rate-distortion (RD) theory is one of the key concepts in information\ntheory, providing theoretical limits for compression performance and guiding\nthe source coding design, with both theoretical and practical significance. The\nBlahut-Arimoto (BA) algorithm, as a classical algorithm to compute RD\nfunctions, encounters computational challenges when applied to high-dimensional\nscenarios. In recent years, many neural methods have attempted to compute\nhigh-dimensional RD problems from the perspective of implicit generative\nmodels. Nevertheless, these approaches often neglect the reconstruction of the\noptimal conditional distribution or rely on unreasonable prior assumptions. In\nface of these issues, we propose an innovative energy-based modeling framework\nthat leverages the connection between the RD dual form and the free energy in\nstatistical physics, achieving effective reconstruction of the optimal\nconditional distribution.The proposed algorithm requires training only a single\nneural network and circumvents the challenge of computing the normalization\nfactor in energy-based models using the Markov chain Monte Carlo (MCMC)\nsampling. Experimental results demonstrate the significant effectiveness of the\nproposed algorithm in estimating high-dimensional RD functions and\nreconstructing the optimal conditional distribution.", "comment": "6 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.15700v2", "cate": "cs.IT", "date": "2025-07-21", "updated": "2025-07-25", "AI": {"title_translation": "使用基于能量的模型估计率失真函数", "tldr": "提出一种基于能量模型的新框架，用于有效估计高维率失真函数并重建最优条件分布，解决了现有算法的计算挑战和神经方法的局限性。", "motivation": "现有率失真函数计算算法（如Blahut-Arimoto）在高维场景下存在计算挑战；近期神经方法忽略最优条件分布重建或依赖不合理先验假设。", "method": "提出一种创新的基于能量的建模框架，利用率失真对偶形式与统计物理中自由能的联系，实现最优条件分布的有效重建。该算法只需训练一个神经网络，并通过马尔可夫链蒙特卡罗（MCMC）采样规避了计算基于能量模型中归一化因子的挑战。", "result": "实验结果表明，所提出的算法在估计高维率失真函数和重建最优条件分布方面具有显著有效性。", "conclusion": "提出的基于能量的模型框架能够有效解决高维率失真函数估计和最优条件分布重建的挑战，为信息论中的率失真理论提供了新的计算方法。", "translation": "率失真（RD）理论是信息论中的关键概念之一，为压缩性能提供了理论极限并指导了信源编码设计，具有理论和实践意义。Blahut-Arimoto（BA）算法作为计算RD函数的经典算法，在应用于高维场景时遇到计算挑战。近年来，许多神经方法试图从隐式生成模型的角度计算高维RD问题。然而，这些方法往往忽略了最优条件分布的重建或依赖不合理的先验假设。面对这些问题，我们提出了一种创新的基于能量的建模框架，利用RD对偶形式与统计物理中自由能的联系，实现了最优条件分布的有效重建。所提出的算法只需训练一个神经网络，并通过马尔可夫链蒙特卡罗（MCMC）采样规避了计算基于能量模型中归一化因子的挑战。实验结果表明，所提出的算法在估计高维RD函数和重建最优条件分布方面具有显著有效性。", "summary": "本文提出一种创新的基于能量的建模框架，旨在解决高维率失真（RD）函数估计中现有算法的计算挑战和神经方法的局限性。该框架利用RD对偶形式与统计物理中自由能的联系，有效重建最优条件分布，并且仅需训练一个神经网络，通过MCMC采样避免归一化因子计算问题。实验证明该方法在估计高维RD函数和重建最优条件分布方面表现出显著效果。", "keywords": "率失真函数, 能量模型, 信息论, 高维估计, 最优条件分布", "comments": "本文的创新点在于将率失真理论与统计物理中的能量模型相结合，并有效解决了高维RD函数估计中常见的计算复杂度和最优条件分布重建难题。通过规避归一化因子计算，提高了算法的实用性。"}}
{"id": "2507.19272", "title": "Video Self-Distillation for Single-Image Encoders: A Step Toward Physically Plausible Perception", "authors": ["Marcel Simon", "Tae-Ho Kim", "Seul-Ki Yeom"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      4 pages, 2 figures, 2 tables", "url": "http://arxiv.org/abs/2507.19272v1", "summary": "Self-supervised image encoders such as DINO have recently gained significant\ninterest for learning robust visual features without labels. However, most SSL\nmethods train on static images and miss the temporal cues inherent in videos.\nWe introduce a video-distilled single-image encoder trained to predict the\nnext-frame representation from the current frame. This simple objective injects\n3D spatial and temporal priors without optical flow or tracking. When\npre-training on a single 2-hour video, our approach raises the mean\nIntersection-over-Union (mIoU) on ADE20K from 35.0 (DoRA) to 36.4 while\nremaining a drop-in replacement for image-only pipelines. Our results highlight\nvideo self-distillation as a lightweight route to geometry-aware perception an\nessential ingredient for physically plausible world models and Physical AI.", "comment": "4 pages, 2 figures, 2 tables", "pdf_url": "http://arxiv.org/pdf/2507.19272v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "视频自蒸馏用于单图像编码器：迈向物理合理感知的一步", "tldr": "本文提出一种视频自蒸馏方法，训练单图像编码器预测下一帧表示，从而在不使用光流或跟踪的情况下注入3D空间和时间先验。该方法提高了图像分割性能，为物理AI奠定基础。", "motivation": "现有的自监督学习（SSL）方法主要在静态图像上训练，忽略了视频中固有的时间线索，导致其无法捕捉3D空间和时间先验，这对于构建物理合理的世界模型和物理AI至关重要。", "method": "本文提出一种视频蒸馏的单图像编码器。该编码器通过预测当前帧的下一帧表示来训练。这种简单目标在不使用光流或跟踪的情况下，注入了3D空间和时间先验。", "result": "在单个2小时视频上进行预训练时，该方法将ADE20K上的平均交并比（mIoU）从35.0（DoRA）提高到36.4。同时，它仍然可以作为图像专用管道的直接替代品。", "conclusion": "视频自蒸馏是一种实现几何感知感知的轻量级途径，是构建物理合理世界模型和物理AI的重要组成部分。", "translation": "自监督图像编码器，如DINO，最近在无需标签学习鲁棒视觉特征方面获得了显著关注。然而，大多数自监督学习方法在静态图像上训练，并错过了视频中固有的时间线索。我们引入了一种视频蒸馏的单图像编码器，该编码器经过训练，可以从当前帧预测下一帧的表示。这个简单的目标在不使用光流或跟踪的情况下，注入了3D空间和时间先验。当在单个2小时视频上进行预训练时，我们的方法将ADE20K上的平均交并比（mIoU）从35.0（DoRA）提高到36.4，同时仍然可以作为仅图像管道的直接替代品。我们的结果强调，视频自蒸馏是实现几何感知感知的轻量级途径，这是构建物理合理世界模型和物理AI的重要组成部分。", "summary": "本文提出一种新颖的视频自蒸馏方法，用于训练单图像编码器。通过让编码器预测下一帧表示，该方法在不依赖光流或跟踪的情况下，有效地为图像编码器注入了3D空间和时间先验。实验结果表明，在少量视频数据上预训练后，该方法显著提升了图像分割任务的性能（ADE20K mIoU从35.0提升至36.4），验证了视频自蒸馏在实现几何感知感知方面的有效性，为物理合理的世界模型和物理AI铺平了道路。", "keywords": "视频自蒸馏, 单图像编码器, 自监督学习, 几何感知, 物理AI", "comments": "该研究创新性地将视频中的时间信息通过自蒸馏的方式引入到单图像编码器的训练中，提供了一种轻量级且有效的方法来注入3D空间和时间先验，避免了复杂的光流或跟踪算法。其重要性在于，它为构建更具物理合理性的世界模型和物理AI提供了关键技术，有助于弥补现有自监督学习方法在处理动态场景时的不足。"}}
{"id": "2507.18725", "title": "The Right to be Forgotten in Pruning: Unveil Machine Unlearning on Sparse Models", "authors": ["Yang Xiao", "Gen Li", "Jie Ji", "Ruimeng Ye", "Xiaolong Ma", "Bo Hui"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages for main part", "url": "http://arxiv.org/abs/2507.18725v1", "summary": "Machine unlearning aims to efficiently eliminate the memory about deleted\ndata from trained models and address the right to be forgotten. Despite the\nsuccess of existing unlearning algorithms, unlearning in sparse models has not\nyet been well studied. In this paper, we empirically find that the deleted data\nhas an impact on the pruned topology in a sparse model. Motivated by the\nobservation and the right to be forgotten, we define a new terminology\n``un-pruning\" to eliminate the impact of deleted data on model pruning. Then we\npropose an un-pruning algorithm to approximate the pruned topology driven by\nretained data. We remark that any existing unlearning algorithm can be\nintegrated with the proposed un-pruning workflow and the error of un-pruning is\nupper-bounded in theory. Also, our un-pruning algorithm can be applied to both\nstructured sparse models and unstructured sparse models. In the experiment, we\nfurther find that Membership Inference Attack (MIA) accuracy is unreliable for\nassessing whether a model has forgotten deleted data, as a small change in the\namount of deleted data can produce arbitrary MIA results. Accordingly, we\ndevise new performance metrics for sparse models to evaluate the success of\nun-pruning. Lastly, we conduct extensive experiments to verify the efficacy of\nun-pruning with various pruning methods and unlearning algorithms. Our code is\nreleased at https://anonymous.4open.science/r/UnlearningSparseModels-FBC5/.", "comment": "9 pages for main part", "pdf_url": "http://arxiv.org/pdf/2507.18725v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "剪枝中的遗忘权：揭示稀疏模型上的机器遗忘", "tldr": "针对稀疏模型中机器遗忘的研究不足，本文提出了“反剪枝”概念和算法，以消除已删除数据对模型剪枝拓扑的影响，并设计了新的性能评估指标，实验验证了其有效性。", "motivation": "尽管现有遗忘算法取得了成功，但稀疏模型中的遗忘尚未得到充分研究。本文实证发现，已删除数据对稀疏模型中的剪枝拓扑有影响。受此观察和“被遗忘权”的启发，需要解决稀疏模型中删除数据记忆的消除问题。", "method": "本文定义了“反剪枝”这一新术语，旨在消除已删除数据对模型剪枝的影响。提出了一种反剪枝算法，以近似由保留数据驱动的剪枝拓扑。该算法可与任何现有遗忘算法集成，并且其反剪枝误差在理论上是有上限的。它适用于结构化和非结构化稀疏模型。此外，还设计了新的性能指标来评估反剪枝的成功性，因为成员推理攻击（MIA）的准确性被发现不可靠。", "result": "实验发现，成员推理攻击（MIA）的准确性在评估模型是否忘记已删除数据方面是不可靠的，因为已删除数据量的微小变化可能导致任意的MIA结果。通过广泛的实验，验证了所提出的反剪枝方法在各种剪枝方法和遗忘算法下的有效性。", "conclusion": "本文提出了“反剪枝”的概念和算法，有效解决了稀疏模型中的机器遗忘问题，并通过理论分析和大量实验验证了其有效性。同时，指出了MIA在稀疏模型遗忘评估中的局限性，并提出了新的评估指标。", "translation": "机器遗忘旨在高效地从训练模型中消除关于已删除数据的记忆，并解决“被遗忘权”问题。尽管现有遗忘算法取得了成功，但稀疏模型中的遗忘尚未得到充分研究。在本文中，我们实证发现已删除数据对稀疏模型中的剪枝拓扑有影响。受此观察和“被遗忘权”的启发，我们定义了一个新术语“反剪枝”来消除已删除数据对模型剪枝的影响。然后，我们提出了一种反剪枝算法来近似由保留数据驱动的剪枝拓扑。我们指出，任何现有的遗忘算法都可以与所提出的反剪枝工作流集成，并且反剪枝的误差在理论上是有上限的。此外，我们的反剪枝算法可以应用于结构化稀疏模型和非结构化稀疏模型。在实验中，我们进一步发现成员推理攻击（MIA）的准确性在评估模型是否忘记已删除数据方面是不可靠的，因为已删除数据量的微小变化可能产生任意的MIA结果。因此，我们为稀疏模型设计了新的性能指标来评估反剪枝的成功性。最后，我们进行了广泛的实验来验证反剪枝在各种剪枝方法和遗忘算法下的功效。我们的代码已发布在https://anonymous.4open.science/r/UnlearningSparseModels-FBC5/。", "summary": "本文针对稀疏模型中机器遗忘研究的不足，提出了一种新的概念“反剪枝”及其算法，旨在消除已删除数据对模型剪枝拓扑的影响。研究发现已删除数据确实影响剪枝拓扑。所提出的反剪枝算法能够近似由保留数据驱动的剪枝拓扑，并可与现有遗忘算法集成，理论上误差有上限，且适用于多种稀疏模型。此外，文章指出成员推理攻击（MIA）在评估稀疏模型遗忘效果方面并不可靠，并为此设计了新的性能评估指标。广泛的实验验证了反剪枝方法的有效性。", "keywords": "机器遗忘, 稀疏模型, 剪枝, 反剪枝, 被遗忘权", "comments": "本文的创新点在于提出了“反剪枝”这一新颖概念，并设计了专门针对稀疏模型的机器遗忘算法，填补了该领域的研究空白。其理论上对误差的上限保证以及对结构化和非结构化稀疏模型的普适性增加了其价值。此外，对MIA在稀疏模型中评估局限性的发现和新评估指标的提出也具有重要意义，对未来研究提供了指导。"}}
{"id": "2505.19630", "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue", "authors": ["Yichun Feng", "Jiawei Wang", "Lu Zhou", "Zhen Lei", "Yixue Li"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.19630v2", "summary": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Single-round consultation\nsystems require patients to describe all symptoms upfront, leading to vague\ndiagnosis with unclear complaints. Traditional multi-turn dialogue models,\nconstrained by static supervised learning, lack flexibility and fail to\nintelligently extract key clinical information. To address these limitations,\nwe propose \\Ours{}, a reinforcement learning (RL)-based multi-agent\ncollaborative framework that models medical consultations as a dynamic\ndecision-making process under uncertainty. The doctor agent continuously\noptimizes its questioning strategy within the RL framework through multi-turn\ninteractions with the patient agent, dynamically adjusting its\ninformation-gathering path based on comprehensive rewards from the Consultation\nEvaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop\ninteraction strategies aligned with clinical reasoning logic, rather than\nsuperficially imitating patterns in existing dialogue data. Notably, we\nconstructed MTMedDialog, the first English multi-turn medical consultation\ndataset capable of simulating patient interactions. Experiments demonstrate\nthat \\Ours{} outperforms existing models in both multi-turn reasoning\ncapability and final diagnostic performance. This approach shows immense\npractical value by reducing misdiagnosis risks in time-pressured settings,\nfreeing clinicians for complex cases, and pioneering a strategy to optimize\nmedical resource allocation and alleviate workforce shortages. Code and data\nare available at https://github.com/JarvisUSTC/DoctorAgent-RL", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.19630v2", "cate": "cs.CL", "date": "2025-05-26", "updated": "2025-07-25", "AI": {"title_translation": "DoctorAgent-RL：一个用于多轮临床对话的多智能体协作强化学习系统", "tldr": "大型语言模型（LLMs）在真实临床咨询中面临挑战，因为单轮和静态多轮模型存在局限性。DoctorAgent-RL是一个基于强化学习的多智能体系统，旨在优化医生在多轮临床对话中的提问策略，其性能优于现有模型，并具有显著的实用价值。", "motivation": "大型语言模型（LLMs）在生物医学问答方面表现出色，但在实际临床咨询中仍面临挑战。单轮咨询系统要求患者预先描述所有症状，导致诊断模糊不清。传统的基于静态监督学习的多轮对话模型缺乏灵活性，无法智能地提取关键临床信息。", "method": "我们提出了DoctorAgent-RL，一个基于强化学习（RL）的多智能体协作框架，将医疗咨询建模为不确定性下的动态决策过程。医生智能体通过与患者智能体的多轮交互，在RL框架内持续优化其提问策略，并根据咨询评估器的综合奖励动态调整其信息收集路径。这种RL微调机制使LLMs能够自主开发与临床推理逻辑一致的交互策略。我们还构建了MTMedDialog，这是第一个能够模拟患者交互的英语多轮医疗咨询数据集。", "result": "DoctorAgent-RL在多轮推理能力和最终诊断性能方面均优于现有模型。", "conclusion": "该方法通过减少时间压力下的误诊风险、解放临床医生处理复杂病例以及开创优化医疗资源分配和缓解劳动力短缺的策略，展现了巨大的实用价值。", "translation": "大型语言模型（LLMs）在生物医学问答领域展现了卓越的能力，但它们在实际临床咨询中的应用仍面临核心挑战。单轮咨询系统要求患者预先描述所有症状，导致诊断模糊，主诉不清。传统的基于静态监督学习的多轮对话模型缺乏灵活性，无法智能地提取关键临床信息。为了解决这些限制，我们提出了\nOurs{}，一个基于强化学习（RL）的多智能体协作框架，将医疗咨询建模为不确定性下的动态决策过程。医生智能体通过与患者智能体的多轮交互，在RL框架内持续优化其提问策略，并根据咨询评估器的综合奖励动态调整其信息收集路径。这种RL微调机制使LLMs能够自主开发与临床推理逻辑一致的交互策略，而不是肤浅地模仿现有对话数据中的模式。值得注意的是，我们构建了MTMedDialog，这是第一个能够模拟患者交互的英语多轮医疗咨询数据集。实验表明，\nOurs{}在多轮推理能力和最终诊断性能方面均优于现有模型。这种方法通过减少时间压力下的误诊风险、解放临床医生处理复杂病例以及开创优化医疗资源分配和缓解劳动力短缺的策略，展现了巨大的实用价值。代码和数据可在https://github.com/JarvisUSTC/DoctorAgent-RL获取。", "summary": "本研究提出了DoctorAgent-RL，一个基于强化学习的多智能体协作系统，旨在解决大型语言模型在多轮临床对话中面临的挑战。该系统将医疗咨询视为不确定性下的动态决策过程，医生智能体通过与患者智能体的多轮交互和来自咨询评估器的奖励，优化其提问策略，从而使LLMs能够发展出符合临床推理的交互策略。研究还构建了首个可模拟患者交互的英语多轮医疗咨询数据集MTMedDialog。实验结果表明，DoctorAgent-RL在多轮推理能力和诊断性能上均优于现有模型，并具有减少误诊、优化医疗资源分配的实际应用价值。", "keywords": "多智能体系统, 强化学习, 临床对话, 大型语言模型, 医疗咨询", "comments": "这项工作通过引入强化学习和多智能体协作框架，显著提升了LLMs在复杂多轮临床对话中的表现，解决了传统模型在灵活性和智能信息提取方面的不足。构建了首个多轮医疗咨询数据集，为该领域的研究提供了宝贵资源。其应用潜力巨大，有望减轻医疗负担并提高诊断准确性。"}}
{"id": "2507.18911", "title": "Synthetic-to-Real Camouflaged Object Detection", "authors": ["Zhihao Luo", "Luojun Lin", "Zheng Lin"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18911v2", "summary": "Due to the high cost of collection and labeling, there are relatively few\ndatasets for camouflaged object detection (COD). In particular, for certain\nspecialized categories, the available image dataset is insufficiently\npopulated. Synthetic datasets can be utilized to alleviate the problem of\nlimited data to some extent. However, directly training with synthetic datasets\ncompared to real datasets can lead to a degradation in model performance. To\ntackle this problem, in this work, we investigate a new task, namely\nSyn-to-Real Camouflaged Object Detection (S2R-COD). In order to improve the\nmodel performance in real world scenarios, a set of annotated synthetic\ncamouflaged images and a limited number of unannotated real images must be\nutilized. We propose the Cycling Syn-to-Real Domain Adaptation Framework\n(CSRDA), a method based on the student-teacher model. Specially, CSRDA\npropagates class information from the labeled source domain to the unlabeled\ntarget domain through pseudo labeling combined with consistency regularization.\nConsidering that narrowing the intra-domain gap can improve the quality of\npseudo labeling, CSRDA utilizes a recurrent learning framework to build an\nevolving real domain for bridging the source and target domain. Extensive\nexperiments demonstrate the effectiveness of our framework, mitigating the\nproblem of limited data and handcraft annotations in COD. Our code is publicly\navailable at: https://github.com/Muscape/S2R-COD.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18911v2", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-28", "AI": {"title_translation": "合成到真实伪装目标检测", "tldr": "针对伪装目标检测数据稀缺问题，本文提出了一种从合成数据到真实数据的域适应框架CSRDA，通过学生-教师模型、伪标签和循环学习，有效利用合成数据和少量真实数据提升模型在真实场景下的性能。", "motivation": "伪装目标检测（COD）数据集的收集和标注成本高昂，导致数据稀缺，特别是对于特定专业类别。直接使用合成数据集进行训练会导致模型性能下降，因此需要一种方法来弥合合成数据与真实数据之间的差距。", "method": "本文提出循环合成到真实域适应框架（CSRDA），这是一种基于学生-教师模型的方法。CSRDA通过伪标签结合一致性正则化，将类别信息从有标签的源域传播到无标签的目标域。为缩小域内差距并提高伪标签质量，CSRDA利用循环学习框架构建一个不断演变的真实域，以连接源域和目标域。", "result": "大量的实验证明了所提出的CSRDA框架的有效性，成功缓解了伪装目标检测中数据有限和手动标注的问题。", "conclusion": "通过提出的CSRDA框架，可以有效利用合成数据和少量未标注的真实数据来提升伪装目标检测模型在真实场景中的性能，从而解决数据稀缺和标注成本高的问题。", "translation": "由于收集和标注成本高昂，伪装目标检测（COD）的数据集相对较少。特别是对于某些特殊类别，可用的图像数据集数量不足。合成数据集可以在一定程度上缓解数据有限的问题。然而，与真实数据集相比，直接使用合成数据集进行训练会导致模型性能下降。为了解决这个问题，本文研究了一项新任务，即合成到真实伪装目标检测（S2R-COD）。为了提高模型在真实世界场景中的性能，必须利用一组带标注的合成伪装图像和少量未标注的真实图像。我们提出了循环合成到真实域适应框架（CSRDA），这是一种基于学生-教师模型的方法。具体而言，CSRDA通过伪标签结合一致性正则化，将类别信息从有标签的源域传播到无标签的目标域。考虑到缩小域内差距可以提高伪标签的质量，CSRDA利用循环学习框架构建一个不断演变的真实域，以弥合源域和目标域之间的鸿沟。大量的实验证明了我们框架的有效性，缓解了COD中数据有限和手动标注的问题。我们的代码已公开可用：https://github.com/Muscape/S2R-COD。", "summary": "本文针对伪装目标检测（COD）领域数据稀缺和标注成本高的问题，提出了一项新任务——合成到真实伪装目标检测（S2R-COD）。为解决直接使用合成数据训练导致性能下降的问题，作者提出了循环合成到真实域适应框架（CSRDA）。该框架基于学生-教师模型，利用带标注的合成图像和少量未标注的真实图像，通过伪标签和一致性正则化将标签信息从源域传递到目标域。同时，CSRDA采用循环学习来缩小域内差距，提高伪标签质量。实验结果表明，该框架能有效缓解COD中数据和标注的限制。", "keywords": "伪装目标检测, 域适应, 合成数据, 学生-教师模型, 数据稀缺", "comments": "这篇论文的创新点在于提出了S2R-COD任务，并针对性地设计了CSRDA框架来解决合成数据到真实数据的域适应问题。其核心思想是利用学生-教师模型、伪标签和循环学习来弥合合成与真实数据之间的差距，这对于数据稀缺的视觉任务具有重要意义。该方法有效利用了有限的真实数据，降低了数据标注成本。"}}
{"id": "2507.19182", "title": "Faster Lifting for Ordered Domains with Predecessor Relations", "authors": ["Kuncheng Zou", "Jiahao Mai", "Yonggang Zhang", "Yuyi Wang", "Ondřej Kuželka", "Yuanhong Wang", "Yi Chang"], "categories": ["cs.AI", "cs.LO"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19182v1", "summary": "We investigate lifted inference on ordered domains with predecessor\nrelations, where the elements of the domain respect a total (cyclic) order, and\nevery element has a distinct (clockwise) predecessor. Previous work has\nexplored this problem through weighted first-order model counting (WFOMC),\nwhich computes the weighted sum of models for a given first-order logic\nsentence over a finite domain. In WFOMC, the order constraint is typically\nencoded by the linear order axiom introducing a binary predicate in the\nsentence to impose a linear ordering on the domain elements. The immediate and\nsecond predecessor relations are then encoded by the linear order predicate.\nAlthough WFOMC with the linear order axiom is theoretically tractable, existing\nalgorithms struggle with practical applications, particularly when the\npredecessor relations are involved. In this paper, we treat predecessor\nrelations as a native part of the axiom and devise a novel algorithm that\ninherently supports these relations. The proposed algorithm not only provides\nan exponential speedup for the immediate and second predecessor relations,\nwhich are known to be tractable, but also handles the general k-th predecessor\nrelations. The extensive experiments on lifted inference tasks and\ncombinatorics math problems demonstrate the efficiency of our algorithm,\nachieving speedups of a full order of magnitude.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19182v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "有序域与前驱关系上的更快提升推理", "tldr": "本文提出了一种新的算法，用于处理有序域中带有前驱关系的提升推理，相较于现有WFOMC方法，在处理前驱关系时能实现指数级加速和数量级提升。", "motivation": "现有通过加权一阶模型计数（WFOMC）处理有序域中带有前驱关系的提升推理方法，尽管理论上可行，但在实际应用中，特别是在涉及前驱关系时，算法表现不佳。", "method": "本文将前驱关系作为公理的固有部分，设计了一种新颖的算法，该算法固有地支持这些关系。", "result": "所提出的算法不仅对已知可处理的直接和第二前驱关系提供了指数级加速，而且还能处理一般的k阶前驱关系。在提升推理任务和组合数学问题上的广泛实验表明，该算法效率高，实现了数量级的加速。", "conclusion": "本文提出的新算法通过将前驱关系作为公理的固有部分处理，显著提升了有序域中带前驱关系的提升推理效率，解决了现有WFOMC方法的实际应用难题。", "translation": "我们研究了带有前驱关系的有序域上的提升推理，其中域的元素遵循一个全序（循环序），并且每个元素都有一个独特的（顺时针）前驱。先前的工作通过加权一阶模型计数（WFOMC）探索了这个问题，该方法计算给定有限域上一阶逻辑语句的模型加权和。在WFOMC中，顺序约束通常通过引入二元谓词的线性序公理来编码，以对域元素施加线性排序。直接和第二前驱关系随后通过线性序谓词编码。尽管带有线性序公理的WFOMC在理论上是可处理的，但现有算法在实际应用中表现不佳，特别是在涉及前驱关系时。在本文中，我们将前驱关系视为公理的固有部分，并设计了一种新颖的算法，该算法固有地支持这些关系。所提出的算法不仅对已知可处理的直接和第二前驱关系提供了指数级加速，而且还能处理一般的k阶前驱关系。在提升推理任务和组合数学问题上的广泛实验表明，我们的算法效率高，实现了数量级的加速。", "summary": "本文针对有序域中带有前驱关系的提升推理问题，提出了一种新的算法。该算法将前驱关系作为公理的固有部分进行处理，解决了现有加权一阶模型计数（WFOMC）方法在实际应用中，特别是处理前驱关系时的效率问题。实验证明，该算法对直接和第二前驱关系提供了指数级加速，并能处理更一般的k阶前驱关系，整体效率提升了一个数量级。", "keywords": "提升推理, 有序域, 前驱关系, 加权一阶模型计数, 算法加速", "comments": "本文的主要创新在于将前驱关系作为公理的“原生”部分进行处理，而非像传统WFOMC那样通过线性序公理间接编码。这种方法显著提升了算法在处理涉及前驱关系的提升推理任务时的实际效率，解决了现有方法在理论可处理性与实践性能之间的鸿沟。其对直接和第二前驱关系的指数级加速以及对一般k阶前驱关系的处理能力，显示了该方法的重要性和实用性。"}}
{"id": "2507.19111", "title": "Radio Map Assisted Routing and Predictive Resource Allocation over Dynamic Low Altitude Networks", "authors": ["Bowen Li", "Junting Chen"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19111v1", "summary": "Dynamic low altitude networks offer significant potential for efficient and\nreliable data transport via unmanned aerial vehicles (UAVs) relays which\nusually operate with predetermined trajectories. However, it is challenging to\noptimize the data routing and resource allocation due to the time-varying\ntopology and the need to control interference with terrestrial systems.\nTraditional schemes rely on time-expanded graphs with uniform and fine time\nsubdivisions, making them impractical for interference-aware applications. This\npaper develops a dynamic space-time graph model with a cross-layer optimization\nframework that converts a joint routing and predictive resource allocation\nproblem into a joint bottleneck path planning and resource allocation problem.\nWe develop explicit deterministic bounds to handle the channel uncertainty and\nprove a monotonicity property in the problem structure that enables us to\nefficiently reach the globally optimal solution to the predictive resource\nallocation subproblem. Then, this approach is extended to multi-commodity\ntransmission tasks through time-frequency allocation, and a bisection search\nalgorithm is developed to find the optimum solution by leveraging the\nmonotonicity of the feasible set family. Simulations verify that the\nsingle-commodity algorithm approaches global optimality with more than 30 dB\nperformance gain over the classical graph-based methods for delay-sensitive and\nlarge data transportation. At the same time, the multi-commodity method\nachieves 100X improvements in dense service scenarios and enables an additional\n20 dB performance gain by data segmenting.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19111v1", "cate": "eess.SY", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "无线电地图辅助的动态低空网络路由与预测资源分配", "tldr": "本文提出了一种动态时空图模型和跨层优化框架，用于动态低空无人机网络中的路由和资源分配，相较于传统方法实现了显著的性能提升。", "motivation": "由于时变拓扑结构以及需要控制对地面系统的干扰，优化动态低空无人机（UAV）网络中的数据路由和资源分配具有挑战性，传统的时域扩展图方案在干扰感知应用中不切实际。", "method": "本文开发了一种动态时空图模型，并提出了一个跨层优化框架，将联合路由和预测资源分配问题转化为联合瓶颈路径规划和资源分配问题。该方法利用明确的确定性边界处理信道不确定性，并利用问题结构中的单调性属性有效地找到预测资源分配子问题的全局最优解。此外，通过时频分配将该方法扩展到多商品传输任务，并开发了一种二分搜索算法来寻找最优解。", "result": "单商品算法在延迟敏感和大数据传输方面，相对于经典基于图的方法，性能增益超过30 dB，接近全局最优。多商品方法在密集服务场景中实现了100倍的改进，并通过数据分段实现了额外的20 dB性能增益。", "conclusion": "所提出的动态时空图模型和优化框架为动态低空网络中的路由和预测资源分配提供了显著的性能改进，尤其适用于延迟敏感和多商品传输任务。", "translation": "动态低空网络通过无人机（UAV）中继器在数据传输方面具有巨大的潜力，这些中继器通常按照预定轨迹运行。然而，由于时变拓扑结构以及需要控制对地面系统的干扰，优化数据路由和资源分配具有挑战性。传统方案依赖于具有均匀和精细时间细分的时域扩展图，这使得它们在干扰感知应用中不切实际。本文开发了一种动态时空图模型，并提出了一个跨层优化框架，将联合路由和预测资源分配问题转化为联合瓶颈路径规划和资源分配问题。我们开发了明确的确定性边界来处理信道不确定性，并证明了问题结构中的单调性，这使我们能够有效地达到预测资源分配子问题的全局最优解。然后，通过时频分配将该方法扩展到多商品传输任务，并开发了一种二分搜索算法，通过利用可行集族的单调性来找到最优解。仿真验证了单商品算法在延迟敏感和大数据传输方面，相对于经典基于图的方法，性能增益超过30 dB，接近全局最优。同时，多商品方法在密集服务场景中实现了100倍的改进，并通过数据分段实现了额外的20 dB性能增益。", "summary": "本文针对动态低空无人机网络中因时变拓扑和干扰控制导致的数据路由与资源分配难题，提出了一种新颖的动态时空图模型和跨层优化框架。该框架将问题转化为联合瓶颈路径规划与资源分配，并通过引入确定性边界处理信道不确定性，利用单调性属性高效地找到全局最优解。对于多商品传输，该方法通过时频分配和二分搜索算法进行扩展。仿真结果显示，相较于传统方法，单商品算法性能提升超过30 dB，多商品方法在密集服务场景中实现100倍改进并额外获得20 dB性能增益。", "keywords": "UAV网络, 资源分配, 路由, 时空图, 跨层优化", "comments": "这篇论文提出了一种创新的动态时空图模型和跨层优化框架，解决了无人机网络中路由和资源分配的复杂问题。利用确定性边界和单调性属性来确保全局最优解是其一大亮点。所展示的显著性能增益，无论是在单商品还是多商品场景下，都突显了其在延迟敏感和密集服务应用中的实际重要性。"}}
{"id": "2507.18940", "title": "LLaVA-NeuMT: Selective Layer-Neuron Modulation for Efficient Multilingual Multimodal Translation", "authors": ["Jingxuan Wei", "Caijun Jia", "Qi Chen", "Yujun Cai", "Linzhuang Sun", "Xiangxiang Zhang", "Gaowei Wu", "Bihui Yu"], "categories": ["cs.CL", "cs.MM"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18940v1", "summary": "Multimodal Machine Translation (MMT) enhances translation quality by\nincorporating visual context, helping to resolve textual ambiguities. While\nexisting MMT methods perform well in bilingual settings, extending them to\nmultilingual translation remains challenging due to cross-lingual interference\nand ineffective parameter-sharing strategies. To address this, we propose\nLLaVA-NeuMT, a novel multimodal multilingual translation framework that\nexplicitly models language-specific and language-agnostic representations to\nmitigate multilingual interference. Our approach consists of a layer selection\nmechanism that identifies the most informative layers for different language\npairs and a neuron-level adaptation strategy that dynamically selects\nlanguage-specific and agnostic neurons to improve translation quality while\nreducing redundancy. We conduct extensive experiments on the M3-Multi30K and\nM3-AmbigCaps datasets, demonstrating that LLaVA-NeuMT, while fine-tuning only\n40\\% of the model parameters, surpasses full fine-tuning approaches and\nultimately achieves SOTA results on both datasets. Our analysis further\nprovides insights into the importance of selected layers and neurons in\nmultimodal multilingual adaptation, offering an efficient and scalable solution\nto cross-lingual adaptation in multimodal translation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18940v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "LLaVA-NeuMT：用于高效多语言多模态翻译的选择性层神经元调制", "tldr": "LLaVA-NeuMT是一种新型的多模态多语言翻译框架，通过选择性层和神经元调制，仅用40%的参数就超越了全量微调方法，并在两个数据集上取得了SOTA结果，有效解决了多语言干扰问题。", "motivation": "现有多模态机器翻译（MMT）方法在双语设置中表现良好，但扩展到多语言翻译时，由于跨语言干扰和无效的参数共享策略，仍然面临挑战。", "method": "提出LLaVA-NeuMT框架，通过显式建模语言特异性和语言无关表示来减轻多语言干扰。该方法包含一个层选择机制，用于识别不同语言对最具信息量的层，以及一个神经元级别的适应策略，动态选择语言特异性和无关的神经元，以提高翻译质量并减少冗余。", "result": "LLaVA-NeuMT仅微调40%的模型参数，就超越了全量微调方法，并在M3-Multi30K和M3-AmbigCaps两个数据集上取得了最先进（SOTA）的结果。", "conclusion": "LLaVA-NeuMT为多模态翻译中的跨语言适应提供了一种高效且可扩展的解决方案，并通过对所选层和神经元重要性的分析，提供了深入的见解。", "translation": "多模态机器翻译（MMT）通过结合视觉上下文来增强翻译质量，有助于解决文本歧义。尽管现有的MMT方法在双语设置中表现良好，但由于跨语言干扰和无效的参数共享策略，将其扩展到多语言翻译仍然具有挑战性。为了解决这个问题，我们提出了LLaVA-NeuMT，一个新颖的多模态多语言翻译框架，它明确地建模语言特异性和语言无关的表示，以减轻多语言干扰。我们的方法包括一个层选择机制，用于识别不同语言对最具信息量的层，以及一个神经元级别的适应策略，动态选择语言特异性和无关的神经元，以提高翻译质量同时减少冗余。我们在M3-Multi30K和M3-AmbigCaps数据集上进行了广泛的实验，结果表明，LLaVA-NeuMT在仅微调40%的模型参数的情况下，超越了全量微调方法，并最终在两个数据集上都取得了SOTA结果。我们的分析进一步提供了关于在多模态多语言适应中选择层和神经元重要性的见解，为多模态翻译中的跨语言适应提供了一种高效且可扩展的解决方案。", "summary": "LLaVA-NeuMT是一种新颖的多模态多语言翻译框架，旨在解决现有MMT方法在多语言设置中面临的跨语言干扰和参数共享效率低下的问题。它通过引入层选择机制和神经元级适应策略，显式区分语言特异性和语言无关表示，从而有效提高翻译质量并减少冗余。实验证明，该方法仅需微调40%的参数，即可超越全量微调模型，并在M3-Multi30K和M3-AmbigCaps数据集上实现最先进的性能，为多模态翻译的跨语言适应提供高效且可扩展的方案。", "keywords": "多模态机器翻译, 多语言翻译, 层选择, 神经元调制, 参数效率", "comments": "该论文的创新点在于提出了选择性层和神经元调制机制，以高效地处理多语言多模态翻译中的跨语言干扰。其通过仅微调少量参数（40%）即可超越全量微调方法，显示出极高的参数效率和性能优势，为未来的多语言多模态模型设计提供了重要启示。该方法的可扩展性也使其在实际应用中具有巨大潜力。"}}
{"id": "2507.19010", "title": "On Automating Proofs of Multiplier Adder Trees using the RTL Books", "authors": ["Mayank Manjrekar"], "categories": ["cs.LO", "cs.SC"], "primary_category": "Subjects:       Logic in Computer Science (cs.LO)", "pdf_link": null, "comments": "Comments:      In Proceedings ACL2 2025, arXiv:2507.18567", "url": "http://arxiv.org/abs/2507.19010v1", "summary": "We present an experimental, verified clause processor ctv-cp that fits into\nthe framework used at Arm for formal verification of arithmetic hardware\ndesigns. This largely automates the ACL2 proof development effort for integer\nmultiplier modules that exist in designs ranging from floating-point division\nto matrix multiplication.", "comment": "In Proceedings ACL2 2025, arXiv:2507.18567", "pdf_url": "http://arxiv.org/pdf/2507.19010v1", "cate": "cs.LO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "关于使用RTL Books自动化乘法器加法树证明的研究", "tldr": "本文介绍了一个名为ctv-cp的实验性验证子句处理器，该处理器能够自动化Arm框架下整数乘法器模块的ACL2证明开发，从而简化算术硬件设计的形式验证。", "motivation": "自动化Arm公司算术硬件设计中整数乘法器模块的ACL2证明开发工作，以提高验证效率。", "method": "提出了一个实验性的、经过验证的子句处理器ctv-cp，该处理器与Arm公司用于算术硬件设计形式验证的现有框架兼容。", "result": "显著自动化了整数乘法器模块的ACL2证明开发工作，这些模块广泛应用于从浮点除法到矩阵乘法等各种设计中。", "conclusion": "通过引入ctv-cp子句处理器，可以大幅自动化Arm公司算术硬件设计中整数乘法器模块的ACL2证明过程，从而提高验证效率。", "translation": "我们提出了一个实验性的、经过验证的子句处理器ctv-cp，它符合Arm公司用于算术硬件设计形式验证的框架。这在很大程度上自动化了整数乘法器模块的ACL2证明开发工作，这些模块存在于从浮点除法到矩阵乘法的各种设计中。", "summary": "本文介绍了一个实验性且经过验证的子句处理器ctv-cp，该处理器旨在集成到Arm公司的形式验证框架中，用于算术硬件设计。其主要贡献在于大幅自动化了整数乘法器模块的ACL2证明开发工作，这些模块广泛应用于浮点除法和矩阵乘法等复杂设计。", "keywords": "形式验证, ACL2, 乘法器, 硬件设计, 自动化证明", "comments": "这项工作通过开发一个专门的子句处理器，有效地解决了复杂算术硬件设计中ACL2证明的自动化难题。其创新性在于将高效的证明自动化工具集成到现有的工业级验证流程中，对于提高硬件验证的效率和可靠性具有重要的实践价值。"}}
{"id": "2507.18638", "title": "Prompt Engineering and the Effectiveness of Large Language Models in Enhancing Human Productivity", "authors": ["Rizal Khoirul Anam"], "categories": ["cs.HC", "cs.AI", "68T50 68T50 68T50", "I.2.7"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      38 pages, 15 tables, 5 figures. Submitted as a research paper draft for arXiv. Based on survey data collected in 2025", "url": "http://arxiv.org/abs/2507.18638v1", "summary": "The widespread adoption of large language models (LLMs) such as ChatGPT,\nGemini, and DeepSeek has significantly changed how people approach tasks in\neducation, professional work, and creative domains. This paper investigates how\nthe structure and clarity of user prompts impact the effectiveness and\nproductivity of LLM outputs. Using data from 243 survey respondents across\nvarious academic and occupational backgrounds, we analyze AI usage habits,\nprompting strategies, and user satisfaction. The results show that users who\nemploy clear, structured, and context-aware prompts report higher task\nefficiency and better outcomes. These findings emphasize the essential role of\nprompt engineering in maximizing the value of generative AI and provide\npractical implications for its everyday use.", "comment": "38 pages, 15 tables, 5 figures. Submitted as a research paper draft\n  for arXiv. Based on survey data collected in 2025", "pdf_url": "http://arxiv.org/pdf/2507.18638v1", "cate": "cs.HC", "date": "2025-05-10", "updated": "2025-05-10", "AI": {"title_translation": "提示工程与大型语言模型在提高人类生产力方面的有效性", "tldr": "本研究调查了提示工程如何影响大型语言模型（LLM）的有效性及用户生产力，发现清晰、结构化和上下文感知的提示能显著提高任务效率和成果。", "motivation": "大型语言模型（LLMs）的广泛应用改变了人们完成任务的方式，本研究旨在探讨用户提示的结构和清晰度如何影响LLM输出的有效性和生产力。", "method": "通过对243名来自不同学术和职业背景的受访者进行调查，分析了人工智能使用习惯、提示策略和用户满意度。", "result": "结果表明，使用清晰、结构化和上下文感知提示的用户报告了更高的任务效率和更好的结果。", "conclusion": "研究强调了提示工程在最大化生成式AI价值中的重要作用，并为日常使用提供了实践启示。", "translation": "大型语言模型（LLMs）如ChatGPT、Gemini和DeepSeek的广泛采用，显著改变了人们在教育、专业工作和创意领域处理任务的方式。本文研究了用户提示的结构和清晰度如何影响LLM输出的有效性和生产力。我们利用来自243名具有不同学术和职业背景的调查受访者的数据，分析了AI使用习惯、提示策略和用户满意度。结果显示，使用清晰、结构化和上下文感知提示的用户报告了更高的任务效率和更好的结果。这些发现强调了提示工程在最大化生成式AI价值中的重要作用，并为日常使用提供了实践启示。", "summary": "本研究探讨了提示工程对大型语言模型（LLMs）有效性和人类生产力的影响。通过对243名用户的调查分析，结果显示清晰、结构化和上下文感知的提示能够显著提高任务效率和成果。这强调了提示工程在充分发挥生成式AI潜力方面的关键作用，并为其实际应用提供了指导。", "keywords": "提示工程, 大型语言模型, 生产力, 用户调查, AI有效性", "comments": "该论文创新性地量化了提示工程对LLM效能的影响，通过实证调查数据支持了清晰提示的重要性。其重要性在于为LLM用户和开发者提供了实用的指导，以优化人机交互并提高生产力。研究的局限性可能在于其依赖于自我报告的调查数据，而非直接的任务性能测量。"}}
{"id": "2507.18792", "title": "Decompiling Rust: An Empirical Study of Compiler Optimizations and Reverse Engineering Challenges", "authors": ["Zixu Zhou"], "categories": ["cs.PL", "cs.SE"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18792v1", "summary": "Decompiling Rust binaries is challenging due to the language's rich type\nsystem, aggressive compiler optimizations, and widespread use of high-level\nabstractions. In this work, we conduct a benchmark-driven evaluation of\ndecompilation quality across core Rust features and compiler build modes. Our\nautomated scoring framework shows that generic types, trait methods, and error\nhandling constructs significantly reduce decompilation quality, especially in\nrelease builds. Through representative case studies, we analyze how specific\nlanguage constructs affect control flow, variable naming, and type information\nrecovery. Our findings provide actionable insights for tool developers and\nhighlight the need for Rust-aware decompilation strategies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18792v1", "cate": "cs.PL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "反编译Rust：编译器优化和逆向工程挑战的实证研究", "tldr": "Rust二进制文件反编译困难，尤其是在泛型、trait方法和错误处理方面，发布版本更甚。研究通过基准测试评估反编译质量，并提供了改进建议。", "motivation": "由于Rust语言丰富的类型系统、激进的编译器优化以及高级抽象的广泛使用，反编译Rust二进制文件极具挑战性。", "method": "本研究通过基准测试驱动的方式，评估了核心Rust特性和编译器构建模式下的反编译质量。研究使用了自动化评分框架，并通过代表性案例研究分析了特定语言构造如何影响控制流、变量命名和类型信息恢复。", "result": "研究发现，泛型类型、trait方法和错误处理构造显著降低了反编译质量，尤其是在发布版本中。", "conclusion": "本研究的发现为工具开发者提供了可操作的见解，并强调了开发对Rust感知（Rust-aware）的反编译策略的必要性。", "translation": "Rust二进制文件的反编译由于其丰富的类型系统、激进的编译器优化以及高级抽象的广泛使用而充满挑战。在这项工作中，我们对核心Rust特性和编译器构建模式下的反编译质量进行了基准测试驱动的评估。我们的自动化评分框架显示，泛型类型、trait方法和错误处理构造显著降低了反编译质量，尤其是在发布版本中。通过代表性案例研究，我们分析了特定的语言构造如何影响控制流、变量命名和类型信息恢复。我们的发现为工具开发者提供了可操作的见解，并强调了对Rust感知（Rust-aware）的反编译策略的需求。", "summary": "本文对Rust二进制文件的反编译挑战进行了实证研究。研究发现，Rust的复杂特性如泛型、trait方法和错误处理，以及编译器优化，显著降低了反编译质量，尤其是在发布版本中。通过基准测试和案例分析，文章揭示了这些特性对反编译结果的具体影响，并提出了开发专门针对Rust的反编译策略的必要性。", "keywords": "Rust反编译, 编译器优化, 逆向工程, 泛型, trait方法", "comments": "这项研究通过系统性的基准测试，量化了Rust语言特性和编译器优化对反编译质量的影响，填补了Rust逆向工程领域的一个空白。其发现对于开发更有效的Rust反编译工具具有重要的指导意义，特别是强调了“Rust-aware”策略的重要性，这可能涉及到对Rust特有语义的深度理解和处理。"}}
{"id": "2507.18730", "title": "Exploiting Movable Antennas in NOMA Networks: Joint Beamforming, Power Allocation and Antenna Position Optimization", "authors": ["Yufeng Zhou", "Wen Chen", "Qingqing Wu", "Xusheng Zhu", "Zhendong Li", "Kunlun Wang", "Qiong Wu"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18730v1", "summary": "This paper investigates the movable antenna (MA)- assisted downlink\nnon-orthogonal multiple access (NOMA) network to maximize system throughput. In\nthe considered scenario, both the base station (BS) and users are equipped with\nMA, and a predetermined successive interference cancellation (SIC) decoding\norder is adopted. Based on the field-response channel model, we formulate a\ncomplex, non-convex problem to jointly optimize the BS beamforming, power\nallocation, and MA positions at both the transmitter and receivers. To address\nthis, we propose an efficient algorithm based on an alternating optimization\n(AO) framework, which decomposes the original problem into three distinct\nsubproblems. By employing sequential parametric convex approximation (SPCA) and\nsuccessive convex approximation (SCA) techniques, the non-convex constraints\nwithin each subproblem are transformed into tractable. This methodology ensures\nthe algorithm converges to a stable, locally optimal solution. Numerical\nresults validate that the proposed system, which fully exploits the degrees of\nfreedom from antenna mobility at both ends, significantly outperforms\nbenchmarks in terms of throughput.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18730v1", "cate": "eess.SP", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "NOMA网络中可移动天线的利用：联合波束成形、功率分配和天线位置优化", "tldr": "该论文研究了可移动天线（MA）辅助的下行非正交多址（NOMA）网络，旨在通过联合优化基站波束成形、功率分配和收发两端MA位置来最大化系统吞吐量。通过交替优化框架和凸近似技术，解决了复杂的非凸问题，数值结果表明所提出的系统在吞吐量方面显著优于基准。", "motivation": "本研究旨在最大化可移动天线（MA）辅助的下行非正交多址（NOMA）网络的系统吞吐量。", "method": "提出了一种基于交替优化（AO）框架的高效算法，将原始问题分解为三个子问题。通过采用序贯参数凸逼近（SPCA）和逐次凸逼近（SCA）技术，将每个子问题中的非凸约束转化为可处理的形式，确保算法收敛到稳定的局部最优解。", "result": "数值结果验证了所提出的系统通过充分利用两端天线移动的自由度，在吞吐量方面显著优于基准。", "conclusion": "通过联合优化基站波束成形、功率分配以及收发两端可移动天线的位置，所提出的系统能够显著提升NOMA网络的吞吐量，充分利用了天线移动带来的自由度。", "translation": "本论文研究了可移动天线（MA）辅助的下行非正交多址（NOMA）网络，以最大化系统吞吐量。在所考虑的场景中，基站（BS）和用户都配备了MA，并采用了预定的逐次干扰消除（SIC）解码顺序。基于场响应信道模型，我们提出了一个复杂的非凸问题，以联合优化基站波束成形、功率分配以及发射器和接收器两端的MA位置。为了解决这个问题，我们提出了一种基于交替优化（AO）框架的高效算法，该算法将原始问题分解为三个不同的子问题。通过采用序贯参数凸逼近（SPCA）和逐次凸逼近（SCA）技术，每个子问题中的非凸约束被转化为可处理的形式。这种方法确保了算法收敛到一个稳定的局部最优解。数值结果验证了所提出的系统，通过充分利用两端天线移动的自由度，在吞吐量方面显著优于基准。", "summary": "该论文旨在最大化可移动天线（MA）辅助的下行非正交多址（NOMA）网络的系统吞吐量。研究人员提出了一个复杂的非凸联合优化问题，涉及基站波束成形、功率分配以及收发两端的MA位置。为解决此问题，论文提出了一种基于交替优化（AO）框架的高效算法，该算法将原问题分解为多个子问题，并利用序贯参数凸逼近（SPCA）和逐次凸逼近（SCA）技术处理非凸约束。数值结果表明，通过充分利用天线移动性，所提出的系统在吞吐量方面显著优于现有基准。", "keywords": "可移动天线, NOMA, 联合优化, 吞吐量", "comments": "该论文的创新点在于将可移动天线技术引入NOMA网络，并首次提出对波束成形、功率分配和天线位置进行联合优化，尤其是在收发两端同时考虑MA位置，这显著增加了系统自由度。通过采用先进的交替优化和凸近似技术，有效解决了由此产生的复杂非凸问题，为未来无线通信系统提供了新的性能提升途径。"}}
{"id": "2406.14117", "title": "An Investigation of Prompt Variations for Zero-shot LLM-based Rankers", "authors": ["Shuoqi Sun", "Shengyao Zhuang", "Shuai Wang", "Guido Zuccon"], "categories": ["cs.IR", "cs.CL"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted for publication at the 47th European Conference on Information Retrieval (ECIR 2025)", "url": "http://arxiv.org/abs/2406.14117v4", "summary": "We provide a systematic understanding of the impact of specific components\nand wordings used in prompts on the effectiveness of rankers based on zero-shot\nLarge Language Models (LLMs). Several zero-shot ranking methods based on LLMs\nhave recently been proposed. Among many aspects, methods differ across (1) the\nranking algorithm they implement, e.g., pointwise vs. listwise, (2) the\nbackbone LLMs used, e.g., GPT3.5 vs. FLAN-T5, (3) the components and wording\nused in prompts, e.g., the use or not of role-definition (role-playing) and the\nactual words used to express this. It is currently unclear whether performance\ndifferences are due to the underlying ranking algorithm, or because of spurious\nfactors such as better choice of words used in prompts. This confusion risks to\nundermine future research. Through our large-scale experimentation and\nanalysis, we find that ranking algorithms do contribute to differences between\nmethods for zero-shot LLM ranking. However, so do the LLM backbones -- but even\nmore importantly, the choice of prompt components and wordings affect the\nranking. In fact, in our experiments, we find that, at times, these latter\nelements have more impact on the ranker's effectiveness than the actual ranking\nalgorithms, and that differences among ranking methods become more blurred when\nprompt variations are considered.", "comment": "Accepted for publication at the 47th European Conference on\n  Information Retrieval (ECIR 2025)", "pdf_url": "http://arxiv.org/pdf/2406.14117v4", "cate": "cs.IR", "date": "2024-06-20", "updated": "2025-07-25", "AI": {"title_translation": "零样本LLM排序器中提示变体研究", "tldr": "本研究系统地揭示了零样本大语言模型（LLM）排序器中提示词组件和措辞对性能的影响，发现提示词变体的影响有时甚至超过实际的排序算法，模糊了不同排序方法之间的区别。", "motivation": "当前零样本LLM排序方法在性能上的差异，不清楚是由于底层的排序算法、所使用的LLM骨干模型，还是提示词的组件和措辞造成的。这种混淆可能阻碍未来的研究，因此需要系统理解提示词变体的影响。", "method": "通过大规模实验和分析，系统地研究了提示词中特定组件和措辞对基于零样本大语言模型（LLM）的排序器有效性的影响。", "result": "研究发现，排序算法确实导致了零样本LLM排序方法之间的差异。LLM骨干模型也有影响。但更重要的是，提示词组件和措辞的选择会影响排序效果。在实验中，这些提示词元素有时对排序器有效性的影响比实际的排序算法更大，并且在考虑提示词变体时，排序方法之间的差异变得更加模糊。", "conclusion": "提示词的组件和措辞对零样本LLM排序器的有效性具有显著影响，有时甚至超过底层排序算法的作用。这表明在零样本LLM排序研究中，提示词工程是一个至关重要的因素。", "translation": "我们系统地理解了提示词中特定组件和措辞对基于零样本大语言模型（LLM）的排序器有效性的影响。最近提出了几种基于LLM的零样本排序方法。在许多方面，这些方法在以下几点上有所不同：（1）它们实现的排序算法，例如，点对点（pointwise）与列表式（listwise），（2）使用的骨干LLM，例如，GPT3.5与FLAN-T5，（3）提示词中使用的组件和措辞，例如，是否使用角色定义（角色扮演）以及表达此意图的实际词语。目前尚不清楚性能差异是由于底层的排序算法，还是由于更好的提示词选择等虚假因素。这种混淆可能阻碍未来的研究。通过我们大规模的实验和分析，我们发现排序算法确实导致了零样本LLM排序方法之间的差异。然而，LLM骨干模型也同样如此——但更重要的是，提示词组件和措辞的选择会影响排序。事实上，在我们的实验中，我们发现，有时这些后者元素对排序器有效性的影响比实际的排序算法更大，并且在考虑提示词变体时，排序方法之间的差异变得更加模糊。", "summary": "本研究系统地探究了零样本大语言模型（LLM）排序器中提示词变体（包括组件和措辞）对其有效性的影响。针对现有方法性能差异来源不明确的问题，通过大规模实验分析，论文发现排序算法和LLM骨干模型均有影响，但提示词组件和措辞对排序效果的影响更为显著，有时甚至超过了排序算法本身，使得不同排序方法间的性能区别变得模糊。", "keywords": "零样本排序, 大语言模型, 提示工程, 排序器, 提示变体", "comments": "这篇论文的创新点在于它系统地揭示了提示词工程在零样本LLM排序中的关键作用，挑战了以往可能过分关注算法或模型本身的观念。其重要性在于为未来LLM应用和研究指明了方向，强调了精心设计的提示词对于实现最佳性能和进行公平比较的必要性。论文的发现对于理解LLM行为及其在实际应用中的部署具有深远意义。"}}
{"id": "2507.13618", "title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters", "authors": ["Shanbo Cheng", "Yu Bao", "Qian Cao", "Luyang Huang", "Liyan Kang", "Zhicheng Liu", "Yu Lu", "Wenhao Zhu", "Jingwen Chen", "Zhichao Huang", "Tao Li", "Yifu Li", "Huiying Lin", "Sitong Liu", "Ningxin Peng", "Shuaijie She", "Lu Xu", "Nuo Xu", "Sen Yang", "Runsheng Yu", "Yiming Yu", "Liehao Zou", "Hang Li", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13618v3", "summary": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13618v3", "cate": "cs.CL", "date": "2025-07-18", "updated": "2025-07-25", "AI": {"title_translation": "Seed-X：构建强大的7B参数多语言翻译大模型", "tldr": "Seed-X是一个7B参数的开源多语言翻译LLM，通过CoT和RL训练，性能媲美领先闭源模型并超越大型开源模型。", "motivation": "多语言翻译对大型语言模型（LLM）来说是一项挑战，因为它们难以处理复杂的语言模式以及自动化翻译中出现的生硬翻译。", "method": "本文介绍了Seed-X，一个由指令和推理模型组成的开源LLM家族，参数规模为7B。基础模型在包含28种语言的多元化、高质量单语和双语数据集上进行预训练。指令模型通过思维链（CoT）推理进行微调，并通过强化学习（RL）进一步增强，以在不同的语言对之间实现更好的泛化。", "result": "Seed-X在28种语言上的性能与领先的闭源模型（包括Gemini-2.5和GPT-4o）相当，并在自动评估指标和人工评估中显著优于更大的开源模型。", "conclusion": "Seed-X展示了其在多语言翻译方面的强大能力，通过公开参数促进了翻译研究和应用。", "translation": "多语言翻译对于大型语言模型（LLM）而言是一项具有挑战性的任务，需要处理复杂的语言模式以及自动化翻译中出现的生硬翻译。在本文中，我们介绍了Seed-X，一个由指令和推理模型组成的开源LLM家族，它以7B的参数规模，突破了翻译能力的极限。基础模型在包含28种语言的多元化、高质量单语和双语数据集上进行预训练，充分利用了多语言数据的潜力。指令模型随后通过思维链（CoT）推理进行微调以进行翻译，并通过强化学习（RL）进一步增强，以在不同的语言对之间实现更好的泛化。Seed-X在28种语言上的性能与领先的闭源模型（包括Gemini-2.5和GPT-4o）相当，并在自动评估指标和人工评估中显著优于更大的开源模型。我们分享了优化过程中的最佳实践，并公开了参数，以促进翻译研究和应用。", "summary": "本文介绍了Seed-X，一个7B参数的开源多语言翻译大型语言模型家族。该模型通过在28种语言的单语和双语数据集上进行预训练，并结合思维链推理和强化学习进行微调，显著提升了多语言翻译能力。Seed-X在性能上与Gemini-2.5和GPT-4o等领先的闭源模型相当，并优于其他大型开源模型，其参数已公开以促进研究。", "keywords": "多语言翻译, 大型语言模型, Seed-X, 思维链, 强化学习", "comments": "Seed-X在7B参数规模下实现了与顶级闭源模型相当的性能，并在多语言翻译领域超越了更大的开源模型，这显示了其在模型效率和翻译质量方面的创新。其开源策略也有助于推动社区在多语言LLM方面的研究和发展。"}}
{"id": "2507.16229", "title": "Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery", "authors": ["Bo Wen", "Chen Wang", "Qiwei Han", "Raquel Norel", "Julia Liu", "Thaddeus Stappenbeck", "Jeffrey L. Rogers"], "categories": ["cs.AI", "cs.CY", "cs.ET", "cs.HC", "cs.SE"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      IEEE International Conference on Digital Health (ICDH) 2025", "url": "http://arxiv.org/abs/2507.16229v1", "summary": "The integration of voice-based AI agents in healthcare presents a\ntransformative opportunity to bridge economic and accessibility gaps in digital\nhealth delivery. This paper explores the role of large language model\n(LLM)-powered voice assistants in enhancing preventive care and continuous\npatient monitoring, particularly in underserved populations. Drawing insights\nfrom the development and pilot study of Agent PULSE (Patient Understanding and\nLiaison Support Engine) -- a collaborative initiative between IBM Research,\nCleveland Clinic Foundation, and Morehouse School of Medicine -- we present an\neconomic model demonstrating how AI agents can provide cost-effective\nhealthcare services where human intervention is economically unfeasible. Our\npilot study with 33 inflammatory bowel disease patients revealed that 70\\%\nexpressed acceptance of AI-driven monitoring, with 37\\% preferring it over\ntraditional modalities. Technical challenges, including real-time\nconversational AI processing, integration with healthcare systems, and privacy\ncompliance, are analyzed alongside policy considerations surrounding\nregulation, bias mitigation, and patient autonomy. Our findings suggest that\nAI-driven voice agents not only enhance healthcare scalability and efficiency\nbut also improve patient engagement and accessibility. For healthcare\nexecutives, our cost-utility analysis demonstrates huge potential savings for\nroutine monitoring tasks, while technologists can leverage our framework to\nprioritize improvements yielding the highest patient impact. By addressing\ncurrent limitations and aligning AI development with ethical and regulatory\nframeworks, voice-based AI agents can serve as a critical entry point for\nequitable, sustainable digital healthcare solutions.", "comment": "IEEE International Conference on Digital Health (ICDH) 2025", "pdf_url": "http://arxiv.org/pdf/2507.16229v1", "cate": "cs.AI", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "基于语音的AI代理：弥补数字健康交付中的经济差距", "tldr": "本文探讨了LLM驱动的语音AI代理如何通过Agent PULSE项目弥补数字健康服务的经济和可及性差距，特别是在服务不足的人群中，并展示了其成本效益和患者接受度。", "motivation": "数字健康服务中存在经济和可及性差距，尤其是在服务不足的人群中，传统的人工干预成本高昂且不可行。本文旨在探索基于语音的AI代理如何弥补这些差距，提升预防性护理和持续患者监测的效率和可及性。", "method": "本文通过开发和试点研究Agent PULSE（患者理解和联络支持引擎）——一个由IBM研究院、克利夫兰诊所基金会和莫尔豪斯医学院合作的倡议来探讨。研究提出了一个经济模型，并对33名炎症性肠病患者进行了试点研究，同时分析了技术挑战和政策考量。", "result": "试点研究显示，70%的患者接受AI驱动的监测，37%的患者偏好AI而非传统方式。经济模型表明AI代理可以在人工干预不可行的情况下提供成本效益高的医疗服务。研究还发现AI驱动的语音代理能增强医疗可扩展性、效率、患者参与度和可及性，并为常规监测任务带来巨大的潜在成本节约。", "conclusion": "基于语音的AI代理可以通过解决现有局限性并与道德和监管框架保持一致，成为公平、可持续的数字医疗解决方案的关键切入点，有效弥补数字健康交付中的经济和可及性差距，并提升患者体验。", "translation": "将语音AI代理整合到医疗保健中，为弥合数字健康交付中的经济和可及性差距提供了变革性机遇。本文探讨了大型语言模型（LLM）驱动的语音助手在增强预防性护理和持续患者监测方面的作用，尤其是在服务不足的人群中。我们借鉴了Agent PULSE（患者理解和联络支持引擎）的开发和试点研究——这是IBM研究院、克利夫兰诊所基金会和莫尔豪斯医学院之间的一项合作倡议——提出了一个经济模型，证明了AI代理如何在人工干预经济上不可行的情况下提供成本效益高的医疗服务。我们对33名炎症性肠病患者进行的试点研究显示，70%的患者表示接受AI驱动的监测，其中37%的患者更倾向于选择AI而非传统模式。文中分析了技术挑战，包括实时会话AI处理、与医疗系统集成和隐私合规性，以及围绕法规、偏见缓解和患者自主权的政策考量。我们的研究结果表明，AI驱动的语音代理不仅能提高医疗保健的可扩展性和效率，还能改善患者参与度和可及性。对于医疗保健高管而言，我们的成本效益分析表明，常规监测任务具有巨大的潜在节约空间，而技术人员可以利用我们的框架优先改进能带来最高患者影响的方面。通过解决当前的局限性并将AI开发与道德和监管框架相结合，基于语音的AI代理可以作为公平、可持续的数字医疗解决方案的关键切入点。", "summary": "本文探讨了基于大型语言模型（LLM）的语音AI代理在弥合数字健康服务经济和可及性差距方面的潜力。通过Agent PULSE项目的开发和试点研究，作者展示了AI代理如何提供成本效益高的医疗服务，特别是在服务不足的地区。研究发现患者对AI驱动的监测接受度高，并强调了其在提升医疗可扩展性、效率、患者参与度和可及性方面的作用，同时指出了需克服的技术和政策挑战。", "keywords": "语音AI代理, 数字健康, 经济差距, 患者监测, LLM", "comments": "本文的创新之处在于其将LLM驱动的语音AI代理应用于解决数字健康领域中的实际经济和可及性问题，并通过Agent PULSE的试点研究提供了实证支持。其提出的经济模型对于医疗机构评估AI投资的潜在回报具有重要指导意义。此外，论文不仅关注技术实现，还深入探讨了隐私、偏见和法规等重要的伦理和政策考量，这对于AI在医疗领域的负责任部署至关重要。局限性可能在于试点研究的样本量相对较小，以及在实际大规模部署中可能面临的复杂集成挑战。"}}
{"id": "2507.19349", "title": "Reconstruction of Sparse Urban Wireless Signals via Group Equivariant Non-Expansive Operators", "authors": ["Lorenzo Mario Amorosa", "Francesco Conti", "Nicola Quercioli", "Flavio Zabini", "Tayebeh Lotfi Mahyari", "Yiqun Ge", "Patrizio Frosini"], "categories": ["cs.LG", "cs.NI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19349v1", "summary": "In emerging communication systems such as sixth generation (6G) wireless\nnetworks, efficient resource management and service delivery rely on accurate\nknowledge of spatially-varying quantities like signal-to-interference-noise\nratio (SINR) maps, which are costly to acquire at high resolution. This work\nexplores the reconstruction of such spatial signals from sparse measurements\nusing Group Equivariant Non-Expansive Operators (GENEOs), offering a\nlow-complexity alternative to traditional neural networks. The concept of\nGENEO, which originated in topological data analysis (TDA), is a mathematical\ntool used in machine learning to represent agents modelled as functional\noperators acting on data while incorporating application-specific invariances.\nLeveraging these invariances reduces the number of parameters with respect to\ntraditional neural networks and mitigates data scarcity by enforcing known\nalgebraic and geometric constraints that reflect symmetries in the agents'\nactions. In this paper, we introduce a novel GENEO-based approach for SINR map\nreconstruction in urban wireless communication networks using extremely sparse\nsampling. We demonstrate that this mathematical framework achieves competitive\nperformance compared to established methods. Our evaluation, conducted using\nboth statistical and TDA metrics, highlights the advantages of our approach in\naccurately reconstructing spatial signals under severe data limitations on the\nnumber of samples.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19349v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "通过群等变非膨胀算子重建稀疏城市无线信号", "tldr": "本文提出了一种基于群等变非膨胀算子（GENEO）的新方法，用于在数据极度稀疏的情况下重建城市无线通信网络中的信号干扰噪声比（SINR）图，并证明其性能与现有方法相当。", "motivation": "在第六代（6G）无线网络等新兴通信系统中，高效的资源管理和服务交付需要准确的、空间变化的信号干扰噪声比（SINR）图知识，而高分辨率获取这些信息成本高昂。", "method": "本文提出了一种新颖的基于群等变非膨胀算子（GENEOs）的方法，用于从稀疏测量中重建空间信号，特别是城市无线通信网络中的SINR图。GENEOs源于拓扑数据分析（TDA），是一种机器学习工具，它通过结合应用特定的不变性来减少参数数量并缓解数据稀缺性，通过强制执行反映代理动作对称性的已知代数和几何约束。", "result": "该数学框架在准确重建严重数据限制下的空间信号方面取得了与现有方法相当的性能。评估结果（使用统计和TDA指标）突出了该方法在样本数量极度受限的情况下重建空间信号的优势。", "conclusion": "该研究成功地利用群等变非膨胀算子（GENEOs）在数据极度稀疏的情况下重建了城市无线信号（如SINR图），提供了一种低复杂度的替代方案，并展现出与传统方法相当的性能。", "translation": "在第六代（6G）无线网络等新兴通信系统中，高效的资源管理和服务交付依赖于对空间变化量（如信号干扰噪声比（SINR）图）的准确了解，而高分辨率获取这些信息成本高昂。本研究探讨了使用群等变非膨胀算子（GENEOs）从稀疏测量中重建此类空间信号的方法，提供了一种比传统神经网络低复杂度的替代方案。GENEO的概念起源于拓扑数据分析（TDA），是机器学习中用于表示建模为作用于数据的功能算子并结合特定应用不变性的代理的数学工具。利用这些不变性可以减少相对于传统神经网络的参数数量，并通过强制执行反映代理动作对称性的已知代数和几何约束来缓解数据稀缺性。在本文中，我们引入了一种新颖的基于GENEO的方法，用于在城市无线通信网络中使用极度稀疏采样进行SINR图重建。我们证明了该数学框架与现有方法相比具有竞争性的性能。我们的评估使用统计和TDA指标进行，突出了我们的方法在样本数量严重受限的情况下准确重建空间信号的优势。", "summary": "本文提出了一种基于群等变非膨胀算子（GENEOs）的新方法，用于从稀疏测量中重建城市无线网络中的信号干扰噪声比（SINR）图。GENEOs是一种源于拓扑数据分析的数学工具，通过利用应用特定的不变性来减少参数并缓解数据稀缺性。研究表明，该方法在极度稀疏采样条件下，在准确重建空间信号方面，达到了与现有方法相当的性能。", "keywords": "群等变非膨胀算子, 稀疏信号重建, SINR图, 6G无线网络, 拓扑数据分析", "comments": "该论文的创新之处在于将源于拓扑数据分析的群等变非膨胀算子（GENEOs）应用于城市无线信号的稀疏重建问题，特别是SINR图。这种方法通过利用数据中的内在对称性来减少模型参数并有效处理数据稀缺性，为6G等未来通信系统中的资源管理提供了低复杂度的解决方案。其重要性在于提供了一种替代传统神经网络的有效途径，尤其是在数据采集成本高昂或数据量受限的场景下。"}}
{"id": "2507.19361", "title": "SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models", "authors": ["Zhen Wan", "Chao-Han Huck Yang", "Yahan Yu", "Jinchuan Tian", "Sheng Li", "Ke Hu", "Zhehuai Chen", "Shinji Watanabe", "Fei Cheng", "Chenhui Chu", "Sadao Kurohashi"], "categories": ["cs.CL", "cs.AI", "cs.SC", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Our Speech-IQ leaderboard will be hosted at this http URL . ACL 2025 main", "url": "http://arxiv.org/abs/2507.19361v1", "summary": "We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human\ncognition-inspired evaluation pipeline for voice understanding large language\nmodels, LLM Voice, designed to assess their voice understanding ability. Moving\nbeyond popular voice understanding metrics such as word error rate (WER), SIQ\nexamines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy:\n(1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e.,\nsimilarity of LLM's interpretations); and (3) Application (i.e., QA accuracy\nfor simulating downstream tasks). We demonstrate that SIQ not only quantifies\nvoice understanding abilities but also provides unified comparisons between\ncascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation\nerrors in existing benchmarks, and detects hallucinations in LLM Voice. Our\nframework represents a first-of-its-kind intelligence examination that bridges\ncognitive principles with voice-oriented benchmarks, while exposing overlooked\nchallenges in multi-modal training.", "comment": "Our Speech-IQ leaderboard will be hosted at\n  huggingface.co/spaces/nvidia/Speech-IQ-leaderboard. ACL 2025 main", "pdf_url": "http://arxiv.org/pdf/2507.19361v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "SpeechIQ：语音理解大型语言模型在认知层面上的语音智商", "tldr": "本文提出了SpeechIQ（语音智商），一个受人类认知启发的语音理解大型语言模型（LLM Voice）评估管道，旨在评估其语音理解能力，并超越传统的词错误率（WER）指标，通过布鲁姆分类法定义的三个认知级别进行评估。", "motivation": "现有的语音理解评估指标（如词错误率WER）无法全面评估大型语言模型的语音理解能力。本文旨在引入一种新的、受人类认知启发的评估方法，以更深入地评估LLM Voice的语音理解能力，并识别多模态训练中被忽视的挑战。", "method": "本文引入了语音智商（SIQ）作为一种新的、受人类认知启发的语音理解大型语言模型（LLM Voice）评估管道。SIQ通过布鲁姆分类法（Bloom's Taxonomy）的三个认知级别来评估LLM Voice：1) 记忆（即逐字准确性的WER）；2) 理解（即LLM解释的相似性）；3) 应用（即模拟下游任务的问答准确性）。", "result": "SIQ不仅能量化语音理解能力，还能在级联方法（如ASR+LLM）和端到端模型之间提供统一的比较，识别现有基准中的标注错误，并检测LLM Voice中的幻觉。", "conclusion": "SpeechIQ框架代表了首个将认知原理与面向语音的基准相结合的智能评估方法，同时揭示了多模态训练中被忽视的挑战。", "translation": "我们引入了基于语音的智商（SIQ）作为一种新形式的、受人类认知启发的语音理解大型语言模型（LLM Voice）评估管道，旨在评估它们的语音理解能力。SIQ超越了流行的语音理解指标，如词错误率（WER），通过布鲁姆分类法（Bloom's Taxonomy）启发的三个认知级别来考察LLM Voice：(1) 记忆（即逐字准确性的WER）；(2) 理解（即LLM解释的相似性相似性）；和(3) 应用（即模拟下游任务的问答准确性）。我们证明，SIQ不仅量化了语音理解能力，还提供了级联方法（例如，ASR+LLM）和端到端模型之间的统一比较，识别了现有基准中的标注错误，并检测了LLM Voice中的幻觉。我们的框架代表了首个将认知原理与面向语音的基准相结合的智能检查，同时揭示了多模态训练中被忽视的挑战。", "summary": "本文提出了SpeechIQ（语音智商），一个受人类认知启发的语音理解大型语言模型（LLM Voice）评估框架。该框架超越了传统的词错误率（WER），通过布鲁姆分类法定义的记忆、理解和应用三个认知级别来全面评估LLM Voice的语音理解能力。实验证明，SIQ不仅能准确量化语音理解能力，还能统一比较不同模型架构，识别标注错误并检测幻觉，为多模态训练中被忽视的挑战提供了新的视角。", "keywords": "语音理解, 大型语言模型, 认知评估, SpeechIQ, 布鲁姆分类法", "comments": "本文的创新之处在于引入了受人类认知启发的SpeechIQ评估框架，将布鲁姆分类法应用于语音理解大型语言模型的评估，超越了传统的单一指标。它不仅提供了一个更全面的评估维度，还揭示了现有基准的潜在问题和多模态训练中的挑战，对于推动LLM Voice的发展具有重要意义。"}}
{"id": "2412.12561", "title": "Tell Me What to Track: Infusing Robust Language Guidance for Enhanced Referring Multi-Object Tracking", "authors": ["Wenjun Huang", "Yang Ni", "Hanning Chen", "Yirui He", "Ian Bryant", "Yezi Liu", "Mohsen Imani"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.12561v3", "summary": "Referring multi-object tracking (RMOT) is an emerging cross-modal task that\naims to localize an arbitrary number of targets based on a language expression\nand continuously track them in a video. This intricate task involves reasoning\non multi-modal data and precise target localization with temporal association.\nHowever, prior studies overlook the imbalanced data distribution between\nnewborn targets and existing targets due to the nature of the task. In\naddition, they only indirectly fuse multi-modal features, struggling to deliver\nclear guidance on newborn target detection. To solve the above issues, we\nconduct a collaborative matching strategy to alleviate the impact of the\nimbalance, boosting the ability to detect newborn targets while maintaining\ntracking performance. In the encoder, we integrate and enhance the cross-modal\nand multi-scale fusion, overcoming the bottlenecks in previous work, where\nlimited multi-modal information is shared and interacted between feature maps.\nIn the decoder, we also develop a referring-infused adaptation that provides\nexplicit referring guidance through the query tokens. The experiments showcase\nthe superior performance of our model (+3.42%) compared to prior works,\ndemonstrating the effectiveness of our designs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.12561v3", "cate": "cs.CV", "date": "2024-12-17", "updated": "2025-07-25", "AI": {"title_translation": "告诉我追踪什么：为增强参考多目标跟踪注入鲁棒语言指导", "tldr": "本文提出了一种新的指代多目标跟踪（RMOT）方法，通过协作匹配策略和明确的指代指导，解决了数据不平衡和间接多模态特征融合问题，实现了更优的性能。", "motivation": "先前的指代多目标跟踪（RMOT）研究忽略了新生目标和现有目标之间不平衡的数据分布，并且仅间接融合多模态特征，难以对新生目标检测提供明确指导。", "method": "本文提出了一种协作匹配策略来缓解数据不平衡的影响，提升新生目标检测能力。在编码器中，整合并增强了跨模态和多尺度融合。在解码器中，开发了一种指代注入式适应，通过查询令牌提供明确的指代指导。", "result": "与现有工作相比，所提出的模型性能优越，提升了3.42%。", "conclusion": "本文提出的协作匹配策略、增强的跨模态和多尺度融合以及明确的指代指导设计，有效提升了指代多目标跟踪的性能。", "translation": "指代多目标跟踪（RMOT）是一项新兴的跨模态任务，旨在根据语言表达定位任意数量的目标并在视频中持续跟踪它们。这项复杂的任务涉及多模态数据推理以及具有时间关联的精确目标定位。然而，由于任务的性质，先前的研究忽略了新生目标和现有目标之间不平衡的数据分布。此外，它们仅间接融合多模态特征，难以对新生目标检测提供明确的指导。为了解决上述问题，我们采用协作匹配策略来缓解不平衡的影响，提升新生目标检测能力，同时保持跟踪性能。在编码器中，我们整合并增强了跨模态和多尺度融合，克服了先前工作中有限的多模态信息在特征图之间共享和交互的瓶颈。在解码器中，我们还开发了一种指代注入式适应，通过查询令牌提供明确的指代指导。实验结果表明，与先前工作相比，我们的模型性能优越（+3.42%），证明了我们设计的有效性。", "summary": "本文针对指代多目标跟踪（RMOT）中新生目标与现有目标之间的数据不平衡以及间接多模态特征融合的挑战，提出了一系列改进。通过采用协作匹配策略，模型能够更好地检测新生目标；在编码器中，增强了跨模态和多尺度融合；在解码器中，引入了指代注入式适应以提供明确的语言指导。实验结果表明，该模型相较于现有工作具有显著的性能提升，验证了其设计的有效性。", "keywords": "指代多目标跟踪, 跨模态融合, 语言指导, 数据不平衡, 目标检测", "comments": "该论文为指代多目标跟踪（RMOT）中的关键问题提供了新颖的解决方案，特别是解决了数据不平衡和改进多模态特征交互。通过查询令牌提供明确的语言指导是一种创新的方法，有望增强目标检测能力。性能上的提升表明了该跨模态任务的显著进展。"}}
{"id": "2507.16039", "title": "Reactivation: Empirical NTK Dynamics Under Task Shifts", "authors": ["Yuzhi Liu", "Zixuan Chen", "Zirui Zhang", "Yufei Liu", "Giulia Lanzillotta"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by the 3rd Workshop on High-dimensional Learning Dynamics (HiLD), ICML 2025", "url": "http://arxiv.org/abs/2507.16039v2", "summary": "The Neural Tangent Kernel (NTK) offers a powerful tool to study the\nfunctional dynamics of neural networks. In the so-called lazy, or kernel\nregime, the NTK remains static during training and the network function is\nlinear in the static neural tangents feature space. The evolution of the NTK\nduring training is necessary for feature learning, a key driver of deep\nlearning success. The study of the NTK dynamics has led to several critical\ndiscoveries in recent years, in generalization and scaling behaviours. However,\nthis body of work has been limited to the single task setting, where the data\ndistribution is assumed constant over time. In this work, we present a\ncomprehensive empirical analysis of NTK dynamics in continual learning, where\nthe data distribution shifts over time. Our findings highlight continual\nlearning as a rich and underutilized testbed for probing the dynamics of neural\ntraining. At the same time, they challenge the validity of static-kernel\napproximations in theoretical treatments of continual learning, even at large\nscale.", "comment": "Accepted by the 3rd Workshop on High-dimensional Learning Dynamics\n  (HiLD), ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.16039v2", "cate": "cs.LG", "date": "2025-07-21", "updated": "2025-07-25", "AI": {"title_translation": "再激活：任务转移下的经验性NTK动态", "tldr": "本文对持续学习中神经网络切线核（NTK）的动态进行了全面的实证分析，挑战了静态核近似在持续学习理论中的有效性。", "motivation": "现有关于神经网络切线核（NTK）动态的研究局限于单任务设置，即数据分布假定为恒定。然而，深度学习的成功离不开特征学习，而特征学习需要NTK在训练过程中的演变。因此，有必要在数据分布随时间变化的持续学习场景中，研究NTK的动态。", "method": "本文对持续学习中的神经网络切线核（NTK）动态进行了全面的实证分析，其中数据分布随时间发生变化。", "result": "研究结果表明，持续学习是探测神经网络训练动态的一个丰富且未被充分利用的试验平台。同时，这些发现挑战了即使在大规模情况下，静态核近似在持续学习理论处理中的有效性。", "conclusion": "在持续学习背景下，NTK的动态行为至关重要，且静态核近似在理论上可能不再有效。持续学习为研究神经网络训练动态提供了新的视角。", "translation": "神经网络切线核（NTK）提供了一个强大的工具来研究神经网络的功能动态。在所谓的“惰性”或“核”机制中，NTK在训练过程中保持静态，并且网络函数在静态神经切线特征空间中是线性的。训练过程中NTK的演变对于特征学习是必要的，这是深度学习成功的关键驱动力。近年来，对NTK动态的研究在泛化和缩放行为方面取得了若干关键发现。然而，这项工作一直局限于单任务设置，其中数据分布假定随时间恒定。在这项工作中，我们对持续学习中的NTK动态进行了全面的实证分析，其中数据分布随时间发生变化。我们的发现强调持续学习是探测神经网络训练动态的一个丰富且未被充分利用的试验平台。同时，它们挑战了即使在大规模情况下，静态核近似在持续学习理论处理中的有效性。", "summary": "本文对持续学习中神经网络切线核（NTK）的动态进行了全面的实证分析。现有NTK动态研究主要集中在单任务设置，但本文关注数据分布随时间变化的持续学习场景。研究结果表明，持续学习是探索神经网络训练动态的理想试验平台，并对静态核近似在持续学习理论中的有效性提出了质疑。", "keywords": "神经网络切线核, 持续学习, 任务转移, 动态, 经验分析", "comments": "本文通过在持续学习背景下对NTK动态进行实证分析，填补了现有NTK研究的空白。其创新之处在于将NTK动态研究扩展到更复杂的任务转移场景，并指出持续学习是一个未被充分利用的研究领域。这项工作的重要性在于挑战了在持续学习中静态核近似的普遍性，为未来持续学习的理论研究提供了新的方向。"}}
{"id": "2507.16181", "title": "Pulse-Level Simulation of Crosstalk Attacks on Superconducting Quantum Hardware", "authors": ["Syed Emad Uddin Shubha", "Tasnuva Farheen"], "categories": ["quant-ph", "cs.CR"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      This paper has been accepted to the Security, Privacy, and Resilience Workshop at IEEE Quantum Week (QCE 2025) and will appear in the workshop proceedings", "url": "http://arxiv.org/abs/2507.16181v2", "summary": "Hardware crosstalk in multi-tenant superconducting quantum computers poses a\nsevere security threat, allowing adversaries to induce targeted errors across\ntenant boundaries by injecting carefully engineered pulses. We present a\nsimulation-based study of active crosstalk attacks at the pulse level,\nanalyzing how adversarial control of pulse timing, shape, amplitude, and\ncoupling can disrupt a victim's computation. Our framework models the\ntime-dependent dynamics of a three-qubit system in the rotating frame,\ncapturing both always-on couplings and injected drive pulses. We examine two\nattack strategies: attacker-first (pulse before victim operation) and\nvictim-first (pulse after), and systematically identify the pulse and coupling\nconfigurations that cause the largest logical errors. Protocol-level\nexperiments on quantum coin flip and XOR classification circuits show that some\nprotocols are highly vulnerable to these attacks, while others remain robust.\nBased on these findings, we discuss practical methods for detection and\nmitigation to improve security in quantum cloud platforms.", "comment": "This paper has been accepted to the Security, Privacy, and Resilience\n  Workshop at IEEE Quantum Week (QCE 2025) and will appear in the workshop\n  proceedings", "pdf_url": "http://arxiv.org/pdf/2507.16181v2", "cate": "quant-ph", "date": "2025-07-22", "updated": "2025-07-25", "AI": {"title_translation": "超导量子硬件上串扰攻击的脉冲级模拟", "tldr": "研究了超导量子计算机中硬件串扰引起的脉冲级攻击，发现某些量子协议易受攻击，并讨论了检测和缓解方法。", "motivation": "多租户超导量子计算机中的硬件串扰构成严重安全威胁，允许攻击者通过注入精心设计的脉冲在租户边界间诱导目标错误。", "method": "提出了一种基于仿真的脉冲级主动串扰攻击研究，分析了脉冲时序、形状、幅度及耦合的对抗性控制如何扰乱受害者计算。框架模拟了旋转框架中三量子比特系统的时间依赖性动力学，捕捉了常开耦合和注入驱动脉冲。检查了两种攻击策略：攻击者优先（在受害者操作前注入脉冲）和受害者优先（在受害者操作后注入脉冲），并系统地识别了导致最大逻辑错误的脉冲和耦合配置。", "result": "在量子抛硬币和XOR分类电路上的协议级实验表明，一些协议对这些攻击高度脆弱，而另一些则保持鲁棒。", "conclusion": "基于这些发现，论文讨论了实际的检测和缓解方法，以提高量子云平台的安全性。", "translation": "多租户超导量子计算机中的硬件串扰构成严重的安全威胁，允许攻击者通过注入精心设计的脉冲，在租户边界间诱导目标错误。我们提出了一项基于仿真的脉冲级主动串扰攻击研究，分析了对抗者如何通过控制脉冲时序、形状、幅度和耦合来扰乱受害者的计算。我们的框架在旋转框架中建模了一个三量子比特系统的时间依赖性动力学，捕获了常开耦合和注入的驱动脉冲。我们检查了两种攻击策略：攻击者优先（在受害者操作前注入脉冲）和受害者优先（在受害者操作后注入脉冲），并系统地识别了导致最大逻辑错误的脉冲和耦合配置。在量子抛硬币和XOR分类电路上的协议级实验表明，一些协议对这些攻击高度脆弱，而另一些则保持鲁棒。基于这些发现，我们讨论了实际的检测和缓解方法，以提高量子云平台的安全性。", "summary": "本文通过脉冲级模拟，深入研究了多租户超导量子计算机中硬件串扰引发的安全威胁。研究构建了一个三量子比特系统模型，分析了脉冲时序、形状、幅度及耦合对受害者计算的影响，并考察了两种攻击策略。实验结果表明，部分量子协议对串扰攻击高度敏感，而而另一些则具有抵抗性。基于这些发现，论文提出了针对量子云平台安全的检测和缓解措施。", "keywords": "量子安全, 硬件串扰, 脉冲级模拟, 超导量子计算, 量子攻击", "comments": "这篇论文创新性地将安全威胁分析深入到量子硬件的脉冲级别，揭示了超导量子计算机中串扰攻击的潜在危害。其模拟方法和对攻击策略的系统性分析为理解量子安全漏洞提供了新视角。研究结果强调了在设计量子协议和平台时考虑硬件层安全的重要性，并为未来的防御机制提供了实践指导。"}}
{"id": "2507.19004", "title": "MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment", "authors": ["Siyi Xun", "Yue Sun", "Jingkun Chen", "Zitong Yu", "Tong Tong", "Xiaohong Liu", "Mingxiang Wu", "Tao Tan"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      We note that the version after peer review of this paper has been provisionally accepted by The 28th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2025)", "url": "http://arxiv.org/abs/2507.19004v1", "summary": "Rapid advances in medical imaging technology underscore the critical need for\nprecise and automated image quality assessment (IQA) to ensure diagnostic\naccuracy. Existing medical IQA methods, however, struggle to generalize across\ndiverse modalities and clinical scenarios. In response, we introduce MedIQA,\nthe first comprehensive foundation model for medical IQA, designed to handle\nvariability in image dimensions, modalities, anatomical regions, and types. We\ndeveloped a large-scale multi-modality dataset with plentiful manually\nannotated quality scores to support this. Our model integrates a salient slice\nassessment module to focus on diagnostically relevant regions feature retrieval\nand employs an automatic prompt strategy that aligns upstream physical\nparameter pre-training with downstream expert annotation fine-tuning. Extensive\nexperiments demonstrate that MedIQA significantly outperforms baselines in\nmultiple downstream tasks, establishing a scalable framework for medical IQA\nand advancing diagnostic workflows and clinical decision-making.", "comment": "We note that the version after peer review of this paper has been\n  provisionally accepted by The 28th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI 2025)", "pdf_url": "http://arxiv.org/pdf/2507.19004v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "MedIQA：一个用于提示驱动医学图像质量评估的可扩展基础模型", "tldr": "MedIQA是首个用于医学图像质量评估的综合基础模型，通过大规模多模态数据集和新颖的模块设计，显著优于现有基线。", "motivation": "医学成像技术的快速发展凸显了精确自动化图像质量评估（IQA）以确保诊断准确性的关键需求。然而，现有医学IQA方法难以在不同模态和临床场景中泛化。", "method": "本文提出了MedIQA，首个用于医学IQA的综合基础模型，旨在处理图像维度、模态、解剖区域和类型上的变异性。研究团队开发了一个带有大量手动标注质量得分的大规模多模态数据集。MedIQA模型集成了显著切片评估模块以聚焦于诊断相关区域的特征检索，并采用自动提示策略，将上游物理参数预训练与下游专家标注微调对齐。", "result": "MedIQA在多个下游任务中显著优于现有基线。", "conclusion": "MedIQA为医学IQA建立了一个可扩展的框架，并推动了诊断工作流程和临床决策的进步。", "translation": "医学成像技术的快速发展凸显了精确自动化图像质量评估（IQA）以确保诊断准确性的关键需求。然而，现有医学IQA方法难以在不同模态和临床场景中泛化。为此，我们引入了MedIQA，这是首个用于医学IQA的综合基础模型，旨在处理图像维度、模态、解剖区域和类型的变异性。我们开发了一个带有大量手动标注质量得分的大规模多模态数据集来支持这项工作。我们的模型集成了显著切片评估模块，以聚焦于诊断相关区域的特征检索，并采用自动提示策略，将上游物理参数预训练与下游专家标注微调对齐。大量实验表明，MedIQA在多个下游任务中显著优于基线，为医学IQA建立了一个可扩展的框架，并推动了诊断工作流程和临床决策的进步。", "summary": "MedIQA是首个用于医学图像质量评估（IQA）的综合基础模型，旨在解决现有方法在不同医学图像模态和临床场景中泛化能力差的问题。该模型通过利用一个大规模、多模态、带手动标注质量得分的数据集进行训练，并结合了显著切片评估模块和自动提示策略。实验结果表明，MedIQA在多项下游任务中表现优异，为医学IQA提供了一个可扩展的框架，有望改进诊断流程和临床决策。", "keywords": "医学图像质量评估, 基础模型, 提示驱动, 可扩展, 多模态", "comments": "MedIQA的创新之处在于它是首个为医学IQA设计的综合基础模型，解决了现有方法在泛化性上的不足。其采用的大规模多模态数据集和结合显著切片评估与自动提示策略的模型设计，使其在处理多样化的医学图像方面具有巨大潜力。该模型对于提高诊断准确性和效率具有重要意义。"}}
{"id": "2507.19437", "title": "Observations Meet Actions: Learning Control-Sufficient Representations for Robust Policy Generalization", "authors": ["Yuliang Gu", "Hongpeng Cao", "Marco Caccamo", "Naira Hovakimyan"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19437v1", "summary": "Capturing latent variations (\"contexts\") is key to deploying\nreinforcement-learning (RL) agents beyond their training regime. We recast\ncontext-based RL as a dual inference-control problem and formally characterize\ntwo properties and their hierarchy: observation sufficiency (preserving all\npredictive information) and control sufficiency (retaining decision-making\nrelevant information). Exploiting this dichotomy, we derive a contextual\nevidence lower bound(ELBO)-style objective that cleanly separates\nrepresentation learning from policy learning and optimizes it with Bottlenecked\nContextual Policy Optimization (BCPO), an algorithm that places a variational\ninformation-bottleneck encoder in front of any off-policy policy learner. On\nstandard continuous-control benchmarks with shifting physical parameters, BCPO\nmatches or surpasses other baselines while using fewer samples and retaining\nperformance far outside the training regime. The framework unifies theory,\ndiagnostics, and practice for context-based RL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19437v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "观察与行动相遇：学习控制充足的表征以实现鲁棒策略泛化", "tldr": "本文提出了一种新的上下文强化学习框架BCPO，通过学习控制充足的表征，显著提高了策略在训练范围外的泛化能力和样本效率。", "motivation": "强化学习代理在训练环境之外部署时，需要捕获潜在的变异（“上下文”），这是当前RL面临的关键挑战。", "method": "将基于上下文的强化学习重构为双重推理-控制问题，并形式化了观察充足性（保留所有预测信息）和控制充足性（保留决策相关信息）这两个属性及其层次结构。利用这种二分法，推导了一个上下文证据下界（ELBO）风格的目标函数，该目标函数将表征学习与策略学习清晰分离，并使用瓶颈上下文策略优化（BCPO）算法进行优化。BCPO在任何离策略学习器之前放置一个变分信息瓶颈编码器。", "result": "在具有物理参数变化的连续控制基准测试中，BCPO与现有基线算法相比，性能相当或更优，同时使用更少的样本，并且在训练范围之外仍能保持出色的性能。", "conclusion": "该框架统一了基于上下文的强化学习的理论、诊断和实践。", "translation": "捕获潜在变异（“上下文”）是将强化学习（RL）代理部署到其训练范围之外的关键。我们将基于上下文的RL重构为一个双重推理-控制问题，并形式化了两个属性及其层次结构：观察充足性（保留所有预测信息）和控制充足性（保留决策相关信息）。利用这种二分法，我们推导了一个上下文证据下界（ELBO）风格的目标函数，该目标函数将表征学习与策略学习清晰分离，并使用瓶颈上下文策略优化（BCPO）算法对其进行优化，该算法在任何离策略策略学习器之前放置一个变分信息瓶颈编码器。在物理参数变化的标准连续控制基准测试中，BCPO与其它基线算法相比，性能相当或更优，同时使用更少的样本，并且在训练范围之外仍能保持出色的性能。该框架统一了基于上下文的RL的理论、诊断和实践。", "summary": "本文提出了一种新颖的上下文强化学习框架，旨在通过学习“控制充足”的表征来提高策略的鲁棒泛化能力。该框架将上下文RL视为双重推理-控制问题，并引入了观察充足性和控制充足性的概念。通过推导一个上下文ELBO风格的目标函数，并开发了瓶颈上下文策略优化（BCPO）算法，实现了表征学习和策略学习的有效分离。实验结果表明，BCPO在连续控制任务中表现优异，样本效率更高，并在训练范围外保持了强大的性能，为上下文RL提供了统一的理论与实践。", "keywords": "强化学习, 上下文, 表征学习, 策略泛化, 信息瓶颈", "comments": "本文的创新之处在于提出了“控制充足性”的概念，并将其与“观察充足性”区分开来，这为学习鲁棒的上下文表征提供了新的理论视角。通过将变分信息瓶颈编码器集成到策略学习中，BCPO算法能够有效地提取决策相关的关键信息，从而显著提高了策略在未知环境中的泛化能力和样本效率。这对于实现RL在现实世界中的广泛部署具有重要意义。"}}
{"id": "2507.19423", "title": "Perfect Clustering in Very Sparse Diverse Multiplex Networks", "authors": ["Marianna Pensky"], "categories": ["stat.ML", "cs.LG", "math.ST", "stat.ME", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      5 figures", "url": "http://arxiv.org/abs/2507.19423v1", "summary": "The paper studies the DIverse MultiPLEx Signed Generalized Random Dot Product\nGraph (DIMPLE-SGRDPG) network model (Pensky (2024)), where all layers of the\nnetwork have the same collection of nodes. In addition, all layers can be\npartitioned into groups such that the layers in the same group are embedded in\nthe same ambient subspace but otherwise matrices of connection probabilities\ncan be all different. This setting includes majority of multilayer network\nmodels as its particular cases. The key task in this model is to recover the\ngroups of layers with unique subspace structures, since the case where all\nlayers of the network are embedded in the same subspace has been fairly well\nstudied. Until now, clustering of layers in such networks was based on the\nlayer-per-layer analysis, which required the multilayer network to be\nsufficiently dense. Nevertheless, in this paper we succeeded in pooling\ninformation in all layers together and providing a tensor-based methodology\nthat ensures perfect clustering for a much sparser network. Our theoretical\nresults, established under intuitive non-restrictive assumptions, assert that\nthe new technique achieves perfect clustering under sparsity conditions that,\nup to logarithmic factors, coincide with the computational lower bound derived\nfor a much simpler model.", "comment": "5 figures", "pdf_url": "http://arxiv.org/pdf/2507.19423v1", "cate": "stat.ML", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "极稀疏多样多重网络中的完美聚类", "tldr": "本文提出了一种基于张量的方法，用于在极稀疏的多样多重网络中实现层完美聚类，显著优于传统方法在稀疏性方面的限制。", "motivation": "在多样多重网络模型中，识别具有独特子空间结构的层组是关键任务，但现有基于逐层分析的聚类方法需要网络足够密集。为了克服这一限制，需要一种能处理稀疏网络的新方法。", "method": "本文通过汇集所有层的信息，并提供一种基于张量的创新方法来解决聚类问题。", "result": "新方法确保了在更稀疏网络中的完美聚类。理论结果表明，在直观且非限制性假设下，新技术的稀疏性条件（在对数因子内）与针对更简单模型导出的计算下限一致。", "conclusion": "本文成功开发了一种张量基方法，使得在极稀疏的多样多重网络中实现完美聚类成为可能，显著扩展了多层网络分析的适用范围。", "translation": "本文研究了多样多重符号广义随机点积图（DIMPLE-SGRDPG）网络模型（Pensky (2024)），其中网络的所有层都具有相同的节点集合。此外，所有层都可以被划分为若干组，使得同一组中的层嵌入在相同的环境子空间中，但连接概率矩阵可以完全不同。这种设置包含了大多数多层网络模型作为其特例。该模型中的关键任务是恢复具有独特子空间结构的层组，因为网络所有层都嵌入在同一子空间的情况已经得到了相当好的研究。到目前为止，此类网络中的层聚类是基于逐层分析的，这要求多层网络足够密集。然而，在本文中，我们成功地汇集了所有层的信息，并提供了一种基于张量的方法，确保了在更稀疏网络中的完美聚类。我们的理论结果在直观且非限制性假设下建立，断言新技术在稀疏性条件下实现了完美聚类，这些条件（在对数因子内）与针对更简单模型导出的计算下限一致。", "summary": "本文提出了一种新颖的基于张量的方法，用于在多样多重符号广义随机点积图（DIMPLE-SGRDPG）模型中对网络层进行完美聚类。与以往需要密集网络才能进行逐层分析的方法不同，本研究通过整合所有层的信息，成功地在极稀疏网络中实现了完美聚类。理论分析表明，该方法在稀疏性条件上达到了与计算下限相匹配的性能，从而显著扩展了多层网络聚类分析的适用范围。", "keywords": "多重网络, 完美聚类, 稀疏网络, 张量方法, DIMPLE-SGRDPG", "comments": "该论文的创新之处在于提出了一种基于张量的方法，成功地解决了在极稀疏多样多重网络中进行完美聚类的难题。这克服了传统逐层分析方法对网络密度的严格要求，使得在更广泛的实际应用中对复杂多层网络进行分析成为可能。其理论结果与计算下限的匹配，也表明了该方法的效率和鲁棒性。"}}
{"id": "2507.19261", "title": "Knowledge Grafting: A Mechanism for Optimizing AI Model Deployment in Resource-Constrained Environments", "authors": ["Osama Almurshed", "Ashish Kaushal", "Asmail Muftah", "Nitin Auluck", "Omer Rana"], "categories": ["cs.AI", "cs.LG", "cs.PF"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      18 pages, 4 figures, ArXiv preprint - Novel \"knowledge grafting\" technique achieving 88.54% AI model size reduction while improving accuracy for resource-constrained deployment", "url": "http://arxiv.org/abs/2507.19261v1", "summary": "The increasing adoption of Artificial Intelligence (AI) has led to larger,\nmore complex models with numerous parameters that require substantial computing\npower -- resources often unavailable in many real-world application scenarios.\nOur paper addresses this challenge by introducing knowledge grafting, a novel\nmechanism that optimizes AI models for resource-constrained environments by\ntransferring selected features (the scion) from a large donor model to a\nsmaller rootstock model. The approach achieves an 88.54% reduction in model\nsize (from 64.39 MB to 7.38 MB), while improving generalization capability of\nthe model. Our new rootstock model achieves 89.97% validation accuracy (vs.\ndonor's 87.47%), maintains lower validation loss (0.2976 vs. 0.5068), and\nperforms exceptionally well on unseen test data with 90.45% accuracy. It\naddresses the typical size vs performance trade-off, and enables deployment of\nAI frameworks on resource-constrained devices with enhanced performance. We\nhave tested our approach on an agricultural weed detection scenario, however,\nit can be extended across various edge computing scenarios, potentially\naccelerating AI adoption in areas with limited hardware/software support -- by\nmirroring in a similar manner the horticultural grafting enables productive\ncultivation in challenging agri-based environments.", "comment": "18 pages, 4 figures, ArXiv preprint - Novel \"knowledge grafting\"\n  technique achieving 88.54% AI model size reduction while improving accuracy\n  for resource-constrained deployment", "pdf_url": "http://arxiv.org/pdf/2507.19261v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "知识嫁接：一种优化资源受限环境下AI模型部署的机制", "tldr": "本文提出了一种名为“知识嫁接”的新机制，通过将大型模型（供体）的选定特征转移到小型模型（砧木）来优化AI模型在资源受限环境中的部署，显著减小了模型大小并提高了泛化能力和准确性。", "motivation": "AI模型的日益普及导致模型规模和复杂性增加，需要大量计算资源，而这些资源在许多实际应用场景中往往不可用。", "method": "提出了一种名为“知识嫁接”的新机制，通过将大型供体模型中选定的特征（接穗）转移到较小的砧木模型中，从而优化AI模型在资源受限环境中的部署。", "result": "模型大小减少了88.54%（从64.39 MB降至7.38 MB）。新砧木模型的验证准确率达到89.97%（供体为87.47%），验证损失更低（0.2976 vs. 0.5068），在未见测试数据上表现出色，准确率为90.45%。", "conclusion": "知识嫁接机制解决了模型大小与性能之间的典型权衡问题，使得AI框架能够在资源受限设备上部署并提高性能。", "translation": "人工智能（AI）的日益普及导致模型规模更大、更复杂，参数众多，需要大量的计算能力——而这些资源在许多现实世界的应用场景中往往不可用。我们的论文通过引入知识嫁接这一新颖机制来解决这一挑战，该机制通过将大型供体模型中选定的特征（接穗）转移到较小的砧木模型中，从而优化资源受限环境下的AI模型。该方法实现了模型大小88.54%的减小（从64.39 MB降至7.38 MB），同时提高了模型的泛化能力。我们的新砧木模型实现了89.97%的验证准确率（而供体为87.47%），保持了较低的验证损失（0.2976 对比 0.5068），并在未见测试数据上表现出色，准确率为90.45%。它解决了典型的尺寸与性能之间的权衡问题，并使AI框架能够在资源受限设备上部署并增强性能。我们已经在农业杂草检测场景中测试了我们的方法，但它也可以扩展到各种边缘计算场景，通过以类似园艺嫁接的方式，促进AI在硬件/软件支持有限的领域中的采用——园艺嫁接使得在具有挑战性的农业环境中进行高效栽培成为可能。", "summary": "本文提出了一种名为“知识嫁接”的创新机制，旨在优化AI模型在资源受限环境中的部署。该方法通过将大型“供体”模型的关键特征转移到小型“砧木”模型中，显著减小了模型体积（88.54%），同时提高了模型的泛化能力和预测准确性。实验结果显示，新模型在保持低损失的同时，验证准确率和测试准确率均超越了原始大型模型，有效解决了模型大小与性能之间的权衡问题，为边缘计算场景中的AI部署提供了高效解决方案。", "keywords": "知识嫁接, 资源受限环境, 模型优化, 边缘计算, 模型部署", "comments": "该论文提出的“知识嫁接”机制具有创新性，它巧妙地借鉴了生物学中的嫁接概念来解决AI模型部署中的实际问题。其核心在于在大幅压缩模型体积的同时，不仅保持了性能，反而有所提升，这对于资源受限的边缘设备AI部署具有重要意义。该方法有望加速AI在硬件/软件支持有限的领域中的普及。"}}
{"id": "2505.20847", "title": "Collision-free Control Barrier Functions for General Ellipsoids via Separating Hyperplane", "authors": ["Zeming Wu", "Lu Liu"], "categories": ["cs.RO", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.20847v2", "summary": "This paper presents a novel collision avoidance method for general ellipsoids\nbased on control barrier functions (CBFs) and separating hyperplanes. First,\ncollision-free conditions for general ellipsoids are analytically derived using\nthe concept of dual cones. These conditions are incorporated into the CBF\nframework by extending the system dynamics of controlled objects with\nseparating hyperplanes, enabling efficient and reliable collision avoidance.\nThe validity of the proposed collision-free CBFs is rigorously proven, ensuring\ntheir effectiveness in enforcing safety constraints. The proposed method\nrequires only single-level optimization, significantly reducing computational\ntime compared to state-of-the-art methods. Numerical simulations and real-world\nexperiments demonstrate the effectiveness and practicality of the proposed\nalgorithm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.20847v2", "cate": "cs.RO", "date": "2025-05-27", "updated": "2025-07-25", "AI": {"title_translation": "适用于一般椭球体的基于分离超平面的无碰撞控制障碍函数", "tldr": "本文提出了一种基于控制障碍函数和分离超平面的新型椭球体无碰撞方法，通过单级优化实现高效可靠的碰撞避免。", "motivation": "开发一种针对一般椭球体的高效可靠的碰撞避免方法。", "method": "该方法结合了控制障碍函数（CBFs）和分离超平面。首先，利用对偶锥概念解析推导了一般椭球体的无碰撞条件。然后，通过扩展受控对象的系统动力学，将这些条件整合到CBF框架中。该方法仅需要单级优化。", "result": "所提出的无碰撞CBFs的有效性得到了严格证明，确保了其在强制安全约束方面的有效性。与现有技术相比，计算时间显著减少。数值模拟和实际实验证明了该算法的有效性和实用性。", "conclusion": "所提出的基于CBF和分离超平面的无碰撞方法对于一般椭球体是有效、实用且计算高效的。", "translation": "本文提出了一种基于控制障碍函数（CBFs）和分离超平面的针对一般椭球体的新型碰撞避免方法。首先，利用对偶锥的概念解析推导了针对一般椭球体的无碰撞条件。通过使用分离超平面扩展受控对象的系统动力学，将这些条件整合到CBF框架中，从而实现高效可靠的碰撞避免。所提出的无碰撞CBFs的有效性得到了严格证明，确保了它们在强制安全约束方面的有效性。与现有技术相比，所提出的方法仅需要单级优化，显著减少了计算时间。数值模拟和实际实验证明了所提出算法的有效性和实用性。", "summary": "本文介绍了一种新颖的针对一般椭球体的无碰撞方法，该方法结合了控制障碍函数（CBFs）和分离超平面。通过对偶锥概念推导无碰撞条件并将其整合到CBF框架中，实现了高效可靠的碰撞避免。该方法仅需单级优化，显著降低了计算成本，并通过理论证明、数值模拟和实际实验验证了其有效性和实用性。", "keywords": "碰撞避免, 控制障碍函数, 分离超平面, 椭球体, 单级优化", "comments": "该研究的创新点在于将对偶锥和分离超平面概念引入CBF框架，以解决一般椭球体的无碰撞问题，并通过单级优化显著提高了计算效率，这对于实时系统具有重要意义。"}}
{"id": "2507.19110", "title": "LISA: A Layer-wise Integration and Suppression Approach for Hallucination Mitigation in Multimodal Large Language Models", "authors": ["Zhihui Guo", "Xin Man", "Hui Xu", "Jie Shao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19110v1", "summary": "Multimodal Large Language Models (MLLMs) excel in vision-language tasks such\nas image captioning but remain prone to object hallucinations, where they\ndescribe objects that do not appear in the image. To mitigate this, we propose\n\\textbf{LISA}, a \\textbf{L}ayer-wise \\textbf{I}ntegration and\n\\textbf{S}uppression \\textbf{A}pproach that enhances generation consistency\nthrough hierarchical modulation and multi-layer fusion. LISA leverages the\nfunctional hierarchy within MLLMs, where shallow layers provide visual\ngrounding, middle layers encode semantics, and deep layers tend to amplify\nspurious signals. First, zone-specific spectral modulation stabilizes attention\nby suppressing over-amplified activations in deeper layers while preserving\nalignment cues in earlier layers. Second, token-level logits from selected\nlayers are fused via anchor-based routing, with token-wise anchor selection and\nsoft logit fusion enabling adaptive integration during decoding. LISA is fully\n\\textbf{plug-and-play} and can be seamlessly integrated into existing MLLMs,\nincluding Qwen2.5-VL. Experiments on multiple benchmarks show that LISA reduces\nhallucinations by up to 53.6\\% in $\\mathrm{CHAIR}_I$ and improves POPE F1 by\n4.5\\%, demonstrating strong generalization across models and tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19110v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "LISA：一种用于多模态大型语言模型中幻觉缓解的逐层集成与抑制方法", "tldr": "LISA是一种即插即用的方法，通过逐层集成和抑制来减少多模态大型语言模型（MLLMs）中的物体幻觉，显著提高了生成的一致性并减少了幻觉。", "motivation": "多模态大型语言模型（MLLMs）在图像描述等视觉-语言任务中表现出色，但仍容易出现物体幻觉，即描述图像中不存在的物体。因此，需要一种方法来缓解这种幻觉。", "method": "本文提出了LISA（逐层集成与抑制方法），通过分层调制和多层融合来增强生成一致性。LISA利用MLLMs内部的功能层次结构：浅层提供视觉基础，中间层编码语义，深层倾向于放大虚假信号。具体包括两步：1. 区域特定谱调制通过抑制深层中过度放大的激活，同时保留早期层中的对齐线索，从而稳定注意力。2. 来自选定层的令牌级logits通过基于锚点的路由进行融合，其中令牌级锚点选择和软logits融合可以在解码期间实现自适应集成。LISA是完全即插即用的。", "result": "LISA在多个基准测试中将幻觉减少了高达53.6%（在CHAIR_I上），并将POPE F1提高了4.5%，显示出在模型和任务上的强大泛化能力。", "conclusion": "LISA通过其逐层集成和抑制方法，有效缓解了多模态大型语言模型中的物体幻觉问题，显著提高了模型生成内容的一致性和准确性，并且具有良好的通用性和即插即用性。", "translation": "多模态大型语言模型（MLLMs）在图像描述等视觉-语言任务中表现出色，但仍容易出现物体幻觉，即描述图像中不存在的物体。为了缓解这个问题，我们提出了LISA，一种逐层集成与抑制方法，通过分层调制和多层融合来增强生成一致性。LISA利用MLLMs内部的功能层次结构，其中浅层提供视觉基础，中间层编码语义，深层倾向于放大虚假信号。首先，区域特定谱调制通过抑制深层中过度放大的激活，同时保留早期层中的对齐线索来稳定注意力。其次，来自选定层的令牌级logits通过基于锚点的路由进行融合，其中令牌级锚点选择和软logits融合可以在解码期间实现自适应集成。LISA是完全即插即用的，可以无缝集成到现有的MLLMs中，包括Qwen2.5-VL。在多个基准测试上的实验表明，LISA在CHAIR_I上将幻觉减少了高达53.6%，并将POPE F1提高了4.5%，显示出在模型和任务上的强大泛化能力。", "summary": "本文提出LISA，一种逐层集成与抑制方法，旨在缓解多模态大型语言模型（MLLMs）中的物体幻觉问题。LISA通过利用MLLMs内部的功能层次结构，采用区域特定谱调制来稳定注意力并抑制深层中的虚假信号，同时通过锚点路由融合来自不同层的令牌级logits，实现自适应集成。该方法即插即用，可无缝集成到现有MLLMs中，并在实验中显著降低了幻觉率，提高了生成准确性，展现出强大的泛化能力。", "keywords": "多模态大型语言模型, 物体幻觉, 逐层集成, 抑制, 即插即用", "comments": "LISA的创新点在于其逐层集成与抑制的思路，通过利用MLLMs不同层的功能特性（浅层视觉基础、中间层语义、深层信号放大），有针对性地进行调制和融合。其即插即用的特性使其具有很高的实用价值和广阔的应用前景。该方法通过直接干预模型内部的信号处理机制来减少幻觉，而非仅仅依赖外部后处理，这可能为其带来更深层次的鲁棒性。"}}
{"id": "2412.08949", "title": "Tuned Reverse Distillation: Enhancing Multimodal Industrial Anomaly Detection with Crossmodal Tuners", "authors": ["Xinyue Liu", "Jianyuan Wang", "Biao Leng", "Shuo Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.08949v3", "summary": "Knowledge distillation (KD) has been widely studied in unsupervised image\nAnomaly Detection (AD), but its application to unsupervised multimodal AD\nremains underexplored. Existing KD-based methods for multimodal AD that use\nfused multimodal features to obtain teacher representations face challenges.\nAnomalies that only exist in one modality may not be effectively captured in\nthe fused teacher features, leading to detection failures. Besides, these\nmethods do not fully leverage the rich intra- and inter-modality information\nthat are critical for effective anomaly detection. In this paper, we propose\nTuned Reverse Distillation (TRD) based on Multi-branch design to realize\nMultimodal Industrial AD. By assigning independent branches to each modality,\nour method enables finer detection of anomalies within each modality.\nFurthermore, we enhance the interaction between modalities during the\ndistillation process by designing two Crossmodal Tuners including Crossmodal\nFilter and Amplifier. With the idea of crossmodal mapping, the student network\nis allowed to better learn normal features while anomalies in all modalities\nare ensured to be effectively detected. Experimental verifications on\nmultimodal AD datasets demonstrate that our method achieves state-of-the-art\nperformance in multimodal anomaly detection and localization. Code is available\nat https://github.com/hito2448/TRD.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.08949v3", "cate": "cs.CV", "date": "2024-12-12", "updated": "2025-07-25", "AI": {"title_translation": "调谐逆向蒸馏：利用跨模态调谐器增强多模态工业异常检测", "tldr": "本文提出了调谐逆向蒸馏（TRD）方法，通过多分支设计和跨模态调谐器，解决了现有知识蒸馏在多模态异常检测中无法有效捕获单模态异常和充分利用模态间信息的问题，实现了最先进的性能。", "motivation": "现有基于知识蒸馏的多模态异常检测方法在融合多模态特征以获取教师表示时面临挑战。仅存在于单一模态中的异常可能无法在融合的教师特征中被有效捕获，导致检测失败。此外，这些方法未能充分利用对有效异常检测至关重要的丰富模态内和模态间信息。", "method": "本文提出了基于多分支设计的调谐逆向蒸馏（TRD）方法，以实现多模态工业异常检测。通过为每个模态分配独立的branches，实现了对每个模态内异常的更精细检测。此外，通过设计包括跨模态滤波器和放大器在内的两种跨模态调谐器，增强了蒸馏过程中的模态间交互。通过跨模态映射的思想，学生网络能够更好地学习正常特征，同时确保有效检测所有模态中的异常。", "result": "在多模态异常检测数据集上的实验验证表明，我们的方法在多模态异常检测和定位方面达到了最先进的性能。", "conclusion": "调谐逆向蒸馏（TRD）方法通过其多分支设计和创新的跨模态调谐器，有效解决了多模态工业异常检测中的挑战，并在性能上超越了现有方法。", "translation": "知识蒸馏（KD）在无监督图像异常检测（AD）中得到了广泛研究，但其在无监督多模态异常检测中的应用仍未被充分探索。现有基于KD的多模态异常检测方法使用融合的多模态特征来获取教师表示，面临挑战。仅存在于单一模态中的异常可能无法在融合的教师特征中被有效捕获，导致检测失败。此外，这些方法未能充分利用对有效异常检测至关重要的丰富模态内和模态间信息。在本文中，我们提出了基于多分支设计的调谐逆向蒸馏（TRD）方法，以实现多模态工业异常检测。通过为每个模态分配独立的branches，我们的方法能够更精细地检测每个模态内的异常。此外，我们通过设计包括跨模态滤波器和放大器在内的两种跨模态调谐器，增强了蒸馏过程中的模态间交互。通过跨模态映射的思想，学生网络能够更好地学习正常特征，同时确保有效检测所有模态中的异常。在多模态异常检测数据集上的实验验证表明，我们的方法在多模态异常检测和定位方面达到了最先进的性能。代码可在https://github.com/hito2448/TRD获取。", "summary": "本文提出了一种名为调谐逆向蒸馏（TRD）的新方法，用于多模态工业异常检测。该方法通过采用多分支设计为每个模态分配独立分支，以实现更精细的模态内异常检测。同时，通过引入跨模态滤波器和放大器两种跨模态调谐器，增强了蒸馏过程中的模态间交互，确保学生网络能更好地学习正常特征并有效检测所有模态中的异常。实验结果表明，TRD在多模态异常检测和定位方面取得了最先进的性能，解决了现有知识蒸馏方法在多模态场景中捕获单模态异常和利用模态信息不足的问题。", "keywords": "多模态异常检测, 知识蒸馏, 跨模态调谐器, 工业异常检测, 逆向蒸馏", "comments": "该论文通过引入多分支设计和创新的跨模态调谐器，有效地解决了多模态异常检测中单一模态异常难以捕捉以及模态间信息利用不足的关键挑战。其提出的TRD框架在知识蒸馏范式下进行改进，实现了对复杂工业场景中多模态数据的精细化异常检测，具有重要的理论和实际应用价值。"}}
{"id": "2507.19280", "title": "RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow", "authors": ["Liang Yao", "Fan Liu", "Hongbo Lu", "Chuanyi Zhang", "Rui Min", "Shengxiang Xu", "Shimin Di", "Pai Peng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19280v1", "summary": "Remote sensing imagery presents vast, inherently unstructured spatial data,\ndemanding sophisticated reasoning to interpret complex user intents and\ncontextual relationships beyond simple recognition tasks. In this paper, we aim\nto construct an Earth observation workflow to handle complex queries by\nreasoning about spatial context and user intent. As a reasoning workflow, it\nshould be somewhat autonomous, where predefined ground-truth reasoning paths do\nnot constrain the learning process. Furthermore, its architecture ought to be\nunified yet flexible, enabling the model to perform diverse reasoning tasks\nwith distinct output formats through a single forward pass. Existing remote\nsensing approaches fail to address these requirements, as they rely on\nsupervised fine-tuning paradigms that constrain the autonomy of reasoning. To\nthis end, we propose RemoteReasoner, a flexible and robust workflow for remote\nsensing reasoning tasks. The design of RemoteReasoner integrates a multi-modal\nlarge language model (MLLM) for interpreting user instructions and localizing\ntargets, together with task adaptation strategies that enable multi-granularity\noutput generation. In contrast to existing methods, our framework is trained\nwith reinforcement learning (RL) to endow the MLLM sufficient autonomy for\nprecise reasoning. At the inference stage, our adaptation strategies enable\ndiverse output formats at inference time without requiring task-specific\ndecoders or further fine-tuning. Preliminary experiments demonstrated that\nRemoteReasoner achieves remarkable performance across multi-granularity\nreasoning tasks, including region-level and pixel-level. Additionally, our\nframework enables novel capabilities such as the contour extraction task beyond\nthe reach of existing reasoning pipelines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19280v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "RemoteReasoner：迈向统一的地理空间推理工作流", "tldr": "RemoteReasoner提出了一个基于多模态大语言模型和强化学习的灵活统一工作流，用于处理遥感图像中复杂的地理空间推理任务，超越了现有方法的局限性。", "motivation": "遥感图像包含大量非结构化空间数据，需要复杂的推理来解释用户意图和上下文关系，而不仅仅是简单的识别任务。现有遥感方法依赖于监督微调，限制了推理的自主性，无法满足处理复杂查询、实现自主学习和统一灵活架构的需求。", "method": "本文提出了RemoteReasoner，一个灵活鲁棒的遥感推理工作流。它集成了多模态大语言模型（MLLM）来解释用户指令和定位目标，并采用任务适应策略以生成多粒度输出。该框架通过强化学习（RL）进行训练，赋予MLLM足够的自主性以进行精确推理。在推理阶段，适应策略无需任务特定的解码器或额外微调即可实现多样化的输出格式。", "result": "初步实验表明，RemoteReasoner在包括区域级和像素级在内的多粒度推理任务中取得了显著性能。此外，该框架还实现了现有推理管道无法触及的新功能，例如轮廓提取任务。", "conclusion": "RemoteReasoner成功地构建了一个统一、灵活且自主的地球观测工作流，能够处理复杂的地理空间推理任务，并在多粒度推理和新能力（如轮廓提取）方面表现出色，超越了现有监督微调方法的局限性。", "translation": "遥感图像呈现出海量的、本质上非结构化的空间数据，需要复杂的推理来解释复杂的用户意图和超越简单识别任务的上下文关系。在本文中，我们旨在构建一个地球观测工作流，通过推理空间上下文和用户意图来处理复杂查询。作为一个推理工作流，它应该具有一定的自主性，其中预定义的真实推理路径不应限制学习过程。此外，其架构应该统一而灵活，使模型能够通过一次前向传递执行具有不同输出格式的各种推理任务。现有的遥感方法未能满足这些要求，因为它们依赖于限制推理自主性的监督微调范式。为此，我们提出了RemoteReasoner，一个用于遥感推理任务的灵活鲁棒的工作流。RemoteReasoner的设计集成了多模态大语言模型（MLLM）来解释用户指令和定位目标，以及支持多粒度输出生成的任务适应策略。与现有方法不同，我们的框架通过强化学习（RL）进行训练，以赋予MLLM足够的自主性进行精确推理。在推理阶段，我们的适应策略能够在推理时实现多样化的输出格式，而无需任务特定的解码器或进一步微调。初步实验表明，RemoteReasoner在包括区域级和像素级在内的多粒度推理任务中取得了显著性能。此外，我们的框架还实现了现有推理管道无法触及的新功能，例如轮廓提取任务。", "summary": "RemoteReasoner是一个为遥感图像设计的统一地理空间推理工作流，旨在克服现有方法在处理复杂用户意图和上下文关系方面的局限性。它整合了一个多模态大语言模型（MLLM）用于指令理解和目标定位，并通过强化学习（RL）训练以增强推理自主性。该框架还引入了任务适应策略，支持在单次前向传递中生成多粒度、多样化的输出，无需额外微调。初步实验证明，RemoteReasoner在多粒度推理任务上表现卓越，并实现了如轮廓提取等现有管道无法实现的新能力。", "keywords": "地理空间推理, 遥感, 多模态大语言模型, 强化学习, 统一工作流", "comments": "RemoteReasoner的创新之处在于其将多模态大语言模型与强化学习相结合，以实现遥感推理的自主性和统一性，这与现有依赖监督微调的方法形成鲜明对比。其能够处理多粒度输出和实现新任务（如轮廓提取）的能力，展示了该框架在地理空间推理领域的潜力和重要性，有望推动遥感数据解释的范式转变。"}}
{"id": "2506.14335", "title": "References Matter: Investigating the Impact of Reference Set Variation on Summarization Evaluation", "authors": ["Silvia Casola", "Yang Janet Liu", "Siyao Peng", "Oliver Kraus", "Albert Gatt", "Barbara Plank"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.14335v2", "summary": "Human language production exhibits remarkable richness and variation,\nreflecting diverse communication styles and intents. However, this variation is\noften overlooked in summarization evaluation. While having multiple reference\nsummaries is known to improve correlation with human judgments, the impact of\nthe reference set on reference-based metrics has not been systematically\ninvestigated. This work examines the sensitivity of widely used reference-based\nmetrics in relation to the choice of reference sets, analyzing three diverse\nmulti-reference summarization datasets: SummEval, GUMSum, and DUC2004. We\ndemonstrate that many popular metrics exhibit significant instability. This\ninstability is particularly concerning for n-gram-based metrics like ROUGE,\nwhere model rankings vary depending on the reference sets, undermining the\nreliability of model comparisons. We also collect human judgments on LLM\noutputs for genre-diverse data and examine their correlation with metrics to\nsupplement existing findings beyond newswire summaries, finding weak-to-no\ncorrelation. Taken together, we recommend incorporating reference set variation\ninto summarization evaluation to enhance consistency alongside correlation with\nhuman judgments, especially when evaluating LLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.14335v2", "cate": "cs.CL", "date": "2025-06-17", "updated": "2025-07-25", "AI": {"title_translation": "参考文献很重要：研究参考集变异对摘要评估的影响", "tldr": "本文系统研究了参考集选择对摘要评估指标稳定性的影响，发现许多常用指标（尤其是ROUGE）对参考集变化敏感，导致模型排名不稳定。研究还发现LLM输出与人类判断的相关性较弱，并建议在摘要评估中考虑参考集变异以提高一致性。", "motivation": "人类语言的丰富性和多样性在摘要评估中常被忽视。尽管已知多参考文献能提高与人类判断的相关性，但参考集对基于参考文献的评估指标影响尚未被系统研究。本文旨在填补这一空白，调查常用指标对参考集选择的敏感性。", "method": "研究分析了三个多样化的多参考文献摘要数据集：SummEval、GUMSum和DUC2004，以检验广泛使用的基于参考文献的指标对参考集选择的敏感性。此外，还收集了LLM输出的人类判断，并分析了它们与指标的相关性，以补充现有发现。", "result": "许多流行的摘要评估指标表现出显著的不稳定性，特别是基于n-gram的指标如ROUGE。模型排名会因参考集的不同而变化，这损害了模型比较的可靠性。对于不同类型的数据，LLM输出与人类判断的相关性被发现是弱到没有相关性。", "conclusion": "鉴于评估指标对参考集选择的显著不稳定性，尤其是在评估大型语言模型（LLMs）时，建议在摘要评估中纳入参考集变异，以提高评估的一致性以及与人类判断的相关性。", "translation": "人类语言的产生表现出显著的丰富性和多样性，反映了不同的交流风格和意图。然而，这种变异在摘要评估中常常被忽视。虽然已知拥有多个参考摘要可以提高与人类判断的相关性，但参考集对基于参考文献的指标的影响尚未被系统研究。这项工作考察了广泛使用的基于参考文献的指标与参考集选择相关的敏感性，分析了三个多样化的多参考文献摘要数据集：SummEval、GUMSum和DUC2004。我们证明了许多流行的指标表现出显著的不稳定性。这种不稳定性对于基于n-gram的指标（如ROUGE）尤其令人担忧，因为模型排名会根据参考集的不同而变化，从而损害了模型比较的可靠性。我们还收集了关于LLM输出在不同类型数据上的人类判断，并检查了它们与指标的相关性，以补充新闻摘要之外的现有发现，结果发现相关性较弱或没有相关性。总而言之，我们建议在摘要评估中纳入参考集变异，以增强一致性以及与人类判断的相关性，尤其是在评估LLMs时。", "summary": "本文系统地研究了参考集变异对摘要评估指标稳定性的影响。研究发现，许多流行的基于参考文献的指标，特别是ROUGE，对参考集选择高度敏感，导致模型排名不稳定，从而影响模型比较的可靠性。此外，对LLM输出的人类判断与现有指标的相关性在不同语料中表现出弱相关甚至无相关性。鉴于这些发现，作者建议在摘要评估中，尤其是在评估LLMs时，应考虑参考集变异，以提高评估的一致性和与人类判断的相关性。", "keywords": "摘要评估, 参考集变异, ROUGE, LLM, 评估稳定性", "comments": "本文揭示了当前摘要评估方法中一个关键但常被忽视的问题：参考集变异对评估指标稳定性的影响。其创新之处在于系统性地验证了这种不稳定性，尤其指出了ROUGE等常用指标的局限性。这对未来的摘要模型开发和评估具有重要指导意义，强调了在评估LLM生成摘要时，多参考和考虑参考集多样性的必要性。研究结果表明，简单依赖现有指标可能导致对模型性能的误判。"}}
{"id": "2507.19146", "title": "Diverse and Adaptive Behavior Curriculum for Autonomous Driving: A Student-Teacher Framework with Multi-Agent RL", "authors": ["Ahmed Abouelazm", "Johannes Ratz", "Philip Schörner", "J. Marius Zöllner"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Paper accepted in IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)", "url": "http://arxiv.org/abs/2507.19146v1", "summary": "Autonomous driving faces challenges in navigating complex real-world traffic,\nrequiring safe handling of both common and critical scenarios. Reinforcement\nlearning (RL), a prominent method in end-to-end driving, enables agents to\nlearn through trial and error in simulation. However, RL training often relies\non rule-based traffic scenarios, limiting generalization. Additionally, current\nscenario generation methods focus heavily on critical scenarios, neglecting a\nbalance with routine driving behaviors. Curriculum learning, which\nprogressively trains agents on increasingly complex tasks, is a promising\napproach to improving the robustness and coverage of RL driving policies.\nHowever, existing research mainly emphasizes manually designed curricula,\nfocusing on scenery and actor placement rather than traffic behavior dynamics.\nThis work introduces a novel student-teacher framework for automatic curriculum\nlearning. The teacher, a graph-based multi-agent RL component, adaptively\ngenerates traffic behaviors across diverse difficulty levels. An adaptive\nmechanism adjusts task difficulty based on student performance, ensuring\nexposure to behaviors ranging from common to critical. The student, though\nexchangeable, is realized as a deep RL agent with partial observability,\nreflecting real-world perception constraints. Results demonstrate the teacher's\nability to generate diverse traffic behaviors. The student, trained with\nautomatic curricula, outperformed agents trained on rule-based traffic,\nachieving higher rewards and exhibiting balanced, assertive driving.", "comment": "Paper accepted in IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)", "pdf_url": "http://arxiv.org/pdf/2507.19146v1", "cate": "cs.RO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "自动驾驶中多样化自适应行为课程：一种基于多智能体强化学习的师生框架", "tldr": "提出一种师生框架，利用多智能体RL教师自动生成多样化交通行为课程，训练学生自动驾驶RL智能体，使其在复杂场景中表现更优。", "motivation": "自动驾驶强化学习训练依赖规则场景，泛化性受限；现有场景生成侧重关键场景，忽视常规；手动课程设计主要关注环境而非交通行为动态，限制了RL驾驶策略的鲁棒性和覆盖范围。", "method": "引入一种新颖的师生框架进行自动课程学习。教师是一个基于图的多智能体强化学习组件，自适应生成不同难度级别的交通行为，并根据学生表现调整任务难度。学生是一个深度强化学习智能体，具有部分可观察性。", "result": "教师能够生成多样化的交通行为。学生在自动课程训练下，表现优于在基于规则交通下训练的智能体，获得了更高的奖励，并展现出平衡且果断的驾驶。", "conclusion": "该研究通过提出一个师生框架，成功实现了自动生成多样化和自适应的交通行为课程，有效提升了自动驾驶RL智能体的泛化能力和鲁棒性，使其在复杂交通环境中表现更优。", "translation": "自动驾驶在应对复杂的真实世界交通中面临挑战，需要安全处理常见和关键场景。强化学习（RL）作为端到端驾驶中的突出方法，使智能体能够在模拟中通过试错学习。然而，RL训练通常依赖基于规则的交通场景，限制了泛化能力。此外，当前的场景生成方法过于侧重关键场景，忽视了与常规驾驶行为的平衡。课程学习，即逐步训练智能体完成日益复杂的任务，是提高RL驾驶策略鲁棒性和覆盖范围的一种有前景的方法。然而，现有研究主要强调手动设计的课程，侧重于风景和参与者位置，而非交通行为动态。本工作引入了一种新颖的师生框架，用于自动课程学习。教师是一个基于图的多智能体RL组件，自适应生成不同难度级别的交通行为。一种自适应机制根据学生表现调整任务难度，确保学生接触到从常见到关键的行为。学生虽然可替换，但被实现为一个具有部分可观察性的深度RL智能体，反映了真实世界的感知限制。结果表明教师能够生成多样化的交通行为。学生在自动课程训练下，表现优于在基于规则交通下训练的智能体，获得了更高的奖励，并展现出平衡、果断的驾驶。", "summary": "本文针对自动驾驶中强化学习训练泛化性差和现有课程学习方法局限性问题，提出了一种新颖的师生框架，用于自动生成多样化和自适应的交通行为课程。该框架中，教师是一个基于图的多智能体强化学习组件，负责根据学生表现动态调整难度，生成从常见到关键的交通行为；学生则是一个具有部分可观察性的深度强化学习智能体。实验结果表明，该教师能够生成多样化的交通行为，且通过自动课程训练的学生智能体在性能上超越了传统基于规则训练的智能体，展现出更高的奖励和更平衡果断的驾驶风格。", "keywords": "自动驾驶, 强化学习, 课程学习, 师生框架, 多智能体系统", "comments": "该论文的创新点在于提出了一个基于多智能体强化学习的师生框架，实现了自动生成多样化和自适应的交通行为课程，而非依赖传统手动设计或侧重环境的课程。这种方法通过动态调整难度，能够有效提升自动驾驶RL智能体在复杂真实世界交通中的鲁棒性和泛化能力，解决了当前RL训练中场景多样性不足和泛化性差的局限性。"}}
{"id": "2507.19046", "title": "Dynamics-Informed Reservoir Computing with Visibility Graphs", "authors": ["Charlotte Geier", "Merten Stender"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      7 pages, 4 figures. The following article has been submitted to by Chaos: An Interdisciplinary Journal of Nonlinear Science", "url": "http://arxiv.org/abs/2507.19046v1", "summary": "Accurate prediction of complex and nonlinear time series remains a\nchallenging problem across engineering and scientific disciplines. Reservoir\ncomputing (RC) offers a computationally efficient alternative to traditional\ndeep learning by training only the read-out layer while employing a randomly\nstructured and fixed reservoir network. Despite its advantages, the largely\nrandom reservoir graph architecture often results in suboptimal and oversized\nnetworks with poorly understood dynamics. Addressing this issue, we propose a\nnovel Dynamics-Informed Reservoir Computing (DyRC) framework that\nsystematically infers the reservoir network structure directly from the input\ntraining sequence. This work proposes to employ the visibility graph (VG)\ntechnique, which converts time series data into networks by representing\nmeasurement points as nodes linked by mutual visibility. The reservoir network\nis constructed by directly adopting the VG network from a training data\nsequence, leveraging the parameter-free visibility graph approach to avoid\nexpensive hyperparameter tuning. This process results in a reservoir that is\ndirectly informed by the specific dynamics of the prediction task under study.\nWe assess the DyRC-VG method through prediction tasks involving the canonical\nnonlinear Duffing oscillator, evaluating prediction accuracy and consistency.\nCompared to an Erd\\H{o}s-R\\'enyi graph of the same size, spectral radius, and\ncomparable density, we observe higher prediction quality and more consistent\nperformance over repeated implementations in the DyRC-VG.", "comment": "7 pages, 4 figures. The following article has been submitted to by\n  Chaos: An Interdisciplinary Journal of Nonlinear Science", "pdf_url": "http://arxiv.org/pdf/2507.19046v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "可见图驱动的动力学感知储层计算", "tldr": "本文提出了一种新的动力学感知储层计算（DyRC）框架，通过可见图（VG）技术直接从输入训练序列中推断储层网络结构，从而在预测任务中实现更高的准确性和一致性，避免了昂贵的超参数调整。", "motivation": "复杂非线性时间序列的准确预测仍然是一个挑战。传统的储层计算（RC）虽然计算效率高，但其随机储层图结构常导致次优和过大的网络，且其动力学特性理解不足。", "method": "本文提出了一种新颖的动力学感知储层计算（DyRC）框架，该框架系统地从输入训练序列中推断储层网络结构。具体地，该方法采用可见图（VG）技术，将时间序列数据转换为网络，其中测量点表示为节点，通过相互可见性连接。储层网络通过直接采用训练数据序列的VG网络来构建，利用无参数的可见图方法避免了昂贵的超参数调优。", "result": "通过对经典的非线性Duffing振子进行预测任务评估，与相同大小、谱半径和可比密度的Erdős-Rényi图相比，DyRC-VG方法在重复实现中表现出更高的预测质量和更一致的性能。", "conclusion": "DyRC-VG方法通过从训练数据中直接学习储层网络结构，有效提高了储层计算的预测准确性和一致性，克服了传统随机储层网络设计的局限性。", "translation": "复杂非线性时间序列的准确预测在工程和科学领域仍然是一个具有挑战性的问题。储层计算（RC）通过仅训练读出层，同时采用随机结构和固定的储层网络，为传统深度学习提供了一种计算效率更高的替代方案。尽管其具有优势，但很大程度上随机的储层图结构通常会导致次优和过大的网络，且其动力学特性理解不足。为了解决这个问题，我们提出了一种新颖的动力学感知储层计算（DyRC）框架，该框架系统地直接从输入训练序列中推断储层网络结构。这项工作提出采用可见图（VG）技术，该技术通过将测量点表示为通过相互可见性连接的节点，将时间序列数据转换为网络。储层网络通过直接采用训练数据序列的VG网络来构建，利用无参数的可见图方法避免了昂贵的超参数调优。这个过程使得储层直接由所研究预测任务的特定动力学特性所驱动。我们通过涉及经典非线性Duffing振子的预测任务评估了DyRC-VG方法，评估了预测准确性和一致性。与相同大小、谱半径和可比密度的Erdős-Rényi图相比，我们在DyRC-VG中观察到更高的预测质量和在重复实现中更一致的性能。", "summary": "本文提出了一种名为动力学感知储层计算（DyRC）的新框架，旨在解决传统储层计算中随机网络结构导致的次优性能问题。DyRC利用可见图（VG）技术，直接从输入时间序列数据中构建储层网络，从而使网络结构能够反映任务的特定动力学特性，并避免了超参数调优。实验结果表明，与随机图相比，DyRC-VG在非线性时间序列预测任务中表现出更高的预测精度和更好的一致性。", "keywords": "储层计算,可见图,时间序列预测,动力学感知,非线性系统", "comments": "该论文提出了一种新颖且具有创新性的方法，通过将时间序列的动力学信息直接编码到储层网络结构中，显著提升了储层计算的性能。利用可见图技术避免了超参数调优，增加了方法的实用性。其重要性在于为解决复杂非线性时间序列预测问题提供了一个更高效、更准确的途径。"}}
{"id": "2507.19244", "title": "Truncated Gaussian Noise Estimation in State-Space Models", "authors": ["Rodrigo A. González", "Angel L. Cedeño", "Koen Tiels", "Tom Oomen"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      6 pages,2 figures", "url": "http://arxiv.org/abs/2507.19244v1", "summary": "Within Bayesian state estimation, considerable effort has been devoted to\nincorporating constraints into state estimation for process optimization, state\nmonitoring, fault detection and control. Nonetheless, in the domain of\nstate-space system identification, the prevalent practice entails constructing\nmodels under Gaussian noise assumptions, which can lead to inaccuracies when\nthe noise follows bounded distributions. With the aim of generalizing the\nGaussian noise assumption to potentially truncated densities, this paper\nintroduces a method for estimating the noise parameters in a state-space model\nsubject to truncated Gaussian noise. Our proposed data-driven approach is\nrooted in maximum likelihood principles combined with the\nExpectation-Maximization algorithm. The efficacy of the proposed approach is\nsupported by a simulation example.", "comment": "6 pages,2 figures", "pdf_url": "http://arxiv.org/pdf/2507.19244v1", "cate": "eess.SY", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "状态空间模型中截断高斯噪声估计", "tldr": "本文提出了一种基于最大似然和EM算法的方法，用于在状态空间模型中估计截断高斯噪声的参数，以解决传统高斯噪声假设在噪声有界时的不准确性问题。", "motivation": "在状态空间系统辨识中，普遍做法是基于高斯噪声假设构建模型。然而，当噪声服从有界分布时，这种假设会导致不准确性。因此，需要一种方法来估计截断高斯噪声的参数，以推广高斯噪声假设到截断密度。", "method": "该论文提出了一种数据驱动的方法，用于估计受截断高斯噪声影响的状态空间模型中的噪声参数。该方法基于最大似然原理，并结合了期望最大化（EM）算法。", "result": "提出的方法通过一个仿真示例验证了其有效性。", "conclusion": "论文提出的基于最大似然和EM算法的截断高斯噪声估计方法在状态空间模型中是有效的，能够解决传统高斯噪声假设在噪声有界时的局限性。", "translation": "在贝叶斯状态估计中，为了过程优化、状态监测、故障检测和控制，人们在将约束纳入状态估计方面付出了巨大努力。然而，在状态空间系统辨识领域，普遍做法是在高斯噪声假设下构建模型，当噪声服从有界分布时，这可能导致不准确。为了将高斯噪声假设推广到潜在的截断密度，本文介绍了一种在受截断高斯噪声影响的状态空间模型中估计噪声参数的方法。我们提出的数据驱动方法植根于最大似然原理，并结合了期望最大化算法。所提出方法的有效性通过一个仿真示例得到支持。", "summary": "本文针对状态空间模型中传统高斯噪声假设在噪声有界时的不足，提出了一种新的噪声参数估计方法。该方法旨在推广高斯噪声假设到截断密度，通过结合最大似然原理和期望最大化（EM）算法，实现对截断高斯噪声的估计。仿真结果验证了所提方法的有效性。", "keywords": "截断高斯噪声, 状态空间模型, 最大似然估计, 期望最大化算法, 噪声估计", "comments": "这篇论文解决了状态空间模型中一个重要的实际问题，即噪声并非总是服从无界的高斯分布。引入截断高斯噪声估计能够提高模型在实际应用中的准确性，特别是在噪声有物理边界的情况下。结合最大似然和EM算法是一种标准且强大的估计方法，使其结果具有可靠性。通过仿真验证，表明该方法具有实际应用潜力。"}}
{"id": "2507.18119", "title": "GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness", "authors": ["Hongjie Chen", "Zehan Li", "Yaodong Song", "Wenming Deng", "Yitong Yao", "Yuxin Zhang", "Hang Lv", "Xuechao Zhu", "Jian Kang", "Jie Lian", "Jie Li", "Chao Wang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18119v2", "summary": "Recent advances in end-to-end spoken language models (SLMs) have\nsignificantly improved the ability of AI systems to engage in natural spoken\ninteractions. However, most existing models treat speech merely as a vehicle\nfor linguistic content, often overlooking the rich paralinguistic and speaker\ncharacteristic cues embedded in human speech, such as dialect, age, emotion,\nand non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel\nspoken language model with paralinguistic and speaker characteristic awareness,\ndesigned to extend spoken language modeling beyond text semantics. GOAT-SLM\nadopts a dual-modality head architecture that decouples linguistic modeling\nfrom acoustic realization, enabling robust language understanding while\nsupporting expressive and adaptive speech generation. To enhance model\nefficiency and versatility, we propose a modular, staged training strategy that\nprogressively aligns linguistic, paralinguistic, and speaker characteristic\ninformation using large-scale speech-text corpora. Experimental results on\nTELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM\nachieves well-balanced performance across both semantic and non-semantic tasks,\nand outperforms existing open-source models in handling emotion, dialectal\nvariation, and age-sensitive interactions. This work highlights the importance\nof modeling beyond linguistic content and advances the development of more\nnatural, adaptive, and socially aware spoken language systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18119v2", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-25", "AI": {"title_translation": "GOAT-SLM：一种具有副语言和说话者特征感知的口语模型", "tldr": "GOAT-SLM是一个新型口语模型，它不仅处理语言内容，还能感知副语言和说话者特征，从而实现更自然、更具表现力的语音交互。", "motivation": "现有口语模型主要关注语言内容，忽视了人类语音中丰富的副语言和说话者特征线索，如方言、年龄、情感和非言语发声，这限制了AI系统进行自然口语交互的能力。", "method": "引入GOAT-SLM，采用双模态头架构将语言建模与声学实现解耦，并提出模块化、分阶段的训练策略，利用大规模语音-文本语料库逐步对齐语言、副语言和说话者特征信息。", "result": "在多维度评估基准TELEVAL上的实验结果表明，GOAT-SLM在语义和非语义任务上均实现了良好平衡的性能，并在处理情感、方言变异和年龄敏感交互方面优于现有开源模型。", "conclusion": "这项工作强调了超越语言内容进行建模的重要性，并推动了更自然、适应性更强、更具社会意识的口语系统发展。", "translation": "摘要:\n端到端口语模型（SLM）的最新进展显著提高了AI系统进行自然口语交互的能力。然而，大多数现有模型将语音仅仅视为语言内容的载体，常常忽视人类语音中嵌入的丰富副语言和说话者特征线索，例如方言、年龄、情感和非言语发声。在这项工作中，我们引入了GOAT-SLM，一种新型的、具有副语言和说话者特征感知的口语模型，旨在将口语建模扩展到文本语义之外。GOAT-SLM采用双模态头架构，将语言建模与声学实现解耦，从而在支持富有表现力和自适应的语音生成的同时，实现鲁棒的语言理解。为了提高模型效率和通用性，我们提出了一种模块化、分阶段的训练策略，利用大规模语音-文本语料库逐步对齐语言、副语言和说话者特征信息。在多维度评估基准TELEVAL上的实验结果表明，GOAT-SLM在语义和非语义任务上均实现了良好平衡的性能，并在处理情感、方言变异和年龄敏感交互方面优于现有开源模型。这项工作强调了超越语言内容进行建模的重要性，并推动了更自然、适应性更强、更具社会意识的口语系统发展。", "summary": "GOAT-SLM是一个新型的口语模型，旨在解决现有模型忽视语音中副语言和说话者特征的问题。它采用双模态头架构解耦语言和声学建模，并通过模块化分阶段训练策略整合多源信息。实验证明，GOAT-SLM在语义和非语义任务上表现均衡，在处理情感、方言和年龄敏感交互方面优于现有模型，推动了更自然、自适应的口语系统发展。", "keywords": "口语模型, 副语言, 说话者特征, 双模态, 语音生成", "comments": "这项工作创新性地将副语言和说话者特征纳入口语模型，突破了传统模型仅关注语言内容的局限。双模态头架构和模块化训练策略是其重要贡献，使得模型能够更好地理解和生成富有表现力的语音。其重要性在于，它为开发更接近人类自然交流的AI系统提供了新的方向。"}}
{"id": "2206.03465", "title": "On entropic and almost multilinear representability of matroids", "authors": ["Lukas Kühne", "Geva Yashfe"], "categories": ["math.CO", "cs.IT", "math.IT", "05B35, 52B40, 14N20, 68P30, 94A17, 20F10, 03D40"], "primary_category": "Subjects:       Combinatorics (math.CO)", "pdf_link": null, "comments": "Comments:      61 pages. To appear in Duke Mathematical Journal", "url": "http://arxiv.org/abs/2206.03465v3", "summary": "This article studies two notions of generalized matroid representations\nmotivated by algorithmic information theory and cryptographic secret sharing.\nThe first (entropic representability) involves discrete random variables, while\nthe second (almost-multilinear representability) deals with approximate\nsubspace arrangements. In both cases, we prove that determining whether an\ninput matroid has such a representation is undecidable. Consequently, the\nconditional independence implication problem is also undecidable, providing an\nindependent answer to a question posed by Geiger and Pearl, recently resolved\nby Cheuk Ting Li. These problems are also closely related to characterizing\nachievable rates in network coding and constructing secret sharing schemes. For\nexample, another corollary of our work is that deciding whether an access\nstructure admits an ideal secret sharing scheme is undecidable. Our approach\nreduces undecidable problems from group theory to matroid representation\nproblems. Specifically, we reduce the uniform word problem for finite groups to\nentropic representability and the word problem for sofic groups to\nalmost-multilinear representability. A key part of this reduction involves\nmodifying group presentations into forms where linear representations are\ngeneric in an appropriate sense when restricted to the generating set.", "comment": "61 pages. To appear in Duke Mathematical Journal", "pdf_url": "http://arxiv.org/pdf/2206.03465v3", "cate": "math.CO", "date": "2022-06-07", "updated": "2025-07-25", "AI": {"title_translation": "关于拟阵的熵表示和近似多线性表示", "tldr": "本文研究了两种广义拟阵表示（熵表示和近似多线性表示），并证明了确定拟阵是否具有这些表示是不可判定的，这为条件独立性问题和理想秘密共享方案的判定提供了独立答案。", "motivation": "本文研究了两种广义拟阵表示，其动机来源于算法信息论和密码学秘密共享。此外，它还旨在独立回答Geiger和Pearl提出的一个问题，并与网络编码中可达速率的表征以及秘密共享方案的构建密切相关。", "method": "本文通过将群论中的不可判定问题归约到拟阵表示问题来证明其结论。具体来说，将有限群的均匀字问题归约到熵表示，将sofic群的字问题归约到近似多线性表示。归约的关键部分涉及修改群表示，使其在线性表示受限于生成集时以适当的意义上是通用的。", "result": "研究证明了确定一个输入拟阵是否具有熵表示或近似多线性表示是不可判定的。因此，条件独立性蕴涵问题也是不可判定的。此外，这项工作的一个推论是，判断一个访问结构是否允许理想秘密共享方案是不可判定的。", "conclusion": "确定拟阵是否具有熵表示或近似多线性表示是不可判定的，这进一步导致条件独立性蕴涵问题以及判断访问结构是否允许理想秘密共享方案也是不可判定的，揭示了这些问题的计算复杂性边界。", "translation": "本文研究了两种广义拟阵表示的概念，其动机来源于算法信息论和密码学秘密共享。第一种（熵表示）涉及离散随机变量，而第二种（近似多线性表示）处理近似子空间排列。在这两种情况下，我们证明了确定一个输入拟阵是否具有这种表示是不可判定的。因此，条件独立性蕴涵问题也是不可判定的，为Geiger和Pearl提出的一个问题提供了独立的答案，该问题最近由Cheuk Ting Li解决。这些问题还与网络编码中可达速率的表征和秘密共享方案的构建密切相关。例如，我们工作的另一个推论是，判断一个访问结构是否允许理想秘密共享方案是不可判定的。我们的方法将群论中的不可判定问题归约到拟阵表示问题。具体来说，我们将有限群的均匀字问题归约到熵表示，将sofic群的字问题归约到近似多线性表示。这种归约的一个关键部分涉及修改群表示，使其在线性表示受限于生成集时以适当的意义上是通用的。", "summary": "本文深入探讨了拟阵的两种广义表示：熵表示和近似多线性表示，这两种表示分别与算法信息论和密码学秘密共享相关。通过将群论中的不可判定问题（如有限群的均匀字问题和sofic群的字问题）巧妙地归约到这些拟阵表示问题，研究首次证明了确定一个拟阵是否具有这两种表示是不可判定的。这一核心发现进一步揭示了条件独立性蕴涵问题以及判断访问结构是否允许理想秘密共享方案同样是不可判定的，为相关领域提供了重要的计算复杂性边界。", "keywords": "熵表示, 近似多线性表示, 拟阵, 不可判定性, 秘密共享", "comments": "本文的创新点在于首次证明了两种广义拟阵表示（熵表示和近似多线性表示）的不可判定性，并通过巧妙地将群论中的不可判定问题归约到拟阵表示问题来实现。这一发现具有重要的理论意义，它不仅独立解决了条件独立性领域的一个长期问题，而且对网络编码中可达速率的表征和秘密共享方案的设计产生了深远影响，揭示了这些问题的内在计算复杂性限制。"}}
{"id": "2507.18756", "title": "Exploitation Over Exploration: Unmasking the Bias in Linear Bandit Recommender Offline Evaluation", "authors": ["Pedro R. Pires", "Gregorio F. Azevedo", "Pietro L. Campos", "Rafael T. Sereicikas", "Tiago A. Almeida"], "categories": ["cs.LG", "cs.IR"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted to be published in RecSys'25, 10 pages, 3 figures", "url": "http://arxiv.org/abs/2507.18756v1", "summary": "Multi-Armed Bandit (MAB) algorithms are widely used in recommender systems\nthat require continuous, incremental learning. A core aspect of MABs is the\nexploration-exploitation trade-off: choosing between exploiting items likely to\nbe enjoyed and exploring new ones to gather information. In contextual linear\nbandits, this trade-off is particularly central, as many variants share the\nsame linear regression backbone and differ primarily in their exploration\nstrategies. Despite its prevalent use, offline evaluation of MABs is\nincreasingly recognized for its limitations in reliably assessing exploration\nbehavior. This study conducts an extensive offline empirical comparison of\nseveral linear MABs. Strikingly, across over 90% of various datasets, a greedy\nlinear model, with no type of exploration, consistently achieves top-tier\nperformance, often outperforming or matching its exploratory counterparts. This\nobservation is further corroborated by hyperparameter optimization, which\nconsistently favors configurations that minimize exploration, suggesting that\npure exploitation is the dominant strategy within these evaluation settings.\nOur results expose significant inadequacies in offline evaluation protocols for\nbandits, particularly concerning their capacity to reflect true exploratory\nefficacy. Consequently, this research underscores the urgent necessity for\ndeveloping more robust assessment methodologies, guiding future investigations\ninto alternative evaluation frameworks for interactive learning in recommender\nsystems.", "comment": "Accepted to be published in RecSys'25, 10 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.18756v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "利用而非探索：揭示线性老虎机推荐系统离线评估中的偏差", "tldr": "本研究发现，在对线性多臂老虎机（MAB）推荐系统的离线评估中，纯粹的利用策略（即贪婪模型）在大多数情况下表现最佳，甚至优于或匹配探索性策略，这表明现有离线评估协议在反映真实探索效率方面存在严重不足。", "motivation": "多臂老虎机（MAB）算法广泛应用于需要持续、增量学习的推荐系统，其核心是探索-利用权衡。然而，MAB的离线评估在可靠评估探索行为方面存在局限性，这促使本研究深入探讨线性MAB的离线评估表现。", "method": "本研究对多种线性多臂老虎机算法进行了广泛的离线实证比较。通过在不同数据集上进行实验，并结合超参数优化来评估不同探索策略的表现。", "result": "在超过90%的不同数据集中，一个没有探索的贪婪线性模型始终能达到顶尖性能，常常优于或匹配其探索性对应模型。超参数优化也持续偏向于最小化探索的配置，表明在这些评估设置中，纯粹的利用是主导策略。", "conclusion": "研究结果揭示了老虎机离线评估协议的显著不足，特别是在反映真实探索效率方面的能力。因此，本研究强调了开发更鲁棒的评估方法，并为未来交互式学习推荐系统替代评估框架的研究提供了指导。", "translation": "多臂老虎机（MAB）算法广泛应用于需要持续、增量学习的推荐系统。MAB的一个核心方面是探索-利用权衡：在利用可能受欢迎的物品和探索新物品以收集信息之间做出选择。在上下文线性老虎机中，这种权衡尤为关键，因为许多变体共享相同的线性回归骨干，主要区别在于其探索策略。尽管其普遍使用，但MAB的离线评估在可靠评估探索行为方面的局限性日益受到认可。本研究对几种线性MAB进行了广泛的离线实证比较。令人惊讶的是，在超过90%的不同数据集中，一个没有探索的贪婪线性模型始终能达到顶尖性能，常常优于或匹配其探索性对应模型。超参数优化进一步证实了这一观察结果，它持续偏向于最小化探索的配置，这表明在这些评估设置中，纯粹的利用是主导策略。我们的结果揭示了老虎机离线评估协议的显著不足，特别是在反映真实探索效率方面的能力。因此，本研究强调了开发更鲁棒的评估方法，指导未来对推荐系统中交互式学习的替代评估框架的调查。", "summary": "本研究深入探讨了线性多臂老虎机（MAB）推荐系统的离线评估问题，发现现有评估协议在反映探索行为方面存在显著偏差。通过广泛的实证比较，研究表明在多数数据集上，一个纯粹利用的贪婪线性模型表现优异，甚至超越了包含探索策略的模型。这突出显示了离线评估在评估MAB探索能力方面的局限性，并强调了开发更可靠评估方法的重要性。", "keywords": "多臂老虎机, 离线评估, 探索-利用权衡, 推荐系统, 偏差", "comments": "该论文揭示了当前多臂老虎机离线评估方法的一个关键盲点，即过度偏向于利用策略，而未能准确反映探索的真正价值。这项工作对于推动推荐系统评估框架的进步具有重要意义，提醒研究人员在设计和评估MAB算法时，需要超越传统的离线指标，寻求更能捕捉交互式学习动态的评估范式。"}}
{"id": "2507.19479", "title": "IoT and Older Adults: Towards Multimodal EMG and AI-Based Interaction with Smart Home", "authors": ["Wiesław Kopeć", "Jarosław Kowalski", "Aleksander Majda", "Anna Duszyk-Bogorodzka", "Anna Jaskulska", "Cezary Biele"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19479v1", "summary": "We report preliminary insights from an exploratory study on non-standard\nnon-invasive interfaces for Smart Home Technologies (SHT). This study is part\nof a broader research project on effective Smart Home ecosystem Sagacity that\nwill target older adults, impaired persons, and other groups disadvantaged in\nthe main technology discourse. Therefore, this research is in line with a\nlong-term research framework of the HASE research group (Human Aspects in\nScience and Engineering) by the Living Lab Kobo. In our study, based on the\nprototype of the comprehensive SHT management system Sagacity, we investigated\nthe potential of bioelectric signals, in particular EMG and EOG as a\ncomplementary interface for SHT. Based on our previous participatory research\nand studies on multimodal interfaces, including VUI and BCI, we prepared an\nin-depth interactive hands-on experience workshops with direct involvement of\nvarious groups of potential end users, including older adults and impaired\npersons (total 18 subjects) to explore and investigate the potential of\nsolutions based on this type of non-standard interfaces. The preliminary\ninsights from the study unveil the potential of EMG/EOG interfaces in\nmultimodal SHT management, alongside limitations and challenges stemming from\nthe current state of technology and recommendations for designing multimodal\ninteraction paradigms pinpointing areas of interest to pursue in further\nstudies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19479v1", "cate": "cs.HC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "物联网与老年人：迈向基于多模态肌电图和人工智能的智能家居交互", "tldr": "一项探索性研究报告了使用肌电图（EMG）和眼电图（EOG）作为非标准非侵入性接口，帮助老年人和残障人士与智能家居技术（SHT）进行多模态交互的初步见解。", "motivation": "该研究旨在为老年人、残障人士及其他在主流技术讨论中处于劣势的群体，探索有效、非标准、非侵入性的智能家居技术（SHT）交互接口，作为更广泛的“Sagacity”智能家居生态系统项目的一部分。", "method": "该研究基于综合SHT管理系统Sagacity原型，通过深入的互动实践研讨会，让包括老年人和残障人士在内的18名潜在终端用户直接参与，探索并研究了基于生物电信号（特别是EMG和EOG）作为SHT补充界面的潜力。", "result": "研究的初步见解揭示了EMG/EOG接口在多模态SHT管理中的潜力，同时也指出了当前技术状态的局限性和挑战，并提出了设计多模态交互范式的建议，明确了未来研究的重点领域。", "conclusion": "肌电图（EMG）和眼电图（EOG）接口在多模态智能家居技术（SHT）管理中具有潜力，但仍面临技术局限性和挑战，需要进一步研究以优化多模态交互范式。", "translation": "我们报告了一项关于智能家居技术（SHT）非标准非侵入性接口的探索性研究的初步见解。这项研究是旨在为老年人、残障人士以及在主流技术讨论中处于劣势的其他群体提供有效智能家居生态系统Sagacity的更广泛研究项目的一部分。因此，这项研究符合Living Lab Kobo的HASE研究组（科学与工程中的人类方面）的长期研究框架。在我们的研究中，基于综合SHT管理系统Sagacity的原型，我们调查了生物电信号，特别是肌电图（EMG）和眼电图（EOG），作为SHT补充接口的潜力。基于我们之前关于多模态接口（包括VUI和BCI）的参与式研究和调查，我们准备了深入的互动实践研讨会，让包括老年人和残障人士在内的各类潜在终端用户（共18名受试者）直接参与，以探索和调查基于此类非标准接口解决方案的潜力。这项研究的初步见解揭示了EMG/EOG接口在多模态SHT管理中的潜力，以及当前技术状态带来的局限性和挑战，并提出了设计多模态交互范式的建议，明确了未来研究中需要关注的领域。", "summary": "本研究是一项探索性工作，旨在为老年人和残障人士等弱势群体开发非标准、非侵入性的智能家居技术（SHT）交互接口。研究基于Sagacity系统原型，通过18名受试者的参与式研讨会，评估了生物电信号（EMG和EOG）作为SHT补充接口的潜力。初步结果表明EMG/EOG接口在多模态SHT管理中具有前景，但也识别了当前技术局限性和未来研究方向。", "keywords": "智能家居技术, EMG, EOG, 多模态交互, 老年人, 残障人士", "comments": "该研究的创新之处在于关注了弱势群体（老年人和残障人士）在智能家居技术交互中的需求，并探索了非标准、非侵入性的生物电信号（EMG/EOG）作为新的交互方式。其重要性在于为实现更普惠的智能家居生态系统提供了初步的技术可行性验证。然而，作为一项探索性研究，其局限性在于样本量较小（18名受试者），且报告的是初步见解，需要更深入、更大规模的研究来验证其长期有效性和可用性。"}}
{"id": "2507.18733", "title": "Max-Min Rate Optimization for Multigroup Multicast MISO Systems Via Novel Transmissive RIS Transceiver", "authors": ["Yuan Guo", "Wen Chen", "Qingqing Wu", "Yanze Zhu", "Yang Liu", "Zhendong Li", "Ying Wang"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18733v1", "summary": "This paper investigates a novel transmissive reconfigurable intelligent\nsurface (RIS) transceiver architectureenabled multigroup multicast downlink\ncommunication system. Under this setup, an optimization problem is formulated\nto maximize the minimum rate of users across all groups, subject to the maximum\navailable power of each RIS unit. Due to the nondifferentiable nature of the\nobjective function, the max-min rate problem is challenging to solve. To tackle\nthis difficult problem, we develop an iterative solution by leveraging the\nsuccessive convex approximation (SCA) and the penalty function method. However,\nthe above approach has high computational complexity and may lead to\ncompromised performance. To overcome these drawbacks, we design an efficient\nsecond-order cone programming (SOCP)-based method using the weighted minimum\nmean squared error (WMMSE) framework to reduce computational complexity.\nFurthermore, to further reduce the computational complexity, we also propose a\nlow-complexity and solver-free algorithm that analytically updates all\nvariables by combining the smooth approximation theory and the\nmajorization-minimization (MM) method. Numerical results are provided to verify\nthe convergence and effectiveness of our proposed three algorithms. It is also\ndemonstrated that the SOCP-based method outperforms the penalty-based algorithm\nin terms of both the achieved min rate and the computational complexity. In\ncontrast, the lowcomplexity design achieves significantly lower complexity with\nonly slightly degraded performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18733v1", "cate": "eess.SP", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于新型透射式RIS收发器的多组多播MISO系统最大最小速率优化", "tldr": "本文研究了基于新型透射式RIS收发器的多组多播MISO系统中的最大最小速率优化问题，并提出了三种算法进行求解。数值结果验证了算法的有效性，其中SOCP方法在性能和复杂度上表现最佳，而低复杂度方法在性能略有下降的情况下显著降低了复杂度。", "motivation": "本文研究了在新型透射式可重构智能表面（RIS）收发器架构下，多组多播下行通信系统中最大化所有用户最小速率的优化问题。由于目标函数的不可微性，解决这个最大最小速率问题具有挑战性。", "method": "论文提出了三种迭代解决方案：1. 利用逐次凸逼近（SCA）和罚函数法。2. 设计了一种基于二阶锥规划（SOCP）的方法，该方法使用加权最小均方误差（WMMSE）框架以降低计算复杂度。3. 提出了一种低复杂度、无需求解器的算法，通过结合平滑逼近理论和主化-最小化（MM）方法解析地更新所有变量。", "result": "数值结果验证了所提出的三种算法的收敛性和有效性。SOCP方法在实现的最小速率和计算复杂度方面均优于基于罚函数的算法。低复杂度设计在性能略有下降的情况下实现了显著更低的复杂度。", "conclusion": "论文成功地提出了三种算法来解决透射式RIS收发器架构下多组多播MISO系统的最大最小速率优化问题，并通过数值结果验证了它们的有效性。SOCP方法在性能和复杂度之间提供了更好的平衡，而低复杂度算法为需要极低计算开销的场景提供了可行方案。", "translation": "本文研究了一种新型透射式可重构智能表面（RIS）收发器架构支持的多组多播下行通信系统。在此设置下，提出了一个优化问题，旨在最大化所有组用户的最小速率，同时受限于每个RIS单元的最大可用功率。由于目标函数的不可微性，最大最小速率问题难以解决。为了解决这个难题，我们利用逐次凸逼近（SCA）和罚函数法开发了一种迭代解决方案。然而，上述方法计算复杂度高，并且可能导致性能受损。为了克服这些缺点，我们设计了一种高效的基于二阶锥规划（SOCP）的方法，该方法使用加权最小均方误差（WMMSE）框架来降低计算复杂度。此外，为了进一步降低计算复杂度，我们还提出了一种低复杂度、无需求解器的算法，该算法通过结合平滑逼近理论和主化-最小化（MM）方法解析地更新所有变量。提供了数值结果以验证我们提出的三种算法的收敛性和有效性。结果还表明，基于SOCP的方法在实现的最小速率和计算复杂度方面均优于基于罚函数的算法。相比之下，低复杂度设计在性能略有下降的情况下实现了显著更低的复杂度。", "summary": "本文研究了基于新型透射式RIS收发器的多组多播MISO系统中的最大最小速率优化问题。针对目标函数不可微的挑战，论文提出了三种迭代算法：基于SCA和罚函数的方法、基于SOCP和WMMSE框架的方法，以及一种低复杂度、无需求解器的解析更新算法。数值结果表明，SOCP方法在性能和复杂度上优于罚函数方法，而低复杂度设计在性能略有损失的情况下显著降低了计算复杂度。", "keywords": "透射式RIS, 最大最小速率优化, 多组多播, SOCP, 低复杂度算法", "comments": "本文的创新点在于提出了新型透射式RIS收发器架构下的最大最小速率优化问题，并针对其非凸性提出了三种不同权衡的解决方案。特别地，引入SOCP和WMMSE框架以及低复杂度解析算法，体现了在性能和计算复杂度之间进行权衡的深入思考。其重要性在于为未来RIS辅助通信系统的实际部署提供了潜在的优化算法。"}}
{"id": "2507.18952", "title": "Legal Document Summarization: Enhancing Judicial Efficiency through Automation Detection", "authors": ["Yongjie Li", "Ruilin Nong", "Jianan Liu", "Lucas Evans"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18952v1", "summary": "Legal document summarization represents a significant advancement towards\nimproving judicial efficiency through the automation of key information\ndetection. Our approach leverages state-of-the-art natural language processing\ntechniques to meticulously identify and extract essential data from extensive\nlegal texts, which facilitates a more efficient review process. By employing\nadvanced machine learning algorithms, the framework recognizes underlying\npatterns within judicial documents to create precise summaries that encapsulate\nthe crucial elements. This automation alleviates the burden on legal\nprofessionals, concurrently reducing the likelihood of overlooking vital\ninformation that could lead to errors. Through comprehensive experiments\nconducted with actual legal datasets, we demonstrate the capability of our\nmethod to generate high-quality summaries while preserving the integrity of the\noriginal content and enhancing processing times considerably. The results\nreveal marked improvements in operational efficiency, allowing legal\npractitioners to direct their efforts toward critical analytical and\ndecision-making activities instead of manual reviews. This research highlights\npromising technology-driven strategies that can significantly alter workflow\ndynamics within the legal sector, emphasizing the role of automation in\nrefining judicial processes.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18952v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "法律文件摘要：通过自动化检测提高司法效率", "tldr": "本文提出了一种利用先进自然语言处理和机器学习技术自动生成法律文件摘要的方法，旨在提高司法效率，减轻法律专业人员的负担，并通过实验证明了其有效性。", "motivation": "本研究的动机是通过自动化关键信息检测来改进司法效率。现有的法律文本审查过程耗时且容易遗漏重要信息，给法律专业人员带来沉重负担，因此需要一种能自动提取和总结法律文本关键信息的方法。", "method": "本研究利用先进的自然语言处理（NLP）技术来识别和提取法律文本中的基本数据。通过采用先进的机器学习算法，该框架能够识别司法文件中的潜在模式，从而创建精确的摘要。该方法旨在生成高质量的摘要，同时保持原始内容的完整性。", "result": "实验结果表明，该方法能够生成高质量的摘要，同时保持原始内容的完整性并显著提高处理时间。结果揭示了操作效率的显著提高，使法律从业者能够将精力集中在关键的分析和决策活动上，而不是手动审查。", "conclusion": "这项研究强调了有前景的技术驱动策略，这些策略可以显著改变法律领域的工作流程动态，强调了自动化在完善司法流程中的作用。", "translation": "法律文件摘要代表了通过自动化关键信息检测来提高司法效率的重大进步。我们的方法利用最先进的自然语言处理技术，从大量的法律文本中细致地识别和提取基本数据，从而促进更高效的审查过程。通过采用先进的机器学习算法，该框架能够识别司法文件中的潜在模式，从而创建包含关键要素的精确摘要。这种自动化减轻了法律专业人员的负担，同时降低了遗漏可能导致错误的重要信息的可能性。通过对实际法律数据集进行的全面实验，我们证明了我们的方法在生成高质量摘要的同时，能够保持原始内容的完整性并显著提高处理时间。结果揭示了操作效率的显著提高，使法律从业者能够将精力集中在关键的分析和决策活动上，而不是手动审查。这项研究强调了有前景的技术驱动策略，这些策略可以显著改变法律领域的工作流程动态，强调了自动化在完善司法流程中的作用。", "summary": "本文提出了一种利用自然语言处理和机器学习技术对法律文件进行自动化摘要的方法。该方法旨在从大量法律文本中识别和提取关键信息，从而生成精确的摘要，以提高司法效率并减轻法律专业人员的负担。通过在实际法律数据集上的实验证明，该方法能够生成高质量的摘要，同时保持内容完整性并显著提升处理速度，最终使法律从业者能更专注于分析和决策。", "keywords": "法律文件摘要, 司法效率, 自动化检测, 自然语言处理, 机器学习", "comments": "该论文提出了一种利用先进NLP和机器学习技术进行法律文件摘要的创新方法，其重要性在于能够显著提升司法效率，减轻法律专业人员的工作负担，并降低错误率。通过自动化关键信息检测和摘要生成，该研究为法律领域的工作流程数字化转型提供了有前景的解决方案。"}}
{"id": "2507.14241", "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models", "authors": ["Rithesh Murthy", "Ming Zhu", "Liangwei Yang", "Jielin Qiu", "Juntao Tan", "Shelby Heinecke", "Caiming Xiong", "Silvio Savarese", "Huan Wang"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14241v3", "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14241v3", "cate": "cs.CL", "date": "2025-07-17", "updated": "2025-07-24", "AI": {"title_translation": "Promptomatix：一个大型语言模型的自动提示优化框架", "tldr": "Promptomatix是一个自动提示优化框架，它能将自然语言任务描述转化为高质量提示，无需手动调优或领域专业知识。", "motivation": "大型语言模型（LLMs）在精心设计的提示下表现最佳，但提示工程仍然是手动的、不一致的，且非专家难以使用。", "method": "Promptomatix是一个自动提示优化框架，支持基于轻量级元提示的优化器和由DSPy驱动的编译器，采用模块化设计。该系统分析用户意图，生成合成训练数据，选择提示策略，并使用成本感知目标优化提示。", "result": "在5个任务类别中，Promptomatix与现有库相比，实现了具有竞争力或更优的性能，同时减少了提示长度和计算开销。", "conclusion": "Promptomatix通过自动化提示优化，使其更具可扩展性和效率，并能生成高质量提示，无需人工干预或专业知识。", "translation": "大型语言模型（LLMs）在精心设计的提示下表现最佳，但提示工程仍然是手动的、不一致的，且非专家难以使用。我们引入了Promptomatix，一个自动提示优化框架，它能将自然语言任务描述转化为高质量提示，无需手动调优或领域专业知识。Promptomatix支持轻量级元提示优化器和由DSPy驱动的编译器，其模块化设计使其未来可扩展到更高级的框架。该系统分析用户意图，生成合成训练数据，选择提示策略，并使用成本感知目标优化提示。在5个任务类别中，Promptomatix与现有库相比，实现了具有竞争力或更优的性能，同时减少了提示长度和计算开销，使提示优化具有可扩展性和效率。", "summary": "Promptomatix是一个创新的自动提示优化框架，旨在解决大型语言模型提示工程中手动、不一致和非专家难以使用的问题。该框架能够将自然语言任务描述自动转化为高质量提示，无需人工干预或领域专业知识。它集成了基于元提示的优化器和DSPy编译器，通过分析用户意图、生成合成数据、选择策略并使用成本感知目标来优化提示。实验结果表明，Promptomatix在性能上与现有库相当或更优，同时显著降低了提示长度和计算成本，提高了提示优化的可扩展性和效率。", "keywords": "提示优化, 大型语言模型, 自动化框架, 提示工程, Promptomatix", "comments": "Promptomatix的创新之处在于其自动化的提示优化方法，极大地降低了LLM提示工程的门槛，使其对非专家用户更友好。通过支持两种优化器和模块化设计，该框架具有良好的扩展性。其在减少提示长度和计算开销方面的表现，也提升了提示优化的实用性和效率，对于LLM的应用普及具有重要意义。"}}
{"id": "2507.18639", "title": "People Are Highly Cooperative with Large Language Models, Especially When Communication Is Possible or Following Human Interaction", "authors": ["Paweł Niszczota", "Tomasz Grzegorczyk", "Alexander Pastukhov"], "categories": ["cs.HC", "cs.CL", "cs.CY", "econ.GN", "q-fin.EC", "I.2.7; H.5.2; H.5.3; K.4.3"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18639v1", "summary": "Machines driven by large language models (LLMs) have the potential to augment\nhumans across various tasks, a development with profound implications for\nbusiness settings where effective communication, collaboration, and stakeholder\ntrust are paramount. To explore how interacting with an LLM instead of a human\nmight shift cooperative behavior in such settings, we used the Prisoner's\nDilemma game -- a surrogate of several real-world managerial and economic\nscenarios. In Experiment 1 (N=100), participants engaged in a thirty-round\nrepeated game against a human, a classic bot, and an LLM (GPT, in real-time).\nIn Experiment 2 (N=192), participants played a one-shot game against a human or\nan LLM, with half of them allowed to communicate with their opponent, enabling\nLLMs to leverage a key advantage over older-generation machines. Cooperation\nrates with LLMs -- while lower by approximately 10-15 percentage points\ncompared to interactions with human opponents -- were nonetheless high. This\nfinding was particularly notable in Experiment 2, where the psychological cost\nof selfish behavior was reduced. Although allowing communication about\ncooperation did not close the human-machine behavioral gap, it increased the\nlikelihood of cooperation with both humans and LLMs equally (by 88%), which is\nparticularly surprising for LLMs given their non-human nature and the\nassumption that people might be less receptive to cooperating with machines\ncompared to human counterparts. Additionally, cooperation with LLMs was higher\nfollowing prior interaction with humans, suggesting a spillover effect in\ncooperative behavior. Our findings validate the (careful) use of LLMs by\nbusinesses in settings that have a cooperative component.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18639v1", "cate": "cs.HC", "date": "2025-05-10", "updated": "2025-05-10", "AI": {"title_translation": "人们与大型语言模型高度合作，尤其是在可以交流或在人类互动之后", "tldr": "人们与大型语言模型（LLM）的合作度很高，尤其是在可交流或有人类互动后，尽管比人类间合作略低。", "motivation": "探索在商业环境中，与大型语言模型（LLM）而非人类互动如何影响合作行为，这对于LLM在商业应用中的有效沟通、协作和信任至关重要。", "method": "使用囚徒困境游戏。实验1（N=100）中，参与者与人类、经典机器人和LLM（GPT）进行30轮重复游戏。实验2（N=192）中，参与者与人类或LLM进行一次性游戏，其中一半允许与对手交流。", "result": "与LLM的合作率很高，尽管比与人类对手低约10-15个百分点。在实验2中，即使自私行为的心理成本降低，合作率也显著。允许交流使与人类和LLM的合作可能性均增加了88%。与LLM的合作在与人类有过互动后更高，表明存在溢出效应。", "conclusion": "研究结果验证了在具有合作成分的商业环境中谨慎使用LLM的有效性。", "translation": "由大型语言模型（LLM）驱动的机器有潜力在各种任务中增强人类，这一发展对商业环境具有深远影响，在商业环境中，有效的沟通、协作和利益相关者信任至关重要。为了探索在这些环境中与LLM而非人类互动如何改变合作行为，我们使用了囚徒困境游戏——这是几个现实世界管理和经济情景的替代。在实验1（N=100）中，参与者与人类、一个经典机器人和一个LLM（GPT，实时）进行了三十轮重复游戏。在实验2（N=192）中，参与者与人类或LLM进行了一次性游戏，其中一半允许与对手交流，从而使LLM能够利用其相对于老一代机器的关键优势。与LLM的合作率——尽管比与人类对手的互动低约10-15个百分点——但仍然很高。这一发现在实验2中尤其显著，在该实验中，自私行为的心理成本降低了。尽管允许就合作进行交流并未弥合人机行为差距，但它同样增加了与人类和LLM合作的可能性（提高了88%），考虑到LLM的非人类性质以及人们可能不如与人类对手合作那样乐于与机器合作的假设，这一点尤其令人惊讶。此外，在与人类有过先前互动后，与LLM的合作更高，这表明合作行为存在溢出效应。我们的发现验证了企业在具有合作成分的环境中（谨慎）使用LLM的有效性。", "summary": "这项研究通过囚徒困境游戏探讨了人们与大型语言模型（LLM）的合作行为。结果显示，尽管与人类对手相比，与LLM的合作率略低，但总体上仍然很高。尤其是在允许沟通或在与人类互动后，与LLM的合作意愿显著提升。研究验证了LLM在商业合作场景中的潜在应用价值。", "keywords": "大型语言模型, 合作行为, 囚徒困境, 人机交互, 沟通效应", "comments": "这项研究创新性地通过博弈论方法，量化了人机合作中的关键因素。其重要性在于，它为LLM在商业和管理场景中的应用提供了实证支持，尤其强调了沟通和先前人类互动对提升人机合作的积极作用。这对于理解和设计未来人机协作系统具有重要指导意义。"}}
{"id": "2507.18921", "title": "HQ-SMem: Video Segmentation and Tracking Using Memory Efficient Object Embedding With Selective Update and Self-Supervised Distillation Feedback", "authors": ["Elham Soltani Kazemi", "Imad Eddine Toubal", "Gani Rahmon", "Jaired Collins", "K. Palaniappan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      submit/6651762", "url": "http://arxiv.org/abs/2507.18921v1", "summary": "Video Object Segmentation (VOS) is foundational to numerous computer vision\napplications, including surveillance, autonomous driving, robotics and\ngenerative video editing. However, existing VOS models often struggle with\nprecise mask delineation, deformable objects, topologically transforming\nobjects, tracking drift and long video sequences. In this paper, we introduce\nHQ-SMem, for High Quality video segmentation and tracking using Smart Memory, a\nnovel method that enhances the performance of VOS base models by addressing\nthese limitations. Our approach incorporates three key innovations: (i)\nleveraging SAM with High-Quality masks (SAM-HQ) alongside appearance-based\ncandidate-selection to refine coarse segmentation masks, resulting in improved\nobject boundaries; (ii) implementing a dynamic smart memory mechanism that\nselectively stores relevant key frames while discarding redundant ones, thereby\noptimizing memory usage and processing efficiency for long-term videos; and\n(iii) dynamically updating the appearance model to effectively handle complex\ntopological object variations and reduce drift throughout the video. These\ncontributions mitigate several limitations of existing VOS models including,\ncoarse segmentations that mix-in background pixels, fixed memory update\nschedules, brittleness to drift and occlusions, and prompt ambiguity issues\nassociated with SAM. Extensive experiments conducted on multiple public\ndatasets and state-of-the-art base trackers demonstrate that our method\nconsistently ranks among the top two on VOTS and VOTSt 2024 datasets. Moreover,\nHQ-SMem sets new benchmarks on Long Video Dataset and LVOS, showcasing its\neffectiveness in challenging scenarios characterized by complex multi-object\ndynamics over extended temporal durations.", "comment": "submit/6651762", "pdf_url": "http://arxiv.org/pdf/2507.18921v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "HQ-SMem：使用选择性更新和自监督蒸馏反馈的内存高效对象嵌入进行视频分割和跟踪", "tldr": "HQ-SMem通过智能内存、高质量掩码细化和动态外观模型更新，解决了现有VOS模型的局限性，实现了高性能视频分割和跟踪。", "motivation": "现有视频对象分割（VOS）模型在精确掩码描绘、可变形对象、拓扑变换对象、跟踪漂移和长视频序列方面存在困难。", "method": "HQ-SMem引入了三项关键创新：1) 利用SAM-HQ和基于外观的候选选择来细化粗糙分割掩码；2) 实现动态智能内存机制，选择性存储关键帧以优化内存和处理效率；3) 动态更新外观模型以处理复杂拓扑对象变化并减少漂移。", "result": "广泛的实验表明，HQ-SMem在VOTS和VOTSt 2024数据集上始终位列前两名，并在长视频数据集和LVOS上创造了新基准，展示了其在复杂多对象动态和长时间跨度等挑战性场景中的有效性。", "conclusion": "HQ-SMem通过其创新方法有效解决了现有VOS模型的局限性，并在视频分割和跟踪方面取得了领先性能，尤其是在处理长视频和复杂场景时表现出色。", "translation": "视频对象分割（VOS）是许多计算机视觉应用的基础，包括监控、自动驾驶、机器人和生成式视频编辑。然而，现有的VOS模型在精确掩码描绘、可变形对象、拓扑变换对象、跟踪漂移和长视频序列方面常常面临挑战。在本文中，我们引入了HQ-SMem，即使用智能内存进行高质量视频分割和跟踪的方法，这是一种通过解决上述局限性来增强VOS基础模型性能的新方法。我们的方法包含三项关键创新：(i) 利用带有高质量掩码的SAM（SAM-HQ）以及基于外观的候选选择来细化粗糙分割掩码，从而改善对象边界；(ii) 实施动态智能内存机制，选择性地存储相关关键帧，同时丢弃冗余帧，从而优化长视频的内存使用和处理效率；(iii) 动态更新外观模型，以有效处理复杂拓扑对象变化并减少整个视频中的漂移。这些贡献缓解了现有VOS模型的多个局限性，包括混入背景像素的粗糙分割、固定的内存更新计划、对漂移和遮挡的脆弱性，以及与SAM相关的提示歧义问题。在多个公共数据集和最先进的基础跟踪器上进行的广泛实验表明，我们的方法在VOTS和VOTSt 2024数据集上始终位列前两名。此外，HQ-SMem在长视频数据集和LVOS上树立了新基准，展示了其在复杂多对象动态和长时间跨度等挑战性场景中的有效性。", "summary": "HQ-SMem是一种新颖的视频对象分割和跟踪方法，旨在解决现有VOS模型在精确度、内存效率和长期跟踪稳定性方面的不足。它通过结合SAM-HQ进行精细掩码细化、动态智能内存管理以优化长视频处理，以及动态外观模型更新来应对对象变化和减少漂移。实验证明，HQ-SMem在多个基准数据集上表现优异，尤其在长视频和复杂场景中设立了新标准。", "keywords": "视频对象分割, 跟踪, 智能内存, SAM-HQ, 长视频", "comments": "这篇论文引入的HQ-SMem具有创新性，因为它采用了三管齐下的方法：利用SAM-HQ实现高质量掩码，动态智能内存提高效率，以及动态外观模型更新增强鲁棒性。其重要性在于解决了VOS中长期存在的挑战，特别是针对长视频和复杂场景，其在基准测试中的领先排名和新基准的建立证明了这一点。"}}
{"id": "2507.18850", "title": "Estimating Sensitivity Maps for X-Nuclei Magnetic Resonance Spectroscopic Imaging", "authors": ["Nicholas Dwork", "Jeremy W. Gordon", "Shuyu Tang", "Peder E. Z. Larson"], "categories": ["eess.IV", "physics.med-ph", "q-bio.QM"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18850v1", "summary": "The purpose of this research is to estimate sensitivity maps when imaging\nX-nuclei that may not have a significant presence throughout the field of view.\nWe propose to estimate the coil's sensitivities by solving a least-squares\nproblem where each row corresponds to an individual estimate of the sensitivity\nfor a given voxel. Multiple estimates come from the multiple bins of the\nspectrum with spectroscopy, multiple times with dynamic imaging, or multiple\nfrequencies when utilizing spectral excitation. The method presented in this\nmanuscript, called the L2 optimal method, is compared to the commonly used\nRefPeak method which uses the spectral bin with the highest energy to estimate\nthe sensitivity maps. The L2 optimal method yields more accurate sensitivity\nmaps when imaging a numerical phantom and is shown to yield a higher\nsignal-to-noise ratio when imaging the brain, pancreas, and heart with\nhyperpolarized pyruvate as the contrast agent with hyperpolarized MRI. The L2\noptimal method is able to better estimate the sensitivity by extracting more\ninformation from the measurements.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18850v1", "cate": "eess.IV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "X核磁共振波谱成像灵敏度图估计", "tldr": "本文提出L2最优方法，通过解决最小二乘问题并利用频谱的多个信息源，更准确地估计X核磁共振成像的灵敏度图，相比传统RefPeak方法能获得更高的信噪比和准确性。", "motivation": "在对X核进行成像时估计灵敏度图，尤其是当X核在整个视野中可能不显著存在时。传统方法RefPeak仅使用最高能量的谱箱进行估计，存在局限性。", "method": "本文提出L2最优方法，通过解决一个最小二乘问题来估计线圈的灵敏度。该方法利用来自波谱的多个谱箱、动态成像的多个时间或利用频谱激励时的多个频率的多个估计，从而从测量中提取更多信息。该方法与常用的RefPeak方法（使用最高能量谱箱）进行了比较。", "result": "L2最优方法在对数值体模成像时产生了更准确的灵敏度图。在使用超极化丙酮酸作为对比剂进行超极化MRI对大脑、胰腺和心脏成像时，L2最优方法显示出更高的信噪比。", "conclusion": "L2最优方法能够通过从测量中提取更多信息来更好地估计灵敏度图，从而在X核磁共振波谱成像中提供比传统RefPeak方法更准确的灵敏度图和更高的信噪比。", "translation": "本研究的目的是在对X核进行成像时估计灵敏度图，这些X核在整个视野中可能不存在显著。我们提出通过解决一个最小二乘问题来估计线圈的灵敏度，其中每一行对应于给定体素灵敏度的个体估计。多个估计来自波谱的多个谱箱、动态成像的多个时间，或利用频谱激励时的多个频率。本手稿中提出的方法，称为L2最优方法，与常用RefPeak方法进行比较，后者使用最高能量的谱箱来估计灵敏度图。L2最优方法在对数值体模成像时产生了更准确的灵敏度图，并且在用超极化丙酮酸作为对比剂进行超极化MRI对大脑、胰腺和心脏成像时，显示出更高的信噪比。L2最优方法能够通过从测量中提取更多信息来更好地估计灵敏度。", "summary": "本文提出了一种名为L2最优的新方法，用于估计X核磁共振波谱成像中的灵敏度图。该方法通过解决最小二乘问题，并整合来自频谱多个谱箱、动态成像多个时间点或频谱激励多个频率的信息，从而更全面地提取测量数据中的信息。与仅依赖最高能量谱箱的传统RefPeak方法相比，L2最优方法在数值体模成像中表现出更高的灵敏度图准确性，并在使用超极化丙酮酸作为对比剂的超极化MRI中，对大脑、胰腺和心脏成像时展现出更高的信噪比。", "keywords": "灵敏度图, X核, 磁共振波谱成像, L2最优方法, 超极化MRI", "comments": "本文提出了一种创新方法，通过利用更丰富的数据信息（如多个谱箱、时间或频率），显著改进了X核磁共振成像中灵敏度图的估计。这种基于最小二乘的L2最优方法克服了传统单峰值方法的局限性，在提高灵敏度图准确性和信噪比方面表现出色，对于超极化MRI的定量应用具有重要意义和潜在临床价值。"}}
{"id": "2409.07237", "title": "Negative Sampling in Recommendation: A Survey and Future Directions", "authors": ["Haokai Ma", "Ruobing Xie", "Lei Meng", "Fuli Feng", "Xiaoyu Du", "Xingwu Sun", "Zhanhui Kang", "Xiangxu Meng"], "categories": ["cs.IR"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      39 pages, 10 figures; Under review", "url": "http://arxiv.org/abs/2409.07237v2", "summary": "Recommender system (RS) aims to capture personalized preferences from massive\nuser behaviors, making them pivotal in the era of information explosion.\nHowever, the presence of ``information cocoons'', interaction sparsity,\ncold-start problem and feedback loops inherent in RS make users interact with a\nlimited number of items. Conventional recommendation algorithms typically focus\non the positive historical behaviors, while neglecting the essential role of\nnegative feedback in user preference understanding. As a promising but\neasy-to-ignored area, negative sampling is proficients in revealing the genuine\nnegative aspect inherent in user behaviors, emerging as an inescapable\nprocedure in RS. In this survey, we first discuss existing user feedback, the\ncritical role of negative sampling and the optimization objectives in RS and\nthoroughly analyze challenges that consistently impede its progress. Then, we\nconduct an extensive literature review on the existing negative sampling\nstrategies in RS and classify them into five categories with their discrepant\ntechniques. Finally, we detail the insights of the tailored negative sampling\nstrategies in diverse RS scenarios and outline an overview of the prospective\nresearch directions toward which the community may engage and benefit.", "comment": "39 pages, 10 figures; Under review", "pdf_url": "http://arxiv.org/pdf/2409.07237v2", "cate": "cs.IR", "date": "2024-09-11", "updated": "2025-07-25", "AI": {"title_translation": "推荐系统中的负采样：一项综述与未来方向", "tldr": "该综述系统地回顾了推荐系统中的负采样策略，将其分为五类，并探讨了未来研究方向，强调了负采样在解决信息茧房和数据稀疏性问题中的关键作用。", "motivation": "推荐系统面临“信息茧房”、交互稀疏性、冷启动问题和反馈循环等挑战。传统推荐算法忽视了负反馈的重要性，而负采样能够揭示用户行为中真实的负面信息，是理解用户偏好不可或缺的步骤。", "method": "本综述首先讨论了现有用户反馈、负采样的关键作用和推荐系统中的优化目标，并分析了其进展所面临的挑战。然后，对推荐系统中现有的负采样策略进行了广泛的文献回顾，并根据不同技术将其分为五类。最后，详细阐述了在不同推荐系统场景中定制负采样策略的见解，并概述了未来的研究方向。", "result": "该综述将推荐系统中现有的负采样策略分为五大类，并详细阐述了在不同推荐系统场景中定制负采样策略的见解，同时概述了未来的研究方向。", "conclusion": "负采样在推荐系统中扮演着关键角色，本综述为该领域提供了全面的概述和未来的研究路线图。", "translation": "推荐系统（RS）旨在从海量用户行为中捕捉个性化偏好，使其在信息爆炸时代变得至关重要。然而，RS固有的“信息茧房”、交互稀疏性、冷启动问题和反馈循环，使得用户只能与有限数量的物品进行交互。传统的推荐算法通常只关注正向历史行为，却忽视了负反馈在理解用户偏好中的重要作用。负采样作为一个有前景但易被忽视的领域，擅长揭示用户行为中固有的真实负面信息，正成为RS中不可或缺的步骤。在这项综述中，我们首先讨论了现有的用户反馈、负采样的关键作用以及RS中的优化目标，并彻底分析了持续阻碍其进展的挑战。然后，我们对RS中现有的负采样策略进行了广泛的文献回顾，并根据其不同的技术将其分为五类。最后，我们详细阐述了在不同RS场景中定制负采样策略的见解，并概述了社区可能参与并受益的未来研究方向。", "summary": "本综述系统地回顾了推荐系统中的负采样策略。它强调了传统推荐系统面临的挑战（如信息茧房和稀疏性）以及负采样在理解用户偏好中的关键作用。论文将现有的负采样方法分为五类，分析了它们的技术，并提供了在各种场景中应用定制策略的见解。最后，它概述了该领域的未来研究方向。", "keywords": "负采样, 推荐系统, 综述, 用户偏好, 信息茧房", "comments": "该综述论文具有重要意义，因为它关注了推荐系统中一个关键但常被忽视的方面——负采样。它提供了结构化的分类和未来的研究路线图，这对于研究人员和实践者都非常有价值。其贡献在于综合现有知识并识别未来的挑战和机遇。"}}
{"id": "2507.17307", "title": "R-Stitch: Dynamic Trajectory Stitching for Efficient Reasoning", "authors": ["Zhuokun Chen", "Zeren Chen", "Jiahao He", "Mingkui Tan", "Jianfei Cai", "Bohan Zhuang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17307v2", "summary": "Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of\nlarge language models by encouraging step-by-step intermediate reasoning during\ninference. While effective, CoT introduces substantial computational overhead\ndue to its reliance on autoregressive decoding over long token sequences.\nExisting acceleration strategies either reduce sequence length through early\nstopping or compressive reward designs, or improve decoding speed via\nspeculative decoding with smaller models. However, speculative decoding suffers\nfrom limited speedup when the agreement between small and large models is low,\nand fails to exploit the potential advantages of small models in producing\nconcise intermediate reasoning. In this paper, we present R-Stitch, a\ntoken-level, confidence-based hybrid decoding framework that accelerates CoT\ninference by switching between a small language model (SLM) and a large\nlanguage model (LLM) along the reasoning trajectory. R-Stitch uses the SLM to\ngenerate tokens by default and delegates to the LLM only when the SLM's\nconfidence falls below a threshold. This design avoids full-sequence rollback\nand selectively invokes the LLM on uncertain steps, preserving both efficiency\nand answer quality. R-Stitch is model-agnostic, training-free, and compatible\nwith standard decoding pipelines. Experiments on math reasoning benchmarks\ndemonstrate that R-Stitch achieves up to 85\\% reduction in inference latency\nwith negligible accuracy drop, highlighting its practical effectiveness in\naccelerating CoT reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17307v2", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-24", "AI": {"title_translation": "R-Stitch：高效推理的动态轨迹拼接", "tldr": "R-Stitch是一种混合解码框架，通过在SLM和LLM之间根据置信度动态切换，显著加速CoT推理，同时保持准确性。", "motivation": "链式思考（CoT）推理虽然有效，但由于依赖长序列自回归解码，引入了巨大的计算开销。现有加速策略（如推测解码）在大小模型一致性低时加速有限，且未能充分利用小型模型生成简洁中间推理的优势。", "method": "本文提出了R-Stitch，一个基于token级别、置信度的混合解码框架，通过在推理轨迹中小型语言模型（SLM）和大型语言模型（LLM）之间切换来加速CoT推理。R-Stitch默认使用SLM生成token，仅当SLM的置信度低于阈值时才将任务委托给LLM。这种设计避免了全序列回滚，并选择性地在不确定步骤调用LLM。", "result": "在数学推理基准测试中，R-Stitch将推理延迟降低了高达85%，同时准确性下降可忽略不计。", "conclusion": "R-Stitch在加速CoT推理方面表现出显著的实用有效性。", "translation": "链式思考（CoT）推理通过在推理过程中鼓励逐步的中间推理，增强了大型语言模型的解决问题能力。尽管CoT有效，但由于其依赖于长token序列上的自回归解码，引入了大量的计算开销。现有的加速策略要么通过提前停止或压缩奖励设计来减少序列长度，要么通过使用较小模型进行推测解码来提高解码速度。然而，当小型模型和大型模型之间的一致性较低时，推测解码的加速效果有限，并且未能利用小型模型在生成简洁中间推理方面的潜在优势。在本文中，我们提出了R-Stitch，一个基于token级别、置信度的混合解码框架，通过在推理轨迹中小型语言模型（SLM）和大型语言模型（LLM）之间切换来加速CoT推理。R-Stitch默认使用SLM生成token，仅当SLM的置信度低于阈值时才将任务委托给LLM。这种设计避免了全序列回滚，并选择性地在不确定步骤调用LLM，从而同时保持效率和答案质量。R-Stitch是模型无关的、无需训练的，并且兼容标准解码流程。在数学推理基准测试上的实验表明，R-Stitch在推理延迟方面实现了高达85%的降低，而准确性下降可忽略不计，突显了其在加速CoT推理方面的实用有效性。", "summary": "本文介绍了R-Stitch，一个用于加速链式思考（CoT）推理的创新混合解码框架。针对CoT推理计算开销大的问题，R-Stitch通过在推理过程中智能地切换小型语言模型（SLM）和大型语言模型（LLM）来解决。它默认使用SLM生成token，仅当SLM的置信度低于预设阈值时才调用LLM。这种基于置信度的动态切换策略避免了全序列回滚，显著提高了效率，同时保持了推理质量。R-Stitch具有模型无关、无需训练和兼容性强的特点。实验结果表明，该方法在保持准确性的前提下，将推理延迟大幅降低了85%。", "keywords": "链式思考, 推理加速, 混合解码, 大型语言模型, 小型语言模型", "comments": "R-Stitch的创新点在于其基于token级别的置信度动态切换机制，有效地结合了小型模型的高效性和大型模型的准确性。这种方法避免了传统推测解码中可能出现的全序列回滚问题，显著提升了CoT推理的效率。其无需训练和模型无关的特性也增加了其实用性和部署便利性。"}}
{"id": "2507.18882", "title": "A Comprehensive Review of AI-based Intelligent Tutoring Systems: Applications and Challenges", "authors": ["Meriem Zerkouk", "Miloud Mihoubi", "Belkacem Chikhaoui"], "categories": ["cs.IR", "cs.AI", "cs.HC"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Journal of Computers in Education ( 2025 )", "url": "http://arxiv.org/abs/2507.18882v1", "summary": "AI-based Intelligent Tutoring Systems (ITS) have significant potential to\ntransform teaching and learning. As efforts continue to design, develop, and\nintegrate ITS into educational contexts, mixed results about their\neffectiveness have emerged. This paper provides a comprehensive review to\nunderstand how ITS operate in real educational settings and to identify the\nassociated challenges in their application and evaluation. We use a systematic\nliterature review method to analyze numerous qualified studies published from\n2010 to 2025, examining domains such as pedagogical strategies, NLP, adaptive\nlearning, student modeling, and domain-specific applications of ITS. The\nresults reveal a complex landscape regarding the effectiveness of ITS,\nhighlighting both advancements and persistent challenges. The study also\nidentifies a need for greater scientific rigor in experimental design and data\nanalysis. Based on these findings, suggestions for future research and\npractical implications are proposed.", "comment": "Journal of Computers in Education ( 2025 )", "pdf_url": "http://arxiv.org/pdf/2507.18882v1", "cate": "cs.IR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "人工智能智能辅导系统综合综述：应用与挑战", "tldr": "本文对2010年至2025年间关于AI智能辅导系统（ITS）的文献进行了系统综述，旨在理解其在教育环境中的应用及挑战，发现其有效性复杂，并强调需提高实验设计和数据分析的科学严谨性。", "motivation": "AI智能辅导系统（ITS）虽有潜力改变教学，但其有效性结果不一。本研究旨在全面审查ITS在实际教育环境中的运作方式，并识别其应用和评估中的挑战。", "method": "采用系统文献综述方法，分析了2010年至2025年间发表的大量合格研究，涉及教学策略、自然语言处理、自适应学习、学生建模和ITS的特定领域应用。", "result": "结果显示ITS的有效性是一个复杂的局面，既有进展也有持续存在的挑战。研究还指出，实验设计和数据分析需要更高的科学严谨性。", "conclusion": "基于研究发现，提出了未来研究的建议和实际应用意义。", "translation": "人工智能（AI）智能辅导系统（ITS）在改变教学和学习方面具有巨大潜力。随着将ITS设计、开发和整合到教育环境中的努力持续进行，关于其有效性结果喜忧参半。本文提供了一项全面的综述，旨在了解ITS如何在真实的教育环境中运作，并识别其应用和评估中相关的挑战。我们采用系统文献综述方法，分析了2010年至2025年间发表的众多合格研究，审查了教学策略、自然语言处理、自适应学习、学生建模和ITS的特定领域应用等领域。结果揭示了ITS有效性方面的复杂局面，突出了进展和持续存在的挑战。该研究还指出，实验设计和数据分析需要更高的科学严谨性。基于这些发现，提出了未来研究的建议和实际应用意义。", "summary": "本文对2010年至2025年间关于AI智能辅导系统（ITS）的文献进行了系统综述，旨在理解其在教育环境中的应用及挑战。研究发现ITS的有效性表现出复杂性，既有进展也有待解决的难题，并强调了未来研究在实验设计和数据分析上需提高科学严谨性。", "keywords": "AI智能辅导系统, 系统综述, 教育技术, 自适应学习, 学生建模", "comments": "这篇综述为AI智能辅导系统的研究和实践提供了宝贵的洞察，特别强调了在评估其有效性时需要更严谨的科学方法，这对于未来ITS的开发和部署具有重要指导意义。其创新之处在于对现有文献的系统梳理和对挑战的深入剖析。"}}
{"id": "2507.19015", "title": "An Enumerative Embedding of the Python Type System in ACL2s", "authors": ["Samuel Xifaras", "Panagiotis Manolios", "Andrew T. Walter", "William Robertson"], "categories": ["cs.PL", "cs.LO", "cs.SE"], "primary_category": "Subjects:       Programming Languages (cs.PL)", "pdf_link": null, "comments": "Comments:      In Proceedings ACL2 2025, arXiv:2507.18567", "url": "http://arxiv.org/abs/2507.19015v1", "summary": "Python is a high-level interpreted language that has become an industry\nstandard in a wide variety of applications. In this paper, we take a first step\ntowards using ACL2s to reason about Python code by developing an embedding of a\nsubset of the Python type system in ACL2s. The subset of Python types we\nsupport includes many of the most commonly used type annotations as well as\nuser-defined types comprised of supported types. We provide ACL2s definitions\nof these types, as well as defdata enumerators that are customized to provide\ncode coverage and identify errors in Python programs. Using the ACL2s\nembedding, we can generate instances of types that can then be used as inputs\nto fuzz Python programs, which allows us to identify bugs in Python code that\nare not detected by state-of-the-art Python type checkers. We evaluate our work\nagainst four open-source repositories, extracting their type information and\ngenerating inputs for fuzzing functions with type signatures that are in the\nsupported subset of Python types. Note that we only use the type signatures of\nfunctions to generate inputs and treat the bodies of functions as black boxes.\nWe measure code coverage, which ranges from about 68% to more than 80%, and\nidentify code patterns that hinder coverage such as complex branch conditions\nand external file system dependencies. We conclude with a discussion of the\nresults and recommendations for future work.", "comment": "In Proceedings ACL2 2025, arXiv:2507.18567", "pdf_url": "http://arxiv.org/pdf/2507.19015v1", "cate": "cs.PL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "Python类型系统在ACL2s中的枚举嵌入", "tldr": "在ACL2s中嵌入部分Python类型系统，能生成模糊测试输入，发现现有类型检查器未检测到的Python代码错误，并实现良好代码覆盖率。", "motivation": "使用ACL2s对Python代码进行推理，并发现现有Python类型检查器无法检测到的错误。", "method": "在ACL2s中嵌入Python类型系统的一个子集，支持常见类型注解和用户定义类型。利用ACL2s定义和defdata枚举器生成类型实例作为输入，对Python程序进行模糊测试。评估通过对四个开源仓库提取类型信息并生成输入，仅使用函数类型签名，将函数体视为黑盒。", "result": "能够生成用于模糊测试Python程序的类型实例，从而发现现有高级Python类型检查器无法检测到的错误。代码覆盖率在68%到80%以上。识别出阻碍覆盖率的代码模式，如复杂的分支条件和外部文件系统依赖。", "conclusion": "通过在ACL2s中嵌入Python类型系统，可以有效地生成模糊测试输入，发现现有类型检查器无法检测到的Python代码错误，并达到较高的代码覆盖率。", "translation": "Python 是一种高级解释型语言，已成为各种应用程序的行业标准。在本文中，我们通过在 ACL2s 中开发 Python 类型系统子集的嵌入，迈出了使用 ACL2s 推理 Python 代码的第一步。我们支持的 Python 类型子集包括许多最常用的类型注解以及由支持类型组成的用户定义类型。我们提供了这些类型的 ACL2s 定义，以及为提供代码覆盖率和识别 Python 程序中的错误而定制的 defdata 枚举器。使用 ACL2s 嵌入，我们可以生成类型实例，然后将其用作模糊测试 Python 程序的输入，这使我们能够识别出最先进的 Python 类型检查器未检测到的 Python 代码中的错误。我们针对四个开源仓库评估了我们的工作，提取了它们的类型信息并为支持的 Python 类型子集中的函数生成了模糊测试输入。请注意，我们仅使用函数的类型签名来生成输入，并将函数体视为黑盒。我们测量了代码覆盖率，范围从大约 68% 到 80% 以上，并识别出阻碍覆盖率的代码模式，例如复杂的分支条件和外部文件系统依赖。最后，我们讨论了结果并对未来的工作提出了建议。", "summary": "本文介绍了将Python类型系统的一个子集枚举嵌入到ACL2s中的方法。该嵌入允许生成Python类型的具体实例，并将其用作模糊测试Python程序的输入。该方法成功识别出最先进的Python类型检查器未能检测到的错误。在对四个开源仓库的评估中，该方法实现了68%到80%以上的代码覆盖率，并揭示了复杂分支条件等挑战。这项工作是使用ACL2s对Python代码进行形式化推理的基础性一步。", "keywords": "Python类型系统, ACL2s, 模糊测试, 错误检测, 形式化方法", "comments": "本文提出了一种新颖的方法，通过利用ACL2s的形式化推理能力进行类型感知模糊测试，来发现Python代码中的错误。其创新之处在于弥合了Python等动态语言与形式化验证系统之间的鸿沟，从而能够根据类型签名生成有针对性的测试输入。能够识别当前类型检查器遗漏的错误具有重要意义。一个已承认的局限性是将函数体视为黑盒，这为未来更深入的分析提供了方向。"}}
{"id": "2507.19374", "title": "Data Augmentation for Spoken Grammatical Error Correction", "authors": ["Penny Karanasou", "Mengjie Qian", "Stefano Bannò", "Mark J. F. Gales", "Kate M. Knill"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      This work has been accepted by ISCA SLaTE 2025", "url": "http://arxiv.org/abs/2507.19374v1", "summary": "While there exist strong benchmark datasets for grammatical error correction\n(GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still\nunder-resourced. In this paper, we propose a fully automated method to generate\naudio-text pairs with grammatical errors and disfluencies. Moreover, we propose\na series of objective metrics that can be used to evaluate the generated data\nand choose the more suitable dataset for SGEC. The goal is to generate an\naugmented dataset that maintains the textual and acoustic characteristics of\nthe original data while providing new types of errors. This augmented dataset\nshould augment and enrich the original corpus without altering the language\nassessment scores of the second language (L2) learners. We evaluate the use of\nthe augmented corpus both for written GEC (the text part) and for SGEC (the\naudio-text pairs). Our experiments are conducted on the S\\&I Corpus, the first\npublicly available speech dataset with grammar error annotations.", "comment": "This work has been accepted by ISCA SLaTE 2025", "pdf_url": "http://arxiv.org/pdf/2507.19374v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "语音语法错误纠正的数据增强", "tldr": "该论文提出了一种全自动方法，用于生成带有语法错误和不流畅的语音-文本对，以解决语音语法错误纠正（SGEC）领域高质量标注数据集不足的问题。文章还提出了评估生成数据的客观指标，并在S&I语料库上对增强数据在书面GEC和SGEC中的应用进行了评估。", "motivation": "高质量的语音语法错误纠正（SGEC）标注数据集资源不足。", "method": "本文提出了一种全自动方法来生成带有语法错误和不流畅的语音-文本对。此外，还提出了一系列客观指标，用于评估生成的数据并选择更适合SGEC的数据集。目标是生成一个增强数据集，该数据集在保持原始数据的文本和声学特征的同时，提供新类型的错误，并且不改变第二语言（L2）学习者的语言评估分数。", "result": "本文评估了增强语料库在书面语法错误纠正（GEC）和语音语法错误纠正（SGEC）中的使用。实验在S&I语料库上进行。", "conclusion": "本文提出了一种生成语音-文本对的自动化数据增强方法，以解决SGEC领域数据资源不足的问题，并引入了评估生成数据质量的客观指标，为书面和语音GEC任务提供了有价值的增强数据。", "translation": "虽然存在强大的语法错误纠正（GEC）基准数据集，但用于语音语法错误纠正（SGEC）的高质量标注语音数据集仍然资源不足。在本文中，我们提出了一种全自动方法来生成带有语法错误和不流畅的语音-文本对。此外，我们提出了一系列客观指标，可用于评估生成的数据并选择更适合SGEC的数据集。目标是生成一个增强数据集，该数据集在保持原始数据的文本和声学特征的同时，提供新类型的错误。这个增强数据集应该在不改变第二语言（L2）学习者语言评估分数的情况下，增强和丰富原始语料库。我们评估了增强语料库在书面GEC（文本部分）和SGEC（语音-文本对）中的使用。我们的实验是在S&I语料库上进行的，这是第一个公开的带有语法错误标注的语音数据集。", "summary": "针对语音语法错误纠正（SGEC）领域高质量标注语音数据集资源不足的问题，本文提出了一种全自动方法来生成带有语法错误和不流畅的语音-文本对。该方法旨在在保持原始数据特征的同时引入新类型错误，并提出了客观指标来评估和选择合适的生成数据。研究评估了增强语料库在书面GEC和SGEC任务中的应用。", "keywords": "数据增强, 语音语法错误纠正, 语音-文本对, 语法错误纠正", "comments": "本文的创新点在于提出了一个全自动的数据增强方法，用于生成带有语法错误和不流畅的语音-文本对，以解决语音语法错误纠正（SGEC）领域的数据稀缺问题。此外，引入客观指标来评估生成数据的质量，也为后续研究提供了新的思路。这项工作对于推动SGEC研究具有重要意义。"}}
{"id": "2506.19256", "title": "Enhancing Generalization of Spiking Neural Networks Through Temporal Regularization", "authors": ["Boxuan Zhang", "Zhen Xu", "Kuan Tao"], "categories": ["cs.NE", "cs.AI"], "primary_category": "Subjects:       Neural and Evolutionary Computing (cs.NE)", "pdf_link": null, "comments": "Comments:      Code is available at this https URL", "url": "http://arxiv.org/abs/2506.19256v3", "summary": "Spiking Neural Networks (SNNs) have received widespread attention due to\ntheir event-driven and low-power characteristics, making them particularly\neffective for processing event-based neuromorphic data. Recent studies have\nshown that directly trained SNNs suffer from severe overfitting issues due to\nthe limited scale of neuromorphic datasets and the gradient mismatching\nproblem, which fundamentally constrain their generalization performance. In\nthis paper, we propose a temporal regularization training (TRT) method by\nintroducing a time-dependent regularization mechanism to enforce stronger\nconstraints on early timesteps. We compare the performance of TRT with other\nstate-of-the-art methods performance on datasets including CIFAR10/100,\nImageNet100, DVS-CIFAR10, and N-Caltech101. To validate the effectiveness of\nTRT, we conducted ablation studies and analyses including loss landscape\nvisualization and learning curve analysis, demonstrating that TRT can\neffectively mitigate overfitting and flatten the training loss landscape,\nthereby enhancing generalizability. Furthermore, we establish a theoretical\ninterpretation of TRT's temporal regularization mechanism based on the results\nof Fisher information analysis. We analyze the temporal information dynamics\ninside SNNs by tracking Fisher information during the TRT training process,\nrevealing the Temporal Information Concentration (TIC) phenomenon, where Fisher\ninformation progressively concentrates in early timesteps. The time-decaying\nregularization mechanism implemented in TRT effectively guides the network to\nlearn robust features in early timesteps with rich information, thereby leading\nto significant improvements in model generalization. Code is available at\nhttps://anonymous.4open.science/r/TRT-7FBFUYT4E.", "comment": "Code is available at https://anonymous.4open.science/r/TRT-7FBFUYT4E", "pdf_url": "http://arxiv.org/pdf/2506.19256v3", "cate": "cs.NE", "date": "2025-06-24", "updated": "2025-07-25", "AI": {"title_translation": "脉冲神经网络通过时间正则化增强泛化能力", "tldr": "提出了一种时间正则化训练（TRT）方法，以解决脉冲神经网络（SNNs）的过拟合问题并提高其泛化能力。", "motivation": "直接训练的脉冲神经网络（SNNs）由于神经形态数据集规模有限和梯度不匹配问题，存在严重的过拟合问题，这从根本上限制了它们的泛化性能。", "method": "提出了一种时间正则化训练（TRT）方法，通过引入时间依赖的正则化机制，对早期时间步施加强约束。通过与现有最先进方法进行性能比较，并在CIFAR10/100、ImageNet100、DVS-CIFAR10和N-Caltech101等数据集上进行验证。通过消融研究、损失景观可视化和学习曲线分析来验证TRT的有效性。基于Fisher信息分析对TRT的时间正则化机制进行了理论解释。", "result": "TRT在CIFAR10/100、ImageNet100、DVS-CIFAR10和N-Caltech101等数据集上表现出优于其他现有方法的性能。TRT能有效缓解过拟合，使训练损失景观平坦化，从而增强泛化能力。通过Fisher信息分析揭示了时间信息集中（TIC）现象，即Fisher信息在早期时间步中逐渐集中。", "conclusion": "TRT中实现的时间衰减正则化机制有效地引导网络在信息丰富的早期时间步学习鲁棒特征，从而显著提高了模型的泛化能力。", "translation": "脉冲神经网络（SNNs）因其事件驱动和低功耗特性而受到广泛关注，使其在处理基于事件的神经形态数据方面特别有效。最近的研究表明，直接训练的SNNs由于神经形态数据集规模有限和梯度不匹配问题，存在严重的过拟合问题，这从根本上限制了它们的泛化性能。在本文中，我们提出了一种时间正则化训练（TRT）方法，通过引入时间依赖的正则化机制，对早期时间步施加强约束。我们将TRT的性能与CIFAR10/100、ImageNet100、DVS-CIFAR10和N-Caltech101等数据集上的其他最先进方法进行了比较。为了验证TRT的有效性，我们进行了消融研究和分析，包括损失景观可视化和学习曲线分析，结果表明TRT可以有效缓解过拟合并使训练损失景观平坦化，从而增强泛化能力。此外，我们基于Fisher信息分析的结果对TRT的时间正则化机制建立了理论解释。我们通过跟踪TRT训练过程中Fisher信息来分析SNNs内部的时间信息动态，揭示了时间信息集中（TIC）现象，即Fisher信息在早期时间步中逐渐集中。TRT中实现的时间衰减正则化机制有效地引导网络在信息丰富的早期时间步学习鲁棒特征，从而显著提高了模型的泛化能力。代码可在https://anonymous.4open.science/r/TRT-7FBFUYT4E获取。", "summary": "本文提出了一种名为时间正则化训练（TRT）的新方法，旨在解决脉冲神经网络（SNNs）在有限神经形态数据集下存在的严重过拟合和泛化能力受限问题。TRT通过引入时间依赖的正则化机制，在早期时间步施加强约束。实验结果表明，TRT能有效缓解过拟合，使损失景观平坦化，显著提高SNNs的泛化能力。理论分析揭示了TRT通过引导网络在早期高信息时间步学习鲁棒特征来提升性能的机制。", "keywords": "脉冲神经网络, 时间正则化, 过拟合, 泛化能力, Fisher信息", "comments": "该论文创新性地引入了时间正则化机制来解决脉冲神经网络的过拟合问题，并通过理论分析（Fisher信息）揭示了其有效性背后的“时间信息集中”现象，为SNNs的训练提供了新的视角和方法。其贡献在于不仅提供了实证改进，还提供了深入的理论解释，对于推动SNNs的实际应用和理论研究具有重要意义。"}}
{"id": "2412.13502", "title": "Level-Set Parameters: Novel Representation for 3D Shape Analysis", "authors": ["Huan Lei", "Hongdong Li", "Andreas Geiger", "Anthony Dick"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2412.13502v2", "summary": "3D shape analysis has been largely focused on traditional 3D representations\nof point clouds and meshes, but the discrete nature of these data makes the\nanalysis susceptible to variations in input resolutions. Recent development of\nneural fields brings in level-set parameters from signed distance functions as\na novel, continuous, and numerical representation of 3D shapes, where the shape\nsurfaces are defined as zero-level-sets of those functions. This motivates us\nto extend shape analysis from the traditional 3D data to these novel parameter\ndata. Since the level-set parameters are not Euclidean like point clouds, we\nestablish correlations across different shapes by formulating them as a\npseudo-normal distribution, and learn the distribution prior from the\nrespective dataset. To further explore the level-set parameters with shape\ntransformations, we propose to condition a subset of these parameters on\nrotations and translations, and generate them with a hypernetwork. This\nsimplifies the pose-related shape analysis compared to using traditional data.\nWe demonstrate the promise of the novel representations through applications in\nshape classification (arbitrary poses), retrieval, and 6D object pose\nestimation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2412.13502v2", "cate": "cs.CV", "date": "2024-12-18", "updated": "2025-07-25", "AI": {"title_translation": "水平集参数：三维形状分析的新颖表示", "tldr": "本文提出将水平集参数作为三维形状分析的新颖连续表示，并通过伪正态分布和超网络处理形状变换，应用于形状分类、检索和6D姿态估计。", "motivation": "传统的三维表示（如点云和网格）的离散性质使其易受输入分辨率变化的影响。神经场的最新发展引入了水平集参数作为三维形状的连续、数值表示，这促使研究人员将形状分析从传统三维数据扩展到这些新颖的参数数据。", "method": "将水平集参数公式化为伪正态分布，以建立不同形状之间的关联；从相应数据集中学习分布先验；提出将这些参数的子集在旋转和平移上进行条件化；使用超网络生成这些参数。", "result": "在形状分类（任意姿态）、检索和6D物体姿态估计的应用中展示了新颖表示的潜力；与使用传统数据相比，简化了与姿态相关的形状分析。", "conclusion": "水平集参数作为三维形状分析的新颖、连续表示，在处理形状变换和应用于形状分类、检索及姿态估计等任务上表现出显著潜力。", "translation": "三维形状分析主要集中在点云和网格等传统三维表示上，但这些数据的离散性质使得分析容易受到输入分辨率变化的影响。神经场的最新发展引入了符号距离函数的水平集参数，作为三维形状的一种新颖、连续且数值化的表示，其中形状表面被定义为这些函数的零水平集。这促使我们将形状分析从传统三维数据扩展到这些新颖的参数数据。由于水平集参数不像点云那样是欧几里得的，我们通过将其公式化为伪正态分布来建立不同形状之间的关联，并从各自的数据集中学习分布先验。为了进一步探索水平集参数与形状变换的关系，我们提出将这些参数的一个子集以旋转和平移为条件，并使用超网络生成它们。与使用传统数据相比，这简化了与姿态相关的形状分析。我们通过在形状分类（任意姿态）、检索和6D物体姿态估计中的应用，展示了这种新颖表示的潜力。", "summary": "该论文提出使用水平集参数作为三维形状分析的新型连续表示，以克服传统点云和网格表示的离散性及其对分辨率变化的敏感性。研究人员将水平集参数建模为伪正态分布以建立形状关联并学习先验，并通过超网络对旋转和平移条件下的参数子集进行生成，从而简化了姿态相关的形状分析。实验结果表明，该新表示在形状分类、检索和6D物体姿态估计等任务中展现出良好前景。", "keywords": "水平集参数, 三维形状分析, 神经场, 形状分类, 姿态估计", "comments": "该论文的创新点在于引入水平集参数作为三维形状分析的连续表示，有效解决了传统离散表示（如点云和网格）对输入分辨率敏感的问题。通过将水平集参数建模为伪正态分布并结合超网络处理姿态变换，该方法为3D形状分析提供了一种更鲁棒和高效的途径，尤其在处理任意姿态的形状分类和6D姿态估计方面具有重要意义。"}}
{"id": "2507.19263", "title": "Modeling Uncertainty: Constraint-Based Belief States in Imperfect-Information Games", "authors": ["Achille Morenville", "Éric Piette"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19263v1", "summary": "In imperfect-information games, agents must make decisions based on partial\nknowledge of the game state. The Belief Stochastic Game model addresses this\nchallenge by delegating state estimation to the game model itself. This allows\nagents to operate on externally provided belief states, thereby reducing the\nneed for game-specific inference logic. This paper investigates two approaches\nto represent beliefs in games with hidden piece identities: a constraint-based\nmodel using Constraint Satisfaction Problems and a probabilistic extension\nusing Belief Propagation to estimate marginal probabilities. We evaluated the\nimpact of both representations using general-purpose agents across two\ndifferent games. Our findings indicate that constraint-based beliefs yield\nresults comparable to those of probabilistic inference, with minimal\ndifferences in agent performance. This suggests that constraint-based belief\nstates alone may suffice for effective decision-making in many settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19263v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "建模不确定性：不完全信息博弈中的基于约束的信念状态", "tldr": "研究了在不完全信息博弈中，使用基于约束的信念状态来表示代理的局部知识，并发现其性能与概率推理相当，表明其足以有效决策。", "motivation": "在不完全信息博弈中，代理需要基于部分知识进行决策。Belief Stochastic Game 模型通过将状态估计委托给游戏模型来解决这一挑战，使代理能够在外部提供的信念状态上操作，从而减少对特定游戏推理逻辑的需求。本文研究了两种表示信念的方法来应对这一挑战。", "method": "本文研究了两种在具有隐藏棋子身份的游戏中表示信念的方法：一种是使用约束满足问题（Constraint Satisfaction Problems, CSP）的基于约束的模型；另一种是使用信念传播（Belief Propagation）估计边际概率的概率扩展。作者使用通用代理在两种不同的游戏中评估了这两种表示方法的影响。", "result": "研究结果表明，基于约束的信念产生的效果与概率推理相当，代理性能差异极小。", "conclusion": "这表明在许多设置中，仅基于约束的信念状态就足以实现有效的决策。", "translation": "在不完全信息博弈中，代理必须根据对博弈状态的部分了解做出决策。信念随机博弈模型通过将状态估计委托给博弈模型本身来解决这一挑战。这使得代理能够在外部提供的信念状态上操作，从而减少了对特定博弈推理逻辑的需求。本文研究了两种在具有隐藏棋子身份的游戏中表示信念的方法：一种是使用约束满足问题（Constraint Satisfaction Problems）的基于约束的模型，以及一种使用信念传播（Belief Propagation）估计边际概率的概率扩展。我们使用通用代理在两种不同的游戏中评估了这两种表示方法的影响。我们的发现表明，基于约束的信念产生了与概率推理相当的结果，代理性能差异极小。这表明仅基于约束的信念状态可能足以在许多设置中进行有效的决策。", "summary": "本文探讨了在不完全信息博弈中表示代理信念的两种方法：基于约束的模型（使用CSP）和概率模型（使用信念传播）。通过在两种游戏中评估通用代理的性能，研究发现基于约束的信念状态在效果上与概率推理相当，且对代理性能影响微乎其微。这表明基于约束的信念可能足以支持不完全信息博弈中的有效决策。", "keywords": "不完全信息博弈, 信念状态, 约束满足问题, 信念传播, 代理性能", "comments": "这项研究的创新之处在于，它提出并验证了在不完全信息博弈中使用基于约束的信念状态作为传统概率推理方法的有效替代方案。其重要性在于，它可能简化复杂博弈中代理的设计，减少对游戏特定推理逻辑的需求，并可能为更高效的决策算法铺平道路。"}}
{"id": "2410.22999", "title": "Towards Constraint-aware Learning for Resource Allocation in NFV Networks", "authors": ["Tianfu Wang", "Long Yang", "Chao Wang", "Chuan Qin", "Liwei Deng", "Wei Wu", "Junyang Wang", "Li Shen", "Hui Xiong"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      ICML 2025 Workshop on ML4Wireless (Best Paper Award)", "url": "http://arxiv.org/abs/2410.22999v2", "summary": "Virtual Network Embedding (VNE) is a fundamental resource allocation\nchallenge that is associated with hard and multifaceted constraints in network\nfunction virtualization (NFV). Existing works for VNE struggle to handle such\ncomplex constraints, leading to compromised system performance and stability.\nIn this paper, we propose a \\textbf{CON}straint-\\textbf{A}ware\n\\textbf{L}earning framework, named \\textbf{CONAL}, for efficient constraint\nhandling in VNE. Concretely, we formulate the VNE problem as a constrained\nMarkov decision process with violation tolerance, enabling precise assessments\nof both solution quality and constraint violations. To achieve the persistent\nzero violation to guarantee solutions' feasibility, we propose a\nreachability-guided optimization with an adaptive reachability budget method.\nThis method also stabilizes policy optimization by appropriately handling\nscenarios with no feasible solutions. Furthermore, we propose a\nconstraint-aware graph representation method to efficiently learn cross-graph\nrelations and constrained path connectivity in VNE. Finally, extensive\nexperimental results demonstrate the superiority of our proposed method over\nstate-of-the-art baselines. Our code is available at\n\\href{https://github.com/GeminiLight/conal-vne}{https://github.com/GeminiLight/conal-vne}.", "comment": "ICML 2025 Workshop on ML4Wireless (Best Paper Award)", "pdf_url": "http://arxiv.org/pdf/2410.22999v2", "cate": "cs.NI", "date": "2024-10-30", "updated": "2025-07-25", "AI": {"title_translation": "面向NFV网络资源分配的约束感知学习", "tldr": "本文提出了CONAL框架，通过约束感知学习解决虚拟网络嵌入（VNE）中复杂的资源分配约束问题，提高了系统性能和稳定性。", "motivation": "虚拟网络嵌入（VNE）在网络功能虚拟化（NFV）中面临硬性且多方面的资源分配约束。现有VNE方法难以处理这些复杂约束，导致系统性能和稳定性下降。", "method": "本文提出了一个名为CONAL的约束感知学习框架，用于高效处理VNE中的约束。具体地，将VNE问题表述为具有违规容忍度的约束马尔可夫决策过程。为实现持久的零违规，提出了带有自适应可达性预算的可达性引导优化方法，该方法通过适当处理无可行解的情况来稳定策略优化。此外，还提出了一种约束感知图表示方法，以有效学习VNE中的跨图关系和约束路径连通性。", "result": "广泛的实验结果表明，本文提出的方法优于现有最先进的基线方法。", "conclusion": "通过提出的CONAL框架，该研究成功地解决了虚拟网络嵌入中复杂的约束问题，并通过实验证明了其在性能和稳定性方面的优越性。", "translation": "虚拟网络嵌入（VNE）是网络功能虚拟化（NFV）中一个基本的资源分配挑战，它与硬性且多方面的约束相关联。现有的VNE工作难以处理此类复杂约束，导致系统性能和稳定性受损。在本文中，我们提出了一个名为CONAL的约束感知学习框架，用于VNE中高效的约束处理。具体来说，我们将VNE问题表述为一个具有违规容忍度的约束马尔可夫决策过程，从而能够精确评估解决方案质量和约束违规。为了实现持久的零违规以保证解决方案的可行性，我们提出了一种带有自适应可达性预算的可达性引导优化方法。该方法还通过适当处理无可行解的情况来稳定策略优化。此外，我们提出了一种约束感知图表示方法，以有效学习VNE中的跨图关系和约束路径连通性。最后，广泛的实验结果表明，我们提出的方法优于现有最先进的基线方法。我们的代码可在https://github.com/GeminiLight/conal-vne获取。", "summary": "本文提出了一种名为CONAL的约束感知学习框架，旨在解决网络功能虚拟化（NFV）中虚拟网络嵌入（VNE）的复杂资源分配约束问题。该框架将VNE建模为具有违规容忍度的约束马尔可夫决策过程，并通过可达性引导优化和约束感知图表示方法，实现了对约束的有效处理和解决方案的零违规保证，显著提升了系统性能和稳定性。", "keywords": "虚拟网络嵌入, 约束感知学习, 资源分配, NFV, 马尔可夫决策过程", "comments": "本文创新性地将VNE问题建模为带有违规容忍度的约束马尔可夫决策过程，并引入了可达性引导优化来确保零违规可行性，这对于处理VNE中的复杂约束具有重要意义。同时，提出的约束感知图表示方法有助于更有效地学习复杂网络关系。其贡献在于提供了一个更鲁棒和高效的解决方案，以应对NFV网络中资源分配的挑战。"}}
{"id": "2507.19455", "title": "Forest-Guided Clustering -- Shedding Light into the Random Forest Black Box", "authors": ["Lisa Barros de Andrade e Sousa", "Gregor Miller", "Ronan Le Gleut", "Dominik Thalmeier", "Helena Pelin", "Marie Piraud"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19455v1", "summary": "As machine learning models are increasingly deployed in sensitive application\nareas, the demand for interpretable and trustworthy decision-making has\nincreased. Random Forests (RF), despite their widespread use and strong\nperformance on tabular data, remain difficult to interpret due to their\nensemble nature. We present Forest-Guided Clustering (FGC), a model-specific\nexplainability method that reveals both local and global structure in RFs by\ngrouping instances according to shared decision paths. FGC produces\nhuman-interpretable clusters aligned with the model's internal logic and\ncomputes cluster-specific and global feature importance scores to derive\ndecision rules underlying RF predictions. FGC accurately recovered latent\nsubclass structure on a benchmark dataset and outperformed classical clustering\nand post-hoc explanation methods. Applied to an AML transcriptomic dataset, FGC\nuncovered biologically coherent subpopulations, disentangled disease-relevant\nsignals from confounders, and recovered known and novel gene expression\npatterns. FGC bridges the gap between performance and interpretability by\nproviding structure-aware insights that go beyond feature-level attribution.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19455v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "森林引导聚类——揭示随机森林黑箱的奥秘", "tldr": "FGC是一种模型特定的可解释性方法，通过根据共享决策路径对实例进行分组，揭示随机森林的局部和全局结构，提升其可解释性。", "motivation": "随着机器学习模型在敏感应用领域的部署，对可解释和可信决策的需求增加。随机森林（RF）尽管性能强大且广泛使用，但由于其集成性质，难以解释。", "method": "本文提出了森林引导聚类（Forest-Guided Clustering, FGC），这是一种模型特定的可解释性方法。FGC通过根据共享决策路径对实例进行分组，揭示随机森林中的局部和全局结构。它生成与模型内部逻辑一致的人类可解释聚类，并计算特定聚类和全局的特征重要性分数，以推导出随机森林预测背后的决策规则。", "result": "FGC在基准数据集上准确地恢复了潜在的子类结构，并且优于经典聚类和事后解释方法。应用于AML转录组数据集时，FGC揭示了生物学上一致的亚群，将疾病相关信号与混杂因素分离，并恢复了已知和新颖的基因表达模式。", "conclusion": "FGC通过提供超越特征层面归因的结构感知洞察，弥合了模型性能和可解释性之间的鸿沟。", "translation": "随着机器学习模型越来越多地部署在敏感应用领域，对可解释和可信决策的需求也随之增加。随机森林（RF）尽管在表格数据上广泛使用且性能强大，但由于其集成性质，仍然难以解释。我们提出了森林引导聚类（FGC），这是一种模型特定的可解释性方法，通过根据共享决策路径对实例进行分组，揭示随机森林中的局部和全局结构。FGC生成与模型内部逻辑一致的人类可解释聚类，并计算特定聚类和全局的特征重要性分数，以推导出随机森林预测背后的决策规则。FGC在基准数据集上准确地恢复了潜在的子类结构，并且优于经典聚类和事后解释方法。应用于AML转录组数据集时，FGC揭示了生物学上一致的亚群，将疾病相关信号与混杂因素分离，并恢复了已知和新颖的基因表达模式。FGC通过提供超越特征层面归因的结构感知洞察，弥合了模型性能和可解释性之间的鸿沟。", "summary": "本文提出了一种名为森林引导聚类（FGC）的新型模型特定可解释性方法，旨在解决随机森林（RF）因其集成性质而难以解释的问题。FGC通过根据共享决策路径对实例进行分组，揭示RF的局部和全局结构，从而生成与模型内部逻辑一致的、人类可解释的聚类。该方法还能计算特征重要性分数并推导决策规则。实验证明，FGC在恢复潜在子类结构方面表现出色，并优于现有方法，同时在生物医学数据集上成功揭示了有意义的生物学模式。FGC有效地提升了RF的可解释性，弥合了模型性能和透明度之间的差距。", "keywords": "随机森林, 可解释性, 聚类, 模型特定, 特征重要性", "comments": "FGC通过将实例根据共享决策路径进行聚类，提供了一种新颖且直观的方式来理解随机森林的内部决策逻辑，这超越了传统的特征重要性或局部解释方法。其创新之处在于将聚类技术与随机森林的决策过程深度结合，从而揭示了模型内部的结构性洞察。这对于需要高可信度决策的敏感应用领域具有重要意义，因为它不仅提高了透明度，还可能帮助发现数据中潜在的、有意义的子群体。"}}
{"id": "2501.00989", "title": "Bootstrapped Reward Shaping", "authors": ["Jacob Adamczyk", "Volodymyr Makarenko", "Stas Tiomkin", "Rahul V. Kulkarni"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at AAAI-2025, Main Track", "url": "http://arxiv.org/abs/2501.00989v2", "summary": "In reinforcement learning, especially in sparse-reward domains, many\nenvironment steps are required to observe reward information. In order to\nincrease the frequency of such observations, \"potential-based reward shaping\"\n(PBRS) has been proposed as a method of providing a more dense reward signal\nwhile leaving the optimal policy invariant. However, the required \"potential\nfunction\" must be carefully designed with task-dependent knowledge to not deter\ntraining performance. In this work, we propose a \"bootstrapped\" method of\nreward shaping, termed BSRS, in which the agent's current estimate of the\nstate-value function acts as the potential function for PBRS. We provide\nconvergence proofs for the tabular setting, give insights into training\ndynamics for deep RL, and show that the proposed method improves training speed\nin the Atari suite.", "comment": "Accepted at AAAI-2025, Main Track", "pdf_url": "http://arxiv.org/pdf/2501.00989v2", "cate": "cs.LG", "date": "2025-01-02", "updated": "2025-07-24", "AI": {"title_translation": "自举奖励塑造", "tldr": "提出了一种名为BSRS的自举奖励塑造方法，它使用智能体的当前状态价值函数作为潜在函数，以加速稀疏奖励环境中的强化学习训练。", "motivation": "在稀疏奖励的强化学习领域，智能体需要大量的环境步骤才能观察到奖励信息。虽然基于潜力的奖励塑造（PBRS）可以提供更密集的奖励信号并保持最优策略不变，但其所需的“潜力函数”需要根据任务知识进行精心设计，否则会影响训练性能。", "method": "本文提出了一种名为“自举奖励塑造”（BSRS）的方法。在该方法中，智能体当前对状态价值函数的估计被用作PBRS的潜力函数。", "result": "作者为表格设置提供了收敛性证明，深入探讨了深度强化学习的训练动态，并表明所提出的方法在Atari套件中提高了训练速度。", "conclusion": "BSRS通过利用智能体自身的状态价值估计作为潜力函数，有效解决了传统潜力函数设计困难的问题，并在稀疏奖励环境中显著提高了强化学习的训练效率。", "translation": "在强化学习中，尤其是在稀疏奖励领域，需要大量的环境步骤才能观察到奖励信息。为了增加此类观测的频率，已经提出了“基于潜力的奖励塑造”（PBRS）作为一种提供更密集奖励信号同时保持最优策略不变的方法。然而，所需的“潜力函数”必须根据任务相关知识精心设计，以免影响训练性能。在这项工作中，我们提出了一种“自举”奖励塑造方法，称为BSRS，其中智能体当前对状态价值函数的估计充当PBRS的潜力函数。我们为表格设置提供了收敛性证明，深入探讨了深度强化学习的训练动态，并表明所提出的方法在Atari套件中提高了训练速度。", "summary": "这篇论文提出了一种名为“自举奖励塑造”（BSRS）的新型强化学习奖励塑造方法，旨在解决稀疏奖励环境中训练效率低下的问题。与传统的基于潜力的奖励塑造（PBRS）需要精心设计的潜力函数不同，BSRS利用智能体自身对状态价值函数的当前估计作为潜力函数。研究证明了该方法在表格设置下的收敛性，提供了深度强化学习训练动态的见解，并展示了其在Atari套件中显著提高训练速度的能力。", "keywords": "强化学习, 奖励塑造, 自举, 稀疏奖励, 潜力函数", "comments": "BSRS的创新之处在于其将自举思想引入奖励塑造，巧妙地利用智能体自身学习到的价值函数作为潜力函数，避免了传统PBRS中手动设计潜力函数的复杂性和挑战性。这大大降低了奖励塑造的应用门槛，使其在稀疏奖励RL任务中更具普适性和实用性。该方法对于加速RL训练具有重要意义。"}}
{"id": "2507.15850", "title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking", "authors": ["Basma El Amel Boussaha", "Leen AlQadi", "Mugariya Farooq", "Shaikha Alsuwaidi", "Giulia Campesan", "Ahmed Alzubaidi", "Mohammed Alyafeai", "Hakim Hacid"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15850v3", "summary": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15850v3", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-25", "AI": {"title_translation": "3LM：通过基准测试连接阿拉伯语、STEM和代码", "tldr": "该论文介绍了3LM，一套针对阿拉伯语的STEM和代码领域的新基准套件，旨在解决这些领域中阿拉伯语大型语言模型资源不足的问题。", "motivation": "阿拉伯语是世界上使用最广泛的语言之一，但针对阿拉伯语的大型语言模型（LLM）的开发和评估工作相对有限。现有的大多数阿拉伯语基准测试侧重于语言、文化或宗教内容，在STEM和代码等日益重要的领域存在显著空白，而这些领域对现实世界的LLM应用至关重要。因此，该论文旨在弥补这一空白。", "method": "该论文提出了3LM，一套包含三个专门为阿拉伯语设计的基准测试：1. 一组源自阿拉伯语教科书和教育工作表的STEM相关问答对。2. 一组使用相同来源合成生成的STEM问题。3. 一个代码生成基准，通过人工审核流程精心翻译了两个广泛使用的代码基准构建而成。", "result": "该论文公开发布了所有三个基准测试，以支持阿拉伯语大型语言模型在STEM和代码这些重要但代表性不足的领域的研究增长。", "conclusion": "该论文通过发布3LM基准套件，旨在支持阿拉伯语大型语言模型在STEM和代码领域的研究发展。", "translation": "阿拉伯语是世界上使用最广泛的语言之一，然而，为阿拉伯语开发和评估大型语言模型（LLM）的努力仍然相对有限。大多数现有的阿拉伯语基准测试侧重于语言、文化或宗教内容，在STEM和代码等领域存在显著空白，而这些领域对于现实世界的LLM应用越来越重要。为了帮助弥补这一空白，我们提出了3LM，这是一套专门为阿拉伯语设计的三个基准测试。第一个是STEM相关的问答对，自然来源于阿拉伯语教科书和教育工作表。第二个包括使用相同来源合成生成的STEM问题。第三个基准测试侧重于代码生成，通过仔细翻译两个广泛使用的代码基准测试构建而成，并结合了多轮人工审核过程，以确保高质量和忠实的翻译。我们公开发布所有这三个基准测试，以支持阿拉伯语LLM在这些重要但代表性不足的领域的研究增长。", "summary": "本论文介绍了3LM，一套新的阿拉伯语基准测试套件，旨在解决大型语言模型（LLM）在STEM和代码领域阿拉伯语资源匮乏的问题。该套件包含自然获取和合成生成的STEM问题，以及一个通过人工辅助翻译现有基准创建的代码生成基准。所有基准均已公开发布，以促进阿拉伯语LLM在这些关键但服务不足领域的研究。", "keywords": "阿拉伯语LLM, 基准测试, STEM, 代码生成, 3LM", "comments": "该论文的创新之处在于解决了阿拉伯语大型语言模型发展中的一个关键空白，即关注STEM和代码领域，这些领域在现有阿拉伯语基准测试中经常被忽视。代码基准测试中采用的人工审核流程提高了其质量。其重要性在于提供了急需的公共资源，以推动阿拉伯语大型语言模型研究超越传统的语言和文化内容。"}}
{"id": "2507.19196", "title": "Towards Multimodal Social Conversations with Robots: Using Vision-Language Models", "authors": ["Ruben Janssens", "Tony Belpaeme"], "categories": ["cs.RO", "cs.CL", "cs.HC"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Submitted to the workshop \"Human - Foundation Models Interaction: A Focus On Multimodal Information\" (FoMo-HRI) at IEEE RO-MAN 2025", "url": "http://arxiv.org/abs/2507.19196v1", "summary": "Large language models have given social robots the ability to autonomously\nengage in open-domain conversations. However, they are still missing a\nfundamental social skill: making use of the multiple modalities that carry\nsocial interactions. While previous work has focused on task-oriented\ninteractions that require referencing the environment or specific phenomena in\nsocial interactions such as dialogue breakdowns, we outline the overall needs\nof a multimodal system for social conversations with robots. We then argue that\nvision-language models are able to process this wide range of visual\ninformation in a sufficiently general manner for autonomous social robots. We\ndescribe how to adapt them to this setting, which technical challenges remain,\nand briefly discuss evaluation practices.", "comment": "Submitted to the workshop \"Human - Foundation Models Interaction: A\n  Focus On Multimodal Information\" (FoMo-HRI) at IEEE RO-MAN 2025", "pdf_url": "http://arxiv.org/pdf/2507.19196v1", "cate": "cs.RO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "迈向与机器人的多模态社交对话：使用视觉-语言模型", "tldr": "本文探讨了如何利用视觉-语言模型，使社交机器人能够进行多模态社交对话，弥补当前大型语言模型在处理多模态社交互动方面的不足。", "motivation": "尽管大型语言模型赋予了社交机器人进行开放域对话的能力，但它们仍缺乏一项基本的社交技能：利用承载社交互动的多种模态。现有工作主要集中于任务导向的交互或特定的社交现象（如对话中断），而忽略了多模态社交对话的整体需求。", "method": "本文概述了机器人多模态社交对话系统的整体需求，并论证了视觉-语言模型能够以足够通用的方式处理广泛的视觉信息，适用于自主社交机器人。文章还描述了如何调整这些模型以适应此设置，讨论了仍存在的技挑战，并简要探讨了评估实践。", "result": "Not mentioned in abstract", "conclusion": "视觉-语言模型能够以足够通用的方式处理广泛的视觉信息，有望使社交机器人具备多模态社交对话能力，尽管仍存在技术挑战和评估问题。", "translation": "大型语言模型赋予了社交机器人自主进行开放域对话的能力。然而，它们仍然缺少一项基本的社交技能：利用承载社交互动的多种模态。虽然之前的工作主要集中于任务导向的交互（需要引用环境）或社交互动中的特定现象（如对话中断），但我们概述了机器人多模态社交对话系统的整体需求。然后，我们论证了视觉-语言模型能够以足够通用的方式处理广泛的视觉信息，适用于自主社交机器人。我们描述了如何将它们适应这种设置，仍然存在哪些技术挑战，并简要讨论了评估实践。", "summary": "本文探讨了社交机器人如何通过利用视觉-语言模型实现多模态社交对话，以弥补当前大型语言模型在此方面能力的不足。文章概述了多模态社交对话系统的需求，并提出视觉-语言模型能够通用地处理视觉信息，从而赋能自主社交机器人。同时，论文也讨论了模型适应性、技术挑战和评估方法。", "keywords": "社交机器人, 多模态对话, 视觉-语言模型, 人机交互, 自然语言处理", "comments": "本文提出了一种重要且及时的方向，即利用视觉-语言模型来增强社交机器人的多模态交互能力。其创新之处在于将研究重点从任务导向或特定问题转移到更通用的社交对话，这对于机器人融入人类社会至关重要。尽管论文偏概念性，未提供具体实验结果，但其对未来研究方向的指引和挑战的识别具有重要价值。"}}
{"id": "2507.19035", "title": "Dual Path Learning -- learning from noise and context for medical image denoising", "authors": ["Jitindra Fartiyal", "Pedro Freire", "Yasmeen Whayeb", "James S. Wolffsohn", "Sergei K. Turitsyn", "Sergei G. Sokolov"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      10 pages, 7 figures", "url": "http://arxiv.org/abs/2507.19035v1", "summary": "Medical imaging plays a critical role in modern healthcare, enabling\nclinicians to accurately diagnose diseases and develop effective treatment\nplans. However, noise, often introduced by imaging devices, can degrade image\nquality, leading to misinterpretation and compromised clinical outcomes.\nExisting denoising approaches typically rely either on noise characteristics or\non contextual information from the image. Moreover, they are commonly developed\nand evaluated for a single imaging modality and noise type. Motivated by Geng\net.al CNCL, which integrates both noise and context, this study introduces a\nDual-Pathway Learning (DPL) model architecture that effectively denoises\nmedical images by leveraging both sources of information and fusing them to\ngenerate the final output. DPL is evaluated across multiple imaging modalities\nand various types of noise, demonstrating its robustness and generalizability.\nDPL improves PSNR by 3.35% compared to the baseline UNet when evaluated on\nGaussian noise and trained across all modalities. The code is available at\n10.5281/zenodo.15836053.", "comment": "10 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.19035v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "双路径学习——从噪声和语境中学习以进行医学图像去噪", "tldr": "本文提出了一种名为双路径学习（DPL）的新型模型架构，通过同时利用噪声特征和上下文信息来有效去噪医学图像，并在多种成像模态和噪声类型上表现出鲁棒性和泛化性，相比基线UNet在PSNR上提升了3.35%。", "motivation": "医学图像中的噪声会降低图像质量，导致误诊。现有去噪方法通常只依赖噪声特性或上下文信息，并且通常只针对单一成像模态和噪声类型开发和评估。受Geng等人CNCL整合噪声和上下文的启发，本研究旨在开发一种更通用、更鲁棒的去噪方法。", "method": "本研究引入了一种双路径学习（DPL）模型架构。该模型通过利用噪声信息和图像上下文信息，并将其融合以生成最终输出，从而有效地对医学图像进行去噪。DPL在多种成像模态和各种类型的噪声下进行了评估。", "result": "DPL模型在多种成像模态和各种噪声类型下表现出鲁棒性和泛化能力。与基线UNet相比，当在所有模态上进行训练并评估高斯噪声时，DPL将PSNR提高了3.35%。", "conclusion": "DPL模型通过结合噪声和上下文信息，提供了一种有效且通用的医学图像去噪方法，显著优于现有基线方法，并在不同模态和噪声类型中展现出良好的性能。", "translation": "医学成像在现代医疗保健中扮演着关键角色，使临床医生能够准确诊断疾病并制定有效的治疗方案。然而，通常由成像设备引入的噪声会降低图像质量，导致误判和临床结果受损。现有的去噪方法通常依赖于噪声特性或图像的上下文信息。此外，它们通常是为单一成像模态和噪声类型开发和评估的。受Geng等人CNCL（它整合了噪声和上下文）的启发，本研究引入了一种双路径学习（DPL）模型架构，该架构通过利用这两种信息来源并将其融合以生成最终输出，从而有效地对医学图像进行医学图像去噪。DPL在多种成像模态和各种类型的噪声下进行了评估，展示了其鲁棒性和泛化性。与基线UNet相比，当在高斯噪声上评估并在所有模态上进行训练时，DPL将PSNR提高了3.35%。代码可在10.5281/zenodo.15836053获取。", "summary": "本文提出了一种名为双路径学习（DPL）的新型模型架构，旨在解决医学图像中噪声引起的图像质量下降问题。与传统方法仅依赖噪声特性或上下文信息不同，DPL模型通过整合噪声和上下文两种信息来源进行去噪，并将其融合以生成高质量的去噪图像。该模型在多种成像模态和不同噪声类型下进行了广泛评估，结果表明其具有出色的鲁棒性和泛化能力。实验结果显示，在处理高斯噪声并跨所有模态训练时，DPL模型相比基线UNet在峰值信噪比（PSNR）上提升了3.35%。", "keywords": "医学图像去噪, 双路径学习, 噪声抑制, 上下文信息, 图像质量", "comments": "该论文的创新点在于提出了双路径学习（DPL）模型，它同时利用了噪声特征和图像上下文信息进行医学图像去噪，这与现有方法通常只关注其中一方面形成对比。其重要性在于，通过在多种成像模态和不同噪声类型上进行评估，验证了模型的鲁棒性和泛化能力，这对于实际临床应用具有重要意义。性能提升3.35%的PSNR也显示了其有效性。"}}
{"id": "2507.19118", "title": "Cross Spatial Temporal Fusion Attention for Remote Sensing Object Detection via Image Feature Matching", "authors": ["Abu Sadat Mohammad Salehin Amit", "Xiaoli Zhang", "Md Masum Billa Shagar", "Zhaojun Liu", "Xiongfei Li", "Fanlong Meng"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19118v1", "summary": "Effectively describing features for cross-modal remote sensing image matching\nremains a challenging task due to the significant geometric and radiometric\ndifferences between multimodal images. Existing methods primarily extract\nfeatures at the fully connected layer but often fail to capture cross-modal\nsimilarities effectively. We propose a Cross Spatial Temporal Fusion (CSTF)\nmechanism that enhances feature representation by integrating scale-invariant\nkeypoints detected independently in both reference and query images. Our\napproach improves feature matching in two ways: First, by creating\ncorrespondence maps that leverage information from multiple image regions\nsimultaneously, and second, by reformulating the similarity matching process as\na classification task using SoftMax and Fully Convolutional Network (FCN)\nlayers. This dual approach enables CSTF to maintain sensitivity to distinctive\nlocal features while incorporating broader contextual information, resulting in\nrobust matching across diverse remote sensing modalities. To demonstrate the\npractical utility of improved feature matching, we evaluate CSTF on object\ndetection tasks using the HRSC2016 and DOTA benchmark datasets. Our method\nachieves state-of-theart performance with an average mAP of 90.99% on HRSC2016\nand 90.86% on DOTA, outperforming existing models. The CSTF model maintains\ncomputational efficiency with an inference speed of 12.5 FPS. These results\nvalidate that our approach to crossmodal feature matching directly enhances\ndownstream remote sensing applications such as object detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19118v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "用于遥感目标检测的跨时空融合注意力与图像特征匹配", "tldr": "本文提出了一种名为跨时空融合（CSTF）的新机制，通过整合尺度不变关键点和将相似性匹配重新定义为分类任务，显著提升了跨模态遥感图像的特征匹配能力，并在目标检测任务上实现了最先进的性能。", "motivation": "由于多模态图像之间显著的几何和辐射差异，有效描述跨模态遥感图像匹配的特征仍然是一项具有挑战性的任务。现有方法主要在全连接层提取特征，但往往无法有效捕获跨模态相似性。", "method": "我们提出了一种跨时空融合（CSTF）机制，通过整合在参考图像和查询图像中独立检测到的尺度不变关键点来增强特征表示。我们的方法通过两种方式改进特征匹配：首先，创建同时利用多个图像区域信息的对应图；其次，使用SoftMax和全卷积网络（FCN）层将相似性匹配过程重新定义为分类任务。这种双重方法使CSTF能够保持对独特局部特征的敏感性，同时结合更广泛的上下文信息。", "result": "CSTF在HRSC2016和DOTA基准数据集上的目标检测任务中实现了最先进的性能，HRSC2016的平均mAP为90.99%，DOTA为90.86%，优于现有模型。CSTF模型保持计算效率，推理速度为12.5 FPS。", "conclusion": "我们的跨模态特征匹配方法直接增强了下游遥感应用，如目标检测。", "translation": "有效描述跨模态遥感图像匹配的特征仍然是一项具有挑战性的任务，因为多模态图像之间存在显著的几何和辐射差异。现有方法主要在全连接层提取特征，但往往无法有效捕获跨模态相似性。我们提出了一种跨时空融合（CSTF）机制，通过整合在参考图像和查询图像中独立检测到的尺度不变关键点来增强特征表示。我们的方法通过两种方式改进特征匹配：首先，创建同时利用多个图像区域信息的对应图；其次，使用SoftMax和全卷积网络（FCN）层将相似性匹配过程重新定义为分类任务。这种双重方法使CSTF能够保持对独特局部特征的敏感性，同时结合更广泛的上下文信息，从而在各种遥感模态中实现鲁棒匹配。为了证明改进特征匹配的实用性，我们使用HRSC2016和DOTA基准数据集在目标检测任务上评估了CSTF。我们的方法实现了最先进的性能，HRSC2016的平均mAP为90.99%，DOTA为90.86%，优于现有模型。CSTF模型保持计算效率，推理速度为12.5 FPS。这些结果验证了我们的跨模态特征匹配方法直接增强了下游遥感应用，如目标检测。", "summary": "本文针对跨模态遥感图像匹配中存在的特征描述挑战，提出了一种名为跨时空融合（CSTF）的新机制。CSTF通过整合尺度不变关键点并利用多区域信息创建对应图，同时将相似性匹配重新定义为基于SoftMax和FCN的分类任务，有效增强了特征表示和匹配能力。该方法在HRSC2016和DOTA数据集上的目标检测任务中表现出色，实现了90.99%和90.86%的mAP，并保持了12.5 FPS的推理速度，证明了其在遥感应用中的实用性。", "keywords": "跨时空融合, 遥感, 目标检测, 图像特征匹配, 跨模态", "comments": "该论文通过引入跨时空融合（CSTF）机制，创新性地解决了跨模态遥感图像特征匹配的难题。其核心贡献在于将特征匹配重构为分类任务，并有效利用了多区域上下文信息和尺度不变关键点，显著提升了匹配的鲁棒性。在遥感目标检测领域的卓越性能证明了其重要性，为未来的跨模态感知研究提供了有价值的方向。"}}
{"id": "2507.19438", "title": "Gradient-based grand canonical optimization enabled by graph neural networks with fractional atomic existence", "authors": ["Mads-Peter Verner Christiansen", "Bjørk Hammer"], "categories": ["cond-mat.mtrl-sci", "cs.LG"], "primary_category": "Subjects:       Materials Science (cond-mat.mtrl-sci)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19438v1", "summary": "Machine learning interatomic potentials have become an indispensable tool for\nmaterials science, enabling the study of larger systems and longer timescales.\nState-of-the-art models are generally graph neural networks that employ message\npassing to iteratively update atomic embeddings that are ultimately used for\npredicting properties. In this work we extend the message passing formalism\nwith the inclusion of a continuous variable that accounts for fractional atomic\nexistence. This allows us to calculate the gradient of the Gibbs free energy\nwith respect to both the Cartesian coordinates of atoms and their existence.\nUsing this we propose a gradient-based grand canonical optimization method and\ndocument its capabilities for a Cu(110) surface oxide.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19438v1", "cate": "cond-mat.mtrl-sci", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于图神经网络和分数原子存在性的梯度大正则系综优化", "tldr": "该工作通过引入分数原子存在性的连续变量扩展了图神经网络的消息传递机制，从而提出了一种梯度大正则系综优化方法。", "motivation": "机器学习原子间势已成为材料科学不可或缺的工具，但现有模型可能在处理大正则系综优化方面存在局限性，特别是在需要考虑原子分数存在性的情况下。", "method": "本文通过引入一个考虑分数原子存在性的连续变量来扩展消息传递形式，从而能够计算吉布斯自由能相对于原子笛卡尔坐标及其存在性的梯度。在此基础上，提出了一种基于梯度的巨正则系综优化方法。", "result": "该方法展示了其在Cu(110)表面氧化物上的能力。", "conclusion": "通过扩展图神经网络的消息传递机制以包含分数原子存在性，本文成功开发了一种新的基于梯度的巨正则系综优化方法，并验证了其在材料系统中的应用潜力。", "translation": "机器学习原子间势已成为材料科学中不可或缺的工具，使得研究更大的系统和更长的时间尺度成为可能。最先进的模型通常是图神经网络，它们采用消息传递来迭代更新原子嵌入，最终用于预测性质。在这项工作中，我们通过引入一个考虑分数原子存在性的连续变量来扩展消息传递形式。这使我们能够计算吉布斯自由能相对于原子笛卡尔坐标及其存在性的梯度。利用这一点，我们提出了一种基于梯度的巨正则系综优化方法，并记录了其在Cu(110)表面氧化物上的能力。", "summary": "本文扩展了图神经网络的消息传递机制，通过引入一个表示分数原子存在性的连续变量，从而能够计算吉布斯自由能的梯度。基于此，提出了一种新的基于梯度的巨正则系综优化方法，并在一项Cu(110)表面氧化物研究中验证了其有效性。", "keywords": "图神经网络, 分数原子存在性, 大正则系综优化, 梯度优化, 机器学习原子间势", "comments": "这项工作通过将分数原子存在性纳入图神经网络的消息传递框架，为材料科学中的大正则系综优化提供了一种新颖的梯度方法。其创新之处在于能够同时优化原子坐标和组成，这对于理解和设计复杂材料系统具有重要意义。"}}
{"id": "2410.16660", "title": "Difficulties Constructing Lattices with Exponential Kissing Number from Codes", "authors": ["Huck Bennett", "Alexander Golovnev", "Noah Stephens-Davidowitz"], "categories": ["math.MG", "cs.IT", "math.IT", "math.NT"], "primary_category": "Subjects:       Metric Geometry (math.MG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.16660v2", "summary": "In this note, we present examples showing that several natural ways of\nconstructing lattices from error-correcting codes do not in general yield a\ncorrespondence between minimum-weight non-zero codewords and shortest non-zero\nlattice vectors. From these examples, we conclude that the main results in two\nworks of Vl\\u{a}du\\c{t} (Moscow J. Comb. Number Th., 2019 and Discrete Comput.\nGeom., 2021) on constructing lattices with exponential kissing number from\nerror-correcting codes are invalid. A more recent preprint (arXiv, 2024) that\nVl\\u{a}du\\c{t} posted after an initial version of this work was made public is\nalso invalid.\n  Exhibiting a family of lattices with exponential kissing number therefore\nremains an open problem (as of July 2025).", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.16660v2", "cate": "math.MG", "date": "2024-10-22", "updated": "2025-07-24", "AI": {"title_translation": "从编码构造具有指数接吻数的格的困难", "tldr": "本文通过示例指出，从纠错码构造格的几种常用方法未能建立码字和格向量之间的对应关系，从而证明了Vlăduț关于从纠错码构造具有指数接吻数的格的主要研究结果是无效的。因此，展示具有指数接吻数的一族格仍然是一个开放问题。", "motivation": "本文的动机是展示从纠错码构造格的几种自然方法未能建立码字和格向量之间的预期对应关系，并据此指出先前关于从纠错码构造具有指数接吻数的格的研究结果是无效的。", "method": "本文通过提供具体的例子来证明从纠错码构造格的几种常见方法无法在最小重量非零码字和最短非零格向量之间建立对应关系。", "result": "结果表明，从纠错码构造格的几种自然方法通常不会在最小重量非零码字和最短非零格向量之间产生对应关系。因此，Vlăduț在2019年和2021年的两项工作以及他2024年的一份预印本中关于从纠错码构造具有指数接吻数的格的主要结果被证明是无效的。", "conclusion": "由于现有方法未能成功，展示一族具有指数接吻数的格仍然是一个开放问题。", "translation": "在这篇笔记中，我们提供了例子，表明从纠错码构造格的几种自然方法通常不会在最小重量非零码字和最短非零格向量之间产生对应关系。从这些例子中，我们得出结论，Vlăduț（Moscow J. Comb. Number Th., 2019 和 Discrete Comput. Geom., 2021）关于从纠错码构造具有指数接吻数的格的两项工作中的主要结果是无效的。Vlăduț在本文的初始版本公开后发布的一份最新预印本（arXiv, 2024）也是无效的。因此，展示一族具有指数接吻数的格仍然是一个开放问题（截至2025年7月）。", "summary": "本文通过具体示例揭示了从纠错码构造格的常用方法在建立码字与格向量对应关系上的缺陷。研究指出，这些缺陷导致Vlăduț先前关于利用纠错码构造具有指数接吻数格的主要结论无效，包括其2019年、2021年的工作以及2024年的预印本。因此，构建具有指数接吻数的格族仍是一个待解决的难题。", "keywords": "格, 接吻数, 纠错码, 构造, 无效性", "comments": "这篇论文的重要性在于它纠正了现有文献中的错误，明确指出了一种特定类型的格构造方法的局限性。它强调了在数论和编码理论交叉领域中一个重要的开放问题，即如何构造具有指数接吻数的格族。这项工作对于研究格理论和编码理论的学者具有重要的警示作用，避免在无效的基础上继续研究。"}}
{"id": "2507.18794", "title": "CLEAR: Unlearning Spurious Style-Content Associations with Contrastive LEarning with Anti-contrastive Regularization", "authors": ["Minghui Sun", "Benjamin A. Goldstein", "Matthew M. Engelhard"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      10 pages main text, 24 pages in total", "url": "http://arxiv.org/abs/2507.18794v1", "summary": "Learning representations unaffected by superficial characteristics is\nimportant to ensure that shifts in these characteristics at test time do not\ncompromise downstream prediction performance. For instance, in healthcare\napplications, we might like to learn features that contain information about\npathology yet are unaffected by race, sex, and other sources of physiologic\nvariability, thereby ensuring predictions are equitable and generalizable\nacross all demographics. Here we propose Contrastive LEarning with\nAnti-contrastive Regularization (CLEAR), an intuitive and easy-to-implement\nframework that effectively separates essential (i.e., task-relevant)\ncharacteristics from superficial (i.e., task-irrelevant) characteristics during\ntraining, leading to better performance when superficial characteristics shift\nat test time. We begin by supposing that data representations can be\nsemantically separated into task-relevant content features, which contain\ninformation relevant to downstream tasks, and task-irrelevant style features,\nwhich encompass superficial attributes that are irrelevant to these tasks, yet\nmay degrade performance due to associations with content present in training\ndata that do not generalize. We then prove that our anti-contrastive penalty,\nwhich we call Pair-Switching (PS), minimizes the Mutual Information between the\nstyle attributes and content labels. Finally, we instantiate CLEAR in the\nlatent space of a Variational Auto-Encoder (VAE), then perform experiments to\nquantitatively and qualitatively evaluate the resulting CLEAR-VAE over several\nimage datasets. Our results show that CLEAR-VAE allows us to: (a) swap and\ninterpolate content and style between any pair of samples, and (b) improve\ndownstream classification performance in the presence of previously unseen\ncombinations of content and style. Our code will be made publicly available.", "comment": "10 pages main text, 24 pages in total", "pdf_url": "http://arxiv.org/pdf/2507.18794v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "CLEAR：使用对比学习与反对比正则化解耦虚假风格-内容关联", "tldr": "CLEAR是一种新的框架，通过反对比正则化有效分离任务相关内容和任务无关风格，从而提高在风格偏移情况下的预测性能。", "motivation": "在机器学习应用中，特别是在医疗保健等领域，学习不受表面特征（如种族、性别）影响的表示至关重要，以确保在测试时这些特征发生变化时，下游预测性能不会受到影响，从而保证预测的公平性和泛化性。", "method": "本文提出了对比学习与反对比正则化（CLEAR）框架，旨在训练期间有效分离任务相关内容特征和任务无关风格特征。CLEAR通过一个名为Pair-Switching（PS）的反对比惩罚来最小化风格属性和内容标签之间的互信息。该框架在变分自编码器（VAE）的潜在空间中实现，形成CLEAR-VAE。", "result": "CLEAR-VAE允许在任何样本对之间交换和插值内容和风格，并且在出现以前未见的内容和风格组合时，能够提高下游分类性能。", "conclusion": "CLEAR框架，特别是其实例化CLEAR-VAE，能够有效解耦数据中的内容和风格特征，从而在表面特征发生偏移时，显著提高下游预测任务的泛化性和性能。", "translation": "学习不受表面特征影响的表示至关重要，以确保在测试时这些特征发生变化时，下游预测性能不会受到影响。例如，在医疗保健应用中，我们可能希望学习包含病理信息但不受种族、性别和其他生理变异来源影响的特征，从而确保预测在所有人口统计学上都是公平和可泛化的。本文提出了一种直观且易于实现的框架——对比学习与反对比正则化（CLEAR），它在训练期间有效地将基本（即任务相关）特征与表面（即任务无关）特征分离，从而在测试时表面特征发生变化时获得更好的性能。我们首先假设数据表示可以在语义上分为任务相关的内容特征（包含与下游任务相关的信息）和任务无关的风格特征（包含与这些任务无关的表面属性，但由于与训练数据中存在的、无法泛化的内容关联而可能降低性能）。然后，我们证明了我们的反对比惩罚，我们称之为Pair-Switching（PS），最小化了风格属性和内容标签之间的互信息。最后，我们在变分自编码器（VAE）的潜在空间中实例化了CLEAR，然后通过实验在多个图像数据集上定量和定性地评估了所得的CLEAR-VAE。我们的结果表明，CLEAR-VAE使我们能够：（a）在任何样本对之间交换和插值内容和风格，以及（b）在存在以前未见的内容和风格组合时，提高下游分类性能。我们的代码将公开发布。", "summary": "本文提出了CLEAR（Contrastive LEarning with Anti-contrastive Regularization）框架，旨在解决机器学习模型在测试时因表面特征（如风格）变化而导致的性能下降问题。CLEAR通过一个名为Pair-Switching（PS）的反对比惩罚，最小化任务无关风格和任务相关内容标签之间的互信息，从而有效分离两者。在变分自编码器（VAE）中实现后，CLEAR-VAE展示了在内容和风格解耦方面的能力，并显著提升了在存在未见风格内容组合情况下的下游分类性能，确保了模型的公平性和泛化性。", "keywords": "对比学习, 解耦表示, 风格-内容分离, 反对比正则化, 泛化性", "comments": "CLEAR框架通过引入反对比正则化（Pair-Switching）来显式地最小化风格和内容标签之间的互信息，这是一种新颖且直观的方法，用于解耦数据表示中的任务相关和任务无关特征。其创新性在于提出了一种简单易行却高效的机制来处理虚假关联，这对于提高模型在领域偏移下的泛化能力和公平性具有重要意义。在医疗保健等敏感领域，这种方法尤其有价值。"}}
{"id": "2507.04790", "title": "Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning", "authors": ["Giwon Lee", "Wooseong Jeong", "Daehee Park", "Jaewoo Jeong", "Kuk-Jin Yoon"], "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025 (Highlight)", "url": "http://arxiv.org/abs/2507.04790v3", "summary": "Motion planning is a crucial component of autonomous robot driving. While\nvarious trajectory datasets exist, effectively utilizing them for a target\ndomain remains challenging due to differences in agent interactions and\nenvironmental characteristics. Conventional approaches, such as domain\nadaptation or ensemble learning, leverage multiple source datasets but suffer\nfrom domain imbalance, catastrophic forgetting, and high computational costs.\nTo address these challenges, we propose Interaction-Merged Motion Planning\n(IMMP), a novel approach that leverages parameter checkpoints trained on\ndifferent domains during adaptation to the target domain. IMMP follows a\ntwo-step process: pre-merging to capture agent behaviors and interactions,\nsufficiently extracting diverse information from the source domain, followed by\nmerging to construct an adaptable model that efficiently transfers diverse\ninteractions to the target domain. Our method is evaluated on various planning\nbenchmarks and models, demonstrating superior performance compared to\nconventional approaches.", "comment": "Accepted at ICCV 2025 (Highlight)", "pdf_url": "http://arxiv.org/pdf/2507.04790v3", "cate": "cs.RO", "date": "2025-07-07", "updated": "2025-07-25", "AI": {"title_translation": "交互融合运动规划：有效利用多样化运动数据集实现鲁棒规划", "tldr": "提出了一种名为IMMP的新方法，通过两步合并过程有效利用多样化运动数据集，以解决传统方法在运动规划中面临的领域不平衡和计算成本高昂等挑战，并在各种规划基准上表现出优越性能。", "motivation": "现有的轨迹数据集利用效率低下，因为不同智能体交互和环境特征的差异导致传统方法（如领域适应或集成学习）存在领域不平衡、灾难性遗忘和高计算成本等问题。", "method": "提出交互融合运动规划（IMMP），这是一种新颖的方法，在适应目标域期间利用在不同域上训练的参数检查点。IMMP遵循两步过程：首先是预合并，以捕获智能体行为和交互，充分提取源域中的多样化信息；然后是合并，以构建一个可适应的模型，将多样化交互有效地转移到目标域。", "result": "该方法在各种规划基准和模型上进行了评估，与传统方法相比，表现出卓越的性能。", "conclusion": "通过提出IMMP，该研究成功地解决了在运动规划中有效利用多样化运动数据集的挑战，并实现了优于传统方法的鲁棒规划。", "translation": "运动规划是自主机器人驾驶的关键组成部分。虽然存在各种轨迹数据集，但由于智能体交互和环境特征的差异，有效利用它们来实现目标领域仍然具有挑战性。传统的域适应或集成学习等方法利用多个源数据集，但存在域不平衡、灾难性遗忘和高计算成本等问题。为了应对这些挑战，我们提出了一种名为交互融合运动规划（IMMP）的新方法，该方法在适应目标域期间利用在不同域上训练的参数检查点。IMMP遵循两步过程：首先是预合并，以捕获智能体行为和交互，充分提取源域中的多样化信息；然后是合并，以构建一个可适应的模型，将多样化交互有效地转移到目标域。我们的方法在各种规划基准和模型上进行了评估，与传统方法相比，表现出卓越的性能。", "summary": "该论文提出了一种名为交互融合运动规划（IMMP）的新颖方法，旨在解决自主机器人运动规划中有效利用多样化运动数据集的挑战。针对传统方法在多源数据集利用中存在的领域不平衡、灾难性遗忘和高计算成本等问题，IMMP采用两步合并过程：首先进行预合并以捕捉智能体行为并提取源域信息，然后进行合并以构建一个可适应模型，将多样化交互高效转移到目标域。实验结果表明，IMMP在各种规划基准上均优于传统方法。", "keywords": "运动规划, 数据集利用, 领域适应, 交互融合, 鲁棒规划", "comments": "该论文的创新点在于提出了两步合并过程的IMMP方法，有效解决了传统领域适应和集成学习在处理多样化运动数据集时面临的挑战。通过利用参数检查点，它能够更有效地整合不同领域的信息，避免了灾难性遗忘和高计算成本，这对于提高自主驾驶系统的鲁棒性和泛化能力具有重要意义。"}}
{"id": "2507.19260", "title": "Cell-based VSC Analysis Methodology: From Graph Laplacian to Converter Degrees of Freedom", "authors": ["Daniele Falchi", "Eduardo Prieto-Araujo", "Oriol Gomis-Bellmunt"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19260v1", "summary": "Power-electronics-based converters are being considerably employed through\nthe power system to interconnect multiple heterogeneous electrical layers.\nFurthermore, the intrinsic versatility to play with the converter network\ntopology is widely exploited to accommodate a certain number of terminals and\nports according with the specific application. On this regard, several\nconverter arrangements can be encountered in power applications. Moreover, to\nproperly establish both the operation and the control, the so-called degrees of\nfreedom (DOFs) need to be assessed per each converter topology. On this matter,\nsimilarly to the well-known Clarke transformation, which clearly reveals the\nDOFs for the star-based topology system, further similar transformations can be\nachieved to depict the independent set of variables characterizing a certain\nconverter structure. Referring to the cell-based class of Voltage Source\nConverter (VSC) topologies, including Modular Multilevel Converter (MMC); this\narticle proposes a general methodology to determine the change of variable\nmatrix transformation for several converter arrangements which are related to\ncomplete bi-partite and multi-partite graphs. The methodology lies in the graph\nLaplacian spectral analysis, which remarks the structural normal modes at the\nconverter points of connections. Furthermore, for a complete characterization,\nthe instantaneous power patterns formulations, based on the DOFs, are also\nintroduced.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19260v1", "cate": "eess.SY", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于单元的VSC分析方法：从图拉普拉斯到变换器自由度", "tldr": "本文提出了一种基于图拉普拉斯谱分析的通用方法，用于确定各种电池型电压源变换器（VSC）拓扑的变量变换矩阵和自由度。", "motivation": "电力电子变换器在电力系统中被广泛用于互连异构电气层，并且其网络拓扑的内在多功能性被广泛利用以适应特定的应用。为了正确建立变换器的运行和控制，需要评估每种变换器拓扑的自由度（DOFs）。类似于Clarke变换揭示了星形拓扑系统的DOFs，需要进一步的变换来描绘表征特定变换器结构的独立变量集。", "method": "本文提出了一种通用方法来确定适用于多种与完整二分图和多分图相关的变换器配置的变量变换矩阵变换。该方法基于图拉普拉斯谱分析，它揭示了变换器连接点的结构法向模式。此外，为了进行完整的表征，还引入了基于DOFs的瞬时功率模式公式。", "result": "该方法能够确定各种电池型电压源变换器（VSC）拓扑（包括模块化多电平变换器MMC）的变量变换矩阵，并揭示变换器连接点的结构法向模式。此外，还引入了基于自由度（DOFs）的瞬时功率模式公式，以实现完整的表征。", "conclusion": "本文提出了一种基于图拉普拉斯谱分析的通用方法，能够系统地确定电池型电压源变换器（VSC）拓扑的自由度（DOFs）和变量变换矩阵，从而有助于其操作和控制的建立。", "translation": "基于电力电子的变换器在电力系统中被大量用于互连多个异构电气层。此外，其固有的多功能性，即可以利用变换器网络拓扑结构来适应特定数量的终端和端口，也根据具体应用被广泛利用。在这方面，电力应用中可以遇到多种变换器配置。而且，为了正确建立运行和控制，需要针对每种变换器拓扑评估所谓的自由度（DOFs）。在这方面，类似于众所周知的Clarke变换，它清楚地揭示了星形拓扑系统的DOFs，可以实现进一步的类似变换来描绘表征特定变换器结构的独立变量集。针对基于单元的电压源变换器（VSC）拓扑类别，包括模块化多电平变换器（MMC）；本文提出了一种通用方法来确定适用于多种与完整二分图和多分图相关的变换器配置的变量变换矩阵变换。该方法基于图拉普拉斯谱分析，它揭示了变换器连接点的结构法向模式。此外，为了进行完整的表征，还引入了基于DOFs的瞬时功率模式公式。", "summary": "本文提出了一种针对电池型电压源变换器（VSC）拓扑（如模块化多电平变换器MMC）的通用分析方法。该方法利用图拉普拉斯谱分析来确定变换器的变量变换矩阵和自由度（DOFs），从而揭示其连接点的结构法向模式。此外，还引入了基于DOFs的瞬时功率模式公式，以实现对变换器特性的全面表征，旨在协助其操作和控制的建立。", "keywords": "电压源变换器, 自由度, 图拉普拉斯, 谱分析, 模块化多电平变换器", "comments": "本文的创新之处在于将图拉普拉斯谱分析应用于电压源变换器（VSC）的自由度（DOFs）确定，为理解和控制复杂的变换器拓扑提供了一种系统性的方法。这种基于图论的视角为电力电子变换器分析提供了一个新颖且通用的框架。"}}
{"id": "2507.19292", "title": "PINO: Person-Interaction Noise Optimization for Long-Duration and Customizable Motion Generation of Arbitrary-Sized Groups", "authors": ["Sakuya Ota", "Qing Yu", "Kent Fujiwara", "Satoshi Ikehata", "Ikuro Sato"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025, Project page: this https URL", "url": "http://arxiv.org/abs/2507.19292v1", "summary": "Generating realistic group interactions involving multiple characters remains\nchallenging due to increasing complexity as group size expands. While existing\nconditional diffusion models incrementally generate motions by conditioning on\npreviously generated characters, they rely on single shared prompts, limiting\nnuanced control and leading to overly simplified interactions. In this paper,\nwe introduce Person-Interaction Noise Optimization (PINO), a novel,\ntraining-free framework designed for generating realistic and customizable\ninteractions among groups of arbitrary size. PINO decomposes complex group\ninteractions into semantically relevant pairwise interactions, and leverages\npretrained two-person interaction diffusion models to incrementally compose\ngroup interactions. To ensure physical plausibility and avoid common artifacts\nsuch as overlapping or penetration between characters, PINO employs\nphysics-based penalties during noise optimization. This approach allows precise\nuser control over character orientation, speed, and spatial relationships\nwithout additional training. Comprehensive evaluations demonstrate that PINO\ngenerates visually realistic, physically coherent, and adaptable multi-person\ninteractions suitable for diverse animation, gaming, and robotics applications.", "comment": "Accepted to ICCV 2025, Project page: https://sinc865.github.io/pino/", "pdf_url": "http://arxiv.org/pdf/2507.19292v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "PINO：用于任意大小群体长时间、可定制运动生成的个体交互噪声优化", "tldr": "PINO是一个无需训练的框架，通过分解为双人交互并结合物理惩罚，为任意大小的群体生成逼真、可定制的多人交互运动，解决了现有模型控制不足和物理不真实的问题。", "motivation": "现有条件扩散模型在生成多人组交互时，随着组规模扩大复杂性增加，且依赖单一共享提示，导致控制有限和交互过于简化，难以生成逼真、细致的群体交互。", "method": "PINO（个体交互噪声优化）是一个新颖的、无需训练的框架。它将复杂的群体交互分解为语义相关的成对交互，并利用预训练的双人交互扩散模型逐步合成群体交互。为确保物理合理性并避免重叠或穿透等问题，PINO在噪声优化过程中采用基于物理的惩罚机制，从而实现对角色方向、速度和空间关系的精确用户控制，而无需额外训练。", "result": "PINO能够生成视觉逼真、物理连贯且适应性强的多人交互，适用于各种动画、游戏和机器人应用。", "conclusion": "PINO通过创新的分解和物理惩罚机制，成功地解决了大规模群体交互生成中的复杂性与控制难题，提供了一种无需训练即可生成高质量、可定制多人运动的有效方法。", "translation": "生成涉及多个角色的逼真群体交互仍然具有挑战性，因为随着群体规模的扩大，复杂性也随之增加。虽然现有的条件扩散模型通过以先前生成的角色为条件来逐步生成运动，但它们依赖于单一共享提示，限制了细致的控制，并导致交互过于简化。在本文中，我们引入了个体交互噪声优化（PINO），这是一个新颖的、无需训练的框架，旨在生成任意大小群体之间逼真且可定制的交互。PINO将复杂的群体交互分解为语义相关的成对交互，并利用预训练的双人交互扩散模型逐步合成群体交互。为了确保物理合理性并避免字符之间的重叠或穿透等常见伪影，PINO在噪声优化过程中采用了基于物理的惩罚。这种方法允许用户精确控制角色的方向、速度和空间关系，而无需额外训练。全面的评估表明，PINO生成的多人交互在视觉上逼真、物理上连贯且适应性强，适用于各种动画、游戏和机器人应用。", "summary": "PINO是一个无需训练的框架，用于生成任意大小群体的逼真和可定制交互。它通过将复杂的群体交互分解为成对交互，并利用预训练的双人交互扩散模型逐步合成。为确保物理真实性，PINO在噪声优化中加入物理惩罚，从而实现对角色运动的精确控制。该方法能够生成视觉逼真、物理连贯且适应性强的多人交互，适用于动画、游戏和机器人等领域。", "keywords": "群体交互, 运动生成, 噪声优化, 无需训练, 扩散模型", "comments": "PINO的创新之处在于其“无需训练”的框架，通过将复杂的多人交互分解为更简单的双人交互，并结合物理惩罚来保证真实性，显著提升了大规模群体运动生成的效率和质量。这种方法在实际应用中具有很高的价值，因为它避免了昂贵的再训练成本，并提供了细致的用户控制。"}}
{"id": "2507.17958", "title": "VIBE: Video-Input Brain Encoder for fMRI Response Modeling", "authors": ["Daniel Carlström Schad", "Shrey Dixit", "Janis Keck", "Viktor Studenyak", "Aleksandr Shpilevoi", "Andrej Bicanski"], "categories": ["cs.LG", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17958v2", "summary": "We present VIBE, a two-stage Transformer that fuses multi-modal video, audio,\nand text features to predict fMRI activity. Representations from open-source\nmodels (Qwen2.5, BEATs, Whisper, SlowFast, V-JEPA) are merged by a\nmodality-fusion transformer and temporally decoded by a prediction transformer\nwith rotary embeddings. Trained on 65 hours of movie data from the CNeuroMod\ndataset and ensembled across 20 seeds, VIBE attains mean parcel-wise Pearson\ncorrelations of 0.3225 on in-distribution Friends S07 and 0.2125 on six\nout-of-distribution films. An earlier iteration of the same architecture\nobtained 0.3198 and 0.2096, respectively, winning Phase-1 and placing second\noverall in the Algonauts 2025 Challenge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17958v2", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-25", "AI": {"title_translation": "VIBE：用于fMRI响应建模的视频输入大脑编码器", "tldr": "VIBE是一个两阶段Transformer模型，它融合视频、音频和文本多模态特征来预测fMRI活动，并在电影数据集上表现出色，赢得了Algonauts 2025挑战赛的部分奖项。", "motivation": "该研究旨在开发一个能够融合多模态视频、音频和文本特征来准确预测fMRI（功能性磁共振成像）活动的新模型。", "method": "研究提出了VIBE模型，这是一个两阶段的Transformer架构。第一阶段是模态融合Transformer，用于合并来自开源模型（如Qwen2.5、BEATs、Whisper、SlowFast、V-JEPA）的视频、音频和文本表示。第二阶段是预测Transformer，带有旋转嵌入，用于时间解码。模型在CNeuroMod数据集的65小时电影数据上进行训练，并对20个种子进行集成。", "result": "VIBE模型在分布内数据（Friends S07）上取得了0.3225的平均体素级皮尔逊相关系数，在六部分布外电影上取得了0.2125的相关系数。该架构的早期版本在Algonauts 2025挑战赛中赢得了第一阶段冠军并获得总成绩第二名，相关系数分别为0.3198和0.2096。", "conclusion": "VIBE模型通过有效融合多模态视频、音频和文本特征，能够准确预测fMRI活动，并在脑解码挑战赛中表现出卓越的性能，证明了其在神经影像建模方面的有效性和潜力。", "translation": "我们提出了VIBE，一个两阶段的Transformer模型，它融合了多模态视频、音频和文本特征来预测fMRI活动。来自开源模型（Qwen2.5、BEATs、Whisper、SlowFast、V-JEPA）的表示通过一个模态融合Transformer进行合并，并通过一个带有旋转嵌入的预测Transformer进行时间解码。VIBE在CNeuroMod数据集的65小时电影数据上进行训练，并对20个种子进行集成，在分布内数据《老友记》S07上获得了0.3225的平均体素级皮尔逊相关系数，在六部分布外电影上获得了0.2125的相关系数。相同架构的早期版本分别获得了0.3198和0.2096，赢得了Algonauts 2025挑战赛的第一阶段冠军并获得总成绩第二名。", "summary": "VIBE是一个创新的两阶段Transformer模型，旨在通过融合视频、音频和文本等多模态特征来预测fMRI活动。它利用Qwen2.5、BEATs等开源模型提取特征，并通过模态融合和预测Transformer进行处理。该模型在CNeuroMod电影数据集上训练，并在fMRI响应预测任务中表现出色，在分布内和分布外数据上均取得了高相关性，并在Algonauts 2025挑战赛中获得佳绩。", "keywords": "fMRI响应建模, 多模态融合, Transformer, VIBE, 脑编码器", "comments": "VIBE的创新之处在于其多模态特征融合能力，结合了视频、音频和文本信息来预测大脑活动，这对于理解复杂的大脑响应至关重要。其两阶段Transformer架构和对多种先进开源模型的利用，展示了其强大的数据处理和预测能力。在Algonauts 2025挑战赛中的出色表现进一步验证了其有效性和实用性，为fMRI响应建模领域提供了一个高性能的解决方案。"}}
{"id": "2507.18640", "title": "How good are humans at detecting AI-generated images? Learnings from an experiment", "authors": ["Thomas Roca", "Anthony Cintron Roman", "Jehú Torres Vega", "Marcelo Duarte", "Pengce Wang", "Kevin White", "Amit Misra", "Juan Lavista Ferres"], "categories": ["cs.HC", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18640v1", "summary": "As AI-powered image generation improves, a key question is how well human\nbeings can differentiate between \"real\" and AI-generated or modified images.\nUsing data collected from the online game \"Real or Not Quiz.\", this study\ninvestigates how effectively people can distinguish AI-generated images from\nreal ones. Participants viewed a randomized set of real and AI-generated\nimages, aiming to identify their authenticity. Analysis of approximately\n287,000 image evaluations by over 12,500 global participants revealed an\noverall success rate of only 62\\%, indicating a modest ability, slightly above\nchance. Participants were most accurate with human portraits but struggled\nsignificantly with natural and urban landscapes. These results highlight the\ninherent challenge humans face in distinguishing AI-generated visual content,\nparticularly images without obvious artifacts or stylistic cues. This study\nstresses the need for transparency tools, such as watermarks and robust AI\ndetection tools to mitigate the risks of misinformation arising from\nAI-generated content", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18640v1", "cate": "cs.HC", "date": "2025-05-12", "updated": "2025-05-12", "AI": {"title_translation": "人类检测AI生成图像的能力如何？一项实验的启示", "tldr": "人类在检测AI生成图像方面的能力有限，准确率仅为62%，略高于随机猜测。他们在识别肖像时表现较好，但在自然和城市景观方面表现不佳。研究强调需要透明度工具和强大的AI检测工具来应对虚假信息风险。", "motivation": "随着AI图像生成技术的进步，一个关键问题是人类区分“真实”图像和AI生成或修改图像的能力。本研究旨在调查人们识别AI生成图像和真实图像的有效性。", "method": "本研究使用从在线游戏“真实还是不真实测验”中收集的数据。参与者（超过12,500名全球参与者）查看了一组随机的真实和AI生成图像，旨在识别其真实性。研究分析了大约287,000次图像评估数据。", "result": "分析显示，总体成功率仅为62%，表明人类的识别能力一般，略高于随机猜测。参与者在识别人类肖像时最准确，但在自然和城市景观方面则表现出显著的困难。", "conclusion": "人类在区分AI生成视觉内容方面面临固有的挑战，特别是那些没有明显伪影或风格线索的图像。本研究强调需要透明度工具（如水印）和强大的AI检测工具，以减轻AI生成内容引起的错误信息风险。", "translation": "随着AI驱动的图像生成技术不断进步，一个关键问题是人类在“真实”图像和AI生成或修改图像之间区分的能力如何。本研究利用从在线游戏“真实还是不真实测验”中收集的数据，调查了人们区分AI生成图像和真实图像的有效性。参与者查看了一组随机的真实和AI生成图像，旨在识别它们的真实性。对来自全球超过12,500名参与者的大约287,000次图像评估的分析显示，总体成功率仅为62%，表明能力一般，略高于随机猜测。参与者在识别人类肖像时最准确，但在自然和城市景观方面则表现出显著的困难。这些结果凸显了人类在区分AI生成视觉内容方面所面临的固有挑战，特别是那些没有明显伪影或风格线索的图像。本研究强调需要透明度工具，例如水印和强大的AI检测工具，以减轻AI生成内容引起的错误信息风险。", "summary": "一项基于在线游戏“真实还是不真实测验”的研究，分析了全球超过12,500名参与者对约287,000张图像的评估数据，旨在探究人类区分AI生成图像和真实图像的能力。结果显示，人类的总体识别成功率仅为62%，表明能力有限，略高于随机猜测。尽管在识别肖像方面表现较好，但在自然和城市景观方面则显著困难。研究强调人类在检测AI生成视觉内容上的挑战，并呼吁开发透明度工具和强大的AI检测工具来应对虚假信息风险。", "keywords": "AI生成图像, 人类检测, 虚假信息, 透明度工具, 图像真实性", "comments": "这项研究通过大规模实验量化了人类在识别AI生成图像方面的能力，具有重要的现实意义。其数据量庞大（超过12,500名参与者和287,000次评估），增强了结果的可靠性。研究结果揭示了在当前AI技术背景下，人类面对虚假信息传播的脆弱性，并提出了实用的解决方案，即推广透明度工具和更强大的AI检测工具，这对于应对AI内容带来的社会挑战至关重要。"}}
{"id": "2507.19057", "title": "Exploring molecular assembly as a biosignature using mass spectrometry and machine learning", "authors": ["Lindsay A. Rutter", "Abhishek Sharma", "Ian Seet", "David Obeh Alobo", "An Goto", "Leroy Cronin"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      35 pages,7 figures, 62 references", "url": "http://arxiv.org/abs/2507.19057v1", "summary": "Molecular assembly offers a promising path to detect life beyond Earth, while\nminimizing assumptions based on terrestrial life. As mass spectrometers will be\ncentral to upcoming Solar System missions, predicting molecular assembly from\ntheir data without needing to elucidate unknown structures will be essential\nfor unbiased life detection. An ideal agnostic biosignature must be\ninterpretable and experimentally measurable. Here, we show that molecular\nassembly, a recently developed approach to measure objects that have been\nproduced by evolution, satisfies both criteria. First, it is interpretable for\nlife detection, as it reflects the assembly of molecules with their bonds as\nbuilding blocks, in contrast to approaches that discount construction history.\nSecond, it can be determined without structural elucidation, as it can be\nphysically measured by mass spectrometry, a property that distinguishes it from\nother approaches that use structure-based information measures for molecular\ncomplexity. Whilst molecular assembly is directly measurable using mass\nspectrometry data, there are limits imposed by mission constraints. To address\nthis, we developed a machine learning model that predicts molecular assembly\nwith high accuracy, reducing error by three-fold compared to baseline models.\nSimulated data shows that even small instrumental inconsistencies can double\nmodel error, emphasizing the need for standardization. These results suggest\nthat standardized mass spectrometry databases could enable accurate molecular\nassembly prediction, without structural elucidation, providing a\nproof-of-concept for future astrobiology missions.", "comment": "35 pages,7 figures, 62 references", "pdf_url": "http://arxiv.org/pdf/2507.19057v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "探索分子组装作为生物标记物，利用质谱和机器学习", "tldr": "该研究提出利用分子组装作为一种普适的生物标记物来探测地球外生命，通过质谱数据结合机器学习模型进行预测，无需结构解析，为未来的天体生物学任务提供了概念验证，并强调了数据标准化的重要性。", "motivation": "为了无偏见地探测地球以外的生命，最大限度地减少基于地球生命的假设。由于质谱仪是未来太阳系任务的核心，因此需要一种无需阐明未知结构即可从质谱数据中预测生物标记物的方法。分子组装被提出作为一种可解释且可测量的普适生物标记物。", "method": "该研究提出分子组装作为一种普适的生物标记物，它能反映分子的构建历史，并可通过质谱仪直接测量，无需结构阐明。为了克服任务限制，研究人员开发了一个机器学习模型，能够高精度地从质谱数据中预测分子组装。", "result": "开发的机器学习模型能够高精度预测分子组装，与基线模型相比，误差降低了三倍。模拟数据表明，即使是小的仪器不一致也可能使模型误差增加一倍，强调了标准化质谱数据库的必要性。", "conclusion": "分子组装，结合机器学习模型和标准化质谱数据进行预测，为未来天体生物学任务中准确、独立于结构的生物标记物探测提供了概念验证，并强调了标准化数据库的重要性。", "translation": "分子组装为探测地球以外的生命提供了一条有前景的途径，同时最大限度地减少了基于地球生命的假设。由于质谱仪将成为未来太阳系任务的核心，因此无需阐明未知结构即可从其数据中预测分子组装对于无偏见的生命探测至关重要。一个理想的不可知论生物标记物必须是可解释且可实验测量的。在此，我们展示了分子组装，一种最近开发的用于测量通过进化产生的物体的方法，满足了这两个标准。首先，它对于生命探测是可解释的，因为它反映了分子以其键作为构建块的组装，这与不考虑构建历史的方法形成对比。其次，它可以在不进行结构阐明的情况下确定，因为可以通过质谱仪进行物理测量，这一特性使其区别于其他使用基于结构的信息测量方法来衡量分子复杂性的方法。虽然分子组装可以直接利用质谱数据进行测量，但任务限制会带来局限。为了解决这个问题，我们开发了一个机器学习模型，可以高精度地预测分子组装，与基线模型相比，误差减少了三倍。模拟数据显示，即使是小的仪器不一致也可能使模型误差增加一倍，强调了标准化的必要性。这些结果表明，标准化的质谱数据库可以在不进行结构阐明的情况下实现准确的分子组装预测，为未来的天体生物学任务提供了概念验证。", "summary": "该论文探索了将分子组装作为一种普适的生物标记物，用于地外生命探测，并结合了质谱技术和机器学习。分子组装被选作生物标记物，因为它既可解释（反映构建历史），又可通过质谱测量而无需结构解析。为克服任务限制，作者开发了一个机器学习模型，能高精度地从质谱数据中预测分子组装，将误差降低了三倍。研究展示了其在未来天体生物学任务中的概念验证，但强调了由于模型精度对仪器不一致性高度敏感，因此标准化质谱数据库至关重要。", "keywords": "分子组装, 生物标记物, 质谱, 机器学习, 天体生物学", "comments": "这篇论文提出了一种创新性的地外生命探测方法，将分子组装作为一种普适的生物标记物，这与传统的基于结构的方法有显著不同。将质谱与机器学习相结合，实现无需结构阐明即可进行预测，为空间任务提供了实用的解决方案。模拟结果对标准化的强调突出了一个关键的操作挑战，并为未来天体生物学任务的规划和数据管理提供了宝贵的预见。"}}
{"id": "2507.18764", "title": "Max-Min Fairness-Oriented Beamforming Design in HAPS-Enabled ISAC for 6G Networks", "authors": ["Parisa Kanani", "Mohammad Javad Omidi", "Mahmoud Modarres-Hashemi", "Halim Yanikomeroglu"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18764v1", "summary": "This paper presents a high-altitude platform station (HAPS)-enabled\nintegrated sensing and communication (ISAC) system designed for\nsixth-generation (6G) networks. Positioned in the stratosphere, HAPS serves as\na super-macro base station, leveraging advanced beamforming techniques to\nenable communication and sensing simultaneously. This research addresses the\nneed for equitable service distribution in 6G networks by focusing on fairness\nwithin the HAPS-ISAC system. It tackles a non-convex optimization problem that\nbalances sensing beampattern gain and signal-to-interference-plus-noise ratio\n(SINR) requirements among communication users (CUs) using a max-min fairness\napproach while adhering to power constraints. The proposed HAPS-ISAC framework\nensures efficient resource allocation, reliable coverage, and improved sensing\naccuracy. Simulation results validate the potential of HAPS-ISAC as a pivotal\nenabler for 6G networks and integrated communication-sensing systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18764v1", "cate": "eess.SP", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "面向6G网络中HAPS使能的ISAC系统的最大最小公平波束成形设计", "tldr": "本文提出了一种用于6G网络的HAPS使能的ISAC系统，通过最大最小公平波束成形解决非凸优化问题，以实现公平服务分配、高效资源利用和提升传感精度。", "motivation": "本研究旨在解决第六代（6G）网络中公平服务分配的需求，特别是在高空平台站（HAPS）使能的集成传感与通信（ISAC）系统中实现公平性。", "method": "本文提出了一种HAPS使能的ISAC系统，并利用先进的波束成形技术。研究通过采用最大最小公平方法，解决了一个非凸优化问题，该问题在遵守功率约束的同时，平衡了传感波束图增益和通信用户（CU）的信干噪比（SINR）要求。", "result": "所提出的HAPS-ISAC框架确保了高效的资源分配、可靠的覆盖和改进的传感精度。仿真结果验证了HAPS-ISAC作为6G网络和集成通信传感系统关键使能者的潜力。", "conclusion": "HAPS-ISAC系统在6G网络中具有巨大潜力，能够实现高效资源分配、可靠覆盖和高精度传感，是未来集成通信传感系统的关键技术。", "translation": "本文提出了一种专为第六代（6G）网络设计的高空平台站（HAPS）使能的集成传感与通信（ISAC）系统。HAPS位于平流层，作为超级宏基站，利用先进的波束成形技术同时实现通信和传感。本研究通过关注HAPS-ISAC系统内的公平性，解决了6G网络中公平服务分配的需求。它通过最大最小公平方法，在遵守功率约束的同时，处理了一个非凸优化问题，该问题平衡了传感波束图增益和通信用户（CU）的信干噪比（SINR）要求。所提出的HAPS-ISAC框架确保了高效的资源分配、可靠的覆盖和改进的传感精度。仿真结果验证了HAPS-ISAC作为6G网络和集成通信传感系统关键使能者的潜力。", "summary": "本文介绍了一种用于6G网络的HAPS使能的ISAC系统，旨在通过最大最小公平波束成形设计解决公平服务分配问题。该方法通过解决一个非凸优化问题，平衡了传感增益与通信用户SINR需求，并在仿真中展示了其在资源分配、覆盖和传感精度方面的优势，证明了其作为6G关键技术的潜力。", "keywords": "HAPS, ISAC, 6G, 波束成形, 最大最小公平", "comments": "该研究创新性地将HAPS、ISAC和最大最小公平波束成形结合，以解决6G网络中的公平服务分配问题。其在平衡传感与通信性能方面的优化方法具有重要意义，为未来集成系统提供了新的设计思路。"}}
{"id": "2507.18956", "title": "A Similarity Measure for Comparing Conversational Dynamics", "authors": ["Sang Min Jung", "Kaixiang Zhang", "Cristian Danescu-Niculescu-Mizil"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Code and demos available in ConvoKit ( this https URL )", "url": "http://arxiv.org/abs/2507.18956v1", "summary": "The quality of a conversation goes beyond the individual quality of each\nreply, and instead emerges from how these combine into interactional patterns\nthat give the conversation its distinctive overall \"shape\". However, there is\nno robust automated method for comparing conversations in terms of their\noverall interactional dynamics. Such methods could enhance the analysis of\nconversational data and help evaluate conversational agents more holistically.\n  In this work, we introduce a similarity measure for comparing conversations\nwith respect to their dynamics. We design a validation framework for testing\nthe robustness of the metric in capturing differences in conversation dynamics\nand for assessing its sensitivity to the topic of the conversations. Finally,\nto illustrate the measure's utility, we use it to analyze conversational\ndynamics in a large online community, bringing new insights into the role of\nsituational power in conversations.", "comment": "Code and demos available in ConvoKit (https://convokit.cornell.edu/)", "pdf_url": "http://arxiv.org/pdf/2507.18956v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "一种用于比较对话动态的相似性度量", "tldr": "该研究提出了一种新的相似性度量方法，用于量化和比较对话的整体动态，并通过一个验证框架评估了其鲁棒性，并在一个大型在线社区中应用以揭示情境权力在对话中的作用。", "motivation": "当前缺乏一种鲁棒的自动化方法来比较对话的整体交互动态，而这种方法对于增强对话数据分析和更全面地评估对话代理至关重要。", "method": "本文引入了一种用于比较对话动态的相似性度量方法。为验证其鲁棒性及对对话主题的敏感性，研究人员设计了一个验证框架。随后，该度量被应用于分析一个大型在线社区的对话动态。", "result": "该相似性度量成功地用于分析大型在线社区的对话动态，并为理解情境权力在对话中的作用提供了新的见解。", "conclusion": "该研究成功引入了一种用于比较对话动态的鲁棒相似性度量，并证明了其在分析真实对话数据和揭示对话中情境权力作用方面的实用性。", "translation": "对话的质量超越了单个回复的质量，而是源于这些回复如何结合形成交互模式，赋予对话独特的整体“形状”。然而，目前还没有一种鲁棒的自动化方法可以从整体交互动态方面比较对话。这种方法可以增强对话数据分析，并更全面地评估对话代理。在这项工作中，我们引入了一种用于比较对话动态的相似性度量。我们设计了一个验证框架，用于测试该度量在捕获对话动态差异方面的鲁棒性，并评估其对对话主题的敏感性。最后，为了说明该度量的实用性，我们将其用于分析大型在线社区的对话动态，为情境权力在对话中的作用带来了新的见解。", "summary": "本文提出了一种用于比较对话动态的相似性度量方法，旨在解决现有缺乏有效自动化工具的问题。研究人员设计了一个验证框架来评估该度量的鲁棒性和对主题的敏感性。此外，该度量被应用于分析大型在线社区的对话，揭示了情境权力在对话中的作用，从而为对话分析和会话代理评估提供了新的视角。", "keywords": "对话动态, 相似性度量, 交互模式, 在线社区, 情境权力", "comments": "该论文提出了一种创新的方法来量化和比较对话的动态，这对于理解对话的整体质量和评估对话系统具有重要意义。其验证框架和在真实社区的应用展示了该度量的实用性和潜力，为未来对话分析提供了新的工具。"}}
{"id": "2507.17792", "title": "Causal Mechanism Estimation in Multi-Sensor Systems Across Multiple Domains", "authors": ["Jingyi Yu", "Tim Pychynski", "Marco F. Huber"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17792v2", "summary": "To gain deeper insights into a complex sensor system through the lens of\ncausality, we present common and individual causal mechanism estimation\n(CICME), a novel three-step approach to inferring causal mechanisms from\nheterogeneous data collected across multiple domains. By leveraging the\nprinciple of Causal Transfer Learning (CTL), CICME is able to reliably detect\ndomain-invariant causal mechanisms when provided with sufficient samples. The\nidentified common causal mechanisms are further used to guide the estimation of\nthe remaining causal mechanisms in each domain individually. The performance of\nCICME is evaluated on linear Gaussian models under scenarios inspired from a\nmanufacturing process. Building upon existing continuous optimization-based\ncausal discovery methods, we show that CICME leverages the benefits of applying\ncausal discovery on the pooled data and repeatedly on data from individual\ndomains, and it even outperforms both baseline methods under certain scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17792v2", "cate": "cs.LG", "date": "2025-07-23", "updated": "2025-07-25", "AI": {"title_translation": "多领域多传感器系统中的因果机制估计", "tldr": "本文提出了一种名为CICME的新型三步法，用于从多领域异构数据中推断因果机制，通过因果迁移学习识别领域不变的因果机制，并在特定场景下优于基线方法。", "motivation": "为了通过因果关系视角深入了解复杂传感器系统。", "method": "本文提出了一种名为共同和个体因果机制估计（CICME）的新型三步法，用于从跨多个领域收集的异构数据中推断因果机制。该方法利用因果迁移学习（CTL）原理，能够可靠地检测领域不变的因果机制，并利用这些共同机制指导每个领域中剩余因果机制的个体估计。它建立在现有的基于连续优化的因果发现方法之上。", "result": "CICME在受制造过程启发的线性高斯模型场景中进行了评估。结果表明，CICME利用了在合并数据和个体领域数据上重复应用因果发现的优势，并且在某些场景下甚至优于两种基线方法。", "conclusion": "CICME方法能够有效估计多领域多传感器系统中的因果机制，并在特定场景下表现出优于现有方法的性能。", "translation": "为了通过因果关系视角深入了解复杂的传感器系统，我们提出了一种共同和个体因果机制估计（CICME）方法，这是一种新颖的三步法，用于从跨多个领域收集的异构数据中推断因果机制。通过利用因果迁移学习（CTL）原理，CICME能够在提供足够样本的情况下可靠地检测领域不变的因果机制。识别出的共同因果机制进一步用于指导每个领域中剩余因果机制的个体估计。CICME的性能在受制造过程启发的线性高斯模型场景中进行了评估。在现有基于连续优化的因果发现方法的基础上，我们表明CICME利用了在合并数据上和在个体领域数据上重复应用因果发现的优势，并且在某些场景下甚至优于两种基线方法。", "summary": "本文提出了一种新颖的三步法——共同和个体因果机制估计（CICME），旨在从跨多个领域的异构传感器数据中推断因果机制。该方法利用因果迁移学习识别领域不变的因果关系，并以此指导个体领域的机制估计。实验在受制造过程启发的线性高斯模型上进行，结果显示CICME能够有效结合合并数据和个体数据的优势，并在特定场景下优于现有基线方法。", "keywords": "因果机制估计, 多传感器系统, 多领域, 因果迁移学习, CICME", "comments": "该论文的创新之处在于提出了一种在多领域异构数据中估计因果机制的CICME方法，并巧妙地结合了因果迁移学习（CTL）来识别领域不变的因果关系。这种方法对于理解和分析复杂的多传感器系统具有重要意义，尤其是在工业制造等领域。其优势在于能够有效处理跨领域数据，并通过共同机制指导个体机制的发现，从而提高因果发现的可靠性和效率。"}}
{"id": "2507.19039", "title": "Autocallable Options Pricing with Integration-Based Exponential Amplitude Loading", "authors": ["Francesca Cibrario", "Ron Cohen", "Emanuele Dri", "Christian Mattia", "Or Samimi Golan", "Tamuz Danzig", "Giacomo Ranieri", "Hanan Rosemarin", "Davide Corbelletto", "Amir Naveh", "Bartolomeo Montrucchio"], "categories": ["quant-ph", "cs.ET", "q-fin.PR"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      11 pages, to be published in the proceedings of the IEEE International Conference on Quantum Computing and Engineering - QCE25", "url": "http://arxiv.org/abs/2507.19039v1", "summary": "We present a comprehensive quantum algorithm tailored for pricing\nautocallable options, offering a full implementation and experimental\nvalidation. Our experiments include simulations conducted on high-performance\ncomputing (HPC) hardware, along with an empirical analysis of convergence to\nthe classically estimated value. Our key innovation is an improved\nintegration-based exponential amplitude loading technique that reduces circuit\ndepth compared to state-of-the-art approaches. A detailed complexity analysis\nin a relevant setting shows an approximately 50x reduction in T-depth for the\npayoff component relative to previous methods. These contributions represent a\nstep toward more efficient quantum approaches to pricing complex financial\nderivatives.", "comment": "11 pages, to be published in the proceedings of the IEEE\n  International Conference on Quantum Computing and Engineering - QCE25", "pdf_url": "http://arxiv.org/pdf/2507.19039v1", "cate": "quant-ph", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "用基于积分的指数振幅加载对可赎回期权进行定价", "tldr": "提出一种新的量子算法，用于定价可赎回期权，通过改进的指数振幅加载技术显著减少了电路深度。", "motivation": "旨在开发更高效的量子方法来定价复杂的金融衍生品。", "method": "提出了一种全面的量子算法，专门用于可赎回期权定价，并采用了改进的基于积分的指数振幅加载技术。通过在高性能计算（HPC）硬件上进行模拟，并对收敛到经典估计值进行了经验分析。", "result": "关键创新是改进的基于积分的指数振幅加载技术，与现有方法相比，该技术显著减少了电路深度。在相关设置下的详细复杂性分析显示，支付组件的T-深度相对于以前的方法减少了大约50倍。", "conclusion": "这些贡献代表了在定价复杂金融衍生品方面，迈向更高效量子方法的一步。", "translation": "我们提出了一种全面的量子算法，专门用于可赎回期权定价，并提供了完整的实现和实验验证。我们的实验包括在高性能计算（HPC）硬件上进行的模拟，以及对经典估计值收敛的经验分析。我们的关键创新是一种改进的基于积分的指数振幅加载技术，与现有方法相比，该技术减少了电路深度。在相关设置下的详细复杂性分析显示，支付组件的T-深度相对于以前的方法减少了大约50倍。这些贡献代表了在定价复杂金融衍生品方面，迈向更高效量子方法的一步。", "summary": "该论文提出了一种新的量子算法，用于对可赎回期权进行定价。其核心创新在于采用了一种改进的基于积分的指数振幅加载技术，该技术能够显著降低量子电路的深度。通过在高性能计算硬件上的模拟和收敛性分析，实验验证了其有效性，并显示相比现有方法，在支付组件的T-深度上实现了约50倍的减少。这项工作为更高效的复杂金融衍生品量子定价方法奠定了基础。", "keywords": "量子算法, 可赎回期权, 指数振幅加载, 电路深度, 金融衍生品", "comments": "该论文的创新点在于提出了改进的基于积分的指数振幅加载技术，显著减少了量子电路深度，这对于量子金融应用具有重要意义。通过实验验证和详细的复杂性分析，增强了研究的可信度。其贡献在于推动了量子计算在金融衍生品定价领域的应用效率。"}}
{"id": "2507.18905", "title": "Large language models provide unsafe answers to patient-posed medical questions", "authors": ["Rachel L. Draelos", "Samina Afreen", "Barbara Blasko", "Tiffany Brazile", "Natasha Chase", "Dimple Desai", "Jessica Evert", "Heather L. Gardner", "Lauren Herrmann", "Aswathy Vaikom House", "Stephanie Kass", "Marianne Kavan", "Kirshma Khemani", "Amanda Koire", "Lauren M. McDonald", "Zahraa Rabeeah", "Amy Shah"], "categories": ["cs.CL", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      20 pages", "url": "http://arxiv.org/abs/2507.18905v1", "summary": "Millions of patients are already using large language model (LLM) chatbots\nfor medical advice on a regular basis, raising patient safety concerns. This\nphysician-led red-teaming study compares the safety of four publicly available\nchatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and\nLlama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation\nframework that enables quantitative and qualitative analysis. In total, 888\nchatbot responses are evaluated for 222 patient-posed advice-seeking medical\nquestions on primary care topics spanning internal medicine, women's health,\nand pediatrics. We find statistically significant differences between chatbots.\nThe rate of problematic responses varies from 21.6 percent (Claude) to 43.2\npercent (Llama), with unsafe responses varying from 5 percent (Claude) to 13\npercent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the\npotential to lead to serious patient harm. This study suggests that millions of\npatients could be receiving unsafe medical advice from publicly available\nchatbots, and further work is needed to improve the clinical safety of these\npowerful tools.", "comment": "20 pages", "pdf_url": "http://arxiv.org/pdf/2507.18905v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "大型语言模型为患者提出的医疗问题提供不安全的答案", "tldr": "大型语言模型（LLM）在提供医疗建议时存在不安全问题，许多回复不当或不安全，可能对患者造成伤害。", "motivation": "数百万患者已定期使用大型语言模型（LLM）聊天机器人获取医疗建议，这引发了患者安全担忧。", "method": "本研究是一项由医生主导的红队测试研究，使用名为HealthAdvice的新数据集和定量定性评估框架，比较了四种公开可用聊天机器人（Anthropic的Claude、Google的Gemini、OpenAI的GPT-4o和Meta的Llama3-70B）的安全性。总共评估了222个患者提出的初级保健医疗问题（涵盖内科、妇科和儿科）的888个聊天机器人回复。", "result": "聊天机器人之间存在统计学上的显著差异。问题回复率从21.6%（Claude）到43.2%（Llama）不等，不安全回复率从5%（Claude）到13%（GPT-4o、Llama）不等。定性结果显示，聊天机器人的回复有可能导致严重的患者伤害。", "conclusion": "本研究表明，数百万患者可能正在从公开可用的聊天机器人那里获得不安全的医疗建议，需要进一步努力来提高这些强大工具的临床安全性。", "translation": "数百万患者已定期使用大型语言模型（LLM）聊天机器人获取医疗建议，这引发了患者安全担忧。这项由医生主导的红队测试研究，在一个名为HealthAdvice的新数据集上，使用一个能够进行定量和定性分析的评估框架，比较了四种公开可用聊天机器人（Anthropic的Claude、Google的Gemini、OpenAI的GPT-4o和Meta的Llama3-70B）的安全性。总共评估了222个患者提出的、寻求建议的初级保健医疗问题（涵盖内科、妇科和儿科）的888个聊天机器人回复。我们发现聊天机器人之间存在统计学上的显著差异。问题回复率从21.6%（Claude）到43.2%（Llama）不等，不安全回复率从5%（Claude）到13%（GPT-4o、Llama）不等。定性结果显示，聊天机器人的回复有可能导致严重的患者伤害。这项研究表明，数百万患者可能正在从公开可用的聊天机器人那里获得不安全的医疗建议，需要进一步努力来提高这些强大工具的临床安全性。", "summary": "一项针对大型语言模型（LLM）聊天机器人提供医疗建议安全性的研究表明，这些工具经常提供不安全或有问题的回复。研究比较了Claude、Gemini、GPT-4o和Llama3-70B在222个患者提出的医疗问题上的表现，发现问题回复率和不安全回复率存在显著差异，且一些回复可能导致严重的患者伤害。这突出表明，迫切需要提升这些强大工具的临床安全性。", "keywords": "大型语言模型, 医疗建议, 患者安全, 聊天机器人, 红队测试", "comments": "这项研究揭示了LLM在医疗健康领域广泛应用所带来的一个关键的患者安全问题。其采用的红队测试方法以及定量和定性分析相结合的方式具有重要价值。研究结果强调了当前LLM在医疗建议等敏感应用方面的不足，并凸显了建立健全的安全机制和监管的紧迫性。"}}
{"id": "2507.19138", "title": "RealisVSR: Detail-enhanced Diffusion for Real-World 4K Video Super-Resolution", "authors": ["Weisong Zhao", "Jingkai Zhou", "Xiangyu Zhu", "Weihua Chen", "Xiao-Yu Zhang", "Zhen Lei", "Fan Wang"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19138v1", "summary": "Video Super-Resolution (VSR) has achieved significant progress through\ndiffusion models, effectively addressing the over-smoothing issues inherent in\nGAN-based methods. Despite recent advances, three critical challenges persist\nin VSR community: 1) Inconsistent modeling of temporal dynamics in foundational\nmodels; 2) limited high-frequency detail recovery under complex real-world\ndegradations; and 3) insufficient evaluation of detail enhancement and 4K\nsuper-resolution, as current methods primarily rely on 720P datasets with\ninadequate details. To address these challenges, we propose RealisVSR, a\nhigh-frequency detail-enhanced video diffusion model with three core\ninnovations: 1) Consistency Preserved ControlNet (CPC) architecture integrated\nwith the Wan2.1 video diffusion to model the smooth and complex motions and\nsuppress artifacts; 2) High-Frequency Rectified Diffusion Loss (HR-Loss)\ncombining wavelet decomposition and HOG feature constraints for texture\nrestoration; 3) RealisVideo-4K, the first public 4K VSR benchmark containing\n1,000 high-definition video-text pairs. Leveraging the advanced spatio-temporal\nguidance of Wan2.1, our method requires only 5-25% of the training data volume\ncompared to existing approaches. Extensive experiments on VSR benchmarks (REDS,\nSPMCS, UDM10, YouTube-HQ, VideoLQ, RealisVideo-720P) demonstrate our\nsuperiority, particularly in ultra-high-resolution scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19138v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "RealisVSR：面向真实世界4K视频超分辨率的细节增强扩散模型", "tldr": "RealisVSR提出了一种细节增强的扩散模型，通过新架构、损失函数和4K基准数据集解决了真实世界4K视频超分辨率中的细节恢复和评估挑战。", "motivation": "现有VSR扩散模型在处理真实世界复杂退化时，存在时间动态建模不一致、高频细节恢复受限以及缺乏4K细节增强评估基准等问题。", "method": "提出RealisVSR，一个高频细节增强的视频扩散模型，包含三项创新：1) 一致性保留ControlNet (CPC)：与Wan2.1视频扩散集成，用于建模平滑复杂运动并抑制伪影。2) 高频修正扩散损失 (HR-Loss)：结合小波分解和HOG特征约束以恢复纹理。3) RealisVideo-4K：首个公共4K VSR基准数据集，包含1,000个高清视频-文本对。该方法利用Wan2.1的先进时空引导，仅需现有方法5-25%的训练数据。", "result": "在多个VSR基准数据集（REDS, SPMCS, UDM10, YouTube-HQ, VideoLQ, RealisVideo-720P）上进行了广泛实验，证明了RealisVSR的优越性，尤其是在超高分辨率场景下。", "conclusion": "RealisVSR通过其创新的架构、损失函数和4K基准，有效地解决了真实世界4K视频超分辨率中的高频细节恢复和评估挑战，并展现出卓越的性能和训练数据效率。", "translation": "视频超分辨率（VSR）通过扩散模型取得了显著进展，有效解决了基于GAN的方法固有的过平滑问题。尽管最近取得了进展，VSR领域仍存在三个关键挑战：1）基础模型中时间动态建模不一致；2）在复杂真实世界退化下高频细节恢复有限；3）细节增强和4K超分辨率评估不足，因为当前方法主要依赖细节不足的720P数据集。为了解决这些挑战，我们提出了RealisVSR，一个高频细节增强的视频扩散模型，具有三项核心创新：1）与Wan2.1视频扩散集成的“一致性保留ControlNet（CPC）”架构，用于建模平滑和复杂运动并抑制伪影；2）结合小波分解和HOG特征约束的“高频修正扩散损失（HR-Loss）”用于纹理恢复；3）“RealisVideo-4K”，首个包含1,000个高清视频-文本对的公共4K VSR基准。利用Wan2.1先进的时空引导，我们的方法与现有方法相比，仅需5-25%的训练数据量。在VSR基准（REDS、SPMCS、UDM10、YouTube-HQ、VideoLQ、RealisVideo-720P）上的广泛实验表明，我们的方法具有优越性，特别是在超高分辨率场景下。", "summary": "RealisVSR提出了一种针对真实世界4K视频超分辨率的细节增强扩散模型，旨在解决现有方法在时间动态建模、高频细节恢复和4K评估方面的不足。该模型引入了三项核心创新：一致性保留ControlNet (CPC) 用于运动建模和伪影抑制，高频修正扩散损失 (HR-Loss) 用于纹理恢复，以及首个公共4K VSR基准RealisVideo-4K。实验证明，RealisVSR在多个基准测试中表现优异，尤其在超高分辨率场景下，并且训练数据效率高。", "keywords": "视频超分辨率, 扩散模型, 细节增强, 4K, RealisVSR", "comments": "这篇论文通过引入专门的架构（CPC）和损失函数（HR-Loss）来解决扩散模型在VSR中细节恢复的挑战，同时提供了一个急需的4K基准数据集（RealisVideo-4K），这对于推动该领域在高分辨率场景下的评估至关重要。其在数据效率方面的提升也值得关注。"}}
{"id": "2506.22648", "title": "Interact2Vec -- An efficient neural network-based model for simultaneously learning users and items embeddings in recommender systems", "authors": ["Pedro R. Pires", "Tiago A. Almeida"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Published in Applied Soft Computing (ASOC), 49 pages, 14 figures", "url": "http://arxiv.org/abs/2506.22648v3", "summary": "Over the past decade, recommender systems have experienced a surge in\npopularity. Despite notable progress, they grapple with challenging issues,\nsuch as high data dimensionality and sparseness. Representing users and items\nas low-dimensional embeddings learned via neural networks has become a leading\nsolution. However, while recent studies show promising results, many approaches\nrely on complex architectures or require content data, which may not always be\navailable. This paper presents Interact2Vec, a novel neural network-based model\nthat simultaneously learns distributed embeddings for users and items while\ndemanding only implicit feedback. The model employs state-of-the-art strategies\nthat natural language processing models commonly use to optimize the training\nphase and enhance the final embeddings. Two types of experiments were conducted\nregarding the extrinsic and intrinsic quality of the model. In the former, we\nbenchmarked the recommendations generated by Interact2Vec's embeddings in a\ntop-$N$ ranking problem, comparing them with six other recommender algorithms.\nThe model achieved the second or third-best results in 30% of the datasets,\nbeing competitive with other recommenders, and has proven to be very efficient\nwith an average training time reduction of 274% compared to other\nembedding-based models. Later, we analyzed the intrinsic quality of the\nembeddings through similarity tables. Our findings suggest that Interact2Vec\ncan achieve promising results, especially on the extrinsic task, and is an\nexcellent embedding-generator model for scenarios of scarce computing\nresources, enabling the learning of item and user embeddings simultaneously and\nefficiently.", "comment": "Published in Applied Soft Computing (ASOC), 49 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2506.22648v3", "cate": "cs.IR", "date": "2025-06-27", "updated": "2025-07-24", "AI": {"title_translation": "Interact2Vec -- 一种用于推荐系统中同时学习用户和物品嵌入的高效神经网络模型", "tldr": "Interact2Vec是一种高效的神经网络模型，仅通过隐式反馈同时学习用户和物品嵌入，在推荐任务中表现良好，尤其在计算资源有限的情况下。", "motivation": "推荐系统面临高数据维度和稀疏性问题。现有许多基于神经网络的嵌入学习方法要么架构复杂，要么需要内容数据，而内容数据并非总是可用。", "method": "本文提出Interact2Vec模型，一个新颖的基于神经网络的模型，仅通过隐式反馈同时学习用户和物品的分布式嵌入。该模型采用自然语言处理模型常用的先进策略来优化训练阶段并增强最终嵌入。进行了外在（top-N排名）和内在（相似性表）质量实验。", "result": "在top-N排名问题中，Interact2Vec在30%的数据集中取得了第二或第三好的结果，与其他推荐算法具有竞争力。与其它基于嵌入的模型相比，平均训练时间减少了274%，效率很高。内在质量分析也显示了积极结果。", "conclusion": "Interact2Vec在推荐任务中，尤其是在计算资源稀缺的情况下，能够高效地同时学习用户和物品嵌入，并取得有希望的结果，是一个优秀的嵌入生成模型。", "translation": "在过去的十年中，推荐系统的人气激增。尽管取得了显著进展，但它们仍面临着高数据维度和稀疏性等挑战性问题。将用户和物品表示为通过神经网络学习的低维嵌入已成为一种领先的解决方案。然而，尽管最近的研究显示出有希望的结果，但许多方法依赖于复杂的架构或需要内容数据，而这些数据可能并非总是可用。本文提出Interact2Vec，一种新颖的基于神经网络的模型，它仅通过隐式反馈同时学习用户和物品的分布式嵌入。该模型采用自然语言处理模型常用的最先进策略来优化训练阶段并增强最终嵌入。针对模型的外在和内在质量进行了两类实验。在前一类实验中，我们将Interact2Vec嵌入生成的推荐在top-N排名问题中与六种其他推荐算法进行了比较。该模型在30%的数据集中取得了第二或第三好的结果，与其他推荐器具有竞争力，并且事实证明它非常高效，与基于嵌入的其他模型相比，平均训练时间减少了274%。随后，我们通过相似性表分析了嵌入的内在质量。我们的发现表明，Interact2Vec可以取得有希望的结果，尤其是在外在任务上，并且在计算资源稀缺的情况下，它是一个出色的嵌入生成模型，能够高效地同时学习物品和用户嵌入。", "summary": "本文介绍了Interact2Vec，一种高效的神经网络模型，旨在解决推荐系统中数据维度高和稀疏性问题。该模型仅利用隐式反馈，通过借鉴自然语言处理技术，同时学习用户和物品的低维嵌入。实验结果表明，Interact2Vec在推荐准确性上与现有算法具有竞争力，尤其在训练效率方面显著优于其他基于嵌入的模型，使其成为计算资源受限场景下生成高质量用户和物品嵌入的理想选择。", "keywords": "推荐系统, 嵌入学习, 神经网络, 隐式反馈, 效率", "comments": "Interact2Vec的创新之处在于其能够在仅依赖隐式反馈的情况下，高效地同时学习用户和物品嵌入，并采用NLP领域的优化策略。其重要性在于提供了一种计算效率高且性能良好的解决方案，尤其适用于资源受限的推荐系统。"}}
{"id": "2503.10210", "title": "TARS: Traffic-Aware Radar Scene Flow Estimation", "authors": ["Jialong Wu", "Marco Braun", "Dominic Spata", "Matthias Rottmann"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.10210v2", "summary": "Scene flow provides crucial motion information for autonomous driving. Recent\nLiDAR scene flow models utilize the rigid-motion assumption at the instance\nlevel, assuming objects are rigid bodies. However, these instance-level methods\nare not suitable for sparse radar point clouds. In this work, we present a\nnovel Traffic-Aware Radar Scene-Flow (TARS) estimation method, which utilizes\nmotion rigidity at the traffic level. To address the challenges in radar scene\nflow, we perform object detection and scene flow jointly and boost the latter.\nWe incorporate the feature map from the object detector, trained with detection\nlosses, to make radar scene flow aware of the environment and road users. From\nthis, we construct a Traffic Vector Field (TVF) in the feature space to achieve\nholistic traffic-level scene understanding in our scene flow branch. When\nestimating the scene flow, we consider both point-level motion cues from point\nneighbors and traffic-level consistency of rigid motion within the space. TARS\noutperforms the state of the art on a proprietary dataset and the View-of-Delft\ndataset, improving the benchmarks by 23% and 15%, respectively.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.10210v2", "cate": "cs.CV", "date": "2025-03-13", "updated": "2025-07-25", "AI": {"title_translation": "TARS: 交通感知雷达场景流估计", "tldr": "TARS是一种新的雷达场景流估计方法，通过结合交通级刚性运动和目标检测，显著提升了稀疏雷达点云的场景流估计性能。", "motivation": "现有LiDAR场景流模型依赖于实例级刚性运动假设，不适用于稀疏雷达点云。因此，需要一种新的方法来解决雷达场景流的挑战。", "method": "本文提出了一种名为交通感知雷达场景流（TARS）的估计方法，该方法利用交通级别的运动刚性。为解决雷达场景流的挑战，TARS联合进行目标检测和场景流估计，并利用经过检测损失训练的目标检测器特征图，使雷达场景流感知环境和道路使用者。在此基础上，在特征空间中构建交通矢量场（TVF），以在场景流分支中实现整体交通级场景理解。在估计场景流时，同时考虑来自点邻居的点级运动线索和空间内刚性运动的交通级一致性。", "result": "TARS在专有数据集和View-of-Delft数据集上均超越了现有技术水平，分别将基准提高了23%和15%。", "conclusion": "TARS通过引入交通级刚性运动假设和联合学习目标检测与场景流，有效解决了稀疏雷达点云的场景流估计难题，并显著提升了性能，为自动驾驶提供了更精确的运动信息。", "translation": "场景流为自动驾驶提供关键的运动信息。最近的激光雷达（LiDAR）场景流模型在实例级别利用刚性运动假设，即假设物体是刚体。然而，这些实例级方法不适用于稀疏的雷达点云。在这项工作中，我们提出了一种新颖的交通感知雷达场景流（TARS）估计方法，该方法利用交通级别的运动刚性。为了解决雷达场景流中的挑战，我们联合进行目标检测和场景流估计，并增强后者。我们整合了来自目标检测器的特征图（该检测器通过检测损失进行训练），使雷达场景流能够感知环境和道路使用者。由此，我们在特征空间中构建了一个交通矢量场（TVF），以在我们的场景流分支中实现整体交通级场景理解。在估计场景流时，我们同时考虑来自点邻居的点级运动线索和空间内刚性运动的交通级一致性。TARS在专有数据集和View-of-Delft数据集上均超越了现有技术水平，分别将基准提高了23%和15%。", "summary": "本文提出了一种名为TARS的新型雷达场景流估计方法，旨在解决现有LiDAR方法不适用于稀疏雷达点云的问题。TARS通过利用交通级的运动刚性，并联合进行目标检测和场景流估计，构建交通矢量场（TVF），同时考虑点级和交通级运动一致性。实验结果表明，TARS在多个数据集上显著提升了雷达场景流的估计性能，超越了现有技术水平。", "keywords": "雷达场景流, 交通感知, 刚性运动, 目标检测, 自动驾驶", "comments": "TARS的创新点在于提出了“交通级”的运动刚性假设，这与以往针对LiDAR的“实例级”刚性假设形成对比，更适用于雷达稀疏点云的特性。此外，将目标检测与场景流估计联合学习，并利用检测器特征图增强场景流，是一个巧妙的设计，能够有效提升雷达场景流的准确性。这对于依赖雷达的自动驾驶系统来说具有重要意义。"}}
{"id": "2507.18645", "title": "Quantum-Cognitive Tunnelling Neural Networks for Military-Civilian Vehicle Classification and Sentiment Analysis", "authors": ["Milan Maksimovic", "Anna Bohdanets", "Immaculate Motsi-Omoijiade", "Guido Governatori", "Ivan S. Maksymov"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18645v1", "summary": "Prior work has demonstrated that incorporating well-known quantum tunnelling\n(QT) probability into neural network models effectively captures important\nnuances of human perception, particularly in the recognition of ambiguous\nobjects and sentiment analysis. In this paper, we employ novel QT-based neural\nnetworks and assess their effectiveness in distinguishing customised\nCIFAR-format images of military and civilian vehicles, as well as sentiment,\nusing a proprietary military-specific vocabulary. We suggest that QT-based\nmodels can enhance multimodal AI applications in battlefield scenarios,\nparticularly within human-operated drone warfare contexts, imbuing AI with\ncertain traits of human reasoning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18645v1", "cate": "cs.CV", "date": "2025-07-18", "updated": "2025-07-18", "AI": {"title_translation": "量子认知隧穿神经网络用于军用-民用车辆分类和情感分析", "tldr": "本文将量子隧穿神经网络应用于军用-民用车辆分类和情感分析，旨在增强战场AI应用。", "motivation": "先前研究表明，将量子隧穿概率引入神经网络能有效捕捉人类感知（特别是在识别模糊对象和情感分析方面）的细微差别。本文的动机是将这种方法应用于军用-民用车辆分类和情感分析，以增强战场AI应用。", "method": "本文采用新颖的基于量子隧穿（QT）的神经网络模型，并使用定制的CIFAR格式的军用和民用车辆图像以及专有的军事特定词汇进行情感分析来评估其有效性。", "result": "研究表明，基于量子隧穿的模型能够有效区分军用和民用车辆图像以及进行情感分析。作者认为，这些模型可以增强战场场景中的多模态AI应用，特别是在人类操作的无人机战争中，赋予AI某些人类推理的特质。", "conclusion": "基于量子隧穿的神经网络模型能够有效处理军用-民用车辆分类和情感分析任务，并有望在战场AI应用中提升人工智能的人类推理能力。", "translation": "先前的工作已经证明，将众所周知的量子隧穿（QT）概率融入神经网络模型中，能有效捕捉人类感知的细微差别，特别是在模糊对象的识别和情感分析方面。在本文中，我们采用新颖的基于QT的神经网络，并评估它们在使用专有的军事特定词汇区分定制的CIFAR格式的军用和民用车辆图像以及情感方面的有效性。我们认为，基于QT的模型可以增强战场场景中的多模态AI应用，特别是在人类操作的无人机战争背景下，使AI具备人类推理的某些特质。", "summary": "本文提出并评估了基于量子隧穿（QT）的神经网络，用于军事和民用车辆的图像分类以及情感分析。研究表明，这些QT-based模型能够有效处理这些任务，并有望在战场AI应用中，尤其是在无人机战争中，通过赋予AI人类推理特性来增强多模态AI能力。", "keywords": "量子隧穿, 神经网络, 车辆分类, 情感分析, 军事AI", "comments": "本文的创新之处在于将量子隧穿效应引入神经网络，以模拟人类感知和推理能力，并将其应用于军事领域的特定问题。这种方法为提升AI在复杂、模糊环境下的决策能力提供了新的视角，尤其在战场AI和无人机作战中具有潜在的重要性。然而，抽象中并未提及具体的量化性能指标或与现有方法的详细比较。"}}
{"id": "2507.18923", "title": "Gaussian Set Surface Reconstruction through Per-Gaussian Optimization", "authors": ["Zhentao Huang", "Di Wu", "Zhenbang He", "Minglun Gong"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18923v1", "summary": "3D Gaussian Splatting (3DGS) effectively synthesizes novel views through its\nflexible representation, yet fails to accurately reconstruct scene geometry.\nWhile modern variants like PGSR introduce additional losses to ensure proper\ndepth and normal maps through Gaussian fusion, they still neglect individual\nplacement optimization. This results in unevenly distributed Gaussians that\ndeviate from the latent surface, complicating both reconstruction refinement\nand scene editing. Motivated by pioneering work on Point Set Surfaces, we\npropose Gaussian Set Surface Reconstruction (GSSR), a method designed to\ndistribute Gaussians evenly along the latent surface while aligning their\ndominant normals with the surface normal. GSSR enforces fine-grained geometric\nalignment through a combination of pixel-level and Gaussian-level single-view\nnormal consistency and multi-view photometric consistency, optimizing both\nlocal and global perspectives. To further refine the representation, we\nintroduce an opacity regularization loss to eliminate redundant Gaussians and\napply periodic depth- and normal-guided Gaussian reinitialization for a\ncleaner, more uniform spatial distribution. Our reconstruction results\ndemonstrate significantly improved geometric precision in Gaussian placement,\nenabling intuitive scene editing and efficient generation of novel\nGaussian-based 3D environments. Extensive experiments validate GSSR's\neffectiveness, showing enhanced geometric accuracy while preserving\nhigh-quality rendering performance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18923v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "通过逐高斯优化进行高斯集表面重建", "tldr": "GSSR通过优化每个高斯分布的位置和法线，解决了3DGS在几何重建上的不足，实现了更精确的高斯集表面重建，支持直观的场景编辑。", "motivation": "3D Gaussian Splatting (3DGS) 虽然能有效合成新视图，但在准确重建场景几何方面表现不佳。现有变体如PGSR通过高斯融合引入额外损失以确保深度和法线图，但仍忽略了单个高斯分布的放置优化，导致高斯分布不均匀且偏离潜在表面，这使得重建精炼和场景编辑变得复杂。", "method": "本文提出了高斯集表面重建 (GSSR) 方法，旨在使高斯分布均匀地沿着潜在表面分布，并使其主法线与表面法线对齐。GSSR通过结合像素级和高斯级的单视图法线一致性以及多视图光度一致性，强制执行细粒度的几何对齐，从而优化局部和全局视角。为了进一步完善表示，引入了不透明度正则化损失以消除冗余高斯，并应用周期性的深度和法线引导的高斯重新初始化，以实现更干净、更均匀的空间分布。", "result": "GSSR的重建结果显著提高了高斯放置的几何精度，实现了直观的场景编辑和高效的新高斯基3D环境生成。大量实验验证了GSSR的有效性，表明其在保持高质量渲染性能的同时，增强了几何精度。", "conclusion": "GSSR成功地通过优化高斯分布的放置和法线对齐，显著改善了3DGS在几何重建方面的不足，为3D场景的高精度重建和编辑提供了有效方案。", "translation": "3D高斯散射（3DGS）通过其灵活的表示有效地合成新视图，但未能准确重建场景几何。虽然像PGSR这样的现代变体通过高斯融合引入额外的损失以确保适当的深度和法线图，但它们仍然忽略了单个高斯分布的放置优化。这导致高斯分布不均匀，偏离潜在表面，从而使重建精炼和场景编辑变得复杂。受点集表面开创性工作的启发，我们提出了高斯集表面重建（GSSR），这是一种旨在使高斯分布均匀地沿着潜在表面分布，同时将其主法线与表面法线对齐的方法。GSSR通过结合像素级和高斯级的单视图法线一致性以及多视图光度一致性，强制执行细粒度的几何对齐，从而优化局部和全局视角。为了进一步完善表示，我们引入了不透明度正则化损失以消除冗余高斯，并应用周期性的深度和法线引导的高斯重新初始化，以实现更干净、更均匀的空间分布。我们的重建结果显著提高了高斯放置的几何精度，实现了直观的场景编辑和高效的新高斯基3D环境生成。大量实验验证了GSSR的有效性，表明其在保持高质量渲染性能的同时，增强了几何精度。", "summary": "本文提出高斯集表面重建（GSSR），旨在解决3DGS在几何重建中高斯分布不均匀和偏离潜在表面的问题。GSSR通过优化高斯分布的放置和法线对齐，结合多视图光度一致性、单视图法线一致性、不透明度正则化和周期性高斯重新初始化，实现了高精度的几何重建。实验证明GSSR显著提升了几何精度，同时保持了高质量渲染性能，并支持直观的场景编辑。", "keywords": "高斯集表面重建, 3DGS, 几何精度, 逐高斯优化, 场景编辑", "comments": "GSSR的创新之处在于其对单个高斯分布的精细化优化，特别是引入了像素级和高斯级的法线一致性以及不透明度正则化和周期性重初始化，这有效地解决了3DGS在几何精度上的主要缺陷。该方法对于需要高精度几何表示的应用，如场景编辑和高质量3D环境生成，具有重要意义。"}}
{"id": "2507.19398", "title": "CXR-CML: Improved zero-shot classification of long-tailed multi-label diseases in Chest X-Rays", "authors": ["Rajesh Madhipati", "Sheethal Bhat", "Lukas Buess", "Andreas Maier"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19398v1", "summary": "Chest radiography (CXR) plays a crucial role in the diagnosis of various\ndiseases. However, the inherent class imbalance in the distribution of clinical\nfindings presents a significant challenge for current self-supervised deep\nlearning models. These models often fail to accurately classify long-tailed\nclasses. Current Vision-Language models such as Contrastive Language Image\nPre-training (CLIP) models effectively model the manifold distribution of the\nlatent space, enabling high zero-shot classification accuracies. Although CLIP\nperforms well on most of the primary classes in the dataset, our work reveals\nthat its effectiveness decreases significantly for classes with a long-tailed\ndistribution. Our approach employs a class-weighting mechanism that directly\naligns with the distribution of classes within the latent space. This method\nensures a substantial improvement in overall classification performance, with\nparticular emphasis on enhancing the recognition and accuracy of rarely\nobserved classes. We accomplish this by applying Gaussian Mixture Model (GMM)\nclustering to the latent space. The subsequent clusters are further refined by\nStudent t-distribution, followed by a metric loss that utilizes the altered\nembeddings. Our approach facilitates stable and adaptive clustering of the\nfeatures. This results in a notable average improvement of 7\\% points in\nzero-shot AUC scores across 40 classes in the MIMIC-CXR-JPG dataset from\nprevious SOTA models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19398v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "CXR-CML：改进胸部X射线中长尾多标签疾病的零样本分类", "tldr": "CXR-CML提出了一种结合GMM聚类和学生t分布的类别加权机制，显著提高了胸部X射线中长尾多标签疾病的零样本分类精度，AUC分数平均提升7%。", "motivation": "当前自监督深度学习模型和视觉-语言模型（如CLIP）在胸部X射线诊断中，面对固有的临床发现类别不平衡（长尾分布）时，难以准确分类罕见（长尾）类别，其有效性显著降低。", "method": "本研究提出CXR-CML方法，采用一种直接与潜在空间中类别分布对齐的类别加权机制。具体通过对潜在空间应用高斯混合模型（GMM）聚类，随后使用学生t分布进一步细化聚类，并结合利用修改后嵌入的度量损失，以实现特征的稳定自适应聚类。", "result": "在MIMIC-CXR-JPG数据集中，对于40个类别，零样本AUC分数平均提高了7个百分点，超越了之前的SOTA模型。特别是显著增强了罕见类别的识别和准确性。", "conclusion": "CXR-CML方法通过有效处理潜在空间中的类别分布失衡问题，显著提升了胸部X射线中长尾多标签疾病的零样本分类性能，尤其改善了对罕见疾病的识别精度。", "translation": "胸部X射线检查（CXR）在多种疾病的诊断中发挥着关键作用。然而，临床发现分布中固有的类别不平衡对当前的自监督深度学习模型提出了重大挑战。这些模型通常无法准确分类长尾类别。当前的视觉-语言模型，例如对比语言图像预训练（CLIP）模型，能够有效地建模潜在空间的流形分布，从而实现高零样本分类精度。尽管CLIP在数据集中大多数主要类别上表现良好，但我们的工作表明，其对长尾分布类别的有效性显著降低。我们的方法采用了一种类别加权机制，直接与潜在空间中的类别分布对齐。这种方法确保了整体分类性能的大幅提升，特别强调增强罕见类别的识别和准确性。我们通过对潜在空间应用高斯混合模型（GMM）聚类来实现这一点。随后的聚类通过学生t分布进一步细化，然后是利用修改后的嵌入的度量损失。我们的方法促进了特征的稳定和自适应聚类。这导致在MIMIC-CXR-JPG数据集中40个类别的零样本AUC分数平均提高了7个百分点，超越了之前的SOTA模型。", "summary": "本论文提出了CXR-CML，一种改进胸部X射线中长尾多标签疾病零样本分类的新方法。针对当前模型（包括CLIP）在处理罕见类别时因类别不平衡而表现不佳的问题，CXR-CML引入了一种类别加权机制。该机制利用高斯混合模型（GMM）聚类和学生t分布细化来对齐潜在空间中的类别分布，并结合度量损失。该方法实现了稳定的特征聚类，在MIMIC-CXR-JPG数据集上使零样本AUC分数平均提高了7%，显著提升了对罕见疾病的识别精度。", "keywords": "胸部X射线, 零样本分类, 长尾分布, 多标签疾病, 高斯混合模型", "comments": "该论文解决了医学图像分析中一个关键挑战：疾病的长尾分布导致模型对罕见但可能重要的疾病表现不佳。其创新点在于将类别加权机制与GMM聚类和学生t分布细化相结合应用于潜在空间，旨在更好地表示和分类未充分表示的类别。所报告的7%AUC提升在医学图像这一挑战性领域的零样本学习中具有重要意义，可能对临床诊断工具的鲁棒性产生积极影响。"}}
{"id": "2502.17844", "title": "LeanKAN: A Parameter-Lean Kolmogorov-Arnold Network Layer with Improved Memory Efficiency and Convergence Behavior", "authors": ["Benjamin C. Koenig", "Suyong Kim", "Sili Deng"], "categories": ["cs.LG", "cs.NE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      21 pages, 7 figures, and 4 tables. Updated after acceptance to journal", "url": "http://arxiv.org/abs/2502.17844v2", "summary": "The recently proposed Kolmogorov-Arnold network (KAN) is a promising\nalternative to multi-layer perceptrons (MLPs) for data-driven modeling. While\noriginal KAN layers were only capable of representing the addition operator,\nthe recently-proposed MultKAN layer combines addition and multiplication\nsubnodes in an effort to improve representation performance. Here, we find that\nMultKAN layers suffer from a few key drawbacks including limited applicability\nin output layers, bulky parameterizations with extraneous activations, and the\ninclusion of complex hyperparameters. To address these issues, we propose\nLeanKANs, a direct and modular replacement for MultKAN and traditional AddKAN\nlayers. LeanKANs address these three drawbacks of MultKAN through general\napplicability as output layers, significantly reduced parameter counts for a\ngiven network structure, and a smaller set of hyperparameters. As a one-to-one\nlayer replacement for standard AddKAN and MultKAN layers, LeanKAN is able to\nprovide these benefits to traditional KAN learning problems as well as\naugmented KAN structures in which it serves as the backbone, such as KAN\nOrdinary Differential Equations (KAN-ODEs) or Deep Operator KANs (DeepOKAN). We\ndemonstrate LeanKAN's simplicity and efficiency in a series of demonstrations\ncarried out across a standard KAN toy problem as well as ordinary and partial\ndifferential equations learned via KAN-ODEs, where we find that its sparser\nparameterization and compact structure serve to increase its expressivity and\nlearning capability, leading it to outperform similar and even much larger\nMultKANs in various tasks.", "comment": "21 pages, 7 figures, and 4 tables. Updated after acceptance to\n  journal", "pdf_url": "http://arxiv.org/pdf/2502.17844v2", "cate": "cs.LG", "date": "2025-02-25", "updated": "2025-07-24", "AI": {"title_translation": "LeanKAN：一种参数精简的科尔莫哥洛夫-阿诺德网络层，具有改进的内存效率和收敛行为", "tldr": "LeanKAN是一种新的科尔莫哥洛夫-阿诺德网络层，旨在解决MultKAN的局限性，通过减少参数、简化超参数并提高作为输出层的适用性来提升性能和效率，并在各种任务中超越MultKAN。", "motivation": "最近提出的MultKAN层虽然结合了加法和乘法子节点以提高表示性能，但存在几个关键缺点，包括在输出层适用性有限、参数化臃肿包含多余激活以及包含复杂的超参数。为了解决这些问题，本文提出了LeanKAN。", "method": "本文提出了LeanKAN，作为MultKAN和传统AddKAN层的直接模块化替代。LeanKAN通过以下方式解决了MultKAN的三个缺点：作为输出层的通用适用性、显著减少给定网络结构的参数数量以及更小的超参数集。它作为标准AddKAN和MultKAN层的一对一替换。", "result": "在标准KAN玩具问题以及通过KAN-ODEs学习的常微分方程和偏微分方程的一系列演示中，LeanKAN展现了其简单性和效率。研究发现，其更稀疏的参数化和紧凑的结构增强了其表达能力和学习能力，使其在各种任务中表现优于相似甚至更大的MultKAN。", "conclusion": "LeanKAN通过参数精简、超参数简化和更广泛的适用性，显著提升了科尔莫哥洛夫-阿诺德网络的性能和效率，并在多项任务中超越了现有方法。", "translation": "最近提出的科尔莫哥洛夫-阿诺德网络（KAN）是数据驱动建模中多层感知机（MLPs）的一个有前景的替代方案。虽然原始的KAN层只能表示加法操作，但最近提出的MultKAN层结合了加法和乘法子节点，以努力提高表示性能。在此，我们发现MultKAN层存在几个关键缺点，包括在输出层适用性有限、参数化臃肿包含多余激活以及包含复杂的超参数。为了解决这些问题，我们提出了LeanKANs，作为MultKAN和传统AddKAN层的直接模块化替代。LeanKANs通过作为输出层的通用适用性、显著减少给定网络结构的参数数量以及更小的超参数集，解决了MultKAN的这三个缺点。作为标准AddKAN和MultKAN层的一对一替换，LeanKAN能够为传统的KAN学习问题以及作为骨干的增强型KAN结构（如KAN常微分方程（KAN-ODEs）或深度算子KANs（DeepOKAN））提供这些优势。我们在一系列演示中展示了LeanKAN的简单性和效率，这些演示在标准KAN玩具问题以及通过KAN-ODEs学习的常微分方程和偏微分方程中进行，我们发现其更稀疏的参数化和紧凑的结构有助于提高其表达能力和学习能力，使其在各种任务中优于相似甚至更大的MultKAN。", "summary": "本文提出了一种名为LeanKAN的新型科尔莫哥洛夫-阿诺德网络（KAN）层，旨在解决现有MultKAN层的局限性，包括输出层适用性有限、参数量大和超参数复杂。LeanKAN作为MultKAN和AddKAN的直接替代，通过通用输出层适用性、显著减少参数数量和简化超参数集来改进这些问题。实验证明，LeanKAN在各种KAN学习任务中，包括常微分方程和偏微分方程的求解，展现出更高的表达能力和学习效率，性能优于相似或更大的MultKAN模型。", "keywords": "科尔莫哥洛夫-阿诺德网络, LeanKAN, MultKAN, 内存效率, 收敛行为", "comments": "LeanKAN的创新之处在于其对KAN结构进行了精简和优化，解决了MultKAN在实际应用中面临的参数臃肿和超参数复杂性问题。其“参数精简”和“模块化替代”的特点，使得KANs在保持强大表示能力的同时，更具实用性和部署潜力。这项工作对于推动KANs在更广泛领域（如物理信息神经网络）的应用具有重要意义。"}}
{"id": "2507.16331", "title": "Re:Form -- Reducing Human Priors in Scalable Formal Software Verification with RL in LLMs: A Preliminary Study on Dafny", "authors": ["Chuanhao Yan", "Fengdi Che", "Xuhan Huang", "Xu Xu", "Xin Li", "Yizhi Li", "Xingwei Qu", "Jingzhe Shi", "Zhuangzhuang He", "Chenghua Lin", "Yaodong Yang", "Binhang Yuan", "Hang Zhao", "Yu Qiao", "Bowen Zhou", "Jie Fu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16331v2", "summary": "Existing informal language-based (e.g., human language) Large Language Models\n(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:\ntheir verification processes, which provide crucial training signals, are\nneither reliable nor scalable. In fact, the prevalent large proprietary models\ncould hardly generate verifiable programs. A promising yet largely uncharted\nalternative is formal language-based reasoning. Grounding LLMs in rigorous\nformal systems where generative models operate in formal language spaces (e.g.,\nDafny) enables the automatic and mathematically provable verification of their\nreasoning processes and outcomes. This capability is pivotal for achieving\nlarge-scale, reliable formal software verification. It is a common practice to\nemploy human-annotated chain-of-thought and other human priors to induce the\nreasoning and coding capabilities of LLMs. Unfortunately, it becomes\nunacceptably all-consuming to provide such priors for supervising complex\nprogramming tasks. In this work, we systematically explore ways to reduce human\npriors with the formal language, Dafny, as the main environment for our pilot\nstudy. Our pipeline mainly relies on introducing an automatic and scalable data\ncuration pipeline, and careful RL designs integrated with feedback from the\nformal language verifier. We introduce DafnyComp, a benchmark of compositional\nformal programs with auto-formalized specifications for specification\nreasoning. Our supervised fine-tuning (SFT) stage enables even small models\n(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,\nsurpassing proprietary models. RL with regularization further improves\nperformance, achieving stronger generalization to out-of-domain tasks and\noutperforming all strong baselines on the challenging DafnyComp benchmark.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16331v2", "cate": "cs.CL", "date": "2025-07-22", "updated": "2025-07-25", "AI": {"title_translation": "Re:Form -- 在LLMs中结合强化学习减少可扩展形式化软件验证中的人类先验：一项关于Dafny的初步研究", "tldr": "使用强化学习和形式语言Dafny在LLMs中减少人类先验，实现可扩展的形式化软件验证，并构建了DafnyComp基准，小模型表现优异。", "motivation": "现有基于非正式语言的LLMs在软件验证中不可靠且不可扩展，难以生成可验证的程序；同时，依赖人类标注的思维链和先验来诱导LLM的推理和编码能力，对于复杂的编程任务来说过于耗时和难以承受。", "method": "本文系统性地探索了在Dafny这一形式语言环境中减少人类先验的方法。主要方法包括引入自动化且可扩展的数据整理流程，以及将精心设计的强化学习与形式语言验证器的反馈相结合。此外，还引入了DafnyComp，一个包含自动形式化规范的组合式形式程序基准，用于规范推理。", "result": "通过监督微调（SFT）阶段，即使是小型模型（如0.5B）也能生成语法正确且可验证的Dafny代码，并超越了专有模型。结合正则化的强化学习进一步提高了性能，实现了对域外任务的更强泛化能力，并在具有挑战性的DafnyComp基准测试中超越了所有强基线。", "conclusion": "将LLMs建立在严谨的形式系统中有助于实现其推理过程和结果的自动化、数学可证明的验证，这对于实现大规模、可靠的形式化软件验证至关重要。通过结合形式语言和强化学习，可以有效减少对人类先验的依赖。", "translation": "现有基于非正式语言（如人类语言）的大型语言模型（LLMs）在通过强化学习（RL）训练时面临一个重大挑战：它们提供关键训练信号的验证过程既不可靠也无法扩展。事实上，目前流行的专有大型模型很难生成可验证的程序。一个有前景但很大程度上尚未探索的替代方案是基于形式语言的推理。将LLMs建立在严谨的形式系统上，使得生成模型在形式语言空间（如Dafny）中操作，从而能够自动且数学可证明地验证其推理过程和结果。这种能力对于实现大规模、可靠的形式化软件验证至关重要。通常的做法是采用人类标注的思维链和其他人类先验来诱导LLMs的推理和编码能力。不幸的是，为监督复杂的编程任务提供此类先验变得无法接受地耗时。在这项工作中，我们系统性地探索了使用形式语言Dafny作为我们初步研究的主要环境来减少人类先验的方法。我们的流程主要依赖于引入一个自动化且可扩展的数据整理流程，以及将精心设计的强化学习与形式语言验证器的反馈相结合。我们引入了DafnyComp，一个包含自动形式化规范的组合式形式程序基准，用于规范推理。我们的监督微调（SFT）阶段使得即使是小型模型（如0.5B）也能生成语法正确且可验证的Dafny代码，超越了专有模型。结合正则化的强化学习进一步提高了性能，实现了对域外任务的更强泛化能力，并在具有挑战性的DafnyComp基准测试中超越了所有强基线。", "summary": "本文提出了Re:Form，旨在解决LLM在软件验证中存在的不可靠、不可扩展以及对人类先验过度依赖的问题。通过将LLM与Dafny等形式语言结合，并引入自动化数据整理流程和强化学习设计，研究表明即使是小型模型也能生成可验证的代码，并在新引入的DafnyComp基准测试中超越了专有模型和现有强基线，展现了更好的泛化能力。", "keywords": "形式化验证, 大型语言模型, 强化学习, Dafny, 人类先验", "comments": "本文的创新点在于将强化学习与形式语言（Dafny）结合，以减少大型语言模型在软件验证中对人类先验的依赖，从而提升验证的可扩展性和可靠性。引入DafnyComp基准测试是其另一贡献。研究证明即使小型模型也能生成可验证的代码并超越专有模型，这对于推动LLM在形式化软件验证领域的实际应用具有重要意义。作为一项初步研究，其成果为未来在形式化验证领域结合LLM和强化学习提供了有价值的方向。"}}
{"id": "2507.19372", "title": "Learning neuro-symbolic convergent term rewriting systems", "authors": ["Flavio Petruzzellis", "Alberto Testolin", "Alessandro Sperduti"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      48 pages, 31 figures. Submitted for review by Artificial Intelligence Journal", "url": "http://arxiv.org/abs/2507.19372v1", "summary": "Building neural systems that can learn to execute symbolic algorithms is a\nchallenging open problem in artificial intelligence, especially when aiming for\nstrong generalization and out-of-distribution performance. In this work, we\nintroduce a general framework for learning convergent term rewriting systems\nusing a neuro-symbolic architecture inspired by the rewriting algorithm itself.\nWe present two modular implementations of such architecture: the Neural\nRewriting System (NRS) and the Fast Neural Rewriting System (FastNRS). As a\nresult of algorithmic-inspired design and key architectural elements, both\nmodels can generalize to out-of-distribution instances, with FastNRS offering\nsignificant improvements in terms of memory efficiency, training speed, and\ninference time. We evaluate both architectures on four tasks involving the\nsimplification of mathematical formulas and further demonstrate their\nversatility in a multi-domain learning scenario, where a single model is\ntrained to solve multiple types of problems simultaneously. The proposed system\nsignificantly outperforms two strong neural baselines: the Neural Data Router,\na recent transformer variant specifically designed to solve algorithmic\nproblems, and GPT-4o, one of the most powerful general-purpose large-language\nmodels. Moreover, our system matches or outperforms the latest o1-preview model\nfrom OpenAI that excels in reasoning benchmarks.", "comment": "48 pages, 31 figures. Submitted for review by Artificial Intelligence\n  Journal", "pdf_url": "http://arxiv.org/pdf/2507.19372v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "学习神经符号收敛项重写系统", "tldr": "本文提出了一种受算法启发的新型神经符号架构（NRS和FastNRS），用于学习收敛项重写系统，能够实现强大的泛化能力和域外性能，并在数学公式简化等任务上显著优于现有基线模型。", "motivation": "在人工智能领域，构建能够学习执行符号算法的神经系统是一个具有挑战性的开放问题，尤其是在实现强大的泛化能力和域外性能方面。", "method": "本文引入了一个通用的框架，用于使用受重写算法本身启发的神经符号架构来学习收敛项重写系统。具体提出了两种模块化实现：神经重写系统（NRS）和快速神经重写系统（FastNRS）。这些架构在四项涉及数学公式简化的任务以及多领域学习场景中进行了评估。", "result": "两种模型都能泛化到域外实例，其中FastNRS在内存效率、训练速度和推理时间方面有显著改进。所提出的系统在数学公式简化任务上显著优于两个强大的神经基线模型（Neural Data Router和GPT-4o），并且与OpenAI最新的o1-preview模型持平或超越。", "conclusion": "本文提出的神经符号收敛项重写系统（NRS和FastNRS）在学习执行符号算法方面表现出色，尤其是在泛化能力和域外性能上，并且在多种任务上优于或匹配了先进的基线模型，证明了其在解决复杂符号问题上的有效性和多功能性。", "translation": "建立能够学习执行符号算法的神经系统是人工智能领域一个具有挑战性的开放问题，尤其是在追求强大的泛化能力和域外性能时。在这项工作中，我们引入了一个通用的框架，用于使用受重写算法本身启发的神经符号架构来学习收敛项重写系统。我们提出了这种架构的两种模块化实现：神经重写系统（NRS）和快速神经重写系统（FastNRS）。由于受算法启发的设计和关键的架构元素，这两种模型都可以泛化到域外实例，其中FastNRS在内存效率、训练速度和推理时间方面提供了显著改进。我们评估了这两种架构在四项涉及数学公式简化的任务上的表现，并进一步展示了它们在多领域学习场景中的多功能性，即训练一个模型同时解决多种类型的问题。所提出的系统显著优于两个强大的神经基线模型：Neural Data Router（一个最近专门设计用于解决算法问题的Transformer变体）和GPT-4o（最强大的通用大型语言模型之一）。此外，我们的系统与OpenAI在推理基准测试中表现出色的最新o1-preview模型持平或超越。", "summary": "本文针对神经系统学习执行符号算法的挑战，提出了一个名为神经符号收敛项重写系统（NRS和FastNRS）的通用框架。该框架采用受重写算法启发的神经符号架构，实现了强大的域外泛化能力。实验结果表明，NRS和FastNRS在数学公式简化和多领域学习任务上表现出色，尤其FastNRS在效率上有显著提升，并且在性能上显著超越了Neural Data Router和GPT-4o等现有先进模型，甚至与OpenAI的o1-preview模型持平或更优。", "keywords": "神经符号系统, 项重写系统, 泛化, 数学公式简化, 多领域学习", "comments": "这篇论文通过结合神经和符号方法，为解决AI中长期存在的符号算法学习和泛化问题提供了一个新颖且有效的解决方案。其创新点在于受算法启发的设计，这使得模型能够更好地泛化到未知数据。FastNRS的效率提升也增强了其实用性。该工作在超越强大基线模型方面取得了显著成果，表明神经符号混合方法在处理复杂推理和算法任务方面的巨大潜力。"}}
{"id": "2507.19457", "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning", "authors": ["Lakshya A Agrawal", "Shangyin Tan", "Dilara Soylu", "Noah Ziems", "Rishi Khare", "Krista Opsahl-Ong", "Arnav Singhvi", "Herumb Shandilya", "Michael J Ryan", "Meng Jiang", "Christopher Potts", "Koushik Sen", "Alexandros G. Dimakis", "Ion Stoica", "Dan Klein", "Matei Zaharia", "Omar Khattab"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE", "I.2.7; I.2.6; I.2.4; I.2.8"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19457v1", "summary": "Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization\n(GRPO), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt\noptimizer that thoroughly incorporates natural language reflection to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the Pareto frontier of its own attempts. As a result of GEPA's design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for code optimization.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19457v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "GEPA：反思性提示演化可超越强化学习", "tldr": "GEPA是一种基于自然语言反思的提示优化器，它通过学习高层规则，在少量试错中就能显著提升大型语言模型（LLM）的表现，并且比强化学习方法更高效。", "motivation": "大型语言模型（LLM）通过强化学习（RL）方法（如GRPO）适应下游任务时，通常需要数千次模拟才能学习新任务。本文认为，语言的可解释性为LLM提供了比稀疏标量奖励派生的策略梯度更丰富的学习介质。", "method": "本文引入了GEPA（Genetic-Pareto），一个彻底整合自然语言反思的提示优化器，通过试错学习高层规则。GEPA通过采样系统级轨迹（例如，推理、工具调用和工具输出），并用自然语言对其进行反思，以诊断问题、提出并测试提示更新，并结合其自身尝试的帕累托前沿的互补经验。", "result": "GEPA在四项任务中平均比GRPO高出10%，最高达20%，同时使用的模拟次数减少了35倍。GEPA在两个LLM上比领先的提示优化器MIPROv2高出10%以上，并作为代码优化的推理时搜索策略显示出有前景的结果。", "conclusion": "GEPA通过利用自然语言反思进行提示演化，能够在显著减少模拟次数的情况下，在多项任务中超越传统的强化学习方法和现有的提示优化器，证明了语言作为LLM学习介质的优越性。", "translation": "大型语言模型（LLM）正越来越多地通过强化学习（RL）方法（如群体相对策略优化GRPO）适应下游任务，这些方法通常需要数千次模拟才能学习新任务。我们认为，语言的可解释性通常可以为LLM提供比稀疏标量奖励派生的策略梯度更丰富的学习介质。为了验证这一点，我们引入了GEPA（Genetic-Pareto），一个彻底整合自然语言反思的提示优化器，通过试错学习高层规则。给定任何包含一个或多个LLM提示的AI系统，GEPA会采样系统级轨迹（例如，推理、工具调用和工具输出），并用自然语言对其进行反思，以诊断问题，提出并测试提示更新，并结合其自身尝试的帕累度前沿的互补经验。由于GEPA的设计，它通常只需少量模拟就能带来巨大的质量提升。在四项任务中，GEPA平均比GRPO高出10%，最高达20%，同时使用的模拟次数减少了35倍。GEPA还在两个LLM上比领先的提示优化器MIPROv2高出10%以上，并作为代码优化的推理时搜索策略显示出有前景的结果。", "summary": "本文提出GEPA，一种新颖的提示优化器，它利用大型语言模型（LLM）的自然语言反思能力，通过试错学习高层规则。与需要大量模拟的强化学习方法不同，GEPA能够通过少量模拟实现显著的性能提升。实验结果表明，GEPA在多项任务中不仅在性能上超越了传统的强化学习方法GRPO和现有的提示优化器MIPROv2，而且在效率上大幅领先，证明了自然语言作为LLM学习介质的优越性。", "keywords": "大型语言模型, 提示优化, 反思性学习, 强化学习, 自然语言处理", "comments": "GEPA的创新点在于它利用了语言本身的可解释性作为LLM的“学习介质”，而非依赖于稀疏的标量奖励。这种通过自然语言反思进行“自我进化”的提示优化方法，显著提高了学习效率和最终性能，尤其是在减少所需模拟次数方面表现出色，这对于实际应用具有重要意义。该方法为LLM的适应性学习开辟了新的途径。"}}
{"id": "2507.19477", "title": "Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts", "authors": ["Sang-Woo Lee", "Sohee Yang", "Donghyun Kwak", "Noah Y. Siegel"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19477v1", "summary": "Many recent papers have studied the development of superforecaster-level\nevent forecasting LLMs. While methodological problems with early studies cast\ndoubt on the use of LLMs for event forecasting, recent studies with improved\nevaluation methods have shown that state-of-the-art LLMs are gradually reaching\nsuperforecaster-level performance, and reinforcement learning has also been\nreported to improve future forecasting. Additionally, the unprecedented success\nof recent reasoning models and Deep Research-style models suggests that\ntechnology capable of greatly improving forecasting performance has been\ndeveloped. Therefore, based on these positive recent trends, we argue that the\ntime is ripe for research on large-scale training of superforecaster-level\nevent forecasting LLMs. We discuss two key research directions: training\nmethods and data acquisition. For training, we first introduce three\ndifficulties of LLM-based event forecasting training: noisiness-sparsity,\nknowledge cut-off, and simple reward structure problems. Then, we present\nrelated ideas to mitigate these problems: hypothetical event Bayesian networks,\nutilizing poorly-recalled and counterfactual events, and auxiliary reward\nsignals. For data, we propose aggressive use of market, public, and crawling\ndatasets to enable large-scale training and evaluation. Finally, we explain how\nthese technical advances could enable AI to provide predictive intelligence to\nsociety in broader areas. This position paper presents promising specific paths\nand considerations for getting closer to superforecaster-level AI technology,\naiming to call for researchers' interest in these directions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19477v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "通过大规模训练大型语言模型推进事件预测：挑战、解决方案和更广泛的影响", "tldr": "本文提出，鉴于LLM在事件预测方面的积极进展，现在是时候进行大规模训练以达到超级预测者水平。论文讨论了训练方法（解决噪音、知识截止和奖励结构问题）和数据获取（利用市场、公共和爬取数据）的挑战与解决方案，并探讨了更广泛的社会影响。", "motivation": "鉴于最近的研究表明最先进的大型语言模型（LLMs）正逐渐达到超级预测者级别的事件预测性能，并且推理模型和深度研究模型的成功预示着预测性能的巨大提升，作者认为大规模训练超级预测者级别的事件预测LLMs的时机已经成熟。", "method": "本文提出了两个关键研究方向：训练方法和数据获取。训练方法方面，针对噪声-稀疏性、知识截止和简单奖励结构问题，提出了假设事件贝叶斯网络、利用召回不佳和反事实事件以及辅助奖励信号来缓解。数据获取方面，建议积极使用市场、公共和爬取数据集以实现大规模训练和评估。", "result": "Not mentioned in abstract", "conclusion": "本文提出了实现超级预测者级别AI技术的有前景的具体路径和考虑因素，旨在引起研究人员对这些方向的兴趣。", "translation": "许多最近的论文研究了开发超级预测者级别事件预测大型语言模型（LLMs）。虽然早期研究的方法学问题对LLMs用于事件预测产生了怀疑，但最近采用改进评估方法的研究表明，最先进的LLMs正逐渐达到超级预测者级别的性能，并且据报道强化学习也能改善未来预测。此外，最近推理模型和深度研究风格模型的空前成功表明，能够极大提高预测性能的技术已经发展起来。因此，基于这些积极的近期趋势，我们认为现在是研究大规模训练超级预测者级别事件预测LLMs的成熟时机。我们讨论了两个关键研究方向：训练方法和数据获取。对于训练，我们首先介绍了基于LLM的事件预测训练的三个困难：噪声-稀疏性、知识截止和简单奖励结构问题。然后，我们提出了缓解这些问题的相关想法：假设事件贝叶斯网络、利用召回不佳和反事实事件以及辅助奖励信号。对于数据，我们建议积极使用市场、公共和爬取数据集，以实现大规模训练和评估。最后，我们解释了这些技术进步如何使AI在更广泛的领域为社会提供预测智能。这篇立场论文提出了有前景的具体路径和考虑因素，以更接近超级预测者级别的AI技术，旨在呼吁研究人员对这些方向的兴趣。", "summary": "这篇立场论文探讨了通过大规模训练大型语言模型（LLMs）实现超级预测者级别事件预测的可行性。鉴于LLMs在事件预测方面的最新进展，作者认为现在是进行大规模研究的理想时机。论文重点讨论了训练方法（如何解决数据噪声、知识更新和奖励结构简单等挑战，提出使用贝叶斯网络、反事实事件和辅助奖励等）和数据获取（利用市场、公共和爬取数据）两大关键方向，并展望了AI提供预测智能的社会影响，旨在激发相关研究兴趣。", "keywords": "事件预测, 大型语言模型, 超级预测者, 大规模训练, 人工智能预测", "comments": "该论文是一篇立场性文章，而非实证研究，因此没有提供具体的实验结果。其创新之处在于明确提出了大规模训练LLM以实现超级预测者级别事件预测的愿景，并系统性地梳理了当前面临的挑战（如数据噪声、知识截止、奖励结构）及潜在的解决方案，同时强调了数据获取的重要性。这对于指引未来研究方向具有重要意义，尤其是在LLM应用日益广泛的背景下，提升其预测能力具有巨大的社会价值。然而，作为一篇设想性论文，其提出的解决方案仍需在实践中验证。"}}
{"id": "2501.01986", "title": "FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Vision Language Models", "authors": ["Tianyu Fu", "Tengxuan Liu", "Qinghao Han", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Xuefei Ning", "Yu Wang"], "categories": ["cs.CV", "cs.AI", "68T45, 68T50", "I.2.7; I.2.10"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025", "url": "http://arxiv.org/abs/2501.01986v2", "summary": "The increasing demand to process long and high-resolution videos\nsignificantly burdens Large Vision-Language Models (LVLMs) due to the enormous\nnumber of visual tokens. Existing token reduction methods primarily prune\ntokens based on importance metrics, such as cumulative attention scores.\nHowever, even important tokens may exhibit high redundancy caused by similarity\namong adjacent video frames and repetitive visual elements. To address this\nlimitation, we propose FrameFusion, a novel token reduction approach\nintegrating similarity-based merging with importance-based pruning. We conduct\na thorough study on token similarity characteristics, revealing three key\ninsights: (1) spatially corresponding visual tokens between adjacent frames\nhave higher cosine similarities compared to other token pairs; (2) high token\nsimilarities prominently decrease in deeper model layers; and (3) token\nsimilarity rankings are highly consistent across different layers. Guided by\nthese observations, FrameFusion computes token similarities exclusively between\ncorresponding visual tokens from adjacent frames, applies token merging at\ninitial successive layers followed by pruning in deeper layers, and adopts a\ncascaded merging strategy to further enhance efficiency. We evaluate\nFrameFusion comprehensively across six diverse LVLMs, ranging from 2B to 72B\nparameters, using five video benchmarks encompassing video retrieval,\nquestion-answering, and spatial-temporal understanding tasks. Experiments show\nthat FrameFusion reduces visual tokens by 70%, achieving 1.6-3.6x end-to-end\nspeedups, with an average performance impact of less than 3%. Our code is\navailable at: https://github.com/thu-nics/FrameFusion.", "comment": "ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2501.01986v2", "cate": "cs.CV", "date": "2024-12-30", "updated": "2025-07-24", "AI": {"title_translation": "FrameFusion：融合相似性和重要性以在大视觉语言模型中减少视频令牌", "tldr": "FrameFusion通过结合相似性合并和重要性剪枝，有效减少大视觉语言模型（LVLM）的视频令牌，实现显著加速且性能影响小。", "motivation": "处理长高分辨率视频对大视觉语言模型（LVLMs）造成巨大负担，现有令牌减少方法主要基于重要性，但忽略了相邻帧和重复视觉元素带来的冗余。", "method": "提出FrameFusion，一种结合相似性合并和重要性剪枝的令牌减少方法。通过研究令牌相似性特性，发现：1) 相邻帧对应视觉令牌相似度高；2) 高令牌相似度在更深的模型层中显著下降；3) 令牌相似度排名在不同层中高度一致。FrameFusion在相邻帧对应视觉令牌间计算相似度，在初始连续层进行令牌合并，随后在更深层进行剪枝，并采用级联合并策略以进一步提高效率。", "result": "FrameFusion在六个LVLMs（2B到72B参数）和五个视频基准（视频检索、问答、时空理解）上进行评估。结果显示，它减少了70%的视觉令牌，实现了1.6-3.6倍的端到端加速，平均性能影响小于3%。", "conclusion": "FrameFusion通过有效结合相似性合并和重要性剪枝，成功解决了LVLMs处理长视频时令牌冗余的问题，显著提高了效率，同时保持了模型性能。", "translation": "处理长高分辨率视频的需求日益增长，由于庞大的视觉令牌数量，给大视觉语言模型（LVLMs）带来了巨大的巨大负担。现有的令牌减少方法主要基于重要性指标（如累积注意力分数）来修剪令牌。然而，即使是重要的令牌也可能因相邻视频帧之间的相似性和重复的视觉元素而表现出高度冗余。为了解决这一限制，我们提出了FrameFusion，一种新颖的令牌减少方法，它将基于相似性的合并与基于重要性的剪枝相结合。我们对令牌相似性特征进行了深入研究，揭示了三个关键见解：(1) 相邻帧之间空间对应的视觉令牌比其他令牌对具有更高的余弦相似度；(2) 高令牌相似度在更深的模型层中显著下降；(3) 令牌相似度排名在不同层之间高度一致。在这些观察的指导下，FrameFusion专门计算相邻帧对应视觉令牌之间的相似度，在初始连续层应用令牌合并，随后在更深层进行剪枝，并采用级联合并策略以进一步提高效率。我们使用涵盖视频检索、问答和时空理解任务的五个视频基准，在六个不同的LVLMs（参数范围从2B到72B）上对FrameFusion进行了全面评估。实验表明，FrameFusion减少了70%的视觉令牌，实现了1.6-3.6倍的端到端加速，平均性能影响小于3%。我们的代码可在以下地址获取：https://github.com/thu-nics/FrameFusion。", "summary": "本文提出了FrameFusion，一种针对大视觉语言模型（LVLMs）的视频令牌减少新方法。它创新性地结合了基于相似性的令牌合并与基于重要性的令牌剪枝，以解决现有方法中因帧间相似性导致的冗余问题。通过深入研究令牌相似性特性，FrameFusion在模型早期层进行相似性合并，后期层进行重要性剪枝，并采用级联策略。实验证明，FrameFusion能显著减少70%的视觉令牌，实现1.6-3.6倍的端到端加速，同时保持小于3%的平均性能损失，有效提升了LVLMs处理长视频的效率。", "keywords": "视频令牌减少, 大视觉语言模型, 相似性合并, 重要性剪枝, FrameFusion", "comments": "该论文的创新点在于首次将相似性合并与重要性剪枝相结合来减少视频令牌，解决了现有方法仅关注重要性而忽略冗余的问题。其对令牌相似性特性的深入研究为方法设计提供了坚实基础，并实现了显著的效率提升，对LVLMs处理长视频具有重要意义。"}}
{"id": "2502.17841", "title": "Quantifying interdisciplinary synergy in higher STEM education", "authors": ["Gahyoun Gim", "Jinhyuk Yun", "Sang Hoon Lee"], "categories": ["physics.soc-ph", "cond-mat.stat-mech", "cs.IT", "math.IT", "physics.ed-ph"], "primary_category": "Subjects:       Physics and Society (physics.soc-ph)", "pdf_link": null, "comments": "Comments:      24 pages, 9 figures, 3 supplementary tables", "url": "http://arxiv.org/abs/2502.17841v3", "summary": "We propose a framework to quantify and utilize interdisciplinarity in science\nand engineering curricula at the university-level higher education. We analyze\ninterdisciplinary relations by standardizing large-scale official educational\ndata in Korea using a cutting-edge large language model and constructing\nknowledge maps for disciplines of scientific education. We design and evaluate\nsingle-field and integrated dual-field curricula by adapting pedagogical theory\nand utilizing information theory-based metrics. We develop standard curricula\nfor individual disciplines and integrated curricula combining two fields, with\ntheir interdisciplinarity quantified by the curriculum synergy score. The\nresults indicate higher interdisciplinarity for combinations within or across\nclosely related fields, especially in engineering fields. Based on the\nanalysis, engineering fields constitute the core structure of our design for\ncurriculum interdisciplinarity, while basic natural science fields are located\nat peripheral stems to provide fundamental concepts.", "comment": "24 pages, 9 figures, 3 supplementary tables", "pdf_url": "http://arxiv.org/pdf/2502.17841v3", "cate": "physics.soc-ph", "date": "2025-02-25", "updated": "2025-07-25", "AI": {"title_translation": "量化高等STEM教育中的跨学科协同作用", "tldr": "本文提出了一个框架，利用大型语言模型和信息理论指标量化并利用大学STEM教育课程中的跨学科性，发现工程领域在跨学科设计中处于核心地位。", "motivation": "旨在量化并利用大学阶段高等教育中科学和工程课程的跨学科性。", "method": "通过使用尖端大型语言模型标准化韩国大规模官方教育数据，构建学科知识图谱。通过调整教学理论和利用基于信息理论的指标，设计并评估了单领域和综合双领域课程。开发了标准学科课程和结合两个领域的综合课程，其跨学科性通过课程协同分数进行量化。", "result": "结果表明，在密切相关领域内部或跨领域组合中，跨学科性更高，尤其是在工程领域。基于分析，工程领域构成了课程跨学科设计的核心结构，而基础自然科学领域则位于外围，提供基本概念。", "conclusion": "工程领域在课程跨学科设计中构成核心结构，而基础自然科学领域则作为外围提供基础概念。", "translation": "我们提出了一个框架，用于量化和利用大学高等教育中科学和工程课程的跨学科性。我们通过使用尖端大型语言模型标准化韩国大规模官方教育数据，并构建科学教育学科的知识图谱来分析跨学科关系。我们通过调整教学理论和利用基于信息理论的指标来设计和评估单领域和综合双领域课程。我们开发了独立学科的标准课程和结合两个领域的综合课程，其跨学科性通过课程协同分数进行量化。结果表明，在密切相关领域内部或跨领域组合中，跨学科性更高，尤其是在工程领域。基于分析，工程领域构成了我们课程跨学科设计的核心结构，而基础自然科学领域则位于外围，提供基本概念。", "summary": "本研究提出了一个量化和利用大学STEM教育中跨学科性的框架。该框架通过使用大型语言模型处理韩国教育数据并构建知识图谱来分析跨学科关系。研究设计并评估了单领域和双领域课程，并使用信息理论指标量化了课程协同作用。结果显示，工程领域在跨学科组合中表现出更高的协同性，并在课程设计中占据核心地位，而基础自然科学则提供基础概念。", "keywords": "跨学科性, STEM教育, 大型语言模型, 课程协同, 知识图谱", "comments": "本文的创新之处在于提出了一个利用大型语言模型和信息理论指标来量化和分析高等教育课程中跨学科性的框架。这种方法为教育课程设计提供了量化依据，特别是其发现工程领域在跨学科教育中的核心作用，对未来STEM教育的课程规划具有重要指导意义。"}}
{"id": "2507.19242", "title": "Foundation Model-Driven Grasping of Unknown Objects via Center of Gravity Estimation", "authors": ["Kang Xiangli", "Yage He", "Xianwu Gong", "Zehan Liu", "Yuru Bai"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19242v1", "summary": "This study presents a grasping method for objects with uneven mass\ndistribution by leveraging diffusion models to localize the center of gravity\n(CoG) on unknown objects. In robotic grasping, CoG deviation often leads to\npostural instability, where existing keypoint-based or affordance-driven\nmethods exhibit limitations. We constructed a dataset of 790 images featuring\nunevenly distributed objects with keypoint annotations for CoG localization. A\nvision-driven framework based on foundation models was developed to achieve\nCoG-aware grasping. Experimental evaluations across real-world scenarios\ndemonstrate that our method achieves a 49\\% higher success rate compared to\nconventional keypoint-based approaches and an 11\\% improvement over\nstate-of-the-art affordance-driven methods. The system exhibits strong\ngeneralization with a 76\\% CoG localization accuracy on unseen objects,\nproviding a novel solution for precise and stable grasping tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19242v1", "cate": "cs.RO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于基础模型通过重心估计抓取未知物体", "tldr": "本研究提出了一种利用扩散模型估计未知物体重心（CoG）的抓取方法，解决了现有方法在抓取质量分布不均物体时姿态不稳定的问题，实验表明其抓取成功率显著提高，并具有良好的泛化性。", "motivation": "现有机器人抓取方法在处理质量分布不均的物体时，由于重心偏移常导致姿态不稳定，且基于关键点或可供性驱动的方法存在局限性。", "method": "本研究提出了一种利用扩散模型定位未知物体重心（CoG）的抓取方法，以解决质量分布不均物体的抓取问题。为此，构建了一个包含790张具有关键点标注的不均匀分布物体图像数据集。开发了一个基于基础模型的视觉驱动框架，以实现重心感知的抓取。", "result": "该方法在真实场景中，相比传统基于关键点的方法，抓取成功率提高了49%；相比最先进的可供性驱动方法，提高了11%。系统对未见物体表现出强大的泛化能力，重心定位准确率达到76%。", "conclusion": "本研究为精确和稳定的抓取任务提供了一种新颖的解决方案，尤其适用于抓取质量分布不均的未知物体。", "translation": "标题：基于基础模型通过重心估计抓取未知物体\n摘要：本研究提出了一种通过利用扩散模型定位未知物体重心（CoG）来抓取质量分布不均物体的方法。在机器人抓取中，CoG偏差常导致姿态不稳定，而现有基于关键点或可供性驱动的方法存在局限性。我们构建了一个包含790张具有关键点标注的不均匀分布物体图像数据集，用于CoG定位。开发了一个基于基础模型的视觉驱动框架，以实现CoG感知的抓取。真实世界场景的实验评估表明，我们的方法比传统基于关键点的方法成功率高出49%，比最先进的可供性驱动方法提高了11%。该系统对未见物体表现出强大的泛化能力，CoG定位准确率达到76%，为精确和稳定的抓取任务提供了一种新颖的解决方案。", "summary": "本文提出了一种创新的机器人抓取方法，专门针对质量分布不均的未知物体。该方法利用扩散模型来精确估计物体的重心（CoG），从而解决传统抓取方法因CoG偏差导致的姿态不稳定性问题。通过构建专用数据集并开发基于基础模型的视觉驱动框架，实验结果显示，该方法在抓取成功率上显著优于现有技术（比关键点方法高49%，比可供性方法高11%），并对未见物体展现出强大的泛化能力（76%的CoG定位准确率），为稳定和精确的抓取提供了新途径。", "keywords": "机器人抓取, 重心估计, 扩散模型, 基础模型, 未知物体", "comments": "本文的创新点在于将扩散模型引入机器人抓取领域，用于精确估计未知物体的重心，从而有效解决了传统方法在处理质量分布不均物体时的姿态不稳定性问题。其基于基础模型的视觉驱动框架和专用数据集的构建也值得关注。该研究为未来机器人抓取复杂物体提供了新的思路和强大的泛化能力，具有重要的实践意义。"}}
{"id": "2506.03219", "title": "HARNode: A Time-Synchronised, Open-Source, Multi-Device, Wearable System for Ad Hoc Field Studies", "authors": ["Philipp Lepold", "Tobias Röddiger", "Michael Beigl"], "categories": ["cs.NI", "C.3; I.2.9"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      6 pages, 6 figures", "url": "http://arxiv.org/abs/2506.03219v2", "summary": "Human activity recognition (HAR) research often lacks accessible,\ncomprehensive field data. Commercial systems are rarely open source, hard to\nexpand, and limited by issues like node synchronisation, data throughput,\nunclear sensor placement, complexity, and high cost. As a result, researchers\ntypically use only a few intuitively placed sensors and conduct limited field\ntrials. HARNode overcomes these challenges with a fully open-source hardware\nand software platform. Each node includes an ESP32-S3 module (AtomS3), a 9-axis\nIMU (Bosch BMX160), pressure and temperature sensors (Bosch BMP388), a display,\nand an I2C port. Data is streamed via Wi-Fi, with NTP-based time\nsynchronisation achieving roughly 1 ms accuracy. The system runs for up to 8\nhours and is built using off-the-shelf parts, a simple online PCB service, and\na compact 3D-printed housing with Velcro straps, enabling flexible and scalable\nbody placement while requiring little hardware knowledge. In a study with ten\nsubjects wearing eleven HARNodes each, setup took under five minutes per\nperson. A random forest classifier distinguished walking from stair-climbing\ntransitions, showing the benefits of sensor-overprovisioning: Seven nodes\nachieved approx. 98% accuracy, matching the performance of all eleven. These\nfindings confirm HARNode's value as a fast-deploying, scalable tool for\nfield-based HAR research and optimised sensor placement.", "comment": "6 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2506.03219v2", "cate": "cs.NI", "date": "2025-06-03", "updated": "2025-07-25", "AI": {"title_translation": "HARNode：一种用于临时实地研究的时间同步、开源、多设备可穿戴系统", "tldr": "HARNode是一个开源、时间同步的可穿戴系统，用于人体活动识别（HAR）的实地研究，解决了现有商业系统在数据采集、扩展性和部署方面的挑战。", "motivation": "现有的人体活动识别（HAR）研究缺乏可访问、全面的实地数据。商业系统通常不是开源的，难以扩展，并受到节点同步、数据吞吐量、传感器放置不明确、复杂性和高成本等问题的限制，导致研究人员通常只能使用少量传感器进行有限的实地试验。", "method": "HARNode是一个完全开源的软硬件平台。每个节点包含一个ESP32-S3模块（AtomS3）、一个9轴IMU（Bosch BMX160）、压力和温度传感器（Bosch BMP388）、一个显示屏和一个I2C端口。数据通过Wi-Fi传输，采用基于NTP的时间同步，精度约为1毫秒。系统可运行长达8小时，使用市售部件、简单的在线PCB服务和紧凑的3D打印外壳构建，通过魔术贴实现灵活可扩展的身体放置，且仅需少量硬件知识。", "result": "在对十名受试者进行的佩戴HARNode（每人十一个）的研究中，每人设置时间不到五分钟。一个随机森林分类器成功区分了步行和爬楼梯的转换，并显示了传感器过量配置的优势：七个节点实现了约98%的准确率，与所有十一个节点的性能匹配。", "conclusion": "这些发现证实了HARNode作为一种快速部署、可扩展的工具，在基于实地的HAR研究和优化传感器放置方面的价值。", "translation": "人体活动识别（HAR）研究通常缺乏可访问、全面的实地数据。商业系统很少开源，难以扩展，并受到节点同步、数据吞吐量、传感器放置不明确、复杂性和高成本等问题的限制。因此，研究人员通常只使用少数凭直觉放置的传感器并进行有限的实地试验。HARNode通过一个完全开源的硬件和软件平台克服了这些挑战。每个节点包括一个ESP32-S3模块（AtomS3）、一个9轴IMU（Bosch BMX160）、压力和温度传感器（Bosch BMP388）、一个显示屏和一个I2C端口。数据通过Wi-Fi传输，采用基于NTP的时间同步，精度约为1毫秒。该系统可运行长达8小时，使用市售部件、简单的在线PCB服务和紧凑的3D打印外壳构建，通过魔术贴实现灵活和可扩展的身体放置，同时仅需少量硬件知识。在一项针对十名受试者（每人佩戴十一个HARNode）的研究中，每人设置时间不到五分钟。一个随机森林分类器区分了步行和爬楼梯的转换，显示了传感器过量配置的优势：七个节点实现了约98%的准确率，与所有十一个节点的性能匹配。这些发现证实了HARNode作为一种快速部署、可扩展的工具，在基于实地的HAR研究和优化传感器放置方面的价值。", "summary": "HARNode是一个开源、时间同步、多设备可穿戴系统，专为解决人体活动识别（HAR）实地研究中数据采集的挑战而设计。它提供了一个包含多种传感器的硬件平台和易于部署的软件，支持Wi-Fi数据流和毫秒级时间同步。该系统易于构建，成本低廉，且可灵活佩戴。实验证明，HARNode能快速设置，并有效区分活动，同时揭示了传感器冗余配置在保持高准确率方面的潜力，从而验证了其作为HAR实地研究和传感器优化工具的有效性。", "keywords": "HARNode, 人体活动识别, 开源系统, 可穿戴设备, 实地研究", "comments": "HARNode的创新之处在于其完全开源的软硬件设计，这极大地降低了HAR实地研究的门槛，并提高了系统的可扩展性和可定制性。时间同步的精确度（约1毫秒）是其关键优势之一，解决了多节点数据集成中的常见难题。此外，其低成本、易于构建和灵活部署的特性，使其成为HAR领域研究人员的有力工具。通过实验证明传感器过量配置的有效性，也为未来传感器布局优化提供了宝贵的见解。"}}
{"id": "2507.19121", "title": "Preserving Topological and Geometric Embeddings for Point Cloud Recovery", "authors": ["Kaiyue Zhou", "Zelong Tan", "Hongxiao Wang", "Ya-li Li", "Shengjin Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19121v1", "summary": "Recovering point clouds involves the sequential process of sampling and\nrestoration, yet existing methods struggle to effectively leverage both\ntopological and geometric attributes. To address this, we propose an end-to-end\narchitecture named \\textbf{TopGeoFormer}, which maintains these critical\nfeatures throughout the sampling and restoration phases. First, we revisit\ntraditional feature extraction techniques to yield topological embedding using\na continuous mapping of relative relationships between neighboring points, and\nintegrate it in both phases for preserving the structure of the original space.\nSecond, we propose the \\textbf{InterTwining Attention} to fully merge\ntopological and geometric embeddings, which queries shape with local awareness\nin both phases to form a learnable shape context facilitated with point-wise,\npoint-shape-wise, and intra-shape features. Third, we introduce a full geometry\nloss and a topological constraint loss to optimize the embeddings in both\nEuclidean and topological spaces. The geometry loss uses inconsistent matching\nbetween coarse-to-fine generations and targets for reconstructing better\ngeometric details, and the constraint loss limits embedding variances for\nbetter approximation of the topological space. In experiments, we\ncomprehensively analyze the circumstances using the conventional and\nlearning-based sampling/upsampling algorithms. The quantitative and qualitative\nresults demonstrate that our method significantly outperforms existing sampling\nand recovery methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19121v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "点云恢复中拓扑和几何嵌入的保持", "tldr": "提出TopGeoFormer架构，通过保持拓扑和几何嵌入来显著提升点云恢复性能。", "motivation": "现有方法在点云恢复的采样和恢复过程中，难以有效利用拓扑和几何属性。", "method": "提出端到端架构TopGeoFormer。首先，通过连续映射邻近点间相对关系得到拓扑嵌入，并将其整合到采样和恢复阶段。其次，提出InterTwining Attention机制，用于充分融合拓扑和几何嵌入，形成可学习的形状上下文。最后，引入完整几何损失和拓扑约束损失，分别用于优化欧几里得空间和拓扑空间中的嵌入，以重建更好的几何细节并限制嵌入方差。", "result": "通过定量和定性分析，实验结果表明所提出的方法显著优于现有采样和恢复方法。", "conclusion": "所提出的TopGeoFormer架构通过有效保持拓扑和几何嵌入，显著提升了点云恢复的性能。", "translation": "点云恢复涉及采样和恢复的顺序过程，然而现有方法难以有效利用拓扑和几何属性。为了解决这个问题，我们提出了一种名为TopGeoFormer的端到端架构，它在采样和恢复阶段保持了这些关键特征。首先，我们重新审视了传统的特征提取技术，通过邻近点之间相对关系的连续映射来生成拓扑嵌入，并将其整合到两个阶段中，以保持原始空间的结构。其次，我们提出了InterTwining Attention来充分融合拓扑和几何嵌入，它在两个阶段中以局部感知的方式查询形状，形成一个由点级、点形状级和形状内特征辅助的可学习形状上下文。第三，我们引入了完整的几何损失和拓扑约束损失，以优化欧几里得空间和拓扑空间中的嵌入。几何损失使用粗到细生成和目标之间不一致的匹配来重建更好的几何细节，而约束损失限制了嵌入方差，以更好地近似拓扑空间。在实验中，我们使用传统和基于学习的采样/上采样算法全面分析了各种情况。定量和定性结果表明，我们的方法显著优于现有的采样和恢复方法。", "summary": "该论文提出了一种名为TopGeoFormer的端到端架构，旨在解决现有方法在点云恢复中未能有效利用拓扑和几何属性的问题。TopGeoFormer通过重新审视传统特征提取方法来生成拓扑嵌入，并引入InterTwining Attention机制来融合拓扑和几何嵌入。此外，论文还提出了完整的几何损失和拓扑约束损失来优化嵌入。实验结果表明，该方法在点云采样和恢复方面显著优于现有方法。", "keywords": "点云恢复, 拓扑嵌入, 几何嵌入, TopGeoFormer, 注意力机制", "comments": "该论文的创新点在于提出了一个端到端的架构TopGeoFormer，并特别关注了在点云恢复过程中同时保持拓扑和几何嵌入。引入InterTwining Attention和两种新型损失函数（几何损失和拓扑约束损失）是其方法的核心，有效解决了现有方法在处理这两种关键属性时的不足。这项工作对于提升点云处理的精度和鲁棒性具有重要意义。"}}
{"id": "2507.19465", "title": "Linearly Convergent Algorithms for Nonsmooth Problems with Unknown Smooth Pieces", "authors": ["Zhe Zhang", "Suvrit Sra"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19465v1", "summary": "We develop efficient algorithms for optimizing piecewise smooth (PWS)\nfunctions where the underlying partition of the domain into smooth pieces is\n\\emph{unknown}. For PWS functions satisfying a quadratic growth (QG) condition,\nwe propose a bundle-level (BL) type method that achieves global linear\nconvergence -- to our knowledge, the first such result for any algorithm for\nthis problem class. We extend this method to handle approximately PWS functions\nand to solve weakly-convex PWS problems, improving the state-of-the-art\ncomplexity to match the benchmark for smooth non-convex optimization.\nFurthermore, we introduce the first verifiable and accurate termination\ncriterion for PWS optimization. Similar to the gradient norm in smooth\noptimization, this certificate tightly characterizes the optimality gap under\nthe QG condition, and can moreover be evaluated without knowledge of any\nproblem parameters. We develop a search subroutine for this certificate and\nembed it within a guess-and-check framework, resulting in an almost\nparameter-free algorithm for both the convex QG and weakly-convex settings.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19465v1", "cate": "math.OC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "具有未知光滑部分的非光滑问题的线性收敛算法", "tldr": "本文为域分区未知的非光滑分段（PWS）函数优化开发了高效算法，首次实现了全局线性收敛，并引入了首个可验证且无需参数的终止准则，构建了近乎无参数的优化方法。", "motivation": "现有PWS函数优化方法通常需要已知其域的基础分区，但在实际应用中，这种分区往往是未知的，这限制了现有算法的适用性。", "method": "提出了一种束级（BL）类型的方法，用于优化满足二次增长（QG）条件的PWS函数。该方法被扩展到处理近似PWS函数和解决弱凸PWS问题。此外，引入了首个可验证且精确的PWS优化终止准则，并为此开发了一个搜索子程序，将其嵌入到猜测-检查框架中，从而形成一个几乎无参数的算法。", "result": "实现了PWS函数优化的全局线性收敛，这是此类问题算法的首次此类结果。将现有技术的复杂性提高到与光滑非凸优化的基准相匹配。提出了首个可验证且精确的PWS优化终止准则，该准则无需已知问题参数即可评估，并适用于凸QG和弱凸设置。", "conclusion": "本文为域分区未知的PWS函数优化提供了高效且几乎无参数的算法，首次实现了全局线性收敛，并引入了创新的可验证终止准则，显著推动了非光滑优化领域的进展。", "translation": "我们开发了优化分段光滑（PWS）函数的有效算法，其中域的基础分区是\n未知\n的。对于满足二次增长（QG）条件的PWS函数，我们提出了一种束级（BL）类型的方法，该方法实现了全局线性收敛——据我们所知，这是此类问题任何算法的首次此类结果。我们将此方法扩展到处理近似PWS函数并解决弱凸PWS问题，将现有技术水平的复杂性提高到与光滑非凸优化的基准相匹配。此外，我们引入了PWS优化的首个可验证且精确的终止准则。与光滑优化中的梯度范数类似，该证书在QG条件下紧密表征了最优性间隙，并且可以在不了解任何问题参数的情况下进行评估。我们为该证书开发了一个搜索子程序，并将其嵌入到猜测-检查框架中，从而为凸QG和弱凸设置提供了一个几乎无参数的算法。", "summary": "本文提出了一系列高效算法，用于优化域分区未知的非光滑分段（PWS）函数。针对满足二次增长条件的PWS函数，开发了一种束级方法，首次实现了全局线性收敛，并将其扩展至近似PWS和弱凸问题，显著提升了算法复杂性。此外，研究还引入了首个可验证、无需参数的PWS优化终止准则，并通过结合搜索子程序和猜测-检查框架，构建了一个几乎无参数的通用算法。", "keywords": "分段光滑函数, 线性收敛, 非光滑优化, 束级方法, 终止准则", "comments": "本文的创新点在于首次为未知光滑分段的PWS函数优化实现了全局线性收敛，并提出了一个无需问题参数即可评估的、可验证的终止准则，这在非光滑优化领域是一个重要的突破，极大地提升了算法的实用性和理论完整性。其“几乎无参数”的特性也使其在实际应用中更具吸引力。"}}
{"id": "2507.17974", "title": "Natural Language Processing for Tigrinya: Current State and Future Directions", "authors": ["Fitsum Gaim", "Jong C. Park"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17974v2", "summary": "Despite being spoken by millions of people, Tigrinya remains severely\nunderrepresented in Natural Language Processing (NLP) research. This work\npresents a comprehensive survey of NLP research for Tigrinya, analyzing over 40\nstudies spanning more than a decade of work from 2011 to 2025. We\nsystematically review the current state of computational resources, models, and\napplications across ten distinct downstream tasks, including morphological\nprocessing, machine translation, speech recognition, and question-answering.\nOur analysis reveals a clear trajectory from foundational, rule-based systems\nto modern neural architectures, with progress consistently unlocked by resource\ncreation milestones. We identify key challenges rooted in Tigrinya's\nmorphological complexity and resource scarcity, while highlighting promising\nresearch directions, including morphology-aware modeling, cross-lingual\ntransfer, and community-centered resource development. This work serves as both\na comprehensive reference for researchers and a roadmap for advancing Tigrinya\nNLP. A curated metadata of the surveyed studies and resources is made publicly\navailable.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17974v2", "cate": "cs.CL", "date": "2025-07-23", "updated": "2025-07-25", "AI": {"title_translation": "提格利尼亚语的自然语言处理：现状与未来方向", "tldr": "该论文全面调查了提格利尼亚语的自然语言处理（NLP）研究现状，分析了2011年至2025年间的40多项研究，揭示了从基于规则系统到现代神经网络架构的演变，并指出了挑战和未来研究方向。", "motivation": "尽管提格利尼亚语有数百万人使用，但在自然语言处理（NLP）研究中仍严重不足。", "method": "本文对提格利尼亚语的NLP研究进行了全面调查，分析了2011年至2025年间超过40项研究。系统地回顾了计算资源、模型和跨十个不同下游任务（包括形态处理、机器翻译、语音识别和问答）的应用现状。", "result": "分析揭示了从基础的、基于规则的系统到现代神经网络架构的清晰轨迹，进展始终伴随着资源创建的里程碑。识别出根源于提格利尼亚语形态复杂性和资源稀缺性的关键挑战，并强调了有前景的研究方向，包括形态感知建模、跨语言迁移和以社区为中心的资源开发。", "conclusion": "这项工作既为研究人员提供了全面的参考，也为推进提格利尼亚语NLP提供了路线图。", "translation": "尽管有数百万人使用，但提格利尼亚语在自然语言处理（NLP）研究中仍然严重不足。这项工作对提格利尼亚语的NLP研究进行了全面调查，分析了2011年至2025年间跨越十多年的40多项研究。我们系统地回顾了计算资源、模型以及跨十个不同下游任务（包括形态处理、机器翻译、语音识别和问答）的应用现状。我们的分析揭示了从基础的、基于规则的系统到现代神经网络架构的清晰轨迹，进展始终伴随着资源创建的里程碑。我们识别出根源于提格利尼亚语形态复杂性和资源稀缺性的关键挑战，同时强调了有前景的研究方向，包括形态感知建模、跨语言迁移和以社区为中心的资源开发。这项工作既为研究人员提供了全面的参考，也为推进提格利尼亚语NLP提供了路线图。调查研究和资源的精选元数据已公开可用。", "summary": "本文对提格利尼亚语的自然语言处理（NLP）研究进行了全面调查，分析了2011年至2025年间的40多项研究。研究回顾了计算资源、模型和应用在十个下游任务中的现状，揭示了从规则系统到神经网络架构的演变，并指出资源创建是进展的关键。论文识别了形态复杂性和资源稀缺性带来的挑战，并提出了形态感知建模、跨语言迁移和社区资源开发等未来研究方向。该工作旨在为提格利尼亚语NLP研究提供参考和路线图。", "keywords": "提格利尼亚语, 自然语言处理, 语言资源, 机器翻译, 形态学", "comments": "本文作为一篇综述性论文，其重要性在于系统地梳理了提格利尼亚语NLP领域的现有工作，并为未来研究指明了方向。对于资源匮乏语言的NLP研究而言，这种全面的现状分析和路线图具有重要的指导意义和实用价值。"}}
{"id": "2507.18804", "title": "Ralts: Robust Aggregation for Enhancing Graph Neural Network Resilience on Bit-flip Errors", "authors": ["Wencheng Zou", "Nan Wu"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18804v1", "summary": "Graph neural networks (GNNs) have been widely applied in safety-critical\napplications, such as financial and medical networks, in which compromised\npredictions may cause catastrophic consequences. While existing research on GNN\nrobustness has primarily focused on software-level threats, hardware-induced\nfaults and errors remain largely underexplored. As hardware systems progress\ntoward advanced technology nodes to meet high-performance and energy efficiency\ndemands, they become increasingly susceptible to transient faults, which can\ncause bit flips and silent data corruption, a prominent issue observed by major\ntechnology companies (e.g., Meta and Google). In response, we first present a\ncomprehensive analysis of GNN robustness against bit-flip errors, aiming to\nreveal system-level optimization opportunities for future reliable and\nefficient GNN systems. Second, we propose Ralts, a generalizable and\nlightweight solution to bolster GNN resilience to bit-flip errors.\nSpecifically, Ralts exploits various graph similarity metrics to filter out\noutliers and recover compromised graph topology, and incorporates these\nprotective techniques directly into aggregation functions to support any\nmessage-passing GNNs. Evaluation results demonstrate that Ralts effectively\nenhances GNN robustness across a range of GNN models, graph datasets, error\npatterns, and both dense and sparse architectures. On average, under a BER of\n$3\\times10^{-5}$, these robust aggregation functions improve prediction\naccuracy by at least 20\\% when errors present in model weights or node\nembeddings, and by at least 10\\% when errors occur in adjacency matrices. Ralts\nis also optimized to deliver execution efficiency comparable to built-in\naggregation functions in PyTorch Geometric.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18804v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "Ralts：用于增强图神经网络对位翻转错误弹性的鲁棒聚合方法", "tldr": "Ralts提出了一种通用且轻量级的鲁棒聚合方法，通过利用图相似性指标来过滤异常值并恢复受损的图拓扑结构，从而显著提高图神经网络（GNNs）在硬件位翻转错误下的弹性，同时保持高效的执行效率。", "motivation": "图神经网络（GNNs）广泛应用于金融和医疗等安全关键领域，但现有研究主要关注软件层面的威胁，而硬件诱发的瞬态故障（如位翻转错误）却被严重忽视。随着硬件技术发展到更先进的节点，它们对这些瞬态故障的敏感性增加，可能导致静默数据损坏，这在主流科技公司中已成为一个突出问题。因此，有必要解决GNNs在硬件位翻转错误下的鲁棒性问题。", "method": "本研究首先对GNNs在位翻转错误下的鲁棒性进行了全面分析。其次，提出了名为Ralts的通用且轻量级解决方案，以增强GNNs对位翻转错误的弹性。Ralts利用各种图相似性指标来过滤异常值并恢复受损的图拓扑结构，并将这些保护技术直接整合到聚合函数中，以支持任何消息传递GNNs。", "result": "评估结果表明，Ralts能有效提高GNNs在各种GNN模型、图数据集、错误模式以及密集和稀疏架构下的鲁棒性。在BER为$3\times10^{-5}$的条件下，当错误出现在模型权重或节点嵌入中时，这些鲁棒聚合函数可将预测精度平均提高至少20%；当错误发生在邻接矩阵中时，预测精度平均提高至少10%。Ralts还经过优化，其执行效率与PyTorch Geometric中内置的聚合函数相当。", "conclusion": "Ralts提供了一种有效且高效的聚合方法，显著增强了图神经网络在面对硬件位翻转错误时的鲁棒性，从而使其能够更可靠地应用于安全关键领域。", "translation": "图神经网络（GNNs）已广泛应用于金融和医疗网络等安全关键应用中，其中预测受损可能导致灾难性后果。尽管现有GNN鲁棒性研究主要集中在软件级威胁上，但硬件诱发的故障和错误在很大程度上仍未被充分探索。随着硬件系统朝着先进技术节点发展以满足高性能和能效需求，它们也变得越来越容易受到瞬态故障的影响，这可能导致位翻转和静默数据损坏，这是主要科技公司（如Meta和Google）观察到的一个突出问题。为此，我们首先对GNNs对位翻转错误的鲁棒性进行了全面分析，旨在为未来可靠高效的GNN系统揭示系统级优化机会。其次，我们提出了Ralts，一个通用且轻量级的解决方案，以增强GNNs对位翻转错误的弹性。具体而言，Ralts利用各种图相似性指标来过滤异常值并恢复受损的图拓扑结构，并将这些保护技术直接整合到聚合函数中，以支持任何消息传递GNNs。评估结果表明，Ralts有效增强了GNN在各种GNN模型、图数据集、错误模式以及密集和稀疏架构下的鲁棒性。平均而言，在误码率（BER）为$3\times10^{-5}$的情况下，当错误出现在模型权重或节点嵌入中时，这些鲁棒聚合函数将预测精度提高了至少20%；当错误发生在邻接矩阵中时，预测精度提高了至少10%。Ralts还经过优化，其执行效率与PyTorch Geometric中内置的聚合函数相当。", "summary": "本论文针对图神经网络（GNNs）在安全关键应用中面临的硬件位翻转错误问题，提出了一种名为Ralts的鲁棒聚合方法。现有研究多关注软件威胁，但硬件故障日益突出。Ralts通过利用图相似性指标来识别并恢复受损的图拓扑结构，并将这些保护机制集成到聚合函数中，适用于各类消息传递GNN。实验证明，Ralts显著提升了GNN在不同模型、数据集和错误模式下的预测精度，且保持了与现有聚合函数相当的执行效率。", "keywords": "图神经网络, 位翻转错误, 鲁棒性, 聚合函数, 硬件故障", "comments": "Ralts的创新之处在于它首次系统性地解决了GNNs在硬件层面，特别是位翻转错误下的鲁棒性问题，这是一个在安全关键应用中至关重要的未被充分探索的领域。其通用且轻量级的聚合函数设计，通过利用图相似性来修复受损拓扑，为提升GNNs的硬件容错能力提供了高效实用的解决方案。该方法不仅在性能上展现出显著的精度提升，同时兼顾了执行效率，使其具有很高的实际应用价值。"}}
{"id": "2507.19054", "title": "Closing the Modality Gap for Mixed Modality Search", "authors": ["Binxu Li", "Yuhui Zhang", "Xiaohan Wang", "Weixin Liang", "Ludwig Schmidt", "Serena Yeung-Levy"], "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.19054v1", "summary": "Mixed modality search -- retrieving information across a heterogeneous corpus\ncomposed of images, texts, and multimodal documents -- is an important yet\nunderexplored real-world application. In this work, we investigate how\ncontrastive vision-language models, such as CLIP, perform on the mixed modality\nsearch task. Our analysis reveals a critical limitation: these models exhibit a\npronounced modality gap in the embedding space, where image and text embeddings\nform distinct clusters, leading to intra-modal ranking bias and inter-modal\nfusion failure. To address this issue, we propose GR-CLIP, a lightweight\npost-hoc calibration method that removes the modality gap in CLIP's embedding\nspace. Evaluated on MixBench -- the first benchmark specifically designed for\nmixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage points\nover CLIP, surpasses recent vision-language generative embedding models by 4\npercentage points, while using 75x less compute.", "comment": "Project page: https://yuhui-zh15.github.io/MixedModalitySearch/", "pdf_url": "http://arxiv.org/pdf/2507.19054v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "弥合混合模态搜索的模态鸿沟", "tldr": "本文提出了GR-CLIP，一种轻量级的后处理校准方法，用于消除CLIP模型在混合模态搜索中的模态鸿沟，显著提升了搜索性能。", "motivation": "混合模态搜索是一个重要但未被充分探索的实际应用。现有的对比视觉-语言模型（如CLIP）在混合模态搜索任务中存在明显的模态鸿沟，导致嵌入空间中图像和文本嵌入形成不同的簇，进而产生模态内排序偏差和模态间融合失败。", "method": "提出GR-CLIP，一种轻量级的后处理校准方法，旨在消除CLIP嵌入空间中的模态鸿沟。", "result": "在MixBench基准测试中，GR-CLIP相较于CLIP将NDCG@10提高了多达26个百分点，同时比最新的视觉-语言生成嵌入模型高出4个百分点，并且计算量减少了75倍。", "conclusion": "GR-CLIP成功地弥合了CLIP模型在混合模态搜索中的模态鸿沟，显著提升了搜索性能，并表现出高效率。", "translation": "混合模态搜索——在由图像、文本和多模态文档组成的异构语料库中检索信息——是一个重要但尚未充分探索的实际应用。在这项工作中，我们研究了对比视觉-语言模型，如CLIP，在混合模态搜索任务上的表现。我们的分析揭示了一个关键的局限性：这些模型在嵌入空间中表现出明显的模态鸿沟，其中图像和文本嵌入形成不同的簇，导致模态内排序偏差和模态间融合失败。为了解决这个问题，我们提出了GR-CLIP，一种轻量级的后处理校准方法，可以消除CLIP嵌入空间中的模态鸿沟。在MixBench——第一个专门为混合模态搜索设计的基准测试上进行评估，GR-CLIP在NDCG@10上比CLIP提高了多达26个百分点，超过了最近的视觉-语言生成嵌入模型4个百分点，同时使用的计算量减少了75倍。", "summary": "本研究关注混合模态搜索这一重要但未充分探索的应用。分析发现，CLIP等对比视觉-语言模型存在显著的模态鸿沟问题，导致搜索性能受限。为解决此问题，论文提出GR-CLIP，一种轻量级的后处理校准方法，用于消除CLIP嵌入空间中的模态鸿沟。在MixBench基准测试上，GR-CLIP在NDCG@10指标上比CLIP提升高达26个百分点，并以75倍更低的计算量超越了最新的视觉-语言生成嵌入模型4个百分点，证明了其有效性和效率。", "keywords": "混合模态搜索, 模态鸿沟, CLIP, GR-CLIP, 校准", "comments": "该论文的创新点在于提出了GR-CLIP这一轻量级后处理校准方法，有效解决了CLIP模型在混合模态搜索中存在的模态鸿沟问题。其重要性体现在显著提升了混合模态搜索的性能，并且计算效率极高，使其在实际应用中具有巨大潜力。"}}
{"id": "2507.19210", "title": "Optimal Control of Hybrid Systems via Measure Relaxations", "authors": ["Etienne Buehrle", "Ömer Şahin Taş", "Christoph Stiller"], "categories": ["math.OC", "cs.SY", "eess.SY", "90C22, 93C10, 28A99"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      7 pages, 6 figures, accepted at CDC 2025", "url": "http://arxiv.org/abs/2507.19210v1", "summary": "We propose an approach to trajectory optimization for piecewise polynomial\nsystems based on the recently proposed graphs of convex sets framework. We\ninstantiate the framework with a convex relaxation of optimal control based on\noccupation measures, resulting in a convex optimization problem resembling the\ndiscrete shortest-paths linear program that can be solved efficiently to global\noptimality. While this approach inherits the limitations of semidefinite\nprogramming, scalability to large numbers of discrete modes improves compared\nto the NP-hard mixed-integer formulation. We use this to plan trajectories\nunder temporal logic specifications, comparing the computed cost lower bound to\na nonconvex optimization approach with fixed mode sequence. In our numerical\nexperiments, we find that this bound is typically in the vicinity of the\nnonconvex solution, while the runtime speedup is significant compared to the\noften intractable mixed-integer formulation. Our implementation is available at\nhttps://github.com/ebuehrle/hpoc.", "comment": "7 pages, 6 figures, accepted at CDC 2025", "pdf_url": "http://arxiv.org/pdf/2507.19210v1", "cate": "math.OC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "混合系统测度松弛最优控制", "tldr": "提出了一种基于凸集图和测度松弛的混合系统轨迹优化方法，通过凸优化实现全局最优，显著提高了大规模离散模式下的可伸缩性和运行速度。", "motivation": "解决混合系统轨迹优化中，传统混合整数公式NP-难且难以处理的挑战，寻求一种更高效、可伸缩且能达到全局最优的替代方案。", "method": "该方法基于最近提出的凸集图框架，并结合了基于占据测度的最优控制凸松弛。这产生了一个类似于离散最短路径线性规划的凸优化问题，可以高效地求解到全局最优。该方法还用于在时序逻辑规范下规划轨迹。", "result": "该方法能够高效地求解到全局最优。与NP-难的混合整数公式相比，其在大数量离散模式下的可伸缩性得到改善，并且运行时间显著加快。数值实验表明，计算出的成本下界通常接近非凸解。", "conclusion": "通过基于测度松弛的凸优化方法，可以有效且高效地解决混合系统的轨迹优化问题，提供接近非凸解的下界，并显著优于传统的混合整数公式。", "translation": "我们提出了一种基于最近提出的凸集图框架的用于分段多项式系统轨迹优化的方法。我们用基于占据测度的最优控制凸松弛来实例化该框架，从而得到一个类似于离散最短路径线性规划的凸优化问题，该问题可以高效地求解到全局最优。虽然这种方法继承了半定规划的局限性，但与NP难的混合整数公式相比，其对大量离散模式的可伸缩性有所提高。我们使用这种方法在时序逻辑规范下规划轨迹，并将计算出的成本下界与固定模式序列的非凸优化方法进行比较。在我们的数值实验中，我们发现这个下界通常在非凸解的附近，而与通常难以处理的混合整数公式相比，运行时间显著加快。我们的实现代码可在 https://github.com/ebuehrle/hpoc 获取。", "summary": "本文提出了一种用于分段多项式系统轨迹优化的新方法，该方法结合了凸集图框架和基于占据测度的最优控制凸松弛。通过将问题转化为可高效求解的凸优化问题，实现了全局最优，并在处理大量离散模式时表现出比传统混合整数公式更强的可伸缩性和显著的运行速度提升。数值实验表明，该方法的成本下界接近非凸解。", "keywords": "混合系统, 轨迹优化, 测度松弛, 凸优化", "comments": "这项工作创新性地将凸集图框架与测度松弛相结合，为混合系统的轨迹优化提供了一个高效且可伸缩的解决方案。它克服了传统混合整数规划在处理大规模问题时的计算障碍，尽管继承了半定规划的局限性，但其在可伸缩性和运行速度上的显著提升使其在实际应用中具有重要价值。开源实现也促进了研究的可重复性和进一步发展。"}}
{"id": "2507.19296", "title": "ABCD: Automatic Blood Cell Detection via Attention-Guided Improved YOLOX", "authors": ["Ahmed Endris Hasen", "Yang Shangming", "Chiagoziem C. Ukwuoma", "Biniyam Gashaw", "Abel Zenebe Yutra"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19296v1", "summary": "Detection of blood cells in microscopic images has become a major focus of\nmedical image analysis, playing a crucial role in gaining valuable insights\ninto a patient's health. Manual blood cell checks for disease detection are\nknown to be time-consuming, inefficient, and error-prone. To address these\nlimitations, analyzing blood cells using deep learning-based object detectors\ncan be regarded as a feasible solution. In this study, we propose automatic\nblood cell detection method (ABCD) based on an improved version of YOLOX, an\nobject detector, for detecting various types of blood cells, including white\nblood cells, red blood cells, and platelets. Firstly, we introduce the\nConvolutional Block Attention Module (CBAM) into the network's backbone to\nenhance the efficiency of feature extraction. Furthermore, we introduce the\nAdaptively Spatial Feature Fusion (ASFF) into the network's neck, which\noptimizes the fusion of different features extracted from various stages of the\nnetwork. Finally, to speed up the model's convergence, we substitute the\nIntersection over Union (IOU) loss function with the Complete Intersection over\nUnion (CIOU) loss function. The experimental results demonstrate that the\nproposed method is more effective than other existing methods for BCCD dataset.\nCompared to the baseline algorithm, our method ABCD achieved 95.49 % mAP@0.5\nand 86.89 % mAP@0.5-0.9, which are 2.8% and 23.41% higher, respectively, and\nincreased the detection speed by 2.9%, making it highly efficient for real-time\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19296v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "ABCD：基于注意力引导改进YOLOX的自动血细胞检测", "tldr": "本研究提出了一种名为ABCD的自动血细胞检测方法，通过改进YOLOX模型，引入CBAM、ASFF和CIOU损失函数，显著提高了血细胞检测的准确性和效率，适用于实时应用。", "motivation": "在显微图像中检测血细胞对了解患者健康状况至关重要。手动血细胞检查耗时、低效且容易出错。为解决这些局限性，使用基于深度学习的目标检测器分析血细胞是一种可行的解决方案。", "method": "本研究提出了一种名为ABCD的自动血细胞检测方法，该方法基于改进版的YOLOX目标检测器，用于检测白细胞、红细胞和血小板等各种类型的血细胞。具体改进包括：1. 在网络骨干中引入卷积块注意力模块（CBAM），以增强特征提取效率。2. 在网络颈部引入自适应空间特征融合（ASFF），优化网络不同阶段提取的不同特征的融合。3. 将IOU损失函数替换为CIOU损失函数，以加快模型收敛。", "result": "实验结果表明，所提出的方法在BCCD数据集上比其他现有方法更有效。与基线算法相比，ABCD方法在mAP@0.5上达到了95.49 %，提高了2.8%；在mAP@0.5-0.9上达到了86.89 %，提高了23.41%。此外，检测速度提升了2.9%。", "conclusion": "所提出的ABCD方法在血细胞检测方面表现出更高的效率和准确性，并且由于检测速度的提升，非常适用于实时应用。", "translation": "在显微图像中检测血细胞已成为医学图像分析的重点，在深入了解患者健康状况方面发挥着关键作用。众所周知，用于疾病检测的手动血细胞检查耗时、低效且容易出错。为了解决这些局限性，使用基于深度学习的目标检测器分析血细胞被认为是一种可行的解决方案。在本研究中，我们提出了一种基于改进版YOLOX目标检测器的自动血细胞检测方法（ABCD），用于检测各种类型的血细胞，包括白细胞、红细胞和血小板。首先，我们将卷积块注意力模块（CBAM）引入网络骨干，以提高特征提取的效率。此外，我们在网络颈部引入了自适应空间特征融合（ASFF），它优化了从网络不同阶段提取的不同特征的融合。最后，为了加快模型的收敛速度，我们将交并比（IOU）损失函数替换为完全交并比（CIOU）损失函数。实验结果表明，所提出的方法在BCCD数据集上比其他现有方法更有效。与基线算法相比，我们的ABCD方法在mAP@0.5上达到了95.49 %，mAP@0.5-0.9上达到了86.89 %，分别提高了2.8%和23.41%，并且检测速度提高了2.9%，使其在实时应用中高效。", "summary": "本研究提出了一种名为ABCD的自动血细胞检测方法，旨在解决手动血细胞检查耗时且易出错的问题。该方法基于改进的YOLOX模型，通过引入卷积块注意力模块（CBAM）增强特征提取，引入自适应空间特征融合（ASFF）优化特征融合，并使用完全交并比（CIOU）损失函数加速模型收敛。实验结果表明，ABCD在BCCD数据集上显著优于现有方法，在mAP@0.5和mAP@0.5-0.9上分别提高了2.8%和23.41%，同时检测速度提升了2.9%，证明其在实时血细胞检测应用中的高效性和准确性。", "keywords": "血细胞检测, 深度学习, YOLOX, 注意力机制, 目标检测", "comments": "该论文提出了一种针对血细胞检测的深度学习方法，通过对YOLOX模型进行多方面改进，包括引入注意力机制（CBAM）、特征融合策略（ASFF）和改进的损失函数（CIOU），显著提升了检测性能和效率。其创新点在于将多种先进技术集成到现有框架中，并取得了可量化的性能提升，尤其是在实时性方面。这对于医疗图像分析领域具有重要意义，有助于推动自动化诊断和提高工作效率。"}}
{"id": "2507.18153", "title": "When Noisy Labels Meet Class Imbalance on Graphs: A Graph Augmentation Method with LLM and Pseudo Label", "authors": ["Riting Xia", "Rucong Wang", "Yulin Liu", "Anchen Li", "Xueyan Liu", "Yan Zhang"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18153v2", "summary": "Class-imbalanced graph node classification is a practical yet underexplored\nresearch problem. Although recent studies have attempted to address this issue,\nthey typically assume clean and reliable labels when processing\nclass-imbalanced graphs. This assumption often violates the nature of\nreal-world graphs, where labels frequently contain noise. Given this gap, this\npaper systematically investigates robust node classification for\nclass-imbalanced graphs with noisy labels. We propose GraphALP, a novel Graph\nAugmentation framework based on Large language models (LLMs) and\nPseudo-labeling techniques. Specifically, we design an LLM-based oversampling\nmethod to generate synthetic minority nodes, producing label-accurate minority\nnodes to alleviate class imbalance. Based on the class-balanced graphs, we\ndevelop a dynamically weighted pseudo-labeling method to obtain high-confidence\npseudo labels to reduce label noise ratio. Additionally, we implement a\nsecondary LLM-guided oversampling mechanism to mitigate potential class\ndistribution skew caused by pseudo labels. Experimental results show that\nGraphALP achieves superior performance over state-of-the-art methods on\nclass-imbalanced graphs with noisy labels.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18153v2", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-25", "AI": {"title_translation": "当噪声标签遇到图上的类别不平衡：一种结合LLM和伪标签的图增强方法", "tldr": "GraphALP是一种新的图增强框架，它结合大型语言模型（LLMs）和伪标签技术，用于解决图上带有噪声标签的类别不平衡节点分类问题，并取得了优异的性能。", "motivation": "现有的图节点分类方法在处理类别不平衡问题时，通常假设标签是干净可靠的，但这与现实世界图中标签常含有噪声的性质不符。本文旨在解决这一研究空白，即在噪声标签下进行类别不平衡图的鲁棒节点分类。", "method": "本文提出了GraphALP，一个基于大型语言模型（LLMs）和伪标签技术的图增强框架。具体方法包括：1. 设计了一个基于LLM的过采样方法来生成合成的少数类节点，以缓解类别不平衡。2. 在类别平衡的图上，开发了一个动态加权伪标签方法，以获得高置信度的伪标签，从而降低标签噪声比。3. 实施了一个二次LLM引导的过采样机制，以减轻伪标签可能引起的类别分布偏差。", "result": "实验结果表明，GraphALP在带有噪声标签的类别不平衡图上，比现有最先进的方法取得了更优越的性能。", "conclusion": "GraphALP通过结合LLM驱动的过采样和动态伪标签技术，有效解决了图上带有噪声标签的类别不平衡节点分类这一复杂问题，并显著提升了分类性能。", "translation": "类别不平衡的图节点分类是一个实际但尚未充分研究的问题。尽管最近的研究试图解决这个问题，但它们在处理类别不平衡图时通常假设标签是干净可靠的。这一假设常常违背了真实世界图的本质，其中标签经常包含噪声。鉴于这一空白，本文系统地研究了带有噪声标签的类别不平衡图的鲁棒节点分类。我们提出了GraphALP，一个基于大型语言模型（LLMs）和伪标签技术的新颖图增强框架。具体来说，我们设计了一种基于LLM的过采样方法来生成合成的少数类节点，从而产生标签准确的少数类节点以缓解类别不平衡。基于类别平衡的图，我们开发了一种动态加权伪标签方法来获得高置信度的伪标签，以降低标签噪声比。此外，我们实施了一个二次LLM引导的过采样机制，以减轻伪标签可能导致的潜在类别分布偏差。实验结果表明，GraphALP在带有噪声标签的类别不平衡图上，比最先进的方法取得了优越的性能。", "summary": "本文提出了一种名为GraphALP的新型图增强框架，旨在解决图上带有噪声标签的类别不平衡节点分类问题。GraphALP利用大型语言模型（LLMs）进行少数类节点过采样以平衡类别分布，并采用动态加权伪标签技术来减少标签噪声。此外，它还通过二次LLM引导的过采样来纠正伪标签可能引入的分布偏差。实验证明，GraphALP在处理此类复杂图数据方面优于现有先进方法。", "keywords": "图节点分类, 类别不平衡, 噪声标签, 大语言模型, 伪标签", "comments": "GraphALP的创新之处在于其将大型语言模型（LLMs）与图数据处理中的过采样和伪标签技术相结合，以同时解决图上类别不平衡和标签噪声两大挑战。这种多阶段的增强方法，特别是LLM在生成高质量合成数据和指导伪标签过程中的应用，为实际图数据分析提供了新的思路和强大的工具。该方法对于提升真实世界图中节点分类的鲁棒性和准确性具有重要意义。"}}
{"id": "2507.18641", "title": "Comparing Human and AI Performance in Visual Storytelling through Creation of Comic Strips: A Case Study", "authors": ["Uğur Önal", "Sanem Sariel", "Metin Sezgin", "Ergun Akleman"], "categories": ["cs.HC", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      This paper is accepted to be presented in Digital Humanities Conference 2025, and it will also appear in their proceedings", "url": "http://arxiv.org/abs/2507.18641v1", "summary": "This article presents a case study comparing the capabilities of humans and\nartificial intelligence (AI) for visual storytelling. We developed detailed\ninstructions to recreate a three-panel Nancy cartoon strip by Ernie Bushmiller\nand provided them to both humans and AI systems. The human participants were\n20-something students with basic artistic training but no experience or\nknowledge of this comic strip. The AI systems used were popular commercial\nmodels trained to draw and paint like artists, though their training sets may\nnot necessarily include Bushmiller's work. Results showed that AI systems excel\nat mimicking professional art but struggle to create coherent visual stories.\nIn contrast, humans proved highly adept at transforming instructions into\nmeaningful visual narratives.", "comment": "This paper is accepted to be presented in Digital Humanities\n  Conference 2025, and it will also appear in their proceedings", "pdf_url": "http://arxiv.org/pdf/2507.18641v1", "cate": "cs.HC", "date": "2025-05-27", "updated": "2025-05-27", "AI": {"title_translation": "通过创作漫画条比较人类和人工智能在视觉叙事中的表现：一项案例研究", "tldr": "本研究通过让参与者和AI系统根据详细指令重现一个三格漫画来比较人类和AI在视觉叙事方面的能力。结果显示，AI擅长模仿专业艺术但难以创作连贯的视觉故事，而人类则擅长将指令转化为有意义的视觉叙事。", "motivation": "本研究旨在比较人类和人工智能在视觉叙事方面的能力。", "method": "研究开发了详细的指令，让20多岁的有基本艺术训练但无漫画经验的学生以及流行的商业AI系统重现了厄尼·布什米勒的三格南希漫画。AI系统是经过训练可以像艺术家一样绘画的商业模型。", "result": "结果显示，AI系统擅长模仿专业艺术，但在创作连贯的视觉故事方面存在困难。相比之下，人类非常擅长将指令转化为有意义的视觉叙事。", "conclusion": "人类在将指令转化为有意义的视觉叙事方面比当前的人工智能系统更具优势，尽管AI在艺术模仿方面表现出色。", "translation": "本文介绍了一项案例研究，旨在比较人类和人工智能（AI）在视觉叙事方面的能力。我们制定了详细的指令，以重现厄尼·布什米勒创作的三格南希漫画，并将其提供给人类和AI系统。人类参与者是20多岁的学生，他们接受过基本的艺术训练，但没有该漫画的经验或知识。所使用的AI系统是流行的商业模型，它们经过训练可以像艺术家一样绘画，尽管它们的训练集不一定包含布什米勒的作品。结果显示，AI系统擅长模仿专业艺术，但难以创作连贯的视觉故事。相比之下，人类被证明非常擅长将指令转化为有意义的视觉叙事。", "summary": "本案例研究旨在比较人类与人工智能在视觉叙事中的表现。研究通过让20多岁的学生和商业AI系统根据详细指令重现一幅经典三格漫画来评估其能力。结果表明，AI擅长模仿艺术风格，但在构建连贯的视觉叙事方面表现不足，而人类则能有效地将指令转化为有意义的视觉故事。", "keywords": "视觉叙事, 人工智能, 漫画创作, 人机比较, 案例研究", "comments": "该研究通过一个具体的漫画创作任务，清晰地展示了当前AI在理解和生成复杂叙事逻辑方面的局限性，即使它们在艺术风格模仿上表现出色。这强调了人类在更高层次认知和创造性叙事方面的独特优势。"}}
{"id": "2507.10543", "title": "MP1: MeanFlow Tames Policy Learning in 1-step for Robotic Manipulation", "authors": ["Juyi Sheng", "Ziyi Wang", "Peiming Li", "Yong Liu", "Mengyuan Liu"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.10543v2", "summary": "In robot manipulation, robot learning has become a prevailing approach.\nHowever, generative models within this field face a fundamental trade-off\nbetween the slow, iterative sampling of diffusion models and the architectural\nconstraints of faster Flow-based methods, which often rely on explicit\nconsistency losses. To address these limitations, we introduce MP1, which pairs\n3D point-cloud inputs with the MeanFlow paradigm to generate action\ntrajectories in one network function evaluation (1-NFE). By directly learning\nthe interval-averaged velocity via the \"MeanFlow Identity\", our policy avoids\nany additional consistency constraints. This formulation eliminates numerical\nODE-solver errors during inference, yielding more precise trajectories. MP1\nfurther incorporates CFG for improved trajectory controllability while\nretaining 1-NFE inference without reintroducing structural constraints. Because\nsubtle scene-context variations are critical for robot learning, especially in\nfew-shot learning, we introduce a lightweight Dispersive Loss that repels state\nembeddings during training, boosting generalization without slowing inference.\nWe validate our method on the Adroit and Meta-World benchmarks, as well as in\nreal-world scenarios. Experimental results show MP1 achieves superior average\ntask success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its\naverage inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster\nthan FlowPolicy. Our code is available at https://github.com/LogSSim/MP1.git.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.10543v2", "cate": "cs.RO", "date": "2025-07-14", "updated": "2025-07-25", "AI": {"title_translation": "MP1：MeanFlow 在机器人操作中一步驯服策略学习", "tldr": "MP1 引入了一种基于 MeanFlow 的方法，通过直接学习区间平均速度，在一步内为机器人操作生成精确且可控的动作轨迹，显著提高了效率和成功率。", "motivation": "在机器人操作中，生成模型面临扩散模型采样慢和基于流的方法依赖显式一致性损失的结构限制之间的权衡。", "method": "MP1 将 3D 点云输入与 MeanFlow 范式结合，通过一次网络函数评估（1-NFE）生成动作轨迹。它通过“MeanFlow 恒等式”直接学习区间平均速度，避免了额外的一致性约束。MP1 还整合了 CFG 以提高轨迹可控性，并引入了轻量级的分散损失（Dispersive Loss）来排斥训练期间的状态嵌入，以增强泛化能力。", "result": "MP1 在 Adroit 和 Meta-World 基准测试以及真实世界场景中进行了验证。实验结果显示，MP1 实现了卓越的平均任务成功率，比 DP3 高 10.2%，比 FlowPolicy 高 7.3%。其平均推理时间仅为 6.8 毫秒，比 DP3 快 19 倍，比 FlowPolicy 快近 2 倍。", "conclusion": "MP1 通过引入 MeanFlow 范式和分散损失，有效解决了机器人操作中策略学习的效率和精度问题，实现了快速、精确且可控的动作轨迹生成，并在多个基准测试中展现出优越的性能。", "translation": "在机器人操作中，机器人学习已成为一种主流方法。然而，该领域内的生成模型面临着扩散模型缓慢的迭代采样与速度更快的基于流的方法的架构约束之间的基本权衡，后者通常依赖于显式一致性损失。为了解决这些限制，我们引入了 MP1，它将 3D 点云输入与 MeanFlow 范式配对，通过一次网络函数评估（1-NFE）生成动作轨迹。通过“MeanFlow 恒等式”直接学习区间平均速度，我们的策略避免了任何额外的一致性约束。这种公式消除了推理过程中的数值 ODE 求解器误差，从而产生更精确的轨迹。MP1 进一步结合了 CFG 以提高轨迹可控性，同时保留了 1-NFE 推理而无需重新引入结构约束。由于细微的场景上下文变化对于机器人学习至关重要，尤其是在少样本学习中，我们引入了一种轻量级的分散损失（Dispersive Loss），它在训练期间排斥状态嵌入，从而在不减慢推理速度的情况下提高泛化能力。我们在 Adroit 和 Meta-World 基准测试以及真实世界场景中验证了我们的方法。实验结果表明，MP1 实现了卓越的平均任务成功率，比 DP3 高 10.2%，比 FlowPolicy 高 7.3%。其平均推理时间仅为 6.8 毫秒——比 DP3 快 19 倍，比 FlowPolicy 快近 2 倍。我们的代码可在 https://github.com/LogSSim/MP1.git 获取。", "summary": "该论文提出了 MP1，一种针对机器人操作的新型策略学习方法。它结合 3D 点云输入与 MeanFlow 范式，通过一次网络评估生成动作轨迹，并通过直接学习区间平均速度来避免一致性约束和 ODE 求解器误差。MP1 还引入了 CFG 提高可控性以及分散损失增强泛化能力。实验证明，MP1 在任务成功率和推理速度上均显著优于现有方法。", "keywords": "机器人操作, 策略学习, MeanFlow, 1-NFE, 轨迹生成", "comments": "MP1 的创新之处在于将 MeanFlow 范式引入机器人操作的策略学习中，通过“MeanFlow 恒等式”实现了单步（1-NFE）轨迹生成，有效解决了传统生成模型效率低和结构约束多的问题。其引入的分散损失也为少样本学习中的泛化能力提升提供了新思路，对实时机器人控制具有重要意义。"}}
{"id": "2507.18793", "title": "Flexible Intelligent Metasurfaces in High-Mobility MIMO Integrated Sensing and Communications", "authors": ["Kuranage Roche Rayan Ranasinghe", "Jiancheng An", "Iván Alexander Morales Sandoval", "Hyeon Seok Rou", "Giuseppe Thadeu Freitas de Abreu", "Chau Yuen", "Mérouane Debbah"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      Submitted to an IEEE journal", "url": "http://arxiv.org/abs/2507.18793v1", "summary": "We propose a novel doubly-dispersive (DD) multiple-input multiple-output\n(MIMO) channel model incorporating flexible intelligent metasurfaces (FIMs),\nwhich is suitable for integrated sensing and communications (ISAC) in\nhigh-mobility scenarios. We then discuss how the proposed FIM-parameterized DD\n(FPDD) channel model can be applied in a logical manner to ISAC waveforms that\nare known to perform well in DD environments, namely, orthogonal frequency\ndivision multiplexing (OFDM), orthogonal time frequency space (OTFS), and\naffine frequency division multiplexing (AFDM). Leveraging the proposed model,\nwe formulate an achievable rate maximization problem with a strong sensing\nconstraint for all the aforementioned waveforms, which we then solve via a\ngradient ascent algorithm with closed-form gradients presented as a bonus. Our\nnumerical results indicate that the achievable rate is significantly impacted\nby the emerging FIM technology with careful parametrization essential in\nobtaining strong ISAC performance across all waveforms suitable to mitigating\nthe effects of DD channels.", "comment": "Submitted to an IEEE journal", "pdf_url": "http://arxiv.org/pdf/2507.18793v1", "cate": "eess.SP", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "高机动性MIMO集成感知与通信中的柔性智能超表面", "tldr": "本文提出了一种结合柔性智能超表面（FIM）的新型双分散（DD）MIMO信道模型，适用于高机动性场景下的集成感知与通信（ISAC），并通过优化FIM参数显著提高了可达速率。", "motivation": "在高机动性场景下，为集成感知与通信（ISAC）系统设计一个合适的信道模型，并探讨如何利用柔性智能超表面（FIMs）来提升其性能。", "method": "提出了一种结合柔性智能超表面（FIMs）的新型双分散（DD）多输入多输出（MIMO）信道模型，即FIM参数化DD（FPDD）信道模型。将此模型应用于OFDM、OTFS和AFDM等适用于DD环境的ISAC波形。在此模型基础上，针对所有波形，构建了一个具有强感知约束的可达速率最大化问题，并通过梯度上升算法求解，提供了闭式梯度。", "result": "数值结果表明，新兴的FIM技术显著影响可达速率，并且仔细的参数化对于在所有适用于缓解DD信道影响的波形中获得强大的ISAC性能至关重要。", "conclusion": "柔性智能超表面（FIM）技术及其参数化对于提升高机动性双分散信道下集成感知与通信（ISAC）系统的可达速率和整体性能至关重要。", "translation": "我们提出了一种结合柔性智能超表面（FIMs）的新型双分散（DD）多输入多输出（MIMO）信道模型，该模型适用于高机动性场景下的集成感知与通信（ISAC）。然后，我们讨论了所提出的FIM参数化DD（FPDD）信道模型如何逻辑地应用于已知在DD环境中表现良好的ISAC波形，即正交频分复用（OFDM）、正交时频空间（OTFS）和仿射频分复用（AFDM）。利用所提出的模型，我们针对所有上述波形，提出了一个具有强感知约束的可达速率最大化问题，并通过梯度上升算法求解，并额外提供了闭式梯度。我们的数值结果表明，新兴的FIM技术显著影响可达速率，并且仔细的参数化对于在所有适用于缓解DD信道影响的波形中获得强大的ISAC性能至关重要。", "summary": "本文提出了一种结合柔性智能超表面（FIMs）的双分散（DD）MIMO信道模型，以支持高机动性场景下的集成感知与通信（ISAC）。该模型适用于OFDM、OTFS和AFDM等多种ISAC波形。研究者通过构建并求解一个可达速率最大化问题，发现FIM技术对可达速率有显著影响，且通过精细的参数化，能够在DD信道环境下显著提升ISAC性能。", "keywords": "柔性智能超表面, 集成感知与通信, 双分散信道, MIMO, 可达速率最大化", "comments": "该论文的创新点在于提出了一个将柔性智能超表面（FIM）整合到双分散（DD）MIMO信道中的新型模型，并将其应用于高机动性ISAC场景。这为利用FIM技术优化复杂信道环境下的通信与感知性能提供了理论基础和优化方法，具有重要的实际意义和潜在应用价值。"}}
{"id": "2507.19081", "title": "Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement", "authors": ["Hao Li", "Yizheng Sun", "Viktor Schlegel", "Kailai Yang", "Riza Batista-Navarro", "Goran Nenadic"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.19081v1", "summary": "Argument summarization aims to generate concise, structured representations\nof complex, multi-perspective debates. While recent work has advanced the\nidentification and clustering of argumentative components, the generation stage\nremains underexplored. Existing approaches typically rely on single-pass\ngeneration, offering limited support for factual correction or structural\nrefinement. To address this gap, we introduce Arg-LLaDA, a novel large language\ndiffusion framework that iteratively improves summaries via sufficiency-guided\nremasking and regeneration. Our method combines a flexible masking controller\nwith a sufficiency-checking module to identify and revise unsupported,\nredundant, or incomplete spans, yielding more faithful, concise, and coherent\noutputs. Empirical results on two benchmark datasets demonstrate that Arg-LLaDA\nsurpasses state-of-the-art baselines in 7 out of 10 automatic evaluation\nmetrics. In addition, human evaluations reveal substantial improvements across\ncore dimensions, coverage, faithfulness, and conciseness, validating the\neffectiveness of our iterative, sufficiency-aware generation strategy.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.19081v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "Arg-LLaDA：基于大型语言扩散模型和充分性感知细化的论证摘要", "tldr": "Arg-LLaDA 提出了一种新颖的大型语言扩散框架，通过充分性引导的重新掩码和再生来迭代改进论证摘要，并在自动和人工评估中超越了现有技术水平。", "motivation": "现有的论证摘要方法在生成阶段探索不足，并且通常依赖于单次生成，对事实纠正或结构细化的支持有限。", "method": "我们引入了 Arg-LLaDA，一个新颖的大型语言扩散框架，通过充分性引导的重新掩码和再生来迭代改进摘要。该方法结合了一个灵活的掩码控制器和一个充分性检查模块，以识别和修改不受支持、冗余或不完整的跨度。", "result": "在两个基准数据集上的实证结果表明，Arg-LLaDA 在 10 个自动评估指标中的 7 个方面超越了最先进的基线。此外，人工评估显示在核心维度、覆盖范围、忠实度和简洁性方面有显著改进。", "conclusion": "我们的迭代、充分性感知生成策略的有效性得到了验证。", "translation": "论证摘要旨在生成复杂、多视角辩论的简洁、结构化表示。尽管最近的工作在论证组件的识别和聚类方面取得了进展，但生成阶段仍未得到充分探索。现有方法通常依赖于单次生成，对事实纠正或结构细化的支持有限。为了解决这一差距，我们引入了 Arg-LLaDA，一个新颖的大型语言扩散框架，通过充分性引导的重新掩码和再生来迭代改进摘要。我们的方法结合了一个灵活的掩码控制器和一个充分性检查模块，以识别和修改不受支持、冗余或不完整的跨度，从而产生更忠实、简洁和连贯的输出。在两个基准数据集上的实证结果表明，Arg-LLaDA 在 10 个自动评估指标中的 7 个方面超越了最先进的基线。此外，人工评估显示在核心维度、覆盖范围、忠实度和简洁性方面有显著改进，验证了我们迭代的、充分性感知生成策略的有效性。", "summary": "本文提出了 Arg-LLaDA，一个基于大型语言扩散模型的新型框架，用于论证摘要的生成。针对现有方法在生成阶段的不足和单次生成限制，Arg-LLaDA 采用迭代改进策略，通过充分性引导的重新掩码和再生来优化摘要。该框架整合了掩码控制器和充分性检查模块，以确保生成摘要的忠实性、简洁性和连贯性。实验结果表明，Arg-LLaDA 在自动和人工评估中均显著优于现有技术水平。", "keywords": "论证摘要, 大型语言扩散模型, 充分性感知, 迭代生成, 文本生成", "comments": "Arg-LLaDA 的创新之处在于其引入了迭代式的扩散框架和充分性感知细化机制，有效解决了论证摘要生成中事实纠正和结构优化的难题。这种迭代改进策略对于生成高质量、高忠实度的长文本摘要具有重要意义，为未来的摘要研究提供了新的方向。"}}
{"id": "2503.10410", "title": "RoCo-Sim: Enhancing Roadside Collaborative Perception through Foreground Simulation", "authors": ["Yuwen Du", "Anning Hu", "Zichen Chao", "Yifan Lu", "Junhao Ge", "Genjia Liu", "Weitao Wu", "Lanjun Wang", "Siheng Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.10410v2", "summary": "Roadside Collaborative Perception refers to a system where multiple roadside\nunits collaborate to pool their perceptual data, assisting vehicles in\nenhancing their environmental awareness. Existing roadside perception methods\nconcentrate on model design but overlook data issues like calibration errors,\nsparse information, and multi-view consistency, leading to poor performance on\nrecent published datasets. To significantly enhance roadside collaborative\nperception and address critical data issues, we present the first simulation\nframework RoCo-Sim for road-side collaborative perception. RoCo-Sim is capable\nof generating diverse, multi-view consistent simulated roadside data through\ndynamic foreground editing and full-scene style transfer of a single image.\nRoCo-Sim consists of four components: (1) Camera Extrinsic Optimization ensures\naccurate 3D to 2D projection for roadside cameras; (2) A novel Multi-View\nOcclusion-Aware Sampler (MOAS) determines the placement of diverse digital\nassets within 3D space; (3) DepthSAM innovatively models foreground-background\nrelationships from single-frame fixed-view images, ensuring multi-view\nconsistency of foreground; and (4) Scalable Post-Processing Toolkit generates\nmore realistic and enriched scenes through style transfer and other\nenhancements. RoCo-Sim significantly improves roadside 3D object detection,\noutperforming SOTA methods by 83.74 on Rcooper-Intersection and 83.12 on\nTUMTraf-V2X for AP70. RoCo-Sim fills a critical gap in roadside perception\nsimulation. Code and pre-trained models will be released soon:\nhttps://github.com/duyuwen-duen/RoCo-Sim", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.10410v2", "cate": "cs.CV", "date": "2025-03-13", "updated": "2025-07-25", "AI": {"title_translation": "RoCo-Sim：通过前景模拟增强路侧协同感知", "tldr": "RoCo-Sim是一个新的模拟框架，通过生成多样化、多视角一致的路侧数据来改善路侧协同感知，通过解决数据质量问题显著优于现有方法。", "motivation": "现有的路侧感知方法侧重于模型设计，但忽略了校准误差、信息稀疏和多视角一致性等数据问题，导致在最近发布的公共数据集上性能不佳。", "method": "RoCo-Sim是首个用于路侧协同感知的模拟框架。它通过动态前景编辑和单图像全场景风格迁移生成多样化、多视角一致的模拟路侧数据。RoCo-Sim包含四个组件：(1) 摄像机外参优化确保路侧摄像机的准确3D到2D投影；(2) 新颖的多视角遮挡感知采样器（MOAS）确定不同数字资产在3D空间中的放置；(3) DepthSAM创新性地从单帧固定视角图像中建模前景-背景关系，确保前景的多视角一致性；(4) 可扩展的后处理工具包通过风格迁移和其他增强生成更真实、更丰富的场景。", "result": "RoCo-Sim显著改善了路侧3D目标检测性能，在Rcooper-Intersection数据集上AP70超越SOTA方法83.74，在TUMTraf-V2X数据集上超越83.12。", "conclusion": "RoCo-Sim填补了路侧感知模拟的关键空白，通过解决数据质量问题并显著增强协同感知性能。", "translation": "路侧协同感知是指多个路侧单元协作汇集其感知数据，以帮助车辆增强环境感知能力的系统。现有的路侧感知方法侧重于模型设计，但忽视了校准误差、信息稀疏和多视角一致性等数据问题，导致在最近发布的公共数据集上性能不佳。为了显著增强路侧协同感知并解决关键数据问题，我们提出了首个用于路侧协同感知的模拟框架RoCo-Sim。RoCo-Sim能够通过动态前景编辑和单图像全场景风格迁移生成多样化、多视角一致的模拟路侧数据。RoCo-Sim由四个组件组成：(1) 摄像机外参优化确保路侧摄像机的准确3D到2D投影；(2) 新颖的多视角遮挡感知采样器（MOAS）确定不同数字资产在3D空间中的放置；(3) DepthSAM创新性地从单帧固定视角图像中建模前景-背景关系，确保前景的多视角一致性；(4) 可扩展的后处理工具包通过风格迁移和其他增强生成更真实、更丰富的场景。RoCo-Sim显著改善了路侧3D目标检测性能，在Rcooper-Intersection数据集上AP70超越SOTA方法83.74，在TUMTraf-V2X数据集上超越83.12。RoCo-Sim填补了路侧感知模拟的关键空白。代码和预训练模型将很快发布：https://github.com/duyuwen-duen/RoCo-Sim", "summary": "本文介绍了RoCo-Sim，一个新颖的模拟框架，旨在通过解决校准误差和多视角一致性等常见数据问题来增强路侧协同感知。RoCo-Sim利用动态前景编辑和风格迁移生成多样化、多视角一致的模拟数据。该框架由四个关键模块组成：摄像机外参优化、多视角遮挡感知采样器（MOAS）、DepthSAM和可扩展后处理工具包。RoCo-Sim显著提升了路侧3D目标检测性能，在Rcooper-Intersection和TUMTraf-V2X等基准数据集上取得了显著的性能提升。", "keywords": "路侧协同感知, 模拟框架, 数据生成, 3D目标检测, 多视角一致性", "comments": "RoCo-Sim的创新之处在于它是专门为路侧协同感知设计的“首个模拟框架”，直接解决了现有以模型为中心的方法所忽视的关键数据质量问题（校准、稀疏性、多视角一致性）。其模块化设计，特别是新颖的MOAS和DepthSAM，似乎能够稳健地生成真实且一致的多视角数据。这项工作非常重要，因为高质量的模拟数据可以显著降低自动驾驶应用中真实世界数据收集和标注的成本和复杂性。"}}
{"id": "2507.19206", "title": "Implementing Credit Risk Analysis with Quantum Singular Value Transformation", "authors": ["Davide Veronelli", "Francesca Cibrario", "Emanuele Dri", "Valeria Zaffaroni", "Giacomo Ranieri", "Davide Corbelletto", "Bartolomeo Montrucchio"], "categories": ["quant-ph", "cs.ET", "q-fin.RM"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      10 pages, to be published in the proceedings of the IEEE International Conference on Quantum Computing and Engineering - QCE25", "url": "http://arxiv.org/abs/2507.19206v1", "summary": "The analysis of credit risk is crucial for the efficient operation of\nfinancial institutions. Quantum Amplitude Estimation (QAE) offers the potential\nfor a quadratic speed-up over classical methods used to estimate metrics such\nas Value at Risk (VaR) and Conditional Value at Risk (CVaR). However, numerous\nlimitations remain in efficiently scaling the implementation of quantum\ncircuits that solve these estimation problems. One of the main challenges is\nthe use of costly and restrictive arithmetic that must be implemented within\nthe quantum circuit. In this paper, we propose using Quantum Singular Value\nTransformation (QSVT) to significantly reduce the cost of implementing the\nstate preparation operator, which underlies QAE for credit risk analysis. We\nalso present an end-to-end code implementation and the results of a simulation\nstudy to validate the proposed approach and demonstrate its benefits.", "comment": "10 pages, to be published in the proceedings of the IEEE\n  International Conference on Quantum Computing and Engineering - QCE25", "pdf_url": "http://arxiv.org/pdf/2507.19206v1", "cate": "quant-ph", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "使用量子奇异值变换实现信用风险分析", "tldr": "本文提出使用量子奇异值变换（QSVT）来降低信用风险分析中量子振幅估计算法（QAE）的实现成本，并通过仿真验证了其有效性。", "motivation": "信用风险分析对金融机构的有效运作至关重要。量子振幅估计（QAE）在估计风险指标（如VaR和CVaR）方面具有二次加速潜力，但其量子电路实现存在扩展性限制，主要挑战是电路中昂贵且受限的算术运算。", "method": "提出使用量子奇异值变换（QSVT）来显著降低信用风险分析中量子振幅估计（QAE）基础的状态准备算子的实现成本。", "result": "提供了端到端的代码实现和仿真研究结果，以验证所提出方法的有效性并展示其优势。", "conclusion": "量子奇异值变换（QSVT）能够显著降低信用风险分析中量子振幅估计（QAE）的状态准备算子的实现成本，从而克服了现有量子电路实现中的主要限制。", "translation": "信用风险分析对于金融机构的有效运作至关重要。量子振幅估计（QAE）在估计诸如风险价值（VaR）和条件风险价值（CVaR）等指标方面，相较于经典方法具有二次加速的潜力。然而，在有效扩展解决这些估计问题的量子电路实现方面，仍然存在诸多限制。其中一个主要挑战是必须在量子电路中实现的昂贵且受限的算术运算。在本文中，我们提出使用量子奇异值变换（QSVT）来显著降低状态准备算子的实现成本，该算子是信用风险分析中量子振幅估计（QAE）的基础。我们还提供了一个端到端的代码实现和仿真研究结果，以验证所提出的方法并展示其优势。", "summary": "本文针对金融机构信用风险分析中量子振幅估计（QAE）存在的量子电路实现成本高昂和扩展性受限问题，提出了一种基于量子奇异值变换（QSVT）的新方法。该方法旨在显著降低QAE中状态准备算子的实现成本。研究通过端到端代码实现和仿真研究验证了所提出方法的有效性及其带来的益处。", "keywords": "信用风险分析, 量子奇异值变换, 量子振幅估计, 状态准备, 量子金融", "comments": "本文的创新点在于将量子奇异值变换（QSVT）应用于信用风险分析中的量子振幅估计（QAE），以解决现有量子电路实现中昂贵算术运算的限制。通过降低状态准备算子的成本，该方法有望推动量子计算在金融风险管理领域的实际应用。其重要性在于为实现QAE的二次加速潜力提供了更具可行性的路径。"}}
{"id": "2507.19085", "title": "Clustering-Oriented Generative Attribute Graph Imputation", "authors": ["Mulin Chen", "Bocheng Wang", "Jiaxin Zhong", "Zongcheng Miao", "Xuelong Li"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM'25", "url": "http://arxiv.org/abs/2507.19085v1", "summary": "Attribute-missing graph clustering has emerged as a significant unsupervised\ntask, where only attribute vectors of partial nodes are available and the graph\nstructure is intact. The related models generally follow the two-step paradigm\nof imputation and refinement. However, most imputation approaches fail to\ncapture class-relevant semantic information, leading to sub-optimal imputation\nfor clustering. Moreover, existing refinement strategies optimize the learned\nembedding through graph reconstruction, while neglecting the fact that some\nattributes are uncorrelated with the graph. To remedy the problems, we\nestablish the Clustering-oriented Generative Imputation with reliable\nRefinement (CGIR) model. Concretely, the subcluster distributions are estimated\nto reveal the class-specific characteristics precisely, and constrain the\nsampling space of the generative adversarial module, such that the imputation\nnodes are impelled to align with the correct clusters. Afterwards, multiple\nsubclusters are merged to guide the proposed edge attention network, which\nidentifies the edge-wise attributes for each class, so as to avoid the\nredundant attributes in graph reconstruction from disturbing the refinement of\noverall embedding. To sum up, CGIR splits attribute-missing graph clustering\ninto the search and mergence of subclusters, which guides to implement node\nimputation and refinement within a unified framework. Extensive experiments\nprove the advantages of CGIR over state-of-the-art competitors.", "comment": "Accepted by ACM MM'25", "pdf_url": "http://arxiv.org/pdf/2507.19085v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "面向聚类的生成式属性图补全", "tldr": "本文提出CGIR模型，通过估计子簇分布和边缘注意力网络，在统一框架下解决属性缺失图聚类中的不准确补全和冗余属性干扰问题，实验证明优于现有方法。", "motivation": "现有的属性缺失图聚类模型在补全阶段未能捕获类相关语义信息，导致次优的补全效果；在细化阶段通过图重建优化嵌入，却忽略了某些属性与图不相关，导致冗余属性干扰整体嵌入的细化。", "method": "本文提出了面向聚类的生成式补全与可靠细化（CGIR）模型。具体而言，CGIR首先估计子簇分布以揭示类特定特征，并约束生成对抗模块的采样空间，使补全节点与正确簇对齐。然后，合并多个子簇以指导所提出的边缘注意力网络，该网络识别每个类别的边缘属性，从而避免图重建中的冗余属性干扰整体嵌入的细化。CGIR将属性缺失图聚类分解为子簇的搜索和合并，从而在统一框架内实现节点补全和细化。", "result": "大量的实验证明CGIR模型优于最先进的竞争方法。", "conclusion": "CGIR模型通过将属性缺失图聚类分解为子簇的搜索和合并，并在统一框架下指导节点补全和细化，有效解决了现有方法的缺陷，实现了更好的性能。", "translation": "属性缺失图聚类已成为一项重要的无监督任务，其中只有部分节点的属性向量可用，而图结构完整。相关模型通常遵循补全和细化的两步范式。然而，大多数补全方法未能捕获类相关语义信息，导致聚类补全效果不佳。此外，现有细化策略通过图重建优化学习到的嵌入，却忽略了一些属性与图不相关的事实。为了弥补这些问题，我们建立了面向聚类的生成式补全与可靠细化（CGIR）模型。具体而言，估计子簇分布以精确揭示类特定特征，并约束生成对抗模块的采样空间，从而促使补全节点与正确簇对齐。之后，合并多个子簇以指导所提出的边缘注意力网络，该网络识别每个类别的边缘属性，从而避免图重建中冗余属性干扰整体嵌入的细化。总而言之，CGIR将属性缺失图聚类分解为子簇的搜索和合并，从而在统一框架内指导节点补全和细化。大量的实验证明CGIR优于最先进的竞争方法。", "summary": "本文针对属性缺失图聚类中现有方法补全不准确和冗余属性干扰的问题，提出了CGIR模型。该模型通过估计子簇分布来捕捉类特定语义信息，约束生成式模块进行精确补全，并通过边缘注意力网络识别类别的边缘属性，避免冗余属性的干扰。CGIR将聚类过程整合为子簇的搜索与合并，实现了节点补全和细化在统一框架内的协同优化，实验证明其性能优于现有先进方法。", "keywords": "属性缺失图聚类, 生成式补全, 子簇分布, 边缘注意力网络, CGIR", "comments": "CGIR的创新点在于将子簇分布估计引入生成对抗模块以实现更精确的类相关属性补全，并通过边缘注意力网络识别和避免冗余属性的干扰，这种将补全与细化统一在子簇搜索和合并框架下的方法具有较高的创新性。"}}
{"id": "2507.19155", "title": "RegScore: Scoring Systems for Regression Tasks", "authors": ["Michal K. Grzeszczyk", "Tomasz Szczepański", "Pawel Renc", "Siyeop Yoon", "Jerome Charton", "Tomasz Trzciński", "Arkadiusz Sitek"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      Accepted for the 28th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2025", "url": "http://arxiv.org/abs/2507.19155v1", "summary": "Scoring systems are widely adopted in medical applications for their inherent\nsimplicity and transparency, particularly for classification tasks involving\ntabular data. In this work, we introduce RegScore, a novel, sparse, and\ninterpretable scoring system specifically designed for regression tasks. Unlike\nconventional scoring systems constrained to integer-valued coefficients,\nRegScore leverages beam search and k-sparse ridge regression to relax these\nrestrictions, thus enhancing predictive performance. We extend RegScore to\nbimodal deep learning by integrating tabular data with medical images. We\nutilize the classification token from the TIP (Tabular Image Pretraining)\ntransformer to generate Personalized Linear Regression parameters and a\nPersonalized RegScore, enabling individualized scoring. We demonstrate the\neffectiveness of RegScore by estimating mean Pulmonary Artery Pressure using\ntabular data and further refine these estimates by incorporating cardiac MRI\nimages. Experimental results show that RegScore and its personalized bimodal\nextensions achieve performance comparable to, or better than, state-of-the-art\nblack-box models. Our method provides a transparent and interpretable approach\nfor regression tasks in clinical settings, promoting more informed and\ntrustworthy decision-making. We provide our code at\nhttps://github.com/SanoScience/RegScore.", "comment": "Accepted for the 28th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI) 2025", "pdf_url": "http://arxiv.org/pdf/2507.19155v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "RegScore：用于回归任务的评分系统", "tldr": "RegScore是一种新型的、可解释的、用于回归任务的评分系统，它通过结合束搜索和k稀疏岭回归，并扩展到多模态深度学习，实现了与最先进的黑盒模型相当或更好的性能，同时保持了透明度。", "motivation": "传统的评分系统在医疗应用中因其简单性和透明度而被广泛采用，尤其适用于分类任务。然而，这些系统通常受限于整数系数，并且主要针对分类任务，缺乏适用于回归任务的、同样简单透明的解决方案。", "method": "本文提出了一种名为RegScore的新型、稀疏且可解释的评分系统，专门用于回归任务。RegScore利用束搜索（beam search）和k稀疏岭回归（k-sparse ridge regression）来放宽传统评分系统对整数系数的限制，从而提高预测性能。此外，RegScore还扩展到双模态深度学习，通过整合表格数据和医学图像。它使用TIP（Tabular Image Pretraining）transformer的分类令牌来生成个性化线性回归参数和个性化RegScore，实现个体化评分。", "result": "RegScore通过估计平均肺动脉压（Mean Pulmonary Artery Pressure）来证明其有效性，首先使用表格数据，然后通过结合心脏MRI图像进一步优化估计。实验结果表明，RegScore及其个性化双模态扩展的性能与最先进的黑盒模型相当或更优。", "conclusion": "RegScore为临床环境中的回归任务提供了一种透明且可解释的方法，有助于做出更明智和值得信赖的决策。", "translation": "评分系统因其固有的简单性和透明性而在医疗应用中被广泛采用，尤其适用于涉及表格数据的分类任务。在这项工作中，我们引入了RegScore，一种新型的、稀疏且可解释的评分系统，专门为回归任务设计。与受限于整数系数的传统评分系统不同，RegScore利用束搜索和k稀疏岭回归来放宽这些限制，从而提高预测性能。我们将RegScore扩展到双模态深度学习，通过整合表格数据和医学图像。我们利用TIP（表格图像预训练）transformer的分类令牌来生成个性化线性回归参数和个性化RegScore，实现个体化评分。我们通过使用表格数据估计平均肺动脉压来证明RegScore的有效性，并通过整合心脏MRI图像进一步细化这些估计。实验结果表明，RegScore及其个性化双模态扩展的性能与最先进的黑盒模型相当或更优。我们的方法为临床环境中的回归任务提供了一种透明和可解释的方法，促进了更明智和值得信赖的决策。我们提供了代码：https://github.com/SanoScience/RegScore。", "summary": "RegScore是一种新颖的、可解释的评分系统，专为回归任务设计。它克服了传统评分系统整数系数的限制，通过结合束搜索和k稀疏岭回归提高预测性能。该系统还扩展到双模态深度学习，利用TIP transformer整合表格数据和医学图像，实现个性化评分。实验证明，RegScore在估计平均肺动脉压等任务上表现出色，性能与最先进的黑盒模型相当或更优，为临床决策提供了透明且可信赖的工具。", "keywords": "回归任务, 评分系统, 可解释性, 双模态深度学习, 医疗应用", "comments": "RegScore的创新之处在于其将传统评分系统的可解释性优势扩展到回归任务，并结合了现代机器学习技术（如束搜索、k稀疏岭回归和多模态深度学习）。它解决了医疗领域对透明度高、同时预测准确的模型的需求，尤其是在处理复杂的多源数据时。其个性化评分和与黑盒模型媲美的性能是重要的亮点，有望在临床实践中推广。"}}
{"id": "2507.15742", "title": "A Fisher's exact test justification of the TF-IDF term-weighting scheme", "authors": ["Paul Sheridan", "Zeyad Ahmed", "Aitazaz A. Farooque"], "categories": ["cs.CL", "cs.IR", "math.ST", "stat.TH"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      23 pages, 4 tables, accepted in The American Statistician 2025", "url": "http://arxiv.org/abs/2507.15742v2", "summary": "Term frequency-inverse document frequency, or TF-IDF for short, is arguably\nthe most celebrated mathematical expression in the history of information\nretrieval. Conceived as a simple heuristic quantifying the extent to which a\ngiven term's occurrences are concentrated in any one given document out of\nmany, TF-IDF and its many variants are routinely used as term-weighting schemes\nin diverse text analysis applications. There is a growing body of scholarship\ndedicated to placing TF-IDF on a sound theoretical foundation. Building on that\ntradition, this paper justifies the use of TF-IDF to the statistics community\nby demonstrating how the famed expression can be understood from a significance\ntesting perspective. We show that the common TF-IDF variant TF-ICF is, under\nmild regularity conditions, closely related to the negative logarithm of the\n$p$-value from a one-tailed version of Fisher's exact test of statistical\nsignificance. As a corollary, we establish a connection between TF-IDF and the\nsaid negative log-transformed $p$-value under certain idealized assumptions. We\nfurther demonstrate, as a limiting case, that this same quantity converges to\nTF-IDF in the limit of an infinitely large document collection. The Fisher's\nexact test justification of TF-IDF equips the working statistician with a ready\nexplanation of the term-weighting scheme's long-established effectiveness.", "comment": "23 pages, 4 tables, accepted in The American Statistician 2025", "pdf_url": "http://arxiv.org/pdf/2507.15742v2", "cate": "cs.CL", "date": "2025-07-21", "updated": "2025-07-24", "AI": {"title_translation": "TF-IDF词项加权方案的费舍尔精确检验合理性证明", "tldr": "本文通过展示TF-IDF如何从显著性检验的角度被理解，从而为TF-IDF词项加权方案提供了一个基于费舍尔精确检验的统计学理论基础。", "motivation": "TF-IDF是信息检索领域最著名的数学表达式之一，但其理论基础尚不完善。现有研究致力于为TF-IDF奠定坚实的理论基础，本文在此基础上，旨在从统计学角度，特别是通过显著性检验的视角，为TF-IDF的有效性提供合理性证明，以使其被统计学界所理解和接受。", "method": "研究表明，在温和的正则条件下，常见的TF-IDF变体TF-ICF与单尾费舍尔精确检验的p值的负对数密切相关。作为推论，在某些理想化假设下，TF-IDF与上述负对数转换的p值建立了联系。此外，作为一种极限情况，当文档集合无限大时，这个量收敛于TF-IDF。", "result": "TF-ICF（一种TF-IDF变体）在温和正则条件下与单尾费舍尔精确检验的p值的负对数密切相关。在特定理想化假设下，TF-IDF与该负对数转换的p值之间存在联系。在无限大文档集合的极限情况下，该量收敛于TF-IDF。", "conclusion": "基于费舍尔精确检验的TF-IDF合理性证明，为统计学家提供了一个现成的解释，阐明了该词项加权方案长期以来被证实有效的原因。", "translation": "词频-逆文档频率，简称TF-IDF，可以说是信息检索史上最著名的数学表达式。TF-IDF及其众多变体被认为是量化给定词项在众多文档中集中出现在某一个文档中的程度的简单启发式方法，并被常规用作各种文本分析应用中的词项加权方案。越来越多的学术研究致力于为TF-IDF奠定坚实的理论基础。本文在此传统之上，通过展示这个著名的表达式如何从显著性检验的角度被理解，从而向统计学界证明了TF-IDF的使用是合理的。我们表明，常见的TF-IDF变体TF-ICF，在温和的正则条件下，与单尾费舍尔精确检验的p值的负对数密切相关。作为推论，我们在某些理想化假设下建立了TF-IDF与上述负对数转换的p值之间的联系。我们进一步证明，作为一种极限情况，当文档集合无限大时，这个量收敛于TF-IDF。TF-IDF的费舍尔精确检验合理性证明为在职统计学家提供了一个现成的解释，阐明了该词项加权方案长期以来被证实有效的原因。", "summary": "本文旨在为信息检索领域广泛使用的TF-IDF词项加权方案提供一个坚实的统计学理论基础。通过将TF-IDF与费舍尔精确检验的p值联系起来，研究表明TF-ICF（一种TF-IDF变体）在温和条件下与单尾费舍尔精确检验的p值的负对数密切相关。进一步，在理想化假设和无限文档集合的极限情况下，TF-IDF与该统计量建立了联系。这项工作为统计学家理解TF-IDF的有效性提供了理论依据。", "keywords": "TF-IDF, 费舍尔精确检验, 词项加权, 显著性检验, 理论基础", "comments": "本文的创新之处在于它为TF-IDF这一经验性有效的词项加权方案提供了一个严谨的统计学理论基础，特别是通过将其与费舍尔精确检验的p值建立联系。这不仅增强了TF-IDF在统计学界的接受度，也为理解其长期有效性提供了新的视角。这项工作对于信息检索和文本分析领域具有重要意义，因为它将一个广泛应用但缺乏严格理论支撑的方法置于更坚实的科学框架内。"}}
{"id": "2507.18013", "title": "Technical Report of TeleChat2, TeleChat2.5 and T1", "authors": ["Zihan Wang", "Xinzhang Liu", "Yitong Yao", "Chao Wang", "Yu Zhao", "Zhihao Yang", "Wenmin Deng", "Kaipeng Jia", "Jiaxin Peng", "Yuyao Huang", "Sishi Xiong", "Zhuo Jiang", "Kaidong Yu", "Xiaohui Hu", "Fubei Yao", "Ruiyu Fang", "Zhuoru Jiang", "Ruiting Song", "Qiyi Xie", "Rui Xue", "Xuewei He", "Yanlei Xue", "Zhu Yuan", "Zhaoxi Zhang", "Zilu Huang", "Shiquan Wang", "Xin Wang", "Hanming Wu", "Mingyuan Wang", "Xufeng Zhan", "Yuhan Sun", "Zhaohu Xing", "Yuhao Jiang", "Bingkai Yang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "categories": ["cs.CL", "I.2.7"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      32 pages, 5 figures", "url": "http://arxiv.org/abs/2507.18013v2", "summary": "We introduce the latest series of TeleChat models: \\textbf{TeleChat2},\n\\textbf{TeleChat2.5}, and \\textbf{T1}, offering a significant upgrade over\ntheir predecessor, TeleChat. Despite minimal changes to the model architecture,\nthe new series achieves substantial performance gains through enhanced training\nstrategies in both pre-training and post-training stages. The series begins\nwith \\textbf{TeleChat2}, which undergoes pretraining on 10 trillion\nhigh-quality and diverse tokens. This is followed by Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO) to further enhance its\ncapabilities. \\textbf{TeleChat2.5} and \\textbf{T1} expand the pipeline by\nincorporating a continual pretraining phase with domain-specific datasets,\ncombined with reinforcement learning (RL) to improve performance in code\ngeneration and mathematical reasoning tasks. The \\textbf{T1} variant is\ndesigned for complex reasoning, supporting long Chain-of-Thought (CoT)\nreasoning and demonstrating substantial improvements in mathematics and coding.\nIn contrast, \\textbf{TeleChat2.5} prioritizes speed, delivering rapid\ninference. Both flagship models of \\textbf{T1} and \\textbf{TeleChat2.5} are\ndense Transformer-based architectures with 115B parameters, showcasing\nsignificant advancements in reasoning and general task performance compared to\nthe original TeleChat. Notably, \\textbf{T1-115B} outperform proprietary models\nsuch as OpenAI's o1-mini and GPT-4o. We publicly release \\textbf{TeleChat2},\n\\textbf{TeleChat2.5} and \\textbf{T1}, including post-trained versions with 35B\nand 115B parameters, to empower developers and researchers with\nstate-of-the-art language models tailored for diverse applications.", "comment": "32 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.18013v2", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-25", "AI": {"title_translation": "TeleChat2、TeleChat2.5和T1的技术报告", "tldr": "本文介绍了TeleChat模型的最新系列：TeleChat2、TeleChat2.5和T1。这些模型在架构变化极小的情况下，通过增强的预训练和后训练策略实现了显著的性能提升。它们在推理、代码生成和数学任务中表现出色，其中T1-115B甚至超越了某些专有模型，并且已公开发布。", "motivation": "旨在通过增强训练策略，显著升级前代TeleChat模型，实现性能上的巨大飞跃，并提供最先进的语言模型以支持各种应用，赋能开发者和研究人员。", "method": "该系列模型（TeleChat2、TeleChat2.5、T1）是基于密集Transformer架构的。TeleChat2在10万亿高质量和多样化Token上进行预训练，随后进行监督微调（SFT）和直接偏好优化（DPO）。TeleChat2.5和T1通过整合领域特定数据集的持续预训练阶段，并结合强化学习（RL）来扩展流水线，以提高在代码生成和数学推理任务中的性能。T1变体专为复杂推理设计，支持长链式思维（CoT）推理，而TeleChat2.5则优先考虑速度。旗舰模型T1和TeleChat2.5都是具有115B参数的密集Transformer架构。", "result": "新系列模型在架构变化极小的情况下实现了显著的性能提升。T1在数学和编码方面表现出显著改进，并支持长链式思维（CoT）推理。T1-115B在性能上超越了OpenAI的o1-mini和GPT-4o等专有模型。TeleChat2.5提供了快速推理。T1和TeleChat2.5在推理和通用任务性能方面均取得了显著进展。", "conclusion": "TeleChat2、TeleChat2.5和T1模型系列代表了对其前身的重大升级，通过先进的训练策略在各种任务（包括复杂推理、代码生成和数学推理）中实现了最先进的性能，并已公开发布，以促进进一步的研究和开发。", "translation": "我们推出了TeleChat模型的最新系列：\\textbf{TeleChat2}、\\textbf{TeleChat2.5}和\\textbf{T1}，它们比其前身TeleChat有了显著升级。尽管模型架构变化极小，但新系列通过在预训练和后训练阶段增强训练策略，实现了实质性的性能提升。该系列始于\\textbf{TeleChat2}，它在10万亿高质量和多样化Token上进行预训练。随后进行监督微调（SFT）和直接偏好优化（DPO），以进一步增强其能力。\\textbf{TeleChat2.5}和\\textbf{T1}通过整合领域特定数据集的持续预训练阶段，并结合强化学习（RL）来扩展流水线，以提高在代码生成和数学推理任务中的性能。\\textbf{T1}变体专为复杂推理设计，支持长链式思维（CoT）推理，并在数学和编码方面表现出实质性改进。相比之下，\\textbf{TeleChat2.5}优先考虑速度，提供快速推理。\\textbf{T1}和\\textbf{TeleChat2.5}这两款旗舰模型都是具有115B参数的密集Transformer架构，与原始TeleChat相比，在推理和通用任务性能方面展现出显著进步。值得注意的是，\\textbf{T1-115B}超越了OpenAI的o1-mini和GPT-4o等专有模型。我们公开发布了\\textbf{TeleChat2}、\\textbf{TeleChat2.5}和\\textbf{T1}，包括35B和115B参数的后训练版本，旨在为开发者和研究人员提供适用于各种应用的最先进语言模型。", "summary": "本文介绍了TeleChat2、TeleChat2.5和T1，这是TeleChat模型系列的最新升级。尽管模型架构变化不大，但通过改进的预训练和后训练策略（包括SFT、DPO、领域特定数据集的持续预训练和强化学习），这些模型实现了显著的性能提升。T1在复杂推理、数学和编码方面表现卓越，甚至超越了某些专有模型，而TeleChat2.5则侧重于快速推理。这些模型（包括35B和115B参数版本）已公开发布，旨在为多样化应用提供最先进的语言模型。", "keywords": "TeleChat, 大型语言模型, 训练策略, 推理, 代码生成", "comments": "本文的创新之处在于，在模型架构仅有微小变化的情况下，通过优化训练策略（特别是结合持续预训练和强化学习）实现了显著的性能提升。T1-115B模型能够超越GPT-4o等领先的专有模型，这一点尤其重要，它为开源社区提供了强大的竞争力，有助于推动大型语言模型研究的民主化。此外，该系列模型针对不同应用场景（如复杂推理与快速推理）进行优化，展现了其在实际部署中的灵活性和实用性。"}}
{"id": "2507.18647", "title": "XAI-Guided Analysis of Residual Networks for Interpretable Pneumonia Detection in Paediatric Chest X-rays", "authors": ["Rayyan Ridwan"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      13 pages, 14 figures", "url": "http://arxiv.org/abs/2507.18647v1", "summary": "Pneumonia remains one of the leading causes of death among children\nworldwide, underscoring a critical need for fast and accurate diagnostic tools.\nIn this paper, we propose an interpretable deep learning model on Residual\nNetworks (ResNets) for automatically diagnosing paediatric pneumonia on chest\nX-rays. We enhance interpretability through Bayesian Gradient-weighted Class\nActivation Mapping (BayesGrad-CAM), which quantifies uncertainty in visual\nexplanations, and which offers spatial locations accountable for the\ndecision-making process of the model. Our ResNet-50 model, trained on a large\npaediatric chest X-rays dataset, achieves high classification accuracy\n(95.94%), AUC-ROC (98.91%), and Cohen's Kappa (0.913), accompanied by\nclinically meaningful visual explanations. Our findings demonstrate that high\nperformance and interpretability are not only achievable but critical for\nclinical AI deployment.", "comment": "13 pages, 14 figures", "pdf_url": "http://arxiv.org/pdf/2507.18647v1", "cate": "eess.IV", "date": "2025-07-18", "updated": "2025-07-18", "AI": {"title_translation": "XAI引导的残差网络分析，用于儿科胸部X射线中可解释的肺炎检测", "tldr": "开发了一个基于ResNet和BayesGrad-CAM的可解释深度学习模型，用于儿科肺炎的自动诊断，实现了高准确性和临床意义的可解释性。", "motivation": "肺炎仍然是全球儿童死亡的主要原因之一，因此迫切需要快速准确的诊断工具。", "method": "本文提出了一个基于残差网络（ResNets）的可解释深度学习模型，用于自动诊断儿科胸部X射线上的肺炎。通过贝叶斯梯度加权类激活映射（BayesGrad-CAM）增强可解释性，该方法量化了视觉解释中的不确定性，并提供了模型决策过程的空间位置。", "result": "ResNet-50模型在大型儿科胸部X射线数据集上实现了95.94%的分类准确率、98.91%的AUC-ROC和0.913的Cohen's Kappa，并伴有具有临床意义的视觉解释。", "conclusion": "高性能和可解释性不仅可以实现，而且对于临床AI部署至关重要。", "translation": "肺炎仍然是全球儿童死亡的主要原因之一，这突显了对快速准确诊断工具的迫切需求。在本文中，我们提出了一个基于残差网络（ResNets）的可解释深度学习模型，用于自动诊断儿科胸部X射线上的肺炎。我们通过贝叶斯梯度加权类激活映射（BayesGrad-CAM）增强了可解释性，该方法量化了视觉解释中的不确定性，并提供了模型决策过程的空间位置。我们的ResNet-50模型在一个大型儿科胸部X射线数据集上进行训练，实现了高分类准确率（95.94%）、AUC-ROC（98.91%）和Cohen's Kappa（0.913），并伴有具有临床意义的视觉解释。我们的研究结果表明，高性能和可解释性不仅可以实现，而且对于临床AI部署至关重要。", "summary": "本文提出了一个基于ResNet的可解释深度学习模型，用于儿科胸部X射线肺炎的自动诊断。该模型利用BayesGrad-CAM增强了可解释性，能够量化视觉解释的不确定性并指示决策关键区域。在大型数据集上，该模型展现出高准确率和AUC-ROC，并提供了具有临床意义的解释，强调了AI在临床应用中性能与可解释性并重的重要性。", "keywords": "肺炎检测, 可解释AI, 残差网络, 胸部X射线, BayesGrad-CAM", "comments": "本文的创新点在于结合了ResNet的强大分类能力和BayesGrad-CAM的可解释性，特别是在医学诊断领域，可解释性对于临床医生理解和信任AI决策至关重要。量化解释的不确定性是其重要贡献，有助于提高模型的可靠性和临床部署潜力。"}}
{"id": "2507.19156", "title": "An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case", "authors": ["Gioele Giachino", "Marco Rondina", "Antonio Vetrò", "Riccardo Coppola", "Juan Carlos De Martin"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      16 pages, European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2025) - 5th Workshop on Bias and Fairness in AI (BIAS25)", "url": "http://arxiv.org/abs/2507.19156v1", "summary": "The increasing use of Large Language Models (LLMs) in a large variety of\ndomains has sparked worries about how easily they can perpetuate stereotypes\nand contribute to the generation of biased content. With a focus on gender and\nprofessional bias, this work examines in which manner LLMs shape responses to\nungendered prompts, contributing to biased outputs. This analysis uses a\nstructured experimental method, giving different prompts involving three\ndifferent professional job combinations, which are also characterized by a\nhierarchical relationship. This study uses Italian, a language with extensive\ngrammatical gender differences, to highlight potential limitations in current\nLLMs' ability to generate objective text in non-English languages. Two popular\nLLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google\nGemini (gemini-1.5-flash). Through APIs, we collected a range of 3600\nresponses. The results highlight how content generated by LLMs can perpetuate\nstereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'\npronouns to the 'assistant' rather than the 'manager'. The presence of bias in\nAI-generated text can have significant implications in many fields, such as in\nthe workplaces or in job selections, raising ethical concerns about its use.\nUnderstanding these risks is pivotal to developing mitigation strategies and\nassuring that AI-based systems do not increase social inequalities, but rather\ncontribute to more equitable outcomes. Future research directions include\nexpanding the study to additional chatbots or languages, refining prompt\nengineering methods or further exploiting a larger experimental base.", "comment": "16 pages, European Conference on Machine Learning and Principles and\n  Practice of Knowledge Discovery in Databases (ECML PKDD 2025) - 5th Workshop\n  on Bias and Fairness in AI (BIAS25)", "pdf_url": "http://arxiv.org/pdf/2507.19156v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "大型语言模型中性别刻板印象表示的实证研究：以意大利语为例", "tldr": "本研究调查了大型语言模型（LLMs）在意大利语中如何通过非性别化提示传播性别刻板印象，发现ChatGPT和Gemini在职业角色上存在显著的性别偏见，例如将“她”与“助理”而非“经理”关联。", "motivation": "大型语言模型（LLMs）的广泛应用引发了对其传播刻板印象和生成偏见内容的担忧。本研究旨在探究LLMs在处理非性别化提示时，如何形成偏向性输出，尤其关注性别和职业偏见。", "method": "本研究采用结构化实验方法，使用涉及三种不同职业组合的提示，这些职业具有层级关系。研究使用意大利语（一种语法性别差异显著的语言）进行，以揭示当前LLMs在非英语语言中生成客观文本的潜在局限性。实验考察了OpenAI ChatGPT (gpt-4o-mini) 和 Google Gemini (gemini-1.5-flash) 这两款流行的LLM聊天机器人，通过API收集了3600个响应。", "result": "结果显示LLMs生成的内容会延续刻板印象。例如，Gemini将100%（ChatGPT为97%）的“她”代词与“助理”而非“经理”相关联。", "conclusion": "AI生成文本中存在的偏见可能在工作场所或招聘等许多领域产生重大影响，引发伦理担忧。理解这些风险对于制定缓解策略和确保基于AI的系统不加剧社会不平等至关重要。", "translation": "大型语言模型（LLMs）在各种领域的日益广泛使用引发了人们对其轻易延续刻板印象并助长偏见内容生成的担忧。本研究侧重于性别和职业偏见，考察了LLMs如何塑造对非性别化提示的响应，从而导致偏见输出。本分析采用结构化实验方法，给出涉及三种不同职业组合的提示，这些职业也具有层级关系。本研究使用意大利语，一种具有广泛语法性别差异的语言，以突出当前LLMs在非英语语言中生成客观文本的潜在局限性。研究考察了两种流行的大型语言模型聊天机器人，即OpenAI ChatGPT (gpt-4o-mini) 和 Google Gemini (gemini-1.5-flash)。通过API，我们收集了3600个响应。结果突出显示了LLMs生成的内容如何延续刻板印象。例如，Gemini将100%（ChatGPT为97%）的“她”代词与“助理”而非“经理”相关联。AI生成文本中存在的偏见可能在许多领域产生重大影响，例如在工作场所或招聘中，引发对其使用的伦理担忧。理解这些风险对于制定缓解策略和确保基于AI的系统不加剧社会不平等，而是促成更公平的结果至关重要。未来的研究方向包括将研究扩展到其他聊天机器人或语言，改进提示工程方法或进一步利用更大的实验基础。", "summary": "本研究通过对意大利语的实证调查，探讨了大型语言模型（LLMs）中存在的性别刻板印象。研究使用结构化实验方法，测试了ChatGPT和Gemini在处理非性别化职业提示时的表现。结果表明，LLMs生成的内容显著延续了性别偏见，例如将女性代词与较低层级的职业角色关联。这凸显了LLMs在非英语语言中生成客观文本的局限性，并引发了AI在就业等领域应用中的伦理担忧。", "keywords": "大型语言模型, 性别刻板印象, 偏见, 意大利语, ChatGPT, Gemini", "comments": "这项研究通过聚焦意大利语，弥补了现有研究中对非英语LLMs偏见分析的不足，特别是强调了语法性别差异对偏见生成的影响。其创新之处在于使用了分层的职业组合进行测试，并量化了两种主流LLMs的偏见程度。研究结果对LLMs的负责任开发和部署具有重要指导意义，提醒开发者和使用者警惕AI可能加剧社会不平等的风险。"}}
{"id": "2506.19037", "title": "Plan for Speed: Dilated Scheduling for Masked Diffusion Language Models", "authors": ["Omer Luxembourg", "Haim Permuter", "Eliya Nachmani"], "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.NE", "math.IT"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.19037v3", "summary": "Masked diffusion language models (MDLMs) promise fast, non-autoregressive\ntext generation, yet existing samplers, which pick tokens to unmask based on\nmodel confidence, ignore interactions when unmasking multiple positions in\nparallel and effectively reduce to slow, autoregressive behavior. We propose\nthe Dilated Unmasking Scheduler (DUS), an inference-only, planner-model-free\nmethod that partitions sequence positions into non-adjacent dilated groups and\nunmasked them in parallel so as to minimize an upper bound on joint entropy\ngain at each denoising step. By explicitly trading off the number of network\ncalls against generation quality, DUS recovers most of the performance lost\nunder traditional parallel unmasking strategies. Across math (GSM8K, MATH500),\ncode (HumanEval, MBPP) and general-knowledge benchmarks (BBH, MMLU-Pro), DUS\noutperforms confidence-based planners, without modifying the underlying\ndenoiser, and reveals the true speed-quality frontier of MDLMs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.19037v3", "cate": "cs.CL", "date": "2025-06-23", "updated": "2025-07-24", "AI": {"title_translation": "速度计划：掩码扩散语言模型的膨胀调度", "tldr": "现有的掩码扩散语言模型（MDLMs）采样器在并行去掩码时效率低下，本文提出了一种名为膨胀去掩码调度器（DUS）的方法，通过将序列位置分组并并行去掩码，显著提高了生成速度和质量，超越了传统的基于置信度的规划器。", "motivation": "掩码扩散语言模型（MDLMs）虽然有望实现快速的非自回归文本生成，但现有采样器在并行去掩码多个位置时，忽略了相互作用，导致其行为退化为缓慢的自回归模式，未能充分发挥MDLMs的速度潜力。", "method": "本文提出了一种名为膨胀去掩码调度器（DUS）的推理专用、无规划器模型的方法。DUS将序列位置划分为非相邻的膨胀组，并并行进行去掩码，以最小化每个去噪步骤的联合熵增益上限。通过明确权衡网络调用次数与生成质量，DUS恢复了传统并行去掩码策略下损失的大部分性能。", "result": "DUS在数学（GSM8K、MATH500）、代码（HumanEval、MBPP）和通用知识基准（BBH、MMLU-Pro）上均优于基于置信度的规划器，且无需修改底层去噪器。这揭示了MDLMs真正的速度-质量前沿。", "conclusion": "DUS通过创新的膨胀调度方法，显著提升了掩码扩散语言模型的生成速度和质量，成功克服了现有采样器的局限性，并展现了MDLMs在不牺牲性能的前提下实现高速生成的潜力。", "translation": "掩码扩散语言模型（MDLMs）有望实现快速、非自回归的文本生成，然而，现有采样器（其根据模型置信度选择去掩码的标记）在并行去掩码多个位置时忽略了相互作用，并有效地退化为缓慢的自回归行为。我们提出了膨胀去掩码调度器（DUS），这是一种推理专用、无规划器模型的方法，它将序列位置划分为非相邻的膨胀组，并并行去掩码，以最小化每个去噪步骤的联合熵增益上限。通过明确权衡网络调用次数与生成质量，DUS恢复了传统并行去掩码策略下损失的大部分性能。在数学（GSM8K、MATH500）、代码（HumanEval、MBPP）和通用知识基准（BBH、MMLU-Pro）上，DUS均优于基于置信度的规划器，且无需修改底层去噪器，并揭示了MDLMs真正的速度-质量前沿。", "summary": "本文针对掩码扩散语言模型（MDLMs）现有采样器在并行去掩码时效率低下、行为退化的问题，提出了一种创新的膨胀去掩码调度器（DUS）。DUS通过将序列位置划分为非相邻的膨胀组并并行去掩码，旨在最小化联合熵增益，从而在不修改底层模型的情况下，显著提升了MDLMs的生成速度和质量。实验证明，DUS在多个基准测试中均优于传统的基于置信度的规划器，揭示了MDLMs在速度与质量之间的真实权衡边界。", "keywords": "掩码扩散语言模型, 非自回归生成, 膨胀调度, 文本生成, 速度-质量权衡", "comments": "该论文提出了一种新颖的膨胀调度方法（DUS），解决了掩码扩散语言模型（MDLMs）在并行去掩码时效率低下的核心问题。其创新点在于通过非相邻分组并行去掩码，并明确权衡速度与质量，有效恢复了性能。DUS无需修改底层去噪器即可提升性能，这使其具有很高的实用价值和通用性。该工作对于推动非自回归文本生成领域的发展具有重要意义。"}}
{"id": "2507.19473", "title": "Let It Go? Not Quite: Addressing Item Cold Start in Sequential Recommendations with Content-Based Initialization", "authors": ["Anton Pembek", "Artem Fatkulin", "Anton Klenitskiy", "Alexey Vasilev"], "categories": ["cs.IR", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19473v1", "summary": "Many sequential recommender systems suffer from the cold start problem, where\nitems with few or no interactions cannot be effectively used by the model due\nto the absence of a trained embedding. Content-based approaches, which leverage\nitem metadata, are commonly used in such scenarios. One possible way is to use\nembeddings derived from content features such as textual descriptions as\ninitialization for the model embeddings. However, directly using frozen content\nembeddings often results in suboptimal performance, as they may not fully adapt\nto the recommendation task. On the other hand, fine-tuning these embeddings can\ndegrade performance for cold-start items, as item representations may drift far\nfrom their original structure after training. We propose a novel approach to\naddress this limitation. Instead of entirely freezing the content embeddings or\nfine-tuning them extensively, we introduce a small trainable delta to frozen\nembeddings that enables the model to adapt item representations without letting\nthem go too far from their original semantic structure. This approach\ndemonstrates consistent improvements across multiple datasets and modalities,\nincluding e-commerce datasets with textual descriptions and a music dataset\nwith audio-based representation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19473v1", "cate": "cs.IR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "放手？不尽然：通过基于内容的初始化解决序列推荐中的物品冷启动问题", "tldr": "提出一种新方法，通过对冻结的基于内容嵌入添加可训练的微小增量，解决序列推荐系统中物品冷启动问题，既能适应任务又能保持语义结构。", "motivation": "序列推荐系统面临物品冷启动问题，即缺乏交互的物品无法有效训练嵌入。现有内容方法存在弊端：直接使用冻结嵌入表现不佳，而微调则可能使冷启动物品的表示偏离原始结构。", "method": "提出一种新方法，不完全冻结或过度微调内容嵌入，而是向冻结的嵌入引入一个小的可训练增量（delta），使模型能够调整物品表示，同时避免其偏离原始语义结构。", "result": "该方法在多个数据集和模态（包括带有文本描述的电商数据集和带有音频表示的音乐数据集）上都显示出持续的改进。", "conclusion": "通过引入可训练的微小增量，本方法成功解决了现有内容基线方法在物品冷启动问题上的局限性，在适应性和保持语义结构之间取得了平衡，从而持续提升了推荐性能。", "translation": "许多序列推荐系统都受到冷启动问题的困扰，即由于缺乏训练好的嵌入，互动较少或没有互动的物品无法被模型有效利用。利用物品元数据的基于内容的方法在这种情况下很常用。一种可能的方法是使用从文本描述等内容特征派生出的嵌入作为模型嵌入的初始化。然而，直接使用冻结的内容嵌入通常会导致次优性能，因为它们可能无法完全适应推荐任务。另一方面，微调这些嵌入可能会降低冷启动物品的性能，因为物品表示在训练后可能会偏离其原始结构。我们提出了一种新颖的方法来解决这一限制。我们没有完全冻结内容嵌入或对其进行大量微调，而是向冻结的嵌入引入一个小的可训练增量，使模型能够调整物品表示，同时不让它们偏离原始语义结构太远。这种方法在多个数据集和模态上都显示出持续的改进，包括带有文本描述的电子商务数据集和带有基于音频表示的音乐数据集。", "summary": "本文针对序列推荐系统中物品冷启动问题提出了一种新颖的解决方案。现有方法或直接使用冻结的内容嵌入导致性能次优，或通过微调导致冷启动物品表示偏离。作者提出一种折衷方案，即在冻结的内容嵌入上添加一个小的可训练增量，这使得模型能够在适应推荐任务的同时，保持物品表示的原始语义结构。实验结果表明，该方法在多种数据集和模态上均取得了持续的性能提升。", "keywords": "序列推荐, 冷启动, 内容初始化, 嵌入, 可训练增量", "comments": "本文的创新点在于提出了一个巧妙的折衷方案，解决了内容基线方法在处理物品冷启动时“适应性不足”和“结构漂移”之间的矛盾。通过引入一个小的可训练增量，该方法在保持物品原有语义结构的同时，允许模型进行必要的调整，这对于提高冷启动物品的推荐性能具有重要意义，且在不同模态数据上均表现出有效性。"}}
{"id": "2507.05736", "title": "Tight Bound for Quantum Unitary Time-Reversal", "authors": ["Kean Chen", "Nengkun Yu", "Zhicheng Zhang"], "categories": ["quant-ph", "cs.IT", "math.IT"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      29 pages; minor revision; removed the result about hardness of unitary controlization due to an error", "url": "http://arxiv.org/abs/2507.05736v3", "summary": "Time-reversal of unitary evolution is fundamental in quantum information\nprocessing. Many scenarios, particularly those in quantum learning and\nmetrology, assume free access to the time-reverse of an unknown unitary. In\nthis paper, we settle the query complexity of the unitary time-reversal task:\napproximately implementing $U^{-1}$ given only black-box access to an unknown\n$d$-dimensional unitary $U$. We provide a tight query lower bound\n$\\Omega((1-\\epsilon)d^2)$ for the unitary time-reversal to within diamond norm\nerror $\\epsilon$. Notably, our lower bound applies to general coherent\nprotocols with unbounded ancillas, and holds even when $\\epsilon$ is an\naverage-case distance error and access to control-$U$ is available.", "comment": "29 pages; minor revision; removed the result about hardness of\n  unitary controlization due to an error", "pdf_url": "http://arxiv.org/pdf/2507.05736v3", "cate": "quant-ph", "date": "2025-07-08", "updated": "2025-07-25", "AI": {"title_translation": "量子酉变换时间反演的紧致界限", "tldr": "该论文确定了量子酉变换时间反演的查询复杂度，并给出了一个紧致的查询下界，对于量子信息处理中的基本操作具有重要意义。", "motivation": "量子信息处理中，酉变换的时间反演是基础操作，许多场景（特别是量子学习和计量）都假设可以自由访问未知酉变换的时间反演。本文旨在解决酉变换时间反演任务的查询复杂度问题。", "method": "通过对未知$d$维酉变换$U$的黑盒访问，近似实现$U^{-1}$，并提供了针对金刚石范数误差$\\epsilon$的紧致查询下界$\\Omega((1-\\epsilon)d^2)$。", "result": "得到了量子酉变换时间反演的查询复杂度紧致下界为$\\Omega((1-\\epsilon)d^2)$，适用于金刚石范数误差$\\epsilon$。", "conclusion": "该下界适用于具有无界辅助比特的通用相干协议，即使误差$\\epsilon$是平均情况距离误差且可访问control-$U$也成立。", "translation": "酉变换演化的时间反演是量子信息处理中的基础。许多场景，特别是量子学习和计量中的场景，都假设可以自由访问未知酉变换的时间反演。在本文中，我们解决了酉变换时间反演任务的查询复杂度问题：在仅能黑盒访问未知$d$维酉变换$U$的情况下，近似实现$U^{-1}$。我们为酉变换时间反演在金刚石范数误差$\\epsilon$内的实现提供了一个紧致的查询下界$\\Omega((1-\\epsilon)d^2)$。值得注意的是，我们的下界适用于具有无界辅助比特的通用相干协议，即使$\\epsilon$是平均情况距离误差且可访问control-$U$也成立。", "summary": "本文解决了量子酉变换时间反演的查询复杂度问题。研究人员在仅能黑盒访问未知酉变换$U$的情况下，探讨了如何近似实现$U^{-1}$，并给出了一个在金刚石范数误差$\\epsilon$下的紧致查询下界$\\Omega((1-\\epsilon)d^2)$。该下界对通用相干协议及其多种误差条件均有效。", "keywords": "量子信息处理, 酉变换, 时间反演, 查询复杂度, 紧致下界", "comments": "这项研究为量子信息处理中的一个基本操作——酉变换时间反演的实现提供了严格的理论界限。其创新性在于确定了该任务的查询复杂度，并提出了一个在广泛条件下都适用的紧致下界，这对于理解量子算法的资源需求和优化量子协议具有重要意义。"}}
{"id": "2507.19458", "title": "Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints", "authors": ["Amir Fard", "Arnold X. -X. Yuan"], "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "math.OC"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19458v1", "summary": "Budget planning and maintenance optimization are crucial for infrastructure\nasset management, ensuring cost-effectiveness and sustainability. However, the\ncomplexity arising from combinatorial action spaces, diverse asset\ndeterioration, stringent budget constraints, and environmental uncertainty\nsignificantly limits existing methods' scalability. This paper proposes a\nHierarchical Deep Reinforcement Learning methodology specifically tailored to\nmulti-year infrastructure planning. Our approach decomposes the problem into\ntwo hierarchical levels: a high-level Budget Planner allocating annual budgets\nwithin explicit feasibility bounds, and a low-level Maintenance Planner\nprioritizing assets within the allocated budget. By structurally separating\nmacro-budget decisions from asset-level prioritization and integrating linear\nprogramming projection within a hierarchical Soft Actor-Critic framework, the\nmethod efficiently addresses exponential growth in the action space and ensures\nrigorous budget compliance. A case study evaluating sewer networks of varying\nsizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed\napproach. Compared to conventional Deep Q-Learning and enhanced genetic\nalgorithms, our methodology converges more rapidly, scales effectively, and\nconsistently delivers near-optimal solutions even as network size grows.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19458v1", "cate": "cs.AI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "预算约束下多年期资产管理的层次深度强化学习框架", "tldr": "本文提出了一种层次深度强化学习方法，用于在预算约束下进行多年期基础设施资产管理。该方法通过将问题分解为预算规划和维护规划两个层次，有效解决了动作空间复杂性和预算合规性问题，并展现出更快的收敛速度、更好的可扩展性和接近最优的解决方案。", "motivation": "基础设施资产管理的预算规划和维护优化至关重要，但现有方法面临组合动作空间复杂性、资产劣化多样性、严格预算约束和环境不确定性等挑战，限制了其可扩展性。", "method": "本文提出了一种分层深度强化学习方法。该方法将问题分解为两个层次：高层预算规划器负责在明确的可行性范围内分配年度预算，低层维护规划器负责在分配的预算内确定资产优先级。通过结构性地分离宏观预算决策和资产层级优先级，并将线性规划投影集成到分层软Actor-Critic框架中，有效处理了动作空间的指数增长并确保了严格的预算合规性。", "result": "在针对不同规模（10、15和20个下水道流域）的下水道网络进行的案例研究中，所提出的方法与传统的深度Q学习和增强遗传算法相比，收敛速度更快、可扩展性更强，并且即使在网络规模增长的情况下也能持续提供接近最优的解决方案。", "conclusion": "所提出的分层深度强化学习方法在预算约束下的多年期资产管理方面表现出卓越的性能，能够有效应对复杂性、确保预算合规性并提供接近最优的解决方案，具有良好的可扩展性。", "translation": "预算规划和维护优化对于基础设施资产管理至关重要，可确保成本效益和可持续性。然而，组合动作空间、多样化的资产劣化、严格的预算约束和环境不确定性带来的复杂性显著限制了现有方法的可扩展性。本文提出了一种专门针对多年期基础设施规划的层次深度强化学习方法。我们的方法将问题分解为两个层次：高层预算规划器在明确的可行性范围内分配年度预算，低层维护规划器在分配的预算内确定资产优先级。通过结构性地分离宏观预算决策和资产层级优先级，并将线性规划投影集成到层次软Actor-Critic框架中，该方法有效解决了动作空间的指数增长并确保了严格的预算合规性。一项评估不同规模（10、15和20个下水道流域）下水道网络的案例研究说明了所提出方法的有效性。与传统的深度Q学习和增强遗传算法相比，我们的方法收敛速度更快，可有效扩展，即使网络规模增长也能持续提供接近最优的解决方案。", "summary": "本文提出一种层次深度强化学习框架，旨在解决预算约束下多年期基础设施资产管理的复杂性与可扩展性挑战。该方法将问题分解为高层预算规划与低层维护规划，通过融合线性规划投影和分层软Actor-Critic框架，有效应对了组合动作空间和严格预算限制。案例研究表明，相比传统方法，该框架在收敛速度、可扩展性及解决方案质量上均表现出显著优势。", "keywords": "层次深度强化学习, 资产管理, 预算约束, 基础设施, 软Actor-Critic", "comments": "该论文的创新点在于将复杂的多年期资产管理问题分解为层次化的深度强化学习任务，并巧妙地结合了线性规划投影，有效解决了动作空间爆炸和预算合规性难题。这种分层处理方式提高了模型的可扩展性和决策的精细度，对于实际基础设施管理具有重要意义。其在复杂约束条件下的有效性也为类似资源分配问题提供了新的解决思路。"}}
{"id": "2507.18939", "title": "PDT: Point Distribution Transformation with Diffusion Models", "authors": ["Jionghao Wang", "Cheng Lin", "Yuan Liu", "Rui Xu", "Zhiyang Dou", "Xiao-Xiao Long", "Hao-Xiang Guo", "Taku Komura", "Wenping Wang", "Xin Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2507.18939v1", "summary": "Point-based representations have consistently played a vital role in\ngeometric data structures. Most point cloud learning and processing methods\ntypically leverage the unordered and unconstrained nature to represent the\nunderlying geometry of 3D shapes. However, how to extract meaningful structural\ninformation from unstructured point cloud distributions and transform them into\nsemantically meaningful point distributions remains an under-explored problem.\nWe present PDT, a novel framework for point distribution transformation with\ndiffusion models. Given a set of input points, PDT learns to transform the\npoint set from its original geometric distribution into a target distribution\nthat is semantically meaningful. Our method utilizes diffusion models with\nnovel architecture and learning strategy, which effectively correlates the\nsource and the target distribution through a denoising process. Through\nextensive experiments, we show that our method successfully transforms input\npoint clouds into various forms of structured outputs - ranging from\nsurface-aligned keypoints, and inner sparse joints to continuous feature lines.\nThe results showcase our framework's ability to capture both geometric and\nsemantic features, offering a powerful tool for various 3D geometry processing\ntasks where structured point distributions are desired. Code will be available\nat this link: https://github.com/shanemankiw/PDT.", "comment": "Project page: https://shanemankiw.github.io/PDT/", "pdf_url": "http://arxiv.org/pdf/2507.18939v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "PDT：基于扩散模型的点分布变换", "tldr": "PDT是一种利用扩散模型将非结构化点云转换为语义上有意义的结构化点分布的新框架，能够捕捉几何和语义特征，并应用于多种3D几何处理任务。", "motivation": "如何从非结构化点云分布中提取有意义的结构信息并将其转换为语义上有意义的点分布，仍然是一个探索不足的问题。", "method": "我们提出了PDT，一个用于点分布变换的新框架，它利用扩散模型。给定一组输入点，PDT学习将点集从其原始几何分布转换为语义上有意义的目标分布。该方法利用具有新颖架构和学习策略的扩散模型，通过去噪过程有效地关联源分布和目标分布。", "result": "通过广泛的实验，我们证明了我们的方法成功地将输入点云转换为各种形式的结构化输出，包括表面对齐的关键点、内部稀疏关节到连续特征线。", "conclusion": "结果表明，我们的框架能够同时捕获几何和语义特征，为需要结构化点分布的各种3D几何处理任务提供了强大的工具。", "translation": "点基表示在几何数据结构中一直发挥着至关重要的作用。大多数点云学习和处理方法通常利用其无序和无约束的特性来表示3D形状的底层几何结构。然而，如何从非结构化点云分布中提取有意义的结构信息并将其转换为语义上有意义的点分布，仍然是一个探索不足的问题。我们提出了PDT，一个利用扩散模型进行点分布变换的新颖框架。给定一组输入点，PDT学习将点集从其原始几何分布转换为语义上有意义的目标分布。我们的方法利用具有新颖架构和学习策略的扩散模型，通过去噪过程有效地关联源分布和目标分布。通过广泛的实验，我们表明我们的方法成功地将输入点云转换为各种形式的结构化输出——从表面对齐的关键点、内部稀疏关节到连续特征线。结果展示了我们的框架捕获几何和语义特征的能力，为需要结构化点分布的各种3D几何处理任务提供了强大的工具。代码将在此链接提供：https://github.com/shanemankiw/PDT。", "summary": "PDT是一个新颖的框架，利用扩散模型将非结构化点云转换为语义上有意义的结构化点分布。它通过一个去噪过程有效地关联源和目标分布。实验证明，PDT能够成功地将点云转换为各种结构化输出，如关键点、关节和特征线，展现了其捕获几何和语义特征的能力，为3D几何处理任务提供了强大的工具。", "keywords": "点分布变换, 扩散模型, 点云处理, 几何特征, 语义特征", "comments": "该论文提出了一种新颖的点分布变换框架PDT，创新性地将扩散模型应用于点云结构化信息提取。它解决了从非结构化点云中获取语义结构信息的挑战，通过将点云转换为多种形式的结构化输出，极大地扩展了点云的应用潜力，对于3D几何处理领域具有重要意义。"}}
{"id": "2507.18378", "title": "A comparison of stretched-grid and limited-area modelling for data-driven regional weather forecasting", "authors": ["Jasper S. Wijnands", "Michiel Van Ginderachter", "Bastien François", "Sophie Buurman", "Piet Termonia", "Dieter Van den Bleeken"], "categories": ["physics.ao-ph", "cs.LG"], "primary_category": "Subjects:       Atmospheric and Oceanic Physics (physics.ao-ph)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18378v1", "summary": "Regional machine learning weather prediction (MLWP) models based on graph\nneural networks have recently demonstrated remarkable predictive accuracy,\noutperforming numerical weather prediction models at lower computational costs.\nIn particular, limited-area model (LAM) and stretched-grid model (SGM)\napproaches have emerged for generating high-resolution regional forecasts,\nbased on initial conditions from a regional (re)analysis. While LAM uses\nlateral boundaries from an external global model, SGM incorporates a global\ndomain at lower resolution. This study aims to understand how the differences\nin model design impact relative performance and potential applications.\nSpecifically, the strengths and weaknesses of these two approaches are\nidentified for generating deterministic regional forecasts over Europe. Using\nthe Anemoi framework, models of both types are built by minimally adapting a\nshared architecture and trained using global and regional reanalyses in a\nnear-identical setup. Several inference experiments have been conducted to\nexplore their relative performance and highlight key differences. Results show\nthat both LAM and SGM are competitive deterministic MLWP models with generally\naccurate and comparable forecasting performance over the regional domain.\nVarious differences were identified in the performance of the models across\napplications. LAM is able to successfully exploit high-quality boundary\nforcings to make predictions within the regional domain and is suitable in\ncontexts where global data is difficult to acquire. SGM is fully self-contained\nfor easier operationalisation, can take advantage of more training data and\nsignificantly surpasses LAM in terms of (temporal) generalisability. Our paper\ncan serve as a starting point for meteorological institutes to guide their\nchoice between LAM and SGM in developing an operational data-driven forecasting\nsystem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18378v1", "cate": "physics.ao-ph", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "数据驱动区域天气预报中拉伸网格与有限区域建模的比较", "tldr": "本研究比较了两种基于图神经网络的区域机器学习天气预报模型（有限区域模型LAM和拉伸网格模型SGM），发现两者性能相当，但各有优势：LAM适用于边界条件高质量的区域预测，SGM则更易于操作且泛化能力强。", "motivation": "区域机器学习天气预报模型（MLWP）在预测准确性和计算成本方面表现出色，其中有限区域模型（LAM）和拉伸网格模型（SGM）是生成高分辨率区域预报的两种主要方法。本研究旨在理解这两种模型设计上的差异如何影响其相对性能和潜在应用。", "method": "研究使用Anemoi框架，通过最小化调整共享架构，构建了LAM和SGM两种类型的模型。这些模型利用全球和区域再分析数据在几乎相同的设置下进行训练。通过多项推断实验来探索它们的相对性能并突出关键差异，特别是在欧洲区域生成确定性区域预报。", "result": "结果显示，LAM和SGM都是有竞争力的确定性MLWP模型，在区域域内具有普遍准确且可比较的预测性能。在不同应用中，模型性能存在差异：LAM能成功利用高质量的边界强迫进行区域预测，适用于难以获取全球数据的场景；SGM则完全自包含，更易于操作，能利用更多训练数据，并在（时间）泛化能力方面显著超越LAM。", "conclusion": "LAM和SGM都是有效的区域机器学习天气预报模型，各有优缺点。LAM适用于高质量边界数据可用的情况，而SGM在操作便利性和泛化能力方面更具优势。本研究可为气象机构选择开发数据驱动预报系统时提供指导。", "translation": "区域机器学习天气预报（MLWP）模型基于图神经网络，近期已展现出卓越的预测精度，在计算成本更低的情况下超越了数值天气预报模型。特别是，有限区域模型（LAM）和拉伸网格模型（SGM）方法已兴起，用于基于区域（再）分析的初始条件生成高分辨率区域预报。LAM使用外部全球模型的侧向边界，而SGM则以较低分辨率包含全球域。本研究旨在理解模型设计上的差异如何影响相对性能和潜在应用。具体而言，研究确定了这两种方法在欧洲生成确定性区域预报的优缺点。使用Anemoi框架，通过最小限度地调整共享架构，构建了两种类型的模型，并在几乎相同的设置下使用全球和区域再分析数据进行训练。进行了多项推断实验，以探索它们的相对性能并突出关键差异。结果表明，LAM和SGM都是具有竞争力的确定性MLWP模型，在区域域内具有普遍准确且可比较的预测性能。在不同应用中，模型性能存在各种差异。LAM能够成功利用高质量的边界强迫在区域域内进行预测，适用于难以获取全球数据的场景。SGM完全自包含，更易于操作，可以利用更多的训练数据，并在（时间）泛化能力方面显著超越LAM。我们的论文可以作为气象机构的起点，指导他们在开发业务数据驱动预报系统时在LAM和SGM之间做出选择。", "summary": "本研究比较了两种基于图神经网络的区域机器学习天气预报（MLWP）模型：有限区域模型（LAM）和拉伸网格模型（SGM）。通过在欧洲区域使用Anemoi框架进行实验，研究发现两者在区域预测性能上均表现出色且可比较。LAM擅长利用高质量边界数据进行区域预测，适用于全球数据受限情况；而SGM则在操作便利性和时间泛化能力上更具优势，且能利用更多训练数据。本研究为气象机构选择MLWP系统提供了指导。", "keywords": "区域天气预报, 机器学习, 拉伸网格模型, 有限区域模型, 图神经网络", "comments": "该论文对两种主流的区域机器学习天气预报模型进行了详细比较，填补了该领域的一个空白。其创新之处在于使用了统一的Anemoi框架进行对比，确保了实验设置的公平性。研究结果不仅揭示了两种模型的性能差异，还明确指出了它们各自的适用场景和优势，这对于实际应用具有重要的指导意义。特别是对SGM在泛化能力上的发现，为未来数据驱动预报系统的发展提供了新的思路。"}}
{"id": "2502.02470", "title": "Studying Cross-cluster Modularity in Neural Networks", "authors": ["Satvik Golechha", "Maheep Chaudhary", "Joan Velja", "Alessandro Abate", "Nandi Schoots"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, under review. arXiv admin note: text overlap with arXiv:2409.15747 (author note: this is an extension of that paper but has different authors)", "url": "http://arxiv.org/abs/2502.02470v3", "summary": "An approach to improve neural network interpretability is via clusterability,\ni.e., splitting a model into disjoint clusters that can be studied\nindependently. We define a measure for clusterability and show that pre-trained\nmodels form highly enmeshed clusters via spectral graph clustering. We thus\ntrain models to be more modular using a \"clusterability loss\" function that\nencourages the formation of non-interacting clusters. We then investigate the\nemerging properties of these highly clustered models. We find our trained\nclustered models do not exhibit more task specialization, but do form smaller\ncircuits. We investigate CNNs trained on MNIST and CIFAR, small transformers\ntrained on modular addition, and GPT-2 and Pythia on the Wiki dataset, and\nGemma on a Chemistry dataset. This investigation shows what to expect from\nclustered models.", "comment": "8 pages, under review. arXiv admin note: text overlap with\n  arXiv:2409.15747 (author note: this is an extension of that paper but has\n  different authors)", "pdf_url": "http://arxiv.org/pdf/2502.02470v3", "cate": "cs.LG", "date": "2025-02-04", "updated": "2025-07-25", "AI": {"title_translation": "研究神经网络中的跨簇模块化", "tldr": "该论文提出了一种通过“可聚类性损失”函数训练神经网络以提高模块化的方法，旨在增强可解释性，并发现训练后的模型虽然没有表现出更多的任务特化，但形成了更小的电路。", "motivation": "通过可聚类性来提高神经网络的可解释性，即将模型分解为可以独立研究的不相交的簇。", "method": "定义了一种可聚类性度量，利用谱图聚类分析预训练模型，并使用“可聚类性损失”函数训练模型，以促进非交互簇的形成。", "result": "预训练模型通过谱图聚类形成高度纠缠的簇。通过“可聚类性损失”训练的聚类模型并未表现出更多的任务特化，但形成了更小的电路。研究在MNIST和CIFAR上的CNN、在模块化加法上的小型Transformer，以及在Wiki数据集上的GPT-2和Pythia，以及在化学数据集上的Gemma。", "conclusion": "这项研究展示了对聚类模型的预期特性。", "translation": "提高神经网络可解释性的一种方法是通过可聚类性，即将模型分成可以独立研究的不相交的簇。我们定义了一种可聚类性度量，并表明预训练模型通过谱图聚类形成高度纠缠的簇。因此，我们使用“可聚类性损失”函数训练模型使其更具模块化，该函数鼓励形成非交互的簇。然后，我们研究了这些高度聚类模型的涌现特性。我们发现我们训练的聚类模型并未表现出更多的任务特化，但确实形成了更小的电路。我们研究了在MNIST和CIFAR上训练的CNN、在模块化加法上训练的小型Transformer，以及在Wiki数据集上的GPT-2和Pythia，以及在化学数据集上的Gemma。这项研究展示了对聚类模型的预期。", "summary": "该论文引入了一种“可聚类性损失”函数，用于训练神经网络以提高其模块化和可解释性。研究发现，虽然预训练模型显示出高度纠缠的特性，但经过该损失函数训练的模型形成了更小、非交互的电路，尽管没有增加任务特化。这项研究通过对多种网络架构和数据集的评估，为理解聚类模型的特性提供了见解。", "keywords": "神经网络, 可解释性, 模块化, 聚类性, 谱图聚类", "comments": "这篇论文引入了一种创新的“可聚类性损失”来增强神经网络的可解释性，通过鼓励模块化。虽然它成功地创建了更小、更独立的电路，但值得注意的是，它发现任务特化并没有提高，这是一个有趣的负面结果。对不同模型类型和数据集的广泛评估增加了研究结果的鲁棒性，为模块化神经网络的权衡和特性提供了实用见解。"}}
{"id": "2507.18143", "title": "HIVMedQA: Benchmarking large language models for HIV medical decision support", "authors": ["Gonzalo Cardenal-Antolin", "Jacques Fellay", "Bashkim Jaha", "Roger Kouyos", "Niko Beerenwinkel", "Diane Duroux"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18143v2", "summary": "Large language models (LLMs) are emerging as valuable tools to support\nclinicians in routine decision-making. HIV management is a compelling use case\ndue to its complexity, including diverse treatment options, comorbidities, and\nadherence challenges. However, integrating LLMs into clinical practice raises\nconcerns about accuracy, potential harm, and clinician acceptance. Despite\ntheir promise, AI applications in HIV care remain underexplored, and LLM\nbenchmarking studies are scarce. This study evaluates the current capabilities\nof LLMs in HIV management, highlighting their strengths and limitations. We\nintroduce HIVMedQA, a benchmark designed to assess open-ended medical question\nanswering in HIV care. The dataset consists of curated, clinically relevant\nquestions developed with input from an infectious disease physician. We\nevaluated seven general-purpose and three medically specialized LLMs, applying\nprompt engineering to enhance performance. Our evaluation framework\nincorporates both lexical similarity and an LLM-as-a-judge approach, extended\nto better reflect clinical relevance. We assessed performance across key\ndimensions: question comprehension, reasoning, knowledge recall, bias,\npotential harm, and factual accuracy. Results show that Gemini 2.5 Pro\nconsistently outperformed other models across most dimensions. Notably, two of\nthe top three models were proprietary. Performance declined as question\ncomplexity increased. Medically fine-tuned models did not always outperform\ngeneral-purpose ones, and larger model size was not a reliable predictor of\nperformance. Reasoning and comprehension were more challenging than factual\nrecall, and cognitive biases such as recency and status quo were observed.\nThese findings underscore the need for targeted development and evaluation to\nensure safe, effective LLM integration in clinical care.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18143v2", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-25", "AI": {"title_translation": "HIVMedQA：基准测试大型语言模型在HIV医疗决策支持中的应用", "tldr": "本研究引入HIVMedQA基准测试，评估大型语言模型在HIV管理中的能力，发现Gemini 2.5 Pro表现最佳，但模型仍需针对性开发以安全整合。", "motivation": "大型语言模型（LLMs）在临床决策支持方面潜力巨大，而HIV管理因其复杂性（多样治疗、并发症、依从性挑战）是一个理想的应用场景。然而，LLMs在临床实践中的整合面临准确性、潜在危害和医生接受度等问题，且HIV护理中的AI应用及LLM基准测试研究稀缺。因此，本研究旨在评估LLMs在HIV管理中的现有能力及其优缺点。", "method": "本研究引入了HIVMedQA，一个旨在评估HIV护理中开放式医学问题回答能力的基准测试数据集。该数据集包含与传染病医生合作开发的、经过整理的、临床相关的问题。研究评估了七个通用型和三个医学专业型LLM，并应用了提示工程来提高性能。评估框架结合了词汇相似度分析和“LLM作为评判者”的方法，并进行了扩展以更好地反映临床相关性。评估维度包括问题理解、推理、知识回忆、偏见、潜在危害和事实准确性。", "result": "结果显示，Gemini 2.5 Pro在大多数维度上始终优于其他模型。排名前三的模型中有两个是专有模型。问题复杂性增加时，模型性能下降。医学微调模型并非总优于通用模型，且模型大小不是性能的可靠预测指标。推理和理解比事实回忆更具挑战性，并观察到近因效应和现状偏见等认知偏见。", "conclusion": "这些发现强调了需要进行有针对性的开发和评估，以确保大型语言模型安全有效地整合到临床护理中。", "translation": "大型语言模型（LLMs）正在成为支持临床医生日常决策的宝贵工具。HIV管理因其复杂性，包括多样化的治疗方案、并发症和依从性挑战，是一个引人注目的用例。然而，将LLMs整合到临床实践中引发了对其准确性、潜在危害和临床医生接受度的担忧。尽管前景广阔，但AI在HIV护理中的应用仍未得到充分探索，且LLM的基准测试研究稀缺。本研究评估了LLMs在HIV管理中的当前能力，突出了它们的优点和局限性。我们引入了HIVMedQA，一个旨在评估HIV护理中开放式医学问题回答能力的基准测试数据集。该数据集由与传染病医生合作开发的、经过整理的、临床相关的问题组成。我们评估了七个通用型和三个医学专业型LLM，并应用了提示工程来增强性能。我们的评估框架结合了词汇相似度分析和“LLM作为评判者”的方法，并进行了扩展以更好地反映临床相关性。我们评估了关键维度上的性能：问题理解、推理、知识回忆、偏见、潜在危害和事实准确性。结果显示，Gemini 2.5 Pro在大多数维度上始终优于其他模型。值得注意的是，排名前三的模型中有两个是专有模型。问题复杂性增加时，性能下降。医学微调模型并非总优于通用模型，且模型大小不是性能的可靠预测指标。推理和理解比事实回忆更具挑战性，并观察到近因效应和现状偏见等认知偏见。这些发现强调了需要进行有针对性的开发和评估，以确保大型语言模型安全有效地整合到临床护理中。", "summary": "本研究旨在评估大型语言模型（LLMs）在HIV管理中的应用潜力。为此，研究团队构建了HIVMedQA，一个包含临床相关问题的基准测试数据集。通过评估七个通用型和三个医学专业型LLMs，研究发现Gemini 2.5 Pro表现最佳，但LLM在处理复杂问题时性能下降，且存在认知偏见。研究强调了对LLM进行有针对性开发和评估的重要性，以确保其在临床实践中的安全有效整合。", "keywords": "大型语言模型, HIV管理, 医学决策支持, 基准测试, HIVMedQA", "comments": "本文的创新之处在于构建了首个针对HIV医疗决策支持的开放式医学问答基准测试数据集HIVMedQA，并提出了一种结合词汇相似度和“LLM作为评判者”的全面评估框架，且该框架进一步优化以提升临床相关性。其重要性在于揭示了当前LLMs在医疗领域，特别是在HIV管理中的实际能力、局限性以及潜在风险（如认知偏见），为未来LLMs在临床环境中的安全、有效部署提供了关键的实证依据和发展方向。研究结果强调了在医疗LLM开发中，不能简单依赖模型大小或通用微调，而需进行针对性的优化和严格评估。"}}
{"id": "2507.19335", "title": "How Age Influences the Interpretation of Emotional Body Language in Humanoid Robots -- long paper version", "authors": ["Ilaria Consoli", "Claudio Mattutino", "Cristina Gena", "Berardina de Carolis", "Giuseppe Palestra"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19335v1", "summary": "This paper presents an empirical study investigating how individuals across\ndifferent age groups, children, young and older adults, interpret emotional\nbody language expressed by the humanoid robot NAO. The aim is to offer insights\ninto how users perceive and respond to emotional cues from robotic agents,\nthrough an empirical evaluation of the robot's effectiveness in conveying\nemotions to different groups of users. By analyzing data collected from elderly\nparticipants and comparing these findings with previously gathered data from\nyoung adults and children, the study highlights similarities and differences\nbetween the groups, with younger and older users more similar but different\nfrom young adults.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19335v1", "cate": "cs.RO", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "年龄如何影响类人机器人的情感肢体语言解读——长论文版本", "tldr": "一项实证研究，探讨了儿童、年轻人和老年人如何解读NAO类人机器人的情感肢体语言，发现年幼和老年用户在解读方式上更相似，但与年轻人不同。", "motivation": "旨在深入了解用户如何感知和回应机器人代理的情感线索，并通过实证评估机器人向不同用户群体传达情感的有效性。", "method": "本文提出了一项实证研究，调查了不同年龄组的个体（儿童、年轻人和老年人）如何解读类人机器人NAO所表达的情感肢体语言。研究通过分析从老年参与者收集的数据，并将其与之前从年轻人和儿童那里收集的数据进行比较。", "result": "研究突出了不同群体之间的相似点和差异点，其中年幼用户和老年用户更相似，但与年轻人不同。", "conclusion": "不同年龄段的用户在解读机器人情感肢体语言方面存在相似性和差异性，这对于未来机器人设计和人机交互具有指导意义。", "translation": "本文提出了一项实证研究，调查了不同年龄组的个体（儿童、年轻人和老年人）如何解读类人机器人NAO所表达的情感肢体语言。其目的是通过对机器人向不同用户群体传达情感有效性的实证评估，深入了解用户如何感知和回应机器人代理的情感线索。通过分析从老年参与者收集的数据，并将其与之前从年轻人和儿童那里收集的数据进行比较，该研究强调了不同群体之间的相似点和差异点，其中年幼用户和老年用户更相似，但与年轻人不同。", "summary": "本文通过一项实证研究，探讨了儿童、年轻人和老年人等不同年龄组的个体如何解读NAO类人机器人表达的情感肢体语言。研究旨在评估机器人向不同用户群体传达情感的有效性，并分析了老年参与者的数据，与年轻人和儿童的数据进行比较。研究结果显示，年幼用户和老年用户在解读机器人情感方面更为相似，但与年轻人有所不同，揭示了年龄对人机情感交互理解的影响。", "keywords": "情感肢体语言, 类人机器人, 年龄影响, NAO, 用户感知", "comments": "这项研究具有重要的实际意义，因为它揭示了年龄在人机情感交互中的作用。研究结果可以指导未来类人机器人的设计，使其能够更好地适应不同年龄段用户的认知和情感解读特点，从而提升人机交互的自然性和有效性。其创新点在于对不同年龄群体（特别是老年人）的关注，填补了该领域的一些空白。"}}
{"id": "2502.10802", "title": "CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code Generation", "authors": ["Kefan Li", "Yuan Yuan", "Hongyue Yu", "Tingyu Guo", "Shijie Cao"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.10802v2", "summary": "Large Language Models (LLMs) have shown remarkable performance in automated\ncode generation. However, existing approaches often rely heavily on pre-defined\ntest cases, which become impractical in scenarios where such cases are\nunavailable. While prior works explore filtering techniques between programs\nand test cases, they overlook the refinement of test cases. To address this\nlimitation, we introduce CoCoEvo, a novel LLM-based co-evolution framework that\nsimultaneously evolves programs and test cases. CoCoEvo eliminates the\ndependency on pre-defined test cases by generating both programs and test cases\ndirectly from natural language problem descriptions and function headers. The\nframework employs specialized evolutionary operators, including LLM-based\ncrossover and mutation operators for program evolution, along with an\nadditional test case generation operator for test case evolution. Additionally,\nwe propose optimization strategies such as a crossover rate scheduler to\nbalance exploration and convergence, and a multi-objective optimization method\nfor test case selection. Experimental results on multiple state-of-the-art LLMs\ndemonstrate that CoCoEvo surpasses existing methods, achieving state-of-the-art\nperformance in automated code generation and testing. These results underscore\nthe potential of co-evolutionary techniques in advancing the field of automated\nprogramming.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.10802v2", "cate": "cs.SE", "date": "2025-02-15", "updated": "2025-07-25", "AI": {"title_translation": "CoCoEvo：程序与测试用例的协同演化以增强代码生成", "tldr": "CoCoEvo是一个基于LLM的协同演化框架，它同时生成和优化程序与测试用例，解决了对预定义测试用例的依赖，并在代码生成和测试中实现了最先进的性能。", "motivation": "现有的大语言模型代码生成方法过度依赖预定义的测试用例，但在测试用例不可用的情况下不切实际。此外，现有工作忽视了测试用例的细化。", "method": "提出CoCoEvo，一个新颖的基于LLM的协同演化框架，它同时演化程序和测试用例。CoCoEvo通过从自然语言问题描述和函数头直接生成程序和测试用例，消除了对预定义测试用例的依赖。该框架采用专门的演化算子，包括基于LLM的交叉和变异算子用于程序演化，以及一个额外的测试用例生成算子用于测试用例演化。此外，还提出了优化策略，如交叉率调度器以平衡探索和收敛，以及用于测试用例选择的多目标优化方法。", "result": "在多个最先进的LLM上进行的实验结果表明，CoCoEvo超越了现有方法，在自动化代码生成和测试中取得了最先进的性能。", "conclusion": "这些结果强调了协同演化技术在推进自动化编程领域方面的潜力。", "translation": "大型语言模型（LLMs）在自动化代码生成方面展现出卓越的性能。然而，现有方法往往严重依赖预定义的测试用例，这在这些用例不可用的场景中变得不切实际。虽然先前的研究探索了程序和测试用例之间的过滤技术，但它们忽视了测试用例的细化。为了解决这一局限性，我们引入了CoCoEvo，一个新颖的基于LLM的协同演化框架，它同时演化程序和测试用例。CoCoEvo通过直接从自然语言问题描述和函数头生成程序和测试用例，消除了对预定义测试用例的依赖。该框架采用专门的演化算子，包括基于LLM的交叉和变异算子用于程序演化，以及一个额外的测试用例生成算子用于测试用例演化。此外，我们提出了优化策略，如交叉率调度器以平衡探索和收敛，以及用于测试用例选择的多目标优化方法。在多个最先进的LLMs上进行的实验结果表明，CoCoEvo超越了现有方法，在自动化代码生成和测试中取得了最先进的性能。这些结果强调了协同演化技术在推进自动化编程领域方面的潜力。", "summary": "CoCoEvo是一个创新的LLM驱动框架，通过协同演化程序和测试用例来改进代码生成，从而摆脱了对预定义测试用例的依赖。它利用LLM进行程序的交叉和变异，并生成新的测试用例，同时采用优化策略平衡探索与收敛。实验证明CoCoEvo在自动化代码生成和测试方面优于现有方法，达到了SOTA水平。", "keywords": "代码生成, 协同演化, 大语言模型, 测试用例生成, 自动化编程", "comments": "CoCoEvo的创新之处在于其协同演化程序和测试用例的框架，这有效地解决了现有LLM代码生成对预定义测试用例的依赖问题，并引入了LLM驱动的演化操作符，具有很高的实用价值和研究潜力。"}}
{"id": "2507.19125", "title": "Learned Image Compression with Hierarchical Progressive Context Modeling", "authors": ["Yuqi Li", "Haotian Zhang", "Li Li", "Dong Liu"], "categories": ["eess.IV", "cs.CV", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      17 pages, ICCV 2025", "url": "http://arxiv.org/abs/2507.19125v1", "summary": "Context modeling is essential in learned image compression for accurately\nestimating the distribution of latents. While recent advanced methods have\nexpanded context modeling capacity, they still struggle to efficiently exploit\nlong-range dependency and diverse context information across different coding\nsteps. In this paper, we introduce a novel Hierarchical Progressive Context\nModel (HPCM) for more efficient context information acquisition. Specifically,\nHPCM employs a hierarchical coding schedule to sequentially model the\ncontextual dependencies among latents at multiple scales, which enables more\nefficient long-range context modeling. Furthermore, we propose a progressive\ncontext fusion mechanism that incorporates contextual information from previous\ncoding steps into the current step, effectively exploiting diverse contextual\ninformation. Experimental results demonstrate that our method achieves\nstate-of-the-art rate-distortion performance and strikes a better balance\nbetween compression performance and computational complexity. The code is\navailable at https://github.com/lyq133/LIC-HPCM.", "comment": "17 pages, ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.19125v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "学习型图像压缩的分层渐进上下文建模", "tldr": "本文提出一种新颖的分层渐进上下文模型（HPCM），用于学习型图像压缩，有效处理长距离依赖和多样上下文信息，实现了最先进的压缩性能和计算效率平衡。", "motivation": "现有的学习型图像压缩方法在上下文建模中难以有效利用长距离依赖和跨不同编码步骤的多样上下文信息。", "method": "本文提出一种新颖的分层渐进上下文模型（HPCM）。HPCM采用分层编码调度来顺序建模多尺度潜在变量之间的上下文依赖，以实现更高效的长距离上下文建模。此外，还提出了一种渐进上下文融合机制，将先前编码步骤的上下文信息融入当前步骤，有效利用多样上下文信息。", "result": "实验结果表明，该方法实现了最先进的率失真性能，并在压缩性能和计算复杂度之间取得了更好的平衡。", "conclusion": "HPCM通过其分层编码调度和渐进上下文融合机制，显著提升了学习型图像压缩的上下文建模效率，从而实现了优越的压缩性能和计算效率。", "translation": "上下文建模在学习型图像压缩中对于准确估计潜在变量的分布至关重要。尽管最近的先进方法已经扩展了上下文建模能力，但它们仍然难以有效地利用长距离依赖和跨不同编码步骤的多样上下文信息。在本文中，我们引入了一种新颖的分层渐进上下文模型（HPCM），以实现更高效的上下文信息获取。具体来说，HPCM采用分层编码调度来顺序建模多尺度潜在变量之间的上下文依赖，从而实现更高效的长距离上下文建模。此外，我们提出了一种渐进上下文融合机制，将先前编码步骤的上下文信息融入当前步骤，有效地利用多样上下文信息。实验结果表明，我们的方法实现了最先进的率失真性能，并在压缩性能和计算复杂度之间取得了更好的平衡。代码可在https://github.com/lyq133/LIC-HPCM获取。", "summary": "本文针对学习型图像压缩中现有上下文建模方法难以有效利用长距离依赖和多样上下文信息的问题，提出了一种新颖的分层渐进上下文模型（HPCM）。HPCM通过分层编码调度实现高效的长距离上下文建模，并通过渐进上下文融合机制有效利用多样上下文信息。实验证明，该方法在率失真性能上达到了最先进水平，并在压缩性能和计算复杂度之间取得了更好的平衡。", "keywords": "学习型图像压缩, 上下文建模, 分层模型, 渐进融合, 率失真性能", "comments": "这篇论文通过引入分层结构和渐进融合机制，解决了学习型图像压缩中长期以来存在的上下文建模效率问题，特别是在处理长距离依赖和多样上下文信息方面。其创新性在于将上下文建模分解为多尺度和多步骤的融合，从而提高了压缩效率和性能。在追求更高压缩率的同时，兼顾了计算复杂度的平衡，这对于实际应用具有重要意义。"}}
{"id": "2305.09063", "title": "Bounded KRnet and its applications to density estimation and approximation", "authors": ["Li Zeng", "Xiaoliang Wan", "Tao Zhou"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      26 pages, 16 figures", "url": "http://arxiv.org/abs/2305.09063v4", "summary": "In this paper, we develop an invertible mapping, called B-KRnet, on a bounded\ndomain and apply it to density estimation/approximation for data or the\nsolutions of PDEs such as the Fokker-Planck equation and the Keller-Segel\nequation. Similar to KRnet, B-KRnet consists of a series of coupling layers\nwith progressively fewer active transformation dimensions, inspired by the\ntriangular structure of the Knothe-Rosenblatt (KR) rearrangement. The main\ndifference between B-KRnet and KRnet is that B-KRnet is defined on a hypercube\nwhile KRnet is defined on the whole space, in other words, a new mechanism is\nintroduced in B-KRnet to maintain the exact invertibility. Using B-KRnet as a\ntransport map, we obtain an explicit probability density function (PDF) model\nthat corresponds to the pushforward of a base (uniform) distribution on the\nhypercube. It can be directly applied to density estimation when only data are\navailable. By coupling KRnet and B-KRnet, we define a deep generative model on\na high-dimensional domain where some dimensions are bounded and other\ndimensions are unbounded. A typical case is the solution of the stationary\nkinetic Fokker-Planck equation, which is a PDF of position and momentum. Based\non B-KRnet, we develop an adaptive learning approach to approximate partial\ndifferential equations whose solutions are PDFs or can be treated as PDFs. A\nvariety of numerical experiments is presented to demonstrate the effectiveness\nof B-KRnet.", "comment": "26 pages, 16 figures", "pdf_url": "http://arxiv.org/pdf/2305.09063v4", "cate": "cs.LG", "date": "2023-05-15", "updated": "2025-07-25", "AI": {"title_translation": "有界KRnet及其在密度估计和近似中的应用", "tldr": "开发了B-KRnet，一种用于有界域的可逆映射，并将其应用于密度估计和偏微分方程求解，证明了其有效性。", "motivation": "开发B-KRnet旨在将其应用于数据或偏微分方程解的密度估计/近似。主要动机是处理有界域，通过引入新机制在超立方体上保持精确可逆性，这是KRnet无法做到的。", "method": "B-KRnet是一种在有界域（超立方体）上定义的可逆映射，由受Knothe-Rosenblatt（KR）重排的三角结构启发的耦合层组成。与KRnet的主要区别在于B-KRnet引入了新机制以在超立方体上保持精确可逆性。它被用作传输映射以获得显式概率密度函数（PDF）模型。通过耦合KRnet和B-KRnet，定义了一个在高维域上的深度生成模型，其中一些维度有界，另一些无界。基于B-KRnet，还开发了一种自适应学习方法来近似其解为PDF的偏微分方程。", "result": "各种数值实验证明了B-KRnet的有效性。", "conclusion": "本文证明了B-KRnet在有界域上进行密度估计和近似（包括偏微分方程解）的有效性。", "translation": "在本文中，我们开发了一种在有界域上的可逆映射，称为B-KRnet，并将其应用于数据或偏微分方程（如Fokker-Planck方程和Keller-Segel方程）解的密度估计/近似。与KRnet类似，B-KRnet由一系列耦合层组成，这些层具有逐渐减少的活动变换维度，其灵感来自于Knothe-Rosenblatt（KR）重排的三角结构。B-KRnet和KRnet之间的主要区别在于B-KRnet定义在超立方体上，而KRnet定义在整个空间上，换句话说，B-KRnet中引入了一种新机制来保持精确可逆性。使用B-KRnet作为传输映射，我们获得了显式概率密度函数（PDF）模型，该模型对应于超立方体上基（均匀）分布的前向传播。当只有数据可用时，它可以直接应用于密度估计。通过耦合KRnet和B-KRnet，我们定义了一个在高维域上的深度生成模型，其中一些维度有界，另一些维度无界。一个典型案例是平稳动理学Fokker-Planck方程的解，它是位置和动量的PDF。基于B-KRnet，我们开发了一种自适应学习方法来近似其解为PDF或可视为PDF的偏微分方程。本文提供了各种数值实验来证明B-KRnet的有效性。", "summary": "本文介绍了B-KRnet，这是一种专为有界域设计的可逆映射，与作用于整个空间的KRnet不同。B-KRnet在超立方体上保持精确可逆性，并采用类似于Knothe-Rosenblatt重排的耦合层结构。它可用作传输映射来建模显式概率密度函数，并可直接应用于数据密度估计。此外，通过将B-KRnet与KRnet结合，作者提出了一种深度生成模型，能够处理混合有界和无界维度的高维域，例如动理学Fokker-Planck方程的解。论文还提出了一种基于B-KRnet的自适应学习方法，用于近似解为PDF的偏微分方程，并通过数值实验证明了其有效性。", "keywords": "B-KRnet, 密度估计, 传输映射, Fokker-Planck方程, 深度生成模型", "comments": "创新点在于将KRnet的概念扩展到有界域，通过B-KRnet解决了在此类设置中保持精确可逆性的挑战。这对于数据或物理现象本质上有界（例如概率密度）的应用非常重要。B-KRnet和KRnet的耦合以处理混合有界/无界维度也是一个显著的贡献，将这种传输映射方法的可应用性扩展到更复杂的高维问题，如动理学偏微分方程。"}}
{"id": "2507.18555", "title": "Neural Tangent Kernels and Fisher Information Matrices for Simple ReLU Networks with Random Hidden Weights", "authors": ["Jun'ichi Takeuchi", "Yoshinari Takeishi", "Noboru Murata", "Kazushi Mimura", "Ka Long Keith Ho", "Hiroshi Nagaoka"], "categories": ["cs.LG", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18555v2", "summary": "Fisher information matrices and neural tangent kernels (NTK) for 2-layer ReLU\nnetworks with random hidden weight are argued. We discuss the relation between\nboth notions as a linear transformation and show that spectral decomposition of\nNTK with concrete forms of eigenfunctions with major eigenvalues. We also\nobtain an approximation formula of the functions presented by the 2-layer\nneural networks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18555v2", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-25", "AI": {"title_translation": "随机隐藏权重的简单ReLU网络的神经正切核和费舍尔信息矩阵", "tldr": "本文讨论了具有随机隐藏权重的两层ReLU网络的费舍尔信息矩阵和神经正切核（NTK），阐述了它们之间的线性变换关系，展示了NTK的谱分解及其主要特征值对应的特征函数，并获得了两层神经网络表示的函数的近似公式。", "motivation": "研究随机隐藏权重的两层ReLU网络中费舍尔信息矩阵（FIM）和神经正切核（NTK）的性质及它们之间的关系。", "method": "通过讨论FIM和NTK之间的关系为线性变换，展示NTK的谱分解，并获得两层神经网络表示的函数的近似公式。", "result": "研究结果表明，FIM和NTK之间存在线性变换关系；NTK具有具体形式的特征函数和主要特征值的谱分解；获得了由两层神经网络表示的函数的近似公式。", "conclusion": "本文深入探讨了随机隐藏权重的两层ReLU网络的费舍尔信息矩阵和神经正切核，揭示了它们之间的线性变换关系，并提供了NTK的谱分解以及神经网络函数表示的近似公式。", "translation": "费舍尔信息矩阵和神经正切核（NTK）被用于讨论具有随机隐藏权重的两层ReLU网络。我们讨论了这两种概念之间的关系，将其视为线性变换，并展示了NTK的谱分解，其中包含具有主要特征值的具体特征函数形式。我们还获得了由两层神经网络表示的函数的近似公式。", "summary": "本文针对具有随机隐藏权重的两层ReLU网络，探讨了费舍尔信息矩阵（FIM）和神经正切核（NTK）的性质。研究阐明了FIM和NTK之间的线性变换关系，并给出了NTK的谱分解，包括其具体形式的特征函数和主要特征值。此外，论文还推导出了由这类两层神经网络表示的函数的近似公式。", "keywords": "神经正切核, 费舍尔信息矩阵, ReLU网络, 随机隐藏权重, 谱分解", "comments": "该论文对简单ReLU网络中的神经正切核和费舍尔信息矩阵进行了理论分析，通过揭示它们之间的线性变换关系、提供NTK的谱分解以及导出函数近似公式，深化了对神经网络工作机制的理解。这对于理解神经网络的优化景观和泛化能力具有重要意义。"}}
{"id": "2507.18807", "title": "Fishers for Free? Approximating the Fisher Information Matrix by Recycling the Squared Gradient Accumulator", "authors": ["YuXin Li", "Felix Dangel", "Derek Tam", "Colin Raffel"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      19 pages, 2 figures. Accepted as a spotlight poster at ICML 2025", "url": "http://arxiv.org/abs/2507.18807v1", "summary": "The diagonal of a model's Fisher Information Matrix (the \"Fisher diagonal\")\nhas frequently been used as a way to measure parameter sensitivity. Typically,\nthe Fisher diagonal is estimated via squared sampled gradients of the model's\nlikelihood with respect to its parameters, averaged over a few hundred or\nthousand examples -- a process which incurs nontrivial computational costs. At\nthe same time, adaptive gradient methods like the ubiquitous Adam optimizer\ncompute a moving average of the squared gradient over the course of training.\nThis paper therefore explores whether an approximation of the Fisher diagonal\ncan be obtained \"for free\" by recycling the squared gradient accumulator that\nhas already been computed over the course of training. Through a comprehensive\nset of experiments covering five applications of the Fisher diagonal, we\ndemonstrate that the \"Squisher\" (SQUared gradient accumulator as an\napproximation of the FISHER) consistently performs similarly to the Fisher\ndiagonal while outperforming baseline methods. Additionally, we clarify the\nexact differences between the Squisher and the Fisher diagonal and provide\nempirical quantification of their respective impact.", "comment": "19 pages, 2 figures. Accepted as a spotlight poster at ICML 2025", "pdf_url": "http://arxiv.org/pdf/2507.18807v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "免费的费雪信息矩阵？通过循环利用平方梯度累加器来近似费雪信息矩阵", "tldr": "本文提出“Squisher”，通过循环利用自适应优化器中的平方梯度累加器来近似费雪对角线，实现了与费雪对角线相当的性能并优于基线方法，从而“免费”获得该近似。", "motivation": "费雪信息矩阵对角线的估计计算成本高昂，而自适应梯度方法在训练过程中已计算平方梯度移动平均值。本文旨在探索是否能通过循环利用这些已计算的平方梯度累加器来“免费”近似费雪对角线，以降低计算成本。", "method": "本文研究了将自适应梯度方法（如Adam优化器）在训练过程中计算的平方梯度累加器作为费雪对角线的近似值。这种近似被命名为“Squisher”，并通过涵盖费雪对角线五种应用的全面实验进行评估。论文还阐明了Squisher与费雪对角线之间的确切差异。", "result": "通过涵盖五种应用的综合实验，结果表明“Squisher”的性能始终与费雪对角线相似，同时优于基线方法。此外，论文还澄清了Squisher和费雪对角线之间的确切差异，并提供了它们各自影响的经验量化。", "conclusion": "“Squisher”方法通过循环利用现有的平方梯度累加器，提供了一种计算上“免费”且有效的费雪对角线近似，其性能与标准费雪对角线相当，并优于基线方法。", "translation": "模型的费雪信息矩阵对角线（“费雪对角线”）经常被用来衡量参数敏感度。通常，费雪对角线是通过模型似然函数相对于其参数的平方采样梯度进行估计的，并在数百或数千个样本上进行平均——这个过程会产生不小的计算成本。与此同时，像无处不在的Adam优化器这样的自适应梯度方法在训练过程中会计算平方梯度的移动平均值。因此，本文探讨了是否可以通过循环利用在训练过程中已经计算出的平方梯度累加器来“免费”获得费雪对角线的近似值。通过涵盖费雪对角线五种应用的全面实验，我们证明了“Squisher”（作为费雪信息矩阵近似的平方梯度累加器）的性能始终与费雪对角线相似，同时优于基线方法。此外，我们澄清了Squisher和费雪对角线之间的确切差异，并提供了它们各自影响的经验量化。", "summary": "本文提出了一种名为“Squisher”的新方法，通过重新利用自适应优化器在模型训练期间已计算的平方梯度累加器来近似费雪信息矩阵的对角线。该方法旨在“免费”获取费雪对角线，从而避免传统估计方法的高昂计算成本。通过涵盖五种应用的广泛实验，Squisher被证明与费雪对角线性能相当，并优于基线方法。该研究还详细阐述了Squisher与费雪对角线之间的精确区别，并对它们各自的影响进行了经验量化。", "keywords": "费雪信息矩阵, 平方梯度, Adam优化器, 参数敏感度, 近似", "comments": "本文提出了一种创新且实用的解决方案，有效降低了费雪信息矩阵对角线估计的计算负担。通过巧妙地重用自适应优化器中已计算的值，它提供了一种“免费”的近似方法，这对于使参数敏感度分析更易于访问和高效是一个重要贡献。在多个应用中的实证验证进一步增强了其实用性。"}}
{"id": "2507.13140", "title": "RIDAS: A Multi-Agent Framework for AI-RAN with Representation- and Intention-Driven Agents", "authors": ["Kuiyuan Ding", "Caili Guo", "Yang Yang", "Jianzhang Guo"], "categories": ["cs.NI"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "Comments:      6 pages, 7 figures", "url": "http://arxiv.org/abs/2507.13140v2", "summary": "Sixth generation (6G) networks demand tight integration of artificial\nintelligence (AI) into radio access networks (RANs) to meet stringent quality\nof service (QoS) and resource efficiency requirements. Existing solutions\nstruggle to bridge the gap between high level user intents and the low level,\nparameterized configurations required for optimal performance. To address this\nchallenge, we propose RIDAS, a multi agent framework composed of representation\ndriven agents (RDAs) and an intention driven agent (IDA). RDAs expose open\ninterface with tunable control parameters (rank and quantization bits, enabling\nexplicit trade) offs between distortion and transmission rate. The IDA employs\na two stage planning scheme (bandwidth pre allocation and reallocation) driven\nby a large language model (LLM) to map user intents and system state into\noptimal RDA configurations. Experiments demonstrate that RIDAS supports 44.71\\%\nmore users than WirelessAgent under equivalent QoS constraints. These results\nvalidate ability of RIDAS to capture user intent and allocate resources more\nefficiently in AI RAN environments.", "comment": "6 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.13140v2", "cate": "cs.NI", "date": "2025-07-17", "updated": "2025-07-25", "AI": {"title_translation": "RIDAS：一种基于表示驱动和意图驱动代理的AI-RAN多智能体框架", "tldr": "RIDAS是一个多智能体框架，通过表示驱动代理和意图驱动代理，将用户意图映射到无线资源配置，以提高AI-RAN中的资源效率和用户支持数量。", "motivation": "现有解决方案难以弥合高级用户意图与低级参数化配置之间的差距，导致在AI-RAN中难以满足严格的服务质量（QoS）和资源效率要求。", "method": "RIDAS框架包含表示驱动代理（RDAs）和意图驱动代理（IDA）。RDAs提供可调谐的控制参数（秩和量化比特），实现失真和传输速率之间的权衡。IDA采用两阶段规划方案（带宽预分配和重新分配），由大型语言模型（LLM）驱动，将用户意图和系统状态映射到最优的RDA配置。", "result": "实验表明，在同等QoS约束下，RIDAS比WirelessAgent支持的用户数量多44.71%。", "conclusion": "RIDAS能够捕捉用户意图并更有效地在AI-RAN环境中分配资源，从而提高用户支持数量。", "translation": "第六代（6G）网络要求人工智能（AI）与无线接入网络（RAN）紧密集成，以满足严格的服务质量（QoS）和资源效率要求。现有解决方案难以弥合高级用户意图与低级参数化配置之间的差距，从而实现最佳性能。为了解决这一挑战，我们提出了RIDAS，一个由表示驱动代理（RDAs）和意图驱动代理（IDA）组成的多智能体框架。RDAs提供开放接口，带有可调谐的控制参数（秩和量化比特），从而实现失真和传输速率之间的明确权衡。IDA采用由大型语言模型（LLM）驱动的两阶段规划方案（带宽预分配和重新分配），将用户意图和系统状态映射到最优的RDA配置。实验证明，在同等QoS约束下，RIDAS比WirelessAgent支持的用户数量多44.71%。这些结果验证了RIDAS在AI-RAN环境中捕捉用户意图和更有效分配资源的能力。", "summary": "本文提出了RIDAS，一个针对AI-RAN的多智能体框架，旨在解决用户意图与网络配置之间的鸿沟。该框架包含表示驱动代理（RDAs）和意图驱动代理（IDA），其中RDAs处理低级参数配置，而IDA利用大型语言模型将用户意图转化为最优的RDA配置。实验结果表明，RIDAS在相同QoS下比现有方案支持更多用户，验证了其在AI-RAN中有效捕捉用户意图和优化资源分配的能力。", "keywords": "AI-RAN, 多智能体框架, 6G网络, 资源分配, 大型语言模型", "comments": "RIDAS的创新之处在于其独特的多智能体架构，特别是结合了表示驱动代理和由LLM驱动的意图驱动代理，有效弥合了高级用户意图与低级网络配置之间的差距，为6G网络中的AI-RAN资源管理提供了新的思路。"}}
{"id": "2507.19327", "title": "Real-time rail vehicle localisation using spatially resolved magnetic field measurements", "authors": ["Niklas Dieckow", "Katharina Ostaszewski", "Philip Heinisch", "Henriette Struckmann", "Hendrik Ranocha"], "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19327v1", "summary": "This work presents two complementary real-time rail vehicle localization\nmethods based on magnetic field measurements and a pre-recorded magnetic map.\nThe first uses a particle filter reweighted via magnetic similarity, employing\na heavy-tailed non-Gaussian kernel for enhanced stability. The second is a\nstateless sequence alignment technique that transforms real-time magnetic\nsignals into the spatial domain and matches them to the map using a similarity\nmeasure. Experiments with operational train data show that the particle filter\nachieves track-selective, sub-5-meter accuracy over 21.6 km, though its\nperformance degrades at low speeds and during cold starts. Accuracy tests were\nconstrained by the GNSS-based reference system. In contrast, the\nalignment-based method excels in cold-start scenarios, localizing within 30 m\nin 92 % of tests (100 % using top-3 matches). A hybrid approach combines both\nmethods$\\unicode{x2014}$alignment-based initialization followed by particle\nfilter tracking. Runtime analysis confirms real-time capability on\nconsumer-grade hardware. The system delivers accurate, robust localization\nsuitable for safety-critical rail applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19327v1", "cate": "eess.SP", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "使用空间分辨磁场测量的实时轨道车辆定位", "tldr": "本文提出了两种基于磁场测量的实时轨道车辆定位方法（粒子滤波器和序列对齐），并结合为混合方法，在实际列车数据上验证了其准确性和鲁棒性。", "motivation": "旨在开发一种基于磁场测量的实时轨道车辆定位系统，以满足安全关键型轨道应用的需求。", "method": "提出了两种互补的实时轨道车辆定位方法：一种是基于磁场相似性重加权的粒子滤波器，采用重尾非高斯核以增强稳定性；另一种是无状态序列对齐技术，将实时磁信号转换为空间域并与预记录的磁图匹配。还提出了一种混合方法，结合了对齐初始化和粒子滤波跟踪。", "result": "粒子滤波器在21.6公里范围内实现了轨道选择性、亚5米精度，但在低速和冷启动时性能下降。对齐方法在冷启动场景表现出色，92%的测试中定位精度在30米以内（使用前3名匹配时为100%）。混合方法结合了两种优势。系统在消费级硬件上实现了实时运行。", "conclusion": "该系统提供了准确、鲁棒的定位能力，适用于安全关键型轨道应用。", "translation": "这项工作提出了两种基于磁场测量和预记录磁图的互补实时轨道车辆定位方法。第一种方法使用通过磁场相似性进行重加权的粒子滤波器，采用重尾非高斯核以增强稳定性。第二种方法是一种无状态序列对齐技术，它将实时磁信号转换为空间域，并使用相似性度量将其与地图匹配。使用运行列车数据的实验表明，粒子滤波器在21.6公里范围内实现了轨道选择性、亚5米精度，但其性能在低速和冷启动时会下降。精度测试受到基于GNSS的参考系统的限制。相比之下，基于对齐的方法在冷启动场景中表现出色，在92%的测试中定位精度在30米以内（使用前3名匹配时为100%）。一种混合方法结合了两种方法——基于对齐的初始化，然后是粒子滤波跟踪。运行时分析证实了在消费级硬件上的实时能力。该系统提供了准确、鲁棒的定位，适用于安全关键型轨道应用。", "summary": "本文提出了两种基于磁场测量和预记录磁图的实时轨道车辆定位方法：一种是鲁棒的粒子滤波器，另一种是序列对齐技术。通过结合两种方法的混合方法，实验证明该系统在实际列车数据上实现了高精度（粒子滤波器亚5米，对齐方法冷启动30米内）和实时性能，适用于安全关键型轨道应用。", "keywords": "轨道车辆定位, 磁场测量, 粒子滤波器, 序列对齐, 实时", "comments": "本文的创新点在于利用空间分辨磁场测量进行轨道定位，并提出了两种互补的方法（粒子滤波器和序列对齐），以及一个结合两者优点的混合方案。对齐方法在冷启动场景下的鲁棒性是一个显著优势。系统在消费级硬件上实现实时运行，也突出了其实用性。局限性方面，粒子滤波器在低速和冷启动时性能下降，但混合方法有效弥补了这一点。"}}
{"id": "2507.19328", "title": "NerT-CA: Efficient Dynamic Reconstruction from Sparse-view X-ray Coronary Angiography", "authors": ["Kirsten W. H. Maas", "Danny Ruijters", "Nicola Pezzotti", "Anna Vilanova"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19328v1", "summary": "Three-dimensional (3D) and dynamic 3D+time (4D) reconstruction of coronary\narteries from X-ray coronary angiography (CA) has the potential to improve\nclinical procedures. However, there are multiple challenges to be addressed,\nmost notably, blood-vessel structure sparsity, poor background and blood vessel\ndistinction, sparse-views, and intra-scan motion. State-of-the-art\nreconstruction approaches rely on time-consuming manual or error-prone\nautomatic segmentations, limiting clinical usability. Recently, approaches\nbased on Neural Radiance Fields (NeRF) have shown promise for automatic\nreconstructions in the sparse-view setting. However, they suffer from long\ntraining times due to their dependence on MLP-based representations. We propose\nNerT-CA, a hybrid approach of Neural and Tensorial representations for\naccelerated 4D reconstructions with sparse-view CA. Building on top of the\nprevious NeRF-based work, we model the CA scene as a decomposition of low-rank\nand sparse components, utilizing fast tensorial fields for low-rank static\nreconstruction and neural fields for dynamic sparse reconstruction. Our\napproach outperforms previous works in both training time and reconstruction\naccuracy, yielding reasonable reconstructions from as few as three angiogram\nviews. We validate our approach quantitatively and qualitatively on\nrepresentative 4D phantom datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19328v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "NerT-CA：基于稀疏视图X射线冠状动脉造影的高效动态重建", "tldr": "NerT-CA是一种结合神经场和张量场的新方法，用于从稀疏视图X射线冠状动脉造影中进行快速准确的4D冠状动脉重建，解决了现有方法训练时间长和分割困难的问题。", "motivation": "X射线冠状动脉造影的3D和4D重建具有改善临床操作的潜力。然而，现有方法面临血管结构稀疏性、背景与血管区分度差、稀疏视图和扫描内运动等挑战。最先进的重建方法依赖耗时的人工或易错的自动分割，限制了临床可用性。基于神经辐射场（NeRF）的方法虽然对稀疏视图重建有前景，但由于依赖MLP表示而训练时间长。", "method": "我们提出了NerT-CA，一种结合神经场和张量表示的混合方法，用于加速稀疏视图冠状动脉造影的4D重建。该方法在现有NeRF工作的基础上，将冠状动脉造影场景建模为低秩和稀疏分量的分解，利用快速张量场进行低秩静态重建，并利用神经场进行动态稀疏重建。", "result": "NerT-CA在训练时间和重建精度上均优于现有工作，仅需三张血管造影视图即可实现合理的重建。该方法在代表性的4D体模数据集上进行了定量和定性验证。", "conclusion": "NerT-CA通过结合神经和张量表示，有效地解决了稀疏视图X射线冠状动脉造影的4D重建中的挑战，实现了更快的训练速度和更高的重建精度，有望提高临床应用潜力。", "translation": "三维（3D）和动态3D+时间（4D）冠状动脉重建从X射线冠状动脉造影（CA）中提取，有潜力改善临床操作。然而，存在多个挑战需要解决，最显著的是血管结构稀疏性、背景和血管区分度差、稀疏视图以及扫描内运动。最先进的重建方法依赖耗时的人工或易错的自动分割，限制了临床可用性。最近，基于神经辐射场（NeRF）的方法在稀疏视图设置下显示出自动重建的潜力。然而，由于它们依赖基于MLP的表示，导致训练时间长。我们提出了NerT-CA，一种结合神经和张量表示的混合方法，用于加速稀疏视图CA的4D重建。在之前的基于NeRF的工作基础上，我们将CA场景建模为低秩和稀疏分量的分解，利用快速张量场进行低秩静态重建，并利用神经场进行动态稀疏重建。我们的方法在训练时间和重建精度上均优于现有工作，仅需三张血管造影视图即可实现合理的重建。我们在代表性的4D体模数据集上对我们的方法进行了定量和定性验证。", "summary": "本文提出NerT-CA，一种新颖的混合方法，结合神经和张量表示，旨在从稀疏视图X射线冠状动脉造影中高效地进行4D冠状动脉动态重建。该方法通过将场景分解为低秩静态和动态稀疏分量，利用张量场加速静态重建，神经场处理动态重建，有效解决了现有NeRF方法训练时间长以及稀疏视图重建的挑战。实验结果表明，NerT-CA在训练时间和重建精度上均优于现有方法，甚至只需三张视图即可实现高质量重建。", "keywords": "冠状动脉造影, 4D重建, 稀疏视图, 神经辐射场, 张量表示", "comments": "NerT-CA的创新之处在于其混合的神经和张量表示，有效结合了两种方法的优势：张量场的高效性和神经场的表达能力。这种分解策略显著缩短了训练时间，并提高了稀疏视图下的重建精度，对于临床应用具有重要意义，因为它能从极少量的图像中获取高质量的4D信息，减少患者辐射暴露并提高效率。"}}
{"id": "2503.16289", "title": "SceneMI: Motion In-betweening for Modeling Human-Scene Interactions", "authors": ["Inwoo Hwang", "Bing Zhou", "Young Min Kim", "Jian Wang", "Chuan Guo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025. Project page: this http URL", "url": "http://arxiv.org/abs/2503.16289v2", "summary": "Modeling human-scene interactions (HSI) is essential for understanding and\nsimulating everyday human behaviors. Recent approaches utilizing generative\nmodeling have made progress in this domain; however, they are limited in\ncontrollability and flexibility for real-world applications. To address these\nchallenges, we propose reformulating the HSI modeling problem as Scene-aware\nMotion In-betweening - a more tractable and practical task. We introduce\nSceneMI, a framework that supports several practical applications, including\nkeyframe-guided character animation in 3D scenes and enhancing the motion\nquality of imperfect HSI data. SceneMI employs dual scene descriptors to\ncomprehensively encode global and local scene context. Furthermore, our\nframework leverages the inherent denoising nature of diffusion models to\ngeneralize on noisy keyframes. Experimental results demonstrate SceneMI's\neffectiveness in scene-aware keyframe in-betweening and generalization to the\nreal-world GIMO dataset, where motions and scenes are acquired by noisy IMU\nsensors and smartphones. We further showcase SceneMI's applicability in HSI\nreconstruction from monocular videos.", "comment": "Accepted to ICCV 2025. Project page: http://inwoohwang.me/SceneMI", "pdf_url": "http://arxiv.org/pdf/2503.16289v2", "cate": "cs.CV", "date": "2025-03-20", "updated": "2025-07-25", "AI": {"title_translation": "SceneMI：用于建模人-场景交互的运动中间帧生成", "tldr": "SceneMI是一个新的框架，通过将人-场景交互建模重构为场景感知运动中间帧生成，解决了现有方法的局限性，并支持关键帧动画和提升运动质量。", "motivation": "现有的人-场景交互（HSI）建模方法在可控性和灵活性方面存在局限性，不适用于实际应用。", "method": "本文提出将人-场景交互建模问题重新定义为“场景感知运动中间帧生成”——一个更易处理和实用的任务。引入了SceneMI框架，该框架采用双重场景描述符来全面编码全局和局部场景上下文，并利用扩散模型固有的去噪特性来泛化处理噪声关键帧。", "result": "实验结果表明，SceneMI在场景感知关键帧中间帧生成方面表现出有效性，并能泛化到真实世界的GIMO数据集（其中的运动和场景是通过噪声IMU传感器和智能手机获取的）。此外，还展示了SceneMI在单目视频中进行人-场景交互重建的适用性。", "conclusion": "SceneMI通过将人-场景交互建模重新表述为场景感知运动中间帧生成，并结合双重场景描述符和扩散模型，为复杂的人-场景交互建模提供了有效且实用的解决方案，支持关键帧动画和提升数据质量等多种实际应用。", "translation": "建模人-场景交互（HSI）对于理解和模拟日常人类行为至关重要。最近利用生成建模的方法在该领域取得了进展；然而，它们在实际应用中的可控性和灵活性方面存在局限性。为了解决这些挑战，我们提出将HSI建模问题重新表述为场景感知运动中间帧生成——一个更易处理和实用的任务。我们介绍了SceneMI，一个支持多种实际应用的框架，包括3D场景中的关键帧引导角色动画以及提升不完美HSI数据的运动质量。SceneMI采用双重场景描述符来全面编码全局和局部场景上下文。此外，我们的框架利用扩散模型固有的去噪特性来泛化处理噪声关键帧。实验结果表明，SceneMI在场景感知关键帧中间帧生成方面表现出有效性，并能泛化到真实世界的GIMO数据集，该数据集中的运动和场景是通过噪声IMU传感器和智能手机获取的。我们进一步展示了SceneMI在单目视频中进行HSI重建的适用性。", "summary": "本文提出了SceneMI框架，通过将人-场景交互（HSI）建模重新定义为“场景感知运动中间帧生成”来解决现有方法的局限性。SceneMI利用双重场景描述符捕获场景上下文，并利用扩散模型的去噪能力处理噪声数据。实验证明，SceneMI在关键帧中间帧生成方面表现出色，并能泛化到真实世界的噪声数据集，还可应用于单目视频的HSI重建。", "keywords": "人-场景交互, 运动中间帧生成, 扩散模型, 关键帧动画, 场景感知", "comments": "SceneMI的创新之处在于将HSI建模问题重新构想为运动中间帧生成，这提供了一种更具可控性和实用性的方法。其利用双重场景描述符和扩散模型的结合，增强了对复杂场景和噪声数据的处理能力，使其在真实世界应用中具有重要潜力。"}}
{"id": "2507.18802", "title": "DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition", "authors": ["Danqing Shi", "Furui Cheng", "Tino Weinkauf", "Antti Oulasvirta", "Mennatallah El-Assady"], "categories": ["cs.HC", "cs.AI"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18802v1", "summary": "Human preferences are widely used to align large language models (LLMs)\nthrough methods such as reinforcement learning from human feedback (RLHF).\nHowever, the current user interfaces require annotators to compare text\nparagraphs, which is cognitively challenging when the texts are long or\nunfamiliar. This paper contributes by studying the decomposition principle as\nan approach to improving the quality of human feedback for LLM alignment. This\napproach breaks down the text into individual claims instead of directly\ncomparing two long-form text responses. Based on the principle, we build a\nnovel user interface DxHF. It enhances the comparison process by showing\ndecomposed claims, visually encoding the relevance of claims to the\nconversation and linking similar claims. This allows users to skim through key\ninformation and identify differences for better and quicker judgment. Our\ntechnical evaluation shows evidence that decomposition generally improves\nfeedback accuracy regarding the ground truth, particularly for users with\nuncertainty. A crowdsourcing study with 160 participants indicates that using\nDxHF improves feedback accuracy by an average of 5%, although it increases the\naverage feedback time by 18 seconds. Notably, accuracy is significantly higher\nin situations where users have less certainty. The finding of the study\nhighlights the potential of HCI as an effective method for improving human-AI\nalignment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18802v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "DxHF：通过交互式分解为大型语言模型对齐提供高质量的人类反馈", "tldr": "DxHF通过将文本分解为单独的主张，提供了一种新的用户界面来改进LLM对齐中的人类反馈质量，实验证明其能提高反馈准确性，尤其是在用户不确定时。", "motivation": "当前用于大型语言模型（LLM）对齐的人类反馈界面要求标注者比较文本段落，当文本较长或不熟悉时，这在认知上具有挑战性，导致反馈质量受限。", "method": "本研究提出并研究了分解原则，即将长文本分解为单独的主张，而不是直接比较两个长篇文本响应。基于此原则，开发了一个名为DxHF的新型用户界面，通过展示分解后的主张、视觉编码主张与对话的相关性以及链接相似主张来增强比较过程。", "result": "技术评估显示，分解通常能提高反馈准确性，特别是对于不确定的用户。一项有160名参与者的众包研究表明，使用DxHF平均提高了5%的反馈准确性，尽管平均反馈时间增加了18秒。值得注意的是，在用户不太确定的情况下，准确性显著更高。", "conclusion": "研究结果强调了人机交互（HCI）作为改进人机对齐的有效方法的潜力。", "translation": "人类偏好被广泛用于通过强化学习（RLHF）等方法来对齐大型语言模型（LLM）。然而，当前的用户界面要求标注者比较文本段落，当文本较长或不熟悉时，这在认知上具有挑战性。本文通过研究分解原则，作为提高LLM对齐中人类反馈质量的一种方法。这种方法将文本分解为单独的主张，而不是直接比较两个长篇文本响应。基于该原则，我们构建了一个新颖的用户界面DxHF。它通过显示分解后的主张、视觉编码主张与对话的相关性以及链接相似主张来增强比较过程。这使得用户能够快速浏览关键信息并识别差异，从而做出更好、更快的判断。我们的技术评估显示，分解通常能提高反馈准确性，特别是对于不确定的用户。一项有160名参与者的众包研究表明，使用DxHF平均提高了5%的反馈准确性，尽管平均反馈时间增加了18秒。值得注意的是，在用户不太确定的情况下，准确性显著更高。这项研究的发现突显了人机交互（HCI）作为改进人机对齐的有效方法的潜力。", "summary": "该论文介绍了一种名为DxHF的新型用户界面，旨在通过“分解原则”改进大型语言模型（LLM）对齐中的人类反馈质量。该方法将长文本响应分解为独立的主张进行比较，从而降低认知负担。技术评估和一项众包研究表明，DxHF提高了反馈准确性，尤其是在用户不确定时，尽管反馈时间略有增加。研究强调了人机交互在提升人机对齐方面的潜力。", "keywords": "LLM对齐, 人类反馈, 交互式分解, 用户界面, HCI", "comments": "DxHF通过引入交互式分解这一创新方法，有效解决了现有LLM对齐中人类反馈界面认知负担过重的问题。其核心价值在于将复杂的文本比较任务简化为对单个主张的判断，并通过可视化和链接相似主张来优化用户体验。研究结果证明了该方法在提高反馈准确性方面的有效性，特别是在用户不确定时的显著提升，这对于需要高质量、细粒度人类反馈的LLM对齐任务至关重要。尽管反馈时间略有增加，但准确性的提升使得这种权衡是值得的。这篇论文也突显了人机交互设计在人工智能发展中的重要作用。"}}
{"id": "2507.19133", "title": "3DGauCIM: Accelerating Static/Dynamic 3D Gaussian Splatting via Digital CIM for High Frame Rate Real-Time Edge Rendering", "authors": ["Wei-Hsing Huang", "Cheng-Jhih Shih", "Jian-Wei Su", "Samuel Wade Wang", "Vaidehi Garg", "Yuyao Kong", "Jen-Chun Tien", "Nealson Li", "Arijit Raychowdhury", "Meng-Fan Chang", "Yingyan", "Lin", "Shimeng Yu"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19133v1", "summary": "Dynamic 3D Gaussian splatting (3DGS) extends static 3DGS to render dynamic\nscenes, enabling AR/VR applications with moving objects. However, implementing\ndynamic 3DGS on edge devices faces challenges: (1) Loading all Gaussian\nparameters from DRAM for frustum culling incurs high energy costs. (2)\nIncreased parameters for dynamic scenes elevate sorting latency and energy\nconsumption. (3) Limited on-chip buffer capacity with higher parameters reduces\nbuffer reuse, causing frequent DRAM access. (4) Dynamic 3DGS operations are not\nreadily compatible with digital compute-in-memory (DCIM). These challenges\nhinder real-time performance and power efficiency on edge devices, leading to\nreduced battery life or requiring bulky batteries. To tackle these challenges,\nwe propose algorithm-hardware co-design techniques. At the algorithmic level,\nwe introduce three optimizations: (1) DRAM-access reduction frustum culling to\nlower DRAM access overhead, (2) Adaptive tile grouping to enhance on-chip\nbuffer reuse, and (3) Adaptive interval initialization Bucket-Bitonic sort to\nreduce sorting latency. At the hardware level, we present a DCIM-friendly\ncomputation flow that is evaluated using the measured data from a 16nm DCIM\nprototype chip. Our experimental results on Large-Scale Real-World\nStatic/Dynamic Datasets demonstrate the ability to achieve high frame rate\nreal-time rendering exceeding 200 frame per second (FPS) with minimal power\nconsumption, merely 0.28 W for static Large-Scale Real-World scenes and 0.63 W\nfor dynamic Large-Scale Real-World scenes. This work successfully addresses the\nsignificant challenges of implementing static/dynamic 3DGS technology on\nresource-constrained edge devices.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19133v1", "cate": "cs.AR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "3DGauCIM：通过数字CIM加速静态/动态3D高斯泼溅实现高帧率实时边缘渲染", "tldr": "3DGauCIM通过算法-硬件协同设计，在边缘设备上实现了高帧率、低功耗的静态/动态3D高斯泼溅渲染。", "motivation": "在边缘设备上实现动态3D高斯泼溅面临多项挑战：从DRAM加载高斯参数进行视锥体剔除导致高能耗；动态场景参数增加导致排序延迟和能耗升高；片上缓冲区容量有限导致复用率降低和频繁DRAM访问；动态3DGS操作与数字存内计算（DCIM）不兼容。这些问题阻碍了边缘设备的实时性能和能效，导致电池续航缩短或需要大型电池。", "method": "本文提出了算法-硬件协同设计技术。在算法层面，引入了三种优化：(1) 减少DRAM访问的视锥体剔除，(2) 自适应瓦片分组以增强片上缓冲区复用，(3) 自适应间隔初始化桶式-比特位排序以减少排序延迟。在硬件层面，提出了一个DCIM友好的计算流程，并使用16nm DCIM原型芯片的实测数据进行了评估。", "result": "在大型真实世界静态/动态数据集上的实验结果表明，该方法能够实现超过200帧/秒（FPS）的高帧率实时渲染，同时功耗极低，静态大型真实世界场景仅0.28 W，动态大型真实世界场景仅0.63 W。", "conclusion": "这项工作成功解决了在资源受限的边缘设备上实现静态/动态3D高斯泼溅技术的重大挑战。", "translation": "动态3D高斯泼溅（3DGS）将静态3DGS扩展到渲染动态场景，从而使AR/VR应用能够处理移动物体。然而，在边缘设备上实现动态3DGS面临挑战：(1) 从DRAM加载所有高斯参数进行视锥体剔除会产生高能耗。(2) 动态场景参数的增加会提高排序延迟和能耗。(3) 参数增加导致片上缓冲区容量有限，降低了缓冲区复用率，导致频繁DRAM访问。(4) 动态3DGS操作不易与数字存内计算（DCIM）兼容。这些挑战阻碍了边缘设备的实时性能和能效，导致电池续航时间缩短或需要体积庞大的电池。为了解决这些挑战，我们提出了算法-硬件协同设计技术。在算法层面，我们引入了三种优化：(1) 减少DRAM访问的视锥体剔除以降低DRAM访问开销，(2) 自适应瓦片分组以增强片上缓冲区复用，(3) 自适应间隔初始化桶式-比特位排序以减少排序延迟。在硬件层面，我们提出了一个DCIM友好的计算流程，并使用16nm DCIM原型芯片的实测数据进行了评估。我们在大型真实世界静态/动态数据集上的实验结果表明，该方法能够实现超过200帧/秒（FPS）的高帧率实时渲染，同时功耗极低，静态大型真实世界场景仅0.28 W，动态大型真实世界场景仅0.63 W。这项工作成功解决了在资源受限的边缘设备上实现静态/动态3DGS技术的重大挑战。", "summary": "本文提出了一种名为3DGauCIM的算法-硬件协同设计方案，旨在解决在边缘设备上实现静态/动态3D高斯泼溅（3DGS）所面临的能耗高、延迟大、内存访问频繁以及与DCIM不兼容等挑战。通过DRAM访问减少的视锥体剔除、自适应瓦片分组和自适应间隔初始化桶式-比特位排序等算法优化，以及DCIM友好的计算流程，该方案在大型真实世界数据集上实现了超过200 FPS的高帧率实时渲染，同时保持极低的功耗（静态场景0.28W，动态场景0.63W），成功推动了3DGS技术在资源受限边缘设备上的应用。", "keywords": "3D高斯泼溅, 边缘渲染, DCIM, 算法-硬件协同设计, 实时性能", "comments": "本文通过创新的算法-硬件协同设计，针对3D高斯泼溅在边缘设备上的性能和能耗瓶颈提出了有效的解决方案。其亮点在于将特定算法优化（如DRAM访问减少、自适应分组和改进排序）与数字存内计算（DCIM）硬件架构紧密结合，显著提升了实时渲染能力并降低了功耗。这对于AR/VR等需要高性能边缘计算的应用具有重要意义，展示了在资源受限环境下实现复杂图形渲染的潜力。"}}
{"id": "2507.18927", "title": "A Fingerprint Database Generation Method for RIS-Assisted Indoor Positioning", "authors": ["Xin Cheng", "Yu He", "Menglu Li", "Ruoguang Li", "Feng Shu", "Guangjie Han"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18927v1", "summary": "Reconfigurable intelligent surface (RIS) has emerged as a promising\ntechnology to enhance indoor wireless communication and sensing performance.\nHowever, the construction of reliable received signal strength (RSS)-based\nfingerprint databases for RIS-assisted indoor positioning remains an open\nchallenge due to the lack of realistic and spatially consistent channel\nmodeling methods. In this paper, we propose a novel method with open-source\ncodes for generating RIS-assisted RSS fingerprint databases. Our method\ncaptures the complex RIS-assisted multipath behaviors by extended cluster-based\nchannel modeling and the physical and electromagnetic properties of RIS and\ntransmitter (Tx). And the spatial consistency is incorporated when simulating\nthe fingerprint data collection across neighboring positions. Furthermore, the\nproposed method offers exceptional flexibility in configuring RIS and Tx\nparameters. Extensive simulations are conducted to evaluate the fingerprint\ndatabase generated by the proposed method. Moreover, the positioning\nperformance on the database using K-nearest neighbors (KNN) and deep neural\nnetwork (DNN) is analyzed, providing valuable insights for the system design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18927v1", "cate": "eess.SP", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "一种用于RIS辅助室内定位的指纹数据库生成方法", "tldr": "该论文提出了一种新的、开源的方法，用于生成RIS辅助室内定位的可靠RSS指纹数据库，解决了空间一致性信道建模的挑战。", "motivation": "由于缺乏真实且空间一致的信道建模方法，为RIS辅助室内定位构建可靠的基于接收信号强度（RSS）的指纹数据库仍然是一个开放的挑战。", "method": "本文提出了一种新的、带有开源代码的方法，用于生成RIS辅助RSS指纹数据库。该方法通过扩展的基于簇的信道建模，以及RIS和发射器（Tx）的物理和电磁特性来捕获复杂的RIS辅助多径行为。在模拟相邻位置的指纹数据收集时，还考虑了空间一致性。此外，所提出的方法在配置RIS和Tx参数方面提供了卓越的灵活性。", "result": "进行了广泛的仿真以评估所提出方法生成的指纹数据库。此外，还分析了使用K近邻（KNN）和深度神经网络（DNN）在该数据库上的定位性能，为系统设计提供了有价值的见解。", "conclusion": "所提出的方法有效地生成了RIS辅助RSS指纹数据库，并且对这些数据库上的定位性能进行分析，为系统设计提供了有价值的见解。", "translation": "可重构智能表面（RIS）已成为一种有前景的技术，可增强室内无线通信和传感性能。然而，由于缺乏真实且空间一致的信道建模方法，为RIS辅助室内定位构建可靠的基于接收信号强度（RSS）的指纹数据库仍然是一个开放的挑战。在本文中，我们提出了一种新的方法，并提供了开源代码，用于生成RIS辅助RSS指纹数据库。我们的方法通过扩展的基于簇的信道建模以及RIS和发射器（Tx）的物理和电磁特性来捕获复杂的RIS辅助多径行为。在模拟相邻位置的指纹数据收集时，还考虑了空间一致性。此外，所提出的方法在配置RIS和Tx参数方面提供了卓越的灵活性。进行了广泛的仿真以评估所提出方法生成的指纹数据库。此外，还分析了使用K近邻（KNN）和深度神经网络（DNN）在该数据库上的定位性能，为系统设计提供了有价值的见解。", "summary": "本文提出了一种新颖的、开源的方法，用于为RIS辅助室内定位生成可靠的RSS指纹数据库。它通过结合扩展的基于簇的信道建模、RIS/Tx物理特性以及数据仿真过程中的空间一致性来解决空间一致性信道建模的挑战。该方法在参数配置方面具有灵活性。仿真验证了数据库的有效性，并且对KNN和DNN在该数据库上的定位性能分析为系统设计提供了见解。", "keywords": "RIS, 室内定位, 指纹数据库, RSS, 信道建模", "comments": "该论文通过为指纹数据库生成提供实用的解决方案，解决了RIS辅助室内定位中的一个关键挑战。对真实信道建模、空间一致性和开源可用性的强调使其具有重要价值。使用常用算法（KNN、DNN）分析定位性能增加了实用相关性，并为未来的系统开发提供了见解。"}}
{"id": "2507.19090", "title": "Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents", "authors": ["Haorui He", "Yupeng Li", "Dacheng Wen", "Reynold Cheng", "Francis C. M. Lau"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19090v1", "summary": "Claim verification is critical for enhancing digital literacy. However, the\nstate-of-the-art single-LLM methods struggle with complex claim verification\nthat involves multi-faceted evidences. Inspired by real-world fact-checking\npractices, we propose DebateCV, the first claim verification framework that\nadopts a debate-driven methodology using multiple LLM agents. In our framework,\ntwo Debaters take opposing stances on a claim and engage in multi-round\nargumentation, while a Moderator evaluates the arguments and renders a verdict\nwith justifications. To further improve the performance of the Moderator, we\nintroduce a novel post-training strategy that leverages synthetic debate data\ngenerated by the zero-shot DebateCV, effectively addressing the scarcity of\nreal-world debate-driven claim verification data. Experimental results show\nthat our method outperforms existing claim verification methods under varying\nlevels of evidence quality. Our code and dataset are publicly available at\nhttps://anonymous.4open.science/r/DebateCV-6781.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19090v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "辩论真相：基于多大型语言模型代理的辩论驱动式事实核查", "tldr": "提出DebateCV，一个多LLM代理的辩论驱动式事实核查框架，通过模拟辩论和合成数据训练，在复杂事实核查中优于现有方法。", "motivation": "当前单一大型语言模型在处理涉及多方面证据的复杂事实核查时表现不佳，难以有效提升数字素养。", "method": "提出DebateCV框架，该框架采用多LLM代理的辩论驱动方法。其中，两个“辩论者”对同一主张采取对立立场进行多轮论证，而一个“主持人”评估论证并给出带理由的裁决。为了提升主持人的性能，引入了一种利用零样本DebateCV生成的合成辩论数据进行后训练的新策略，以解决真实世界辩论驱动事实核查数据稀缺的问题。", "result": "实验结果表明，该方法在不同证据质量水平下均优于现有的事实核查方法。", "conclusion": "该研究成功开发并验证了DebateCV框架，通过引入多LLM代理的辩论机制和创新的后训练策略，显著提升了复杂事实核查的性能，为数字素养的提升提供了有效工具。", "translation": "事实核查对于提升数字素养至关重要。然而，最先进的单一大型语言模型（LLM）方法难以应对涉及多方面证据的复杂事实核查。受现实世界事实核查实践的启发，我们提出了DebateCV，这是第一个采用多LLM代理辩论驱动方法的事实核查框架。在我们的框架中，两名“辩论者”对一个主张采取对立立场并进行多轮论证，同时一名“主持人”评估论证并给出带理由的裁决。为了进一步提高主持人的性能，我们引入了一种新颖的后训练策略，该策略利用零样本DebateCV生成的合成辩论数据，有效地解决了真实世界辩论驱动事实核查数据稀缺的问题。实验结果表明，我们的方法在不同证据质量水平下均优于现有的事实核查方法。我们的代码和数据集已公开可用。", "summary": "本文提出了DebateCV，一个创新的多大型语言模型（LLM）代理框架，旨在解决现有单一LLM在复杂事实核查中遇到的挑战。受现实世界事实核查启发，DebateCV通过模拟辩论过程，让两个LLM“辩论者”进行多轮论证，并由一个“主持人”裁决。为克服数据稀缺，研究引入了利用零样本DebateCV生成的合成数据进行主持人后训练的策略。实验证明，DebateCV在不同证据质量下均优于现有事实核查方法。", "keywords": "事实核查, 大型语言模型, 多代理系统, 辩论驱动, 合成数据", "comments": "这篇论文通过引入多代理辩论机制，为复杂事实核查提供了一个新颖且直观的解决方案，模仿了人类事实核查的协作过程。其创新点在于利用多个LLM代理模拟辩论，并通过合成数据增强模型性能，有效解决了真实数据稀缺的问题。这种方法有望提高事实核查的准确性和透明度，尤其是在处理需要多角度分析的复杂主张时。"}}
{"id": "2006.12926", "title": "A self-supervised neural-analytic method to predict the evolution of COVID-19 in Romania", "authors": ["Radu D. Stochiţoiu", "Marian Petrica", "Traian Rebedea", "Ionel Popescu", "Marius Leordeanu"], "categories": ["q-bio.PE", "cs.LG"], "primary_category": "Subjects:       Populations and Evolution (q-bio.PE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2006.12926v3", "summary": "Analysing and understanding the transmission and evolution of the COVID-19\npandemic is mandatory to be able to design the best social and medical\npolicies, foresee their outcomes and deal with all the subsequent\nsocio-economic effects. We address this important problem from a computational\nand machine learning perspective. More specifically, we want to statistically\nestimate all the relevant parameters for the new coronavirus COVID-19, such as\nthe reproduction number, fatality rate or length of infectiousness period,\nbased on Romanian patients, as well as be able to predict future outcomes. This\nendeavor is important, since it is well known that these factors vary across\nthe globe, and might be dependent on many causes, including social, medical,\nage and genetic factors. We use a recently published improved version of SEIR,\nwhich is the classic, established model for infectious diseases. We want to\ninfer all the parameters of the model, which govern the evolution of the\npandemic in Romania, based on the only reliable, true measurement, which is the\nnumber of deaths. Once the model parameters are estimated, we are able to\npredict all the other relevant measures, such as the number of exposed and\ninfectious people. To this end, we propose a self-supervised approach to train\na deep convolutional network to guess the correct set of Modified-SEIR model\nparameters, given the observed number of daily fatalities. Then, we refine the\nsolution with a stochastic coordinate descent approach. We compare our deep\nlearning optimization scheme with the classic grid search approach and show\ngreat improvement in both computational time and prediction accuracy. We find\nan optimistic result in the case fatality rate for Romania which may be around\n0.3% and we also demonstrate that our model is able to correctly predict the\nnumber of daily fatalities for up to three weeks in the future.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2006.12926v3", "cate": "q-bio.PE", "date": "2020-06-23", "updated": "2025-07-25", "AI": {"title_translation": "罗马尼亚COVID-19演变预测的自监督神经分析方法", "tldr": "本文提出一种自监督深度学习方法，利用死亡人数预测罗马尼亚COVID-19疫情参数及未来演变，并在计算时间与预测精度上优于传统方法。", "motivation": "分析和理解COVID-19的传播和演变对于制定有效的社会和医疗政策、预测结果以及应对随后的社会经济影响至关重要。鉴于这些因素在全球范围内存在差异，且可能受多种因素影响，因此需要针对特定地区（如罗马尼亚）进行参数估计和未来预测。", "method": "作者使用了一种改进的SEIR模型，并提出了一种自监督方法来训练一个深度卷积网络，以根据每日死亡人数估计修正SEIR模型的参数。然后，通过随机坐标下降法对解决方案进行优化。该深度学习优化方案与经典的网格搜索方法进行了比较。", "result": "该方法在计算时间和预测精度上相较于经典网格搜索方法有显著提升。研究发现罗马尼亚的病例死亡率约为0.3%，并且模型能够准确预测未来长达三周的每日死亡人数。", "conclusion": "论文提出了一种有效的自监督神经分析方法，能够基于每日死亡人数准确预测罗马尼亚COVID-19的演变和相关参数，并在效率和准确性方面表现出色。", "translation": "分析和理解COVID-19大流行的传播和演变对于设计最佳社会和医疗政策、预见其结果以及应对所有随后的社会经济影响至关重要。我们从计算和机器学习的角度解决了这个重要问题。更具体地说，我们希望根据罗马尼亚患者的数据，统计估算新型冠状病毒COVID-19的所有相关参数，例如传染数、死亡率或传染期长度，并能够预测未来结果。这项工作很重要，因为众所周知，这些因素在全球范围内各不相同，并且可能取决于许多原因，包括社会、医疗、年龄和遗传因素。我们使用最近发布的一种改进版SEIR模型，它是传染病的经典、成熟模型。我们希望根据唯一可靠的真实测量——死亡人数，推断出控制罗马尼亚疫情演变的所有模型参数。一旦模型参数被估算出来，我们就能够预测所有其他相关指标，例如暴露和感染人数。为此，我们提出了一种自监督方法来训练一个深度卷积网络，使其根据观察到的每日死亡人数猜测正确的修正SEIR模型参数集。然后，我们通过随机坐标下降法改进解决方案。我们将我们的深度学习优化方案与经典的网格搜索方法进行比较，结果显示在计算时间和预测精度方面都有很大的改进。我们发现罗马尼亚的病例死亡率结果乐观，可能约为0.3%，并且我们还证明我们的模型能够正确预测未来长达三周的每日死亡人数。", "summary": "本文提出了一种针对罗马尼亚COVID-19疫情的自监督神经分析方法，旨在通过深度卷积网络和随机坐标下降法，基于每日死亡人数准确估计改进SEIR模型的关键参数（如传染数、死亡率）并预测未来演变。该方法在计算效率和预测精度上均优于传统网格搜索，并成功预测了未来三周的每日死亡人数，估算出罗马尼亚病例死亡率约为0.3%。", "keywords": "COVID-19, 自监督学习, SEIR模型, 疫情预测, 罗马尼亚", "comments": "这篇论文的创新点在于结合了改进的SEIR模型与自监督深度学习方法（深度卷积网络），并利用每日死亡人数这一可靠数据进行参数估计和疫情预测。这种方法克服了传统网格搜索的计算效率问题，并提高了预测准确性。其重要性在于为特定区域（罗马尼亚）的疫情预测提供了高效准确的工具，有助于当地政策制定。"}}
{"id": "2504.13376", "title": "Addressing the Minor-Embedding Problem in Quantum Annealing and Evaluating State-of-the-Art Algorithm Performance", "authors": ["Aitor Gomez-Tejedor", "Eneko Osaba", "Esther Villar-Rodriguez"], "categories": ["quant-ph", "cs.AI", "cs.ET"], "primary_category": "Subjects:       Quantum Physics (quant-ph)", "pdf_link": null, "comments": "Comments:      Paper submitted for review in the Future Generation Computer Systems journal", "url": "http://arxiv.org/abs/2504.13376v3", "summary": "This study addresses the minor-embedding problem, which involves mapping the\nvariables of an Ising model onto a quantum annealing processor. The primary\nmotivation stems from the observed performance disparity of quantum annealers\nwhen solving problems suited to the processor's architecture versus those with\nnon-hardware-native topologies. Our research has two main objectives: i) to\nanalyze the impact of embedding quality on the performance of D-Wave Systems\nquantum annealers, and ii) to evaluate the quality of the embeddings generated\nby Minorminer, the standard minor-embedding technique in the quantum annealing\nliterature, provided by D-Wave. Regarding the first objective, our experiments\nreveal a clear correlation between the average chain length of embeddings and\nthe relative errors of the solutions sampled. This underscores the critical\ninfluence of embedding quality on quantum annealing performance. For the second\nobjective, we evaluate Minorminer's embedding capabilities, the quality and\nrobustness of its embeddings, and its execution-time performance. We also\ncompare its performance with Clique Embedding, another algorithm developed by\nD-Wave, which is deterministic and designed to embed fully connected Ising\nmodels into quantum annealing processors, serving as a worst-case scenario. The\nresults demonstrate that there is significant room for improvement for\nMinorminer, suggesting that more effective embedding strategies could lead to\nmeaningful gains in quantum annealing performance.", "comment": "Paper submitted for review in the Future Generation Computer Systems\n  journal", "pdf_url": "http://arxiv.org/pdf/2504.13376v3", "cate": "quant-ph", "date": "2025-04-17", "updated": "2025-07-25", "AI": {"title_translation": "解决量子退火中的小嵌入问题并评估最先进算法的性能", "tldr": "本研究探讨了量子退火中的小嵌入问题，发现嵌入质量对D-Wave量子退火器性能有显著影响，并指出标准嵌入算法Minorminer仍有很大的改进空间。", "motivation": "量子退火器在处理非硬件原生拓扑问题时，其性能表现出明显差异，这促使研究小嵌入问题以及它对量子退火性能的影响。", "method": "本研究通过实验分析了嵌入质量（平均链长）对D-Wave Systems量子退火器性能的影响，并评估了标准小嵌入技术Minorminer的嵌入质量、鲁棒性及执行时间性能，同时将其与D-Wave开发的Clique Embedding算法进行了比较。", "result": "实验结果表明，嵌入的平均链长与采样解的相对误差之间存在明确关联，强调了嵌入质量对量子退火性能的关键影响。研究还发现，Minorminer算法有显著的改进空间。", "conclusion": "嵌入质量对量子退火性能至关重要，且当前的标准嵌入算法Minorminer仍有很大的提升潜力，开发更有效的嵌入策略有望显著提升量子退火的性能。", "translation": "本研究旨在解决小嵌入问题，即将伊辛模型变量映射到量子退火处理器。主要动机源于观察到量子退火器在解决适合处理器架构的问题与解决具有非硬件原生拓扑的问题时性能存在差异。我们的研究有两个主要目标：i) 分析嵌入质量对D-Wave Systems量子退火器性能的影响，以及 ii) 评估由D-Wave提供的量子退火文献中标准小嵌入技术Minorminer生成的嵌入质量。关于第一个目标，我们的实验揭示了嵌入的平均链长与采样解的相对误差之间存在明确关联。这强调了嵌入质量对量子退火性能的关键影响。对于第二个目标，我们评估了Minorminer的嵌入能力、其嵌入的质量和鲁棒性，以及其执行时间性能。我们还将其性能与D-Wave开发的另一种算法Clique Embedding进行了比较，该算法是确定性的，旨在将全连接的伊辛模型嵌入到量子退火处理器中，作为最坏情况的场景。结果表明，Minorminer有显著的改进空间，这表明更有效的嵌入策略可以带来量子退火性能的显著提升。", "summary": "本研究旨在解决量子退火中的小嵌入问题，即将伊辛模型变量映射到量子退火处理器。研究动机源于量子退火器在处理非硬件原生拓扑问题时观察到的性能差异。论文主要目标是分析嵌入质量对D-Wave量子退火器性能的影响，并评估标准嵌入技术Minorminer的性能。实验结果表明，嵌入的平均链长与采样解的相对误差之间存在明确相关性，凸显了嵌入质量的重要性。研究还发现Minorminer有显著的改进空间，暗示更有效的嵌入策略能带来量子退火性能的显著提升。", "keywords": "量子退火, 小嵌入, 伊辛模型, Minorminer, 嵌入质量", "comments": "这篇论文深入探讨了量子退火领域的一个核心挑战——小嵌入问题。其创新之处在于量化了嵌入质量（通过平均链长）对量子退火器性能的具体影响，并对广泛使用的Minorminer算法进行了详尽的性能评估。研究结果揭示了现有算法的局限性，并为未来更高效的嵌入算法设计指明了方向，这对于提升量子退火器的实际应用性能具有重要意义。"}}
{"id": "2507.18729", "title": "CUTHERMO: Understanding GPU Memory Inefficiencies with Heat Map Profiling", "authors": ["Yanbo Zhao", "Jinku Cui", "Zecheng Li", "Shuyin Jiao", "Xu Liu", "Jiajia Li"], "categories": ["cs.DC", "cs.PF"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18729v1", "summary": "GPUs have become indispensable in high-performance computing, machine\nlearning, and many other domains. Efficiently utilizing the memory subsystem on\nGPUs is critical for maximizing computing power through massive parallelism.\nAnalyzing memory access patterns has proven to be an effective method for\nunderstanding memory bottlenecks in applications. However, comprehensive\nruntime and fine-grained memory profiling support is lacking on GPU\narchitectures. In this work, we introduce cuThermo, a lightweight and practical\nprofiling tool for GPU memory analysis. It operates on GPU binaries without\nrequiring any modifications to hardware, operating system, or application\nsource code. Given a CUDA application, cuThermo identifies memory\ninefficiencies at runtime via a heat map based on distinct visited warp counts\nto represent word-sector-level data sharing and provides optimization guidance\nin performance tuning iterations. Through our experiments on six applications,\nwe identified five memory access patterns that are portable across different\nGPU architectures. By evaluating optimization on two GPUs, cuThermo achieves up\nto $721.79\\%$ performance improvement.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18729v1", "cate": "cs.DC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "CUTHERMO：通过热图分析理解GPU内存效率低下问题", "tldr": "cuThermo是一个轻量级的GPU内存分析工具，通过热图识别内存低效并提供优化指导，实现了显著的性能提升。", "motivation": "当前GPU架构缺乏全面的运行时和细粒度内存分析支持，导致难以理解和解决GPU应用程序中的内存瓶颈，而GPU内存子系统的有效利用对于最大化计算能力至关重要。", "method": "cuThermo是一个轻量级且实用的GPU内存分析工具。它无需修改硬件、操作系统或应用程序源代码，即可在GPU二进制文件上运行。对于CUDA应用程序，cuThermo通过基于不同访问的warp计数的热图来识别运行时内存低效，以表示字扇区级别的数据共享，并在性能调优迭代中提供优化指导。", "result": "通过在六个应用程序上的实验，cuThermo识别了五种可在不同GPU架构上移植的内存访问模式。通过在两块GPU上评估优化，cuThermo实现了高达721.79%的性能提升。", "conclusion": "cuThermo是一个有效的工具，可以识别GPU内存低效，并通过热图分析和优化指导显著提高应用程序性能。", "translation": "GPU已成为高性能计算、机器学习和许多其他领域不可或缺的一部分。有效利用GPU上的内存子系统对于通过大规模并行化最大化计算能力至关重要。分析内存访问模式已被证明是理解应用程序中内存瓶颈的有效方法。然而，GPU架构上缺乏全面的运行时和细粒度内存分析支持。在这项工作中，我们引入了cuThermo，一个用于GPU内存分析的轻量级实用分析工具。它在GPU二进制文件上运行，无需修改硬件、操作系统或应用程序源代码。对于给定的CUDA应用程序，cuThermo通过基于不同访问的warp计数的热图在运行时识别内存低效，以表示字扇区级别的数据共享，并在性能调优迭代中提供优化指导。通过我们对六个应用程序的实验，我们识别了五种可在不同GPU架构上移植的内存访问模式。通过在两块GPU上评估优化，cuThermo实现了高达721.79%的性能提升。", "summary": "本文介绍了cuThermo，一个用于分析GPU内存低效的轻量级工具。cuThermo通过热图技术，基于不同访问的warp计数来识别字扇区级别的数据共享和内存瓶颈，并提供优化建议。该工具无需修改源代码即可在GPU二进制文件上运行。实验结果表明，cuThermo能够识别可移植的内存访问模式，并在优化后实现显著的性能提升，最高可达721.79%。", "keywords": "GPU内存, 性能分析, 热图, cuThermo, 内存低效", "comments": "cuThermo的创新之处在于其轻量级和无需修改源代码的特性，这大大降低了GPU内存分析的门槛。通过引入基于热图的分析方法，它为理解和优化GPU内存访问模式提供了一个直观且高效的途径。高达721.79%的性能提升证明了其在实际应用中的巨大潜力，对于提升GPU应用程序的性能具有重要意义。"}}
{"id": "2507.19165", "title": "Extreme Cardiac MRI Analysis under Respiratory Motion: Results of the CMRxMotion Challenge", "authors": ["Kang Wang", "Chen Qin", "Zhang Shi", "Haoran Wang", "Xiwen Zhang", "Chen Chen", "Cheng Ouyang", "Chengliang Dai", "Yuanhan Mo", "Chenchen Dai", "Xutong Kuang", "Ruizhe Li", "Xin Chen", "Xiuzheng Yue", "Song Tian", "Alejandro Mora-Rubio", "Kumaradevan Punithakumar", "Shizhan Gong", "Qi Dou", "Sina Amirrajab", "Yasmina Al Khalil", "Cian M. Scannell", "Lexiaozi Fan", "Huili Yang", "Xiaowu Sun", "Rob van der Geest", "Tewodros Weldebirhan Arega", "Fabrice Meriaudeau", "Caner Özer", "Amin Ranem", "John Kalkhof", "İlkay Öksüz", "Anirban Mukhopadhyay", "Abdul Qayyum", "Moona Mazher", "Steven A Niederer", "Carles Garcia-Cabrera", "Eric Arazo", "Michal K. Grzeszczyk", "Szymon Płotka", "Wanqin Ma", "Xiaomeng Li", "Rongjun Ge", "Yongqing Kou", "Xinrong Chen", "He Wang", "Chengyan Wang", "Wenjia Bai", "Shuo Wang"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19165v1", "summary": "Deep learning models have achieved state-of-the-art performance in automated\nCardiac Magnetic Resonance (CMR) analysis. However, the efficacy of these\nmodels is highly dependent on the availability of high-quality, artifact-free\nimages. In clinical practice, CMR acquisitions are frequently degraded by\nrespiratory motion, yet the robustness of deep learning models against such\nartifacts remains an underexplored problem. To promote research in this domain,\nwe organized the MICCAI CMRxMotion challenge. We curated and publicly released\na dataset of 320 CMR cine series from 40 healthy volunteers who performed\nspecific breathing protocols to induce a controlled spectrum of motion\nartifacts. The challenge comprised two tasks: 1) automated image quality\nassessment to classify images based on motion severity, and 2) robust\nmyocardial segmentation in the presence of motion artifacts. A total of 22\nalgorithms were submitted and evaluated on the two designated tasks. This paper\npresents a comprehensive overview of the challenge design and dataset, reports\nthe evaluation results for the top-performing methods, and further investigates\nthe impact of motion artifacts on five clinically relevant biomarkers. All\nresources and code are publicly available at: https://github.com/CMRxMotion", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19165v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "极端心脏MRI呼吸运动分析：CMRxMotion挑战赛结果", "tldr": "本文介绍了CMRxMotion挑战赛，旨在通过提供数据集并评估图像质量评估和心肌分割算法，促进深度学习模型在心脏MRI中对抗呼吸运动伪影的鲁棒性研究。", "motivation": "深度学习模型在自动化心脏磁共振（CMR）分析中表现出色，但其有效性高度依赖于高质量、无伪影的图像。在临床实践中，CMR采集经常因呼吸运动而质量下降，然而深度学习模型对此类伪影的鲁棒性仍是一个未充分探索的问题。", "method": "为促进该领域的研究，我们组织了MICCAI CMRxMotion挑战赛。我们策划并公开发布了一个包含40名健康志愿者320个CMR电影序列的数据集，这些志愿者执行了特定的呼吸方案以诱导受控范围的运动伪影。挑战赛包含两项任务：1）自动化图像质量评估，根据运动严重程度对图像进行分类；2）在存在运动伪影的情况下进行鲁棒的心肌分割。总共有22种算法提交并针对两项指定任务进行了评估。", "result": "本文全面概述了挑战赛的设计和数据集，报告了表现最佳方法的评估结果，并进一步调查了运动伪影对五种临床相关生物标志物的影响。", "conclusion": "CMRxMotion挑战赛提供了一个宝贵的数据集和平台，以推进深度学习模型在CMR中对抗呼吸运动的鲁棒性研究，所有资源均已公开。", "translation": "深度学习模型在自动化心脏磁共振（CMR）分析中取得了最先进的性能。然而，这些模型的有效性高度依赖于高质量、无伪影图像的可用性。在临床实践中，CMR采集经常因呼吸运动而质量下降，然而深度学习模型对此类伪影的鲁棒性仍是一个未充分探索的问题。为促进该领域的研究，我们组织了MICCAI CMRxMotion挑战赛。我们策划并公开发布了一个包含40名健康志愿者320个CMR电影序列的数据集，这些志愿者执行了特定的呼吸方案以诱导受控范围的运动伪影。挑战赛包含两项任务：1）自动化图像质量评估，根据运动严重程度对图像进行分类；2）在存在运动伪影的情况下进行鲁棒的心肌分割。总共有22种算法提交并针对两项指定任务进行了评估。本文全面概述了挑战赛的设计和数据集，报告了表现最佳方法的评估结果，并进一步调查了运动伪影对五种临床相关生物标志物的影响。所有资源和代码均可在https://github.com/CMRxMotion公开获取。", "summary": "CMRxMotion挑战赛旨在解决心脏MRI中深度学习模型对呼吸运动伪影鲁棒性这一未充分探索的问题。该挑战赛提供了一个包含受控运动的心脏MRI电影序列数据集，并设定了图像质量评估和心肌分割两项任务。本文详细介绍了挑战赛的设计、数据集、顶级算法的评估结果以及运动对生物标志物的影响，所有资源均已公开，以促进进一步的研究。", "keywords": "心脏MRI, 呼吸运动, 深度学习, 图像质量, 心肌分割", "comments": "CMRxMotion挑战赛是一项重要的倡议，解决了深度学习在临床CMR应用中的关键空白。通过提供带有受控运动伪影的标准化数据集和明确定义的任务，它能够系统地评估并促进更鲁棒的深度学习模型的开发，这对于实际临床部署至关重要。资源的公开可用性进一步增强了其影响力。"}}
{"id": "2507.18717", "title": "A conservative invariant-domain preserving projection technique for hyperbolic systems under adaptive mesh refinement", "authors": ["Jake Harmon", "Martin Kronbichler", "Matthias Maier", "Eric Tovar"], "categories": ["math.NA", "cs.NA", "65M60, 65M12, 35L50, 35L65, 76M10"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18717v1", "summary": "We propose a rigorous, conservative invariant-domain preserving (IDP)\nprojection technique for hierarchical discretizations that enforces membership\nin physics-implied convex sets when mapping between solution spaces. When\ncoupled with suitable refinement indicators, the proposed scheme enables a\nprovably IDP adaptive numerical method for hyperbolic systems where\npreservation of physical properties is essential. In addition to proofs of\nthese characteristics, we supply a detailed construction of the method in the\ncontext of a high-performance finite element code. To illustrate our proposed\nscheme, we study a suite of computationally challenging benchmark problems,\ndemonstrating enhanced accuracy and efficiency properties while entirely\navoiding \\emph{ad hoc} corrections to preserve physical invariants.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18717v1", "cate": "math.NA", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "双曲系统自适应网格细化下的一种守恒不变域保持投影技术", "tldr": "提出了一种严格、守恒的不变域保持(IDP)投影技术，用于双曲系统的自适应数值方法，该方法在解决方案空间映射时强制满足物理隐含的凸集成员关系，并被证明可以提高精度和效率。", "motivation": "在双曲系统中，物理性质的保持至关重要，因此需要一种能够确保解在物理隐含的凸集中保持不变的投影技术，以避免临时的修正。", "method": "提出了一种严格、守恒的不变域保持(IDP)投影技术，用于分层离散化，该技术在解决方案空间之间映射时强制满足物理隐含的凸集成员关系。结合合适的细化指标，该方案能够实现双曲系统的可证明IDP自适应数值方法。提供了特性证明和在高性有限元代码中的详细构建。", "result": "该方案在计算上具有挑战性的基准问题上表现出更高的精度和效率特性，同时完全避免了为保持物理不变量而进行的临时修正。", "conclusion": "该研究提出了一种严格、守恒的不变域保持投影技术，能够为双曲系统提供可证明的IDP自适应数值方法，并在实践中展示了其优越的精度和效率，且无需临时修正。", "translation": "我们提出了一种严格、守恒的不变域保持（IDP）投影技术，用于分层离散化，该技术在解决方案空间之间映射时强制满足物理隐含的凸集成员关系。当与合适的细化指标结合时，所提出的方案能够为物理性质保持至关重要的双曲系统提供可证明的IDP自适应数值方法。除了这些特性的证明外，我们还在高性能有限元代码的背景下提供了该方法的详细构建。为了说明我们提出的方案，我们研究了一系列计算上具有挑战性的基准问题，展示了增强的精度和效率特性，同时完全避免了为保持物理不变量而进行的临时修正。", "summary": "该论文提出了一种针对双曲系统的新型守恒不变域保持（IDP）投影技术，特别适用于自适应网格细化。该方法通过在解空间映射时强制满足物理隐含的凸集成员关系，确保了物理性质的保持。研究不仅提供了该方案的理论证明，还详细介绍了其在高性能有限元代码中的实现。通过对一系列基准问题的测试，结果表明该技术在提高精度和效率的同时，避免了传统方法中为保持物理不变量所需的临时修正。", "keywords": "不变域保持, 投影技术, 双曲系统, 自适应网格细化, 守恒", "comments": "该论文的关键创新在于提出了一个“严格、守恒的不变域保持投影技术”，它解决了双曲系统中物理性质保持的关键挑战，尤其是在自适应网格细化背景下。其重要性在于，通过理论证明和实际应用展示，该方法能够避免传统上为保持物理不变量而采用的“临时修正”，从而提高了数值模拟的可靠性和效率。这对于需要高精度和物理一致性的计算流体力学等领域具有重要意义。"}}
{"id": "2504.09540", "title": "EmbodiedOcc++: Boosting Embodied 3D Occupancy Prediction with Plane Regularization and Uncertainty Sampler", "authors": ["Hao Wang", "Xiaobao Wei", "Xiaoan Zhang", "Jianing Li", "Chengyu Bai", "Ying Li", "Ming Lu", "Wenzhao Zheng", "Shanghang Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2504.09540v2", "summary": "Online 3D occupancy prediction provides a comprehensive spatial understanding\nof embodied environments. While the innovative EmbodiedOcc framework utilizes\n3D semantic Gaussians for progressive indoor occupancy prediction, it overlooks\nthe geometric characteristics of indoor environments, which are primarily\ncharacterized by planar structures. This paper introduces EmbodiedOcc++,\nenhancing the original framework with two key innovations: a Geometry-guided\nRefinement Module (GRM) that constrains Gaussian updates through plane\nregularization, along with a Semantic-aware Uncertainty Sampler (SUS) that\nenables more effective updates in overlapping regions between consecutive\nframes. GRM regularizes the position update to align with surface normals. It\ndetermines the adaptive regularization weight using curvature-based and\ndepth-based constraints, allowing semantic Gaussians to align accurately with\nplanar surfaces while adapting in complex regions. To effectively improve\ngeometric consistency from different views, SUS adaptively selects proper\nGaussians to update. Comprehensive experiments on the EmbodiedOcc-ScanNet\nbenchmark demonstrate that EmbodiedOcc++ achieves state-of-the-art performance\nacross different settings. Our method demonstrates improved edge accuracy and\nretains more geometric details while ensuring computational efficiency, which\nis essential for online embodied perception. The code will be released at:\nhttps://github.com/PKUHaoWang/EmbodiedOcc2.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2504.09540v2", "cate": "cs.CV", "date": "2025-04-13", "updated": "2025-07-25", "AI": {"title_translation": "EmbodiedOcc++: 通过平面正则化和不确定性采样器提升具身3D占用预测", "tldr": "EmbodiedOcc++通过引入几何引导的细化模块和语义感知不确定性采样器，解决了EmbodiedOcc在具身3D占用预测中忽视平面结构的问题，实现了SOTA性能。", "motivation": "现有的EmbodiedOcc框架在具身3D占用预测中，尽管使用了3D语义高斯，但忽视了室内环境主要由平面结构构成的几何特性。", "method": "本文提出了EmbodiedOcc++，通过两个主要创新来增强原框架：1. 几何引导细化模块（GRM），通过平面正则化约束高斯更新，并使用基于曲率和深度的约束来确定自适应正则化权重，使语义高斯与平面表面精确对齐。2. 语义感知不确定性采样器（SUS），在连续帧之间的重叠区域实现更有效的更新，并自适应选择合适的高斯进行更新，以提高几何一致性。", "result": "EmbodiedOcc++在EmbodiedOcc-ScanNet基准测试中，在不同设置下均实现了最先进的性能。该方法提高了边缘精度，保留了更多几何细节，同时确保了计算效率。", "conclusion": "EmbodiedOcc++通过引入平面正则化和不确定性采样器，显著提升了具身3D占用预测的性能，在保持计算效率的同时，提高了几何细节和边缘精度，对于在线具身感知至关重要。", "translation": "在线3D占用预测提供了对具身环境的全面空间理解。虽然创新的EmbodiedOcc框架利用3D语义高斯进行渐进式室内占用预测，但它忽视了室内环境的几何特征，这些特征主要由平面结构构成。本文介绍了EmbodiedOcc++，通过两项关键创新增强了原始框架：一个几何引导的细化模块（GRM），通过平面正则化约束高斯更新；以及一个语义感知不确定性采样器（SUS），可以在连续帧之间的重叠区域实现更有效的更新。GRM正则化位置更新以与表面法线对齐。它使用基于曲率和深度的约束来确定自适应正则化权重，允许语义高斯精确地与平面表面对齐，同时在复杂区域进行适应。为了有效提高不同视角的几何一致性，SUS自适应地选择合适的高斯进行更新。在EmbodiedOcc-ScanNet基准测试上的综合实验表明，EmbodiedOcc++在不同设置下均实现了最先进的性能。我们的方法在确保计算效率的同时，提高了边缘精度并保留了更多几何细节，这对于在线具身感知至关重要。代码将发布在：https://github.com/PKUHaoWang/EmbodiedOcc2。", "summary": "本文提出了EmbodiedOcc++，旨在提升具身3D占用预测的性能。针对现有EmbodiedOcc框架忽视室内环境平面几何特性的问题，EmbodiedOcc++引入了几何引导细化模块（GRM）和语义感知不确定性采样器（SUS）。GRM通过平面正则化约束高斯更新以对齐表面法线，并自适应调整权重；SUS则在帧间重叠区域实现有效更新，并自适应选择高斯以提高几何一致性。实验证明，EmbodiedOcc++在EmbodiedOcc-ScanNet基准上达到了最先进的性能，显著提高了边缘精度和几何细节，同时保持了计算效率。", "keywords": "3D占用预测, 平面正则化, 不确定性采样, 具身感知, 几何引导", "comments": "EmbodiedOcc++的创新之处在于其结合了几何先验（平面正则化）与不确定性感知更新策略。通过GRM利用室内环境的平面结构特性进行高斯更新的约束，显著提升了模型的几何精度。SUS则通过智能采样解决了连续帧间重叠区域的更新效率问题，进一步提高了几何一致性。这对于在线具身感知至关重要，因为它在提高精度和细节的同时，保持了计算效率，解决了实际应用中的关键挑战。"}}
{"id": "2507.18649", "title": "Livatar-1: Real-Time Talking Heads Generation with Tailored Flow Matching", "authors": ["Haiyang Liu", "Xiaolin Hong", "Xuancheng Yang", "Yudi Ruan", "Xiang Lian", "Michael Lingelbach", "Hongwei Yi", "Wei Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Technical Report", "url": "http://arxiv.org/abs/2507.18649v1", "summary": "We present Livatar, a real-time audio-driven talking heads videos generation\nframework. Existing baselines suffer from limited lip-sync accuracy and\nlong-term pose drift. We address these limitations with a flow matching based\nframework. Coupled with system optimizations, Livatar achieves competitive\nlip-sync quality with a 8.50 LipSync Confidence on the HDTF dataset, and\nreaches a throughput of 141 FPS with an end-to-end latency of 0.17s on a single\nA10 GPU. This makes high-fidelity avatars accessible to broader applications.\nOur project is available at https://www.hedra.com/ with with examples at\nhttps://h-liu1997.github.io/Livatar-1/", "comment": "Technical Report", "pdf_url": "http://arxiv.org/pdf/2507.18649v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "Livatar-1：基于定制流匹配的实时说话人头像生成", "tldr": "Livatar是一个实时音频驱动的说话人头像生成框架，通过流匹配和系统优化解决了现有方法的唇形同步不佳和姿态漂移问题，实现了高保真和高效率。", "motivation": "现有实时音频驱动的说话人头像生成框架存在唇形同步准确性有限和长期姿态漂移的问题。", "method": "提出Livatar框架，采用基于流匹配的方法，并结合系统优化，以解决现有方法的局限性。", "result": "在HDTF数据集上实现了8.50的唇形同步置信度，在单A10 GPU上达到141 FPS的吞吐量和0.17秒的端到端延迟。", "conclusion": "Livatar使高保真虚拟形象能够应用于更广泛的场景。", "translation": "我们提出了Livatar，一个实时音频驱动的说话人视频生成框架。现有的基线方法存在唇形同步准确性有限和长期姿态漂移的问题。我们通过一个基于流匹配的框架解决了这些局限性。结合系统优化，Livatar在HDTF数据集上实现了8.50的唇形同步置信度，达到了具有竞争力的唇形同步质量，并在单A10 GPU上实现了141 FPS的吞吐量和0.17秒的端到端延迟。这使得高保真虚拟形象可以应用于更广泛的场景。我们的项目可在https://www.hedra.com/获取，示例可在https://h-liu1997.github.io/Livatar-1/查看。", "summary": "Livatar是一个实时音频驱动的说话人头像生成框架，旨在解决现有方法中唇形同步准确性不足和长期姿态漂移的问题。该框架采用定制的流匹配方法并结合系统优化，显著提高了唇形同步质量和系统吞吐量。Livatar在HDTF数据集上展现出优异的性能，实现了高保真虚拟形象的实时生成，使其适用于更广泛的应用。", "keywords": "实时说话人头像生成, 流匹配, 唇形同步, 高保真, Livatar", "comments": "这篇论文通过引入定制的流匹配方法和系统优化，显著提升了实时说话人头像生成的质量和效率。其创新点在于有效地解决了唇形同步和姿态漂移的难题，并实现了在单GPU上的高帧率和低延迟，这对于实时交互和更广泛的应用场景具有重要意义。"}}
{"id": "2507.19067", "title": "PBiLoss: Popularity-Aware Regularization to Improve Fairness in Graph-Based Recommender Systems", "authors": ["Mohammad Naeimi", "Mostafa Haghir Chehreghani"], "categories": ["cs.IR", "cs.AI", "cs.NE"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19067v1", "summary": "Recommender systems, especially those based on graph neural networks (GNNs),\nhave achieved remarkable success in capturing user-item interaction patterns.\nHowever, they remain susceptible to popularity bias--the tendency to\nover-recommend popular items--resulting in reduced content diversity and\ncompromised fairness. In this paper, we propose PBiLoss, a novel\nregularization-based loss function designed to counteract popularity bias in\ngraph-based recommender models explicitly. PBiLoss augments traditional\ntraining objectives by penalizing the model's inclination toward popular items,\nthereby encouraging the recommendation of less popular but potentially more\npersonalized content. We introduce two sampling strategies: Popular Positive\n(PopPos) and Popular Negative (PopNeg), which respectively modulate the\ncontribution of the positive and negative popular items during training. We\nfurther explore two methods to distinguish popular items: one based on a fixed\npopularity threshold and another without any threshold, making the approach\nflexible and adaptive. Our proposed method is model-agnostic and can be\nseamlessly integrated into state-of-the-art graph-based frameworks such as\nLightGCN and its variants. Comprehensive experiments across multiple real-world\ndatasets demonstrate that PBiLoss significantly improves fairness, as\ndemonstrated by reductions in the Popularity-Rank Correlation for Users (PRU)\nand Popularity-Rank Correlation for Items (PRI), while maintaining or even\nenhancing standard recommendation accuracy and ranking metrics. These results\nhighlight the effectiveness of directly embedding fairness objectives into the\noptimization process, providing a practical and scalable solution for balancing\naccuracy and equitable content exposure in modern recommender systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19067v1", "cate": "cs.IR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "PBiLoss：一种基于流行度感知的正则化方法，旨在提高图基推荐系统中的公平性", "tldr": "本文提出了PBiLoss，一个对抗流行度偏差的正则化损失函数，用于提高图基推荐系统的公平性，同时保持准确性。", "motivation": "图神经网络（GNN）推荐系统存在流行度偏差，导致内容多样性降低和公平性受损。", "method": "本文提出了PBiLoss，一种新颖的基于正则化的损失函数，旨在明确地抵消图基推荐模型中的流行度偏差。它通过惩罚模型对流行项的倾向来增强传统训练目标，并引入了两种采样策略（Popular Positive和Popular Negative）以及两种区分流行项的方法（固定阈值和无阈值）。该方法是模型无关的，可与LightGCN等现有图基框架无缝集成。", "result": "PBiLoss在多个真实世界数据集上的实验表明，它显著提高了公平性（降低了用户流行度-排名相关性PRU和物品流行度-排名相关性PRI），同时保持或甚至提升了标准推荐准确性和排名指标。", "conclusion": "将公平性目标直接嵌入优化过程是有效的，为现代推荐系统中平衡准确性和公平内容曝光提供了一个实用且可扩展的解决方案。", "translation": "推荐系统，特别是那些基于图神经网络（GNNs）的系统，在捕获用户-物品交互模式方面取得了显著成功。然而，它们仍然容易受到流行度偏差的影响——即过度推荐流行物品的倾向——这导致内容多样性降低和公平性受损。在本文中，我们提出了PBiLoss，一种新颖的基于正则化的损失函数，旨在明确地抵消图基推荐模型中的流行度偏差。PBiLoss通过惩罚模型对流行物品的倾向来增强传统的训练目标，从而鼓励推荐那些不那么流行但可能更具个性化的内容。我们引入了两种采样策略：流行正例（PopPos）和流行负例（PopNeg），它们分别在训练过程中调节正向和负向流行物品的贡献。我们进一步探索了两种区分流行物品的方法：一种基于固定的流行度阈值，另一种则不设任何阈值，使该方法灵活且适应性强。我们提出的方法是模型无关的，可以无缝集成到LightGCN及其变体等最先进的图基框架中。在多个真实世界数据集上进行的全面实验表明，PBiLoss显著提高了公平性，表现为用户流行度-排名相关性（PRU）和物品流行度-排名相关性（PRI）的降低，同时保持甚至提升了标准的推荐准确性和排名指标。这些结果突出了将公平性目标直接嵌入优化过程的有效性，为现代推荐系统中平衡准确性和内容公平曝光提供了实用且可扩展的解决方案。", "summary": "本文提出了PBiLoss，一种新颖的正则化损失函数，旨在解决图基推荐系统中的流行度偏差问题。该方法通过惩罚模型对流行项的过度推荐倾向，鼓励推荐更多个性化但可能不那么流行的内容。PBiLoss引入了两种采样策略和灵活的流行项识别方法，并且具有模型无关性，可无缝集成到现有框架中。实验结果表明，PBiLoss在提高公平性的同时，能够保持甚至提升推荐准确性。", "keywords": "推荐系统, 公平性, 流行度偏差, 图神经网络, 正则化", "comments": "PBiLoss的创新之处在于其将流行度感知正则化直接融入损失函数，并通过采样策略和灵活的阈值处理，有效地缓解了流行度偏差，提升了推荐系统的公平性。其模型无关性使其具有广泛的应用潜力。该研究提供了一个实用且可扩展的解决方案，以应对现代推荐系统中准确性和内容曝光公平性之间的权衡挑战。"}}
{"id": "2504.04121", "title": "Improving Question Embeddings with Cognitive Representation Optimization for Knowledge Tracing", "authors": ["Lixiang Xu", "Xianwei Ding", "Xin Yuan", "Zhanlong Wang", "Lu Bai", "Enhong Chen", "Philip S. Yu", "Yuanyan Tang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.04121v2", "summary": "Designed to track changes in students' knowledge status and predict their\nfuture answers based on students' historical answer records. Current research\non KT modeling focuses on predicting future student performance based on\nexisting, unupdated records of student learning interactions. However, these\nmethods ignore distractions in the response process (such as slipping and\nguessing) and ignore that static cognitive representations are temporary and\nlimited. Most of them assume that there are no distractions during the\nanswering process, and that the recorded representation fully represents the\nstudent's understanding and proficiency in knowledge. This can lead to many\ndissonant and uncoordinated issues in the original record. Therefore, we\npropose a knowledge-tracking cognitive representation optimization (CRO-KT)\nmodel that uses dynamic programming algorithms to optimize the structure of\ncognitive representation. This ensures that the structure matches the student's\ncognitive patterns in terms of practice difficulty. In addition, we use a\nsynergistic optimization algorithm to optimize the cognitive representation of\nsub-target exercises based on the overall picture of exercise responses by\nconsidering all exercises with synergistic relationships as one goal. At the\nsame time, the CRO-KT model integrates the relationship embedding learned in\nthe dichotomous graph with the optimized record representation in a weighted\nmanner, which enhances students' cognitive expression ability. Finally,\nexperiments were conducted on three public datasets to verify the effectiveness\nof the proposed cognitive representation optimization model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.04121v2", "cate": "cs.AI", "date": "2025-04-05", "updated": "2025-07-25", "AI": {"title_translation": "结合认知表征优化改进知识追踪中的问题嵌入", "tldr": "提出CRO-KT模型，通过动态规划和协同优化改进知识追踪中的认知表征，以解决现有模型忽略干扰和静态表征的问题。", "motivation": "现有知识追踪模型忽略了答题过程中的干扰（如失误和猜测），并假设静态认知表征足以代表学生的理解和熟练程度，这导致原始记录中存在许多不协调的问题。", "method": "提出知识追踪认知表征优化（CRO-KT）模型。该模型使用动态规划算法优化认知表征结构以匹配学生的认知模式（练习难度）。此外，它采用协同优化算法优化子目标练习的认知表征，并以加权方式整合二分图中学习到的关系嵌入与优化的记录表征。", "result": "在三个公共数据集上的实验验证了所提出的认知表征优化模型的有效性。", "conclusion": "所提出的CRO-KT模型通过优化认知表征，能够有效提升知识追踪的性能。", "translation": "**标题：** 结合认知表征优化改进知识追踪中的问题嵌入\n**摘要：** 知识追踪（KT）旨在根据学生的历史答题记录来追踪学生知识状态的变化并预测他们未来的答案。当前关于KT模型的研究侧重于根据学生现有、未更新的学习交互记录来预测学生未来的表现。然而，这些方法忽略了答题过程中的干扰（如失误和猜测），并且忽略了静态认知表征是暂时的和有限的。它们大多数假设答题过程中没有干扰，并且记录的表征完全代表了学生对知识的理解和熟练程度。这可能导致原始记录中出现许多不协调的问题。因此，我们提出了一种知识追踪认知表征优化（CRO-KT）模型，该模型使用动态规划算法来优化认知表征的结构。这确保了结构在练习难度方面与学生的认知模式相匹配。此外，我们使用协同优化算法，通过将所有具有协同关系的练习视为一个目标，基于练习响应的整体情况来优化子目标练习的认知表征。同时，CRO-KT模型以加权方式将二分图中学习到的关系嵌入与优化的记录表征相结合，这增强了学生的认知表达能力。最后，在三个公共数据集上进行了实验，以验证所提出的认知表征优化模型的有效性。", "summary": "本文针对现有知识追踪（KT）模型忽略答题干扰和静态认知表征局限性的问题，提出了一种名为知识追踪认知表征优化（CRO-KT）的新模型。CRO-KT利用动态规划算法优化认知表征结构以匹配学生认知模式，并通过协同优化算法提升子目标练习的表征，同时整合关系嵌入。实验结果表明，该模型在公共数据集上表现出有效性。", "keywords": "知识追踪, 认知表征优化, 动态规划, 协同优化, 问题嵌入", "comments": "该研究通过引入动态认知表征优化，解决了现有知识追踪模型中长期存在的静态假设和对干扰处理不足的问题，具有较强的创新性。其结合动态规划和协同优化的方法为提升学生知识追踪的准确性提供了新的视角。"}}
{"id": "2502.05773", "title": "PIPA: Preference Alignment as Prior-Informed Statistical Estimation", "authors": ["Junbo Li", "Zhangyang Wang", "Qiang Liu"], "categories": ["cs.LG", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.05773v2", "summary": "Offline preference alignment for language models such as Direct Preference\nOptimization (DPO) is favored for its effectiveness and simplicity, eliminating\nthe need for costly reinforcement learning. Various offline algorithms have\nbeen developed for different data settings, yet they lack a unified\nunderstanding.\n  In this study, we introduce Pior-Informed Preference Alignment (PIPA), a\nunified, RL-free probabilistic framework that formulates language model\npreference alignment as a Maximum Likelihood Estimation (MLE) problem with\nprior constraints. This method effectively accommodates both paired and\nunpaired data, as well as answer and step-level annotations. We illustrate that\nDPO and KTO are special cases with different prior constraints within our\nframework. By integrating different types of prior information, we developed\ntwo variations of PIPA: PIPA-M and PIPA-N. Both algorithms demonstrate a\n$3\\sim10\\%$ performance enhancement on the GSM8K and MATH benchmarks across all\nconfigurations, achieving these gains without additional training or\ncomputational costs compared to existing algorithms.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.05773v2", "cate": "cs.LG", "date": "2025-02-09", "updated": "2025-07-24", "AI": {"title_translation": "PIPA：作为先验信息统计估计的偏好对齐", "tldr": "PIPA是一种统一的、无RL的概率框架，将语言模型偏好对齐建模为带先验约束的最大似然估计问题，并能将DPO和KTO作为其特例，在基准测试中表现出性能提升且无额外成本。", "motivation": "现有离线偏好对齐算法（如DPO）虽然有效且简单，但缺乏统一的理解，且无法有效地处理不同数据设置。", "method": "本研究提出了PIPA（Pior-Informed Preference Alignment），一个统一的、无强化学习的概率框架，将语言模型偏好对齐问题表述为带有先验约束的最大似然估计（MLE）问题。该方法能够适应配对和非配对数据，以及答案和步骤级别的标注。DPO和KTO被证明是PIPA框架内具有不同先验约束的特例。通过整合不同类型的先验信息，开发了PIPA-M和PIPA-N两种变体。", "result": "PIPA-M和PIPA-N两种算法在GSM8K和MATH基准测试中，所有配置下均实现了3%~10%的性能提升。与现有算法相比，这些性能提升无需额外的训练或计算成本。", "conclusion": "PIPA框架提供了一种统一的视角来理解和改进离线偏好对齐算法，并通过整合先验信息，在不增加成本的情况下显著提高了语言模型在数学推理等任务上的性能。", "translation": "离线偏好对齐，例如直接偏好优化（DPO），因其有效性和简单性而受到青睐，无需昂贵的强化学习。尽管已经为不同的数据设置开发了各种离线算法，但它们缺乏统一的理解。\n在本研究中，我们引入了先验信息偏好对齐（PIPA），这是一个统一的、无强化学习的概率框架，它将语言模型偏好对齐公式化为一个带有先验约束的最大似然估计（MLE）问题。这种方法有效地适应了配对和非配对数据，以及答案和步骤级别的标注。我们阐明了DPO和KTO是我们框架中具有不同先验约束的特例。通过整合不同类型的先验信息，我们开发了PIPA的两种变体：PIPA-M和PIPA-N。这两种算法在GSM8K和MATH基准测试的所有配置中都表现出3%~10%的性能提升，并且与现有算法相比，实现这些增益无需额外的训练或计算成本。", "summary": "PIPA提出了一种统一的、无强化学习的概率框架，将语言模型偏好对齐视为带有先验约束的最大似然估计问题。该框架能够兼容多种数据类型和标注级别，并揭示了DPO和KTO是其特殊情况。通过引入先验信息，PIPA-M和PIPA-N两种变体在数学基准测试上实现了显著的性能提升，且无需额外计算成本，解决了现有离线偏好对齐算法缺乏统一理解的问题。", "keywords": "偏好对齐, 最大似然估计, 先验信息, 语言模型, DPO", "comments": "PIPA的创新之处在于提供了一个统一的概率框架来理解和概括现有的离线偏好对齐算法，如DPO和KTO。其将偏好对齐问题重新定义为带先验约束的MLE问题，提供了一种新的理论视角。该方法不仅提升了性能，而且保持了计算效率，这对于实际应用非常重要。其能够处理不同类型的数据和标注，也增加了其实用性。"}}
{"id": "2507.18667", "title": "Gen-AI Police Sketches with Stable Diffusion", "authors": ["Nicholas Fidalgo", "Aaron Contreras", "Katherine Harvey", "Johnny Ni"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18667v1", "summary": "This project investigates the use of multimodal AI-driven approaches to\nautomate and enhance suspect sketching. Three pipelines were developed and\nevaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model\nintegrated with a pre-trained CLIP model for text-image alignment, and (3)\nnovel approach incorporating LoRA fine-tuning of the CLIP model, applied to\nself-attention and cross-attention layers, and integrated with Stable\nDiffusion. An ablation study confirmed that fine-tuning both self- and\ncross-attention layers yielded the best alignment between text descriptions and\nsketches. Performance testing revealed that Model 1 achieved the highest\nstructural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of\n25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced\nperceptual similarity (LPIPS), with Model 3 showing improvement over Model 2\nbut still trailing Model 1. Qualitatively, sketches generated by Model 1\ndemonstrated the clearest facial features, highlighting its robustness as a\nbaseline despite its simplicity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18667v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "基于Stable Diffusion的生成式AI警用素描", "tldr": "该项目研究了使用AI（特别是Stable Diffusion）进行警用素描，发现一个简单的基线模型在结构上表现最佳，而迭代改进有助于感知相似性。", "motivation": "该项目旨在利用多模态AI驱动的方法自动化并增强嫌疑人素描。", "method": "开发并评估了三种流程：(1) 基线图像到图像的Stable Diffusion模型；(2) 集成预训练CLIP模型以实现文本图像对齐的相同模型；(3) 结合LoRA微调CLIP模型（应用于自注意力层和交叉注意力层）并与Stable Diffusion集成的创新方法。通过消融研究确认了微调自注意力和交叉注意力层能产生最佳的文本描述与素描对齐效果。", "result": "性能测试显示，模型1实现了最高的结构相似性（SSIM为0.72）和峰值信噪比（PSNR为25 dB），优于模型2和模型3。迭代细化增强了感知相似性（LPIPS），其中模型3相对于模型2有所改进但仍落后于模型1。在质量上，模型1生成的素描展现出最清晰的面部特征。", "conclusion": "模型1（基线Stable Diffusion模型）尽管简单，但表现出鲁棒性，并生成了最清晰的面部特征，这表明它在结构和质量上都是该任务的最佳表现者。", "translation": "本项目研究了使用多模态AI驱动方法来自动化和增强嫌疑人素描。开发并评估了三个流程：(1) 基线图像到图像的Stable Diffusion模型；(2) 与预训练CLIP模型集成以实现文本-图像对齐的相同模型；(3) 一种新颖的方法，结合了CLIP模型的LoRA微调，应用于自注意力和交叉注意力层，并与Stable Diffusion集成。消融研究证实，同时微调自注意力和交叉注意力层能产生文本描述和素描之间最佳的对齐。性能测试显示，模型1实现了最高的结构相似性（SSIM）0.72和峰值信噪比（PSNR）25 dB，优于模型2和模型3。迭代细化增强了感知相似性（LPIPS），其中模型3相对于模型2有所改进但仍落后于模型1。在质量上，模型1生成的素描展现出最清晰的面部特征，突显了其作为基线的鲁棒性，尽管其简单。", "summary": "本文研究了多模态AI驱动方法在自动化和增强嫌疑人素描方面的应用。开发并评估了三种不同的流程，包括基线图像到图像的Stable Diffusion模型、集成预训练CLIP模型以及结合LoRA微调CLIP模型的创新方法。性能测试表明，基线Stable Diffusion模型（模型1）在结构相似性（SSIM）和峰值信噪比（PSNR）方面表现最佳，并且在质量上生成了最清晰的面部特征，优于更复杂的模型。迭代细化改善了其他模型的感知相似性，但模型1在关键指标上仍保持优势。", "keywords": "警用素描, Stable Diffusion, CLIP, LoRA, 生成式AI", "comments": "这项研究的有趣之处在于，它表明对于特定应用（警用素描），一个更简单的基线模型可能比更复杂、经过微调的模型表现出更好的鲁棒性和效果。这强调了在AI模型选择中，简单性有时能带来更好的实际应用性能。它也展示了Stable Diffusion在图像生成领域的潜力。"}}
{"id": "2303.11844", "title": "Doubly Regularized Entropic Wasserstein Barycenters", "authors": ["Lénaïc Chizat"], "categories": ["math.OC", "cs.LG", "stat.ML", "49N99 (Primary) 62G05, 90C30 (Secondary)"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2303.11844v2", "summary": "We study a general formulation of regularized Wasserstein barycenters that\nenjoys favorable regularity, approximation, stability and (grid-free)\noptimization properties. This barycenter is defined as the unique probability\nmeasure that minimizes the sum of entropic optimal transport (EOT) costs with\nrespect to a family of given probability measures, plus an entropy term. We\ndenote it $(\\lambda,\\tau)$-barycenter, where $\\lambda$ is the inner\nregularization strength and $\\tau$ the outer one. This formulation recovers\nseveral previously proposed EOT barycenters for various choices of\n$\\lambda,\\tau \\geq 0$ and generalizes them. First, in spite of -- and in fact\nowing to -- being \\emph{doubly} regularized, we show that our formulation is\ndebiased for $\\tau=\\lambda/2$: the suboptimality in the (unregularized)\nWasserstein barycenter objective is, for smooth densities, of the order of the\nstrength $\\lambda^2$ of entropic regularization, instead of\n$\\max\\{\\lambda,\\tau\\}$ in general. We discuss this phenomenon for isotropic\nGaussians where all $(\\lambda,\\tau)$-barycenters have closed form. Second, we\nshow that for $\\lambda,\\tau>0$, this barycenter has a smooth density and is\nstrongly stable under perturbation of the marginals. In particular, it can be\nestimated efficiently: given $n$ samples from each of the probability measures,\nit converges in relative entropy to the population barycenter at a rate\n$n^{-1/2}$. And finally, this formulation lends itself naturally to a grid-free\noptimization algorithm: we propose a simple \\emph{noisy particle gradient\ndescent} which, in the mean-field limit, converges globally at an exponential\nrate to the barycenter.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2303.11844v2", "cate": "math.OC", "date": "2023-03-21", "updated": "2025-07-25", "AI": {"title_translation": "双重正则化熵Wasserstein重心", "tldr": "提出并分析了一种双重正则化的熵Wasserstein重心，它在特定条件下具有去偏性、强稳定性和高效的无网格优化算法。", "motivation": "研究一种具有良好正则性、近似性、稳定性以及无网格优化特性的正则化Wasserstein重心的一般性公式。", "method": "定义了一种$(\\lambda,\\tau)$-重心，即通过最小化一组给定概率测度与熵最优传输（EOT）成本之和，再加上一个熵项来获得的唯一概率测度。其中$\\lambda$是内部正则化强度，$\\tau$是外部正则化强度。提出了一种简单的“噪声粒子梯度下降”算法，用于无网格优化。", "result": "该双重正则化公式在$\\tau=\\lambda/2$时是去偏的，此时（未正则化）Wasserstein重心目标中的次优性对于平滑密度而言是熵正则化强度$\\lambda^2$的数量级，而不是一般的$\\max\\{\\lambda,\\tau\\}$。对于$\\lambda,\\tau>0$，该重心具有平滑密度，并且在边缘分布扰动下表现出强稳定性。可以有效地估计该重心：从每个概率测度中抽取$n$个样本时，它以$n^{-1/2}$的速率在相对熵上收敛到总体重心。所提出的噪声粒子梯度下降算法在平均场极限下以指数速率全局收敛到重心。", "conclusion": "论文提出并深入分析了一种双重正则化的熵Wasserstein重心，证明了其在特定条件下的去偏性、在正正则化参数下的强稳定性和高效估计能力，并开发了一种有效的无网格优化算法。", "translation": "我们研究了一种正则化Wasserstein重心的一般性公式，它具有良好的正则性、近似性、稳定性和（无网格）优化特性。该重心被定义为唯一的概率测度，它最小化了一组给定概率测度与熵最优传输（EOT）成本之和，再加上一个熵项。我们将其表示为$(\\lambda,\\tau)$-重心，其中$\\lambda$是内部正则化强度，$\\tau$是外部正则化强度。该公式通过选择不同的$\\lambda,\\tau \\geq 0$可以恢复并推广几种先前提出的EOT重心。首先，尽管——事实上也正因为——是“双重”正则化的，我们证明了当$\\tau=\\lambda/2$时，我们的公式是去偏的：对于平滑密度，（未正则化）Wasserstein重心目标中的次优性是熵正则化强度$\\lambda^2$的数量级，而不是一般的$\\max\\{\\lambda,\\tau\\}$。我们讨论了等向高斯分布的这种现象，其中所有$(\\lambda,\\tau)$-重心都有封闭形式。其次，我们证明了对于$\\lambda,\\tau>0$，该重心具有平滑密度，并且在边缘分布扰动下表现出强稳定性。特别是，它可以被有效地估计：给定从每个概率测度中抽取的$n$个样本，它以$n^{-1/2}$的速率在相对熵上收敛到总体重心。最后，该公式自然地适用于无网格优化算法：我们提出了一种简单的“噪声粒子梯度下降”，它在平均场极限下以指数速率全局收敛到重心。", "summary": "本文提出并研究了一种新颖的“双重正则化熵Wasserstein重心”通用公式，该重心通过最小化熵最优传输成本与额外熵项之和来定义，并由内外正则化参数$(\\lambda,\\tau)$控制。研究表明，该重心在特定条件（$\\tau=\\lambda/2$）下表现出去偏性，次优性为$\\lambda^2$量级。此外，当正则化参数为正时，该重心具有平滑密度和强稳定性，且能以$n^{-1/2}$的速率高效估计。论文还提出了一种适用于该公式的无网格“噪声粒子梯度下降”优化算法，并证明了其全局指数收敛性。", "keywords": "Wasserstein Barycenter, Entropic Optimal Transport, Regularization, Debiasing, Grid-free Optimization", "comments": "该论文的创新点在于提出了双重正则化 Wasserstein 重心，并揭示了其在特定正则化参数设置下的去偏特性，这对于实际应用中处理正则化引起的偏差具有重要意义。同时，该研究不仅提供了理论上的稳定性、收敛性保证，还提出了一种实用的无网格优化算法，提升了其在处理大数据集时的效率和普适性。"}}
{"id": "2507.13225", "title": "Signal Temporal Logic Compliant Co-design of Planning and Control", "authors": ["Manas Sashank Juvvi", "Tushar Dilip Kurne", "Vaishnavi J", "Shishir Kolathaya", "Pushpak Jagtap"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.13225v2", "summary": "This work presents a novel co-design strategy that integrates trajectory\nplanning and control to handle STL-based tasks in autonomous robots. The method\nconsists of two phases: $(i)$ learning spatio-temporal motion primitives to\nencapsulate the inherent robot-specific constraints and $(ii)$ constructing an\nSTL-compliant motion plan from these primitives. Initially, we employ\nreinforcement learning to construct a library of control policies that perform\ntrajectories described by the motion primitives. Then, we map motion primitives\nto spatio-temporal characteristics. Subsequently, we present a sampling-based\nSTL-compliant motion planning strategy tailored to meet the STL specification.\nThe proposed model-free approach, which generates feasible STL-compliant motion\nplans across various environments, is validated on differential-drive and\nquadruped robots across various STL specifications. Demonstration videos are\navailable at https://tinyurl.com/m6zp7rsm.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.13225v2", "cate": "cs.RO", "date": "2025-07-17", "updated": "2025-07-25", "AI": {"title_translation": "信号时序逻辑兼容的规划与控制协同设计", "tldr": "本文提出一种新颖的协同设计策略，将轨迹规划和控制集成，以处理自主机器人中的STL任务。", "motivation": "处理自主机器人中基于信号时序逻辑（STL）的任务，并通过集成轨迹规划和控制来解决。", "method": "该方法包括两个阶段：(i) 学习时空运动原语以封装固有的机器人特定约束（通过强化学习构建控制策略库）；(ii) 从这些原语构建STL兼容的运动计划（将运动原语映射到时空特性，然后采用基于采样的STL兼容运动规划策略）。该方法是无模型的。", "result": "该方法在差动驱动机器人和四足机器人上针对各种STL规范进行了验证，能够生成在各种环境中可行的STL兼容运动计划。", "conclusion": "提出的协同设计策略有效地实现了自主机器人中STL兼容的规划和控制。", "translation": "这项工作提出了一种新颖的协同设计策略，该策略集成了轨迹规划和控制，以处理自主机器人中基于STL的任务。该方法包括两个阶段：(i) 学习时空运动原语以封装固有的机器人特定约束，以及(ii) 从这些原语构建STL兼容的运动计划。最初，我们采用强化学习来构建一个控制策略库，以执行由运动原语描述的轨迹。然后，我们将运动原语映射到时空特性。随后，我们提出了一种基于采样的STL兼容运动规划策略，专门用于满足STL规范。所提出的无模型方法在差动驱动机器人和四足机器人上针对各种STL规范进行了验证，该方法能在各种环境中生成可行的STL兼容运动计划。演示视频可在 https://tinyurl.com/m6zp7rsm 获取。", "summary": "本文介绍了一种创新的协同设计策略，旨在为自主机器人提供信号时序逻辑（STL）兼容的规划与控制。该策略分为两步：首先通过强化学习学习封装机器人约束的时空运动原语；其次，基于这些原语构建STL兼容的运动计划。该无模型方法已在多种机器人上验证，能生成可行且满足STL规范的运动计划。", "keywords": "信号时序逻辑, 协同设计, 轨迹规划, 机器人控制, 强化学习", "comments": "这篇论文的创新点在于提出了一个将轨迹规划和控制深度融合的协同设计框架，并利用强化学习来学习运动原语，从而实现对STL任务的有效处理。其无模型的特性和在不同机器人上的验证显示了其通用性和实用性。"}}
{"id": "2507.18643", "title": "A Regression-Based Share Market Prediction Model for Bangladesh", "authors": ["Syeda Tasnim Fabiha", "Rubaiyat Jahan Mumu", "Farzana Aktar", "B M Mainul Hossain"], "categories": ["q-fin.ST", "cs.LG", "62J05, 62P20 62J05, 62P20 62J05, 62P20", "I.2.6; J.1; J.4"], "primary_category": "Subjects:       Statistical Finance (q-fin.ST)", "pdf_link": null, "comments": "Comments:      Originally written in 2018. Updated in 2025 for open-access archiving. Not previously published", "url": "http://arxiv.org/abs/2507.18643v1", "summary": "Share market is one of the most important sectors of economic development of\na country. Everyday almost all companies issue their shares and investors buy\nand sell shares of these companies. Generally investors want to buy shares of\nthe companies whose market liquidity is comparatively greater. Market liquidity\ndepends on the average price of a share. In this paper, a thorough linear\nregression analysis has been performed on the stock market data of Dhaka Stock\nExchange. Later, the linear model has been compared with random forest based on\ndifferent metrics showing better results for random forest model. However, the\namount of individual significance of different factors on the variability of\nstock price has been identified and explained. This paper also shows that the\ntime series data is not capable of generating a predictive linear model for\nanalysis.", "comment": "Originally written in 2018. Updated in 2025 for open-access\n  archiving. Not previously published", "pdf_url": "http://arxiv.org/pdf/2507.18643v1", "cate": "q-fin.ST", "date": "2025-07-10", "updated": "2025-07-10", "AI": {"title_translation": "孟加拉国基于回归的股票市场预测模型", "tldr": "本文提出并比较了孟加拉国股票市场的预测模型，发现随机森林模型优于线性回归，并指出时间序列数据不适合构建预测性线性模型。", "motivation": "股票市场是国家经济发展的重要组成部分，投资者希望购买流动性更高的股票，而流动性受股票平均价格影响，因此需要有效的预测模型来辅助决策。", "method": "对达卡证券交易所的股票市场数据进行了详尽的线性回归分析。随后，将线性模型与随机森林模型根据不同指标进行比较。同时，识别并解释了不同因素对股价波动性的个体显著性。", "result": "随机森林模型在不同指标上显示出比线性回归模型更好的预测结果。研究识别并解释了不同因素对股价波动性的个体显著性。此外，研究发现时间序列数据无法生成用于分析的预测性线性模型。", "conclusion": "随机森林模型在孟加拉国股票市场预测中表现优于线性回归模型。时间序列数据不适合构建预测性线性模型。", "translation": "股票市场是一个国家经济发展最重要的部门之一。几乎所有公司每天都发行股票，投资者买卖这些公司的股票。通常，投资者希望购买市场流动性相对较高的公司的股票。市场流动性取决于股票的平均价格。本文对达卡证券交易所的股票市场数据进行了彻底的线性回归分析。随后，将线性模型与基于不同指标的随机森林模型进行比较，结果显示随机森林模型表现更好。然而，不同因素对股价波动性个体显著性的程度已被识别和解释。本文还表明，时间序列数据无法生成用于分析的预测性线性模型。", "summary": "本文针对孟加拉国股票市场，提出了一个基于回归的预测模型。研究对达卡证券交易所的股票数据进行了线性回归分析，并将其与随机森林模型进行比较。结果表明，随机森林模型在预测性能上优于线性回归。此外，研究还量化并解释了影响股价波动的各个因素的显著性，并指出时间序列数据不适合构建预测性线性预测模型。", "keywords": "股票市场预测, 线性回归, 随机森林, 达卡证券交易所, 时间序列数据", "comments": "本文通过比较线性回归和随机森林模型，为孟加拉国股票市场预测提供了实用见解，特别是指出了随机森林的优越性。其创新点在于对不同因素个体显著性的识别，以及对时间序列数据在构建线性预测模型方面局限性的揭示，这对于后续研究具有指导意义。"}}
{"id": "2310.09278", "title": "Disentangled Latent Spaces Facilitate Data-Driven Auxiliary Learning", "authors": ["Geri Skenderi", "Luigi Capogrosso", "Andrea Toaiari", "Matteo Denitto", "Franco Fummi", "Simone Melzi"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Accepted at ICIAP25", "url": "http://arxiv.org/abs/2310.09278v3", "summary": "Auxiliary tasks facilitate learning in situations where data is scarce or the\nprincipal task of interest is extremely complex. This idea is primarily\ninspired by the improved generalization capability induced by solving multiple\ntasks simultaneously, which leads to a more robust shared representation.\nNevertheless, finding optimal auxiliary tasks is a crucial problem that often\nrequires hand-crafted solutions or expensive meta-learning approaches. In this\npaper, we propose a novel framework, dubbed Detaux, whereby a weakly supervised\ndisentanglement procedure is used to discover a new unrelated auxiliary\nclassification task, which allows us to go from a Single-Task Learning (STL) to\na Multi-Task Learning (MTL) problem. The disentanglement procedure works at the\nrepresentation level, isolating the variation related to the principal task\ninto an isolated subspace and additionally producing an arbitrary number of\northogonal subspaces, each of which encourages high separability among\nprojections. We generate the auxiliary classification task through a clustering\nprocedure on the most disentangled subspace, obtaining a discrete set of\nlabels. Subsequently, the original data, the labels associated with the\nprincipal task, and the newly discovered ones can be fed into any MTL\nframework. Experimental validation on both synthetic and real data, along with\nvarious ablation studies, demonstrates promising results, revealing the\npotential in what has been, so far, an unexplored connection between learning\ndisentangled representations and MTL. The source code is available at\nhttps://github.com/intelligolabs/Detaux.", "comment": "Accepted at ICIAP25", "pdf_url": "http://arxiv.org/pdf/2310.09278v3", "cate": "cs.LG", "date": "2023-10-13", "updated": "2025-07-25", "AI": {"title_translation": "解耦潜在空间促进数据驱动的辅助学习", "tldr": "本文提出Detaux框架，通过弱监督解耦程序自动发现辅助分类任务，将单任务学习转化为多任务学习，并在合成和真实数据上取得了有希望的结果。", "motivation": "在数据稀缺或主要任务极其复杂的情况下，辅助任务有助于学习，但寻找最优辅助任务通常需要手工设计或昂贵的元学习方法。", "method": "本文提出了一个名为Detaux的新颖框架，它使用弱监督解耦过程来发现一个新的无关辅助分类任务。该解耦过程在表示层进行，将与主要任务相关的变异隔离到一个独立的子空间中，并生成任意数量的正交子空间。通过对最解耦的子空间进行聚类，生成辅助分类任务的离散标签集，然后将原始数据、主要任务标签和新发现的标签输入任何多任务学习框架。", "result": "在合成数据和真实数据上的实验验证，以及各种消融研究，都展示了有希望的结果，揭示了解耦表示学习与多任务学习之间迄今未被探索的联系的潜力。", "conclusion": "本文揭示了解耦表示学习与多任务学习之间迄今未被探索的联系的潜力。", "translation": "辅助任务在数据稀缺或主要任务极其复杂的情况下有助于学习。这一思想主要受到通过同时解决多个任务来提高泛化能力的启发，这导致了更鲁棒的共享表示。然而，寻找最优辅助任务是一个关键问题，通常需要手工设计解决方案或昂贵的元学习方法。在本文中，我们提出了一个新颖的框架，名为Detaux，其中使用弱监督解耦过程来发现一个新的无关辅助分类任务，这使我们能够从单任务学习（STL）问题转向多任务学习（MTL）问题。解耦过程在表示层进行，将与主要任务相关的变异隔离到一个独立的子空间中，并额外生成任意数量的正交子空间，每个子空间都鼓励投影之间的高度可分离性。我们通过对最解耦的子空间进行聚类来生成辅助分类任务，从而获得一组离散的标签。随后，原始数据、与主要任务相关的标签以及新发现的标签可以输入任何MTL框架。在合成数据和真实数据上的实验验证，以及各种消融研究，都展示了有希望的结果，揭示了解耦表示学习与MTL之间迄今未被探索的联系的潜力。源代码可在https://github.com/intelligolabs/Detaux获取。", "summary": "本文提出了一种名为Detaux的新颖框架，旨在通过数据驱动的方式自动发现辅助任务，以促进多任务学习。该框架利用弱监督解耦过程在表示层分离出与主要任务无关的潜在子空间，并在此基础上通过聚类生成新的辅助分类标签。这种方法将单任务学习问题转化为多任务学习问题，避免了传统方法中手工设计或昂贵的元学习需求。实验结果表明，该方法在合成和真实数据集上均表现出良好性能，揭示了解耦表示与多任务学习之间未被探索的潜在联系。", "keywords": "解耦表示, 辅助学习, 多任务学习, 数据驱动, Detaux", "comments": "该论文的创新点在于提出了一种数据驱动的方法来自动发现辅助任务，避免了对人工设计或昂贵元学习的依赖。通过利用解耦潜在空间来生成辅助任务，它揭示了解耦表示学习与多任务学习之间一个未被充分探索的联系，这对于在数据稀缺或复杂任务场景下的学习具有重要意义。该方法提供了一个通用的框架，可以集成到任何多任务学习设置中。"}}
{"id": "2504.10568", "title": "AgMMU: A Comprehensive Agricultural Multimodal Understanding Benchmark", "authors": ["Aruna Gauba", "Irene Pi", "Yunze Man", "Ziqi Pang", "Vikram S. Adve", "Yu-Xiong Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project Website: this https URL Huggingface: this https URL", "url": "http://arxiv.org/abs/2504.10568v2", "summary": "We present AgMMU, a challenging real-world benchmark for evaluating and\nadvancing vision-language models (VLMs) in the knowledge-intensive domain of\nagriculture. Unlike prior datasets that rely on crowdsourced prompts, AgMMU is\ndistilled from 116,231 authentic dialogues between everyday growers and\nUSDA-authorized Cooperative Extension experts. Through a three-stage pipeline:\nautomated knowledge extraction, QA generation, and human verification, we\nconstruct (i) AgMMU, an evaluation set of 746 multiple-choice questions (MCQs)\nand 746 open-ended questions (OEQs), and (ii) AgBase, a development corpus of\n57,079 multimodal facts covering five high-stakes agricultural topics: insect\nidentification, species identification, disease categorization, symptom\ndescription, and management instruction. Benchmarking 12 leading VLMs reveals\npronounced gaps in fine-grained perception and factual grounding. Open-sourced\nmodels trail after proprietary ones by a wide margin. Simple fine-tuning on\nAgBase boosts open-sourced model performance on challenging OEQs for up to\n11.6% on average, narrowing this gap and also motivating future research to\npropose better strategies in knowledge extraction and distillation from AgBase.\nWe hope AgMMU stimulates research on domain-specific knowledge integration and\ntrustworthy decision support in agriculture AI development.", "comment": "Project Website: https://agmmu.github.io/ Huggingface:\n  https://huggingface.co/datasets/AgMMU/AgMMU_v1/", "pdf_url": "http://arxiv.org/pdf/2504.10568v2", "cate": "cs.CV", "date": "2025-04-14", "updated": "2025-07-24", "AI": {"title_translation": "AgMMU：一个全面的农业多模态理解基准", "tldr": "AgMMU是一个新的农业领域多模态理解基准，由真实对话数据构建，用于评估和提升视觉语言模型，并揭示了现有模型的不足。", "motivation": "现有的视觉语言模型在农业知识密集型领域的评估和发展面临挑战，且先前的农业数据集依赖众包提示，缺乏真实性。本研究旨在通过构建一个基于真实对话的综合基准来解决这些问题。", "method": "提出AgMMU基准，该基准从116,231个农民与美国农业部（USDA）专家的真实对话中提取。通过三阶段管道（自动化知识提取、问答生成和人工验证）构建：1) AgMMU评估集，包含746个选择题和746个开放式问题；2) AgBase开发语料库，包含57,079个多模态事实，涵盖昆虫识别、物种识别、疾病分类、症状描述和管理指导五个农业主题。该研究还对12个领先的视觉语言模型进行了基准测试，并在AgBase上对开源模型进行了微调。", "result": "对12个领先的视觉语言模型进行基准测试后，发现它们在细粒度感知和事实基础方面存在显著差距。开源模型表现远低于专有模型。在AgBase上对开源模型进行简单微调，可使开放式问题的性能平均提升高达11.6%，缩小了与专有模型之间的差距。", "conclusion": "AgMMU基准揭示了当前视觉语言模型在农业领域知识密集型任务中的不足，并证明了通过领域特定数据微调可以有效提升模型性能，激励未来在知识提取和可信决策支持方面的研究。", "translation": "我们提出了AgMMU，这是一个具有挑战性的真实世界基准，用于评估和推进农业知识密集型领域的视觉语言模型（VLMs）。与之前依赖众包提示的数据集不同，AgMMU是从116,231个日常种植者和美国农业部授权的合作推广专家之间的真实对话中提炼出来的。通过三阶段管道：自动化知识提取、问答生成和人工验证，我们构建了（i）AgMMU，一个包含746个多项选择题（MCQs）和746个开放式问题（OEQs）的评估集，以及（ii）AgBase，一个包含57,079个多模态事实的开发语料库，涵盖五个高风险农业主题：昆虫识别、物种识别、疾病分类、症状描述和管理指导。对12个领先的VLM进行基准测试揭示了在细粒度感知和事实基础方面的显著差距。开源模型与专有模型之间存在巨大差距。在AgBase上进行简单的微调，可使开源模型在具有挑战性的开放式问题上的性能平均提高高达11.6%，缩小了这一差距，也激励了未来的研究提出更好的知识提取和AgBase知识蒸馏策略。我们希望AgMMU能刺激农业AI开发中领域特定知识整合和可信决策支持的研究。", "summary": "本文介绍了AgMMU，一个从真实农业对话中提炼出的综合性多模态理解基准，旨在评估和提升农业领域的视觉语言模型。该基准包含评估集（AgMMU）和开发语料库（AgBase）。通过对现有模型的基准测试，发现模型在细粒度感知和事实基础方面存在显著不足，尤其开源模型表现不佳。研究表明，在AgBase上进行简单微调可显著提升开源模型性能，并鼓励未来在领域知识集成和可信农业AI决策支持方面的研究。", "keywords": "农业AI, 多模态理解, 视觉语言模型, 基准测试, 知识提取", "comments": "AgMMU的创新之处在于其数据来源的真实性，即从农民与专家的实际对话中提取，这极大地提升了数据集的实用性和领域相关性，避免了众包数据的偏见。它不仅提供了一个评估基准，还提供了一个开发语料库，有助于领域知识的集成。该研究明确指出了当前视觉语言模型在农业特定知识理解上的不足，并提供了有效的改进方向（如微调），对推动农业AI的发展具有重要意义。"}}
{"id": "2507.18661", "title": "Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by Reinforcement Learning from Visual Map Feed Back", "authors": ["Ruixing Zhang", "Yang Zhang", "Tongyu Zhu", "Leilei Sun", "Weifeng Lv"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18661v2", "summary": "Next Location Prediction is a fundamental task in the study of human\nmobility, with wide-ranging applications in transportation planning, urban\ngovernance, and epidemic forecasting. In practice, when humans attempt to\npredict the next location in a trajectory, they often visualize the trajectory\non a map and reason based on road connectivity and movement trends. However,\nthe vast majority of existing next-location prediction models do not reason\nover maps \\textbf{in the way that humans do}. Fortunately, the recent\ndevelopment of Vision-Language Models (VLMs) has demonstrated strong\ncapabilities in visual perception and even visual reasoning. This opens up a\nnew possibility: by rendering both the road network and trajectory onto an\nimage and leveraging the reasoning abilities of VLMs, we can enable models to\nperform trajectory inference in a human-like manner. To explore this idea, we\nfirst propose a method called Vision-Guided Location Search (VGLS), which\nevaluates whether a general-purpose VLM is capable of trajectory-based\nreasoning without modifying any of its internal parameters. Based on insights\nfrom the VGLS results, we further propose our main approach: VLMLocPredictor,\nwhich is composed of two stages: In the first stage, we design two Supervised\nFine-Tuning (SFT) tasks that help the VLM understand road network and\ntrajectory structures and acquire basic reasoning ability on such visual\ninputs. In the second stage, we introduce Reinforcement Learning from Visual\nMap Feedback, enabling the model to self-improve its next-location prediction\nability through interaction with the environment. Experiments conducted on\ndatasets from four different cities show that our method achieves\nstate-of-the-art (SOTA) performance and exhibits superior cross-city\ngeneralization compared to other LLM-based approaches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18661v2", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-28", "AI": {"title_translation": "闭目：一种基于视觉的下一代GPS位置预测模型，通过视觉地图反馈强化学习", "tldr": "该研究提出VLMLocPredictor模型，利用视觉语言模型（VLM）和强化学习，通过将道路网络和轨迹渲染成图像，实现类似人类的下一地点预测，并在多城市数据集上达到SOTA性能和优越的跨城市泛化能力。", "motivation": "现有的大多数下一地点预测模型无法像人类一样通过可视化地图进行推理。随着视觉语言模型（VLMs）在视觉感知和推理方面的进展，研究人员希望利用VLMs使模型能够以类人方式进行轨迹推理。", "method": "1. 提出Vision-Guided Location Search (VGLS) 方法，评估通用VLM在不修改内部参数的情况下是否能进行基于轨迹的推理。2. 基于VGLS结果，提出主要方法VLMLocPredictor，分为两阶段：a) 第一阶段，设计两个监督微调（SFT）任务，帮助VLM理解道路网络和轨迹结构，并获得对视觉输入的基本推理能力。b) 第二阶段，引入从视觉地图反馈中进行的强化学习，使模型通过与环境交互自我提升下一地点预测能力。", "result": "在来自四个不同城市的数据集上进行的实验表明，该方法实现了最先进（SOTA）的性能，并且与其他基于LLM的方法相比，展现出卓越的跨城市泛化能力。", "conclusion": "该研究成功地利用视觉语言模型和强化学习，通过模拟人类对地图的视觉推理方式，显著提升了下一地点预测的准确性和跨城市泛化能力。", "translation": "下一地点预测是人类出行研究中的一项基础任务，在交通规划、城市治理和流行病预测等领域有着广泛的应用。在实践中，当人类尝试预测轨迹中的下一个地点时，他们通常会在地图上可视化轨迹，并基于道路连通性和移动趋势进行推理。然而，绝大多数现有的下一地点预测模型并不能以人类的方式对地图进行推理。幸运的是，视觉语言模型（VLMs）的最新发展在视觉感知甚至视觉推理方面展现出强大的能力。这开辟了一种新的可能性：通过将道路网络和轨迹都渲染成图像，并利用VLMs的推理能力，我们可以使模型以类人方式进行轨迹推断。为了探索这一想法，我们首先提出了一种名为Vision-Guided Location Search (VGLS) 的方法，该方法评估通用VLM在不修改任何内部参数的情况下是否能够进行基于轨迹的推理。基于VGLS结果的见解，我们进一步提出了我们的主要方法：VLMLocPredictor，它由两个阶段组成：在第一阶段，我们设计了两个监督微调（SFT）任务，帮助VLM理解道路网络和轨迹结构，并获取对此类视觉输入的基本推理能力。在第二阶段，我们引入了来自视觉地图反馈的强化学习，使模型能够通过与环境的交互来自我提升其下一地点预测能力。在来自四个不同城市的数据集上进行的实验表明，我们的方法实现了最先进（SOTA）的性能，并且与其他基于LLM的方法相比，展现出卓越的跨城市泛化能力。", "summary": "本文提出VLMLocPredictor，一个基于视觉的下一地点预测模型，旨在模仿人类通过可视化地图进行推理的方式。该模型利用视觉语言模型（VLM）处理渲染的道路网络和轨迹图像，并通过监督微调（SFT）和从视觉地图反馈中进行的强化学习来提升其推理能力。实验结果表明，该方法在多个城市数据集上取得了最先进的性能，并展现出优于其他大型语言模型（LLM）方法的跨城市泛化能力。", "keywords": "下一地点预测, 视觉语言模型, 强化学习, 轨迹推理, 跨城市泛化", "comments": "本文的创新点在于将视觉语言模型引入到下一地点预测任务中，并模拟人类通过视觉地图进行推理的过程。通过结合监督微调和强化学习，模型能够有效学习复杂的视觉空间关系，并显著提升了模型的泛化能力，尤其是在跨城市应用方面。这种将视觉信息与轨迹预测相结合的方法，为未来的交通规划和城市管理提供了新的思路。"}}
{"id": "2507.19095", "title": "GCL-GCN: Graphormer and Contrastive Learning Enhanced Attributed Graph Clustering Network", "authors": ["Binxiong Li", "Xu Xiang", "Xue Li", "Binyu Zhao", "Yujie Liu", "Huijie Tang", "Benhan Yang", "Zhixuan Chen"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The source code for this study is available at this https URL", "url": "http://arxiv.org/abs/2507.19095v1", "summary": "Attributed graph clustering holds significant importance in modern data\nanalysis. However, due to the complexity of graph data and the heterogeneity of\nnode attributes, leveraging graph information for clustering remains\nchallenging. To address this, we propose a novel deep graph clustering model,\nGCL-GCN, specifically designed to address the limitations of existing models in\ncapturing local dependencies and complex structures when dealing with sparse\nand heterogeneous graph data. GCL-GCN introduces an innovative Graphormer\nmodule that combines centrality encoding and spatial relationships, effectively\ncapturing both global and local information between nodes, thereby enhancing\nthe quality of node representations. Additionally, we propose a novel\ncontrastive learning module that significantly enhances the discriminative\npower of feature representations. In the pre-training phase, this module\nincreases feature distinction through contrastive learning on the original\nfeature matrix, ensuring more identifiable initial representations for\nsubsequent graph convolution and clustering tasks. Extensive experimental\nresults on six datasets demonstrate that GCL-GCN outperforms 14 advanced\nmethods in terms of clustering quality and robustness. Specifically, on the\nCora dataset, it improves ACC, NMI, and ARI by 4.94%, 13.01%, and 10.97%,\nrespectively, compared to the primary comparison method MBN.", "comment": "The source code for this study is available at\n  https://github.com/YF-W/GCL-GCN", "pdf_url": "http://arxiv.org/pdf/2507.19095v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "GCL-GCN：图Transformer和对比学习增强的属性图聚类网络", "tldr": "GCL-GCN是一个新的深度图聚类模型，它结合了Graphormer模块来捕获全局和局部图信息，以及对比学习模块来增强特征表示的区分性，从而在属性图聚类任务中显著优于现有SOTA方法。", "motivation": "由于图数据的复杂性和节点属性的异构性，利用图信息进行聚类仍然具有挑战性。现有模型在处理稀疏和异构图数据时，在捕获局部依赖和复杂结构方面存在局限性。", "method": "本文提出了GCL-GCN模型。它引入了一个创新的Graphormer模块，结合了中心性编码和空间关系，有效捕获节点间的全局和局部信息，以增强节点表示质量。此外，还提出了一个新颖的对比学习模块，通过在原始特征矩阵上进行对比学习，显著增强特征表示的区分性，确保后续图卷积和聚类任务的初始表示更具可识别性。", "result": "在六个数据集上的大量实验结果表明，GCL-GCN在聚类质量和鲁棒性方面优于14种先进方法。具体而言，在Cora数据集上，与主要对比方法MBN相比，其ACC、NMI和ARI分别提高了4.94%、13.01%和10.97%。", "conclusion": "GCL-GCN通过结合Graphormer和对比学习，有效解决了属性图聚类中捕获复杂结构和增强特征区分性的挑战，实现了卓越的聚类性能和鲁棒性。", "translation": "属性图聚类在现代数据分析中具有重要意义。然而，由于图数据的复杂性和节点属性的异构性，利用图信息进行聚类仍然具有挑战性。为了解决这个问题，我们提出了一种新颖的深度图聚类模型GCL-GCN，专门设计用于解决现有模型在处理稀疏和异构图数据时，在捕获局部依赖和复杂结构方面的局限性。GCL-GCN引入了一个创新的Graphormer模块，该模块结合了中心性编码和空间关系，有效捕获节点间的全局和局部信息，从而增强了节点表示的质量。此外，我们提出了一种新颖的对比学习模块，显著增强了特征表示的区分能力。在预训练阶段，该模块通过在原始特征矩阵上进行对比学习来增加特征区分度，确保后续图卷积和聚类任务的初始表示更具可识别性。在六个数据集上的大量实验结果表明，GCL-GCN在聚类质量和鲁棒性方面优于14种先进方法。具体而言，在Cora数据集上，与主要对比方法MBN相比，其ACC、NMI和ARI分别提高了4.94%、13.01%和10.97%。", "summary": "本文提出了GCL-GCN，一个用于属性图聚类的新型深度模型，旨在克服现有方法在处理稀疏和异构图数据时捕获复杂结构和局部依赖的不足。GCL-GCN整合了Graphormer模块，用于有效捕获节点的全局和局部信息以优化节点表示，并结合了一个对比学习模块，通过预训练阶段的对比学习显著提升特征表示的区分能力。实验结果表明，GCL-GCN在多个数据集上的聚类质量和鲁棒性均超越了现有先进方法。", "keywords": "属性图聚类, Graphormer, 对比学习, 图神经网络, 深度学习", "comments": "GCL-GCN的创新之处在于其将Graphormer模块与对比学习相结合，有效地解决了属性图聚类中捕获复杂图信息和增强特征区分性的难题。Graphormer模块通过结合中心性编码和空间关系，能够全面地捕捉图的全局和局部结构，而对比学习则进一步提升了特征表示的鲁棒性和可识别性，这对于处理复杂和异构的图数据至关重要。该方法在多个数据集上的显著性能提升，证明了其在深度图聚类领域的先进性和实用价值。"}}
{"id": "2507.19131", "title": "MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective", "authors": ["Weitian Wang", "Rai Shubham", "Cecilia De La Parra", "Akash Kumar"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.19131v1", "summary": "In this paper, we propose MixA-Q, a mixed-precision activation quantization\nframework that leverages intra-layer activation sparsity (a concept widely\nexplored in activation pruning methods) for efficient inference of quantized\nwindow-based vision transformers. For a given uniform-bit quantization\nconfiguration, MixA-Q separates the batched window computations within Swin\nblocks and assigns a lower bit width to the activations of less important\nwindows, improving the trade-off between model performance and efficiency. We\nintroduce a Two-Branch Swin Block that processes activations separately in\nhigh- and low-bit precision, enabling seamless integration of our method with\nmost quantization-aware training (QAT) and post-training quantization (PTQ)\nmethods, or with simple modifications. Our experimental evaluations over the\nCOCO dataset demonstrate that MixA-Q achieves a training-free 1.35x\ncomputational speedup without accuracy loss in PTQ configuration. With QAT,\nMixA-Q achieves a lossless 1.25x speedup and a 1.53x speedup with only a 1% mAP\ndrop by incorporating activation pruning. Notably, by reducing the quantization\nerror in important regions, our sparsity-aware quantization adaptation improves\nthe mAP of the quantized W4A4 model (with both weights and activations in 4-bit\nprecision) by 0.7%, reducing quantization degradation by 24%.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.19131v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "MixA-Q：从混合精度量化角度重新审视视觉Transformer中的激活稀疏性", "tldr": "MixA-Q通过对视觉Transformer中不重要的激活窗口使用低位宽量化，提高了计算效率并保持了性能。", "motivation": "本论文旨在利用层内激活稀疏性，为量化窗口基视觉Transformer实现高效推理，以改善模型性能和效率之间的权衡。", "method": "提出MixA-Q框架，该框架在Swin块内分离批处理窗口计算，并为不重要的窗口激活分配较低的位宽。引入了双分支Swin块，分别以高位和低位精度处理激活，从而能够无缝集成到大多数量化感知训练（QAT）和训练后量化（PTQ）方法中。", "result": "在COCO数据集上，MixA-Q在PTQ配置下实现了1.35倍的无精度损失计算加速。通过QAT，MixA-Q实现了无损1.25倍加速，结合激活剪枝可在仅1% mAP下降的情况下实现1.53倍加速。通过减少重要区域的量化误差，量化W4A4模型（权重和激活均为4位）的mAP提高了0.7%，量化退化减少了24%。", "conclusion": "MixA-Q通过利用激活稀疏性进行混合精度量化，显著提高了视觉Transformer的推理效率，同时保持或提升了模型性能，尤其在减少量化误差方面表现突出。", "translation": "在本文中，我们提出了MixA-Q，一个混合精度激活量化框架，它利用层内激活稀疏性（一个在激活剪枝方法中广泛探索的概念）来实现量化窗口基视觉Transformer的高效推理。对于给定的统一位宽量化配置，MixA-Q在Swin块内分离批处理窗口计算，并为不重要的窗口激活分配较低的位宽，从而改善了模型性能和效率之间的权衡。我们引入了一个双分支Swin块，分别以高位和低位精度处理激活，这使得我们的方法可以无缝集成到大多数量化感知训练（QAT）和训练后量化（PTQ）方法中，或者只需进行简单修改即可。我们在COCO数据集上的实验评估表明，MixA-Q在PTQ配置下实现了1.35倍的无精度损失计算加速。通过QAT，MixA-Q实现了无损1.25倍加速，并且在仅有1% mAP下降的情况下，通过结合激活剪枝实现了1.53倍加速。值得注意的是，通过减少重要区域的量化误差，我们这种稀疏性感知量化适应方法将量化W4A4模型（权重和激活均为4位精度）的mAP提高了0.7%，将量化退化降低了24%。", "summary": "MixA-Q是一种混合精度激活量化框架，专为视觉Transformer设计，通过利用层内激活稀疏性，为不重要的窗口激活分配较低位宽，从而提升推理效率和性能。该方法引入了双分支Swin块，便于与现有QAT和PTQ方法集成。实验结果表明，MixA-Q在保持甚至提高模型精度的同时，显著加速了计算。", "keywords": "混合精度量化, 激活稀疏性, 视觉Transformer, 量化感知训练, 训练后量化", "comments": "MixA-Q的创新之处在于将激活稀疏性与混合精度量化相结合，并引入了双分支Swin块，使得该方法能够灵活地应用于现有的量化训练范式。其在不损失精度的情况下实现计算加速，并能减少量化误差，对视觉Transformer的部署具有重要意义。"}}
{"id": "2507.19359", "title": "SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning", "authors": ["Lanmiao Liu", "Esam Ghaleb", "Aslı Özyürek", "Zerrin Yumak"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to IEEE/CVF International Conference on Computer Vision (ICCV) 2025", "url": "http://arxiv.org/abs/2507.19359v1", "summary": "Creating a virtual avatar with semantically coherent gestures that are\naligned with speech is a challenging task. Existing gesture generation research\nmainly focused on generating rhythmic beat gestures, neglecting the semantic\ncontext of the gestures. In this paper, we propose a novel approach for\nsemantic grounding in co-speech gesture generation that integrates semantic\ninformation at both fine-grained and global levels. Our approach starts with\nlearning the motion prior through a vector-quantized variational autoencoder.\nBuilt on this model, a second-stage module is applied to automatically generate\ngestures from speech, text-based semantics and speaker identity that ensures\nconsistency between the semantic relevance of generated gestures and\nco-occurring speech semantics through semantic coherence and relevance modules.\nExperimental results demonstrate that our approach enhances the realism and\ncoherence of semantic gestures. Extensive experiments and user studies show\nthat our method outperforms state-of-the-art approaches across two benchmarks\nin co-speech gesture generation in both objective and subjective metrics. The\nqualitative results of our model, code, dataset and pre-trained models can be\nviewed at https://semgesture.github.io/.", "comment": "Accepted to IEEE/CVF International Conference on Computer Vision\n  (ICCV) 2025", "pdf_url": "http://arxiv.org/pdf/2507.19359v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "SemGes：基于语义连贯性和相关性学习的语义感知协同语音手势生成", "tldr": "SemGes提出了一种新的方法，用于生成与语音语义连贯的协同语音手势，通过整合细粒度和全局语义信息，并在客观和主观指标上超越了现有最先进的方法。", "motivation": "为虚拟形象创建与语音语义连贯的手势是一项挑战性任务。现有的手势生成研究主要集中于生成有节奏的节拍手势，却忽略了手势的语义上下文。", "method": "本文提出了一种新的协同语音手势生成中的语义接地方法，该方法在细粒度和全局层面整合了语义信息。该方法首先通过向量量化变分自编码器学习运动先验。在此模型基础上，应用第二阶段模块，从语音、基于文本的语义和说话人身份自动生成手势，通过语义连贯性和相关性模块确保生成手势的语义相关性与协同发生的语音语义之间的一致性。", "result": "实验结果表明，该方法增强了语义手势的真实性和连贯性。大量的实验和用户研究表明，该方法在协同语音手势生成的两个基准测试中，在客观和主观指标上均优于现有最先进的方法。", "conclusion": "该研究成功提出了一种新颖的语义感知协同语音手势生成方法SemGes，通过有效整合语义信息，显著提高了生成手势的真实性和语义连贯性，并在性能上超越了现有技术。", "translation": "创建具有与语音对齐的语义连贯手势的虚拟形象是一项具有挑战性的任务。现有的手势生成研究主要集中于生成有节奏的节拍手势，而忽略了手势的语义上下文。在本文中，我们提出了一种用于协同语音手势生成中语义接地的创新方法，该方法在细粒度和全局层面整合了语义信息。我们的方法首先通过向量量化变分自编码器学习运动先验。在此模型基础上，应用第二阶段模块，从语音、基于文本的语义和说话人身份自动生成手势，通过语义连贯性和相关性模块确保生成手势的语义相关性与协同发生的语音语义之间的一致性。实验结果表明，我们的方法增强了语义手势的真实性和连贯性。大量的实验和用户研究表明，我们的方法在协同语音手势生成的两个基准测试中，在客观和主观指标上均优于现有最先进的方法。我们模型的定性结果、代码、数据集和预训练模型可在 https://semgesture.github.io/ 查看。", "summary": "本文提出了一种名为SemGes的新型协同语音手势生成方法，旨在解决现有方法忽略手势语义上下文的问题。SemGes通过向量量化变分自编码器学习运动先验，并结合一个两阶段模块，从语音、文本语义和说话人身份生成手势，确保生成手势的语义相关性与语音语义的一致性。实验证明，SemGes显著提升了语义手势的真实性和连贯性，并在多项客观和主观指标上超越了现有最先进的方法。", "keywords": "协同语音手势生成, 语义感知, 虚拟形象, 深度学习, 语义连贯性", "comments": "该论文的创新点在于其将语义信息整合到协同语音手势生成中的方法，尤其是在细粒度和全局层面进行整合。这解决了现有方法主要关注节奏性节拍手势而忽略语义上下文的局限性，显著提升了生成手势的语义连贯性和真实感，对于虚拟形象和人机交互领域具有重要意义。"}}
{"id": "2411.09476", "title": "Mean flow data assimilation using physics-constrained Graph Neural Networks", "authors": ["M. Quattromini", "M. A. Bucci", "S. Cherubini", "O. Semeraro"], "categories": ["physics.flu-dyn", "cs.LG"], "primary_category": "Subjects:       Fluid Dynamics (physics.flu-dyn)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.09476v3", "summary": "Despite their widespread use, purely data-driven methods often suffer from\noverfitting, lack of physical consistency, and high data dependency,\nparticularly when physical constraints are not incorporated. This study\nintroduces a novel data assimilation approach that integrates Graph Neural\nNetworks (GNNs) with optimisation techniques to enhance the accuracy of mean\nflow reconstruction, using Reynolds-Averaged Navier-Stokes (RANS) equations as\na baseline. The method leverages the adjoint approach, incorporating\nRANS-derived gradients as optimisation terms during GNN training, ensuring that\nthe learned model adheres to physical laws and maintains consistency.\nAdditionally, the GNN framework is well-suited for handling unstructured data,\nwhich is common in the complex geometries encountered in Computational Fluid\nDynamics (CFD). The GNN is interfaced with the Finite Element Method (FEM) for\nnumerical simulations, enabling accurate modelling in unstructured domains. We\nconsider the reconstruction of mean flow past bluff bodies at low Reynolds\nnumbers as a test case, addressing tasks such as sparse data recovery,\ndenoising, and inpainting of missing flow data. The key strengths of the\napproach lie in its integration of physical constraints into the GNN training\nprocess, leading to accurate predictions with limited data, making it\nparticularly valuable when data are scarce or corrupted. Results demonstrate\nsignificant improvements in the accuracy of mean flow reconstructions, even\nwith limited training data, compared to analogous purely data-driven models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.09476v3", "cate": "physics.flu-dyn", "date": "2024-11-14", "updated": "2025-07-25", "AI": {"title_translation": "使用物理约束图神经网络的平均流数据同化", "tldr": "本研究提出了一种结合图神经网络和优化技术的物理约束数据同化方法，用于在有限数据下准确重建平均流，解决了纯数据驱动方法的物理不一致性和数据依赖问题。", "motivation": "纯数据驱动方法在缺乏物理约束时常出现过拟合、缺乏物理一致性和高度数据依赖等问题。", "method": "本研究引入了一种新的数据同化方法，将图神经网络（GNNs）与优化技术相结合，以提高平均流重建的准确性。该方法利用伴随方法，在GNN训练过程中将RANS方程导出的梯度作为优化项，确保模型符合物理定律和保持一致性。GNN框架适用于处理非结构化数据，并与有限元法（FEM）接口进行数值模拟。", "result": "结果表明，即使在训练数据有限的情况下，与纯数据驱动模型相比，平均流重建的准确性显著提高。", "conclusion": "该方法的关键优势在于将物理约束集成到GNN训练过程中，即使数据稀疏或损坏也能实现准确预测。", "translation": "尽管纯数据驱动方法应用广泛，但它们常受限于过拟合、缺乏物理一致性以及高度数据依赖，尤其是在未纳入物理约束的情况下。本研究引入了一种新颖的数据同化方法，将图神经网络（GNNs）与优化技术相结合，以提高平均流重建的准确性，并以雷诺平均纳维-斯托克斯（RANS）方程作为基线。该方法利用伴随方法，在GNN训练期间将RANS导出的梯度作为优化项，确保学习到的模型符合物理定律并保持一致性。此外，GNN框架非常适合处理计算流体动力学（CFD）中复杂几何形状常见的非结构化数据。GNN与有限元法（FEM）接口进行数值模拟，从而能够在非结构化域中进行精确建模。我们考虑了低雷诺数下钝体周围平均流的重建作为测试案例，解决了稀疏数据恢复、去噪和缺失流数据填补等任务。该方法的关键优势在于将物理约束集成到GNN训练过程中，从而在有限数据下实现准确预测，这使得它在数据稀疏或损坏时尤为有价值。结果表明，即使在训练数据有限的情况下，与类似的纯数据驱动模型相比，平均流重建的准确性显著提高。", "summary": "本研究提出了一种新颖的物理约束数据同化方法，通过将图神经网络（GNNs）与优化技术相结合，利用雷诺平均纳维-斯托克斯（RANS）方程导出的梯度作为训练过程中的物理约束，克服了纯数据驱动方法在平均流重建中存在的物理不一致性和数据依赖性问题。该方法适用于非结构化数据，并与有限元法（FEM）结合，在有限数据下显著提高了平均流重建的准确性，尤其适用于数据稀疏或损坏的情况。", "keywords": "图神经网络, 数据同化, 物理约束, 雷诺平均纳维-斯托克斯, 有限元法", "comments": "该论文的创新之处在于将物理约束（通过RANS方程的伴随梯度）直接集成到图神经网络的训练过程中，这有效地解决了纯数据驱动模型在流体动力学领域中常见的物理不一致性和数据依赖性问题。这种混合方法在数据有限的情况下仍能提供高准确度的预测，对于实际工程应用具有重要价值。"}}
{"id": "2507.18809", "title": "Test-time Offline Reinforcement Learning on Goal-related Experience", "authors": ["Marco Bagatella", "Mert Albaba", "Jonas Hübotter", "Georg Martius", "Andreas Krause"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18809v1", "summary": "Foundation models compress a large amount of information in a single, large\nneural network, which can then be queried for individual tasks. There are\nstrong parallels between this widespread framework and offline goal-conditioned\nreinforcement learning algorithms: a universal value function is trained on a\nlarge number of goals, and the policy is evaluated on a single goal in each\ntest episode. Extensive research in foundation models has shown that\nperformance can be substantially improved through test-time training,\nspecializing the model to the current goal. We find similarly that test-time\noffline reinforcement learning on experience related to the test goal can lead\nto substantially better policies at minimal compute costs. We propose a novel\nself-supervised data selection criterion, which selects transitions from an\noffline dataset according to their relevance to the current state and quality\nwith respect to the evaluation goal. We demonstrate across a wide range of\nhigh-dimensional loco-navigation and manipulation tasks that fine-tuning a\npolicy on the selected data for a few gradient steps leads to significant\nperformance gains over standard offline pre-training. Our goal-conditioned\ntest-time training (GC-TTT) algorithm applies this routine in a\nreceding-horizon fashion during evaluation, adapting the policy to the current\ntrajectory as it is being rolled out. Finally, we study compute allocation at\ninference, demonstrating that, at comparable costs, GC-TTT induces performance\ngains that are not achievable by scaling model size.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18809v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "目标相关经验上的测试时离线强化学习", "tldr": "本文提出了一种在测试时使用与目标相关的离线经验进行微调的方法，以显著提高离线目标条件强化学习策略的性能，且计算成本极低，类似于基础模型中的测试时训练。", "motivation": "基础模型通过测试时训练显著提高性能，作者发现离线目标条件强化学习也存在类似之处。本文的动机是探索在测试时使用与测试目标相关的离线经验进行训练，以提升策略性能。", "method": "本文提出了一种新颖的自监督数据选择标准，根据与当前状态的相关性和对评估目标的质量，从离线数据集中选择转换。然后，通过在选定数据上进行少量梯度步骤的微调，应用目标条件测试时训练（GC-TTT）算法，以滚动视界的方式在评估期间调整策略。", "result": "在广泛的高维运动导航和操作任务中，与标准离线预训练相比，在选定数据上微调策略可带来显著的性能提升。在可比较的计算成本下，GC-TTT带来的性能增益是扩大模型规模无法实现的。", "conclusion": "本文证明了在测试时利用与目标相关的离线经验进行微调，可以以最小的计算成本显著提高离线强化学习策略的性能，并且在计算分配上，GC-TTT能够带来优于简单扩大模型规模的性能提升。", "translation": "基础模型将大量信息压缩在一个单一的大型神经网络中，然后可以针对单个任务进行查询。这种广泛的框架与离线目标条件强化学习算法之间存在着很强的相似性：通用价值函数在大量目标上进行训练，并且策略在每个测试回合中针对单个目标进行评估。基础模型方面的广泛研究表明，通过测试时训练（将模型专门化到当前目标）可以大幅提高性能。我们类似地发现，在与测试目标相关的经验上进行测试时离线强化学习，可以以最小的计算成本带来显著更好的策略。我们提出了一种新颖的自监督数据选择标准，根据转换与当前状态的相关性以及相对于评估目标的质量，从离线数据集中选择转换。我们在广泛的高维运动导航和操作任务中证明，在选定数据上对策略进行少量梯度步骤的微调，比标准离线预训练带来了显著的性能提升。我们的目标条件测试时训练（GC-TTT）算法在评估期间以滚动视界的方式应用此例程，在策略展开时对其进行调整。最后，我们研究了推理时的计算分配，证明在可比较的成本下，GC-TTT带来的性能增益是扩大模型规模无法实现的。", "summary": "本文提出了一种名为目标条件测试时训练（GC-TTT）的新方法，用于离线目标条件强化学习。该方法受基础模型中测试时训练的启发，通过在测试时利用与当前目标相关的离线经验对策略进行微调。论文引入了一种自监督数据选择标准，用于筛选相关且高质量的经验数据。实验结果表明，GC-TTT在多种高维任务中，以极低的计算成本实现了比标准离线预训练显著的性能提升，并且这种提升是单纯扩大模型规模无法达到的。", "keywords": "离线强化学习, 测试时训练, 目标条件, 自监督学习, 性能提升", "comments": "本文的创新点在于将基础模型中的“测试时训练”思想引入到离线强化学习领域，并提出了有效的数据选择策略。这种方法能够显著提升策略性能，同时保持较低的计算成本，为实际应用提供了新的思路。其重要性在于，它提供了一种在推理阶段动态适应目标的新范式，克服了传统离线RL在特定目标泛化上的局限性。"}}
{"id": "2507.19316", "title": "Human-AI Synergy in Adaptive Active Learning for Continuous Lithium Carbonate Crystallization Optimization", "authors": ["Shayan S. Mousavi Masouleh", "Corey A. Sanz", "Ryan P. Jansonius", "Cara Cronin", "Jason E. Hein", "Jason Hattrick-Simpers"], "categories": ["cs.CE", "cond-mat.mtrl-sci", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Materials Science (cond-mat.mtrl-sci)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19316v1", "summary": "As demand for high-purity lithium surges with the growth of the electric\nvehicle (EV) industry, cost-effective extraction from lower-grade North\nAmerican sources like the Smackover Formation is critical. These resources,\nunlike high-purity South American brines, require innovative purification\ntechniques to be economically viable. Continuous crystallization is a promising\nmethod for producing battery-grade lithium carbonate, but its optimization is\nchallenged by a complex parameter space and limited data. This study introduces\na Human-in-the-Loop (HITL) assisted active learning framework to optimize the\ncontinuous crystallization of lithium carbonate. By integrating human expertise\nwith data-driven insights, our approach accelerates the optimization of lithium\nextraction from challenging sources. Our results demonstrate the framework's\nability to rapidly adapt to new data, significantly improving the process's\ntolerance to critical impurities like magnesium from the industry standard of a\nfew hundred ppm to as high as 6000 ppm. This breakthrough makes the\nexploitation of low-grade, impurity-rich lithium resources feasible,\npotentially reducing the need for extensive pre-refinement processes. By\nleveraging artificial intelligence, we have refined operational parameters and\ndemonstrated that lower-grade materials can be used without sacrificing product\nquality. This advancement is a significant step towards economically harnessing\nNorth America's vast lithium reserves, such as those in the Smackover\nFormation, and enhancing the sustainability of the global lithium supply chain.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19316v1", "cate": "cs.CE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "人机协同自适应主动学习在连续碳酸锂结晶优化中的应用", "tldr": "本研究提出了一种人机协同的主动学习框架，用于优化碳酸锂的连续结晶过程，显著提高了对杂质的耐受性，使低品位锂资源得以经济利用。", "motivation": "随着电动汽车行业的发展，对高纯度锂的需求激增。从北美低品位资源（如Smackover组地层）中经济高效地提取锂至关重要，但这需要创新的纯化技术。连续结晶是生产电池级碳酸锂的有前景方法，但其优化面临参数空间复杂和数据有限的挑战。", "method": "本研究引入了一种人机协同（Human-in-the-Loop, HITL）辅助的主动学习框架，通过整合人类专业知识与数据驱动的洞察，加速碳酸锂连续结晶的优化。", "result": "该框架能够快速适应新数据，显著提高了工艺对关键杂质（如镁）的耐受性，从行业标准的几百ppm提升至高达6000ppm。这使得开发低品位、富含杂质的锂资源成为可能，并有望减少大量的预精炼过程。通过利用人工智能，优化了操作参数，证明了可以使用低品位材料而不牺牲产品质量。", "conclusion": "这项进展是经济开发北美（如Smackover组地层） vast 锂储量并增强全球锂供应链可持续性的重要一步。", "translation": "随着电动汽车（EV）行业的增长，对高纯度锂的需求激增，从Smackover组地层等北美低品位资源中经济高效地提取锂至关重要。这些资源与高纯度的南美盐湖卤水不同，需要创新的纯化技术才能实现经济可行性。连续结晶是生产电池级碳酸锂的一种有前景的方法，但其优化面临参数空间复杂和数据有限的挑战。本研究引入了一种人机协同（Human-in-the-Loop, HITL）辅助的主动学习框架，用于优化碳酸锂的连续结晶。通过整合人类专业知识与数据驱动的洞察，我们的方法加速了从挑战性来源中提取锂的优化。我们的结果表明，该框架能够快速适应新数据，显著提高了工艺对关键杂质（如镁）的耐受性，从行业标准的几百ppm提升至高达6000ppm。这一突破使得开发低品位、富含杂质的锂资源成为可能，有望减少大量的预精炼过程。通过利用人工智能，我们优化了操作参数，并证明了可以使用低品位材料而不牺牲产品质量。这一进展是经济开发北美（如Smackover组地层） vast 锂储量并增强全球锂供应链可持续性的重要一步。", "summary": "本研究针对电动汽车行业对高纯度锂日益增长的需求，以及从北美低品位锂资源中经济提取锂的挑战，提出了一种人机协同（HITL）辅助的主动学习框架，以优化碳酸锂的连续结晶过程。该框架通过结合人类专业知识和数据驱动洞察，显著提高了结晶过程对关键杂质（如镁）的耐受性，从几百ppm提升至高达6000ppm。实验结果表明，该方法能够快速适应新数据，使得利用低品位、富含杂质的锂资源成为可能，同时保持产品质量，为经济开发北美锂储量和增强全球锂供应链的可持续性迈出了重要一步。", "keywords": "人机协同, 主动学习, 碳酸锂结晶, 锂提取, 过程优化", "comments": "该论文的创新点在于提出了人机协同（HITL）辅助的主动学习框架，有效地解决了连续结晶优化中参数空间复杂和数据有限的问题。通过整合人类经验与AI数据洞察，显著提升了对低品位锂资源中杂质的容忍度，实现了经济效益和资源利用的可持续性。这是一个重要的突破，有望改变锂提取的传统模式。"}}
{"id": "2412.09052", "title": "GREAT: Grassmannian REcursive Algorithm for Tracking & Online System Identification", "authors": ["András Sasfi", "Alberto Padoan", "Ivan Markovsky", "Florian Dörfler"], "categories": ["eess.SY", "cs.SY", "math.OC"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Submitted to IEEE Transactions on Automatic Control", "url": "http://arxiv.org/abs/2412.09052v2", "summary": "This paper introduces an online approach for identifying time-varying\nsubspaces defined by linear dynamical systems. The approach of representing\nlinear systems by non-parametric subspace models has received significant\ninterest in the field of data-driven control recently. This system\nrepresentation enables us to provide rigorous guarantees for linear\ntime-varying systems, which are difficult to obtain for parametric system\nmodels. The proposed method leverages optimization on the Grassmann manifold\nleading to the Grassmannian Recursive Algorithm for Tracking (GREAT). We view\nsubspaces as points on the Grassmann manifold and adapt the estimate based on\nonline data by performing optimization on the manifold. At each time step, a\nsingle measurement from the current subspace corrupted by a bounded error is\navailable. The subspace estimate is updated online using Grassmannian gradient\ndescent on a cost function incorporating a window of the most recent data.\nUnder suitable assumptions on the signal-to-noise ratio of the online data and\nthe subspace's rate of change, we establish theoretical guarantees for the\nresulting algorithm. More specifically, we prove an exponential convergence\nrate and provide an uncertainty quantification of the estimates in terms of an\nupper bound on their distance to the true subspace. The applicability of the\nproposed algorithm is demonstrated by means of numerical examples.", "comment": "Submitted to IEEE Transactions on Automatic Control", "pdf_url": "http://arxiv.org/pdf/2412.09052v2", "cate": "eess.SY", "date": "2024-12-12", "updated": "2025-07-25", "AI": {"title_translation": "GREAT：格拉斯曼递归跟踪与在线系统辨识算法", "tldr": "本文提出了一种基于格拉斯曼流形优化的在线算法GREAT，用于在线辨识时变子空间，并提供了理论收敛性保证。", "motivation": "识别时变子空间对于线性动力系统至关重要，而传统的参数系统模型难以提供严格保证。非参数子空间模型在数据驱动控制领域受到关注，能为线性时变系统提供严格保证。", "method": "本文提出了一种名为GREAT（Grassmannian Recursive Algorithm for Tracking）的算法。该方法利用格拉斯曼流形上的优化，将子空间视为格拉斯曼流形上的点。算法通过对包含最新数据窗口的成本函数进行格拉斯曼梯度下降，在线更新子空间估计。每个时间步长，从当前子空间获得受限误差污染的单个测量值。", "result": "在在线数据信噪比和子空间变化率的适当假设下，该算法建立了理论保证。具体来说，证明了指数收敛速度，并提供了估计不确定性量化，即估计与真实子空间距离的上限。通过数值示例验证了所提出算法的适用性。", "conclusion": "所提出的GREAT算法能够有效地在线辨识时变子空间，并具有严格的理论收敛性和不确定性量化保证，其有效性已通过数值示例得到验证。", "translation": "本文提出了一种用于识别由线性动力系统定义的时变子空间的在线方法。通过非参数子空间模型表示线性系统的方法最近在数据驱动控制领域引起了极大的兴趣。这种系统表示使我们能够为线性时变系统提供严格的保证，这对于参数系统模型来说很难获得。所提出的方法利用格拉斯曼流形上的优化，从而产生了格拉斯曼递归跟踪算法（GREAT）。我们将子空间视为格拉斯曼流形上的点，并通过在流形上执行优化来根据在线数据调整估计。在每个时间步长，可获得来自当前子空间的单个测量值，该测量值受到有界误差的污染。使用包含最新数据窗口的成本函数上的格拉斯曼梯度下降，在线更新子空间估计。在在线数据信噪比和子空间变化率的适当假设下，我们为所得算法建立了理论保证。更具体地说，我们证明了指数收敛速度，并提供了估计的不确定性量化，即它们与真实子空间距离的上限。通过数值示例证明了所提出算法的适用性。", "summary": "本文介绍了一种名为GREAT的在线算法，用于识别由线性动力系统定义的时变子空间。该方法利用格拉斯曼流形上的优化，将子空间视为流形上的点，并通过在线格拉斯曼梯度下降更新估计。文章证明了该算法在适当条件下具有指数收敛速度和不确定性量化，并通过数值示例验证了其有效性。", "keywords": "格拉斯曼流形, 在线系统辨识, 时变子空间, 跟踪, 子空间模型", "comments": "该论文的创新之处在于将格拉斯曼流形上的优化应用于在线时变子空间识别，并为算法提供了严格的理论保证，包括指数收敛性和不确定性量化，这对于实际应用具有重要意义。其重要性在于为数据驱动控制领域的时变系统辨识提供了新的工具和理论基础。"}}
{"id": "2507.19117", "title": "Objectifying the Subjective: Cognitive Biases in Topic Interpretations", "authors": ["Swapnil Hingmire", "Ze Shi Li", "Shiyu", "Zeng", "Ahmed Musa Awon", "Luiz Franciscatto Guerra", "Neil Ernst"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted for publication at the Transactions of ACL (TACL) (pre-MIT Press publication version)", "url": "http://arxiv.org/abs/2507.19117v1", "summary": "Interpretation of topics is crucial for their downstream applications.\nState-of-the-art evaluation measures of topic quality such as coherence and\nword intrusion do not measure how much a topic facilitates the exploration of a\ncorpus. To design evaluation measures grounded on a task, and a population of\nusers, we do user studies to understand how users interpret topics. We propose\nconstructs of topic quality and ask users to assess them in the context of a\ntopic and provide rationale behind evaluations. We use reflexive thematic\nanalysis to identify themes of topic interpretations from rationales. Users\ninterpret topics based on availability and representativeness heuristics rather\nthan probability. We propose a theory of topic interpretation based on the\nanchoring-and-adjustment heuristic: users anchor on salient words and make\nsemantic adjustments to arrive at an interpretation. Topic interpretation can\nbe viewed as making a judgment under uncertainty by an ecologically rational\nuser, and hence cognitive biases aware user models and evaluation frameworks\nare needed.", "comment": "Accepted for publication at the Transactions of ACL (TACL) (pre-MIT\n  Press publication version)", "pdf_url": "http://arxiv.org/pdf/2507.19117v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "客观化主观：主题解释中的认知偏差", "tldr": "当前主题质量评估方法缺乏以用户为中心，本研究通过用户研究揭示了认知偏差（可用性、代表性、锚定与调整）如何影响主题解释，并呼吁建立认知偏差感知的评估框架。", "motivation": "当前最先进的主题质量评估方法（如连贯性和词语入侵）未能衡量主题在多大程度上促进语料库的探索，也未基于特定任务和用户群体。本研究旨在通过理解用户如何解释主题，以设计更贴近任务的评估方法。", "method": "进行了用户研究，让用户评估提出的主题质量构建概念并提供理由。采用反思性主题分析从用户提供的理由中识别主题解释的主题。", "result": "用户解释主题是基于可用性和代表性启发式而非概率。提出了基于锚定和调整启发式的主题解释理论：用户锚定在显著词汇上并进行语义调整以得出解释。", "conclusion": "主题解释可视为生态理性用户在不确定性下做出的判断，因此需要建立认知偏差感知的用户模型和评估框架。", "translation": "主题的解释对其下游应用至关重要。当前最先进的主题质量评估方法，如连贯性和词语入侵，未能衡量主题在多大程度上促进语料库的探索。为了设计基于任务和用户群体的评估方法，我们进行了用户研究，以了解用户如何解释主题。我们提出了主题质量的构建概念，并要求用户在主题背景下对其进行评估，并提供评估背后的理由。我们使用反思性主题分析从理由中识别主题解释的主题。用户基于可用性和代表性启发式而非概率来解释主题。我们提出了一种基于锚定和调整启发式的主题解释理论：用户锚定在显著词汇上，并进行语义调整以得出解释。主题解释可以被视为生态理性用户在不确定性下做出判断，因此需要认知偏差感知的用户模型和评估框架。", "summary": "本论文研究了用户如何解释主题，并强调了当前主题质量评估指标的局限性。通过用户研究和反思性主题分析，论文揭示了用户的解释受到可用性、代表性和锚定与调整等认知启发式偏差的影响。研究提出了一种新的主题解释理论，并强调了在主题质量评估中需要考虑认知偏差的用户模型和评估框架。", "keywords": "主题解释, 认知偏差, 用户研究, 主题质量, 启发式", "comments": "该论文的创新之处在于将主题质量评估的焦点从纯粹的统计或语言学测量转向以用户为中心的认知过程。通过识别主题解释中具体的认知偏差（可用性、代表性、锚定与调整），它提供了一个在该领域常被忽视的关键人因视角。这些发现对于开发更稳健、更符合人类认知的专题建模评估指标和用户界面至关重要。"}}
{"id": "2504.20498", "title": "Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection", "authors": ["Jianhong Han", "Yupei Wang", "Liang Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Manuscript submitted to IEEE Transactions on Circuits and Systems for Video Technology", "url": "http://arxiv.org/abs/2504.20498v2", "summary": "Single-source domain generalization (SDG) in object detection aims to develop\na detector using only source domain data that generalizes well to unseen target\ndomains. Existing methods are primarily CNN-based and improve robustness\nthrough data augmentation combined with feature alignment. However, these\nmethods are limited, as augmentation is only effective when the synthetic\ndistribution approximates that of unseen domains, thus failing to ensure\ngeneralization across diverse scenarios.\n  While DEtection TRansformer (DETR) has shown strong generalization in domain\nadaptation due to global context modeling, its potential for SDG remains\nunderexplored. To this end, we propose Style-Adaptive DEtection TRansformer\n(SA-DETR), a DETR-based detector tailored for SDG. SA-DETR introduces an online\ndomain style adapter that projects the style representation of unseen domains\ninto the source domain via a dynamic memory bank. This bank self-organizes into\ndiverse style prototypes and is continuously updated under a test-time\nadaptation framework, enabling effective style rectification.\n  Additionally, we design an object-aware contrastive learning module to\npromote extraction of domain-invariant features. By applying gating masks that\nconstrain contrastive learning in both spatial and semantic dimensions, this\nmodule facilitates instance-level cross-domain contrast and enhances\ngeneralization.\n  Extensive experiments across five distinct weather scenarios demonstrate that\nSA-DETR consistently outperforms existing methods in both detection accuracy\nand domain generalization capability.", "comment": "Manuscript submitted to IEEE Transactions on Circuits and Systems for\n  Video Technology", "pdf_url": "http://arxiv.org/pdf/2504.20498v2", "cate": "cs.CV", "date": "2025-04-29", "updated": "2025-07-25", "AI": {"title_translation": "用于单源域泛化目标检测的风格自适应检测Transformer", "tldr": "SA-DETR是一个基于DETR的检测器，通过在线域风格适配器和目标感知对比学习模块，解决了单源域泛化目标检测中的风格多样性问题，并在不同天气场景下表现优异。", "motivation": "现有的单源域泛化目标检测方法主要基于CNN，并通过数据增强和特征对齐来提高鲁棒性。然而，这些方法受限于合成分布必须近似于未见域，导致在多样化场景下的泛化能力不足。虽然DETR在域适应中显示出强大的泛化能力，但其在单源域泛化中的潜力尚未被充分探索。", "method": "本文提出了Style-Adaptive DEtection TRansformer (SA-DETR)，一个专门为单源域泛化（SDG）设计的基于DETR的检测器。SA-DETR引入了一个在线域风格适配器，通过动态记忆库将未见域的风格表示投影到源域，该记忆库自组织成多样化的风格原型并在测试时适应框架下持续更新，以实现有效的风格校正。此外，设计了一个目标感知对比学习模块，通过应用在空间和语义维度上约束对比学习的门控掩码，促进实例级跨域对比并增强泛化能力。", "result": "在五种不同天气场景下的广泛实验表明，SA-DETR在检测精度和域泛化能力方面均持续优于现有方法。", "conclusion": "SA-DETR通过其创新的风格自适应机制和目标感知对比学习模块，有效提升了单源域泛化目标检测的性能，并在多样化未见域中展现出卓越的泛化能力。", "translation": "单源域泛化（SDG）在目标检测中的目标是开发一种仅使用源域数据，但能很好地泛化到未见目标域的检测器。现有方法主要基于CNN，并通过数据增强结合特征对齐来提高鲁棒性。然而，这些方法受到限制，因为增强仅在合成分布近似未见域时才有效，因此未能确保在多样化场景下的泛化能力。\n尽管DEtection TRansformer (DETR) 由于全局上下文建模在域适应中表现出强大的泛化能力，但其在SDG中的潜力仍未被充分探索。为此，我们提出了Style-Adaptive DEtection TRansformer (SA-DETR)，一个专为SDG量身定制的基于DETR的检测器。SA-DETR引入了一个在线域风格适配器，通过动态记忆库将未见域的风格表示投影到源域。该记忆库自组织成多样化的风格原型，并在测试时适应框架下持续更新，从而实现有效的风格校正。\n此外，我们设计了一个目标感知对比学习模块，以促进领域不变特征的提取。通过应用在空间和语义维度上约束对比学习的门控掩码，该模块促进了实例级跨域对比并增强了泛化能力。\n在五种不同天气场景下的广泛实验表明，SA-DETR在检测精度和域泛化能力方面均持续优于现有方法。", "summary": "本研究提出了一种名为SA-DETR的检测器，专为单源域泛化（SDG）目标检测设计，以解决现有方法在多样化未见域泛化能力不足的问题。SA-DETR基于DETR，并引入了在线域风格适配器和目标感知对比学习模块。风格适配器通过动态记忆库将未见域风格投影到源域，实现风格校正；对比学习模块则促进领域不变特征的提取。实验证明，SA-DETR在多种天气场景下均优于现有方法，显著提升了SDG的性能。", "keywords": "单源域泛化, 目标检测, DETR, 风格自适应, 对比学习", "comments": "该论文创新性地将DETR引入单源域泛化任务，并通过引入在线域风格适配器和目标感知对比学习模块，有效解决了跨域风格差异和领域不变特征提取的挑战。动态记忆库的自组织和测试时适应机制是其亮点，有望为未来域泛化研究提供新思路。"}}
{"id": "2507.18828", "title": "Ethical Considerations for Observational Research in Social VR", "authors": ["Victoria Chang", "Caro Williams-Pierce", "Huaishu Peng", "Ge Gao"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      CSCW Companion '25, October 18-22, 2025, Bergen, Norway", "url": "http://arxiv.org/abs/2507.18828v1", "summary": "Social VR introduces new ethical challenges for observational research. The\ncurrent paper presents a narrative literature review of ethical considerations\nin observational methods, with a focus on work in HCI. We examine how\nunobtrusive or selectively disclosed observation is implemented in public\nface-to-face and social VR settings. Our review extends ethical discussions\nfrom traditional public research into the context of social VR, highlighting\ntensions between observer visibility, data traceability, and participant\nautonomy. Drawing on insights distilled from prior literature, we propose five\nconstructive guidelines for ethical observational research in public social VR\nenvironments. Our work offers key implications for future research, addressing\nanticipated improvements in platform design, the management of researcher\npresence, and the development of community-informed consent mechanisms.", "comment": "CSCW Companion '25, October 18-22, 2025, Bergen, Norway", "pdf_url": "http://arxiv.org/pdf/2507.18828v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "社交VR中观察性研究的伦理考量", "tldr": "社交VR为观察性研究带来了新的伦理挑战。本文通过叙述性文献综述，探讨了观察者可见性、数据可追溯性和参与者自主性之间的张力，并提出了在公共社交VR环境中进行伦理观察性研究的五项建设性指导原则。", "motivation": "社交VR为观察性研究带来了新的伦理挑战，需要将传统公共研究的伦理讨论扩展到社交VR的语境中。", "method": "本文对观察性方法中的伦理考量进行了叙述性文献综述，重点关注人机交互（HCI）领域的工作，并审视了在公共面对面和社交VR环境中如何实施不引人注意或选择性披露的观察。", "result": "该综述将伦理讨论从传统公共研究扩展到社交VR语境，强调了观察者可见性、数据可追溯性和参与者自主性之间的张力，并为公共社交VR环境中的伦理观察性研究提出了五项建设性指导原则。", "conclusion": "本研究对未来研究具有重要意义，涉及平台设计的预期改进、研究者在场管理以及社区知情同意机制的开发。", "translation": "社交VR为观察性研究带来了新的伦理挑战。本文对观察性方法中的伦理考量进行了叙述性文献综述，重点关注人机交互领域的工作。我们研究了在公共面对面和社交VR环境中如何实施不引人注意或选择性披露的观察。我们的综述将传统公共研究的伦理讨论扩展到社交VR的语境中，强调了观察者可见性、数据可追溯性和参与者自主性之间的张力。借鉴先前文献中提炼出的见解，我们为公共社交VR环境中的伦理观察性研究提出了五项建设性指导原则。我们的工作对未来研究具有重要意义，涉及平台设计的预期改进、研究者在场管理以及社区知情同意机制的开发。", "summary": "本文探讨了社交VR中观察性研究带来的新伦理挑战。通过对人机交互领域观察性方法伦理考量的叙述性文献综述，文章分析了在公共面对面和社交VR环境中如何实施非侵入性观察，并强调了观察者可见性、数据可追溯性和参与者自主性之间的张力。基于现有文献，论文提出了在公共社交VR环境中进行伦理观察性研究的五项建设性指导原则，并为未来平台设计、研究者在场管理和社区知情同意机制的发展提供了关键启示。", "keywords": "社交VR, 伦理考量, 观察性研究, HCI, 指导原则", "comments": "该论文积极应对了快速发展技术领域中出现的新兴伦理困境，具有重要意义。它将传统伦理框架扩展到社交VR这一新颖语境，特别是通过提出具体的指导原则，对研究人员和平台开发者而言都极具价值。对平台设计和知情同意机制等实际方面的强调，使得其贡献具有可操作性。"}}
{"id": "2507.18944", "title": "Structure Matters: Revisiting Boundary Refinement in Video Object Segmentation", "authors": ["Guanyi Qin", "Ziyue Wang", "Daiyun Shen", "Haofeng Liu", "Hantao Zhou", "Junde Wu", "Runze Hu", "Yueming Jin"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18944v1", "summary": "Given an object mask, Semi-supervised Video Object Segmentation (SVOS)\ntechnique aims to track and segment the object across video frames, serving as\na fundamental task in computer vision. Although recent memory-based methods\ndemonstrate potential, they often struggle with scenes involving occlusion,\nparticularly in handling object interactions and high feature similarity. To\naddress these issues and meet the real-time processing requirements of\ndownstream applications, in this paper, we propose a novel bOundary Amendment\nvideo object Segmentation method with Inherent Structure refinement, hereby\nnamed OASIS. Specifically, a lightweight structure refinement module is\nproposed to enhance segmentation accuracy. With the fusion of rough edge priors\ncaptured by the Canny filter and stored object features, the module can\ngenerate an object-level structure map and refine the representations by\nhighlighting boundary features. Evidential learning for uncertainty estimation\nis introduced to further address challenges in occluded regions. The proposed\nmethod, OASIS, maintains an efficient design, yet extensive experiments on\nchallenging benchmarks demonstrate its superior performance and competitive\ninference speed compared to other state-of-the-art methods, i.e., achieving the\nF values of 91.6 (vs. 89.7 on DAVIS-17 validation set) and G values of 86.6\n(vs. 86.2 on YouTubeVOS 2019 validation set) while maintaining a competitive\nspeed of 48 FPS on DAVIS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18944v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "结构至关重要：重新审视视频目标分割中的边界细化", "tldr": "提出了一种名为OASIS的新型视频目标分割方法，通过边界细化和不确定性估计，在保持效率的同时提高了遮挡场景下的分割精度。", "motivation": "半监督视频目标分割（SVOS）在处理遮挡场景，特别是物体交互和高特征相似性时，现有基于记忆的方法表现不佳。此外，下游应用对实时处理有要求。", "method": "提出了一种名为OASIS的边界修正视频目标分割方法，具有固有结构细化。该方法包含一个轻量级结构细化模块，通过融合Canny滤波器捕获的粗略边缘先验和存储的对象特征来生成对象级结构图并细化表示，突出边界特征。引入了证据学习进行不确定性估计，以进一步解决遮挡区域的挑战。", "result": "所提出的OASIS方法在DAVIS-17验证集上F值达到91.6（SOTA为89.7），在YouTubeVOS 2019验证集上G值达到86.6（SOTA为86.2），同时在DAVIS上保持48 FPS的竞争速度。", "conclusion": "OASIS方法通过有效的边界细化和不确定性估计，在具有挑战性的基准测试中表现出卓越的性能和竞争性的推理速度，有效解决了视频目标分割中遮挡带来的挑战。", "translation": "给定一个对象掩膜，半监督视频目标分割（SVOS）技术旨在跨视频帧跟踪和分割对象，是计算机视觉中的一项基础任务。尽管最近基于记忆的方法显示出潜力，但它们在涉及遮挡的场景中常常遇到困难，特别是在处理对象交互和高特征相似性方面。为了解决这些问题并满足下游应用的实时处理要求，本文提出了一种新颖的边界修正视频目标分割方法，具有固有结构细化，简称OASIS。具体而言，提出了一个轻量级结构细化模块来提高分割精度。通过融合Canny滤波器捕获的粗略边缘先验和存储的对象特征，该模块可以生成对象级结构图并通过突出边界特征来细化表示。引入了证据学习进行不确定性估计，以进一步解决遮挡区域的挑战。所提出的OASIS方法保持了高效的设计，然而，在具有挑战性的基准测试中进行的广泛实验表明，与现有最先进的方法相比，它具有卓越的性能和竞争性的推理速度，即在DAVIS-17验证集上实现了91.6的F值（对比89.7）和在YouTubeVOS 2019验证集上实现了86.6的G值（对比86.2），同时在DAVIS上保持了48 FPS的竞争速度。", "summary": "本文提出了一种名为OASIS的新型半监督视频目标分割方法，旨在解决现有方法在处理遮挡场景时的不足。OASIS引入了一个轻量级结构细化模块，结合边缘先验和对象特征来增强边界表示，并利用证据学习进行不确定性估计以应对遮挡挑战。实验证明，OASIS在性能和速度上均优于或媲美现有最先进方法。", "keywords": "视频目标分割, 边界细化, 遮挡, 不确定性估计, OASIS", "comments": "该论文创新性地将边界细化与不确定性估计相结合，有效提升了视频目标分割在复杂遮挡场景下的性能，同时保持了实时处理能力，具有重要的实践意义。"}}
{"id": "2507.18980", "title": "Max-Min Beamforming for Large-Scale Cell-Free Massive MIMO: A Randomized ADMM Algorithm", "authors": ["Bin Wang", "Jun Fang", "Yue Xiao", "Martin Haardt"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18980v1", "summary": "We consider the problem of max-min beamforming (MMB) for cell-free massive\nmulti-input multi-output (MIMO) systems, where the objective is to maximize the\nminimum achievable rate among all users. Existing MMB methods are mainly based\non deterministic optimization methods, which are computationally inefficient\nwhen the problem size grows large. To address this issue, we, in this paper,\npropose a randomized alternating direction method of multiplier (ADMM)\nalgorithm for large-scale MMB problems. We first propose a novel formulation\nthat transforms the highly challenging feasibility-checking problem into a\nlinearly constrained optimization problem. An efficient randomized ADMM is then\ndeveloped for solving the linearly constrained problem. Unlike standard ADMM,\nrandomized ADMM only needs to solve a small number of subproblems at each\niteration to ensure convergence, thus achieving a substantial complexity\nreduction. Our theoretical analysis reveals that the proposed algorithm\nexhibits an O(1/\\bar{t}) convergence rate (\\bar{t} represents the number of\niterations), which is on the same order as its deterministic counterpart.\nNumerical results show that the proposed algorithm offers a significant\ncomplexity advantage over existing methods in solving the MMB problem.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18980v1", "cate": "eess.SP", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "大规模无蜂窝大规模MIMO的最大最小波束成形：一种随机ADMM算法", "tldr": "本文提出了一种随机ADMM算法，用于解决大规模无蜂窝大规模MIMO系统中的最大最小波束成形问题，该算法比现有方法具有显著的计算复杂度优势。", "motivation": "现有的最大最小波束成形（MMB）方法主要基于确定性优化方法，当问题规模变大时，计算效率低下。", "method": "本文提出了一种随机交替方向乘子法（ADMM）算法来解决大规模MMB问题。首先，提出了一种新颖的公式，将具有挑战性的可行性检查问题转化为线性约束优化问题。然后，开发了一种有效的随机ADMM来解决该线性约束问题。与标准ADMM不同，随机ADMM在每次迭代中只需解决少量子问题即可确保收敛。", "result": "理论分析表明，所提出的算法具有O(1/\bar{t})的收敛速度，与确定性算法的收敛速度相同。数值结果表明，在解决MMB问题时，所提出的算法比现有方法具有显著的复杂度优势。", "conclusion": "本文提出的随机ADMM算法能够有效且高效地解决大规模无蜂窝大规模MIMO系统中的最大最小波束成形问题，在计算复杂度方面优于现有方法。", "translation": "我们考虑无蜂窝大规模多输入多输出（MIMO）系统中的最大最小波束成形（MMB）问题，其目标是最大化所有用户的最小可实现速率。现有的MMB方法主要基于确定性优化方法，当问题规模变大时，计算效率低下。为了解决这个问题，本文提出了一种用于大规模MMB问题的随机交替方向乘子法（ADMM）算法。我们首先提出了一种新颖的公式，将极具挑战性的可行性检查问题转化为线性约束优化问题。然后，开发了一种高效的随机ADMM来解决线性约束问题。与标准ADMM不同，随机ADMM在每次迭代中只需解决少量子问题即可确保收敛，从而实现显著的复杂度降低。我们的理论分析表明，所提出的算法表现出O(1/\bar{t})的收敛速度（\bar{t}代表迭代次数），这与确定性算法的收敛速度相同。数值结果表明，所提出的算法在解决MMB问题方面比现有方法具有显著的复杂度优势。", "summary": "本文针对无蜂窝大规模多输入多输出（MIMO）系统中的最大最小波束成形（MMB）问题，旨在最大化所有用户的最小可实现速率。鉴于现有MMB方法在问题规模较大时计算效率低下的问题，本文提出了一种随机交替方向乘子法（ADMM）算法。该算法首先将复杂的可行性检查问题转化为线性约束优化问题，然后利用随机ADMM进行求解。与传统ADMM不同，随机ADMM每次迭代仅需解决少量子问题即可收敛，从而显著降低了复杂度。理论分析表明，该算法具有与确定性方法相同的收敛速度，数值结果也验证了其在解决MMB问题上相对于现有方法的显著复杂度优势。", "keywords": "最大最小波束成形, 无蜂窝大规模MIMO, 随机ADMM, 复杂度降低, 大规模系统", "comments": "该论文通过引入随机ADMM算法，有效地解决了大规模无蜂窝大规模MIMO系统中最大最小波束成形问题的计算效率瓶颈。将可行性检查问题转化为线性约束优化问题是其创新点之一，而随机ADMM在保持收敛速度的同时显著降低了每迭代的计算量，这对于实际系统部署具有重要意义。该研究在理论分析和数值验证上都表现出较强的说服力。"}}
{"id": "2507.18780", "title": "Symmetry-reduced model reduction of shift-equivariant systems via operator inference", "authors": ["Yu Shuai", "Clarence W. Rowley"], "categories": ["math.NA", "cs.NA", "math.OC", "physics.comp-ph", "35B06 (Primary), 65D15 (Primary), 68W25 (Primary), 35Q35\n  (Secondary), 47A58 (Secondary)", "G.1.2; G.1.6; I.2.6; J.2"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      30 pages, 7 figures. Orally presented at the SIAM Conference on Applications of Dynamical Systems (SIAM DS25). Submitted to Advances in Computational Mathematics (ACOM)", "url": "http://arxiv.org/abs/2507.18780v1", "summary": "We consider data-driven reduced-order models of partial differential\nequations with shift equivariance. Shift-equivariant systems typically admit\ntraveling solutions, and the main idea of our approach is to represent the\nsolution in a traveling reference frame, in which it can be described by a\nrelatively small number of basis functions. Existing methods for operator\ninference allow one to approximate a reduced-order model directly from data,\nwithout knowledge of the full-order dynamics. Our method adds additional terms\nto ensure that the reduced-order model not only approximates the spatially\nfrozen profile of the solution, but also estimates the traveling speed as a\nfunction of that profile. We validate our approach using the\nKuramoto-Sivashinsky equation, a one-dimensional partial differential equation\nthat exhibits traveling solutions and spatiotemporal chaos. Results indicate\nthat our method robustly captures traveling solutions, and exhibits improved\nnumerical stability over the standard operator inference approach.", "comment": "30 pages, 7 figures. Orally presented at the SIAM Conference on\n  Applications of Dynamical Systems (SIAM DS25). Submitted to Advances in\n  Computational Mathematics (ACOM)", "pdf_url": "http://arxiv.org/pdf/2507.18780v1", "cate": "math.NA", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "算子推断法对移位等变系统的对称降维模型降阶", "tldr": "本文提出一种通过在行进参考系中表示解来对移位等变系统进行降阶建模的新方法，该方法能同时估计解的空间轮廓和行进速度，并展现出更好的数值稳定性。", "motivation": "移位等变系统通常存在行进解，而现有降阶模型方法可能无法有效捕捉或高效描述这些行进解，尤其是在估计其行进速度方面。", "method": "论文提出了一种基于算子推断法的对称降维模型降阶方法。其核心思想是在一个行进参考系中表示解，使得解可以用相对较少的基础函数来描述。该方法通过添加额外项，不仅能近似解的空间冻结轮廓，还能估计作为该轮廓函数的行进速度。", "result": "该方法能够稳健地捕捉行进解，并且与标准算子推断方法相比，表现出更高的数值稳定性。通过Kuramoto-Sivashinsky方程验证了其有效性。", "conclusion": "论文提出的新方法有效解决了移位等变系统中行进解的降阶建模问题，通过同时估计空间轮廓和行进速度，并在数值稳定性上优于现有方法。", "translation": "我们考虑具有移位等变性的偏微分方程的数据驱动降阶模型。移位等变系统通常允许行进解，我们方法的主要思想是在一个行进参考系中表示解，在该参考系中，解可以通过相对较少的基础函数来描述。现有的算子推断方法允许直接从数据中近似降阶模型，而无需了解全阶动力学。我们的方法添加了额外的项，以确保降阶模型不仅近似解的空间冻结轮廓，而且还估计作为该轮廓函数的行进速度。我们使用Kuramoto-Sivashinsky方程验证了我们的方法，这是一个表现出行进解和时空混沌的一维偏微分方程。结果表明，我们的方法能够稳健地捕捉行进解，并且比标准算子推断方法表现出更高的数值稳定性。", "summary": "本文针对具有移位等变性的偏微分方程，提出了一种基于算子推断的对称降维模型降阶方法。该方法的核心在于将解表示在一个行进参考系中，从而能用少量基函数描述。与现有算子推断不同的是，本方法增加了额外项，使其不仅能近似解的空间轮廓，还能估计其行进速度。在Kuramoto-Sivashinsky方程上的验证表明，该方法能稳健捕捉行进解，并提升了数值稳定性。", "keywords": "移位等变系统, 降阶模型, 算子推断, 行进解, 数值稳定性", "comments": "该研究创新性地将行进参考系的概念引入到算子推断的降阶建模中，解决了移位等变系统中行进解的特殊挑战。其能够同时估计空间轮廓和行进速度的能力，以及在数值稳定性上的提升，是其重要的贡献。这对于需要高效模拟和分析具有行进波特性的复杂系统具有实际意义。"}}
{"id": "2503.15013", "title": "Ambient Noise Full Waveform Inversion with Neural Operators", "authors": ["Caifeng Zou", "Zachary E. Ross", "Robert W. Clayton", "Fan-Chi Lin", "Kamyar Azizzadenesheli"], "categories": ["physics.geo-ph", "cs.LG"], "primary_category": "Subjects:       Geophysics (physics.geo-ph)", "pdf_link": null, "comments": "Comments:      Revision", "url": "http://arxiv.org/abs/2503.15013v3", "summary": "Numerical simulations of seismic wave propagation are crucial for\ninvestigating velocity structures and improving seismic hazard assessment.\nHowever, standard methods such as finite difference or finite element are\ncomputationally expensive. Recent studies have shown that a new class of\nmachine learning models, called neural operators, can solve the elastodynamic\nwave equation orders of magnitude faster than conventional methods. Full\nwaveform inversion is a prime beneficiary of the accelerated simulations.\nNeural operators, as end-to-end differentiable operators, combined with\nautomatic differentiation, provide an alternative approach to the adjoint-state\nmethod. State-of-the-art optimization techniques built into PyTorch provide\nneural operators with greater flexibility to improve the optimization dynamics\nof full waveform inversion, thereby mitigating cycle-skipping problems. In this\nstudy, we demonstrate the first application of neural operators for full\nwaveform inversion on a real seismic dataset, which consists of several nodal\ntransects collected across the San Gabriel, Chino, and San Bernardino basins in\nthe Los Angeles metropolitan area.", "comment": "Revision", "pdf_url": "http://arxiv.org/pdf/2503.15013v3", "cate": "physics.geo-ph", "date": "2025-03-19", "updated": "2025-07-25", "AI": {"title_translation": "基于神经算子的环境噪声全波形反演", "tldr": "神经算子被首次应用于实际地震数据的全波形反演，以加速模拟并改善优化。", "motivation": "地震波传播的数值模拟对于研究速度结构和改善地震灾害评估至关重要，但标准方法计算成本高昂。", "method": "本文提出使用神经算子进行全波形反演。神经算子作为端到端可微分算子，结合自动微分，提供了替代伴随状态法的新方法。PyTorch中先进的优化技术赋予神经算子更大的灵活性，以改善全波形反演的优化动态，从而缓解周波跳跃问题。", "result": "首次将神经算子应用于实际地震数据集的全波形反演，该数据集包含洛杉矶都会区圣盖博、奇诺和圣贝纳迪诺盆地收集的节点横断面数据。", "conclusion": "本文证明了神经算子在实际地震数据上进行全波形反演的可行性和潜力，为地震波传播模拟和全波形反演提供了一种更高效、更灵活的替代方案。", "translation": "地震波传播的数值模拟对于研究速度结构和改善地震灾害评估至关重要。然而，有限差分或有限元等标准方法计算成本高昂。最近的研究表明，一类称为神经算子的新型机器学习模型可以比传统方法快几个数量级地求解弹性动力学波动方程。全波形反演是加速模拟的主要受益者。神经算子作为端到端可微分算子，结合自动微分，为伴随状态法提供了一种替代方法。PyTorch中内置的先进优化技术赋予神经算子更大的灵活性，以改善全波形反演的优化动态，从而缓解周波跳跃问题。在本研究中，我们首次展示了神经算子在实际地震数据集上进行全波形反演的应用，该数据集由在洛杉矶都会区圣盖博、奇诺和圣贝纳迪诺盆地收集的多个节点横断面组成。", "summary": "本文提出利用神经算子进行全波形反演，以解决传统地震波传播数值模拟计算成本高昂的问题。神经算子作为端到端可微分模型，结合自动微分和先进优化技术，能够显著加速模拟并缓解全波形反演中的周波跳跃问题。研究首次将该方法应用于实际地震数据集，验证了其在复杂地质结构成像中的潜力。", "keywords": "神经算子, 全波形反演, 地震波传播, 环境噪声, 计算效率", "comments": "本文的创新点在于首次将神经算子应用于实际地震数据的全波形反演，解决了传统方法计算成本高昂的问题，并通过其可微分特性和优化技术缓解了周波跳跃。这为地震速度结构研究和地震灾害评估提供了更高效、更具潜力的工具。"}}
{"id": "2507.18650", "title": "Features extraction for image identification using computer vision", "authors": ["Venant Niyonkuru", "Sylla Sekou", "Jimmy Jackson Sinzinkayo"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18650v1", "summary": "This study examines various feature extraction techniques in computer vision,\nthe primary focus of which is on Vision Transformers (ViTs) and other\napproaches such as Generative Adversarial Networks (GANs), deep feature models,\ntraditional approaches (SIFT, SURF, ORB), and non-contrastive and contrastive\nfeature models. Emphasizing ViTs, the report summarizes their architecture,\nincluding patch embedding, positional encoding, and multi-head self-attention\nmechanisms with which they overperform conventional convolutional neural\nnetworks (CNNs). Experimental results determine the merits and limitations of\nboth methods and their utilitarian applications in advancing computer vision.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18650v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "计算机视觉中用于图像识别的特征提取", "tldr": "本研究探讨了计算机视觉中各种特征提取技术，重点关注Vision Transformers (ViTs)及其架构，并比较其与传统CNNs的性能，同时评估了不同方法的优缺点。", "motivation": "本研究旨在考察计算机视觉中各种特征提取技术，特别是Vision Transformers (ViTs)的特点、性能及其在图像识别中的应用，并评估不同方法的优缺点和实用价值。", "method": "本研究考察了多种特征提取技术，包括Vision Transformers (ViTs)、生成对抗网络 (GANs)、深度特征模型、传统方法 (SIFT, SURF, ORB) 以及非对比和对比特征模型。研究重点强调了ViTs的架构，包括补丁嵌入、位置编码和多头自注意力机制，并将其性能与传统卷积神经网络 (CNNs) 进行了比较。", "result": "实验结果确定了所研究方法的优点和局限性，以及它们在推动计算机视觉发展中的实用应用。", "conclusion": "实验结果确定了所研究方法的优点和局限性，以及它们在推动计算机视觉发展中的实用应用。", "translation": "本研究探讨了计算机视觉中各种特征提取技术，主要关注Vision Transformers (ViTs)以及生成对抗网络 (GANs)、深度特征模型、传统方法 (SIFT, SURF, ORB) 以及非对比和对比特征模型等其他方法。报告强调了ViTs，总结了其架构，包括补丁嵌入、位置编码和多头自注意力机制，这些使其性能优于传统的卷积神经网络 (CNNs)。实验结果确定了这两种方法的优点和局限性，以及它们在推动计算机视觉发展中的实用应用。", "summary": "本研究对计算机视觉领域的多种特征提取技术进行了考察，其中尤其侧重于Vision Transformers (ViTs)。文章详细阐述了ViTs的架构，包括补丁嵌入、位置编码和多头自注意力机制，并指出其在性能上超越了传统的卷积神经网络 (CNNs)。此外，研究还涵盖了生成对抗网络 (GANs)、深度特征模型以及SIFT、SURF、ORB等传统方法。通过实验结果，本研究评估了这些方法的优缺点及其在计算机视觉应用中的价值。", "keywords": "特征提取, 计算机视觉, Vision Transformers, 图像识别, 深度学习", "comments": "这篇论文的创新点在于对多种特征提取技术进行了综合性考察，特别是对Vision Transformers (ViTs)的深入分析，并将其与传统CNNs进行了性能比较。其重要性在于为图像识别领域的特征提取方法提供了全面的视角，并指出了ViTs的优势，这对于推动计算机视觉技术的发展具有指导意义。"}}
{"id": "2504.14641", "title": "HLSTester: Efficient Testing of Behavioral Discrepancies with LLMs for High-Level Synthesis", "authors": ["Kangwei Xu", "Bing Li", "Grace Li Zhang", "Ulf Schlichtmann"], "categories": ["cs.SE", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      arXiv admin note: text overlap with arXiv:2407.03889", "url": "http://arxiv.org/abs/2504.14641v3", "summary": "In high-level synthesis (HLS), C/C++ programs with synthesis directives are\nused to generate circuits for FPGA implementations. However, hardware-specific\nand platform-dependent characteristics in these implementations can introduce\nbehavioral discrepancies between the original C/C++ programs and the circuits\nafter high-level synthesis. Existing methods for testing behavioral\ndiscrepancies in HLS are still immature, and the testing workflow requires\nsignificant human efforts. To address this challenge, we propose HLSTester, a\nlarge language model (LLM) aided testing framework that efficiently detects\nbehavioral discrepancies in HLS. To mitigate hallucinations in LLMs and enhance\nprompt quality, the testbenches for original C/C++ programs are leveraged to\nguide LLMs in generating HLS-compatible testbenches, effectively eliminating\ncertain traditional C/C++ constructs that are incompatible with HLS tools. Key\nvariables are pinpointed through a backward slicing technique in both C/C++ and\nHLS programs to monitor their runtime spectra, enabling an in-depth analysis of\nthe discrepancy symptoms. To reduce test time, a testing input generation\nmechanism is introduced to integrate dynamic mutation with insights from an\nLLM-based progressive reasoning chain. In addition, repetitive hardware testing\nis skipped by a redundancy-aware filtering technique for the generated test\ninputs. Experimental results demonstrate that the proposed LLM-aided testing\nframework significantly accelerates the testing workflow while achieving higher\ntestbench simulation pass rates compared with the traditional method and the\ndirect use of LLMs on the same HLS programs.", "comment": "arXiv admin note: text overlap with arXiv:2407.03889", "pdf_url": "http://arxiv.org/pdf/2504.14641v3", "cate": "cs.SE", "date": "2025-04-20", "updated": "2025-07-24", "AI": {"title_translation": "HLSTester：使用大型语言模型高效测试高层次综合中的行为差异", "tldr": "HLSTester是一个LLM辅助的测试框架，用于高效检测高层次综合（HLS）中的行为差异，通过利用现有测试台、反向切片、动态变异和冗余感知过滤来提高测试效率和通过率。", "motivation": "高层次综合（HLS）中，C/C++程序生成的电路与原始程序之间可能存在行为差异。现有检测方法不成熟，且测试流程需要大量人工投入。", "method": "提出HLSTester框架，利用大型语言模型（LLM）辅助测试。通过利用原始C/C++程序的测试台来引导LLMs生成HLS兼容的测试台，以减少LLM幻觉并提升提示质量。使用反向切片技术识别C/C++和HLS程序中的关键变量，以监控其运行时谱。引入测试输入生成机制，结合动态变异和LLM驱动的渐进推理链。通过冗余感知过滤技术跳过重复的硬件测试。", "result": "实验结果表明，所提出的LLM辅助测试框架显著加速了测试流程，并且与传统方法和直接使用LLMs相比，在相同的HLS程序上实现了更高的测试台仿真通过率。", "conclusion": "HLSTester通过引入LLM辅助的测试方法，有效解决了高层次综合中行为差异检测的效率和准确性问题，显著提升了测试工作流。", "translation": "在高层次综合（HLS）中，带有综合指令的C/C++程序用于生成FPGA实现的电路。然而，这些实现中与硬件特定和平台相关的特性可能会在原始C/C++程序和高层次综合后的电路之间引入行为差异。现有的HLS行为差异测试方法仍不成熟，且测试工作流程需要大量人工投入。为了解决这一挑战，我们提出了HLSTester，一个由大型语言模型（LLM）辅助的测试框架，可高效检测HLS中的行为差异。为了减轻LLM中的幻觉并提高提示质量，利用原始C/C++程序的测试台来引导LLM生成HLS兼容的测试台，有效消除了某些与HLS工具不兼容的传统C/C++结构。通过C/C++和HLS程序中的反向切片技术精确定位关键变量，以监控其运行时谱，从而深入分析差异症状。为了减少测试时间，引入了一种测试输入生成机制，将动态变异与基于LLM的渐进推理链的洞察力相结合。此外，通过对生成的测试输入采用冗余感知过滤技术，跳过了重复的硬件测试。实验结果表明，所提出的LLM辅助测试框架显著加速了测试流程，并且与传统方法以及直接在相同HLS程序上使用LLM相比，实现了更高的测试台仿真通过率。", "summary": "HLSTester是一个创新的大型语言模型（LLM）辅助测试框架，旨在解决高层次综合（HLS）中C/C++程序与生成电路之间的行为差异检测问题。该框架通过利用现有C/C++测试台引导LLM生成HLS兼容的测试台，并结合反向切片技术识别关键变量进行差异分析。为了提高效率，它引入了基于LLM推理和动态变异的测试输入生成机制，并通过冗余感知过滤减少重复测试。实验证明，HLSTester显著提高了测试效率和通过率，优于传统方法和直接使用LLM。", "keywords": "高层次综合, LLMs, 行为差异, 测试, FPGA", "comments": "HLSTester的创新之处在于其将大型语言模型应用于高层次综合的行为差异测试，并巧妙地通过利用现有测试台来引导LLM生成兼容代码，有效缓解了LLM的“幻觉”问题，这是LLM在特定领域应用中的关键挑战。此外，结合反向切片、动态变异和冗余过滤等技术，构建了一个全面且高效的测试流程，对于提升HLS设计的可靠性具有重要意义。"}}
{"id": "2507.19282", "title": "SAM2-Aug: Prior knowledge-based Augmentation for Target Volume Auto-Segmentation in Adaptive Radiation Therapy Using Segment Anything Model 2", "authors": ["Guoping Xu", "Yan Dai", "Hengrui Zhao", "Ying Zhang", "Jie Deng", "Weiguo Lu", "You Zhang"], "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      26 pages, 10 figures", "url": "http://arxiv.org/abs/2507.19282v1", "summary": "Purpose: Accurate tumor segmentation is vital for adaptive radiation therapy\n(ART) but remains time-consuming and user-dependent. Segment Anything Model 2\n(SAM2) shows promise for prompt-based segmentation but struggles with tumor\naccuracy. We propose prior knowledge-based augmentation strategies to enhance\nSAM2 for ART.\n  Methods: Two strategies were introduced to improve SAM2: (1) using prior MR\nimages and annotations as contextual inputs, and (2) improving prompt\nrobustness via random bounding box expansion and mask erosion/dilation. The\nresulting model, SAM2-Aug, was fine-tuned and tested on the One-Seq-Liver\ndataset (115 MRIs from 31 liver cancer patients), and evaluated without\nretraining on Mix-Seq-Abdomen (88 MRIs, 28 patients) and Mix-Seq-Brain (86\nMRIs, 37 patients).\n  Results: SAM2-Aug outperformed convolutional, transformer-based, and\nprompt-driven models across all datasets, achieving Dice scores of 0.86(liver),\n0.89(abdomen), and 0.90(brain). It demonstrated strong generalization across\ntumor types and imaging sequences, with improved performance in\nboundary-sensitive metrics.\n  Conclusions: Incorporating prior images and enhancing prompt diversity\nsignificantly boosts segmentation accuracy and generalizability. SAM2-Aug\noffers a robust, efficient solution for tumor segmentation in ART. Code and\nmodels will be released at https://github.com/apple1986/SAM2-Aug.", "comment": "26 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.19282v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "SAM2-Aug：基于先验知识增强的自适应放射治疗目标体积自动分割，使用Segment Anything Model 2", "tldr": "SAM2-Aug通过先验知识增强和提示多样性改进了SAM2，显著提高了自适应放射治疗中肿瘤分割的准确性和泛化能力。", "motivation": "自适应放射治疗（ART）中的准确肿瘤分割至关重要，但耗时且依赖用户。SAM2在提示引导分割方面有潜力，但在肿瘤精度方面表现不佳。", "method": "引入两种策略改进SAM2：1) 使用先验MR图像和标注作为上下文输入；2) 通过随机边界框扩展和掩膜侵蚀/膨胀提高提示鲁棒性。将SAM2-Aug在One-Seq-Liver数据集（115张MRI，31名肝癌患者）上微调和测试，并在Mix-Seq-Abdomen（88张MRI，28名患者）和Mix-Seq-Brain（86张MRI，37名患者）上进行评估，无需再训练。", "result": "SAM2-Aug在所有数据集上均优于卷积、基于Transformer和提示驱动模型，Dice分数分别为0.86（肝脏）、0.89（腹部）和0.90（大脑）。它在肿瘤类型和成像序列方面表现出强大的泛化能力，并在边界敏感指标上有所改进。", "conclusion": "结合先验图像和增强提示多样性显著提高了分割精度和泛化能力。SAM2-Aug为ART中的肿瘤分割提供了一个鲁棒、高效的解决方案。", "translation": "目的：在自适应放射治疗（ART）中，准确的肿瘤分割至关重要，但仍然耗时且依赖用户。Segment Anything Model 2 (SAM2) 在基于提示的分割方面显示出前景，但在肿瘤精度方面表现不佳。我们提出了基于先验知识的增强策略来增强 SAM2 在 ART 中的应用。\n方法：引入了两种策略来改进 SAM2：(1) 使用先验 MR 图像和标注作为上下文输入，以及 (2) 通过随机边界框扩展和掩膜侵蚀/膨胀来提高提示鲁棒性。由此产生的模型 SAM2-Aug 在 One-Seq-Liver 数据集（来自 31 名肝癌患者的 115 张 MRI）上进行了微调和测试，并在不进行再训练的情况下，在 Mix-Seq-Abdomen（88 张 MRI，28 名患者）和 Mix-Seq-Brain（86 张 MRI，37 名患者）上进行了评估。\n结果：SAM2-Aug 在所有数据集上均优于卷积、基于 Transformer 和提示驱动的模型，在肝脏、腹部和大脑数据集上分别达到了 0.86、0.89 和 0.90 的 Dice 分数。它在肿瘤类型和成像序列方面表现出强大的泛化能力，并在边界敏感指标上有所改进。\n结论：结合先验图像和增强提示多样性显著提高了分割精度和泛化能力。SAM2-Aug 为 ART 中的肿瘤分割提供了一个鲁棒、高效的解决方案。代码和模型将在 https://github.com/apple1986/SAM2-Aug 发布。", "summary": "本文提出了SAM2-Aug模型，通过引入基于先验MR图像和标注的上下文输入以及增强提示鲁棒性（通过随机边界框扩展和掩膜操作）的策略，改进了Segment Anything Model 2 (SAM2) 在自适应放射治疗(ART)中肿瘤分割的精度和泛化能力。实验结果表明，SAM2-Aug在多个数据集上均优于现有模型，并在不同肿瘤类型和成像序列上展现出强大的泛化能力，为ART中的肿瘤自动分割提供了一个高效鲁棒的解决方案。", "keywords": "SAM2-Aug, 肿瘤分割, 自适应放射治疗, 先验知识, 图像增强", "comments": "这篇论文通过引入先验知识和增强提示多样性，有效地解决了SAM2在肿瘤分割精度上的不足，是一个重要的创新。其方法具有通用性，可以潜在地应用于其他医学图像分割任务，提高了模型的鲁棒性和泛化能力。"}}
{"id": "2502.08606", "title": "Distillation Scaling Laws", "authors": ["Dan Busbridge", "Amitis Shidani", "Floris Weers", "Jason Ramapuram", "Etai Littwin", "Russ Webb"], "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Version accepted to ICML 2025. 69 pages, 54 figures, 13 tables", "url": "http://arxiv.org/abs/2502.08606v2", "summary": "We propose a distillation scaling law that estimates distilled model\nperformance based on a compute budget and its allocation between the student\nand teacher. Our findings mitigate the risks associated with large-scale\ndistillation by enabling compute-optimal allocation for both the teacher and\nstudent to maximize student performance. We provide compute-optimal\ndistillation recipes for two key scenarios: when a teacher already exists, and\nwhen a teacher needs training. In settings involving many students or an\nexisting teacher, distillation outperforms supervised learning up to a compute\nlevel that scales predictably with student size. Conversely, if only one\nstudent is to be distilled and a teacher also requires training, supervised\nlearning is generally preferable. Additionally, our large-scale study of\ndistillation increases our understanding of the process and helps inform\nexperimental design.", "comment": "Version accepted to ICML 2025. 69 pages, 54 figures, 13 tables", "pdf_url": "http://arxiv.org/pdf/2502.08606v2", "cate": "cs.LG", "date": "2025-02-12", "updated": "2025-07-25", "AI": {"title_translation": "蒸馏缩放定律", "tldr": "提出了一种蒸馏缩放定律，用于根据计算预算和师生模型间的分配来优化蒸馏性能，并提供了不同场景下的计算最优蒸馏方案。", "motivation": "旨在通过实现计算最优的师生模型资源分配，来降低大规模蒸馏的风险，从而最大化学生模型的性能。", "method": "提出了一种蒸馏缩放定律，用于估算蒸馏模型的性能，并基于此定律提供了两种关键场景下的计算最优蒸馏方案：教师模型已存在时，以及教师模型需要训练时。此外，还进行了大规模蒸馏研究以增进对蒸馏过程的理解。", "result": "研究发现，在多学生或教师模型已存在的场景下，蒸馏性能优于监督学习，其计算水平随学生模型大小可预测地扩展；而如果只有一个学生模型需要蒸馏且教师模型也需训练，则监督学习通常更优。", "conclusion": "本研究通过提出蒸馏缩放定律和提供计算最优的蒸馏方案，不仅有助于降低大规模蒸馏的风险并优化性能，还加深了对蒸馏过程的理解，为实验设计提供了指导。", "translation": "我们提出了一种蒸馏缩放定律，该定律根据计算预算及其在学生和教师模型之间的分配来估计蒸馏模型的性能。我们的发现通过为教师和学生模型实现计算最优的分配来最大化学生性能，从而降低了与大规模蒸馏相关的风险。我们为两种关键场景提供了计算最优的蒸馏方案：当教师模型已存在时，以及当教师模型需要训练时。在涉及许多学生或现有教师模型的设置中，蒸馏性能优于监督学习，其计算水平与学生模型大小呈可预测的比例关系。相反，如果只有一个学生模型需要蒸馏且教师模型也需要训练，则监督学习通常更可取。此外，我们对蒸馏的大规模研究增加了我们对该过程的理解，并有助于指导实验设计。", "summary": "本文提出了一种蒸馏缩放定律，旨在根据计算预算和师生模型间的资源分配来预测和优化蒸馏模型的性能。研究提供了两种核心场景下的计算最优蒸馏策略，即教师模型已存在或需训练时。结果表明，在多学生或教师模型已有的情况下，蒸馏表现优于监督学习，但若仅有一个学生模型且教师模型也需训练，则监督学习更佳。这项大规模研究不仅降低了大规模蒸馏的风险，也加深了对蒸馏过程的理解。", "keywords": "蒸馏缩放定律, 计算预算, 师生模型, 性能优化, 监督学习", "comments": "该论文的创新点在于提出了“蒸馏缩放定律”，这是一个量化计算资源分配对蒸馏性能影响的理论框架。其重要性在于为大规模模型蒸馏提供了实际指导，帮助研究人员和工程师在不同场景下做出计算最优的决策，从而提高效率并降低风险。这对于优化资源受限环境下的模型部署具有重要意义。"}}
{"id": "2505.01737", "title": "Learning Multi-frame and Monocular Prior for Estimating Geometry in Dynamic Scenes", "authors": ["Seong Hyeon Park", "Jinwoo Shin"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This paper was supported by RLWRD", "url": "http://arxiv.org/abs/2505.01737v3", "summary": "In monocular videos that capture dynamic scenes, estimating the 3D geometry\nof video contents has been a fundamental challenge in computer vision.\nSpecifically, the task is significantly challenged by the object motion, where\nexisting models are limited to predict only partial attributes of the dynamic\nscenes, such as depth or pointmaps spanning only over a pair of frames. Since\nthese attributes are inherently noisy under multiple frames, test-time global\noptimizations are often employed to fully recover the geometry, which is liable\nto failure and incurs heavy inference costs. To address the challenge, we\npresent a new model, coined MMP, to estimate the geometry in a feed-forward\nmanner, which produces a dynamic pointmap representation that evolves over\nmultiple frames. Specifically, based on the recent Siamese architecture, we\nintroduce a new trajectory encoding module to project point-wise dynamics on\nthe representation for each frame, which can provide significantly improved\nexpressiveness for dynamic scenes. In our experiments, we find MMP can achieve\nstate-of-the-art quality in feed-forward pointmap prediction, e.g., 15.1%\nenhancement in the regression error.", "comment": "This paper was supported by RLWRLD", "pdf_url": "http://arxiv.org/pdf/2505.01737v3", "cate": "cs.CV", "date": "2025-05-03", "updated": "2025-07-28", "AI": {"title_translation": "学习多帧和单目先验用于动态场景中的几何估计", "tldr": "MMP模型通过结合多帧和单目先验，以端到端的方式估计动态场景的3D几何，实现了SOTA性能。", "motivation": "在单目视频中估计动态场景的3D几何是一个基本挑战，现有模型受限于只能预测部分属性，且在多帧下固有的噪声导致测试时全局优化容易失败并产生高昂的推理成本。", "method": "提出了一种名为MMP的新模型，以端到端的方式估计动态场景的几何。该模型生成一个随多帧演变的动态点图表示，并基于Siamese架构引入了一个新的轨迹编码模块，用于将点状动态投影到每帧的表示上。", "result": "MMP在端到端点图预测中实现了最先进的质量，例如，回归误差提升了15.1%。", "conclusion": "MMP模型通过其多帧动态点图表示和轨迹编码模块，有效解决了单目动态场景中3D几何估计的挑战，并取得了显著的性能提升。", "translation": "在捕获动态场景的单目视频中，估计视频内容的3D几何一直是计算机视觉领域的一个基本挑战。具体而言，该任务受到物体运动的显著挑战，现有模型仅限于预测动态场景的部分属性，例如仅跨越一对帧的深度或点图。由于这些属性在多帧下固有地存在噪声，因此通常采用测试时全局优化来完全恢复几何，这容易失败并产生高昂的推理成本。为了应对这一挑战，我们提出了一种名为MMP的新模型，以端到端的方式估计几何，该模型生成一个随多帧演变的动态点图表示。具体而言，基于最近的Siamese架构，我们引入了一个新的轨迹编码模块，用于将点状动态投影到每帧的表示上，这可以为动态场景提供显著改进的表达能力。在我们的实验中，我们发现MMP可以在端到端点图预测中实现最先进的质量，例如，回归误差提升了15.1%。", "summary": "本文提出了一种名为MMP的新模型，旨在解决在单目动态场景中估计3D几何的挑战。针对现有方法在处理物体运动、多帧噪声和高推理成本方面的局限性，MMP采用端到端的方式，生成随时间演变的动态点图表示。该模型基于Siamese架构，并引入了轨迹编码模块来捕获点状动态。实验结果表明，MMP在端到端点图预测方面达到了最先进的性能，回归误差提升了15.1%。", "keywords": "动态场景, 几何估计, 单目视频, 多帧, 点图", "comments": "该论文的创新点在于提出了MMP模型，它通过结合多帧信息和单目先验，以端到端的方式解决了动态场景下的3D几何估计问题，避免了传统方法中耗时且易错的全局优化步骤。其引入的轨迹编码模块增强了模型对动态场景的表达能力，并在性能上取得了显著提升，具有重要的实际应用价值。"}}
{"id": "2507.18726", "title": "Exploring the Landscape of Fairness Interventions in Software Engineering", "authors": ["Sadia Afrin Mim"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18726v1", "summary": "Current developments in AI made it broadly significant for reducing human\nlabor and expenses across several essential domains, including healthcare and\nfinance. However, the application of AI in the actual world poses multiple\nrisks and disadvantages due to potential risk factors in data (e.g., biased\ndataset). Practitioners developed a number of fairness interventions for\naddressing these kinds of problems. The paper acts as a survey, summarizing the\nvarious studies and approaches that have been developed to address fairness\nissues", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18726v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "探索软件工程中公平性干预的现状", "tldr": "本文综述了为解决人工智能应用中因数据偏差引起的公平性问题而开发的各种干预措施和方法。", "motivation": "人工智能在各个领域广泛应用，但由于数据（如偏见数据集）中潜在的风险因素，其应用带来了多重风险和劣势。研究人员开发了多种公平性干预措施来解决这些问题。", "method": "本文作为一项调查研究，总结了为解决公平性问题而开发的各种研究和方法。", "result": "本文总结了为解决公平性问题而开发的各种研究和方法。", "conclusion": "本文综述了为解决人工智能应用中因数据偏差引起的公平性问题而开发的各种干预措施。", "translation": "当前人工智能的发展在医疗保健和金融等几个重要领域显著减少了人力和开支。然而，人工智能在实际世界中的应用由于数据中潜在的风险因素（例如，偏见数据集）带来了多重风险和劣势。从业者开发了许多公平性干预措施来解决这些问题。本文作为一项调查研究，总结了为解决公平性问题而开发的各种研究和方法。", "summary": "本文对软件工程领域中为解决人工智能应用中因数据偏差导致的公平性问题所开发的各种干预措施和方法进行了全面的综述。它强调了人工智能在实际应用中面临的挑战，并总结了现有的解决方案。", "keywords": "公平性干预, 软件工程, 人工智能, 数据偏差, 综述", "comments": "本文作为一篇综述性文章，其重要性在于系统地梳理和总结了软件工程领域中针对人工智能公平性问题的现有干预措施，为研究人员和从业者提供了宝贵的参考，有助于理解当前研究的广度和深度。"}}
{"id": "2507.19142", "title": "A3D-MoE: Acceleration of Large Language Models with Mixture of Experts via 3D Heterogeneous Integration", "authors": ["Wei-Hsing Huang", "Janak Sharda", "Cheng-Jhih Shih", "Yuyao Kong", "Faaiq Waqar", "Pin-Jun Chen", "Yingyan", "Lin", "Shimeng Yu"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19142v1", "summary": "Conventional large language models (LLMs) are equipped with dozens of GB to\nTB of model parameters, making inference highly energy-intensive and costly as\nall the weights need to be loaded to onboard processing elements during\ncomputation. Recently, the Mixture-of-Experts (MoE) architecture has emerged as\nan efficient alternative, promising efficient inference with less activated\nweights per token. Nevertheless, fine-grained MoE-based LLMs face several\nchallenges: 1) Variable workloads during runtime create arbitrary GEMV-GEMM\nratios that reduce hardware utilization, 2) Traditional MoE-based scheduling\nfor LLM serving cannot fuse attention operations with MoE operations, leading\nto increased latency and decreased hardware utilization, and 3) Despite being\nmore efficient than conventional LLMs, loading experts from DRAM still consumes\nsignificant energy and requires substantial DRAM bandwidth. Addressing these\nchallenges, we propose: 1) A3D-MoE, a 3D Heterogeneous Integration system that\nemploys state-of-the-art vertical integration technology to significantly\nenhance memory bandwidth while reducing Network-on-Chip (NoC) overhead and\nenergy consumption. 2) A 3D-Adaptive GEMV-GEMM-ratio systolic array with\nV-Cache efficient data reuse and a novel unified 3D dataflow to solve the\nproblem of reduced hardware utilization caused by arbitrary GEMV-GEMM ratios\nfrom different workloads, 3) A Hardware resource-aware operation fusion\nscheduler that fuses attention operations with MoE operations to enhance\nhardware performance, and 4) MoE Score-Aware HBM access reduction with even-odd\nexpert placement that reduces DRAM access and bandwidth requirements. Our\nevaluation results indicate that A3D-MoE delivers significant performance\nenhancements, reducing latency by a factor of 1.8x to 2x and energy consumption\nby 2x to 4x, while improving throughput by 1.44x to 1.8x compared to the\nstate-of-the-art.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19142v1", "cate": "cs.AR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "A3D-MoE：通过3D异构集成加速专家混合（MoE）的大型语言模型", "tldr": "A3D-MoE是一种3D异构集成系统，通过创新的硬件设计和调度策略，显著加速了基于专家混合（MoE）的大型语言模型，降低了延迟和能耗，并提高了吞吐量。", "motivation": "传统大型语言模型（LLMs）参数量巨大，导致推理能耗高且成本昂贵。新兴的专家混合（MoE）架构虽能减少每token激活的权重，但仍面临挑战：1）运行时工作负载可变导致GEMV-GEMM比率不确定，降低硬件利用率；2）传统MoE调度无法融合注意力与MoE操作，增加延迟并降低硬件利用率；3）尽管MoE更高效，但从DRAM加载专家仍消耗大量能量并需要高带宽。", "method": "本文提出了A3D-MoE，一个3D异构集成系统，包含以下四点：1）采用最先进的垂直集成技术，显著提升内存带宽，同时减少片上网络（NoC）开销和能耗；2）提出具有V-Cache高效数据复用和新型统一3D数据流的3D自适应GEMV-GEMM比率脉动阵列，解决不同工作负载导致硬件利用率降低的问题；3）设计硬件资源感知的操作融合调度器，融合注意力与MoE操作以提升硬件性能；4）引入MoE分数感知HBM访问减少与奇偶专家放置策略，降低DRAM访问和带宽需求。", "result": "评估结果表明，A3D-MoE相较于现有技术，延迟降低1.8倍至2倍，能耗降低2倍至4倍，吞吐量提升1.44倍至1.8倍。", "conclusion": "A3D-MoE通过创新的3D异构集成、自适应硬件设计和优化调度策略，有效解决了MoE-based LLMs面临的挑战，实现了显著的性能提升和能耗降低。", "translation": "传统大型语言模型（LLMs）拥有数十GB到TB级的模型参数，使得推理过程能耗高昂且成本巨大，因为在计算过程中所有权重都需要加载到板载处理单元。最近，专家混合（MoE）架构作为一种高效的替代方案出现，有望通过每token激活更少的权重实现高效推理。然而，细粒度的MoE-based LLMs面临多项挑战：1）运行时可变的工作负载产生任意的GEMV-GEMM比率，降低了硬件利用率；2）传统的MoE-based LLM服务调度无法融合注意力操作与MoE操作，导致延迟增加和硬件利用率下降；3）尽管比传统LLMs更高效，但从DRAM加载专家仍然消耗大量能量并需要大量DRAM带宽。为了解决这些挑战，我们提出了：1）A3D-MoE，一个3D异构集成系统，采用最先进的垂直集成技术显著增强内存带宽，同时减少片上网络（NoC）开销和能耗。2）一个具有V-Cache高效数据复用和新型统一3D数据流的3D自适应GEMV-GEMM比率脉动阵列，解决由不同工作负载引起的任意GEMV-GEMM比率导致硬件利用率降低的问题。3）一个硬件资源感知的操作融合调度器，融合注意力操作与MoE操作以提升硬件性能。4）具有奇偶专家放置的MoE分数感知HBM访问减少策略，降低DRAM访问和带宽需求。我们的评估结果表明，与现有技术相比，A3D-MoE显著提升了性能，延迟降低1.8倍至2倍，能耗降低2倍至4倍，同时吞吐量提升1.44倍至1.8倍。", "summary": "本文提出了一种名为A3D-MoE的3D异构集成系统，旨在加速基于专家混合（MoE）的大型语言模型（LLMs）的推理过程。针对MoE-based LLMs面临的硬件利用率低、操作融合不足以及DRAM访问能耗高等问题，A3D-MoE通过垂直集成技术提升内存带宽，设计了3D自适应脉动阵列以优化数据复用和统一数据流，引入了硬件资源感知的操作融合调度器，并实现了MoE分数感知的HBM访问减少策略。实验结果表明，A3D-MoE显著提升了性能，降低了延迟和能耗，并提高了吞吐量，超越了现有最先进的技术。", "keywords": "大型语言模型, 专家混合, 3D异构集成, 硬件加速, 能耗优化", "comments": "A3D-MoE的创新性在于将3D异构集成技术与MoE架构的特点深度结合，从硬件层面解决了MoE在实际部署中遇到的关键性能瓶颈。其提出的四项具体技术，特别是3D-Adaptive GEMV-GEMM比率脉动阵列和操作融合调度器，展现了对MoE工作负载特性深刻理解后的精妙设计。这项工作对于推动高效LLM推理，特别是MoE模型的硬件加速具有重要意义，有望降低LLM的部署成本和能耗。"}}
{"id": "2507.19040", "title": "FD-Bench: A Full-Duplex Benchmarking Pipeline Designed for Full Duplex Spoken Dialogue Systems", "authors": ["Yizhou Peng", "Yi-Wen Chao", "Dianwen Ng", "Yukun Ma", "Chongjia Ni", "Bin Ma", "Eng Siong Chng"], "categories": ["eess.AS", "cs.CL"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted to Interspeech 2025. 5 pages", "url": "http://arxiv.org/abs/2507.19040v1", "summary": "Full-duplex spoken dialogue systems (FDSDS) enable more natural human-machine\ninteractions by allowing real-time user interruptions and backchanneling,\ncompared to traditional SDS that rely on turn-taking. However, existing\nbenchmarks lack metrics for FD scenes, e.g., evaluating model performance\nduring user interruptions. In this paper, we present a comprehensive FD\nbenchmarking pipeline utilizing LLMs, TTS, and ASR to address this gap. It\nassesses FDSDS's ability to handle user interruptions, manage delays, and\nmaintain robustness in challenging scenarios with diverse novel metrics. We\napplied our benchmark to three open-source FDSDS (Moshi, Freeze-omni, and\nVITA-1.5) using over 40 hours of generated speech, with 293 simulated\nconversations and 1,200 interruptions. The results show that all models\ncontinue to face challenges, such as failing to respond to user interruptions,\nunder frequent disruptions and noisy conditions. Demonstrations, data, and code\nwill be released.", "comment": "Accepted to Interspeech 2025. 5 pages", "pdf_url": "http://arxiv.org/pdf/2507.19040v1", "cate": "eess.AS", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "FD-Bench：一个为全双工语音对话系统设计的全双工基准测试流程", "tldr": "FD-Bench是一个新的全双工语音对话系统基准测试流程，用于评估模型在用户中断等场景下的性能，发现现有模型仍面临挑战。", "motivation": "传统语音对话系统依赖轮流对话，而全双工语音对话系统（FDSDS）通过允许实时用户中断和回溯，实现了更自然的人机交互。然而，现有基准测试缺乏针对全双工场景的指标，例如评估模型在用户中断时的性能，因此需要一个全面的全双工基准测试流程来填补这一空白。", "method": "本文提出了一个利用大型语言模型（LLMs）、文本转语音（TTS）和自动语音识别（ASR）的综合全双工基准测试流程FD-Bench。它通过多样化的新指标评估FDSDS处理用户中断、管理延迟以及在挑战性场景中保持鲁棒性的能力。该基准测试应用于三个开源FDSDS（Moshi、Freeze-omni和VITA-1.5），使用了超过40小时的生成语音，包括293个模拟对话和1,200次中断。", "result": "结果显示，所有测试模型在频繁中断和嘈杂条件下，仍面临挑战，例如未能响应用户中断。", "conclusion": "全双工语音对话系统在处理用户中断和复杂环境方面仍需改进，FD-Bench提供了一个有效的评估工具来识别这些挑战。", "translation": "全双工语音对话系统（FDSDS）通过允许实时用户中断和回溯，与依赖轮流对话的传统SDS相比，实现了更自然的人机交互。然而，现有基准测试缺乏针对全双工场景的指标，例如评估模型在用户中断时的性能。在本文中，我们提出了一个利用大型语言模型（LLMs）、文本转语音（TTS）和自动语音识别（ASR）的综合全双工基准测试流程FD-Bench，以解决这一空白。它通过多样化的新指标，评估FDSDS处理用户中断、管理延迟以及在挑战性场景中保持鲁棒性的能力。我们将该基准测试应用于三个开源FDSDS（Moshi、Freeze-omni和VITA-1.5），使用了超过40小时的生成语音，包括293个模拟对话和1,200次中断。结果显示，所有模型在频繁中断和嘈杂条件下，仍面临挑战，例如未能响应用户中断。演示、数据和代码将发布。", "summary": "本文介绍了FD-Bench，一个为全双工语音对话系统（FDSDS）设计的新型基准测试流程。针对现有基准缺乏全双工场景（如用户中断）评估指标的空白，FD-Bench利用大型语言模型、文本转语音和自动语音识别技术，全面评估FDSDS处理用户中断、管理延迟及在复杂场景下的鲁棒性。研究团队将FD-Bench应用于三个开源FDSDS，通过大量模拟对话和中断测试，发现现有模型在响应用户中断、面对频繁干扰和噪声时仍存在显著挑战。", "keywords": "全双工语音对话系统, 基准测试, 用户中断, LLM, FD-Bench", "comments": "FD-Bench的创新在于其为全双工语音对话系统提供了一个急需的、全面的基准测试框架，填补了现有评估工具的空白。其利用LLM、TTS和ASR生成大量模拟对话和中断场景，使得对FDSDS在真实世界交互中的表现进行细致评估成为可能。这项工作对于推动FDSDS的发展和改进具有重要意义，因为它明确指出了当前模型的局限性，特别是在处理用户中断和复杂环境方面的挑战。"}}
{"id": "2407.02348", "title": "Agreement-Based Cascading for Efficient Inference", "authors": ["Steven Kolawole", "Don Dennis", "Ameet Talwalkar", "Virginia Smith"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Published at TMLR (July 2025)", "url": "http://arxiv.org/abs/2407.02348v3", "summary": "Adaptive inference schemes reduce the cost of machine learning inference by\nassigning smaller models to easier examples, attempting to avoid invocation of\nlarger models when possible. In this work we explore a simple, effective\nadaptive inference technique we term Agreement-Based Cascading (ABC). ABC\nbuilds a cascade of models of increasing size/complexity, and uses agreement\nbetween ensembles of models at each level of the cascade as a basis for\ndata-dependent routing. Although ensemble execution introduces additional\nexpense, we show that these costs can be easily offset in practice due to large\nexpected differences in model sizes, parallel inference execution capabilities,\nand accuracy benefits of ensembling. We examine ABC theoretically and\nempirically in terms of these parameters, showing that the approach can\nreliably act as a drop-in replacement for existing models and surpass the best\nsingle model it aims to replace in terms of both efficiency and accuracy.\nAdditionally, we explore the performance of ABC relative to existing cascading\nmethods in three common scenarios: (1) edge-to-cloud inference, where ABC\nreduces communication costs by up to 14x; (2) cloud-based model serving, where\nit achieves a 3x reduction in rental costs; and (3) inference via model API\nservices, where ABC achieves a 2-25x reduction in average price per\ntoken/request relative to state-of-the-art LLM cascades.", "comment": "Published at TMLR (July 2025)", "pdf_url": "http://arxiv.org/pdf/2407.02348v3", "cate": "cs.LG", "date": "2024-07-02", "updated": "2025-07-25", "AI": {"title_translation": "基于一致性的级联实现高效推理", "tldr": "提出了一种名为“基于一致性的级联”（ABC）的自适应推理技术，通过模型集成和数据依赖路由，在效率和准确性上超越现有模型，并在多种场景下显著降低成本。", "motivation": "现有的自适应推理方案通过为较简单的示例分配较小的模型来降低机器学习推理成本，以尽可能避免调用较大的模型。本文旨在探索一种简单有效的自适应推理技术来进一步优化。", "method": "提出了一种名为“基于一致性的级联”（ABC）的自适应推理技术。ABC构建了一个由大小/复杂性递增的模型组成的级联，并利用级联中每个级别模型集成之间的一致性作为数据依赖路由的基础。尽管集成执行会引入额外开销，但由于模型大小的预期差异、并行推理执行能力以及集成带来的准确性优势，这些成本在实践中可以很容易地抵消。", "result": "ABC在效率和准确性方面超越了其旨在替代的最佳单一模型，并可以作为现有模型的可靠替代品。在边缘到云推理场景中，ABC将通信成本降低了高达14倍。在基于云的模型服务场景中，ABC将租赁成本降低了3倍。在通过模型API服务进行推理的场景中，ABC相对于最先进的LLM级联，将平均每令牌/请求价格降低了2-25倍。", "conclusion": "基于一致性的级联（ABC）是一种有效且高效的自适应推理技术，在多种实际应用场景中能够显著降低成本并提升性能，超越了现有的单一模型和级联方法。", "translation": "自适应推理方案通过为较简单的示例分配较小的模型来降低机器学习推理成本，旨在尽可能避免调用较大的模型。在这项工作中，我们探索了一种简单、有效的自适应推理技术，我们称之为基于一致性的级联（ABC）。ABC构建了一个由大小/复杂性递增的模型组成的级联，并利用级联中每个级别模型集成之间的一致性作为数据依赖路由的基础。尽管集成执行会引入额外开销，但我们表明，由于模型大小的预期差异、并行推理执行能力以及集成带来的准确性优势，这些成本在实践中可以很容易地抵消。我们从理论和经验上考察了ABC在这些参数方面的表现，结果表明该方法可以可靠地替代现有模型，并在效率和准确性方面超越其旨在替代的最佳单一模型。此外，我们还在三种常见场景中探讨了ABC相对于现有级联方法的性能：(1) 边缘到云推理，ABC将通信成本降低了高达14倍；(2) 基于云的模型服务，ABC实现了租赁成本3倍的降低；(3) 通过模型API服务进行推理，ABC相对于最先进的LLM级联，将平均每令牌/请求价格降低了2-25倍。", "summary": "本文提出了一种名为“基于一致性的级联”（ABC）的自适应推理技术，旨在提高机器学习推理的效率和准确性。ABC通过构建模型级联并在每个级别利用模型集成之间的一致性进行数据依赖路由。研究表明，尽管存在集成开销，但ABC在效率和准确性上均优于现有单一模型，并能显著降低边缘到云推理（通信成本降低14倍）、云模型服务（租赁成本降低3倍）和模型API服务（每请求价格降低2-25倍）等多种实际场景的成本。", "keywords": "自适应推理, 模型级联, 模型集成, 效率, 成本降低", "comments": "这篇论文提出了一种新颖的自适应推理方法ABC，其创新点在于结合了模型级联和模型集成的一致性进行数据依赖路由。这种方法不仅在理论上证明了其有效性，并通过在多个实际应用场景中的显著成本降低和性能提升验证了其实用价值。特别是在LLM服务中能大幅降低成本，显示了其潜在的经济效益和广泛应用前景。"}}
{"id": "2503.21211", "title": "Interpretable Cross-Sphere Multiscale Deep Learning Predicts ENSO Skilfully Beyond 2 Years", "authors": ["Rixu Hao", "Yuxin Zhao", "Shaoqing Zhang", "Guihua Wang", "Xiong Deng"], "categories": ["physics.ao-ph", "cs.LG"], "primary_category": "Subjects:       Atmospheric and Oceanic Physics (physics.ao-ph)", "pdf_link": null, "comments": "Comments:      13 pages, 4 figures", "url": "http://arxiv.org/abs/2503.21211v2", "summary": "El Ni\\~no-Southern Oscillation (ENSO) exerts global climate and societal\nimpacts, but real-time prediction with lead times beyond one year remains\nchallenging. Dynamical models suffer from large biases and uncertainties, while\ndeep learning struggles with interpretability and multi-scale dynamics. Here,\nwe introduce PTSTnet, an interpretable model that unifies dynamical processes\nand cross-scale spatiotemporal learning in an innovative neural-network\nframework with physics-encoding learning. PTSTnet produces interpretable\npredictions significantly outperforming state-of-the-art benchmarks with lead\ntimes beyond 24 months, providing physical insights into error propagation in\nocean-atmosphere interactions. PTSTnet learns feature representations with\nphysical consistency from sparse data to tackle inherent multi-scale and\nmulti-physics challenges underlying ocean-atmosphere processes, thereby\ninherently enhancing long-term prediction skill. Our successful realizations\nmark substantial steps forward in interpretable insights into innovative neural\nocean modelling.", "comment": "13 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2503.21211v2", "cate": "physics.ao-ph", "date": "2025-03-27", "updated": "2025-07-25", "AI": {"title_translation": "可解释的跨领域多尺度深度学习能成功预测两年以上的ENSO", "tldr": "PTSTnet是一种可解释的深度学习模型，通过结合动力学过程和跨尺度时空学习，显著提高了ENSO的长期预测能力，并提供了物理见解。", "motivation": "厄尔尼诺-南方涛动（ENSO）对全球气候和社会有巨大影响，但目前实时预测的提前期超过一年仍然具有挑战性。动态模型存在较大的偏差和不确定性，而深度学习则面临可解释性和多尺度动力学问题。", "method": "本研究引入了PTSTnet，这是一种可解释的模型，它在一个创新的神经网络框架中统一了动力学过程和跨尺度时空学习，并融入了物理编码学习。PTSTnet从稀疏数据中学习具有物理一致性的特征表示，以解决海洋-大气过程中固有的多尺度和多物理挑战。", "result": "PTSTnet的预测能力显著优于现有基准模型，提前期超过24个月。它还提供了对海洋-大气相互作用中误差传播的物理见解，并增强了长期预测技能。", "conclusion": "PTSTnet的成功实现标志着在可解释的创新海洋神经网络建模方面迈出了实质性的一步，为ENSO的长期预测提供了新的可能性和物理见解。", "translation": "厄尔尼诺-南方涛动（ENSO）对全球气候和社会产生影响，但提前期超过一年的实时预测仍然具有挑战性。动力学模型存在较大的偏差和不确定性，而深度学习则在可解释性和多尺度动力学方面面临困难。在此，我们引入了PTSTnet，一个可解释的模型，它在一个创新的神经网络框架中统一了动力学过程和跨尺度时空学习，并具有物理编码学习功能。PTSTnet生成的可解释预测在提前期超过24个月的情况下，显著优于最先进的基准模型，并为海洋-大气相互作用中的误差传播提供了物理见解。PTSTnet从稀疏数据中学习具有物理一致性的特征表示，以解决海洋-大气过程中固有的多尺度和多物理挑战，从而内在增强了长期预测技能。我们成功的实现标志着在可解释的创新海洋神经网络建模方面迈出了实质性的一步。", "summary": "本研究提出了一种名为PTSTnet的可解释深度学习模型，旨在解决ENSO长期预测中的挑战。该模型通过整合动力学过程和跨尺度时空学习，并结合物理编码，显著提高了ENSO预测的提前期至两年以上，并超越了现有最佳模型。PTSTnet还能从稀疏数据中学习具有物理一致性的特征，为海洋-大气相互作用中的误差传播提供了物理见解，从而增强了长期预测能力。", "keywords": "ENSO预测, 深度学习, 可解释性, 多尺度, 海洋-大气相互作用", "comments": "该论文的创新之处在于提出了PTSTnet，一个结合了深度学习与物理编码，并能处理多尺度和跨领域问题的模型。它不仅提高了ENSO长期预测的准确性，更重要的是，提供了可解释性，揭示了误差传播的物理机制，这对于传统深度学习模型来说是一个显著的进步。其重要性在于为气候预测领域带来了更可靠、更具洞察力的工具。"}}
{"id": "2507.18678", "title": "Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting", "authors": ["Xingyu Miao", "Haoran Duan", "Quanhao Qian", "Jiuniu Wang", "Yang Long", "Ling Shao", "Deli Zhao", "Ran Xu", "Gongjie Zhang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025 (Highlight)", "url": "http://arxiv.org/abs/2507.18678v1", "summary": "Spatial intelligence is emerging as a transformative frontier in AI, yet it\nremains constrained by the scarcity of large-scale 3D datasets. Unlike the\nabundant 2D imagery, acquiring 3D data typically requires specialized sensors\nand laborious annotation. In this work, we present a scalable pipeline that\nconverts single-view images into comprehensive, scale- and appearance-realistic\n3D representations - including point clouds, camera poses, depth maps, and\npseudo-RGBD - via integrated depth estimation, camera calibration, and scale\ncalibration. Our method bridges the gap between the vast repository of imagery\nand the increasing demand for spatial scene understanding. By automatically\ngenerating authentic, scale-aware 3D data from images, we significantly reduce\ndata collection costs and open new avenues for advancing spatial intelligence.\nWe release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D,\nand demonstrate through extensive experiments that our generated data can\nbenefit various 3D tasks, ranging from fundamental perception to MLLM-based\nreasoning. These results validate our pipeline as an effective solution for\ndeveloping AI systems capable of perceiving, understanding, and interacting\nwith physical environments.", "comment": "ICCV 2025 (Highlight)", "pdf_url": "http://arxiv.org/pdf/2507.18678v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "通过2D到3D数据提升实现可扩展空间智能", "tldr": "该研究提出了一种将2D图像转换为3D数据表示的可扩展管道，以解决大规模3D数据集稀缺的问题，并促进空间智能的发展。", "motivation": "空间智能作为AI的前沿领域，受到大规模3D数据集稀缺的限制。获取3D数据通常需要专业传感器和繁琐的标注，而2D图像数据则非常丰富。", "method": "本研究提出了一种可扩展的管道，通过集成的深度估计、相机校准和尺度校准，将单视图图像转换为全面的、具有尺度和外观真实感的3D表示，包括点云、相机姿态、深度图和伪RGBD。", "result": "该方法通过自动从图像生成真实的、尺度感知的3D数据，显著降低了数据收集成本。研究发布了COCO-3D和Objects365-v2-3D两个生成的空间数据集，并通过大量实验证明其生成的数据能够赋能各种3D任务，从基本感知到基于MLLM的推理。", "conclusion": "这些结果验证了该管道作为一种有效解决方案，可用于开发能够感知、理解和与物理环境交互的AI系统。", "translation": "空间智能正成为人工智能领域一个具有变革性的前沿，但仍受限于大规模3D数据集的稀缺。与丰富的2D图像不同，获取3D数据通常需要专业的传感器和费力的标注。在这项工作中，我们提出了一种可扩展的管道，通过集成的深度估计、相机校准和尺度校准，将单视图图像转换为全面的、具有尺度和外观真实感的3D表示——包括点云、相机姿态、深度图和伪RGBD。我们的方法弥合了图像的巨大存储库与对空间场景理解日益增长的需求之间的鸿沟。通过自动从图像生成真实的、尺度感知的3D数据，我们显著降低了数据收集成本，并为推动空间智能开辟了新途径。我们发布了两个生成的空间数据集，即COCO-3D和Objects365-v2-3D，并通过大量实验证明我们生成的数据可以有益于各种3D任务，从基本感知到基于MLLM的推理。这些结果验证了我们的管道是开发能够感知、理解和与物理环境交互的AI系统的有效解决方案。", "summary": "本研究提出了一种创新的、可扩展的管道，旨在通过将丰富的2D图像转换为全面的、具有尺度和外观真实感的3D数据（包括点云、相机姿态、深度图和伪RGBD），以解决大规模3D数据集稀缺的问题。该方法通过集成深度估计、相机校准和尺度校准，显著降低了3D数据收集成本，并促进了空间智能的发展。研究发布了两个新生成的3D数据集，并通过实验证明其数据能有效支持从基本感知到高级推理的多种3D任务，验证了其在构建与物理环境交互的AI系统方面的潜力。", "keywords": "空间智能, 2D到3D, 数据生成, 3D数据集, 深度估计", "comments": "这项工作通过提出一种从2D图像生成高质量、大规模3D数据的可扩展管道，解决了当前空间智能发展面临的核心瓶颈——3D数据集稀缺问题。其创新之处在于能够自动生成尺度感知且外观真实的3D表示，并已验证其数据对多种3D任务的有效性，具有重要的实际应用价值和推动AI领域发展的潜力。"}}
{"id": "2507.18748", "title": "PPipe: Efficient Video Analytics Serving on Heterogeneous GPU Clusters via Pool-Based Pipeline Parallelism", "authors": ["Z. Jonny Kong", "Qiang Xu", "Y. Charlie Hu"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18748v1", "summary": "With the rapid innovation of GPUs, heterogeneous GPU clusters in both public\nclouds and on-premise data centers have become increasingly commonplace. In\nthis paper, we demonstrate how pipeline parallelism, a technique wellstudied\nfor throughput-oriented deep learning model training, can be used effectively\nfor serving latency-bound model inference, e.g., in video analytics systems, on\nheterogeneous GPU clusters. Our work exploits the synergy between diversity in\nmodel layers and diversity in GPU architectures, which results in comparable\ninference latency for many layers when running on low-class and high-class\nGPUs. We explore how such overlooked capability of low-class GPUs can be\nexploited using pipeline parallelism and present a novel inference serving\nsystem, PPipe, that employs pool-based pipeline parallelism via an MILP-based\ncontrol plane and a data plane that performs resource reservation-based\nadaptive batching. Evaluation results on diverse workloads (18 CNN models) show\nthat PPipe achieves 41.1% - 65.5% higher utilization of low-class GPUs while\nmaintaining high utilization of high-class GPUs, leading to 32.2% - 75.1%\nhigher serving throughput compared to various baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18748v1", "cate": "cs.DC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "PPipe：通过基于池的管道并行在异构GPU集群上提供高效视频分析服务", "tldr": "PPipe利用管道并行在异构GPU集群上提供高效视频分析服务，提高了低端GPU利用率和整体吞吐量。", "motivation": "随着异构GPU集群的日益普及，需要一种有效的方法来利用管道并行技术在这些集群上提供低延迟的模型推理服务，特别是解决低端GPU的潜在能力未被充分利用的问题。", "method": "本文提出了一种名为PPipe的新型推理服务系统，该系统采用基于池的管道并行，通过一个基于MILP的控制平面和一个执行基于资源预留的自适应批处理的数据平面。该方法利用了模型层多样性和GPU架构多样性之间的协同作用，使得在低端和高端GPU上运行许多层时具有可比的推理延迟。", "result": "在18个CNN模型的不同工作负载下评估，PPipe实现了低端GPU 41.1% - 65.5%的更高利用率，同时保持高端GPU的高利用率，与各种基线相比，服务吞吐量提高了32.2% - 75.1%。", "conclusion": "PPipe通过有效地利用异构GPU集群中的低端GPU，显著提高了视频分析服务的吞吐量和资源利用率。", "translation": "随着GPU的快速创新，公共云和本地数据中心中的异构GPU集群变得越来越普遍。在本文中，我们展示了管道并行（一种针对吞吐量导向的深度学习模型训练而深入研究的技术）如何有效地用于在异构GPU集群上提供延迟受限的模型推理服务，例如在视频分析系统中。我们的工作利用了模型层多样性和GPU架构多样性之间的协同作用，这使得许多层在低端和高端GPU上运行时具有可比的推理延迟。我们探讨了如何利用管道并行来利用低端GPU的这种被忽视的能力，并提出了一种新颖的推理服务系统PPipe，该系统通过基于MILP的控制平面和执行基于资源预留的自适应批处理的数据平面来实现基于池的管道并行。在不同工作负载（18个CNN模型）下的评估结果表明，PPipe在保持高端GPU高利用率的同时，实现了低端GPU 41.1% - 65.5%的更高利用率，与各种基线相比，服务吞吐量提高了32.2% - 75.1%。", "summary": "本文提出PPipe系统，旨在异构GPU集群上高效提供视频分析服务。它创新性地将管道并行应用于延迟敏感的模型推理，并通过利用模型层和GPU架构多样性，充分发挥低端GPU的潜力。PPipe采用基于MILP的控制平面和自适应批处理的数据平面，实现了基于池的管道并行。实验证明，PPipe显著提升了低端GPU的利用率，并大幅提高了整体服务吞吐量。", "keywords": "异构GPU集群, 视频分析, 管道并行, 推理服务, 资源利用率", "comments": "该论文的创新点在于将传统用于吞吐量导向训练的管道并行技术，创造性地应用于延迟敏感的推理服务，并特别关注了异构GPU集群中低端GPU的有效利用。通过协同利用模型和硬件多样性，PPipe提供了一种在成本效益和性能之间取得平衡的有效方案，对于云服务提供商和数据中心优化资源利用具有重要意义。"}}
{"id": "2504.18880", "title": "Reshaping MOFs text mining with a dynamic multi-agents framework of large language model", "authors": ["Zuhong Lin", "Daoyuan Ren", "Kai Ran", "Jing Sun", "Songlin Yu", "Xuefeng Bai", "Xiaotiang Huang", "Haiyang He", "Pengxu Pan", "Xiaohang Zhang", "Ying Fang", "Tianying Wang", "Minli Wu", "Zhanglin Li", "Xiaochuan Zhang", "Haipu Li", "Jingjing Yao"], "categories": ["cs.AI", "cond-mat.mtrl-sci"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.18880v2", "summary": "Accurately identifying synthesis conditions for metal-organic frameworks\n(MOFs) remains a critical bottleneck in materials research, as translating\nliterature-derived knowledge into actionable insights is hindered by the\nunstructured and heterogeneous nature of scientific texts. Here we present\nMOFh6, a large language model (LLM)-based multi-agent system designed to\nextract, structure, and apply synthesis knowledge from diverse input formats,\nincluding raw literature and crystal codes. Built on gpt-4o-mini and fine-tuned\nwith up to few-shot expert-annotated data, MOFh6 achieves 99% accuracy in\nsynthesis data parsing and resolves 94.1% of complex co-reference\nabbreviations. It processes a single full-text document in 9.6 seconds and\nlocalizes structured synthesis descriptions within 36 seconds, with the cost\nper 100 papers reduced to USD 4.24, a 76% saving over existing systems. By\naddressing long-standing limitations in cross-paragraph semantic fusion and\nterminology standardization, MOFh6 reshapes the LLM-based paradigm for MOF\nsynthesis research, transforming static retrieval into an integrated and\ndynamic knowledge acquisition process. This shift bridges the gap between\nscientific literature and actionable synthesis design, providing a scalable\nframework for accelerating materials discovery.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.18880v2", "cate": "cs.AI", "date": "2025-04-26", "updated": "2025-07-25", "AI": {"title_translation": "利用大型语言模型的动态多智能体框架重塑MOFs文本挖掘", "tldr": "MOFh6是一个基于LLM的多智能体系统，用于从科学文献中高效、准确地提取和结构化MOF合成条件，显著提高了数据解析效率并降低了成本。", "motivation": "准确识别金属有机框架（MOFs）的合成条件是材料研究中的一个关键瓶颈，因为将文献知识转化为可操作的见解受到科学文本非结构化和异构性质的阻碍。", "method": "本文提出了MOFh6，一个基于大型语言模型（LLM）的多智能体系统，旨在从包括原始文献和晶体代码在内的不同输入格式中提取、结构化和应用合成知识。MOFh6基于gpt-4o-mini构建，并使用少量专家标注数据进行了微调。", "result": "MOFh6在合成数据解析中实现了99%的准确率，解决了94.1%的复杂共指缩写问题。它在9.6秒内处理单个全文文档，并在36秒内定位结构化合成描述。每100篇论文的成本降至4.24美元，比现有系统节省了76%。", "conclusion": "通过解决跨段落语义融合和术语标准化方面的长期限制，MOFh6重塑了基于LLM的MOF合成研究范式，将静态检索转变为集成动态知识获取过程，弥合了科学文献与可操作合成设计之间的差距，为加速材料发现提供了可扩展的框架。", "translation": "准确识别金属有机框架（MOFs）的合成条件仍然是材料研究中的一个关键瓶颈，因为将文献知识转化为可操作的见解受到科学文本非结构化和异构性质的阻碍。本文提出了MOFh6，一个基于大型语言模型（LLM）的多智能体系统，旨在从包括原始文献和晶体代码在内的不同输入格式中提取、结构化和应用合成知识。MOFh6基于gpt-4o-mini构建，并使用少量专家标注数据进行了微调，在合成数据解析中实现了99%的准确率，解决了94.1%的复杂共指缩写问题。它在9.6秒内处理单个全文文档，并在36秒内定位结构化合成描述，每100篇论文的成本降至4.24美元，比现有系统节省了76%。通过解决跨段落语义融合和术语标准化方面的长期限制，MOFh6重塑了基于LLM的MOF合成研究范式，将静态检索转变为集成动态知识获取过程。这一转变弥合了科学文献与可操作合成设计之间的差距，为加速材料发现提供了可扩展的框架。", "summary": "MOFh6是一个基于gpt-4o-mini并经微调的大型语言模型多智能体系统，旨在解决MOF合成条件识别中科学文献非结构化的问题。该系统能从多种输入格式中高效提取、结构化和应用合成知识，实现了99%的数据解析准确率和94.1%的共指消解率。它显著提高了处理速度并大幅降低了成本，将MOF研究中的静态知识检索转变为动态知识获取过程，加速了材料发现。", "keywords": "MOFs, 文本挖掘, 大型语言模型, 多智能体系统, 合成条件", "comments": "MOFh6的创新之处在于其将LLM与多智能体框架相结合，专门针对MOF合成数据的复杂性进行优化，有效解决了跨段落语义融合和术语标准化等长期挑战。该系统在准确性、处理速度和成本效益方面均取得了显著提升，为材料科学领域的自动化知识提取和合成设计提供了强大的工具，具有重要的应用前景。"}}
{"id": "2507.19360", "title": "EA-ViT: Efficient Adaptation for Elastic Vision Transformer", "authors": ["Chen Zhu", "Wangbo Zhao", "Huiwen Zhang", "Samir Khaki", "Yuhao Zhou", "Weidong Tang", "Shuo Wang", "Zhihang Yuan", "Yuzhang Shang", "Xiaojiang Peng", "Kai Wang", "Dawei Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Published as a conference paper at ICCV 2025", "url": "http://arxiv.org/abs/2507.19360v1", "summary": "Vision Transformers (ViTs) have emerged as a foundational model in computer\nvision, excelling in generalization and adaptation to downstream tasks.\nHowever, deploying ViTs to support diverse resource constraints typically\nrequires retraining multiple, size-specific ViTs, which is both time-consuming\nand energy-intensive. To address this issue, we propose an efficient ViT\nadaptation framework that enables a single adaptation process to generate\nmultiple models of varying sizes for deployment on platforms with various\nresource constraints. Our approach comprises two stages. In the first stage, we\nenhance a pre-trained ViT with a nested elastic architecture that enables\nstructural flexibility across MLP expansion ratio, number of attention heads,\nembedding dimension, and network depth. To preserve pre-trained knowledge and\nensure stable adaptation, we adopt a curriculum-based training strategy that\nprogressively increases elasticity. In the second stage, we design a\nlightweight router to select submodels according to computational budgets and\ndownstream task demands. Initialized with Pareto-optimal configurations derived\nvia a customized NSGA-II algorithm, the router is then jointly optimized with\nthe backbone. Extensive experiments on multiple benchmarks demonstrate the\neffectiveness and versatility of EA-ViT. The code is available at\nhttps://github.com/zcxcf/EA-ViT.", "comment": "Published as a conference paper at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.19360v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "EA-ViT：弹性视觉Transformer的高效自适应", "tldr": "EA-ViT提出了一种高效的视觉Transformer（ViT）自适应框架，通过单一自适应过程生成不同大小的模型，以适应多样化的资源约束。", "motivation": "部署视觉Transformer（ViT）通常需要为不同的资源限制重新训练多个特定大小的ViT模型，这既耗时又耗能。", "method": "EA-ViT包含两个阶段：第一阶段，通过嵌套弹性架构增强预训练ViT，使其在MLP扩展比、注意力头数量、嵌入维度和网络深度上具有结构灵活性，并采用基于课程的训练策略以保持预训练知识和稳定自适应。第二阶段，设计一个轻量级路由器，根据计算预算和下游任务需求选择子模型，该路由器通过定制的NSGA-II算法得到的帕累托最优配置初始化，并与主干网络联合优化。", "result": "在多个基准测试上的大量实验证明了EA-ViT的有效性和多功能性。", "conclusion": "EA-ViT提供了一种高效且灵活的解决方案，使得视觉Transformer能够以单一自适应过程适应不同的资源约束，从而解决了多模型训练的耗时耗能问题。", "translation": "视觉Transformer（ViTs）已成为计算机视觉领域的基础模型，在泛化和适应下游任务方面表现出色。然而，部署ViTs以支持多样化的资源约束通常需要重新训练多个特定大小的ViTs，这既耗时又耗能。为了解决这个问题，我们提出了一种高效的ViT自适应框架，该框架能够通过单一的自适应过程生成不同大小的多个模型，以便部署在具有各种资源约束的平台上。我们的方法包括两个阶段。在第一阶段，我们通过嵌套弹性架构增强预训练的ViT，该架构在MLP扩展比、注意力头数量、嵌入维度和网络深度方面实现了结构灵活性。为了保留预训练知识并确保稳定的自适应，我们采用了基于课程的训练策略，逐步增加弹性。在第二阶段，我们设计了一个轻量级路由器，根据计算预算和下游任务需求选择子模型。路由器通过定制的NSGA-II算法导出的帕累托最优配置进行初始化，然后与主干网络联合优化。在多个基准测试上的大量实验证明了EA-ViT的有效性和多功能性。代码可在https://github.com/zcxcf/EA-ViT获取。", "summary": "EA-ViT提出了一种高效的视觉Transformer（ViT）自适应框架，旨在解决ViT在不同资源约束下部署时需要重复训练的问题。该框架分两阶段：首先，通过引入嵌套弹性架构和课程学习策略，增强预训练ViT的结构灵活性并保持知识；其次，设计一个轻量级路由器，通过帕累托最优配置初始化并与主干网络联合优化，以根据计算预算选择合适的子模型。实验证明了该方法的有效性和多功能性，实现了单一自适应过程生成多尺寸模型。", "keywords": "视觉Transformer, 模型自适应, 弹性架构, 资源约束, NSGA-II", "comments": "EA-ViT的创新之处在于其提出的两阶段高效自适应框架，特别是嵌套弹性架构和基于课程的训练策略，以及利用NSGA-II算法初始化路由器来选择子模型。这极大地提高了ViT在不同资源约束下的部署效率，解决了传统方法中耗时耗能的多模型训练问题，具有重要的实际应用价值。"}}
{"id": "2505.04963", "title": "ViCTr: Vital Consistency Transfer for Pathology Aware Image Synthesis", "authors": ["Onkar Susladkar", "Gayatri Deshmukh", "Yalcin Tur", "Gorkhem Durak", "Ulas Bagci"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted in ICCV 2025", "url": "http://arxiv.org/abs/2505.04963v3", "summary": "Synthesizing medical images remains challenging due to limited annotated\npathological data, modality domain gaps, and the complexity of representing\ndiffuse pathologies such as liver cirrhosis. Existing methods often struggle to\nmaintain anatomical fidelity while accurately modeling pathological features,\nfrequently relying on priors derived from natural images or inefficient\nmulti-step sampling. In this work, we introduce ViCTr (Vital Consistency\nTransfer), a novel two-stage framework that combines a rectified flow\ntrajectory with a Tweedie-corrected diffusion process to achieve high-fidelity,\npathology-aware image synthesis. First, we pretrain ViCTr on the ATLAS-8k\ndataset using Elastic Weight Consolidation (EWC) to preserve critical\nanatomical structures. We then fine-tune the model adversarially with Low-Rank\nAdaptation (LoRA) modules for precise control over pathology severity. By\nreformulating Tweedie's formula within a linear trajectory framework, ViCTr\nsupports one-step sampling, reducing inference from 50 steps to just 4, without\nsacrificing anatomical realism. We evaluate ViCTr on BTCV (CT), AMOS (MRI), and\nCirrMRI600+ (cirrhosis) datasets. Results demonstrate state-of-the-art\nperformance, achieving a Medical Frechet Inception Distance (MFID) of 17.01 for\ncirrhosis synthesis 28% lower than existing approaches and improving nnUNet\nsegmentation by +3.8% mDSC when used for data augmentation. Radiologist reviews\nindicate that ViCTr-generated liver cirrhosis MRIs are clinically\nindistinguishable from real scans. To our knowledge, ViCTr is the first method\nto provide fine-grained, pathology-aware MRI synthesis with graded severity\ncontrol, closing a critical gap in AI-driven medical imaging research.", "comment": "Accepted in ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2505.04963v3", "cate": "cs.CV", "date": "2025-05-08", "updated": "2025-07-25", "AI": {"title_translation": "ViCTr：用于病理感知图像合成的生命一致性转移", "tldr": "ViCTr是一个新颖的两阶段框架，结合校正流和Tweedie校正扩散过程，实现了高保真、病理感知的医学图像合成。它通过一步采样显著减少推理时间，并在多个数据集上取得了最先进的性能，生成的图像在临床上与真实扫描无法区分。", "motivation": "医学图像合成面临挑战，包括病理注释数据有限、模态领域差距以及肝硬化等弥漫性病理的复杂性。现有方法难以在准确建模病理特征的同时保持解剖保真度，且常依赖自然图像先验或低效的多步采样。", "method": "本文引入了ViCTr（Vital Consistency Transfer），一个新颖的两阶段框架。首先，使用弹性权重整合（EWC）在ATLAS-8k数据集上预训练ViCTr，以保留关键解剖结构。然后，利用低秩适应（LoRA）模块进行对抗性微调，以精确控制病理严重程度。通过在线性轨迹框架内重新制定Tweedie公式，ViCTr支持一步采样，将推理步数从50步减少到仅4步，同时不牺牲解剖真实性。", "result": "ViCTr在BTCV（CT）、AMOS（MRI）和CirrMRI600+（肝硬化）数据集上进行了评估，表现出最先进的性能。对于肝硬化合成，其医学Frechet Inception距离（MFID）为17.01，比现有方法低28%。当用于数据增强时，nnUNet分割性能提高了+3.8% mDSC。放射科医生评估表明，ViCTr生成的肝硬化MRI在临床上与真实扫描无法区分。", "conclusion": "ViCTr是第一个提供具有分级严重度控制的精细、病理感知MRI合成的方法，弥补了AI驱动医学成像研究中的一个关键空白。", "translation": "由于带注释的病理数据有限、模态领域差距以及肝硬化等弥漫性病理的复杂性，合成医学图像仍然充满挑战。现有方法在准确建模病理特征的同时往往难以保持解剖保真度，并且经常依赖于来自自然图像的先验或低效的多步采样。在这项工作中，我们引入了ViCTr（Vital Consistency Transfer），一个新颖的两阶段框架，它结合了校正流轨迹和Tweedie校正扩散过程，以实现高保真、病理感知的图像合成。首先，我们使用弹性权重整合（EWC）在ATLAS-8k数据集上预训练ViCTr，以保留关键解剖结构。然后，我们使用低秩适应（LoRA）模块对模型进行对抗性微调，以精确控制病理严重程度。通过在线性轨迹框架内重新制定Tweedie公式，ViCTr支持一步采样，将推理步数从50步减少到仅4步，而不会牺牲解剖真实性。我们在BTCV（CT）、AMOS（MRI）和CirrMRI600+（肝硬化）数据集上评估了ViCTr。结果表明其性能达到最先进水平，对于肝硬化合成，其医学Frechet Inception距离（MFID）为17.01，比现有方法低28%，并且当用于数据增强时，nnUNet分割性能提高了+3.8% mDSC。放射科医生评估表明，ViCTr生成的肝硬化MRI在临床上与真实扫描无法区分。据我们所知，ViCTr是第一个提供具有分级严重度控制的精细、病理感知MRI合成的方法，弥补了AI驱动医学成像研究中的一个关键空白。", "summary": "本文提出ViCTr，一个用于高保真、病理感知医学图像合成的两阶段框架。针对医学图像数据稀缺和现有方法不足，ViCTr结合校正流和Tweedie校正扩散过程，并利用EWC预训练以保留解剖结构，LoRA微调以控制病理严重度。该模型支持一步采样，显著提高推理效率。实验结果表明，ViCTr在多个医学数据集上实现了最先进的性能，生成的病理图像在临床上与真实图像无法区分，有效弥补了医学影像合成领域的空白。", "keywords": "医学图像合成, 病理感知, 扩散模型, 校正流, 一步采样", "comments": "ViCTr的创新性在于其两阶段框架设计，巧妙结合了校正流轨迹和Tweedie校正扩散过程，实现了高保真和病理感知的图像合成。特别值得一提的是，它通过在线性轨迹框架内重新制定Tweedie公式，实现了革命性的一步采样，将推理时间从50步大幅缩减至4步，极大提升了效率。此外，利用EWC和LoRA模块分别处理解剖结构保留和病理严重度控制，展现了精细化的模型控制能力。其结果在MFID上显著优于现有方法，并且通过放射科医生的盲评证实了其临床实用性，这对于解决医学影像领域数据稀缺问题具有重要意义。"}}
{"id": "2507.18713", "title": "SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time", "authors": ["Yun Chen", "Matthew Haines", "Jingkang Wang", "Krzysztof Baron-Lis", "Sivabalan Manivasagam", "Ze Yang", "Raquel Urtasun"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18713v1", "summary": "High-fidelity sensor simulation of light-based sensors such as cameras and\nLiDARs is critical for safe and accurate autonomy testing. Neural radiance\nfield (NeRF)-based methods that reconstruct sensor observations via ray-casting\nof implicit representations have demonstrated accurate simulation of driving\nscenes, but are slow to train and render, hampering scale. 3D Gaussian\nSplatting (3DGS) has demonstrated faster training and rendering times through\nrasterization, but is primarily restricted to pinhole camera sensors,\npreventing usage for realistic multi-sensor autonomy evaluation. Moreover, both\nNeRF and 3DGS couple the representation with the rendering procedure (implicit\nnetworks for ray-based evaluation, particles for rasterization), preventing\ninteroperability, which is key for general usage. In this work, we present\nSparse Local Fields (SaLF), a novel volumetric representation that supports\nrasterization and raytracing. SaLF represents volumes as a sparse set of 3D\nvoxel primitives, where each voxel is a local implicit field. SaLF has fast\ntraining (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS\nLiDAR), has adaptive pruning and densification to easily handle large scenes,\nand can support non-pinhole cameras and spinning LiDARs. We demonstrate that\nSaLF has similar realism as existing self-driving sensor simulation methods\nwhile improving efficiency and enhancing capabilities, enabling more scalable\nsimulation. https://waabi.ai/salf/", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18713v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "SaLF：用于实时多传感器渲染的稀疏局部场", "tldr": "SaLF是一种新型的稀疏体积表示方法，支持光栅化和光线追踪，实现了高保真、实时的多传感器仿真，解决了现有方法在训练速度、渲染效率和传感器类型支持方面的局限性。", "motivation": "高保真传感器仿真对于自动驾驶测试至关重要。现有的基于NeRF的方法训练和渲染速度慢，难以扩展。3D Gaussian Splatting虽然更快，但主要限于针孔相机，且NeRF和3DGS都将表示与渲染过程耦合，缺乏互操作性，限制了通用性。", "method": "本文提出了稀疏局部场（SaLF），这是一种新颖的体积表示方法，支持光栅化和光线追踪。SaLF将体素表示为一组稀疏的3D体素基元，其中每个体素都是一个局部隐式场。", "result": "SaLF训练速度快（<30分钟），渲染能力强（相机50+ FPS，LiDAR 600+ FPS），具有自适应剪枝和稠密化功能，可轻松处理大型场景，并支持非针孔相机和旋转LiDAR。SaLF在保持与现有自动驾驶传感器仿真方法相似真实感的同时，提高了效率并增强了能力。", "conclusion": "SaLF通过提高效率和增强能力，实现了更具可扩展性的自动驾驶传感器仿真。", "translation": "高保真光基传感器（如相机和LiDAR）的传感器仿真对于安全准确的自动驾驶测试至关重要。基于神经辐射场（NeRF）的方法通过对隐式表示进行光线投射来重建传感器观测，已证明能够准确仿真驾驶场景，但训练和渲染速度慢，阻碍了规模化应用。3D Gaussian Splatting（3DGS）通过光栅化实现了更快的训练和渲染时间，但主要限于针孔相机传感器，无法用于真实的、多传感器的自动驾驶评估。此外，NeRF和3DGS都将表示与渲染过程耦合（隐式网络用于基于光线评估，粒子用于光栅化），阻碍了互操作性，而互操作性是通用性的关键。在这项工作中，我们提出了稀疏局部场（SaLF），这是一种支持光栅化和光线追踪的新型体积表示方法。SaLF将体积表示为一组稀疏的3D体素基元，其中每个体素都是一个局部隐式场。SaLF具有快速训练（<30分钟）和渲染能力（相机50+ FPS，LiDAR 600+ FPS），具有自适应剪枝和稠密化功能，可轻松处理大型场景，并支持非针孔相机和旋转LiDAR。我们证明SaLF与现有的自动驾驶传感器仿真方法具有相似的真实感，同时提高了效率并增强了能力，从而实现了更具可扩展性的仿真。", "summary": "本文介绍了SaLF（稀疏局部场），这是一种创新的体积表示方法，旨在解决现有高保真传感器仿真方法（如NeRF和3DGS）在训练速度、渲染效率和传感器兼容性方面的局限性。SaLF通过将体积表示为稀疏的3D体素基元（每个基元都是局部隐式场），实现了对光栅化和光线追踪的同时支持。实验结果表明，SaLF具有极快的训练速度（<30分钟）和高帧率渲染能力（相机50+ FPS，LiDAR 600+ FPS），能够处理大型场景并支持多种传感器类型（包括非针孔相机和旋转LiDAR），同时保持了与现有方法相当的真实感，从而为自动驾驶测试提供了更高效、更具扩展性的仿真解决方案。", "keywords": "稀疏局部场, 多传感器渲染, 实时仿真, 自动驾驶, 神经辐射场", "comments": "SaLF的创新之处在于其提出的稀疏局部场表示，它巧妙地结合了体素基元和局部隐式场，并同时支持光栅化和光线追踪，这解决了NeRF和3DGS在互操作性和传感器类型支持方面的固有局限性。其在训练和渲染速度上的显著提升，以及对大型场景和多种传感器类型的支持，对于推动自动驾驶领域的仿真测试具有重要意义，能够加速开发和验证过程。"}}
{"id": "2507.19098", "title": "MedSymmFlow: Bridging Generative Modeling and Classification in Medical Imaging through Symmetrical Flow Matching", "authors": ["Francisco Caetano", "Lemar Abdi", "Christiaan Viviers", "Amaan Valiuddin", "Fons van der Sommen"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      DGM4MICCAI 2025", "url": "http://arxiv.org/abs/2507.19098v1", "summary": "Reliable medical image classification requires accurate predictions and\nwell-calibrated uncertainty estimates, especially in high-stakes clinical\nsettings. This work presents MedSymmFlow, a generative-discriminative hybrid\nmodel built on Symmetrical Flow Matching, designed to unify classification,\ngeneration, and uncertainty quantification in medical imaging. MedSymmFlow\nleverages a latent-space formulation that scales to high-resolution inputs and\nintroduces a semantic mask conditioning mechanism to enhance diagnostic\nrelevance. Unlike standard discriminative models, it naturally estimates\nuncertainty through its generative sampling process. The model is evaluated on\nfour MedMNIST datasets, covering a range of modalities and pathologies. The\nresults show that MedSymmFlow matches or exceeds the performance of established\nbaselines in classification accuracy and AUC, while also delivering reliable\nuncertainty estimates validated by performance improvements under selective\nprediction.", "comment": "DGM4MICCAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.19098v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "MedSymmFlow：通过对称流匹配桥接医学图像中的生成建模与分类", "tldr": "MedSymmFlow是一个基于对称流匹配的生成-判别混合模型，旨在统一医学图像的分类、生成和不确定性量化，并在分类准确率和不确定性估计方面表现出色。", "motivation": "在临床环境中，可靠的医学图像分类需要准确的预测和校准良好的不确定性估计。", "method": "MedSymmFlow是一个基于对称流匹配的生成-判别混合模型，它利用潜在空间公式处理高分辨率输入，并引入语义掩码条件机制以增强诊断相关性。它通过其生成采样过程自然地估计不确定性。", "result": "在四个MedMNIST数据集上，MedSymmFlow的分类准确率和AUC与现有基线模型持平或超越，同时通过选择性预测下的性能改进提供了可靠的不确定性估计。", "conclusion": "MedSymmFlow成功地将生成建模与分类相结合，为医学图像分析提供了高准确性和可靠不确定性估计的统一框架。", "translation": "可靠的医学图像分类需要准确的预测和校准良好的不确定性估计，尤其是在高风险的临床环境中。这项工作提出了MedSymmFlow，一个基于对称流匹配构建的生成-判别混合模型，旨在统一医学图像中的分类、生成和不确定性量化。MedSymmFlow利用潜在空间公式，可扩展到高分辨率输入，并引入语义掩码条件机制以增强诊断相关性。与标准判别模型不同，它通过其生成采样过程自然地估计不确定性。该模型在四个MedMNIST数据集上进行了评估，涵盖了多种模态和病理。结果表明，MedSymmFlow在分类准确率和AUC方面与现有基线模型持平或超越，同时还通过选择性预测下的性能改进提供了可靠的不确定性估计。", "summary": "MedSymmFlow是一个基于对称流匹配的生成-判别混合模型，旨在解决医学图像分类中对准确预测和不确定性估计的需求。它通过潜在空间和语义掩码条件机制处理高分辨率图像，并利用生成采样过程自然地量化不确定性。实验结果表明，MedSymmFlow在分类性能上与现有模型相当或更优，并能提供可靠的不确定性估计。", "keywords": "医学图像分类, 生成模型, 流匹配, 不确定性量化, MedSymmFlow", "comments": "MedSymmFlow的创新之处在于其对称流匹配的生成-判别混合架构，成功地将医学图像的分类、生成和不确定性量化统一在一个框架内，这对于高风险的临床应用具有重要意义。其处理高分辨率输入和自然估计不确定性的能力是其主要优势。"}}
{"id": "2507.18700", "title": "Adaptive Neural Quantum States: A Recurrent Neural Network Perspective", "authors": ["Jake McNaughton", "Mohamed Hibat-Allah"], "categories": ["cond-mat.dis-nn", "cond-mat.str-el", "cs.LG", "physics.comp-ph", "quant-ph"], "primary_category": "Subjects:       Disordered Systems and Neural Networks (cond-mat.dis-nn)", "pdf_link": null, "comments": "Comments:      14 pages, 7 figures, 3 tables. Link to GitHub repository: this https URL", "url": "http://arxiv.org/abs/2507.18700v1", "summary": "Neural-network quantum states (NQS) are powerful neural-network ans\\\"atzes\nthat have emerged as promising tools for studying quantum many-body physics\nthrough the lens of the variational principle. These architectures are known to\nbe systematically improvable by increasing the number of parameters. Here we\ndemonstrate an Adaptive scheme to optimize NQSs, through the example of\nrecurrent neural networks (RNN), using a fraction of the computation cost while\nreducing training fluctuations and improving the quality of variational\ncalculations targeting ground states of prototypical models in one- and\ntwo-spatial dimensions. This Adaptive technique reduces the computational cost\nthrough training small RNNs and reusing them to initialize larger RNNs. This\nwork opens up the possibility for optimizing graphical processing unit (GPU)\nresources deployed in large-scale NQS simulations.", "comment": "14 pages, 7 figures, 3 tables. Link to GitHub repository:\n  https://github.com/jakemcnaughton/AdaptiveRNNWaveFunctions/", "pdf_url": "http://arxiv.org/pdf/2507.18700v1", "cate": "cond-mat.dis-nn", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "自适应神经量子态：循环神经网络视角", "tldr": "本文提出一种自适应方案优化神经量子态（NQS），通过训练小型循环神经网络（RNN）并重用其参数初始化大型RNN，显著降低了计算成本，减少了训练波动，并提高了变分计算质量。", "motivation": "神经网络量子态（NQS）是研究量子多体物理的强大工具，其性能可通过增加参数量系统性提升。然而，大规模NQS模拟计算成本高昂，需要更高效的优化方法。", "method": "本文提出了一种自适应方案来优化神经网络量子态（NQS），以循环神经网络（RNN）为例。该技术通过训练小型RNN并重用其参数来初始化更大的RNN，从而降低计算成本。", "result": "该自适应技术显著降低了计算成本，减少了训练波动，并提高了针对一维和二维原型模型基态的变分计算质量。", "conclusion": "这项工作为优化大规模神经网络量子态（NQS）模拟中图形处理单元（GPU）资源的使用开辟了可能性。", "translation": "神经网络量子态（NQS）是一种强大的神经网络Ansätze，已成为通过变分原理研究量子多体物理的有前景的工具。这些架构已知可以通过增加参数数量来系统地改进。本文以循环神经网络（RNN）为例，展示了一种优化NQS的自适应方案，该方案在降低计算成本的同时，减少了训练波动，并提高了针对一维和二维原型模型基态的变分计算质量。这种自适应技术通过训练小型RNN并重用它们来初始化大型RNN，从而降低了计算成本。这项工作为优化大规模NQS模拟中部署的图形处理单元（GPU）资源开辟了可能性。", "summary": "本文提出了一种名为“自适应”的新方案，用于优化神经网络量子态（NQS），并以循环神经网络（RNN）为例进行演示。该方法通过训练小型RNN并将其参数用于初始化大型RNN，显著降低了计算成本，同时减少了训练波动并提升了变分计算的准确性，尤其适用于量子多体物理中的基态计算。这项工作为大规模NQS模拟中GPU资源的有效利用提供了新的途径。", "keywords": "神经量子态, 循环神经网络, 自适应优化, 量子多体物理, 计算效率", "comments": "该论文提出了一种创新的自适应训练策略，通过参数重用有效解决了神经量子态（NQS）规模化训练中的计算成本问题。这种方法不仅提高了计算效率，还提升了模型的稳定性和准确性，对于推动量子多体物理模拟具有重要意义。其对GPU资源优化的关注也具有很强的实践价值。"}}
{"id": "2507.19227", "title": "Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation", "authors": ["Yuanhe Zhang", "Fangzhou Xie", "Zhenhong Zhou", "Zherui Li", "Hao Chen", "Kun Wang", "Yufei Guo"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19227v1", "summary": "Large Language Diffusion Models (LLDMs) exhibit comparable performance to\nLLMs while offering distinct advantages in inference speed and mathematical\nreasoning tasks.The precise and rapid generation capabilities of LLDMs amplify\nconcerns of harmful generations, while existing jailbreak methodologies\ndesigned for Large Language Models (LLMs) prove limited effectiveness against\nLLDMs and fail to expose safety vulnerabilities.Successful defense cannot\ndefinitively resolve harmful generation concerns, as it remains unclear whether\nLLDMs possess safety robustness or existing attacks are incompatible with\ndiffusion-based architectures.To address this, we first reveal the\nvulnerability of LLDMs to jailbreak and demonstrate that attack failure in\nLLDMs stems from fundamental architectural differences.We present a PArallel\nDecoding jailbreak (PAD) for diffusion-based language models. PAD introduces\nMulti-Point Attention Attack, which guides parallel generative processes toward\nharmful outputs that inspired by affirmative response patterns in LLMs.\nExperimental evaluations across four LLDMs demonstrate that PAD achieves\njailbreak attack success rates by 97%, revealing significant safety\nvulnerabilities. Furthermore, compared to autoregressive LLMs of the same size,\nLLDMs increase the harmful generation speed by 2x, significantly highlighting\nrisks of uncontrolled misuse.Through comprehensive analysis, we provide an\ninvestigation into LLDM architecture, offering critical insights for the secure\ndeployment of diffusion-based language models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19227v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "攻破大型语言扩散模型：揭示基于扩散的文本生成中的隐藏安全漏洞", "tldr": "本文揭示了大型语言扩散模型（LLDMs）的安全漏洞，并提出了一个名为PAD的越狱方法，该方法成功率高达97%，并发现LLDMs生成有害内容的速度比同等大小的自回归LLMs快2倍。", "motivation": "大型语言扩散模型（LLDMs）在推理速度和数学推理任务上表现出优势，但其快速生成能力也加剧了有害内容生成的担忧。现有的针对大型语言模型（LLMs）的越狱方法对LLDMs效果有限，未能揭示其安全漏洞。目前尚不清楚LLDMs是否具有安全鲁棒性，或现有攻击是否与扩散架构不兼容，因此需要深入研究以揭示其漏洞。", "method": "本文首先揭示了LLDMs的越狱漏洞，并指出攻击失败源于其基本的架构差异。研究提出了一种名为并行解码越狱（PAD）的方法，专为基于扩散的语言模型设计。PAD引入了多点注意力攻击，通过借鉴LLMs中肯定响应模式的启发，引导并行生成过程产生有害输出。", "result": "在对四种LLDMs进行的实验评估中，PAD实现了97%的越狱攻击成功率，揭示了显著的安全漏洞。此外，与相同大小的自回归LLMs相比，LLDMs生成有害内容的速度提高了2倍，显著强调了不受控制滥用的风险。", "conclusion": "本文通过全面的分析，深入研究了LLDM架构，为基于扩散的语言模型的安全部署提供了关键见解，并成功揭示了其显著的安全漏洞和快速生成有害内容的能力。", "translation": "大型语言扩散模型（LLDMs）表现出与LLMs相当的性能，同时在推理速度和数学推理任务方面具有明显的优势。LLDMs精确和快速的生成能力加剧了有害内容生成的担忧，而现有为大型语言模型（LLMs）设计的越狱方法对LLDMs的有效性有限，未能暴露安全漏洞。成功的防御不能彻底解决有害内容生成问题，因为目前尚不清楚LLDMs是否具有安全鲁棒性，或者现有攻击是否与基于扩散的架构不兼容。为了解决这个问题，我们首先揭示了LLDMs的漏洞，并证明LLDMs中攻击失败源于基本的架构差异。我们提出了一种用于基于扩散的语言模型的并行解码越狱（PAD）方法。PAD引入了多点注意力攻击，通过受LLMs中肯定响应模式的启发，引导并行生成过程产生有害输出。对四种LLDMs的实验评估表明，PAD的越狱攻击成功率达到97%，揭示了显著的安全漏洞。此外，与相同大小的自回归LLMs相比，LLDMs生成有害内容的速度提高了2倍，显著突出了不受控制滥用的风险。通过全面分析，我们对LLDM架构进行了调查，为基于扩散的语言模型的安全部署提供了关键见解。", "summary": "本文研究了大型语言扩散模型（LLDMs）的安全漏洞，指出现有越狱方法对LLDMs无效的原因在于其独特的架构。为此，论文提出了一种新的并行解码越狱（PAD）方法，该方法通过多点注意力攻击引导LLDMs生成有害内容。实验结果显示，PAD对四种LLDMs的越狱成功率高达97%，并发现LLDMs生成有害内容的速度比同等大小的自回归LLMs快2倍，揭示了LLDMs在安全部署方面的重大风险。", "keywords": "大型语言扩散模型, 越狱, 安全漏洞, 并行解码, 有害生成", "comments": "本文创新性地提出了针对大型语言扩散模型（LLDMs）的越狱方法PAD，填补了现有越狱技术对LLDMs有效性不足的空白。其发现LLDMs在生成有害内容方面的高效率和高成功率，对未来LLDMs的安全部署和防御机制的开发具有重要指导意义，揭示了扩散模型在安全性方面潜在的巨大风险。"}}
{"id": "2504.09567", "title": "From Conditional to Unconditional Independence: Testing Conditional Independence via Transport Maps", "authors": ["Chenxuan He", "Yuan Gao", "Liping Zhu", "Jian Huang"], "categories": ["stat.ML", "cs.LG", "stat.ME", "62G05, 62G08, 68T07"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "Comments:      41 pages", "url": "http://arxiv.org/abs/2504.09567v3", "summary": "Testing conditional independence between two random vectors given a third is\na fundamental and challenging problem in statistics, particularly in\nmultivariate nonparametric settings due to the complexity of conditional\nstructures. We propose a novel method for testing conditional independence by\ntransforming it to an unconditional independence test problem. We achieve this\nby constructing two transport maps that transform conditional independence into\nunconditional independence, this substantially simplifies the problem. These\ntransport maps are estimated from data using conditional continuous normalizing\nflow models. Within this framework, we derive a test statistic and prove its\nasymptotic validity under both the null and alternative hypotheses. A\npermutation-based procedure is employed to evaluate the significance of the\ntest. We validate the proposed method through extensive simulations and\nreal-data analysis. Our numerical studies demonstrate the practical\neffectiveness of the proposed method for conditional independence", "comment": "41 pages", "pdf_url": "http://arxiv.org/pdf/2504.09567v3", "cate": "stat.ML", "date": "2025-04-13", "updated": "2025-07-25", "AI": {"title_translation": "从条件独立到无条件独立：通过传输映射检验条件独立性", "tldr": "本文提出了一种新颖的方法，通过将条件独立性检验转化为无条件独立性检验来解决，该方法利用传输映射并通过条件连续归一化流模型进行估计，并在模拟和实际数据中验证了其有效性。", "motivation": "在统计学中，检验两个随机向量在给定第三个向量情况下的条件独立性是一个基础且具有挑战性的问题，特别是在多元非参数设置中，由于条件结构的复杂性，这个问题尤为困难。", "method": "本文提出了一种新颖的方法，通过构建两个传输映射将条件独立性检验转化为无条件独立性检验问题。这些传输映射是利用条件连续归一化流模型从数据中估计得到的。在此框架下，推导了一个检验统计量并证明了其在零假设和备择假设下的渐近有效性。同时，采用基于置换的程序来评估检验的显著性。", "result": "通过广泛的模拟和实际数据分析验证了所提出的方法。数值研究表明，该方法在条件独立性检验方面具有实际有效性。", "conclusion": "所提出的通过传输映射将条件独立性转化为无条件独立性检验的方法在实践中是有效的。", "translation": "检验给定第三个随机向量时，两个随机向量之间的条件独立性是统计学中一个基础且具有挑战性的问题，尤其是在多元非参数设置中，由于条件结构的复杂性而更显困难。我们提出了一种新颖的方法，通过将其转化为无条件独立性检验问题来检验条件独立性。我们通过构建两个传输映射来实现这一点，这些映射将条件独立性转化为无条件独立性，这大大简化了问题。这些传输映射是使用条件连续归一化流模型从数据中估计的。在此框架内，我们推导了一个检验统计量，并证明了其在零假设和备择假设下的渐近有效性。采用基于置换的程序来评估检验的显著性。我们通过广泛的模拟和实际数据分析验证了所提出的方法。我们的数值研究表明，所提出的条件独立性方法具有实际有效性。", "summary": "本文提出了一种新颖的条件独立性检验方法，通过构建传输映射将其转化为简化的无条件独立性检验问题。这些传输映射利用条件连续归一化流模型从数据中估计。该方法推导了一个渐近有效的检验统计量，并采用置换程序进行显著性评估。广泛的模拟和实际数据分析验证了该方法在条件独立性检验中的实际有效性。", "keywords": "条件独立性, 传输映射, 无条件独立性, 归一化流", "comments": "这项工作通过将复杂的条件独立性检验问题转化为更简单的无条件独立性检验，展示了创新性。利用传输映射和条件连续归一化流模型是其方法学上的亮点，提供了一种处理多元非参数设置中条件结构复杂性的有效途径。其理论推导和通过模拟及实际数据验证进一步增强了该方法的可靠性和实用性。"}}
{"id": "2507.18811", "title": "Even Faster Simulations with Flow Matching: A Study of Zero Degree Calorimeter Responses", "authors": ["Maksymilian Wojnar"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18811v1", "summary": "Recent advances in generative neural networks, particularly flow matching\n(FM), have enabled the generation of high-fidelity samples while significantly\nreducing computational costs. A promising application of these models is\naccelerating simulations in high-energy physics (HEP), helping research\ninstitutions meet their increasing computational demands. In this work, we\nleverage FM to develop surrogate models for fast simulations of zero degree\ncalorimeters in the ALICE experiment. We present an effective training strategy\nthat enables the training of fast generative models with an exceptionally low\nnumber of parameters. This approach achieves state-of-the-art simulation\nfidelity for both neutron (ZN) and proton (ZP) detectors, while offering\nsubstantial reductions in computational costs compared to existing methods. Our\nFM model achieves a Wasserstein distance of 1.27 for the ZN simulation with an\ninference time of 0.46 ms per sample, compared to the current best of 1.20 with\nan inference time of approximately 109 ms. The latent FM model further improves\nthe inference speed, reducing the sampling time to 0.026 ms per sample, with a\nminimal trade-off in accuracy. Similarly, our approach achieves a Wasserstein\ndistance of 1.30 for the ZP simulation, outperforming the current best of 2.08.\nThe source code is available at https://github.com/m-wojnar/faster_zdc.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18811v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "使用流匹配实现更快模拟：零度量能器响应研究", "tldr": "该研究利用流匹配技术显著加速了高能物理中零度量能器的模拟，同时保持高精度并大幅降低计算成本。", "motivation": "高能物理（HEP）研究机构面临日益增长的计算需求，需要加速模拟过程。", "method": "利用流匹配（FM）开发替代模型，用于ALICE实验中零度量能器的快速模拟。提出了一种有效的训练策略，能够以极少的参数训练出快速生成模型。", "result": "对于中子（ZN）和质子（ZP）探测器，实现了最先进的模拟保真度，并大幅降低了计算成本。ZN模拟的Wasserstein距离为1.27，推理时间为0.46毫秒/样本（现有最佳为1.20，109毫秒）。潜在FM模型将采样时间进一步缩短至0.026毫秒/样本。ZP模拟的Wasserstein距离为1.30（现有最佳为2.08）。", "conclusion": "流匹配模型能够以极高的效率和精度加速高能物理模拟，显著优于现有方法，为满足日益增长的计算需求提供了有效解决方案。", "translation": "生成神经网络，特别是流匹配（FM）的最新进展，使得生成高保真样本成为可能，同时显著降低了计算成本。这些模型的一个有前景的应用是加速高能物理（HEP）中的模拟，帮助研究机构满足其日益增长的计算需求。在这项工作中，我们利用FM开发了ALICE实验中零度量能器快速模拟的替代模型。我们提出了一种有效的训练策略，能够以极少的参数训练出快速生成模型。这种方法在中子（ZN）和质子（ZP）探测器上都实现了最先进的模拟保真度，同时与现有方法相比，大幅降低了计算成本。我们的FM模型在ZN模拟中实现了1.27的Wasserstein距离，推理时间为每个样本0.46毫秒，而目前最佳方法的Wasserstein距离为1.20，推理时间约为109毫秒。潜在FM模型进一步提高了推理速度，将采样时间缩短至每个样本0.026毫秒，而精度损失极小。同样，我们的方法在ZP模拟中实现了1.30的Wasserstein距离，优于目前最佳的2.08。源代码可在https://github.com/m-wojnar/faster_zdc 获取。", "summary": "本研究利用流匹配（FM）技术，为ALICE实验中的零度量能器开发了快速模拟替代模型，以应对高能物理日益增长的计算需求。通过提出一种参数量极低的有效训练策略，该方法在中子和质子探测器模拟中均达到了最先进的保真度，并大幅降低了计算成本。实验结果显示，FM模型在ZN模拟中将推理时间从109毫秒显著缩短至0.46毫秒（潜在FM模型甚至达到0.026毫秒），同时Wasserstein距离保持在较低水平；在ZP模拟中性能也优于现有最佳方法。", "keywords": "流匹配, 高能物理, 零度量能器, 快速模拟, 生成模型", "comments": "该论文创新性地将流匹配技术应用于高能物理模拟，解决了计算效率低下的痛点。其主要亮点在于在大幅提升模拟速度的同时，仍能保持甚至超越现有方法的精度。特别是潜在FM模型的推理速度提升令人瞩目，这将对高能物理数据处理产生深远影响。该方法的普适性可能使其在其他需要高效率模拟的科学领域也具有应用潜力。"}}
{"id": "2507.17275", "title": "Prolonging Tool Life: Learning Skillful Use of General-purpose Tools through Lifespan-guided Reinforcement Learning", "authors": ["Po-Yen Wu", "Cheng-Yu Kuo", "Yuki Kadokawa", "Takamitsu Matsubara"], "categories": ["cs.RO", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Under review", "url": "http://arxiv.org/abs/2507.17275v2", "summary": "In inaccessible environments with uncertain task demands, robots often rely\non general-purpose tools that lack predefined usage strategies. These tools are\nnot tailored for particular operations, making their longevity highly sensitive\nto how they are used. This creates a fundamental challenge: how can a robot\nlearn a tool-use policy that both completes the task and prolongs the tool's\nlifespan? In this work, we address this challenge by introducing a\nreinforcement learning (RL) framework that incorporates tool lifespan as a\nfactor during policy optimization. Our framework leverages Finite Element\nAnalysis (FEA) and Miner's Rule to estimate Remaining Useful Life (RUL) based\non accumulated stress, and integrates the RUL into the RL reward to guide\npolicy learning toward lifespan-guided behavior. To handle the fact that RUL\ncan only be estimated after task execution, we introduce an Adaptive Reward\nNormalization (ARN) mechanism that dynamically adjusts reward scaling based on\nestimated RULs, ensuring stable learning signals. We validate our method across\nsimulated and real-world tool use tasks, including Object-Moving and\nDoor-Opening with multiple general-purpose tools. The learned policies\nconsistently prolong tool lifespan (up to 8.01x in simulation) and transfer\neffectively to real-world settings, demonstrating the practical value of\nlearning lifespan-guided tool use strategies.", "comment": "Under review", "pdf_url": "http://arxiv.org/pdf/2507.17275v2", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-25", "AI": {"title_translation": "延长工具寿命：通过寿命引导的强化学习学习通用工具的熟练使用", "tldr": "本文提出了一种强化学习框架，通过将工具寿命作为策略优化因素，使机器人能够学习既能完成任务又能延长通用工具寿命的使用策略，并在模拟和现实世界中验证了其有效性。", "motivation": "在任务需求不确定的难以接近的环境中，机器人依赖通用工具，但这些工具没有预定义的使用策略，其寿命对使用方式高度敏感。核心挑战在于：机器人如何学习既能完成任务又能延长工具寿命的工具使用策略。", "method": "引入了一个强化学习（RL）框架，将工具寿命作为策略优化因素。该框架利用有限元分析（FEA）和Miner's Rule来估算基于累积应力的剩余使用寿命（RUL），并将RUL整合到RL奖励中以引导策略学习。为解决RUL只能在任务执行后估算的问题，引入了自适应奖励归一化（ARN）机制，根据估算的RUL动态调整奖励尺度，确保稳定的学习信号。", "result": "该方法在模拟和现实世界的工具使用任务（包括物体移动和开门）中得到了验证。学习到的策略持续延长了工具寿命（模拟中最高达8.01倍），并有效地迁移到现实世界设置中。", "conclusion": "通过将工具寿命纳入强化学习奖励机制，并辅以自适应奖励归一化，机器人能够学习到既能高效完成任务又能显著延长通用工具寿命的使用策略，这在实际应用中具有重要价值。", "translation": "在任务需求不确定的难以接近的环境中，机器人通常依赖缺乏预定义使用策略的通用工具。这些工具并非为特定操作量身定制，因此其寿命对其使用方式高度敏感。这带来了一个根本性挑战：机器人如何学习一种既能完成任务又能延长工具寿命的工具使用策略？在这项工作中，我们通过引入一个将工具寿命作为策略优化因素的强化学习（RL）框架来解决这一挑战。我们的框架利用有限元分析（FEA）和Miner's Rule来根据累积应力估算剩余使用寿命（RUL），并将RUL整合到RL奖励中，以引导策略学习朝向寿命引导的行为。为了处理RUL只能在任务执行后估算的事实，我们引入了一种自适应奖励归一化（ARN）机制，该机制根据估算的RUL动态调整奖励尺度，确保稳定的学习信号。我们通过模拟和现实世界的工具使用任务（包括物体移动和使用多种通用工具开门）验证了我们的方法。学习到的策略持续延长了工具寿命（模拟中最高达8.01倍），并有效地迁移到现实世界设置中，展示了学习寿命引导工具使用策略的实际价值。", "summary": "本文提出了一种新颖的强化学习框架，旨在解决机器人在不确定环境中有效使用通用工具并同时延长其寿命的挑战。该框架通过结合有限元分析和Miner's Rule来估算工具的剩余使用寿命（RUL），并将其作为奖励信号整合到RL训练中。为应对RUL后验性估算的挑战，引入了自适应奖励归一化机制以稳定学习过程。实验结果表明，该方法在模拟和现实世界任务中显著延长了工具寿命，并展示了其在实际应用中的有效性。", "keywords": "强化学习, 工具寿命, 通用工具, 剩余使用寿命, 自适应奖励归一化", "comments": "该论文的创新点在于将工具寿命这一实际且重要的物理约束引入到强化学习的奖励设计中，解决了通用工具在机器人操作中寿命敏感性的问题。通过结合工程领域的FEA和Miner's Rule来量化工具损耗，并设计了ARN机制来处理RUL的后验性，使得学习过程更加稳定有效。这项工作为机器人长期自主操作和资源管理提供了新的视角和实用方法，具有重要的工程应用价值。"}}
{"id": "2507.19140", "title": "Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot Segmentation", "authors": ["Tianyu Zou", "Shengwu Xiong", "Ruilin Yao", "Yi Rong"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures", "url": "http://arxiv.org/abs/2507.19140v1", "summary": "This paper studies the few-shot segmentation (FSS) task, which aims to\nsegment objects belonging to unseen categories in a query image by learning a\nmodel on a small number of well-annotated support samples. Our analysis of two\nmainstream FSS paradigms reveals that the predictions made by prototype\nlearning methods are usually conservative, while those of affinity learning\nmethods tend to be more aggressive. This observation motivates us to balance\nthe conservative and aggressive information captured by these two types of FSS\nframeworks so as to improve the segmentation performance. To achieve this, we\npropose a **P**rototype-**A**ffinity **H**ybrid **Net**work (PAHNet), which\nintroduces a Prototype-guided Feature Enhancement (PFE) module and an Attention\nScore Calibration (ASC) module in each attention block of an affinity learning\nmodel (called affinity learner). These two modules utilize the predictions\ngenerated by a pre-trained prototype learning model (called prototype\npredictor) to enhance the foreground information in support and query image\nrepresentations and suppress the mismatched foreground-background (FG-BG)\nrelationships between them, respectively. In this way, the aggressiveness of\nthe affinity learner can be effectively mitigated, thereby eventually\nincreasing the segmentation accuracy of our PAHNet method. Experimental results\nshow that PAHNet outperforms most recently proposed methods across 1-shot and\n5-shot settings on both PASCAL-5$^i$ and COCO-20$^i$ datasets, suggesting its\neffectiveness. The code is available at: [GitHub - tianyu-zou/PAHNet: Balancing\nConservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot\nSegmentation (ICCV'25)](https://github.com/tianyu-zou/PAHNet)", "comment": "8 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.19140v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "平衡保守性与激进性：用于小样本分割的原型-亲和混合网络", "tldr": "本文研究小样本分割任务，发现原型学习方法保守，亲和学习方法激进。为平衡二者，提出PAHNet，通过引入PFE和ASC模块，有效缓解了亲和学习的激进性，提高了分割精度。", "motivation": "现有小样本分割（FSS）范式中，原型学习方法预测通常保守，而亲和学习方法预测倾向激进。为了提高分割性能，需要平衡这两种FSS框架所捕获的保守和激进信息。", "method": "提出原型-亲和混合网络（PAHNet），在亲和学习模型的每个注意力块中引入原型引导特征增强（PFE）模块和注意力分数校准（ASC）模块。PFE模块利用预训练原型学习模型（原型预测器）的预测来增强支持和查询图像表示中的前景信息，ASC模块则抑制它们之间不匹配的前景-背景（FG-BG）关系，从而有效缓解亲和学习器的激进性。", "result": "实验结果表明，PAHNet在PASCAL-5$^i$和COCO-20$^i$数据集上的1-shot和5-shot设置中，均优于大多数最新提出的方法。", "conclusion": "PAHNet通过平衡原型学习的保守性和亲和学习的激进性，有效提高了小样本分割的准确性。", "translation": "本文研究小样本分割（FSS）任务，旨在通过在少量标注良好的支持样本上学习模型，来分割查询图像中属于未见类别的对象。我们对两种主流FSS范式的分析表明，原型学习方法的预测通常保守，而亲和学习方法的预测倾向于更激进。这一观察促使我们平衡这两种FSS框架所捕获的保守和激进信息，以提高分割性能。为此，我们提出了一种原型-亲和混合网络（PAHNet），它在亲和学习模型（称为亲和学习器）的每个注意力块中引入了原型引导特征增强（PFE）模块和注意力分数校准（ASC）模块。这两个模块分别利用预训练原型学习模型（称为原型预测器）生成的预测来增强支持和查询图像表示中的前景信息，并抑制它们之间不匹配的前景-背景（FG-BG）关系。通过这种方式，亲和学习器的激进性可以被有效缓解，从而最终提高我们PAHNet方法的分割精度。实验结果表明，PAHNet在PASCAL-5$^i$和COCO-20$^i$数据集上的1-shot和5-shot设置中，均优于大多数最新提出的方法，表明了其有效性。代码可在以下地址获取：[GitHub - tianyu-zou/PAHNet: Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot Segmentation (ICCV'25)]", "summary": "本文针对小样本分割任务，分析了原型学习和亲和学习两种主流方法的特点，发现前者预测保守，后者预测激进。为解决这一问题并提高分割性能，作者提出了一种原型-亲和混合网络（PAHNet）。该网络在亲和学习模型中集成了原型引导特征增强（PFE）和注意力分数校准（ASC）模块，利用预训练原型学习模型的预测来增强前景信息并纠正前景-背景关系，从而有效缓解亲和学习的激进性。实验证明，PAHNet在多个标准数据集和设置下均展现出超越现有方法的优越性能。", "keywords": "小样本分割, 原型学习, 亲和学习, 混合网络, 注意力机制", "comments": "该论文创新性地指出了小样本分割中原型学习和亲和学习方法的固有缺陷（保守与激进），并提出了一种有效的混合网络PAHNet来弥补这些缺陷。通过PFE和ASC模块的巧妙设计，实现了对两种方法优势的结合，为小样本分割领域提供了一个新颖且高性能的解决方案。其方法论清晰，实验结果具有说服力。"}}
{"id": "2505.11013", "title": "Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion", "authors": ["Zongye Zhang", "Bohan Kong", "Qingjie Liu", "Yunhong Wang"], "categories": ["cs.CV", "cs.MM", "I.3.8"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2505.11013v2", "summary": "Generating 3D human motion from text descriptions remains challenging due to\nthe diverse and complex nature of human motion. While existing methods excel\nwithin the training distribution, they often struggle with out-of-distribution\nmotions, limiting their applicability in real-world scenarios. Existing\nVQVAE-based methods often fail to represent novel motions faithfully using\ndiscrete tokens, which hampers their ability to generalize beyond seen data.\nMeanwhile, diffusion-based methods operating on continuous representations\noften lack fine-grained control over individual frames. To address these\nchallenges, we propose a robust motion generation framework MoMADiff, which\ncombines masked modeling with diffusion processes to generate motion using\nframe-level continuous representations. Our model supports flexible\nuser-provided keyframe specification, enabling precise control over both\nspatial and temporal aspects of motion synthesis. MoMADiff demonstrates strong\ngeneralization capability on novel text-to-motion datasets with sparse\nkeyframes as motion prompts. Extensive experiments on two held-out datasets and\ntwo standard benchmarks show that our method consistently outperforms\nstate-of-the-art models in motion quality, instruction fidelity, and keyframe\nadherence. The code is available at: https://github.com/zzysteve/MoMADiff", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2505.11013v2", "cate": "cs.CV", "date": "2025-05-16", "updated": "2025-07-25", "AI": {"title_translation": "迈向通过掩码自回归扩散实现鲁棒且可控的文本到动作生成", "tldr": "现有文本到动作生成方法在泛化性和控制性方面存在不足。本文提出了MoMADiff，一个结合掩码建模和扩散过程的模型，能够生成鲁棒且可控的动作，并在性能上超越了现有最先进模型。", "motivation": "由于人体动作的多样性和复杂性，从文本描述生成3D人体动作仍然具有挑战性。现有方法（基于VQVAE的方法）难以处理分布外动作，且无法忠实表示新颖动作，泛化能力受限。同时，基于扩散的方法缺乏对单个帧的细粒度控制。", "method": "本文提出了MoMADiff，一个鲁棒的动作生成框架。它将掩码建模与扩散过程相结合，使用帧级连续表示来生成动作。MoMADiff支持灵活的用户提供关键帧规范，从而能够精确控制动作合成的空间和时间方面。", "result": "MoMADiff在以稀疏关键帧作为动作提示的新颖文本到动作数据集中展示了强大的泛化能力。在两个独立数据集和两个标准基准上的大量实验表明，MoMADiff在动作质量、指令忠实度和关键帧依从性方面始终优于最先进的模型。", "conclusion": "MoMADiff为文本到动作生成提供了一个鲁棒且可控的解决方案，解决了现有方法的局限性，并展示了卓越的性能和泛化能力。", "translation": "由于人体动作的多样性和复杂性，从文本描述生成3D人体动作仍然具有挑战性。尽管现有方法在训练分布内表现出色，但它们通常难以处理分布外动作，限制了其在现实世界场景中的适用性。现有的基于VQVAE的方法通常无法使用离散token忠实地表示新颖动作，这阻碍了它们泛化到未见过数据的能力。同时，基于扩散的方法在连续表示上操作时，通常缺乏对单个帧的细粒度控制。为了解决这些挑战，我们提出了一种鲁棒的动作生成框架MoMADiff，它将掩码建模与扩散过程相结合，使用帧级连续表示来生成动作。我们的模型支持灵活的用户提供关键帧规范，从而能够精确控制动作合成的空间和时间方面。MoMADiff在以稀疏关键帧作为动作提示的新颖文本到动作数据集中展示了强大的泛化能力。在两个独立数据集和两个标准基准上的大量实验表明，我们的方法在动作质量、指令忠实度和关键帧依从性方面始终优于最先进的模型。代码可在以下地址获取：https://github.com/zzysteve/MoMADiff", "summary": "本文介绍了MoMADiff，一个用于鲁棒且可控的文本到3D人体动作生成的新颖框架。它通过将掩码建模与扩散过程相结合，并利用帧级连续表示，解决了现有基于VQVAE和扩散方法的局限性。MoMADiff支持灵活的关键帧指定，提供了精确的空间和时间控制。实验表明，它实现了卓越的泛化能力，并在动作质量、指令忠实度和关键帧依从性方面优于最先进的模型。", "keywords": "文本到动作, 扩散模型, 掩码建模, 3D人体动作, 可控生成", "comments": "本文通过将掩码建模与扩散过程相结合，为文本到动作生成提供了一种创新方法，特别解决了现有方法在处理分布外泛化和细粒度控制方面的关键限制。对用户定义关键帧的支持显著增强了其实用性。"}}
{"id": "2507.18799", "title": "Fourth-Order Compact FDMs for Steady and Time-Dependent Nonlinear Convection-Diffusion Equations", "authors": ["Qiwei Feng", "Catalin Trenchea"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      22 pages and 6 figures", "url": "http://arxiv.org/abs/2507.18799v1", "summary": "In this paper, we discuss the steady and time-dependent nonlinear\nconvection-diffusion (advection-diffusion) equations with the Dirichlet\nboundary condition. For the steady nonlinear equation, we use an iteration\nmethod to reformulate the nonlinear equation into its linear counterpart, and\nderive a fourth-order compact 9-point finite difference method (FDM) to solve\nthe reformulated equation on a uniform Cartesian grid. To increase the\naccuracy, we modify the FDM to reduce the pollution effect. The linear system\nof the FDM generates an M-matrix, provided the mesh size $h$ is sufficiently\nsmall. For the time dependent nonlinear equation, we discrete the temporal\ndomain using the Crank-Nicolson (CN), BDF3, BDF4 time stepping methods, and\napply a similar iterative method to rewrite the nonlinear equation as the same\nlinear convection-diffusion equation. Then we propose the second-order to\nfourth-order compact 9-point FDMs with the reduced pollution effects on a\nuniform Cartesian grid. We prove that all FDMs satisfy the discrete maximum\nprinciple for sufficiently small $h$. Several examples with the variable and\ntime-dependent diffusion coefficients and challenging nonlinear terms (not\nlimited to the Burgers equation) are provided to verify the accuracy and the\ndesired convergence rates in the $l_2$ and $l_{\\infty}$ norms in space and\ntime. We also compare our second-order CN method with the third-order BDF3\nmethod and the discontinuous Galerkin (DG) method, and the numerical results\ndemonstrate that our FDM with the coarse time step generates the small error.\nEspecially, if the same BDF3 scheme is applied, our error is 1.6\\% of that\nobtained from the DG method. The proposed methods can be easily extended to a\n3D spatial domain and more general nonlinear convection-diffusion-reaction\nequations.", "comment": "22 pages and 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.18799v1", "cate": "math.NA", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "稳态和瞬态非线性对流-扩散方程的四阶紧致有限差分方法", "tldr": "本文提出并验证了用于求解稳态和瞬态非线性对流-扩散方程的四阶紧致有限差分方法，该方法通过迭代线性化和减少污染效应，实现了高精度和鲁棒性，并在数值上表现出优于现有方法的性能。", "motivation": "解决稳态和瞬态非线性对流-扩散方程的高精度数值求解问题，特别是处理其非线性和提高计算精度。", "method": "对于稳态方程，使用迭代法将其线性化，并推导了四阶紧致9点有限差分方法（FDM），并通过修改以减少污染效应。对于瞬态方程，采用Crank-Nicolson、BDF3、BDF4时间步进方法离散时间域，并应用类似的迭代法进行线性化，然后提出了二阶到四阶的紧致9点FDM，同样减少了污染效应。所提出的FDM在小网格尺寸下生成M-矩阵，并满足离散最大值原理。", "result": "所提出的方法在具有可变和时变扩散系数以及复杂非线性项的多个算例中验证了其准确性和期望的l2和l∞范数收敛率。数值结果表明，该FDM在粗时间步长下也能产生较小的误差。特别是，在使用相同的BDF3方案时，该方法的误差仅为间断伽辽金（DG）方法的1.6%。这些方法易于扩展到3D空间域和更一般的非线性对流-扩散-反应方程。", "conclusion": "本文成功开发并验证了针对稳态和瞬态非线性对流-扩散方程的高效、高精度四阶紧致有限差分方法，该方法在处理复杂非线性问题和精度方面表现出色，并具有良好的可扩展性。", "translation": "在本文中，我们讨论了具有Dirichlet边界条件的稳态和瞬态非线性对流-扩散（平流-扩散）方程。对于稳态非线性方程，我们使用迭代法将其重新表述为其线性对应方程，并推导了在均匀笛卡尔网格上求解该重新表述方程的四阶紧致9点有限差分方法（FDM）。为了提高精度，我们修改了FDM以减少污染效应。如果网格尺寸h足够小，FDM的线性系统会生成一个M-矩阵。对于瞬态非线性方程，我们使用Crank-Nicolson（CN）、BDF3、BDF4时间步进方法离散时间域，并应用类似的迭代法将非线性方程改写为相同的线性对流-扩散方程。然后，我们提出了在均匀笛卡尔网格上具有减小污染效应的二阶到四阶紧致9点FDM。我们证明了所有FDM在h足够小时都满足离散最大值原理。提供了几个具有可变和时变扩散系数以及挑战性非线性项（不限于Burgers方程）的例子来验证在空间和时间上的l2和l∞范数下的精度和期望收敛率。我们还将我们的二阶CN方法与三阶BDF3方法和间断伽辽金（DG）方法进行了比较，数值结果表明我们的FDM在粗时间步长下也能产生较小的误差。特别是，如果应用相同的BDF3方案，我们的误差是DG方法所得误差的1.6%。所提出的方法可以很容易地扩展到3D空间域和更一般的非线性对流-扩散-反应方程。", "summary": "本文针对稳态和瞬态非线性对流-扩散方程，提出了一系列高阶紧致有限差分方法。通过迭代线性化和引入减少污染效应的修正，实现了在均匀笛卡尔网格上的四阶空间精度。对于时间依赖问题，结合了Crank-Nicolson、BDF3和BDF4等时间步进方案。数值实验验证了这些方法在处理复杂非线性项和可变系数时的准确性和收敛性，并显示出在误差控制方面优于现有方法的显著优势。", "keywords": "非线性对流-扩散方程, 有限差分方法, 四阶精度, 迭代法, 污染效应减少", "comments": "本文的创新点在于结合迭代线性化技术和高阶紧致有限差分方法，有效地解决了非线性对流-扩散方程的求解难题。特别值得注意的是，通过修改FDM以减少污染效应，显著提高了精度。与DG方法的比较结果（1.6%的误差）突显了其在精度和效率上的重要优势。方法的普适性和可扩展性（到3D和更广义方程）也增加了其应用价值。"}}
{"id": "2507.18836", "title": "Uncertainty on Display: The Effects of Communicating Confidence Cues in Autonomous Vehicle-Pedestrian Interactions", "authors": ["Yue Luo", "Xinyan Yu", "Tram Thi Minh Tran", "Marius Hoggenmueller"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18836v1", "summary": "Uncertainty is an inherent aspect of autonomous vehicle (AV) decision-making,\nyet it is rarely communicated to pedestrians, which hinders transparency. This\nstudy investigates how AV uncertainty can be conveyed through two approaches:\nexplicit communication (confidence percentage displays) and implicit\ncommunication (vehicle motion cues), across different confidence levels (high\nand low). Through a within-subject VR experiment (N=26), we evaluated these\napproaches in a crossing scenario, assessing interface qualities (visibility\nand intuitiveness), how well the information conveyed the vehicle's level of\nconfidence, and their impact on participants' perceived safety, trust, and user\nexperience. Our results show that explicit communication is more effective and\npreferred for conveying uncertainty, enhancing safety, trust, and user\nexperience. Conversely, implicit communication introduces ambiguity, especially\nwhen AV confidence is low. This research provides empirical insights into how\nuncertainty communication shapes pedestrian interpretation of AV behaviour and\noffer design guidance for external interfaces that integrate uncertainty as a\ncommunicative element.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18836v1", "cate": "cs.HC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "不确定性展示：在自动驾驶汽车-行人交互中传达置信度线索的影响", "tldr": "研究发现，显式地（如百分比显示）向行人传达自动驾驶汽车的不确定性比隐式（如车辆运动）更有效，能增强安全感和信任。", "motivation": "自动驾驶汽车决策中固有的不确定性很少向行人传达，这阻碍了透明度。本研究旨在探讨如何有效传达这种不确定性。", "method": "通过一项被试内VR实验（N=26），在一个过马路场景中评估了两种不确定性传达方式（显式：置信度百分比显示；隐式：车辆运动线索）在不同置信度水平（高、低）下的效果。评估指标包括界面质量（可见性和直观性）、信息传达的置信度水平，以及对参与者感知安全、信任和用户体验的影响。", "result": "显式沟通在传达不确定性、增强安全、信任和用户体验方面更有效且更受欢迎。相反，隐式沟通会引入歧义，尤其是在自动驾驶汽车置信度低时。", "conclusion": "本研究提供了关于不确定性沟通如何影响行人对自动驾驶汽车行为解读的实证见解，并为将不确定性作为沟通元素的外部界面设计提供了指导。", "translation": "不确定性是自动驾驶汽车（AV）决策的一个固有方面，但它很少向行人传达，这阻碍了透明度。本研究调查了如何通过两种方法传达自动驾驶汽车的不确定性：显式沟通（置信度百分比显示）和隐式沟通（车辆运动线索），以及在不同置信度水平（高和低）下的效果。通过一项被试内VR实验（N=26），我们在一个过马路场景中评估了这些方法，评估了界面质量（可见性和直观性）、信息传达车辆置信度水平的程度，以及它们对参与者感知安全、信任和用户体验的影响。我们的结果表明，显式沟通在传达不确定性、增强安全、信任和用户体验方面更有效且更受欢迎。相反，隐式沟通会引入歧义，尤其是在自动驾驶汽车置信度低时。这项研究为不确定性沟通如何影响行人对自动驾驶汽车行为的解读提供了实证见解，并为整合不确定性作为沟通元素的外部界面提供了设计指导。", "summary": "本研究探讨了在自动驾驶汽车-行人交互中有效传达自动驾驶汽车不确定性的方法。通过VR实验，比较了显式（置信度百分比）和隐式（车辆运动）沟通方式的效果。结果表明，显式沟通在提高行人对AV的理解、安全感和信任方面更优，而隐式沟通可能导致混淆。研究为AV外部接口设计提供了指导。", "keywords": "自动驾驶汽车, 行人交互, 不确定性沟通, 置信度, 用户体验", "comments": "这项研究通过实证实验，明确指出了在自动驾驶汽车与行人交互中，显式传达不确定性信息的重要性及其优越性。其创新点在于对比了显式和隐式两种沟通方式，并提供了具体的设计指导，对于提升未来自动驾驶汽车的透明度、用户接受度和安全性具有重要意义。"}}
{"id": "2507.19116", "title": "Graph Structure Learning with Privacy Guarantees for Open Graph Data", "authors": ["Muhao Guo", "Jiaqi Wu", "Yang Weng", "Yizheng Liao", "Shengzhe Chen"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19116v1", "summary": "Ensuring privacy in large-scale open datasets is increasingly challenging\nunder regulations such as the General Data Protection Regulation (GDPR). While\ndifferential privacy (DP) provides strong theoretical guarantees, it primarily\nfocuses on noise injection during model training, neglecting privacy\npreservation at the data publishing stage. Existing privacy-preserving data\npublishing (PPDP) approaches struggle to balance privacy and utility,\nparticularly when data publishers and users are distinct entities. To address\nthis gap, we focus on the graph recovery problem and propose a novel\nprivacy-preserving estimation framework for open graph data, leveraging\nGaussian DP (GDP) with a structured noise-injection mechanism. Unlike\ntraditional methods that perturb gradients or model updates, our approach\nensures unbiased graph structure recovery while enforcing DP at the data\npublishing stage. Moreover, we provide theoretical guarantees on estimation\naccuracy and extend our method to discrete-variable graphs, a setting often\noverlooked in DP research. Experimental results in graph learning demonstrate\nrobust performance, offering a viable solution for privacy-conscious graph\nanalysis.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19116v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "图结构学习与开放图数据隐私保障", "tldr": "针对开放图数据，提出一种基于高斯差分隐私的隐私保护图结构估计框架，在数据发布阶段提供无偏图结构恢复和隐私保障，并在图学习中表现良好。", "motivation": "在大规模开放数据集中，尤其在GDPR等法规下，隐私保护日益挑战。现有差分隐私（DP）主要关注模型训练时的噪声注入，而忽略了数据发布阶段的隐私保护。此外，现有隐私保护数据发布（PPDP）方法难以平衡隐私和效用，特别当数据发布者和用户是不同实体时。本文旨在解决图恢复问题中的这些挑战。", "method": "提出一种新颖的隐私保护开放图数据估计框架，该框架利用高斯差分隐私（GDP）和结构化噪声注入机制。与传统扰动梯度或模型更新的方法不同，本方法在数据发布阶段确保无偏图结构恢复，同时强制执行差分隐私。此外，该方法还扩展到离散变量图。", "result": "研究提供了估计准确性的理论保证。在图学习中的实验结果显示出鲁棒的性能。", "conclusion": "本研究为注重隐私的图分析提供了一个可行的解决方案。", "translation": "在大规模开放数据集中确保隐私在通用数据保护条例（GDPR）等法规下变得越来越具有挑战性。虽然差分隐私（DP）提供了强大的理论保证，但它主要侧重于模型训练期间的噪声注入，而忽略了数据发布阶段的隐私保护。现有的隐私保护数据发布（PPDP）方法难以平衡隐私和效用，特别是当数据发布者和用户是不同实体时。为了解决这一空白，我们专注于图恢复问题，并提出了一种新颖的隐私保护开放图数据估计框架，该框架利用高斯差分隐私（GDP）和结构化噪声注入机制。与传统扰动梯度或模型更新的方法不同，我们的方法在数据发布阶段确保无偏图结构恢复，同时强制执行DP。此外，我们提供了估计准确性的理论保证，并将我们的方法扩展到离散变量图，这是一个在DP研究中经常被忽视的设置。图学习中的实验结果显示出鲁棒的性能，为注重隐私的图分析提供了一个可行的解决方案。", "summary": "这篇论文提出了一种针对开放图数据的隐私保护图结构估计框架，旨在解决现有方法在数据发布阶段隐私保护不足以及隐私与效用平衡的挑战。该框架利用高斯差分隐私和结构化噪声注入机制，确保在数据发布时实现无偏的图结构恢复和差分隐私。研究还提供了估计准确性的理论保证，并将其方法推广到离散变量图。实验结果表明，该方法在图学习中表现出强大的性能，为隐私敏感的图分析提供了一个实用的解决方案。", "keywords": "图结构学习, 隐私保护, 差分隐私, 开放图数据, 数据发布", "comments": "该论文的创新点在于将差分隐私的应用重点从模型训练阶段转移到数据发布阶段，并采用结构化噪声注入机制以实现无偏图结构恢复。同时，它将方法扩展到离散变量图，填补了现有差分隐私研究中的一个空白。这项工作为在大规模开放图数据背景下，尤其是在GDPR等严格隐私法规下，提供了一个理论上可证明且实践中表现良好的隐私保护解决方案，具有重要的现实意义。"}}
{"id": "2501.19340", "title": "Adaptive Self-Improvement for Smarter Energy Systems using Agentic Policy Search", "authors": ["Alexander Sommer", "Peter Bazan", "Behnam Babaeian", "Jonathan Fellerer", "Warren B. Powell", "Reinhard German"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2501.19340v2", "summary": "Controlling energy systems usually involves manually designed policies for\ndecision-making, which can be complex and time-consuming to develop. This\nprocess requires interdisciplinary collaboration among multiple domain experts,\nresulting in slow and inflexible adaptation to rapidly changing environments.\nLarge Language Models (LLMs) offer a promising paradigm shift by integrating\nextensive contextual knowledge with the capability to generate structured,\nexecutable code.\n  We present Agentic Policy Search (APS) -- a novel hierarchical optimization\nframework in which LLMs act as autonomous agents that propose complete control\nlogics, translate them into executable code, and iteratively improve them\nthrough direct system feedback. We apply APS to a residential energy system\nwith PV, battery, demand, and dynamic electricity prices. Within just seven\nsimulated days, the method yields a net profit of up to 6.20 EUR compared to\nthe no-battery reference scenario (-10.70 EUR), nearly matching the global\noptimum of a perfectly informed linear program. By combining LLM-driven policy\nsearch with the generation of human-interpretable control logic, APS\neffectively bridges adaptability and traceability in energy management -- while\nalso offering a transferable framework for agentic optimization in other\ndomains.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2501.19340v2", "cate": "eess.SY", "date": "2025-01-31", "updated": "2025-07-25", "AI": {"title_translation": "使用智能体策略搜索实现智能能源系统的自适应自改进", "tldr": "提出Agentic Policy Search (APS)框架，利用LLM生成并迭代改进能源系统控制策略，实现快速适应和显著盈利，接近最优解。", "motivation": "控制能源系统通常涉及手动设计且复杂耗时的策略，需要多学科协作，导致对快速变化环境的适应缓慢且不灵活。大型语言模型（LLMs）有望通过结合上下文知识和生成可执行代码的能力来解决此问题。", "method": "本文提出了智能体策略搜索（APS）——一种新颖的层次优化框架。在此框架中，LLMs充当自主智能体，负责提出完整的控制逻辑，将其转换为可执行代码，并通过直接系统反馈进行迭代改进。该方法应用于一个具有光伏、电池、需求和动态电价的住宅能源系统。", "result": "在仅仅七个模拟天内，该方法与无电池参考情景（-10.70欧元）相比，产生了高达6.20欧元的净利润，几乎与完美信息的线性规划的全局最优解相匹配。", "conclusion": "APS通过将LLM驱动的策略搜索与生成人类可解释的控制逻辑相结合，有效地弥合了能源管理中的适应性和可追溯性。此外，它还为其他领域的智能体优化提供了一个可转移的框架。", "translation": "控制能源系统通常涉及手动设计的决策策略，这些策略的开发过程复杂且耗时。此过程需要多个领域专家之间的跨学科协作，导致对快速变化的环境适应缓慢且不灵活。大型语言模型（LLMs）通过将广泛的上下文知识与生成结构化、可执行代码的能力相结合，提供了一种有前景的范式转变。\n我们提出了智能体策略搜索（APS）——一种新颖的层次优化框架，其中LLMs充当自主智能体，提出完整的控制逻辑，将其转换为可执行代码，并通过直接系统反馈迭代改进。我们将APS应用于一个具有光伏、电池、需求和动态电价的住宅能源系统。在短短七个模拟天内，该方法与无电池参考情景（-10.70欧元）相比，产生了高达6.20欧元的净利润，几乎与完美信息的线性规划的全局最优解相匹配。通过将LLM驱动的策略搜索与生成人类可解释的控制逻辑相结合，APS有效地弥合了能源管理中的适应性和可追溯性——同时还为其他领域的智能体优化提供了可转移的框架。", "summary": "本文提出了一种新颖的层次优化框架——智能体策略搜索（APS），该框架利用大型语言模型（LLM）作为自主智能体，为能源系统设计、翻译并迭代改进控制策略。将APS应用于住宅能源系统，结果表明其在短短7天内实现了显著的盈利（净利润高达6.20欧元，接近最优解），并且与传统手动方法相比，提高了适应性和可追溯性。该框架还被认为可推广应用于其他领域。", "keywords": "能源系统, 大型语言模型, 智能体策略搜索, 策略优化, 自适应控制", "comments": "创新点在于利用大型语言模型作为自主智能体进行策略搜索和迭代改进，直接生成可执行且人类可解释的代码。这解决了手动策略设计（复杂、缓慢、不灵活）的局限性，并弥合了能源管理中适应性和可追溯性之间的差距。其在其他领域的普适性也是一个显著优势。"}}
{"id": "2502.11439", "title": "An Efficient Sparse Fine-Tuning with Low Quantization Error via Neural Network Pruning", "authors": ["Cen-Jhih Li", "Aditya Bhaskara"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.11439v2", "summary": "Fine-tuning is an important step in adapting foundation models such as large\nlanguage models to downstream tasks. To make this step more accessible to users\nwith limited computational budgets, it is crucial to develop fine-tuning\nmethods that are memory and computationally efficient. Sparse Fine-tuning\n(SpFT) and Low-rank adaptation (LoRA) are two frameworks that have emerged for\naddressing this problem and have been adopted widely in practice. In this work,\nwe develop a new SpFT framework, based on ideas from neural network pruning. At\na high level, we first identify ``important'' neurons/nodes using feature\nimportance metrics from network pruning (specifically, we use the structural\npruning method), and then perform fine-tuning by restricting to weights\ninvolving these neurons. Experiments on common language tasks show our method\nimproves SpFT's memory efficiency by 20-50\\% while matching the accuracy of\nstate-of-the-art methods like LoRA's variants.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.11439v2", "cate": "cs.CL", "date": "2025-02-17", "updated": "2025-07-24", "AI": {"title_translation": "神经网络剪枝实现低量化误差的高效稀疏微调", "tldr": "提出一种基于神经网络剪枝的新型稀疏微调（SpFT）框架，显著提高内存效率并保持与LoRA相当的准确性。", "motivation": "基础模型（如大型语言模型）的微调对于下游任务至关重要，但对计算资源有限的用户而言，需要开发内存和计算效率更高的微调方法。", "method": "开发了一种新的稀疏微调（SpFT）框架，该框架基于神经网络剪枝。具体地，首先使用网络剪枝中的特征重要性度量（特别是结构化剪枝方法）识别“重要”神经元/节点，然后通过限制仅对涉及这些神经元的权重进行微调。", "result": "在常见语言任务上的实验表明，该方法将SpFT的内存效率提高了20-50%，同时与LoRA变体等最先进方法的准确性相匹配。", "conclusion": "该研究提出的基于神经网络剪枝的稀疏微调方法，在显著提高内存效率的同时保持了与现有先进方法相当的准确性，为计算资源有限的用户提供了更可行的微调方案。", "translation": "微调是将基础模型（如大型语言模型）适应下游任务的重要步骤。为了让计算预算有限的用户更容易进行此步骤，开发内存和计算效率高的微调方法至关重要。稀疏微调（SpFT）和低秩适应（LoRA）是解决此问题并已在实践中广泛采用的两种框架。在这项工作中，我们开发了一种新的SpFT框架，该框架基于神经网络剪枝的思想。从高层次来看，我们首先使用网络剪枝中的特征重要性度量（具体来说，我们使用结构化剪枝方法）识别“重要”神经元/节点，然后通过限制仅对涉及这些神经元的权重进行微调。在常见语言任务上的实验表明，我们的方法将SpFT的内存效率提高了20-50%，同时与LoRA变体等最先进方法的准确性相匹配。", "summary": "本论文提出了一种基于神经网络剪枝的新型稀疏微调（SpFT）框架，旨在提高大型语言模型微调的内存和计算效率。该方法通过识别重要神经元并仅微调相关权重来实现。实验证明，该方法在常见语言任务上将SpFT的内存效率提升20-50%，同时保持与LoRA等先进方法相当的准确性。", "keywords": "稀疏微调, 神经网络剪枝, 大型语言模型, 内存效率, 微调", "comments": "该论文提出了一种结合神经网络剪枝思想的稀疏微调方法，创新性在于利用剪枝技术来识别关键权重，从而在保持模型性能的同时显著降低微调所需的内存和计算资源。这对于资源受限的用户和设备部署大型模型具有重要意义。其贡献在于提供了一种高效且准确的微调替代方案。"}}
{"id": "2504.20011", "title": "Curiosity Driven Exploration to Optimize Structure-Property Learning in Microscopy", "authors": ["Aditya Vatsavai", "Ganesh Narasimha", "Yongtao Liu", "Jawad Chowdhury", "Jan-Chi Yang", "Hiroshi Funakubo", "Maxim Ziatdinov", "Rama Vasudevan"], "categories": ["cond-mat.mtrl-sci", "cs.LG"], "primary_category": "Subjects:       Materials Science (cond-mat.mtrl-sci)", "pdf_link": null, "comments": "Comments:      12 pages, 8 figures", "url": "http://arxiv.org/abs/2504.20011v2", "summary": "Rapidly determining structure-property correlations in materials is an\nimportant challenge in better understanding fundamental mechanisms and greatly\nassists in materials design. In microscopy, imaging data provides a direct\nmeasurement of the local structure, while spectroscopic measurements provide\nrelevant functional property information. Deep kernel active learning\napproaches have been utilized to rapidly map local structure to functional\nproperties in microscopy experiments, but are computationally expensive for\nmulti-dimensional and correlated output spaces. Here, we present an alternative\nlightweight curiosity algorithm which actively samples regions with unexplored\nstructure-property relations, utilizing a deep-learning based surrogate model\nfor error prediction. We show that the algorithm outperforms random sampling\nfor predicting properties from structures, and provides a convenient tool for\nefficient mapping of structure-property relationships in materials science.", "comment": "12 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2504.20011v2", "cate": "cond-mat.mtrl-sci", "date": "2025-04-28", "updated": "2025-07-24", "AI": {"title_translation": "好奇心驱动的探索以优化显微镜中的结构-性能学习", "tldr": "本文提出了一种轻量级的好奇心算法，用于在显微镜中高效地进行结构-性能映射，该算法通过主动采样未探索区域并利用深度学习代理模型预测误差，表现优于随机采样。", "motivation": "快速确定材料中的结构-性能关联对于理解基本机制和材料设计至关重要。现有深度核主动学习方法在处理多维和相关输出空间时计算成本高昂，因此需要一种更轻量级的替代方案。", "method": "本文提出了一种轻量级的好奇心算法，该算法利用基于深度学习的代理模型进行误差预测，并主动采样具有未探索结构-性能关系的区域。该方法旨在克服现有深度核主动学习方法的计算成本高的问题。", "result": "该算法在从结构预测性能方面表现优于随机采样。", "conclusion": "该算法为材料科学中结构-性能关系的有效映射提供了一个方便且高效的工具。", "translation": "在材料中快速确定结构-性能关联是更好地理解基本机制和极大地辅助材料设计的一个重要挑战。在显微镜学中，成像数据提供了局部结构的直接测量，而光谱测量提供了相关的功能特性信息。深度核主动学习方法已被用于在显微镜实验中快速将局部结构映射到功能特性，但对于多维和相关输出空间而言，其计算成本很高。在此，我们提出了一种替代的轻量级好奇心算法，该算法主动采样具有未探索结构-性能关系的区域，利用基于深度学习的代理模型进行误差预测。我们表明，该算法在从结构预测性能方面优于随机采样，并为材料科学中结构-性能关系的有效映射提供了一个方便的工具。", "summary": "本文提出了一种轻量级的好奇心算法，旨在优化显微镜中的结构-性能学习。该算法利用深度学习代理模型预测误差，并主动探索未知结构-性能区域。实验结果表明，该算法在从结构预测性能方面优于随机采样，为材料科学中高效映射结构-性能关系提供了一个实用工具。", "keywords": "显微镜, 结构-性能关系, 好奇心驱动, 主动学习, 深度学习", "comments": "这项工作提出了一种创新的轻量级好奇心算法，解决了现有主动学习方法计算成本高的问题。通过利用深度学习代理模型进行误差预测和主动采样未探索区域，该方法显著提高了结构-性能映射的效率和准确性，对于材料科学中的快速材料设计和理解具有重要意义。其“好奇心驱动”的探索策略是一个亮点。"}}
{"id": "2507.18653", "title": "Adapt, But Don't Forget: Fine-Tuning and Contrastive Routing for Lane Detection under Distribution Shift", "authors": ["Mohammed Abdul Hafeez Khan", "Parth Ganeriwala", "Sarah M. Lehman", "Siddhartha Bhattacharyya", "Amy Alvarez", "Natasha Neogi"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025, 2COOOL Workshop. Total 14 pages, 5 tables, and 4 figures", "url": "http://arxiv.org/abs/2507.18653v1", "summary": "Lane detection models are often evaluated in a closed-world setting, where\ntraining and testing occur on the same dataset. We observe that, even within\nthe same domain, cross-dataset distribution shifts can cause severe\ncatastrophic forgetting during fine-tuning. To address this, we first train a\nbase model on a source distribution and then adapt it to each new target\ndistribution by creating separate branches, fine-tuning only selected\ncomponents while keeping the original source branch fixed. Based on a\ncomponent-wise analysis, we identify effective fine-tuning strategies for\ntarget distributions that enable parameter-efficient adaptation. At inference\ntime, we propose using a supervised contrastive learning model to identify the\ninput distribution and dynamically route it to the corresponding branch. Our\nframework achieves near-optimal F1-scores while using significantly fewer\nparameters than training separate models for each distribution.", "comment": "Accepted to ICCV 2025, 2COOOL Workshop. Total 14 pages, 5 tables, and\n  4 figures", "pdf_url": "http://arxiv.org/pdf/2507.18653v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "适应但勿忘：分布偏移下车道线检测的微调与对比路由", "tldr": "本文提出一种新颖的微调和对比路由框架，用于在不同数据分布下进行车道线检测，有效避免灾难性遗忘并实现参数高效的适应。", "motivation": "现有的车道线检测模型在跨数据集分布偏移时，微调会导致严重的灾难性遗忘问题。", "method": "首先，在源分布上训练一个基础模型；然后，为每个新的目标分布创建独立分支，仅微调选定组件并保持源分支固定。在推理时，使用监督对比学习模型识别输入分布并动态路由到相应分支。", "result": "该框架在F1分数上达到接近最优的性能，并且比为每个分布训练单独模型使用显著更少的参数。", "conclusion": "本文提出的方法有效解决了分布偏移下的灾难性遗忘问题，实现了参数高效且性能优异的车道线检测。", "translation": "车道线检测模型通常在封闭世界设置中进行评估，即训练和测试都在同一数据集上进行。我们观察到，即使在同一领域内，跨数据集的分布偏移也会在微调过程中导致严重的灾难性遗忘。为了解决这个问题，我们首先在源分布上训练一个基础模型，然后通过创建单独的分支来使其适应每个新的目标分布，仅微调选定的组件，同时保持原始源分支固定。基于组件级分析，我们确定了针对目标分布的有效微调策略，从而实现了参数高效的适应。在推理时，我们建议使用监督对比学习模型来识别输入分布并将其动态路由到相应的分支。我们的框架实现了接近最优的F1分数，同时比为每个分布训练单独模型使用的参数显著更少。", "summary": "本文提出了一种应对车道线检测模型在跨数据集分布偏移下微调时面临的灾难性遗忘问题的新框架。该方法首先在源数据上训练基础模型，然后为每个目标分布创建独立分支，仅微调部分组件以实现参数高效适应。在推理阶段，利用监督对比学习模型动态路由输入到对应的分支。实验结果表明，该框架在保持接近最优F1分数的同时，显著减少了所需参数。", "keywords": "车道线检测, 分布偏移, 微调, 对比学习, 灾难性遗忘", "comments": "这篇论文通过结合分支微调和对比路由，巧妙地解决了车道线检测中跨域分布偏移导致的灾难性遗忘问题。其创新点在于提出了一种参数高效的适应策略，并通过动态路由机制提高了模型的泛化能力和实用性。这一方法在资源受限或需要快速部署的实际应用中具有重要意义。"}}
{"id": "2507.19470", "title": "Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models", "authors": ["Son Quoc Tran", "Tushaar Gangavarapu", "Nicholas Chernogor", "Jonathan P. Chang", "Cristian Danescu-Niculescu-Mizil"], "categories": ["cs.CL", "cs.HC"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Code and data available as part of ConvoKit: this https URL", "url": "http://arxiv.org/abs/2507.19470v1", "summary": "We often rely on our intuition to anticipate the direction of a conversation.\nEndowing automated systems with similar foresight can enable them to assist\nhuman-human interactions. Recent work on developing models with this predictive\ncapacity has focused on the Conversations Gone Awry (CGA) task: forecasting\nwhether an ongoing conversation will derail. In this work, we revisit this task\nand introduce the first uniform evaluation framework, creating a benchmark that\nenables direct and reliable comparisons between different architectures. This\nallows us to present an up-to-date overview of the current progress in CGA\nmodels, in light of recent advancements in language modeling. Our framework\nalso introduces a novel metric that captures a model's ability to revise its\nforecast as the conversation progresses.", "comment": "Code and data available as part of ConvoKit:\n  https://convokit.cornell.edu", "pdf_url": "http://arxiv.org/pdf/2507.19470v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "对话跑偏了，然后呢？评估对话预测模型", "tldr": "本文为“对话偏离”预测任务引入了一个统一的评估框架和基准，并提出了一个新颖的指标来衡量模型在对话进展中修正预测的能力，从而实现对对话预测模型的直接可靠比较。", "motivation": "为了使自动化系统能够像人类一样预见对话方向，从而辅助人机交互。具体来说，是为了改进评估预测对话是否会偏离（CGA任务）的模型。", "method": "本文提出了第一个统一的“对话偏离”（CGA）任务评估框架和基准，以实现不同架构之间的直接可靠比较。此外，该框架还引入了一个新颖的指标，用于衡量模型随着对话进展修正其预测的能力。", "result": "该框架能够实现不同模型架构在CGA任务上的直接可靠比较，并提供了CGA模型最新进展的概览。新指标则捕捉了预测修正能力。", "conclusion": "本文通过引入统一的框架、基准和新颖的修正指标，为评估对话预测模型，特别是“对话偏离”任务的模型，提供了一种标准化的方法，从而提升了该领域比较和评估这些模型的能力。", "translation": "我们经常依靠直觉来预测对话的方向。赋予自动化系统类似的预见能力可以帮助它们辅助人机交互。最近开发具有这种预测能力的模型的工作，主要集中在“对话偏离”（Conversations Gone Awry, CGA）任务上：预测正在进行的对话是否会偏离轨道。在这项工作中，我们重新审视了这项任务，并引入了第一个统一的评估框架，创建了一个基准，使得不同架构之间能够进行直接可靠的比较。这使我们能够根据语言建模的最新进展，提供CGA模型当前进展的最新概览。我们的框架还引入了一个新颖的指标，捕捉模型随着对话进展修正其预测的能力。", "summary": "本文旨在改进对话预测模型的评估，特别是针对“对话偏离”（CGA）任务。它首次提出了一个统一的评估框架和一个基准，以便对不同模型架构进行直接可靠的比较。此外，该研究还引入了一个新颖的指标，用于评估模型在对话进行过程中修正其预测的能力，从而为全面理解当前CGA模型的性能做出了贡献。", "keywords": "对话预测, 评估框架, 对话偏离, 新颖指标", "comments": "这篇论文具有创新性，因为它解决了会话AI评估中的一个关键空白：缺乏用于预测对话偏离的标准化基准和指标。引入“统一评估框架”和“捕捉模型修正预测能力的新颖指标”是重要的贡献。这有望通过提供更清晰的性能比较和鼓励开发更具适应性的模型，加速鲁棒会话AI的研究。"}}
{"id": "2507.19149", "title": "Machine Learning based Radio Environment Map Estimation for Indoor Visible Light Communication", "authors": ["Helena Serpi", "Christina", "Politi"], "categories": ["eess.SP", "I.2.6; C.2.1; C.2.3; C.4"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      10 pages, 10 figures", "url": "http://arxiv.org/abs/2507.19149v1", "summary": "An innovative method for radio map estimation in optical wireless\ncommunications is proposed that is based on Machine Learning rather than\nsimulation techniques. Multi-Layer Perceptron (MLP) representation of indoor\nVisible Light Communication (VLC) systems is suggested, and signal propagation\nis estimated. The simulation and performance predictions are accurate, fast and\nrequire a reduced set of training sample size with respect to other\ncounterparts, making this solution very suitable for real time estimation of an\nindoor VLC system. It is shown that by tweaking MLP parameters, such as sample\nsize, number of epochs and batch size, one can balance the desired level of\ninference accuracy with training time and optimize the model's performance to\nmeet real-time requirements.", "comment": "10 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.19149v1", "cate": "eess.SP", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于机器学习的室内可见光通信无线环境图估计", "tldr": "本文提出了一种基于机器学习（多层感知器）而非传统仿真技术来估计室内可见光通信（VLC）系统无线环境图的方法，该方法具有高精度、快速性，并能通过调整参数实现实时估计。", "motivation": "为了解决传统仿真技术在光学无线通信中进行无线电图估计的局限性，并实现更准确、快速且需要更少训练样本的实时估计。", "method": "提出了一种基于机器学习的方法，具体采用多层感知器（MLP）来表示室内可见光通信（VLC）系统，并估计信号传播。通过调整MLP参数（如样本大小、训练轮次和批次大小）来平衡推理精度和训练时间。", "result": "该方法在模拟和性能预测方面表现出高精度和快速性，并且相比其他方法需要更少的训练样本。通过调整MLP参数，可以优化模型性能以满足实时要求，平衡推理精度与训练时间。", "conclusion": "所提出的基于机器学习的无线电图估计方法非常适合室内可见光通信系统的实时估计，因为它能实现精度、速度和训练效率的良好平衡。", "translation": "本文提出了一种创新性的光学无线通信无线电图估计方法，该方法基于机器学习而非仿真技术。建议采用多层感知器（MLP）表示室内可见光通信（VLC）系统，并估计信号传播。仿真和性能预测结果显示该方法准确、快速，并且相对于其他同类方法所需的训练样本量更少，这使得该解决方案非常适用于室内VLC系统的实时估计。研究表明，通过调整MLP参数，如样本大小、训练轮次和批次大小，可以平衡所需的推理精度与训练时间，并优化模型的性能以满足实时要求。", "summary": "本文提出了一种用于室内可见光通信（VLC）系统无线环境图估计的机器学习方法，该方法利用多层感知器（MLP）取代传统仿真技术。该方法在精度和速度上表现出色，并能有效减少所需的训练样本量。通过灵活调整MLP参数，模型性能可以得到优化，从而满足VLC系统实时估计的需求。", "keywords": "机器学习, 无线电图估计, 可见光通信, 多层感知器, 实时估计", "comments": "该论文的创新点在于将机器学习，特别是多层感知器（MLP），应用于光学无线通信的无线电图估计，替代了传统的仿真技术。这为实现VLC系统的实时、高效性能预测提供了新的途径。其重要性体现在能够以更少的训练样本实现高精度和快速估计，这对于实际部署和运行具有显著优势。通过参数调优以平衡精度和训练时间的灵活性也是一个亮点。"}}
{"id": "2411.11467", "title": "Integrating Physics and Topology in Neural Networks for Learning Rigid Body Dynamics", "authors": ["Amaury Wei", "Olga Fink"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      20 pages, 10 figures. Published in Nature Communications", "url": "http://arxiv.org/abs/2411.11467v3", "summary": "Rigid body interactions are fundamental to numerous scientific disciplines,\nbut remain challenging to simulate due to their abrupt nonlinear nature and\nsensitivity to complex, often unknown environmental factors. These challenges\ncall for adaptable learning-based methods capable of capturing complex\ninteractions beyond explicit physical models and simulations. While graph\nneural networks can handle simple scenarios, they struggle with complex scenes\nand long-term predictions. We introduce a novel framework for modeling rigid\nbody dynamics and learning collision interactions, addressing key limitations\nof existing graph-based methods. Our approach extends the traditional\nrepresentation of meshes by incorporating higher-order topology complexes,\noffering a physically consistent representation. Additionally, we propose a\nphysics-informed message-passing neural architecture, embedding physical laws\ndirectly in the model. Our method demonstrates superior accuracy, even during\nlong rollouts, and exhibits strong generalization to unseen scenarios.\nImportantly, this work addresses the challenge of multi-entity dynamic\ninteractions, with applications spanning diverse scientific and engineering\ndomains.", "comment": "20 pages, 10 figures. Published in Nature Communications", "pdf_url": "http://arxiv.org/pdf/2411.11467v3", "cate": "cs.LG", "date": "2024-11-18", "updated": "2025-07-25", "AI": {"title_translation": "在神经网络中整合物理和拓扑以学习刚体动力学", "tldr": "本文提出了一种结合高阶拓扑和物理信息消息传递神经网络的新框架，用于建模刚体动力学和学习碰撞交互，解决了现有图神经网络在复杂场景和长期预测方面的局限性，并展示了卓越的准确性和泛化能力。", "motivation": "刚体相互作用的模拟因其突发的非线性性质和对复杂、未知环境因素的敏感性而具有挑战性。现有的图神经网络在处理复杂场景和长期预测时表现不佳，这促使研究人员寻求能够超越显式物理模型和模拟的自适应学习方法来捕捉复杂交互。", "method": "本研究引入了一个新颖的框架来建模刚体动力学和学习碰撞交互。该方法通过整合高阶拓扑复形来扩展传统的网格表示，提供物理上一致的表示。此外，它提出了一种物理信息消息传递神经网络架构，将物理定律直接嵌入到模型中。", "result": "该方法在长时间模拟中也表现出卓越的准确性，并对未见过的场景展现出强大的泛化能力。", "conclusion": "这项工作成功解决了多实体动态交互的挑战，其应用范围涵盖了各种科学和工程领域。", "translation": "刚体相互作用是众多科学学科的基础，但由于其突发的非线性性质以及对复杂、通常未知的环境因素的敏感性，模拟起来仍然具有挑战性。这些挑战要求可适应的学习方法，能够捕捉超出显式物理模型和模拟的复杂相互作用。尽管图神经网络可以处理简单的场景，但它们在复杂场景和长期预测方面表现不佳。我们引入了一个新颖的框架，用于建模刚体动力学和学习碰撞相互作用，解决了现有基于图的方法的关键局限性。我们的方法通过整合更高阶的拓扑复形来扩展传统的网格表示，提供了物理上一致的表示。此外，我们提出了一种物理信息消息传递神经架构，将物理定律直接嵌入到模型中。我们的方法即使在长时间的推演中也表现出卓越的准确性，并对未见过的场景展现出强大的泛化能力。重要的是，这项工作解决了多实体动态相互作用的挑战，其应用范围涵盖了各种科学和工程领域。", "summary": "本文提出了一种新颖的框架，用于通过神经网络学习刚体动力学和碰撞交互。该框架通过整合高阶拓扑复形来增强传统网格表示，并引入了物理信息消息传递神经网络架构，从而将物理定律直接嵌入模型中。与现有图神经网络相比，该方法在复杂场景和长期预测方面表现出卓越的准确性和强大的泛化能力，有效解决了多实体动态交互的挑战。", "keywords": "刚体动力学, 神经网络, 物理信息, 拓扑, 碰撞交互", "comments": "这项工作通过结合高阶拓扑表示和物理信息神经网络，为刚体动力学模拟提供了一种创新的学习范式。其创新点在于将物理一致性与神经网络的泛化能力相结合，有效克服了传统图神经网络在处理复杂非线性交互和长期预测上的局限性。这对于需要高精度和强泛化能力的科学与工程领域具有重要意义。"}}
{"id": "2507.18958", "title": "PerioDet: Large-Scale Panoramic Radiograph Benchmark for Clinical-Oriented Apical Periodontitis Detection", "authors": ["Xiaocheng Fang", "Jieyi Cai", "Huanyu Liu", "Chengju Zhou", "Minhua Lu", "Bingzhi Chen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      MICCAI 2025(Early Accept)", "url": "http://arxiv.org/abs/2507.18958v1", "summary": "Apical periodontitis is a prevalent oral pathology that presents significant\npublic health challenges. Despite advances in automated diagnostic systems\nacross various medical fields, the development of Computer-Aided Diagnosis\n(CAD) applications for apical periodontitis is still constrained by the lack of\na large-scale, high-quality annotated dataset. To address this issue, we\nrelease a large-scale panoramic radiograph benchmark called \"PerioXrays\",\ncomprising 3,673 images and 5,662 meticulously annotated instances of apical\nperiodontitis. To the best of our knowledge, this is the first benchmark\ndataset for automated apical periodontitis diagnosis. This paper further\nproposes a clinical-oriented apical periodontitis detection (PerioDet)\nparadigm, which jointly incorporates Background-Denoising Attention (BDA) and\nIoU-Dynamic Calibration (IDC) mechanisms to address the challenges posed by\nbackground noise and small targets in automated detection. Extensive\nexperiments on the PerioXrays dataset demonstrate the superiority of PerioDet\nin advancing automated apical periodontitis detection. Additionally, a\nwell-designed human-computer collaborative experiment underscores the clinical\napplicability of our method as an auxiliary diagnostic tool for professional\ndentists.", "comment": "MICCAI 2025(Early Accept)", "pdf_url": "http://arxiv.org/pdf/2507.18958v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "PerioDet：面向临床的根尖周炎检测大规模全景X光片基准", "tldr": "该研究发布了一个名为“PerioXrays”的大规模根尖周炎检测全景X光片数据集，并提出了一个名为PerioDet的检测范式，该范式结合了背景去噪注意力和IoU动态校准机制，以提高自动化检测的性能和临床实用性。", "motivation": "根尖周炎是一种常见的口腔病理，对公共健康构成重大挑战。尽管自动化诊断系统在其他医学领域有所进展，但由于缺乏大规模、高质量的标注数据集，根尖周炎计算机辅助诊断（CAD）应用的发展受到限制。", "method": "研究发布了一个名为“PerioXrays”的大规模全景X光片基准数据集，包含3,673张图像和5,662个根尖周炎实例的精确标注。在此基础上，提出了一种面向临床的根尖周炎检测（PerioDet）范式，该范式联合集成了背景去噪注意力（BDA）和IoU动态校准（IDC）机制，以解决自动化检测中背景噪声和小目标带来的挑战。", "result": "在PerioXrays数据集上的大量实验证明了PerioDet在推进自动化根尖周炎检测方面的优越性。此外，一项精心设计的人机协作实验强调了该方法作为专业牙医辅助诊断工具的临床适用性。", "conclusion": "PerioDet及其伴随的PerioXrays数据集显著推动了根尖周炎的自动化检测，并展示了其作为临床辅助诊断工具的潜力。", "translation": "根尖周炎是一种常见的口腔病理，对公共健康构成重大挑战。尽管自动化诊断系统在各种医学领域取得了进展，但由于缺乏大规模、高质量的标注数据集，根尖周炎计算机辅助诊断（CAD）应用的发展仍然受到限制。为了解决这个问题，我们发布了一个名为“PerioXrays”的大规模全景X光片基准数据集，包含3,673张图像和5,662个根尖周炎的精心标注实例。据我们所知，这是第一个用于自动化根尖周炎诊断的基准数据集。本文进一步提出了一种面向临床的根尖周炎检测（PerioDet）范式，该范式联合集成了背景去噪注意力（BDA）和IoU动态校准（IDC）机制，以解决自动化检测中背景噪声和小目标带来的挑战。在PerioXrays数据集上的大量实验证明了PerioDet在推进自动化根尖周炎检测方面的优越性。此外，一项精心设计的人机协作实验强调了我们方法作为专业牙医辅助诊断工具的临床适用性。", "summary": "本文针对根尖周炎自动化诊断中缺乏大规模高质量数据集的问题，发布了首个根尖周炎大规模全景X光片基准数据集“PerioXrays”。在此基础上，提出了一种名为PerioDet的检测范式，该范式通过整合背景去噪注意力和IoU动态校准机制，有效应对了背景噪声和小目标检测的挑战。实验证明PerioDet在自动化根尖周炎检测方面表现优异，并通过人机协作实验验证了其作为牙医辅助诊断工具的临床实用性。", "keywords": "根尖周炎, 全景X光片, 计算机辅助诊断, 数据集, 目标检测", "comments": "该论文的主要创新点在于首次构建并发布了大规模、高质量的根尖周炎全景X光片基准数据集PerioXrays，填补了该领域的数据空白。同时，提出的PerioDet范式通过引入BDA和IDC机制，有效提升了复杂临床图像中根尖周炎的检测精度，尤其是在处理背景噪声和小目标方面。其临床适用性的验证也增加了研究的实际价值。"}}
{"id": "2507.18755", "title": "Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback", "authors": ["Chandra Maddila", "Adam Tait", "Claire Chang", "Daniel Cheng", "Nauman Ahmad", "Vijayaraghavan Murali", "Marshall Roch", "Arnaud Avondet", "Aaron Meltzer", "Victor Montalvao", "Michael Hopko", "Chris Waterson", "Parth Thakkar", "Renuka Fernandez", "Kristian Kristensen", "Sivan Barzily", "Sherry Chen", "Rui Abreu", "Nachiappan Nagappan", "Payam Shodjai", "Killian Murphy", "James Everingham", "Aparna Ramani", "Peter C. Rigby"], "categories": ["cs.SE", "cs.AI", "cs.PL"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18755v1", "summary": "Aim: With the advent of LLMs, sophisticated agentic program repair has become\nviable at large organizations with large codebases. In this work, we develop an\nEngineering Agent that fixes the source code based on test failures at scale\nacross diverse software offerings internally.\n  Method: Using Llama as the base, we employ the ReAct harness to develop an\nagent. We start with a test failure that was triaged by a rule-based test\nfailure bot. We then set up an agentic harness and allow the agent to reason\nand run a set of 15 actions from reading a file to generating a patch. We\nprovide feedback to the agent through static analysis and test failures so it\ncan refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch\nconforms to the standards followed by a human review to land fixes.\n  Benchmark Findings: We curated offline benchmarks for our patch generator,\nthe Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we\nfound that a specialized 70B model is highly competitive with the much larger\nbut vanilla Llama-405B. In an ablation study, we found that the ReAct harness\n(neural model) benefited from the symbolic information from static analysis\ntools and test execution traces. A model that strikes a balance between the\nsolve rate and error rate vs the cost and latency has a benchmark solve rate of\n42.3% using an average 11.8 feedback iterations.\n  Production Findings: In a three month period, 80% of the generated fixes were\nreviewed, of which 31.5% were landed (25.5% of the total number of generated\nfixes).\n  Feedback from Engineers: We used open coding to extract qualitative themes\nfrom engineers' feedback. We saw positive feedback in the form of quick\napprovals, gratitude, and surprise. We also found mixed feedback when the\nEngineering Agent's solution was partially correct and it served as a good\nstarting point.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18755v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "规模化测试失败的智能程序修复：一种结合静态分析和测试执行反馈的神经符号方法", "tldr": "本文开发了一个基于LLM的工程代理，利用神经符号方法（ReAct与静态分析/测试反馈）大规模自动修复代码，实现了25.5%的修复落地率，并获得了工程师的积极反馈。", "motivation": "随着大型语言模型的兴起，为拥有庞大代码库的大型组织实现复杂的智能程序修复变得可行。本文旨在开发一个工程代理，能够根据测试失败在大规模、多样化的内部软件产品中修复源代码。", "method": "该方法以Llama为基础，采用ReAct框架开发代理。流程始于由基于规则的测试失败机器人分类的测试失败。代理通过推理执行15项操作（从读取文件到生成补丁）。通过静态分析和测试失败提供反馈，使代理能够完善解决方案。利用LLM作为判断器来确保补丁符合人工审查标准。", "result": "离线评估显示，一个专门的70B模型与更大的Llama-405B模型竞争力相当。消融研究表明，ReAct框架（神经模型）从静态分析工具和测试执行跟踪的符号信息中受益。在平衡解决率、错误率、成本和延迟的模型中，基准解决率为42.3%，平均经过11.8次反馈迭代。在三个月内，80%的生成修复被审查，其中31.5%被采纳（占生成修复总数的25.5%）。工程师反馈显示积极，如快速批准、感激和惊喜，部分正确方案也作为良好起点。", "conclusion": "该研究成功开发并部署了一个基于LLM的工程代理，利用神经符号方法在规模化环境下进行程序修复。离线和生产结果均验证了其有效性，特别是其在实际代码库中生成可采纳修复的能力，并获得了工程师的积极反馈，证明了其作为辅助工具的潜力。", "translation": "目的：随着大型语言模型的出现，复杂的智能程序修复在拥有庞大代码库的大型组织中变得可行。在这项工作中，我们开发了一个工程代理，能够根据内部多样化软件产品的大规模测试失败来修复源代码。\n方法：我们以Llama为基础，采用ReAct框架来开发一个代理。我们从一个由基于规则的测试失败机器人分类的测试失败开始。然后，我们建立一个代理框架，允许代理推理并运行从读取文件到生成补丁的15个动作。我们通过静态分析和测试失败向代理提供反馈，使其能够完善解决方案。我们利用LLM作为判断器来确保补丁符合人工审查标准，以便修复能够落地。\n基准发现：我们为补丁生成器、工程代理循环和LLM作为判断器策划了离线基准。在离线评估中，我们发现一个专门的70B模型与大得多但普通的Llama-405B模型具有高度竞争力。在消融研究中，我们发现ReAct框架（神经模型）受益于静态分析工具和测试执行跟踪提供的符号信息。一个在解决率和错误率与成本和延迟之间取得平衡的模型，其基准解决率为42.3%，平均经过11.8次反馈迭代。\n生产发现：在三个月内，80%的生成修复被审查，其中31.5%被采纳（占生成修复总数的25.5%）。\n工程师反馈：我们使用开放编码从工程师的反馈中提取定性主题。我们看到了积极的反馈，形式为快速批准、感激和惊喜。当工程代理的解决方案部分正确时，我们也发现了混合反馈，它作为一个很好的起点。", "summary": "本文提出了一种大规模智能程序修复的神经符号方法，开发了一个基于Llama和ReAct框架的工程代理。该代理结合静态分析和测试执行反馈来迭代生成和优化代码补丁，并利用LLM作为判断器进行验证。离线评估显示其性能优于大型通用模型，并在实际生产环境中实现了25.5%的修复落地率，获得了工程师的积极反馈，证明了其在辅助软件开发中的潜力。", "keywords": "智能程序修复, 大型语言模型, 神经符号方法, 静态分析, 测试执行反馈", "comments": "这篇论文的创新点在于将大型语言模型（LLM）与神经符号方法（ReAct框架结合静态分析和测试执行反馈）相结合，实现了大规模的智能程序修复。其重要性体现在将程序修复从理论研究推向实际生产应用，并展示了LLM在复杂工程任务中的巨大潜力。通过引入LLM-as-a-Judge进行补丁验证，提高了修复质量。论文也提供了实际生产数据和工程师反馈，增加了其可信度和实用价值。"}}
{"id": "2505.11493", "title": "GIE-Bench: Towards Grounded Evaluation for Text-Guided Image Editing", "authors": ["Yusu Qian", "Jiasen Lu", "Tsu-Jui Fu", "Xinze Wang", "Chen Chen", "Yinfei Yang", "Wenze Hu", "Zhe Gan"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Project page: this https URL", "url": "http://arxiv.org/abs/2505.11493v3", "summary": "Editing images using natural language instructions has become a natural and\nexpressive way to modify visual content; yet, evaluating the performance of\nsuch models remains challenging. Existing evaluation approaches often rely on\nimage-text similarity metrics like CLIP, which lack precision. In this work, we\nintroduce a new benchmark designed to evaluate text-guided image editing models\nin a more grounded manner, along two critical dimensions: (i) functional\ncorrectness, assessed via automatically generated multiple-choice questions\nthat verify whether the intended change was successfully applied; and (ii)\nimage content preservation, which ensures that non-targeted regions of the\nimage remain visually consistent using an object-aware masking technique and\npreservation scoring. The benchmark includes over 1000 high-quality editing\nexamples across 20 diverse content categories, each annotated with detailed\nediting instructions, evaluation questions, and spatial object masks. We\nconduct a large-scale study comparing GPT-Image-1, the latest flagship in the\ntext-guided image editing space, against several state-of-the-art editing\nmodels, and validate our automatic metrics against human ratings. Results show\nthat GPT-Image-1 leads in instruction-following accuracy, but often\nover-modifies irrelevant image regions, highlighting a key trade-off in the\ncurrent model behavior. GIE-Bench provides a scalable, reproducible framework\nfor advancing more accurate evaluation of text-guided image editing.", "comment": "Project page: https://sueqian6.github.io/GIE-Bench-web/", "pdf_url": "http://arxiv.org/pdf/2505.11493v3", "cate": "cs.CV", "date": "2025-05-16", "updated": "2025-07-25", "AI": {"title_translation": "GIE-Bench：迈向文本引导图像编辑的接地评估", "tldr": "引入GIE-Bench，一个用于文本引导图像编辑模型更精确评估的新基准，关注功能正确性和图像内容保留。", "motivation": "现有文本引导图像编辑模型的评估方法（如CLIP）缺乏精确性，难以准确评估模型性能。", "method": "提出了GIE-Bench基准，通过两个维度进行评估：1. 功能正确性，通过自动生成的多项选择题验证预期更改；2. 图像内容保留，使用对象感知遮罩技术和保留评分确保非目标区域视觉一致。基准包含1000多个编辑示例，涵盖20个类别。", "result": "对GPT-Image-1和SOTA模型进行大规模研究，结果显示GPT-Image-1在指令遵循准确性方面领先，但常过度修改不相关区域，揭示了当前模型行为的关键权衡。自动指标已通过人工评分验证。", "conclusion": "GIE-Bench提供了一个可扩展、可复现的框架，以推动文本引导图像编辑更准确的评估。", "translation": "使用自然语言指令编辑图像已成为修改视觉内容的一种自然而富有表现力的方式；然而，评估此类模型的性能仍然具有挑战性。现有的评估方法通常依赖于图像-文本相似性度量（如CLIP），但这些方法缺乏精确性。在这项工作中，我们引入了一个新的基准，旨在以更“接地”的方式评估文本引导图像编辑模型，该评估基于两个关键维度：(i) 功能正确性，通过自动生成的多项选择题来验证预期更改是否成功应用；以及 (ii) 图像内容保留，该维度使用对象感知遮罩技术和保留评分来确保图像的非目标区域保持视觉一致性。该基准包含20个不同内容类别中1000多个高质量编辑示例，每个示例都标注了详细的编辑指令、评估问题和空间对象遮罩。我们进行了一项大规模研究，比较了文本引导图像编辑领域最新的旗舰模型GPT-Image-1与几种最先进的编辑模型，并根据人工评分验证了我们的自动度量。结果表明，GPT-Image-1在指令遵循准确性方面处于领先地位，但通常会过度修改不相关的图像区域，这突显了当前模型行为中的一个关键权衡。GIE-Bench提供了一个可扩展、可复现的框架，以推动文本引导图像编辑更准确的评估。", "summary": "本文提出GIE-Bench，一个用于评估文本引导图像编辑模型的新基准，解决了现有评估方法精度不足的问题。GIE-Bench从功能正确性和图像内容保留两个维度进行评估，并包含1000多个高质量编辑示例。研究表明，GPT-Image-1在指令遵循上表现优秀，但在内容保留方面存在过度修改的问题。GIE-Bench为文本引导图像编辑的准确评估提供了可扩展的框架。", "keywords": "文本引导图像编辑, 评估基准, GIE-Bench, 功能正确性, 图像内容保留", "comments": "GIE-Bench通过引入功能正确性和图像内容保留两个关键维度，显著提升了文本引导图像编辑模型评估的“接地”性。其创新之处在于结合了自动生成的多项选择题和对象感知遮罩技术，并提供了大规模高质量数据集。这对于推动该领域模型的发展至关重要，因为它揭示了模型性能的细微差别，如GPT-Image-1在准确性与区域修改之间的权衡。"}}
{"id": "2507.19404", "title": "A multi-dynamic low-rank deep image prior (ML-DIP) for real-time 3D cardiovascular MRI", "authors": ["Chong Chen", "Marc Vornehm", "Preethi Chandrasekaran", "Muhammad A. Sultan", "Syed M. Arshad", "Yingmin Liu", "Yuchi Han", "Rizwan Ahmad"], "categories": ["eess.IV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19404v1", "summary": "Purpose: To develop a reconstruction framework for 3D real-time cine\ncardiovascular magnetic resonance (CMR) from highly undersampled data without\nrequiring fully sampled training data.\n  Methods: We developed a multi-dynamic low-rank deep image prior (ML-DIP)\nframework that models spatial image content and temporal deformation fields\nusing separate neural networks. These networks are optimized per scan to\nreconstruct the dynamic image series directly from undersampled k-space data.\nML-DIP was evaluated on (i) a 3D cine digital phantom with simulated premature\nventricular contractions (PVCs), (ii) ten healthy subjects (including two\nscanned during both rest and exercise), and (iii) five patients with PVCs.\nPhantom results were assessed using peak signal-to-noise ratio (PSNR) and\nstructural similarity index measure (SSIM). In vivo performance was evaluated\nby comparing left-ventricular function quantification (against 2D real-time\ncine) and image quality (against 2D real-time cine and binning-based 5D-Cine).\n  Results: In the phantom study, ML-DIP achieved PSNR > 29 dB and SSIM > 0.90\nfor scan times as short as two minutes, while recovering cardiac motion,\nrespiratory motion, and PVC events. In healthy subjects, ML-DIP yielded\nfunctional measurements comparable to 2D cine and higher image quality than\n5D-Cine, including during exercise with high heart rates and bulk motion. In\nPVC patients, ML-DIP preserved beat-to-beat variability and reconstructed\nirregular beats, whereas 5D-Cine showed motion artifacts and information loss\ndue to binning.\n  Conclusion: ML-DIP enables high-quality 3D real-time CMR with acceleration\nfactors exceeding 1,000 by learning low-rank spatial and temporal\nrepresentations from undersampled data, without relying on external fully\nsampled training datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19404v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "用于实时三维心血管磁共振成像的多动态低秩深度图像先验 (ML-DIP)", "tldr": "ML-DIP 是一种新的框架，无需完全采样训练数据即可从高度欠采样数据中实现高质量的实时三维心血管磁共振 (CMR) 图像重建。", "motivation": "开发一种用于从高度欠采样数据中重建三维实时电影心血管磁共振 (CMR) 的框架，且无需完全采样训练数据。", "method": "本研究开发了一种多动态低秩深度图像先验 (ML-DIP) 框架，该框架使用独立的神经网络对空间图像内容和时间形变场进行建模。这些网络针对每次扫描进行优化，以直接从欠采样 k 空间数据重建动态图像序列。ML-DIP 在三维电影数字体模（模拟室性早搏）、十名健康受试者和五名室性早搏患者中进行了评估。性能通过峰值信噪比 (PSNR)、结构相似性指数 (SSIM)、左心室功能量化以及图像质量进行评估。", "result": "在体模研究中，ML-DIP 在短短两分钟的扫描时间内实现了 PSNR > 29 dB 和 SSIM > 0.90，同时恢复了心脏运动、呼吸运动和室性早搏事件。在健康受试者中，ML-DIP 产生的功能测量结果与二维电影相当，图像质量优于 5D-Cine，即使在高心率和大范围运动的运动过程中也是如此。在室性早搏患者中，ML-DIP 保留了逐搏变异性并重建了不规则搏动，而 5D-Cine 则显示出运动伪影和由于分箱导致的信息丢失。", "conclusion": "ML-DIP 通过从欠采样数据中学习低秩空间和时间表示，实现了高质量的三维实时 CMR，加速因子超过 1,000，且不依赖于外部完全采样训练数据集。", "translation": "目的：开发一种从高度欠采样数据重建三维实时电影心血管磁共振 (CMR) 的框架，且无需完全采样训练数据。\n方法：我们开发了一种多动态低秩深度图像先验 (ML-DIP) 框架，该框架使用独立的神经网络对空间图像内容和时间形变场进行建模。这些网络针对每次扫描进行优化，以直接从欠采样 k 空间数据重建动态图像序列。ML-DIP 在 (i) 模拟室性早搏 (PVC) 的三维电影数字体模上，(ii) 十名健康受试者（包括两名在休息和运动期间均进行扫描的受试者），以及 (iii) 五名 PVC 患者中进行了评估。体模结果使用峰值信噪比 (PSNR) 和结构相似性指数度量 (SSIM) 进行评估。体内性能通过比较左心室功能量化（与二维实时电影对比）和图像质量（与二维实时电影和基于分箱的 5D-Cine 对比）进行评估。\n结果：在体模研究中，ML-DIP 在短短两分钟的扫描时间内实现了 PSNR > 29 dB 和 SSIM > 0.90，同时恢复了心脏运动、呼吸运动和 PVC 事件。在健康受试者中，ML-DIP 产生的功测量结果与二维电影相当，图像质量优于 5D-Cine，包括在高心率和大范围运动的运动期间。在 PVC 患者中，ML-DIP 保留了逐搏变异性并重建了不规则搏动，而 5D-Cine 则显示出运动伪影和由于分箱导致的信息丢失。\n结论：ML-DIP 通过从欠采样数据中学习低秩空间和时间表示，实现了高质量的三维实时 CMR，加速因子超过 1,000，且不依赖于外部完全采样训练数据集。", "summary": "本文提出了一种名为多动态低秩深度图像先验 (ML-DIP) 的新型框架，旨在从高度欠采样数据中重建高质量的三维实时心血管磁共振 (CMR) 图像，且无需外部完全采样的训练数据。ML-DIP 利用独立的神经网络对空间图像内容和时间形变场进行建模，并针对每次扫描进行优化。实验结果表明，ML-DIP 在体模研究中表现出色，并在健康受试者和室性早搏患者中均能提供与现有方法相当或更优的功能测量结果和图像质量，尤其是在处理不规则心跳和高加速因子方面显示出显著优势。", "keywords": "ML-DIP, 3D实时CMR, 欠采样数据, 深度图像先验, 图像重建", "comments": "该论文的创新点在于其 ML-DIP 框架无需完全采样的训练数据，这对于实际临床应用非常重要，因为此类数据通常难以获取。此外，ML-DIP 能够有效地处理不规则心跳和实现超过 1000 的高加速因子，这大大提高了三维实时 CMR 的可行性和应用范围。"}}
{"id": "2507.19370", "title": "BEV-LLM: Leveraging Multimodal BEV Maps for Scene Captioning in Autonomous Driving", "authors": ["Felix Brandstaetter", "Erik Schuetz", "Katharina Winter", "Fabian Flohr"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19370v1", "summary": "Autonomous driving technology has the potential to transform transportation,\nbut its wide adoption depends on the development of interpretable and\ntransparent decision-making systems. Scene captioning, which generates natural\nlanguage descriptions of the driving environment, plays a crucial role in\nenhancing transparency, safety, and human-AI interaction. We introduce BEV-LLM,\na lightweight model for 3D captioning of autonomous driving scenes. BEV-LLM\nleverages BEVFusion to combine 3D LiDAR point clouds and multi-view images,\nincorporating a novel absolute positional encoding for view-specific scene\ndescriptions. Despite using a small 1B parameter base model, BEV-LLM achieves\ncompetitive performance on the nuCaption dataset, surpassing state-of-the-art\nby up to 5\\% in BLEU scores. Additionally, we release two new datasets - nuView\n(focused on environmental conditions and viewpoints) and GroundView (focused on\nobject grounding) - to better assess scene captioning across diverse driving\nscenarios and address gaps in current benchmarks, along with initial\nbenchmarking results demonstrating their effectiveness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19370v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "BEV-LLM：利用多模态BEV地图进行自动驾驶场景描述", "tldr": "BEV-LLM是一个轻量级的自动驾驶3D场景描述模型，在nuCaption数据集上表现优异，并发布了nuView和GroundView两个新数据集以促进研究。", "motivation": "自动驾驶技术需要可解释和透明的决策系统才能广泛应用。场景描述能够生成驾驶环境的自然语言描述，对于提升透明度、安全性以及人机交互至关重要。", "method": "本文提出了BEV-LLM，一个用于自动驾驶场景3D描述的轻量级模型。BEV-LLM利用BEVFusion结合3D LiDAR点云和多视图图像，并引入了新颖的绝对位置编码以实现特定视角的场景描述。此外，研究还发布了nuView和GroundView两个新数据集，旨在更好地评估不同驾驶场景下的场景描述能力。", "result": "BEV-LLM尽管使用了小型10亿参数的基础模型，但在nuCaption数据集上取得了有竞争力的性能，BLEU分数超越了现有技术高达5%。新发布的数据集nuView和GroundView的初步基准测试结果也证明了它们的有效性。", "conclusion": "BEV-LLM是一个高效的轻量级自动驾驶3D场景描述模型，其卓越性能和新数据集的发布有助于推动该领域的发展，提升自动驾驶系统的可解释性和透明度。", "translation": "自动驾驶技术有潜力改变交通运输，但其广泛采用取决于可解释和透明的决策系统的开发。场景描述（生成驾驶环境的自然语言描述）在增强透明度、安全性以及人机交互方面发挥着关键作用。我们引入了BEV-LLM，一个用于自动驾驶场景3D描述的轻量级模型。BEV-LLM利用BEVFusion结合3D LiDAR点云和多视图图像，并引入了一种新颖的绝对位置编码，用于特定视角的场景描述。尽管使用了小型10亿参数的基础模型，BEV-LLM在nuCaption数据集上取得了有竞争力的性能，在BLEU分数上超越了现有技术高达5%。此外，我们发布了两个新数据集——nuView（侧重于环境条件和视角）和GroundView（侧重于对象接地），以更好地评估不同驾驶场景下的场景描述能力，并弥补当前基准测试中的不足，同时提供了初步的基准测试结果，证明了它们的有效性。", "summary": "本文介绍了BEV-LLM，一个轻量级的自动驾驶3D场景描述模型，它通过融合3D LiDAR点云和多视图图像，并引入新颖的绝对位置编码来生成场景描述。BEV-LLM在nuCaption数据集上表现出色，超越了现有技术。为进一步推动场景描述研究，论文还发布了nuView和GroundView两个新数据集。", "keywords": "场景描述, 自动驾驶, BEV-LLM, 多模态, 数据集", "comments": "BEV-LLM的创新之处在于其轻量级设计却能达到最先进的性能，这对于资源受限的自动驾驶系统具有重要意义。同时，发布两个新数据集，特别是针对环境条件和对象接地的关注，有效弥补了现有基准的不足，对于促进自动驾驶场景描述的全面评估和发展具有重要价值。"}}
{"id": "2506.06410", "title": "Delphos: A reinforcement learning framework for assisting discrete choice model specification", "authors": ["Gabriel Nova", "Stephane Hess", "Sander van Cranenburgh"], "categories": ["econ.GN", "cs.LG", "q-fin.EC"], "primary_category": "Subjects:       General Economics (econ.GN)", "pdf_link": null, "comments": "Comments:      13 pages, 7 figures", "url": "http://arxiv.org/abs/2506.06410v2", "summary": "We introduce Delphos, a deep reinforcement learning framework for assisting\nthe discrete choice model specification process. Unlike traditional approaches\nthat treat model specification as a static optimisation problem, Delphos\nrepresents a paradigm shift: it frames this specification challenge as a\nsequential decision-making problem, formalised as a Markov Decision Process. In\nthis setting, an agent learns to specify well-performing model candidates by\nchoosing a sequence of modelling actions - such as selecting variables,\naccommodating both generic and alternative-specific taste parameters, applying\nnon-linear transformations, and including interactions with covariates - and\ninteracting with a modelling environment that estimates each candidate and\nreturns a reward signal. Specifically, Delphos uses a Deep Q-Network that\nreceives delayed rewards based on modelling outcomes (e.g., log-likelihood) and\nbehavioural expectations (e.g., parameter signs), and distributes rewards\nacross the sequence of actions to learn which modelling decisions lead to\nwell-performing candidates. We evaluate Delphos on both simulated and empirical\ndatasets, varying the size of the modelling space and the reward function. To\nassess the agent's performance in navigating the model space, we analyse the\nlearning curve, the distribution of Q-values, occupancy metrics, and Pareto\nfronts. Our results show that the agent learns to adaptively explore strategies\nto identify well-performing models across search spaces, even without prior\ndomain knowledge. It efficiently explores large modelling spaces, concentrates\nits search in high-reward regions, and suggests candidates that define Pareto\nfrontiers balancing model fit and behavioural plausibility. These findings\nhighlight the potential of this novel adaptive, learning-based framework to\nassist in the model specification process.", "comment": "13 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2506.06410v2", "cate": "econ.GN", "date": "2025-06-06", "updated": "2025-07-25", "AI": {"title_translation": "Delphos：一个用于辅助离散选择模型规格的强化学习框架", "tldr": "Delphos是一个使用深度强化学习方法（DQN）来辅助离散选择模型规格的框架，它将模型规格视为一个序列决策问题，并能高效探索模型空间以找到高性能模型。", "motivation": "传统的模型规格方法将问题视为静态优化，而Delphos提出将其视为一个序列决策问题，以克服传统方法的局限性。", "method": "Delphos框架将模型规格形式化为一个马尔可夫决策过程。一个代理通过选择一系列建模动作（如变量选择、非线性变换、交互项）与建模环境交互，环境会评估每个候选模型并返回奖励信号。具体地，Delphos使用深度Q网络（DQN），根据建模结果（如对数似然）和行为预期（如参数符号）接收延迟奖励，并将奖励分配给动作序列，以学习哪些决策能产生高性能模型。", "result": "Delphos代理能够自适应地探索策略，即使没有先验领域知识也能在搜索空间中识别出高性能模型。它能高效探索大型建模空间，将搜索集中在高奖励区域，并提出在模型拟合和行为合理性之间取得平衡的帕累托前沿候选模型。", "conclusion": "这些发现突出了这种新颖的自适应、基于学习的框架在模型规格过程中的巨大潜力。", "translation": "我们引入了Delphos，这是一个深度强化学习框架，用于辅助离散选择模型规格过程。与将模型规格视为静态优化问题的传统方法不同，Delphos代表了一种范式转变：它将这一规格挑战框架化为一个序列决策问题，并将其形式化为马尔可夫决策过程。在这种设置中，一个代理通过选择一系列建模动作——例如选择变量、适应通用和特定替代方案的偏好参数、应用非线性变换以及包含与协变量的交互——并与一个建模环境交互来学习指定表现良好的模型候选者，该环境会估计每个候选者并返回奖励信号。具体而言，Delphos使用一个深度Q网络，该网络根据建模结果（例如对数似然）和行为预期（例如参数符号）接收延迟奖励，并将奖励分配给动作序列，以学习哪些建模决策能导致表现良好的候选者。我们在模拟和经验数据集上评估了Delphos，改变了建模空间的大小和奖励函数。为了评估代理在模型空间中导航的性能，我们分析了学习曲线、Q值分布、占用度量和帕累托前沿。我们的结果表明，即使没有先验领域知识，代理也能学习自适应地探索策略，以在搜索空间中识别表现良好的模型。它能高效探索大型建模空间，将其搜索集中在高奖励区域，并提出定义帕累托前沿的候选模型，这些模型平衡了模型拟合和行为合理性。这些发现突出了这种新颖的自适应、基于学习的框架在模型规格过程中的潜力。", "summary": "Delphos是一个新颖的深度强化学习框架，旨在辅助离散选择模型的规格过程。它将传统的静态优化问题转化为马尔可夫决策过程中的序列决策问题。通过使用深度Q网络，Delphos代理学习选择一系列建模动作，并根据模型性能和行为预期获得延迟奖励，从而高效地探索大型模型空间。实验结果表明，Delphos能够在没有先验知识的情况下找到高性能模型，集中搜索高奖励区域，并提出在模型拟合和行为合理性之间取得平衡的帕累托前沿候选模型，展现了其在模型规格中的巨大潜力。", "keywords": "离散选择模型, 强化学习, 模型规格, 深度Q网络, 序列决策", "comments": "这项工作通过将模型规格问题重新定义为序列决策问题，并引入深度强化学习方法，为离散选择模型规格提供了一个创新的、自适应的解决方案。其亮点在于能够高效探索复杂的模型空间，并在缺乏先验领域知识的情况下识别出高性能且行为合理的模型，这对于提高模型构建的自动化和效率具有重要意义。"}}
{"id": "2507.18732", "title": "Multi-Year Maintenance Planning for Large-Scale Infrastructure Systems: A Novel Network Deep Q-Learning Approach", "authors": ["Amir Fard", "Arnold X. -X. Yuan"], "categories": ["math.OC", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18732v1", "summary": "Infrastructure asset management is essential for sustaining the performance\nof public infrastructure such as road networks, bridges, and utility networks.\nTraditional maintenance and rehabilitation planning methods often face\nscalability and computational challenges, particularly for large-scale networks\nwith thousands of assets under budget constraints. This paper presents a novel\ndeep reinforcement learning (DRL) framework that optimizes asset management\nstrategies for large infrastructure networks. By decomposing the network-level\nMarkov Decision Process (MDP) into individual asset-level MDPs while using a\nunified neural network architecture, the proposed framework reduces\ncomputational complexity, improves learning efficiency, and enhances\nscalability. The framework directly incorporates annual budget constraints\nthrough a budget allocation mechanism, ensuring maintenance plans are both\noptimal and cost-effective. Through a case study on a large-scale pavement\nnetwork of 68,800 segments, the proposed DRL framework demonstrates significant\nimprovements over traditional methods like Progressive Linear Programming and\ngenetic algorithms, both in efficiency and network performance. This\nadvancement contributes to infrastructure asset management and the broader\napplication of reinforcement learning in complex, large-scale environments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18732v1", "cate": "math.OC", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "大规模基础设施系统的多年维护规划：一种新颖的网络深度Q学习方法", "tldr": "本文提出了一种新颖的深度强化学习（DRL）框架，用于优化大型基础设施网络的资产管理，解决了传统方法的可扩展性和计算挑战，并在一个大型路面网络案例研究中表现出显著优于传统方法的效率和性能。", "motivation": "基础设施资产管理对于维持公共基础设施的性能至关重要，但传统维护规划方法在处理受预算限制的大规模网络时面临可扩展性和计算挑战。", "method": "本文提出了一种新颖的深度强化学习（DRL）框架，用于优化大型基础设施网络的资产管理策略。该框架通过将网络级马尔可夫决策过程（MDP）分解为独立的资产级MDP，并使用统一的神经网络架构，以降低计算复杂性、提高学习效率和增强可扩展性。同时，它通过预算分配机制直接整合年度预算约束。", "result": "在一个包含68,800个路段的大规模路面网络案例研究中，所提出的DRL框架在效率和网络性能方面均显著优于传统的渐进线性规划和遗传算法。", "conclusion": "该研究推进了基础设施资产管理领域，并拓宽了强化学习在复杂、大规模环境中的应用。", "translation": "基础设施资产管理对于维持道路网络、桥梁和公用事业网络等公共基础设施的性能至关重要。传统的维护和修复规划方法经常面临可扩展性和计算挑战，特别是对于在预算限制下拥有数千个资产的大规模网络。本文提出了一种新颖的深度强化学习（DRL）框架，用于优化大型基础设施网络的资产管理策略。通过将网络级马尔可夫决策过程（MDP）分解为单独的资产级MDP，同时使用统一的神经网络架构，所提出的框架降低了计算复杂性，提高了学习效率，并增强了可扩展性。该框架通过预算分配机制直接纳入年度预算限制，确保维护计划既优化又具有成本效益。通过对一个包含68,800个路段的大规模路面网络的案例研究，所提出的DRL框架在效率和网络性能方面均显示出比渐进线性规划和遗传算法等传统方法的显著改进。这一进展有助于基础设施资产管理以及强化学习在复杂、大规模环境中的更广泛应用。", "summary": "本文介绍了一种创新的深度强化学习（DRL）框架，旨在解决大型基础设施系统维护规划中传统方法面临的可扩展性和计算效率问题。该框架通过将网络级决策过程分解为单个资产级过程，并利用统一的神经网络，有效降低了复杂性并提高了学习效率。它还直接整合了年度预算约束，确保维护计划的成本效益和优化。在对一个包含68,800个路段的大型路面网络的案例研究中，该DRL方法在效率和性能上均显著超越了传统规划技术，为基础设施资产管理和强化学习在复杂环境中的应用做出了贡献。", "keywords": "基础设施资产管理, 深度强化学习, 维护规划, 网络优化, 可扩展性", "comments": "这篇论文的创新点在于将深度强化学习应用于大规模基础设施的多年维护规划，有效地解决了传统方法在可扩展性和计算效率上的瓶颈。通过分解MDP和采用统一神经网络，该方法显著提升了处理复杂大型系统的能力，并直接考虑了预算约束，使其解决方案更具实用性。其在大型路面网络上的成功案例验证了其有效性，对资产管理领域具有重要意义。"}}
{"id": "2506.15135", "title": "Towards Bug-Free Distributed Go Programs", "authors": ["Zhengqun Koo"], "categories": ["cs.SE", "cs.LO", "cs.PL", "F.3.1; F.1.2"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      Version 1. this http URL . Dissertation", "url": "http://arxiv.org/abs/2506.15135v2", "summary": "Programmers of distributed systems need to reason about concurrency to avoid\nraces. However, reasoning about concurrency is difficult, and unexpected races\nshow up as bugs. Data race detection in shared memory systems is well-studied\n(dynamic data race detection [13], behavioral types [15], dynamic race\ndetection [31]). Similar to how a data race consists of reads and writes not\nrelated by happens-before at a shared memory location, a communication race\nconsists of receives and sends not related by happens-before on a shared\nchannel. Communication races are problematic: a receiver expects a specific\nmessage from a specific sender, but with a communication race, the receiver can\nreceive a message meant for another receiver, or not receive anything at all.\nIn this work, we describe a verification framework that can prove the absence\nof communication races for distributed programs that use a subset of the Go\nprogramming language, where synchronization is mainly achieved via message\npassing. We statically reason about how a distributed program executes, using a\nhappens-before order, extended to buffered and unbuffered channels.", "comment": "Version 1. B.Comp. Dissertation", "pdf_url": "http://arxiv.org/pdf/2506.15135v2", "cate": "cs.SE", "date": "2025-06-16", "updated": "2025-07-25", "AI": {"title_translation": "迈向无错误分布式 Go 程序", "tldr": "提出一个验证框架，通过静态推理和扩展的 happens-before 顺序，证明 Go 语言子集中的分布式程序没有通信竞态。", "motivation": "分布式系统中的并发推理困难，导致意外竞态成为错误。特别是通信竞态，它发生在共享通道上，可能导致消息错收或不接收，对程序的正确性造成严重影响。", "method": "描述了一个验证框架，该框架能够证明使用 Go 语言子集（主要通过消息传递进行同步）的分布式程序不存在通信竞态。该框架通过静态推理分布式程序的执行方式，并使用扩展到缓冲和非缓冲通道的 happens-before 顺序。", "result": "该框架能够证明分布式程序中不存在通信竞态。", "conclusion": "论文提出了一个有效的验证框架，通过静态分析 Go 语言分布式程序中的 happens-before 关系，解决了通信竞态问题，有助于构建更可靠的分布式系统。", "translation": "分布式系统的程序员需要对并发进行推理以避免竞态。然而，对并发进行推理很困难，意外的竞态会表现为错误。共享内存系统中的数据竞态检测已经得到了充分研究（动态数据竞态检测 [13]、行为类型 [15]、动态竞态检测 [31]）。类似于数据竞态由共享内存位置上不通过 happens-before 关系关联的读写操作组成，通信竞态由共享通道上不通过 happens-before 关系关联的接收和发送操作组成。通信竞态是成问题的：接收方期望从特定发送方接收特定消息，但如果存在通信竞态，接收方可能会收到本应发给另一个接收方的消息，或者根本收不到任何消息。在这项工作中，我们描述了一个验证框架，该框架可以证明使用 Go 编程语言子集的分布式程序不存在通信竞态，其中同步主要通过消息传递实现。我们使用 happens-before 顺序，并将其扩展到缓冲和非缓冲通道，静态地推理分布式程序的执行方式。", "summary": "本文提出了一个验证框架，旨在解决分布式 Go 程序中难以检测的通信竞态问题。该框架通过静态分析，利用扩展的 happens-before 顺序推理程序的并发执行，从而证明特定 Go 语言子集中的分布式程序不存在通信竞态，确保消息传递的正确性。", "keywords": "通信竞态, 分布式系统, Go 语言, 静态验证, happens-before", "comments": "这项工作通过专注于 Go 语言中特有的通信竞态，填补了现有数据竞态检测研究的空白。其创新之处在于将 happens-before 顺序扩展到缓冲和非缓冲通道，并进行静态验证，这对于提高分布式 Go 程序的可靠性具有重要意义。"}}
{"id": "2507.19204", "title": "Should Top-Down Clustering Affect Boundaries in Unsupervised Word Discovery?", "authors": ["Simon Malan", "Benjamin van Niekerk", "Herman Kamper"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      5 figures, 5 tables", "url": "http://arxiv.org/abs/2507.19204v2", "summary": "We investigate the problem of segmenting unlabeled speech into word-like\nunits and clustering these to create a lexicon. Prior work can be categorized\ninto two frameworks. Bottom-up methods first determine boundaries and then\ncluster the fixed segmented words into a lexicon. In contrast, top-down methods\nincorporate information from the clustered words to inform boundary selection.\nHowever, it is unclear whether top-down information is necessary to improve\nsegmentation. To explore this, we look at two similar approaches that differ in\nwhether top-down clustering informs boundary selection. Our simple bottom-up\nstrategy predicts word boundaries using the dissimilarity between adjacent\nself-supervised features, then clusters the resulting segments to construct a\nlexicon. Our top-down system is an updated version of the ES-KMeans dynamic\nprogramming method that iteratively uses K-means to update its boundaries. On\nthe five-language ZeroSpeech benchmarks, both approaches achieve comparable\nstate-of-the-art results, with the bottom-up system being nearly five times\nfaster. Through detailed analyses, we show that the top-down influence of\nES-KMeans can be beneficial (depending on factors like the candidate\nboundaries), but in many cases the simple bottom-up method performs just as\nwell. For both methods, we show that the clustering step is a limiting factor.\nTherefore, we recommend that future work focus on improved clustering\ntechniques and learning more discriminative word-like representations. Project\ncode repository: https://github.com/s-malan/prom-seg-clus.", "comment": "Submitted to the IEEE/ACM Transactions on Audio, Speech and Language\n  Processing", "pdf_url": "http://arxiv.org/pdf/2507.19204v2", "cate": "eess.AS", "date": "2025-07-25", "updated": "2025-07-28", "AI": {"title_translation": "顶层聚类应该影响无监督词汇发现中的边界吗？", "tldr": "本文研究无监督语音中的词汇发现，比较了顶层和底层方法对边界选择的影响。结果表明，简单的底层方法与顶层方法性能相当，但速度快近五倍。研究建议未来工作应关注改进聚类技术。", "motivation": "现有无监督词汇发现方法分为先确定边界再聚类的底层方法，和结合聚类信息选择边界的顶层方法。本文旨在探究顶层聚类信息是否对提高语音分割效果是必要的。", "method": "论文比较了两种方法：一种是简单的底层策略，通过相邻自监督特征的不相似性预测词边界，然后聚类；另一种是更新后的ES-KMeans动态规划顶层系统，迭代使用K-means更新边界。", "result": "在五种语言的ZeroSpeech基准测试中，两种方法都取得了可与现有最佳结果媲美的性能，其中底层系统速度快近五倍。分析表明，ES-KMeans的顶层影响有时有益，但在许多情况下，简单的底层方法表现同样好。对于两种方法，聚类步骤都是一个限制因素。", "conclusion": "顶层聚类对边界的影响可能是有益的，但并非总是必要的，简单的底层方法在无监督词汇发现中也能取得同样好的结果且速度更快。未来的工作应侧重于改进聚类技术和学习更具判别性的词汇表示。", "translation": "我们研究将未标记语音分割成类似词汇的单元并将其聚类以创建词典的问题。先前的工作可分为两种框架。自底向上方法首先确定边界，然后将固定的分割词汇聚类成词典。相比之下，自顶向下方法结合了来自已聚类词汇的信息来指导边界选择。然而，目前尚不清楚自顶向下信息是否对改进分割是必要的。为了探索这一点，我们研究了两种相似的方法，它们的不同之处在于自顶向下聚类是否影响边界选择。我们简单的自底向上策略使用相邻自监督特征之间的不相似性来预测词边界，然后聚类所得片段以构建词典。我们的自顶向下系统是ES-KMeans动态规划方法的更新版本，它迭代地使用K-means来更新其边界。在五种语言的ZeroSpeech基准测试中，两种方法都取得了可与现有最佳结果媲美的性能，其中自底向上系统速度快近五倍。通过详细分析，我们表明ES-KMeans的自顶向下影响可能是有益的（取决于候选边界等因素），但在许多情况下，简单的自底向上方法表现同样好。对于这两种方法，我们表明聚类步骤是一个限制因素。因此，我们建议未来的工作应侧重于改进聚类技术和学习更具判别性的类似词汇表示。项目代码库：https://github.com/s-malan/prom-seg-clus。", "summary": "本文探讨了无监督词汇发现中顶层聚类对语音边界选择的影响。研究比较了一种简单的底层方法和一种更新的顶层ES-KMeans方法。实验表明，底层方法在性能上与顶层方法相当，但在速度上快近五倍。论文指出，顶层影响并非总是必要，且聚类步骤是当前方法的瓶颈，建议未来研究关注改进聚类和词汇表示。", "keywords": "无监督词汇发现, 语音分割, 聚类, 自底向上, 自顶向下", "comments": "本文通过实证比较，挑战了顶层信息在无监督词汇发现中对边界选择的必要性，并揭示了简单底层方法的效率优势。其发现聚类是当前方法的限制因素，为未来研究指明了方向，具有重要的实践指导意义。"}}
{"id": "2507.19303", "title": "Identifying Fine-grained Forms of Populism in Political Discourse: A Case Study on Donald Trump's Presidential Campaigns", "authors": ["Ilias Chalkidis", "Stephanie Brandl", "Paris Aslanidis"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Pre-print", "url": "http://arxiv.org/abs/2507.19303v1", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of instruction-following tasks, yet their grasp of nuanced social\nscience concepts remains underexplored. This paper examines whether LLMs can\nidentify and classify fine-grained forms of populism, a complex and contested\nconcept in both academic and media debates. To this end, we curate and release\nnovel datasets specifically designed to capture populist discourse. We evaluate\na range of pre-trained (large) language models, both open-weight and\nproprietary, across multiple prompting paradigms. Our analysis reveals notable\nvariation in performance, highlighting the limitations of LLMs in detecting\npopulist discourse. We find that a fine-tuned RoBERTa classifier vastly\noutperforms all new-era instruction-tuned LLMs, unless fine-tuned.\nAdditionally, we apply our best-performing model to analyze campaign speeches\nby Donald Trump, extracting valuable insights into his strategic use of\npopulist rhetoric. Finally, we assess the generalizability of these models by\nbenchmarking them on campaign speeches by European politicians, offering a lens\ninto cross-context transferability in political discourse analysis. In this\nsetting, we find that instruction-tuned LLMs exhibit greater robustness on\nout-of-domain data.", "comment": "Pre-print", "pdf_url": "http://arxiv.org/pdf/2507.19303v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "识别政治话语中民粹主义的细粒度形式：以唐纳德·特朗普总统竞选为例", "tldr": "本文探讨了大型语言模型（LLMs）在识别和分类细粒度民粹主义方面的能力。研究发现，LLMs在此任务上表现不一，且微调的RoBERTa分类器通常优于未经微调的LLMs。此外，模型被应用于分析特朗普的竞选演讲以及欧洲政治家的演讲，以评估其泛化能力。", "motivation": "大型语言模型（LLMs）在处理细致的社会科学概念方面的能力尚未得到充分探索，尤其是在识别和分类民粹主义这种复杂且有争议的概念上。", "method": "研究团队策划并发布了专门用于捕捉民粹主义话语的新数据集。他们评估了多种预训练（大型）语言模型（包括开源和专有模型），并采用了多种提示范式。此外，他们还使用了一个微调的RoBERTa分类器进行比较。最后，将表现最佳的模型应用于分析唐纳德·特朗普的竞选演讲和欧洲政治家的竞选演讲。", "result": "分析显示，LLMs在检测民粹主义话语方面的表现存在显著差异，这突显了它们的局限性。研究发现，除非进行微调，否则微调的RoBERTa分类器在性能上大大优于所有新时代的指令调优LLMs。在跨上下文可转移性方面，指令调优的LLMs在域外数据上表现出更强的鲁棒性。", "conclusion": "大型语言模型在识别细粒度民粹主义方面存在局限性，但经过微调的模型（如RoBERTa）可以取得更好的效果。指令调优的LLMs在处理域外数据时表现出更好的泛化能力，这对于政治话语分析具有重要意义。", "translation": "大型语言模型（LLMs）在广泛的指令遵循任务中展现出卓越的能力，然而它们对细致的社会科学概念的理解仍未得到充分探索。本文研究了LLMs是否能够识别和分类细粒度的民粹主义，这是一个在学术和媒体辩论中都复杂且有争议的概念。为此，我们策划并发布了专门用于捕捉民粹主义话语的新数据集。我们评估了一系列预训练（大型）语言模型，包括开源和专有模型，并采用了多种提示范式。我们的分析揭示了性能的显著差异，突显了LLMs在检测民粹主义话语方面的局限性。我们发现，除非经过微调，否则微调的RoBERTa分类器在性能上大大优于所有新时代的指令调优LLMs。此外，我们应用我们表现最佳的模型分析了唐纳德·特朗普的竞选演讲，从而获得了关于他战略性使用民粹主义修辞的宝贵见解。最后，我们通过在欧洲政治家的竞选演讲上进行基准测试，评估了这些模型的泛化能力，为政治话语分析中的跨上下文可转移性提供了一个视角。在这种情况下，我们发现指令调优的LLMs在域外数据上表现出更强的鲁棒性。", "summary": "本研究探讨了大型语言模型（LLMs）在识别政治话语中细粒度民粹主义方面的能力。论文构建了新的民粹主义话语数据集，并评估了多种预训练LLMs。结果表明，LLMs在检测民粹主义方面存在局限性，而微调的RoBERTa分类器表现优异。此外，研究将最佳模型应用于分析唐纳德·特朗普和欧洲政治家的竞选演讲，发现指令调优的LLMs在跨领域数据上表现出更好的鲁棒性，为政治话语分析提供了新见解。", "keywords": "大型语言模型, 民粹主义, 政治话语, RoBERTa, 特朗普", "comments": "该论文创新性地探讨了LLMs在识别复杂社会科学概念（如细粒度民粹主义）方面的潜力与局限性。其贡献在于创建了新的数据集，并对比了不同LLM架构和微调策略的性能，揭示了微调对性能的关键影响。此外，通过对特朗普和欧洲政治家演讲的案例分析，该研究提供了实证见解，并评估了模型的跨上下文泛化能力，对于计算社会科学和政治学研究具有重要意义。"}}
{"id": "2506.04555", "title": "Enhancing Frequency for Single Image Super-Resolution with Learnable Separable Kernels", "authors": ["Heng Tian"], "categories": ["cs.CV", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.04555v2", "summary": "Existing approaches often enhance the performance of single-image\nsuper-resolution (SISR) methods by incorporating auxiliary structures, such as\nspecialized loss functions, to indirectly boost the quality of low-resolution\nimages. In this paper, we propose a plug-and-play module called Learnable\nSeparable Kernels (LSKs), which are formally rank-one matrices designed to\ndirectly enhance image frequency components. We begin by explaining why LSKs\nare particularly suitable for SISR tasks from a frequency perspective. Baseline\nmethods incorporating LSKs demonstrate a significant reduction of over 60\\% in\nboth the number of parameters and computational requirements. This reduction is\nachieved through the decomposition of LSKs into orthogonal and mergeable\none-dimensional kernels. Additionally, we perform an interpretable analysis of\nthe feature maps generated by LSKs. Visualization results reveal the capability\nof LSKs to enhance image frequency components effectively. Extensive\nexperiments show that incorporating LSKs not only reduces the number of\nparameters and computational load but also improves overall model performance.\nMoreover, these experiments demonstrate that models utilizing LSKs exhibit\nsuperior performance, particularly as the upscaling factor increases.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.04555v2", "cate": "cs.CV", "date": "2025-06-05", "updated": "2025-07-25", "AI": {"title_translation": "使用可学习可分离核增强单图像超分辨率的频率", "tldr": "本文提出可学习可分离核（LSKs）模块，直接增强单图像超分辨率（SISR）的图像频率成分。LSKs能显著减少参数和计算量（超60%），同时提升模型性能，尤其适用于高放大倍数。", "motivation": "现有单图像超分辨率（SISR）方法常通过引入辅助结构（如特殊损失函数）来间接提升低分辨率图像质量，本文旨在提出一种直接增强图像频率成分的方法。", "method": "本文提出一种即插即用模块——可学习可分离核（LSKs），它们是形式上的秩一矩阵，旨在直接增强图像频率成分。通过将LSKs分解为正交且可合并的一维核，实现了参数和计算需求的显著降低。", "result": "将LSKs整合到基线方法中，参数和计算需求均减少了60%以上。LSKs生成的特征图可视化结果表明其能有效增强图像频率成分。大量实验表明，LSKs不仅减少了参数和计算负载，还提升了整体模型性能，并且在放大倍数增加时，采用LSKs的模型表现出更优越的性能。", "conclusion": "可学习可分离核（LSKs）模块能够直接增强图像频率成分，在单图像超分辨率（SISR）任务中，它不仅显著减少了模型的参数和计算负担，而且有效提升了整体性能，尤其在高放大倍数下表现更佳。", "translation": "现有方法通常通过整合辅助结构（如专门的损失函数）来提高单图像超分辨率（SISR）方法的性能，以间接提升低分辨率图像的质量。在本文中，我们提出了一种即插即用模块，称为可学习可分离核（LSKs），它们是形式上的秩一矩阵，旨在直接增强图像频率成分。我们首先从频率角度解释了为什么LSKs特别适合SISR任务。整合LSKs的基线方法在参数数量和计算要求方面都显著减少了60%以上。这种减少是通过将LSKs分解为正交且可合并的一维核来实现的。此外，我们对LSKs生成的特征图进行了可解释性分析。可视化结果揭示了LSKs有效增强图像频率成分的能力。大量实验表明，整合LSKs不仅减少了参数和计算负载，还提高了整体模型性能。此外，这些实验表明，利用LSKs的模型表现出优越的性能，特别是在放大倍数增加时。", "summary": "本文提出了一种名为可学习可分离核（LSKs）的即插即用模块，用于单图像超分辨率（SISR），旨在直接增强图像频率成分，区别于现有方法通过辅助结构间接提升性能。LSKs被设计为秩一矩阵，并可分解为一维核，从而使模型参数和计算量减少了60%以上。实验证明，LSKs能有效增强图像频率成分，不仅降低了模型复杂度，还显著提升了整体性能，尤其在高放大倍数下表现更优。", "keywords": "单图像超分辨率, 频率增强, 可学习可分离核, 参数减少, 计算效率", "comments": "本文的创新点在于提出了可学习可分离核（LSKs）模块，直接从频率域增强图像，为SISR提供了一种新颖且高效的解决方案。该方法通过核分解显著减少了模型参数和计算量，同时保持甚至提升了性能，这对于资源受限的应用场景具有重要意义。其即插即用的特性也增加了其实用性。"}}
{"id": "2507.18763", "title": "Diffusion-FS: Multimodal Free-Space Prediction via Diffusion for Autonomous Driving", "authors": ["Keshav Gupta", "Tejas S. Stanley", "Pranjal Paul", "Arun K. Singh", "K. Madhava Krishna"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 7 figures, IROS 2025", "url": "http://arxiv.org/abs/2507.18763v1", "summary": "Drivable Free-space prediction is a fundamental and crucial problem in\nautonomous driving. Recent works have addressed the problem by representing the\nentire non-obstacle road regions as the free-space. In contrast our aim is to\nestimate the driving corridors that are a navigable subset of the entire road\nregion. Unfortunately, existing corridor estimation methods directly assume a\nBEV-centric representation, which is hard to obtain. In contrast, we frame\ndrivable free-space corridor prediction as a pure image perception task, using\nonly monocular camera input. However such a formulation poses several\nchallenges as one doesn't have the corresponding data for such free-space\ncorridor segments in the image. Consequently, we develop a novel\nself-supervised approach for free-space sample generation by leveraging future\nego trajectories and front-view camera images, making the process of visual\ncorridor estimation dependent on the ego trajectory. We then employ a diffusion\nprocess to model the distribution of such segments in the image. However, the\nexisting binary mask-based representation for a segment poses many limitations.\nTherefore, we introduce ContourDiff, a specialized diffusion-based architecture\nthat denoises over contour points rather than relying on binary mask\nrepresentations, enabling structured and interpretable free-space predictions.\nWe evaluate our approach qualitatively and quantitatively on both nuScenes and\nCARLA, demonstrating its effectiveness in accurately predicting safe multimodal\nnavigable corridors in the image.", "comment": "8 pages, 7 figures, IROS 2025", "pdf_url": "http://arxiv.org/pdf/2507.18763v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "Diffusion-FS：基于扩散的多模态自由空间预测用于自动驾驶", "tldr": "该论文提出Diffusion-FS，一种通过自监督学习和基于扩散的ContourDiff模型，仅使用单目图像输入来预测自动驾驶中可导航自由空间走廊的方法，解决了现有方法对BEV表示的依赖和二值掩码的局限性。", "motivation": "可驾驶自由空间预测是自动驾驶中一个基础且关键的问题。现有方法通常将整个非障碍道路区域表示为自由空间，而本文的目标是估计作为整个道路区域可导航子集的可驾驶走廊。不幸的是，现有走廊估计方法直接假设难以获得的BEV（鸟瞰图）中心表示。此外，将可驾驶自由空间走廊预测构建为纯图像感知任务（仅使用单目相机输入）会面临挑战，因为图像中没有此类自由空间走廊段的相应数据。现有的基于二值掩码的段表示也存在许多局限性。", "method": "本文将可驾驶自由空间走廊预测构建为纯图像感知任务，仅使用单目相机输入。为解决数据缺乏问题，本文开发了一种新颖的自监督方法，通过利用未来的自我轨迹和前视图相机图像来生成自由空间样本，使视觉走廊估计依赖于自我轨迹。随后，采用扩散过程来建模图像中此类段的分布。为了克服现有二值掩码表示的局限性，本文引入了ContourDiff，这是一种专门的基于扩散的架构，它对轮廓点进行去噪，而不是依赖二值掩码表示，从而实现结构化和可解释的自由空间预测。", "result": "本文在nuScenes和CARLA数据集上对所提出的方法进行了定性和定量评估，证明了其在图像中准确预测安全多模态可导航走廊的有效性。", "conclusion": "Diffusion-FS方法通过创新性地将可驾驶自由空间走廊预测作为纯图像感知任务，并结合自监督数据生成和基于轮廓点去噪的ContourDiff扩散模型，能够有效且准确地从单目图像中预测出安全的可导航走廊，克服了现有BEV依赖和二值掩码表示的局限性。", "translation": "可驾驶自由空间预测是自动驾驶中一个基础且关键的问题。最近的工作通过将整个非障碍道路区域表示为自由空间来解决这个问题。相比之下，我们的目标是估计作为整个道路区域可导航子集的可驾驶走廊。不幸的是，现有走廊估计方法直接假设难以获得的BEV（鸟瞰图）中心表示。相比之下，我们将可驾驶自由空间走廊预测构建为纯图像感知任务，仅使用单目相机输入。然而，这种构建方式带来了几个挑战，因为图像中没有此类自由空间走廊段的相应数据。因此，我们开发了一种新颖的自监督方法，通过利用未来的自我轨迹和前视图相机图像来生成自由空间样本，使视觉走廊估计依赖于自我轨迹。然后，我们采用扩散过程来建模图像中此类段的分布。然而，现有基于二值掩码的段表示存在许多局限性。因此，我们引入了ContourDiff，这是一种专门的基于扩散的架构，它对轮廓点进行去噪，而不是依赖二值掩码表示，从而实现结构化和可解释的自由空间预测。我们在nuScenes和CARLA上对所提出的方法进行了定性和定量评估，证明了其在图像中准确预测安全多模态可导航走廊的有效性。", "summary": "该论文提出了Diffusion-FS，一种针对自动驾驶中可驾驶自由空间走廊预测的新方法。与现有关注整个自由空间或依赖难以获取的BEV表示的方法不同，Diffusion-FS将走廊预测视为纯粹的单目图像感知任务。为了解决图像数据中缺乏相应走廊段的问题，该方法开发了一种新颖的自监督数据生成策略，利用未来自我轨迹和前视图图像。随后，通过扩散过程建模这些图像段的分布。针对传统二值掩码表示的局限性，论文引入了ContourDiff，一个专门的扩散架构，它直接对轮廓点进行去噪，从而实现结构化且可解释的自由空间预测。在nuScenes和CARLA上的评估表明，该方法能有效准确地预测图像中的安全多模态可导航走廊。", "keywords": "扩散模型, 自由空间预测, 自动驾驶, 单目相机, 轮廓点", "comments": "该论文的创新点在于将可驾驶走廊预测任务从BEV依赖转向纯单目图像感知，这对于实际部署的自动驾驶系统具有重要意义。其次，提出的自监督数据生成方法有效地解决了图像中自由空间走廊标签缺失的问题。最突出的是ContourDiff架构，它通过直接去噪轮廓点而非二值掩码，提供了一种更结构化和可解释的预测方式，克服了传统表示的局限性，为未来自由空间预测模型提供了新的思路。"}}
{"id": "2311.10207", "title": "Stella Nera: A Differentiable Maddness-Based Hardware Accelerator for Efficient Approximate Matrix Multiplication", "authors": ["Jannis Schönleber", "Lukas Cavigelli", "Matteo Perotti", "Luca Benini", "Renzo Andri"], "categories": ["cs.AR", "cs.CV", "cs.LG", "stat.ML"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      Accepted as full paper at IEEE Computer Society Annual Symposium on VLSI (ISVLSI) 2025", "url": "http://arxiv.org/abs/2311.10207v2", "summary": "Artificial intelligence has surged in recent years, with advancements in\nmachine learning rapidly impacting nearly every area of life. However, the\ngrowing complexity of these models has far outpaced advancements in available\nhardware accelerators, leading to significant computational and energy demands,\nprimarily due to matrix multiplications, which dominate the compute workload.\nMaddness (i.e., Multiply-ADDitioN-lESS) presents a hash-based version of\nproduct quantization, which renders matrix multiplications into lookups and\nadditions, eliminating the need for multipliers entirely. We present Stella\nNera, the first Maddness-based accelerator achieving an energy efficiency of\n161 TOp/s/W@0.55V, 25x better than conventional MatMul accelerators due to its\nsmall components and reduced computational complexity. We further enhance\nMaddness with a differentiable approximation, allowing for gradient-based\nfine-tuning and achieving an end-to-end performance of 92.5% Top-1 accuracy on\nCIFAR-10.", "comment": "Accepted as full paper at IEEE Computer Society Annual Symposium on\n  VLSI (ISVLSI) 2025", "pdf_url": "http://arxiv.org/pdf/2311.10207v2", "cate": "cs.AR", "date": "2023-11-16", "updated": "2025-07-25", "AI": {"title_translation": "Stella Nera: 一种基于可微分Maddness的高效近似矩阵乘法硬件加速器", "tldr": "Stella Nera是一种基于Maddness的硬件加速器，用于高效近似矩阵乘法，显著提高了能效并保持了高精度。", "motivation": "现有AI模型复杂度增长远超硬件加速器发展，导致矩阵乘法计算和能耗需求巨大。", "method": "提出Stella Nera，第一个基于Maddness（一种将矩阵乘法转换为查找和加法的哈希方法）的硬件加速器，并引入可微分近似以支持梯度微调。", "result": "Stella Nera实现了161 TOp/s/W@0.55V的能效，比传统矩阵乘法加速器高25倍；在CIFAR-10上实现了92.5%的Top-1端到端精度。", "conclusion": "Stella Nera通过基于Maddness的硬件加速器设计和可微分近似，显著提高了近似矩阵乘法的能效，同时保持了高精度，有效解决了AI计算的能耗和性能瓶颈。", "translation": "人工智能近年来迅猛发展，机器学习的进步迅速影响着生活的几乎每个领域。然而，这些模型日益增长的复杂性远超现有硬件加速器的发展，导致巨大的计算和能源需求，这主要归因于主导计算工作负载的矩阵乘法。Maddness（即Multiply-ADDitioN-lESS）提出了一种基于哈希的产品量化版本，它将矩阵乘法转化为查找和加法，完全消除了对乘法器的需求。我们提出了Stella Nera，这是第一个基于Maddness的加速器，实现了161 TOp/s/W@0.55V的能效，由于其小组件和降低的计算复杂度，比传统矩阵乘法加速器好25倍。我们通过可微分近似进一步增强了Maddness，允许基于梯度的微调，并在CIFAR-10上实现了92.5%的Top-1端到端准确率。", "summary": "本文介绍了Stella Nera，一种创新的基于Maddness的硬件加速器，专为高效近似矩阵乘法设计。Maddness通过将乘法运算转换为查找和加法，显著降低了计算复杂性。Stella Nera在此基础上实现了卓越的能效，比传统加速器高出25倍，同时引入了可微分近似以支持模型微调，并在图像分类任务上展示了高精度。", "keywords": "硬件加速器, 矩阵乘法, Maddness, 能效, 深度学习", "comments": "该论文的创新点在于首次将Maddness（一种无乘法器的近似矩阵乘法方法）应用于硬件加速器设计，并引入了可微分特性，使其能够与深度学习的梯度优化流程兼容。这对于解决当前AI模型日益增长的计算和能耗挑战具有重要意义，尤其是在边缘计算和资源受限设备上。其显著的能效提升是其重要性所在。"}}
{"id": "2505.01482", "title": "Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers", "authors": ["Alice Rueda", "Mohammed S. Hassan", "Argyrios Perivolaris", "Bazen G. Teferra", "Reza Samavi", "Sirisha Rambhatla", "Yuqi Wu", "Yanbo Zhang", "Bo Cao", "Divya Sharma", "Sridhar Krishnan", "Venkat Bhat"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.01482v2", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding, reasoning, and problem-solving across various\ndomains. However, their ability to perform complex, multi-step reasoning\ntask-essential for applications in science, medicine, and law-remains an area\nof active investigation. This paper examines the reasoning capabilities of\ncontemporary LLMs, analyzing their strengths, limitations, and potential for\nimprovement. The study uses prompt engineering techniques on the Graduate-Level\nGoogleProof Q&A (GPQA) dataset to assess the scientific reasoning of GPT-4o.\nFive popular prompt engineering techniques and two tailored promptings were\ntested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot\nCoT, self-ask, self-consistency, decomposition, and multipath promptings. Our\nfindings indicate that while LLMs exhibit emergent reasoning abilities, they\noften rely on pattern recognition rather than true logical inference, leading\nto inconsistencies in complex problem-solving. The results indicated that\nself-consistency outperformed the other prompt engineering technique with an\naccuracy of 52.99%, followed by direct answer (52.23%). Zero-shot CoT (50%)\noutperformed multipath (48.44%), decomposition (47.77%), self-ask (46.88%), and\nCoT (43.75%). Self-consistency performed the second worst in explaining the\nanswers. Simple techniques such as direct answer, CoT, and zero-shot CoT have\nthe best scientific reasoning. We propose a research agenda aimed at bridging\nthese gaps by integrating structured reasoning frameworks, hybrid AI\napproaches, and human-in-the-loop methodologies. By critically evaluating the\nreasoning mechanisms of LLMs, this paper contributes to the ongoing discourse\non the future of artificial general intelligence and the development of more\nrobust, trustworthy AI systems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.01482v2", "cate": "cs.AI", "date": "2025-05-02", "updated": "2025-07-25", "AI": {"title_translation": "通过提示和模型对答案的解释来理解大型语言模型的科学推理", "tldr": "本文深入研究了大型语言模型（LLM）的科学推理能力，通过在GPQA数据集上测试GPT-4o并评估多种提示工程技术。研究发现LLM的推理能力虽已显现，但常依赖模式识别而非逻辑推理，导致复杂问题解决中的不一致。文章还提出了未来改进LLM推理能力的研究方向。", "motivation": "大型语言模型（LLM）在复杂、多步骤推理任务（如科学、医学和法律）中的能力仍然是一个活跃的研究领域，本文旨在分析其优势、局限性和改进潜力。", "method": "本研究使用GoogleProof Q&A (GPQA) 数据集，通过提示工程技术评估GPT-4o的科学推理能力。测试了五种流行提示工程技术和两种定制提示：基线直接回答（零样本）、思维链（CoT）、零样本CoT、自我提问、自我一致性、分解和多路径提示。", "result": "研究发现，LLM表现出涌现推理能力，但通常依赖模式识别而非真正的逻辑推理，导致在复杂问题解决中出现不一致。自我一致性在所有提示工程技术中表现最佳，准确率为52.99%，其次是直接回答（52.23%）。零样本CoT（50%）优于多路径（48.44%）、分解（47.77%）、自我提问（46.88%）和CoT（43.75%）。自我一致性在解释答案方面表现第二差。直接回答、CoT和零样本CoT等简单技术具有最佳的科学推理能力。", "conclusion": "本文批判性评估了LLM的推理机制，提出通过整合结构化推理框架、混合AI方法和人机协作方法来弥合当前差距，以促进通用人工智能和更健壮、可信AI系统的发展。", "translation": "大型语言模型（LLM）在自然语言理解、推理和跨领域问题解决方面表现出卓越的能力。然而，它们执行复杂、多步骤推理任务（这对于科学、医学和法律应用至关重要）的能力仍然是一个活跃的研究领域。本文研究了当代LLM的推理能力，分析了它们的优势、局限性和改进潜力。该研究使用GoogleProof Q&A (GPQA) 数据集上的提示工程技术来评估GPT-4o的科学推理能力。测试了五种流行的提示工程技术和两种定制提示：基线直接回答（零样本）、思维链（CoT）、零样本CoT、自我提问、自我一致性、分解和多路径提示。我们的发现表明，虽然LLM表现出涌现的推理能力，但它们通常依赖模式识别而非真正的逻辑推理，导致在复杂问题解决中出现不一致。结果表明，自我一致性在其他提示工程技术中表现最佳，准确率为52.99%，其次是直接回答（52.23%）。零样本CoT（50%）优于多路径（48.44%）、分解（47.77%）、自我提问（46.88%）和CoT（43.75%）。自我一致性在解释答案方面表现第二差。直接回答、CoT和零样本CoT等简单技术具有最佳的科学推理能力。我们提出了一项研究议程，旨在通过整合结构化推理框架、混合AI方法和人机协作方法来弥合这些差距。通过批判性评估LLM的推理机制，本文为通用人工智能的未来和更健壮、可信AI系统的发展做出了贡献。", "summary": "本文深入探讨了大型语言模型（LLM）的科学推理能力，通过在GoogleProof Q&A (GPQA) 数据集上使用多种提示工程技术对GPT-4o进行评估。研究发现，尽管LLM展现出推理能力，但其主要依赖模式识别而非深层逻辑推理，导致在复杂问题解决中出现不一致。在测试的提示技术中，自我一致性在准确率方面表现最佳，而简单的提示技术在科学推理方面表现良好。论文最后提出了通过整合结构化推理框架、混合AI方法和人机协作来提升LLM推理能力的未来研究方向，强调其对通用人工智能发展的贡献。", "keywords": "大型语言模型, 科学推理, 提示工程, GPT-4o, GPQA", "comments": "这篇论文通过系统地比较不同提示工程技术对LLM科学推理能力的影响，提供了关于LLM当前推理机制的宝贵见解。其创新点在于不仅评估了推理结果，还触及了模型解释能力，并明确指出LLM可能更多依赖模式识别而非深层逻辑推理，这对于理解LLM的局限性至关重要。研究结果为未来如何设计更有效的提示策略和构建更可靠的AI系统提供了方向，尤其是在需要高精度逻辑推理的科学、医学等领域。"}}
{"id": "2507.18875", "title": "Neural Correction Operator: A Reliable and Fast Approach for Electrical Impedance Tomography", "authors": ["Amit Bhat", "Ke Chen", "Chunmei Wang"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18875v1", "summary": "Electrical Impedance Tomography (EIT) is a non-invasive medical imaging\nmethod that reconstructs electrical conductivity mediums from boundary\nvoltage-current measurements, but its severe ill-posedness renders direct\noperator learning with neural networks unreliable. We propose the neural\ncorrection operator framework, which learns the inverse map as a composition of\ntwo operators: a reconstruction operator using L-BFGS optimization with limited\niterations to obtain an initial estimate from measurement data and a correction\noperator implemented with deep learning models to reconstruct the true media\nfrom this initial guess. We explore convolutional neural network architectures\nand conditional diffusion models as alternative choices for the correction\noperator. We evaluate the neural correction operator by comparing with L-BFGS\nmethods as well as neural operators and conditional diffusion models that\ndirectly learn the inverse map over several benchmark datasets. Our numerical\nexperiments demonstrate that our approach achieves significantly better\nreconstruction quality compared to both iterative methods and direct neural\noperator learning methods with the same architecture. The proposed framework\nalso exhibits robustness to measurement noise while achieving substantial\ncomputational speedup compared to conventional methods. The neural correction\noperator provides a general paradigm for approaching neural operator learning\nin severely ill-posed inverse problems.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18875v1", "cate": "math.NA", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "神经校正算子：一种可靠快速的电阻抗断层扫描方法", "tldr": "本文提出了神经校正算子框架，通过结合有限迭代的L-BFGS优化和深度学习校正，解决了电阻抗断层扫描（EIT）中神经网络直接算子学习的不可靠性问题，实现了显著更好的重建质量、对噪声的鲁棒性以及计算速度的提升。", "motivation": "电阻抗断层扫描（EIT）是一种非侵入式医学成像方法，但其严重的病态性导致神经网络直接学习逆映射不可靠。", "method": "本文提出了神经校正算子框架，将逆映射学习分解为两个算子的组合：一个重建算子（使用有限迭代的L-BFGS优化获取初始估计）和一个校正算子（使用深度学习模型，如卷积神经网络或条件扩散模型，从初始估计中重建真实介质）。", "result": "数值实验表明，与迭代方法和直接神经网络算子学习方法相比，所提出的方法在重建质量上显著更优，对测量噪声表现出鲁棒性，并且计算速度大幅提升。", "conclusion": "神经校正算子为解决严重病态逆问题中的神经算子学习提供了一个通用的范式。", "translation": "电阻抗断层扫描（EIT）是一种非侵入式医学成像方法，它从边界电压-电流测量中重建电导率介质，但其严重的病态性使得神经网络直接进行算子学习不可靠。我们提出了神经校正算子框架，该框架将逆映射学习为两个算子的组合：一个重建算子，使用有限迭代的L-BFGS优化从测量数据中获取初始估计；一个校正算子，通过深度学习模型实现，从该初始猜测中重建真实介质。我们探索了卷积神经网络架构和条件扩散模型作为校正算子的替代选择。我们通过与L-BFGS方法以及直接学习逆映射的神经算子和条件扩散模型在多个基准数据集上进行比较，评估了神经校正算子。我们的数值实验表明，与迭代方法和具有相同架构的直接神经算子学习方法相比，我们的方法实现了显著更好的重建质量。所提出的框架还表现出对测量噪声的鲁棒性，同时与传统方法相比，计算速度大幅提升。神经校正算子为解决严重病态逆问题中的神经算子学习提供了一个通用的范式。", "summary": "本文提出了一种名为神经校正算子（Neural Correction Operator）的新框架，旨在解决电阻抗断层扫描（EIT）中因其严重病态性导致的神经网络直接学习逆映射不可靠的问题。该框架将逆映射分解为两部分：首先通过有限迭代的L-BFGS优化获得初始重建，然后利用深度学习模型（如CNN或条件扩散模型）作为校正算子，将初始估计精炼为真实介质。实验结果表明，与传统的迭代方法和直接神经算子学习方法相比，该方法在重建质量上显著提升，同时对噪声具有鲁棒性并大幅加快了计算速度，为处理严重病态逆问题中的神经算子学习提供了一个通用范式。", "keywords": "电阻抗断层扫描, 神经校正算子, 逆问题, 深度学习, 病态性", "comments": "该论文提出了一种创新的混合方法，结合了传统优化算法的初始估计能力和深度学习模型的强大校正能力，有效解决了EIT等病态逆问题中直接端到端深度学习的局限性。其分步学习的范式增强了模型的可靠性和性能，同时保持了计算效率，对于未来解决类似逆问题具有重要的指导意义。"}}
{"id": "2506.20933", "title": "Lower Bounds on the Size of Markov Equivalence Classes", "authors": ["Erik Jahn", "Frederick Eberhardt", "Leonard J. Schulman"], "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.20933v2", "summary": "Causal discovery algorithms typically recover causal graphs only up to their\nMarkov equivalence classes unless additional parametric assumptions are made.\nThe sizes of these equivalence classes reflect the limits of what can be\nlearned about the underlying causal graph from purely observational data. Under\nthe assumptions of acyclicity, causal sufficiency, and a uniform model prior,\nMarkov equivalence classes are known to be small on average. In this paper, we\nshow that this is no longer the case when any of these assumptions is relaxed.\nSpecifically, we prove exponentially large lower bounds for the expected size\nof Markov equivalence classes in three settings: sparse random directed acyclic\ngraphs, uniformly random acyclic directed mixed graphs, and uniformly random\ndirected cyclic graphs.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.20933v2", "cate": "stat.ML", "date": "2025-06-26", "updated": "2025-07-25", "AI": {"title_translation": "马尔可夫等价类大小的下界", "tldr": "放松因果发现算法的常见假设后，马尔可夫等价类的大小不再平均较小，而是可能呈指数级增长。", "motivation": "了解因果发现算法在纯观测数据下能学到什么，以及放松某些假设后这些限制如何变化。马尔可夫等价类的大小反映了这些限制。", "method": "通过在三种不同设置（稀疏随机有向无环图、均匀随机无环有向混合图、均匀随机有向循环图）中放松因果发现的常见假设（无环性、因果充分性、均匀模型先验），证明了马尔可夫等价类期望大小的指数级下界。", "result": "证明了当放松无环性、因果充分性或均匀模型先验等假设时，马尔可夫等价类的期望大小会呈指数级增长。", "conclusion": "放松因果发现的常见假设会导致马尔可夫等价类的大小显著增加，这表明在这些情况下从纯观测数据中学习底层因果图的难度更大。", "translation": "因果发现算法通常只能恢复因果图直到其马尔可夫等价类，除非做出额外的参数假设。这些等价类的大小反映了仅从纯观测数据中可以了解底层因果图的限制。在无环性、因果充分性和均匀模型先验的假设下，马尔可夫等价类平均而言是小的。在本文中，我们表明当放松这些假设中的任何一个时，情况就不再如此。具体而言，我们在三种设置中证明了马尔可夫等价类期望大小的指数级下界：稀疏随机有向无环图、均匀随机无环有向混合图和均匀随机有向循环图。", "summary": "本文研究了因果发现中马尔可夫等价类的大小。传统观点认为在无环性、因果充分性和均匀模型先验等假设下，这些等价类平均较小。然而，本文证明了当放松这些假设中的任何一个时，马尔可夫等价类的期望大小会呈指数级增长。具体来说，研究在稀疏随机有向无环图、均匀随机无环有向混合图和均匀随机有向循环图三种情况下，给出了马尔可夫等价类大小的指数级下界。这表明在更一般的设置下，从观测数据中进行精确因果发现的难度显著增加。", "keywords": "马尔可夫等价类, 因果发现, 下界, 有向无环图, 混合图", "comments": "这项研究揭示了在放松标准假设后，因果发现的固有局限性会变得更加显著。其创新之处在于量化了这些局限性，通过证明指数级下界，强调了在更复杂的图模型中，从纯观测数据中推断精确因果结构面临的挑战。这对于理解和开发更鲁棒的因果发现算法具有重要意义。"}}
{"id": "2507.18813", "title": "Scale-Consistent Learning for Partial Differential Equations", "authors": ["Zongyi Li", "Samuel Lanthaler", "Catherine Deng", "Michael Chen", "Yixuan Wang", "Kamyar Azizzadenesheli", "Anima Anandkumar"], "categories": ["cs.LG", "cs.NA", "math.NA"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18813v1", "summary": "Machine learning (ML) models have emerged as a promising approach for solving\npartial differential equations (PDEs) in science and engineering. Previous ML\nmodels typically cannot generalize outside the training data; for example, a\ntrained ML model for the Navier-Stokes equations only works for a fixed\nReynolds number ($Re$) on a pre-defined domain. To overcome these limitations,\nwe propose a data augmentation scheme based on scale-consistency properties of\nPDEs and design a scale-informed neural operator that can model a wide range of\nscales. Our formulation leverages the facts: (i) PDEs can be rescaled, or more\nconcretely, a given domain can be re-scaled to unit size, and the parameters\nand the boundary conditions of the PDE can be appropriately adjusted to\nrepresent the original solution, and (ii) the solution operators on a given\ndomain are consistent on the sub-domains. We leverage these facts to create a\nscale-consistency loss that encourages matching the solutions evaluated on a\ngiven domain and the solution obtained on its sub-domain from the rescaled PDE.\nSince neural operators can fit to multiple scales and resolutions, they are the\nnatural choice for incorporating scale-consistency loss during training of\nneural PDE solvers. We experiment with scale-consistency loss and the\nscale-informed neural operator model on the Burgers' equation, Darcy Flow,\nHelmholtz equation, and Navier-Stokes equations. With scale-consistency, the\nmodel trained on $Re$ of 1000 can generalize to $Re$ ranging from 250 to 10000,\nand reduces the error by 34% on average of all datasets compared to baselines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18813v1", "cate": "cs.LG", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "偏微分方程的尺度一致性学习", "tldr": "提出一种尺度一致性学习方法和尺度感知神经算子，解决了ML模型求解PDEs时泛化能力差的问题，显著提高了模型在不同尺度上的泛化能力和准确性。", "motivation": "现有的机器学习模型在求解偏微分方程时，通常无法泛化到训练数据之外的情况，例如对特定雷诺数或预定义域的模型无法适用于其他条件。", "method": "提出了一种基于偏微分方程尺度一致性特性的数据增强方案，并设计了一个尺度感知的神经算子。该方法利用了以下事实：(i) 偏微分方程可以重新缩放，即将给定域重新缩放为单位大小，并适当调整方程参数和边界条件以表示原始解；(ii) 给定域上的解算子在子域上保持一致。通过这些事实，论文创建了一个尺度一致性损失函数，鼓励匹配在给定域上评估的解与从重新缩放的偏微分方程的子域上获得的解。神经算子因其能够适应多个尺度和分辨率而被选为融合尺度一致性损失的自然选择。", "result": "实验结果表明，该模型在雷诺数($Re$)为1000的训练数据上，能够泛化到$Re$范围从250到10000的情况，并且与基线模型相比，在所有数据集上平均将误差降低了34%。", "conclusion": "通过引入尺度一致性学习和尺度感知神经算子，显著提升了机器学习模型在求解偏微分方程时的泛化能力，使其能够有效地应对不同尺度范围的问题，并提高了预测精度。", "translation": "机器学习（ML）模型已成为科学和工程中解决偏微分方程（PDEs）的一种有前景的方法。以前的ML模型通常无法泛化到训练数据之外；例如，一个针对纳维-斯托克斯方程训练的ML模型仅适用于预定义域上的固定雷诺数（$Re$）。为了克服这些限制，我们提出了一种基于偏微分方程尺度一致性特性的数据增强方案，并设计了一个能够模拟各种尺度的尺度感知神经算子。我们的公式利用了以下事实：(i) 偏微分方程可以重新缩放，更具体地说，给定域可以重新缩放到单位大小，并且偏微分方程的参数和边界条件可以适当调整以表示原始解，以及 (ii) 给定域上的解算子在子域上是一致的。我们利用这些事实创建了一个尺度一致性损失，鼓励匹配在给定域上评估的解与从重新缩放的偏微分方程的子域上获得的解。由于神经算子可以适应多个尺度和分辨率，它们是训练神经偏微分方程求解器时引入尺度一致性损失的自然选择。我们使用尺度一致性损失和尺度感知神经算子模型对 Burgers 方程、达西流、亥姆霍兹方程和纳维-斯托克斯方程进行了实验。通过尺度一致性，在$Re$为1000上训练的模型可以泛化到$Re$范围从250到10000，并且与基线相比，在所有数据集上平均将误差降低了34%。", "summary": "本文提出了一种针对偏微分方程（PDEs）的尺度一致性学习方法，旨在解决现有机器学习模型在求解PDEs时泛化能力不足的问题。通过利用PDEs的尺度一致性特性，研究者设计了一种数据增强方案和一个尺度感知神经算子。该方法引入了一个尺度一致性损失函数，确保模型在不同尺度和子域上的解保持一致。实验结果表明，该方法显著提升了模型在不同雷诺数范围内的泛化能力，并相较于基线模型平均降低了34%的误差。", "keywords": "偏微分方程, 机器学习, 尺度一致性, 神经算子, 泛化能力", "comments": "这篇论文的创新点在于提出了基于偏微分方程固有尺度一致性特性的数据增强和损失函数设计，有效解决了机器学习模型在求解PDEs时泛化能力差的关键限制。通过引入尺度一致性损失和尺度感知神经算子，模型能够更好地适应不同尺度和参数范围，显著提升了实际应用中的鲁棒性和准确性，为PDEs的机器学习求解提供了新的思路和重要进展。"}}
{"id": "2507.18864", "title": "Deadline-Aware Joint Task Scheduling and Offloading in Mobile Edge Computing Systems", "authors": ["Ngoc Hung Nguyen", "Van-Dinh Nguyen", "Anh Tuan Nguyen", "Nguyen Van Thieu", "Hoang Nam Nguyen", "Symeon Chatzinotas"], "categories": ["cs.DC", "cs.CC", "C.2.4; I.2.8"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      14 pages, 13 figures. Accepted for publication in IEEE Internet of Things Journal (JIOT)", "url": "http://arxiv.org/abs/2507.18864v1", "summary": "The demand for stringent interactive quality-of-service has intensified in\nboth mobile edge computing (MEC) and cloud systems, driven by the imperative to\nimprove user experiences. As a result, the processing of computation-intensive\ntasks in these systems necessitates adherence to specific deadlines or\nachieving extremely low latency. To optimize task scheduling performance,\nexisting research has mainly focused on reducing the number of late jobs whose\ndeadlines are not met. However, the primary challenge with these methods lies\nin the total search time and scheduling efficiency. In this paper, we present\nthe optimal job scheduling algorithm designed to determine the optimal task\norder for a given set of tasks. In addition, users are enabled to make informed\ndecisions for offloading tasks based on the information provided by servers.\nThe details of performance analysis are provided to show its optimality and low\ncomplexity with the linearithmic time O(nlogn), where $n$ is the number of\ntasks. To tackle the uncertainty of the randomly arriving tasks, we further\ndevelop an online approach with fast outage detection that achieves rapid\nacceptance times with time complexity of O(n). Extensive numerical results are\nprovided to demonstrate the effectiveness of the proposed algorithm in terms of\nthe service ratio and scheduling cost.", "comment": "14 pages, 13 figures. Accepted for publication in IEEE Internet of\n  Things Journal (JIOT)", "pdf_url": "http://arxiv.org/pdf/2507.18864v1", "cate": "cs.DC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "移动边缘计算系统中面向截止时间的联合任务调度与卸载", "tldr": "本文提出了一种最优任务调度算法，用于移动边缘计算系统中的任务调度和卸载，旨在满足严格的截止时间要求，并解决了现有方法搜索时间长和调度效率低的问题。该算法具有低复杂度，并针对随机到达的任务提供了在线解决方案。", "motivation": "移动边缘计算（MEC）和云系统中对严格交互式服务质量的需求日益增加，要求计算密集型任务必须满足特定截止时间或实现极低延迟。然而，现有研究主要关注减少未满足截止时间的迟到任务数量，但其主要挑战在于总搜索时间和调度效率。", "method": "本文提出了一种最优作业调度算法，用于确定给定任务集的最优任务顺序。此外，用户可以根据服务器提供的信息做出明智的任务卸载决策。针对随机到达的任务，进一步开发了一种具有快速中断检测的在线方法。", "result": "所提出的最优调度算法具有最优性和线性对数时间复杂度O(nlogn)。开发的在线方法实现了快速接受时间，时间复杂度为O(n)。大量的数值结果表明，该算法在服务比和调度成本方面是有效的。", "conclusion": "本文提出的最优任务调度算法和在线方法，有效解决了移动边缘计算系统中计算密集型任务的截止时间要求和调度效率问题，并展现出优异的性能和低复杂度。", "translation": "在移动边缘计算（MEC）和云系统中，为了改善用户体验，对严格的交互式服务质量的需求日益增强。因此，这些系统中计算密集型任务的处理需要遵守特定的截止时间或实现极低的延迟。为了优化任务调度性能，现有研究主要集中于减少未满足截止时间的迟到任务数量。然而，这些方法的主要挑战在于总搜索时间和调度效率。在本文中，我们提出了一种最优作业调度算法，旨在为给定任务集确定最优任务顺序。此外，用户能够根据服务器提供的信息做出明智的任务卸载决策。本文提供了性能分析的详细信息，以展示其最优性和低复杂度，其时间复杂度为线性对数O(nlogn)，其中n是任务数量。为了应对随机到达任务的不确定性，我们进一步开发了一种具有快速中断检测的在线方法，该方法以O(n)的时间复杂度实现了快速接受时间。本文提供了大量的数值结果，以证明所提出算法在服务比和调度成本方面的有效性。", "summary": "本文针对移动边缘计算（MEC）系统中计算密集型任务的截止时间要求和调度效率挑战，提出了一种最优作业调度算法，以确定最优任务顺序。该算法支持用户基于服务器信息进行智能卸载决策，并针对随机到达任务设计了低复杂度的在线处理方法。性能分析显示，该算法具有最优性、O(nlogn)的时间复杂度，在线方法为O(n)，并通过数值结果验证了其在服务比和调度成本方面的有效性。", "keywords": "移动边缘计算, 任务调度, 卸载, 截止时间, 在线算法", "comments": "本文提出了一种创新的最优任务调度算法，旨在解决移动边缘计算中严格的截止时间要求和调度效率问题。其创新之处在于不仅考虑了最优任务排序，还结合了基于服务器信息的智能卸载决策，并针对动态环境下的随机任务提出了高效的在线解决方案。低复杂度（O(nlogn)和O(n)）是其重要优势，表明该方法在实际应用中具有可行性。"}}
{"id": "2507.18723", "title": "SCORE-SET: A dataset of GuitarPro files for Music Phrase Generation and Sequence Learning", "authors": ["Vishakh Begari"], "categories": ["cs.SD", "cs.LG", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      6 pages, 6 figures", "url": "http://arxiv.org/abs/2507.18723v1", "summary": "A curated dataset of Guitar Pro tablature files (.gp5 format), tailored for\ntasks involving guitar music generation, sequence modeling, and\nperformance-aware learning is provided. The dataset is derived from MIDI notes\nin MAESTRO and GiantMIDI which have been adapted into rhythm guitar tracks.\nThese tracks are further processed to include a variety of expression settings\ntypical of guitar performance, such as bends, slides, vibrato, and palm muting,\nto better reflect the nuances of real-world guitar playing.", "comment": "6 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.18723v1", "cate": "cs.SD", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "SCORE-SET：一个用于音乐短语生成和序列学习的GuitarPro文件数据集", "tldr": "SCORE-SET是一个精选的Guitar Pro制谱文件数据集，专为吉他音乐生成、序列建模和性能感知学习任务而设计，其中包含各种吉他演奏表情设置。", "motivation": "该论文的动机是为吉他音乐生成、序列建模和性能感知学习任务提供一个专门定制的数据集，以更好地反映真实吉他演奏的细微差别。", "method": "该数据集SCORE-SET来源于MAESTRO和GiantMIDI中的MIDI音符，这些音符被改编成节奏吉他音轨。这些音轨经过进一步处理，包含了多种典型的吉他演奏表情设置，例如推弦、滑弦、颤音和切音。", "result": "该研究提供了一个名为SCORE-SET的精选Guitar Pro制谱文件（.gp5格式）数据集，该数据集专为吉他音乐生成、序列建模和性能感知学习任务量身定制。", "conclusion": "Not mentioned in abstract", "translation": "提供了一个精选的Guitar Pro制谱文件（.gp5格式）数据集，专为吉他音乐生成、序列建模和性能感知学习任务而定制。该数据集来源于MAESTRO和GiantMIDI中的MIDI音符，这些音符已被改编成节奏吉他音轨。这些音轨经过进一步处理，包含了多种典型的吉他演奏表情设置，例如推弦、滑弦、颤音和切音，以更好地反映真实吉他演奏的细微差别。", "summary": "SCORE-SET是一个新颖的Guitar Pro制谱文件数据集，旨在支持吉他音乐生成、序列建模和性能感知学习。它通过将现有MIDI数据转换为节奏吉他音轨并加入推弦、滑弦、颤音和切音等丰富的演奏表情，从而捕捉真实吉他演奏的细微之处。", "keywords": "GuitarPro数据集, 音乐生成, 序列学习, 吉他演奏, SCORE-SET", "comments": "该论文的创新之处在于其创建了一个包含丰富吉他演奏表情设置的专用数据集，这对于提升吉他音乐生成和序列学习的真实感和表现力具有重要意义。它填补了现有数据集在吉他演奏细节方面的空白。"}}
{"id": "2502.11809", "title": "Geometric Origins of Bias in Deep Neural Networks: A Human Visual System Perspective", "authors": ["Yanbiao Ma", "Bowei Liu", "Andi Zhang"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.11809v4", "summary": "Bias formation in deep neural networks (DNNs) remains a critical yet poorly\nunderstood challenge, influencing both fairness and reliability in artificial\nintelligence systems. Inspired by the human visual system, which decouples\nobject manifolds through hierarchical processing to achieve object recognition,\nwe propose a geometric analysis framework linking the geometric complexity of\nclass-specific perceptual manifolds in DNNs to model bias. Our findings reveal\nthat differences in geometric complexity can lead to varying recognition\ncapabilities across categories, introducing biases. To support this analysis,\nwe present the Perceptual-Manifold-Geometry library, designed for calculating\nthe geometric properties of perceptual manifolds. The toolkit has been\ndownloaded and installed over 4,500 times. This work provides a novel geometric\nperspective on bias formation in modern learning systems and lays a theoretical\nfoundation for developing more equitable and robust artificial intelligence.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.11809v4", "cate": "cs.CV", "date": "2025-02-17", "updated": "2025-07-25", "AI": {"title_translation": "深度神经网络中偏差的几何起源：一个人眼视觉系统的视角", "tldr": "该论文受人眼视觉系统启发，提出了一个几何分析框架，将深度神经网络中类别特定感知流形的几何复杂性与模型偏差联系起来，以解释偏差的形成。", "motivation": "深度神经网络（DNNs）中偏差的形成是一个关键但知之甚少的挑战，它影响着人工智能系统的公平性和可靠性。", "method": "受人眼视觉系统通过分层处理解耦物体流形以实现物体识别的启发，我们提出了一个几何分析框架，将DNNs中特定类别感知流形的几何复杂性与模型偏差联系起来。为支持此分析，我们推出了Perceptual-Manifold-Geometry库，用于计算感知流形的几何特性。", "result": "我们的发现揭示，几何复杂性的差异可能导致不同类别之间识别能力的差异，从而引入偏差。此外，我们发布的工具包已被下载和安装超过4,500次。", "conclusion": "这项工作为现代学习系统中偏差的形成提供了一个新颖的几何视角，并为开发更公平和更强大的人工智能奠定了理论基础。", "translation": "深度神经网络（DNNs）中偏差的形成仍然是一个关键但知之甚少的挑战，影响着人工智能系统的公平性和可靠性。受人眼视觉系统通过分层处理解耦物体流形以实现物体识别的启发，我们提出了一个几何分析框架，将DNNs中特定类别感知流形的几何复杂性与模型偏差联系起来。我们的发现表明，几何复杂性的差异可能导致不同类别之间识别能力的差异，从而引入偏差。为了支持这项分析，我们推出了Perceptual-Manifold-Geometry库，旨在计算感知流形的几何特性。该工具包已被下载和安装超过4,500次。这项工作为现代学习系统中偏差的形成提供了一个新颖的几何视角，并为开发更公平和更强大的人工智能奠定了理论基础。", "summary": "本文通过提出一个几何分析框架，探讨了深度神经网络中偏差的起源。受人眼视觉系统的启发，该框架将特定类别感知流形的几何复杂性与模型偏差关联起来。研究发现，几何复杂性的差异可能导致不同类别间识别能力的差异，从而引入偏差。作者还介绍了Perceptual-Manifold-Geometry库以支持此分析，为理解和缓解人工智能偏差提供了一个新颖的几何视角。", "keywords": "深度神经网络, 偏差, 几何复杂性, 感知流形, 人眼视觉系统", "comments": "该论文从生物视觉的角度为深度神经网络偏差提供了一个新颖的几何视角，这种方法具有创新性。它还提供了一个实用的工具（Perceptual-Manifold-Geometry库）来支持其理论框架，增强了其影响力。这项工作对于开发更公平和更强大的人工智能至关重要。"}}
{"id": "2506.22531", "title": "Preserve Anything: Controllable Image Synthesis with Object Preservation", "authors": ["Prasen Kumar Sharma", "Neeraj Matiyali", "Siddharth Srivastava", "Gaurav Sharma"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025 (main conference)", "url": "http://arxiv.org/abs/2506.22531v2", "summary": "We introduce \\textit{Preserve Anything}, a novel method for controlled image\nsynthesis that addresses key limitations in object preservation and semantic\nconsistency in text-to-image (T2I) generation. Existing approaches often fail\n(i) to preserve multiple objects with fidelity, (ii) maintain semantic\nalignment with prompts, or (iii) provide explicit control over scene\ncomposition. To overcome these challenges, the proposed method employs an\nN-channel ControlNet that integrates (i) object preservation with size and\nplacement agnosticism, color and detail retention, and artifact elimination,\n(ii) high-resolution, semantically consistent backgrounds with accurate\nshadows, lighting, and prompt adherence, and (iii) explicit user control over\nbackground layouts and lighting conditions. Key components of our framework\ninclude object preservation and background guidance modules, enforcing lighting\nconsistency and a high-frequency overlay module to retain fine details while\nmitigating unwanted artifacts. We introduce a benchmark dataset consisting of\n240K natural images filtered for aesthetic quality and 18K 3D-rendered\nsynthetic images with metadata such as lighting, camera angles, and object\nrelationships. This dataset addresses the deficiencies of existing benchmarks\nand allows a complete evaluation. Empirical results demonstrate that our method\nachieves state-of-the-art performance, significantly improving feature-space\nfidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining\ncompetitive aesthetic quality. We also conducted a user study to demonstrate\nthe efficacy of the proposed work on unseen benchmark and observed a remarkable\nimprovement of $\\sim25\\%$, $\\sim19\\%$, $\\sim13\\%$, and $\\sim14\\%$ in terms of\nprompt alignment, photorealism, the presence of AI artifacts, and natural\naesthetics over existing works.", "comment": "Accepted at ICCV 2025 (main conference)", "pdf_url": "http://arxiv.org/pdf/2506.22531v2", "cate": "cs.CV", "date": "2025-06-27", "updated": "2025-07-25", "AI": {"title_translation": "保留一切：可控图像合成与物体保留", "tldr": "该论文提出“保留一切”方法，通过N通道ControlNet实现可控图像合成，有效保留物体、保持语义一致性并提供场景控制，达到SOTA性能。", "motivation": "现有文本到图像（T2I）生成方法在以下方面存在局限性：(i) 难以高保真地保留多个物体；(ii) 难以保持与提示词的语义对齐；(iii) 无法对场景构成提供明确控制。", "method": "该方法采用一个N通道ControlNet，整合了：(i) 物体保留功能，支持大小和位置无关性、颜色和细节保留以及伪影消除；(ii) 高分辨率、语义一致的背景，具有准确的阴影、光照和提示词符合性；(iii) 用户对背景布局和光照条件的明确控制。其关键组件包括物体保留和背景引导模块（用于强制光照一致性）以及高频叠加模块。此外，还引入了一个包含24万张自然图像和1.8万张3D渲染合成图像的基准数据集。", "result": "该方法实现了最先进的性能，显著提升了特征空间保真度（FID 15.26）和语义对齐（CLIP-S 32.85），同时保持了有竞争力的美学质量。用户研究表明，在提示词对齐、照片真实感、AI伪影和自然美学方面，相较于现有工作分别有约25%、19%、13%和14%的显著改进。", "conclusion": "该论文提出了“保留一切”这一新颖的可控图像合成方法，有效解决了文本到图像（T2I）生成中物体保留和语义一致性的关键局限，并通过实证研究和用户反馈验证了其最先进的性能。", "translation": "我们引入了“保留一切”，这是一种新颖的可控图像合成方法，解决了文本到图像（T2I）生成中物体保留和语义一致性的关键局限。现有方法常常未能：(i) 高保真地保留多个物体，(ii) 保持与提示词的语义对齐，或(iii) 提供对场景构成的明确控制。为了克服这些挑战，所提出的方法采用了一个N通道ControlNet，该网络整合了：(i) 物体保留功能，具有大小和位置无关性、颜色和细节保留以及伪影消除；(ii) 高分辨率、语义一致的背景，具有准确的阴影、光照和提示词符合性；(iii) 用户对背景布局和光照条件的明确控制。我们框架的关键组件包括物体保留和背景引导模块，它们强制执行光照一致性，以及一个高频叠加模块，用于保留精细细节同时减轻不必要的伪影。我们引入了一个基准数据集，包含24万张经过美学质量筛选的自然图像和1.8万张带有光照、相机角度和物体关系等元数据的3D渲染合成图像。该数据集解决了现有基准的不足，并允许进行完整的评估。实证结果表明，我们的方法实现了最先进的性能，显著提高了特征空间保真度（FID 15.26）和语义对齐（CLIP-S 32.85），同时保持了有竞争力的美学质量。我们还进行了一项用户研究，以证明所提工作在未见基准上的有效性，并观察到在提示词对齐、照片真实感、AI伪影存在和自然美学方面，相较于现有工作分别有约25%、约19%、约13%和约14%的显著改进。", "summary": "“保留一切”是一种新颖的可控文本到图像合成方法，旨在解决物体保留、语义一致性和场景控制方面的现有局限。该方法采用N通道ControlNet，集成了强大的物体保留功能、语义一致的高分辨率背景以及明确的用户控制。其框架包含物体保留、背景引导和高频叠加模块。为全面评估，研究还引入了一个新的基准数据集。实证结果和用户研究表明，该方法在保真度、语义对齐、照片真实感和美学质量方面均达到最先进水平，显著优于现有方法。", "keywords": "物体保留, 可控图像合成, 文本到图像, ControlNet, 语义一致性", "comments": "该论文的创新之处在于其N通道ControlNet及其集成模块，实现了强大的物体保留、语义一致性和明确的场景控制，这些都是文本到图像生成中的主要挑战。此外，引入一个全面且新的基准数据集也为未来的研究做出了重要贡献。"}}
{"id": "2507.18877", "title": "A Survey on Methodological Approaches to Collaborative Embodiment in Virtual Reality", "authors": ["Hongyu Zhou", "Yihao Dong", "Masahiko Inami", "Zhanna Sarsenbayeva", "Anusha Withana"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18877v1", "summary": "The application and implementation of collaborative embodiment in virtual\nreality (VR) are a critical aspect of the computer science landscape, aiming to\nenhance multi-user interaction and teamwork in immersive environments. A\nnotable and enduring area of collaborative embodiment research focuses on\napproaches that enable multiple users to share control, interact, and\ninvestigate scenarios involving supernumerary arms in virtual spaces. In this\nsurvey, we will present an extensive overview of the methodologies employed in\nthe past decade to enable collaboration in VR environments, particularly\nthrough embodiment. Using the PRISMA guidelines, we plan to analyze the study\ndetails from over 137 relevant research papers. Through this analysis, a\ncritical assessment of the effectiveness of these methodologies will be\nconducted, highlighting current challenges and limitations in implementing\ncollaborative embodiment in VR. Lastly, we discuss potential future research\ndirections and opportunities for enhancing collaboration embodiment in virtual\nenvironments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18877v1", "cate": "cs.HC", "date": "2025-06-11", "updated": "2025-06-11", "AI": {"title_translation": "虚拟现实中协同具身化方法论综述", "tldr": "对虚拟现实中协同具身化方法论的综述。", "motivation": "虚拟现实（VR）中协同具身化的应用和实施是计算机科学领域的一个关键方面，旨在增强沉浸式环境中的多用户交互和团队协作。", "method": "本综述将对过去十年中用于实现虚拟现实环境中协作（特别是通过具身化）的方法论进行广泛概述。计划使用PRISMA指南分析137篇以上相关研究论文的详细信息。", "result": "本综述旨在对这些方法论的有效性进行批判性评估，突出当前在虚拟现实中实施协同具身化所面临的挑战和局限性。", "conclusion": "本综述将讨论未来潜在的研究方向以及增强虚拟环境中协同具身化的机会。", "translation": "虚拟现实（VR）中协同具身化的应用和实施是计算机科学领域的一个关键方面，旨在增强沉浸式环境中的多用户交互和团队协作。协同具身化研究的一个显著且持久的领域侧重于使多个用户能够共享控制、交互和调查涉及虚拟空间中额外手臂的场景的方法。在本综述中，我们将对过去十年中用于实现虚拟现实环境中协作（特别是通过具身化）的方法论进行广泛概述。计划使用PRISMA指南分析137篇以上相关研究论文的详细信息。通过这项分析，将对这些方法论的有效性进行批判性评估，突出当前在虚拟现实中实施协同具身化所面临的挑战和局限性。最后，我们将讨论未来潜在的研究方向以及增强虚拟环境中协同具身化的机会。", "summary": "本综述旨在对过去十年中虚拟现实中协同具身化的方法论进行全面概述。它计划使用PRISMA指南分析超过137篇相关研究论文，以批判性评估方法论的有效性，突出挑战，并探讨未来的研究方向。", "keywords": "协同具身化, 虚拟现实, 综述, 方法论, 多用户交互", "comments": "这是一篇重要的综述性论文，通过系统性方法（PRISMA指南，分析137+篇论文）对虚拟现实中协同具身化这一关键领域进行了深入梳理。它不仅总结了现有方法，还指出了当前挑战和未来研究方向，对于推动该领域的发展具有重要价值。"}}
{"id": "2507.19102", "title": "Distilling a Small Utility-Based Passage Selector to Enhance Retrieval-Augmented Generation", "authors": ["Hengran Zhang", "Keping Bi", "Jiafeng Guo", "Jiaming Zhang", "Shuaiqiang Wang", "Dawei Yin", "Xueqi Cheng"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures", "url": "http://arxiv.org/abs/2507.19102v1", "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating retrieved information. Standard retrieval process prioritized\nrelevance, focusing on topical alignment between queries and passages. In\ncontrast, in RAG, the emphasis has shifted to utility, which considers the\nusefulness of passages for generating accurate answers. Despite empirical\nevidence showing the benefits of utility-based retrieval in RAG, the high\ncomputational cost of using LLMs for utility judgments limits the number of\npassages evaluated. This restriction is problematic for complex queries\nrequiring extensive information. To address this, we propose a method to\ndistill the utility judgment capabilities of LLMs into smaller, more efficient\nmodels. Our approach focuses on utility-based selection rather than ranking,\nenabling dynamic passage selection tailored to specific queries without the\nneed for fixed thresholds. We train student models to learn pseudo-answer\ngeneration and utility judgments from teacher LLMs, using a sliding window\nmethod that dynamically selects useful passages. Our experiments demonstrate\nthat utility-based selection provides a flexible and cost-effective solution\nfor RAG, significantly reducing computational costs while improving answer\nquality. We present the distillation results using Qwen3-32B as the teacher\nmodel for both relevance ranking and utility-based selection, distilled into\nRankQwen1.7B and UtilityQwen1.7B. Our findings indicate that for complex\nquestions, utility-based selection is more effective than relevance ranking in\nenhancing answer generation performance. We will release the relevance ranking\nand utility-based selection annotations for the MS MARCO dataset, supporting\nfurther research in this area.", "comment": "9 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.19102v1", "cate": "cs.IR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "蒸馏小型基于效用的段落选择器以增强检索增强生成", "tldr": "为了解决大型语言模型在检索增强生成（RAG）中进行效用判断的计算成本问题，本文提出了一种将LLM的效用判断能力蒸馏到更小、更高效模型的方法，并通过实验证明了其在降低成本和提高答案质量方面的有效性，尤其对于复杂查询。", "motivation": "尽管经验证据表明基于效用的检索在检索增强生成（RAG）中具有优势，但使用大型语言模型（LLMs）进行效用判断的计算成本很高，这限制了评估的段落数量，对于需要大量信息的复杂查询来说，这是一个问题。", "method": "我们提出了一种将LLMs的效用判断能力蒸馏到更小、更高效模型的方法。该方法侧重于基于效用的选择而非排名，实现动态的段落选择。我们训练学生模型，使其通过滑动窗口方法从教师LLMs（如Qwen3-32B）学习伪答案生成和效用判断，动态选择有用段落，并将其蒸馏为RankQwen1.7B和UtilityQwen1.7B。", "result": "实验证明，基于效用的选择为RAG提供了一种灵活且经济高效的解决方案，显著降低了计算成本，同时提高了答案质量。对于复杂问题，基于效用的选择在增强答案生成性能方面比相关性排名更有效。", "conclusion": "将大型语言模型的效用判断能力蒸馏到小型模型中，可以为检索增强生成提供一个成本效益高且灵活的解决方案，尤其在处理复杂查询时，基于效用的选择比传统的相关性排名更有效。", "translation": "检索增强生成（RAG）通过整合检索到的信息来增强大型语言模型（LLMs）。标准的检索过程优先考虑相关性，侧重于查询和段落之间的主题一致性。相比之下，在RAG中，重点已转向效用，即考虑段落对于生成准确答案的有用性。尽管经验证据表明基于效用的检索在RAG中具有优势，但使用LLMs进行效用判断的高计算成本限制了评估的段落数量。这种限制对于需要大量信息的复杂查询来说是个问题。为了解决这个问题，我们提出了一种将LLMs的效用判断能力蒸馏到更小、更高效模型的方法。我们的方法侧重于基于效用的选择而非排名，从而实现无需固定阈值即可根据特定查询动态选择段落。我们训练学生模型，使其通过滑动窗口方法从教师LLMs学习伪答案生成和效用判断，该方法动态选择有用段落。我们的实验表明，基于效用的选择为RAG提供了一种灵活且经济高效的解决方案，显著降低了计算成本，同时提高了答案质量。我们展示了使用Qwen3-32B作为教师模型进行相关性排名和基于效用选择的蒸馏结果，分别蒸馏到RankQwen1.7B和UtilityQwen1.7B。我们的发现表明，对于复杂问题，基于效用的选择在增强答案生成性能方面比相关性排名更有效。我们将发布MS MARCO数据集的相关性排名和基于效用选择的注释，以支持该领域的进一步研究。", "summary": "本文提出了一种解决检索增强生成（RAG）中大型语言模型（LLMs）进行效用判断计算成本高昂问题的方法。通过将LLMs的效用判断能力蒸馏到更小、更高效的模型中，该研究实现了动态的、基于效用的段落选择，而非传统的相关性排名。实验结果表明，这种蒸馏方法显著降低了计算成本，同时提高了答案质量，尤其对于复杂查询，基于效用的选择比相关性排名更有效。研究还发布了相关数据集以促进未来研究。", "keywords": "检索增强生成, 效用判断, 模型蒸馏, 段落选择, 大型语言模型", "comments": "这项研究的创新之处在于，它通过模型蒸馏解决了大型语言模型在RAG中进行效用判断的计算效率问题。通过将LLM的复杂判断能力转移到更小的模型中，它为RAG提供了一个实用的、成本效益高的解决方案，特别是在处理需要精确信息选择的复杂查询时。这种方法对于推动RAG的实际应用具有重要意义，因为它克服了当前部署LLM所面临的主要障碍之一。"}}
{"id": "2507.19141", "title": "DASH: 4D Hash Encoding with Self-Supervised Decomposition for Real-Time Dynamic Scene Rendering", "authors": ["Jie Chen", "Zhangchi Hu", "Peixi Wu", "Huyue Zhu", "Hebei Li", "Xiaoyan Sun"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19141v1", "summary": "Dynamic scene reconstruction is a long-term challenge in 3D vision. Existing\nplane-based methods in dynamic Gaussian splatting suffer from an unsuitable\nlow-rank assumption, causing feature overlap and poor rendering quality.\nAlthough 4D hash encoding provides an explicit representation without low-rank\nconstraints, directly applying it to the entire dynamic scene leads to\nsubstantial hash collisions and redundancy. To address these challenges, we\npresent DASH, a real-time dynamic scene rendering framework that employs 4D\nhash encoding coupled with self-supervised decomposition. Our approach begins\nwith a self-supervised decomposition mechanism that separates dynamic and\nstatic components without manual annotations or precomputed masks. Next, we\nintroduce a multiresolution 4D hash encoder for dynamic elements, providing an\nexplicit representation that avoids the low-rank assumption. Finally, we\npresent a spatio-temporal smoothness regularization strategy to mitigate\nunstable deformation artifacts. Experiments on real-world datasets demonstrate\nthat DASH achieves state-of-the-art dynamic rendering performance, exhibiting\nenhanced visual quality at real-time speeds of 264 FPS on a single 4090 GPU.\nCode: https://github.com/chenj02/DASH.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19141v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "DASH：基于自监督分解的4D哈希编码，用于实时动态场景渲染", "tldr": "DASH提出了一种结合自监督分解的4D哈希编码方法，实现了实时动态场景的高质量渲染。", "motivation": "现有的动态高斯溅射平面方法存在不合适的低秩假设，导致特征重叠和渲染质量差。直接将4D哈希编码应用于整个动态场景会导致大量的哈希冲突和冗余。", "method": "DASH是一个实时动态场景渲染框架，采用4D哈希编码与自监督分解相结合。首先，通过自监督分解机制分离动态和静态组件，无需手动标注。其次，为动态元素引入多分辨率4D哈希编码器，提供避免低秩假设的显式表示。最后，提出时空平滑正则化策略以减轻不稳定的变形伪影。", "result": "DASH在真实世界数据集上实现了最先进的动态渲染性能，在单张4090 GPU上以264 FPS的实时速度展示了增强的视觉质量。", "conclusion": "DASH通过结合自监督分解和4D哈希编码，成功解决了动态场景重建中的挑战，实现了高质量的实时渲染。", "translation": "动态场景重建是3D视觉领域的一个长期挑战。现有动态高斯溅射中的基于平面的方法存在不合适的低秩假设，导致特征重叠和渲染质量差。虽然4D哈希编码提供了没有低秩约束的显式表示，但直接将其应用于整个动态场景会导致大量的哈希冲突和冗余。为了解决这些挑战，我们提出了DASH，一个实时动态场景渲染框架，它采用了4D哈希编码与自监督分解相结合。我们的方法首先采用自监督分解机制，在没有手动标注或预计算掩码的情况下分离动态和静态组件。接下来，我们为动态元素引入了一个多分辨率4D哈希编码器，提供了一个避免低秩假设的显式表示。最后，我们提出了一个时空平滑正则化策略，以减轻不稳定的变形伪影。在真实世界数据集上的实验表明，DASH实现了最先进的动态渲染性能，在单张4090 GPU上以264 FPS的实时速度展示了增强的视觉质量。代码：https://github.com/chenj02/DASH。", "summary": "DASH提出了一种创新的实时动态场景渲染框架，通过结合4D哈希编码和自监督分解来解决现有方法的局限性。它首先无监督地将动态和静态组件分离，然后为动态部分使用多分辨率4D哈希编码以避免低秩假设，并引入时空平滑正则化。实验证明DASH在视觉质量和实时性能（264 FPS）上均达到SOTA水平。", "keywords": "4D哈希编码, 自监督分解, 动态场景渲染, 实时性能, 高斯溅射", "comments": "该论文通过引入自监督分解和优化的4D哈希编码，有效地解决了动态场景重建中常见的低秩假设问题和哈希冲突。其创新点在于无需手动标注即可分离动静态组件，并结合时空平滑性，显著提升了渲染质量和实时性，是动态场景渲染领域的重要进展。"}}
{"id": "2502.03032", "title": "Analyze Feature Flow to Enhance Interpretation and Steering in Language Models", "authors": ["Daniil Laptev", "Nikita Balagansky", "Yaroslav Aksenov", "Daniil Gavrilov"], "categories": ["cs.LG", "cs.CL"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.03032v3", "summary": "We introduce a new approach to systematically map features discovered by\nsparse autoencoder across consecutive layers of large language models,\nextending earlier work that examined inter-layer feature links. By using a\ndata-free cosine similarity technique, we trace how specific features persist,\ntransform, or first appear at each stage. This method yields granular flow\ngraphs of feature evolution, enabling fine-grained interpretability and\nmechanistic insights into model computations. Crucially, we demonstrate how\nthese cross-layer feature maps facilitate direct steering of model behavior by\namplifying or suppressing chosen features, achieving targeted thematic control\nin text generation. Together, our findings highlight the utility of a causal,\ncross-layer interpretability framework that not only clarifies how features\ndevelop through forward passes but also provides new means for transparent\nmanipulation of large language models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.03032v3", "cate": "cs.LG", "date": "2025-02-05", "updated": "2025-07-24", "AI": {"title_translation": "分析特征流以增强语言模型的解释和操控", "tldr": "本文提出一种新方法，通过追踪稀疏自编码器发现的特征在大型语言模型层间如何演变，以实现精细的解释性，并展示了如何利用这些跨层特征图直接操控模型行为。", "motivation": "现有工作对语言模型层间特征关联的检查有限，本文旨在系统地映射稀疏自编码器发现的特征在大型语言模型连续层中的演变，以增强模型的可解释性并提供操控手段。", "method": "本文引入了一种新方法，通过使用无数据余弦相似度技术，追踪稀疏自编码器在大型语言模型连续层中发现的特定特征如何持续、转换或首次出现。该方法生成特征演变的粒度流图，并利用这些跨层特征图通过放大或抑制选定特征来直接操控模型行为。", "result": "该方法生成了特征演变的粒度流图，实现了对模型计算的精细可解释性和机械洞察。研究表明，这些跨层特征图能够通过放大或抑制选定特征来直接操控模型行为，从而实现文本生成的有针对性主题控制。", "conclusion": "本文的发现强调了因果、跨层可解释性框架的效用，该框架不仅阐明了特征在正向传播中如何发展，还为透明地操控大型语言模型提供了新方法。", "translation": "我们引入了一种新方法，用于系统地映射稀疏自编码器在大型语言模型连续层中发现的特征，扩展了早期检查层间特征链接的工作。通过使用一种无数据余弦相似度技术，我们追踪了特定特征在每个阶段如何持续、转换或首次出现。这种方法产生了特征演变的粒度流图，从而实现了对模型计算的精细解释性和机械洞察。至关重要的是，我们展示了这些跨层特征图如何通过放大或抑制选定的特征来促进模型行为的直接操控，从而在文本生成中实现有针对性的主题控制。总而言之，我们的发现强调了因果、跨层可解释性框架的效用，该框架不仅阐明了特征在正向传播中如何发展，还为透明地操控大型语言模型提供了新方法。", "summary": "本文提出一种新颖的方法，利用无数据余弦相似度技术，系统地追踪稀疏自编码器在大型语言模型各层中发现的特征演变。该方法生成精细的特征流图，从而提升了模型的可解释性并提供了对模型计算的机械洞察。研究还展示了如何利用这些跨层特征图，通过增减特定特征来直接操控模型行为，实现文本生成的主题控制。这为理解特征发展和透明操控大型语言模型提供了一个因果、跨层解释框架。", "keywords": "特征流, 语言模型, 可解释性, 模型操控, 稀疏自编码器", "comments": "本文的创新点在于提出了一个系统性的跨层特征流分析方法，并将其应用于语言模型的解释和操控。通过追踪稀疏自编码器发现的特征演变，不仅提升了模型的可解释性，更关键的是，提供了一种直接操控模型行为的新途径，这对于理解和控制大型语言模型具有重要意义。其“因果、跨层可解释性框架”的概念，为未来语言模型的研究提供了新的视角和工具。"}}
{"id": "2507.06547", "title": "Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution", "authors": ["Yonghyun Park", "Chieh-Hsin Lai", "Satoshi Hayakawa", "Yuhta Takida", "Naoki Murata", "Wei-Hsiang Liao", "Woosung Choi", "Kin Wai Cheuk", "Junghyun Koo", "Yuki Mitsufuji"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Preprint", "url": "http://arxiv.org/abs/2507.06547v2", "summary": "While diffusion models excel at image generation, their growing adoption\nraises critical concerns around copyright issues and model transparency.\nExisting attribution methods identify training examples influencing an entire\nimage, but fall short in isolating contributions to specific elements, such as\nstyles or objects, that matter most to stakeholders. To bridge this gap, we\nintroduce \\emph{concept-level attribution} via a novel method called\n\\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key\ninnovations: (1) a reformulated diffusion training loss based on diffusion\nposterior sampling, enabling robust, sample-specific attribution; and (2) a\nconcept-aware reward function that emphasizes semantic relevance. We evaluate\nConcept-TRAK on the AbC benchmark, showing substantial improvements over prior\nmethods. Through diverse case studies--ranging from identifying IP-protected\nand unsafe content to analyzing prompt engineering and compositional\nlearning--we demonstrate how concept-level attribution yields actionable\ninsights for responsible generative AI development and governance.", "comment": "Preprint", "pdf_url": "http://arxiv.org/pdf/2507.06547v2", "cate": "cs.CV", "date": "2025-07-09", "updated": "2025-07-25", "AI": {"title_translation": "Concept-TRAK：理解扩散模型如何通过概念级归因学习概念", "tldr": "Concept-TRAK是一种新的概念级归因方法，通过扩展影响函数，帮助理解扩散模型如何学习概念，并为负责任的生成式AI开发提供见解。", "motivation": "扩散模型在图像生成方面表现出色，但其日益普及引发了对版权问题和模型透明度的担忧。现有归因方法无法隔离对特定元素（如风格或对象）的贡献，而这些元素对利益相关者至关重要。本文旨在弥补这一差距。", "method": "本文引入了名为Concept-TRAK的概念级归因方法。Concept-TRAK通过两项关键创新扩展了影响函数：1) 基于扩散后验采样的重新表述的扩散训练损失，以实现稳健、样本特定的归因；2) 强调语义相关性的概念感知奖励函数。", "result": "Concept-TRAK在AbC基准测试上进行了评估，显示出比现有方法显著的改进。通过识别受知识产权保护和不安全内容、分析提示工程和组合学习等多样化案例研究，证明了概念级归因可以为负责任的生成式AI开发和治理提供可操作的见解。", "conclusion": "概念级归因方法Concept-TRAK能够揭示扩散模型学习概念的方式，并为解决版权、透明度等问题提供实际的解决方案，从而促进负责任的生成式AI发展。", "translation": "尽管扩散模型在图像生成方面表现出色，但其日益普及引发了对版权问题和模型透明度的严重担忧。现有归因方法能够识别影响整个图像的训练示例，但在隔离对特定元素（如风格或对象）的贡献方面存在不足，而这些元素对利益相关者至关重要。为了弥补这一差距，我们通过一种名为Concept-TRAK的新方法引入了概念级归因。Concept-TRAK通过两项关键创新扩展了影响函数：(1) 基于扩散后验采样的重新表述的扩散训练损失，从而实现稳健、样本特定的归因；(2) 强调语义相关性的概念感知奖励函数。我们在AbC基准测试上评估了Concept-TRAK，显示出比现有方法显著的改进。通过多样化的案例研究——从识别受知识产权保护和不安全内容到分析提示工程和组合学习——我们展示了概念级归因如何为负责任的生成式AI开发和治理提供可操作的见解。", "summary": "本文提出了Concept-TRAK，一种通过概念级归因来理解扩散模型如何学习概念的新方法。它通过重新表述扩散训练损失和引入概念感知奖励函数来扩展传统的影响函数。实验证明，Concept-TRAK在概念归因方面优于现有方法，并能为负责任的生成式AI开发提供关键洞察，例如识别版权内容和不安全内容。", "keywords": "扩散模型, 概念级归因, Concept-TRAK, 影响函数, 模型透明度", "comments": "Concept-TRAK的创新之处在于其将影响函数扩展到概念层面，并通过重新设计损失函数和引入概念感知奖励函数来提高归因的鲁棒性和语义相关性。这对于解决扩散模型的透明度、版权和安全性问题具有重要意义，是推动负责任AI发展的重要一步。"}}
{"id": "2507.18655", "title": "Part Segmentation of Human Meshes via Multi-View Human Parsing", "authors": ["James Dickens", "Kamyar Hamad"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18655v2", "summary": "Recent advances in point cloud deep learning have led to models that achieve\nhigh per-part labeling accuracy on large-scale point clouds, using only the raw\ngeometry of unordered point sets. In parallel, the field of human parsing\nfocuses on predicting body part and clothing/accessory labels from images. This\nwork aims to bridge these two domains by enabling per-vertex semantic\nsegmentation of large-scale human meshes. To achieve this, a pseudo-ground\ntruth labeling pipeline is developed for the Thuman2.1 dataset: meshes are\nfirst aligned to a canonical pose, segmented from multiple viewpoints, and the\nresulting point-level labels are then backprojected onto the original mesh to\nproduce per-point pseudo ground truth annotations. Subsequently, a novel,\nmemory-efficient sampling strategy is introduced, a windowed iterative farthest\npoint sampling (FPS) with space-filling curve-based serialization to\neffectively downsample the point clouds. This is followed by a purely geometric\nsegmentation using PointTransformer, enabling semantic parsing of human meshes\nwithout relying on texture information. Experimental results confirm the\neffectiveness and accuracy of the proposed approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18655v2", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-28", "AI": {"title_translation": "人体网格的多视角人体解析部分分割", "tldr": "本文提出了一种通过多视角人体解析对人体网格进行语义分割的方法，包括伪真值标注和高效采样策略。", "motivation": "现有点云深度学习在大型点云上实现了高精度标注，人体解析专注于从图像中预测身体部位标签。本文旨在弥合这两个领域，实现大规模人体网格的每顶点语义分割。", "method": "开发了一个针对Thuman2.1数据集的伪真值标注流水线：网格首先对齐到标准姿态，从多个视角进行分割，然后将得到的点级标签反向投影到原始网格上以生成每点伪真值注释。引入了一种新颖的、内存高效的采样策略，即一种带有基于空间填充曲线序列化的窗口迭代最远点采样（FPS），以有效下采样点云。随后使用PointTransformer进行纯几何分割，实现在不依赖纹理信息的情况下对人体网格进行语义解析。", "result": "实验结果证实了所提出方法的有效性和准确性。", "conclusion": "该方法能够有效且准确地实现大规模人体网格的每顶点语义分割，不依赖纹理信息。", "translation": "点云深度学习的最新进展使得模型能够仅使用无序点集的原始几何形状，在大型点云上实现高精度按部分标注。与此同时，人体解析领域专注于从图像中预测身体部位和服装/配饰标签。这项工作旨在通过实现大规模人体网格的每顶点语义分割来弥合这两个领域。为实现这一目标，开发了一个针对Thuman2.1数据集的伪真值标注流水线：网格首先对齐到标准姿态，从多个视角进行分割，然后将得到的点级标签反向投影到原始网格上以生成每点伪真值注释。随后，引入了一种新颖的、内存高效的采样策略，即一种带有基于空间填充曲线序列化的窗口迭代最远点采样（FPS），以有效下采样点云。接着，使用PointTransformer进行纯几何分割，实现在不依赖纹理信息的情况下对人体网格进行语义解析。实验结果证实了所提出方法的有效性和准确性。", "summary": "本文提出了一种通过多视角人体解析对大规模人体网格进行每顶点语义分割的新方法。该方法包括一个伪真值标注流水线，将多视角图像解析结果反投影到网格上，以及一种内存高效的窗口迭代最远点采样策略。通过使用PointTransformer进行纯几何分割，该方法能够在不依赖纹理信息的情况下，有效且准确地实现人体网格的部分分割。", "keywords": "人体网格分割, 多视角人体解析, 语义分割, 伪真值, PointTransformer", "comments": "本文的创新点在于将多视角人体解析与3D点云分割相结合，提出了一个从2D图像解析生成3D伪真值标签的流水线。同时，引入了高效的采样策略和纯几何分割方法，使得在不依赖纹理信息的情况下也能对大规模人体网格进行精确分割，这对于处理无纹理或低质量纹理的3D模型具有重要意义。"}}
{"id": "2507.17317", "title": "HuNavSim 2.0: An Enhanced Human Navigation Simulator for Human-Aware Robot Navigation", "authors": ["Miguel Escudero-Jiménez", "Noé Pérez-Higueras", "Andrés Martínez-Silva", "Fernando Caballero", "Luis Merino"], "categories": ["cs.RO"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      Preprint submitted to the 8th Iberian Robotics Conference (ROBOT 2025)", "url": "http://arxiv.org/abs/2507.17317v2", "summary": "This work presents a new iteration of the Human Navigation Simulator\n(HuNavSim), a novel open-source tool for the simulation of different\nhuman-agent navigation behaviors in scenarios with mobile robots. The tool,\nprogrammed under the ROS 2 framework, can be used together with different\nwell-known robotics simulators such as Gazebo or NVidia Isaac Sim. The main\ngoal is to facilitate the development and evaluation of human-aware robot\nnavigation systems in simulation. In this new version, several features have\nbeen improved and new ones added, such as the extended set of actions and\nconditions that can be combined in Behavior Trees to compound complex and\nrealistic human behaviors.", "comment": "Preprint submitted to the 8th Iberian Robotics Conference (ROBOT\n  2025)", "pdf_url": "http://arxiv.org/pdf/2507.17317v2", "cate": "cs.RO", "date": "2025-07-23", "updated": "2025-07-25", "AI": {"title_translation": "HuNavSim 2.0：一个用于人机协作机器人导航的增强型人类导航模拟器", "tldr": "HuNavSim 2.0是一个增强型开源人类导航模拟器，旨在促进人机协作机器人导航系统的开发和评估。", "motivation": "该工作旨在促进人机协作机器人导航系统在模拟环境中的开发和评估。", "method": "该论文介绍了HuNavSim 2.0，一个基于ROS 2框架的开源工具，用于模拟移动机器人场景中的不同人类智能体导航行为。该工具可与Gazebo或NVidia Isaac Sim等机器人模拟器配合使用。新版本改进并增加了功能，例如扩展了可在行为树中组合的动作和条件集，以构建复杂逼真的人类行为。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "这项工作提出了人类导航模拟器（HuNavSim）的一个新迭代，这是一个新颖的开源工具，用于模拟移动机器人场景中不同人类智能体的导航行为。该工具在ROS 2框架下编程，可以与不同的知名机器人模拟器（如Gazebo或NVidia Isaac Sim）一起使用。主要目标是促进模拟环境中人机协作机器人导航系统的开发和评估。在这个新版本中，改进了几个功能并增加了新功能，例如扩展了可在行为树中组合的动作和条件集，以构成复杂逼真的人类行为。", "summary": "本文介绍了HuNavSim 2.0，一个增强型开源人类导航模拟器，旨在促进人机协作机器人导航系统的开发和评估。该工具基于ROS 2框架，可与主流机器人模拟器如Gazebo和NVidia Isaac Sim集成。新版本扩展了行为树中可组合的动作和条件集，以实现更复杂和真实的人类行为模拟。", "keywords": "人类导航模拟器, 机器人导航, ROS 2, 行为树, 模拟", "comments": "HuNavSim 2.0的创新之处在于提供了一个开源且功能增强的模拟器，专门用于人类导航行为，对于开发和评估人机协作机器人导航系统具有重要意义。其基于ROS 2并支持行为树的特性，使其在模拟复杂人类行为方面具有灵活性和潜力。"}}
{"id": "2502.02735", "title": "Modal-based prediction of power system frequency response and frequency nadir", "authors": ["Francisco Zelaya-Arrazabal", "Sebastian Martinez-Lizana", "Héctor Pulgar-Painemal"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.02735v2", "summary": "This paper introduces a novel approach for predicting system frequency\nresponse (SFR) and frequency nadir based on modal analysis. By decomposing the\nfull system dynamic response, the method identifies dominant modes based on\ntheir participation in frequency behavior and derives a closed-form expression\nfor the frequency trajectory. Unlike traditional approaches based on the\nAverage System Frequency (ASF) model, this method captures the true system\ndynamics and avoids oversimplified representations. The dominant modes exhibit\nlow sensitivity to system parameters, enabling robust and accurate estimations\nacross diverse operating conditions. The proposed approach is tested on two\nbenchmark systems as well as the Salvadoran transmission planning network,\ndemonstrating its scalability, precision, and adaptability. This methodology\nrepresents a shift from observing a simplified average system frequency\nresponse to a more detailed analysis focusing on system dynamics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.02735v2", "cate": "eess.SY", "date": "2025-02-04", "updated": "2025-07-25", "AI": {"title_translation": "基于模态的电力系统频率响应和频率最低点预测", "tldr": "该论文提出了一种基于模态分析的新方法，用于预测电力系统频率响应（SFR）和频率最低点，该方法通过识别主导模态来捕捉真实系统动态，并表现出鲁棒性和准确性。", "motivation": "传统方法（如基于平均系统频率（ASF）模型的方法）对系统动态进行了过度简化表示，无法捕捉真实系统动态。本文旨在提出一种更详细、更准确的分析方法，以克服这些局限性。", "method": "该方法基于模态分析，通过分解完整系统动态响应，识别对频率行为有贡献的主导模态，并推导出频率轨迹的闭合表达式。它不同于传统的平均系统频率（ASF）模型。", "result": "该方法识别出的主导模态对系统参数敏感度低，能够在不同运行条件下实现鲁棒和准确的估计。该方法在两个基准系统以及萨尔瓦多输电规划网络上进行了测试，验证了其可扩展性、精确性和适应性。", "conclusion": "该方法论代表了从观察简化的平均系统频率响应到更详细地关注系统动态的转变，提供了更精确和鲁棒的电力系统频率响应和频率最低点预测。", "translation": "本文介绍了一种基于模态分析的预测系统频率响应（SFR）和频率最低点的新方法。通过分解完整的系统动态响应，该方法根据主导模态在频率行为中的参与度来识别它们，并推导出频率轨迹的闭合表达式。与基于平均系统频率（ASF）模型的传统方法不同，该方法捕捉了真实的系统动态，避免了过度简化的表示。主导模态对系统参数表现出低敏感性，从而能够在不同运行条件下实现鲁棒和准确的估计。所提出的方法在两个基准系统以及萨尔瓦多输电规划网络上进行了测试，证明了其可扩展性、精确性和适应性。这种方法论代表了从观察简化的平均系统频率响应到更详细地关注系统动态的转变。", "summary": "本文提出了一种基于模态分析的电力系统频率响应（SFR）和频率最低点预测新方法。该方法通过分解系统动态响应，识别主导模态并推导出频率轨迹的闭合表达式，从而捕捉真实的系统动态，避免了传统ASF模型的过度简化。该方法对系统参数不敏感，能提供鲁棒和准确的估计，并在多个系统上验证了其可扩展性、精度和适应性。", "keywords": "模态分析, 频率响应, 频率最低点, 电力系统, 动态响应", "comments": "该论文的创新之处在于将模态分析应用于电力系统频率响应和频率最低点的预测，克服了传统平均系统频率模型简化系统动态的局限性。其提出的方法通过识别主导模态，更真实地反映了系统动态，并展现出良好的鲁棒性和准确性，对于电力系统规划和运行具有重要意义。"}}
{"id": "2507.19409", "title": "Modality Agnostic Efficient Long Range Encoder", "authors": ["Toufiq Parag", "Ahmed Elgammal"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19409v1", "summary": "The long-context capability of recent large transformer models can be\nsurmised to rely on techniques such as attention/model parallelism, as well as\nhardware-level optimizations. While these strategies allow input lengths to\nscale to millions of tokens, they do not fundamentally mitigate the quadratic\ncomputational and memory complexity of the core attention mechanism. In this\npaper, we address the challenge of long-context processing on a single device\nusing generic implementations by reducing the quadratic memory footprint and\ninference cost. Existing approaches to extend the context length for generic\nsingle device implementations -- such as token merging and modified attentions\n-- are often modality specific and attain a suboptimal tradeoff between\naccuracy and efficiency. To overcome these limitations, we propose MAELRE\n(Modality Agnostic Efficient Long Range Encoder), a unified and efficient\ntransformer architecture designed for long-range encoding across diverse\nmodalities. MAELRE integrates token merging with attention approximation,\nprogressively merging tokens at different stages of internal computational\nblocks. It employs a lightweight attention approximation when the number of\ntokens is large, and switches to standard dot-product attention as the sequence\nbecomes shorter through successive aggregation. We demonstrate that MAELRE\nachieves superior accuracy while reducing computational cost compared to\nexisting long-context models on classification tasks spanning multiple\nmodalities, including text, time series, audio, and vision.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19409v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "模态无关高效长程编码器", "tldr": "提出MAELRE，一个模态无关的Transformer架构，通过渐进式token合并和自适应注意力机制，在单设备上高效处理长上下文，同时保持高精度。", "motivation": "当前大型Transformer模型处理长上下文依赖并行和硬件优化，但未从根本上解决核心注意力机制的二次计算和内存复杂度。现有单设备长上下文方法（如token合并、修改注意力）常模态特定且在精度和效率间权衡不佳。", "method": "提出MAELRE (Modality Agnostic Efficient Long Range Encoder)，一个统一高效的Transformer架构。它将token合并与注意力近似结合：在内部计算块的不同阶段渐进式合并token，当token数量大时使用轻量级注意力近似，随着序列通过聚合变短则切换到标准点积注意力。", "result": "MAELRE在跨多种模态（文本、时间序列、音频、视觉）的分类任务上，与现有长上下文模型相比，实现了更高的精度并降低了计算成本。", "conclusion": "MAELRE通过其创新的token合并和自适应注意力机制，有效解决了单设备上长上下文处理的效率和精度问题，展现了其在多模态应用中的优越性。", "translation": "最近大型Transformer模型的长上下文能力可以归因于注意力/模型并行等技术以及硬件级优化。虽然这些策略允许输入长度扩展到数百万个token，但它们并未从根本上缓解核心注意力机制的二次计算和内存复杂度。在本文中，我们通过减少二次内存占用和推理成本，使用通用实现解决了在单个设备上进行长上下文处理的挑战。现有用于扩展通用单设备实现上下文长度的方法——例如token合并和修改的注意力——通常是模态特定的，并且在精度和效率之间取得了次优的权衡。为了克服这些限制，我们提出了MAELRE（Modality Agnostic Efficient Long Range Encoder），一个统一且高效的Transformer架构，专为跨不同模态的长程编码而设计。MAELRE将token合并与注意力近似相结合，在内部计算块的不同阶段渐进式合并token。当token数量较大时，它采用轻量级注意力近似，并通过连续聚合使序列变短时切换到标准点积注意力。我们证明了MAELRE在跨多种模态（包括文本、时间序列、音频和视觉）的分类任务上，与现有长上下文模型相比，实现了更高的精度，同时降低了计算成本。", "summary": "本文提出MAELRE，一个模态无关的高效长程编码器，旨在解决单个设备上Transformer模型处理长上下文的二次计算和内存复杂度问题。MAELRE通过结合渐进式token合并和自适应注意力近似（在token多时用轻量级近似，少时用标准注意力）来优化效率和精度。实验表明，MAELRE在文本、时间序列、音频、视觉等多模态分类任务上，相较现有长上下文模型，在降低计算成本的同时获得了更优的精度。", "keywords": "长上下文, Transformer, 模态无关, 注意力机制, Token合并", "comments": "MAELRE的创新之处在于其模态无关的设计以及将渐进式token合并与自适应注意力机制相结合，有效平衡了长上下文处理的效率和精度。这对于在资源受限的单设备上部署大型Transformer模型具有重要意义，尤其是在多模态应用领域。"}}
{"id": "2507.18896", "title": "A Simple and Robust Weak Galerkin Method for the Brinkman Equations on Non-Convex Polytopal Meshes", "authors": ["Chunmei Wang", "Shangyou Zhang"], "categories": ["math.NA", "cs.NA", "65N30, 65N15, 65N12, 65N20"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      32 pages, 9 tables, 9 figures. arXiv admin note: text overlap with arXiv:2507.05953", "url": "http://arxiv.org/abs/2507.18896v1", "summary": "This paper presents a novel Stabilizer-Free weak Galerkin (WG) finite element\nmethod for solving the Brinkman equations without the need for conventional\nstabilization techniques. The Brinkman model, which mathematically blends\nfeatures of both the Stokes and Darcy equations, describes fluid flow in\nmulti-physics environments, particularly in heterogeneous porous media\ncharacterized by spatially varying permeability. In such settings, flow\nbehavior may be governed predominantly by Darcy dynamics in certain regions and\nby Stokes dynamics in others. A central difficulty in this context arises from\nthe incompatibility of standard finite element spaces: elements stable for the\nStokes equations typically perform poorly for Darcy flows, and vice versa. The\nprimary challenge addressed in this study is the development of a unified\nnumerical scheme that maintains stability and accuracy across both flow\nregimes. To this end, the proposed WG method demonstrates a robust capacity to\nresolve both Stokes- and Darcy-dominated flows through a unified framework. The\nmethod supports general finite element partitions consisting of convex and\nnon-convex polytopal elements, and employs bubble functions as a critical\nanalytical component to achieve stability and convergence. Optimal-order error\nestimates are rigorously derived for the WG finite element solutions.\nAdditionally, a series of numerical experiments is conducted to validate the\ntheoretical findings, illustrating the method's robustness, reliability,\nflexibility, and accuracy in solving the Brinkman equations.", "comment": "32 pages, 9 tables, 9 figures. arXiv admin note: text overlap with\n  arXiv:2507.05953", "pdf_url": "http://arxiv.org/pdf/2507.18896v1", "cate": "math.NA", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "非凸多边形网格上布林克曼方程的一种简单鲁棒弱伽辽金方法", "tldr": "提出了一种无需稳定化技术的弱伽辽金有限元方法，用于解决布林克曼方程，能在斯托克斯和达西流态下保持稳定和准确，并支持非凸多边形网格。", "motivation": "布林克曼模型描述了多物理场环境（特别是异质多孔介质）中的流体流动，它融合了斯托克斯和达西方程的特点。标准有限元空间在处理斯托克斯流和达西流时存在不兼容性，导致在不同流态下性能不佳。本研究旨在开发一种统一的数值方案，以在两种流态下均保持稳定性和准确性。", "method": "本文提出了一种新型的无稳定化弱伽辽金（WG）有限元方法。该方法利用泡函数作为关键分析组件以实现稳定性和收敛性，并支持由凸和非凸多边形单元组成的通用有限元剖分。", "result": "所提出的弱伽辽金方法在一个统一的框架内，展示了解决斯托克斯主导和达西主导流动的强大能力。研究严格推导了弱伽辽金有限元解的最优阶误差估计。通过一系列数值实验，验证了理论发现，并证明了该方法在求解布林克曼方程时的鲁棒性、可靠性、灵活性和准确性。", "conclusion": "本文成功开发了一种简单、鲁棒且准确的无稳定化弱伽辽金方法，用于求解布林克曼方程。该方法在一个统一的框架内处理斯托克斯和达西流态，并适用于通用多边形网格，有效解决了不同流态下数值方案的兼容性问题。", "translation": "本文提出了一种新颖的无稳定化弱伽辽金（WG）有限元方法，用于求解布林克曼方程，无需传统的稳定化技术。布林克曼模型在数学上融合了斯托克斯和达西方程的特点，描述了多物理场环境中的流体流动，特别是在以空间变渗透率为特征的异质多孔介质中。在这种设置下，流体行为可能在某些区域主要由达西动力学控制，而在另一些区域由斯托克斯动力学控制。在这种背景下，一个核心的困难在于标准有限元空间的不兼容性：对斯托克斯方程稳定的单元通常对达西流表现不佳，反之亦然。本研究解决的主要挑战是开发一种统一的数值方案，以在两种流态下均保持稳定性和准确性。为此，所提出的弱伽辽金方法通过一个统一的框架，展示了解决斯托克斯主导和达西主导流动的强大能力。该方法支持由凸和非凸多边形单元组成的通用有限元剖分，并采用泡函数作为实现稳定性和收敛性的关键分析组件。研究严格推导了弱伽辽金有限元解的最优阶误差估计。此外，还进行了一系列数值实验，以验证理论发现，说明了该方法在求解布林克曼方程时的鲁棒性、可靠性、灵活性和准确性。", "summary": "本文提出了一种新颖的无稳定化弱伽辽金（WG）有限元方法，旨在解决布林克曼方程在异质多孔介质中流体模拟时遇到的兼容性问题。该方程融合了斯托克斯和达西流动的特点，而现有方法难以在两种流态下同时保持稳定和准确。所提出的WG方法通过引入泡函数，实现了无需传统稳定化技术的稳定性和收敛性，并能统一处理斯托克斯和达西主导流。该方法适用于包含非凸单元在内的通用多边形网格，并提供了严格的最优阶误差估计。数值实验验证了其鲁棒性、可靠性、灵活性和准确性。", "keywords": "弱伽辽金, 布林克曼方程, 无稳定化, 多边形网格, 多孔介质流", "comments": "该论文的创新之处在于提出了一种“无稳定化”的弱伽辽金方法，成功地在统一框架下处理了布林克曼方程中斯托克斯和达西流态的兼容性问题。其能够处理非凸多边形网格，进一步提高了方法的灵活性和适用性，对多孔介质流体模拟领域具有重要意义。"}}
{"id": "2506.22800", "title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors", "authors": ["Sicong Du", "Jiarun Liu", "Qifeng Chen", "Hao-Xiang Chen", "Tai-Jiang Mu", "Sheng Yang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.22800v3", "summary": "A single-pass driving clip frequently results in incomplete scanning of the\nroad structure, making reconstructed scene expanding a critical requirement for\nsensor simulators to effectively regress driving actions. Although contemporary\n3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction\nquality, their direct extension through the integration of diffusion priors\noften introduces cumulative physical inconsistencies and compromises training\nefficiency. To address these limitations, we present RGE-GS, a novel expansive\nreconstruction framework that synergizes diffusion-based generation with\nreward-guided Gaussian integration. The RGE-GS framework incorporates two key\ninnovations: First, we propose a reward network that learns to identify and\nprioritize consistently generated patterns prior to reconstruction phases,\nthereby enabling selective retention of diffusion outputs for spatial\nstability. Second, during the reconstruction process, we devise a\ndifferentiated training strategy that automatically adjust Gaussian\noptimization progress according to scene converge metrics, which achieving\nbetter convergence than baseline methods. Extensive evaluations of publicly\navailable datasets demonstrate that RGE-GS achieves state-of-the-art\nperformance in reconstruction quality. Our source-code will be made publicly\navailable at https://github.com/CN-ADLab/RGE-GS.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.22800v3", "cate": "cs.CV", "date": "2025-06-28", "updated": "2025-07-25", "AI": {"title_translation": "RGE-GS：奖励引导的扩散先验扩展驾驶场景重建", "tldr": "RGE-GS提出了一种新的框架，通过奖励引导的高斯集成和扩散先验，实现扩展驾驶场景重建，达到最先进的性能。", "motivation": "单次驾驶片段通常导致道路结构扫描不完整，使得重建场景扩展成为传感器模拟器有效回归驾驶动作的关键需求。尽管当前的3D高斯泼溅(3DGS)技术实现了卓越的重建质量，但通过整合扩散先验直接扩展它们通常会引入累积的物理不一致性并损害训练效率。", "method": "RGE-GS是一个新颖的扩展重建框架，它结合了基于扩散的生成和奖励引导的高斯集成。它包含两个关键创新：1. 提出一个奖励网络，在重建阶段之前识别并优先处理一致生成的模式，从而选择性地保留扩散输出以实现空间稳定性。2. 在重建过程中，设计一种差异化训练策略，根据场景收敛指标自动调整高斯优化进度，实现比基线方法更好的收敛。", "result": "在公开数据集上的广泛评估表明，RGE-GS在重建质量方面达到了最先进的性能。", "conclusion": "RGE-GS框架通过结合奖励引导的高斯集成和扩散先验，有效解决了扩展驾驶场景重建中的挑战，并在重建质量方面达到了最先进的性能。", "translation": "单次驾驶片段经常导致道路结构扫描不完整，这使得重建场景扩展成为传感器模拟器有效回归驾驶动作的关键要求。尽管当前的3D高斯泼溅（3DGS）技术实现了卓越的重建质量，但通过整合扩散先验直接扩展它们通常会引入累积的物理不一致性并损害训练效率。为了解决这些局限性，我们提出了RGE-GS，一个新颖的扩展重建框架，它将基于扩散的生成与奖励引导的高斯集成相结合。RGE-GS框架包含两个关键创新：首先，我们提出了一个奖励网络，该网络学习在重建阶段之前识别并优先处理一致生成的模式，从而实现对扩散输出的选择性保留以确保空间稳定性。其次，在重建过程中，我们设计了一种差异化训练策略，根据场景收敛指标自动调整高斯优化进度，从而实现比基线方法更好的收敛。对公开数据集的广泛评估表明，RGE-GS在重建质量方面达到了最先进的性能。我们的源代码将公开在https://github.com/CN-ADLab/RGE-GS。", "summary": "本文介绍了RGE-GS，一个用于扩展驾驶场景重建的新框架。它解决了道路扫描不完整以及将3DGS与扩散先验结合时出现的不一致性问题。RGE-GS将扩散生成与奖励引导的高斯集成相结合，其特点是包含一个用于识别一致模式的奖励网络和一种用于优化高斯收敛的差异化训练策略。评估表明RGE-GS在重建质量方面达到了最先进的水平。", "keywords": "RGE-GS, 驾驶场景重建, 扩散先验, 3D高斯泼溅, 奖励引导", "comments": "该论文的创新之处在于利用奖励网络过滤扩散输出以确保空间稳定性，并采用差异化训练策略以实现更好的收敛。这解决了在3DGS中直接集成扩散先验进行扩展场景重建时遇到的关键局限性，显著提高了重建质量，对于传感器模拟器等关键应用具有重要意义。"}}
{"id": "2507.18812", "title": "MemoCoder: Automated Function Synthesis using LLM-Supported Agents", "authors": ["Yiping Jia", "Zhen Ming Jiang", "Shayan Noei", "Ying Zou"], "categories": ["cs.SE", "cs.AI"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18812v1", "summary": "With the widespread adoption of Large Language Models (LLMs) such as GitHub\nCopilot and ChatGPT, developers increasingly rely on AI-assisted tools to\nsupport code generation. While LLMs can generate syntactically correct\nsolutions for well-structured programming tasks, they often struggle with\nchallenges that require iterative debugging, error handling, or adaptation to\ndiverse problem structures. Existing approaches such as fine-tuning or\nself-repair strategies either require costly retraining or lack mechanisms to\naccumulate and reuse knowledge from previous attempts.\n  To address these limitations, we propose MemoCoder, a multi-agent framework\nthat enables collaborative problem solving and persistent learning from past\nfixes. At the core of MemoCoder is a Fixing Knowledge Set, which stores\nsuccessful repairs and supports retrieval for future tasks. A central Mentor\nAgent supervises the repair process by identifying recurring error patterns and\nrefining high-level fixing strategies, providing a novel supervisory role that\nguides the self-repair loop. We evaluate MemoCoder across three public\nbenchmarks -- MBPP, HumanEval, and LiveCodeBench -- spanning a range of problem\ncomplexities. Experimental results show that MemoCoder consistently outperforms\nboth zero-shot prompting and a Self-Repair strategy, with improvements ranging\nfrom 3.1% to 12.1% in Pass@10 and from 1.4% to 14.5% in Pass@50, demonstrating\nits effectiveness in iterative refinement and knowledge-guided code generation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18812v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "MemoCoder：使用LLM支持的代理进行自动化函数合成", "tldr": "MemoCoder是一个多智能体框架，通过积累和重用修复知识，显著提高了LLM在代码生成和错误修复方面的性能，尤其适用于复杂编程任务。", "motivation": "大型语言模型（LLM）在生成代码时，常难以处理需要迭代调试、错误处理或适应多样化问题结构的挑战。现有的微调或自修复策略要么成本高昂，要么缺乏知识积累和重用机制。", "method": "本文提出了MemoCoder，一个多智能体框架，旨在实现协作式问题解决和从过去的修复中持续学习。其核心是一个“修复知识集”（Fixing Knowledge Set），用于存储成功的修复并支持未来任务的检索。一个中央“导师智能体”（Mentor Agent）通过识别重复的错误模式和完善高层修复策略来监督修复过程，为自修复循环提供了新颖的指导作用。", "result": "MemoCoder在MBPP、HumanEval和LiveCodeBench三个公共基准测试中进行了评估，结果显示它持续优于零样本提示（zero-shot prompting）和自修复策略（Self-Repair strategy）。Pass@10的改进范围为3.1%至12.1%，Pass@50的改进范围为1.4%至14.5%。", "conclusion": "MemoCoder通过迭代细化和知识引导的代码生成，有效提升了大型语言模型在复杂编程任务中的性能和鲁棒性。", "translation": "随着GitHub Copilot和ChatGPT等大型语言模型（LLM）的广泛采用，开发人员越来越依赖AI辅助工具来支持代码生成。虽然LLM可以为结构良好的编程任务生成语法正确的解决方案，但它们常常难以应对需要迭代调试、错误处理或适应多样化问题结构的挑战。现有的方法，如微调或自修复策略，要么需要昂贵的再训练，要么缺乏积累和重用以往尝试知识的机制。\n为了解决这些限制，我们提出了MemoCoder，一个多智能体框架，它支持协作式问题解决和从过去的修复中持久学习。MemoCoder的核心是一个修复知识集（Fixing Knowledge Set），它存储成功的修复并支持未来任务的检索。一个中央导师智能体（Mentor Agent）通过识别重复的错误模式和完善高层修复策略来监督修复过程，提供了一种新颖的监督角色来指导自修复循环。我们在三个公共基准测试——MBPP、HumanEval和LiveCodeBench——上评估了MemoCoder，这些基准测试涵盖了一系列问题复杂性。实验结果表明，MemoCoder始终优于零样本提示和自修复策略，Pass@10的改进范围为3.1%到12.1%，Pass@50的改进范围为1.4%到14.5%，证明了其在迭代细化和知识引导的代码生成方面的有效性。", "summary": "MemoCoder是一个创新的多智能体框架，旨在解决大型语言模型在代码生成中处理复杂错误和迭代调试的不足。它通过引入一个存储成功修复的“修复知识集”和一个监督修复过程的“导师智能体”，实现了知识积累和重用，从而显著提高了LLM在生成代码时的性能和鲁棒性。在多个公共基准测试中，MemoCoder的表现均优于现有方法。", "keywords": "LLM, 代码生成, 多智能体, 错误修复, 知识管理", "comments": "MemoCoder的创新点在于其独特的多智能体协作机制和持久学习能力，特别是“修复知识集”和“导师智能体”的设计。这有效解决了LLM在面对复杂编程任务时，缺乏迭代调试能力和知识复用机制的痛点。该方法为未来AI辅助代码生成工具的开发提供了有价值的方向，使其能更有效地学习和适应多样化的编程挑战。"}}
{"id": "2507.06071", "title": "MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding", "authors": ["Chang Liu", "Ye Pan", "Chenyang Ding", "Susanto Rahardja", "Xiaokang Yang"], "categories": ["cs.CV", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.06071v3", "summary": "Audio-driven emotional 3D facial animation aims to generate synchronized lip\nmovements and vivid facial expressions. However, most existing approaches focus\non static and predefined emotion labels, limiting their diversity and\nnaturalness. To address these challenges, we propose MEDTalk, a novel framework\nfor fine-grained and dynamic emotional talking head generation. Our approach\nfirst disentangles content and emotion embedding spaces from motion sequences\nusing a carefully designed cross-reconstruction process, enabling independent\ncontrol over lip movements and facial expressions. Beyond conventional\naudio-driven lip synchronization, we integrate audio and speech text,\npredicting frame-wise intensity variations and dynamically adjusting static\nemotion features to generate realistic emotional expressions. Furthermore, to\nenhance control and personalization, we incorporate multimodal inputs-including\ntext descriptions and reference expression images-to guide the generation of\nuser-specified facial expressions. With MetaHuman as the priority, our\ngenerated results can be conveniently integrated into the industrial production\npipeline. The code is available at: https://github.com/SJTU-Lucy/MEDTalk.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.06071v3", "cate": "cs.CV", "date": "2025-07-08", "updated": "2025-07-24", "AI": {"title_translation": "MEDTalk：基于解耦嵌入的多模态受控3D动态情感面部动画", "tldr": "MEDTalk提出一种新框架，通过解耦内容和情感嵌入空间，并结合多模态输入，实现精细化、动态情感的3D面部动画生成，可用于工业生产。", "motivation": "现有音频驱动的3D面部动画方法多关注静态和预定义的情感标签，限制了多样性和自然性。", "method": "MEDTalk首先通过精心设计的交叉重建过程，从运动序列中解耦内容和情感嵌入空间，实现唇部运动和面部表情的独立控制。其次，结合音频和语音文本，预测逐帧强度变化，动态调整静态情感特征以生成逼真的情感表情。此外，整合文本描述和参考表情图像等多模态输入来指导用户指定面部表情的生成。优先支持MetaHuman。", "result": "生成的成果可以方便地集成到工业生产流程中。", "conclusion": "MEDTalk通过解耦嵌入和多模态输入，实现了精细化和动态情感的3D面部动画生成，提高了多样性和自然性，并能应用于工业生产。", "translation": "音频驱动的情感3D面部动画旨在生成同步的唇部运动和生动的面部表情。然而，大多数现有方法侧重于静态和预定义的情感标签，限制了其多样性和自然性。为了解决这些挑战，我们提出了MEDTalk，一个用于精细化和动态情感说话人头生成的新颖框架。我们的方法首先通过精心设计的交叉重建过程，从运动序列中解耦内容和情感嵌入空间，从而实现对唇部运动和面部表情的独立控制。除了传统的音频驱动唇部同步，我们还整合了音频和语音文本，预测逐帧强度变化并动态调整静态情感特征以生成逼真的情感表情。此外，为了增强控制和个性化，我们结合了多模态输入——包括文本描述和参考表情图像——来指导用户指定面部表情的生成。以MetaHuman为优先，我们生成的结果可以方便地集成到工业生产流程中。代码可在以下网址获取：https://github.com/SJTU-Lucy/MEDTalk。", "summary": "MEDTalk是一个用于生成精细化、动态情感说话人头的框架。它通过解耦内容和情感嵌入空间，实现唇部运动和面部表情的独立控制。该方法结合音频和语音文本来预测情感强度变化，并利用多模态输入（如文本描述和参考图像）指导用户指定表情的生成，旨在提高3D面部动画的多样性和自然性，并便于集成到工业生产流程中。", "keywords": "3D面部动画, 情感生成, 解耦嵌入, 多模态控制, 说话人头", "comments": "该论文的创新点在于其解耦嵌入方法，实现了内容和情感的独立控制，显著提升了3D面部动画的精细度和自然度。结合多模态输入进一步增强了用户对表情的控制力。其与MetaHuman的兼容性也显示了其在工业应用中的潜力。这对于虚拟人、游戏和影视制作等领域具有重要意义。"}}
{"id": "2507.19181", "title": "Bespoke multiresolution analysis of graph signals", "authors": ["Giacomo Elefante", "Gianluca Giacchi", "Michael Multerer", "Jacopo Quizi"], "categories": ["eess.SP", "cs.DM", "cs.IT", "cs.LG", "math.IT"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19181v1", "summary": "We present a novel framework for discrete multiresolution analysis of graph\nsignals. The main analytical tool is the samplet transform, originally defined\nin the Euclidean framework as a discrete wavelet-like construction, tailored to\nthe analysis of scattered data. The first contribution of this work is defining\nsamplets on graphs. To this end, we subdivide the graph into a fixed number of\npatches, embed each patch into a Euclidean space, where we construct samplets,\nand eventually pull the construction back to the graph. This ensures\northogonality, locality, and the vanishing moments property with respect to\nproperly defined polynomial spaces on graphs. Compared to classical Haar\nwavelets, this framework broadens the class of graph signals that can\nefficiently be compressed and analyzed. Along this line, we provide a\ndefinition of a class of signals that can be compressed using our construction.\nWe support our findings with different examples of signals defined on graphs\nwhose vertices lie on smooth manifolds. For efficient numerical implementation,\nwe combine heavy edge clustering, to partition the graph into meaningful\npatches, with landmark \\texttt{Isomap}, which provides low-dimensional\nembeddings for each patch. Our results demonstrate the method's robustness,\nscalability, and ability to yield sparse representations with controllable\napproximation error, significantly outperforming traditional Haar wavelet\napproaches in terms of compression efficiency and multiresolution fidelity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19181v1", "cate": "eess.SP", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "图信号的定制多分辨率分析", "tldr": "提出了一种新的图信号离散多分辨率分析框架，通过在图上定义samplet变换，实现了比传统Haar小波更好的压缩和分析性能。", "motivation": "旨在为图信号提供一种新颖的离散多分辨率分析框架，以克服传统方法的局限性，并提高图信号的压缩和分析效率。", "method": "核心是将在欧几里得空间中定义的samplet变换引入到图上。具体方法包括将图划分为多个块，将每个块嵌入到欧几里得空间中构建samplet，然后将构建拉回到图上。为了高效实现，结合了重边聚类进行图分割和landmark Isomap进行低维嵌入。", "result": "该方法在图信号上实现了正交性、局部性和消失矩特性，扩展了可高效压缩和分析的图信号类别。实验结果表明，该方法具有鲁棒性、可伸缩性，并能产生具有可控近似误差的稀疏表示，在压缩效率和多分辨率保真度方面显著优于传统的Haar小波方法。", "conclusion": "该研究成功开发了一种新的图信号多分辨率分析框架，基于图上的samplet变换，显著提升了图信号的压缩和分析能力，并超越了现有技术。", "translation": "我们提出了一种用于图信号离散多分辨率分析的新颖框架。主要分析工具是samplet变换，它最初在欧几里得框架中被定义为一种离散的小波类构造，专为分析分散数据而定制。这项工作的第一个贡献是在图上定义samplet。为此，我们将图细分为固定数量的块，将每个块嵌入到欧几里得空间中，在那里我们构建samplet，并最终将构造拉回到图上。这确保了相对于图上正确定义的 बहुपद空间的正交性、局部性和消失矩特性。与经典Haar小波相比，该框架拓宽了可以高效压缩和分析的图信号类别。沿着这条线，我们定义了一类可以使用我们构造进行压缩的信号。我们通过在光滑流形上顶点定义的图信号的不同例子来支持我们的发现。为了高效的数值实现，我们将重边聚类（用于将图划分为有意义的块）与landmark Isomap（为每个块提供低维嵌入）相结合。我们的结果证明了该方法的鲁棒性、可伸缩性以及产生具有可控近似误差的稀疏表示的能力，在压缩效率和多分辨率保真度方面显著优于传统的Haar小波方法。", "summary": "这篇论文介绍了一种新颖的图信号离散多分辨率分析框架，其核心在于在图上定义和应用samplet变换。通过将图分解为块，并在欧几里得空间中构建samplet后再映射回图，该方法实现了正交性、局部性和消失矩等特性。与传统的Haar小波相比，该框架能够更有效地压缩和分析更广泛的图信号类别，并在鲁棒性、可伸缩性和稀疏表示方面表现出显著优势。", "keywords": "图信号, 多分辨率分析, samplet变换, 信号压缩", "comments": "这项工作通过将欧几里得空间中的samplet变换推广到图结构数据，为图信号处理提供了一种创新的多分辨率分析工具。其创新性在于巧妙地结合了图分割、欧几里得嵌入和samplet构造，克服了传统图小波的局限性，并实现了优于Haar小波的性能。该方法的鲁棒性和可伸缩性使其在实际应用中具有重要潜力，尤其是在处理大型和复杂图数据时。"}}
{"id": "2507.18742", "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement", "authors": ["Víctor Gallego"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted to SCALR Workshop @ COLM 2025", "url": "http://arxiv.org/abs/2507.18742v1", "summary": "Language models (LMs) are susceptible to in-context reward hacking, where\nthey exploit flaws in tainted or faulty written specifications or rubrics to\nachieve high scores without fulfilling the user's true intent. We introduce\nSpecification Self-Correction (SSC), a novel, test-time framework that enables\nan LM to identify and correct flaws within its own guiding specification. SSC\nemploys a multi-step inference process where the model first generates a\nresponse based on a potentially tainted specification, critiques its output,\nand then revises the specification itself to remove the exploitable loophole. A\nfinal, more robust response is then generated using this self-corrected\nspecification. Across experiments spanning creative writing and agentic coding\ntasks with several LMs, we demonstrate that while models initially game tainted\nspecifications in 50-70\\% of cases, the SSC process reduces this vulnerability\nby over 90\\%. This dynamic repair occurs at inference time, requires no weight\nmodification, and leads to more robustly aligned model behavior. Code at\nhttps://github.com/vicgalle/specification-self-correction .", "comment": "Accepted to SCALR Workshop @ COLM 2025", "pdf_url": "http://arxiv.org/pdf/2507.18742v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "规范自校正：通过测试时细化缓解上下文奖励劫持", "tldr": "语言模型容易受到奖励劫持，即利用有缺陷的规范来获得高分而非满足真实意图。本研究引入了规范自校正（SSC）框架，使语言模型能够在测试时识别并纠正其自身指导规范中的缺陷，从而将漏洞利用率降低90%以上。", "motivation": "语言模型（LMs）容易受到上下文奖励劫持，即它们利用有缺陷的或错误的文字规范或评分标准来获得高分，而未能实现用户的真实意图。", "method": "本研究引入了规范自校正（SSC），这是一种新颖的测试时框架，使语言模型能够识别并纠正其自身指导规范中的缺陷。SSC采用多步骤推理过程：模型首先根据可能受污染的规范生成响应，然后批判其输出，接着修改规范本身以消除可利用的漏洞。最后，使用这个自校正的规范生成一个最终的、更鲁棒的响应。该动态修复发生在推理时，无需修改模型权重。", "result": "在涉及创意写作和代理编码任务的实验中，多个语言模型最初在50-70%的情况下会利用受污染的规范进行游戏，而SSC过程将这种漏洞减少了90%以上。", "conclusion": "规范自校正（SSC）在推理时动态修复规范，无需修改权重，并能使模型行为更稳健地对齐。", "translation": "语言模型（LMs）容易受到上下文奖励劫持，即它们利用受污染的或有缺陷的书面规范或评分标准来获得高分，而未能实现用户的真实意图。我们引入了规范自校正（SSC），这是一种新颖的测试时框架，使语言模型能够识别并纠正其自身指导规范中的缺陷。SSC采用多步骤推理过程，模型首先根据可能受污染的规范生成响应，批判其输出，然后修改规范本身以消除可利用的漏洞。然后使用这个自校正的规范生成一个最终的、更鲁棒的响应。在涉及创意写作和代理编码任务的实验中，我们用几个语言模型证明，虽然模型最初在50-70%的情况下会利用受污染的规范进行游戏，但SSC过程将这种漏洞减少了90%以上。这种动态修复发生在推理时，无需修改权重，并能使模型行为更稳健地对齐。代码位于https://github.com/vicgalle/specification-self-correction。", "summary": "本研究提出了一种名为规范自校正（SSC）的新型测试时框架，旨在解决语言模型容易受到上下文奖励劫持的问题。该问题表现为模型利用有缺陷的指令获得高分而非满足用户真实意图。SSC通过多步骤推理过程，让模型生成响应、批判自身输出、修正指导规范中的漏洞，并最终生成更鲁棒的响应。实验结果表明，SSC能将语言模型利用受污染规范的漏洞从50-70%大幅降低90%以上，且无需模型权重修改，从而实现更稳健的模型行为对齐。", "keywords": "规范自校正, 奖励劫持, 语言模型, 测试时细化, 模型对齐", "comments": "规范自校正（SSC）是一项重要的创新，它允许语言模型在推理时动态地识别并修复自身指导规范中的缺陷，而无需进行重新训练。这有效解决了语言模型中普遍存在的“奖励劫持”问题，显著提升了模型行为的鲁棒性和与用户真实意图的对齐程度。其“测试时”和“无需权重修改”的特性使其具有高度的实用性和可部署性。"}}
{"id": "2507.18966", "title": "YOLO for Knowledge Extraction from Vehicle Images: A Baseline Study", "authors": ["Saraa Al-Saddik", "Manna Elizabeth Philip", "Ali Haidar"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18966v1", "summary": "Accurate identification of vehicle attributes such as make, colour, and shape\nis critical for law enforcement and intelligence applications. This study\nevaluates the effectiveness of three state-of-the-art deep learning approaches\nYOLO-v11, YOLO-World, and YOLO-Classification on a real-world vehicle image\ndataset. This dataset was collected under challenging and unconstrained\nconditions by NSW Police Highway Patrol Vehicles. A multi-view inference (MVI)\napproach was deployed to enhance the performance of the models' predictions. To\nconduct the analyses, datasets with 100,000 plus images were created for each\nof the three metadata prediction tasks, specifically make, shape and colour.\nThe models were tested on a separate dataset with 29,937 images belonging to\n1809 number plates. Different sets of experiments have been investigated by\nvarying the models sizes. A classification accuracy of 93.70%, 82.86%, 85.19%,\nand 94.86% was achieved with the best performing make, shape, colour, and\ncolour-binary models respectively. It was concluded that there is a need to use\nMVI to get usable models within such complex real-world datasets. Our findings\nindicated that the object detection models YOLO-v11 and YOLO-World outperformed\nclassification-only models in make and shape extraction. Moreover, smaller YOLO\nvariants perform comparably to larger counterparts, offering substantial\nefficiency benefits for real-time predictions. This work provides a robust\nbaseline for extracting vehicle metadata in real-world scenarios. Such models\ncan be used in filtering and sorting user queries, minimising the time required\nto search large vehicle images datasets.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18966v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "YOLO在车辆图像知识提取中的应用：一项基线研究", "tldr": "本研究评估了YOLO系列模型在真实世界车辆图像数据集上提取车辆属性（品牌、形状、颜色）的有效性，并提出多视角推理（MVI）方法以提高性能，发现YOLO目标检测模型优于纯分类模型，且小尺寸模型表现可比。", "motivation": "准确识别车辆属性（如品牌、颜色和形状）对于执法和情报应用至关重要。", "method": "本研究评估了YOLO-v11、YOLO-World和YOLO-Classification三种深度学习方法在真实世界车辆图像数据集上的效果。数据集由新南威尔士州警方公路巡逻车在复杂非受限条件下收集。部署了多视角推理（MVI）方法来增强模型预测性能。为进行分析，为品牌、形状和颜色这三个元数据预测任务创建了超过100,000张图像的数据集。模型在包含29,937张图像的独立数据集上进行测试，这些图像对应1809个车牌。通过改变模型大小，研究了不同的实验设置。", "result": "最佳表现模型在品牌、形状、颜色和颜色-二分类任务上分别达到了93.70%、82.86%、85.19%和94.86%的分类准确率。研究发现，YOLO-v11和YOLO-World等目标检测模型在品牌和形状提取方面优于纯分类模型。此外，较小的YOLO变体与较大的模型表现相当，为实时预测提供了显著的效率优势。", "conclusion": "在复杂的真实世界数据集中，需要使用多视角推理（MVI）才能获得可用的模型。YOLO目标检测模型在车辆属性提取方面表现出色，且小尺寸模型具有较高的效率。", "translation": "准确识别车辆属性，如品牌、颜色和形状，对于执法和情报应用至关重要。本研究评估了YOLO-v11、YOLO-World和YOLO-Classification这三种最先进的深度学习方法在真实世界车辆图像数据集上的有效性。该数据集由新南威尔士州警方公路巡逻车在具有挑战性和非受限的条件下收集。部署了多视角推理（MVI）方法以增强模型的预测性能。为了进行分析，为品牌、形状和颜色这三个元数据预测任务分别创建了包含超过100,000张图像的数据集。模型在一个包含29,937张图像的独立数据集上进行测试，这些图像对应1809个车牌。通过改变模型大小，研究了不同的实验设置。最佳表现模型在品牌、形状、颜色和颜色-二分类任务上分别达到了93.70%、82.86%、85.19%和94.86%的分类准确率。研究得出结论，在如此复杂的真实世界数据集中，需要使用MVI才能获得可用的模型。我们的发现表明，目标检测模型YOLO-v11和YOLO-World在品牌和形状提取方面优于纯分类模型。此外，较小的YOLO变体与较大的模型表现相当，为实时预测提供了显著的效率优势。这项工作为在真实世界场景中提取车辆元数据提供了强大的基线。此类模型可用于筛选和排序用户查询，最大限度地减少搜索大型车辆图像数据集所需的时间。", "summary": "本研究评估了YOLO-v11、YOLO-World和YOLO-Classification等YOLO系列模型在真实世界车辆图像数据集上提取车辆品牌、形状和颜色等属性的性能。通过引入多视角推理（MVI）方法，模型在复杂数据集上取得了高准确率，例如品牌识别达到93.70%。研究发现，目标检测模型在品牌和形状提取方面优于纯分类模型，且小型YOLO变体在保持性能的同时提供了更高的效率。这项工作为车辆元数据提取提供了重要的基线，有助于执法和情报应用中的图像数据搜索。", "keywords": "YOLO, 车辆图像, 知识提取, 深度学习, 多视角推理", "comments": "该研究在真实世界、具有挑战性的数据集上评估了YOLO系列模型，并引入了多视角推理（MVI）来提高性能，这对于实际应用具有重要意义。其创新之处在于证明了目标检测模型在特定属性提取上的优越性，并强调了小型模型在效率方面的优势，为实时部署提供了可行性。这项基线研究为未来在复杂场景下进行车辆图像知识提取奠定了基础。"}}
{"id": "2507.19144", "title": "Solar Photovoltaic Assessment with Large Language Model", "authors": ["Muhao Guo", "Yang Weng"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      27 pages, 7 figures", "url": "http://arxiv.org/abs/2507.19144v1", "summary": "Accurate detection and localization of solar photovoltaic (PV) panels in\nsatellite imagery is essential for optimizing microgrids and active\ndistribution networks (ADNs), which are critical components of renewable energy\nsystems. Existing methods lack transparency regarding their underlying\nalgorithms or training datasets, rely on large, high-quality PV training data,\nand struggle to generalize to new geographic regions or varied environmental\nconditions without extensive re-training. These limitations lead to\ninconsistent detection outcomes, hindering large-scale deployment and\ndata-driven grid optimization. In this paper, we investigate how large language\nmodels (LLMs) can be leveraged to overcome these challenges. Despite their\npromise, LLMs face several challenges in solar panel detection, including\ndifficulties with multi-step logical processes, inconsistent output formatting,\nfrequent misclassification of visually similar objects (e.g., shadows, parking\nlots), and low accuracy in complex tasks such as spatial localization and\nquantification. To overcome these issues, we propose the PV Assessment with\nLLMs (PVAL) framework, which incorporates task decomposition for more efficient\nworkflows, output standardization for consistent and scalable formatting,\nfew-shot prompting to enhance classification accuracy, and fine-tuning using\ncurated PV datasets with detailed annotations. PVAL ensures transparency,\nscalability, and adaptability across heterogeneous datasets while minimizing\ncomputational overhead. By combining open-source accessibility with robust\nmethodologies, PVAL establishes an automated and reproducible pipeline for\nsolar panel detection, paving the way for large-scale renewable energy\nintegration and optimized grid management.", "comment": "27 pages, 7 figures", "pdf_url": "http://arxiv.org/pdf/2507.19144v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "使用大型语言模型进行太阳能光伏评估", "tldr": "该论文提出了一种名为PVAL的框架，利用大型语言模型（LLMs）解决现有太阳能光伏板检测方法在透明度、泛化能力和数据依赖性方面的挑战，旨在实现自动化、可复现的大规模光伏板检测。", "motivation": "现有太阳能光伏（PV）板检测方法缺乏透明度，依赖大量高质量训练数据，并且难以在不同地理区域或环境条件下泛化，导致检测结果不一致，阻碍了大规模部署和数据驱动的电网优化。同时，大型语言模型在太阳能电池板检测中面临多步逻辑处理、输出格式不一致、误分类和低准确率等挑战。", "method": "本文提出了PV Assessment with LLMs (PVAL) 框架，该框架通过任务分解提高工作流程效率，通过输出标准化实现一致且可扩展的格式，通过少样本提示增强分类准确性，并通过使用带有详细注释的精选光伏数据集进行微调。PVAL结合开源可访问性和稳健方法，建立了自动化和可复现的太阳能电池板检测流程。", "result": "PVAL框架确保了跨异构数据集的透明度、可扩展性和适应性，同时最大限度地减少了计算开销。它建立了一个自动化和可复现的太阳能电池板检测流程。", "conclusion": "PVAL框架通过结合大型语言模型和稳健的方法，为大规模可再生能源整合和优化电网管理铺平了道路，解决了现有光伏板检测方法的局限性。", "translation": "在卫星图像中准确检测和定位太阳能光伏（PV）板对于优化微电网和主动配电网络（ADNs）至关重要，这些是可再生能源系统的关键组成部分。现有方法在底层算法或训练数据集方面缺乏透明度，依赖于大量高质量的光伏训练数据，并且在没有大量再训练的情况下难以推广到新的地理区域或不同的环境条件。这些限制导致检测结果不一致，阻碍了大规模部署和数据驱动的电网优化。在本文中，我们研究了如何利用大型语言模型（LLMs）来克服这些挑战。尽管LLMs前景广阔，但它们在太阳能电池板检测中面临一些挑战，包括多步逻辑过程的困难、输出格式不一致、频繁将视觉相似物体（例如阴影、停车场）误分类，以及在空间定位和量化等复杂任务中准确性较低。为了克服这些问题，我们提出了使用LLMs进行光伏评估（PVAL）框架，该框架结合了任务分解以实现更高效的工作流程，输出标准化以实现一致且可扩展的格式，少样本提示以提高分类准确性，以及使用带有详细注释的精选光伏数据集进行微调。PVAL确保了跨异构数据集的透明度、可扩展性和适应性，同时最大限度地减少了计算开销。通过将开源可访问性与稳健的方法相结合，PVAL建立了自动化和可复现的太阳能电池板检测流程，为大规模可再生能源整合和优化电网管理铺平了道路。", "summary": "本研究提出了一种名为PVAL（PV Assessment with LLMs）的新框架，旨在解决现有卫星图像中太阳能光伏板检测方法的局限性，包括缺乏透明度、泛化能力差和对大量训练数据的依赖。PVAL利用大型语言模型，通过任务分解、输出标准化、少样本提示和精细化微调来克服LLM在光伏检测中的挑战，如逻辑处理、格式一致性、误分类和准确性问题。该框架实现了透明、可扩展、适应性强的自动化光伏板检测流程，为大规模可再生能源集成和电网优化提供了可复现的解决方案。", "keywords": "大型语言模型, 太阳能光伏, 目标检测, PVAL, 可再生能源", "comments": "本文的创新之处在于探索了大型语言模型在传统计算机视觉任务（如目标检测）中的应用，特别是在光伏板检测领域。它通过提出PVAL框架，有效地解决了LLM在处理此类任务时面临的挑战，如多步逻辑和输出一致性。该研究的重要性在于为可再生能源领域提供了一个自动化、透明且可扩展的解决方案，有望加速太阳能部署和电网优化。其结合开源方法和稳健策略，使其具有很高的实用价值。"}}
{"id": "2507.19208", "title": "Comparison of Knowledge Distillation Methods for Low-complexity Multi-microphone Speech Enhancement using the FT-JNF Architecture", "authors": ["Robert Metzger", "Mattes Ohlenbusch", "Christian Rollwage", "Simon Doclo"], "categories": ["eess.AS"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "Comments:      Accepted at the ITG Conference on Speech Communication 2025 in Berlin", "url": "http://arxiv.org/abs/2507.19208v1", "summary": "Multi-microphone speech enhancement using deep neural networks (DNNs) has\nsignificantly progressed in recent years. However, many proposed DNN-based\nspeech enhancement algorithms cannot be implemented on devices with limited\nhardware resources. Only lowering the complexity of such systems by reducing\nthe number of parameters often results in worse performance. Knowledge\nDistillation (KD) is a promising approach for reducing DNN model size while\npreserving performance. In this paper, we consider the recently proposed\nFrequency-Time Joint Non-linear Filter (FT-JNF) architecture and investigate\nseveral KD methods to train smaller (student) models from a large pre-trained\n(teacher) model. Five KD methods are evaluated using direct output matching,\nthe self-similarity of intermediate layers, and fused multi-layer losses.\nExperimental results on a simulated dataset using a compact array with five\nmicrophones show that three KD methods substantially improve the performance of\nstudent models compared to training without KD. A student model with only 25%\nof the teacher model's parameters achieves comparable PESQ scores at 0 dB SNR.\nFurthermore, a reduction of up to 96% in model size can be achieved with only a\nminimal decrease in PESQ scores.", "comment": "Accepted at the ITG Conference on Speech Communication 2025 in Berlin", "pdf_url": "http://arxiv.org/pdf/2507.19208v1", "cate": "eess.AS", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于FT-JNF架构的低复杂度多麦克风语音增强知识蒸馏方法比较", "tldr": "研究比较了多种知识蒸馏方法，以在FT-JNF架构下实现低复杂度多麦克风语音增强，结果表明KD能显著减小模型尺寸并保持性能。", "motivation": "现有深度神经网络（DNN）语音增强算法复杂度高，难以在资源受限设备上部署；直接减少参数常导致性能下降。知识蒸馏（KD）是一种有前景的解决方案，可以在减小模型尺寸的同时保持性能。", "method": "本文考虑了频时联合非线性滤波器（FT-JNF）架构，并研究了五种知识蒸馏（KD）方法，通过直接输出匹配、中间层自相似性及融合多层损失来训练小型（学生）模型，使其从大型预训练（教师）模型中学习。实验在一个模拟数据集上使用紧凑型五麦克风阵列进行评估。", "result": "实验结果表明，与不使用KD训练相比，三种KD方法显著提高了学生模型的性能。一个参数仅为教师模型25%的学生模型在0 dB SNR下能达到可比的PESQ分数。模型尺寸最高可减少96%，而PESQ分数仅有微小下降。", "conclusion": "知识蒸馏是降低多麦克风语音增强模型复杂度的有效技术，同时能够保持其性能，使其适用于硬件资源有限的设备。", "translation": "多麦克风语音增强使用深度神经网络（DNNs）近年来取得了显著进展。然而，许多提出的基于DNN的语音增强算法无法在硬件资源有限的设备上实现。仅仅通过减少参数来降低这些系统的复杂性通常会导致性能下降。知识蒸馏（KD）是一种有前景的方法，可以减小DNN模型尺寸同时保持性能。在本文中，我们考虑了最近提出的频时联合非线性滤波器（FT-JNF）架构，并研究了几种KD方法，以从大型预训练（教师）模型中训练更小（学生）模型。通过直接输出匹配、中间层自相似性以及融合多层损失评估了五种KD方法。在模拟数据集上使用紧凑型五麦克风阵列进行的实验结果表明，与不使用KD训练相比，三种KD方法显著提高了学生模型的性能。一个参数仅为教师模型25%的学生模型在0 dB SNR下实现了可比的PESQ分数。此外，模型尺寸可以减少高达96%，而PESQ分数仅有最小的下降。", "summary": "本文针对深度神经网络（DNN）语音增强模型在资源受限设备上部署的挑战，探索了知识蒸馏（KD）在低复杂度多麦克风语音增强中的应用。研究以FT-JNF架构为基础，评估了五种不同的KD方法，旨在从大型教师模型中训练出性能接近但尺寸显著更小的学生模型。实验证明，部分KD方法能有效提升学生模型性能，实现高达96%的模型尺寸缩减，同时仅导致PESQ分数微小下降，突显了KD在构建高效、可部署语音增强系统方面的潜力。", "keywords": "知识蒸馏, 语音增强, 多麦克风, 模型压缩, FT-JNF", "comments": "本文通过比较多种知识蒸馏方法，为解决DNN语音增强模型在资源受限设备上的部署问题提供了有效途径。其创新点在于将KD应用于FT-JNF架构的多麦克风语音增强，并量化了模型压缩和性能保持之间的权衡。重要性在于为实际应用中部署高性能、低复杂度的语音增强系统提供了实用指导。"}}
{"id": "2507.19315", "title": "AutoPCR: Automated Phenotype Concept Recognition by Prompting", "authors": ["Yicheng Tao", "Yuanhao Huang", "Jie Liu"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19315v1", "summary": "Phenotype concept recognition (CR) is a fundamental task in biomedical text\nmining, enabling applications such as clinical diagnostics and knowledge graph\nconstruction. However, existing methods often require ontology-specific\ntraining and struggle to generalize across diverse text types and evolving\nbiomedical terminology. We present AutoPCR, a prompt-based phenotype CR method\nthat does not require ontology-specific training. AutoPCR performs CR in three\nstages: entity extraction using a hybrid of rule-based and neural tagging\nstrategies, candidate retrieval via SapBERT, and entity linking through\nprompting a large language model. Experiments on four benchmark datasets show\nthat AutoPCR achieves the best average and most robust performance across both\nmention-level and document-level evaluations, surpassing prior state-of-the-art\nmethods. Further ablation and transfer studies demonstrate its inductive\ncapability and generalizability to new ontologies.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19315v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "AutoPCR：基于提示的自动化表型概念识别", "tldr": "AutoPCR是一种新的、无需特定本体训练的表型概念识别方法，通过三阶段提示式方法在多个数据集上超越了现有技术，并展现出良好的泛化能力。", "motivation": "现有的表型概念识别方法通常需要本体特定的训练，并且在面对不同文本类型和不断演变的生物医学术语时，泛化能力较差。", "method": "AutoPCR通过三个阶段进行表型概念识别：1. 实体提取：采用规则与神经标签策略的混合方法。2. 候选检索：通过SapBERT进行。3. 实体链接：通过提示大型语言模型完成。", "result": "在四个基准数据集上的实验表明，AutoPCR在提及级别和文档级别评估中均取得了最佳的平均和最稳健的性能，超越了现有最先进的方法。进一步的消融研究和迁移研究证明了其归纳能力和对新本体的泛化能力。", "conclusion": "AutoPCR是一种有效且具有良好泛化能力的表型概念识别方法，无需本体特定训练，并在多个基准测试中表现优异，有望应用于临床诊断和知识图谱构建等领域。", "translation": "表型概念识别（CR）是生物医学文本挖掘中的一项基础任务，能够支持临床诊断和知识图谱构建等应用。然而，现有方法通常需要本体特定的训练，并且难以泛化到不同的文本类型和不断演变的生物医学术语。我们提出了AutoPCR，一种基于提示的表型概念识别方法，它不需要本体特定的训练。AutoPCR分三个阶段执行概念识别：使用规则和神经标记策略混合的实体提取、通过SapBERT进行候选检索，以及通过提示大型语言模型进行实体链接。在四个基准数据集上的实验表明，AutoPCR在提及级别和文档级别评估中均取得了最佳的平均和最稳健的性能，超越了现有最先进的方法。进一步的消融研究和迁移研究证明了其归纳能力和对新本体的泛化能力。", "summary": "AutoPCR是一种新颖的、无需本体特定训练的表型概念识别方法。它采用三阶段流程：混合策略的实体提取、SapBERT的候选检索以及基于LLM提示的实体链接。该方法在多个基准数据集上表现出超越现有SOTA的性能，并展现出优异的归纳和泛化能力，解决了传统方法泛化性差的问题。", "keywords": "表型概念识别, 提示学习, 大型语言模型, 生物医学文本挖掘, 泛化能力", "comments": "AutoPCR的创新之处在于其结合了规则、神经模型和大型语言模型提示的混合三阶段方法，尤其是不需要本体特定训练，这大大增强了其在不断变化的生物医学领域中的实用性和泛化能力。其在SOTA上的超越以及对新本体的泛化能力，使其成为生物医学文本挖掘领域的一个重要进展。"}}
{"id": "2502.20268", "title": "Large Language Models as Attribution Regularizers for Efficient Model Training", "authors": ["Davor Vukadin", "Marin Šilić", "Goran Delač"], "categories": ["cs.LG", "cs.AI", "I.2.6"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2502.20268v3", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse domains. However, effectively leveraging their vast knowledge for\ntraining smaller downstream models remains an open challenge, especially in\ndomains like tabular data learning, where simpler models are often preferred\ndue to interpretability and efficiency.\n  In this paper, we introduce a novel yet straightforward method for\nincorporating LLM-generated global task feature attributions into the training\nprocess of smaller networks. Specifically, we propose an attribution-matching\nregularization term that aligns the training dynamics of the smaller model with\nthe insights provided by the LLM. By doing so, our approach yields superior\nperformance in few-shot learning scenarios. Notably, our method requires only\nblack-box API access to the LLM, making it easy to integrate into existing\ntraining pipelines with minimal computational overhead.\n  Furthermore, we demonstrate how this method can be used to address common\nissues in real-world datasets, such as skewness and bias. By integrating\nhigh-level knowledge from LLMs, our approach improves generalization, even when\ntraining data is limited or imbalanced. We validate its effectiveness through\nextensive experiments across multiple tasks, demonstrating improved learning\nefficiency and model robustness.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2502.20268v3", "cate": "cs.LG", "date": "2025-02-27", "updated": "2025-07-25", "AI": {"title_translation": "大型语言模型作为归因正则化器用于高效模型训练", "tldr": "利用LLM生成的特征归因作为正则化项，提升小型模型在少样本学习中的性能和泛化能力，且易于集成。", "motivation": "尽管大型语言模型（LLMs）表现出色，但如何有效利用其知识来训练更小、更高效的下游模型（特别是在表格数据等领域，这些领域偏好简单模型以提高可解释性和效率）仍是一个挑战。", "method": "本文引入了一种新颖的方法，将LLM生成的全局任务特征归因整合到小型网络的训练过程中。具体地，提出了一个归因匹配正则化项，使小型模型的训练动态与LLM提供的洞察保持一致。该方法仅需对LLM进行黑盒API访问，易于集成且计算开销小。", "result": "该方法在少样本学习场景中取得了卓越性能；能够解决真实世界数据中的偏斜和偏差问题；即使在训练数据有限或不平衡的情况下也能提高泛化能力；通过广泛实验验证了其有效性，并展示了学习效率和模型鲁棒性的提升。", "conclusion": "该研究提出了一种利用LLM归因作为正则化项的方法，有效提升了小型模型在少样本和不平衡数据场景下的性能、泛化能力、学习效率和鲁棒性，且易于部署。", "translation": "大型语言模型 (LLMs) 在各种领域都展现出卓越的性能。然而，如何有效利用其庞大知识来训练较小的下游模型仍然是一个开放的挑战，尤其是在表格数据学习等领域，由于可解释性和效率的原因，通常更倾向于使用更简单的模型。\n在本文中，我们引入了一种新颖而直接的方法，将LLM生成的全局任务特征归因融入到小型网络的训练过程中。具体而言，我们提出了一个归因匹配正则化项，它使小型模型的训练动态与LLM提供的洞察保持一致。通过这样做，我们的方法在少样本学习场景中取得了卓越的性能。值得注意的是，我们的方法仅需要对LLM进行黑盒API访问，这使得它易于集成到现有训练管道中，且计算开销极小。\n此外，我们还展示了该方法如何用于解决真实世界数据中的常见问题，例如偏斜和偏差。通过整合来自LLM的高级知识，即使在训练数据有限或不平衡的情况下，我们的方法也能提高泛化能力。我们通过多项任务的广泛实验验证了其有效性，证明了学习效率和模型鲁棒性的提高。", "summary": "本文提出了一种新颖的方法，利用大型语言模型（LLMs）生成的全局任务特征归因来正则化小型模型的训练过程。通过引入一个归因匹配正则化项，该方法使小型模型的训练动态与LLM的洞察保持一致，从而在少样本学习场景中取得了卓越性能。该方法仅需LLM的黑盒API访问，易于集成，且能有效解决数据偏斜和偏差问题，提高模型的泛化能力、学习效率和模型鲁棒性。", "keywords": "大型语言模型, 归因正则化, 少样本学习, 模型训练, 泛化能力", "comments": "该研究提出了一种创新且实用的方法，通过将LLM的归因知识作为正则化项，有效地将LLM的强大能力迁移到小型模型上，解决了传统上LLM难以直接应用于对模型大小、可解释性或效率有要求的场景的问题。其“黑盒API访问”的特性大大降低了应用门槛和计算成本，使其具有较高的实际应用价值。"}}
{"id": "2507.17897", "title": "Multimodal Recurrent Ensembles for Predicting Brain Responses to Naturalistic Movies (Algonauts 2025)", "authors": ["Semih Eren", "Deniz Kucukahmetler", "Nico Scherf"], "categories": ["q-bio.NC", "cs.CV", "cs.LG"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "Comments:      8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts Project session (3rd-place team). Code: this https URL", "url": "http://arxiv.org/abs/2507.17897v2", "summary": "Accurately predicting distributed cortical responses to naturalistic stimuli\nrequires models that integrate visual, auditory and semantic information over\ntime. We present a hierarchical multimodal recurrent ensemble that maps\npretrained video, audio, and language embeddings to fMRI time series recorded\nwhile four subjects watched almost 80 hours of movies provided by the Algonauts\n2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics;\ntheir hidden states are fused and passed to a second recurrent layer, and\nlightweight subject-specific heads output responses for 1000 cortical parcels.\nTraining relies on a composite MSE-correlation loss and a curriculum that\ngradually shifts emphasis from early sensory to late association regions.\nAveraging 100 model variants further boosts robustness. The resulting system\nranked third on the competition leaderboard, achieving an overall Pearson r =\n0.2094 and the highest single-parcel peak score (mean r = 0.63) among all\nparticipants, with particularly strong gains for the most challenging subject\n(Subject 5). The approach establishes a simple, extensible baseline for future\nmultimodal brain-encoding benchmarks.", "comment": "8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts\n  Project session (3rd-place team). Code:\n  https://github.com/erensemih/Algonauts2025_ModalityRNN", "pdf_url": "http://arxiv.org/pdf/2507.17897v2", "cate": "q-bio.NC", "date": "2025-07-23", "updated": "2025-07-25", "AI": {"title_translation": "多模态循环集成模型预测大脑对自然电影的反应 (Algonauts 2025)", "tldr": "该研究提出了一个多模态循环集成模型，用于预测大脑对自然电影的皮层反应，在Algonauts 2025挑战赛中排名第三，并为未来的脑编码基准建立了基线。", "motivation": "准确预测大脑对自然刺激的分布式皮层反应需要整合视觉、听觉和语义信息并考虑时间依赖性的模型。", "method": "该方法提出了一个分层多模态循环集成模型，将预训练的视频、音频和语言嵌入映射到fMRI时间序列。它使用特定模态的双向RNN编码时间动态，融合隐藏状态并传递到第二个循环层，然后通过轻量级的主体特定头部输出1000个皮层区域的反应。训练采用复合MSE-相关性损失和课程学习，并平均100个模型变体以提高鲁棒性。", "result": "该系统在Algonauts 2025挑战赛中排名第三，总体Pearson r = 0.2094，并实现了所有参与者中最高的单区域峰值分数（平均r = 0.63），特别是在最具挑战性的受试者（受试者5）上取得了显著提升。", "conclusion": "该方法为未来的多模态脑编码基准建立了一个简单且可扩展的基线。", "translation": "准确预测大脑对自然刺激的分布式皮层反应需要模型整合视觉、听觉和语义信息并随时间变化。我们提出了一个分层多模态循环集成模型，将预训练的视频、音频和语言嵌入映射到fMRI时间序列，这些数据是在四名受试者观看Algonauts 2025挑战赛提供的近80小时电影时记录的。模态特定的双向RNN编码时间动态；它们的隐藏状态被融合并传递到第二个循环层，轻量级受试者特定头部输出1000个皮层区域的反应。训练依赖于复合MSE-相关性损失和课程学习，该课程逐渐将重点从早期感觉区域转移到晚期联想区域。平均100个模型变体进一步提高了鲁棒性。由此产生的系统在比赛排行榜上排名第三，总体Pearson r = 0.2094，并在所有参与者中实现了最高的单区域峰值分数（平均r = 0.63），尤其在最具挑战性的受试者（受试者5）上取得了显著提升。该方法为未来的多模态脑编码基准建立了一个简单、可扩展的基线。", "summary": "该论文介绍了一个分层多模态循环集成模型，旨在预测大脑对自然电影的皮层反应。该模型整合了视频、音频和语言嵌入，并通过双向RNNs处理时间动态，再结合主题特定的输出层。通过复合损失函数和课程学习进行训练，并在Algonauts 2025挑战赛中表现出色，排名第三，并取得了最高的单区域峰值分数，为多模态脑编码研究提供了可靠的基线方法。", "keywords": "多模态, 循环神经网络, 大脑响应预测, fMRI, Algonauts 2025", "comments": "该论文的创新之处在于其分层多模态循环集成模型，有效整合了多种信息源来预测复杂的脑响应。其在Algonauts 2025挑战赛中取得的优异成绩，特别是单区域峰值分数，凸显了其方法的有效性。此外，该方法被确立为一个简单且可扩展的基线，对于未来的多模态脑编码研究具有重要意义。"}}
{"id": "2507.15264", "title": "On exploration of an interior mirror descent flow for stochastic nonconvex constrained problem", "authors": ["Kuangyu Ding", "Kim-Chuan Toh"], "categories": ["math.OC", "cs.LG"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      34 Pages", "url": "http://arxiv.org/abs/2507.15264v3", "summary": "We study a nonsmooth nonconvex optimization problem defined over nonconvex\nconstraints, where the feasible set is given by the intersection of the closure\nof an open set and a smooth manifold. By endowing the open set with a\nRiemannian metric induced by a barrier function, we obtain a Riemannian\nsubgradient flow formulated as a differential inclusion, which remains strictly\nwithin the interior of the feasible set. This continuous dynamical system\nunifies two classes of iterative optimization methods, namely the Hessian\nbarrier method and mirror descent scheme, by revealing that these methods can\nbe interpreted as discrete approximations of the continuous flow. We explore\nthe long-term behavior of the trajectories generated by this dynamical system\nand show that the existing deficient convergence properties of the Hessian\nbarrier and mirror descent scheme can be unifily and more insightfully\ninterpreted through these of the continuous trajectory. For instance, the\nnotorious spurious stationary points \\cite{chen2024spurious} observed in\nHessian barrier method and mirror descent scheme are interpreted as stable\nequilibria of the dynamical system that do not correspond to real stationary\npoints of the original optimization problem. We provide two sufficient\ncondition such that these spurious stationary points can be avoided if the\nstrict complementarity conditions holds. In the absence of these regularity\ncondition, we propose a random perturbation strategy that ensures the\ntrajectory converges (subsequentially) to an approximate stationary point.\nBuilding on these insights, we introduce two iterative Riemannian subgradient\nmethods, form of interior point methods, that generalizes the existing Hessian\nbarrier method and mirror descent scheme for solving nonsmooth nonconvex\noptimization problems.", "comment": "34 Pages", "pdf_url": "http://arxiv.org/pdf/2507.15264v3", "cate": "math.OC", "date": "2025-07-21", "updated": "2025-07-25", "AI": {"title_translation": "关于随机非凸约束问题内点镜像下降流的探索", "tldr": "本文研究了非光滑非凸约束优化问题，通过引入黎曼度量和障碍函数，提出了一个内点黎曼次梯度流。该流统一了Hessian障碍法和镜像下降法，并提供了对现有方法收敛性缺陷（如虚假驻点）的更深入解释。在此基础上，提出了两种新的迭代黎曼次梯度方法。", "motivation": "研究一个定义在非凸约束上的非光滑非凸优化问题，并统一解释现有Hessian障碍法和镜像下降法的收敛性问题，特别是虚假驻点的出现。", "method": "通过为开放集赋予由障碍函数诱导的黎曼度量，得到了一个黎曼次梯度流，该流被表述为一个微分包含，并严格保持在可行集的内部。该连续动力系统将Hessian障碍法和镜像下降方案统一起来，揭示它们是连续流的离散近似。通过探索该动力系统的长期行为，解释了现有方法的收敛性缺陷。提出了避免虚假驻点的充分条件，并在缺乏正则性条件时，提出了随机扰动策略。", "result": "现有的Hessian障碍法和镜像下降方案的收敛性缺陷可以通过连续轨迹得到统一和更深入的解释。例如，这些方法中观察到的臭名昭著的虚假驻点被解释为动力系统的稳定平衡点，与原始优化问题的真实驻点不符。提供了两个充分条件，如果严格互补条件成立，可以避免这些虚假驻点。在缺乏正则性条件的情况下，随机扰动策略确保轨迹（子序列地）收敛到近似驻点。", "conclusion": "基于这些洞察，本文提出了两种迭代黎曼次梯度方法，它们是内点方法的变体，推广了现有用于解决非光滑非凸优化问题的Hessian障碍法和镜像下降方案。", "translation": "我们研究了一个定义在非凸约束上的非光滑非凸优化问题，其中可行集由一个开集的闭包和一个光滑流形的交集给出。通过为开集赋予由障碍函数诱导的黎曼度量，我们得到了一个表述为微分包含的黎曼次梯度流，该流严格保持在可行集的内部。这个连续动力系统统一了两类迭代优化方法，即Hessian障碍法和镜像下降方案，揭示了这些方法可以被解释为连续流的离散近似。我们探索了该动力系统生成的轨迹的长期行为，并表明Hessian障碍法和镜像下降方案现有的收敛性缺陷可以通过连续轨迹得到统一和更深入的解释。例如，Hessian障碍法和镜像下降方案中观察到的臭名昭著的虚假驻点被解释为动力系统的稳定平衡点，它们不对应于原始优化问题的真实驻点。我们提供了两个充分条件，使得在严格互补条件成立时可以避免这些虚假驻点。在缺乏这些正则性条件的情况下，我们提出了一种随机扰动策略，确保轨迹（子序列地）收敛到近似驻点。基于这些洞察，我们引入了两种迭代黎曼次梯度方法，它们是内点方法的形式，推广了现有用于解决非光滑非凸优化问题的Hessian障碍法和镜像下降方案。", "summary": "本文研究了在非凸约束下定义的非光滑非凸优化问题。作者引入了一个基于障碍函数诱导的黎曼度量的黎曼次梯度流，该流以微分包含的形式表示，并始终保持在可行集的内部。该连续动力系统成功地统一了Hessian障碍法和镜像下降方案，并揭示了它们是该连续流的离散近似。研究表明，通过分析该动力系统的长期行为，可以更深入地理解并统一解释现有方法中出现的收敛性问题，特别是虚假驻点（被解释为动力系统的稳定平衡点）。文章还提出了避免虚假驻点的充分条件，并在条件不满足时提出随机扰动策略以确保收敛。最后，基于这些理论洞察，提出了两种新的迭代黎曼次梯度内点方法，推广了现有算法。", "keywords": "非光滑非凸优化, 黎曼次梯度流, 内点法, Hessian障碍法, 镜像下降", "comments": "本文的创新之处在于提出了一个统一的连续动力系统（黎曼次梯度流），它不仅将Hessian障碍法和镜像下降法这两种重要的优化方法联系起来，还为理解它们在非凸问题中遇到的收敛性问题（特别是虚假驻点）提供了新的视角和更深刻的解释。通过将虚假驻点解释为动力系统的稳定平衡点，并提出避免条件和随机扰动策略，为设计更鲁棒的优化算法奠定了基础。"}}
{"id": "2507.18652", "title": "Fixed points of Personalized PageRank centrality: From irreducible to reducible networks", "authors": ["David Aleja", "Julio Flores", "Eva Primo", "Daniel Rodríguez", "Miguel Romance"], "categories": ["cs.SI", "15A60, 47J26"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      28 pages, 4 figures", "url": "http://arxiv.org/abs/2507.18652v1", "summary": "In this paper we analyze the PageRank of a complex network as a function of\nits personalization vector. By using this approach, a complete characterization\nof the existence and uniqueness of fixed points of PageRank of a graph is given\nin terms of the number and nature of its strongly connected components. The\nmethod presented includes the use of a feedback-PageRank in order to compute\nexactly the fixed points following the classic Power's Method in terms of the\n(left-hand) Perron vector of each strongly connected components.", "comment": "28 pages, 4 figures", "pdf_url": "http://arxiv.org/pdf/2507.18652v1", "cate": "cs.SI", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "个性化PageRank中心性的不动点：从不可约网络到可约网络", "tldr": "本文分析了复杂网络中个性化PageRank中心性的不动点，并根据其强连通分量的数量和性质，完整地描述了这些不动点的存在性和唯一性。", "motivation": "分析复杂网络中PageRank作为其个性化向量的函数，并完整地刻画图的PageRank不动点的存在性和唯一性。", "method": "通过使用反馈PageRank方法，并结合经典幂法根据每个强连通分量的（左手）佩伦向量，精确计算PageRank的不动点。", "result": "给出了图的PageRank不动点的存在性和唯一性的完整刻画，其描述基于图的强连通分量的数量和性质。", "conclusion": "成功地刻画了复杂网络中个性化PageRank中心性的不动点的存在性和唯一性，并将其与网络的强连通分量相关联。", "translation": "在本文中，我们分析了复杂网络中PageRank作为其个性化向量的函数。通过这种方法，根据图的强连通分量的数量和性质，给出了图的PageRank不动点存在性和唯一性的完整刻画。所提出的方法包括使用反馈PageRank，以便根据每个强连通分量的（左手）佩伦向量，按照经典的幂法精确计算不动点。", "summary": "本文深入分析了复杂网络中个性化PageRank中心性的不动点。研究提出了一种方法，能够根据网络的强连通分量的数量和性质，完整地刻画PageRank不动点的存在性和唯一性。该方法利用反馈PageRank，并结合经典幂法和每个强连通分量的佩伦向量来精确计算这些不动点，从而将PageRank分析扩展到更复杂的、可约的网络。", "keywords": "PageRank, 不动点, 个性化PageRank, 强连通分量, 可约网络", "comments": "本文的创新之处在于将PageRank不动点的分析扩展到可约网络，并通过利用强连通分量和反馈PageRank方法提供了不动点存在性和唯一性的完整刻画，这对于理解复杂网络中的信息传播和排名机制具有重要意义。"}}
{"id": "2507.18881", "title": "Perspective from a Higher Dimension: Can 3D Geometric Priors Help Visual Floorplan Localization?", "authors": ["Bolei Chen", "Jiaxu Kang", "Haonan Yang", "Ping Zhong", "Jianxin Wang"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ACM MM 2025", "url": "http://arxiv.org/abs/2507.18881v1", "summary": "Since a building's floorplans are easily accessible, consistent over time,\nand inherently robust to changes in visual appearance, self-localization within\nthe floorplan has attracted researchers' interest. However, since floorplans\nare minimalist representations of a building's structure, modal and geometric\ndifferences between visual perceptions and floorplans pose challenges to this\ntask. While existing methods cleverly utilize 2D geometric features and pose\nfilters to achieve promising performance, they fail to address the localization\nerrors caused by frequent visual changes and view occlusions due to variously\nshaped 3D objects. To tackle these issues, this paper views the 2D Floorplan\nLocalization (FLoc) problem from a higher dimension by injecting 3D geometric\npriors into the visual FLoc algorithm. For the 3D geometric prior modeling, we\nfirst model geometrically aware view invariance using multi-view constraints,\ni.e., leveraging imaging geometric principles to provide matching constraints\nbetween multiple images that see the same points. Then, we further model the\nview-scene aligned geometric priors, enhancing the cross-modal geometry-color\ncorrespondences by associating the scene's surface reconstruction with the RGB\nframes of the sequence. Both 3D priors are modeled through self-supervised\ncontrastive learning, thus no additional geometric or semantic annotations are\nrequired. These 3D priors summarized in extensive realistic scenes bridge the\nmodal gap while improving localization success without increasing the\ncomputational burden on the FLoc algorithm. Sufficient comparative studies\ndemonstrate that our method significantly outperforms state-of-the-art methods\nand substantially boosts the FLoc accuracy. All data and code will be released\nafter the anonymous review.", "comment": "Accepted by ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2507.18881v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "来自更高维度的视角：3D几何先验能否帮助视觉平面图定位？", "tldr": "本文通过注入自监督学习的3D几何先验，解决了视觉平面图定位中因视觉变化和遮挡导致的错误，显著提升了定位精度。", "motivation": "尽管平面图易于获取且稳定，但视觉感知与平面图之间的模态和几何差异，以及频繁的视觉变化和3D物体遮挡导致的定位误差，对视觉平面图定位提出了挑战。现有2D方法未能有效解决这些问题。", "method": "本文通过注入3D几何先验来解决2D平面图定位问题。具体方法包括：1) 利用多视图约束建模几何感知的视图不变性，即利用成像几何原理提供多图像间的匹配约束；2) 通过关联场景表面重建与RGB帧，进一步建模视图-场景对齐的几何先验。这两种3D先验均通过自监督对比学习建模，无需额外标注。", "result": "本文提出的方法能够弥合模态差距，在不增加计算负担的情况下提高定位成功率。充分的比较研究表明，该方法显著优于现有最先进的方法，并大幅提升了平面图定位的精度。", "conclusion": "通过引入自监督学习的3D几何先验，本研究成功克服了视觉平面图定位中由视觉变化和遮挡引起的挑战，显著提升了定位性能，为该领域提供了新的视角和有效解决方案。", "translation": "由于建筑物的平面图易于获取、随时间保持一致且本质上对视觉外观变化具有鲁棒性，因此在平面图内进行自定位吸引了研究人员的兴趣。然而，由于平面图是建筑物结构的极简表示，视觉感知与平面图之间的模态和几何差异给这项任务带来了挑战。尽管现有方法巧妙地利用2D几何特征和姿态滤波器取得了可喜的性能，但它们未能解决由频繁视觉变化和各种形状3D物体造成的视图遮挡引起的定位误差。为了解决这些问题，本文通过将3D几何先验注入到视觉平面图定位（FLoc）算法中，从更高维度审视2D平面图定位问题。对于3D几何先验建模，我们首先利用多视图约束建模几何感知的视图不变性，即利用成像几何原理提供多个看到相同点的图像之间的匹配约束。然后，我们通过将场景的表面重建与序列的RGB帧相关联，进一步建模视图-场景对齐的几何先验。这两种3D先验都通过自监督对比学习建模，因此不需要额外的几何或语义标注。这些在广泛真实场景中总结的3D先验弥合了模态差距，同时在不增加FLoc算法计算负担的情况下提高了定位成功率。充分的比较研究表明，我们的方法显著优于最先进的方法，并大幅提升了FLoc精度。所有数据和代码将在匿名评审后发布。", "summary": "本文提出了一种新颖的视觉平面图定位（FLoc）方法，通过从更高维度引入3D几何先验来解决现有2D方法在面对视觉变化和遮挡时的局限性。该方法利用自监督对比学习建模两种3D先验：几何感知的视图不变性和视图-场景对齐的几何先验，无需额外标注。实验证明，该方法有效弥合了视觉模态与平面图之间的差距，显著提升了定位精度和成功率，同时不增加计算负担，优于现有最先进的方法。", "keywords": "平面图定位, 3D几何先验, 自监督学习, 视觉定位, 视图不变性", "comments": "本文的创新点在于将3D几何先验引入2D平面图定位问题，有效解决了传统2D方法在复杂视觉环境下的局限性。通过自监督学习建模3D先验，避免了繁琐的标注工作，提高了方法的实用性。该方法在不增加计算负担的情况下显著提升了定位精度，具有重要的实际应用价值。"}}
{"id": "2507.14554", "title": "Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review", "authors": ["Ruoyu Su", "Noman Ahmad", "Matteo Esposito", "Andrea Janes", "Davide Taibi", "Valentina Lenarduzzi"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.14554v2", "summary": "Software architecture plays a central role in the design, development, and\nmaintenance of software systems. With the rise of cloud computing,\nmicroservices, and containers, architectural practices have diversified.\nUnderstanding these shifts is vital. This study analyzes software architecture\ntrends across eight leading industry conferences over five years. We\ninvestigate the evolution of software architecture by analyzing talks from top\npractitioner conferences, focusing on the motivations and contexts driving\ntechnology adoption. We analyzed 5,677 talks from eight major industry\nconferences, using large language models and expert validation to extract\ntechnologies, their purposes, and usage contexts. We also explored how\ntechnologies interrelate and fit within DevOps and deployment pipelines. Among\n450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate\nby frequency and centrality. Practitioners present technology mainly related to\ndeployment, communication, AI, and observability. We identify five technology\ncommunities covering automation, coordination, cloud AI, monitoring, and\ncloud-edge. Most technologies span multiple DevOps stages and support hybrid\ndeployment. Our study reveals that a few core technologies, like Kubernetes and\nServerless, dominate the contemporary software architecture practice. These are\nmainly applied in later DevOps stages, with limited focus on early phases like\nplanning and coding. We also show how practitioners frame technologies by\npurpose and context, reflecting evolving industry priorities. Finally, we\nobserve how only research can provide a more holistic lens on architectural\ndesign, quality, and evolution.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.14554v2", "cate": "cs.SE", "date": "2025-07-19", "updated": "2025-07-25", "AI": {"title_translation": "软件架构的新兴趋势：从实践者视角进行的五年回顾", "tldr": "一项对过去五年八个主要行业会议上5,677场演讲的分析显示，Kubernetes、云原生、无服务器和容器等核心技术在软件架构实践中占据主导地位，主要应用于DevOps的后期阶段。", "motivation": "随着云计算、微服务和容器的兴起，软件架构实践日益多样化，理解这些转变对于设计、开发和维护软件系统至关重要，因此本研究旨在分析和理解软件架构的新兴趋势。", "method": "本研究分析了过去五年中八个领先行业会议的5,677场演讲，利用大型语言模型和专家验证来提取技术、其目的和使用背景。研究还探讨了技术如何相互关联并融入DevOps和部署管道。", "result": "在分析的450项技术中，Kubernetes、云原生、无服务器和容器在频率和中心性上占据主导地位。实践者主要关注与部署、通信、AI和可观察性相关的技术。研究识别出涵盖自动化、协调、云AI、监控和云-边缘的五大技术社区。大多数技术跨越多个DevOps阶段并支持混合部署。研究发现，核心技术如Kubernetes和无服务器主导了当前的软件架构实践，主要应用于DevOps的后期阶段，而对规划和编码等早期阶段的关注有限。", "conclusion": "本研究揭示了实践者视角下软件架构的核心趋势，并指出只有研究才能为架构设计、质量和演进提供更全面的视角。", "translation": "软件架构在软件系统的设计、开发和维护中扮演着核心角色。随着云计算、微服务和容器的兴起，架构实践变得多样化。理解这些转变至关重要。本研究分析了过去五年中八个领先行业会议的软件架构趋势。我们通过分析顶级实践者会议的演讲来调查软件架构的演变，重点关注驱动技术采纳的动机和背景。我们分析了八个主要行业会议的5,677场演讲，使用大型语言模型和专家验证来提取技术、其目的和使用背景。我们还探讨了技术如何相互关联并融入DevOps和部署管道。在450项技术中，Kubernetes、云原生、无服务器和容器在频率和中心性上占据主导地位。实践者主要呈现与部署、通信、AI和可观察性相关的技术。我们识别出涵盖自动化、协调、云AI、监控和云-边缘的五大技术社区。大多数技术跨越多个DevOps阶段并支持混合部署。我们的研究表明，少数核心技术，如Kubernetes和无服务器，在当代软件架构实践中占据主导地位。这些技术主要应用于DevOps的后期阶段，而对规划和编码等早期阶段的关注有限。我们还展示了实践者如何根据目的和背景来构建技术，反映了不断变化的行业优先事项。最后，我们观察到只有研究才能为架构设计、质量和演进提供更全面的视角。", "summary": "本研究对过去五年中八个主要行业会议的5,677场演讲进行了分析，以揭示软件架构从实践者视角的新兴趋势。通过使用大型语言模型和专家验证，研究发现Kubernetes、云原生、无服务器和容器等核心技术在当代实践中占据主导地位，主要应用于DevOps的后期阶段。研究还识别了关键技术社区，并指出实践者如何根据目的和背景来采纳技术，反映了行业优先级的演变。", "keywords": "软件架构, 实践者视角, 云原生, DevOps, Kubernetes", "comments": "本研究通过大规模分析行业会议演讲，为软件架构的实践趋势提供了数据驱动的洞察，其利用大型语言模型进行数据提取是方法上的创新点。研究揭示了云原生技术的普遍性以及实践者对DevOps后期阶段的侧重。然而，它也指出，仅凭实践者视角不足以提供架构设计、质量和演进的全面视图，强调了研究的必要性。"}}
{"id": "2411.04576", "title": "\"I Always Felt that SomethingWasWrong.\": Understanding Compliance Risks and Mitigation Strategies when Highly-Skilled Compliance Knowledge Workers Use Large Language Models", "authors": ["Siying Hu", "Piaohong Wang", "Ka I Chan", "Yaxing Yao", "Zhicong Lu"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2411.04576v3", "summary": "The rapid advancement of Large Language Models (LLMs) has transformed\nknowledge-intensive has led to its widespread usage by knowledge workers to\nenhance their productivity. As these professionals handle sensitive\ninformation, and the training of text-based GenAI models involves the use of\nextensive data, there are thus concerns about privacy, security, and broader\ncompliance with regulations and laws. While existing research has addressed\nprivacy and security concerns, the specific compliance risks faced by\nhighly-skilled knowledge workers when using the LLMs, and their mitigation\nstrategies, remain underexplored. As understanding these risks and strategies\nis crucial for the development of industry-specific compliant LLM mechanisms,\nthis research conducted semi-structured interviews with 24 knowledge workers\nfrom knowledge-intensive industries to understand their practices and\nexperiences when integrating LLMs into their workflows. Our research explored\nhow these workers ensure compliance and the resources and challenges they\nencounter when minimizing risks. Our preliminary findings showed that knowledge\nworkers were concerned about the leakage of sensitive information and took\nproactive measures such as distorting input data and limiting prompt details to\nmitigate such risks. Their ability to identify and mitigate risks, however, was\nsignificantly hampered by a lack of LLM-specific compliance guidance and\ntraining. Our findings highlight the importance of improving knowledge workers'\ncompliance awareness and establishing support systems and compliance cultures\nwithin organizations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2411.04576v3", "cate": "cs.HC", "date": "2024-11-07", "updated": "2025-07-25", "AI": {"title_translation": "“我总觉得哪里不对劲。”：了解高技能合规知识工作者使用大型语言模型时的合规风险与缓解策略", "tldr": "本研究探讨了高技能知识工作者在使用大型语言模型（LLMs）时面临的合规风险和缓解策略，发现他们主要担心敏感信息泄露，并采取了主动措施，但缺乏LLM特定的合规指导和培训严重阻碍了其风险识别和缓解能力，因此强调提高合规意识和建立组织支持系统的重要性。", "motivation": "大型语言模型（LLMs）的快速发展已推动其在知识密集型工作中的广泛应用，以提高知识工作者的生产力。然而，由于这些专业人员处理敏感信息，且基于文本的生成式AI模型训练涉及大量数据，因此引发了对隐私、安全以及更广泛的法规和法律合规性的担忧。现有研究已解决了隐私和安全问题，但高技能知识工作者在使用LLMs时面临的具体合规风险及其缓解策略仍未得到充分探索。理解这些风险和策略对于开发行业特定、符合合规要求的LLM机制至关重要，这是本研究的动机。", "method": "本研究对24位来自知识密集型行业的知识工作者进行了半结构化访谈，以了解他们将大型语言模型整合到工作流程中的实践和经验。研究还探讨了这些工作者如何确保合规性，以及他们在最小化风险时遇到的资源和挑战。", "result": "初步研究结果表明，知识工作者担心敏感信息泄露，并采取了主动措施来缓解此类风险，例如扭曲输入数据和限制提示细节。然而，他们识别和缓解风险的能力因缺乏LLM特定的合规指导和培训而受到严重阻碍。", "conclusion": "本研究结果强调了提高知识工作者合规意识以及在组织内部建立支持系统和合规文化的重要性。", "translation": "大型语言模型（LLMs）的快速发展已经改变了知识密集型工作，并导致知识工作者广泛使用它们来提高生产力。由于这些专业人员处理敏感信息，并且基于文本的生成式AI模型训练涉及使用大量数据，因此存在关于隐私、安全和更广泛法规与法律合规性的担忧。尽管现有研究已解决了隐私和安全问题，但高技能知识工作者在使用LLMs时面临的具体合规风险及其缓解策略仍未得到充分探索。由于理解这些风险和策略对于开发行业特定的合规LLM机制至关重要，本研究对24位来自知识密集型行业的知识工作者进行了半结构化访谈，以了解他们在将LLMs整合到工作流程中的实践和经验。我们的研究探讨了这些工作者如何确保合规性以及他们在最小化风险时遇到的资源和挑战。我们的初步发现表明，知识工作者担心敏感信息泄露，并采取了主动措施，例如扭曲输入数据和限制提示细节，以缓解此类风险。然而，他们识别和缓解风险的能力因缺乏LLM特定的合规指导和培训而受到严重阻碍。我们的研究结果强调了提高知识工作者合规意识以及在组织内部建立支持系统和合规文化的重要性。", "summary": "本研究旨在探讨高技能知识工作者在使用大型语言模型（LLMs）时面临的合规风险及其缓解策略。通过对24位知识工作者的半结构化访谈，研究发现受访者主要担忧敏感信息泄露，并会采取扭曲输入数据、限制提示细节等主动措施来规避风险。然而，由于缺乏LLM特定的合规指导和培训，他们识别和缓解风险的能力受到限制。研究结果强调了提高知识工作者合规意识以及在组织内部建立支持系统和合规文化的重要性。", "keywords": "大型语言模型, 合规风险, 知识工作者, 缓解策略, 信息泄露", "comments": "这项研究填补了现有文献中关于高技能知识工作者在使用LLMs时合规风险的空白，特别是关注了“人”的因素。其创新之处在于通过访谈深入了解了用户实际的担忧和应对策略。研究结果揭示了现有指导和培训的不足，为组织制定LLM使用策略和培训提供了重要指导，具有较强的实践指导意义。"}}
{"id": "2505.18380", "title": "RedactOR: An LLM-Powered Framework for Automatic Clinical Data De-Identification", "authors": ["Praphul Singh", "Charlotte Dzialo", "Jangwon Kim", "Sumana Srivatsa", "Irfan Bulu", "Sri Gadde", "Krishnaram Kenthapadi"], "categories": ["cs.AI", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      Accepted to ACL 2025 Industry Track. To appear", "url": "http://arxiv.org/abs/2505.18380v2", "summary": "Ensuring clinical data privacy while preserving utility is critical for\nAI-driven healthcare and data analytics. Existing de-identification (De-ID)\nmethods, including rule-based techniques, deep learning models, and large\nlanguage models (LLMs), often suffer from recall errors, limited\ngeneralization, and inefficiencies, limiting their real-world applicability. We\npropose a fully automated, multi-modal framework, RedactOR for de-identifying\nstructured and unstructured electronic health records, including clinical audio\nrecords. Our framework employs cost-efficient De-ID strategies, including\nintelligent routing, hybrid rule and LLM based approaches, and a two-step audio\nredaction approach. We present a retrieval-based entity relexicalization\napproach to ensure consistent substitutions of protected entities, thereby\nenhancing data coherence for downstream applications. We discuss key design\ndesiderata, de-identification and relexicalization methodology, and modular\narchitecture of RedactOR and its integration with the Oracle Health Clinical AI\nsystem. Evaluated on the i2b2 2014 De-ID dataset using standard metrics with\nstrict recall, our approach achieves competitive performance while optimizing\ntoken usage to reduce LLM costs. Finally, we discuss key lessons and insights\nfrom deployment in real-world AI- driven healthcare data pipelines.", "comment": "Accepted to ACL 2025 Industry Track. To appear", "pdf_url": "http://arxiv.org/pdf/2505.18380v2", "cate": "cs.AI", "date": "2025-05-23", "updated": "2025-07-24", "AI": {"title_translation": "RedactOR：一个基于LLM的临床数据自动去识别框架", "tldr": "RedactOR是一个LLM驱动的多模态框架，用于对临床数据（包括音频记录）进行去识别，通过智能路由、混合规则和LLM方法以及双步音频编辑来提高效率并降低成本，并在i2b2 2014数据集上表现出竞争力。", "motivation": "在保证数据可用性的同时确保临床数据隐私对AI驱动的医疗保健和数据分析至关重要。现有的去识别（De-ID）方法（包括基于规则的技术、深度学习模型和大型语言模型）常存在召回错误、泛化能力有限和效率低下等问题，限制了其实际应用。", "method": "我们提出了一个名为RedactOR的全自动、多模态框架，用于去识别结构化和非结构化电子健康记录，包括临床音频记录。该框架采用成本效益高的De-ID策略，包括智能路由、混合规则和基于LLM的方法，以及两步式音频编辑方法。我们还提出了一个基于检索的实体重新词化方法，以确保受保护实体的一致性替换，从而增强下游应用的数据连贯性。", "result": "在i2b2 2014 De-ID数据集上使用严格召回的标准指标进行评估，我们的方法在优化token使用以降低LLM成本的同时，取得了具有竞争力的性能。", "conclusion": "论文讨论了RedactOR的关键设计要求、去识别和重新词化方法、模块化架构及其与Oracle Health Clinical AI系统的集成，并分享了在真实世界AI驱动医疗保健数据管道中部署的关键经验和见解。", "translation": "在保证数据可用性的同时确保临床数据隐私对AI驱动的医疗保健和数据分析至关重要。现有的去识别（De-ID）方法，包括基于规则的技术、深度学习模型和大型语言模型（LLMs），常常存在召回错误、泛化能力有限和效率低下等问题，限制了其实际应用。我们提出了一个全自动、多模态框架RedactOR，用于去识别结构化和非结构化电子健康记录，包括临床音频记录。我们的框架采用了成本效益高的De-ID策略，包括智能路由、混合规则和基于LLM的方法，以及两步式音频编辑方法。我们提出了一个基于检索的实体重新词化方法，以确保受保护实体的一致性替换，从而增强下游应用的数据连贯性。我们讨论了RedactOR的关键设计要求、去识别和重新词化方法、模块化架构及其与Oracle Health Clinical AI系统的集成。在i2b2 2014 De-ID数据集上使用严格召回的标准指标进行评估，我们的方法在优化token使用以降低LLM成本的同时，取得了具有竞争力的性能。最后，我们讨论了在真实世界AI驱动医疗保健数据管道中部署的关键经验和见解。", "summary": "RedactOR是一个新颖的、基于LLM的多模态框架，旨在解决现有临床数据去识别方法的不足，如召回错误、泛化性差和效率低下。它通过智能路由、混合规则与LLM结合以及两步音频编辑等成本效益策略，实现了对结构化和非结构化电子健康记录（包括音频）的自动化去识别。该框架还引入了基于检索的实体重新词化方法，以保证数据一致性。在i2b2 2014数据集上的评估表明，RedactOR在降低LLM成本的同时，实现了有竞争力的性能。", "keywords": "临床数据去识别, LLM, RedactOR, 医疗保健AI, 数据隐私", "comments": "RedactOR的创新之处在于其多模态处理能力（包括音频记录）、结合LLM与传统规则的混合方法以及成本效益优化策略。其提出的实体重新词化方法对于保持去识别后数据的可用性至关重要。该框架在真实世界的部署经验也增加了其重要性，表明其具有实际应用价值。"}}
{"id": "2506.11821", "title": "Framework of a multiscale data-driven DT of the musculoskeletal system", "authors": ["Martina Paccini", "Simone Cammarasana", "Giuseppe Patanè"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.11821v2", "summary": "Musculoskeletal disorders (MSDs) are a leading cause of disability worldwide,\nrequiring advanced diagnostic and therapeutic tools for personalised assessment\nand treatment. Effective management of MSDs involves the interaction of\nheterogeneous data sources, making the Digital Twin (DT) paradigm a valuable\noption. This paper introduces the Musculoskeletal Digital Twin (MS-DT), a novel\nframework that integrates multiscale biomechanical data with computational\nmodelling to create a detailed, patient-specific representation of the\nmusculoskeletal system. By combining motion capture, ultrasound imaging,\nelectromyography, and medical imaging, the MS-DT enables the analysis of spinal\nkinematics, posture, and muscle function. An interactive visualisation platform\nprovides clinicians and researchers with an intuitive interface for exploring\nbiomechanical parameters and tracking patient-specific changes. Results\ndemonstrate the effectiveness of MS-DT in extracting precise kinematic and\ndynamic tissue features, offering a comprehensive tool for monitoring spine\nbiomechanics and rehabilitation. This framework provides high-fidelity\nmodelling and real-time visualization to improve patient-specific diagnosis and\nintervention planning.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.11821v2", "cate": "eess.IV", "date": "2025-06-13", "updated": "2025-07-25", "AI": {"title_translation": "多尺度数据驱动的肌肉骨骼系统数字孪生框架", "tldr": "本文介绍了肌肉骨骼数字孪生（MS-DT），一个集成了多尺度生物力学数据和计算模型的框架，用于个性化的肌肉骨骼系统分析、诊断和干预规划。", "motivation": "肌肉骨骼疾病（MSD）是全球致残的主要原因，需要先进的诊断和治疗工具进行个性化评估和治疗。MSD的有效管理涉及异构数据源的交互，因此数字孪生（DT）范式成为一个有价值的选择。", "method": "本文引入了肌肉骨骼数字孪生（MS-DT）框架。它将多尺度生物力学数据（包括运动捕捉、超声成像、肌电图和医学成像）与计算模型相结合，以创建肌肉骨骼系统的详细、患者特定的表示。该框架能够分析脊柱运动学、姿势和肌肉功能，并提供一个交互式可视化平台。", "result": "结果表明，MS-DT在提取精确的运动学和动态组织特征方面是有效的。它为监测脊柱生物力学和康复提供了一个综合工具，并提供了高保真建模和实时可视化。", "conclusion": "MS-DT框架通过提供高保真建模和实时可视化，改进了肌肉骨骼疾病的患者特定诊断和干预计划。", "translation": "肌肉骨骼疾病 (MSD) 是全球致残的主要原因，需要先进的诊断和治疗工具进行个性化评估和治疗。MSD 的有效管理涉及异构数据源的交互，这使得数字孪生 (DT) 范式成为一个有价值的选择。本文介绍了肌肉骨骼数字孪生 (MS-DT)，这是一个新颖的框架，它将多尺度生物力学数据与计算模型相结合，以创建肌肉骨骼系统详细的、患者特定的表示。通过结合运动捕捉、超声成像、肌电图和医学成像，MS-DT 能够分析脊柱运动学、姿势和肌肉功能。一个交互式可视化平台为临床医生和研究人员提供了探索生物力学参数和跟踪患者特定变化的直观界面。结果证明了 MS-DT 在提取精确的运动学和动态组织特征方面的有效性，为监测脊柱生物力学和康复提供了一个综合工具。该框架提供了高保真建模和实时可视化，以改进患者特定的诊断和干预计划。", "summary": "本文提出了一种新颖的肌肉骨骼数字孪生（MS-DT）框架，旨在增强肌肉骨骼疾病（MSD）的个性化评估和治疗。MS-DT整合了运动捕捉、超声、肌电图和医学成像等多尺度生物力学数据与计算模型，以创建详细的、患者特定的数字表示。该框架能够全面分析脊柱运动学、姿势和肌肉功能，并由交互式可视化平台支持。结果表明，该框架在提取精确的运动学和动态组织特征方面有效，提供了一个高保真建模和实时可视化工具，从而改善了MSD的患者特定诊断和干预计划。", "keywords": "肌肉骨骼数字孪生, 生物力学, 多尺度数据, 个性化医疗, 脊柱运动学", "comments": "本文将数字孪生概念创新性地应用于肌肉骨骼系统，解决了MSD管理中对个性化和精确工具的关键需求。其优势在于整合了多样化的数据源和计算模型，以创建高保真的、患者特定表示。交互式可视化平台是一个重要的实际优势。其创新之处在于在这个特定医疗领域中，为数字孪生实现了全面的多尺度数据集成，为临床决策提供了实时洞察。"}}
{"id": "2502.04327", "title": "Value-Based Deep RL Scales Predictably", "authors": ["Oleh Rybkin", "Michal Nauman", "Preston Fu", "Charlie Snell", "Pieter Abbeel", "Sergey Levine", "Aviral Kumar"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      ICML 2025", "url": "http://arxiv.org/abs/2502.04327v2", "summary": "Scaling data and compute is critical to the success of modern ML. However,\nscaling demands predictability: we want methods to not only perform well with\nmore compute or data, but also have their performance be predictable from\nsmall-scale runs, without running the large-scale experiment. In this paper, we\nshow that value-based off-policy RL methods are predictable despite community\nlore regarding their pathological behavior. First, we show that data and\ncompute requirements to attain a given performance level lie on a Pareto\nfrontier, controlled by the updates-to-data (UTD) ratio. By estimating this\nfrontier, we can predict this data requirement when given more compute, and\nthis compute requirement when given more data. Second, we determine the optimal\nallocation of a total resource budget across data and compute for a given\nperformance and use it to determine hyperparameters that maximize performance\nfor a given budget. Third, this scaling is enabled by first estimating\npredictable relationships between hyperparameters, which is used to manage\neffects of overfitting and plasticity loss unique to RL. We validate our\napproach using three algorithms: SAC, BRO, and PQL on DeepMind Control, OpenAI\ngym, and IsaacGym, when extrapolating to higher levels of data, compute,\nbudget, or performance.", "comment": "ICML 2025", "pdf_url": "http://arxiv.org/pdf/2502.04327v2", "cate": "cs.LG", "date": "2025-02-06", "updated": "2025-07-25", "AI": {"title_translation": "基于价值的深度强化学习可预测地扩展", "tldr": "本文证明基于价值的离策略强化学习方法在扩展时具有可预测性，并提出了预测和优化资源分配的方法。", "motivation": "现代机器学习的成功依赖于数据和计算的扩展，但这需要可预测性，即能够从小规模运行中预测大规模性能。然而，基于价值的强化学习方法通常被认为表现出病态行为，缺乏可预测性。", "method": "作者首先证明了达到给定性能水平所需的数据和计算资源存在一个帕累托前沿，由更新-数据比（UTD）控制，并可用于预测资源需求。其次，他们确定了在给定性能下数据和计算的总资源预算的最佳分配，并用其来确定在给定预算下最大化性能的超参数。第三，这种扩展性是通过估计超参数之间可预测的关系来实现的，这有助于管理强化学习特有的过拟合和可塑性损失效应。该方法在SAC、BRO和PQL算法上，以及DeepMind Control、OpenAI gym和IsaacGym基准上进行了验证。", "result": "研究表明，基于价值的离策略强化学习方法在扩展时具有可预测性，这与社区普遍看法相反。通过估计帕累托前沿，可以预测在给定更多计算或数据时的资源需求。此外，该方法能够确定最佳资源分配和超参数以最大化给定预算下的性能。", "conclusion": "基于价值的深度强化学习方法能够以可预测的方式进行扩展，并且通过本文提出的方法可以有效地管理和优化其在数据、计算和超参数方面的扩展行为。", "translation": "现代机器学习的成功离不开数据和计算的扩展。然而，扩展需要可预测性：我们希望方法不仅在拥有更多计算或数据时表现良好，而且其性能可以从小规模运行中预测，而无需运行大规模实验。在本文中，我们展示了基于价值的离策略强化学习方法是可预测的，尽管社区流传着关于其病态行为的说法。首先，我们展示了达到给定性能水平所需的数据和计算需求位于一个帕累托前沿上，由更新-数据比（UTD）控制。通过估计这个前沿，我们可以在给定更多计算时预测数据需求，在给定更多数据时预测计算需求。其次，我们确定了在给定性能下数据和计算的总资源预算的最佳分配，并用其来确定在给定预算下最大化性能的超参数。第三，这种扩展性是通过首先估计超参数之间可预测的关系来实现的，这用于管理强化学习特有的过拟合和可塑性损失效应。我们在DeepMind Control、OpenAI gym和IsaacGym上使用三种算法：SAC、BRO和PQL验证了我们的方法，在推断到更高水平的数据、计算、预算或性能时。", "summary": "本文挑战了基于价值的离策略强化学习方法在扩展时表现出病态行为的传统观念，证明它们实际上是可预测的。研究提出了一种方法，通过识别数据和计算需求之间的帕累托前沿来预测性能，并优化资源分配（数据与计算）和超参数以最大化给定预算下的性能。该方法考虑了强化学习特有的过拟合和可塑性损失，并在多个算法和环境中得到验证，为深度强化学习的可预测扩展提供了新的视角。", "keywords": "深度强化学习, 可预测性, 价值函数, 资源扩展, 超参数优化", "comments": "这篇论文的创新点在于它挑战了强化学习领域中关于基于价值方法扩展性差的普遍看法，并提供了量化的证据和方法来证明其可预测性。通过引入帕累托前沿的概念来管理数据和计算需求，以及优化超参数，该研究为深度强化学习的实际应用和大规模部署提供了重要的指导，有助于更有效地利用计算资源和数据。"}}
{"id": "2507.06230", "title": "Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion", "authors": ["Aleksandar Jevtić", "Christoph Reich", "Felix Wimbauer", "Oliver Hahn", "Christian Rupprecht", "Stefan Roth", "Daniel Cremers"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Christoph Reich and Aleksandar Jevtić - both authors contributed equally. Code: this https URL Project page: this https URL", "url": "http://arxiv.org/abs/2507.06230v2", "summary": "Semantic scene completion (SSC) aims to infer both the 3D geometry and\nsemantics of a scene from single images. In contrast to prior work on SSC that\nheavily relies on expensive ground-truth annotations, we approach SSC in an\nunsupervised setting. Our novel method, SceneDINO, adapts techniques from\nself-supervised representation learning and 2D unsupervised scene understanding\nto SSC. Our training exclusively utilizes multi-view consistency\nself-supervision without any form of semantic or geometric ground truth. Given\na single input image, SceneDINO infers the 3D geometry and expressive 3D DINO\nfeatures in a feed-forward manner. Through a novel 3D feature distillation\napproach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised\nscene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.\nLinear probing our 3D features matches the segmentation accuracy of a current\nsupervised SSC approach. Additionally, we showcase the domain generalization\nand multi-view consistency of SceneDINO, taking the first steps towards a\nstrong foundation for single image 3D scene understanding.", "comment": "ICCV 2025. Christoph Reich and Aleksandar Jevti\\'c - both authors\n  contributed equally. Code: https://github.com/tum-vision/scenedino Project\n  page: https://visinf.github.io/scenedino", "pdf_url": "http://arxiv.org/pdf/2507.06230v2", "cate": "cs.CV", "date": "2025-07-08", "updated": "2025-07-25", "AI": {"title_translation": "前向SceneDINO用于无监督语义场景补全", "tldr": "SceneDINO是一种新的无监督方法，用于从单张图像中推断3D几何和语义，通过自监督学习和多视图一致性实现了最先进的分割精度。", "motivation": "现有的语义场景补全（SSC）方法严重依赖昂贵的真值标注。本文旨在在无监督设置下解决SSC问题。", "method": "本文提出了一种名为SceneDINO的新方法，它将自监督表示学习和二维无监督场景理解技术应用于语义场景补全。训练过程仅利用多视图一致性自监督，不依赖任何形式的语义或几何真值。SceneDINO以一种前向方式从单张输入图像推断3D几何和富有表现力的3D DINO特征。通过一种新颖的3D特征蒸馏方法，实现了无监督的3D语义。", "result": "在3D和2D无监督场景理解中，SceneDINO均达到了最先进的分割精度。对3D特征进行线性探测，其分割精度与当前监督式SSC方法相当。此外，SceneDINO还展示了领域泛化能力和多视图一致性。", "conclusion": "SceneDINO为单图像3D场景理解奠定了坚实的基础，通过无监督方法实现了与监督方法相当的性能，并展现了良好的泛化能力。", "translation": "语义场景补全（SSC）旨在从单张图像中推断场景的3D几何和语义。与之前严重依赖昂贵真值标注的SSC工作不同，我们以无监督的方式处理SSC。我们新颖的方法SceneDINO，将自监督表示学习和2D无监督场景理解的技术应用于SSC。我们的训练完全利用多视图一致性自监督，不使用任何形式的语义或几何真值。给定单张输入图像，SceneDINO以前向方式推断3D几何和富有表现力的3D DINO特征。通过一种新颖的3D特征蒸馏方法，我们获得了无监督的3D语义。在3D和2D无监督场景理解中，SceneDINO均达到了最先进的分割精度。对我们的3D特征进行线性探测，其分割精度与当前监督式SSC方法相当。此外，我们展示了SceneDINO的领域泛化能力和多视图一致性，为单图像3D场景理解的强大基础迈出了第一步。", "summary": "本文提出了一种名为SceneDINO的无监督语义场景补全（SSC）方法，旨在解决传统SSC方法对昂贵真值标注的依赖。SceneDINO利用自监督表示学习和多视图一致性进行训练，无需任何语义或几何真值。该方法能够从单张图像前向推断3D几何和3D DINO特征，并通过3D特征蒸馏获得无监督的3D语义。实验结果表明，SceneDINO在无监督场景理解中达到了最先进的分割精度，并且其3D特征的分割精度可与监督式SSC方法媲美，同时展示了良好的领域泛化能力和多视图一致性。", "keywords": "无监督语义场景补全, 自监督学习, 3D场景理解, DINO特征, 多视图一致性", "comments": "本文最大的创新在于提出了一种完全无监督的语义场景补全方法，成功摆脱了对大量标注数据的依赖，这对于3D视觉领域是一个重要的进步。其将2D自监督学习技术巧妙地扩展到3D场景理解，并通过特征蒸馏实现语义推断，展现了方法的新颖性和有效性。该研究为未来更可扩展和通用的3D场景理解模型奠定了基础。"}}
{"id": "2507.18858", "title": "Weak-to-Strong Generalization with Failure Trajectories: A Tree-based Approach to Elicit Optimal Policy in Strong Models", "authors": ["Ruimeng Ye", "Zihan Wang", "Yang Xiao", "Zinan Ling", "Manling Li", "Bo Hui"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18858v2", "summary": "Weak-to-Strong generalization (W2SG) is a new trend to elicit the full\ncapabilities of a strong model with supervision from a weak model. While\nexisting W2SG studies focus on simple tasks like binary classification, we\nextend this paradigm to complex interactive decision-making environments.\nSpecifically, we fine-tune a strong model with trajectories of intermediate\nactions generated by a weak model. Motivated by the human learning process, we\npropose to generalize not only success knowledge but also failure experience so\nthat the strong model can learn from failed trajectories accumulated by weak\nmodels. To effectively and efficiently elicit the potential of strong agents,\nwe further construct ``trajectory trees,\" a hierarchical representation that\norganizes weak model-generated action trajectories, coupled with Monte Carlo\nTree Search (MCTS) to optimize the strong model. Through theoretical analysis,\nwe provide formal guarantees for the effectiveness of our method in improving\nW2SG performance. Our empirical evaluations demonstrate substantial\nimprovements in reasoning and decision-making capabilities across diverse task\ndomains, validating the scalability and robustness of our proposed framework.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18858v2", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-28", "AI": {"title_translation": "弱到强泛化与失败轨迹：一种基于树的方法以引出强模型的最佳策略", "tldr": "本文提出一种新的弱到强泛化方法，通过利用弱模型的成功和失败轨迹，结合轨迹树和蒙特卡洛树搜索，来提升强模型在复杂决策环境中的性能。", "motivation": "现有的弱到强泛化（W2SG）研究主要集中在二元分类等简单任务，但本文旨在将此范式扩展到复杂的交互式决策制定环境中。此外，受人类学习过程启发，提出不仅泛化成功知识，还要利用失败经验。", "method": "提出通过弱模型生成的中间动作轨迹来微调强模型。引入“轨迹树”作为分层表示来组织弱模型生成的动作轨迹，并结合蒙特卡洛树搜索（MCTS）来优化强模型。该方法特别之处在于，不仅学习成功知识，还从弱模型积累的失败轨迹中学习。", "result": "理论分析为该方法在提高W2SG性能方面的有效性提供了形式化保证。实证评估表明，在不同任务领域，推理和决策能力都有显著提高，验证了所提出框架的可扩展性和鲁棒性。", "conclusion": "本文提出的基于失败轨迹和轨迹树的弱到强泛化方法，在理论和实践上都有效提升了强模型在复杂决策环境中的性能，并具有良好的可扩展性和鲁棒性。", "translation": "弱到强泛化（W2SG）是一种利用弱模型监督来充分发挥强模型能力的最新趋势。虽然现有的W2SG研究侧重于二元分类等简单任务，但我们将此范式扩展到复杂的交互式决策制定环境中。具体来说，我们使用弱模型生成的中间动作轨迹来微调强模型。受人类学习过程的启发，我们提出不仅泛化成功知识，还要泛化失败经验，以便强模型可以从弱模型积累的失败轨迹中学习。为了有效且高效地引出强代理的潜力，我们进一步构建了“轨迹树”，这是一种组织弱模型生成动作轨迹的分层表示，并结合蒙特卡洛树搜索（MCTS）来优化强模型。通过理论分析，我们为我们方法在提高W2SG性能方面的有效性提供了形式化保证。我们的实证评估表明，在不同任务领域，推理和决策能力都有显著提高，验证了我们所提出框架的可扩展性和鲁棒性。", "summary": "本文提出了一种新颖的弱到强泛化（W2SG）框架，旨在将W2SG扩展到复杂的交互式决策环境。该方法通过利用弱模型生成的成功和失败轨迹来微调强模型。核心创新在于引入“轨迹树”作为分层表示，并结合蒙特卡洛树搜索（MCTS）来有效优化强模型。理论分析和实证评估均证明了该方法在提升强模型推理和决策能力方面的有效性、可扩展性和鲁棒性。", "keywords": "弱到强泛化, 失败轨迹, 轨迹树, 蒙特卡洛树搜索, 决策制定", "comments": "本文的创新点在于将弱到强泛化扩展到复杂决策环境，并引入了“失败轨迹”这一概念，这与人类从错误中学习的机制相契合。结合“轨迹树”和MCTS，提供了一种结构化且高效的策略优化方法。该研究对于提升大型模型在复杂任务中的性能具有重要意义，尤其是在数据稀缺或标注成本高昂的场景下。"}}
{"id": "2507.15300", "title": "GCC: A 3DGS Inference Architecture with Gaussian-Wise and Cross-Stage Conditional Processing", "authors": ["Minnan Pei", "Gang Li", "Junwen Si", "Zeyu Zhu", "Zitao Mo", "Peisong Wang", "Zhuoran Song", "Xiaoyao Liang", "Jian Cheng"], "categories": ["cs.AR"], "primary_category": "Subjects:       Hardware Architecture (cs.AR)", "pdf_link": null, "comments": "Comments:      Accepted to MICRO 2025", "url": "http://arxiv.org/abs/2507.15300v3", "summary": "3D Gaussian Splatting (3DGS) has emerged as a leading neural rendering\ntechnique for high-fidelity view synthesis, prompting the development of\ndedicated 3DGS accelerators for resource-constrained platforms. The\nconventional decoupled preprocessing-rendering dataflow in existing\naccelerators has two major limitations: 1) a significant portion of\npreprocessed Gaussians are not used in rendering, and 2) the same Gaussian gets\nrepeatedly loaded across different tile renderings, resulting in substantial\ncomputational and data movement overhead. To address these issues, we propose\nGCC, a novel accelerator designed for fast and energy-efficient 3DGS inference.\nGCC introduces a novel dataflow featuring: 1) \\textit{cross-stage conditional\nprocessing}, which interleaves preprocessing and rendering to dynamically skip\nunnecessary Gaussian preprocessing; and 2) \\textit{Gaussian-wise rendering},\nensuring that all rendering operations for a given Gaussian are completed\nbefore moving to the next, thereby eliminating duplicated Gaussian loading. We\nalso propose an alpha-based boundary identification method to derive compact\nand accurate Gaussian regions, thereby reducing rendering costs. We implement\nour GCC accelerator in 28nm technology. Extensive experiments demonstrate that\nGCC significantly outperforms the state-of-the-art 3DGS inference accelerator,\nGSCore, in both performance and energy efficiency.", "comment": "Accepted to MICRO 2025", "pdf_url": "http://arxiv.org/pdf/2507.15300v3", "cate": "cs.AR", "date": "2025-07-21", "updated": "2025-07-25", "AI": {"title_translation": "GCC：一种具有高斯级和跨阶段条件处理的3DGS推理架构", "tldr": "GCC是一种新型的3DGS推理加速器，通过创新的数据流解决了现有加速器中预处理冗余和数据重复加载的问题，显著提升了性能和能效。", "motivation": "现有3DGS加速器中传统的解耦预处理-渲染数据流存在两个主要限制：1) 大部分预处理的高斯未用于渲染；2) 同一高斯在不同瓦片渲染中重复加载，导致巨大的计算和数据移动开销。", "method": "GCC引入了一种新型数据流，其特点是：1) 交叉阶段条件处理，交错预处理和渲染以动态跳过不必要的高斯预处理；2) 高斯级渲染，确保给定高斯的所有渲染操作在移动到下一个高斯之前完成，从而消除重复的高斯加载。此外，还提出了一种基于alpha的边界识别方法来导出紧凑准确的高斯区域，以减少渲染成本。GCC在28nm技术中实现。", "result": "广泛的实验表明，GCC在性能和能效方面显著优于最先进的3DGS推理加速器GSCore。", "conclusion": "GCC通过其创新的数据流和优化方法，有效解决了3DGS推理中的效率瓶颈，在性能和能效上实现了显著提升。", "translation": "3D高斯泼溅（3DGS）已成为一种领先的神经渲染技术，用于高保真视图合成，促使为资源受限平台开发专用3DGS加速器。现有加速器中传统的解耦预处理-渲染数据流存在两个主要限制：1）大部分预处理的高斯未用于渲染；2）同一高斯在不同瓦片渲染中重复加载，导致巨大的计算和数据移动开销。为了解决这些问题，我们提出了GCC，这是一种专为快速高效能3DGS推理设计的新型加速器。GCC引入了一种新颖的数据流，其特点是：1）交叉阶段条件处理，该处理交错预处理和渲染以动态跳过不必要的高斯预处理；2）高斯级渲染，确保给定高斯的所有渲染操作在移动到下一个高斯之前完成，从而消除重复的高斯加载。我们还提出了一种基于alpha的边界识别方法，以导出紧凑准确的高斯区域，从而降低渲染成本。我们在28nm技术中实现了我们的GCC加速器。大量实验表明，GCC在性能和能效方面显著优于最先进的3DGS推理加速器GSCore。", "summary": "本文提出了GCC，一种用于3DGS推理的新型加速器，旨在解决现有加速器中预处理冗余和数据重复加载的问题。GCC通过引入交叉阶段条件处理和高斯级渲染两种创新数据流，分别动态跳过不必要的高斯预处理和消除重复高斯加载。此外，还提出了基于alpha的边界识别方法以优化渲染成本。实验证明，GCC在性能和能效上显著优于现有最先进的3DGS推理加速器GSCore。", "keywords": "3D Gaussian Splatting, 3DGS, Neural Rendering, Hardware Accelerator, Inference Architecture", "comments": "GCC的创新点在于其独特的数据流设计，特别是交叉阶段条件处理和高斯级渲染，这些机制直接解决了现有3DGS加速器中数据冗余和计算效率低下的核心痛点。通过在硬件层面优化数据流和处理顺序，它显著提升了3DGS在资源受限平台上的实用性，对于实时神经渲染领域具有重要意义。"}}
{"id": "2507.18769", "title": "ylmmcl at Multilingual Text Detoxification 2025: Lexicon-Guided Detoxification and Classifier-Gated Rewriting", "authors": ["Nicole Lai-Lopez", "Lusha Wang", "Su Yuan", "Liza Zhang"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      16 pages, 5 figures, 3 tables,", "url": "http://arxiv.org/abs/2507.18769v1", "summary": "In this work, we introduce our solution for the Multilingual Text\nDetoxification Task in the PAN-2025 competition for the ylmmcl team: a robust\nmultilingual text detoxification pipeline that integrates lexicon-guided\ntagging, a fine-tuned sequence-to-sequence model (s-nlp/mt0-xl-detox-orpo) and\nan iterative classifier-based gatekeeping mechanism. Our approach departs from\nprior unsupervised or monolingual pipelines by leveraging explicit toxic word\nannotation via the multilingual_toxic_lexicon to guide detoxification with\ngreater precision and cross-lingual generalization. Our final model achieves\nthe highest STA (0.922) from our previous attempts, and an average official J\nscore of 0.612 for toxic inputs in both the development and test sets. It also\nachieved xCOMET scores of 0.793 (dev) and 0.787 (test). This performance\noutperforms baseline and backtranslation methods across multiple languages, and\nshows strong generalization in high-resource settings (English, Russian,\nFrench). Despite some trade-offs in SIM, the model demonstrates consistent\nimprovements in detoxification strength. In the competition, our team achieved\nninth place with a score of 0.612.", "comment": "16 pages, 5 figures, 3 tables,", "pdf_url": "http://arxiv.org/pdf/2507.18769v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "ylmmcl在2025年多语言文本解毒：词典引导的解毒和分类器门控重写", "tldr": "ylmmcl团队为PAN-2025多语言文本解毒任务提出了一种鲁棒的、词典引导和分类器门控的解毒管道，该模型在多语言设置下表现优异，并在比赛中获得第九名。", "motivation": "该研究旨在为PAN-2025竞赛中的多语言文本解毒任务提供一种解决方案，以提高多语言文本解毒的精度和跨语言泛化能力。", "method": "该方法引入了一个鲁棒的多语言文本解毒管道，整合了词典引导的标记、一个微调的序列到序列模型（s-nlp/mt0-xl-detox-orpo）和一个迭代的基于分类器的门控机制。它通过利用多语言有毒词汇表进行显式有毒词注释来指导解毒。", "result": "该模型实现了0.922的最高STA，在开发集和测试集上有毒输入的平均官方J分数为0.612。它还获得了开发集0.793和测试集0.787的xCOMET分数。该性能优于基线和回译方法，并在高资源设置（英语、俄语、法语）中显示出强大的泛化能力。尽管在SIM方面存在一些权衡，但模型在解毒强度方面表现出持续改进。在比赛中，该团队以0.612分获得第九名。", "conclusion": "该论文提出的多语言文本解毒模型通过整合词典引导和分类器门控机制，实现了高精度的多语言解毒，并在高资源语言中展示了强大的泛化能力，在PAN-2025竞赛中表现出色。", "translation": "在这项工作中，我们介绍了我们为ylmmcl团队参加PAN-2025竞赛中的多语言文本解毒任务而提出的解决方案：一个鲁棒的多语言文本解毒管道，它整合了词典引导的标记、一个微调的序列到序列模型（s-nlp/mt0-xl-detox-orpo）和一个迭代的基于分类器的门控机制。我们的方法通过利用multilingual_toxic_lexicon进行显式有毒词注释来指导解毒，从而以更高的精度和跨语言泛化能力，区别于以往的无监督或单语言管道。我们的最终模型从我们之前的尝试中获得了最高的STA（0.922），并且在开发集和测试集中有毒输入的平均官方J分数为0.612。它还获得了0.793（开发集）和0.787（测试集）的xCOMET分数。这一性能优于多种语言的基线和回译方法，并在高资源设置（英语、俄语、法语）中显示出强大的泛化能力。尽管在SIM方面存在一些权衡，但该模型在解毒强度方面表现出持续改进。在比赛中，我们团队以0.612分获得第九名。", "summary": "本文介绍了ylmmcl团队参加PAN-2025多语言文本解毒任务的解决方案。该方案提出了一种鲁棒的多语言解毒管道，结合了词典引导的标记、微调的序列到序列模型和迭代的分类器门控机制。该方法通过显式利用多语言有毒词典来提高解毒的精度和跨语言泛化能力。实验结果显示，该模型在STA、J分数和xCOMET分数上均表现优异，超越了基线和回译方法，尤其在高资源语言中展现出强大的泛化能力。尽管在SIM方面有所取舍，但其解毒能力显著提升，并在竞赛中取得了第九名的成绩。", "keywords": "多语言文本解毒, 词典引导, 分类器门控, 序列到序列模型, PAN-2025", "comments": "该论文的创新点在于其结合了词典引导的标记和分类器门控机制，这使得解毒过程更加精确，并能有效处理多语言环境。与以往的无监督或单语言方法相比，其显式利用多语言有毒词典的策略是其优势所在。该模型在多个评价指标上均表现出色，尤其在高资源语言中的泛化能力值得肯定。然而，抽象中也提到在SIM（相似度）方面存在权衡，这可能意味着在保持文本原始语义方面存在一些挑战，是未来研究可以改进的方向。"}}
{"id": "2507.19418", "title": "DEFNet: Multitasks-based Deep Evidential Fusion Network for Blind Image Quality Assessment", "authors": ["Yiwei Lou", "Yuanpeng He", "Rongchao Zhang", "Yongzhi Cao", "Hanpin Wang", "Yu Huang"], "categories": ["cs.CV", "eess.IV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19418v1", "summary": "Blind image quality assessment (BIQA) methods often incorporate auxiliary\ntasks to improve performance. However, existing approaches face limitations due\nto insufficient integration and a lack of flexible uncertainty estimation,\nleading to suboptimal performance. To address these challenges, we propose a\nmultitasks-based Deep Evidential Fusion Network (DEFNet) for BIQA, which\nperforms multitask optimization with the assistance of scene and distortion\ntype classification tasks. To achieve a more robust and reliable\nrepresentation, we design a novel trustworthy information fusion strategy. It\nfirst combines diverse features and patterns across sub-regions to enhance\ninformation richness, and then performs local-global information fusion by\nbalancing fine-grained details with coarse-grained context. Moreover, DEFNet\nexploits advanced uncertainty estimation technique inspired by evidential\nlearning with the help of normal-inverse gamma distribution mixture. Extensive\nexperiments on both synthetic and authentic distortion datasets demonstrate the\neffectiveness and robustness of the proposed framework. Additional evaluation\nand analysis are carried out to highlight its strong generalization capability\nand adaptability to previously unseen scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19418v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "DEFNet：基于多任务的深度证据融合网络用于盲图像质量评估", "tldr": "DEFNet是一个基于多任务的深度证据融合网络，用于盲图像质量评估（BIQA），通过新颖的信任信息融合策略和证据学习驱动的不确定性估计，解决了现有BIQA方法集成不足和不确定性估计不灵活的问题，并在实验中展现出有效性、鲁棒性和泛化能力。", "motivation": "现有盲图像质量评估（BIQA）方法在整合辅助任务时面临集成不足和缺乏灵活的不确定性估计的局限性，导致性能不佳。", "method": "本文提出DEFNet，一个基于多任务的深度证据融合网络，用于盲图像质量评估。该网络通过场景和失真类型分类任务辅助进行多任务优化。为实现更鲁棒可靠的表示，DEFNet设计了一种新颖的可信信息融合策略，该策略首先结合子区域的多种特征和模式以增强信息丰富性，然后通过平衡细粒度细节和粗粒度上下文进行局部-全局信息融合。此外，DEFNet利用受证据学习启发的先进不确定性估计技术，并借助正态逆伽马分布混合。", "result": "在合成和真实失真数据集上进行的大量实验证明了所提出框架的有效性和鲁棒性。额外的评估和分析突出了其强大的泛化能力以及对先前未见场景的适应性。", "conclusion": "DEFNet通过其多任务优化、新颖的可信信息融合策略和先进的不确定性估计技术，有效且鲁棒地解决了盲图像质量评估中的挑战，并展现出强大的泛化能力和适应性。", "translation": "盲图像质量评估（BIQA）方法通常会结合辅助任务以提高性能。然而，现有方法由于集成不足和缺乏灵活的不确定性估计而面临局限性，导致性能不佳。为应对这些挑战，我们提出了一种基于多任务的深度证据融合网络（DEFNet），用于BIQA，该网络在场景和失真类型分类任务的辅助下执行多任务优化。为实现更鲁棒和可靠的表示，我们设计了一种新颖的可信信息融合策略。它首先结合子区域的多种特征和模式以增强信息丰富性，然后通过平衡细粒度细节和粗粒度上下文进行局部-全局信息融合。此外，DEFNet利用受证据学习启发的高级不确定性估计技术，并借助正态逆伽马分布混合。在合成和真实失真数据集上进行的大量实验证明了所提出框架的有效性和鲁棒性。额外的评估和分析进一步突出了其强大的泛化能力以及对先前未见场景的适应性。", "summary": "本文提出DEFNet，一种基于多任务的深度证据融合网络，用于盲图像质量评估（BIQA）。该网络旨在解决现有BIQA方法在多任务集成和不确定性估计方面的不足。DEFNet通过场景和失真类型分类辅助多任务优化，并引入新颖的可信信息融合策略，以增强信息丰富性和实现局部-全局融合。此外，它利用基于证据学习的不确定性估计技术。实验结果表明，DEFNet在合成和真实数据集上均表现出卓越的有效性、鲁棒性、泛化能力和对新场景的适应性。", "keywords": "盲图像质量评估, 深度证据融合网络, 多任务优化, 不确定性估计, 信息融合", "comments": "DEFNet的创新点在于其结合了多任务学习、新颖的可信信息融合策略以及基于证据学习的不确定性估计，以解决BIQA中现有方法的局限性。特别是，引入不确定性估计使得模型能够更可靠地评估图像质量，这对于实际应用至关重要。其在泛化能力和适应性方面的表现也突显了其重要性。"}}
{"id": "2507.18330", "title": "GVCCS: A Dataset for Contrail Identification and Tracking on Visible Whole Sky Camera Sequences", "authors": ["Gabriel Jarry", "Ramon Dalmau", "Philippe Very", "Franck Ballerini", "Stefania-Denisa Bocu"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18330v2", "summary": "Aviation's climate impact includes not only CO2 emissions but also\nsignificant non-CO2 effects, especially from contrails. These ice clouds can\nalter Earth's radiative balance, potentially rivaling the warming effect of\naviation CO2. Physics-based models provide useful estimates of contrail\nformation and climate impact, but their accuracy depends heavily on the quality\nof atmospheric input data and on assumptions used to represent complex\nprocesses like ice particle formation and humidity-driven persistence.\nObservational data from remote sensors, such as satellites and ground cameras,\ncould be used to validate and calibrate these models. However, existing\ndatasets don't explore all aspect of contrail dynamics and formation: they\ntypically lack temporal tracking, and do not attribute contrails to their\nsource flights. To address these limitations, we present the Ground Visible\nCamera Contrail Sequences (GVCCS), a new open data set of contrails recorded\nwith a ground-based all-sky camera in the visible range. Each contrail is\nindividually labeled and tracked over time, allowing a detailed analysis of its\nlifecycle. The dataset contains 122 video sequences (24,228 frames) and\nincludes flight identifiers for contrails that form above the camera. As\nreference, we also propose a unified deep learning framework for contrail\nanalysis using a panoptic segmentation model that performs semantic\nsegmentation (contrail pixel identification), instance segmentation (individual\ncontrail separation), and temporal tracking in a single architecture. By\nproviding high-quality, temporally resolved annotations and a benchmark for\nmodel evaluation, our work supports improved contrail monitoring and will\nfacilitate better calibration of physical models. This sets the groundwork for\nmore accurate climate impact understanding and assessments.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18330v2", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-25", "AI": {"title_translation": "GVCCS：一个用于可见光全天空相机序列中凝结尾迹识别与跟踪的数据集", "tldr": "本文提出了GVCCS，一个新的开放数据集，用于从地面可见光全天空相机序列中识别和跟踪凝结尾迹，并提供了一个统一的深度学习框架作为基准，以改进凝结尾迹监测和气候模型校准。", "motivation": "航空飞机的凝结尾迹对气候有显著的非二氧化碳影响，但现有的物理模型受限于大气输入数据质量和复杂过程的假设。此外，现有的观测数据集缺乏凝结尾迹的时间跟踪和来源归属，限制了对凝结尾迹动力学和形成的全面探索。", "method": "本文提出了地面可见光相机凝结尾迹序列（GVCCS），这是一个新的开放数据集，包含使用地面全天空相机在可见光范围内记录的凝结尾迹。数据集中每个凝结尾迹都被单独标注并随时间跟踪。此外，本文还提出了一个统一的深度学习框架，使用全景分割模型进行凝结尾迹分析，该模型在一个架构中执行语义分割（凝结尾迹像素识别）、实例分割（个体凝结尾迹分离）和时间跟踪。", "result": "GVCCS数据集包含122个视频序列（24,228帧），并为相机上方形成的凝结尾迹提供了飞行标识符。所提出的深度学习框架能够进行凝结尾迹像素识别、个体凝结尾迹分离和时间跟踪。", "conclusion": "通过提供高质量、时间分辨的标注和模型评估基准，本工作支持改进凝结尾迹监测，并促进物理模型的更好校准。这为更准确地理解和评估气候影响奠定了基础。", "translation": "航空对气候的影响不仅包括二氧化碳排放，还包括显著的非二氧化碳效应，特别是凝结尾迹。这些冰云可以改变地球的辐射平衡，其升温效应可能与航空二氧化碳的升温效应相媲美。基于物理的模型提供了凝结尾迹形成和气候影响的有用估计，但其准确性严重依赖于大气输入数据的质量以及用于表示冰粒形成和湿度驱动持久性等复杂过程的假设。来自遥感器（如卫星和地面相机）的观测数据可用于验证和校准这些模型。然而，现有数据集未能探索凝结尾迹动力学和形成的所有方面：它们通常缺乏时间跟踪，并且不将凝结尾迹归因于其来源航班。为了解决这些限制，我们提出了地面可见光相机凝结尾迹序列（GVCCS），这是一个新的开放数据集，包含使用地面全天空相机在可见光范围内记录的凝结尾迹。每个凝结尾迹都被单独标注并随时间跟踪，从而可以对其生命周期进行详细分析。该数据集包含122个视频序列（24,228帧），并包含在相机上方形成的凝结尾迹的飞行标识符。作为参考，我们还提出了一个统一的深度学习框架，用于使用全景分割模型进行凝结尾迹分析，该模型在一个架构中执行语义分割（凝结尾迹像素识别）、实例分割（个体凝结尾迹分离）和时间跟踪。通过提供高质量、时间分辨的标注和模型评估基准，我们的工作支持改进凝结尾迹监测，并将促进物理模型的更好校准。这为更准确地理解和评估气候影响奠定了基础。", "summary": "本文介绍了GVCCS（地面可见光相机凝结尾迹序列），这是一个新的开放数据集，旨在解决现有凝结尾迹观测数据在时间跟踪和来源归属方面的不足。该数据集包含122个视频序列，记录了地面全天空相机在可见光范围内捕获的凝结尾迹，并对每个凝结尾迹进行了个体标注和时间跟踪，还包括了飞行标识符。为了促进凝结尾迹分析，本文还提出了一个统一的深度学习框架，该框架利用全景分割模型在一个架构中实现凝结尾迹的语义分割、实例分割和时间跟踪。这项工作通过提供高质量、时间分辨的标注和模型评估基准，为改进凝结尾迹监测和更准确地校准物理气候模型奠定了基础。", "keywords": "凝结尾迹, 数据集, 深度学习, 全天空相机, 气候影响", "comments": "GVCCS数据集的创新之处在于其对凝结尾迹的个体标注和时间跟踪，以及包含飞行标识符，这弥补了现有数据集的不足。所提出的统一深度学习框架提供了一个多任务处理的基准模型，对于未来的凝结尾迹分析和模型开发具有重要意义。该工作为更准确地评估航空气候影响提供了宝贵的数据和工具，具有重要的应用前景。"}}
{"id": "2507.18878", "title": "Improving the State of the Art for Training Human-AI Teams: Technical Report #5 -- Individual Differences and Team Qualities to Measure in a Human-AI Teaming Testbed", "authors": ["Lillian Asiala", "James E. McCarthy"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18878v1", "summary": "Sonalysts, Inc. (Sonalysts) is working on an initiative to expand our\nexpertise in teaming to include Human-Artificial Intelligence (AI) teams. The\nfirst step of this process is to develop a Synthetic Task Environment (STE) to\nsupport our original research. Prior knowledge elicitation efforts within the\nHuman-AI teaming research stakeholder community revealed a desire to support\ndata collection using pre- and post-performance surveys. In this technical\nreport, we review a number of constructs that capture meaningful individual\ndifferences and teaming qualities. Additionally, we explore methods of\nmeasuring those constructs within the STE.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18878v1", "cate": "cs.HC", "date": "2025-06-05", "updated": "2025-06-05", "AI": {"title_translation": "改进人机协作团队训练的最新技术：技术报告#5——在人机协作测试平台中衡量个体差异和团队素质", "tldr": "该技术报告回顾了用于人机协作团队研究的个体差异和团队素质的构建，并探讨了在合成任务环境中测量这些构建的方法。", "motivation": "Sonalysts公司正在努力扩展其在团队协作方面的专业知识，以涵盖人机协作团队，并开发一个合成任务环境（STE）来支持其原创研究。先前的知识获取工作表明，人机协作研究的利益相关者希望通过绩效前后调查来支持数据收集。", "method": "本报告回顾了大量能够捕捉有意义的个体差异和团队素质的构建，并探讨了在合成任务环境（STE）中测量这些构建的方法。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "Sonalysts公司（Sonalysts）正在努力扩展其在团队协作方面的专业知识，以涵盖人机协作（AI）团队。此过程的第一步是开发一个合成任务环境（STE）来支持我们的原创研究。人机协作研究利益相关者社区内先前的知识获取工作表明，他们希望通过绩效前后调查来支持数据收集。在本技术报告中，我们回顾了许多能够捕捉有意义的个体差异和团队素质的构建。此外，我们探讨了在STE中测量这些构建的方法。", "summary": "本技术报告由Sonalysts公司发布，旨在拓展其在人机协作团队领域的专业知识。报告重点介绍了开发合成任务环境（STE）作为原创研究的基础，并响应了利益相关者对使用绩效前后调查进行数据收集的需求。文中详细审查了捕捉个体差异和团队素质的重要构建，并探讨了在STE中测量这些构建的方法。", "keywords": "人机协作, 个体差异, 团队素质, 合成任务环境, 数据收集", "comments": "这份技术报告的创新点在于其专注于在人机协作背景下识别和测量个体差异与团队素质，这对于优化未来人机协作系统的设计和训练至关重要。其重要性在于为构建有效的人机协作测试平台提供了方法论基础。然而，由于是技术报告，它主要侧重于方法和概念回顾，而非具体实验结果，因此其局限性在于缺乏实证数据支持。"}}
{"id": "2507.19175", "title": "Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers", "authors": ["Yuki Igaue", "Hiroaki Aizawa"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19175v1", "summary": "Multi-head self-attention is a distinctive feature extraction mechanism of\nvision transformers that computes pairwise relationships among all input\npatches, contributing significantly to their high performance. However, it is\nknown to incur a quadratic computational complexity with respect to the number\nof patches. One promising approach to address this issue is patch pruning,\nwhich improves computational efficiency by identifying and removing redundant\npatches. In this work, we propose a patch pruning strategy that evaluates the\nimportance of each patch based on the variance of attention weights across\nmultiple attention heads. This approach is inspired by the design of multi-head\nself-attention, which aims to capture diverse attention patterns across\ndifferent subspaces of feature representations. The proposed method can be\neasily applied during both training and inference, and achieves improved\nthroughput while maintaining classification accuracy in scenarios such as\nfine-tuning with pre-trained models. In addition, we also found that using\nrobust statistical measures, such as the median absolute deviation in place of\nvariance, to assess patch importance can similarly lead to strong performance.\nFurthermore, by introducing overlapping patch embeddings, our method achieves\nbetter performance with comparable throughput to conventional approaches that\nutilize all patches.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19175v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于视觉Transformer中注意力权重多样性鲁棒统计度量的补丁剪枝策略", "tldr": "本文提出了一种新的补丁剪枝策略，通过评估注意力权重多样性来识别并移除Vision Transformer中的冗余补丁，从而在保持分类精度的同时提高计算效率。", "motivation": "视觉Transformer中的多头自注意力机制计算所有输入补丁之间的两两关系，导致计算复杂度与补丁数量呈二次关系，这带来了显著的计算开销。", "method": "本文提出一种补丁剪枝策略，通过评估多头注意力中每个补丁的注意力权重方差来衡量其重要性。该方法可在训练和推理阶段应用，并通过引入重叠补丁嵌入进一步提升性能。此外，研究还发现使用鲁棒统计度量（如中位数绝对偏差）替代方差也能获得良好性能。", "result": "该方法在微调预训练模型等场景中，在保持分类精度的同时提高了吞吐量。使用鲁棒统计度量评估补丁重要性也能带来强大的性能。引入重叠补丁嵌入后，该方法在与使用所有补丁的传统方法相当的吞吐量下实现了更好的性能。", "conclusion": "本文提出的基于注意力权重多样性鲁棒统计度量的补丁剪枝策略，有效解决了Vision Transformer中多头自注意力的二次计算复杂度问题，显著提高了计算效率，同时保持了模型性能，尤其是在引入重叠补丁嵌入后表现更优。", "translation": "多头自注意力是视觉Transformer独特的特征提取机制，它计算所有输入补丁之间的两两关系，对其高性能贡献巨大。然而，众所周知，它会产生与补丁数量相关的二次计算复杂度。解决这个问题的一个有前景的方法是补丁剪枝，它通过识别和移除冗余补丁来提高计算效率。在这项工作中，我们提出了一种补丁剪枝策略，该策略基于跨多个注意力头的注意力权重方差来评估每个补丁的重要性。这种方法受到多头自注意力设计的启发，该设计旨在捕获特征表示不同子空间中多样化的注意力模式。所提出的方法可以很容易地在训练和推理期间应用，并在微调预训练模型等场景中，在保持分类精度的同时实现了吞吐量的提高。此外，我们还发现，使用鲁棒统计度量（例如中位数绝对偏差代替方差）来评估补丁重要性同样可以带来强大的性能。此外，通过引入重叠补丁嵌入，我们的方法在与利用所有补丁的传统方法相当的吞吐量下实现了更好的性能。", "summary": "本文提出了一种用于视觉Transformer的补丁剪枝策略，旨在解决多头自注意力机制带来的二次计算复杂度问题。该策略通过评估注意力权重在不同注意力头之间的多样性（使用方差或鲁棒统计度量如中位数绝对偏差）来确定补丁的重要性并进行剪枝。实验证明，该方法在提高计算效率（吞吐量）的同时，能保持甚至提升分类精度，尤其是在引入重叠补丁嵌入后性能更佳。", "keywords": "补丁剪枝, 视觉Transformer, 注意力权重, 计算效率, 鲁棒统计", "comments": "该论文提出了一种新颖的补丁剪枝策略，通过利用Vision Transformer自身的多头自注意力机制的特性，即注意力权重的多样性来评估补丁重要性，具有较高的创新性。其引入鲁棒统计度量和重叠补丁嵌入的改进，进一步提升了方法的实用性和性能，为降低Vision Transformer的计算成本提供了一个有效途径。"}}
{"id": "2507.18928", "title": "GPUnion: Autonomous GPU Sharing on Campus", "authors": ["Yufang Li", "Yuanbo Zhang", "Hanlong Liao", "Guoming Tang", "Deke Guo"], "categories": ["cs.DC", "C.2.4; C.2.1"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      7 pages, 3 figures, 1 table. Submitted to the ACM Workshop on Hot Topics in Networks (HOTNETS) 2025", "url": "http://arxiv.org/abs/2507.18928v1", "summary": "A pronounced imbalance in GPU resources exists on campus, where some\nlaboratories own underutilized servers while others lack the compute needed for\nAI research. GPU sharing can alleviate this disparity, while existing platforms\ntypically rely on centralized oversight and persistent allocation models,\nconflicting with the voluntary and autonomous nature of academic resource\nownership. We present GPUnion, a campus-scale GPU sharing platform enabling\nvoluntary participation while preserving full provider autonomy. GPUnion\nincorporates three core mechanisms: i) container-based task dispatching and\nexecution, ii) resource provider-first architecture, and iii) resilient\nexecution featuring automatic checkpointing and migration. GPUnion also\nsupports custom data storage and integrates the non-root execution and image\nattestation for isolation and security improvement for containerization. Case\nstudies across multiple campus scenarios demonstrate 30% more GPU utilization\nimprovement, 40% increase in interactive sessions, and 94% successful workload\nmigration during provider departures. GPUnion demonstrates that provider\nautonomy and platform reliability can coexist, challenging conventional\ncentralized paradigms and democratizing access to computational resources\nwithin campus networks.", "comment": "7 pages, 3 figures, 1 table. Submitted to the ACM Workshop on Hot\n  Topics in Networks (HOTNETS) 2025", "pdf_url": "http://arxiv.org/pdf/2507.18928v1", "cate": "cs.DC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "GPUnion：校园自主GPU共享平台", "tldr": "GPUnion是一个校园GPU共享平台，它通过容器化、提供者优先架构和弹性执行，实现了在保证资源提供者自主性的同时提高GPU利用率和可靠性。", "motivation": "校园GPU资源存在显著不平衡，一些实验室的服务器未充分利用，而另一些则缺乏AI研究所需的计算资源。现有GPU共享平台通常依赖中心化管理和持久分配模式，这与学术资源所有权的自愿和自主性质相冲突。", "method": "GPUnion是一个校园规模的GPU共享平台，允许自愿参与并保留提供者的完全自主权。它包含三个核心机制：1) 基于容器的任务调度和执行；2) 资源提供者优先架构；3) 具有自动检查点和迁移的弹性执行。GPUnion还支持自定义数据存储，并集成了非root执行和镜像认证，以提高容器化的隔离性和安全性。", "result": "案例研究表明，GPUnion使GPU利用率提高了30%，交互式会话增加了40%，并在提供者离开时实现了94%的工作负载迁移成功率。", "conclusion": "GPUnion证明了提供者自主性和平台可靠性可以共存，挑战了传统的中心化范式，并使校园网络内的计算资源访问民主化。", "translation": "校园内GPU资源存在显著不平衡，一些实验室的服务器未充分利用，而另一些则缺乏人工智能研究所需的计算能力。GPU共享可以缓解这种差距，但现有平台通常依赖中心化监督和持久分配模型，这与学术资源所有权的自愿和自主性质相冲突。我们提出了GPUnion，一个校园规模的GPU共享平台，它支持自愿参与，同时保留了提供者的完全自主权。GPUnion包含了三个核心机制：i) 基于容器的任务调度和执行；ii) 资源提供者优先架构；iii) 具有自动检查点和迁移的弹性执行。GPUnion还支持自定义数据存储，并集成了非root执行和镜像认证，以提高容器化的隔离性和安全性。在多个校园场景中的案例研究表明，GPU利用率提高了30%，交互式会话增加了40%，并在提供者离开时实现了94%的工作负载迁移成功率。GPUnion证明了提供者自主性和平台可靠性可以共存，挑战了传统的中心化范式，并使校园网络内的计算资源访问民主化。", "summary": "GPUnion是一个为解决校园GPU资源不平衡问题而设计的自主GPU共享平台。它通过引入容器化任务调度、提供者优先架构和弹性执行（包括自动检查点和迁移）等核心机制，实现了在保证资源提供者自主性的前提下，提高GPU利用率和共享效率。该平台还关注安全性与隔离性。实验结果表明，GPUnion显著提升了GPU利用率和交互会话数量，并有效保障了工作负载的迁移成功率，为校园计算资源的民主化访问提供了可行方案。", "keywords": "GPU共享, 校园资源管理, 自主性, 容器化, 弹性执行", "comments": "GPUnion的创新之处在于其“提供者优先”的架构设计，这与传统中心化资源管理模式形成鲜明对比，成功解决了学术环境中资源所有者自主性与资源共享之间的冲突。通过结合容器化技术和弹性执行机制，它在保证安全性和可靠性的同时，有效提升了校园GPU资源的利用率，具有重要的实践意义和推广价值。"}}
{"id": "2507.19356", "title": "Enhancing Speech Emotion Recognition Leveraging Aligning Timestamps of ASR Transcripts and Speaker Diarization", "authors": ["Hsuan-Yu Wang", "Pei-Ying Lee", "Berlin Chen"], "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7; I.5.1"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      6 pages, 3 figures, to appear in the Proceedings of the 2025 International Conference on Asian Language Processing (IALP)", "url": "http://arxiv.org/abs/2507.19356v1", "summary": "In this paper, we investigate the impact of incorporating timestamp-based\nalignment between Automatic Speech Recognition (ASR) transcripts and Speaker\nDiarization (SD) outputs on Speech Emotion Recognition (SER) accuracy.\nMisalignment between these two modalities often reduces the reliability of\nmultimodal emotion recognition systems, particularly in conversational\ncontexts. To address this issue, we introduce an alignment pipeline utilizing\npre-trained ASR and speaker diarization models, systematically synchronizing\ntimestamps to generate accurately labeled speaker segments. Our multimodal\napproach combines textual embeddings extracted via RoBERTa with audio\nembeddings from Wav2Vec, leveraging cross-attention fusion enhanced by a gating\nmechanism. Experimental evaluations on the IEMOCAP benchmark dataset\ndemonstrate that precise timestamp alignment improves SER accuracy,\noutperforming baseline methods that lack synchronization. The results highlight\nthe critical importance of temporal alignment, demonstrating its effectiveness\nin enhancing overall emotion recognition accuracy and providing a foundation\nfor robust multimodal emotion analysis.", "comment": "6 pages, 3 figures, to appear in the Proceedings of the 2025\n  International Conference on Asian Language Processing (IALP)", "pdf_url": "http://arxiv.org/pdf/2507.19356v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于ASR转录和说话人识别时间戳对齐的语音情感识别增强", "tldr": "该研究通过对齐ASR转录和说话人识别的时间戳，显著提高了语音情感识别的准确性，解决了多模态系统中的时间错位问题。", "motivation": "多模态情感识别系统中ASR转录和说话人识别输出之间的时间戳错位会降低系统可靠性，尤其是在对话场景中。", "method": "引入了一个利用预训练ASR和说话人识别模型的时间戳对齐流程，生成精确标注的说话人片段。多模态方法结合了RoBERTa提取的文本嵌入和Wav2Vec提取的音频嵌入，并通过门控机制增强的交叉注意力融合。", "result": "在IEMOCAP数据集上的实验评估表明，精确的时间戳对齐改善了语音情感识别（SER）准确性，优于缺乏同步的基线方法。", "conclusion": "时间对齐对于增强整体情感识别准确性至关重要，并为鲁棒的多模态情感分析奠定了基础。", "translation": "本文研究了将自动语音识别（ASR）转录与说话人识别（SD）输出之间基于时间戳的对齐整合到语音情感识别（SER）准确性中的影响。这两种模态之间的错位常常会降低多模态情感识别系统的可靠性，尤其是在对话情境中。为了解决这个问题，我们引入了一个利用预训练ASR和说话人识别模型进行对齐的流程，系统地同步时间戳以生成精确标注的说话人片段。我们的多模态方法结合了通过RoBERTa提取的文本嵌入和来自Wav2Vec的音频嵌入，利用通过门控机制增强的交叉注意力融合。在IEMOCAP基准数据集上的实验评估表明，精确的时间戳对齐提高了SER准确性，优于缺乏同步的基线方法。结果突出了时间对齐的至关重要性，证明了其在提高整体情感识别准确性方面的有效性，并为鲁棒的多模态情感分析奠定了基础。", "summary": "该论文提出了一种通过对齐自动语音识别（ASR）转录和说话人识别（SD）输出的时间戳来提升语音情感识别（SER）准确性的方法。针对多模态系统中常见的错位问题，研究引入了基于预训练模型的对齐流程，并结合RoBERTa文本嵌入和Wav2Vec音频嵌入，通过门控交叉注意力机制进行融合。实验证明，精确的时间戳对齐显著提高了SER性能，强调了时间同步在多模态情感分析中的关键作用。", "keywords": "语音情感识别, 时间戳对齐, 多模态融合, ASR, 说话人识别", "comments": "该研究的创新之处在于明确指出了ASR和说话人识别时间戳错位对多模态情感识别的负面影响，并提出了一个实用的对齐流程。通过结合成熟的RoBERTa和Wav2Vec模型以及创新的门控交叉注意力机制，提升了系统的鲁棒性和准确性。这项工作为未来的多模态情感分析系统奠定了坚实的基础，特别是在处理真实对话数据时具有重要意义。"}}
{"id": "2507.17998", "title": "Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold", "authors": ["Jaeho Shin", "Hyeonjae Gil", "Junwoo Jang", "Maani Ghaffari", "Ayoung Kim"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17998v2", "summary": "Affine Grassmannian has been favored for expressing proximity between lines\nand planes due to its theoretical exactness in measuring distances among\nfeatures. Despite this advantage, the existing method can only measure the\nproximity without yielding the distance as an explicit function of rigid body\ntransformation. Thus, an optimizable distance function on the manifold has\nremained underdeveloped, stifling its application in registration problems.\nThis paper is the first to explicitly derive an optimizable cost function\nbetween two Grassmannian features with respect to rigid body transformation\n($\\mathbf{R}$ and $\\mathbf{t}$). Specifically, we present a rigorous\nmathematical proof demonstrating that the bases of high-dimensional linear\nsubspaces can serve as an explicit representation of the cost. Finally, we\npropose an optimizable cost function based on the transformed bases that can be\napplied to the registration problem of any affine subspace. Compared to vector\nparameter-based approaches, our method is able to find a globally optimal\nsolution by directly minimizing the geodesic distance which is agnostic to\nrepresentation ambiguity. The resulting cost function and its extension to the\ninlier-set maximizing Branch-and-Bound (BnB) solver have been demonstrated to\nimprove the convergence of existing solutions or outperform them in various\ncomputer vision tasks. The code is available on\nhttps://github.com/joomeok/GrassmannRegistration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17998v2", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-25", "AI": {"title_translation": "超越点的配准：基于格拉斯曼流形测地距离的广义仿射子空间对齐", "tldr": "本文首次推导并提出了一个可优化的成本函数，用于在刚体变换下对格拉斯曼流形上的仿射子空间进行配准，实现了全局最优解并改进了现有方法在计算机视觉任务中的表现。", "motivation": "现有方法虽能测量格拉斯曼流形上特征间的接近度，但无法提供一个明确的、可优化的刚体变换距离函数，这限制了其在配准问题中的应用。", "method": "本文首次明确推导了一个关于刚体变换（R和t）的格拉斯曼特征之间可优化的成本函数。通过严格的数学证明，展示了高维线性子空间的基础可以作为成本的显式表示，并提出了一个基于变换后的基础的可优化成本函数，可应用于任何仿射子空间的配准问题。", "result": "与基于向量参数的方法相比，该方法能够通过直接最小化对表示模糊性不敏感的测地距离来找到全局最优解。所得成本函数及其在内点集最大化分枝定界（BnB）求解器中的扩展，在各种计算机视觉任务中均表现出改善现有解决方案的收敛性或超越其性能。", "conclusion": "本文成功推导并验证了一个针对任意仿射子空间配准的可优化成本函数，解决了该领域中缺乏显式可优化距离函数的问题，并显著提升了配准性能。", "translation": "仿射格拉斯曼流形因其在测量线和平面之间距离方面的理论精确性而备受青睐。尽管有此优势，现有方法只能测量接近度，而无法将距离作为刚体变换的显式函数。因此，流形上可优化的距离函数一直未得到充分发展，阻碍了其在配准问题中的应用。本文首次明确推导了两个格拉斯曼特征之间关于刚体变换（R和t）的可优化成本函数。具体而言，我们提出了一个严谨的数学证明，表明高维线性子空间的基础可以作为成本的显式表示。最后，我们提出了一个基于变换后的基础的可优化成本函数，可应用于任何仿射子空间的配准问题。与基于向量参数的方法相比，我们的方法能够通过直接最小化对表示模糊性不敏感的测地距离来找到全局最优解。所得到的成本函数及其在内点集最大化分枝定界（BnB）求解器中的扩展，已被证明可以改善现有解决方案的收敛性或在各种计算机视觉任务中超越它们。代码可在https://github.com/joomeok/GrassmannRegistration获取。", "summary": "本文提出了一种新颖的方法，通过在格拉斯曼流形上推导一个针对刚体变换的可优化成本函数，解决了仿射子空间配准问题。该方法利用高维线性子空间的基础作为成本的显式表示，并证明了其能够通过直接最小化测地距离来找到全局最优解，从而避免了表示模糊性。实验结果表明，该成本函数及其与分枝定界求解器的结合，在多种计算机视觉任务中显著提高了现有解决方案的收敛性或超越了其性能。", "keywords": "格拉斯曼流形, 仿射子空间, 配准, 测地距离, 成本函数", "comments": "本文的核心创新在于首次为格拉斯曼流形上的特征配准问题提供了一个显式且可优化的成本函数，这填补了该领域的一个关键空白。通过利用测地距离的全局优化特性，该方法克服了传统基于向量参数方法的局限性，实现了更鲁棒和精确的配准。其在各种计算机视觉任务中的性能提升，证明了该理论突破的实际应用价值。"}}
{"id": "2507.18656", "title": "ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance in Machine Learning-based Advanced Driver Assistance Systems", "authors": ["Muhammad Zaeem Shahzad", "Muhammad Abdullah Hanif", "Bassem Ouni", "Muhammad Shafique"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      8 pages, 8 figures, 1 table", "url": "http://arxiv.org/abs/2507.18656v1", "summary": "Advanced Driver Assistance Systems (ADAS) significantly enhance road safety\nby detecting potential collisions and alerting drivers. However, their reliance\non expensive sensor technologies such as LiDAR and radar limits accessibility,\nparticularly in low- and middle-income countries. Machine learning-based ADAS\n(ML-ADAS), leveraging deep neural networks (DNNs) with only standard camera\ninput, offers a cost-effective alternative. Critical to ML-ADAS is the\ncollision avoidance feature, which requires the ability to detect objects and\nestimate their distances accurately. This is achieved with specialized DNNs\nlike YOLO, which provides real-time object detection, and a lightweight,\ndetection-wise distance estimation approach that relies on key features\nextracted from the detections like bounding box dimensions and size. However,\nthe robustness of these systems is undermined by security vulnerabilities in\nobject detectors. In this paper, we introduce ShrinkBox, a novel backdoor\nattack targeting object detection in collision avoidance ML-ADAS. Unlike\nexisting attacks that manipulate object class labels or presence, ShrinkBox\nsubtly shrinks ground truth bounding boxes. This attack remains undetected in\ndataset inspections and standard benchmarks while severely disrupting\ndownstream distance estimation. We demonstrate that ShrinkBox can be realized\nin the YOLOv9m object detector at an Attack Success Rate (ASR) of 96%, with\nonly a 4% poisoning ratio in the training instances of the KITTI dataset.\nFurthermore, given the low error targets introduced in our relaxed poisoning\nstrategy, we find that ShrinkBox increases the Mean Absolute Error (MAE) in\ndownstream distance estimation by more than 3x on poisoned samples, potentially\nresulting in delays or prevention of collision warnings altogether.", "comment": "8 pages, 8 figures, 1 table", "pdf_url": "http://arxiv.org/pdf/2507.18656v1", "cate": "cs.CV", "date": "2025-07-22", "updated": "2025-07-22", "AI": {"title_translation": "ShrinkBox：针对基于机器学习的高级驾驶辅助系统中的目标检测后门攻击，以破坏防撞功能", "tldr": "ShrinkBox是一种新型后门攻击，通过微小地缩小目标检测的边界框来破坏ML-ADAS中的防撞功能，且难以被发现。", "motivation": "高级驾驶辅助系统（ADAS）通过检测潜在碰撞来提高道路安全，但其对昂贵传感器技术的依赖限制了可及性。基于机器学习的ADAS（ML-ADAS）利用深度神经网络和标准摄像头输入，提供了一种经济高效的替代方案。ML-ADAS中的防撞功能至关重要，它需要准确检测物体并估计距离，但这易受目标检测器安全漏洞的影响。", "method": "本文引入了ShrinkBox，一种针对ML-ADAS中防撞目标检测的新型后门攻击。与现有操纵对象类别标签或存在性的攻击不同，ShrinkBox巧妙地缩小了真实边界框，从而在数据集检查和标准基准测试中保持未被检测到，同时严重破坏下游距离估计。", "result": "ShrinkBox可以在YOLOv9m目标检测器上实现，攻击成功率（ASR）达到96%，在KITTI数据集的训练实例中仅需4%的投毒率。此外，在放松的投毒策略下，ShrinkBox使得投毒样本的下游距离估计的平均绝对误差（MAE）增加了3倍以上，可能导致碰撞警告的延迟或完全失效。", "conclusion": "ShrinkBox是一种隐蔽且有效的后门攻击，通过微妙地缩小边界框来破坏ML-ADAS中的防撞功能，对基于机器学习的自动驾驶系统的安全性构成了严重威胁。", "translation": "高级驾驶辅助系统（ADAS）通过检测潜在碰撞并提醒驾驶员，显著提高了道路安全。然而，它们对激光雷达和雷达等昂贵传感器技术的依赖限制了其可及性，特别是在中低收入国家。基于机器学习的ADAS（ML-ADAS）利用深度神经网络（DNN）仅使用标准摄像头输入，提供了一种经济高效的替代方案。对ML-ADAS至关重要的是防撞功能，这需要准确检测物体并估计其距离。这通过专门的DNN（如YOLO）实现，YOLO提供实时目标检测，以及一种轻量级的、基于检测的距离估计方法，该方法依赖于从检测中提取的关键特征，如边界框尺寸和大小。然而，这些系统的鲁棒性受到目标检测器中安全漏洞的损害。在本文中，我们引入了ShrinkBox，一种针对ML-ADAS中防撞目标检测的新型后门攻击。与现有操纵对象类别标签或存在性的攻击不同，ShrinkBox巧妙地缩小了真实边界框。这种攻击在数据集检查和标准基准测试中保持未被检测到，同时严重破坏下游距离估计。我们证明，ShrinkBox可以在YOLOv9m目标检测器中实现，攻击成功率（ASR）达到96%，在KITTI数据集的训练实例中仅需4%的投毒率。此外，考虑到我们放松的投毒策略中引入的低误差目标，我们发现ShrinkBox使投毒样本的下游距离估计的平均绝对误差（MAE）增加了3倍以上，可能导致碰撞警告的延迟或完全阻止。", "summary": "本论文介绍了ShrinkBox，一种针对ML-ADAS中目标检测的新型后门攻击。该攻击通过微小地缩小真实边界框来破坏下游距离估计，从而影响防撞功能。与现有攻击不同，ShrinkBox在数据集检查和标准基准测试中难以被检测。实验表明，在YOLOv9m上，使用4%的投毒率，ShrinkBox能达到96%的攻击成功率，并使距离估计误差增加3倍以上，可能导致碰撞预警失效。", "keywords": "后门攻击, 目标检测, ADAS, 碰撞避免, ShrinkBox", "comments": "ShrinkBox的创新之处在于其独特的攻击方式——缩小边界框，而非改变类别或存在性，这使其具有高度的隐蔽性。该研究揭示了ML-ADAS，特别是依赖于目标检测和距离估计的防撞系统，所面临的严重安全漏洞。其重要性在于提醒研究人员和开发者，即使是细微的数据操纵也可能对关键安全功能造成灾难性影响，强调了在自动驾驶领域进行更深入的安全审计的必要性。"}}
{"id": "2507.18636", "title": "Higher-order transmissibility and its linear approximation for in-service crack identification in train wheelset axles", "authors": ["Ehsan Naghizadeh", "Paolo Tiso", "Eleni Chatzi"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18636v1", "summary": "In-service structural health monitoring is a so far rarely exploited, yet\npotent option for early-stage crack detection and identification in train\nwheelset axles. This procedure is non-trivial to enforce on the basis of a\npurely data-driven approach and typically requires the adoption of numerical,\ne.g. finite element-based, simulation schemes of the dynamic behavior of these\naxles. Damage in this particular case can be formulated as a breathing crack\nproblem, which further complicates simulation by introducing response-dependent\nnonlinearities into the picture. In this study, first, a new crack detection\nfeature based on higher-order harmonics of the breathing crack is proposed,\ntermed Higher-Order Transmissibility (HOTr), and, secondly, its sensitivity and\nefficacy are assessed within the context of crack identification. Next, the\nmentioned feature is approximated via use of linear system theory, delivering a\nsurrogate model which facilitates the computation and speeds up the crack\nidentification procedure. The accuracy of the proposed method in reproducing\nthe delivered HOTr is compared against the nonlinear simulation model. The\nobtained results suggest that the approximation of the HOTr can significantly\nreduce the computational burden by eliminating the need for an iterative\nsolution of the governing nonlinear equation of motion, while maintaining a\nhigh level of accuracy when compared against the reference model. This implies\ngreat potential for adoption in in-service damage identification for wheelset\naxles, feasibly within a near real-time context.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18636v1", "cate": "cs.CE", "date": "2025-05-02", "updated": "2025-05-02", "AI": {"title_translation": "列车轮对轴在役裂纹识别中的高阶传递率及其线性近似", "tldr": "本研究提出了一种基于呼吸裂纹高阶谐波的高阶传递率（HOTr）特征，并利用线性系统理论对其进行近似，以实现列车轮对轴在役裂纹的快速准确识别。", "motivation": "在役结构健康监测对于列车轮对轴的早期裂纹检测和识别具有重要意义，但由于呼吸裂纹引入的响应依赖非线性，纯数据驱动方法难以实施，数值模拟也变得复杂，因此需要一种更高效的裂纹识别方法。", "method": "首先，提出了一种基于呼吸裂纹高阶谐波的新型裂纹检测特征，命名为高阶传递率（HOTr）。其次，评估了HOTr在裂纹识别中的敏感性和有效性。最后，利用线性系统理论对HOTr进行近似，建立了一个替代模型，并将其准确性与非线性仿真模型进行比较。", "result": "研究结果表明，高阶传递率的线性近似方法在保持较高精度的同时，显著降低了计算负担，因为它消除了对控制非线性运动方程进行迭代求解的需要。", "conclusion": "高阶传递率的线性近似方法在列车轮对轴的在役损伤识别中具有巨大的应用潜力，有望实现近实时检测。", "translation": "在役结构健康监测是一种迄今为止很少被利用但对于列车轮对轴早期裂纹检测和识别具有强大潜力的选择。这种程序在纯数据驱动方法的基础上难以强制执行，通常需要采用数值，例如基于有限元，模拟这些轴的动态行为。在这种特殊情况下，损伤可以表述为呼吸裂纹问题，这通过引入响应依赖的非线性进一步使模拟复杂化。在本研究中，首先，提出了一种基于呼吸裂纹高阶谐波的新型裂纹检测特征，命名为高阶传递率（HOTr），其次，在裂纹识别的背景下评估了其敏感性和有效性。接下来，通过使用线性系统理论对上述特征进行近似，提供了一个替代模型，该模型有助于计算并加速裂纹识别过程。所提出方法在再现所提供HOTr方面的准确性与非线性仿真模型进行了比较。获得的结果表明，HOTr的近似可以显著减少计算负担，因为它消除了对控制非线性运动方程进行迭代求解的需要，同时与参考模型相比保持了高水平的准确性。这意味着在轮对轴的在役损伤识别中具有巨大的采用潜力，可能在近实时背景下实现。", "summary": "本研究提出了一种用于列车轮对轴在役裂纹识别的新方法。该方法引入了基于呼吸裂纹高阶谐波的高阶传递率（HOTr）作为裂纹检测特征，并进一步利用线性系统理论对其进行近似，以构建一个计算效率更高的替代模型。实验结果表明，该线性近似模型在保持高精度的同时，显著降低了计算量，从而为列车轮对轴的近实时损伤识别提供了巨大潜力。", "keywords": "高阶传递率, 线性近似, 裂纹识别, 轮对轴, 在役监测", "comments": "该论文的创新之处在于提出了高阶传递率（HOTr）这一新的裂纹检测特征，并巧妙地利用线性系统理论对其进行近似，有效解决了呼吸裂纹引入的非线性复杂性导致的计算负担问题。其重要性在于为列车轮对轴的在役结构健康监测提供了一种高效、准确且具实时潜力的解决方案，对于保障铁路运行安全具有重要意义。"}}
{"id": "2505.11027", "title": "A User-centric Game for Balancing V2G Benefits with Battery Degradation of Electric Vehicles", "authors": ["Arghya Mallick", "Georgios Pantazis", "Peyman Mohajerin Esfahani", "Sergio Grammatico"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.11027v2", "summary": "We present a novel user-centric vehicle-to-grid (V2G) framework that enables\nelectric vehicle (EV) users to balance the trade-off between financial benefits\nfrom V2G and battery health degradation based on individual preference signals.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.11027v2", "cate": "eess.SY", "date": "2025-05-16", "updated": "2025-07-25", "AI": {"title_translation": "一种以用户为中心的游戏，用于平衡V2G收益与电动汽车电池退化", "tldr": "本文提出了一种以用户为中心的V2G框架，使电动汽车用户能够根据个人偏好平衡V2G的经济收益与电池健康退化之间的权衡。", "motivation": "该研究的动机是帮助电动汽车用户根据个人偏好，平衡从车网互动（V2G）中获得的经济收益与电池健康退化之间的权衡。", "method": "本文提出了一种新颖的以用户为中心的车网互动（V2G）框架。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "我们提出了一种新颖的以用户为中心的车网互动（V2G）框架，该框架使电动汽车（EV）用户能够根据个人偏好信号，平衡V2G带来的经济收益与电池健康退化之间的权衡。", "summary": "本文介绍了一种新颖的以用户为中心的车网互动（V2G）框架。该框架旨在帮助电动汽车用户，根据其个体偏好，有效地平衡参与V2G所带来的经济收益与其电池健康可能发生的退化之间的权衡关系。", "keywords": "V2G, 用户中心, 电池退化, 电动汽车, 权衡", "comments": "该论文的创新之处在于提出了一个以用户为中心的V2G框架，允许用户根据个人偏好来平衡经济收益和电池健康之间的冲突，这对于推广V2G技术和保护用户资产具有重要意义。"}}
{"id": "2507.18968", "title": "Double Fourier Sphere Methods with Low Rank Approximation for Block Copolymer Systems on Sphere", "authors": ["Wangbo Luo", "Yanxiang Zhao"], "categories": ["math.NA", "cs.NA", "math-ph", "math.MP"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      36 pages, 8 figures", "url": "http://arxiv.org/abs/2507.18968v2", "summary": "We introduce spectral methods for the Ohta-Kawasaki (OK) and Nakazawa-Ohta\n(NO) models on a spherical domain, examining their coarsening dynamics and\nequilibrium pattern formations. We employed the Double Fourier Sphere (DFS)\nmethod for spatial discretization and the second-order Backward Differentiation\nFormula (BDF2) scheme for time evolution, resulting in an efficient\nenergy-stable scheme to simulate the OK and NO models on the unit sphere. Our\nnumerical experiments reveal various self-assembled patterns, such as\nsingle-bubble assemblies in binary systems and double-bubble and mixed-bubble\nassemblies in ternary systems. These patterns closely resemble experimental\nbiomembrane patterns, demonstrating the effectiveness of the OK model in\nreal-world applications. Additionally, our study explores the relationship\nbetween repulsive strength and the number of bubbles in assemblies, confirming\nthe two-thirds law in the OK model. This provides quantitative evidence of how\nself-assembled patterns depend on system parameters in copolymer systems.", "comment": "36 pages, 8 figures", "pdf_url": "http://arxiv.org/pdf/2507.18968v2", "cate": "math.NA", "date": "2025-07-25", "updated": "2025-07-28", "AI": {"title_translation": "球面上嵌段共聚物系统的低秩近似双傅里叶球面方法", "tldr": "该研究介绍了在球形域上模拟Ohta-Kawasaki和Nakazawa-Ohta模型的双傅里叶球面方法，并揭示了各种自组装模式，验证了OK模型中的三分之二定律。", "motivation": "研究Ohta-Kawasaki (OK) 和 Nakazawa-Ohta (NO) 模型在球形域上的粗化动力学和平衡模式形成，并开发高效的数值方法来模拟这些模型。", "method": "引入了用于Ohta-Kawasaki (OK) 和 Nakazawa-Ohta (NO) 模型在球形域上的谱方法。具体采用了双傅里叶球面 (DFS) 方法进行空间离散化，并使用二阶后向微分公式 (BDF2) 方案进行时间演化，从而得到一个高效的能量稳定方案。", "result": "数值实验揭示了各种自组装模式，包括二元系统中的单泡组装以及三元系统中的双泡和混合泡组装。这些模式与实验性生物膜模式相似，证明了OK模型在实际应用中的有效性。此外，研究还探索了排斥强度与组装中气泡数量之间的关系，证实了OK模型中的三分之二定律。", "conclusion": "该研究成功地开发并应用了高效的能量稳定方案来模拟球形域上的OK和NO模型，并通过数值实验揭示了多种自组装模式，并证实了OK模型中的三分之二定律，为共聚物系统中自组装模式如何依赖系统参数提供了定量证据。", "translation": "我们引入了球形域上Ohta-Kawasaki (OK) 和 Nakazawa-Ohta (NO) 模型的谱方法，研究它们的粗化动力学和平衡模式形成。我们采用双傅里叶球面 (DFS) 方法进行空间离散化，并采用二阶后向微分公式 (BDF2) 方案进行时间演化，从而得到一个高效的能量稳定方案，用于模拟单位球体上的OK和NO模型。我们的数值实验揭示了各种自组装模式，例如二元系统中的单泡组装和三元系统中的双泡和混合泡组装。这些模式与实验性生物膜模式非常相似，证明了OK模型在实际应用中的有效性。此外，我们的研究还探讨了排斥强度与组装中气泡数量之间的关系，证实了OK模型中的三分之二定律。这为共聚物系统中自组装模式如何依赖系统参数提供了定量证据。", "summary": "本文介绍了一种在球形域上模拟Ohta-Kawasaki (OK) 和 Nakazawa-Ohta (NO) 模型的谱方法。该方法结合了双傅里叶球面 (DFS) 空间离散化和二阶后向微分公式 (BDF2) 时间演化，实现了高效能量稳定的模拟。数值实验成功再现了多种自组装模式，包括单泡、双泡和混合泡结构，并证明了OK模型在生物膜模式中的有效性。研究还量化了排斥强度与气泡数量的关系，验证了OK模型中的三分之二定律，为理解共聚物系统中的自组装提供了参数依赖性证据。", "keywords": "双傅里叶球面方法, Ohta-Kawasaki模型, Nakazawa-Ohta模型, 自组装模式, 球形域", "comments": "这篇论文的创新点在于将双傅里叶球面方法与低秩近似结合，应用于球形域上的嵌段共聚物系统模拟，特别是Ohta-Kawasaki和Nakazawa-Ohta模型。其重要性在于提供了一种高效且能量稳定的数值方案，能够准确模拟复杂的自组装模式，并验证了理论模型（如三分之二定律）在实际系统中的适用性，对理解生物膜等实际应用中的自组装现象具有重要意义。"}}
{"id": "2507.18104", "title": "A Multimodal Seq2Seq Transformer for Predicting Brain Responses to Naturalistic Stimuli", "authors": ["Qianyi He", "Yuan Chang Leong"], "categories": ["cs.CV", "q-bio.NC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18104v2", "summary": "The Algonauts 2025 Challenge called on the community to develop encoding\nmodels that predict whole-brain fMRI responses to naturalistic multimodal\nmovies. In this submission, we propose a sequence-to-sequence Transformer that\nautoregressively predicts fMRI activity from visual, auditory, and language\ninputs. Stimulus features were extracted using pretrained models including\nVideoMAE, HuBERT, Qwen, and BridgeTower. The decoder integrates information\nfrom prior brain states and current stimuli via dual cross-attention mechanisms\nthat attend to both perceptual information extracted from the stimulus as well\nas narrative information provided by high-level summaries of the content. One\ncore innovation of our approach is the use of sequences of multimodal context\nto predict sequences of brain activity, enabling the model to capture\nlong-range temporal structure in both stimuli and neural responses. Another is\nthe combination of a shared encoder with partial subject-specific decoder,\nwhich leverages common representational structure across subjects while\naccounting for individual variability. Our model achieves strong performance on\nboth in-distribution and out-of-distribution data, demonstrating the\neffectiveness of temporally-aware, multimodal sequence modeling for brain\nactivity prediction. The code is available at\nhttps://github.com/Angelneer926/Algonauts_challenge.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18104v2", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-25", "AI": {"title_translation": "多模态Seq2Seq Transformer用于预测自然刺激下的脑部反应", "tldr": "该论文提出了一个多模态Seq2Seq Transformer模型，用于预测自然主义多模态电影刺激下的全脑fMRI反应，并通过序列建模和混合编码器-解码器架构实现了良好性能。", "motivation": "响应Algonauts 2025挑战赛，开发能够预测自然主义多模态电影下全脑fMRI反应的编码模型。", "method": "提出一个序列到序列的Transformer模型，从视觉、听觉和语言输入中自回归地预测fMRI活动。利用VideoMAE、HuBERT、Qwen和BridgeTower等预训练模型提取刺激特征。解码器通过双重交叉注意力机制整合先前的脑状态和当前刺激，关注感知信息和叙事信息。核心创新包括使用多模态上下文序列来预测脑活动序列以捕捉长程时间结构，以及结合共享编码器和部分受试者特异性解码器来处理个体差异。", "result": "模型在分布内和分布外数据上均表现出强大的性能。", "conclusion": "证明了时间感知、多模态序列建模在脑活动预测方面的有效性。", "translation": "Algonauts 2025挑战赛号召社区开发编码模型，以预测对自然主义多模态电影的全脑fMRI反应。在此提交中，我们提出了一个序列到序列的Transformer模型，该模型从视觉、听觉和语言输入中自回归地预测fMRI活动。刺激特征使用VideoMAE、HuBERT、Qwen和BridgeTower等预训练模型提取。解码器通过双重交叉注意力机制整合先前的脑状态和当前刺激，这些机制同时关注从刺激中提取的感知信息以及由内容高级摘要提供的叙事信息。我们方法的一个核心创新是使用多模态上下文序列来预测脑活动序列，使模型能够捕获刺激和神经反应中的长程时间结构。另一个创新是结合了共享编码器和部分受试者特异性解码器，这利用了跨受试者的共同表征结构，同时考虑了个体变异性。我们的模型在分布内和分布外数据上均取得了强大的性能，证明了时间感知、多模态序列建模在脑活动预测方面的有效性。代码可在https://github.com/Angelneer926/Algonauts_challenge 获取。", "summary": "这篇论文提出了一个用于Algonauts 2025挑战赛的多模态序列到序列Transformer模型，旨在预测自然主义电影刺激下的全脑fMRI反应。该模型整合了视觉、听觉和语言输入，利用预训练模型提取特征，并通过双重交叉注意力解码器结合感知与叙事信息。其创新点在于通过序列建模捕捉长程时间结构，并结合共享编码器与受试者特异性解码器处理个体差异。模型在多种数据集上表现出色，验证了其在脑活动预测中的有效性。", "keywords": "fMRI预测, 多模态, Transformer, 序列到序列, 脑反应", "comments": "该论文的核心创新在于其对时间序列建模的强调，特别是通过利用多模态上下文序列来预测脑活动序列，这对于捕捉自然刺激下复杂、动态的脑反应至关重要。此外，结合共享编码器和受试者特异性解码器的混合架构，有效地平衡了跨个体共性与个体差异，提升了模型的泛化能力和实用性。这一方法在Algonauts挑战赛中取得的强大性能，凸显了其在神经科学与机器学习交叉领域的重要贡献。"}}
{"id": "2507.18833", "title": "Exploring the Jupyter Ecosystem: An Empirical Study of Bugs and Vulnerabilities", "authors": ["Wenyuan Jiang", "Diany Pressato", "Harsh Darji", "Thibaud Lutellier"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18833v1", "summary": "Background. Jupyter notebooks are one of the main tools used by data\nscientists. Notebooks include features (configuration scripts, markdown,\nimages, etc.) that make them challenging to analyze compared to traditional\nsoftware. As a result, existing software engineering models, tools, and studies\ndo not capture the uniqueness of Notebook's behavior. Aims. This paper aims to\nprovide a large-scale empirical study of bugs and vulnerabilities in the\nNotebook ecosystem. Method. We collected and analyzed a large dataset of\nNotebooks from two major platforms. Our methodology involved quantitative\nanalyses of notebook characteristics (such as complexity metrics, contributor\nactivity, and documentation) to identify factors correlated with bugs.\nAdditionally, we conducted a qualitative study using grounded theory to\ncategorize notebook bugs, resulting in a comprehensive bug taxonomy. Finally,\nwe analyzed security-related commits and vulnerability reports to assess risks\nassociated with Notebook deployment frameworks. Results. Our findings highlight\nthat configuration issues are among the most common bugs in notebook documents,\nfollowed by incorrect API usage. Finally, we explore common vulnerabilities\nassociated with popular deployment frameworks to better understand risks\nassociated with Notebook development. Conclusions. This work highlights that\nnotebooks are less well-supported than traditional software, resulting in more\ncomplex code, misconfiguration, and poor maintenance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18833v1", "cate": "cs.SE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "探索Jupyter生态系统：一项关于错误和漏洞的实证研究", "tldr": "本研究对Jupyter Notebooks中的错误和漏洞进行了大规模实证分析，发现配置问题和不当API使用是常见问题，并指出其维护支持不足导致的代码复杂性、配置错误和维护不善。", "motivation": "Jupyter Notebooks是数据科学家常用的工具，但其特性（如配置脚本、Markdown、图像等）使得其分析比传统软件更具挑战性。现有的软件工程模型、工具和研究未能捕捉到Notebook行为的独特性。因此，本文旨在对Notebook生态系统中的错误和漏洞进行大规模实证研究。", "method": "研究收集并分析了来自两个主要平台的Notebooks大型数据集。方法包括对Notebook特性（如复杂性指标、贡献者活动和文档）进行定量分析，以识别与错误相关的因素。此外，使用扎根理论进行了定性研究，对Notebook错误进行分类，形成了全面的错误分类法。最后，分析了与安全相关的提交和漏洞报告，以评估与Notebook部署框架相关的风险。", "result": "研究结果表明，配置问题是Notebook文档中最常见的错误之一，其次是不正确的API使用。研究还探讨了与流行部署框架相关的常见漏洞，以更好地理解与Notebook开发相关的风险。", "conclusion": "这项工作强调，Notebooks的维护支持不如传统软件，导致代码更复杂、配置错误和维护不善。", "translation": "背景。Jupyter Notebooks是数据科学家使用的主要工具之一。Notebooks包含使其分析比传统软件更具挑战性的特性（配置脚本、markdown、图像等）。因此，现有的软件工程模型、工具和研究未能捕捉到Notebook行为的独特性。目的。本文旨在对Notebook生态系统中的错误和漏洞进行大规模实证研究。方法。我们收集并分析了来自两个主要平台的Notebooks大型数据集。我们的方法包括对Notebook特性（如复杂性指标、贡献者活动和文档）进行定量分析，以识别与错误相关的因素。此外，我们使用扎根理论进行了一项定性研究，以对Notebook错误进行分类，从而形成了一个全面的错误分类法。最后，我们分析了与安全相关的提交和漏洞报告，以评估与Notebook部署框架相关的风险。结果。我们的发现强调，配置问题是Notebook文档中最常见的错误之一，其次是不正确的API使用。最后，我们探讨了与流行部署框架相关的常见漏洞，以更好地理解与Notebook开发相关的风险。结论。这项工作强调，Notebooks的维护支持不如传统软件，导致代码更复杂、配置错误和维护不善。", "summary": "本研究对Jupyter生态系统中的错误和漏洞进行了首次大规模实证分析。通过对来自两大平台的大型Notebook数据集进行定量和定性分析，研究发现配置问题和不当API使用是Notebook中最常见的错误。此外，研究还揭示了与部署框架相关的常见漏洞。论文总结指出，Jupyter Notebooks相比传统软件缺乏良好的支持，导致代码复杂、配置错误和维护不善。", "keywords": "Jupyter Notebooks, 错误, 漏洞, 实证研究, 数据科学", "comments": "这项研究的创新之处在于它是首次对Jupyter Notebook生态系统中的错误和漏洞进行大规模实证研究，填补了现有软件工程研究在Notebooks领域空白。通过结合定量和定性分析，它提供了一个全面的错误分类法，并揭示了Jupyter Notebooks在配置和API使用方面的常见问题，以及部署框架的安全风险。其重要性在于为Notebook开发者和维护者提供了宝贵的见解，有助于改进Notebook的开发实践和工具支持。研究也突出了Notebooks维护支持不足的局限性。"}}
{"id": "2507.19119", "title": "PatchTraj: Dynamic Patch Representation Learning for Time-Frequency Trajectory Prediction", "authors": ["Yanghong Liu", "Xingping Dong", "Ming Li", "Weixing Zhang", "Yidong Lou"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19119v2", "summary": "Pedestrian trajectory prediction is crucial for autonomous driving and\nrobotics. While existing point-based and grid-based methods expose two key\nlimitations: insufficiently modeling human motion dynamics, as they fail to\nbalance local motion details with long-range spatiotemporal dependencies, and\nthe time representation lacks interaction with the frequency domain in modeling\ntrajectory sequences. To address these challenges, we propose PatchTraj, a\ndynamic patch-based trajectory prediction framework that unifies time-domain\nand frequency-domain representations. Specifically, we decompose the trajectory\ninto raw time sequences and frequency components, employing dynamic patch\npartitioning for multi-scale trajectory segmentation to capture hierarchical\nmotion patterns. Each patch is processed by an adaptive embedding layer with\nscale-aware feature extraction, followed by hierarchical feature aggregation to\nmodel both fine-grained and long-range dependencies. The outputs of two\nbranches interact via cross-modal attention, enabling complementary fusion of\ntemporal and spectral cues. Finally, a Transformer encoder-decoder integrates\nboth modalities to autoregressively predict future trajectories. Extensive\nexperiments on ETH-UCY, SDD, NBA, and JRDB datasets demonstrate that our method\nachieves state-of-the-art performance with high efficiency.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19119v2", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-28", "AI": {"title_translation": "PatchTraj：时频轨迹预测的动态补丁表示学习", "tldr": "PatchTraj提出了一种动态基于补丁的时频统一框架，用于行人轨迹预测，解决了现有方法在运动动力学建模和时频交互方面的不足，并实现了最先进的性能。", "motivation": "现有的基于点和基于网格的方法在建模人类运动动力学方面存在不足，未能平衡局部运动细节与长程时空依赖性；时间表示在建模轨迹序列时缺乏与频域的交互。", "method": "提出PatchTraj，一个动态基于补丁的轨迹预测框架，统一了时域和频域表示。具体而言，将轨迹分解为原始时间序列和频率分量，采用动态补丁划分进行多尺度轨迹分割以捕捉分层运动模式。每个补丁由具有尺度感知特征提取的自适应嵌入层处理，然后进行分层特征聚合以建模细粒度和长程依赖。两个分支的输出通过跨模态注意力进行交互，实现时间线索和频谱线索的互补融合。最后，Transformer编码器-解码器整合两种模态以自回归预测未来轨迹。", "result": "在ETH-UCY、SDD、NBA和JRDB数据集上的大量实验表明，该方法以高效率实现了最先进的性能。", "conclusion": "PatchTraj通过统一时域和频域表示，并采用动态补丁学习，有效解决了行人轨迹预测中运动动力学建模和时频交互的挑战，实现了卓越的预测性能。", "translation": "行人轨迹预测对于自动驾驶和机器人技术至关重要。然而，现有的基于点和基于网格的方法存在两个关键限制：它们未能平衡局部运动细节与长程时空依赖性，从而无法充分建模人类运动动力学；时间表示在建模轨迹序列时缺乏与频域的交互。为了解决这些挑战，我们提出了PatchTraj，一个动态的基于补丁的轨迹预测框架，它统一了时域和频域表示。具体而言，我们将轨迹分解为原始时间序列和频率分量，采用动态补丁划分进行多尺度轨迹分割，以捕捉分层运动模式。每个补丁都由一个具有尺度感知特征提取的自适应嵌入层进行处理，然后进行分层特征聚合，以建模细粒度和长程依赖性。两个分支的输出通过跨模态注意力进行交互，从而实现时间线索和频谱线索的互补融合。最后，一个Transformer编码器-解码器整合两种模态，以自回归方式预测未来轨迹。在ETH-UCY、SDD、NBA和JRDB数据集上的大量实验表明，我们的方法以高效率实现了最先进的性能。", "summary": "PatchTraj是一种新颖的行人轨迹预测框架，旨在克服现有方法在运动动力学建模和时频交互方面的不足。它通过动态补丁划分将轨迹分解为时间序列和频率分量，并采用分层特征聚合和跨模态注意力融合，统一了时域和频域表示。该方法利用Transformer编码器-解码器进行未来轨迹的自回归预测，并在多个基准数据集上取得了最先进的性能和高效率。", "keywords": "轨迹预测, 动态补丁, 时频表示, Transformer, 跨模态注意力", "comments": "本文的创新点在于首次将动态补丁表示学习应用于轨迹预测，并成功地统一了时域和频域信息，以更全面地捕捉复杂的运动模式。这种时频结合的方法为轨迹预测领域提供了一个新的视角和有效的解决方案，对于提升自动驾驶和机器人技术的安全性与鲁棒性具有重要意义。"}}
{"id": "2507.19202", "title": "Latent Granular Resynthesis using Neural Audio Codecs", "authors": ["Nao Tokui", "Tom Baker"], "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      Accepted at ISMIR 2025 Late Breaking Demos", "url": "http://arxiv.org/abs/2507.19202v1", "summary": "We introduce a novel technique for creative audio resynthesis that operates\nby reworking the concept of granular synthesis at the latent vector level. Our\napproach creates a \"granular codebook\" by encoding a source audio corpus into\nlatent vector segments, then matches each latent grain of a target audio signal\nto its closest counterpart in the codebook. The resulting hybrid sequence is\ndecoded to produce audio that preserves the target's temporal structure while\nadopting the source's timbral characteristics. This technique requires no model\ntraining, works with diverse audio materials, and naturally avoids the\ndiscontinuities typical of traditional concatenative synthesis through the\ncodec's implicit interpolation during decoding. We include supplementary\nmaterial at https://github.com/naotokui/latentgranular/ , as well as a\nproof-of-concept implementation to allow users to experiment with their own\nsounds at https://huggingface.co/spaces/naotokui/latentgranular .", "comment": "Accepted at ISMIR 2025 Late Breaking Demos", "pdf_url": "http://arxiv.org/pdf/2507.19202v1", "cate": "cs.SD", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "神经音频编解码器实现的潜在粒度重合成", "tldr": "一种利用神经音频编解码器在潜在向量层面进行粒度重合成的新技术，无需训练，能结合目标音频的时间结构和源音频的音色特性，并避免传统拼接合成的跳变。", "motivation": "为创造性音频重合成提供一种新颖、高效且能避免传统拼接合成中不连续性的方法。", "method": "该方法在潜在向量层面重构粒度合成。首先，通过神经音频编解码器将源音频语料库编码成潜在向量片段，创建“粒度码本”。然后，将目标音频信号的每个潜在粒度与其在码本中最接近的对应物进行匹配。最后，解码生成的混合序列以产生新的音频。", "result": "生成的音频保留了目标音频的时间结构，同时吸收了源音频的音色特性。该技术无需模型训练，适用于多种音频材料，并通过编解码器在解码过程中的隐式插值，自然地避免了传统拼接合成中常见的不连续性。", "conclusion": "论文提出了一种新颖、高效且鲁棒的音频重合成技术，该技术通过在潜在向量层面操作粒度合成，克服了传统方法的局限性，并为创造性音频处理提供了新的途径。", "translation": "我们介绍了一种新颖的创造性音频重合成技术，该技术通过在潜在向量层面改造粒度合成的概念来运行。我们的方法通过将源音频语料库编码成潜在向量片段来创建“粒度码本”，然后将目标音频信号的每个潜在粒度与其在码本中最接近的对应物进行匹配。由此产生的混合序列被解码以产生保留目标时间结构同时采用源音色特性的音频。该技术无需模型训练，适用于多种音频材料，并通过编解码器在解码过程中的隐式插值，自然地避免了传统拼接合成中常见的不连续性。我们提供了补充材料，网址为 https://github.com/naotokui/latentgranular/ ，以及一个概念验证实现，允许用户在 https://huggingface.co/spaces/naotokui/latentgranular 上使用他们自己的声音进行实验。", "summary": "本文提出了一种创新的音频重合成技术，即“潜在粒度重合成”。该方法利用神经音频编解码器，将源音频编码为潜在向量片段以构建“粒度码本”，然后将目标音频的潜在粒度与码本中的片段匹配，最终解码生成混合音频。这种技术能够在保持目标音频时间结构的同时，融合源音频的音色特征，并且无需训练，适用于不同音频类型，有效避免了传统拼接合成中常见的不连续性。", "keywords": "粒度合成, 潜在向量, 音频重合成, 神经音频编解码器, 音色迁移", "comments": "这项研究通过将粒度合成提升到潜在向量层面，并结合神经音频编解码器，提供了一种新颖且高效的音频重合成范式。其创新之处在于无需模型训练即可实现高质量的音色迁移和结构保留，并巧妙地解决了传统拼接合成的跳变问题。这对于音乐创作、音效设计等领域具有重要的应用潜力。"}}
{"id": "2507.18727", "title": "RIS Codebook Index Assignment under Imperfect Control Links Using TSP-Inspired Optimization", "authors": ["Liangshun Wu", "Wen Chen", "Qingqing Wu", "Xudong Bai", "Kunlun Wang"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      RIS codebook", "url": "http://arxiv.org/abs/2507.18727v1", "summary": "Reconfigurable Intelligent Surfaces (RIS) promise transformative gains in\nwireless communications by enabling programmable control of the propagation\nenvironment through discrete phase configurations. In practical deployments,\nthe control of RIS phase states is typically managed using finite codebooks,\nwith configuration indices transmitted over low latency, yet imperfect,\nwireless feedback channels. Even rare feedback bit errors can lead to\nsignificant mismatches between intended and applied RIS states, degrading\nsystem performance. This paper addresses the challenge of robust RIS codebook\nindex assignment by formulating it as a combinatorial optimization problem,\nequivalent to the Traveling Salesman Problem (TSP), where codewords are\n\"cities\" and edge weights reflect SNR degradation under codeword confusion. A\nnovel three-phase heuristic algorithm is proposed to solve this, consisting of\na provision phase, a shotgun phase, and a fuzzy concatenation phase. Simulation\nresults show that the method outperforms conventional indexing strategies and\nachieves near-optimal robustness to index errors, while also being scalable and\nhardwareagnostic for real time deployment. Future work includes multiple bits\nerror correction and online adaptive mapping for time varying channels.", "comment": "RIS codebook", "pdf_url": "http://arxiv.org/pdf/2507.18727v1", "cate": "cs.IT", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "不完美控制链路下基于TSP优化算法的RIS码本索引分配", "tldr": "本文提出了一种受旅行商问题启发的优化算法，用于在存在不完美控制链路的情况下实现鲁棒的RIS码本索引分配，以减少错误导致的性能下降。", "motivation": "在实际部署中，可重构智能表面（RIS）的相位状态控制通过有限码本进行，其配置索引通过低延迟但不完美的无线反馈信道传输。即使是罕见的反馈比特错误也可能导致预期与实际RIS状态之间的严重不匹配，从而降低系统性能。", "method": "将鲁棒的RIS码本索引分配问题建模为一个组合优化问题，等同于旅行商问题（TSP），其中码字是“城市”，边缘权重反映了码字混淆下的信噪比（SNR）下降。提出了一种新颖的三阶段启发式算法来解决该问题，包括：准备阶段、散弹枪阶段和模糊连接阶段。", "result": "仿真结果表明，该方法优于传统的索引策略，对索引错误表现出接近最优的鲁棒性，同时具有可扩展性且与硬件无关，适用于实时部署。", "conclusion": "本文提出的基于TSP启发式优化算法的RIS码本索引分配方法，在不完美控制链路下能有效提升系统鲁棒性，并优于传统策略，具有实际部署潜力。", "translation": "可重构智能表面（RIS）通过实现传播环境的可编程控制，有望在无线通信中带来变革性增益。在实际部署中，RIS相位状态的控制通常使用有限码本进行管理，其配置索引通过低延迟但并不完美的无线反馈信道传输。即使是罕见的反馈比特错误也可能导致预期与实际RIS状态之间的严重不匹配，从而降低系统性能。本文通过将其表述为一个组合优化问题来解决鲁棒RIS码本索引分配的挑战，该问题等同于旅行商问题（TSP），其中码字是“城市”，边缘权重反映了码字混淆下的信噪比（SNR）下降。提出了一种新颖的三阶段启发式算法来解决该问题，包括准备阶段、散弹枪阶段和模糊连接阶段。仿真结果表明，该方法优于传统的索引策略，对索引错误表现出接近最优的鲁棒性，同时具有可扩展性且与硬件无关，适用于实时部署。未来的工作包括多比特纠错和时变信道在线自适应映射。", "summary": "本文针对可重构智能表面（RIS）在不完美控制链路下码本索引分配的鲁棒性问题，将其建模为旅行商问题（TSP），并提出了一种创新的三阶段启发式算法。该算法通过优化码字分配，有效降低了反馈错误对系统性能的影响，仿真结果显示其性能优于传统方法，并具有良好的鲁棒性、可扩展性和实时部署潜力。", "keywords": "可重构智能表面, 码本索引分配, 旅行商问题, 鲁棒性, 启发式算法", "comments": "本文的创新点在于将RIS码本索引分配问题巧妙地转化为经典的旅行商问题，并提出了一种新颖的三阶段启发式算法来解决。这种方法有效地提高了RIS系统在存在控制链路错误时的鲁棒性，对于RIS的实际部署具有重要意义。该方法具有可扩展性和硬件无关性，进一步增强了其实用性。未来的多比特纠错和在线自适应映射研究将使其更加完善。"}}
{"id": "2507.18192", "title": "TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance", "authors": ["Minghao Fu", "Guo-Hua Wang", "Xiaohao Chen", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025. The code is publicly available at this https URL", "url": "http://arxiv.org/abs/2507.18192v2", "summary": "Recent advances in text-to-image synthesis largely benefit from sophisticated\nsampling strategies and classifier-free guidance (CFG) to ensure high-quality\ngeneration. However, CFG's reliance on two forward passes, especially when\ncombined with intricate sampling algorithms, results in prohibitively high\ninference costs. To address this, we introduce TeEFusion (Text Embeddings\nFusion), a novel and efficient distillation method that directly incorporates\nthe guidance magnitude into the text embeddings and distills the teacher\nmodel's complex sampling strategy. By simply fusing conditional and\nunconditional text embeddings using linear operations, TeEFusion reconstructs\nthe desired guidance without adding extra parameters, simultaneously enabling\nthe student model to learn from the teacher's output produced via its\nsophisticated sampling approach. Extensive experiments on state-of-the-art\nmodels such as SD3 demonstrate that our method allows the student to closely\nmimic the teacher's performance with a far simpler and more efficient sampling\nstrategy. Consequently, the student model achieves inference speeds up to\n6$\\times$ faster than the teacher model, while maintaining image quality at\nlevels comparable to those obtained through the teacher's complex sampling\napproach. The code is publicly available at\nhttps://github.com/AIDC-AI/TeEFusion.", "comment": "Accepted by ICCV 2025. The code is publicly available at\n  https://github.com/AIDC-AI/TeEFusion", "pdf_url": "http://arxiv.org/pdf/2507.18192v2", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-25", "AI": {"title_translation": "TeEFusion：融合文本嵌入以蒸馏无分类器引导", "tldr": "无分类器引导（CFG）导致文本到图像合成推理成本高昂。TeEFusion通过将CFG蒸馏到文本嵌入中来解决此问题，可将推理速度提高高达6倍，同时保持图像质量。", "motivation": "无分类器引导（CFG）对两次前向传播的依赖，特别是与复杂的采样算法结合时，导致文本到图像合成的推理成本过高。", "method": "TeEFusion（文本嵌入融合）是一种新颖的蒸馏方法，通过线性操作融合条件和无条件文本嵌入，将引导幅度直接整合到文本嵌入中，从而在不增加额外参数的情况下重建引导。它还使学生模型能够学习教师模型的复杂采样策略。", "result": "在SD3等模型上的实验表明，TeEFusion使学生模型能够以更简单、更高效的采样策略密切模仿教师模型的性能。学生模型的推理速度比教师模型快高达6倍，同时保持了与教师模型复杂采样方法获得的图像质量相当的水平。", "conclusion": "TeEFusion成功地将复杂的无分类器引导和采样策略蒸馏到更高效的学生模型中，显著降低了推理成本（高达6倍的速度提升），同时在文本到图像合成中保持了图像质量。", "translation": "文本到图像合成的最新进展在很大程度上受益于复杂的采样策略和无分类器引导（CFG）以确保高质量生成。然而，CFG对两次前向传播的依赖，特别是与复杂的采样算法结合时，导致了过高的推理成本。为了解决这个问题，我们引入了TeEFusion（文本嵌入融合），这是一种新颖高效的蒸馏方法，它直接将引导幅度整合到文本嵌入中，并蒸馏教师模型的复杂采样策略。通过简单地使用线性操作融合条件和无条件文本嵌入，TeEFusion在不增加额外参数的情况下重建了所需的引导，同时使学生模型能够从教师通过其复杂的采样方法产生的输出中学习。在SD3等最先进模型上的大量实验表明，我们的方法允许学生模型以更简单、更高效的采样策略密切模仿教师的性能。因此，学生模型实现了比教师模型快6倍的推理速度，同时保持了与教师复杂采样方法所获得图像质量相当的水平。代码已在https://github.com/AIDC-AI/TeEFusion 公开。", "summary": "TeEFusion是一种新颖高效的蒸馏方法，旨在解决文本到图像合成中无分类器引导（CFG）带来的高推理成本问题。它通过线性融合条件和无条件文本嵌入，将引导幅度直接整合到文本嵌入中，并在不增加额外参数的情况下重建引导。该方法还允许学生模型学习教师模型的复杂采样策略。实验证明，TeEFusion使学生模型在保持图像质量的同时，将推理速度提高了高达6倍。", "keywords": "文本到图像合成, 无分类器引导, 蒸馏, 文本嵌入, 推理加速", "comments": "TeEFusion的创新之处在于其通过直接融合文本嵌入来蒸馏CFG，从而避免了额外的参数和复杂的采样步骤，显著提高了推理效率。其重要性在于为文本到图像生成模型提供了一种有效的成本优化方案，使得高质量生成更加实用和可访问。"}}
{"id": "2507.19300", "title": "Negative news posts are less prevalent and generate lower user engagement than non-negative news posts across six countries", "authors": ["Szymon Talaga", "Dominik Batorski", "Magdalena Wojcieszak"], "categories": ["cs.SI", "cs.LG"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19300v1", "summary": "Although news negativity is often studied, missing is comparative evidence on\nthe prevalence of and engagement with negative political and non-political news\nposts on social media. We use 6,081,134 Facebook posts published between\nJanuary 1, 2020, and April 1, 2024, by 97 media organizations in six countries\n(U.S., UK, Ireland, Poland, France, Spain) and develop two multilingual\nclassifiers for labeling posts as (non-)political and (non-)negative. We show\nthat: (1) negative news posts constitute a relatively small fraction (12.6%);\n(2) political news posts are neither more nor less negative than non-political\nnews posts; (3) U.S. political news posts are less negative relative to the\nother countries on average (40% lower odds); (4) Negative news posts get 15%\nfewer likes and 13% fewer comments than non-negative news posts. Lastly, (5) we\nprovide estimates of the proportion of the total volume of user engagement with\nnegative news posts and show that only between 10.2% to 13.1% of engagement is\nlinked to negative posts by the analyzed news organizations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19300v1", "cate": "cs.SI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "负面新闻帖子的流行度低于非负面新闻帖子，且用户参与度也较低（涵盖六个国家）", "tldr": "在六个国家中，社交媒体上的负面新闻帖子数量较少，且用户参与度低于非负面新闻帖子。", "motivation": "尽管新闻负面性常被研究，但缺乏关于社交媒体上负面政治和非政治新闻帖子流行度及参与度的比较证据。", "method": "研究使用了2020年1月1日至2024年4月1日期间，由六个国家（美国、英国、爱尔兰、波兰、法国、西班牙）的97家媒体机构发布的6,081,134个Facebook帖子。开发了两个多语言分类器，用于将帖子标记为（非）政治性和（非）负面性。", "result": "1. 负面新闻帖子所占比例相对较小（12.6%）。2. 政治新闻帖子与非政治新闻帖子在负面性上没有显著差异。3. 美国政治新闻帖子的负面性相对其他国家平均较低（低40%的几率）。4. 负面新闻帖子的点赞量比非负面新闻帖子少15%，评论量少13%。5. 用户对负面新闻帖子总参与量的估计显示，只有10.2%至13.1%的参与量与负面帖子相关。", "conclusion": "社交媒体上的负面新闻帖子不仅数量较少，而且其用户参与度也明显低于非负面新闻帖子，这挑战了负面新闻主导社交媒体的普遍看法。", "translation": "尽管新闻的负面性经常被研究，但关于社交媒体上负面政治和非政治新闻帖子的流行度及其参与度的比较证据却缺失。我们使用了2020年1月1日至2024年4月1日期间，由六个国家（美国、英国、爱尔兰、波兰、法国、西班牙）的97家媒体机构发布的6,081,134个Facebook帖子，并开发了两个多语言分类器，用于将帖子标记为（非）政治性和（非）负面性。我们发现：(1) 负面新闻帖子所占比例相对较小（12.6%）；(2) 政治新闻帖子与非政治新闻帖子在负面性上没有显著差异；(3) 美国政治新闻帖子的负面性相对其他国家平均较低（低40%的几率）；(4) 负面新闻帖子的点赞量比非负面新闻帖子少15%，评论量少13%。最后，(5) 我们提供了用户对负面新闻帖子总参与量的估计，并显示在所分析的新闻机构中，只有10.2%至13.1%的参与量与负面帖子相关。", "summary": "本研究利用来自六个国家97家媒体机构的数百万个Facebook帖子，分析了社交媒体上负面新闻帖子的流行度及其用户参与度。研究发现，负面新闻帖子的数量远少于非负面新闻帖子，且其获得的赞和评论也显著低于非负面新闻。此外，政治新闻的负面性与非政治新闻无异，而美国政治新闻的负面性相对较低。这些结果表明，社交媒体上的负面新闻并非如普遍认为的那样占据主导地位。", "keywords": "负面新闻, 用户参与度, 社交媒体, 新闻流行度, Facebook", "comments": "这项研究通过大规模多国数据分析，提供了关于社交媒体新闻负面性及其用户参与度的重要实证证据，挑战了负面新闻主导社交媒体的常见观点。其创新之处在于使用了多语言分类器和跨国比较，为理解社交媒体新闻生态提供了新的视角。"}}
{"id": "2507.18262", "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation", "authors": ["Chenyu Su", "Weiwei Shang", "Chen Qian", "Fei Zhang", "Shuang Cong"], "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "primary_category": "Subjects:       Robotics (cs.RO)", "pdf_link": null, "comments": "Comments:      12 pages,9 figures", "url": "http://arxiv.org/abs/2507.18262v2", "summary": "Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos are available at\nhttps://github.com/scy-v/ReSem3D and https://resem3d.github.io.", "comment": "12 pages,9 figures", "pdf_url": "http://arxiv.org/pdf/2507.18262v2", "cate": "cs.RO", "date": "2025-07-24", "updated": "2025-07-25", "AI": {"title_translation": "ReSem3D：通过细粒度语义接地实现可泛化机器人操作的可精炼3D空间约束", "tldr": "ReSem3D提出了一个统一的机器人操作框架，通过结合多模态大语言模型和视觉基础模型，实现细粒度语义接地和动态构建分层3D空间约束，以解决现有方法在语义粒度、实时性和鲁棒性方面的局限性，从而在零样本条件下实现多样化操作任务。", "motivation": "现有方法在语义驱动的3D空间约束构建方面存在三个主要限制：1) 约束建模中的语义粒度粗糙；2) 缺乏实时闭环规划；3) 在语义多样化环境中的鲁棒性受损。", "method": "ReSem3D是一个统一的操作框架，利用视觉基础模型（VFMs）和多模态大语言模型（MLLMs）的协同作用，实现细粒度视觉接地并动态构建分层3D空间约束。该框架由MLLMs中的分层递归推理驱动，通过两阶段（部分级提取和区域级细化）与VFMs交互，从自然语言指令和RGB-D观测中自动构建3D空间约束。这些约束随后被编码为关节空间中的实时优化目标，以实现对动态扰动的反应行为。", "result": "ReSem3D在语义丰富的家庭环境和稀疏的化学实验室环境中进行了广泛的模拟和真实世界实验。结果表明，ReSem3D在零样本条件下执行多样化的操作任务，表现出强大的适应性和泛化能力。", "conclusion": "ReSem3D通过结合MLLMs和VFMs的优势，成功解决了现有语义驱动3D空间约束方法的局限性，实现了在语义多样化环境中对动态扰动具有鲁棒性和泛化能力的细粒度、实时机器人操作。", "translation": "语义驱动的3D空间约束将高级语义表示与低级动作空间对齐，促进了机器人操作中任务理解和执行的统一。多模态大语言模型（MLLMs）和视觉基础模型（VFMs）的协同推理使得跨模态3D空间约束的构建成为可能。然而，现有方法存在三个关键限制：(1) 约束建模中的语义粒度粗糙，(2) 缺乏实时闭环规划，(3) 在语义多样化环境中的鲁棒性受损。为了应对这些挑战，我们提出了ReSem3D，一个用于语义多样化环境的统一操作框架，利用VFMs和MLLMs之间的协同作用，实现细粒度视觉接地并动态构建分层3D空间约束以进行实时操作。具体而言，该框架由MLLMs中的分层递归推理驱动，通过两阶段：部分级提取和区域级细化，与VFMs交互，从自然语言指令和RGB-D观测中自动构建3D空间约束。随后，这些约束被编码为关节空间中的实时优化目标，从而能够对动态扰动做出反应行为。在语义丰富的家庭和稀疏的化学实验室环境中进行了广泛的模拟和真实世界实验。结果表明，ReSem3D在零样本条件下执行多样化的操作任务，表现出强大的适应性和泛化能力。代码和视频可在https://github.com/scy-v/ReSem3D和https://resem3d.github.io获取。", "summary": "ReSem3D是一个创新的机器人操作框架，旨在解决现有语义驱动3D空间约束方法在语义粒度、实时性和鲁棒性方面的不足。它通过协同利用多模态大语言模型（MLLMs）和视觉基础模型（VFMs），实现了细粒度视觉接地和分层3D空间约束的动态构建。该框架采用两阶段推理（部分级提取和区域级细化）从自然语言指令和RGB-D数据中生成约束，并将其编码为实时优化目标以应对动态扰动。实验证明ReSem3D在零样本条件下，在多样化环境中展现出强大的适应性和泛化能力，能够执行多项操作任务。", "keywords": "机器人操作, 3D空间约束, 语义接地, 多模态大语言模型, 视觉基础模型", "comments": "本文的创新点在于提出了ReSem3D框架，通过结合MLLMs和VFMs的优势，解决了现有方法在细粒度语义接地、实时闭环规划以及在语义多样化环境中的鲁棒性不足的问题。其分层递归推理和两阶段约束构建方法，以及将约束编码为实时优化目标，是实现其卓越性能的关键。该研究对于提升机器人操作的泛化能力和适应性具有重要意义。"}}
{"id": "2507.15193", "title": "A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT", "authors": ["Tanjin Taher Toma", "Tejas Sudharshan Mathai", "Bikash Santra", "Pritam Mukherjee", "Jianfei Liu", "Wesley Jong", "Darwish Alabyad", "Vivek Batheja", "Abhishek Jha", "Mayank Patel", "Darko Pucar", "Jayadira del Rivero", "Karel Pacak", "Ronald M. Summers"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15193v2", "summary": "Accurate segmentation of pheochromocytoma (PCC) in abdominal CT scans is\nessential for tumor burden estimation, prognosis, and treatment planning. It\nmay also help infer genetic clusters, reducing reliance on expensive testing.\nThis study systematically evaluates anatomical priors to identify\nconfigurations that improve deep learning-based PCC segmentation. We employed\nthe nnU-Net framework to evaluate eleven annotation strategies for accurate 3D\nsegmentation of pheochromocytoma, introducing a set of novel multi-class\nschemes based on organ-specific anatomical priors. These priors were derived\nfrom adjacent organs commonly surrounding adrenal tumors (e.g., liver, spleen,\nkidney, aorta, adrenal gland, and pancreas), and were compared against a broad\nbody-region prior used in previous work. The framework was trained and tested\non 105 contrast-enhanced CT scans from 91 patients at the NIH Clinical Center.\nPerformance was measured using Dice Similarity Coefficient (DSC), Normalized\nSurface Distance (NSD), and instance-wise F1 score. Among all strategies, the\nTumor + Kidney + Aorta (TKA) annotation achieved the highest segmentation\naccuracy, significantly outperforming the previously used Tumor + Body (TB)\nannotation across DSC (p = 0.0097), NSD (p = 0.0110), and F1 score (25.84%\nimprovement at an IoU threshold of 0.5), measured on a 70-30 train-test split.\nThe TKA model also showed superior tumor burden quantification (R^2 = 0.968)\nand strong segmentation across all genetic subtypes. In five-fold\ncross-validation, TKA consistently outperformed TB across IoU thresholds (0.1\nto 0.5), reinforcing its robustness and generalizability. These findings\nhighlight the value of incorporating relevant anatomical context into deep\nlearning models to achieve precise PCC segmentation, offering a valuable tool\nto support clinical assessment and longitudinal disease monitoring in PCC\npatients.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15193v2", "cate": "eess.IV", "date": "2025-07-21", "updated": "2025-07-24", "AI": {"title_translation": "腹部CT中基于深度学习的嗜铬细胞瘤分割的解剖先验研究", "tldr": "本研究系统评估了在腹部CT中基于深度学习分割嗜铬细胞瘤时，整合器官特异性解剖先验（特别是肿瘤+肾脏+主动脉 TKA）能显著提高分割精度和肿瘤负荷量化，优于传统方法。", "motivation": "嗜铬细胞瘤（PCC）在腹部CT中的准确分割对于肿瘤负荷评估、预后和治疗计划至关重要，并有助于推断遗传聚类，减少对昂贵检测的依赖。", "method": "本研究采用nnU-Net框架，系统评估了11种基于器官特异性解剖先验（如肾脏、主动脉等）的新型多类别标注策略，以改善深度学习在腹部CT中嗜铬细胞瘤（PCC）的3D分割。这些策略与传统的身体区域先验进行比较。模型在NIH临床中心的105例增强CT扫描数据上进行训练和测试，性能通过Dice相似系数（DSC）、归一化表面距离（NSD）和实例F1分数进行衡量。", "result": "在所有策略中，肿瘤+肾脏+主动脉（TKA）标注实现了最高的分割精度，在DSC、NSD和F1分数（在IoU阈值0.5时提高25.84%）方面显著优于传统的肿瘤+身体（TB）标注。TKA模型还显示出卓越的肿瘤负荷量化能力（R^2 = 0.968），并在所有遗传亚型中表现出强大的分割能力。在五折交叉验证中，TKA在所有IoU阈值下均持续优于TB，证实了其鲁棒性和泛化能力。", "conclusion": "研究结果强调了将相关解剖背景信息整合到深度学习模型中以实现精确嗜铬细胞瘤（PCC）分割的价值，为PCC患者的临床评估和疾病纵向监测提供了一个有价值的工具。", "translation": "腹部CT扫描中嗜铬细胞瘤（PCC）的准确分割对于肿瘤负荷估计、预后和治疗计划至关重要。它还有助于推断遗传聚类，减少对昂贵检测的依赖。本研究系统评估了解剖先验，以确定能够改善基于深度学习的PCC分割的配置。我们采用nnU-Net框架评估了11种标注策略，用于嗜铬细胞瘤的精确3D分割，引入了一系列基于器官特异性解剖先验的新型多类别方案。这些先验来自肾上腺肿瘤周围常见的相邻器官（例如肝脏、脾脏、肾脏、主动脉、肾上腺和胰腺），并与先前工作中使用的广泛身体区域先验进行了比较。该框架在NIH临床中心的91名患者的105次增强CT扫描数据上进行了训练和测试。性能通过Dice相似系数（DSC）、归一化表面距离（NSD）和实例F1分数进行衡量。在所有策略中，肿瘤+肾脏+主动脉（TKA）标注实现了最高的分割精度，在DSC（p = 0.0097）、NSD（p = 0.0110）和F1分数（在IoU阈值0.5时提高25.84%）方面显著优于先前使用的肿瘤+身体（TB）标注（在70-30的训练-测试分割上测量）。TKA模型还显示出卓越的肿瘤负荷量化能力（R^2 = 0.968），并在所有遗传亚型中表现出强大的分割能力。在五折交叉验证中，TKA在所有IoU阈值（0.1至0.5）下始终优于TB，证实了其鲁棒性和泛化能力。这些发现强调了将相关解剖背景信息整合到深度学习模型中以实现精确PCC分割的价值，为PCC患者的临床评估和疾病纵向监测提供了一个有价值的工具。", "summary": "本研究旨在提高腹部CT中嗜铬细胞瘤（PCC）的深度学习分割精度。通过系统评估nnU-Net框架下的11种标注策略，并引入基于器官特异性解剖先验（如肾脏、主动脉）的新型多类别方案，研究发现肿瘤+肾脏+主动脉（TKA）标注策略在分割精度、肿瘤负荷量化和对不同遗传亚型的泛化能力方面均显著优于传统的肿瘤+身体（TB）标注。结果表明，整合相关的解剖上下文信息对于实现精确的PCC分割至关重要，为临床评估和疾病监测提供了有效工具。", "keywords": "嗜铬细胞瘤, 深度学习, 分割, 解剖先验, CT", "comments": "本研究的创新点在于系统地评估和引入了器官特异性解剖先验，以提升深度学习模型在医学图像分割中的性能。特别地，发现“肿瘤+肾脏+主动脉”（TKA）这一新的解剖先验组合能显著提高嗜铬细胞瘤的分割精度和肿瘤负荷量化，这为未来的医学图像分割研究提供了新的方向，强调了利用特定解剖上下文信息的重要性。其结果对于嗜铬细胞瘤的临床诊断和治疗规划具有重要意义。"}}
{"id": "2502.20719", "title": "Generating Clinically Realistic EHR Data via a Hierarchy- and Semantics-Guided Transformer", "authors": ["Guanglin Zhou", "Sebastiano Barbieri"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      The camera ready version for ECAI-2025", "url": "http://arxiv.org/abs/2502.20719v2", "summary": "Generating realistic synthetic electronic health records (EHRs) holds\ntremendous promise for accelerating healthcare research, facilitating AI model\ndevelopment and enhancing patient privacy. However, existing generative methods\ntypically treat EHRs as flat sequences of discrete medical codes. This approach\noverlooks two critical aspects: the inherent hierarchical organization of\nclinical coding systems and the rich semantic context provided by code\ndescriptions. Consequently, synthetic patient sequences often lack high\nclinical fidelity and have limited utility in downstream clinical tasks. In\nthis paper, we propose the Hierarchy- and Semantics-Guided Transformer (HiSGT),\na novel framework that leverages both hierarchical and semantic information for\nthe generative process. HiSGT constructs a hierarchical graph to encode\nparent-child and sibling relationships among clinical codes and employs a graph\nneural network to derive hierarchy-aware embeddings. These are then fused with\nsemantic embeddings extracted from a pre-trained clinical language model (e.g.,\nClinicalBERT), enabling the Transformer-based generator to more accurately\nmodel the nuanced clinical patterns inherent in real EHRs. Extensive\nexperiments on the MIMIC-III and MIMIC-IV datasets demonstrate that HiSGT\nsignificantly improves the statistical alignment of synthetic data with real\npatient records, as well as supports robust downstream applications such as\nchronic disease classification. By addressing the limitations of conventional\nraw code-based generative models, HiSGT represents a significant step toward\nclinically high-fidelity synthetic data generation and a general paradigm\nsuitable for interpretable medical code representation, offering valuable\napplications in data augmentation and privacy-preserving healthcare analytics.", "comment": "The camera ready version for ECAI-2025", "pdf_url": "http://arxiv.org/pdf/2502.20719v2", "cate": "cs.LG", "date": "2025-02-28", "updated": "2025-07-25", "AI": {"title_translation": "通过层级和语义引导的Transformer生成临床真实EHR数据", "tldr": "HiSGT是一种新的框架，通过整合临床编码系统的层级和语义信息来生成高临床保真度的合成电子健康记录（EHR）数据，并在统计对齐和下游任务中表现优异。", "motivation": "生成真实的合成电子健康记录（EHR）对于加速医疗保健研究、促进AI模型开发和增强患者隐私具有巨大潜力。然而，现有生成方法通常将EHR视为离散医疗代码的扁平序列，忽略了临床编码系统的内在层级组织和代码描述提供的丰富语义上下文，导致合成患者序列缺乏高临床保真度且在下游临床任务中效用有限。", "method": "本文提出了层级和语义引导的Transformer（HiSGT）框架。HiSGT构建一个层级图来编码临床代码之间的父子和兄弟关系，并利用图神经网络导出层级感知的嵌入。这些嵌入与从预训练临床语言模型（如ClinicalBERT）中提取的语义嵌入融合，使基于Transformer的生成器能够更准确地建模真实EHR中固有的细微临床模式。", "result": "在MIMIC-III和MIMIC-IV数据集上的大量实验表明，HiSGT显著提高了合成数据与真实患者记录的统计对齐，并支持强大的下游应用，例如慢性病分类。", "conclusion": "通过解决传统基于原始代码的生成模型的局限性，HiSGT代表了实现临床高保真合成数据生成的重要一步，并且是一种适用于可解释医疗代码表示的通用范式，在数据增强和隐私保护医疗保健分析中提供了有价值的应用。", "translation": "生成真实的合成电子健康记录（EHR）对于加速医疗保健研究、促进AI模型开发和增强患者隐私具有巨大潜力。然而，现有生成方法通常将EHR视为离散医疗代码的扁平序列。这种方法忽略了两个关键方面：临床编码系统固有的层级组织和代码描述提供的丰富语义上下文。因此，合成患者序列往往缺乏高临床保真度，并且在下游临床任务中效用有限。在本文中，我们提出了层级和语义引导的Transformer（HiSGT），这是一个新颖的框架，它在生成过程中利用了层级和语义信息。HiSGT构建一个层级图来编码临床代码之间的父子和兄弟关系，并采用图神经网络来导出层级感知的嵌入。然后将这些嵌入与从预训练临床语言模型（例如ClinicalBERT）中提取的语义嵌入融合，从而使基于Transformer的生成器能够更准确地建模真实EHR中固有的细微临床模式。在MIMIC-III和MIMIC-IV数据集上的大量实验表明，HiSGT显著提高了合成数据与真实患者记录的统计对齐，并支持强大的下游应用，例如慢性病分类。通过解决传统基于原始代码的生成模型的局限性，HiSGT代表了实现临床高保真合成数据生成的重要一步，并且是一种适用于可解释医疗代码表示的通用范式，在数据增强和隐私保护医疗保健分析中提供了有价值的应用。", "summary": "本文提出了一种名为层级和语义引导的Transformer（HiSGT）的新型框架，旨在生成高临床保真度的合成电子健康记录（EHR）数据。针对现有方法忽视临床编码系统层级结构和语义上下文的问题，HiSGT通过构建层级图和利用图神经网络获取层级感知嵌入，并将其与来自预训练临床语言模型的语义嵌入融合，从而使Transformer模型能更准确地捕捉真实EHR中的复杂临床模式。在MIMIC-III和MIMIC-IV数据集上的实验证明，HiSGT显著提升了合成数据的统计对齐度，并有效支持了如慢性病分类等下游临床应用，为数据增强和隐私保护医疗分析提供了重要价值。", "keywords": "合成EHR数据, Transformer, 层级信息, 语义信息, 医疗保健AI", "comments": "该论文的创新之处在于其HiSGT框架，它通过有效地整合临床编码系统的层级和语义信息来克服现有EHR合成方法的主要限制。这种方法显著提高了生成数据的临床保真度，使其在实际医疗应用中更具实用性。其重要性体现在为隐私保护的医疗数据共享和AI模型开发提供了更可靠的合成数据源。"}}
{"id": "2502.17382", "title": "Unraveling the geometry of visual relational reasoning", "authors": ["Jiaqi Shang", "Gabriel Kreiman", "Haim Sompolinsky"], "categories": ["q-bio.NC", "cs.CV"], "primary_category": "Subjects:       Neurons and Cognition (q-bio.NC)", "pdf_link": null, "comments": "Comments:      27 pages, 7 figures, 8 SI figures, 2 SI tables", "url": "http://arxiv.org/abs/2502.17382v2", "summary": "Humans readily generalize abstract relations, such as recognizing \"constant\"\nin shape or color, whereas neural networks struggle, limiting their flexible\nreasoning. To investigate mechanisms underlying such generalization, we\nintroduce SimplifiedRPM, a novel benchmark for systematically evaluating\nabstract relational reasoning, addressing limitations in prior datasets. In\nparallel, we conduct human experiments to quantify relational difficulty,\nenabling direct model-human comparisons. Testing four models, ResNet-50, Vision\nTransformer, Wild Relation Network, and Scattering Compositional Learner (SCL),\nwe find that SCL generalizes best and most closely aligns with human behavior.\nUsing a geometric approach, we identify key representation properties that\naccurately predict generalization and uncover a fundamental trade-off between\nsignal and dimensionality: novel relations compress into training-induced\nsubspaces. Layer-wise analysis reveals where relational structure emerges,\nhighlights bottlenecks, and generates concrete hypotheses about abstract\nreasoning in the brain. Motivated by these insights, we propose SNRloss, a\nnovel objective explicitly balancing representation geometry. Our results\nestablish a geometric foundation for relational reasoning, paving the way for\nmore human-like visual reasoning in AI and opening promising avenues for\nextending geometric analysis to broader cognitive tasks.", "comment": "27 pages, 7 figures, 8 SI figures, 2 SI tables", "pdf_url": "http://arxiv.org/pdf/2502.17382v2", "cate": "q-bio.NC", "date": "2025-02-24", "updated": "2025-07-25", "AI": {"title_translation": "揭示视觉关系推理的几何学", "tldr": "人类擅长泛化抽象关系，但神经网络在此方面表现不佳。本文引入了SimplifiedRPM新基准，进行了人类实验，发现SCL模型表现最佳且与人类行为最接近。研究采用几何方法揭示了关系推理的关键表征属性和信号与维度间的权衡，并提出了SNRloss以优化表征几何，为AI中类人视觉推理奠定了几何基础。", "motivation": "人类能够轻易地泛化抽象关系，例如识别形状或颜色中的“恒定性”，而神经网络则难以做到，这限制了它们的灵活推理能力。本文旨在调查这种泛化背后的机制，并解决先前数据集的局限性。", "method": "本文引入了新型基准SimplifiedRPM，用于系统评估抽象关系推理。同时，进行了人类实验以量化关系难度，实现模型与人类的直接比较。测试了ResNet-50、Vision Transformer、Wild Relation Network和Scattering Compositional Learner (SCL) 四种模型。研究采用几何方法识别关键表征属性，并进行了逐层分析。受这些见解启发，提出了SNRloss这一新型目标函数。", "result": "Scattering Compositional Learner (SCL) 模型的泛化能力最佳，且与人类行为最为接近。研究识别出能够准确预测泛化能力的关键表征属性，并发现信号与维度之间存在一个基本权衡：新颖的关系被压缩到训练诱导的子空间中。逐层分析揭示了关系结构出现的位置并突出了瓶颈。", "conclusion": "本文结果为关系推理奠定了几何基础，为人工智能中更像人类的视觉推理铺平了道路，并为将几何分析扩展到更广泛的认知任务开辟了有希望的途径。", "translation": "人类能够轻易地泛化抽象关系，例如识别形状或颜色中的“恒定性”，而神经网络则难以做到，这限制了它们的灵活推理能力。为了研究这种泛化背后的机制，我们引入了SimplifiedRPM，这是一个用于系统评估抽象关系推理的新型基准，解决了先前数据集的局限性。同时，我们进行了人类实验来量化关系难度，从而实现模型与人类的直接比较。我们测试了四种模型：ResNet-50、Vision Transformer、Wild Relation Network和Scattering Compositional Learner (SCL)，发现SCL的泛化能力最佳，并且与人类行为最为接近。我们采用几何方法，识别出能够准确预测泛化能力的关键表征属性，并揭示了信号与维度之间的一个基本权衡：新颖的关系被压缩到训练诱导的子空间中。逐层分析揭示了关系结构出现的位置，突出了瓶颈，并对大脑中的抽象推理产生了具体的假设。受这些见解的启发，我们提出了SNRloss，这是一种明确平衡表征几何的新型目标函数。我们的结果为关系推理奠定了几何基础，为人工智能中更像人类的视觉推理铺平了道路，并为将几何分析扩展到更广泛的认知任务开辟了有希望的途径。", "summary": "本文旨在解决神经网络在抽象关系泛化方面的不足，而人类在这方面表现出色。为此，研究引入了新型基准SimplifiedRPM，并结合人类实验进行直接比较。通过测试多种模型，发现SCL模型泛化能力最佳且与人类行为最接近。研究采用几何方法分析表征属性，揭示了信号与维度之间的权衡，并指出新颖关系被压缩到学习到的子空间中。此外，论文还提出了SNRloss以优化表征几何，最终为关系推理奠定了几何基础，以促进人工智能中更像人类的视觉推理。", "keywords": "视觉关系推理, 几何学, 神经网络, 泛化, SimplifiedRPM", "comments": "本文的创新之处在于引入了新基准SimplifiedRPM并结合人类实验进行直接比较，这对于理解类人推理至关重要。采用几何方法分析表征属性以及提出的SNRloss是重要的贡献，为改进AI中的关系推理提供了新颖视角，超越了单纯的架构或数据驱动解决方案。信号与维度权衡的发现为当前模型的局限性提供了深刻见解。"}}
{"id": "2507.18788", "title": "Tell Me What You See: An Iterative Deep Learning Framework for Image Captioning", "authors": ["Hitesh Kumar Gupta"], "categories": ["cs.CV", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      16 pages, 12 total figures (including a 7-figure appendix), 4 tables", "url": "http://arxiv.org/abs/2507.18788v1", "summary": "Image captioning, a task at the confluence of computer vision and natural\nlanguage processing, requires a sophisticated understanding of both visual\nscenes and linguistic structure. While modern approaches are dominated by\nlarge-scale Transformer architectures, this paper documents a systematic,\niterative development of foundational image captioning models, progressing from\na simple CNN-LSTM encoder-decoder to a competitive attention-based system. We\npresent a series of five models, beginning with Genesis and concluding with\nNexus, an advanced model featuring an EfficientNetV2B3 backbone and a dynamic\nattention mechanism. Our experiments chart the impact of architectural\nenhancements and demonstrate a key finding within the classic CNN-LSTM\nparadigm: merely upgrading the visual backbone without a corresponding\nattention mechanism can degrade performance, as the single-vector bottleneck\ncannot transmit the richer visual detail. This insight validates the\narchitectural shift to attention. Trained on the MS COCO 2017 dataset, our\nfinal model, Nexus, achieves a BLEU-4 score of 31.4, surpassing several\nfoundational benchmarks and validating our iterative design process. This work\nprovides a clear, replicable blueprint for understanding the core architectural\nprinciples that underpin modern vision-language tasks.", "comment": "16 pages, 12 total figures (including a 7-figure appendix), 4 tables", "pdf_url": "http://arxiv.org/pdf/2507.18788v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "告诉我你看到了什么：一个迭代深度学习图像字幕框架", "tldr": "本文通过迭代开发CNN-LSTM基础模型，最终提出了一个基于注意力的图像字幕系统（Nexus），该系统实现了具有竞争力的性能，并揭示了注意力机制在处理更强视觉骨干网络时的关键作用。", "motivation": "旨在系统地开发基础图像字幕模型，并理解核心架构原理，特别是架构增强（如注意力机制）的影响，因为在缺乏相应注意力机制的情况下，单纯升级视觉骨干网络可能会导致性能下降。", "method": "本文采用迭代开发过程，构建了一系列五个图像字幕模型，从基本的CNN-LSTM编码器-解码器开始，逐步发展到一个先进的基于注意力的系统Nexus。Nexus模型结合了EfficientNetV2B3骨干网络和动态注意力机制。通过实验评估了架构增强的影响，并在MS COCO 2017数据集上对模型进行了训练。", "result": "一个关键发现是，在经典的CNN-LSTM范式中，仅仅升级视觉骨干网络而没有相应的注意力机制可能会由于单向量瓶颈而导致性能下降。最终模型Nexus在MS COCO 2017数据集上达到了31.4的BLEU-4分数，超越了几个基础基准。", "conclusion": "这项工作验证了图像字幕中向注意力机制的架构转变，并为理解支撑现代视觉-语言任务的核心架构原理提供了清晰、可复用的蓝图。迭代设计过程被证明是有效的。", "translation": "图像字幕，作为计算机视觉和自然语言处理交叉领域的一项任务，需要对视觉场景和语言结构都有复杂的理解。虽然现代方法主要由大规模Transformer架构主导，但本文记录了基础图像字幕模型的系统性、迭代式开发，从简单的CNN-LSTM编码器-解码器逐步发展到一个有竞争力的基于注意力的系统。我们提出了一系列五个模型，从Genesis开始，到Nexus结束，Nexus是一个具有EfficientNetV2B3骨干网络和动态注意力机制的先进模型。我们的实验描绘了架构增强的影响，并在经典的CNN-LSTM范式中发现了一个关键结论：仅仅升级视觉骨干网络而没有相应的注意力机制可能会降低性能，因为单向量瓶颈无法传输更丰富的视觉细节。这一见解验证了向注意力机制的架构转变。我们的最终模型Nexus在MS COCO 2017数据集上进行训练，达到了31.4的BLEU-4分数，超越了几个基础基准，验证了我们的迭代设计过程。这项工作为理解支撑现代视觉-语言任务的核心架构原理提供了清晰、可复用的蓝图。", "summary": "本文提出了一个用于图像字幕的迭代深度学习框架，系统地开发了从CNN-LSTM编码器-解码器到名为Nexus的先进注意力机制基础模型。通过一系列五个模型，作者研究了架构增强的影响。一个关键发现是，仅仅升级视觉骨干网络而没有注意力机制可能会因信息瓶颈而降低性能。最终模型Nexus，利用EfficientNetV2B3骨干网络和动态注意力，在MS COCO 2017数据集上实现了31.4的BLEU-4分数，超越了多个基准。这项工作验证了注意力在传输丰富视觉细节方面的重要性，并为理解核心视觉-语言架构原理提供了可复用的蓝图。", "keywords": "图像字幕, 深度学习, CNN-LSTM, 注意力机制, 迭代开发", "comments": "本文通过系统性地探索基础图像字幕模型的开发，尤其是在Transformer广泛应用之前CNN-LSTM范式内的研究，做出了宝贵贡献。其迭代方法清晰地揭示了架构选择的影响，特别是强调了当视觉骨干网络变得更强大时，注意力机制的关键作用。关于单向量瓶颈的发现对于理解注意力在传输更丰富视觉细节方面的必要性至关重要，使其成为未来研究的“可复用蓝图”。"}}
{"id": "2507.19168", "title": "Explainable AI guided unsupervised fault diagnostics for high-voltage circuit breakers", "authors": ["Chi-Ching Hsu", "Gaëtan Frusque", "Florent Forest", "Felipe Macedo", "Christian M. Franck", "Olga Fink"], "categories": ["cs.LG", "eess.SP"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19168v1", "summary": "Commercial high-voltage circuit breaker (CB) condition monitoring systems\nrely on directly observable physical parameters such as gas filling pressure\nwith pre-defined thresholds. While these parameters are crucial, they only\ncover a small subset of malfunctioning mechanisms and usually can be monitored\nonly if the CB is disconnected from the grid. To facilitate online condition\nmonitoring while CBs remain connected, non-intrusive measurement techniques\nsuch as vibration or acoustic signals are necessary. Currently, CB condition\nmonitoring studies using these signals typically utilize supervised methods for\nfault diagnostics, where ground-truth fault types are known due to artificially\nintroduced faults in laboratory settings. This supervised approach is however\nnot feasible in real-world applications, where fault labels are unavailable. In\nthis work, we propose a novel unsupervised fault detection and segmentation\nframework for CBs based on vibration and acoustic signals. This framework can\ndetect deviations from the healthy state. The explainable artificial\nintelligence (XAI) approach is applied to the detected faults for fault\ndiagnostics. The specific contributions are: (1) we propose an integrated\nunsupervised fault detection and segmentation framework that is capable of\ndetecting faults and clustering different faults with only healthy data\nrequired during training (2) we provide an unsupervised explainability-guided\nfault diagnostics approach using XAI to offer domain experts potential\nindications of the aged or faulty components, achieving fault diagnostics\nwithout the prerequisite of ground-truth fault labels. These contributions are\nvalidated using an experimental dataset from a high-voltage CB under healthy\nand artificially introduced fault conditions, contributing to more reliable CB\nsystem operation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19168v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "高压断路器可解释人工智能引导的无监督故障诊断", "tldr": "本文提出了一种基于振动和声学信号的无监督故障检测和分割框架，并结合可解释人工智能，为高压断路器提供无需故障标签的在线故障诊断。", "motivation": "现有的高压断路器状态监测系统依赖可观测的物理参数，覆盖的故障机制有限，且通常需要在断路器断开电网时进行。使用非侵入式测量信号的监督方法在实际应用中因缺乏故障标签而不可行。", "method": "本文提出了一种新颖的无监督故障检测和分割框架，利用振动和声学信号，仅需健康数据进行训练即可检测并聚类不同故障。此外，应用可解释人工智能（XAI）对检测到的故障进行诊断，以提供潜在老化或故障部件的指示。", "result": "所提出的贡献已通过高压断路器在健康和人工引入故障条件下的实验数据集进行了验证。", "conclusion": "本文提出的框架实现了无需先验故障标签的高压断路器在线故障诊断，有助于提高断路器系统运行的可靠性。", "translation": "商业高压断路器（CB）状态监测系统依赖于气体填充压力等可直接观测的物理参数，并设有预定义阈值。尽管这些参数至关重要，但它们仅涵盖一小部分故障机制，并且通常只有在断路器与电网断开时才能进行监测。为了在断路器保持连接状态下进行在线状态监测，非侵入式测量技术，如振动或声学信号，是必不可少的。目前，利用这些信号进行断路器状态监测的研究通常采用监督方法进行故障诊断，其中由于实验室环境中人工引入的故障，地面真实故障类型是已知的。然而，这种监督方法在实际应用中是不可行的，因为故障标签是不可用的。在这项工作中，我们提出了一种基于振动和声学信号的新型高压断路器无监督故障检测和分割框架。该框架可以检测与健康状态的偏差。可解释人工智能（XAI）方法应用于检测到的故障进行故障诊断。具体贡献包括：（1）我们提出了一种集成的无监督故障检测和分割框架，能够检测故障并聚类不同故障，训练时仅需要健康数据；（2）我们提供了一种使用XAI的无监督可解释性引导故障诊断方法，为领域专家提供老化或故障部件的潜在指示，从而在无需地面真实故障标签的前提下实现故障诊断。这些贡献通过高压断路器在健康和人工引入故障条件下的实验数据集进行了验证，有助于更可靠的断路器系统运行。", "summary": "本文针对高压断路器（CB）在线状态监测中现有方法的局限性，提出了一种新颖的无监督故障检测与分割框架。该框架利用振动和声学信号，仅需健康数据即可训练，能够检测异常并聚类不同故障。结合可解释人工智能（XAI），该方法无需预先的故障标签即可为专家提供潜在故障部件的指示。实验数据验证了其有效性，有助于提高CB系统运行的可靠性。", "keywords": "高压断路器, 无监督故障诊断, 可解释人工智能, 状态监测", "comments": "该研究的创新之处在于提出了一种无监督的故障诊断方法，解决了实际应用中缺乏故障标签的挑战。同时，结合可解释人工智能（XAI）为领域专家提供了可解释的故障指示，这对于工业场景的实际部署至关重要，增强了系统的实用性和可靠性。"}}
{"id": "2507.19369", "title": "Binaural Target Speaker Extraction using HRTFs and a Complex-Valued Neural Network", "authors": ["Yoav Ellinson", "Sharon Gannot"], "categories": ["eess.AS", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19369v1", "summary": "In this work, we aim to imitate the human ability to selectively attend to a\nsingle speaker, even in the presence of multiple simultaneous talkers. We\npropose a novel approach for binaural target speaker extraction that leverages\nthe listener's Head-Related Transfer Function (HRTF) to isolate the desired\nspeaker. Notably, our method does not rely on speaker embeddings, making it\nspeaker-independent and enabling strong generalization across multiple speech\ndatasets in different languages.\n  We employ a fully complex-valued neural network that operates directly on the\ncomplex-valued Short-Time Fourier Transform (STFT) of the mixed audio signals.\nThis deviates from conventional approaches that use spectrograms or treat the\nreal and imaginary components of the STFT as separate real-valued inputs.\n  We first evaluate the method in an anechoic, noise-free scenario, where it\ndemonstrates excellent extraction performance while effectively preserving the\nbinaural cues of the target signal. We then test a modified variant under mild\nreverberation conditions. This version remains robust in reverberant\nenvironments, maintaining speech clarity, preserving source directionality, and\nsimultaneously reducing reverberation.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19369v1", "cate": "eess.AS", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于HRTF和复值神经网络的双耳目标说话人提取", "tldr": "本文提出了一种利用HRTF和复值神经网络进行双耳目标说话人提取的新方法，该方法无需说话人嵌入，具有良好的泛化能力，并在无混响和轻度混响条件下表现出色。", "motivation": "本文旨在模拟人类在多说话人同时存在的情况下选择性地关注单个说话人的能力。", "method": "本文提出了一种新颖的双耳目标说话人提取方法，该方法利用听者的头部相关传输函数（HRTF）来隔离目标说话人，并且不依赖于说话人嵌入，使其具有说话人独立性和强大的泛化能力。该方法采用一个完全复值神经网络，直接作用于混合音频信号的复值短时傅里er变换（STFT）。", "result": "在消声、无噪声的场景中，该方法展示了出色的提取性能，同时有效地保留了目标信号的双耳线索。在轻度混响条件下，修改后的版本依然稳健，保持了语音清晰度，保留了声源方向性，并同时减少了混响。", "conclusion": "本文提出的方法能够有效地在多说话人环境中提取目标说话人，该方法无需说话人嵌入，具有良好的泛化能力，并且在有混响的环境中也能保持稳健性，同时保留了重要的双耳和方向性线索。", "translation": "在这项工作中，我们旨在模仿人类即使在多个说话人同时存在的情况下也能选择性地关注单个说话人的能力。我们提出了一种新颖的双耳目标说话人提取方法，该方法利用听者的头部相关传输函数（HRTF）来隔离所需说话人。值得注意的是，我们的方法不依赖于说话人嵌入，使其具有说话人独立性，并能在不同语言的多个语音数据集中实现强大的泛化能力。\n我们采用一个完全复值神经网络，它直接作用于混合音频信号的复值短时傅里叶变换（STFT）。这与使用语谱图或将STFT的实部和虚部作为单独的实值输入处理的传统方法有所不同。\n我们首先在消声、无噪声的场景中评估该方法，它展示了出色的提取性能，同时有效地保留了目标信号的双耳线索。然后，我们在轻度混响条件下测试了一个修改后的变体。此版本在混响环境中保持稳健，保持语音清晰度，保留声源方向性，并同时减少混响。", "summary": "本文提出了一种基于HRTF和复值神经网络的双耳目标说话人提取新方法，旨在模拟人类在复杂听觉环境中选择性地关注特定说话人的能力。该方法不依赖说话人嵌入，实现了说话人独立性和跨语言泛化。其核心在于一个直接处理复值STFT的复值神经网络。实验结果表明，该方法在无噪声和无混响环境下能有效提取目标说话人并保留双耳线索，在轻度混响条件下也表现出良好的鲁棒性，能保持语音清晰度、源方向性并减少混响。", "keywords": "双耳, 说话人提取, HRTF, 复值神经网络, 混响", "comments": "本文的创新点在于提出了一个不依赖说话人嵌入、仅利用HRTF和复值神经网络的双耳目标说话人提取方法。直接在复值STFT域操作的复值神经网络是其核心技术，这与传统方法不同。该方法在保留双耳线索和处理轻度混响方面的表现，显示了其在实际应用中的潜力。其说话人独立性和跨语言泛化能力也极大地提升了其实用价值。"}}
{"id": "2502.10112", "title": "Accelerometry-based Energy Expenditure Estimation During Activities of Daily Living: A Comparison Among Different Accelerometer Compositions", "authors": ["Shuhao Que", "Remco Poelarends", "Peter Veltink", "Miriam Vollenbroek-Hutten", "Ying Wang"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      This work has been accepted by IEEE EMBC 2025", "url": "http://arxiv.org/abs/2502.10112v2", "summary": "Physical activity energy expenditure (PAEE) can be measured from\nbreath-by-breath respiratory data, which can serve as a reference.\nAlternatively, PAEE can be predicted from the body movements, which can be\nmeasured and estimated with accelerometers. The body center of mass (COM)\nacceleration reflects the movements of the whole body and thus serves as a good\npredictor for PAEE. However, the wrist has also become a popular location due\nto recent advancements in wrist-worn devices. Therefore, in this work, using\nthe respiratory data measured by COSMED K5 as the reference, we evaluated and\ncompared the performances of COM-based settings and wrist-based settings. The\nCOM-based settings include two different accelerometer compositions, using only\nthe pelvis accelerometer (pelvis-acc) and the pelvis accelerometer with two\naccelerometers from two thighs (3-acc). The wrist-based settings include using\nonly the left wrist accelerometer (l-wrist-acc) and only the right wrist\naccelerometer (r-wrist-acc). We implemented two existing PAEE estimation\nmethods on our collected dataset, where 9 participants performed activities of\ndaily living while wearing 5 accelerometers (i.e., pelvis, two thighs, and two\nwrists). These two methods include a linear regression (LR) model and a\nCNN-LSTM model. Both models yielded the best results with the COM-based 3-acc\nsetting (LR: $R^2$ = 0.41, CNN-LSTM: $R^2$ = 0.53). No significant difference\nwas found between the 3-acc and pelvis-acc settings (p-value = 0.278). For both\nmodels, neither the l-wrist-acc nor the r-wrist-acc settings demonstrated\npredictive power on PAEE with $R^2$ values close to 0, significantly\noutperformed by the two COM-based settings (p-values $<$ 0.05). No significant\ndifference was found between the two wrists (p-value = 0.329).", "comment": "This work has been accepted by IEEE EMBC 2025", "pdf_url": "http://arxiv.org/pdf/2502.10112v2", "cate": "cs.LG", "date": "2025-02-14", "updated": "2025-07-25", "AI": {"title_translation": "基于加速度计的日常生活能量消耗估计：不同加速度计组合的比较", "tldr": "在日常生活活动中，基于身体质心（COM）的加速度计设置（特别是3个加速度计组合）在物理活动能量消耗（PAEE）估计方面优于腕戴式设置，腕戴设备几乎没有预测能力。", "motivation": "物理活动能量消耗（PAEE）可以通过身体运动用加速度计进行预测。虽然身体质心（COM）加速度是PAEE的良好预测因子，但腕戴设备因其普及性也成为一个受欢迎的佩戴位置。本研究旨在评估和比较基于COM的设置和基于手腕的加速度计设置在PAEE估计方面的性能，并以呼吸数据作为参考。", "method": "本研究使用COSMED K5测量的呼吸数据作为参考，评估了基于身体质心（COM）的设置（仅骨盆加速度计，以及骨盆加两个大腿加速度计）和基于手腕的设置（仅左手腕加速度计，仅右手腕加速度计）的性能。9名参与者在进行日常生活活动时佩戴了5个加速度计（骨盆、两条大腿、两只手腕）。研究中实施了两种PAEE估计方法：线性回归（LR）模型和CNN-LSTM模型。", "result": "两种模型（LR和CNN-LSTM）在基于COM的3个加速度计设置下均取得了最佳结果（LR: R² = 0.41, CNN-LSTM: R² = 0.53）。3个加速度计设置与骨盆加速度计设置之间未发现显著差异（p值 = 0.278）。对于两种模型，左手腕或右手腕设置的R²值均接近0，未能显示出对PAEE的预测能力，并且被两种基于COM的设置显著超越（p值 < 0.05）。两只手腕之间未发现显著差异（p值 = 0.329）。", "conclusion": "在日常生活活动中，基于身体质心的加速度计组合，特别是使用骨盆和两个大腿的3个加速度计设置，在估计物理活动能量消耗方面优于腕戴式设置，腕戴设备显示出极低的预测能力。", "translation": "物理活动能量消耗（PAEE）可以通过逐次呼吸的呼吸数据测量，这可以作为参考。或者，PAEE可以通过身体运动来预测，身体运动可以用加速度计测量和估计。身体质心（COM）加速度反映了全身的运动，因此是PAEE的良好预测因子。然而，由于腕戴设备的最新进展，手腕也成为了一个受欢迎的位置。因此，在这项工作中，我们使用COSMED K5测量的呼吸数据作为参考，评估并比较了基于COM的设置和基于手腕的设置的性能。基于COM的设置包括两种不同的加速度计组合：仅使用骨盆加速度计（pelvis-acc）和骨盆加速度计与两个大腿加速度计（3-acc）。基于手腕的设置包括仅使用左手腕加速度计（l-wrist-acc）和仅使用右手腕加速度计（r-wrist-acc）。我们在收集的数据集上实施了两种现有的PAEE估计方法，其中9名参与者在进行日常生活活动时佩戴了5个加速度计（即骨盆、两条大腿和两只手腕）。这两种方法包括线性回归（LR）模型和CNN-LSTM模型。两种模型在基于COM的3-acc设置下都取得了最佳结果（LR：R² = 0.41，CNN-LSTM：R² = 0.53）。在3-acc和pelvis-acc设置之间未发现显著差异（p值 = 0.278）。对于这两种模型，无论是l-wrist-acc还是r-wrist-acc设置，其R²值都接近于0，未能显示出对PAEE的预测能力，并且被两种基于COM的设置显著超越（p值 < 0.05）。两只手腕之间未发现显著差异（p值 = 0.329）。", "summary": "本文比较了不同加速度计组合在日常生活活动中估计物理活动能量消耗（PAEE）的性能，并以呼吸数据作为参考。研究评估了基于身体质心（COM）的设置（仅骨盆，骨盆加两个大腿）与基于手腕的设置（左/右手腕）在使用线性回归和CNN-LSTM模型时的表现。结果表明，基于COM的设置，特别是使用骨盆和两个大腿加速度计的组合，显著优于腕戴式设置，后者对PAEE的预测能力可以忽略不计。", "keywords": "加速度测量, 能量消耗, 日常生活活动, 质心加速度, 腕戴设备", "comments": "本文为精确估计PAEE的加速度计最佳放置提供了有价值的见解，尤其强调了与基于COM的放置相比，流行的腕戴设备在此特定应用中的局限性。同时使用传统（LR）和深度学习（CNN-LSTM）模型增加了研究结果的可靠性。然而，9名参与者的样本量较小，可能会限制结果的普适性。"}}
{"id": "2507.18938", "title": "GMM-Based Time-Varying Coverage Control", "authors": ["Behzad Zamani", "James Kennedy", "Airlie Chapman", "Peter Dower", "Chris Manzie", "Simon Crase"], "categories": ["eess.SY", "cs.RO", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      Submitted to CDC 2025", "url": "http://arxiv.org/abs/2507.18938v1", "summary": "In coverage control problems that involve time-varying density functions, the\ncoverage control law depends on spatial integrals of the time evolution of the\ndensity function. The latter is often neglected, replaced with an upper bound\nor calculated as a numerical approximation of the spatial integrals involved.\nIn this paper, we consider a special case of time-varying density functions\nmodeled as Gaussian Mixture Models (GMMs) that evolve with time via a set of\ntime-varying sources (with known corresponding velocities). By imposing this\nstructure, we obtain an efficient time-varying coverage controller that fully\nincorporates the time evolution of the density function. We show that the\ninduced trajectories under our control law minimise the overall coverage cost.\nWe elicit the structure of the proposed controller and compare it with a\nclassical time-varying coverage controller, against which we benchmark the\ncoverage performance in simulation. Furthermore, we highlight that the\ncomputationally efficient and distributed nature of the proposed control law\nmakes it ideal for multi-vehicle robotic applications involving time-varying\ncoverage control problems. We employ our method in plume monitoring using a\nswarm of drones. In an experimental field trial we show that drones guided by\nthe proposed controller are able to track a simulated time-varying chemical\nplume in a distributed manner.", "comment": "Submitted to CDC 2025", "pdf_url": "http://arxiv.org/pdf/2507.18938v1", "cate": "eess.SY", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于GMM的时变覆盖控制", "tldr": "本文提出了一种基于高斯混合模型（GMM）的时变覆盖控制器，该控制器能有效整合密度函数的时间演变，并最小化覆盖成本，适用于多车机器人应用，并通过无人机群体的烟羽监测实验验证了其分布式跟踪能力。", "motivation": "在涉及时变密度函数的覆盖控制问题中，覆盖控制律依赖于密度函数时间演变的空域积分。然而，这一时间演变部分常常被忽略、替换为上限或通过数值近似计算，导致现有方法未能充分整合其动态性。", "method": "本文将时变密度函数建模为高斯混合模型（GMM），其通过一组具有已知相应速度的时变源随时间演变。通过施加这种结构，我们获得了一种高效的时变覆盖控制器，该控制器能够完全整合密度函数的时间演变。", "result": "研究表明，在我们的控制律下，所诱导的轨迹能够最小化总体覆盖成本。所提出的控制律具有计算效率高和分布式特性，非常适合涉及时变覆盖控制问题的多车机器人应用。在无人机群体的烟羽监测应用中，实验证明由该控制器引导的无人机能够以分布式方式跟踪模拟的时变化学烟羽。", "conclusion": "本文提出了一种基于GMM的时变覆盖控制器，它能有效且完整地整合密度函数的时间演变，最小化覆盖成本，并表现出高计算效率和分布式特性，使其成为多车机器人应用中时变覆盖控制的理想选择，并通过实际应用进行了验证。", "translation": "在涉及时变密度函数的覆盖控制问题中，覆盖控制律依赖于密度函数时间演变的空域积分。后者常常被忽略，被一个上限取代，或者作为所涉及空域积分的数值近似来计算。在本文中，我们考虑了时变密度函数的一种特殊情况，其被建模为高斯混合模型（GMM），并通过一组时变源（具有已知相应的速度）随时间演变。通过施加这种结构，我们获得了一个高效的时变覆盖控制器，该控制器完全整合了密度函数的时间演变。我们表明，在我们的控制律下，所诱导的轨迹能够最小化总体覆盖成本。我们阐明了所提出的控制器的结构，并将其与经典的although时变覆盖控制器进行了比较，我们以此作为模拟中覆盖性能的基准。此外，我们强调所提出的控制律的计算效率高和分布式特性使其非常适合涉及时变覆盖控制问题的多车机器人应用。我们将我们的方法应用于无人机群体的烟羽监测。在一次实验现场试验中，我们展示了由所提出的控制器引导的无人机能够以分布式方式跟踪模拟的时变化学烟羽。", "summary": "本文针对时变密度函数覆盖控制中忽视密度函数时间演变的问题，提出了一种基于高斯混合模型（GMM）的时变覆盖控制器。该控制器通过将密度函数建模为GMM并考虑其时变源，能够有效且完整地整合密度函数的时间演变，从而最小化总体覆盖成本。研究表明，该控制器计算高效且具有分布式特性，非常适用于多车机器人系统。通过无人机群体在模拟化学烟羽监测中的应用，验证了其分布式跟踪能力和实际效果。", "keywords": "GMM, 时变覆盖控制, 多机器人系统, 烟羽监测, 分布式控制", "comments": "本文的创新之处在于其首次将高斯混合模型（GMM）引入到时变覆盖控制中，并成功解决了传统方法中对密度函数时间演变处理不足的问题。通过GMM建模，控制器能够更精确地捕捉环境的动态变化，从而实现更优的覆盖性能。其分布式和计算高效的特性也使其在实际多机器人系统部署中具有显著优势，特别是在环境监测等应用领域展现了巨大潜力。"}}
{"id": "2504.01293", "title": "Cuddle-Fish: Exploring a Soft Floating Robot with Flapping Wings for Physical Interactions", "authors": ["Mingyang Xu", "Jiayi Shao", "Yulan Ju", "Ximing Shen", "Qingyuan Gao", "Weijen Chen", "Qing Zhang", "Yun Suen Pai", "Giulia Barbareschi", "Matthias Hoppe", "Kouta Minamizawa", "Kai Kunze"], "categories": ["cs.HC", "cs.RO"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Augmented Humans International Conference 2025 (AHs '25)", "url": "http://arxiv.org/abs/2504.01293v2", "summary": "Flying robots, such as quadrotor drones, offer new possibilities for\nhuman-robot interaction but often pose safety risks due to fast-spinning\npropellers, rigid structures, and noise. In contrast, lighter-than-air\nflapping-wing robots, inspired by animal movement, offer a soft, quiet, and\ntouch-safe alternative. Building on these advantages, we present Cuddle-Fish, a\nsoft flapping-wing floating robot designed for close-proximity interactions in\nindoor spaces. Through a user study with 24 participants, we explored their\nperceptions of the robot and experiences during a series of co-located\ndemonstrations in which the robot moved near them. Results showed that\nparticipants felt safe, willingly engaged in touch-based interactions with the\nrobot, and exhibited spontaneous affective behaviours, such as patting,\nstroking, hugging, and cheek-touching, without external prompting. They also\nreported positive emotional responses towards the robot. These findings suggest\nthat the soft floating robot with flapping wings can serve as a novel and\nsocially acceptable alternative to traditional rigid flying robots, opening new\npotential for applications in companionship, affective interaction, and play in\neveryday indoor environments.", "comment": "Augmented Humans International Conference 2025 (AHs '25)", "pdf_url": "http://arxiv.org/pdf/2504.01293v2", "cate": "cs.HC", "date": "2025-04-02", "updated": "2025-07-25", "AI": {"title_translation": "Cuddle-Fish：探索一种带扑翼的软体浮动机器人用于物理交互", "tldr": "Cuddle-Fish是一种软体扑翼浮动机器人，旨在解决传统飞行机器人（如无人机）的安全和噪音问题。用户研究表明，参与者对Cuddle-Fish感到安全，愿意进行触觉互动，并表现出积极的情感反应，显示其在陪伴和情感互动方面的新潜力。", "motivation": "传统的飞行机器人如四旋翼无人机，由于快速旋转的螺旋桨、刚性结构和噪音，存在安全风险。为了克服这些缺点，研究者旨在开发一种更安全、安静、触感友好的替代品。", "method": "本研究介绍了一种名为Cuddle-Fish的软体扑翼浮动机器人，专为室内近距离交互设计。通过一项包含24名参与者的用户研究，探索了他们对机器人的感知以及在近距离演示中与机器人互动的体验。", "result": "用户研究结果显示，参与者在与Cuddle-Fish互动时感到安全，愿意进行基于触摸的互动，并自发表现出拍打、抚摸、拥抱和脸颊触碰等情感行为，且无需外部提示。他们还报告了对机器人积极的情绪反应。", "conclusion": "这些发现表明，带扑翼的软体浮动机器人可以作为传统刚性飞行机器人的一种新颖且社会可接受的替代品，为日常室内环境中的陪伴、情感交互和娱乐应用开辟了新的潜力。", "translation": "飞行机器人，例如四旋翼无人机，为人类-机器人交互提供了新的可能性，但由于快速旋转的螺旋桨、刚性结构和噪音，通常会带来安全风险。相比之下，受动物运动启发的气球式扑翼机器人，提供了一种柔软、安静且触感安全的替代方案。基于这些优势，我们推出了Cuddle-Fish，这是一种软体扑翼浮动机器人，专为室内空间的近距离交互而设计。通过一项包含24名参与者的用户研究，我们探索了他们对机器人的感知以及在一系列共址演示中机器人靠近他们时他们的体验。结果显示，参与者感到安全，愿意与机器人进行基于触摸的交互，并在没有外部提示的情况下表现出自发的情感行为，例如拍打、抚摸、拥抱和脸颊触碰。他们还报告了对机器人积极的情绪反应。这些发现表明，带扑翼的软体浮动机器人可以作为传统刚性飞行机器人的一种新颖且社会可接受的替代品，为日常室内环境中的陪伴、情感交互和娱乐应用开辟了新的潜力。", "summary": "本研究提出了一种名为Cuddle-Fish的软体扑翼浮动机器人，旨在提供比传统刚性飞行机器人更安全、更安静的人机交互体验。通过对24名参与者的用户研究，发现Cuddle-Fish在近距离互动中能让用户感到安全，并自发产生积极的触觉和情感反应，例如抚摸和拥抱。这表明该机器人是传统飞行机器人的一种有效替代品，在陪伴、情感交互和娱乐等领域具有广阔的应用前景。", "keywords": "软体机器人, 扑翼机器人, 人机交互, 浮动机器人, 情感交互", "comments": "Cuddle-Fish的创新点在于结合了软体结构和扑翼设计，有效解决了传统飞行机器人存在的安全和噪音问题，尤其在室内近距离人机交互场景中表现出显著优势。用户研究结果有力地证明了其在情感交互和陪伴方面的潜力，为未来家用或公共空间中的机器人应用提供了新的思路。"}}
{"id": "2506.06740", "title": "AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method", "authors": ["Yigui Feng", "Qinglin Wang", "Ke Liu", "Xinhai Chen", "Bo Yang", "Jie Liu"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      I found that some of the experiments were wrong with some data, especially those involving the protocol evaluation area", "url": "http://arxiv.org/abs/2506.06740v2", "summary": "Psychological counseling faces huge challenges due to the growing demand for\nmental health services and the shortage of trained professionals. Large\nlanguage models (LLMs) have shown potential to assist psychological counseling,\nespecially in empathy and emotional support. However, existing models lack a\ndeep understanding of emotions and are unable to generate personalized\ntreatment plans based on fine-grained emotions. To address these shortcomings,\nwe present AI PsyRoom, a multi-agent simulation framework designed to enhance\npsychological counseling by generating empathetic and emotionally nuanced\nconversations. By leveraging fine-grained emotion classification and a\nmulti-agent framework, we construct a multi-agent PsyRoom A for dialogue\nreconstruction, generating a high-quality dialogue dataset EmoPsy, which\ncontains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues.\nWe also propose PsyRoom B for generating personalized treatment plans.\nQuantitative evaluations demonstrate that AI PsyRoom significantly outperforms\nstate-of-the-art methods, achieving 18% improvement in problem orientation, 23%\nin expression, 24% in Empathy, and 16% in interactive communication quality.\nThe datasets and models are publicly available, providing a foundation for\nadvancing AI-assisted psychological counseling research.", "comment": "I found that some of the experiments were wrong with some data,\n  especially those involving the protocol evaluation area", "pdf_url": "http://arxiv.org/pdf/2506.06740v2", "cate": "cs.AI", "date": "2025-06-07", "updated": "2025-07-25", "AI": {"title_translation": "AI PsyRoom：分段渴望和反应性结果优化方法的人工智能平台", "tldr": "AI PsyRoom是一个多智能体模拟框架，通过生成富有同理心和情感细微差别的对话以及个性化治疗方案，显著提升了AI辅助心理咨询的效果。", "motivation": "心理咨询面临巨大挑战，因为心理健康服务需求增长，但专业人员短缺。现有的大语言模型虽有潜力，但缺乏对情感的深入理解，无法生成基于细粒度情感的个性化治疗方案。", "method": "本文提出了AI PsyRoom，一个多智能体模拟框架，旨在通过生成富有同理心和情感细微差别的对话来增强心理咨询。它利用细粒度情感分类和多智能体框架，构建了用于对话重建的多智能体PsyRoom A，生成了包含35种子情感、423个特定情感场景和12,350个对话的高质量对话数据集EmoPsy。同时，还提出了PsyRoom B用于生成个性化治疗方案。", "result": "定量评估表明，AI PsyRoom显著优于现有最先进的方法，在问题导向性方面提升18%，表达能力提升23%，同理心提升24%，互动沟通质量提升16%。数据集和模型已公开可用。", "conclusion": "AI PsyRoom通过其多智能体框架和细粒度情感理解，显著提升了AI辅助心理咨询的质量，并提供了公开的数据集和模型，为未来研究奠定了基础。", "translation": "心理咨询因心理健康服务需求增长和专业人员短缺而面临巨大挑战。大型语言模型（LLMs）已显示出辅助心理咨询的潜力，特别是在同理心和情感支持方面。然而，现有模型缺乏对情感的深入理解，无法根据细粒度情感生成个性化治疗方案。为了解决这些不足，我们提出了AI PsyRoom，一个多智能体模拟框架，旨在通过生成富有同理心和情感细微差别的对话来增强心理咨询。通过利用细粒度情感分类和多智能体框架，我们构建了用于对话重建的多智能体PsyRoom A，生成了一个高质量的对话数据集EmoPsy，其中包含35种子情感、423个特定情感场景和12,350个对话。我们还提出了PsyRoom B用于生成个性化治疗方案。定量评估表明，AI PsyRoom显著优于现有最先进的方法，在问题导向性方面提升18%，表达能力提升23%，同理心提升24%，互动沟通质量提升16%。数据集和模型已公开可用，为推进AI辅助心理咨询研究奠定了基础。", "summary": "AI PsyRoom是一个创新的人工智能平台，旨在解决心理咨询中专业人员短缺和现有LLM情感理解不足的问题。该平台采用多智能体模拟框架，通过PsyRoom A生成包含细粒度情感的对话数据EmoPsy，并利用PsyRoom B生成个性化治疗方案。实验结果表明，AI PsyRoom在多个关键指标上显著优于现有方法，为AI辅助心理咨询提供了高质量的解决方案和可公开访问的资源。", "keywords": "心理咨询, 人工智能, 多智能体系统, 情感理解, 个性化治疗", "comments": "AI PsyRoom的创新之处在于其多智能体模拟框架和对细粒度情感的关注，这使其能够生成更具同理心和个性化的心理咨询对话及治疗方案，克服了现有大语言模型在情感理解上的局限性。其公开数据集和模型为该领域的研究提供了宝贵资源，具有重要的实践和研究价值。"}}
{"id": "2507.19451", "title": "GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous Driving with Gaussian Splatting", "authors": ["Baijun Ye", "Minghui Qin", "Saining Zhang", "Moonjun Gong", "Shaoting Zhu", "Zebang Shen", "Luan Zhang", "Lu Zhang", "Hao Zhao", "Hang Zhao"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project Page: this https URL", "url": "http://arxiv.org/abs/2507.19451v1", "summary": "Occupancy is crucial for autonomous driving, providing essential geometric\npriors for perception and planning. However, existing methods predominantly\nrely on LiDAR-based occupancy annotations, which limits scalability and\nprevents leveraging vast amounts of potential crowdsourced data for\nauto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only\nframework that directly reconstructs occupancy. Vision-only occupancy\nreconstruction poses significant challenges due to sparse viewpoints, dynamic\nscene elements, severe occlusions, and long-horizon motion. Existing\nvision-based methods primarily rely on mesh representation, which suffer from\nincomplete geometry and additional post-processing, limiting scalability. To\novercome these issues, GS-Occ3D optimizes an explicit occupancy representation\nusing an Octree-based Gaussian Surfel formulation, ensuring efficiency and\nscalability. Additionally, we decompose scenes into static background, ground,\nand dynamic objects, enabling tailored modeling strategies: (1) Ground is\nexplicitly reconstructed as a dominant structural element, significantly\nimproving large-area consistency; (2) Dynamic vehicles are separately modeled\nto better capture motion-related occupancy patterns. Extensive experiments on\nthe Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry\nreconstruction results. By curating vision-only binary occupancy labels from\ndiverse urban scenes, we show their effectiveness for downstream occupancy\nmodels on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes.\nIt highlights the potential of large-scale vision-based occupancy\nreconstruction as a new paradigm for autonomous driving perception. Project\nPage: https://gs-occ3d.github.io/", "comment": "ICCV 2025. Project Page: https://gs-occ3d.github.io/", "pdf_url": "http://arxiv.org/pdf/2507.19451v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "GS-Occ3D：使用高斯泼溅扩展自动驾驶的纯视觉占用重建", "tldr": "GS-Occ3D提出了一种可扩展的纯视觉占用重建框架，利用高斯泼溅和场景分解，实现了最先进的几何重建，并能生成大规模纯视觉占用标签，为自动驾驶感知提供了新范式。", "motivation": "现有占用重建方法主要依赖于激光雷达标注，限制了可扩展性，难以利用众包数据进行自动标注。纯视觉方法面临视角稀疏、动态元素、严重遮挡和长距离运动等挑战，且现有基于视觉的网格表示存在几何不完整和额外后处理问题，进一步限制了可扩展性。", "method": "GS-Occ3D是一种可扩展的纯视觉占用重建框架。它通过使用基于八叉树的高斯曲面（Gaussian Surfel）公式来优化显式占用表示，以确保效率和可扩展性。此外，该方法将场景分解为静态背景、地面和动态物体，并采用定制的建模策略：显式重建地面以提高大面积一致性，并单独建模动态车辆以更好地捕获运动相关的占用模式。", "result": "GS-Occ3D在Waymo数据集上实现了最先进的几何重建结果。通过整理纯视觉二值占用标签，该研究展示了这些标签对Occ3D-Waymo下游占用模型的有效性，并在Occ3D-nuScenes上实现了卓越的零样本泛化能力。", "conclusion": "大规模纯视觉占用重建，如GS-Occ3D所示，有望成为自动驾驶感知的新范式。", "translation": "占用对自动驾驶至关重要，它为感知和规划提供了必要的几何先验。然而，现有方法主要依赖于基于激光雷达的占用标注，这限制了可扩展性，并阻碍了利用大量潜在的众包数据进行自动标注。为了解决这个问题，我们提出了GS-Occ3D，一个可扩展的纯视觉框架，直接重建占用。纯视觉占用重建由于稀疏视角、动态场景元素、严重遮挡和长距离运动等问题带来了巨大挑战。现有的基于视觉的方法主要依赖于网格表示，这导致几何不完整和额外的后处理，限制了可扩展性。为了克服这些问题，GS-Occ3D使用基于八叉树的高斯曲面（Gaussian Surfel）公式优化了显式占用表示，确保了效率和可扩展性。此外，我们将场景分解为静态背景、地面和动态物体，从而实现了量身定制的建模策略：(1) 地面作为主要结构元素被显式重建，显著提高了大面积的一致性；(2) 动态车辆被单独建模，以更好地捕获与运动相关的占用模式。在Waymo数据集上进行的广泛实验表明，GS-Occ3D实现了最先进的几何重建结果。通过从不同城市场景中整理纯视觉二值占用标签，我们展示了它们对Occ3D-Waymo下游占用模型的有效性，以及在Occ3D-nuScenes上的卓越零样本泛化能力。这突出了大规模纯视觉占用重建作为自动驾驶感知新范式的潜力。项目页面：https://gs-occ3d.github.io/", "summary": "GS-Occ3D旨在解决自动驾驶中基于激光雷达和网格的纯视觉占用重建方法的可扩展性限制。该论文提出了一种新颖的纯视觉框架，利用基于八叉树的高斯曲面公式实现高效且可扩展的显式占用表示。通过将场景分解为静态背景、地面和动态物体，并采用定制的建模策略，GS-Occ3D显著提高了重建质量。在Waymo和nuScenes数据集上的实验表明，GS-Occ3D实现了最先进的几何重建结果，并能够生成有效的纯视觉占用标签，具有出色的泛化能力，预示着其在大规模自动驾驶感知中的巨大潜力。", "keywords": "占用重建, 自动驾驶, 高斯泼溅, 纯视觉, 场景分解", "comments": "该论文通过引入高斯泼溅（Gaussian Splatting）进行占用重建，摆脱了传统网格表示的局限性，提供了一种创新方法。其场景分解策略，即区分静态和动态元素，是巧妙的设计选择，显著提升了在挑战性纯视觉环境下的鲁棒性和准确性。此外，GS-Occ3D生成可扩展纯视觉标签的能力，解决了自动驾驶数据标注中的关键瓶颈，使其在实际应用中具有重要意义。"}}
{"id": "2507.18866", "title": "Early Mortality Prediction in ICU Patients with Hypertensive Kidney Disease Using Interpretable Machine Learning", "authors": ["Yong Si", "Junyi Fan", "Li Sun", "Shuheng Chen", "Minoo Ahmadi", "Elham Pishgar", "Kamiar Alaei", "Greg Placencia", "Maryam Pishgar"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18866v1", "summary": "Background: Hypertensive kidney disease (HKD) patients in intensive care\nunits (ICUs) face high short-term mortality, but tailored risk prediction tools\nare lacking. Early identification of high-risk individuals is crucial for\nclinical decision-making. Methods: We developed a machine learning framework to\npredict 30-day in-hospital mortality among ICU patients with HKD using early\nclinical data from the MIMIC-IV v2.2 database. A cohort of 1,366 adults was\ncurated with strict criteria, excluding malignancy cases. Eighteen clinical\nfeatures-including vital signs, labs, comorbidities, and therapies-were\nselected via random forest importance and mutual information filtering. Several\nmodels were trained and compared with stratified five-fold cross-validation;\nCatBoost demonstrated the best performance. Results: CatBoost achieved an AUROC\nof 0.88 on the independent test set, with sensitivity of 0.811 and specificity\nof 0.798. SHAP values and Accumulated Local Effects (ALE) plots showed the\nmodel relied on meaningful predictors such as altered consciousness,\nvasopressor use, and coagulation status. Additionally, the DREAM algorithm was\nintegrated to estimate patient-specific posterior risk distributions, allowing\nclinicians to assess both predicted mortality and its uncertainty. Conclusions:\nWe present an interpretable machine learning pipeline for early, real-time risk\nassessment in ICU patients with HKD. By combining high predictive performance\nwith uncertainty quantification, our model supports individualized triage and\ntransparent clinical decisions. This approach shows promise for clinical\ndeployment and merits external validation in broader critical care populations.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18866v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "使用可解释机器学习预测ICU高血压肾病患者的早期死亡率", "tldr": "开发了一个可解释的机器学习模型，用于早期预测ICU高血压肾病患者的30天院内死亡率，并量化不确定性。", "motivation": "ICU高血压肾病患者短期死亡率高，但缺乏量身定制的风险预测工具；早期识别高危个体对临床决策至关重要。", "method": "使用MIMIC-IV v2.2数据库的早期临床数据，构建了1366名成人患者的队列。通过随机森林重要性和互信息过滤选择了18个临床特征。训练并比较了多个模型，CatBoost表现最佳。整合了DREAM算法来估计患者特定的后验风险分布。", "result": "CatBoost在独立测试集上AUROC达到0.88，敏感性0.811，特异性0.798。SHAP值和ALE图显示模型依赖于有意义的预测因子，如意识改变、血管升压药使用和凝血状态。DREAM算法提供了死亡率预测及其不确定性。", "conclusion": "提出了一个可解释的机器学习流程，用于ICU高血压肾病患者的早期实时风险评估，结合高预测性能和不确定性量化，支持个体化分诊和透明的临床决策。", "translation": "背景：重症监护病房（ICU）中的高血压肾病（HKD）患者面临较高的短期死亡率，但缺乏量身定制的风险预测工具。早期识别高危个体对于临床决策至关重要。\n方法：我们开发了一个机器学习框架，利用MIMIC-IV v2.2数据库的早期临床数据，预测ICU高血压肾病患者的30天院内死亡率。通过严格的标准筛选出1366名成人患者，排除了恶性肿瘤病例。通过随机森林重要性分析和互信息过滤，选择了18个临床特征，包括生命体征、实验室检查、合并症和治疗。通过分层五折交叉验证训练并比较了多个模型；CatBoost表现出最佳性能。\n结果：CatBoost在独立测试集上获得了0.88的AUROC，敏感性为0.811，特异性为0.798。SHAP值和累积局部效应（ALE）图显示，该模型依赖于有意义的预测因子，如意识改变、血管升压药使用和凝血状态。此外，还整合了DREAM算法来估计患者特定的后验风险分布，使临床医生能够评估预测死亡率及其不确定性。\n结论：我们提出了一个可解释的机器学习流程，用于ICU高血压肾病患者的早期实时风险评估。通过将高预测性能与不确定性量化相结合，我们的模型支持个体化分诊和透明的临床决策。这种方法有望在临床部署中应用，并值得在更广泛的重症监护人群中进行外部验证。", "summary": "本研究开发并验证了一个可解释的机器学习模型，用于早期预测ICU高血压肾病患者的30天院内死亡率。该模型利用MIMIC-IV数据库的早期临床数据，通过CatBoost实现了高预测性能（AUROC 0.88），并通过SHAP和ALE提供了可解释性，同时结合DREAM算法量化了预测的不确定性，为临床决策提供了支持。", "keywords": "高血压肾病, 早期死亡率预测, 可解释机器学习, ICU, CatBoost", "comments": "该研究的创新点在于结合了高性能的机器学习模型（CatBoost）与可解释性工具（SHAP, ALE），并进一步引入了不确定性量化（DREAM算法），这对于临床部署尤为重要，因为它能帮助临床医生更好地理解预测结果并评估风险。其局限性可能在于模型仍需在更广泛的外部人群中进行验证。"}}
{"id": "2507.18997", "title": "UPP: Unified Point-Level Prompting for Robust Point Cloud Analysis", "authors": ["Zixiang Ai", "Zhenyu Cui", "Yuxin Peng", "Jiahuan Zhou"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted by ICCV 2025 as a Poster", "url": "http://arxiv.org/abs/2507.18997v1", "summary": "Pre-trained point cloud analysis models have shown promising advancements in\nvarious downstream tasks, yet their effectiveness is typically suffering from\nlow-quality point cloud (i.e., noise and incompleteness), which is a common\nissue in real scenarios due to casual object occlusions and unsatisfactory data\ncollected by 3D sensors. To this end, existing methods focus on enhancing point\ncloud quality by developing dedicated denoising and completion models. However,\ndue to the isolation between the point cloud enhancement and downstream tasks,\nthese methods fail to work in various real-world domains. In addition, the\nconflicting objectives between denoising and completing tasks further limit the\nensemble paradigm to preserve critical geometric features. To tackle the above\nchallenges, we propose a unified point-level prompting method that reformulates\npoint cloud denoising and completion as a prompting mechanism, enabling robust\nanalysis in a parameter-efficient manner. We start by introducing a\nRectification Prompter to adapt to noisy points through the predicted\nrectification vector prompts, effectively filtering noise while preserving\nintricate geometric features essential for accurate analysis. Sequentially, we\nfurther incorporate a Completion Prompter to generate auxiliary point prompts\nbased on the rectified point clouds, facilitating their robustness and\nadaptability. Finally, a Shape-Aware Unit module is exploited to efficiently\nunify and capture the filtered geometric features for the downstream point\ncloud analysis.Extensive experiments on four datasets demonstrate the\nsuperiority and robustness of our method when handling noisy and incomplete\npoint cloud data against existing state-of-the-art methods. Our code is\nreleased at https://github.com/zhoujiahuan1991/ICCV2025-UPP.", "comment": "Accepted by ICCV 2025 as a Poster", "pdf_url": "http://arxiv.org/pdf/2507.18997v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "UPP: 统一的点级提示用于鲁棒点云分析", "tldr": "本文提出UPP，一种统一的点级提示方法，将点云去噪和补全重构为提示机制，从而在低质量点云下实现鲁棒的点云分析。", "motivation": "现有预训练点云分析模型在低质量点云（如噪声和不完整性）下表现不佳，这是由于点云增强与下游任务的隔离以及去噪和补全任务之间目标冲突导致的。", "method": "本文提出一种统一的点级提示（UPP）方法，将点云去噪和补全重构为提示机制。具体包括引入Rectification Prompter以适应噪声点并过滤噪声，同时保留几何特征；接着结合Completion Prompter基于校正后的点云生成辅助点提示，增强鲁棒性和适应性；最后利用Shape-Aware Unit模块高效统一并捕获过滤后的几何特征用于下游点云分析。", "result": "在四个数据集上进行的广泛实验表明，该方法在处理噪声和不完整点云数据时，优于现有最先进的方法，并展现出卓越的性能和鲁棒性。", "conclusion": "本文提出的UPP方法通过统一的点级提示机制，有效解决了低质量点云带来的挑战，实现了参数高效且鲁棒的点云分析，并在实验中验证了其优越性。", "translation": "预训练点云分析模型在各种下游任务中取得了可喜的进展，然而，它们的有效性通常受到低质量点云（即噪声和不完整性）的影响，这是由于偶然的物体遮挡和3D传感器收集到的不令人满意的数据在真实场景中普遍存在的问题。为此，现有方法通过开发专门的去噪和补全模型来提高点云质量。然而，由于点云增强和下游任务之间的隔离，这些方法无法在各种真实世界领域中发挥作用。此外，去噪和补全任务之间相互冲突的目标进一步限制了集成范式以保留关键几何特征。为了解决上述挑战，我们提出了一种统一的点级提示方法，将点云去噪和补全重构为一种提示机制，以参数高效的方式实现鲁棒分析。我们首先引入一个校正提示器（Rectification Prompter），通过预测的校正向量提示来适应噪声点，有效地过滤噪声，同时保留对精确分析至关重要的复杂几何特征。随后，我们进一步结合了一个补全提示器（Completion Prompter），基于校正后的点云生成辅助点提示，促进其鲁棒性和适应性。最后，利用一个形状感知单元（Shape-Aware Unit）模块，有效地统一并捕获过滤后的几何特征，用于下游点云分析。在四个数据集上进行的广泛实验证明，在处理噪声和不完整点云数据时，我们的方法优于现有最先进的方法，并展现出卓越的性能和鲁棒性。我们的代码已发布在https://github.com/zhoujiahuan1991/ICCV2025-UPP。", "summary": "预训练点云分析模型在低质量点云（噪声和不完整性）下表现不佳，现有方法因增强与下游任务隔离及去噪与补全目标冲突而受限。本文提出UPP（统一的点级提示方法），将点云去噪和补全重构为参数高效的提示机制。UPP包含Rectification Prompter用于过滤噪声并保留几何特征，以及Completion Prompter用于生成辅助点提示。一个Shape-Aware Unit模块用于统一和捕获特征。实验证明UPP在处理噪声和不完整点云数据时，优于现有SOTA方法并表现出鲁棒性。", "keywords": "点云分析, 统一提示, 去噪, 补全, 鲁棒性", "comments": "该论文的创新点在于将点云去噪和补全任务统一到点级提示机制中，解决了传统方法中增强与下游任务隔离以及任务目标冲突的问题。这种统一和参数高效的方法对于实际应用中常见的低质量点云处理具有重要意义，提升了点云分析的鲁棒性。"}}
{"id": "2507.19396", "title": "Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study", "authors": ["Rachel M. Murphy", "Nishant Mishra", "Nicolette F. de Keizer", "Dave A. Dongelmans", "Kitty J. Jager", "Ameen Abu-Hanna", "Joanna E. Klopotowska", "Iacer Calixto"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      30 Pages, 5 Figures (Main Paper), 19 Pages, 2 Figures(Supplements). Rachel M. Murphy and Nishant Mishra are shared first authors. Joanna E. Klopotowska and Iacer Calixto are shared last authors", "url": "http://arxiv.org/abs/2507.19396v2", "summary": "In this study, we establish a benchmark for adverse drug event (ADE)\ndetection in Dutch clinical free-text documents using several transformer\nmodels, clinical scenarios, and fit-for-purpose performance measures. We\ntrained a Bidirectional Long Short-Term Memory (Bi-LSTM) model and four\ntransformer-based Dutch and/or multilingual encoder models (BERTje, RobBERT,\nMedRoBERTa(.)nl, and NuNER) for the tasks of named entity recognition (NER) and\nrelation classification (RC) using 102 richly annotated Dutch ICU clinical\nprogress notes. Anonymized free-text clinical progress notes of patients\nadmitted to the intensive care unit (ICU) of one academic hospital and\ndischarge letters of patients admitted to Internal Medicine wards of two\nnon-academic hospitals were reused. We evaluated our ADE RC models internally\nusing the gold standard (two-step task) and predicted entities (end-to-end\ntask). In addition, all models were externally validated for detecting ADEs at\nthe document level. We report both micro- and macro-averaged F1 scores, given\nthe dataset imbalance in ADEs. Although differences for the ADE RC task between\nthe models were small, MedRoBERTa(.)nl was the best performing model with a\nmacro-averaged F1 score of 0.63 using the gold standard and 0.62 using\npredicted entities. The MedRoBERTa(.)nl models also performed the best in our\nexternal validation and achieved a recall of between 0.67 to 0.74 using\npredicted entities, meaning between 67 to 74% of discharge letters with ADEs\nwere detected. Our benchmark study presents a robust and clinically meaningful\napproach for evaluating language models for ADE detection in clinical free-text\ndocuments. Our study highlights the need to use appropriate performance\nmeasures fit for the task of ADE detection in clinical free-text documents and\nenvisioned future clinical use.", "comment": "30 Pages, 5 Figures (Main Paper), 19 Pages, 2 Figures(Supplements).\n  Rachel M. Murphy and Nishant Mishra are shared first authors. Joanna E.\n  Klopotowska and Iacer Calixto are shared last authors", "pdf_url": "http://arxiv.org/pdf/2507.19396v2", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-28", "AI": {"title_translation": "使用Transformer模型检测荷兰临床自由文本文件中的不良药物事件：基准研究", "tldr": "本研究为使用多种Transformer模型在荷兰临床自由文本中检测不良药物事件建立了基准，并发现MedRoBERTa(.)nl模型表现最佳。", "motivation": "本研究旨在为使用Transformer模型在荷兰临床自由文本中检测不良药物事件建立一个基准，并强调使用适合该任务和未来临床用途的适当性能度量的重要性。", "method": "研究训练了一个Bi-LSTM模型和四种基于Transformer的荷兰语和/或多语言编码模型（BERTje、RobBERT、MedRoBERTa(.)nl和NuNER），用于命名实体识别（NER）和关系分类（RC）任务。数据使用了102份经过详细标注的荷兰ICU临床病程记录，以及来自一家学术医院ICU患者的匿名自由文本临床病程记录和两家非学术医院内科病房患者的出院信。模型在内部通过金标准（两步任务）和预测实体（端到端任务）进行评估，并在文档级别对ADE检测进行外部验证。性能指标使用微观和宏观平均F1分数。", "result": "尽管ADE RC任务中模型间的差异很小，但MedRoBERTa(.)nl模型表现最佳，使用金标准时宏观平均F1分数为0.63，使用预测实体时为0.62。MedRoBERTa(.)nl模型在外部验证中也表现最佳，使用预测实体时召回率在0.67到0.74之间，意味着67%到74%的出院信中的ADE被检测到。", "conclusion": "本基准研究为评估临床自由文本中ADE检测的语言模型提供了一种稳健且具有临床意义的方法。研究强调了在临床自由文本中检测ADE任务中需要使用适当的性能度量，并展望了未来的临床应用。", "translation": "在本研究中，我们使用多种Transformer模型、临床场景和适用的性能度量，为荷兰临床自由文本文件中的不良药物事件（ADE）检测建立了基准。我们训练了一个双向长短期记忆（Bi-LSTM）模型和四个基于Transformer的荷兰语和/或多语言编码模型（BERTje、RobBERT、MedRoBERTa(.)nl和NuNER），用于命名实体识别（NER）和关系分类（RC）任务，使用了102份经过详细标注的荷兰ICU临床病程记录。我们重用了来自一家学术医院重症监护室（ICU）患者的匿名自由文本临床病程记录，以及两家非学术医院内科病房患者的出院信。我们使用金标准（两步任务）和预测实体（端到端任务）对我们的ADE RC模型进行了内部评估。此外，所有模型都在文档级别进行了ADE检测的外部验证。考虑到ADE数据集中存在不平衡，我们报告了微观和宏观平均F1分数。尽管模型之间在ADE RC任务上的差异很小，但MedRoBERTa(.)nl是表现最佳的模型，使用金标准时宏观平均F1分数为0.63，使用预测实体时为0.62。MedRoBERTa(.)nl模型在我们的外部验证中也表现最佳，使用预测实体时召回率在0.67到0.74之间，这意味着67%到74%的出院信中的ADE被检测到。我们的基准研究为评估临床自由文本文件中ADE检测的语言模型提供了一种稳健且具有临床意义的方法。我们的研究强调了在临床自由文本文件中检测ADE任务中需要使用适当的性能度量，并展望了未来的临床用途。", "summary": "本研究为在荷兰语临床自由文本中检测不良药物事件（ADE）建立了一个基准，评估了Bi-LSTM和多种Transformer模型（BERTje, RobBERT, MedRoBERTa(.)nl, NuNER）在命名实体识别和关系分类任务上的表现。研究使用了ICU病程记录和出院信数据，并进行了内部和外部验证。结果显示，MedRoBERTa(.)nl模型在ADE关系分类和文档级别检测中表现最佳，其宏观平均F1分数最高，且在外部验证中ADE召回率可达67-74%。研究强调了使用适当性能度量的重要性，并提供了一个稳健的临床评估方法。", "keywords": "不良药物事件检测, Transformer模型, 荷兰语临床文本, 基准研究, MedRoBERTa(.nl)", "comments": "这项研究通过建立荷兰语临床自由文本中不良药物事件检测的基准，填补了该领域的一个空白。其创新之处在于评估了多种Transformer模型，并使用了真实世界的临床数据进行内部和外部验证，增强了结果的临床意义和鲁棒性。研究还强调了在不平衡数据集中使用宏观平均F1分数的重要性，以及选择适合任务的性能度量的必要性，这对于未来的临床应用具有指导意义。"}}
{"id": "2507.18771", "title": "Discovering the dynamics of \\emph{Sargassum} rafts' centers of mass", "authors": ["Francisco J. Beron-Vera", "Gage Bonner"], "categories": ["nlin.CD", "cs.LG", "physics.ao-ph"], "primary_category": "Subjects:       Chaotic Dynamics (nlin.CD)", "pdf_link": null, "comments": "Comments:      Submitted to Chaos", "url": "http://arxiv.org/abs/2507.18771v1", "summary": "Since 2011, rafts of floating \\emph{Sargassum} seaweed have frequently\nobstructed the coasts of the Intra-Americas Seas. The motion of the rafts is\nrepresented by a high-dimensional nonlinear dynamical system. Referred to as\nthe eBOMB model, this builds on the Maxey--Riley equation by incorporating\ninteractions between clumps of \\emph{Sargassum} forming a raft and the effects\nof Earth's rotation. The absence of a predictive law for the rafts' centers of\nmass suggests a need for machine learning. In this paper, we evaluate and\ncontrast Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) and\nSparse Identification of Nonlinear Dynamics (SINDy). In both cases, a\nphysics-inspired closure modeling approach is taken rooted in eBOMB.\nSpecifically, the LSTM model learns a mapping from a collection of eBOMB\nvariables to the difference between raft center-of-mass and ocean velocities.\nThe SINDy model's library of candidate functions is suggested by eBOMB\nvariables and includes windowed velocity terms incorporating far-field effects\nof the carrying flow. Both LSTM and SINDy models perform most effectively in\nconditions with tightly bonded clumps, despite declining precision with rising\ncomplexity, such as with wind effects and when assessing loosely connected\nclumps. The LSTM model delivered the best results when designs were\nstraightforward, with fewer neurons and hidden layers. While LSTM model serves\nas an opaque black-box model lacking interpretability, the SINDy model brings\ntransparency by discerning explicit functional relationships through the\nfunction libraries. Integration of the windowed velocity terms enabled\neffective modeling of nonlocal interactions, particularly in datasets featuring\nsparsely connected rafts.", "comment": "Submitted to Chaos", "pdf_url": "http://arxiv.org/pdf/2507.18771v1", "cate": "nlin.CD", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "发现马尾藻筏质心动力学", "tldr": "研究评估了LSTM和SINDy模型用于预测马尾藻筏质心运动，发现它们在特定条件下有效，且SINDy模型提供可解释性。", "motivation": "由于缺乏预测马尾藻筏质心运动的定律，需要机器学习方法来解决这一问题。", "method": "该研究评估并对比了长短期记忆（LSTM）循环神经网络和稀疏非线性动力学识别（SINDy）模型。两种方法都采用了基于eBOMB模型的物理启发式闭合建模方法。LSTM模型学习从eBOMB变量到筏质心与海洋速度差的映射，而SINDy模型的候选函数库由eBOMB变量启发，并包含结合远场效应的窗口速度项。", "result": "LSTM和SINDy模型在紧密连接的藻团条件下表现最佳，但随着复杂性（如风效应和松散连接藻团）的增加，精度会下降。LSTM模型在设计简单时（神经元和隐藏层较少）效果最好。SINDy模型通过函数库提供了可解释的显式函数关系，而LSTM是黑箱模型。窗口速度项的整合使得非局部相互作用的有效建模成为可能，特别是在稀疏连接的藻筏数据集中。", "conclusion": "研究表明，尽管在复杂条件下精度会下降，但LSTM和SINDy模型都能有效预测马尾藻筏的质心动力学。SINDy模型因其可解释性而具有优势，而LSTM在简单设计下表现优异。窗口速度项对于模拟非局部相互作用至关重要。", "translation": "自2011年以来，漂浮的马尾藻筏频繁地阻塞美洲内海沿岸。这些藻筏的运动由一个高维非线性动力系统表示。这个被称为eBOMB模型的系统在Maxey-Riley方程的基础上，结合了形成藻筏的马尾藻团块之间的相互作用以及地球自转效应。由于缺乏对藻筏质心运动的预测定律，这表明需要使用机器学习。在本文中，我们评估并对比了长短期记忆（LSTM）循环神经网络和稀疏非线性动力学识别（SINDy）模型。在这两种情况下，都采用了基于eBOMB模型的物理启发式闭合建模方法。具体来说，LSTM模型学习从一系列eBOMB变量到藻筏质心速度与海洋速度之间差异的映射。SINDy模型的候选函数库由eBOMB变量启发，并包含结合承载流远场效应的窗口速度项。LSTM和SINDy模型在藻团紧密连接的条件下表现最有效，尽管随着复杂性（例如风效应和评估松散连接藻团时）的增加，精度会下降。当设计简单，神经元和隐藏层较少时，LSTM模型提供了最佳结果。虽然LSTM模型是一个缺乏可解释性的不透明黑箱模型，但SINDy模型通过识别函数库中的显式函数关系带来了透明度。窗口速度项的整合使得非局部相互作用的有效建模成为可能，特别是在特征为稀疏连接藻筏的数据集中。", "summary": "本研究旨在解决马尾藻筏质心运动缺乏预测定律的问题，通过评估和对比LSTM循环神经网络和SINDy模型，来预测马尾藻筏的动力学。两种模型均基于eBOMB模型，并采用物理启发式闭合建模。结果显示，在藻团紧密连接时，两种模型均表现良好，但面对复杂性（如风效应或松散连接）时精度会下降。LSTM在简单设计下表现最佳，而SINDy模型因能揭示显式函数关系而提供更好的可解释性。研究还强调了窗口速度项在建模非局部相互作用中的重要性，尤其对于稀疏连接的藻筏。", "keywords": "马尾藻, 质心动力学, LSTM, SINDy, eBOMB模型", "comments": "该论文创新性地将机器学习方法（LSTM和SINDy）应用于预测复杂的海洋生物动力学系统（马尾藻筏的质心运动），并结合了物理模型（eBOMB）的启发，这是一种值得称赞的“物理信息机器学习”方法。其重要性在于为预测马尾藻入侵提供了潜在工具，有助于沿海管理。论文还对比了黑箱模型（LSTM）和可解释模型（SINDy）在实际问题中的优劣，强调了可解释性在科学发现中的价值，尤其是在理解系统动力学方面。然而，论文也指出，在面对高复杂性（如风效应和松散连接藻团）时，模型的预测精度会下降，这可能是一个限制，表明模型在更复杂的现实场景中可能需要进一步改进。"}}
{"id": "2507.18880", "title": "Rethinking Accessible Prototyping Methods for Blind and Visually Impaired Passengers in Highly Automated Vehicles", "authors": ["Luca-Maxim Meinhardt", "Enrico Rukzio"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Workshop paper presented at \"Access InContext: Futuring Accessible Prototyping Tools and Methods\", CHI'25, April 26, 2025, Yokohama, Japan. Submitted Feb 5, accepted Mar 1", "url": "http://arxiv.org/abs/2507.18880v1", "summary": "Highly Automated Vehicles (HAVs) can improve mobility for blind and visually\nimpaired people (BVIPs). However, designing non-visual interfaces that enable\nthem to maintain situation awareness inside the vehicle is a challenge. This\npaper presents two of our participatory design workshops that explored what\ninformation BVIPs need in HAVs and what an interface that meets these needs\nmight look like. Based on the participants' insights, we created final systems\nto improve their situation awareness. The two workshops used different\napproaches: in the first, participants built their own low-fidelity prototypes;\nin the second, they evaluated and discussed the initial prototypes we provided.\nWe will outline how each workshop was set up and share lessons learned about\nprototyping methods for BVIPs and how they could be improved.", "comment": "Workshop paper presented at \"Access InContext: Futuring Accessible\n  Prototyping Tools and Methods\", CHI'25, April 26, 2025, Yokohama, Japan.\n  Submitted Feb 5, accepted Mar 1", "pdf_url": "http://arxiv.org/pdf/2507.18880v1", "cate": "cs.HC", "date": "2025-06-05", "updated": "2025-06-05", "AI": {"title_translation": "重新思考高度自动化车辆中盲人和视障乘客的可访问原型设计方法", "tldr": "本文探讨了为盲人和视障乘客在高度自动化车辆中设计非视觉界面的挑战，通过两次参与式设计工作坊，探索了信息需求并创建了原型系统，分享了原型设计方法的经验教训。", "motivation": "高度自动化车辆（HAVs）可以改善盲人和视障人士（BVIPs）的出行能力，但为他们设计非视觉界面以帮助他们在车内保持情境感知是一个挑战。", "method": "本文介绍了两次参与式设计工作坊。在第一次工作坊中，参与者构建了自己的低保真原型；在第二次工作坊中，参与者评估并讨论了研究人员提供的初始原型。", "result": "基于参与者的见解，创建了最终系统以改善他们的情境感知。论文概述了每个工作坊的设置，并分享了关于为盲人和视障人士进行原型设计方法的经验教训以及如何改进这些方法。", "conclusion": "本文通过实践工作坊验证了为盲人和视障人士设计可访问原型的有效性，并提供了关于改进这些方法的具体经验教训，有助于未来为该群体开发更优化的车载界面。", "translation": "高度自动化车辆（HAVs）可以改善盲人和视障人士（BVIPs）的出行能力。然而，设计非视觉界面以使他们在车内保持情境感知是一个挑战。本文介绍了我们进行的两次参与式设计工作坊，探讨了盲人和视障人士在高度自动化车辆中需要哪些信息，以及满足这些需求的界面可能是什么样子。根据参与者的见解，我们创建了最终系统以改善他们的情境感知。两次工作坊采用了不同的方法：在第一次工作坊中，参与者构建了自己的低保真原型；在第二次工作坊中，他们评估并讨论了我们提供的初始原型。我们将概述每个工作坊的设置，并分享关于为盲人和视障人士进行原型设计方法的经验教训以及如何改进这些方法。", "summary": "本文探讨了在高度自动化车辆（HAVs）中为盲人和视障人士（BVIPs）设计非视觉界面的挑战，旨在帮助他们保持情境感知。通过两次参与式设计工作坊，研究人员与BVIPs共同探索了信息需求并开发了原型系统。工作坊采用了两种不同方法：一是参与者自建低保真原型，二是评估研究人员提供的初始原型。研究基于参与者见解创建了最终系统，并总结了为BVIPs进行原型设计的经验教训及改进方向。", "keywords": "高度自动化车辆, 盲人和视障人士, 参与式设计, 原型设计, 情境感知", "comments": "这篇论文通过实际的参与式设计工作坊，深入探讨了为特定弱势群体（盲人和视障人士）设计人机交互界面的方法。其创新之处在于采用了两种不同的原型设计方法（用户自建和用户评估），这有助于比较不同策略的有效性。这项研究的重要性在于它直接关注了高度自动化车辆的可访问性问题，为未来无障碍交通工具的设计提供了宝贵的实践经验和方法论指导。"}}
{"id": "2507.15831", "title": "Observing Fine-Grained Changes in Jupyter Notebooks During Development Time", "authors": ["Sergey Titov", "Konstantin Grotov", "Cristina Sarasua", "Yaroslav Golubev", "Dhivyabharathi Ramasamy", "Alberto Bacchelli", "Abraham Bernstein", "Timofey Bryksin"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      32 pages, 6 figures", "url": "http://arxiv.org/abs/2507.15831v2", "summary": "In software engineering, numerous studies have focused on the analysis of\nfine-grained logs, leading to significant innovations in areas such as\nrefactoring, security, and code completion. However, no similar studies have\nbeen conducted for computational notebooks in the context of data science.\n  To help bridge this research gap, we make three scientific contributions: we\n(1) introduce a toolset for collecting code changes in Jupyter notebooks during\ndevelopment time; (2) use it to collect more than 100 hours of work related to\na data analysis task and a machine learning task (carried out by 20 developers\nwith different levels of expertise), resulting in a dataset containing 2,655\ncells and 9,207 cell executions; and (3) use this dataset to investigate the\ndynamic nature of the notebook development process and the changes that take\nplace in the notebooks.\n  In our analysis of the collected data, we classified the changes made to the\ncells between executions and found that a significant number of these changes\nwere code iteration modifications. We report a number of other insights and\npropose potential future research directions on the novel data.", "comment": "32 pages, 6 figures", "pdf_url": "http://arxiv.org/pdf/2507.15831v2", "cate": "cs.SE", "date": "2025-07-21", "updated": "2025-07-24", "AI": {"title_translation": "观察Jupyter Notebook开发过程中的细粒度变化", "tldr": "该研究引入了一个工具集来收集Jupyter Notebook在开发过程中的代码变化，并用收集到的数据集分析了Notebook开发过程的动态特性和变化，发现大量变化是代码迭代修改。", "motivation": "在软件工程领域，针对细粒度日志的分析已取得显著进展，但在数据科学领域的计算型Notebook中尚未有类似研究。本文旨在弥补这一研究空白。", "method": "1. 引入了一个用于在开发过程中收集Jupyter Notebook代码变化的工具集。2. 使用该工具集收集了超过100小时的工作数据，包括一个数据分析任务和一个机器学习任务（由20名不同专业水平的开发者完成），形成了一个包含2,655个单元和9,207次单元执行的数据集。3. 利用该数据集研究Notebook开发过程的动态特性和其中发生的变化，并对单元格之间在执行期间发生的变化进行了分类。", "result": "分析收集到的数据显示，单元格之间的大量变化是代码迭代修改。研究还报告了其他一些见解。", "conclusion": "该研究成功地收集并分析了Jupyter Notebook在开发过程中的细粒度变化，揭示了Notebook开发过程的动态特性，并为未来的研究方向提供了新的数据基础。", "translation": "在软件工程领域，大量研究专注于细粒度日志的分析，这在重构、安全和代码完成等领域带来了显著创新。然而，在数据科学背景下的计算型Notebook中，尚未进行类似的研究。为了弥补这一研究空白，我们做出了三项科学贡献：我们（1）引入了一个工具集，用于在开发过程中收集Jupyter Notebook中的代码变化；（2）使用该工具集收集了超过100小时的工作数据，涉及一个数据分析任务和一个机器学习任务（由20名不同专业水平的开发者完成），生成了一个包含2,655个单元格和9,207次单元格执行的数据集；（3）使用该数据集调查了Notebook开发过程的动态特性以及Notebook中发生的变化。在我们对收集到的数据进行分析时，我们对执行之间对单元格进行的更改进行了分类，发现这些更改中有很大一部分是代码迭代修改。我们报告了许多其他见解，并提出了关于这些新数据的潜在未来研究方向。", "summary": "本研究旨在弥补数据科学领域计算型Notebook细粒度变化分析的空白。为此，研究团队开发了一个工具集，用于在Jupyter Notebook开发过程中收集代码变化，并利用该工具收集了一个包含2,655个单元和9,207次单元执行的大型数据集。通过对该数据集的分析，研究发现Jupyter Notebook中的大量变化属于代码迭代修改，并提供了关于Notebook开发过程动态特性的新见解，为未来研究奠定了基础。", "keywords": "Jupyter Notebook, 细粒度变化, 开发过程, 数据集, 代码迭代", "comments": "这项研究的创新之处在于首次将细粒度日志分析方法应用于Jupyter Notebooks，填补了数据科学领域的一个研究空白。其开发的数据收集工具集和生成的数据集具有重要的科学贡献，为理解Notebook开发行为和未来工具开发提供了宝贵资源。该研究揭示了代码迭代修改在Notebook开发中的普遍性，这对于优化开发流程和工具支持具有指导意义。"}}
{"id": "2507.19184", "title": "Continual Learning-Based Unified Model for Unpaired Image Restoration Tasks", "authors": ["Kotha Kartheek", "Lingamaneni Gnanesh Chowdary", "Snehasis Mukherjee"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Under Review", "url": "http://arxiv.org/abs/2507.19184v1", "summary": "Restoration of images contaminated by different adverse weather conditions\nsuch as fog, snow, and rain is a challenging task due to the varying nature of\nthe weather conditions. Most of the existing methods focus on any one\nparticular weather conditions. However, for applications such as autonomous\ndriving, a unified model is necessary to perform restoration of corrupted\nimages due to different weather conditions. We propose a continual learning\napproach to propose a unified framework for image restoration. The proposed\nframework integrates three key innovations: (1) Selective Kernel Fusion layers\nthat dynamically combine global and local features for robust adaptive feature\nselection; (2) Elastic Weight Consolidation (EWC) to enable continual learning\nand mitigate catastrophic forgetting across multiple restoration tasks; and (3)\na novel Cycle-Contrastive Loss that enhances feature discrimination while\npreserving semantic consistency during domain translation. Further, we propose\nan unpaired image restoration approach to reduce the dependance of the proposed\napproach on the training data. Extensive experiments on standard benchmark\ndatasets for dehazing, desnowing and deraining tasks demonstrate significant\nimprovements in PSNR, SSIM, and perceptual quality over the state-of-the-art.", "comment": "Under Review", "pdf_url": "http://arxiv.org/pdf/2507.19184v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于持续学习的无配对图像恢复统一模型", "tldr": "本文提出一个基于持续学习的统一框架，用于处理多种恶劣天气条件下的图像恢复任务，通过整合选择性核融合、弹性权重整合和循环对比损失，实现了对去雾、去雪和去雨任务的显著性能提升。", "motivation": "由于天气条件的多样性，恢复受雾、雪、雨等不同恶劣天气污染的图像是一项具有挑战性的任务。现有方法大多专注于单一特定天气条件，但对于自动驾驶等应用，需要一个统一模型来恢复因不同天气条件损坏的图像。", "method": "本文提出一种基于持续学习的统一图像恢复框架。该框架整合了三项关键创新：1) 选择性核融合层，动态结合全局和局部特征以实现鲁棒的自适应特征选择；2) 弹性权重整合（EWC），实现持续学习并减轻多任务恢复中的灾难性遗忘；3) 一种新颖的循环对比损失，在域转换过程中增强特征辨别力同时保持语义一致性。此外，还提出一种无配对图像恢复方法，以减少对训练数据的依赖。", "result": "在去雾、去雪和去雨任务的标准基准数据集上进行的广泛实验表明，与现有最先进方法相比，PSNR、SSIM和感知质量均有显著改善。", "conclusion": "本文提出的基于持续学习的统一模型，通过其创新的组件和无配对恢复方法，有效解决了多种恶劣天气条件下的图像恢复问题，并在性能上超越了现有技术。", "translation": "图像受雾、雪、雨等不同恶劣天气条件污染后的恢复是一项具有挑战性的任务，因为天气条件具有多变性。现有的大多数方法都侧重于某一种特定的天气条件。然而，对于自动驾驶等应用，需要一个统一的模型来执行因不同天气条件损坏的图像恢复。我们提出了一种基于持续学习的方法，以构建一个统一的图像恢复框架。所提出的框架整合了三项关键创新：(1) 选择性核融合层，动态结合全局和局部特征，以实现鲁棒的自适应特征选择；(2) 弹性权重整合（EWC），以实现持续学习并减轻多任务恢复中的灾难性遗忘；(3) 一种新颖的循环对比损失，在域转换过程中增强特征辨别力同时保持语义一致性。此外，我们提出了一种无配对图像恢复方法，以减少所提方法对训练数据的依赖。在去雾、去雪和去雨任务的标准基准数据集上进行的广泛实验表明，PSNR、SSIM和感知质量均显著优于现有最先进方法。", "summary": "本文提出一个基于持续学习的统一模型，用于解决多种恶劣天气条件下的图像恢复问题。该模型通过结合选择性核融合层、弹性权重整合（EWC）和循环对比损失，有效处理去雾、去雪和去雨等任务，并采用无配对图像恢复方法减少对训练数据的依赖。实验证明，该模型在PSNR、SSIM和感知质量方面均优于现有技术。", "keywords": "持续学习, 图像恢复, 统一模型, 无配对学习, 恶劣天气", "comments": "该论文的创新点在于将持续学习范式引入图像恢复领域，旨在解决多任务统一模型中常见的灾难性遗忘问题，并利用无配对数据训练，大大降低了对大量配对数据集的依赖。这对于实际应用，尤其是在数据获取困难的场景下，具有重要意义。"}}
{"id": "2507.18976", "title": "Weighted least squares subdivision schemes for noisy data on triangular meshes", "authors": ["Costanza Conti", "Sergio López-Ureña", "Dionisio F. Yáñez"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18976v1", "summary": "This paper presents and analyses a new family of linear subdivision schemes\nto refine noisy data given on triangular meshes. The subdivision rules consist\nof locally fitting and evaluating a weighted least squares approximating\nfirst-degree polynomial. This type of rules, applicable to any type of\ntriangular grid, including finite grids or grids containing extraordinary\nvertices, are geometry-dependent which may result in non-uniform schemes. For\nthese new subdivision schemes, we are able to prove reproduction, approximation\norder, denoising capabilities and, for some special type of grids, convergence\nas well. Several numerical experiments demonstrate that their performance is\nsimilar to advanced local linear regression methods but their subdivision\nnature makes them suitable for use within a multiresolution context as well as\nto deal with noisy geometric data as shown with an example.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18976v1", "cate": "math.NA", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "用于三角网格上噪声数据的加权最小二乘细分方案", "tldr": "本文提出并分析了一种新的线性细分方案家族，用于细化三角网格上的噪声数据，该方案基于加权最小二乘多项式拟合，并被证明具有去噪能力、近似阶和收敛性。", "motivation": "在三角网格上细化噪声数据。", "method": "本文提出了一种新的线性细分方案家族，其细分规则包括局部拟合和评估一个加权最小二乘近似一次多项式。这些规则适用于任何类型的三角网格，包括有限网格或包含异常顶点的网格，并且是几何依赖的，可能导致非均匀方案。", "result": "该方案被证明具有再现性、近似阶、去噪能力，并且对于某些特殊类型的网格，还具有收敛性。数值实验表明，其性能与先进的局部线性回归方法相似。", "conclusion": "该方案的细分特性使其适用于多分辨率上下文以及处理噪声几何数据。", "translation": "本文提出并分析了一种新的线性细分方案家族，用于细化三角网格上给定的噪声数据。细分规则包括局部拟合和评估一个加权最小二乘近似一次多项式。这种类型的规则适用于任何类型的三角网格，包括有限网格或包含异常顶点的网格，并且是几何依赖的，这可能导致非均匀方案。对于这些新的细分方案，我们能够证明其再现性、近似阶、去噪能力，并且对于某些特殊类型的网格，还证明了收敛性。多项数值实验表明，它们的性能与先进的局部线性回归方法相似，但其细分特性使其适用于多分辨率上下文，并且能够处理噪声几何数据，如示例所示。", "summary": "本文介绍了一种新颖的线性细分方案，专门用于处理三角网格上的噪声数据。该方案的核心在于采用加权最小二乘法局部拟合一次多项式来生成细分规则。研究证明了该方案的再现性、近似阶、去噪能力以及在特定网格类型上的收敛性。数值实验表明其性能与先进的局部线性回归方法相当，并且由于其固有的细分特性，特别适用于多分辨率应用和噪声几何数据的处理。", "keywords": "加权最小二乘, 细分方案, 噪声数据, 三角网格, 去噪", "comments": "该论文提出了一种创新的加权最小二乘细分方案，其独特之处在于结合了几何依赖的规则，使其能够灵活应用于各种三角网格，包括包含异常顶点的网格。其在去噪能力和多分辨率上下文中的适用性是其重要贡献，为处理复杂的噪声几何数据提供了有效工具。"}}
{"id": "2507.18660", "title": "Fuzzy Theory in Computer Vision: A Review", "authors": ["Adilet Yerkin", "Ayan Igali", "Elnara Kadyrgali", "Maksat Shagyrov", "Malika Ziyada", "Muragul Muratbekova", "Pakizar Shamoi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Submitted to Journal of Intelligent and Fuzzy Systems for consideration (8 pages, 6 figures, 1 table)", "url": "http://arxiv.org/abs/2507.18660v1", "summary": "Computer vision applications are omnipresent nowadays. The current paper\nexplores the use of fuzzy logic in computer vision, stressing its role in\nhandling uncertainty, noise, and imprecision in image data. Fuzzy logic is able\nto model gradual transitions and human-like reasoning and provides a promising\napproach to computer vision. Fuzzy approaches offer a way to improve object\nrecognition, image segmentation, and feature extraction by providing more\nadaptable and interpretable solutions compared to traditional methods. We\ndiscuss key fuzzy techniques, including fuzzy clustering, fuzzy inference\nsystems, type-2 fuzzy sets, and fuzzy rule-based decision-making. The paper\nalso discusses various applications, including medical imaging, autonomous\nsystems, and industrial inspection. Additionally, we explore the integration of\nfuzzy logic with deep learning models such as convolutional neural networks\n(CNNs) to enhance performance in complex vision tasks. Finally, we examine\nemerging trends such as hybrid fuzzy-deep learning models and explainable AI.", "comment": "Submitted to Journal of Intelligent and Fuzzy Systems for\n  consideration (8 pages, 6 figures, 1 table)", "pdf_url": "http://arxiv.org/pdf/2507.18660v1", "cate": "cs.CV", "date": "2025-07-23", "updated": "2025-07-23", "AI": {"title_translation": "计算机视觉中的模糊理论：一篇综述", "tldr": "本文综述了模糊逻辑在计算机视觉中的应用，强调其在处理不确定性、噪声和不精确性方面的作用，并讨论了关键模糊技术、应用以及与深度学习的结合，展望了新兴趋势。", "motivation": "计算机视觉应用无处不在，但图像数据中存在不确定性、噪声和不精确性。本文旨在探讨模糊逻辑在计算机视觉中的应用，以应对这些挑战并提供更具适应性和可解释性的解决方案。", "method": "本文通过探讨模糊逻辑在计算机视觉中的应用，讨论了关键模糊技术（包括模糊聚类、模糊推理系统、2型模糊集和基于模糊规则的决策），并探讨了其在医疗成像、自主系统和工业检测等领域的各种应用。此外，还探讨了模糊逻辑与深度学习模型（如卷积神经网络）的集成，并审视了新兴趋势。", "result": "模糊方法能够通过提供比传统方法更具适应性和可解释性的解决方案，来改进目标识别、图像分割和特征提取。模糊逻辑与深度学习模型的结合可以增强复杂视觉任务的性能。", "conclusion": "模糊逻辑为计算机视觉提供了一种有前景的方法，能够处理不确定性、噪声和不精确性，并通过与深度学习结合来增强性能，具有广阔的应用前景。", "translation": "计算机视觉应用如今无处不在。本文探讨了模糊逻辑在计算机视觉中的应用，强调其在处理图像数据中的不确定性、噪声和不精确性方面的作用。模糊逻辑能够模拟渐变过渡和类人推理，为计算机视觉提供了一种有前景的方法。与传统方法相比，模糊方法通过提供更具适应性和可解释性的解决方案，改进了目标识别、图像分割和特征提取。我们讨论了关键的模糊技术，包括模糊聚类、模糊推理系统、2型模糊集和基于模糊规则的决策。本文还讨论了各种应用，包括医疗成像、自主系统和工业检测。此外，我们探讨了模糊逻辑与深度学习模型（如卷积神经网络）的集成，以增强复杂视觉任务的性能。最后，我们研究了新兴趋势，如混合模糊-深度学习模型和可解释人工智能。", "summary": "本文综述了模糊逻辑在计算机视觉领域的应用，重点阐述了其在处理图像数据不确定性、噪声和不精确性方面的优势。文章探讨了模糊聚类、模糊推理系统等核心模糊技术，并讨论了其在医疗成像、自主系统等领域的具体应用。此外，论文还分析了模糊逻辑与深度学习（如CNNs）的融合，以及混合模型和可解释AI等新兴趋势，强调了模糊方法在提升计算机视觉任务性能和可解释性方面的潜力。", "keywords": "模糊逻辑, 计算机视觉, 不确定性, 深度学习, 综述", "comments": "这是一篇综述性文章，系统性地梳理了模糊理论在计算机视觉中的应用。其重要性在于为研究人员提供了一个全面的视角，了解模糊逻辑如何解决计算机视觉中的核心挑战（如不确定性），并指出了其与新兴深度学习技术结合的潜力，这对于推动可解释AI和鲁棒视觉系统具有重要意义。"}}
{"id": "2507.19287", "title": "The Case for Time-Shared Computing Resources", "authors": ["Pierre Jacquet", "Adrien Luxey-Bitri"], "categories": ["cs.DC"], "primary_category": "Subjects:       Distributed, Parallel, and Cluster Computing (cs.DC)", "pdf_link": null, "comments": "Comments:      Post-proceedings paper presented at LIMITS 2025: 11th Workshop on Computing within Limits, 2025-06-26/27, Online", "url": "http://arxiv.org/abs/2507.19287v2", "summary": "The environmental impact of Information and Communication Technologies (ICT)\ncontinues to grow, driven notably by increasing usage, rebound effects, and\nemerging demands. However, despite the virtual nature of its services, the\nsector remains inherently constrained by its materiality and cannot rely on an\ninfinite pool of resources. As a result, the wide variety of supported services\nmay need to be managed under stricter limits within hosting facilities in the\nfuture. Contrary to common assumptions, we show that tenants typically do not\nshare computing resources, even in environments commonly perceived as\nmutualized, such as cloud platforms. Time-sharing has been progressively phased\nout for reasons of performance, security, predictability, and, perhaps more\nimportantly, due to the decreasing cost of computing resources. This paper\nadvocates for managing fewer physical resources by improving resource sharing\nbetween tenants. It represents a paradigm shift, moving beyond traditional\ntime-sharing at the hardware level to a higher abstraction. This approach\nentails \"doing with fewer resources\" under conditions of \"reduced performance\".\nNonetheless, enhancing the mutualization of infrastructure can reduce cluster\nsizes (through consolidation) and improve energy efficiency, with gains related\nto the accepted performance trade-off, a situation potentially more socially\nacceptable than eliminating services. We review the current state of the art,\nidentify challenges and opportunities, propose interpretations of Time-Shared\nComputing, and outline key research directions.", "comment": "Post-proceedings paper presented at LIMITS 2025: 11th Workshop on\n  Computing within Limits, 2025-06-26/27, Online", "pdf_url": "http://arxiv.org/pdf/2507.19287v2", "cate": "cs.DC", "date": "2025-07-25", "updated": "2025-07-28", "AI": {"title_translation": "分时计算资源的案例", "tldr": "鉴于ICT日益增长的环境影响和资源限制，本文主张通过提高租户之间的资源共享来管理更少的物理资源，这是一种超越传统硬件级分时的新范式，以实现更高的能效和资源整合，即使这意味着性能有所下降。", "motivation": "信息和通信技术（ICT）对环境的影响持续增长，主要原因包括使用量增加、反弹效应和新兴需求。尽管服务是虚拟的，但该行业仍受其物质性的固有限制，无法依赖无限的资源池。因此，未来需要更严格地管理托管设施中支持的各种服务。", "method": "本文主张通过改进租户间的资源共享来管理更少的物理资源，这代表了一种范式转变，超越了传统的硬件级分时，转向更高的抽象层次。这种方法涉及在“性能降低”的条件下“用更少的资源做事”。文章回顾了当前的技术现状，识别了挑战和机遇，提出了分时计算的解释，并概述了关键研究方向。", "result": "研究表明，租户通常不共享计算资源，即使在被普遍认为是共享的环境（如云平台）中也是如此。增强基础设施的共享化可以减少集群规模（通过整合）并提高能源效率，这些收益与可接受的性能权衡相关。", "conclusion": "本文倡导在ICT领域实现范式转变，通过提高租户间的资源共享来更有效地管理物理资源，即使这可能导致性能下降，但它能带来环境效益和资源整合，并可能比取消服务更具社会可接受性。", "translation": "信息和通信技术（ICT）对环境的影响持续增长，主要原因包括使用量增加、反弹效应和新兴需求。然而，尽管其服务具有虚拟性质，但该行业仍受其物质性的固有限制，无法依赖无限的资源池。因此，未来可能需要在托管设施中更严格地管理各种支持的服务。与普遍的假设相反，我们表明租户通常不共享计算资源，即使在通常被认为是共享的环境（如云平台）中也是如此。分时技术已因性能、安全性、可预测性以及可能更重要的是计算资源成本的下降而逐渐被淘汰。本文主张通过改进租户间的资源共享来管理更少的物理资源。这代表了一种范式转变，超越了传统的硬件级分时，转向更高的抽象层次。这种方法涉及在“性能降低”的条件下“用更少的资源做事”。尽管如此，增强基础设施的共享化可以减少集群规模（通过整合）并提高能源效率，这些收益与可接受的性能权衡相关，这种情况可能比取消服务更具社会可接受性。我们回顾了当前的技术现状，识别了挑战和机遇，提出了分时计算的解释，并概述了关键研究方向。", "summary": "鉴于ICT日益增长的环境影响和资源限制，本文探讨了计算资源管理的问题。研究指出，尽管普遍认为云平台是共享的，但租户通常不共享计算资源，且传统的分时技术已因多种原因被淘汰。文章提出了一种新的范式，即通过在更高抽象层次上改进资源共享来管理更少的物理资源，即使这意味着性能有所牺牲。这种方法旨在通过整合来减少集群规模并提高能源效率，从而应对环境挑战，并为未来的ICT资源管理提供关键研究方向。", "keywords": "分时计算, 资源共享, ICT环境影响, 能源效率, 云计算", "comments": "本文的创新之处在于，它在当前ICT环境影响日益增长的背景下，重新审视了“分时计算”的概念，并将其提升到更高的抽象层次。它挑战了云计算中资源共享的普遍假设，并提出了一个以“更少资源，更低性能”为代价，但能带来环境和能源效益的实用方法。其重要性在于，它为应对ICT的可持续发展挑战提供了一个新的视角和潜在解决方案，尤其是在资源日益紧张的未来。"}}
{"id": "2505.21461", "title": "Quasi Steady-State Frequency", "authors": ["Joan Gutierrez-Florensa", "Alvaro Ortega", "Lukas Sigrist", "Federico Milano"], "categories": ["eess.SY", "cs.SY"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2505.21461v2", "summary": "Accurate frequency estimation is critical for the control, monitoring and\nprotection of electrical power systems, in particular, of systems with a high\npenetration of power electronics. This paper introduces the novel concept of\nQuasi Steady-State (QSS) frequency as a quantity that fills the gap between\nstationary and instantaneous frequency. QSS frequency coincides with the\nfundamental frequency of an AC voltage in any stationary conditions, including\nunbalanced and non-sinusoidal, and is able to capture the time-varying\nfundamental frequency in transient conditions. The paper also proposes a metric\nborrowed from fluid dynamics, namely, the time derivative of the circulation,\nto define the scope of validity of the QSS frequency. Analytical examples as\nwell as a case study based on a fully-fledged EMT model of the IEEE 39-bus\nsystem serve to illustrate, respectively, the properties of the QSS frequency\nand its behavior in transient conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2505.21461v2", "cate": "eess.SY", "date": "2025-05-27", "updated": "2025-07-25", "AI": {"title_translation": "准稳态频率", "tldr": "本文提出了一种新的准稳态（QSS）频率概念，它介于稳态频率和瞬时频率之间，能够准确估计电力系统中的频率，尤其适用于高电力电子渗透的系统，并在稳态和暂态条件下表现良好，并通过流体动力学中的度量来定义其有效范围。", "motivation": "准确的频率估计对于电力系统的控制、监测和保护至关重要，特别是在电力电子渗透率高的系统中，现有的稳态和瞬时频率概念可能存在不足。", "method": "本文引入了准稳态（QSS）频率的新概念，旨在弥补稳态频率和瞬时频率之间的空白。QSS频率在任何稳态条件下（包括不平衡和非正弦条件）均与交流电压的基波频率一致，并能捕获暂态条件下的时变基波频率。论文还提出了一种借鉴流体动力学的度量，即环流的时间导数，来定义QSS频率的有效范围。", "result": "分析性示例和基于IEEE 39节点系统完整EMT模型的案例研究分别说明了QSS频率的特性及其在暂态条件下的行为。", "conclusion": "QSS频率是一种有效填补稳态和瞬时频率之间空白的频率估计量，能够准确处理稳态（包括不平衡和非正弦）和暂态条件下的频率变化，并通过流体动力学度量明确了其适用范围。", "translation": "准确的频率估计对于电力系统的控制、监测和保护至关重要，特别是在电力电子渗透率高的系统中。本文引入了准稳态（QSS）频率的新概念，它作为一种介于稳态频率和瞬时频率之间的量。QSS频率在任何稳态条件下，包括不平衡和非正弦条件下，均与交流电压的基波频率一致，并且能够在暂态条件下捕获时变的基波频率。本文还提出了一种借鉴流体动力学的度量，即环流的时间导数，来定义QSS频率的有效范围。分析性示例以及基于IEEE 39节点系统完整EMT模型的案例研究分别用于说明QSS频率的特性及其在暂态条件下的行为。", "summary": "本文提出了一种名为准稳态（QSS）频率的新概念，旨在解决电力系统中精确频率估计的问题，尤其是在高电力电子渗透率的系统中。QSS频率能够弥补稳态和瞬时频率之间的不足，在各种稳态条件下（包括不平衡和非正弦）与基波频率一致，并能有效捕捉暂态条件下的时变基波频率。此外，论文还引入了基于流体动力学概念的度量来界定QSS频率的适用范围。通过分析性示例和IEEE 39节点系统的案例研究，验证了QSS频率的特性及其在暂态条件下的表现。", "keywords": "准稳态频率, 频率估计, 电力系统, 暂态, 电力电子", "comments": "本文创新性地提出了准稳态频率的概念，有效填补了传统频率概念在现代电力系统（特别是高渗透率电力电子系统）中遇到的空白。通过引入流体动力学中的度量来定义其有效范围，增加了理论的严谨性。其能够处理非理想条件（不平衡、非正弦）和暂态条件的能力，使其在实际应用中具有重要价值。"}}
{"id": "2507.18969", "title": "EDPC: Accelerating Lossless Compression via Lightweight Probability Models and Decoupled Parallel Dataflow", "authors": ["Zeyi Lu", "Xiaoxiao Ma", "Yujun Huang", "Minxiao Chen", "Bin Chen", "Baoyi An", "Shu-Tao Xia"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18969v1", "summary": "The explosive growth of multi-source multimedia data has significantly\nincreased the demands for transmission and storage, placing substantial\npressure on bandwidth and storage infrastructures. While Autoregressive\nCompression Models (ACMs) have markedly improved compression efficiency through\nprobabilistic prediction, current approaches remain constrained by two critical\nlimitations: suboptimal compression ratios due to insufficient fine-grained\nfeature extraction during probability modeling, and real-time processing\nbottlenecks caused by high resource consumption and low compression speeds. To\naddress these challenges, we propose Efficient Dual-path Parallel Compression\n(EDPC), a hierarchically optimized compression framework that synergistically\nenhances modeling capability and execution efficiency via coordinated dual-path\noperations. At the modeling level, we introduce the Information Flow Refinement\n(IFR) metric grounded in mutual information theory, and design a Multi-path\nByte Refinement Block (MBRB) to strengthen cross-byte dependency modeling via\nheterogeneous feature propagation. At the system level, we develop a Latent\nTransformation Engine (LTE) for compact high-dimensional feature representation\nand a Decoupled Pipeline Compression Architecture (DPCA) to eliminate\nencoding-decoding latency through pipelined parallelization. Experimental\nresults demonstrate that EDPC achieves comprehensive improvements over\nstate-of-the-art methods, including a 2.7x faster compression speed, and a 3.2%\nhigher compression ratio. These advancements establish EDPC as an efficient\nsolution for real-time processing of large-scale multimedia data in\nbandwidth-constrained scenarios. Our code is available at\nhttps://github.com/Magie0/EDPC.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18969v1", "cate": "cs.IT", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "EDPC：通过轻量级概率模型和解耦并行数据流加速无损压缩", "tldr": "EDPC是一种分层优化的压缩框架，通过协同双路径操作显著提高了无损压缩的建模能力和执行效率，实现了更快的压缩速度和更高的压缩比。", "motivation": "多源多媒体数据的爆炸式增长导致传输和存储需求显著增加，给带宽和存储基础设施带来巨大压力。现有的自回归压缩模型（ACMs）在压缩效率上仍受限于两个关键瓶颈：由于概率建模过程中细粒度特征提取不足导致的次优压缩比，以及高资源消耗和低压缩速度引起的实时处理瓶颈。", "method": "我们提出了高效双路径并行压缩（EDPC）框架。在建模层面，引入了基于互信息理论的信息流细化（IFR）度量，并设计了多路径字节细化块（MBRB）以通过异构特征传播增强跨字节依赖建模。在系统层面，开发了用于紧凑高维特征表示的潜在变换引擎（LTE）和用于通过流水线并行化消除编解码延迟的解耦流水线压缩架构（DPCA）。", "result": "EDPC在压缩速度上比现有最先进方法快2.7倍，压缩比提高了3.2%。", "conclusion": "EDPC为带宽受限场景下大规模多媒体数据的实时处理提供了一种高效的解决方案。", "translation": "多源多媒体数据的爆炸式增长显著增加了传输和存储需求，给带宽和存储基础设施带来了巨大的压力。尽管自回归压缩模型（ACMs）通过概率预测显著提高了压缩效率，但当前方法仍受限于两个关键瓶制约：由于概率建模过程中细粒度特征提取不足导致的次优压缩比，以及高资源消耗和低压缩速度引起的实时处理瓶颈。为了解决这些挑战，我们提出了高效双路径并行压缩（EDPC），一个分层优化的压缩框架，通过协调的双路径操作协同增强建模能力和执行效率。在建模层面，我们引入了基于互信息理论的信息流细化（IFR）度量，并设计了多路径字节细化块（MBRB）以通过异构特征传播加强跨字节依赖建模。在系统层面，我们开发了用于紧凑高维特征表示的潜在变换引擎（LTE）和用于通过流水线并行化消除编解码延迟的解耦流水线压缩架构（DPCA）。实验结果表明，EDPC在各方面均优于现有最先进方法，包括压缩速度快2.7倍，压缩比提高3.2%。这些进步使EDPC成为在带宽受限场景下实时处理大规模多媒体数据的高效解决方案。我们的代码可在 https://github.com/Magie0/EDPC 获取。", "summary": "EDPC是一种针对无损压缩的创新框架，旨在解决现有方法在压缩比和处理速度上的不足。它通过双路径优化，在建模层引入信息流细化（IFR）和多路径字节细化块（MBRB）以增强特征提取和依赖建模；在系统层则开发了潜在变换引擎（LTE）和解耦流水线压缩架构（DPCA）以提高执行效率和消除延迟。实验证明，EDPC在压缩速度和压缩比上均显著优于现有技术，为大规模多媒体数据的实时处理提供了高效方案。", "keywords": "无损压缩, 并行数据流, 概率模型, 多媒体数据, 压缩效率", "comments": "EDPC的创新性在于其分层的双路径优化策略，同时解决了压缩效率和执行速度两大核心问题。通过结合信息论指导的建模优化和系统级的并行化设计，它为无损压缩领域带来了显著的性能提升。特别是在实时处理大规模数据方面，其提速效果非常突出，具有重要的实际应用价值。"}}
{"id": "2507.18957", "title": "SLICEMATE: Accurate and Scalable Static Program Slicing via LLM-Powered Agents", "authors": ["Jianming Chang", "Jieke Shi", "Yunbo Lyu", "Xin Zhou", "Lulu Wang", "Zhou Yang", "Bixin Li", "David Lo"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18957v1", "summary": "Static program slicing, which extracts the executable portions of a program\nthat affect the values at a specific location, supports many software analysis\ntasks such as debugging and security auditing. However, traditional slicing\ntools rely on computationally expensive reachability analysis over dependency\ngraphs, which struggle to scale to large programs and often fail to handle code\nwith incomplete syntax. Recently emerged learning-based methods, while more\nrobust to such cases, still fall short of achieving comparable performance to\ntraditional methods on well-formed code.\n  In this work, we propose SliceMate, a novel static program slicing solution\npowered by Large Language Model (LLM) agents. It bypasses the need for explicit\ndependency graph construction and achieving superior slicing accuracy.\nConcretely, SliceMate integrates three specialized agents: (1) a synthesis\nagent that produces candidate slices by incrementally expanding the scan scope\nacross functions and files guided by LLM-inferred dependencies; (2) a\nverification agent that performs conciseness and completeness checks of the\ncandidate slices, detecting missing or irrelevant statements; and (3) a\nrefinement agent that repairs the slices with minimal edits in accordance with\nthe verification results. These agents are orchestrated by a control module\nthat ensures timely convergence and outputs high-quality slices without manual\nintervention. For rigorous evaluation, we construct a new and high-quality\nbenchmark, SliceBench, comprising 2,200 manually annotated Java and Python\nprograms, with program lengths ranging from 5 to 8,577 lines, significantly\nlarger than those in existing slicing benchmarks. Experimental results show\nthat SliceMate greatly outperforms both traditional and learning-based slicing\ntools.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18957v1", "cate": "cs.SE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "SLICEMATE：通过大型语言模型驱动的代理实现准确且可扩展的静态程序切片", "tldr": "SliceMate是一个基于LLM代理的静态程序切片工具，它通过绕过依赖图构建实现了优于传统和学习方法的切片精度和可扩展性，并在新的大型基准测试中表现出色。", "motivation": "传统的静态程序切片工具依赖于计算昂贵的依赖图可达性分析，难以扩展到大型程序且无法处理不完整语法。新兴的基于学习的方法在处理这些情况时虽然更鲁棒，但在格式良好的代码上性能仍不足。", "method": "本文提出了SliceMate，一个由大型语言模型（LLM）代理驱动的静态程序切片解决方案。它通过三个专门的代理工作：(1) 合成代理，根据LLM推断的依赖关系逐步扩展扫描范围以生成候选切片；(2) 验证代理，检查候选切片的简洁性和完整性；(3) 精修代理，根据验证结果修复切片。这些代理由一个控制模块协调，以确保及时收敛并输出高质量切片。", "result": "为了严格评估，构建了一个包含2,200个手动标注的Java和Python程序的新高质量基准测试SliceBench。实验结果表明，SliceMate的性能显著优于传统的和基于学习的切片工具。", "conclusion": "SliceMate通过LLM驱动的代理克服了传统和基于学习的静态程序切片方法的局限性，实现了卓越的切片精度和可扩展性，并在大型基准测试中展现出优越的性能。", "translation": "静态程序切片从程序中提取影响特定位置值的可执行部分，支持调试和安全审计等多种软件分析任务。然而，传统的切片工具依赖于依赖图上计算昂贵的可达性分析，难以扩展到大型程序，并且通常无法处理语法不完整的代码。最近出现的基于学习的方法虽然在这种情况下更具鲁棒性，但在格式良好的代码上仍未能达到与传统方法相当的性能。\n在这项工作中，我们提出了SliceMate，一种由大型语言模型（LLM）代理驱动的新型静态程序切片解决方案。它绕过了显式依赖图构建的需要，并实现了卓越的切片精度。具体来说，SliceMate集成了三个专门的代理：(1) 一个合成代理，通过LLM推断的依赖关系指导，逐步扩展跨函数和文件的扫描范围来生成候选切片；(2) 一个验证代理，对候选切片执行简洁性和完整性检查，检测缺失或不相关的语句；(3) 一个精修代理，根据验证结果以最小的修改修复切片。这些代理由一个控制模块协调，确保及时收敛并输出高质量切片，无需人工干预。为了严格评估，我们构建了一个新的高质量基准测试SliceBench，包含2,200个手动标注的Java和Python程序，程序长度从5到8,577行不等，显著大于现有切片基准测试中的程序。实验结果表明，SliceMate大大优于传统的和基于学习的切片工具。", "summary": "本文提出SliceMate，一个创新的基于大型语言模型（LLM）代理的静态程序切片工具。它解决了传统方法计算成本高、扩展性差以及学习方法性能不足的问题。SliceMate通过合成、验证和精修三个代理协同工作，绕过依赖图构建，有效生成高质量程序切片。研究团队还构建了大型基准测试SliceBench进行评估，结果显示SliceMate显著优于现有切片工具，在精度和可扩展性方面均表现卓越。", "keywords": "静态程序切片, 大型语言模型, LLM代理, 程序分析, SliceMate", "comments": "SliceMate的创新之处在于利用LLM代理而非传统的依赖图分析进行程序切片，这有望解决现有工具在大型程序和不完整语法代码上的扩展性及鲁棒性问题。其代理协作和新基准测试的构建也增强了研究的严谨性和实用性。这项工作为软件分析领域提供了一个有前景的新方向。"}}
{"id": "2507.18731", "title": "Learning coupled Allen-Cahn and Cahn-Hilliard phase-field equations using Physics-informed neural operator(PINO)", "authors": ["Gaijinliu Gangmei", "Santu Rana", "Bernard Rolfe", "Kishalay Mitra", "Saswata Bhattacharyya"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "Comments:      6 pages, 4 figures, Accepted at the 4th Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE), 2025(non-archival)", "url": "http://arxiv.org/abs/2507.18731v1", "summary": "Phase-field equations, mostly solved numerically, are known for capturing the\nmesoscale microstructural evolution of a material. However, such numerical\nsolvers are computationally expensive as it needs to generate fine mesh systems\nto solve the complex Partial Differential Equations(PDEs) with good accuracy.\nTherefore, we propose an alternative approach of predicting the microstructural\nevolution subjected to periodic boundary conditions using Physics informed\nNeural Operators (PINOs).\n  In this study, we have demonstrated the capability of PINO to predict the\ngrowth of $\\theta^{\\prime}$ precipitates in Al-Cu alloys by learning the\noperator as well as by solving three coupled physics equations simultaneously.\nThe coupling is of two second-order Allen-Cahn equation and one fourth-order\nCahn-Hilliard equation. We also found that using Fourier\nderivatives(pseudo-spectral method and Fourier extension) instead of Finite\nDifference Method improved the Cahn-Hilliard equation loss by twelve orders of\nmagnitude. Moreover, since differentiation is equivalent to multiplication in\nthe Fourier domain, unlike Physics informed Neural Networks(PINNs), we can\neasily compute the fourth derivative of Cahn-Hilliard equation without\nconverting it to coupled second order derivative.", "comment": "6 pages, 4 figures, Accepted at the 4th Annual AAAI Workshop on AI to\n  Accelerate Science and Engineering (AI2ASE), 2025(non-archival)", "pdf_url": "http://arxiv.org/pdf/2507.18731v1", "cate": "cs.CE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "学习使用物理信息神经算子（PINO）耦合Allen-Cahn和Cahn-Hilliard相场方程", "tldr": "本文提出使用物理信息神经算子（PINO）来高效学习和预测耦合相场方程，显著提高了计算精度并简化了高阶导数计算。", "motivation": "传统的相场方程数值求解器计算成本高昂，需要生成精细网格来精确求解复杂的偏微分方程。", "method": "提出使用物理信息神经算子（PINO）来预测周期性边界条件下的微观结构演变。该方法通过学习算子同时求解两个二阶Allen-Cahn方程和一个四阶Cahn-Hilliard方程。研究中还发现，使用傅里叶导数（伪谱法和傅里叶扩展）代替有限差分法可以显著改善Cahn-Hilliard方程的损失。", "result": "PINO成功预测了Al-Cu合金中θ'沉淀物的生长。使用傅里叶导数使得Cahn-Hilliard方程的损失降低了十二个数量级。此外，傅里叶域中的微分等效于乘法，使得计算四阶Cahn-Hilliard导数变得容易，无需转换为耦合的二阶导数。", "conclusion": "PINO是一种有效且高效的方法，可以学习和求解复杂的耦合相场方程，特别是在处理高阶导数和提高计算精度方面表现出色。", "translation": "相场方程，主要通过数值方法求解，以捕捉材料的中尺度微观结构演变而闻名。然而，由于需要生成精细网格系统以高精度求解复杂的偏微分方程（PDEs），此类数值求解器计算成本高昂。因此，我们提出了一种替代方法，即使用物理信息神经算子（PINOs）来预测周期性边界条件下的微观结构演变。\n在这项研究中，我们展示了PINO通过学习算子以及同时求解三个耦合物理方程来预测Al-Cu合金中θ'沉淀物生长的能力。这些耦合方程包括两个二阶Allen-Cahn方程和一个四阶Cahn-Hilliard方程。我们还发现，使用傅里叶导数（伪谱法和傅里叶扩展）代替有限差分法将Cahn-Hilliard方程的损失提高了十二个数量级。此外，由于傅里叶域中的微分等同于乘法，与物理信息神经网络（PINNs）不同，我们可以轻松计算Cahn-Hilliard方程的四阶导数，而无需将其转换为耦合的二阶导数。", "summary": "本文提出了一种基于物理信息神经算子（PINO）的新方法，用于高效地学习和预测耦合Allen-Cahn和Cahn-Hilliard相场方程。该方法克服了传统数值求解器计算成本高昂的缺点，并通过在Al-Cu合金中预测θ'沉淀物生长来验证其有效性。研究表明，采用傅里叶导数能显著提高Cahn-Hilliard方程的精度，并简化了高阶导数的计算，这为相场模拟提供了一种高效且精确的替代方案。", "keywords": "物理信息神经算子, 相场方程, Allen-Cahn, Cahn-Hilliard, 傅里叶导数", "comments": "这篇论文的创新点在于将物理信息神经算子（PINO）应用于耦合相场方程的求解，并特别强调了傅里叶导数在提高精度和简化高阶导数计算方面的优势。这种方法为材料科学中的微观结构演变模拟提供了一个计算效率更高、精度更高的工具，具有重要的实际应用潜力。"}}
{"id": "2407.15226", "title": "Variational Bayesian Inference for Multiple Extended Targets or Unresolved Group Targets Tracking", "authors": ["Yuanhao Cheng", "Yunhe Cao", "Tat-Soon Yeo", "Yulin Zhang", "Fu Jie"], "categories": ["eess.SP", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      21 pages, 14 figures, 4 tables", "url": "http://arxiv.org/abs/2407.15226v4", "summary": "In this work, we propose a method for tracking multiple extended targets or\nunresolvable group targets in a clutter environment. Firstly, based on the\nRandom Matrix Model (RMM), the joint state of the target is modeled as the\nGamma Gaussian Inverse Wishart (GGIW) distribution. Considering the uncertainty\nof measurement origin caused by the clutters, we adopt the idea of\nprobabilistic data association and describe the joint association event as an\nunknown parameter in the joint prior distribution. Then the Variational\nBayesian Inference (VBI) is employed to approximately solve the non-analytical\nposterior distribution. Furthermore, to ensure the practicability of the\nproposed method, we further provide two potential lightweight schemes to reduce\nits computational complexity. One of them is based on clustering, which\neffectively prunes the joint association events. The other is a simplification\nof the variational posterior through marginal association probabilities.\nFinally, the effectiveness of the proposed method is demonstrated by simulation\nand real data experiments, and we show that the proposed method outperforms\ncurrent state-of-the-art methods in terms of accuracy and adaptability.", "comment": "21 pages, 14 figures, 4 tables", "pdf_url": "http://arxiv.org/pdf/2407.15226v4", "cate": "eess.SP", "date": "2024-07-21", "updated": "2025-07-25", "AI": {"title_translation": "变分贝叶斯推理用于多扩展目标或未分辨群目标跟踪", "tldr": "本文提出了一种基于变分贝叶斯推理的方法，用于在杂波环境下跟踪多扩展目标或未分辨群目标，并通过两种轻量化方案提高了实用性，在准确性和适应性上优于现有方法。", "motivation": "在杂波环境下跟踪多扩展目标或未分辨群目标是一个挑战，需要解决测量来源的不确定性以及复杂模型的计算开销问题。", "method": "首先，基于随机矩阵模型（RMM），将目标的联合状态建模为Gamma Gaussian Inverse Wishart (GGIW) 分布。其次，采用概率数据关联思想，将联合关联事件描述为联合先验分布中的未知参数，以处理测量来源的不确定性。然后，使用变分贝叶斯推理（VBI）近似求解非解析的后验分布。最后，提出两种轻量化方案：一种基于聚类来修剪联合关联事件；另一种通过边际关联概率简化变分后验，以降低计算复杂度。", "result": "仿真和真实数据实验证明了所提出方法的有效性，并且在准确性和适应性方面优于当前的最新方法。", "conclusion": "所提出的基于变分贝叶斯推理的多扩展目标或未分辨群目标跟踪方法在杂波环境下表现出优越的性能，并具有良好的实用性。", "translation": "在这项工作中，我们提出了一种在杂波环境中跟踪多个扩展目标或不可分辨群目标的方法。首先，基于随机矩阵模型（RMM），将目标的联合状态建模为伽马高斯逆威沙特（GGIW）分布。考虑到杂波引起的测量来源不确定性，我们采用了概率数据关联的思想，并将联合关联事件描述为联合先验分布中的一个未知参数。然后，采用变分贝叶斯推理（VBI）来近似求解非解析的后验分布。此外，为了确保所提出方法的实用性，我们进一步提供了两种潜在的轻量化方案来降低其计算复杂度。其中一种基于聚类，有效修剪了联合关联事件。另一种是通过边际关联概率简化变分后验。最后，通过仿真和真实数据实验证明了所提出方法的有效性，并且我们表明所提出方法在准确性和适应性方面优于当前最先进的方法。", "summary": "本文提出了一种在杂波环境下跟踪多扩展目标或未分辨群目标的变分贝叶斯推理方法。该方法基于随机矩阵模型，将目标状态建模为GGIW分布，并利用概率数据关联处理测量不确定性。为提高实用性，论文还提出了两种轻量化方案以降低计算复杂度。实验结果表明，该方法在准确性和适应性上优于现有先进方法。", "keywords": "变分贝叶斯推理, 扩展目标跟踪, 群目标跟踪, 随机矩阵模型, 概率数据关联", "comments": "这篇论文的创新点在于将变分贝叶斯推理应用于多扩展目标或未分辨群目标的跟踪问题，并结合了随机矩阵模型和概率数据关联。特别值得注意的是，为了解决VBI方法的计算复杂性问题，作者提出了两种实用的轻量化方案，这大大增强了方法的实用性。其在杂波环境下表现出的优越性能，使其在雷达、声纳等领域具有潜在应用价值。"}}
{"id": "2503.00151", "title": "Palm: A Culturally Inclusive and Linguistically Diverse Dataset for Arabic LLMs", "authors": ["Fakhraddin Alwajih", "Abdellah El Mekki", "Samar Mohamed Magdy", "Abdelrahim A. Elmadany", "Omer Nacar", "El Moatez Billah Nagoudi", "Reem Abdel-Salam", "Hanin Atwany", "Youssef Nafea", "Abdulfattah Mohammed Yahya", "Rahaf Alhamouri", "Hamzah A. Alsayadi", "Hiba Zayed", "Sara Shatnawi", "Serry Sibaee", "Yasir Ech-Chammakhy", "Walid Al-Dhabyani", "Marwa Mohamed Ali", "Imen Jarraya", "Ahmed Oumar El-Shangiti", "Aisha Alraeesi", "Mohammed Anwar Al-Ghrawi", "Abdulrahman S. Al-Batati", "Elgizouli Mohamed", "Noha Taha Elgindi", "Muhammed Saeed", "Houdaifa Atou", "Issam Ait Yahia", "Abdelhak Bouayad", "Mohammed Machrouh", "Amal Makouar", "Dania Alkawi", "Mukhtar Mohamed", "Safaa Taher Abdelfadil", "Amine Ziad Ounnoughene", "Rouabhia Anfel", "Rwaa Assi", "Ahmed Sorkatti", "Mohamedou Cheikh Tourad", "Anis Koubaa", "Ismail Berrada", "Mustafa Jarrar", "Shady Shehata", "Muhammad Abdul-Mageed"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      More information about our dataset is available at our project page: this https URL", "url": "http://arxiv.org/abs/2503.00151v2", "summary": "As large language models (LLMs) become increasingly integrated into daily\nlife, ensuring their cultural sensitivity and inclusivity is paramount. We\nintroduce our dataset, a year-long community-driven project covering all 22\nArab countries. The dataset includes instructions (input, response pairs) in\nboth Modern Standard Arabic (MSA) and dialectal Arabic (DA), spanning 20\ndiverse topics. Built by a team of 44 researchers across the Arab world, all of\nwhom are authors of this paper, our dataset offers a broad, inclusive\nperspective. We use our dataset to evaluate the cultural and dialectal\ncapabilities of several frontier LLMs, revealing notable limitations. For\ninstance, while closed-source LLMs generally exhibit strong performance, they\nare not without flaws, and smaller open-source models face greater challenges.\nMoreover, certain countries (e.g., Egypt, the UAE) appear better represented\nthan others (e.g., Iraq, Mauritania, Yemen). Our annotation guidelines, code,\nand data for reproducibility are publicly available.", "comment": "More information about our dataset is available at our project page:\n  https://github.com/UBC-NLP/palm", "pdf_url": "http://arxiv.org/pdf/2503.00151v2", "cate": "cs.CL", "date": "2025-02-28", "updated": "2025-07-24", "AI": {"title_translation": "Palm: 阿拉伯语大型语言模型的一个文化包容和语言多样性数据集", "tldr": "介绍了一个名为Palm的阿拉伯语LLM数据集，强调文化包容性和语言多样性，并揭示现有LLM在阿拉伯语文化和方言方面的局限性。", "motivation": "随着大型语言模型（LLMs）日益融入日常生活，确保其文化敏感性和包容性至关重要，尤其是在针对阿拉伯语世界时。", "method": "该论文介绍了一个名为Palm的数据集，这是一个历时一年的社区驱动项目，涵盖22个阿拉伯国家。数据集包含现代标准阿拉伯语（MSA）和阿拉伯语方言（DA）的指令（输入、响应对），涵盖20个不同主题。该数据集由来自阿拉伯世界的44名研究人员构建，用于评估前沿LLMs的文化和方言能力。", "result": "评估结果显示现有LLMs存在显著局限性。闭源LLMs表现虽强但仍有缺陷；较小的开源模型面临更大挑战。此外，某些国家（如埃及、阿联酋）的代表性优于其他国家（如伊拉克、毛里塔尼亚、也门）。", "conclusion": "该研究通过构建和使用Palm数据集，揭示了现有大型语言模型在处理阿拉伯语文化和方言方面的不足，强调了未来研究和数据集构建的重要性，以提升LLMs的文化包容性和语言多样性。数据集的公开可用性有助于研究的重现性。", "translation": "随着大型语言模型（LLMs）日益融入日常生活，确保其文化敏感性和包容性至关重要。我们推出了我们的数据集，这是一个历时一年的社区驱动项目，涵盖所有22个阿拉伯国家。该数据集包括现代标准阿拉伯语（MSA）和阿拉伯语方言（DA）的指令（输入、响应对），涵盖20个不同主题。由来自阿拉伯世界的44名研究人员（均为本文作者）组成的团队构建，我们的数据集提供了广泛、包容的视角。我们使用我们的数据集评估了几种前沿LLMs的文化和方言能力，揭示了显著的局限性。例如，虽然闭源LLMs通常表现强劲，但它们并非没有缺陷，而较小的开源模型面临更大的挑战。此外，某些国家（例如，埃及、阿联酋）的代表性似乎优于其他国家（例如，伊拉克、毛里塔尼亚、也门）。我们的注释指南、代码和数据均公开可用，以供重现性。", "summary": "该论文介绍了Palm数据集，一个历时一年、由22个阿拉伯国家44位研究员共同构建的、文化包容且语言多样化的阿拉伯语大型语言模型数据集。该数据集包含现代标准阿拉伯语和方言的指令，涵盖20个主题。利用该数据集对现有前沿LLM进行评估，发现其在文化和方言能力上存在显著局限，特别是小型开源模型和部分地区代表性不足的问题。数据集的注释指南、代码和数据已公开，以促进重现性。", "keywords": "阿拉伯语LLM, 数据集, 文化包容性, 语言多样性, Palm", "comments": "这项工作通过构建一个大规模、文化包容且语言多样化的阿拉伯语数据集，为解决现有LLM在阿拉伯世界特有语言和文化方面的局限性提供了重要基础。其创新性在于社区驱动的构建方式和对22个阿拉伯国家的全面覆盖，这对于推动阿拉伯语LLM的发展和确保其公平性具有重要意义。同时，该研究揭示了当前LLM的不足，为未来研究指明了方向。数据集的公开可用性也极大地促进了研究的透明度和可复现性。"}}
{"id": "2507.19373", "title": "Changes to the Facebook Algorithm Decreased News Visibility Between 2021-2024", "authors": ["Szymon Talaga", "Erin Wertz", "Dominik Batorski", "Magdalena Wojcieszak"], "categories": ["cs.SI"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19373v1", "summary": "Platforms, especially Facebook, are primary news sources in the US. In its\nwidely criticized \"War on News,\" Meta algorithmically deprioritized news and\npolitical content. We use data from 40 news organizations (5,243,302 Facebook\nposts, 7,875,372,958 user reactions) and 21 non-news pages (396,468 posts;\n1,909,088,308 reactions) between January 1, 2016 and February 13, 2025 to\nexamine how these changes influenced news visibility on the platform. Reactions\nto news declined by 78% between 2021 and 2024 while reactions to non-news pages\nincreased, indicating targeted suppression of news visibility. Low-quality\nsources were especially suppressed, yet the 2025 end to \"War on News\" increased\nuser reactions to news, especially low-quality ones. These changes do not\nreflect decreased news supply, Facebook user base, or interest in news over\nthis period.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19373v1", "cate": "cs.SI", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "2021-2024年间Facebook算法变化导致新闻可见度下降", "tldr": "研究发现，2021年至2024年间，Facebook算法调整导致新闻内容可见度显著下降，尤其针对低质量新闻源，但2025年政策变化后有所回升。", "motivation": "鉴于Facebook是美国主要的新闻来源之一，且Meta因其“新闻战”而受到广泛批评，本研究旨在检验Facebook算法变化如何影响新闻在该平台上的可见度。", "method": "本研究使用了来自40家新闻机构（5,243,302个Facebook帖子，7,875,372,958次用户反应）和21个非新闻页面（396,468个帖子；1,909,088,308次反应）的数据，时间范围从2016年1月1日至2025年2月13日，以分析算法变化的影响。", "result": "2021年至2024年间，新闻帖子的用户反应下降了78%，而非新闻页面的用户反应有所增加，这表明新闻可见度受到了有针对性的压制。低质量新闻来源尤其受到压制。然而，2025年“新闻战”结束后，新闻（尤其是低质量新闻）的用户反应有所增加。", "conclusion": "这些变化反映了Facebook对新闻可见度的有针对性压制，而非新闻供应减少、Facebook用户基数下降或用户对新闻兴趣减弱。2025年政策调整后，新闻可见度有所回升。", "translation": "平台，尤其是Facebook，是美国的主要新闻来源。在其广受批评的“新闻战”中，Meta通过算法降低了新闻和政治内容的优先级。我们使用来自40家新闻机构（5,243,302个Facebook帖子，7,875,372,958次用户反应）和21个非新闻页面（396,468个帖子；1,909,088,308次反应）在2016年1月1日至2025年2月13日期间的数据，来检验这些变化如何影响新闻在该平台上的可见度。2021年至2024年间，新闻的用户反应下降了78%，而非新闻页面的用户反应有所增加，这表明新闻可见度受到了有针对性的压制。低质量新闻来源尤其受到压制，但2025年“新闻战”结束后，新闻的用户反应，尤其是低质量新闻的反应有所增加。这些变化并不反映此期间新闻供应的减少、Facebook用户基数的缩小或用户对新闻兴趣的下降。", "summary": "本研究调查了Facebook算法在2021年至2024年间对新闻可见度的影响。通过分析大量新闻和非新闻页面的帖子及用户反应数据，发现新闻内容的反应量急剧下降78%，表明平台对新闻内容进行了有目的的算法降权，特别是针对低质量新闻源。研究强调，这种下降并非由于新闻供应、用户数量或兴趣的减少。此外，文章指出2025年“新闻战”结束后，新闻的反应量有所回升，尤其是低质量新闻，验证了算法干预的直接影响。", "keywords": "Facebook算法, 新闻可见度, 平台治理, 新闻战, 用户反应", "comments": "本研究通过大规模数据分析，量化了Facebook算法对新闻可见度的影响，揭示了平台在“新闻战”中对新闻内容进行压制的具体效果。其创新之处在于利用详实的数据对比新闻与非新闻内容的表现，并排除了其他潜在因素，有力地证明了算法干预是新闻可见度下降的主要原因。研究的重要性在于揭示了大型平台对信息传播的巨大影响力及其潜在的社会影响。"}}
{"id": "2507.19137", "title": "Assessment of Personality Dimensions Across Situations Using Conversational Speech", "authors": ["Alice Zhang", "Skanda Muralidhar", "Daniel Gatica-Perez", "Mathew Magimai-Doss"], "categories": ["eess.AS", "cs.AI", "cs.SD"], "primary_category": "Subjects:       Audio and Speech Processing (eess.AS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19137v1", "summary": "Prior research indicates that users prefer assistive technologies whose\npersonalities align with their own. This has sparked interest in automatic\npersonality perception (APP), which aims to predict an individual's perceived\npersonality traits. Previous studies in APP have treated personalities as\nstatic traits, independent of context. However, perceived personalities can\nvary by context and situation as shown in psychological research. In this\nstudy, we investigate the relationship between conversational speech and\nperceived personality for participants engaged in two work situations (a\nneutral interview and a stressful client interaction). Our key findings are: 1)\nperceived personalities differ significantly across interactions, 2) loudness,\nsound level, and spectral flux features are indicative of perceived\nextraversion, agreeableness, conscientiousness, and openness in neutral\ninteractions, while neuroticism correlates with these features in stressful\ncontexts, 3) handcrafted acoustic features and non-verbal features outperform\nspeaker embeddings in inference of perceived personality, and 4) stressful\ninteractions are more predictive of neuroticism, aligning with existing\npsychological research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19137v1", "cate": "eess.AS", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "使用对话语音评估跨情境的人格维度", "tldr": "研究表明，感知到的人格特质会因情境而异。本研究使用对话语音评估了在两种工作情境（中性面试和压力客户互动）中感知到的人格特质。结果显示人格特质在不同互动中显著不同，且特定声学特征与某些人格维度相关，特别是在压力情境下神经质的预测性更高。", "motivation": "先前的研究表明用户偏好与自身人格匹配的辅助技术，这激发了对自动人格感知（APP）的兴趣。然而，以往的APP研究将人格视为静态特质，忽略了心理学研究表明感知到的人格会随情境和环境变化的事实。因此，本研究旨在探究对话语音与感知人格在不同工作情境下的关系。", "method": "本研究调查了参与者在两种工作情境（中性面试和压力客户互动）中的对话语音与感知人格之间的关系。通过分析语音特征，如响度、声级、谱通量以及手工提取的声学特征和非语言特征，与感知到的人格特质进行关联分析。研究还比较了这些特征与说话人嵌入在预测感知人格方面的表现。", "result": "1) 感知到的人格在不同互动中显著不同；2) 响度、声级和谱通量特征在中性互动中预示着感知到的外向性、宜人性、责任心和开放性，而在压力情境中，神经质与这些特征相关；3) 手工提取的声学特征和非语言特征在推断感知人格方面优于说话人嵌入；4) 压力互动更能预测神经质，这与现有心理学研究一致。", "conclusion": "本研究证实感知到的人格特质并非静态，而是会随情境变化，并且对话语音中的特定声学特征可以有效预测不同情境下的人格维度。特别是在压力情境下，神经质的预测性更高，这为人格的动态评估提供了新的见解和方法。", "translation": "先前的研究表明，用户偏好与自己个性相符的辅助技术。这激发了人们对自动人格感知（APP）的兴趣，该技术旨在预测个体感知到的人格特质。以前的APP研究将人格视为静态特质，与上下文无关。然而，正如心理学研究所示，感知到的人格会因上下文和情境而异。在本研究中，我们调查了参与者在两种工作情境（中性面试和压力客户互动）中对话语音与感知人格之间的关系。我们的主要发现是：1）感知到的人格在不同互动中显著不同；2）响度、声级和谱通量特征在中性互动中预示着感知到的外向性、宜人性、责任心和开放性，而神经质在压力情境中与这些特征相关；3）手工提取的声学特征和非语言特征在推断感知人格方面优于说话人嵌入；4）压力互动更能预测神经质，这与现有心理学研究一致。", "summary": "本研究探讨了对话语音在不同情境下（中性面试和压力客户互动）对感知人格维度评估的有效性，挑战了以往APP研究中人格静态的假设。研究发现，感知人格会随情境显著变化，且特定声学和非语言特征（如响度、声级、谱通量）在中性情境下与外向性、宜人性、责任心和开放性相关，在压力情境下与神经质相关。此外，手工特征在预测感知人格方面优于说话人嵌入，且压力互动对神经质的预测性更强，这支持了人格的动态评估方法。", "keywords": "人格感知, 对话语音, 情境评估, 声学特征, 神经质", "comments": "这项研究的创新之处在于它挑战了自动人格感知（APP）领域中长期以来将人格视为静态特质的假设，首次在对话语音中探索了人格在不同情境下的动态变化。其重要性在于为开发更具情境感知和用户个性化匹配的辅助技术提供了新的方向。发现手工声学特征优于说话人嵌入，也为未来的APP模型开发提供了实用的指导。同时，强调了压力情境下神经质的预测性，为心理学和人机交互交叉领域的研究提供了宝贵的见解。"}}
{"id": "2507.16122", "title": "MLRU++: Multiscale Lightweight Residual UNETR++ with Attention for Efficient 3D Medical Image Segmentation", "authors": ["Nand Kumar Yadav", "Rodrigue Rizk", "William CW Chen", "KC Santosh"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.16122v3", "summary": "Accurate and efficient medical image segmentation is crucial but challenging\ndue to anatomical variability and high computational demands on volumetric\ndata. Recent hybrid CNN-Transformer architectures achieve state-of-the-art\nresults but add significant complexity. In this paper, we propose MLRU++, a\nMultiscale Lightweight Residual UNETR++ architecture designed to balance\nsegmentation accuracy and computational efficiency. It introduces two key\ninnovations: a Lightweight Channel and Bottleneck Attention Module (LCBAM) that\nenhances contextual feature encoding with minimal overhead, and a Multiscale\nBottleneck Block (M2B) in the decoder that captures fine-grained details via\nmulti-resolution feature aggregation. Experiments on four publicly available\nbenchmark datasets (Synapse, BTCV, ACDC, and Decathlon Lung) demonstrate that\nMLRU++ achieves state-of-the-art performance, with average Dice scores of\n87.57% (Synapse), 93.00% (ACDC), and 81.12% (Lung). Compared to existing\nleading models, MLRU++ improves Dice scores by 5.38% and 2.12% on Synapse and\nACDC, respectively, while significantly reducing parameter count and\ncomputational cost. Ablation studies evaluating LCBAM and M2B further confirm\nthe effectiveness of the proposed architectural components. Results suggest\nthat MLRU++ offers a practical and high-performing solution for 3D medical\nimage segmentation tasks. Source code is available at:\nhttps://github.com/1027865/MLRUPP", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.16122v3", "cate": "eess.IV", "date": "2025-07-22", "updated": "2025-07-25", "AI": {"title_translation": "MLRU++：用于高效3D医学图像分割的多尺度轻量级残差UNETR++与注意力机制", "tldr": "MLRU++是一种新的轻量级CNN-Transformer混合架构，通过引入LCBAM和M2B实现高效准确的3D医学图像分割，并在多个基准数据集上达到SOTA性能，同时显著降低计算成本。", "motivation": "准确高效的医学图像分割至关重要但具有挑战性，原因在于解剖学变异性和体数据的高计算需求。现有最先进的混合CNN-Transformer架构虽然效果好但复杂度高，增加了显著的计算负担。", "method": "本文提出了MLRU++，一种多尺度轻量级残差UNETR++架构，旨在平衡分割精度和计算效率。它引入了两个关键创新：轻量级通道和瓶颈注意力模块（LCBAM），以最小开销增强上下文特征编码；以及解码器中的多尺度瓶颈块（M2B），通过多分辨率特征聚合捕获细粒度细节。", "result": "在Synapse、BTCV、ACDC和Decathlon Lung四个公开基准数据集上进行实验。MLRU++实现了最先进的性能，在Synapse上的平均Dice分数为87.57%，ACDC上为93.00%，Lung上为81.12%。与现有领先模型相比，MLRU++在Synapse和ACDC上的Dice分数分别提高了5.38%和2.12%，同时显著减少了参数数量和计算成本。消融研究进一步证实了所提出架构组件的有效性。", "conclusion": "MLRU++为3D医学图像分割任务提供了一个实用且高性能的解决方案。", "translation": "准确高效的医学图像分割至关重要，但由于解剖学变异性和体数据的高计算需求而充满挑战。最近的混合CNN-Transformer架构取得了最先进的结果，但增加了显著的复杂性。在本文中，我们提出了MLRU++，一种多尺度轻量级残差UNETR++架构，旨在平衡分割精度和计算效率。它引入了两个关键创新：轻量级通道和瓶颈注意力模块（LCBAM），以最小开销增强上下文特征编码；以及解码器中的多尺度瓶颈块（M2B），通过多分辨率特征聚合捕获细粒度细节。在四个公开基准数据集（Synapse、BTCV、ACDC和Decathlon Lung）上的实验表明，MLRU++实现了最先进的性能，在Synapse上的平均Dice分数为87.57%，ACDC上为93.00%，Lung上为81.12%。与现有领先模型相比，MLRU++在Synapse和ACDC上的Dice分数分别提高了5.38%和2.12%，同时显著减少了参数数量和计算成本。评估LCBAM和M2B的消融研究进一步证实了所提出架构组件的有效性。结果表明，MLRU++为3D医学图像分割任务提供了一个实用且高性能的解决方案。源代码可在：https://github.com/1027865/MLRUPP 获取。", "summary": "本文提出MLRU++，一种多尺度轻量级残差UNETR++架构，旨在解决3D医学图像分割中精度与效率的平衡问题。通过引入轻量级通道和瓶颈注意力模块（LCBAM）及多尺度瓶颈块（M2B），MLRU++在保持高分割精度的同时显著降低了计算成本和参数量。在多个公开基准数据集上的实验结果表明，MLRU++在Dice分数上超越了现有SOTA模型，并提供了更高效的解决方案。", "keywords": "医学图像分割, 轻量级模型, UNETR++, 注意力机制, 多尺度", "comments": "该论文的创新点在于提出了MLRU++架构，通过结合轻量级注意力机制（LCBAM）和多尺度特征聚合（M2B）来优化UNETR++，在不牺牲性能的前提下显著提高了计算效率。这对于资源受限的医疗场景尤其重要，提供了一个实用的高性能解决方案。"}}
{"id": "2503.05996", "title": "Towards Improving Reward Design in RL: A Reward Alignment Metric for RL Practitioners", "authors": ["Calarina Muslimani", "Kerrick Johnstonbaugh", "Suyog Chandramouli", "Serena Booth", "W. Bradley Knox", "Matthew E. Taylor"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.05996v2", "summary": "Reinforcement learning agents are fundamentally limited by the quality of the\nreward functions they learn from, yet reward design is often overlooked under\nthe assumption that a well-defined reward is readily available. However, in\npractice, designing rewards is difficult, and even when specified, evaluating\ntheir correctness is equally problematic: how do we know if a reward function\nis correctly specified? In our work, we address these challenges by focusing on\nreward alignment -- assessing whether a reward function accurately encodes the\npreferences of a human stakeholder. As a concrete measure of reward alignment,\nwe introduce the Trajectory Alignment Coefficient to quantify the similarity\nbetween a human stakeholder's ranking of trajectory distributions and those\ninduced by a given reward function. We show that the Trajectory Alignment\nCoefficient exhibits desirable properties, such as not requiring access to a\nground truth reward, invariance to potential-based reward shaping, and\napplicability to online RL. Additionally, in an 11 -- person user study of RL\npractitioners, we found that access to the Trajectory Alignment Coefficient\nduring reward selection led to statistically significant improvements. Compared\nto relying only on reward functions, our metric reduced cognitive workload by\n1.5x, was preferred by 82% of users and increased the success rate of selecting\nreward functions that produced performant policies by 41%.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.05996v2", "cate": "cs.LG", "date": "2025-03-08", "updated": "2025-07-24", "AI": {"title_translation": "改进RL中的奖励设计：一个面向RL实践者的奖励对齐度量", "tldr": "本文提出了一种名为轨迹对齐系数（Trajectory Alignment Coefficient, TAC）的度量标准，用于评估强化学习中奖励函数与人类偏好的一致性，并通过用户研究证明其能显著改善奖励选择过程。", "motivation": "强化学习智能体的性能受限于奖励函数的质量，然而，奖励设计在实践中既困难又难以评估其正确性，缺乏一种有效的方法来判断奖励函数是否准确编码了人类的偏好。", "method": "本文引入了轨迹对齐系数（Trajectory Alignment Coefficient）来量化人类利益相关者对轨迹分布的排序与给定奖励函数所诱导的排序之间的相似性。该方法无需访问真实奖励，对基于势函数的奖励整形不变，并适用于在线强化学习。", "result": "在对11名RL实践者进行的用户研究中，使用轨迹对齐系数进行奖励选择显著提高了成功率。与仅依赖奖励函数相比，该指标将认知负荷降低了1.5倍，82%的用户更倾向于使用它，并且选择能够产生高性能策略的奖励函数的成功率提高了41%。", "conclusion": "轨迹对齐系数是一种有效的奖励对齐度量，能够帮助RL实践者更准确地设计和选择奖励函数，从而提高强化学习应用的成功率并减轻认知负担。", "translation": "强化学习智能体从其学习的奖励函数质量中受到根本限制，然而，奖励设计常常被忽视，假设存在一个定义明确的奖励是现成的。然而，在实践中，设计奖励是困难的，即使指定了奖励，评估其正确性也同样有问题：我们如何知道奖励函数是否正确指定？在我们的工作中，我们通过关注奖励对齐来解决这些挑战——评估奖励函数是否准确编码了人类利益相关者的偏好。作为奖励对齐的具体衡量标准，我们引入了轨迹对齐系数（Trajectory Alignment Coefficient）来量化人类利益相关者对轨迹分布的排序与给定奖励函数所诱导的排序之间的相似性。我们表明，轨迹对齐系数具有理想的特性，例如不需要访问真实奖励、对基于势函数的奖励整形不变以及适用于在线强化学习。此外，在一项针对11名RL实践者的用户研究中，我们发现，在奖励选择过程中使用轨迹对齐系数能够带来统计学上的显著改进。与仅依赖奖励函数相比，我们的度量将认知负荷降低了1.5倍，82%的用户更倾向于使用它，并且选择能够产生高性能策略的奖励函数的成功率提高了41%。", "summary": "本文针对强化学习中奖励设计和评估的挑战，提出了一种新的奖励对齐度量——轨迹对齐系数（Trajectory Alignment Coefficient, TAC）。TAC通过量化人类对轨迹分布的偏好与奖励函数诱导的偏好之间的相似性，来评估奖励函数是否准确反映了人类意图。TAC具有无需真实奖励、对奖励整形不变且适用于在线RL的优点。一项用户研究表明，TAC能显著降低RL实践者的认知负荷，提高奖励选择的成功率，从而有效改进奖励设计过程。", "keywords": "强化学习, 奖励设计, 奖励对齐, 轨迹对齐系数, 人类偏好", "comments": "这项工作通过引入轨迹对齐系数，为强化学习中的奖励设计和评估提供了一个量化且实用的工具。其创新之处在于提出了一个不依赖于真实奖励且能直接衡量奖励与人类偏好一致性的度量。在实践中，奖励设计是RL应用落地的主要障碍之一，这项研究通过提供一个可操作的指标，显著降低了奖励设计的复杂性和不确定性，对RL实践者具有重要价值。"}}
{"id": "2507.18815", "title": "Deepfake Detection Via Facial Feature Extraction and Modeling", "authors": ["Benjamin Carter", "Nathan Dilla", "Micheal Callahan", "Atuhaire Ambala"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Keywords: deepfake, facial recognition, feature extraction, artificial intelligence, recurrent neural network, convolutional neural network, artificial neural network", "url": "http://arxiv.org/abs/2507.18815v1", "summary": "The rise of deepfake technology brings forth new questions about the\nauthenticity of various forms of media found online today. Videos and images\ngenerated by artificial intelligence (AI) have become increasingly more\ndifficult to differentiate from genuine media, resulting in the need for new\nmodels to detect artificially-generated media. While many models have attempted\nto solve this, most focus on direct image processing, adapting a convolutional\nneural network (CNN) or a recurrent neural network (RNN) that directly\ninteracts with the video image data. This paper introduces an approach of using\nsolely facial landmarks for deepfake detection. Using a dataset consisting of\nboth deepfake and genuine videos of human faces, this paper describes an\napproach for extracting facial landmarks for deepfake detection, focusing on\nidentifying subtle inconsistencies in facial movements instead of raw image\nprocessing. Experimental results demonstrated that this feature extraction\ntechnique is effective in various neural network models, with the same facial\nlandmarks tested on three neural network models, with promising performance\nmetrics indicating its potential for real-world applications. The findings\ndiscussed in this paper include RNN and artificial neural network (ANN) models\nwith accuracy between 96% and 93%, respectively, with a CNN model hovering\naround 78%. This research challenges the assumption that raw image processing\nis necessary to identify deepfake videos by presenting a facial feature\nextraction approach compatible with various neural network models while\nrequiring fewer parameters.", "comment": "Keywords: deepfake, facial recognition, feature extraction,\n  artificial intelligence, recurrent neural network, convolutional neural\n  network, artificial neural network", "pdf_url": "http://arxiv.org/pdf/2507.18815v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "深度伪造检测：基于面部特征提取与建模", "tldr": "本文提出一种通过提取面部特征点而非直接图像处理来检测深度伪造视频的方法，并在多种神经网络模型中取得了良好效果。", "motivation": "深度伪造技术的发展使得在线媒体的真实性受到质疑，AI生成的视频和图像越来越难以与真实媒体区分，因此需要新的模型来检测人工生成媒体。现有模型多依赖直接图像处理，但本文挑战了这种假设。", "method": "本文提出一种仅使用面部特征点进行深度伪造检测的方法。通过从包含深度伪造和真实人脸视频的数据集中提取面部特征点，专注于识别面部运动中的细微不一致性。该方法在RNN、ANN和CNN三种神经网络模型上进行了测试。", "result": "实验结果表明，该特征提取技术在各种神经网络模型中有效。RNN和ANN模型的准确率分别达到96%和93%，CNN模型约为78%。该方法所需的参数更少。", "conclusion": "通过提出一种兼容多种神经网络模型且所需参数更少的面部特征提取方法，本文挑战了识别深度伪造视频需要原始图像处理的假设。", "translation": "深度伪造技术的兴起带来了关于当今在线各种媒体形式真实性的新问题。人工智能（AI）生成的视频和图像越来越难以与真实媒体区分，这导致了需要新的模型来检测人工生成的媒体。虽然许多模型试图解决这个问题，但大多数都侧重于直接图像处理，采用卷积神经网络（CNN）或循环神经网络（RNN）直接与视频图像数据交互。本文介绍了一种仅使用面部特征点进行深度伪造检测的方法。通过使用包含深度伪造和真实人脸视频的数据集，本文描述了一种用于深度伪造检测的面部特征点提取方法，重点在于识别面部运动中细微的不一致性，而不是原始图像处理。实验结果表明，这种特征提取技术在各种神经网络模型中都是有效的，相同的面部特征点在三种神经网络模型上进行了测试，其有希望的性能指标表明了其在实际应用中的潜力。本文讨论的发现包括RNN和人工神经网络（ANN）模型的准确率分别在96%和93%之间，而CNN模型徘徊在78%左右。这项研究通过提出一种兼容各种神经网络模型且所需参数更少的面部特征提取方法，挑战了识别深度伪造视频需要原始图像处理的假设。", "summary": "本文提出一种新颖的深度伪造检测方法，通过提取和分析面部特征点来识别面部运动中的细微不一致性，而非依赖传统的原始图像处理。该方法在RNN和ANN模型上分别取得了96%和93%的准确率，并证明了其在多种神经网络模型中的有效性及参数效率，挑战了深度伪造检测必须依赖原始图像处理的传统观念。", "keywords": "深度伪造检测, 面部特征提取, 神经网络, 运动不一致性, 人工智能媒体", "comments": "本文的创新点在于提出了一个不依赖于原始图像处理，而是通过分析面部特征点来检测深度伪造的方法，这降低了模型的复杂性和参数需求。其重要性在于为深度伪造检测提供了一个新的视角和更高效的潜在解决方案，特别是在处理计算资源有限的场景下。虽然CNN模型的准确率相对较低，但RNN和ANN的优异表现证明了该特征提取方法的潜力。"}}
{"id": "2210.00541", "title": "Semi-autonomous Prosthesis Control Using Minimal Depth Information and Vibrotactile Feedback", "authors": ["Miguel Nobre Castro", "Strahinja Dosen"], "categories": ["cs.CV", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2210.00541v2", "summary": "Semi-autonomous prosthesis controllers based on computer vision improve\nperformance while reducing cognitive effort. However, controllers relying on\nfull-depth data face challenges in being deployed as embedded prosthesis\ncontrollers due to the computational demands of processing point clouds. To\naddress this, the present study proposes a method to reconstruct the shape of\nvarious daily objects from minimal depth data. This is achieved using four\nconcurrent laser scanner lines instead of a full point cloud. These lines\nrepresent the partial contours of an object's cross-section, enabling its\ndimensions and orientation to be reconstructed using simple geometry. A control\nprototype was implemented using a depth sensor with four laser scanners.\nVibrotactile feedback was also designed to help users to correctly aim the\nsensor at target objects. Ten able-bodied volunteers used a prosthesis equipped\nwith the novel controller to grasp ten objects of varying shapes, sizes, and\norientations. For comparison, they also tested an existing benchmark controller\nthat used full-depth information. The results showed that the novel controller\nhandled all objects and, while performance improved with training, it remained\nslightly below that of the benchmark. This marks an important step towards a\ncompact vision-based system for embedded depth sensing in prosthesis grasping.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2210.00541v2", "cate": "cs.CV", "date": "2022-10-02", "updated": "2025-07-24", "AI": {"title_translation": "使用最小深度信息和振动触觉反馈的半自主假肢控制", "tldr": "本研究提出了一种使用少量激光扫描线而不是完整点云来重建物体形状的假肢控制方法，以减少计算需求，并结合振动触觉反馈。实验证明其有效，是实现紧凑型假肢视觉系统的关键一步。", "motivation": "基于计算机视觉的半自主假肢控制器虽然能提高性能并减少认知负荷，但依赖完整深度数据的控制器由于处理点云的计算需求，难以部署为嵌入式假肢控制器。", "method": "本研究提出了一种从最小深度数据重建各种日常物体形状的方法。该方法使用四条并发的激光扫描线代替完整的点云，这些扫描线代表物体横截面的部分轮廓，通过简单的几何学重建其尺寸和方向。同时设计了振动触觉反馈，以帮助用户正确瞄准目标物体。", "result": "新颖的控制器能够处理所有物体，并且性能随训练而提高，但略低于基准控制器。", "conclusion": "本研究标志着在假肢抓取中实现紧凑型视觉系统嵌入式深度感知的重要一步。", "translation": "半自主假肢控制器基于计算机视觉提高了性能，同时减少了认知努力。然而，依赖完整深度数据的控制器在作为嵌入式假肢控制器部署时面临挑战，因为处理点云的计算需求很高。为了解决这个问题，本研究提出了一种从最小深度数据重建各种日常物体形状的方法。这通过使用四条并发的激光扫描线而非完整的点云来实现。这些线代表了物体横截面的部分轮廓，使得可以使用简单的几何学重建其尺寸和方向。一个控制原型使用带有四个激光扫描仪的深度传感器实现。还设计了振动触觉反馈，以帮助用户正确瞄准目标物体。十名身体健全的志愿者使用配备新型控制器的假肢抓取了十个形状、大小和方向各异的物体。作为比较，他们还测试了使用完整深度信息的现有基准控制器。结果显示，新型控制器能够处理所有物体，并且虽然性能随训练而提高，但仍略低于基准控制器。这标志着朝着在假肢抓取中实现紧凑型视觉系统嵌入式深度感知迈出了重要一步。", "summary": "本研究旨在解决半自主假肢控制器中完整深度数据处理的计算负担问题。为此，提出了一种新方法，通过使用四条激光扫描线而非完整点云来重建物体形状，并利用简单的几何学确定尺寸和方向。该系统还整合了振动触觉反馈以辅助瞄准。实验结果表明，尽管该新型控制器性能略低于使用完整深度信息的基准控制器，但其成功处理了所有测试物体，并且性能随训练提升。这项工作为开发紧凑型嵌入式视觉假肢系统奠定了基础。", "keywords": "假肢控制, 最小深度信息, 振动触觉反馈, 物体重建, 嵌入式系统", "comments": "这项研究的创新之处在于其通过利用极简的深度信息（四条激光扫描线）来重建物体形状，从而显著降低了计算复杂性，使其更适合嵌入式假肢系统。结合振动触觉反馈辅助用户瞄准，也体现了以用户为中心的设计理念。尽管性能略低于基准，但其在资源受限环境下的可行性是其重要性所在，为未来小型化和高效的假肢控制系统铺平了道路。"}}
{"id": "2507.18750", "title": "CatchPhrase: EXPrompt-Guided Encoder Adaptation for Audio-to-Image Generation", "authors": ["Hyunwoo Oh", "SeungJu Cha", "Kwanyoung Lee", "Si-Woo Kim", "Dong-Jin Kim"], "categories": ["cs.MM", "cs.SD", "eess.AS"], "primary_category": "Subjects:       Multimedia (cs.MM)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18750v1", "summary": "We propose CatchPhrase, a novel audio-to-image generation framework designed\nto mitigate semantic misalignment between audio inputs and generated images.\nWhile recent advances in multi-modal encoders have enabled progress in\ncross-modal generation, ambiguity stemming from homographs and auditory\nillusions continues to hinder accurate alignment. To address this issue,\nCatchPhrase generates enriched cross-modal semantic prompts (EXPrompt Mining)\nfrom weak class labels by leveraging large language models (LLMs) and audio\ncaptioning models (ACMs). To address both class-level and instance-level\nmisalignment, we apply multi-modal filtering and retrieval to select the most\nsemantically aligned prompt for each audio sample (EXPrompt Selector). A\nlightweight mapping network is then trained to adapt pre-trained text-to-image\ngeneration models to audio input. Extensive experiments on multiple audio\nclassification datasets demonstrate that CatchPhrase improves audio-to-image\nalignment and consistently enhances generation quality by mitigating semantic\nmisalignment.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18750v1", "cate": "cs.MM", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "CatchPhrase: EXPrompt引导的编码器适应性音频到图像生成", "tldr": "CatchPhrase是一个新的音频到图像生成框架，通过利用大型语言模型和音频字幕模型生成丰富的跨模态语义提示（EXPrompt）来解决音频输入和生成图像之间的语义错位问题，从而提高生成质量。", "motivation": "现有跨模态生成方法在音频到图像生成中存在语义错位问题，例如同音异义词和听觉错觉导致的模糊性，阻碍了准确的对齐。", "method": "CatchPhrase通过以下步骤解决问题：1. EXPrompt挖掘：利用大型语言模型（LLMs）和音频字幕模型（ACMs）从弱类别标签中生成丰富的跨模态语义提示（EXPrompt）。2. EXPrompt选择器：应用多模态过滤和检索，为每个音频样本选择语义上最对齐的提示，以解决类别级和实例级错位。3. 轻量级映射网络：训练一个轻量级映射网络，将预训练的文本到图像生成模型适应音频输入。", "result": "在多个音频分类数据集上的大量实验表明，CatchPhrase改进了音频到图像对齐，并通过缓解语义错位持续提高了生成质量。", "conclusion": "CatchPhrase通过其创新的EXPrompt生成和选择机制，有效解决了音频到图像生成中的语义错位问题，显著提升了生成图像的质量和语义准确性。", "translation": "我们提出了CatchPhrase，一个新颖的音频到图像生成框架，旨在缓解音频输入和生成图像之间的语义错位。尽管多模态编码器的最新进展推动了跨模态生成的进步，但源于同音异义词和听觉错觉的歧义仍然阻碍着准确的对齐。为了解决这个问题，CatchPhrase利用大型语言模型（LLMs）和音频字幕模型（ACMs）从弱类别标签中生成丰富的跨模态语义提示（EXPrompt挖掘）。为了解决类别级和实例级错位，我们应用多模态过滤和检索来为每个音频样本选择语义上最对齐的提示（EXPrompt选择器）。然后训练一个轻量级映射网络，以使预训练的文本到图像生成模型适应音频输入。在多个音频分类数据集上的大量实验表明，CatchPhrase通过缓解语义错位，改进了音频到图像对齐，并持续提高了生成质量。", "summary": "CatchPhrase是一个旨在解决音频到图像生成中语义错位问题的新框架。它通过“EXPrompt挖掘”利用大型语言模型和音频字幕模型从弱类别标签生成丰富的跨模态语义提示，并通过“EXPrompt选择器”进行多模态过滤和检索，为每个音频样本选择最佳提示。随后，一个轻量级映射网络用于将预训练的文本到图像模型适应音频输入。实验证明，该方法有效提高了音频到图像的对齐和生成质量。", "keywords": "音频到图像生成, 语义错位, EXPrompt, 大型语言模型, 音频字幕模型", "comments": "CatchPhrase的创新在于其EXPrompt生成和选择机制，有效利用LLMs和ACMs来丰富语义信息，并精确对齐跨模态数据，从而克服了传统音频到图像生成中语义模糊和错位的问题。这对于提升多模态内容生成的准确性和质量具有重要意义。"}}
{"id": "2507.19468", "title": "Back to the Features: DINO as a Foundation for Video World Models", "authors": ["Federico Baldassarre", "Marc Szafraniec", "Basile Terver", "Vasil Khalidov", "Francisco Massa", "Yann LeCun", "Patrick Labatut", "Maximilian Seitzer", "Piotr Bojanowski"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19468v1", "summary": "We present DINO-world, a powerful generalist video world model trained to\npredict future frames in the latent space of DINOv2. By leveraging a\npre-trained image encoder and training a future predictor on a large-scale\nuncurated video dataset, DINO-world learns the temporal dynamics of diverse\nscenes, from driving and indoor scenes to simulated environments. We show that\nDINO-world outperforms previous models on a variety of video prediction\nbenchmarks, e.g. segmentation and depth forecasting, and demonstrates strong\nunderstanding of intuitive physics. Furthermore, we show that it is possible to\nfine-tune the predictor on observation-action trajectories. The resulting\naction-conditioned world model can be used for planning by simulating candidate\ntrajectories in latent space.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19468v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "回归特征：DINO 作为视频世界模型的基础", "tldr": "DINO-world是一个基于DINOv2潜空间的通用视频世界模型，能预测未来帧，在视频预测任务上表现优异，并可用于动作条件下的规划。", "motivation": "旨在开发一个强大的通用视频世界模型，能够预测未来帧并理解复杂场景的动态。", "method": "提出了DINO-world模型，利用预训练的DINOv2图像编码器，并在大规模无策视频数据集上训练一个未来预测器。此外，该模型可以通过在观察-动作轨迹上进行微调，形成一个动作条件的世界模型，用于在潜空间中模拟轨迹进行规划。", "result": "DINO-world在多种视频预测基准（如分割和深度预测）上超越了现有模型，并展现出对直观物理的强大理解。其动作条件版本可用于规划。", "conclusion": "DINO-world通过利用DINOv2的潜空间，成功构建了一个高效且性能优越的视频世界模型，为视频预测和基于模型的规划提供了新途径。", "translation": "我们提出了DINO-world，一个强大的通用视频世界模型，它在DINOv2的潜在空间中训练以预测未来帧。通过利用预训练的图像编码器并在大规模未整理的视频数据集上训练一个未来预测器，DINO-world学习了从驾驶和室内场景到模拟环境等各种场景的时间动态。我们表明DINO-world在各种视频预测基准测试（例如分割和深度预测）上优于以前的模型，并展示了对直观物理的强大理解。此外，我们还表明可以在观察-动作轨迹上微调预测器。由此产生的动作条件世界模型可以通过在潜在空间中模拟候选轨迹来用于规划。", "summary": "DINO-world是一个创新的通用视频世界模型，它利用DINOv2的潜在空间预测未来帧。该模型通过在大规模视频数据集上训练未来预测器，学习了多样化场景的复杂时间动态。实验证明，DINO-world在视频预测任务上超越了现有模型，并对直观物理有深刻理解。此外，它还能通过微调实现动作条件下的规划能力，为视频理解和决策制定提供了强大工具。", "keywords": "视频世界模型, DINOv2, 未来帧预测, 潜在空间, 动作规划", "comments": "这篇论文的创新点在于将DINOv2的预训练图像编码器与视频世界模型相结合，利用其强大的特征表示能力进行未来帧预测。这种方法有效地利用了大规模无策视频数据，并实现了在多个视频预测任务上的SOTA性能。特别是，将模型扩展到动作条件规划显示了其在机器人和具身智能领域的巨大潜力。"}}
{"id": "2507.18943", "title": "Assessing the Reliability and Validity of a Balance Mat for Measuring Postural Stability: A Combined Robot-Human Approach", "authors": ["Abishek Shrestha", "Damith Herath", "Angie Fearon", "Maryam Ghahramani"], "categories": ["eess.SP", "cs.RO"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18943v1", "summary": "Postural sway assessment is important for detecting balance problems and\nidentifying people at risk of falls. Force plates (FP) are considered the gold\nstandard postural sway assessment method in laboratory conditions, but their\nlack of portability and requirement of high-level expertise limit their\nwidespread usage. This study evaluates the reliability and validity of a novel\nBalance Mat (BM) device, a low-cost portable alternative that uses optical\nfibre technology. The research includes two studies: a robot study and a human\nstudy. In the robot study, a UR10 robotic arm was used to obtain controlled\nsway patterns to assess the reliability and sensitivity of the BM. In the human\nstudy, 51 healthy young participants performed balance tasks on the BM in\ncombination with an FP to evaluate the BM's validity. Sway metrics such as sway\nmean, sway absolute mean, sway root mean square (RMS), sway path, sway range,\nand sway velocity were calculated from both BM and FP and compared. Reliability\nwas evaluated using the intra-class correlation coefficient (ICC), where values\ngreater than 0.9 were considered excellent and values between 0.75 and 0.9 were\nconsidered good. Results from the robot study demonstrated good to excellent\nICC values in both single and double-leg stances. The human study showed\nmoderate to strong correlations for sway path and range. Using Bland-Altman\nplots for agreement analysis revealed proportional bias between the BM and the\nFP where the BM overestimated sway metrics compared to the FP. Calibration was\nused to improve the agreement between the devices. The device demonstrated\nconsistent sway measurement across varied stance conditions, establishing both\nreliability and validity following appropriate calibration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18943v1", "cate": "eess.SP", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "评估平衡垫测量姿势稳定性的可靠性和有效性：机器人与人类结合的方法", "tldr": "本研究评估了一种新型低成本便携式平衡垫设备，通过机器人和人体研究证明其在适当校准后具有测量姿势稳定性的可靠性和有效性。", "motivation": "姿势摇摆评估对于发现平衡问题和识别跌倒风险人群很重要。传统的力板是“金标准”，但其缺乏便携性且需要高水平专业知识，限制了广泛使用。因此，需要一种低成本、便携的替代方案。", "method": "本研究评估了一种使用光纤技术的新型平衡垫（BM）设备的可靠性和有效性。研究包括两部分：1. 机器人研究：使用UR10机械臂获取受控摇摆模式，以评估BM的可靠性和灵敏度。2. 人类研究：51名健康的年轻参与者在BM上结合力板（FP）执行平衡任务，以评估BM的有效性。通过计算和比较BM和FP的摇摆指标（如摇摆均值、摇摆路径、摇摆速度等），并使用组内相关系数（ICC）评估可靠性，使用Bland-Altman图进行一致性分析来评估有效性，并通过校准改善设备间的一致性。", "result": "机器人研究表明，在单腿和双腿站立姿势下，平衡垫的ICC值表现良好到优秀（>0.75），显示出高可靠性。人类研究显示，平衡垫与力板在摇摆路径和范围上具有中度到强相关性。Bland-Altman图显示平衡垫与力板之间存在比例偏差，平衡垫高估了摇摆指标。经过校准后，该设备在不同站立条件下表现出一致的摇摆测量。", "conclusion": "经过适当校准后，该平衡垫设备在测量姿势稳定性方面表现出可靠性和有效性。", "translation": "姿势摇摆评估对于检测平衡问题和识别跌倒风险人群非常重要。力板（FP）在实验室条件下被认为是姿势摇摆评估的“金标准”方法，但其缺乏便携性且需要高水平专业知识，限制了其广泛使用。本研究评估了一种新型平衡垫（BM）设备的可靠性和有效性，这是一种使用光纤技术的低成本便携式替代方案。该研究包括两项研究：一项机器人研究和一项人类研究。在机器人研究中，使用UR10机械臂获取受控摇摆模式，以评估BM的可靠性和灵敏度。在人类研究中，51名健康的年轻参与者在BM上结合FP执行平衡任务，以评估BM的有效性。从BM和FP计算并比较了摇摆指标，如摇摆均值、摇摆绝对均值、摇摆均方根（RMS）、摇摆路径、摇摆范围和摇摆速度。可靠性使用组内相关系数（ICC）进行评估，其中大于0.9的值被认为是优秀，0.75到0.9之间的值被认为是良好。机器人研究的结果表明，在单腿和双腿站立姿势下，ICC值表现良好到优秀。人类研究显示摇摆路径和范围具有中度到强相关性。使用Bland-Altman图进行一致性分析显示BM与FP之间存在比例偏差，其中BM与FP相比高估了摇摆指标。使用校准来改善设备之间的一致性。该设备在不同站立条件下表现出一致的摇摆测量，在适当校准后确立了其可靠性和有效性。", "summary": "本文评估了一种新型低成本便携式平衡垫（BM）的可靠性和有效性，该设备采用光纤技术，旨在替代传统的力板。通过机器人和人类（51名健康年轻人）两项研究，分别测试了BM的可靠性（使用UR10机械臂生成受控摇摆）和有效性（与力板同步测量）。结果显示，机器人研究中BM在不同站姿下表现出良好至优秀的可靠性。人体研究表明BM与力板在摇摆路径和范围上存在中度到强相关性，但存在测量偏差，通过校准后，BM在测量姿势稳定性方面显示出一致的可靠性和有效性。", "keywords": "平衡垫, 姿势稳定性, 可靠性, 有效性, 力板", "comments": "这项研究通过结合机器人和人类研究，提供了一种全面评估新型平衡垫设备的方法，增强了其评估结果的严谨性。该平衡垫作为力板的低成本便携替代品，具有重要的临床应用潜力，尤其是在筛查跌倒风险人群方面。校准对于确保设备准确性至关重要，这表明新设备的实际应用可能需要额外的设置步骤。"}}
{"id": "2507.19174", "title": "Automatic Cough Analysis for Non-Small Cell Lung Cancer Detection", "authors": ["Chiara Giangregorio", "Cristina Maria Licciardello", "Vanja Miskovic", "Leonardo Provenzano", "Alessandra Laura Giulia Pedrocchi", "Andra Diana Dumitrascu", "Arsela Prelaj", "Marina Chiara Garassino", "Emilia Ambrosini", "Simona Ferrante"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      Emilia Ambrosini and Simona Ferrante equally contributed to the work", "url": "http://arxiv.org/abs/2507.19174v1", "summary": "Early detection of non-small cell lung cancer (NSCLC) is critical for\nimproving patient outcomes, and novel approaches are needed to facilitate early\ndiagnosis. In this study, we explore the use of automatic cough analysis as a\npre-screening tool for distinguishing between NSCLC patients and healthy\ncontrols. Cough audio recordings were prospectively acquired from a total of\n227 subjects, divided into NSCLC patients and healthy controls. The recordings\nwere analyzed using machine learning techniques, such as support vector machine\n(SVM) and XGBoost, as well as deep learning approaches, specifically\nconvolutional neural networks (CNN) and transfer learning with VGG16. To\nenhance the interpretability of the machine learning model, we utilized Shapley\nAdditive Explanations (SHAP). The fairness of the models across demographic\ngroups was assessed by comparing the performance of the best model across\ndifferent age groups (less than or equal to 58y and higher than 58y) and gender\nusing the equalized odds difference on the test set. The results demonstrate\nthat CNN achieves the best performance, with an accuracy of 0.83 on the test\nset. Nevertheless, SVM achieves slightly lower performances (accuracy of 0.76\nin validation and 0.78 in the test set), making it suitable in contexts with\nlow computational power. The use of SHAP for SVM interpretation further\nenhances model transparency, making it more trustworthy for clinical\napplications. Fairness analysis shows slightly higher disparity across age\n(0.15) than gender (0.09) on the test set. Therefore, to strengthen our\nfindings' reliability, a larger, more diverse, and unbiased dataset is needed\n-- particularly including individuals at risk of NSCLC and those in early\ndisease stages.", "comment": "Emilia Ambrosini and Simona Ferrante equally contributed to the work", "pdf_url": "http://arxiv.org/pdf/2507.19174v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "非小细胞肺癌检测的自动咳嗽分析", "tldr": "本研究探索使用自动咳嗽分析作为非小细胞肺癌的早期筛查工具，通过机器学习和深度学习方法分析咳嗽音频，并评估模型的性能、可解释性和公平性。", "motivation": "早期检测非小细胞肺癌对于改善患者预后至关重要，因此需要新的方法来促进早期诊断。", "method": "研究前瞻性地收集了227名受试者的咳嗽音频记录，包括非小细胞肺癌患者和健康对照组。使用支持向量机（SVM）、XGBoost、卷积神经网络（CNN）和VGG16迁移学习等机器学习和深度学习技术进行分析。通过Shapley Additive Explanations (SHAP) 增强模型可解释性。通过比较最佳模型在不同年龄组和性别间的表现，使用均衡赔率差异评估模型公平性。", "result": "CNN取得了最佳性能，在测试集上的准确率为0.83。SVM性能略低（验证集准确率0.76，测试集0.78），但在计算能力较低的环境中仍适用。SHAP用于SVM解释增强了模型透明度。公平性分析显示，年龄差异（0.15）略高于性别差异（0.09）。", "conclusion": "自动咳嗽分析作为非小细胞肺癌的预筛查工具具有潜力，CNN表现最佳，SVM在低计算环境下具有实用性。模型可解释性和公平性得到评估，但需要更大、更多样化和无偏的数据集来加强研究结果的可靠性。", "translation": "早期检测非小细胞肺癌（NSCLC）对于改善患者预后至关重要，需要新的方法来促进早期诊断。在本研究中，我们探索使用自动咳嗽分析作为预筛查工具，用于区分非小细胞肺癌患者和健康对照组。前瞻性地从总共227名受试者（分为NSCLC患者和健康对照组）中采集了咳嗽音频记录。使用机器学习技术（如支持向量机（SVM）和XGBoost）以及深度学习方法（特别是卷积神经网络（CNN）和VGG16迁移学习）对记录进行分析。为了增强机器学习模型的可解释性，我们使用了Shapley Additive Explanations (SHAP)。通过比较最佳模型在不同年龄组（小于等于58岁和大于58岁）和性别上的性能，使用测试集上的均衡赔率差异评估了模型在不同人口统计学群体间的公平性。结果表明，CNN取得了最佳性能，在测试集上的准确率为0.83。然而，SVM的性能略低（验证集准确率为0.76，测试集为0.78），使其适用于计算能力较低的环境。使用SHAP进行SVM解释进一步增强了模型透明度，使其在临床应用中更值得信赖。公平性分析显示，在测试集上，年龄差异（0.15）略高于性别差异（0.09）。因此，为了加强我们研究结果的可靠性，需要更大、更多样化和无偏的数据集——特别是包括有NSCLC风险的个体和处于疾病早期阶段的个体。", "summary": "本研究旨在探索自动咳嗽分析作为非小细胞肺癌（NSCLC）的早期预筛查工具。研究收集了227名受试者的咳嗽音频，并应用了多种机器学习和深度学习模型（如SVM、XGBoost、CNN、VGG16）进行分类。结果显示，CNN在测试集上取得了0.83的最佳准确率，而SVM在计算资源有限的情况下仍具实用性。为增强模型可信度，研究还利用SHAP提升了模型可解释性，并评估了模型在不同年龄和性别群体间的公平性。研究强调，为提高结果可靠性，未来需要更大、更多样化的数据集。", "keywords": "咳嗽分析, 非小细胞肺癌, 机器学习, 深度学习, 早期检测", "comments": "这项研究通过将自动咳嗽分析应用于非小细胞肺癌的早期检测，展示了其在无创、便捷筛查方面的潜力。其创新之处在于结合了多种机器学习和深度学习方法，并特别关注了模型的可解释性（使用SHAP）和公平性，这对于临床应用至关重要。虽然CNN表现最佳，但SVM在资源受限环境下的适用性也值得关注。该研究的局限性在于当前数据集的规模和多样性，未来需要更广泛的数据来验证和强化其发现。"}}
{"id": "2507.13558", "title": "Why Isn't Relational Learning Taking Over the World?", "authors": ["David Poole"], "categories": ["cs.AI", "cs.DB", "cs.LG"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "Comments:      10 pages (6 pages + references + appendices)", "url": "http://arxiv.org/abs/2507.13558v2", "summary": "AI seems to be taking over the world with systems that model pixels, words,\nand phonemes. The world is arguably made up, not of pixels, words, and phonemes\nbut of entities (objects, things, including events) with properties and\nrelations among them. Surely we should model these, not the perception or\ndescription of them. You might suspect that concentrating on modeling words and\npixels is because all of the (valuable) data in the world is in terms of text\nand images. If you look into almost any company you will find their most\nvaluable data is in spreadsheets, databases and other relational formats. These\nare not the form that are studied in introductory machine learning, but are\nfull of product numbers, student numbers, transaction numbers and other\nidentifiers that can't be interpreted naively as numbers. The field that\nstudies this sort of data has various names including relational learning,\nstatistical relational AI, and many others. This paper explains why relational\nlearning is not taking over the world -- except in a few cases with restricted\nrelations -- and what needs to be done to bring it to it's rightful prominence.", "comment": "10 pages (6 pages + references + appendices)", "pdf_url": "http://arxiv.org/pdf/2507.13558v2", "cate": "cs.AI", "date": "2025-07-17", "updated": "2025-07-25", "AI": {"title_translation": "为什么关系学习没有主导世界？", "tldr": "本文解释了为什么关系学习，尽管处理有价值的关系数据，却未能在人工智能领域占据主导地位。", "motivation": "人工智能系统主要建模像素、单词和音素，但世界本质上是由带有属性和关系实体组成的。尽管公司最有价值的数据是电子表格、数据库等关系格式，但关系学习领域并未在主流机器学习中得到广泛研究和应用。本文旨在解释关系学习未能主导世界的原因，并探讨如何使其获得应有的地位。", "method": "Not mentioned in abstract", "result": "本文解释了为什么关系学习没有主导世界，除了少数关系受限的特例。", "conclusion": "本文得出的结论是，需要采取措施才能使关系学习达到其应有的重要地位。", "translation": "人工智能似乎正在通过建模像素、单词和音素的系统主导世界。然而，世界可以说不是由像素、单词和音素组成的，而是由具有属性和相互关系的实体（对象、事物，包括事件）组成的。我们理应建模这些实体，而不是它们的感觉或描述。你可能会怀疑，之所以专注于建模单词和像素，是因为世界上所有（有价值的）数据都是以文本和图像的形式存在的。如果你考察几乎任何一家公司，你会发现他们最有价值的数据都在电子表格、数据库和其他关系格式中。这些并不是机器学习入门课程中研究的形式，但却充满了产品编号、学生编号、交易编号和其他不能被天真地解释为数字的标识符。研究这类数据的领域有各种名称，包括关系学习、统计关系人工智能等等。本文解释了为什么关系学习没有主导世界——除了少数关系受限的特例——以及需要做什么才能使其获得应有的突出地位。", "summary": "当前人工智能主要关注图像和文本数据，但现实世界和企业中最有价值的数据是结构化的关系数据。关系学习是处理这类数据的领域，但其影响力远不及主流AI。本文旨在分析关系学习未能普及的原因，并探讨如何提升其地位。", "keywords": "关系学习, 统计关系AI, 关系数据, 实体, 关系", "comments": "本文创新性地指出当前AI研究与实际最有价值数据形式之间的脱节，即AI侧重于感知数据，而企业核心数据多为关系型。其重要性在于，它促使人们反思关系学习的重要性及其未被充分利用的潜力，并为未来研究指明了方向，即如何弥补这一差距，使关系学习获得应有的 prominence。"}}
{"id": "2507.19407", "title": "Towards Domain Specification of Embedding Models in Medicine", "authors": ["Mohammad Khodadad", "Ali Shiraee", "Mahdi Astaraki", "Hamidreza Mahyar"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19407v1", "summary": "Medical text embedding models are foundational to a wide array of healthcare\napplications, ranging from clinical decision support and biomedical information\nretrieval to medical question answering, yet they remain hampered by two\ncritical shortcomings. First, most models are trained on a narrow slice of\nmedical and biological data, beside not being up to date in terms of\nmethodology, making them ill suited to capture the diversity of terminology and\nsemantics encountered in practice. Second, existing evaluations are often\ninadequate: even widely used benchmarks fail to generalize across the full\nspectrum of real world medical tasks.\n  To address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned\non diverse medical corpora through self-supervised contrastive learning across\nmultiple data sources, to deliver robust medical text embeddings.\n  Alongside this model, we propose a comprehensive benchmark suite of 51 tasks\nspanning classification, clustering, pair classification, and retrieval modeled\non the Massive Text Embedding Benchmark (MTEB) but tailored to the nuances of\nmedical text. Our results demonstrate that this combined approach not only\nestablishes a robust evaluation framework but also yields embeddings that\nconsistently outperform state of the art alternatives in different tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19407v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "医学嵌入模型领域规范化研究", "tldr": "针对医学文本嵌入模型训练数据狭窄和评估不足的问题，本文提出了一种基于MEDTE的鲁棒医学文本嵌入模型，并构建了一个包含51项任务的综合基准套件，实验证明其性能优于现有SOTA模型。", "motivation": "现有的医学文本嵌入模型存在两个主要缺陷：一是训练数据范围狭窄且方法过时，难以捕捉医学术语和语义的多样性；二是现有评估方法不足，无法在真实世界的医学任务中泛化。", "method": "本文利用MEDTE模型，通过多源自监督对比学习，在多样化的医学语料库上进行广泛微调，以生成鲁棒的医学文本嵌入。同时，提出了一套包含51项任务的综合基准套件，涵盖分类、聚类、配对分类和检索，该套件基于MTEB并针对医学文本的细微差别进行了定制。", "result": "这种结合方法不仅建立了一个鲁棒的评估框架，而且生成的嵌入在不同任务中始终优于现有最先进的替代方案。", "conclusion": "本文通过提出新的医学文本嵌入模型和全面的评估基准，有效解决了当前医学文本嵌入模型在数据多样性和评估泛化性方面的不足，并显著提升了性能。", "translation": "医学文本嵌入模型是广泛医疗保健应用的基础，包括临床决策支持、生物医学信息检索和医学问答，但它们仍受到两个关键缺陷的阻碍。首先，大多数模型是在狭窄的医学和生物数据切片上训练的，除了方法学上未更新外，这使得它们不适合捕捉实践中遇到的术语和语义多样性。其次，现有评估往往不足：即使是广泛使用的基准也未能推广到真实世界医学任务的完整范围。\n为了弥补这些空白，我们利用MEDTE，一个通过多数据源自监督对比学习在多样化医学语料库上广泛微调的GTE模型，来提供鲁棒的医学文本嵌入。\n除了这个模型，我们还提出了一个包含51项任务的综合基准套件，涵盖分类、聚类、配对分类和检索，该套件基于大规模文本嵌入基准（MTEB）但针对医学文本的细微差别进行了定制。我们的结果表明，这种结合方法不仅建立了一个鲁棒的评估框架，而且生成的嵌入在不同任务中始终优于现有最先进的替代方案。", "summary": "本文旨在解决当前医学文本嵌入模型在数据覆盖范围和评估通用性方面的局限性。为此，研究者提出了MEDTE模型，该模型通过在多样化医学语料库上进行自监督对比学习进行微调，以生成更具鲁棒性的医学嵌入。同时，他们还构建了一个包含51项专门针对医学文本的综合基准测试集。实验结果表明，这种结合方法不仅提供了一个强大的评估框架，而且其生成的嵌入在多项任务中均优于现有最先进的模型。", "keywords": "医学文本嵌入, 领域规范化, MEDTE, 基准测试, 对比学习", "comments": "这篇论文的创新点在于提出了一个专门针对医学领域的鲁棒文本嵌入模型MEDTE，并构建了一个大规模、多样化的医学文本评估基准。这对于推动医学AI应用的发展具有重要意义，因为它解决了现有模型在数据广度和评估真实性方面的痛点，有望提升临床决策支持、信息检索等应用的准确性和泛化能力。"}}
{"id": "2507.18926", "title": "Geometric Multi-color Message Passing Graph Neural Networks for Blood-brain Barrier Permeability Prediction", "authors": ["Trung Nguyen", "Md Masud Rana", "Farjana Tasnim Mukta", "Chang-Guo Zhan", "Duc Duy Nguyen"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18926v1", "summary": "Accurate prediction of blood-brain barrier permeability (BBBP) is essential\nfor central nervous system (CNS) drug development. While graph neural networks\n(GNNs) have advanced molecular property prediction, they often rely on\nmolecular topology and neglect the three-dimensional geometric information\ncrucial for modeling transport mechanisms. This paper introduces the geometric\nmulti-color message-passing graph neural network (GMC-MPNN), a novel framework\nthat enhances standard message-passing architectures by explicitly\nincorporating atomic-level geometric features and long-range interactions. Our\nmodel constructs weighted colored subgraphs based on atom types to capture the\nspatial relationships and chemical context that govern BBB permeability. We\nevaluated GMC-MPNN on three benchmark datasets for both classification and\nregression tasks, using rigorous scaffold-based splitting to ensure a robust\nassessment of generalization. The results demonstrate that GMC-MPNN\nconsistently outperforms existing state-of-the-art models, achieving superior\nperformance in both classifying compounds as permeable/non-permeable (AUC-ROC\nof 0.9704 and 0.9685) and in regressing continuous permeability values (RMSE of\n0.4609, Pearson correlation of 0.7759). An ablation study further quantified\nthe impact of specific atom-pair interactions, revealing that the model's\npredictive power derives from its ability to learn from both common and rare,\nbut chemically significant, functional motifs. By integrating spatial geometry\ninto the graph representation, GMC-MPNN sets a new performance benchmark and\noffers a more accurate and generalizable tool for drug discovery pipelines.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18926v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "几何多色消息传递图神经网络用于血脑屏障渗透性预测", "tldr": "提出了一种新的几何多色消息传递图神经网络（GMC-MPNN），通过整合原子级几何特征和长程相互作用，显著提高了血脑屏障渗透性预测的准确性。", "motivation": "准确预测血脑屏障渗透性（BBBP）对于中枢神经系统（CNS）药物开发至关重要。现有图神经网络（GNNs）通常依赖分子拓扑结构而忽略对传输机制至关重要的三维几何信息。", "method": "本文提出了几何多色消息传递图神经网络（GMC-MPNN），通过显式整合原子级几何特征和长程相互作用来增强标准消息传递架构。该模型基于原子类型构建加权彩色子图，以捕获控制BBBP的空间关系和化学上下文。", "result": "GMC-MPNN在三个基准数据集上，在分类和回归任务中均优于现有最先进模型，分类AUC-ROC分别为0.9704和0.9685，回归RMSE为0.4609，Pearson相关系数为0.7759。消融研究表明模型的预测能力源于学习常见和罕见但具有化学意义的功能基团。", "conclusion": "GMC-MPNN通过将空间几何整合到图表示中，为血脑屏障渗透性预测设定了新的性能基准，并为药物发现流程提供了更准确和泛化能力更强的工具。", "translation": "准确预测血脑屏障渗透性（BBBP）对于中枢神经系统（CNS）药物开发至关重要。尽管图神经网络（GNNs）在分子性质预测方面取得了进展，但它们通常依赖分子拓扑结构，并忽略了对建模传输机制至关重要的三维几何信息。本文引入了几何多色消息传递图神经网络（GMC-MPNN），这是一种新颖的框架，通过明确地整合原子级几何特征和长程相互作用来增强标准消息传递架构。我们的模型基于原子类型构建加权彩色子图，以捕获控制血脑屏障渗透性的空间关系和化学上下文。我们在三个基准数据集上对GMC-MPNN进行了分类和回归任务评估，采用严格的基于骨架的分割方法，以确保对泛化能力进行稳健评估。结果表明，GMC-MPNN始终优于现有最先进模型，在将化合物分类为可渗透/不可渗透（AUC-ROC分别为0.9704和0.9685）和回归连续渗透值（RMSE为0.4609，Pearson相关系数为0.7759）方面均取得了卓越的性能。消融研究进一步量化了特定原子对相互作用的影响，揭示了模型的预测能力源于其从常见和罕见但具有化学意义的功能基团中学习的能力。通过将空间几何整合到图表示中，GMC-MPNN设定了新的性能基准，并为药物发现流程提供了更准确和泛化能力更强的工具。", "summary": "本文提出了一种新颖的几何多色消息传递图神经网络（GMC-MPNN），旨在解决传统图神经网络在预测血脑屏障渗透性（BBBP）时忽略三维几何信息的问题。GMC-MPNN通过整合原子级几何特征和长程相互作用，并构建加权彩色子图来捕捉分子的空间和化学上下文。实验结果表明，GMC-MPNN在BBBP分类和回归任务上均显著优于现有SOTA模型，并提高了药物发现的准确性和泛化能力。", "keywords": "血脑屏障渗透性预测, 图神经网络, 几何特征, 药物发现, 消息传递", "comments": "这篇论文的创新点在于将分子三维几何信息和长程相互作用显式地整合到图神经网络的消息传递机制中，解决了传统GNNs在分子性质预测中对空间信息利用不足的局限。通过引入加权彩色子图，模型能够更精细地捕捉化学上下文。其在BBBP预测上的显著性能提升，为药物发现领域提供了一个更精确和实用的工具。"}}
{"id": "2506.00717", "title": "Vid2Coach: Transforming How-To Videos into Task Assistants", "authors": ["Mina Huh", "Zihui Xue", "Ujjaini Das", "Kumar Ashutosh", "Kristen Grauman", "Amy Pavel"], "categories": ["cs.HC", "cs.CV"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Accepted to UIST 2025 Project website: this https URL", "url": "http://arxiv.org/abs/2506.00717v2", "summary": "People use videos to learn new recipes, exercises, and crafts. Such videos\nremain difficult for blind and low vision (BLV) people to follow as they rely\non visual comparison. Our observations of visual rehabilitation therapists\n(VRTs) guiding BLV people to follow how-to videos revealed that VRTs provide\nboth proactive and responsive support including detailed descriptions,\nnon-visual workarounds, and progress feedback. We propose Vid2Coach, a system\nthat transforms how-to videos into wearable camera-based assistants that\nprovide accessible instructions and mixed-initiative feedback. From the video,\nVid2Coach generates accessible instructions by augmenting narrated instructions\nwith demonstration details and completion criteria for each step. It then uses\nretrieval-augmented-generation to extract relevant non-visual workarounds from\nBLV-specific resources. Vid2Coach then monitors user progress with a camera\nembedded in commercial smart glasses to provide context-aware instructions,\nproactive feedback, and answers to user questions. BLV participants (N=8) using\nVid2Coach completed cooking tasks with 58.5\\% fewer errors than when using\ntheir typical workflow and wanted to use Vid2Coach in their daily lives.\nVid2Coach demonstrates an opportunity for AI visual assistance that strengthens\nrather than replaces non-visual expertise.", "comment": "Accepted to UIST 2025 Project website: https://minahuh.com/Vid2Coach/", "pdf_url": "http://arxiv.org/pdf/2506.00717v2", "cate": "cs.HC", "date": "2025-05-31", "updated": "2025-07-25", "AI": {"title_translation": "Vid2Coach：将操作视频转化为任务助手", "tldr": "Vid2Coach是一个系统，它将操作视频转化为基于可穿戴摄像头的助手，为盲人和低视力（BLV）人群提供可访问的指导和混合主动反馈，显著减少了任务错误。", "motivation": "盲人和低视力（BLV）人群难以通过视觉比较来学习操作视频。现有的视觉康复治疗师（VRTs）提供主动和响应式支持，包括详细描述、非视觉替代方案和进度反馈。本研究旨在开发一个系统来弥补这一差距，为BLV人群提供类似VRTs的辅助。", "method": "Vid2Coach系统通过以下方式将操作视频转化为可穿戴摄像头助手：1. 从视频中生成可访问的指令，通过增加旁白指令、演示细节和每一步的完成标准。2. 使用检索增强生成（RAG）从BLV特定资源中提取相关的非视觉替代方案。3. 利用商业智能眼镜中嵌入的摄像头监控用户进度，提供情境感知指令、主动反馈并回答用户问题。", "result": "使用Vid2Coach的BLV参与者（N=8）在烹饪任务中比使用他们典型工作流程时的错误减少了58.5%。参与者表示希望在日常生活中使用Vid2Coach。", "conclusion": "Vid2Coach展示了AI视觉辅助的一个机会，即它能增强而非取代非视觉专业知识，为盲人和低视力人群提供有效的任务支持。", "translation": "人们使用视频来学习新食谱、锻炼和手工艺。然而，对于盲人和低视力（BLV）人群来说，这些视频很难遵循，因为它们依赖于视觉比较。我们对视觉康复治疗师（VRTs）指导BLV人群观看操作视频的观察表明，VRTs提供了主动和响应式的支持，包括详细描述、非视觉替代方案和进度反馈。我们提出了Vid2Coach，一个将操作视频转化为基于可穿戴摄像头的助手系统，该系统提供可访问的指令和混合主动反馈。Vid2Coach从视频中通过增强旁白指令、演示细节和每一步的完成标准来生成可访问的指令。然后，它使用检索增强生成（retrieval-augmented-generation）从BLV特定资源中提取相关的非视觉替代方案。Vid2Coach随后通过嵌入在商业智能眼镜中的摄像头监控用户进度，以提供情境感知指令、主动反馈并回答用户问题。使用Vid2Coach的BLV参与者（N=8）在烹饪任务中比使用他们典型工作流程时的错误减少了58.5%，并且希望在日常生活中使用Vid2Coach。Vid2Coach展示了AI视觉辅助的一个机会，它能增强而非取代非视觉专业知识。", "summary": "Vid2Coach是一个创新系统，旨在帮助盲人和低视力（BLV）人群更好地学习和完成操作视频中的任务。该系统将操作视频转化为基于智能眼镜的助手，通过增强视频指令、提供非视觉替代方案以及实时监控用户进度并提供情境感知反馈，显著减少了BLV用户在任务中遇到的错误。它强调了AI在增强非视觉专业知识方面的潜力。", "keywords": "Vid2Coach, 盲人和低视力, 操作视频, 任务助手, 可穿戴技术", "comments": "Vid2Coach的创新之处在于它将现有的操作视频转化为可访问的、交互式的任务助手，特别针对盲人和低视力人群的需求。通过结合可穿戴技术、增强现实（尽管未明确提及AR，但智能眼镜和实时反馈暗示了这一点）和检索增强生成，它提供了一个全面的解决方案。该研究的重要性在于其对残障人士无障碍技术的贡献，尤其是在一个视觉主导的学习领域。其局限性可能在于系统的泛化能力、对不同任务复杂度的适应性以及用户对可穿戴设备的接受度。"}}
{"id": "2507.18800", "title": "Semantic IDs for Music Recommendation", "authors": ["M. Jeffrey Mei", "Florian Henkel", "Samuel E. Sandberg", "Oliver Bembom", "Andreas F. Ehmann"], "categories": ["cs.IR", "cs.LG"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      RecSys 2025 Industry Track", "url": "http://arxiv.org/abs/2507.18800v1", "summary": "Training recommender systems for next-item recommendation often requires\nunique embeddings to be learned for each item, which may take up most of the\ntrainable parameters for a model. Shared embeddings, such as using content\ninformation, can reduce the number of distinct embeddings to be stored in\nmemory. This allows for a more lightweight model; correspondingly, model\ncomplexity can be increased due to having fewer embeddings to store in memory.\nWe show the benefit of using shared content-based features ('semantic IDs') in\nimproving recommendation accuracy and diversity, while reducing model size, for\ntwo music recommendation datasets, including an online A/B test on a music\nstreaming service.", "comment": "RecSys 2025 Industry Track", "pdf_url": "http://arxiv.org/pdf/2507.18800v1", "cate": "cs.IR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "音乐推荐中的语义ID", "tldr": "使用语义ID（共享内容特征）可以有效减少音乐推荐模型大小，同时提高推荐准确性和多样性。", "motivation": "训练下一项推荐系统时，为每个项目学习独特的嵌入会占用模型大部分可训练参数，导致模型庞大。", "method": "采用共享内容特征（“语义ID”）来替代独特的项目嵌入，以减少存储在内存中的独立嵌入数量，从而实现更轻量级的模型。并在两个音乐推荐数据集上进行了验证，包括在一个音乐流媒体服务上的在线A/B测试。", "result": "使用语义ID显著提高了推荐准确性和多样性，同时减小了模型大小。", "conclusion": "使用共享内容特征（语义ID）对音乐推荐系统非常有益，能够同时提升性能和效率。", "translation": "训练用于下一项推荐的推荐系统通常需要为每个项目学习独特的嵌入，这可能会占用模型大部分可训练参数。共享嵌入，例如使用内容信息，可以减少内存中需要存储的不同嵌入的数量。这使得模型更加轻量级；相应地，由于需要存储的嵌入更少，模型复杂性可以增加。我们展示了在两个音乐推荐数据集（包括在一个音乐流媒体服务上的在线A/B测试）中，使用共享内容特征（“语义ID”）在提高推荐准确性和多样性、同时减小模型大小方面的好处。", "summary": "本文提出了一种利用共享内容特征（称为“语义ID”）来优化音乐推荐系统的方法。与为每个项目学习独特嵌入的传统方法相比，语义ID显著减少了模型所需的参数量，从而实现了更轻量级的模型。实验结果表明，这种方法不仅减小了模型大小，还在推荐准确性和多样性方面取得了提升，并在实际的在线A/B测试中得到了验证。", "keywords": "音乐推荐, 语义ID, 共享嵌入, 模型大小, 推荐多样性", "comments": "该论文的创新点在于提出了使用“语义ID”（共享内容特征）来解决推荐系统中嵌入参数过多导致模型庞大的问题。这种方法不仅实现了模型轻量化，还同时提升了推荐的准确性和多样性，并通过在线A/B测试验证了其在实际应用中的有效性，具有重要的实践价值。"}}
{"id": "2507.19217", "title": "Barenblatt solutions for the time-fractional porous medium equation: approach via integral equations", "authors": ["Josefa Caballero", "Hanna Okrasińska-Płociniczak", "Łukasz Płociniczak", "Kishin Sadarangani"], "categories": ["math.NA", "cs.NA", "math.AP"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19217v1", "summary": "This paper explores Barenblatt solutions of the time-fractional porous medium\nequation, characterized by a Caputo-type time derivative. Employing an integral\nequation approach, we rigorously prove the existence of these solutions and\nestablish several fundamental properties, including upper and lower estimates,\nmass conservation, regularity, and monotonicity. To bridge theory and practice,\nwe introduce a family of convergent numerical schemes specifically designed to\ncompute the Barenblatt solutions, ensuring reliable and efficient\napproximations. The theoretical framework is enriched with various examples\nthat illustrate the concepts and validate the effectiveness of the proposed\nnumerical methods, enhancing the understanding and applicability of our\nresults.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19217v1", "cate": "math.NA", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "时间分数阶多孔介质方程的Barenblatt解：积分方程方法", "tldr": "本文通过积分方程方法研究了时间分数阶多孔介质方程的Barenblatt解，并提出了收敛的数值方案来计算这些解。", "motivation": "本文旨在探索时间分数阶多孔介质方程的Barenblatt解，并通过理论分析和数值方案设计，弥合理论与实践之间的差距，确保可靠高效的近似。", "method": "本文采用积分方程方法，严格证明了Barenblatt解的存在性并确立了其基本性质。此外，还引入了一系列收敛的数值方案来计算这些Barenblatt解。", "result": "研究证明了Barenblatt解的存在性，并确立了其多个基本性质，包括上下界估计、质量守恒、正则性和单调性。同时，引入了一系列收敛的数值方案，并通过各种例子验证了其有效性。", "conclusion": "本文的理论框架和提出的数值方法增强了对时间分数阶多孔介质方程Barenblatt解的理解和适用性。", "translation": "本文探索了时间分数阶多孔介质方程的Barenblatt解，该方程以Caputo型时间导数表征。通过采用积分方程方法，我们严格证明了这些解的存在性，并确立了几个基本性质，包括上下界估计、质量守恒、正则性和单调性。为了弥合理论与实践，我们引入了一系列收敛的数值方案，专门设计用于计算Barenblatt解，确保了可靠和高效的近似。理论框架通过各种例子得到丰富，这些例子阐明了概念并验证了所提出的数值方法的有效性，从而增强了我们结果的理解和适用性。", "summary": "本文研究了具有Caputo型时间导数的时间分数阶多孔介质方程的Barenblatt解。通过积分方程方法，论文严格证明了这些解的存在性，并确立了包括上下界估计、质量守恒、正则性和单调性在内的基本性质。为实现理论与应用的结合，作者还提出了一系列收敛的数值方案来高效计算这些解，并通过实例验证了其有效性，提升了研究成果的理解与应用价值。", "keywords": "Barenblatt解, 分数阶多孔介质方程, 积分方程, 数值方案, Caputo导数", "comments": "本文的创新之处在于结合积分方程方法严格证明了时间分数阶多孔介质方程Barenblatt解的存在性及性质，并进一步提出了实用的数值计算方案。这不仅深化了理论理解，也为实际应用提供了可靠工具，对于分数阶微分方程的研究具有重要意义。"}}
{"id": "2507.18913", "title": "Limits at a Distance: Design Directions to Address Psychological Distance in Policy Decisions Affecting Planetary Boundaries", "authors": ["Eshta Bhardwaj", "Han Qiao", "Christoph Becker"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      Post-proceedings paper presented at LIMITS 2024: 10th Workshop on Computing within Limits, 2024-06-19/20, Online", "url": "http://arxiv.org/abs/2507.18913v1", "summary": "Policy decisions relevant to the environment rely on tools like dashboards,\nrisk models, and prediction models to provide information and data\nvisualizations that enable decision-makers to make trade-offs. The conventional\nparadigm of data visualization practices for policy and decision-making is to\nconvey data in a supposedly neutral, objective manner for rational\ndecision-makers. Feminist critique advocates for nuanced and reflexive\napproaches that take into account situated decision-makers and their affective\nrelationships to data. This paper sheds light on a key cognitive aspect that\nimpacts how decision-makers interpret data. Because all outcomes from policies\nrelevant to climate change occur at a distance, decision-makers experience\nso-called `psychological distance' to environmental decisions in terms of\nspace, time, social identity, and hypotheticality. This profoundly impacts how\nthey perceive and evaluate outcomes. Since policy decisions to achieve a safe\nplanetary space are urgently needed for immediate transition and change, we\nneed a design practice that takes into account how psychological distance\naffects cognition and decision-making. Our paper explores the role of\nalternative design approaches in developing visualizations used for climate\npolicymaking. We conduct a literature review and synthesis which bridges\npsychological distance with speculative design and data visceralization by\nillustrating the value of affective design methods via examples from previous\nresearch. Through this work, we propose a novel premise for the communication\nand visualization of environmental data. Our paper lays out how future research\non the impacts of alternative design approaches on psychological distance can\nmake data used for policy decisions more tangible and visceral.", "comment": "Post-proceedings paper presented at LIMITS 2024: 10th Workshop on\n  Computing within Limits, 2024-06-19/20, Online", "pdf_url": "http://arxiv.org/pdf/2507.18913v1", "cate": "cs.HC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "距离的限制：解决影响行星边界的政策决策中心理距离的设计方向", "tldr": "本文探讨了心理距离如何影响决策者对环境数据的理解，并提出通过替代设计方法（如推测性设计和数据内脏化）来使气候政策数据更具象化和可感知，以促进更有效的决策。", "motivation": "环境政策决策迫切需要实现安全的行星空间，但决策者对气候变化相关政策结果存在“心理距离”（空间、时间、社会认同、假设性），这严重影响他们对结果的感知和评估。传统的客观数据可视化方法未能有效解决此问题，因此需要一种考虑心理距离如何影响认知和决策的设计实践。", "method": "本文通过文献综述和综合研究，将心理距离与推测性设计和数据内脏化相结合，并通过以往研究的例子阐述了情感设计方法的价值。", "result": "研究结果提出了环境数据交流和可视化的一种新颖前提，并展示了替代设计方法对心理距离的影响如何能使政策决策中使用的环境数据变得更具象化和可感知。", "conclusion": "为了应对气候变化带来的紧迫挑战，未来的研究应探索替代设计方法如何帮助减少决策者的心理距离，从而使环境数据在政策制定中更具说服力、更易理解和感知。", "translation": "与环境相关的政策决策依赖于仪表板、风险模型和预测模型等工具来提供信息和数据可视化，使决策者能够进行权衡。政策和决策制定中数据可视化的传统范式是以一种所谓的“中立、客观”的方式传达数据，以供理性决策者使用。女性主义批评倡导采取细致入微和反思性的方法，考虑情境化的决策者及其与数据的情感关系。本文阐明了影响决策者如何解释数据的关键认知方面。由于气候变化相关政策的所有结果都发生在“远距离”，决策者在空间、时间、社会认同和假设性方面对环境决策体验到所谓的“心理距离”。这深刻影响了他们如何感知和评估结果。由于迫切需要立即过渡和变革的政策决策以实现安全的行星空间，我们需要一种考虑心理距离如何影响认知和决策的设计实践。本文探讨了替代设计方法在开发用于气候政策制定中的可视化工具方面的作用。我们通过文献综述和综合研究，将心理距离与推测性设计和数据内脏化相结合，通过以往研究的例子阐明了情感设计方法的价值。通过这项工作，我们为环境数据的交流和可视化提出了一个新颖的前提。本文阐述了未来关于替代设计方法对心理距离影响的研究如何能使用于政策决策的数据更具象化和可感知。", "summary": "本文探讨了政策决策者在处理气候变化相关数据时所面临的“心理距离”问题，即由于结果发生在遥远的未来或空间，导致其难以感知。传统的数据可视化方法未能有效解决这一认知障碍。文章通过文献综述，将心理距离与推测性设计和数据内脏化相结合，提出了一种新的环境数据交流和可视化前提，旨在通过情感设计方法使数据更具象化和可感知，从而促进更有效的气候政策决策。", "keywords": "心理距离, 环境政策, 数据可视化, 推测性设计, 情感设计", "comments": "本文创新性地将心理距离这一认知心理学概念引入环境政策决策的数据可视化设计中，挑战了传统“中立客观”的数据呈现范式。其提出通过推测性设计和数据内脏化等情感设计方法来克服心理距离，使抽象的环境数据更具象化和可感知，为气候政策制定提供了新的视角和实用方向。这一研究对于提升决策者对长期环境问题的紧迫感和行动力具有重要意义。"}}
{"id": "2507.18998", "title": "GPSMamba: A Global Phase and Spectral Prompt-guided Mamba for Infrared Image Super-Resolution", "authors": ["Yongsong Huang", "Tomo Miyazaki", "Xiaofeng Liu", "Shinichiro Omachi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      This manuscript is under review, and copyright will be transferred without notice", "url": "http://arxiv.org/abs/2507.18998v1", "summary": "Infrared Image Super-Resolution (IRSR) is challenged by the low contrast and\nsparse textures of infrared data, requiring robust long-range modeling to\nmaintain global coherence. While State-Space Models like Mamba offer\nproficiency in modeling long-range dependencies for this task, their inherent\n1D causal scanning mechanism fragments the global context of 2D images,\nhindering fine-detail restoration. To address this, we propose Global Phase and\nSpectral Prompt-guided Mamba (GPSMamba), a framework that synergizes\narchitectural guidance with non-causal supervision. First, our Adaptive\nSemantic-Frequency State Space Module (ASF-SSM) injects a fused\nsemantic-frequency prompt directly into the Mamba block, integrating non-local\ncontext to guide reconstruction. Then, a novel Thermal-Spectral Attention and\nPhase Consistency Loss provides explicit, non-causal supervision to enforce\nglobal structural and spectral fidelity. By combining these two innovations,\nour work presents a systematic strategy to mitigate the limitations of causal\nmodeling. Extensive experiments demonstrate that GPSMamba achieves\nstate-of-the-art performance, validating our approach as a powerful new\nparadigm for infrared image restoration. Code is available at\nhttps://github.com/yongsongH/GPSMamba.", "comment": "This manuscript is under review, and copyright will be transferred\n  without notice", "pdf_url": "http://arxiv.org/pdf/2507.18998v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "GPSMamba：一种用于红外图像超分辨率的全局相位和光谱提示引导Mamba模型", "tldr": "GPSMamba提出了一种结合架构引导和非因果监督的方法，解决了Mamba在红外图像超分辨率中全局上下文碎片化的问题，并取得了最先进的性能。", "motivation": "红外图像超分辨率（IRSR）面临红外数据低对比度和稀疏纹理的挑战，需要鲁棒的远程建模以保持全局一致性。尽管Mamba等状态空间模型擅长建模远程依赖，但其固有的1D因果扫描机制会碎片化2D图像的全局上下文，阻碍了细节恢复。", "method": "提出GPSMamba框架，结合架构引导和非因果监督。具体方法包括：1. 自适应语义-频率状态空间模块（ASF-SSM），将融合的语义-频率提示注入Mamba块，整合非局部上下文指导重建。2. 新颖的热光谱注意力与相位一致性损失，提供显式的非因果监督，以强制实现全局结构和光谱保真度。", "result": "大量实验表明GPSMamba实现了最先进的性能。", "conclusion": "GPSMamba为红外图像恢复提供了一个强大的新范例。", "translation": "红外图像超分辨率（IRSR）面临红外数据低对比度和稀疏纹理的挑战，需要鲁棒的远程建模以保持全局一致性。尽管Mamba等状态空间模型擅长建模此任务的远程依赖，但其固有的1D因果扫描机制会碎片化2D图像的全局上下文，阻碍了细节恢复。为了解决这个问题，我们提出了全局相位和光谱提示引导Mamba（GPSMamba），这是一个结合了架构引导和非因果监督的框架。首先，我们的自适应语义-频率状态空间模块（ASF-SSM）将融合的语义-频率提示直接注入Mamba块，整合非局部上下文以指导重建。然后，一种新颖的热光谱注意力和相位一致性损失提供了显式的非因果监督，以强制实现全局结构和光谱保真度。通过结合这两项创新，我们的工作提出了一种系统的策略来减轻因果建模的局限性。大量实验表明，GPSMamba取得了最先进的性能，验证了我们的方法是红外图像恢复的一个强大新范例。代码可在https://github.com/yongsongH/GPSMamba获取。", "summary": "本文提出了GPSMamba，一种新颖的红外图像超分辨率（IRSR）框架，旨在解决Mamba模型在处理2D图像全局上下文时因其1D因果扫描机制带来的局限性。GPSMamba通过引入自适应语义-频率状态空间模块（ASF-SSM）注入语义-频率提示来指导重建，并利用热光谱注意力和相位一致性损失进行非因果监督，以确保全局结构和光谱保真度。这种系统方法有效缓解了因果建模的局限性，并在IRSR任务中取得了最先进的性能。", "keywords": "红外图像超分辨率, Mamba, 状态空间模型, 全局上下文, 相位一致性", "comments": "该论文的创新点在于系统地结合了架构引导（ASF-SSM）和非因果监督（热光谱注意力与相位一致性损失），以弥补Mamba模型在处理2D图像时因1D因果扫描导致的全局上下文碎片化问题。这对于红外图像超分辨率中低对比度和稀疏纹理的挑战尤为重要。通过这种方法，GPSMamba不仅实现了最先进的性能，还为红外图像恢复提供了一个强大的新范例。"}}
{"id": "2507.19186", "title": "Reconstruct or Generate: Exploring the Spectrum of Generative Modeling for Cardiac MRI", "authors": ["Niklas Bubeck", "Yundi Zhang", "Suprosanna Shit", "Daniel Rueckert", "Jiazhen Pan"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19186v1", "summary": "In medical imaging, generative models are increasingly relied upon for two\ndistinct but equally critical tasks: reconstruction, where the goal is to\nrestore medical imaging (usually inverse problems like inpainting or\nsuperresolution), and generation, where synthetic data is created to augment\ndatasets or carry out counterfactual analysis. Despite shared architecture and\nlearning frameworks, they prioritize different goals: generation seeks high\nperceptual quality and diversity, while reconstruction focuses on data fidelity\nand faithfulness. In this work, we introduce a \"generative model zoo\" and\nsystematically analyze how modern latent diffusion models and autoregressive\nmodels navigate the reconstruction-generation spectrum. We benchmark a suite of\ngenerative models across representative cardiac medical imaging tasks, focusing\non image inpainting with varying masking ratios and sampling strategies, as\nwell as unconditional image generation. Our findings show that diffusion models\noffer superior perceptual quality for unconditional generation but tend to\nhallucinate as masking ratios increase, whereas autoregressive models maintain\nstable perceptual performance across masking levels, albeit with generally\nlower fidelity.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19186v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "重建还是生成：探索心脏MRI的生成建模谱系", "tldr": "本文系统分析了现代生成模型（扩散模型和自回归模型）在心脏MRI重建和生成任务中的表现，发现扩散模型在无条件生成方面表现优异但在重建时易产生幻觉，而自回归模型在不同掩蔽率下感知性能稳定但保真度较低。", "motivation": "医疗成像中，生成模型在重建（恢复图像，如修复、超分辨率）和生成（合成数据）两类任务中日益重要，尽管架构相似但目标不同（生成追求感知质量和多样性，重建关注数据保真度）。作者旨在系统分析不同生成模型如何平衡这些目标。", "method": "作者构建了一个“生成模型动物园”，系统分析了现代潜在扩散模型和自回归模型在重建-生成谱系中的表现。他们在一系列代表性的心脏医学影像任务上对生成模型进行了基准测试，重点关注不同掩蔽率和采样策略下的图像修复以及无条件图像生成。", "result": "扩散模型在无条件生成方面提供卓越的感知质量，但随着掩蔽率增加，倾向于产生幻觉。自回归模型在不同掩蔽水平下保持稳定的感知性能，尽管整体保真度较低。", "conclusion": "不同的生成模型（如扩散模型和自回归模型）在心脏MRI的重建和生成任务中表现出不同的优缺点，需要在感知质量、数据保真度和抗幻觉能力之间进行权衡，以适应特定任务的需求。", "translation": "在医学成像领域，生成模型日益依赖于两项不同但同样关键的任务：重建（目标是恢复医学图像，通常是逆问题，如图像修复或超分辨率）和生成（创建合成数据以扩充数据集或进行反事实分析）。尽管共享架构和学习框架，但它们优先考虑不同的目标：生成寻求高感知质量和多样性，而重建侧重于数据保真度和忠实性。在这项工作中，我们引入了一个“生成模型动物园”，并系统地分析了现代潜在扩散模型和自回归模型如何驾驭重建-生成谱系。我们在一系列代表性的心脏医学成像任务中对一套生成模型进行了基准测试，重点关注具有不同掩蔽率和采样策略的图像修复以及无条件图像生成。我们的研究结果表明，扩散模型为无条件生成提供了卓越的感知质量，但随着掩蔽率的增加倾向于产生幻觉，而自回归模型在不同掩蔽水平下保持稳定的感知性能，尽管整体保真度较低。", "summary": "本文系统研究了生成模型（特别是扩散模型和自回归模型）在心脏MRI重建和生成任务中的表现。研究发现，扩散模型在无条件生成方面表现出色，但在高掩蔽率下易产生幻觉；自回归模型则在不同掩蔽率下保持稳定的感知性能，但保真度较低。这揭示了不同生成模型在医学图像任务中重建与生成能力之间的权衡。", "keywords": "生成模型, 心脏MRI, 图像重建, 图像生成, 扩散模型", "comments": "这项工作通过构建“生成模型动物园”并系统比较扩散模型和自回归模型在心脏MRI重建和生成任务中的表现，为理解这些模型在医学成像中的适用性提供了宝贵的见解。其创新点在于明确区分并量化了模型在“重建-生成谱系”中的行为，指出了不同模型在感知质量、数据保真度和幻觉生成方面的权衡，对于指导未来医学图像生成模型选择和开发具有重要意义。"}}
{"id": "2507.19136", "title": "Dynamic Agile Reconfigurable Intelligent Surface Antenna (DARISA) MIMO: DoF Analysis and Effective DoF Optimization", "authors": ["Jiale Bai", "Hui-Ming Wang", "Liang Jin"], "categories": ["cs.IT", "eess.SP", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "Comments:      Accepted by IEEE Transactions on Wireless Communications", "url": "http://arxiv.org/abs/2507.19136v1", "summary": "In this paper, we propose a dynamic agile reconfigurable intelligent surface\nantenna (DARISA) array integrated into multi-input multi-output (MIMO)\ntransceivers. Each DARISA comprises a number of metasurface elements activated\nsimultaneously via a parallel feed network. The proposed system enables rapid\nand intelligent phase response adjustments for each metasurface element within\na single symbol duration, facilitating a dynamic agile adjustment of phase\nresponse (DAAPR) strategy. By analyzing the theoretical degrees of freedom\n(DoF) of the DARISA MIMO system under the DAAPR framework, we derive an\nexplicit relationship between DoF and critical system parameters, including\nagility frequentness (i.e., the number of phase adjustments of metasurface\nelements during one symbol period), cluster angular spread of wireless\nchannels, DARISA array size, and the number of transmit/receive DARISAs. The\nDoF result reveals a significant conclusion: when the number of receive DARISAs\nis smaller than that of transmit DARISAs, the DAAPR strategy of the DARISA MIMO\nenhances the overall system DoF. Furthermore, relying on DoF alone to measure\nchannel capacity is insufficient, so we analyze the effective DoF (EDoF) that\nreflects the impacts of the DoF and channel matrix singular value distribution\non capacity. We show channel capacity monotonically increases with EDoF, and\noptimize the agile phase responses of metasurface elements by using fractional\nprogramming (FP) and semidefinite relaxation (SDR) algorithms to maximize the\nEDoF. Simulations validate the theoretical DoF gains and reveal that increasing\nagility frequentness, metasurface element density, and phase quantization\naccuracy can enhance the EDoF. Additionally, densely deployed elements can\ncompensate for the loss in communication performance caused by lower phase\nquantization accuracy.", "comment": "Accepted by IEEE Transactions on Wireless Communications", "pdf_url": "http://arxiv.org/pdf/2507.19136v1", "cate": "cs.IT", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "动态敏捷可重构智能表面天线 (DARISA) MIMO：自由度分析和有效自由度优化", "tldr": "本文提出了一种DARISA MIMO系统，分析了其自由度并优化了有效自由度以提高信道容量。", "motivation": "为弥补传统自由度分析在衡量信道容量方面的不足，并提升MIMO系统性能，本文提出一种动态敏捷可重构智能表面天线 (DARISA) 阵列，旨在通过动态相位调整策略增强系统自由度 (DoF) 和有效自由度 (EDoF)，从而优化信道容量。", "method": "本文提出了一种集成到MIMO收发器中的动态敏捷可重构智能表面天线 (DARISA) 阵列，通过并行馈电网络激活超表面单元，实现动态敏捷相位响应 (DAAPR) 策略，能在单个符号周期内快速调整相位。研究分析了DAAPR框架下DARISA MIMO系统的理论自由度 (DoF)，并推导了DoF与关键系统参数的关系。为更准确反映信道容量，引入并分析了有效自由度 (EDoF)，并使用分数规划 (FP) 和半定松弛 (SDR) 算法优化超表面单元的敏捷相位响应以最大化EDoF。", "result": "研究发现，当接收DARISA数量小于发送DARISA数量时，DAAPR策略显著增强了DARISA MIMO的整体系统自由度 (DoF)。信道容量随有效自由度 (EDoF) 单调增加。仿真结果验证了理论DoF增益，并表明增加敏捷频率、超表面单元密度和相位量化精度可以提高EDoF。此外，密集部署的单元能够补偿较低相位量化精度导致的通信性能损失。", "conclusion": "动态敏捷相位响应 (DAAPR) 策略能有效提升DARISA MIMO系统的自由度，特别是在接收DARISA数量受限时。通过优化有效自由度 (EDoF)，可以显著提高信道容量，且增加敏捷频率、单元密度和量化精度是提升性能的有效途径。", "translation": "在本文中，我们提出了一种集成到多输入多输出 (MIMO) 收发器中的动态敏捷可重构智能表面天线 (DARISA) 阵列。每个DARISA包含通过并行馈电网络同时激活的多个超表面单元。所提出的系统能够在单个符号持续时间内对每个超表面单元进行快速智能的相位响应调整，从而促进动态敏捷相位响应 (DAAPR) 策略。通过分析DAAPR框架下DARISA MIMO系统的理论自由度 (DoF)，我们推导了DoF与关键系统参数（包括敏捷频率（即在一个符号周期内超表面单元的相位调整次数）、无线信道的簇角扩展、DARISA阵列大小以及发送/接收DARISA的数量）之间的明确关系。DoF结果揭示了一个重要结论：当接收DARISA的数量小于发送DARISA的数量时，DARISA MIMO的DAAPR策略增强了整体系统DoF。此外，仅依靠DoF来衡量信道容量是不够的，因此我们分析了反映DoF和信道矩阵奇异值分布对容量影响的有效自由度 (EDoF)。我们表明信道容量随EDoF单调增加，并使用分数规划 (FP) 和半定松弛 (SDR) 算法优化超表面单元的敏捷相位响应以最大化EDoF。仿真验证了理论DoF增益，并揭示了增加敏捷频率、超表面单元密度和相位量化精度可以增强EDoF。此外，密集部署的单元可以补偿较低相位量化精度导致的通信性能损失。", "summary": "本文提出并分析了一种动态敏捷可重构智能表面天线 (DARISA) MIMO系统，该系统通过动态敏捷相位响应 (DAAPR) 策略在单个符号周期内调整超表面单元的相位。研究了系统自由度 (DoF) 与关键参数的关系，发现当接收DARISA数量少于发送DARISA时，DAAPR能增强DoF。为更准确衡量信道容量，引入并优化了有效自由度 (EDoF)，结果表明EDoF与容量单调递增，且增加敏捷频率、单元密度和相位量化精度可提升EDoF。", "keywords": "DARISA MIMO, 自由度 (DoF), 有效自由度 (EDoF), 动态敏捷相位响应 (DAAPR), 超表面", "comments": "这篇论文的创新点在于引入了动态敏捷可重构智能表面天线 (DARISA) 和动态敏捷相位响应 (DAAPR) 策略，实现了在单个符号周期内的快速相位调整，这对于提高MIMO系统的灵活性和性能至关重要。此外，论文不仅分析了传统的自由度，还引入了更具实际意义的有效自由度 (EDoF) 来衡量信道容量，并利用先进的优化算法最大化EDoF，这体现了其理论与实践结合的价值。"}}
{"id": "2507.18675", "title": "Advancing Vision-based Human Action Recognition: Exploring Vision-Language CLIP Model for Generalisation in Domain-Independent Tasks", "authors": ["Sanyam Jain", "Marsha Mariya Kappan", "Vijeta Sharma"], "categories": ["cs.CV", "cs.LG"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18675v1", "summary": "Human action recognition plays a critical role in healthcare and medicine,\nsupporting applications such as patient behavior monitoring, fall detection,\nsurgical robot supervision, and procedural skill assessment. While traditional\nmodels like CNNs and RNNs have achieved moderate success, they often struggle\nto generalize across diverse and complex actions. Recent advancements in\nvision-language models, especially the transformer-based CLIP model, offer\npromising capabilities for generalizing action recognition from video data. In\nthis work, we evaluate CLIP on the UCF-101 dataset and systematically analyze\nits performance under three masking strategies: (1) percentage-based and\nshape-based black masking at 10%, 30%, and 50%, (2) feature-specific masking to\nsuppress bias-inducing elements, and (3) isolation masking that retains only\nclass-specific regions. Our results reveal that CLIP exhibits inconsistent\nbehavior and frequent misclassifications, particularly when essential visual\ncues are obscured. To overcome these limitations, we propose incorporating\nclass-specific noise, learned via a custom loss function, to reinforce\nattention to class-defining features. This enhancement improves classification\naccuracy and model confidence while reducing bias. We conclude with a\ndiscussion on the challenges of applying such models in clinical domains and\noutline directions for future work to improve generalizability across\ndomain-independent healthcare scenarios.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18675v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "推进基于视觉的人体动作识别：探索视觉-语言CLIP模型在领域无关任务中的泛化能力", "tldr": "本文评估了CLIP模型在人体动作识别任务中的泛化能力，发现其在视觉线索受损时表现不佳，并提出通过引入类别特异性噪声来提高模型准确性和鲁棒性，以应对医疗领域的挑战。", "motivation": "人体动作识别在医疗健康领域（如患者行为监测、跌倒检测、手术机器人监督和技能评估）中至关重要。传统的模型（如CNN和RNN）难以泛化到多样复杂的动作，而基于Transformer的视觉-语言CLIP模型展现出在视频数据上泛化动作识别的潜力。", "method": "作者在UCF-101数据集上评估了CLIP模型，并系统分析了其在三种遮蔽策略下的性能：1) 基于百分比和形状的黑色遮蔽（10%、30%、50%）；2) 特征特异性遮蔽以抑制偏差；3) 隔离遮蔽仅保留类别特定区域。为克服局限性，提出通过自定义损失函数学习并引入类别特异性噪声，以增强对类别定义特征的关注。", "result": "结果显示，当关键视觉线索被遮挡时，CLIP表现出不一致的行为和频繁的错误分类。提出的增强方法（引入类别特异性噪声）提高了分类准确性和模型置信度，同时减少了偏差。", "conclusion": "本文讨论了在临床领域应用此类模型的挑战，并提出了未来工作方向，以提高模型在领域无关医疗场景中的泛化能力。", "translation": "人体动作识别在医疗保健和医学领域发挥着关键作用，支持患者行为监测、跌倒检测、手术机器人监督和程序技能评估等应用。虽然CNN和RNN等传统模型取得了适度成功，但它们通常难以泛化到多样化和复杂的动作。视觉-语言模型，特别是基于Transformer的CLIP模型的最新进展，为从视频数据中泛化动作识别提供了有前景的能力。在这项工作中，我们在UCF-101数据集上评估了CLIP，并系统分析了其在三种遮蔽策略下的性能：（1）10%、30%和50%的基于百分比和形状的黑色遮蔽，（2）特征特异性遮蔽以抑制诱导偏差的元素，以及（3）仅保留类别特定区域的隔离遮蔽。我们的结果表明，CLIP表现出不一致的行为和频繁的错误分类，特别是在关键视觉线索被遮挡时。为了克服这些局限性，我们建议通过自定义损失函数学习并引入类别特异性噪声，以增强对类别定义特征的关注。这种增强提高了分类准确性和模型置信度，同时减少了偏差。最后，我们讨论了在临床领域应用此类模型的挑战，并概述了未来工作方向，以提高在领域无关医疗场景中的泛化能力。", "summary": "本文探讨了视觉-语言CLIP模型在基于视觉的人体动作识别任务中的泛化能力，特别是在医疗健康领域的应用潜力。研究发现，CLIP在视觉信息受损时表现不稳定。为解决此问题，作者提出了一种通过引入类别特异性噪声来强化模型对关键特征关注的方法，从而提升了分类准确性和置信度，并减少了偏差。文章最后讨论了此类模型在临床应用中的挑战及未来的研究方向。", "keywords": "人体动作识别, CLIP模型, 视觉-语言模型, 泛化能力, 医疗健康", "comments": "本文创新性地探索了视觉-语言CLIP模型在人体动作识别领域的泛化能力，并针对其在视觉线索受损时的局限性提出了有效的改进方案。通过引入类别特异性噪声，提高了模型在复杂场景下的鲁棒性。研究的重要性在于其对医疗健康领域中人体动作识别应用的支持，并指出了未来在真实临床环境中部署此类模型需要克服的挑战。"}}
{"id": "2501.15775", "title": "Do Existing Testing Tools Really Uncover Gender Bias in Text-to-Image Models?", "authors": ["Yunbo Lyu", "Zhou Yang", "Yuqing Niu", "Jing Jiang", "David Lo"], "categories": ["cs.CV", "cs.SE"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ACM MM 2025", "url": "http://arxiv.org/abs/2501.15775v2", "summary": "Text-to-Image (T2I) models have recently gained significant attention due to\ntheir ability to generate high-quality images and are consequently used in a\nwide range of applications. However, there are concerns about the gender bias\nof these models. Previous studies have shown that T2I models can perpetuate or\neven amplify gender stereotypes when provided with neutral text prompts.\nResearchers have proposed automated gender bias uncovering detectors for T2I\nmodels, but a crucial gap exists: no existing work comprehensively compares the\nvarious detectors and understands how the gender bias detected by them deviates\nfrom the actual situation. This study addresses this gap by validating previous\ngender bias detectors using a manually labeled dataset and comparing how the\nbias identified by various detectors deviates from the actual bias in T2I\nmodels, as verified by manual confirmation. We create a dataset consisting of\n6,000 images generated from three cutting-edge T2I models: Stable Diffusion XL,\nStable Diffusion 3, and Dreamlike Photoreal 2.0. During the human-labeling\nprocess, we find that all three T2I models generate a portion (12.48% on\naverage) of low-quality images (e.g., generate images with no face present),\nwhere human annotators cannot determine the gender of the person. Our analysis\nreveals that all three T2I models show a preference for generating male images,\nwith SDXL being the most biased. Additionally, images generated using prompts\ncontaining professional descriptions (e.g., lawyer or doctor) show the most\nbias. We evaluate seven gender bias detectors and find that none fully capture\nthe actual level of bias in T2I models, with some detectors overestimating bias\nby up to 26.95%. We further investigate the causes of inaccurate estimations,\nhighlighting the limitations of detectors in dealing with low-quality images.\nBased on our findings, we propose an enhanced detector...", "comment": "Accepted to ACM MM 2025", "pdf_url": "http://arxiv.org/pdf/2501.15775v2", "cate": "cs.CV", "date": "2025-01-27", "updated": "2025-07-25", "AI": {"title_translation": "现有测试工具真的能揭示文本到图像模型中的性别偏见吗？", "tldr": "本研究通过手动标注数据集验证了现有的文本到图像（T2I）模型性别偏见检测器，发现这些检测器均未能完全捕捉实际偏见水平，有些甚至高估了偏见，尤其是在处理低质量图像时。研究发现所有T2I模型都偏向生成男性图像，且专业描述提示词生成的图像偏见最严重。", "motivation": "文本到图像（T2I）模型因其生成高质量图像的能力而受到广泛关注，但存在性别偏见问题。现有研究表明T2I模型会延续甚至放大性别刻板印象。尽管研究人员提出了自动化性别偏见检测器，但缺乏对这些检测器的全面比较，也未能理解其检测到的偏见与实际情况的偏差。本研究旨在填补这一空白。", "method": "本研究通过使用手动标注数据集来验证现有性别偏见检测器，并比较不同检测器识别的偏见与T2I模型中实际偏见的偏差（通过人工确认）。研究创建了一个包含6,000张图像的数据集，这些图像由三种前沿T2I模型（Stable Diffusion XL、Stable Diffusion 3和Dreamlike Photoreal 2.0）生成。研究评估了七种性别偏见检测器，并进一步调查了不准确估计的原因。", "result": "1. 在人工标注过程中发现，所有三种T2I模型都会生成一部分（平均12.48%）低质量图像（例如，没有脸部的图像），导致人类标注者无法确定人物性别。\n2. 所有三种T2I模型都显示出偏向生成男性图像，其中SDXL的偏见最严重。\n3. 使用包含专业描述（例如，律师或医生）的提示词生成的图像显示出最大的偏见。\n4. 评估发现，七种性别偏见检测器均未能完全捕捉T2I模型中的实际偏见水平，有些检测器甚至将偏见高估了高达26.95%。\n5. 检测器在处理低质量图像方面的局限性是不准确估计的原因之一。", "conclusion": "现有性别偏见检测工具未能准确捕捉文本到图像模型中的实际偏见水平，甚至可能高估偏见，尤其是在处理模型生成的低质量图像时存在局限性。T2I模型普遍存在偏向生成男性图像的问题，且在专业描述提示词下偏见更明显。因此，需要开发更精确、能处理低质量图像的增强型检测器。", "translation": "文本到图像（T2I）模型由于其生成高质量图像的能力最近获得了广泛关注，并因此被广泛应用于各种应用中。然而，人们对其性别偏见存在担忧。先前的研究表明，当提供中性文本提示时，T2I模型会延续甚至放大性别刻板印象。研究人员提出了针对T2I模型的自动化性别偏见揭示检测器，但存在一个关键空白：没有现有工作全面比较各种检测器，也未能理解它们检测到的性别偏见与实际情况的偏差。本研究通过使用手动标注数据集验证先前的性别偏见检测器，并比较不同检测器识别的偏见与T2I模型中实际偏见的偏差（通过人工确认）来解决这一空白。我们创建了一个包含6,000张图像的数据集，这些图像由三种前沿T2I模型生成：Stable Diffusion XL、Stable Diffusion 3和Dreamlike Photoreal 2.0。在人工标注过程中，我们发现所有三种T2I模型都会生成一部分（平均12.48%）低质量图像（例如，生成没有脸部的图像），导致人类标注者无法确定人物性别。我们的分析显示，所有三种T2I模型都偏向生成男性图像，其中SDXL的偏见最严重。此外，使用包含专业描述（例如，律师或医生）的提示词生成的图像显示出最大的偏见。我们评估了七种性别偏见检测器，发现没有一个能完全捕捉T2I模型中实际的偏见水平，有些检测器高估了偏见高达26.95%。我们进一步调查了不准确估计的原因，强调了检测器在处理低质量图像方面的局限性。基于我们的发现，我们提出了一个增强型检测器...", "summary": "本研究旨在评估现有文本到图像（T2I）模型性别偏见检测器的有效性。通过创建一个包含6,000张由SDXL、SD3和Dreamlike Photoreal 2.0生成并手动标注的数据集，研究发现所有T2I模型都偏向生成男性图像，尤其是在专业描述提示词下。同时，T2I模型会生成一定比例的低质量图像，影响性别判断。研究评估了七种现有检测器，发现它们均未能准确捕捉实际偏见水平，且存在高估偏见的现象，部分原因是它们无法有效处理低质量图像。基于此，论文提出了一个增强型检测器。", "keywords": "文本到图像模型, 性别偏见, 偏见检测器, 低质量图像, 人工标注", "comments": "本文通过严谨的实验设计，首次全面比较了现有T2I模型性别偏见检测器的有效性，揭示了这些工具在实际应用中的局限性。其创新点在于构建了大规模的人工标注数据集来作为“黄金标准”，并深入分析了检测器不准确的原因，特别是低质量图像对检测结果的影响。这对于未来开发更鲁棒、更准确的偏见检测方法具有重要指导意义，强调了在评估AI模型偏见时，不仅要关注模型本身，也要考虑检测工具的局限性。"}}
{"id": "2507.16101", "title": "The Sustainability of the Leo Orbit Capacity via Risk-Driven Active Debris Removal", "authors": ["Yacob Medhin", "Simone Servadio"], "categories": ["eess.SY", "cs.SY", "physics.space-ph"], "primary_category": "Subjects:       Systems and Control (eess.SY)", "pdf_link": null, "comments": "Comments:      20 pages, 10 figures", "url": "http://arxiv.org/abs/2507.16101v2", "summary": "The growing number of space debris in Low Earth Orbit (LEO) jeopardizes\nlong-term orbital sustainability, requiring efficient risk assessment for\nactive debris removal (ADR) missions. This study presents the development and\nvalidation of Filtered Modified MITRI (FMM), an enhanced risk index designed to\nimprove the prioritization of high-criticality debris. Leveraging the MOCAT-MC\nsimulation framework, we conducted a comprehensive performance evaluation and\nsensitivity analysis to probe the robustness of the FMM formulation. The\nresults demonstrate that while the FMM provides superior identification of\nhigh-risk targets for annual removal campaigns, a nuanced performance trade-off\nexists between risk models depending on the operational removal cadence. The\nanalysis also confirms that physically grounded mass terms are indispensable\nfor practical risk assessment. By providing a validated open source tool and\ncritical insights into the dynamics of risk, this research enhances our ability\nto select optimal ADR targets and ensure the long-term viability of LEO\noperations.", "comment": "20 pages, 10 figures", "pdf_url": "http://arxiv.org/pdf/2507.16101v2", "cate": "eess.SY", "date": "2025-07-21", "updated": "2025-07-25", "AI": {"title_translation": "通过风险驱动的主动碎片清除实现LEO轨道容量的可持续性", "tldr": "本研究开发并验证了FMM风险指数，旨在提高高危空间碎片的识别和优先级排序，以增强LEO轨道的可持续性。结果表明FMM在识别高风险目标方面表现优异，并强调了物理质量项的重要性。", "motivation": "低地球轨道（LEO）中不断增加的空间碎片危及长期轨道可持续性，因此需要对主动碎片清除（ADR）任务进行有效的风险评估。", "method": "本研究开发并验证了Filtered Modified MITRI (FMM)，一个增强的风险指数，旨在改善高危碎片的优先级排序。利用MOCAT-MC模拟框架，进行了全面的性能评估和敏感性分析，以探究FMM公式的鲁棒性。", "result": "结果表明，FMM在年度清除活动中提供了对高风险目标的优越识别能力，但在不同操作清除频率下，风险模型之间存在细微的性能权衡。分析还证实，基于物理的质量项对于实际风险评估不可或缺。", "conclusion": "本研究通过提供经过验证的开源工具和对风险动态的关键见解，增强了选择最佳ADR目标并确保LEO操作长期可行性的能力。", "translation": "低地球轨道（LEO）中不断增加的空间碎片危及长期轨道可持续性，需要对主动碎片清除（ADR）任务进行有效的风险评估。本研究开发并验证了过滤修正MITRI（FMM），一个增强的风险指数，旨在改善高危碎片的优先级排序。利用MOCAT-MC模拟框架，我们进行了全面的性能评估和敏感性分析，以探究FMM公式的鲁棒性。结果表明，虽然FMM在年度清除活动中提供了对高风险目标的优越识别能力，但在不同操作清除频率下，风险模型之间存在细微的性能权衡。分析还证实，基于物理的质量项对于实际风险评估不可或缺。通过提供经过验证的开源工具和对风险动态的关键见解，本研究增强了我们选择最佳ADR目标并确保LEO操作长期可行性的能力。", "summary": "本研究针对低地球轨道（LEO）空间碎片对轨道可持续性的威胁，开发并验证了FMM（Filtered Modified MITRI）风险指数，旨在优化主动碎片清除（ADR）任务中高危碎片的识别和优先级排序。通过MOCAT-MC模拟框架的评估，FMM被证实能有效识别高风险目标，但其性能与清除频率有关。研究强调了物理质量项在风险评估中的重要性，并提供了一个开源工具，以提升ADR目标选择的效率和LEO运行的长期可行性。", "keywords": "空间碎片, 主动碎片清除, 风险评估, LEO, FMM", "comments": "该论文创新性地提出了FMM风险指数，为LEO空间碎片清除提供了更精准的风险评估工具。其亮点在于对风险模型在不同清除频率下的性能权衡进行了深入分析，并强调了物理质量项的重要性。该研究不仅提供了实用的开源工具，也为未来碎片清除策略的制定提供了关键见解，对于维护LEO轨道的可持续性具有重要意义。"}}
{"id": "2503.18377", "title": "Maximum Redundancy Pruning: A Principle-Driven Layerwise Sparsity Allocation for LLMs", "authors": ["Chang Gao", "Kang Zhao", "Runqi Wang", "Jianfei Chen", "Liping Jing"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.18377v2", "summary": "Large language models (LLMs) have demonstrated impressive capabilities, but\ntheir enormous size poses significant challenges for deployment in real-world\napplications. To address this issue, researchers have sought to apply network\npruning techniques to LLMs. A critical challenge in pruning is allocation the\nsparsity for each layer. Recent sparsity allocation methods is often based on\nheuristics or search that can easily lead to suboptimal performance. In this\npaper, we conducted an extensive investigation into various LLMs and revealed\nthree significant discoveries: (1) the layerwise pruning sensitivity (LPS) of\nLLMs is highly non-uniform, (2) the choice of pruning metric affects LPS, and\n(3) the performance of a sparse model is related to the uniformity of its\nlayerwise redundancy level. Based on these observations, we propose that the\nlayerwise sparsity of LLMs should adhere to three principles:\n\\emph{non-uniformity}, \\emph{pruning metric dependency}, and \\emph{uniform\nlayerwise redundancy level} in the pruned model. To this end, we proposed\nMaximum Redundancy Pruning (MRP), an iterative pruning algorithm that prunes in\nthe most redundant layers (\\emph{i.e.}, those with the highest non-outlier\nratio) at each iteration. The achieved layerwise sparsity aligns with the\noutlined principles. We conducted extensive experiments on publicly available\nLLMs, including the LLaMA2 and OPT, across various benchmarks. Experimental\nresults validate the effectiveness of MRP, demonstrating its superiority over\nprevious methods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.18377v2", "cate": "cs.LG", "date": "2025-03-24", "updated": "2025-07-25", "AI": {"title_translation": "最大冗余剪枝：一种面向LLM的原理驱动层级稀疏性分配", "tldr": "本文提出了一种名为最大冗余剪枝（MRP）的新型剪枝算法，通过遵循非均匀性、剪枝度量依赖性和均匀层级冗余度等原则，为大型语言模型（LLMs）分配层级稀疏性，有效解决了传统剪枝方法中的次优性能问题，并在多个LLM和基准测试上展现出优越性。", "motivation": "大型语言模型（LLMs）的巨大规模给实际应用中的部署带来了显著挑战。为解决此问题，研究人员尝试应用网络剪枝技术，但关键挑战在于如何为每个层分配稀疏性。现有稀疏性分配方法常基于启发式或搜索，容易导致次优性能。", "method": "本文对各种LLMs进行了广泛研究，发现了三个重要现象：(1) LLMs的层级剪枝敏感性（LPS）高度非均匀；(2) 剪枝度量的选择会影响LPS；(3) 稀疏模型的性能与层级冗余度水平的均匀性有关。基于这些发现，提出了LLMs的层级稀疏性应遵循三个原则：非均匀性、剪枝度量依赖性以及剪枝模型中均匀的层级冗余度水平。为此，提出了最大冗余剪枝（MRP），这是一种迭代剪枝算法，每次迭代都在最冗余的层（即非异常值比例最高的层）进行剪枝。所实现的层级稀疏性与上述原则一致。", "result": "在包括LLaMA2和OPT在内的公开LLMs上，以及各种基准测试中进行了广泛实验。实验结果验证了MRP的有效性，表明其优于现有方法。", "conclusion": "最大冗余剪枝（MRP）通过遵循非均匀性、剪枝度量依赖性和均匀层级冗余度等原则，有效地解决了LLMs层级稀疏性分配的挑战，并实现了优于现有方法的剪枝性能。", "translation": "大型语言模型（LLMs）展现出令人印象深刻的能力，但其庞大的规模给在实际应用中的部署带来了显著挑战。为解决此问题，研究人员试图将网络剪枝技术应用于LLMs。剪枝中的一个关键挑战是为每个层分配稀疏性。最近的稀疏性分配方法通常基于启发式或搜索，这很容易导致次优性能。在本文中，我们对各种LLMs进行了广泛调查，并揭示了三个重要发现：(1) LLMs的层级剪枝敏感性（LPS）高度非均匀，(2) 剪枝度量的选择会影响LPS，以及 (3) 稀疏模型的性能与其层级冗余度水平的均匀性有关。基于这些观察，我们提出LLMs的层级稀疏性应遵循三个原则：非均匀性、剪枝度量依赖性以及剪枝模型中均匀的层级冗余度水平。为此，我们提出了最大冗余剪枝（MRP），这是一种迭代剪枝算法，每次迭代都在最冗余的层（即非异常值比例最高的层）进行剪枝。所实现的层级稀疏性与所概述的原则相符。我们在包括LLaMA2和OPT在内的公开LLMs上，以及各种基准测试中进行了广泛实验。实验结果验证了MRP的有效性，表明其优于现有方法。", "summary": "本文针对大型语言模型（LLMs）部署中的规模挑战，提出了一种新的剪枝方法——最大冗余剪枝（MRP）。通过对LLMs的深入研究，发现层级剪枝敏感性（LPS）的非均匀性、剪枝度量对LPS的影响以及层级冗余度均匀性与模型性能的关系。基于这些发现，MRP遵循非均匀性、剪枝度依赖性和均匀层级冗余度三大原则进行层级稀疏性分配。该迭代算法在每次迭代中剪枝最冗余的层。实验结果表明，MRP在LLaMA2和OPT等LLMs上表现出优于现有方法的有效性。", "keywords": "大型语言模型, 模型剪枝, 稀疏性分配, 最大冗余剪枝, 层级敏感性", "comments": "该论文通过揭示LLMs剪枝的内在规律并提出原理驱动的分配方法，而非依赖启发式或搜索，为LLMs的剪枝领域带来了创新。其提出的最大冗余剪枝（MRP）算法，基于对层级冗余度的深入理解，有望在保持模型性能的同时显著减小LLMs的规模，对于实际部署具有重要意义。"}}
{"id": "2507.18982", "title": "Classifying Issues in Open-source GitHub Repositories", "authors": ["Amir Hossain Raaj", "Fairuz Nawer Meem", "Sadia Afrin Mim"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18982v1", "summary": "GitHub is the most widely used platform for software maintenance in the\nopen-source community. Developers report issues on GitHub from time to time\nwhile facing difficulties. Having labels on those issues can help developers\neasily address those issues with prior knowledge of labels. However, most of\nthe GitHub repositories do not maintain regular labeling for the issues. The\ngoal of this work is to classify issues in the open-source community using ML\n\\& DNN models. There are thousands of open-source repositories on GitHub. Some\nof the repositories label their issues properly whereas some of them do not.\nWhen issues are pre-labeled, the problem-solving process and the immediate\nassignment of corresponding personnel are facilitated for the team, thereby\nexpediting the development process. In this work, we conducted an analysis of\nprominent GitHub open-source repositories. We classified the issues in some\ncommon labels which are: API, Documentation, Enhancement, Question, Easy,\nHelp-wanted, Dependency, CI, Waiting for OP's response, Test, Bug, etc. Our\nstudy shows that DNN models outperf", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18982v1", "cate": "cs.SE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "对开源GitHub仓库中问题的分类", "tldr": "本研究旨在利用机器学习和深度神经网络模型对开源GitHub仓库中的问题进行自动分类，以解决现有仓库中问题标签缺失的问题，并发现DNN模型表现更优。", "motivation": "GitHub开源社区中，开发者在遇到问题时会报告问题，但大多数GitHub仓库没有对这些问题进行定期标签维护。为问题添加标签有助于开发者通过预先了解标签来轻松解决问题，并促进问题解决过程和人员分配，从而加快开发进程。", "method": "本研究分析了知名的GitHub开源仓库，并使用机器学习和深度神经网络（DNN）模型对问题进行了分类。分类标签包括：API、文档、增强、问题、简单、需要帮助、依赖、CI、等待OP响应、测试、Bug等。", "result": "研究表明，DNN模型表现优于（此处原文截断，未完全提及具体表现）。", "conclusion": "Not mentioned in abstract", "translation": "GitHub是开源社区中软件维护最广泛使用的平台。开发者在遇到困难时会不时地在GitHub上报告问题。为这些问题打上标签可以帮助开发者通过预先了解标签轻松解决这些问题。然而，大多数GitHub仓库没有对问题进行定期标签维护。这项工作的目标是使用机器学习和深度神经网络（DNN）模型对开源社区中的问题进行分类。GitHub上有成千上万的开源仓库。有些仓库对他们的问题进行了适当的标记，而有些则没有。当问题被预先标记时，团队的问题解决过程和相应人员的即时分配会变得便利，从而加快开发过程。在这项工作中，我们对知名的GitHub开源仓库进行了分析。我们将问题分类为一些常见的标签，例如：API、文档、增强、问题、简单、需要帮助、依赖、CI、等待OP响应、测试、Bug等。我们的研究表明，DNN模型表现优于", "summary": "本研究旨在解决开源GitHub仓库中问题标签缺失的问题，这阻碍了开发者的问题解决和开发效率。通过对知名GitHub仓库的分析，研究人员利用机器学习和深度神经网络模型对问题进行了分类，并使用了API、文档、Bug等多种常见标签。初步结果表明，深度神经网络模型在分类性能上表现更优。", "keywords": "GitHub, 问题分类, 深度学习, 机器学习, 开源仓库", "comments": "Not mentioned in abstract"}}
{"id": "2507.19045", "title": "A New One-Shot Federated Learning Framework for Medical Imaging Classification with Feature-Guided Rectified Flow and Knowledge Distillation", "authors": ["Yufei Ma", "Hanwen Zhang", "Qiya Yang", "Guibo Luo", "Yuesheng Zhu"], "categories": ["cs.CV", "cs.DC"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ECAI 2025", "url": "http://arxiv.org/abs/2507.19045v1", "summary": "In multi-center scenarios, One-Shot Federated Learning (OSFL) has attracted\nincreasing attention due to its low communication overhead, requiring only a\nsingle round of transmission. However, existing generative model-based OSFL\nmethods suffer from low training efficiency and potential privacy leakage in\nthe healthcare domain. Additionally, achieving convergence within a single\nround of model aggregation is challenging under non-Independent and Identically\nDistributed (non-IID) data. To address these challenges, in this paper a\nmodified OSFL framework is proposed, in which a new Feature-Guided Rectified\nFlow Model (FG-RF) and Dual-Layer Knowledge Distillation (DLKD) aggregation\nmethod are developed. FG-RF on the client side accelerates generative modeling\nin medical imaging scenarios while preserving privacy by synthesizing\nfeature-level images rather than pixel-level images. To handle non-IID\ndistributions, DLKD enables the global student model to simultaneously mimic\nthe output logits and align the intermediate-layer features of client-side\nteacher models during aggregation. Experimental results on three non-IID\nmedical imaging datasets show that our new framework and method outperform\nmulti-round federated learning approaches, achieving up to 21.73% improvement,\nand exceeds the baseline FedISCA by an average of 21.75%. Furthermore, our\nexperiments demonstrate that feature-level synthetic images significantly\nreduce privacy leakage risks compared to pixel-level synthetic images.", "comment": "Accepted at ECAI 2025", "pdf_url": "http://arxiv.org/pdf/2507.19045v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "一种用于医学图像分类的新型单次联邦学习框架，结合特征引导整流流和知识蒸馏", "tldr": "本文提出了一种新的单次联邦学习（OSFL）框架，通过特征引导整流流（FG-RF）和双层知识蒸馏（DLKD）来解决现有OSFL在医学图像分类中训练效率低、隐私泄露和非IID数据下收敛困难的问题，并在实验中取得了显著的性能提升和隐私保护效果。", "motivation": "现有的基于生成模型的单次联邦学习（OSFL）方法在医疗健康领域存在训练效率低和潜在隐私泄露的问题。此外，在非独立同分布（non-IID）数据下，单轮模型聚合难以实现收敛。", "method": "本文提出了一种改进的单次联邦学习（OSFL）框架。该框架开发了两种新方法：特征引导整流流模型（FG-RF）和双层知识蒸馏（DLKD）聚合方法。FG-RF在客户端通过合成特征级图像而非像素级图像，加速医学图像场景中的生成建模并保护隐私。DLKD通过使全局学生模型在聚合过程中同时模仿输出logits并对齐客户端教师模型的中间层特征，来处理非IID数据分布。", "result": "在三个非独立同分布（non-IID）医学图像数据集上的实验结果表明，该新框架和方法优于多轮联邦学习方法，性能提升高达21.73%，并且比基线FedISCA平均高出21.75%。此外，实验证明特征级合成图像相比像素级合成图像显著降低了隐私泄露风险。", "conclusion": "本文提出的单次联邦学习框架，通过引入特征引导整流流和双层知识蒸馏，有效解决了医学图像分类中OSFL的训练效率、隐私保护和非IID数据下的收敛性挑战，并取得了优异的性能和隐私保护效果。", "translation": "在多中心场景中，单次联邦学习（OSFL）因其低通信开销，仅需一轮传输而受到越来越多的关注。然而，现有基于生成模型的OSFL方法存在训练效率低和在医疗健康领域潜在隐私泄露的问题。此外，在非独立同分布（non-IID）数据下，单轮模型聚合难以实现收敛。为了解决这些挑战，本文提出了一种改进的OSFL框架，其中开发了新的特征引导整流流模型（FG-RF）和双层知识蒸馏（DLKD）聚合方法。客户端的FG-RF通过合成特征级图像而非像素级图像，加速医学图像场景中的生成建模，同时保护隐私。为了处理非IID分布，DLKD使全局学生模型在聚合过程中能够同时模仿输出logits并对齐客户端教师模型的中间层特征。在三个非IID医学图像数据集上的实验结果表明，我们的新框架和方法优于多轮联邦学习方法，实现了高达21.73%的改进，并比基线FedISCA平均高出21.75%。此外，我们的实验证明，与像素级合成图像相比，特征级合成图像显著降低了隐私泄露风险。", "summary": "本文提出了一种新的单次联邦学习（OSFL）框架，旨在解决医学图像分类中现有OSFL方法面临的训练效率低、隐私泄露和非IID数据下收敛困难等问题。该框架引入了特征引导整流流模型（FG-RF）用于在客户端生成特征级图像以加速建模并保护隐私，以及双层知识蒸馏（DLKD）聚合方法用于在非IID数据下实现模型收敛。实验结果表明，该框架在医学图像数据集上显著优于现有方法，提升了分类性能并有效降低了隐私泄露风险。", "keywords": "单次联邦学习,医学图像分类,特征引导整流流,知识蒸馏,隐私保护", "comments": "该论文提出了一种创新的单次联邦学习框架，特别针对医学图像领域的需求。其创新点在于引入了特征引导整流流（FG-RF）以在特征级别而非像素级别进行图像合成，有效解决了隐私保护和训练效率问题。同时，双层知识蒸馏（DLKD）的引入也巧妙地处理了联邦学习中常见的非IID数据分布挑战。这些方法的结合使得单次联邦学习在实际应用中更具可行性和鲁棒性，尤其在对隐私敏感的医疗领域具有重要意义。"}}
{"id": "2410.00015", "title": "A Multitask VAE for Time Series Preprocessing and Prediction of Blood Glucose Level", "authors": ["Ali AbuSaleh", "Mehdi Rahim"], "categories": ["eess.SP"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2410.00015v2", "summary": "Data preprocessing is a critical part of time series data analysis. Data from\nconnected medical devices often have missing or abnormal values during\nacquisition. Handling such situations requires additional assumptions and\ndomain knowledge. This can be time-consuming, and can introduce a significant\nbias affecting predictive model accuracy and thus, medical interpretation. To\novercome this issue, we propose a new deep learning model to mitigate the\npreprocessing assumptions. The model architecture relies on a variational\nauto-encoder (VAE) to produce a preprocessing latent space, and a recurrent VAE\nto preserve the temporal dynamics of the data. We demonstrate the effectiveness\nof such an architecture on telemonitoring data to forecast glucose-level of\ndiabetic patients. Our results show an improvement in terms of accuracy with\nrespect of existing state-of-the-art methods and architectures.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2410.00015v2", "cate": "eess.SP", "date": "2024-09-14", "updated": "2025-07-25", "AI": {"title_translation": "用于时间序列预处理和血糖水平预测的多任务VAE", "tldr": "本文提出了一种基于变分自编码器（VAE）的深度学习模型，用于处理医疗时间序列数据中的缺失/异常值，并提高血糖水平预测的准确性。", "motivation": "连接医疗设备的数据在采集过程中常有缺失或异常值，处理这些数据耗时且可能引入显著偏差，影响预测模型准确性和医学解释。为克服这一问题，本文提出了一种新的深度学习模型来减少预处理假设。", "method": "该模型架构依赖于一个变分自编码器（VAE）来生成预处理潜在空间，以及一个循环VAE来保留数据的时间动态。", "result": "在远程监测数据上对糖尿病患者的血糖水平预测进行了验证，结果显示，相对于现有最先进的方法和架构，该模型在准确性方面有所提高。", "conclusion": "本文提出的多任务VAE模型能有效处理时间序列数据预处理问题，并显著提高血糖水平预测的准确性，减少了对额外假设和领域知识的需求。", "translation": "数据预处理是时间序列数据分析的关键部分。来自连接医疗设备的数据在采集过程中通常存在缺失或异常值。处理这种情况需要额外的假设和领域知识。这可能非常耗时，并可能引入显著偏差，影响预测模型的准确性，从而影响医学解释。为了克服这个问题，我们提出了一种新的深度学习模型来减轻预处理假设。模型架构依赖于一个变分自编码器（VAE）来产生一个预处理潜在空间，以及一个循环VAE来保留数据的时间动态。我们证明了这种架构在远程监测数据上预测糖尿病患者血糖水平的有效性。我们的结果显示，相对于现有最先进的方法和架构，准确性方面有所提高。", "summary": "本文提出了一种多任务变分自编码器（VAE）深度学习模型，旨在解决医疗时间序列数据（特别是血糖水平数据）在预处理过程中常见的缺失和异常值问题。该模型结合了VAE用于生成预处理潜在空间和循环VAE用于保留时间动态，旨在减少传统预处理方法对额外假设和领域知识的依赖。实验结果表明，该模型在血糖水平预测的准确性方面优于现有先进方法。", "keywords": "多任务VAE, 时间序列预处理, 血糖预测, 缺失数据, 深度学习", "comments": "本文的创新之处在于将VAE和循环VAE结合，形成一个多任务模型，同时解决时间序列数据的预处理（缺失/异常值）和预测问题。这种方法减少了对人工预处理的依赖，降低了引入偏差的风险，对于医疗健康领域的时间序列分析具有重要意义。该模型在提高预测准确性方面展现出潜力。"}}
{"id": "2503.10799", "title": "Fixed-Point RNNs: Interpolating from Diagonal to Dense", "authors": ["Sajad Movahedi", "Felix Sarnthein", "Nicola Muca Cirone", "Antonio Orvieto"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.10799v2", "summary": "Linear recurrent neural networks (RNNs) and state-space models (SSMs) such as\nMamba have become promising alternatives to softmax-attention as sequence\nmixing layers in Transformer architectures. Current models, however, do not\nexhibit the full state-tracking expressivity of RNNs because they rely on\nchannel-wise (i.e. diagonal) sequence mixing. In this paper, we investigate\nparameterizations of a large class of dense linear RNNs as fixed-points of\nparallelizable diagonal linear RNNs. The resulting models can naturally trade\nexpressivity for efficiency at a fixed number of parameters and achieve\nstate-of-the-art results on the commonly used toy tasks $A_5$, $S_5$, copying,\nand modular arithmetics.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.10799v2", "cate": "cs.LG", "date": "2025-03-13", "updated": "2025-07-24", "AI": {"title_translation": "不动点RNNs：从对角线到稠密插值", "tldr": "本文提出了一种将稠密线性RNN参数化为可并行化对角线性RNN不动点的方法，以解决当前线性RNN模型因对角线序列混合而缺乏表达能力的问题，并在基准任务上取得了最先进的结果。", "motivation": "当前的线性循环神经网络（RNNs）和状态空间模型（SSMs）如Mamba，作为Transformer架构中的序列混合层，虽然有前景，但由于依赖于通道式（即对角线）序列混合，未能展现RNNs的完整状态跟踪表达能力。", "method": "本文研究了将一大类稠密线性RNNs参数化为可并行化对角线性RNNs的不动点。", "result": "所产生的模型可以在固定参数数量下自然地权衡表达能力和效率，并在常用的玩具任务（$A_5$, $S_5$, 复制，模算术）上取得了最先进的结果。", "conclusion": "通过将稠密线性RNNs参数化为可并行化对角线性RNNs的不动点，可以有效提升模型的表达能力和性能，同时保持效率，解决现有线性RNNs的局限性。", "translation": "线性循环神经网络（RNNs）和状态空间模型（SSMs）如Mamba，已成为Transformer架构中序列混合层中softmax注意力的有前景替代方案。然而，当前模型未能展现RNNs的完整状态跟踪表达能力，因为它们依赖于通道式（即对角线）序列混合。在本文中，我们研究了将一大类稠密线性RNNs参数化为可并行化对角线性RNNs的不动点。所产生的模型可以在固定参数数量下自然地权衡表达能力和效率，并在常用的玩具任务$A_5$、$S_5$、复制和模算术上取得了最先进的结果。", "summary": "本文提出了一种新型的“不动点RNNs”，旨在解决现有线性RNNs和状态空间模型（如Mamba）因采用对角线序列混合而导致表达能力受限的问题。研究方法是将稠密线性RNNs参数化为可并行化对角线性RNNs的不动点。实验结果表明，这种模型能够在保持参数量不变的情况下，有效平衡表达能力和计算效率，并在多个经典玩具任务（$A_5$, $S_5$, 复制, 模算术）上达到了最先进的性能。", "keywords": "不动点RNNs, 对角线, 稠密, 状态空间模型, 序列混合", "comments": "该论文创新性地将稠密线性RNNs建模为对角线性RNNs的不动点，有效解决了现有模型在表达能力上的局限性。这种方法不仅提升了模型性能，还在效率与表达能力之间提供了灵活的权衡机制，对于推动线性RNNs在序列建模领域的发展具有重要意义。"}}
{"id": "2507.18778", "title": "CityHood: An Explainable Travel Recommender System for Cities and Neighborhoods", "authors": ["Gustavo H Santos", "Myriam Delgado", "Thiago H Silva"], "categories": ["cs.IR", "cs.SI"], "primary_category": "Subjects:       Information Retrieval (cs.IR)", "pdf_link": null, "comments": "Comments:      Accepted at ASONAM'25", "url": "http://arxiv.org/abs/2507.18778v1", "summary": "We present CityHood, an interactive and explainable recommendation system\nthat suggests cities and neighborhoods based on users' areas of interest. The\nsystem models user interests leveraging large-scale Google Places reviews\nenriched with geographic, socio-demographic, political, and cultural\nindicators. It provides personalized recommendations at city (Core-Based\nStatistical Areas - CBSAs) and neighborhood (ZIP code) levels, supported by an\nexplainable technique (LIME) and natural-language explanations. Users can\nexplore recommendations based on their stated preferences and inspect the\nreasoning behind each suggestion through a visual interface. The demo\nillustrates how spatial similarity, cultural alignment, and interest\nunderstanding can be used to make travel recommendations transparent and\nengaging. This work bridges gaps in location-based recommendation by combining\na kind of interest modeling, multi-scale analysis, and explainability in a\nuser-facing system.", "comment": "Accepted at ASONAM'25", "pdf_url": "http://arxiv.org/pdf/2507.18778v1", "cate": "cs.IR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "CityHood：一个针对城市和社区的可解释旅行推荐系统", "tldr": "CityHood是一个交互式、可解释的旅行推荐系统，根据用户兴趣推荐城市和社区，并提供基于LIME的解释。", "motivation": "该研究旨在弥补现有基于位置推荐系统在兴趣建模、多尺度分析和可解释性方面的不足，提供一个用户友好的透明推荐系统。", "method": "CityHood系统通过利用大规模Google Places评论，并结合地理、社会人口、政治和文化指标来建模用户兴趣。它在城市（CBSAs）和社区（邮政编码）层面提供个性化推荐，并使用LIME技术和自然语言解释来增强可解释性，用户可以通过可视化界面查看推荐理由。", "result": "演示展示了空间相似性、文化一致性和兴趣理解如何被用于使旅行推荐变得透明和具有吸引力。", "conclusion": "该工作通过在一个面向用户的系统中结合兴趣建模、多尺度分析和可解释性，弥补了基于位置推荐领域的现有空白。", "translation": "我们提出了CityHood，一个交互式、可解释的推荐系统，它根据用户的兴趣领域推荐城市和社区。该系统利用大规模Google Places评论，并结合地理、社会人口、政治和文化指标来建模用户兴趣。它在城市（核心统计区 - CBSAs）和社区（邮政编码）层面提供个性化推荐，并由一种可解释技术（LIME）和自然语言解释支持。用户可以根据他们陈述的偏好探索推荐，并通过可视化界面检查每个建议背后的推理。该演示说明了空间相似性、文化一致性和兴趣理解如何被用于使旅行推荐透明和具有吸引力。这项工作通过在一个面向用户的系统中结合一种兴趣建模、多尺度分析和可解释性，弥补了基于位置推荐的现有空白。", "summary": "CityHood是一个交互式、可解释的旅行推荐系统，旨在根据用户兴趣推荐城市和社区。它通过整合Google Places评论以及多维指标来建模用户兴趣，并在城市和社区两个粒度层面提供个性化推荐。系统利用LIME技术和自然语言解释，通过可视化界面向用户展示推荐理由，从而提高了推荐的透明度和用户参与度。该研究通过结合兴趣建模、多尺度分析和可解释性，填补了现有基于位置推荐系统的一些空白。", "keywords": "旅行推荐, 可解释人工智能, CityHood, 基于位置服务, 用户兴趣建模", "comments": "CityHood的创新之处在于其将多尺度分析（城市和社区层面）、丰富的兴趣建模（结合多种指标）与强大的可解释性（LIME和自然语言解释）整合到一个用户面向的旅行推荐系统中。这显著提升了用户对推荐的信任度和满意度，解决了传统推荐系统“黑箱”问题，具有重要的实践意义和研究价值。"}}
{"id": "2507.18816", "title": "ThermoRL:Structure-Aware Reinforcement Learning for Protein Mutation Design to Enhance Thermostability", "authors": ["Xiangwen Wang", "Gaojie Jin", "Xiaowei Huang", "Ronghui Mu"], "categories": ["cs.CE"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18816v1", "summary": "Designing mutations to optimize protein thermostability remains challenging\ndue to the complex relationship between sequence variations, structural\ndynamics, and thermostability, often assessed by \\delta\\delta G\n  (the change in free energy of unfolding). Existing methods rely on\nexperimental random mutagenesis or prediction models tested with pre-defined\ndatasets, using sequence-based heuristics and treating enzyme design as a\none-step process without iterative refinement, which limits design space\nexploration and restricts discoveries beyond known variations. We present\nThermoRL, a framework based on reinforcement learning (RL) that leverages graph\nneural networks (GNN) to design mutations with enhanced thermostability. It\ncombines a pre-trained GNN-based encoder with a hierarchical Q-learning network\nand employs a surrogate model for reward feedback, guiding the RL agent on\nwhere (the position) and which (mutant amino acid) to apply for enhanced\nthermostability. Experimental results show that ThermoRL achieves higher or\ncomparable rewards than baselines while maintaining computational efficiency.\nIt filters out destabilizing mutations and identifies stabilizing mutations\naligned with experimental data. Moreover, ThermoRL accurately detects key\nmutation sites in unseen proteins, highlighting its strong generalizability.\nThis RL-guided approach powered by GNN embeddings offers a robust alternative\nto traditional protein mutation design.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18816v1", "cate": "cs.CE", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "ThermoRL: 结构感知强化学习用于蛋白质突变设计以增强热稳定性", "tldr": "ThermoRL是一个基于强化学习和图神经网络的框架，用于设计蛋白质突变以提高热稳定性，表现出良好的性能和泛化能力。", "motivation": "设计突变以优化蛋白质热稳定性具有挑战性，原因在于序列变异、结构动力学和热稳定性之间复杂的相互关系。现有方法依赖实验随机诱变或预定义数据集上的预测模型，使用基于序列的启发式方法，并将酶设计视为一步过程，缺乏迭代细化，限制了设计空间探索和新发现。", "method": "本文提出了ThermoRL框架，结合强化学习（RL）和图神经网络（GNN）。它包含一个预训练的基于GNN的编码器和一个分层Q学习网络，并采用一个替代模型进行奖励反馈，指导RL代理确定突变位置和氨基酸类型。", "result": "ThermoRL与基线相比，实现了更高或相当的奖励，同时保持计算效率。它能过滤掉去稳定突变并识别与实验数据一致的稳定突变。此外，ThermoRL能准确检测未见蛋白质中的关键突变位点，显示出强大的泛化能力。", "conclusion": "这种由GNN嵌入驱动的RL引导方法为传统蛋白质突变设计提供了一种强大的替代方案。", "translation": "设计突变以优化蛋白质热稳定性仍然具有挑战性，原因在于序列变异、结构动力学和热稳定性之间复杂的相互关系，热稳定性通常通过\\delta\\delta G（展开自由能的变化）进行评估。现有方法依赖于实验随机诱变或在预定义数据集上测试的预测模型，使用基于序列的启发式方法，并将酶设计视为一步过程，没有迭代细化，这限制了设计空间探索并限制了对已知变异之外的发现。我们提出了ThermoRL，一个基于强化学习（RL）的框架，它利用图神经网络（GNN）来设计具有增强热稳定性的突变。它结合了一个预训练的基于GNN的编码器和一个分层Q学习网络，并采用一个替代模型进行奖励反馈，指导RL智能体确定在何处（位置）以及应用哪种（突变氨基酸）以增强热稳定性。实验结果表明，ThermoRL在保持计算效率的同时，实现了比基线更高或相当的奖励。它过滤掉去稳定突变并识别与实验数据一致的稳定突变。此外，ThermoRL能准确检测未见蛋白质中的关键突变位点，突出了其强大的泛化能力。这种由GNN嵌入驱动的RL引导方法为传统蛋白质突变设计提供了一种强大的替代方案。", "summary": "ThermoRL是一个创新的强化学习框架，结合了图神经网络，用于优化蛋白质热稳定性。它通过迭代设计过程克服了传统方法的局限性，能够有效地识别稳定突变并具有强大的泛化能力，为蛋白质突变设计提供了新的范式。", "keywords": "蛋白质突变设计, 热稳定性, 强化学习, 图神经网络, ThermoRL", "comments": "该论文通过引入强化学习和图神经网络，为蛋白质突变设计提供了一个新颖的、结构感知的解决方案。其创新之处在于将酶设计视为一个迭代过程，并通过奖励反馈机制有效探索设计空间。该方法在提高热稳定性和泛化能力方面表现出色，为蛋白质工程领域带来了重要进展。"}}
{"id": "2507.17800", "title": "Improving Multislice Electron Ptychography with a Generative Prior", "authors": ["Christian K. Belardi", "Chia-Hao Lee", "Yingheng Wang", "Justin Lovelace", "Kilian Q. Weinberger", "David A. Muller", "Carla P. Gomes"], "categories": ["eess.IV", "cond-mat.mtrl-sci", "cs.CV", "physics.optics"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      16 pages, 10 figures, 5 tables", "url": "http://arxiv.org/abs/2507.17800v2", "summary": "Multislice electron ptychography (MEP) is an inverse imaging technique that\ncomputationally reconstructs the highest-resolution images of atomic crystal\nstructures from diffraction patterns. Available algorithms often solve this\ninverse problem iteratively but are both time consuming and produce suboptimal\nsolutions due to their ill-posed nature. We develop MEP-Diffusion, a diffusion\nmodel trained on a large database of crystal structures specifically for MEP to\naugment existing iterative solvers. MEP-Diffusion is easily integrated as a\ngenerative prior into existing reconstruction methods via Diffusion Posterior\nSampling (DPS). We find that this hybrid approach greatly enhances the quality\nof the reconstructed 3D volumes, achieving a 90.50% improvement in SSIM over\nexisting methods.", "comment": "16 pages, 10 figures, 5 tables", "pdf_url": "http://arxiv.org/pdf/2507.17800v2", "cate": "eess.IV", "date": "2025-07-23", "updated": "2025-07-25", "AI": {"title_translation": "使用生成先验改进多层电子叠层衍射成像", "tldr": "本文开发了MEP-Diffusion，一个基于扩散模型的生成先验，用于增强多层电子叠层衍射成像(MEP)的重建质量，实现了显著的SSIM改进。", "motivation": "现有的多层电子叠层衍射成像(MEP)算法在迭代求解逆问题时耗时且由于病态性导致次优解。", "method": "本文开发了MEP-Diffusion，一个专门针对MEP在大型晶体结构数据库上训练的扩散模型。它通过扩散后采样(DPS)作为生成先验集成到现有重建方法中，以增强迭代求解器。", "result": "这种混合方法显著提高了重建3D体积的质量，与现有方法相比，结构相似性指数(SSIM)提高了90.50%。", "conclusion": "通过引入基于扩散模型的生成先验（MEP-Diffusion），可以显著提高多层电子叠层衍射成像的重建质量，克服现有迭代算法的局限性。", "translation": "多层电子叠层衍射成像（MEP）是一种逆成像技术，它通过计算从衍射图样中重建原子晶体结构的最高分辨率图像。现有算法通常迭代地解决这个逆问题，但由于其病态性，既耗时又产生次优解。我们开发了MEP-Diffusion，这是一个专门为MEP在大型晶体结构数据库上训练的扩散模型，旨在增强现有的迭代求解器。MEP-Diffusion通过扩散后采样（DPS）可以轻松地作为生成先验集成到现有重建方法中。我们发现这种混合方法极大地提升了重建三维体积的质量，与现有方法相比，SSIM提高了90.50%。", "summary": "本文提出MEP-Diffusion，一个基于扩散模型的生成先验，旨在解决多层电子叠层衍射成像（MEP）中现有迭代算法耗时且重建质量次优的问题。MEP-Diffusion在大型晶体结构数据库上训练，并通过扩散后采样（DPS）集成到现有重建方法中。实验结果表明，该混合方法显著提升了3D体积重建质量，SSIM相较于现有方法提高了90.50%。", "keywords": "电子叠层衍射成像, 扩散模型, 生成先验, 图像重建, 逆问题", "comments": "这项工作通过引入扩散模型作为生成先验，有效地解决了多层电子叠层衍射成像中逆问题的病态性及其导致的重建质量和效率问题。90.50%的SSIM提升是一个非常显著的改进，表明该方法在提高图像重建质量方面具有巨大潜力。将生成模型与传统迭代求解器结合的混合方法是当前高维逆问题求解的一个重要方向。"}}
{"id": "2507.19474", "title": "DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations", "authors": ["Ziren Gong", "Xiaohan Li", "Fabio Tosi", "Youmin Zhang", "Stefano Mattoccia", "Jun Wu", "Matteo Poggi"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19474v1", "summary": "This paper presents DINO-SLAM, a DINO-informed design strategy to enhance\nneural implicit (Neural Radiance Field -- NeRF) and explicit representations\n(3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive\nscene representations. Purposely, we rely on a Scene Structure Encoder (SSE)\nthat enriches DINO features into Enhanced DINO ones (EDINO) to capture\nhierarchical scene elements and their structural relationships. Building upon\nit, we propose two foundational paradigms for NeRF and 3DGS SLAM systems\nintegrating EDINO features. Our DINO-informed pipelines achieve superior\nperformance on the Replica, ScanNet, and TUM compared to state-of-the-art\nmethods.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19474v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "DINO-SLAM：用于神经隐式和显式表示的DINO增强型RGB-D SLAM", "tldr": "DINO-SLAM通过DINO增强特征改进了神经隐式（NeRF）和显式（3DGS）SLAM系统，实现了更全面的场景表示，并在多个数据集上表现优异。", "motivation": "通过更全面的场景表示来增强SLAM系统中神经隐式（NeRF）和显式（3DGS）表示的性能。", "method": "提出DINO-SLAM，一个DINO增强的设计策略。该方法依赖于一个场景结构编码器（SSE），将DINO特征丰富为增强型DINO特征（EDINO），以捕获分层场景元素及其结构关系。在此基础上，提出了两种将EDINO特征整合到NeRF和3DGS SLAM系统中的基础范式。", "result": "DINO增强的管道在Replica、ScanNet和TUM数据集上与最先进的方法相比，实现了卓越的性能。", "conclusion": "DINO-SLAM通过引入DINO增强特征，显著提升了神经隐式和显式表示在SLAM系统中的性能。", "translation": "本文提出了DINO-SLAM，这是一种DINO增强的设计策略，旨在通过更全面的场景表示来增强SLAM系统中神经隐式（神经辐射场——NeRF）和显式表示（3D高斯泼溅——3DGS）。为此，我们依赖于一个场景结构编码器（SSE），它将DINO特征丰富为增强型DINO特征（EDINO），以捕获分层场景元素及其结构关系。在此基础上，我们提出了两种将EDINO特征整合到NeRF和3DGS SLAM系统中的基础范式。我们的DINO增强型管道在Replica、ScanNet和TUM数据集上与最先进的方法相比，实现了卓越的性能。", "summary": "DINO-SLAM提出了一种新的DINO增强设计策略，旨在提升基于神经隐式（NeRF）和显式（3DGS）表示的SLAM系统性能。该方法引入了一个场景结构编码器（SSE），将DINO特征转化为增强型DINO特征（EDINO），以更好地捕捉场景的层次结构和关系。通过将EDINO特征集成到NeRF和3DGS SLAM系统中，DINO-SLAM在多个标准数据集上展现出优于现有先进方法的性能。", "keywords": "DINO-SLAM, NeRF, 3DGS, SLAM, 场景表示", "comments": "该论文的创新点在于将DINO特征与SLAM系统中的神经隐式和显式表示相结合，通过增强的特征表示来提升场景理解和重建的质量。其提出的场景结构编码器（SSE）是一个关键的创新，有助于捕获更丰富的场景信息。该方法在多个数据集上的优异表现证明了其有效性和潜力。"}}
{"id": "2507.18838", "title": "Flow Stochastic Segmentation Networks", "authors": ["Fabio De Sousa Ribeiro", "Omar Todd", "Charles Jones", "Avinash Kori", "Raghav Mehta", "Ben Glocker"], "categories": ["cs.CV", "cs.AI", "stat.ML"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted at ICCV 2025", "url": "http://arxiv.org/abs/2507.18838v1", "summary": "We introduce the Flow Stochastic Segmentation Network (Flow-SSN), a\ngenerative segmentation model family featuring discrete-time autoregressive and\nmodern continuous-time flow variants. We prove fundamental limitations of the\nlow-rank parameterisation of previous methods and show that Flow-SSNs can\nestimate arbitrarily high-rank pixel-wise covariances without assuming the rank\nor storing the distributional parameters. Flow-SSNs are also more efficient to\nsample from than standard diffusion-based segmentation models, thanks to most\nof the model capacity being allocated to learning the base distribution of the\nflow, constituting an expressive prior. We apply Flow-SSNs to challenging\nmedical imaging benchmarks and achieve state-of-the-art results. Code\navailable: https://github.com/biomedia-mira/flow-ssn.", "comment": "Accepted at ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.18838v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "流随机分割网络", "tldr": "Flow-SSN是一种新型的生成式分割模型，能解决现有方法低秩参数化的局限性，高效估计高秩像素协方差，并在医学图像分割中取得最先进的结果。", "motivation": "现有分割方法存在低秩参数化的根本局限性，无法估计高秩像素级协方差。Flow-SSN旨在克服这一限制，实现任意高秩像素级协方差的估计，同时提高采样效率。", "method": "本文引入了流随机分割网络（Flow-SSN），这是一个生成式分割模型家族，包括离散时间自回归和现代连续时间流变体。Flow-SSN通过将大部分模型容量分配给学习流的基础分布，构成了一个富有表现力的先验，从而能够估计任意高秩的像素级协方差，而无需假设秩或存储分布参数。与标准基于扩散的分割模型相比，Flow-SSN的采样效率更高。", "result": "Flow-SSN在具有挑战性的医学图像基准测试中取得了最先进的结果。", "conclusion": "Flow-SSN通过解决现有方法的低秩参数化限制，有效地估计了高秩像素级协方差，并在医学图像分割任务中实现了最先进的性能，证明了其优越性和实用性。", "translation": "我们引入了流随机分割网络（Flow-SSN），这是一个生成式分割模型家族，具有离散时间自回归和现代连续时间流变体。我们证明了以前方法低秩参数化的根本局限性，并表明Flow-SSN可以在不假设秩或存储分布参数的情况下，估计任意高秩的像素级协方差。Flow-SSN也比标准的基于扩散的分割模型采样效率更高，这得益于大部分模型容量被分配给学习流的基础分布，从而构成了一个富有表现力的先验。我们将Flow-SSN应用于具有挑战性的医学图像基准测试，并取得了最先进的结果。代码可用：https://github.com/biomedia-mira/flow-ssn。", "summary": "本文提出了一种新型的生成式分割模型——流随机分割网络（Flow-SSN），包含离散时间自回归和连续时间流两种变体。该模型克服了现有方法低秩参数化的局限性，能够高效、无参数地估计任意高秩的像素级协方差。与基于扩散的模型相比，Flow-SSN的采样效率更高，并在医学图像分割基准测试中达到了最先进的性能。", "keywords": "生成式分割, 流模型, 随机分割网络, 医学图像, 高秩协方差", "comments": "Flow-SSN的创新之处在于其解决了传统分割模型在处理高秩像素级协方差时的局限性，通过流模型的高效采样和强大的先验学习能力，实现了性能和效率的双重提升。其在医学图像分割领域取得的SOTA结果，凸显了该模型在实际应用中的重要性和潜力。"}}
{"id": "2507.18842", "title": "Towards reliable use of artificial intelligence to classify otitis media using otoscopic images: Addressing bias and improving data quality", "authors": ["Yixi Xu", "Al-Rahim Habib", "Graeme Crossland", "Hemi Patel", "Chris Perry", "Kris Bock", "Tony Lian", "William B. Weeks", "Rahul Dodhia", "Juan Lavista Ferres", "Narinder Pal Singh"], "categories": ["cs.CY"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18842v1", "summary": "Ear disease contributes significantly to global hearing loss, with recurrent\notitis media being a primary preventable cause in children, impacting\ndevelopment. Artificial intelligence (AI) offers promise for early diagnosis\nvia otoscopic image analysis, but dataset biases and inconsistencies limit\nmodel generalizability and reliability. This retrospective study systematically\nevaluated three public otoscopic image datasets (Chile; Ohio, USA; T\\\"urkiye)\nusing quantitative and qualitative methods. Two counterfactual experiments were\nperformed: (1) obscuring clinically relevant features to assess model reliance\non non-clinical artifacts, and (2) evaluating the impact of hue, saturation,\nand value on diagnostic outcomes. Quantitative analysis revealed significant\nbiases in the Chile and Ohio, USA datasets. Counterfactual Experiment I found\nhigh internal performance (AUC > 0.90) but poor external generalization,\nbecause of dataset-specific artifacts. The T\\\"urkiye dataset had fewer biases,\nwith AUC decreasing from 0.86 to 0.65 as masking increased, suggesting higher\nreliance on clinically meaningful features. Counterfactual Experiment II\nidentified common artifacts in the Chile and Ohio, USA datasets. A logistic\nregression model trained on clinically irrelevant features from the Chile\ndataset achieved high internal (AUC = 0.89) and external (Ohio, USA: AUC =\n0.87) performance. Qualitative analysis identified redundancy in all the\ndatasets and stylistic biases in the Ohio, USA dataset that correlated with\nclinical outcomes. In summary, dataset biases significantly compromise\nreliability and generalizability of AI-based otoscopic diagnostic models.\nAddressing these biases through standardized imaging protocols, diverse dataset\ninclusion, and improved labeling methods is crucial for developing robust AI\nsolutions, improving high-quality healthcare access, and enhancing diagnostic\naccuracy.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18842v1", "cate": "cs.CY", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "利用耳镜图像对中耳炎进行分类的人工智能可靠性研究：解决偏差和提高数据质量", "tldr": "耳镜图像AI诊断中耳炎模型受数据集偏差影响严重，导致泛化性和可靠性差；需标准化数据和多样性数据集以提升AI模型鲁棒性。", "motivation": "耳部疾病导致全球听力损失，中耳炎是儿童可预防的主要原因。人工智能在耳镜图像分析早期诊断方面有前景，但数据集偏差和不一致性限制了模型的泛化性和可靠性。", "method": "本回顾性研究系统评估了三个公共耳镜图像数据集（智利、美国俄亥俄州、土耳其），使用定量和定性方法。进行了两次反事实实验：1) 遮蔽临床相关特征以评估模型对非临床伪影的依赖；2) 评估色调、饱和度和亮度对诊断结果的影响。", "result": "定量分析显示智利和俄亥俄州数据集存在显著偏差。反事实实验I发现高内部性能（AUC > 0.90）但外部泛化性差，原因是数据集特有的伪影。土耳其数据集偏差较少，AUC随遮蔽增加而从0.86降至0.65，表明更依赖临床有意义的特征。反事实实验II识别出智利和俄亥俄州数据集中的常见伪影。一个在智利数据集临床无关特征上训练的逻辑回归模型获得了高内部（AUC = 0.89）和外部（俄亥俄州：AUC = 0.87）性能。定性分析发现所有数据集存在冗余，俄亥俄州数据集存在与临床结果相关的风格偏差。", "conclusion": "数据集偏差严重损害了基于AI的耳镜诊断模型的可靠性和泛化性。通过标准化成像协议、纳入多样化数据集和改进标注方法来解决这些偏差，对于开发鲁棒的AI解决方案、提高高质量医疗保健的可及性和增强诊断准确性至关重要。", "translation": "耳部疾病是全球听力损失的重要原因，其中复发性中耳炎是儿童可预防的主要原因，影响其发育。人工智能（AI）有望通过耳镜图像分析实现早期诊断，但数据集偏差和不一致性限制了模型的泛化性和可靠性。本回顾性研究系统评估了三个公共耳镜图像数据集（智利；美国俄亥俄州；土耳其），使用了定量和定性方法。进行了两次反事实实验：(1) 遮蔽临床相关特征以评估模型对非临床伪影的依赖，以及 (2) 评估色调、饱和度和亮度对诊断结果的影响。定量分析显示智利和美国俄亥俄州数据集存在显著偏差。反事实实验I发现高内部性能（AUC > 0.90）但外部泛化性差，原因是数据集特有的伪影。土耳其数据集偏差较少，AUC随遮蔽增加而从0.86降至0.65，表明更依赖临床有意义的特征。反事实实验II识别出智利和美国俄亥俄州数据集中的常见伪影。一个在智利数据集临床无关特征上训练的逻辑回归模型获得了高内部（AUC = 0.89）和外部（美国俄亥俄州：AUC = 0.87）性能。定性分析发现所有数据集存在冗余，美国俄亥俄州数据集存在与临床结果相关的风格偏差。总之，数据集偏差严重损害了基于AI的耳镜诊断模型的可靠性和泛化性。通过标准化成像协议、纳入多样化数据集和改进标注方法来解决这些偏差，对于开发鲁棒的AI解决方案、提高高质量医疗保健的可及性和增强诊断准确性至关重要。", "summary": "本研究旨在解决AI在耳镜图像诊断中耳炎时面临的数据集偏差和模型可靠性问题。通过对三个公共数据集的定量和定性分析及反事实实验，发现数据集偏差（尤其是非临床伪影）严重影响AI模型的泛化性，导致高内部性能但外部泛化性差。研究强调了标准化成像协议、多样化数据集和改进标注方法对于构建鲁棒AI诊断模型的关键作用。", "keywords": "中耳炎, 人工智能, 数据偏差, 耳镜图像, 模型泛化性", "comments": "该研究揭示了医疗AI模型在实际应用中面临的普遍问题，即数据集偏差对模型可靠性和泛化性的严重影响。其通过反事实实验深入分析了模型对非临床特征的依赖，具有创新性。研究强调了数据质量和标准化在医疗AI开发中的关键作用，为未来开发更可靠的AI诊断工具提供了重要指导。"}}
{"id": "2507.19037", "title": "MLLM-based Speech Recognition: When and How is Multimodality Beneficial?", "authors": ["Yiwen Guan", "Viet Anh Trinh", "Vivek Voleti", "Jacob Whitehill"], "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19037v1", "summary": "Recent advances in multi-modal large language models (MLLMs) have opened new\npossibilities for unified modeling of speech, text, images, and other\nmodalities. Building on our prior work, this paper examines the conditions and\nmodel architectures under which multiple input modalities can improve automatic\nspeech recognition (ASR) accuracy in noisy environments. Through experiments on\nsynthetic and real-world data, we find that (1) harnessing more modalities\nusually improves ASR accuracy, as each modality provides complementary\ninformation, but the improvement depends on the amount of auditory noise. (2)\nSynchronized modalities (e.g., lip movements) are more useful at high noise\nlevels whereas unsynchronized modalities (e.g., image context) are most helpful\nat moderate noise levels. (3) Higher-quality visual representations\nconsistently improve ASR accuracy, highlighting the importance of developing\nmore powerful visual encoders. (4) Mamba exhibits similar trends regarding the\nbenefits of multimodality as do Transformers. (5) The input order of modalities\nas well as their weights in the loss function can significantly impact\naccuracy. These findings both offer practical insights and help to deepen our\nunderstanding of multi-modal speech recognition under challenging conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19037v1", "cate": "cs.SD", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "MLLM语音识别：多模态何时以及如何有益？", "tldr": "本文研究了在嘈杂环境下，多模态输入如何以及何时能提高基于MLLM的语音识别准确性，发现多模态通常有益，且其效用取决于噪声水平、模态同步性、视觉表示质量、模型架构和输入权重。", "motivation": "探索多模态大语言模型（MLLMs）在语音、文本、图像等统一建模方面的新可能性，并具体研究在嘈杂环境下，多模态输入在何种条件和模型架构下能提高自动语音识别（ASR）的准确性。", "method": "通过在合成数据和真实世界数据上进行实验。", "result": "1. 利用更多模态通常能提高ASR准确性，因为每种模态都提供互补信息，但这种改进取决于听觉噪声的量。\n2. 同步模态（如唇部动作）在高噪声水平下更有用，而非同步模态（如图像上下文）在中等噪声水平下最有用。\n3. 更高质量的视觉表示持续提高ASR准确性，这强调了开发更强大的视觉编码器的重要性。\n4. Mamba在多模态效益方面表现出与Transformers相似的趋势。\n5. 模态的输入顺序以及它们在损失函数中的权重可以显著影响准确性。", "conclusion": "这些发现既提供了实践性的见解，也有助于加深对挑战条件下多模态语音识别的理解。", "translation": "标题：基于MLLM的语音识别：多模态何时以及如何有益？\n\n摘要：多模态大语言模型（MLLMs）的最新进展为语音、文本、图像及其他模态的统一建模开辟了新的可能性。基于我们之前的工作，本文研究了在嘈杂环境下，多输入模态在何种条件和模型架构下可以提高自动语音识别（ASR）的准确性。通过在合成数据和真实世界数据上的实验，我们发现：(1) 利用更多模态通常能提高ASR准确性，因为每种模态都提供互补信息，但这种改进取决于听觉噪声的量。(2) 同步模态（例如，唇部动作）在高噪声水平下更有用，而非同步模态（例如，图像上下文）在中等噪声水平下最有用。(3) 更高质量的视觉表示持续提高ASR准确性，这强调了开发更强大的视觉编码器的重要性。(4) Mamba在多模态效益方面表现出与Transformers相似的趋势。(5) 模态的输入顺序以及它们在损失函数中的权重可以显著影响准确性。这些发现既提供了实践性的见解，也有助于加深我们对挑战条件下多模态语音识别的理解。", "summary": "本文研究了在嘈杂环境下，多模态输入（如语音、文本、图像）如何以及何时能提高基于多模态大语言模型（MLLMs）的自动语音识别（ASR）准确性。通过实验发现，多模态通常能提供互补信息以提高ASR性能，但其效益受噪声水平、模态同步性、视觉表示质量、模型架构选择（Mamba与Transformers）以及模态输入顺序和权重的影响。研究结果为在复杂条件下进行多模态语音识别提供了实用指导和更深层次的理解。", "keywords": "多模态大语言模型, 语音识别, 噪声环境, 模态融合, 视觉表示", "comments": "本文深入探讨了多模态在语音识别中的应用，特别是在嘈杂环境下的有效性。其创新点在于细致分析了不同模态的贡献、同步与非同步模态的差异、视觉表示质量的重要性以及模型架构和输入参数对性能的影响。这些发现对于开发更鲁棒的多模态语音识别系统具有重要的实践指导意义。"}}
{"id": "2507.19419", "title": "TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability", "authors": ["Mohammad Aflah Khan", "Ameya Godbole", "Johnny Tian-Zheng Wei", "Ryan Wang", "James Flemings", "Krishna Gummadi", "Willie Neiswanger", "Robin Jia"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19419v1", "summary": "Understanding the relationship between training data and model behavior\nduring pretraining is crucial, but existing workflows make this process\ncumbersome, fragmented, and often inaccessible to researchers. We present\nTokenSmith, an open-source library for interactive editing, inspection, and\nanalysis of datasets used in Megatron-style pretraining frameworks such as\nGPT-NeoX, Megatron, and NVIDIA NeMo. TokenSmith supports a wide range of\noperations including searching, viewing, ingesting, exporting, inspecting, and\nsampling data, all accessible through a simple user interface and a modular\nbackend. It also enables structured editing of pretraining data without\nrequiring changes to training code, simplifying dataset debugging, validation,\nand experimentation.\n  TokenSmith is designed as a plug and play addition to existing large language\nmodel pretraining workflows, thereby democratizing access to production-grade\ndataset tooling. TokenSmith is hosted on GitHub1, with accompanying\ndocumentation and tutorials. A demonstration video is also available on\nYouTube.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19419v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "TokenSmith：简化大规模语言模型训练和可解释性的数据编辑、搜索和检查", "tldr": "TokenSmith是一个开源库，用于交互式编辑、检查和分析大型语言模型预训练数据集，旨在简化数据工作流程并提高可访问性。", "motivation": "在预训练期间理解训练数据与模型行为之间的关系至关重要，但现有工作流程繁琐、零碎且研究人员难以访问。", "method": "本文介绍了TokenSmith，一个开源库，用于交互式编辑、检查和分析Megatron风格预训练框架（如GPT-NeoX、Megatron和NVIDIA NeMo）中使用的数据集。它支持搜索、查看、摄取、导出、检查和采样数据等操作，并通过简单的用户界面和模块化后端实现。它还允许对预训练数据进行结构化编辑，而无需更改训练代码。", "result": "TokenSmith简化了数据集的调试、验证和实验，并作为现有大型语言模型预训练工作流程的即插即用补充，从而普及了生产级数据集工具的访问。", "conclusion": "TokenSmith是一个旨在简化大型语言模型预训练数据处理和可解释性工作流程的开源库，它通过提供全面的数据操作功能和易于使用的界面来解决现有工具的不足。", "translation": "在预训练期间理解训练数据与模型行为之间的关系至关重要，但现有工作流程使这一过程繁琐、零碎，并且通常研究人员难以访问。我们介绍了TokenSmith，一个开源库，用于交互式编辑、检查和分析Megatron风格预训练框架（如GPT-NeoX、Megatron和NVIDIA NeMo）中使用的数据集。TokenSmith支持广泛的操作，包括搜索、查看、摄取、导出、检查和采样数据，所有这些都可以通过简单的用户界面和模块化后端访问。它还可以在不要求更改训练代码的情况下对预训练数据进行结构化编辑，从而简化数据集的调试、验证和实验。\nTokenSmith被设计为现有大型语言模型预训练工作流程的即插即用附加组件，从而普及了生产级数据集工具的访问。TokenSmith托管在GitHub1上，并附有文档和教程。演示视频也可在YouTube上观看。", "summary": "TokenSmith是一个开源库，旨在解决大型语言模型预训练数据处理中存在的复杂性和可访问性问题。它提供了一个用户友好的界面和模块化后端，支持对Megatron风格数据集进行广泛操作，包括编辑、搜索、查看和分析。该工具简化了数据集的调试、验证和实验，并能作为现有预训练工作流程的即插即用组件，从而使生产级数据集工具更易于获取。", "keywords": "TokenSmith, 大型语言模型, 数据编辑, 预训练, 可解释性", "comments": "TokenSmith的创新之处在于它提供了一个集成的、开源的解决方案，用于大型语言模型的数据编辑、检查和分析，而无需修改训练代码。它的“即插即用”设计极大地降低了研究人员和开发者访问和使用生产级数据工具的门槛，对于提高预训练数据质量、简化调试流程以及促进模型可解释性具有重要意义。"}}
{"id": "2507.19354", "title": "EffiComm: Bandwidth Efficient Multi Agent Communication", "authors": ["Melih Yazgan", "Allen Xavier Arasan", "J. Marius Zöllner"], "categories": ["cs.CV", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted for publication at ITSC 2025", "url": "http://arxiv.org/abs/2507.19354v1", "summary": "Collaborative perception allows connected vehicles to exchange sensor\ninformation and overcome each vehicle's blind spots. Yet transmitting raw point\nclouds or full feature maps overwhelms Vehicle-to-Vehicle (V2V) communications,\ncausing latency and scalability problems. We introduce EffiComm, an end-to-end\nframework that transmits less than 40% of the data required by prior art while\nmaintaining state-of-the-art 3D object detection accuracy. EffiComm operates on\nBird's-Eye-View (BEV) feature maps from any modality and applies a two-stage\nreduction pipeline: (1) Selective Transmission (ST) prunes low-utility regions\nwith a confidence mask; (2) Adaptive Grid Reduction (AGR) uses a Graph Neural\nNetwork (GNN) to assign vehicle-specific keep ratios according to role and\nnetwork load. The remaining features are fused with a soft-gated\nMixture-of-Experts (MoE) attention layer, offering greater capacity and\nspecialization for effective feature integration. On the OPV2V benchmark,\nEffiComm reaches 0.84 mAP@0.7 while sending only an average of approximately\n1.5 MB per frame, outperforming previous methods on the accuracy-per-bit curve.\nThese results highlight the value of adaptive, learned communication for\nscalable Vehicle-to-Everything (V2X) perception.", "comment": "Accepted for publication at ITSC 2025", "pdf_url": "http://arxiv.org/pdf/2507.19354v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "EffiComm：带宽高效多智能体通信", "tldr": "EffiComm是一个端到端框架，通过两阶段缩减管道和MoE注意力层，显著降低了多智能体通信的数据传输量，同时保持了最先进的3D目标检测精度。", "motivation": "协作感知中的原始点云或完整特征图传输会使车车（V2V）通信过载，导致延迟和可伸缩性问题。", "method": "本研究提出了EffiComm框架，该框架在鸟瞰图（BEV）特征图上运行，并应用两阶段缩减管道：1. 选择性传输（ST）使用置信度掩码修剪低效区域；2. 自适应网格缩减（AGR）使用图神经网络（GNN）根据车辆角色和网络负载分配车辆特定的保留率。剩余特征通过软门控专家混合（MoE）注意力层进行融合。", "result": "在OPV2V基准测试中，EffiComm在保持最先进3D目标检测精度的同时，数据传输量不到现有技术的40%，平均每帧仅发送约1.5 MB，达到0.84 mAP@0.7，在每比特精度曲线上优于现有方法。", "conclusion": "自适应、学习型通信对于可扩展的车路协同（V2X）感知具有重要价值。", "translation": "协作感知允许联网车辆交换传感器信息并克服每辆车的盲点。然而，传输原始点云或完整特征图会使车车（V2V）通信不堪重负，导致延迟和可伸缩性问题。我们引入了EffiComm，这是一个端到端框架，其传输数据量不到现有技术的40%，同时保持了最先进的3D目标检测精度。EffiComm在任何模态的鸟瞰图（BEV）特征图上运行，并应用两阶段缩减管道：(1) 选择性传输（ST）使用置信度掩码修剪低效区域；(2) 自适应网格缩减（AGR）使用图神经网络（GNN）根据角色和网络负载分配车辆特定的保留率。剩余特征通过软门控专家混合（MoE）注意力层进行融合，为有效的特征集成提供了更大的容量和专业化。在OPV2V基准测试中，EffiComm在发送平均每帧约1.5 MB的数据量时达到了0.84 mAP@0.7，在每比特精度曲线上优于现有方法。这些结果突出了自适应、学习型通信对于可扩展的车路协同（V2X）感知的价值。", "summary": "EffiComm是一个端到端框架，旨在解决协作感知中V2V通信因传输大量数据（如原始点云或完整特征图）导致的延迟和可伸缩性问题。该框架通过两阶段缩减管道（选择性传输和自适应网格缩减）显著减少了数据传输量，同时保持了高精度的3D目标检测性能。EffiComm在OPV2V基准测试中表现出色，以更少的数据量实现了更高的精度，证明了自适应学习通信在可扩展V2X感知中的潜力。", "keywords": "协作感知, 带宽效率, 多智能体通信, V2X, 3D目标检测", "comments": "EffiComm的创新之处在于其结合了选择性传输和自适应网格缩减的两阶段数据压缩策略，以及使用软门控MoE注意力层进行特征融合，有效地解决了多智能体协作感知中的带宽限制问题。该方法在保证高检测精度的前提下大幅降低了通信负载，对于推动V2X通信和自动驾驶技术的发展具有重要意义。"}}
{"id": "2507.19195", "title": "Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?", "authors": ["Chaymaa Abbas", "Mariette Awad", "Razane Tajeddine"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19195v1", "summary": "Despite the ongoing improvements in the design of large language models\n(LLMs) to foster inclusion and balanced responses, these systems remain\nsusceptible to encoding and amplifying social biases. This study examines how\ndialectal variation, specifically African American Vernacular English (AAVE)\nversus Standard American English (SAE), interacts with data poisoning to\ninfluence toxicity in outputs. Using both small- and medium-scale LLaMA models,\nwe show that even minimal exposure to poisoned data significantly increases\ntoxicity for AAVE inputs, while it remains comparatively unaffected for SAE.\nLarger models exhibit a more significant amplification effect which suggests\nheightened susceptibility with scale. To further assess these disparities, we\nemployed GPT-4o as a fairness auditor, which identified harmful stereotypical\npatterns disproportionately tied to AAVE inputs, including portrayals of\naggression, criminality, and intellectual inferiority. These findings\nunderscore the compounding impact of data poisoning and dialectal bias and\nemphasize the need for dialect-aware evaluation, targeted debiasing\ninterventions, and socially responsible training protocols during development.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19195v1", "cate": "cs.CL", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "小规模数据投毒会加剧大型语言模型中的方言相关偏见吗？", "tldr": "研究发现，即使是少量投毒数据，也会显著增加大型语言模型对非洲裔美国人白话英语（AAVE）输入的毒性输出，而对标准美式英语（SAE）影响不大，且模型规模越大，偏见放大效应越明显。", "motivation": "尽管大型语言模型（LLMs）在促进包容性和平衡响应方面不断改进，但它们仍然容易编码和放大社会偏见。本研究旨在探讨方言变体（特别是非洲裔美国人白话英语AAVE与标准美式英语SAE）如何与数据投毒相互作用，从而影响输出的毒性。", "method": "本研究使用小型和中型LLaMA模型，并通过GPT-4o作为公平性审计工具来评估输出差异。", "result": "研究表明，即使是最小程度的投毒数据暴露，也会显著增加AAVE输入的毒性，而SAE输入则相对不受影响。更大的模型表现出更显著的放大效应，这表明随着模型规模的扩大，其脆弱性也随之增加。GPT-4o作为公平性审计工具，识别出与AAVE输入不成比例相关的有害刻板印象模式，包括攻击性、犯罪性和智力低下等描述。", "conclusion": "这些发现强调了数据投毒和方言偏见的复合影响，并强调了在模型开发过程中，需要进行方言感知评估、有针对性的去偏见干预以及对社会负责的训练协议。", "translation": "尽管大型语言模型（LLMs）在促进包容性和平衡响应方面不断改进，但这些系统仍然容易编码和放大社会偏见。本研究探讨了方言变体，特别是非洲裔美国人白话英语（AAVE）与标准美式英语（SAE），如何与数据投毒相互作用，从而影响输出的毒性。我们使用小型和中型LLaMA模型，结果显示即使是最小程度的投毒数据暴露，也会显著增加AAVE输入的毒性，而对SAE的影响相对较小。更大的模型表现出更显著的放大效应，这表明随着规模的扩大，其脆弱性也随之增加。为了进一步评估这些差异，我们使用GPT-4o作为公平性审计工具，它识别出与AAVE输入不成比例相关的有害刻板印象模式，包括攻击性、犯罪性和智力低下等描述。这些发现强调了数据投毒和方言偏见的复合影响，并强调在开发过程中，需要进行方言感知评估、有针对性的去偏见干预以及对社会负责的训练协议。", "summary": "本研究探讨了数据投毒如何加剧大型语言模型中与方言相关的偏见，特别是在非洲裔美国人白话英语（AAVE）与标准美式英语（SAE）之间。结果显示，即使是少量投毒数据，也会显著增加模型对AAVE输入的毒性输出，而对SAE影响甚微，且模型越大，这种偏见放大效应越明显。通过GPT-4o的审计发现，AAVE输入被不成比例地与攻击性、犯罪性及智力低下等刻板印象关联。研究强调了数据投毒和方言偏见的复合影响，并呼吁在LLM开发中采取方言感知评估和负责任的训练方法。", "keywords": "数据投毒, 方言偏见, 大型语言模型, 非洲裔美国人白话英语, 毒性输出", "comments": "这项研究揭示了大型语言模型中一个关键且复杂的偏见来源：数据投毒与方言差异的交互作用。其创新之处在于量化了即使是小规模投毒也能对特定方言（如AAVE）造成不成比例的负面影响，并利用先进的LLM（如GPT-4o）作为审计工具来识别有害的刻板印象。这项工作的重要性在于，它不仅指出了现有LLM的脆弱性，还为未来LLM的公平性评估和去偏见策略提供了明确的方向，特别是在处理语言多样性方面。"}}
{"id": "2507.15855", "title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025", "authors": ["Yichen Huang", "Lin F. Yang"], "categories": ["cs.AI"], "primary_category": "Subjects:       Artificial Intelligence (cs.AI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.15855v3", "summary": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. Using a\nself-verification pipeline with careful prompt design, 5 (out of 6) problems\nare solved correctly. This result underscores the importance of developing\noptimal strategies to harness the full potential of powerful LLMs for complex\nreasoning tasks.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.15855v3", "cate": "cs.AI", "date": "2025-07-21", "updated": "2025-07-25", "AI": {"title_translation": "Gemini 2.5 Pro 有望赢得 2025 年国际数学奥林匹克竞赛金牌", "tldr": "Gemini 2.5 Pro 结合自验证流程和精心设计的提示，在 2025 年国际数学奥林匹克竞赛的 6 道问题中正确解决了 5 道，显示了大型语言模型在复杂推理任务中的巨大潜力。", "motivation": "国际数学奥林匹克竞赛（IMO）的问题对大型语言模型（LLMs）构成了巨大挑战，尽管LLMs在其他数学基准测试中表现良好，但在奥林匹克级别的任务上仍面临困难。", "method": "研究使用了谷歌的 Gemini 2.5 Pro 模型，并将其应用于新发布的 2025 年国际数学奥林匹克竞赛问题，以避免数据污染。通过采用自验证流程和精心设计的提示，模型尝试解决这些问题。", "result": "Gemini 2.5 Pro 在 6 道 2025 年国际数学奥林匹克竞赛问题中成功正确解决了 5 道。", "conclusion": "这项研究强调了开发最佳策略的重要性，以充分发挥强大大型语言模型在复杂推理任务中的潜力。", "translation": "国际数学奥林匹克竞赛（IMO）提出了独具挑战性的问题，需要深刻的洞察力、创造力和形式推理能力。尽管大型语言模型（LLMs）在AIME等数学基准测试中表现良好，但它们在奥林匹克级别的任务上却面临困难。我们使用谷歌的Gemini 2.5 Pro 处理新发布的IMO 2025问题，避免了数据污染。通过采用自验证流程和精心设计的提示，6道问题中有5道被正确解决。这一结果强调了开发最佳策略的重要性，以充分发挥强大LLMs在复杂推理任务中的全部潜力。", "summary": "本研究旨在评估大型语言模型在国际数学奥林匹克竞赛（IMO）级别问题上的表现。研究人员利用谷歌的Gemini 2.5 Pro 模型，结合自验证流程和精心设计的提示，成功解决了 2025 年 IMO 6 道问题中的 5 道。这一结果表明，通过优化策略，强大的LLMs在处理复杂的数学推理任务方面具有巨大潜力。", "keywords": "大型语言模型, 国际数学奥林匹克竞赛, Gemini 2.5 Pro, 自验证, 复杂推理", "comments": "这项研究展示了大型语言模型在解决顶尖数学竞赛问题方面的突破性进展。通过结合强大的模型（Gemini 2.5 Pro）和创新的方法（自验证流程、精心设计的提示），研究团队有效克服了LLMs在复杂推理任务上的固有弱点，为未来AI在解决高难度智力挑战方面开辟了新路径。其创新性在于策略而非模型本身，强调了“如何用”的重要性。"}}
{"id": "2507.19378", "title": "Plug and Play Splitting Techniques for Poisson Image Restoration", "authors": ["Alessandro Benfenati"], "categories": ["math.NA", "cs.NA"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19378v1", "summary": "Plug and Play (PnP) methods achieve remarkable results in the framework of\nimage restoration problems for Gaussian data. Nonetheless, the theory available\nfor the Gaussian case cannot be extended to the Poisson case, due to the\nnon-Lipschitz gradient of the fidelity function, the Kullback-Leibler\nfunctional, or the absence of closed-form solution for the proximal operator of\nsuch term, leading to employ iterative solvers for the inner subproblem. In\nthis work we extend the idea of PIDSplit+ algorithm, exploiting the Alternating\nDirection Method of Multipliers, to PnP scheme: this allows to provide a closed\nform solution for the deblurring step, with no need for iterative solvers. The\nconvergence of the method is assured by employing a firmly non expansive\ndenoiser. The proposed method, namely PnPSplit+, is tested on different Poisson\nimage restoration problems, showing remarkable performance even in presence of\nhigh noise level and severe blurring conditions.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19378v1", "cate": "math.NA", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "即插即用分裂技术用于泊松图像复原", "tldr": "本文提出了一种名为PnPSplit+的即插即用（PnP）分裂方法，用于泊松图像复原。它解决了现有PnP方法在泊松数据中遇到的非Lipschitz梯度和缺乏闭合解的问题，通过利用交替方向乘子法（ADMM）为去模糊步骤提供闭合解，并在高噪声和严重模糊条件下表现出色。", "motivation": "现有的即插即用（PnP）方法在处理高斯数据图像复原问题时表现出色，但其理论不能直接扩展到泊松数据。这主要是因为泊松数据对应的保真函数（Kullback-Leibler泛函）的梯度非Lipschitz，或者其近端算子缺乏闭合解，导致需要迭代求解内部子问题。因此，需要一种新的方法来解决泊松图像复原中的这些限制。", "method": "本文将PIDSplit+算法的思想扩展到即插即用（PnP）框架中，并利用交替方向乘子法（ADMM）。这种方法为去模糊步骤提供了闭合形式的解决方案，无需迭代求解器。通过采用一个严格非扩张的去噪器，确保了方法的收敛性。", "result": "所提出的PnPSplit+方法在不同的泊松图像复原问题上进行了测试，即使在高噪声水平和严重模糊条件下，也表现出卓越的性能。", "conclusion": "本文提出的PnPSplit+方法成功地将即插即用（PnP）方案应用于泊松图像复原，通过提供去模糊步骤的闭合解并确保收敛性，解决了现有方法的局限性，并在挑战性条件下展现了卓越的性能。", "translation": "即插即用（PnP）方法在高斯数据图像复原问题的框架中取得了显著成果。然而，由于保真函数（Kullback-Leibler泛函）的非Lipschitz梯度，或该项近端算子缺乏闭合解，导致需要对内部子问题采用迭代求解器，因此适用于高斯情况的理论不能扩展到泊松情况。在这项工作中，我们利用交替方向乘子法，将PIDSplit+算法的思想扩展到PnP方案：这使得去模糊步骤能够提供闭合形式的解，无需迭代求解器。通过采用一个严格非扩张的去噪器，确保了方法的收敛性。所提出的方法，即PnPSplit+，在不同的泊松图像复原问题上进行了测试，即使在高噪声水平和严重模糊条件下，也表现出卓越的性能。", "summary": "本文提出了一种名为PnPSplit+的即插即用（PnP）分裂技术，专门用于泊松图像复原。针对传统PnP方法在泊松数据上因Kullback-Leibler泛函的非Lipschitz梯度或近端算子无闭合解而面临的挑战，该方法通过将PIDSplit+算法与交替方向乘子法（ADMM）相结合，为去模糊步骤提供了闭合形式的解决方案，从而避免了迭代求解。通过使用严格非扩张的去噪器确保了方法的收敛性。实验结果表明，PnPSplit+在处理高噪声和严重模糊的泊松图像复原问题时，依然展现出卓越的性能。", "keywords": "即插即用, 泊松图像复原, 分裂技术, ADMM, 图像去模糊", "comments": "这篇论文的创新点在于成功地将即插即用（PnP）框架扩展到泊松图像复原领域，解决了现有PnP方法在高斯数据和泊松数据之间理论不兼容的问题。通过引入基于ADMM的PIDSplit+思想，实现了去模糊步骤的闭合解，大大提高了计算效率，避免了耗时的迭代求解。此外，对收敛性的理论保证也增加了方法的可靠性。该方法在高噪声和严重模糊条件下的优异表现，使其在实际应用中具有重要价值。"}}
{"id": "2507.18975", "title": "Secure Best Arm Identification in the Presence of a Copycat", "authors": ["Asaf Cohen", "Onur Günlü"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      To appear in ITW 2025", "url": "http://arxiv.org/abs/2507.18975v2", "summary": "Consider the problem of best arm identification with a security constraint.\nSpecifically, assume a setup of stochastic linear bandits with $K$ arms of\ndimension $d$. In each arm pull, the player receives a reward that is the sum\nof the dot product of the arm with an unknown parameter vector and independent\nnoise. The player's goal is to identify the best arm after $T$ arm pulls.\nMoreover, assume a copycat Chloe is observing the arm pulls. The player wishes\nto keep Chloe ignorant of the best arm.\n  While a minimax--optimal algorithm identifies the best arm with an\n$\\Omega\\left(\\frac{T}{\\log(d)}\\right)$ error exponent, it easily reveals its\nbest-arm estimate to an outside observer, as the best arms are played more\nfrequently. A naive secure algorithm that plays all arms equally results in an\n$\\Omega\\left(\\frac{T}{d}\\right)$ exponent. In this paper, we propose a secure\nalgorithm that plays with \\emph{coded arms}. The algorithm does not require any\nkey or cryptographic primitives, yet achieves an\n$\\Omega\\left(\\frac{T}{\\log^2(d)}\\right)$ exponent while revealing almost no\ninformation on the best arm.", "comment": "To appear in ITW 2025", "pdf_url": "http://arxiv.org/pdf/2507.18975v2", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-28", "AI": {"title_translation": "在存在模仿者的情况下安全地识别最佳臂", "tldr": "本文提出一种使用编码臂的安全算法，在随机线性多臂赌博机设置中，既能识别最佳臂，又能防止模仿者获取信息，且无需加密原语，性能优于朴素安全算法。", "motivation": "在随机线性多臂赌博机中，传统的最佳臂识别算法容易向外部观察者（模仿者）泄露最佳臂信息。研究者希望在识别最佳臂的同时，确保模仿者对最佳臂一无所知。", "method": "提出一种安全算法，通过使用“编码臂”进行游戏。该算法不需要任何密钥或密码学原语。", "result": "所提出的安全算法在揭示几乎不泄露最佳臂信息的情况下，实现了 $\\Omega\\left(\\frac{T}{\\log^2(d)}\\right)$ 的误差指数。这优于朴素安全算法的 $\\Omega\\left(\\frac{T}{d}\\right)$ 误差指数，但略逊于揭露信息的最小最大最优算法的 $\\Omega\\left(\\frac{T}{\\log(d)}\\right)$ 误差指数。", "conclusion": "本文提出了一种无需加密原语的安全最佳臂识别算法，能在存在模仿者的情况下有效保护最佳臂信息，并在性能上取得了良好的平衡。", "translation": "考虑具有安全约束的最佳臂识别问题。具体来说，假设一个具有 $K$ 个维度为 $d$ 的臂的随机线性多臂赌博机设置。每次拉动臂时，玩家都会收到奖励，该奖励是臂与未知参数向量的点积加上独立噪声的总和。玩家的目标是在 $T$ 次拉动臂后识别出最佳臂。此外，假设模仿者克洛伊正在观察臂的拉动。玩家希望克洛伊对最佳臂一无所知。虽然最小最大最优算法以 $\\Omega\\left(\\frac{T}{\\log(d)}\\right)$ 的误差指数识别最佳臂，但它很容易向外部观察者泄露其最佳臂估计，因为最佳臂被更频繁地玩。一个朴素的安全算法，即平等地玩所有臂，导致 $\\Omega\\left(\\frac{T}{d}\\right)$ 的指数。在本文中，我们提出了一种使用“编码臂”的安全算法。该算法不需要任何密钥或密码学原语，却实现了 $\\Omega\\left(\\frac{T}{\\log^2(d)}\\right)$ 的指数，同时几乎不泄露关于最佳臂的信息。", "summary": "本文研究了在存在模仿者的情况下，随机线性多臂赌博机的最佳臂识别问题。传统算法容易泄露最佳臂信息，而朴素安全算法效率低下。为此，作者提出了一种基于“编码臂”的新型安全算法。该算法无需任何加密原语，即可在几乎不泄露最佳臂信息的前提下，达到 $\\Omega\\left(\\frac{T}{\\log^2(d)}\\right)$ 的误差指数，在安全性和效率之间取得了有效平衡。", "keywords": "最佳臂识别, 安全, 模仿者, 随机线性多臂赌博机, 编码臂", "comments": "本文的创新点在于提出了“编码臂”的概念，实现了在多臂赌博机设置中，无需传统加密技术即可抵抗信息泄露的安全最佳臂识别。这对于隐私保护和对抗性学习场景具有重要意义。其局限性可能在于性能略低于完全不考虑安全的最佳算法，但在安全约束下提供了更好的实用性。"}}
{"id": "2503.15557", "title": "Motion Synthesis with Sparse and Flexible Keyjoint Control", "authors": ["Inwoo Hwang", "Jinseok Bae", "Donggeun Lim", "Young Min Kim"], "categories": ["cs.GR", "cs.CV", "cs.RO"], "primary_category": "Subjects:       Graphics (cs.GR)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025. Project Page: this http URL", "url": "http://arxiv.org/abs/2503.15557v2", "summary": "Creating expressive character animations is labor-intensive, requiring\nintricate manual adjustment of animators across space and time. Previous works\non controllable motion generation often rely on a predefined set of dense\nspatio-temporal specifications (e.g., dense pelvis trajectories with exact\nper-frame timing), limiting practicality for animators. To process high-level\nintent and intuitive control in diverse scenarios, we propose a practical\ncontrollable motions synthesis framework that respects sparse and flexible\nkeyjoint signals. Our approach employs a decomposed diffusion-based motion\nsynthesis framework that first synthesizes keyjoint movements from sparse input\ncontrol signals and then synthesizes full-body motion based on the completed\nkeyjoint trajectories. The low-dimensional keyjoint movements can easily adapt\nto various control signal types, such as end-effector position for diverse\ngoal-driven motion synthesis, or incorporate functional constraints on a subset\nof keyjoints. Additionally, we introduce a time-agnostic control formulation,\neliminating the need for frame-specific timing annotations and enhancing\ncontrol flexibility. Then, the shared second stage can synthesize a natural\nwhole-body motion that precisely satisfies the task requirement from dense\nkeyjoint movements. We demonstrate the effectiveness of sparse and flexible\nkeyjoint control through comprehensive experiments on diverse datasets and\nscenarios.", "comment": "Accepted to ICCV 2025. Project Page: http://inwoohwang.me/SFControl", "pdf_url": "http://arxiv.org/pdf/2503.15557v2", "cate": "cs.GR", "date": "2025-03-18", "updated": "2025-07-25", "AI": {"title_translation": "稀疏灵活的关键关节控制的运动合成", "tldr": "提出了一种基于分解扩散的运动合成框架，利用稀疏和灵活的关键关节信号来生成富有表现力的角色动画，解决了传统方法中对密集时空规范的依赖问题。", "motivation": "创建富有表现力的角色动画费时费力，需要动画师进行复杂的空间和时间上的手动调整。现有的可控运动生成方法通常依赖预定义的密集时空规范（如精确到帧的密集骨盆轨迹），这限制了动画师的实用性。", "method": "我们提出了一个实用的可控运动合成框架，该框架尊重稀疏和灵活的关键关节信号。该方法采用分解的基于扩散的运动合成框架，首先从稀疏输入控制信号合成关键关节运动，然后基于完成的关键关节轨迹合成全身运动。低维关键关节运动可以轻松适应各种控制信号类型（如末端执行器位置），或包含对关键关节子集的功能约束。此外，我们引入了时间无关的控制公式，消除了对帧特定时间注释的需求，增强了控制灵活性。", "result": "通过在不同数据集和场景下的综合实验，证明了稀疏和灵活的关键关节控制的有效性。", "conclusion": "该研究提出的稀疏灵活的关键关节控制方法，能够有效地解决角色动画生成中对密集时空规范的依赖问题，提高了动画生成的实用性和灵活性。", "translation": "创建富有表现力的角色动画费时费力，需要动画师进行复杂的空间和时间上的手动调整。现有的可控运动生成方法通常依赖预定义的密集时空规范（例如，带有精确每帧时间信息的密集骨盆轨迹），这限制了动画师的实用性。为了在不同场景中处理高级意图和直观控制，我们提出了一种实用的可控运动合成框架，该框架尊重稀疏和灵活的关键关节信号。我们的方法采用分解的基于扩散的运动合成框架，首先从稀疏输入控制信号合成关键关节运动，然后基于完成的关键关节轨迹合成全身运动。低维关键关节运动可以轻松适应各种控制信号类型，例如用于多样化目标驱动运动合成的末端执行器位置，或者在关键关节子集上合并功能约束。此外，我们引入了一种时间无关的控制公式，消除了对帧特定时间注释的需求，增强了控制灵活性。然后，共享的第二阶段可以从密集的关键关节运动中合成自然全身运动，精确满足任务要求。我们通过在不同数据集和场景下的综合实验，证明了稀疏和灵活关键关节控制的有效性。", "summary": "该论文提出了一种名为“稀疏灵活的关键关节控制”的实用可控运动合成框架，旨在解决传统角色动画生成中对密集时空规范的依赖问题。该框架采用分解的扩散模型，首先从稀疏控制信号生成关键关节运动，然后基于这些运动合成全身动画。其创新之处在于支持稀疏和灵活的关键关节输入以及时间无关的控制，显著提高了动画生成的实用性和灵活性。", "keywords": "运动合成, 关键关节控制, 角色动画, 扩散模型, 稀疏控制", "comments": "这项工作在角色动画领域具有重要意义，通过引入稀疏和灵活的关键关节控制，显著降低了动画制作的复杂性。其分解的扩散模型和时间无关的控制公式是主要的创新点，有望提高动画师的工作效率和创作自由度。"}}
{"id": "2507.18803", "title": "Central limit theorems for the eigenvalues of graph Laplacians on data clouds", "authors": ["Chenghui Li", "Nicolás García Trillos", "Housen Li", "Leo Suchan"], "categories": ["stat.ML", "cs.LG", "math.AP", "math.DG", "math.PR", "62G20 60F05 58J50 35P15 68R10 60D05"], "primary_category": "Subjects:       Machine Learning (stat.ML)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18803v1", "summary": "Given i.i.d.\\ samples $X_n =\\{ x_1, \\dots, x_n \\}$ from a distribution\nsupported on a low dimensional manifold ${M}$ embedded in Eucliden space, we\nconsider the graph Laplacian operator $\\Delta_n$ associated to an\n$\\varepsilon$-proximity graph over $X_n$ and study the asymptotic fluctuations\nof its eigenvalues around their means. In particular, letting\n$\\hat{\\lambda}_l^\\varepsilon$ denote the $l$-th eigenvalue of $\\Delta_n$, and\nunder suitable assumptions on the data generating model and on the rate of\ndecay of $\\varepsilon$, we prove that $\\sqrt{n } (\\hat{\\lambda}_{l}^\\varepsilon\n- \\mathbb{E}[\\hat{\\lambda}_{l}^\\varepsilon] )$ is asymptotically Gaussian with\na variance that we can explicitly characterize. A formal argument allows us to\ninterpret this asymptotic variance as the dissipation of a gradient flow of a\nsuitable energy with respect to the Fisher-Rao geometry. This geometric\ninterpretation allows us to give, in turn, a statistical interpretation of the\nasymptotic variance in terms of a Cramer-Rao lower bound for the estimation of\nthe eigenvalues of certain weighted Laplace-Beltrami operator. The latter\ninterpretation suggests a form of asymptotic statistical efficiency for the\neigenvalues of the graph Laplacian. We also present CLTs for multiple\neigenvalues and through several numerical experiments explore the validity of\nour results when some of the assumptions that we make in our theoretical\nanalysis are relaxed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18803v1", "cate": "stat.ML", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "数据云上图拉普拉斯算子特征值的中心极限定理", "tldr": "本文证明了数据云上图拉普拉斯算子特征值的渐近高斯分布，并对其渐近方差进行了几何和统计解释。", "motivation": "研究从低维流形数据中构建的图拉普拉斯算子特征值围绕其均值的渐近波动。", "method": "在欧几里得空间中嵌入的低维流形上，使用独立同分布样本构建$\\varepsilon$-邻近图，并考虑其图拉普拉斯算子$\\Delta_n$。在适当假设下，证明了单个特征值的中心极限定理。通过形式化论证，将渐近方差解释为Fisher-Rao几何下合适能量的梯度流耗散。将渐近方差从统计学角度解释为加权拉普拉斯-贝尔特拉米算子特征值估计的Cramer-Rao下界。还提出了多个特征值的中心极限定理，并通过数值实验验证了结果在假设放宽时的有效性。", "result": "证明了在适当假设下，图拉普拉斯算子的第$l$个特征值$\\hat{\\lambda}_l^\\varepsilon$的波动量$\\sqrt{n } (\\hat{\\lambda}_{l}^\\varepsilon - \\mathbb{E}[\\hat{\\lambda}_{l}^\\varepsilon] )$渐近服从高斯分布，且其方差可明确表征。该渐近方差可被解释为Fisher-Rao几何下梯度流的耗散。该解释进一步提供了统计学上的Cramer-Rao下界，暗示了图拉普拉斯算子特征值的渐近统计效率。也给出了多个特征值的中心极限定理。", "conclusion": "研究结果为数据云上图拉普拉斯算子特征值的行为提供了严谨的理论基础，证明了它们的渐近高斯性，并对其渐近方差给出了深刻的几何和统计解释。这些解释表明图拉普拉斯算子特征值在统计估计上具有渐近效率。", "translation": "给定从嵌入在欧几里得空间中的低维流形M上的分布中独立同分布采样得到的样本$X_n =\\{ x_1, \\dots, x_n \\}$，我们考虑与$X_n$上的$\\varepsilon$-邻近图相关的图拉普拉斯算子$\\Delta_n$，并研究其特征值围绕其均值的渐近波动。具体而言，令$\\hat{\\lambda}_l^\\varepsilon$表示$\\Delta_n$的第$l$个特征值，在数据生成模型和$\\varepsilon$衰减速率的适当假设下，我们证明了$\\sqrt{n } (\\hat{\\lambda}_{l}^\\varepsilon - \\mathbb{E}[\\hat{\\lambda}_{l}^\\varepsilon] )$渐近服从高斯分布，其方差可以明确表征。一个形式化的论证使我们能够将这种渐近方差解释为在Fisher-Rao几何下，某个合适能量的梯度流的耗散。这种几何解释反过来使我们能够从统计学角度解释渐近方差，即作为某些加权拉普拉斯-贝尔特拉米算子特征值估计的Cramer-Rao下界。后一种解释暗示了图拉普拉斯算子特征值的一种渐近统计效率形式。我们还提出了多个特征值的中心极限定理，并通过几次数值实验探索了当我们的理论分析中的某些假设放宽时，我们结果的有效性。", "summary": "本文研究了从低维流形中采样的$\\varepsilon$-邻近图上的图拉普拉斯算子特征值的渐近行为。研究证明了单个特征值的波动量渐近服从高斯分布，并明确表征了其方差。该方差被几何地解释为Fisher-Rao几何下梯度流的耗散，并从统计学角度解释为特征值估计的Cramer-Rao下界，从而揭示了图拉普拉斯算子特征值的渐近统计效率。此外，研究还提出了多个特征值的中心极限定理，并进行了数值实验验证。", "keywords": "图拉普拉斯算子, 中心极限定理, 特征值, 数据云, Fisher-Rao几何", "comments": "这项工作在数据科学和谱图理论交叉领域具有重要意义。它为基于数据云的图拉普拉斯算子特征值提供了严格的统计学基础，尤其是在低维流形背景下。其创新之处在于不仅证明了中心极限定理，还通过几何和统计解释深入揭示了渐近方差的物理意义和统计效率，为特征值估计的可靠性提供了理论支撑。"}}
{"id": "2507.19177", "title": "Achievable Rates for a Distributed Antenna System with No Channel State Information at the Central Processor", "authors": ["Yi Song", "Hao Xu", "Kai Wan", "Kai-Kit Wong", "Giuseppe Caire", "Shlomo Shamai"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19177v1", "summary": "A recent trend in wireless communications considers the migration of\ntraditional monolithic base stations to the so-called disaggregated\narchitecture, where radio units (RUs) implement only the low-level physical\nlayer functionalities such as demodulation, and A/D conversion, while the\nhigh-level physical layer, such as channel decoding, is implemented as\nsoftware-defined functions running on general-purpose hardware in some remote\ncentral processing unit (CP). The corresponding information theoretic model for\nthe uplink (from the wireless users to the CP) is a multiaccess-relay channel\nwith primitive oblivious relays. The relays (RUs) are oblivious, as they are\nagnostic of the users codebooks, and primitive, since the fronthaul links (from\nRUs to CP) are error-free with limited capacity. This class of networks has\nbeen intensely studied in the information theoretic literature, where several\napproximated or exact (under certain conditions) capacity results have been\nderived. In particular, in the Gaussian case, the model has been analyzed for\nfixed and known channel state. This paper is motivated by the fact that, in\npractice, the channel state is a random process, and it is estimated at the\nbase station side through uplink pilot symbols sent by the users. The pilot\ndimension may take up a large portion of the channel coherence block, i.e., the\nnumber of symbols over which the channel state remains approximately constant.\nHence, sending both pilot and data symbols from the relays to the CP may\nrequire a significant overhead, especially when the fronthaul capacity is\nsmall. As a prototypical problem, we consider the ergodic achievable rate for a\ndiamond network formed by a single user and two relays where the channel state\nis known at the relays, but not known at the CP.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19177v1", "cate": "cs.IT", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "分布式天线系统中央处理器无信道状态信息时的可达速率", "tldr": "本文研究在分布式天线系统中，当中央处理器无法获取信道状态信息时，如何计算可达速率，并考虑了前传链路容量限制和导频开销问题。", "motivation": "传统分布式天线系统中，信道状态信息（CSI）通常是随机过程，并通过导频符号在基站侧估计。然而，将导频和数据符号都从无线电单元（RU）传输到中央处理器（CP）会产生显著的开销，尤其当前传链路容量有限时。因此，本文旨在研究在CP没有CSI的情况下如何确定可达速率。", "method": "本文将上行链路建模为具有原始无感知中继的多址中继信道，其中中继（RUs）对用户码本一无所知，且前传链路无差错但容量有限。作为一个原型问题，研究了一个由单个用户和两个中继组成的菱形网络在信道状态在中继处已知但CP处未知的情况下的遍历可达速率。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "无线通信领域近期的一个趋势是考虑将传统的单片基站迁移到所谓的分解架构，其中无线电单元（RUs）仅实现低层物理层功能，如解调和A/D转换，而高层物理层功能，如信道解码，则以软件定义功能的形式在远程中央处理器（CP）上的通用硬件上运行。相应的上行链路（从无线用户到CP）的信息论模型是一个具有原始无感知中继的多址中继信道。中继（RUs）是无感知的，因为它们不了解用户码本；它们是原始的，因为前传链路（从RUs到CP）是无差错但容量有限的。这类网络在信息论文献中得到了深入研究，并在此条件下推导出了几种近似或精确的容量结果。特别是，在高斯情况下，该模型已针对固定和已知信道状态进行了分析。本文的动机是，在实践中，信道状态是一个随机过程，并通过用户发送的上行导频符号在基站侧进行估计。导频维度可能占据信道相干块的很大一部分，即信道状态近似保持不变的符号数量。因此，从中继向CP发送导频和数据符号可能需要显著的开销，特别是当前传容量较小时。作为一个原型问题，我们考虑了由单个用户和两个中继组成的菱形网络在信道状态在中继处已知但CP处未知的情况下的遍历可达速率。", "summary": "本文研究了分解式分布式天线系统中上行链路的可达速率，其中无线电单元（RUs）执行低层物理功能，而中央处理器（CP）执行高层功能。针对CP无法获取信道状态信息（CSI）的实际场景，且考虑到导频传输可能带来的巨大开销和有限的前传链路容量，本文将上行链路建模为原始无感知中继的多址中继信道。文章以一个单用户双中继的菱形网络为例，分析了当CSI在中继处已知但CP处未知时的遍历可达速率问题。", "keywords": "分布式天线系统, 可达速率, 信道状态信息, 前传链路, 多址中继信道", "comments": "本文关注分布式天线系统中的一个重要实际问题，即中央处理器缺乏信道状态信息。其创新点在于将上行链路建模为具有原始无感知中继的多址中继信道，并考虑了导频开销和前传链路容量限制。这种对实际系统复杂性的深入考虑，对于优化未来无线通信网络性能具有重要意义。"}}
{"id": "2507.18945", "title": "TreeReader: A Hierarchical Academic Paper Reader Powered by Language Models", "authors": ["Zijian Zhang", "Pan Chen", "Fangshi Du", "Runlong Ye", "Oliver Huang", "Michael Liut", "Alán Aspuru-Guzik"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18945v1", "summary": "Efficiently navigating and understanding academic papers is crucial for\nscientific progress. Traditional linear formats like PDF and HTML can cause\ncognitive overload and obscure a paper's hierarchical structure, making it\ndifficult to locate key information. While LLM-based chatbots offer\nsummarization, they often lack nuanced understanding of specific sections, may\nproduce unreliable information, and typically discard the document's\nnavigational structure. Drawing insights from a formative study on academic\nreading practices, we introduce TreeReader, a novel language model-augmented\npaper reader. TreeReader decomposes papers into an interactive tree structure\nwhere each section is initially represented by an LLM-generated concise\nsummary, with underlying details accessible on demand. This design allows users\nto quickly grasp core ideas, selectively explore sections of interest, and\nverify summaries against the source text. A user study was conducted to\nevaluate TreeReader's impact on reading efficiency and comprehension.\nTreeReader provides a more focused and efficient way to navigate and understand\ncomplex academic literature by bridging hierarchical summarization with\ninteractive exploration.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18945v1", "cate": "cs.HC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "TreeReader: 一种由语言模型驱动的层次化学术论文阅读器", "tldr": "TreeReader是一种新型的学术论文阅读器，它将论文分解为交互式树状结构，并使用LLM生成摘要，以提高阅读效率和理解。", "motivation": "传统的PDF和HTML等线性格式会导致认知超载，模糊论文的层次结构，难以找到关键信息。现有的LLM聊天机器人虽然提供摘要，但缺乏对特定部分的细致理解，可能产生不可靠信息，并丢弃文档的导航结构。因此，需要一种更高效的方式来浏览和理解学术论文。", "method": "TreeReader从一项关于学术阅读实践的形成性研究中获取灵感，它将论文分解为交互式树状结构。每个部分最初由语言模型生成的简洁摘要表示，用户可以按需访问底层详细信息。这种设计旨在帮助用户快速掌握核心思想，有选择地探索感兴趣的部分，并对照原文验证摘要。研究人员还进行了一项用户研究来评估其影响。", "result": "Not mentioned in abstract", "conclusion": "TreeReader通过将层次化摘要与交互式探索相结合，提供了一种更专注、更高效的方式来浏览和理解复杂的学术文献。", "translation": "高效地浏览和理解学术论文对于科学进步至关重要。传统的线性格式，如PDF和HTML，可能导致认知超载并模糊论文的层次结构，从而难以定位关键信息。虽然基于LLM的聊天机器人提供摘要功能，但它们往往缺乏对特定部分的细致理解，可能产生不可靠的信息，并且通常会丢弃文档的导航结构。我们从一项关于学术阅读习惯的形成性研究中汲取见解，引入了TreeReader，这是一种新型的语言模型增强型论文阅读器。TreeReader将论文分解为交互式树状结构，其中每个部分最初由LLM生成的简洁摘要表示，底层详细信息可按需访问。这种设计允许用户快速掌握核心思想，有选择地探索感兴趣的部分，并对照原文验证摘要。进行了一项用户研究，以评估TreeReader对阅读效率和理解的影响。TreeReader通过将层次化摘要与交互式探索相结合，提供了一种更专注、更高效的方式来浏览和理解复杂的学术文献。", "summary": "TreeReader是一种新型的语言模型增强型学术论文阅读器，旨在解决传统线性格式和现有LLM工具在学术论文阅读中带来的认知负担和信息定位困难。它将论文分解为交互式树状结构，利用语言模型生成各部分的简洁摘要，并允许用户按需深入查看细节，从而提高阅读效率、理解力并支持信息验证。", "keywords": "TreeReader, 学术论文阅读器, 语言模型, 层次化结构, 交互式阅读", "comments": "TreeReader的创新之处在于其将语言模型生成的层次化摘要与交互式树状导航相结合，有效解决了传统阅读格式的不足和现有LLM工具的局限性。这种设计有望显著提升学术研究者在复杂文献中获取和理解关键信息的能力。"}}
{"id": "2507.19197", "title": "WACA-UNet: Weakness-Aware Channel Attention for Static IR Drop Prediction in Integrated Circuit Design", "authors": ["Youngmin Seo", "Yunhyeong Kwon", "Younghun Park", "HwiRyong Kim", "Seungho Eum", "Jinha Kim", "Taigon Song", "Juho Kim", "Unsang Park"], "categories": ["cs.LG", "cs.AI", "cs.CV", "B.7.2; I.5.1; I.2.10; I.5.4"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      9 pages, 5 figures", "url": "http://arxiv.org/abs/2507.19197v1", "summary": "Accurate spatial prediction of power integrity issues, such as IR drop, is\ncritical for reliable VLSI design. However, traditional simulation-based\nsolvers are computationally expensive and difficult to scale. We address this\nchallenge by reformulating IR drop estimation as a pixel-wise regression task\non heterogeneous multi-channel physical maps derived from circuit layouts.\nPrior learning-based methods treat all input layers (e.g., metal, via, and\ncurrent maps) equally, ignoring their varying importance to prediction\naccuracy. To tackle this, we propose a novel Weakness-Aware Channel Attention\n(WACA) mechanism, which recursively enhances weak feature channels while\nsuppressing over-dominant ones through a two-stage gating strategy. Integrated\ninto a ConvNeXtV2-based attention U-Net, our approach enables adaptive and\nbalanced feature representation. On the public ICCAD-2023 benchmark, our method\noutperforms the ICCAD-2023 contest winner by reducing mean absolute error by\n61.1% and improving F1-score by 71.0%. These results demonstrate that\nchannel-wise heterogeneity is a key inductive bias in physical layout analysis\nfor VLSI.", "comment": "9 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.19197v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "WACA-UNet：集成电路设计中静态IR压降预测的弱点感知通道注意力机制", "tldr": "WACA-UNet是一种新的深度学习方法，通过弱点感知通道注意力机制，显著提高了集成电路IR压降预测的准确性，解决了传统方法计算成本高和现有学习方法忽略输入层重要性差异的问题。", "motivation": "传统的基于仿真的IR压降求解器计算成本高昂且难以扩展。先前的学习方法平等对待所有输入层（如金属、通孔和电流图），忽略了它们对预测精度的不同重要性。", "method": "将IR压降估计重新定义为电路布局导出的异构多通道物理图上的像素级回归任务。提出了一种新颖的弱点感知通道注意力（WACA）机制，通过两阶段门控策略递归地增强弱特征通道，同时抑制过于主导的通道。该机制集成到基于ConvNeXtV2的注意力U-Net中，以实现自适应和平衡的特征表示。", "result": "在ICCAD-2023公共基准测试中，该方法将平均绝对误差降低了61.1%，F1分数提高了71.0%，性能优于ICCAD-2023竞赛获胜者。", "conclusion": "通道异构性是VLSI物理布局分析中的一个关键归纳偏差。", "translation": "准确预测功率完整性问题（如IR压降）的空间分布对于可靠的VLSI设计至关重要。然而，传统的基于仿真的求解器计算成本高昂且难以扩展。我们通过将IR压降估计重新定义为电路布局导出的异构多通道物理图上的像素级回归任务来解决这一挑战。先前的学习方法平等对待所有输入层（如金属、通孔和电流图），忽略了它们对预测精度的不同重要性。为了解决这个问题，我们提出了一种新颖的弱点感知通道注意力（WACA）机制，通过两阶段门控策略递归地增强弱特征通道，同时抑制过于主导的通道。该方法集成到基于ConvNeXtV2的注意力U-Net中，实现了自适应和平衡的特征表示。在ICCAD-2023公共基准测试中，我们的方法将平均绝对误差降低了61.1%，F1分数提高了71.0%，性能优于ICCAD-2023竞赛获胜者。这些结果表明，通道异构性是VLSI物理布局分析中的一个关键归纳偏差。", "summary": "该论文提出WACA-UNet，一种基于深度学习的IR压降预测方法，旨在解决传统仿真计算成本高和现有学习方法忽略输入层重要性差异的问题。通过将IR压降估计转化为像素级回归任务，并引入弱点感知通道注意力（WACA）机制，WACA-UNet能够自适应地平衡不同特征通道的重要性。在ICCAD-2023基准测试中，WACA-UNet显著优于现有最佳方法，证明了通道异构性在VLSI物理布局分析中的关键作用。", "keywords": "IR压降预测, 通道注意力, 深度学习, VLSI设计, WACA-UNet", "comments": "该论文的创新点在于提出了弱点感知通道注意力（WACA）机制，它能动态调整不同特征通道的权重，克服了以往方法对所有输入层一视同仁的局限性。这种机制对于处理多模态或异构数据具有普遍意义。其在IR压降预测任务上的显著性能提升，证明了该方法的有效性和在VLSI设计领域的重要性。"}}
{"id": "2507.19188", "title": "VisHall3D: Monocular Semantic Scene Completion from Reconstructing the Visible Regions to Hallucinating the Invisible Regions", "authors": ["Haoang Lu", "Yuanqi Su", "Xiaoning Zhang", "Longjun Gao", "Yu Xue", "Le Wang"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19188v1", "summary": "This paper introduces VisHall3D, a novel two-stage framework for monocular\nsemantic scene completion that aims to address the issues of feature\nentanglement and geometric inconsistency prevalent in existing methods.\nVisHall3D decomposes the scene completion task into two stages: reconstructing\nthe visible regions (vision) and inferring the invisible regions\n(hallucination). In the first stage, VisFrontierNet, a visibility-aware\nprojection module, is introduced to accurately trace the visual frontier while\npreserving fine-grained details. In the second stage, OcclusionMAE, a\nhallucination network, is employed to generate plausible geometries for the\ninvisible regions using a noise injection mechanism. By decoupling scene\ncompletion into these two distinct stages, VisHall3D effectively mitigates\nfeature entanglement and geometric inconsistency, leading to significantly\nimproved reconstruction quality.\n  The effectiveness of VisHall3D is validated through extensive experiments on\ntwo challenging benchmarks: SemanticKITTI and SSCBench-KITTI-360. VisHall3D\nachieves state-of-the-art performance, outperforming previous methods by a\nsignificant margin and paves the way for more accurate and reliable scene\nunderstanding in autonomous driving and other applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19188v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "VisHall3D：从重建可见区域到幻觉不可见区域的单目语义场景补全", "tldr": "VisHall3D是一个新的两阶段单目语义场景补全框架，通过解耦可见区域重建和不可见区域推断，有效解决了特征纠缠和几何不一致问题，并在两个基准测试中达到了最先进的性能。", "motivation": "现有单目语义场景补全方法存在特征纠缠和几何不一致的问题。", "method": "本文提出了VisHall3D，一个两阶段框架。第一阶段，使用VisFrontierNet（一个可见性感知投影模块）重建可见区域并保留细节。第二阶段，使用OcclusionMAE（一个幻觉网络）通过噪声注入机制生成不可见区域的合理几何形状。这种解耦方法旨在缓解特征纠缠和几何不一致。", "result": "VisHall3D在SemanticKITTI和SSCBench-KITTI-360两个基准测试中进行了验证，取得了最先进的性能，显著优于现有方法。", "conclusion": "VisHall3D通过其独特的两阶段方法，有效解决了单目语义场景补全中的特征纠缠和几何不一致问题，显著提高了重建质量，并为自动驾驶及其他应用中更准确可靠的场景理解铺平了道路。", "translation": "本文介绍了VisHall3D，一个用于单目语义场景补全的新型两阶段框架，旨在解决现有方法中普遍存在的特征纠缠和几何不一致问题。VisHall3D将场景补全任务分解为两个阶段：重建可见区域（视觉）和推断不可见区域（幻觉）。在第一阶段，引入了VisFrontierNet，一个可见性感知投影模块，用于在保留细粒度细节的同时准确追踪视觉边界。在第二阶段，采用OcclusionMAE，一个幻觉网络，通过噪声注入机制为不可见区域生成合理的几何形状。通过将场景补全解耦为这两个不同的阶段，VisHall3D有效缓解了特征纠缠和几何不一致，从而显著提高了重建质量。VisHall3D的有效性通过在两个具有挑战性的基准测试：SemanticKITTI和SSCBench-KITTI-360上进行的广泛实验得到了验证。VisHall3D取得了最先进的性能，以显著优势超越了以前的方法，并为自动驾驶和其他应用中更准确和可靠的场景理解铺平了道路。", "summary": "VisHall3D是一个创新的两阶段单目语义场景补全框架，旨在解决现有方法中的特征纠缠和几何不一致问题。它将任务分解为可见区域重建（通过VisFrontierNet）和不可见区域推断（通过OcclusionMAE）。这种解耦方法有效提高了重建质量，并在SemanticKITTI和SSCBench-KITTI-360基准测试中实现了最先进的性能，为自动驾驶等应用中的场景理解提供了更可靠的解决方案。", "keywords": "单目语义场景补全, 两阶段框架, 特征解耦, 可见区域重建, 不可见区域推断", "comments": "该论文的主要创新点在于将单目语义场景补全任务解耦为可见区域重建和不可见区域幻觉两个独立阶段，并通过VisFrontierNet和OcclusionMAE两个专门网络进行处理。这种两阶段方法有效解决了现有方法中常见的特征纠缠和几何不一致问题，显著提升了补全质量和可靠性，为相关领域提供了新的思路。"}}
{"id": "2504.20314", "title": "Perturbation-efficient Zeroth-order Optimization for Hardware-friendly On-device Training", "authors": ["Qitao Tan", "Sung-En Chang", "Rui Xia", "Huidong Ji", "Chence Yang", "Ci Zhang", "Jun Liu", "Zheng Zhan", "Zhenman Fang", "Zhou Zou", "Yanzhi Wang", "Jin Lu", "Geng Yuan"], "categories": ["cs.LG", "cs.AI"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2504.20314v2", "summary": "Zeroth-order (ZO) optimization is an emerging deep neural network (DNN)\ntraining paradigm that offers computational simplicity and memory savings.\nHowever, this seemingly promising approach faces a significant and long-ignored\nchallenge. ZO requires generating a substantial number of Gaussian random\nnumbers, which poses significant difficulties and even makes it infeasible for\nhardware platforms, such as FPGAs and ASICs. In this paper, we identify this\ncritical issue, which arises from the mismatch between algorithm and hardware\ndesigners. To address this issue, we proposed PeZO, a perturbation-efficient ZO\nframework. Specifically, we design random number reuse strategies to\nsignificantly reduce the demand for random number generation and introduce a\nhardware-friendly adaptive scaling method to replace the costly Gaussian\ndistribution with a uniform distribution. Our experiments show that PeZO\nreduces the required LUTs and FFs for random number generation by 48.6\\% and\n12.7\\%, and saves at maximum 86\\% power consumption, all without compromising\ntraining performance, making ZO optimization feasible for on-device training.\nTo the best of our knowledge, we are the first to explore the potential of\non-device ZO optimization, providing valuable insights for future research.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2504.20314v2", "cate": "cs.LG", "date": "2025-04-28", "updated": "2025-07-24", "AI": {"title_translation": "面向硬件友好型设备端训练的扰动高效零阶优化", "tldr": "本文提出了PeZO框架，通过随机数复用和硬件友好的自适应缩放方法，解决了零阶优化在硬件平台上高斯随机数生成困难的问题，使其在设备端训练中可行。", "motivation": "零阶（ZO）优化作为一种新兴的深度神经网络（DNN）训练范式，具有计算简单和内存节省的优点。然而，它面临一个长期被忽视的重大挑战：ZO需要生成大量的标准高斯随机数，这对于FPGA和ASIC等硬件平台来说，实现起来非常困难甚至不可行，根本原因在于算法与硬件设计者之间的不匹配。", "method": "为了解决零阶优化在硬件平台上生成大量高斯随机数的困难，本文提出了PeZO，一个扰动高效的零阶框架。具体来说，PeZO设计了随机数复用策略以显著减少对随机数生成的需求，并引入了一种硬件友好的自适应缩放方法，用均匀分布取代了成本高昂的高斯分布。", "result": "实验结果表明，PeZO在不损害训练性能的前提下，将随机数生成所需的LUTs和FFs分别减少了48.6%和12.7%，并最多节省了86%的功耗，从而使零阶优化在设备端训练中变得可行。", "conclusion": "PeZO框架解决了零阶优化在硬件平台上随机数生成面临的挑战，使其在设备端训练中成为可能。据作者所知，这是首次探索设备端零阶优化的潜力，为未来的研究提供了宝贵的见解。", "translation": "零阶（ZO）优化是一种新兴的深度神经网络（DNN）训练范式，具有计算简单和内存节省的特点。然而，这种看似有前景的方法面临一个重大且长期被忽视的挑战。ZO需要生成大量的标准高斯随机数，这给FPGA和ASIC等硬件平台带来了巨大困难，甚至使其变得不可行。在本文中，我们指出了这个关键问题，它源于算法和硬件设计者之间的不匹配。为了解决这个问题，我们提出了PeZO，一个扰动高效的ZO框架。具体来说，我们设计了随机数复用策略，以显著减少对随机数生成的需求，并引入了一种硬件友好的自适应缩放方法，用均匀分布取代了成本高昂的高斯分布。我们的实验表明，PeZO在不损害训练性能的情况下，将随机数生成所需的LUTs和FFs分别减少了48.6%和12.7%，并最多节省了86%的功耗，从而使ZO优化在设备端训练中变得可行。据我们所知，我们是首次探索设备端ZO优化的潜力，为未来的研究提供了宝贵的见解。", "summary": "本研究提出了一种名为PeZO的扰动高效零阶优化框架，旨在解决零阶优化在硬件平台（如FPGA和ASIC）上因大量高斯随机数生成而面临的挑战。PeZO通过引入随机数复用策略和用硬件友好的自适应缩放方法替代高斯分布为均匀分布，显著降低了对随机数生成的需求和硬件资源消耗。实验证明，PeZO在不牺牲训练性能的前提下，能有效减少随机数生成所需的硬件资源（LUTs和FFs）和功耗，从而使零阶优化在设备端训练中成为可行方案。该工作首次探索了设备端零阶优化的潜力。", "keywords": "零阶优化, 设备端训练, 硬件友好, 随机数生成, 扰动高效", "comments": "本文创新性地指出了零阶优化在硬件实现中随机数生成这一长期被忽视的关键瓶颈，并提出了实用的解决方案PeZO。其通过随机数复用和用均匀分布替代高斯分布的方法，在保证性能的同时显著降低了硬件成本和功耗，对于推动零阶优化在资源受限的设备端部署具有重要意义。这是一个算法与硬件协同设计的优秀范例。"}}
{"id": "2507.18677", "title": "HeartUnloadNet: A Weakly-Supervised Cycle-Consistent Graph Network for Predicting Unloaded Cardiac Geometry from Diastolic States", "authors": ["Siyu Mu", "Wei Xuan Chan", "Choon Hwai Yap"], "categories": ["cs.CV", "physics.med-ph"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Codes are available at this https URL", "url": "http://arxiv.org/abs/2507.18677v1", "summary": "The unloaded cardiac geometry (i.e., the state of the heart devoid of luminal\npressure) serves as a valuable zero-stress and zero-strain reference and is\ncritical for personalized biomechanical modeling of cardiac function, to\nunderstand both healthy and diseased physiology and to predict the effects of\ncardiac interventions. However, estimating the unloaded geometry from clinical\nimages remains a challenging task. Traditional approaches rely on inverse\nfinite element (FE) solvers that require iterative optimization and are\ncomputationally expensive. In this work, we introduce HeartUnloadNet, a deep\nlearning framework that predicts the unloaded left ventricular (LV) shape\ndirectly from the end diastolic (ED) mesh while explicitly incorporating\nbiophysical priors. The network accepts a mesh of arbitrary size along with\nphysiological parameters such as ED pressure, myocardial stiffness scale, and\nfiber helix orientation, and outputs the corresponding unloaded mesh. It adopts\na graph attention architecture and employs a cycle-consistency strategy to\nenable bidirectional (loading and unloading) prediction, allowing for partial\nself-supervision that improves accuracy and reduces the need for large training\ndatasets. Trained and tested on 20,700 FE simulations across diverse LV\ngeometries and physiological conditions, HeartUnloadNet achieves sub-millimeter\naccuracy, with an average DSC of 0.986 and HD of 0.083 cm, while reducing\ninference time to just 0.02 seconds per case, over 10^5 times faster and\nsignificantly more accurate than traditional inverse FE solvers. Ablation\nstudies confirm the effectiveness of the architecture. Notably, the\ncycle-consistent design enables the model to maintain a DSC of 97% even with as\nfew as 200 training samples. This work thus presents a scalable and accurate\nsurrogate for inverse FE solvers, supporting real-time clinical applications in\nthe future.", "comment": "Codes are available at https://github.com/SiyuMU/Loaded2UnNet", "pdf_url": "http://arxiv.org/pdf/2507.18677v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "HeartUnloadNet：一种弱监督循环一致图网络，用于从舒张状态预测心脏无负荷几何形状", "tldr": "HeartUnloadNet是一个基于图的深度学习框架，能快速准确地从舒张状态预测心脏无负荷几何形状，比传统方法快10^5倍且更准确，并能通过弱监督学习减少数据需求。", "motivation": "预测心脏无负荷几何形状对于个性化心脏生物力学建模、理解生理和疾病以及预测心脏干预效果至关重要。然而，从临床图像中估计无负荷几何形状是一个具有挑战性的任务，传统逆向有限元（FE）求解器计算成本高昂且需要迭代优化。", "method": "本文提出了HeartUnloadNet，一个深度学习框架，直接从舒张末期（ED）网格预测左心室（LV）的无负荷形状，并明确纳入生物物理先验知识。网络接受任意大小的网格以及生理参数（如ED压力、心肌刚度尺度、纤维螺旋方向），并输出相应的无负荷网格。它采用图注意力架构和循环一致性策略，实现双向（加载和卸载）预测，支持部分自监督，从而提高准确性并减少对大量训练数据的需求。", "result": "在20,700次有限元模拟数据上进行训练和测试，HeartUnloadNet实现了亚毫米级精度，平均DSC为0.986，HD为0.083 cm。推理时间仅为0.02秒/病例，比传统逆向FE求解器快10^5倍以上，且显著更准确。消融研究证实了架构的有效性。循环一致性设计使得模型即使只有200个训练样本也能保持97%的DSC。", "conclusion": "HeartUnloadNet为逆向有限元求解器提供了一个可扩展且准确的替代方案，支持未来实时临床应用。", "translation": "无负荷心脏几何形状（即心脏不受腔内压力影响的状态）作为有价值的零应力零应变参考，对于心脏功能个性化生物力学建模至关重要，有助于理解健康和疾病生理学，并预测心脏干预的效果。然而，从临床图像中估计无负荷几何形状仍然是一项具有挑战性的任务。传统方法依赖于逆向有限元（FE）求解器，这些求解器需要迭代优化且计算成本高昂。在这项工作中，我们引入了HeartUnloadNet，一个深度学习框架，它直接从舒张末期（ED）网格预测无负荷左心室（LV）形状，同时明确地纳入了生物物理先验知识。该网络接受任意大小的网格以及生理参数，如ED压力、心肌刚度尺度和纤维螺旋方向，并输出相应的无负荷网格。它采用图注意力架构并运用循环一致性策略，以实现双向（加载和卸载）预测，从而实现部分自监督，提高准确性并减少对大量训练数据集的需求。HeartUnloadNet在涵盖不同左心室几何形状和生理条件的20,700次有限元模拟中进行训练和测试，实现了亚毫米级精度，平均DSC为0.986，HD为0.083厘米，同时将每个病例的推理时间缩短到仅0.02秒，比传统逆向有限元求解器快10^5倍以上且显著更准确。消融研究证实了该架构的有效性。值得注意的是，循环一致性设计使模型即使训练样本少至200个时也能保持97%的DSC。因此，这项工作为逆向有限元求解器提供了一个可扩展且准确的替代方案，支持未来的实时临床应用。", "summary": "本文提出了HeartUnloadNet，一个弱监督循环一致图网络，用于从舒张末期心脏网格预测无负荷左心室几何形状。该网络整合了生物物理先验知识，采用图注意力架构和双向循环一致性策略，实现了高效且高精度的预测。与传统有限元方法相比，HeartUnloadNet在速度和准确性上均有显著提升，并能通过部分自监督减少对大量训练数据的依赖，为心脏生物力学建模和临床应用提供了实时可行的解决方案。", "keywords": "心脏建模, 无负荷几何, 深度学习, 图网络, 循环一致性", "comments": "HeartUnloadNet的创新之处在于其将深度学习与生物物理先验知识结合，并引入了循环一致性策略，实现了弱监督学习，显著降低了数据需求。其在速度和精度上的巨大提升，使其成为传统有限元求解器的一个革命性替代品，有望推动心脏生物力学建模进入实时临床应用阶段。"}}
{"id": "2506.11366", "title": "Meeting Patients Where They're At: Toward the Expansion of Chaplaincy Care into Online Spiritual Care Communities", "authors": ["Alemitu Bezabih", "Shadi Nourriz", "Anne-Marie Snider", "Rosalie Rauenzahn", "George Handzo", "C. Estelle Smith"], "categories": ["cs.HC", "cs.CY"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2506.11366v2", "summary": "Despite a growing need for spiritual care in the US, it is often\nunder-served, inaccessible, or misunderstood, while almost no prior work in\nCSCW/HCI research has engaged with professional chaplains and spiritual care\nproviders. This interdisciplinary study aims to develop a foundational\nunderstanding of how spiritual care may (or may not) be expanded into online\nspaces -- especially focusing on anonymous, asynchronous, and text-based online\ncommunities. We conducted an exploratory mixed-methods study with chaplains\n(N=22) involving interviews and user testing sessions centered around Reddit\nsupport communities to understand participants' perspectives on technology and\ntheir ideations about the role of chaplaincy in prospective Online Spiritual\nCare Communities (OSCCs). Our Grounded Theory Method analysis highlighted\nbenefits of OSCCs including: meeting patients where they are at; accessibility\nand scalability; and facilitating patient-initiated care. Chaplains highlighted\nhow their presence in OSCCs could help with shaping peer interactions,\nmoderation, synchronous chats for group care, and redirecting to external\nresources, while also raising important feasibility concerns, risks, and needs\nfor future design and research. We used an existing taxonomy of chaplaincy\ntechniques to show that some spiritual care strategies may be amenable to\nonline spaces, yet we also exposed the limitations of technology to fully\nmediate spiritual care and the need to develop new online chaplaincy\ninterventions. Based on these findings, we contribute the model of a ``Care\nLoop'' between institutionally-based formal care and platform-based community\ncare to expand access and drive greater awareness and utilization of spiritual\ncare. We also contribute design implications to guide future work in online\nspiritual care.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2506.11366v2", "cate": "cs.HC", "date": "2025-06-12", "updated": "2025-07-24", "AI": {"title_translation": "在患者所在之处：将牧师关怀扩展到在线精神关怀社区", "tldr": "本研究探讨了将牧师精神关怀扩展到匿名、异步、基于文本的在线社区的可行性、益处和挑战，发现线上关怀具有可及性和可扩展性，但也存在技术局限性，并提出了“关怀循环”模型。", "motivation": "尽管美国对精神关怀的需求日益增长，但服务不足、难以获得或被误解；同时，CSCW/HCI领域缺乏对专业牧师和精神关怀提供者的研究。本研究旨在建立对精神关怀如何扩展到在线空间的理解，特别是匿名、异步和文本型在线社区。", "method": "本研究采用探索性混合方法，对22位牧师进行了访谈和用户测试，围绕Reddit支持社区展开，并使用扎根理论方法进行分析。此外，研究还利用现有的牧师技术分类法评估在线空间的可行性。", "result": "在线精神关怀社区（OSCCs）的益处包括在患者所在之处提供服务、可及性、可扩展性以及促进患者主动关怀。牧师认为其在OSCCs中的存在有助于塑造同伴互动、主持、进行小组同步聊天和转介外部资源。研究也指出了技术在完全介导精神关怀方面的局限性，并强调了开发新在线干预措施的必要性，同时牧师也提出了相关的可行性问题和风险。", "conclusion": "本研究提出了一个“关怀循环”模型，旨在连接机构化正式关怀与平台社区关怀，以扩大精神关怀的可及性并提高其认知度和利用率。同时，研究还提供了指导未来在线精神关怀工作的设计启示。", "translation": "尽管美国对精神关怀的需求日益增长，但它往往服务不足、难以获得或被误解，而CSCW/HCI研究中几乎没有先前的研究涉及专业牧师和精神关怀提供者。这项跨学科研究旨在建立对精神关怀如何（或可能不）扩展到在线空间的初步理解——特别是侧重于匿名、异步和基于文本的在线社区。我们对牧师（N=22）进行了一项探索性混合方法研究，包括访谈和围绕Reddit支持社区的用户测试，以了解参与者对技术的看法以及他们对牧师在未来在线精神关怀社区（OSCCs）中作用的构想。我们的扎根理论方法分析强调了OSCCs的益处，包括：在患者所在之处提供服务；可及性和可扩展性；以及促进患者主动的关怀。牧师们强调了他们在OSCCs中的存在如何有助于塑造同伴互动、主持、进行小组同步聊天以及转介到外部资源，同时也提出了重要的可行性问题、风险以及未来设计和研究的需求。我们使用现有的牧师技术分类法来表明某些精神关怀策略可能适用于在线空间，但我们也揭示了技术在完全介导精神关怀方面的局限性以及开发新的在线牧师干预措施的必要性。基于这些发现，我们提出了一个“关怀循环”模型，连接机构化正式关怀和平台社区关怀，以扩大可及性并提高精神关怀的认知和利用。我们还提供了设计启示，以指导未来在线精神关怀的工作。", "summary": "这项跨学科研究旨在探讨将牧师精神关怀扩展到在线社区的可行性和影响。通过对22位牧师进行混合方法研究（包括访谈和用户测试），研究发现在线精神关怀社区（OSCCs）具有提高可及性、可扩展性和促进患者主动关怀的潜力。尽管技术存在局限性，但研究识别了在线关怀的益处，如塑造同伴互动和资源转介，同时也提出了挑战。最终，研究提出了一个“关怀循环”模型和设计启示，以期扩大精神关怀的覆盖范围和利用率。", "keywords": "精神关怀, 在线社区, 牧师关怀, 可及性, 混合方法研究", "comments": "这项研究具有创新性，因为它首次将专业牧师和精神关怀引入到CSCW/HCI研究领域，并探索了将传统精神关怀服务扩展到数字化、在线环境的可能性。其重要性在于解决了精神关怀服务不足和可及性差的问题，并提出了实用的“关怀循环”模型和设计启示，为未来在线精神关怀的开发提供了指导。局限性可能在于其探索性质和样本量相对较小，且需要进一步研究来验证在线干预措施的有效性。"}}
{"id": "2307.15368", "title": "Modeling Nonlinear Control Systems via Koopman Control Family: Universal Forms and Subspace Invariance Proximity", "authors": ["Masih Haseli", "Jorge Cortés"], "categories": ["math.OC", "cs.SY", "eess.SY"], "primary_category": "Subjects:       Optimization and Control (math.OC)", "pdf_link": null, "comments": "Comments:      19 pages", "url": "http://arxiv.org/abs/2307.15368v4", "summary": "This paper introduces the Koopman Control Family (KCF), a mathematical\nframework for modeling general (not necessarily control-affine) discrete-time\nnonlinear control systems with the aim of providing a solid theoretical\nfoundation for the use of Koopman-based methods in systems with inputs. We\ndemonstrate that the concept of KCF captures the behavior of nonlinear control\nsystems on a (potentially infinite-dimensional) function space. By employing a\ngeneralized notion of subspace invariance under the KCF, we establish a\nuniversal form for finite-dimensional models, which encompasses the commonly\nused linear, bilinear, and linear switched models as specific instances. In\ncases where the subspace is not invariant under the KCF, we propose a method\nfor approximating models in general form and characterize the model's accuracy\nusing the concept of invariance proximity. We end by discussing how the\nproposed framework naturally lends itself to data-driven modeling of control\nsystems.", "comment": "19 pages", "pdf_url": "http://arxiv.org/pdf/2307.15368v4", "cate": "math.OC", "date": "2023-07-28", "updated": "2025-07-24", "AI": {"title_translation": "通过Koopman控制族建模非线性控制系统：通用形式和子空间不变性邻近", "tldr": "本文引入Koopman控制族（KCF）为非线性控制系统建模提供理论基础，并提出通用形式和近似方法。", "motivation": "为在带有输入的系统中应用Koopman方法提供坚实的理论基础，以建模一般的离散时间非线性控制系统。", "method": "引入Koopman控制族（KCF）数学框架来建模非线性控制系统；利用广义子空间不变性概念建立有限维模型的通用形式；对于子空间在KCF下不不变的情况，提出一种近似模型的方法并用不变性邻近概念表征模型精度。", "result": "证明了KCF概念能捕捉非线性控制系统在函数空间上的行为；建立了包含线性、双线性和线性切换模型的有限维通用形式；提出了在子空间不不变情况下的模型近似方法，并能用不变性邻近概念表征模型精度。", "conclusion": "所提出的框架自然地适用于控制系统的数据驱动建模。", "translation": "本文引入了Koopman控制族（KCF），这是一个用于建模一般（不一定是控制仿射的）离散时间非线性控制系统的数学框架，旨在为在带有输入的系统中使用Koopman方法提供坚实的理论基础。我们证明了KCF的概念可以捕捉非线性控制系统在（可能无限维的）函数空间上的行为。通过采用KCF下广义的子空间不变性概念，我们为有限维模型建立了一种通用形式，该形式包含常用的线性、双线性和线性切换模型作为特定实例。在子空间在KCF下不不变的情况下，我们提出了一种近似一般形式模型的方法，并使用不变性邻近的概念来表征模型的精度。最后，我们讨论了所提出的框架如何自然地适用于控制系统的数据驱动建模。", "summary": "本文提出Koopman控制族（KCF）作为建模非线性控制系统的数学框架，旨在为Koopman方法在带输入系统中的应用提供理论基础。研究表明KCF能捕捉系统行为，并基于广义子空间不变性建立了包含常见线性模型的有限维通用形式。对于非不变子空间，论文提出了一种近似方法并通过不变性邻近概念量化模型精度。该框架适用于数据驱动的控制系统建模。", "keywords": "Koopman控制族, 非线性控制系统, 子空间不变性, 通用形式, 数据驱动建模", "comments": "本文的创新点在于引入了Koopman控制族（KCF），为在带有输入的非线性控制系统中应用Koopman方法奠定了坚实的理论基础，并提出了一个统一的通用形式，涵盖了多种现有模型。此外，其对非不变子空间的近似方法和精度表征也具有实用价值，为数据驱动的控制系统建模提供了新的视角。"}}
{"id": "2503.13544", "title": "Decision by Supervised Learning with Deep Ensembles: A Practical Framework for Robust Portfolio Optimization", "authors": ["Juhyeong Kim", "Sungyoon Choi", "Youngbin Lee", "Yejin Kim", "Yongmin Choi", "Yongjae Lee"], "categories": ["cs.LG", "q-fin.CP", "q-fin.PM"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      8 pages, 3 figures", "url": "http://arxiv.org/abs/2503.13544v3", "summary": "We propose Decision by Supervised Learning (DSL), a practical framework for\nrobust portfolio optimization. DSL reframes portfolio construction as a\nsupervised learning problem: models are trained to predict optimal portfolio\nweights, using cross-entropy loss and portfolios constructed by maximizing the\nSharpe or Sortino ratio. To further enhance stability and reliability, DSL\nemploys Deep Ensemble methods, substantially reducing variance in portfolio\nallocations. Through comprehensive backtesting across diverse market universes\nand neural architectures, shows superior performance compared to both\ntraditional strategies and leading machine learning-based methods, including\nPrediction-Focused Learning and End-to-End Learning. We show that increasing\nthe ensemble size leads to higher median returns and more stable risk-adjusted\nperformance. The code is available at https://github.com/DSLwDE/DSLwDE.", "comment": "8 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2503.13544v3", "cate": "cs.LG", "date": "2025-03-16", "updated": "2025-07-25", "AI": {"title_translation": "基于深度集成监督学习的决策：稳健投资组合优化的实用框架", "tldr": "本文提出了决策监督学习（DSL），一个用于稳健投资组合优化的实用框架，它将投资组合构建重构为监督学习问题，并结合深度集成方法，在回测中表现优于传统和现有机器学习方法。", "motivation": "为了解决投资组合优化中的稳定性问题，并提高其在实际应用中的可靠性，本文提出了一个实用的框架。", "method": "本文提出了决策监督学习（DSL）框架，将投资组合构建重构为监督学习问题。模型通过预测最优投资组合权重进行训练，使用交叉熵损失，并以最大化夏普比率或索蒂诺比率构建投资组合。为增强稳定性和可靠性，DSL采用了深度集成方法来显著降低投资组合分配的方差。", "result": "通过在不同市场和神经网络架构下的全面回测，DSL表现出优于传统策略和包括预测聚焦学习、端到端学习在内的领先机器学习方法的性能。研究表明，增加集成规模可以带来更高的中位数回报和更稳定的风险调整表现。", "conclusion": "本文的结论是，决策监督学习（DSL）框架，特别是结合深度集成方法，为稳健投资组合优化提供了一个实用且表现卓越的解决方案，能够显著提升投资组合的稳定性、可靠性和收益。", "translation": "我们提出了基于监督学习的决策（DSL），一个用于稳健投资组合优化的实用框架。DSL将投资组合构建重构为一个监督学习问题：模型被训练来预测最优投资组合权重，使用交叉熵损失，并通过最大化夏普比率或索蒂诺比率来构建投资组合。为了进一步增强稳定性和可靠性，DSL采用了深度集成方法，显著降低了投资组合分配的方差。通过在不同市场环境和神经网络架构下的全面回测，结果显示其性能优于传统策略和领先的基于机器学习的方法，包括预测聚焦学习和端到端学习。我们表明，增加集成规模会带来更高的中位数回报和更稳定的风险调整性能。代码可在 https://github.com/DSLwDE/DSLwDE 获取。", "summary": "本文提出了决策监督学习（DSL）框架，通过将投资组合构建视为监督学习问题，并结合深度集成方法，实现了稳健的投资组合优化。DSL利用交叉熵损失训练模型预测最优权重，并以最大化夏普或索蒂诺比率为目标。实验证明，该框架在多样化市场中表现优于传统及现有机器学习方法，且增加集成规模能提升回报和稳定性。", "keywords": "投资组合优化, 监督学习, 深度集成, 机器学习, 夏普比率", "comments": "这项研究的创新之处在于将投资组合优化问题巧妙地转化为监督学习任务，并引入深度集成方法以增强模型的鲁棒性和稳定性。这为金融领域的决策制定提供了一个实用且高性能的框架，尤其是在降低投资组合分配方差方面具有重要意义。其在实际回测中的优异表现也证明了该方法的有效性。"}}
{"id": "2507.19027", "title": "SESR-Eval: Dataset for Evaluating LLMs in the Title-Abstract Screening of Systematic Reviews", "authors": ["Aleksi Huotala", "Miikka Kuutila", "Mika Mäntylä"], "categories": ["cs.SE"], "primary_category": "Subjects:       Software Engineering (cs.SE)", "pdf_link": null, "comments": "Comments:      12 pages (10 + 2 pages for references)", "url": "http://arxiv.org/abs/2507.19027v1", "summary": "Background: The use of large language models (LLMs) in the title-abstract\nscreening process of systematic reviews (SRs) has shown promising results, but\nsuffers from limited performance evaluation. Aims: Create a benchmark dataset\nto evaluate the performance of LLMs in the title-abstract screening process of\nSRs. Provide evidence whether using LLMs in title-abstract screening in\nsoftware engineering is advisable. Method: We start with 169 SR research\nartifacts and find 24 of those to be suitable for inclusion in the dataset.\nUsing the dataset we benchmark title-abstract screening using 9 LLMs. Results:\nWe present the SESR-Eval (Software Engineering Systematic Review Evaluation)\ndataset containing 34,528 labeled primary studies, sourced from 24 secondary\nstudies published in software engineering (SE) journals. Most LLMs performed\nsimilarly and the differences in screening accuracy between secondary studies\nare greater than differences between LLMs. The cost of using an LLM is\nrelatively low - less than $40 per secondary study even for the most expensive\nmodel. Conclusions: Our benchmark enables monitoring AI performance in the\nscreening task of SRs in software engineering. At present, LLMs are not yet\nrecommended for automating the title-abstract screening process, since accuracy\nvaries widely across secondary studies, and no LLM managed a high recall with\nreasonable precision. In future, we plan to investigate factors that influence\nLLM screening performance between studies.", "comment": "12 pages (10 + 2 pages for references)", "pdf_url": "http://arxiv.org/pdf/2507.19027v1", "cate": "cs.SE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "SESR-Eval：用于评估大型语言模型在系统评价标题-摘要筛选中表现的数据集", "tldr": "该研究创建了SESR-Eval数据集，用于评估大型语言模型（LLM）在软件工程系统评价标题-摘要筛选中的表现。结果显示LLM表现相似，但准确性在不同次级研究中差异很大，目前不推荐自动化筛选。", "motivation": "背景：大型语言模型（LLM）在系统评价（SRs）的标题-摘要筛选过程中显示出前景，但性能评估有限。目的：创建一个基准数据集，以评估LLM在SRs标题-摘要筛选过程中的性能。提供证据，说明在软件工程中使用LLM进行标题-摘要筛选是否可取。", "method": "我们从169个SR研究成果中筛选出24个适合纳入数据集。使用该数据集，我们通过9个LLM对标题-摘要筛选进行了基准测试。", "result": "我们提出了SESR-Eval（软件工程系统评价评估）数据集，其中包含34,528个已标记的主要研究，这些研究来源于24篇发表在软件工程（SE）期刊上的次级研究。大多数LLM的表现相似，并且次级研究之间的筛选准确性差异大于LLM之间的差异。使用LLM的成本相对较低——即使是最昂贵的模型，每个次级研究的成本也低于40美元。", "conclusion": "我们的基准能够监测AI在软件工程SRs筛选任务中的表现。目前，不建议使用LLM自动化标题-摘要筛选过程，因为不同次级研究的准确性差异很大，并且没有LLM能在合理精度下达到高召回率。未来，我们计划调查影响LLM筛选性能的因素。", "translation": "背景：大型语言模型（LLM）在系统评价（SRs）的标题-摘要筛选过程中显示出前景，但性能评估有限。目的：创建一个基准数据集，以评估LLM在SRs标题-摘要筛选过程中的性能。提供证据，说明在软件工程中使用LLM进行标题-摘要筛选是否可取。方法：我们从169个SR研究成果中筛选出24个适合纳入数据集。使用该数据集，我们通过9个LLM对标题-摘要筛选进行了基准测试。结果：我们提出了SESR-Eval（软件工程系统评价评估）数据集，其中包含34,528个已标记的主要研究，这些研究来源于24篇发表在软件工程（SE）期刊上的次级研究。大多数LLM的表现相似，并且次级研究之间的筛选准确性差异大于LLM之间的差异。使用LLM的成本相对较低——即使是最昂贵的模型，每个次级研究的成本也低于40美元。结论：我们的基准能够监测AI在软件工程SRs筛选任务中的表现。目前，不建议使用LLM自动化标题-摘要筛选过程，因为不同次级研究的准确性差异很大，并且没有LLM能在合理精度下达到高召回率。未来，我们计划调查影响LLM筛选性能的因素。", "summary": "本研究旨在解决大型语言模型（LLM）在系统评价（SRs）标题-摘要筛选中性能评估不足的问题。研究团队构建了SESR-Eval数据集，该数据集包含34,528个来自24项软件工程次级研究的已标记主要研究。通过对9个LLM进行基准测试，研究发现LLM的表现相似，但不同次级研究间的筛选准确性差异大于LLM间的差异。尽管使用LLM的成本较低，但由于准确性在不同研究中波动较大，且未能同时实现高召回率和合理精度，目前不建议将LLM用于自动化标题-摘要筛选。", "keywords": "大型语言模型, 系统评价, 标题-摘要筛选, 数据集, 软件工程", "comments": "本研究通过构建专门的数据集SESR-Eval，为评估LLM在软件工程系统评价筛选中的应用提供了一个重要基准。其创新之处在于提供了大规模的、真实世界的数据来填补LLM在特定领域性能评估的空白。然而，研究结果也揭示了LLM在自动化筛选任务中的局限性，即其准确性在不同研究之间波动大，且难以同时保证高召回率和合理精度，这对于实际应用具有重要的指导意义。未来的研究方向是探究影响LLM筛选性能的因素，这将有助于提升LLM在该任务中的实用性。"}}
{"id": "2507.19002", "title": "Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment", "authors": ["Ying Ba", "Tianyu Zhang", "Yalong Bai", "Wenyi Mo", "Tao Liang", "Bing Su", "Ji-Rong Wen"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      Accepted to ICCV 2025", "url": "http://arxiv.org/abs/2507.19002v1", "summary": "Contemporary image generation systems have achieved high fidelity and\nsuperior aesthetic quality beyond basic text-image alignment. However, existing\nevaluation frameworks have failed to evolve in parallel. This study reveals\nthat human preference reward models fine-tuned based on CLIP and BLIP\narchitectures have inherent flaws: they inappropriately assign low scores to\nimages with rich details and high aesthetic value, creating a significant\ndiscrepancy with actual human aesthetic preferences. To address this issue, we\ndesign a novel evaluation score, ICT (Image-Contained-Text) score, that\nachieves and surpasses the objectives of text-image alignment by assessing the\ndegree to which images represent textual content. Building upon this\nfoundation, we further train an HP (High-Preference) score model using solely\nthe image modality to enhance image aesthetics and detail quality while\nmaintaining text-image alignment. Experiments demonstrate that the proposed\nevaluation model improves scoring accuracy by over 10\\% compared to existing\nmethods, and achieves significant results in optimizing state-of-the-art\ntext-to-image models. This research provides theoretical and empirical support\nfor evolving image generation technology toward higher-order human aesthetic\npreferences. Code is available at https://github.com/BarretBa/ICTHP.", "comment": "Accepted to ICCV 2025", "pdf_url": "http://arxiv.org/pdf/2507.19002v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "增强高质量图像生成的奖励模型：超越文本-图像对齐", "tldr": "现有图像生成评估模型（基于CLIP/BLIP）未能准确反映人类审美偏好。本文提出了ICT和HP分数，以更好地评估和提升高质量图像生成，实验证明其在评分准确性上提高了10%以上，并能有效优化最先进的文本到图像模型。", "motivation": "当代图像生成系统已达到高保真度和卓越美学质量，但现有评估框架未能同步发展。基于CLIP和BLIP架构微调的人类偏好奖励模型存在固有缺陷，它们不适当地将低分分配给具有丰富细节和高美学价值的图像，与实际人类审美偏好存在显著差异。", "method": "设计了一种新颖的评估分数ICT（Image-Contained-Text）分数，通过评估图像表达文本内容的程度来实现并超越文本-图像对齐的目标。在此基础上，进一步使用纯图像模态训练了一个HP（High-Preference）分数模型，以在保持文本-图像对齐的同时增强图像美学和细节质量。", "result": "所提出的评估模型与现有方法相比，评分准确率提高了10%以上。在优化最先进的文本到图像模型方面取得了显著成果。", "conclusion": "本研究为图像生成技术向更高阶人类审美偏好演进提供了理论和经验支持。", "translation": "当代图像生成系统已实现超越基本文本-图像对齐的高保真度和卓越美学质量。然而，现有评估框架未能同步发展。本研究揭示了基于CLIP和BLIP架构微调的人类偏好奖励模型存在固有缺陷：它们不适当地将低分分配给具有丰富细节和高美学价值的图像，与实际人类审美偏好存在显著差异。为解决此问题，我们设计了一种新颖的评估分数，ICT（Image-Contained-Text）分数，通过评估图像表达文本内容的程度来实现并超越文本-图像对齐的目标。在此基础上，我们进一步使用纯图像模态训练了一个HP（High-Preference）分数模型，以在保持文本-图像对齐的同时增强图像美学和细节质量。实验表明，所提出的评估模型与现有方法相比，评分准确率提高了10%以上，并在优化最先进的文本到图像模型方面取得了显著成果。本研究为图像生成技术向更高阶人类审美偏好演进提供了理论和经验支持。代码可在https://github.com/BarretBa/ICTHP获取。", "summary": "当前基于CLIP/BLIP的图像生成评估模型难以准确反映人类审美偏好，常低估细节丰富、高质量的图像。为解决此问题，本文引入了ICT分数以改进文本-图像对齐评估，并提出了仅基于图像模态的HP分数以提升美学和细节质量。实验结果显示，新模型在评分准确性上提升了10%以上，并能显著优化最先进的文本到图像模型，从而推动图像生成技术更好地与人类审美对齐。", "keywords": "图像生成, 奖励模型, 人类偏好, 文本-图像对齐, 评估指标", "comments": "本文创新性地解决了当前图像生成评估中的一个关键限制——自动化奖励模型与人类审美偏好之间的不匹配。通过引入ICT和HP分数，它提供了一种超越简单文本-图像对齐的更细致的图像质量评估方法，这对于高保真和美学上令人愉悦的生成式AI的发展至关重要。其经验证据表明，在准确性和优化能力方面均有显著提升，突显了其实际重要性。"}}
{"id": "2506.22495", "title": "Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for ECG Analyses", "authors": ["He-Yang Xu", "Hongxiang Gao", "Yuwen Li", "Xiu-Shen Wei", "Chengyu Liu"], "categories": ["eess.SP", "cs.AI", "cs.LG"], "primary_category": "Subjects:       Signal Processing (eess.SP)", "pdf_link": null, "comments": "Comments:      there are factual errors", "url": "http://arxiv.org/abs/2506.22495v4", "summary": "The diagnostic value of electrocardiogram (ECG) lies in its dynamic\ncharacteristics, ranging from rhythm fluctuations to subtle waveform\ndeformations that evolve across time and frequency domains. However, supervised\nECG models tend to overfit dominant and repetitive patterns, overlooking\nfine-grained but clinically critical cues, a phenomenon known as Simplicity\nBias (SB), where models favor easily learnable signals over subtle but\ninformative ones. In this work, we first empirically demonstrate the presence\nof SB in ECG analyses and its negative impact on diagnostic performance, while\nsimultaneously discovering that self-supervised learning (SSL) can alleviate\nit, providing a promising direction for tackling the bias. Following the SSL\nparadigm, we propose a novel method comprising two key components: 1)\nTemporal-Frequency aware Filters to capture temporal-frequency features\nreflecting the dynamic characteristics of ECG signals, and 2) building on this,\nMulti-Grained Prototype Reconstruction for coarse and fine representation\nlearning across dual domains, further mitigating SB. To advance SSL in ECG\nanalyses, we curate a large-scale multi-site ECG dataset with 1.53 million\nrecordings from over 300 clinical centers. Experiments on three downstream\ntasks across six ECG datasets demonstrate that our method effectively reduces\nSB and achieves state-of-the-art performance.", "comment": "Revised Version 4", "pdf_url": "http://arxiv.org/pdf/2506.22495v4", "cate": "eess.SP", "date": "2025-06-25", "updated": "2025-07-28", "AI": {"title_translation": "感知心脏的掩码自编码器：揭示心电图分析中的简单性偏差", "tldr": "监督式心电图模型存在简单性偏差，导致忽略关键细节。本文提出一种基于自监督学习的新方法，利用时频感知滤波器和多粒度原型重建，有效缓解偏差并提升诊断性能。", "motivation": "监督式心电图模型倾向于过拟合主导和重复模式，忽视细微但临床关键的线索，即“简单性偏差”，这会负面影响诊断性能。", "method": "首先经验性地证明了心电图分析中简单性偏差的存在及其负面影响，并发现自监督学习可以缓解该偏差。在此基础上，提出了一种新方法，包含两个关键组件：1）时频感知滤波器，用于捕获反映心电信号动态特征的时频特征；2）多粒度原型重建，用于跨双域进行粗粒度和细粒度表示学习，进一步减轻简单性偏差。为此，还构建了一个大规模多中心心电图数据集。", "result": "在六个心电图数据集上的三个下游任务实验表明，所提出的方法有效降低了简单性偏差，并取得了最先进的性能。", "conclusion": "本文提出的基于自监督学习的方法能够有效减轻心电图分析中的简单性偏差，并显著提升诊断性能。", "translation": "心电图（ECG）的诊断价值在于其动态特性，从节律波动到随时间域和频率域演变的细微波形变形。然而，监督式心电图模型倾向于过拟合主导和重复的模式，忽略细微但临床关键的线索，这种现象被称为简单性偏差（Simplicity Bias, SB），即模型偏爱易于学习的信号而非细微但信息丰富的信号。在这项工作中，我们首先经验性地证明了心电图分析中简单性偏差的存在及其对诊断性能的负面影响，同时发现自监督学习（SSL）可以缓解它，为解决该偏差提供了一个有前景的方向。遵循自监督学习范式，我们提出了一种包含两个关键组件的新方法：1）时频感知滤波器，用于捕获反映心电信号动态特征的时频特征；2）在此基础上，多粒度原型重建，用于跨双域进行粗粒度和细粒度表示学习，进一步减轻简单性偏差。为了推进心电图分析中的自监督学习，我们策划了一个大规模多中心心电图数据集，包含来自300多个临床中心的153万条记录。在六个心电图数据集上的三个下游任务实验表明，我们的方法有效降低了简单性偏差，并取得了最先进的性能。", "summary": "本研究探讨了心电图（ECG）分析中监督模型存在的“简单性偏差”（SB），即模型倾向于学习易懂信号而忽略关键细微特征。论文首先实证了SB的存在及其负面影响，并发现自监督学习（SSL）可有效缓解此问题。基于SSL，作者提出一种新方法，包含时频感知滤波器和多粒度原型重建，以捕捉ECG动态特性并减轻SB。为支持研究，构建了一个大型多中心ECG数据集。实验证明，该方法有效降低了SB并达到了当前最佳性能。", "keywords": "心电图分析, 简单性偏差, 自监督学习, 时频特征, 原型重建", "comments": "该论文的创新点在于首次实证揭示了心电图分析中的“简单性偏差”问题，并明确指出自监督学习是解决该问题的有效途径。提出的时频感知滤波器和多粒度原型重建方法针对心电信号的动态特性设计，具有较强的实用性。同时，构建大规模多中心心电图数据集对推动该领域发展具有重要意义。该研究为提升心电图智能诊断的准确性和鲁棒性提供了新思路和有效工具。"}}
{"id": "2507.19478", "title": "MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents", "authors": ["Xuehui Wang", "Zhenyu Wu", "JingJing Xie", "Zichen Ding", "Bowen Yang", "Zehao Li", "Zhaoyang Liu", "Qingyun Li", "Xuan Dong", "Zhe Chen", "Weiyun Wang", "Xiangyu Zhao", "Jixuan Chen", "Haodong Duan", "Tianbao Xie", "Chenyu Yang", "Shiqian Su", "Yue Yu", "Yuan Huang", "Yiqian Liu", "Xiao Zhang", "Yanting Zhang", "Xiangyu Yue", "Weijie Su", "Xizhou Zhu", "Wei Shen", "Jifeng Dai", "Wenhai Wang"], "categories": ["cs.CV", "cs.CL"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      in progress", "url": "http://arxiv.org/abs/2507.19478v1", "summary": "We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI\nautomation agents across Windows, macOS, Linux, iOS, Android, and Web\nplatforms. It comprises four levels: GUI Content Understanding, Element\nGrounding, Task Automation, and Task Collaboration, covering essential skills\nfor GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA)\nmetric to assess GUI agent execution efficiency in online automation scenarios.\nThrough MMBench-GUI, we identify accurate visual grounding as a critical\ndeterminant of overall task success, emphasizing the substantial benefits of\nmodular frameworks that integrate specialized grounding modules. Furthermore,\nto achieve reliable GUI automation, an agent requires strong task planning and\ncross-platform generalization abilities, with long-context memory, a broad\naction space, and long-term reasoning playing a critical role. More important,\ntask efficiency remains a critically underexplored dimension, and all models\nsuffer from substantial inefficiencies, with excessive redundant steps even\nwhen tasks are ultimately completed. The integration of precise localization,\neffective planning, and early stopping strategies is indispensable to enable\ntruly efficient and scalable GUI automation. Our benchmark code, evaluation\ndata, and running environment will be publicly available at\nhttps://github.com/open-compass/MMBench-GUI.", "comment": "in progress", "pdf_url": "http://arxiv.org/pdf/2507.19478v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "MMBench-GUI：GUI智能体的分层多平台评估框架", "tldr": "引入MMBench-GUI，一个用于评估跨多平台GUI自动化智能体的分层基准，并提出EQA指标，强调视觉定位、任务规划和效率的重要性。", "motivation": "现有GUI自动化智能体评估缺乏一个全面的、分层的多平台评估框架，且任务效率维度未被充分探索，导致模型存在大量冗余步骤。", "method": "引入MMBench-GUI，一个分层的多平台GUI代理评估基准，涵盖GUI内容理解、元素定位、任务自动化和任务协作四个级别。提出了新的效率-质量区域 (EQA) 指标来评估在线自动化场景中的执行效率。", "result": "通过MMBench-GUI，发现准确的视觉定位是整体任务成功的关键决定因素，并强调集成专业定位模块的模块化框架的显著优势。发现智能体需要强大的任务规划和跨平台泛化能力，长上下文记忆、广阔的行动空间和长期推理至关重要。所有模型都存在大量低效率问题，即使任务完成也有过度冗余的步骤。", "conclusion": "为了实现真正高效和可扩展的GUI自动化，精确的本地化、有效的规划和早期停止策略的集成是不可或缺的。", "translation": "我们引入了MMBench-GUI，这是一个用于评估跨Windows、macOS、Linux、iOS、Android和Web平台GUI自动化智能体的分层基准。它包含四个级别：GUI内容理解、元素定位、任务自动化和任务协作，涵盖了GUI智能体的基本技能。此外，我们提出了一种新颖的效率-质量区域（EQA）指标，用于评估在线自动化场景中GUI智能体的执行效率。通过MMBench-GUI，我们发现准确的视觉定位是整体任务成功的关键决定因素，强调了集成专业定位模块的模块化框架的显著优势。此外，为了实现可靠的GUI自动化，智能体需要强大的任务规划和跨平台泛化能力，其中长上下文记忆、广阔的行动空间和长期推理发挥着关键作用。更重要的是，任务效率仍然是一个严重未被充分探索的维度，所有模型都存在严重的低效率问题，即使任务最终完成，也存在过多的冗余步骤。精确的本地化、有效的规划和早期停止策略的集成对于实现真正高效和可扩展的GUI自动化是不可或缺的。我们的基准代码、评估数据和运行环境将在https://github.com/open-compass/MMBench-GUI公开可用。", "summary": "MMBench-GUI是一个为GUI自动化智能体设计的跨平台分层评估基准，涵盖了内容理解、元素定位、任务自动化和任务协作四个核心技能。该框架引入了效率-质量区域 (EQA) 指标以衡量在线自动化效率。研究发现，准确的视觉定位对任务成功至关重要，且当前模型普遍存在效率低下和冗余步骤问题。为实现高效可扩展的GUI自动化，需要整合精确本地化、有效规划和早期停止策略。", "keywords": "GUI自动化, 基准测试, 多平台, 视觉定位, 效率评估", "comments": "这篇论文通过提出MMBench-GUI，一个全面的分层多平台评估框架，填补了GUI自动化智能体评估领域的空白。其创新之处在于提出了EQA指标以量化效率，并强调了视觉定位、任务规划和效率在GUI自动化中的关键作用。论文指出了当前模型在效率方面的显著不足，并为未来的研究指明了方向，即整合精确本地化、有效规划和早期停止策略以提升效率和可扩展性。"}}
{"id": "2411.12949", "title": "Epidemiology-informed Network for Robust Rumor Detection", "authors": ["Wei Jiang", "Tong Chen", "Xinyi Gao", "Wentao Zhang", "Lizhen Cui", "Hongzhi Yin"], "categories": ["cs.SI", "cs.IR"], "primary_category": "Subjects:       Social and Information Networks (cs.SI)", "pdf_link": null, "comments": "Comments:      Accepted by The Web Conference 2025 (WWW2025)", "url": "http://arxiv.org/abs/2411.12949v3", "summary": "The rapid spread of rumors on social media has posed significant challenges\nto maintaining public trust and information integrity. Since an information\ncascade process is essentially a propagation tree, recent rumor detection\nmodels leverage graph neural networks to additionally capture information\npropagation patterns, thus outperforming text-only solutions. Given the\nvariations in topics and social impact of the root node, different source\ninformation naturally has distinct outreach capabilities, resulting in\ndifferent heights of propagation trees. This variation, however, impedes the\ndata-driven design of existing graph-based rumor detectors. Given a shallow\npropagation tree with limited interactions, it is unlikely for graph-based\napproaches to capture sufficient cascading patterns, questioning their ability\nto handle less popular news or early detection needs. In contrast, a deep\npropagation tree is prone to noisy user responses, and this can in turn\nobfuscate the predictions. In this paper, we propose a novel\nEpidemiology-informed Network (EIN) that integrates epidemiological knowledge\nto enhance performance by overcoming data-driven methods sensitivity to data\nquality. Meanwhile, to adapt epidemiology theory to rumor detection, it is\nexpected that each users stance toward the source information will be\nannotated. To bypass the costly and time-consuming human labeling process, we\ntake advantage of large language models to generate stance labels, facilitating\noptimization objectives for learning epidemiology-informed representations. Our\nexperimental results demonstrate that the proposed EIN not only outperforms\nstate-of-the-art methods on real-world datasets but also exhibits enhanced\nrobustness across varying tree depths.", "comment": "Accepted by The Web Conference 2025 (WWW2025)", "pdf_url": "http://arxiv.org/pdf/2411.12949v3", "cate": "cs.SI", "date": "2024-11-20", "updated": "2025-07-25", "AI": {"title_translation": "流行病学启发式网络用于鲁棒谣言检测", "tldr": "本文提出了一种流行病学启发式网络（EIN），通过整合流行病学知识和利用大型语言模型生成立场标签，以克服现有图神经网络在谣言检测中对传播树深度敏感的问题，并在真实世界数据集中表现出优越的性能和鲁棒性。", "motivation": "社交媒体上谣言的迅速传播对维护公众信任和信息完整性构成了重大挑战。现有的基于图神经网络的谣言检测模型对传播树的深度敏感，浅层传播树难以捕获足够的级联模式，而深层传播树则容易受到噪声用户响应的干扰，这限制了它们处理不热门新闻或早期检测需求的能力。", "method": "本文提出了一种新颖的流行病学启发式网络（EIN），该网络整合了流行病学知识以增强性能，克服了数据驱动方法对数据质量的敏感性。为了使流行病学理论适应谣言检测，EIN利用大型语言模型生成用户对源信息的立场标签，从而绕过了耗时的人工标注过程，并促进了流行病学启发式表示的学习。", "result": "实验结果表明，所提出的EIN不仅在真实世界数据集中优于最先进的方法，而且在不同树深下也表现出增强的鲁棒性。", "conclusion": "通过整合流行病学知识和利用大型语言模型生成立场标签，所提出的流行病学启发式网络（EIN）能够有效克服现有图神经网络在谣言检测中对传播树深度敏感的问题，并在鲁棒性和性能上超越现有方法。", "translation": "社交媒体上谣言的迅速传播对维护公众信任和信息完整性构成了重大挑战。由于信息级联过程本质上是一种传播树，最近的谣言检测模型利用图神经网络额外捕获信息传播模式，从而优于纯文本解决方案。鉴于根节点主题和社会影响的变化，不同的源信息自然具有不同的传播能力，导致传播树的高度不同。然而，这种变异阻碍了现有基于图的谣言检测器的数据驱动设计。对于交互有限的浅层传播树，基于图的方法不太可能捕获足够的级联模式，这使得它们处理不热门新闻或早期检测需求的能力受到质疑。相比之下，深层传播树容易产生嘈杂的用户响应，这反过来又会混淆预测。在本文中，我们提出了一种新颖的流行病学启发式网络（EIN），它整合了流行病学知识，通过克服数据驱动方法对数据质量的敏感性来增强性能。同时，为了使流行病学理论适应谣言检测，需要标注每个用户对源信息的立场。为了绕过昂贵且耗时的人工标注过程，我们利用大型语言模型生成立场标签，从而促进了流行病学启发式表示学习的优化目标。我们的实验结果表明，所提出的EIN不仅在真实世界数据集中优于最先进的方法，而且在不同树深下也表现出增强的鲁棒性。", "summary": "本文提出了一种名为流行病学启发式网络（EIN）的新型谣言检测模型，旨在解决现有图神经网络在处理不同深度传播树时遇到的鲁棒性问题。EIN通过整合流行病学知识来增强模型性能，并利用大型语言模型自动生成用户立场标签，以克服传统方法对数据质量的敏感性和高昂的人工标注成本。实验证明，EIN在真实世界数据集上表现出优于现有方法的性能，并在不同传播树深度下展现出更强的鲁棒性。", "keywords": "谣言检测, 图神经网络, 流行病学, 大型语言模型, 鲁棒性", "comments": "本文的创新点在于将流行病学知识引入谣言检测领域，并通过大型语言模型辅助生成用户立场标签，有效解决了现有图神经网络在处理不同深度传播树时的数据敏感性和鲁棒性问题。这种跨领域知识的融合以及对LLM的创造性应用，为谣言检测提供了新的视角和更实用的解决方案。"}}
{"id": "2503.17534", "title": "MetaSel: A Test Selection Approach for Fine-tuned DNN Models", "authors": ["Amin Abbasishahkoo", "Mahboubeh Dadkhah", "Lionel Briand", "Dayi Lin"], "categories": ["cs.LG", "cs.SE"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2503.17534v3", "summary": "Deep Neural Networks (DNNs) face challenges during deployment due to data\ndistribution shifts. Fine-tuning adapts pre-trained models to new contexts\nrequiring smaller labeled sets. However, testing fine-tuned models under\nconstrained labeling budgets remains a critical challenge. This paper\nintroduces MetaSel, a new approach, tailored for fine-tuned DNN models, to\nselect tests from unlabeled inputs. MetaSel assumes that fine-tuned and\npre-trained models share related data distributions and exhibit similar\nbehaviors for many inputs. However, their behaviors diverge within the input\nsubspace where fine-tuning alters decision boundaries, making those inputs more\nprone to misclassification. Unlike general approaches that rely solely on the\nDNN model and its input set, MetaSel leverages information from both the\nfine-tuned and pre-trained models and their behavioral differences to estimate\nmisclassification probability for unlabeled test inputs, enabling more\neffective test selection. Our extensive empirical evaluation, comparing MetaSel\nagainst 11 state-of-the-art approaches and involving 68 fine-tuned models\nacross weak, medium, and strong distribution shifts, demonstrates that MetaSel\nconsistently delivers significant improvements in Test Relative Coverage (TRC)\nover existing baselines, particularly under highly constrained labeling\nbudgets. MetaSel shows average TRC improvements of 28.46% to 56.18% over the\nmost frequent second-best baselines while maintaining a high TRC median and low\nvariability. Our results confirm MetaSel's practicality, robustness, and\ncost-effectiveness for test selection in the context of fine-tuned models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2503.17534v3", "cate": "cs.LG", "date": "2025-03-21", "updated": "2025-07-24", "AI": {"title_translation": "MetaSel：一种针对微调DNN模型的测试选择方法", "tldr": "MetaSel是一种新的测试选择方法，专门用于微调的深度神经网络（DNN）模型，通过利用微调模型和预训练模型之间的行为差异来选择未标记的输入进行测试，显著提高了测试相对覆盖率（TRC），尤其是在标注预算有限的情况下。", "motivation": "深度神经网络（DNN）在部署过程中面临数据分布变化的挑战。虽然微调可以将预训练模型适应新的上下文，但如何在有限的标注预算下测试微调模型仍然是一个关键挑战。", "method": "MetaSel方法利用微调模型和预训练模型的信息及其行为差异来估计未标记测试输入的错误分类概率，从而实现更有效的测试选择。它假设微调模型和预训练模型共享相关的数据分布，并且对于许多输入表现出相似的行为，但在微调改变决策边界的输入子空间中，它们的行为会发散。", "result": "MetaSel在弱、中、强分布偏移下的68个微调模型上，与11种最先进的方法进行比较，结果表明它始终比现有基线显著提高测试相对覆盖率（TRC），尤其是在高度受限的标注预算下。MetaSel相对于次优基线，平均TRC提高了28.46%到56.18%，同时保持了较高的TRC中位数和较低的变异性。", "conclusion": "研究结果证实了MetaSel在微调模型测试选择背景下的实用性、鲁棒性和成本效益。", "translation": "深度神经网络（DNN）在部署过程中面临数据分布变化的挑战。微调使预训练模型适应新的上下文，需要较小的标注数据集。然而，在受限的标注预算下测试微调模型仍然是一个关键挑战。本文介绍了一种名为MetaSel的新方法，专为微调DNN模型量身定制，用于从未标注的输入中选择测试。MetaSel假设微调模型和预训练模型共享相关的数据分布，并且对许多输入表现出相似的行为。然而，它们的行为在微调改变决策边界的输入子空间中发生分歧，使得这些输入更容易被错误分类。与仅依赖DNN模型及其输入集的通用方法不同，MetaSel利用微调模型和预训练模型的信息及其行为差异来估计未标注测试输入的错误分类概率，从而实现更有效的测试选择。我们广泛的实证评估，将MetaSel与11种最先进的方法进行比较，涉及弱、中、强分布偏移下的68个微调模型，结果表明MetaSel始终比现有基线显著提高测试相对覆盖率（TRC），尤其是在高度受限的标注预算下。MetaSel相对于最常见的次优基线，平均TRC提高了28.46%到56.18%，同时保持了较高的TRC中位数和较低的变异性。我们的结果证实了MetaSel在微调模型测试选择背景下的实用性、鲁棒性和成本效益。", "summary": "本文提出了MetaSel，一种针对微调深度神经网络（DNN）模型的测试选择方法。该方法通过利用微调模型和其对应的预训练模型之间的行为差异来估计未标记输入的误分类概率，从而有效选择测试用例。与现有方法不同，MetaSel特别关注微调导致的决策边界变化区域。实验结果表明，在不同程度的数据分布偏移和有限标注预算下，MetaSel显著提高了测试相对覆盖率（TRC），表现出优于11种现有基线的性能，并验证了其在微调模型测试中的实用性、鲁棒性和成本效益。", "keywords": "测试选择, 微调DNN, 深度神经网络, 分布偏移, 错误分类概率", "comments": "MetaSel的创新点在于其利用了微调模型和预训练模型之间的差异信息进行测试选择，这与仅依赖单一模型的方法不同。这种方法特别适用于数据标注预算有限的实际场景，提高了测试效率和模型可靠性，对于微调DNN模型的质量保证具有重要意义。"}}
{"id": "2507.17971", "title": "Benchmarking of Deep Learning Methods for Generic MRI Multi-Organ Abdominal Segmentation", "authors": ["Deepa Krishnaswamy", "Cosmin Ciausu", "Steve Pieper", "Ron Kikinis", "Benjamin Billot", "Andrey Fedorov"], "categories": ["eess.IV", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.17971v2", "summary": "Recent advances in deep learning have led to robust automated tools for\nsegmentation of abdominal computed tomography (CT). Meanwhile, segmentation of\nmagnetic resonance imaging (MRI) is substantially more challenging due to the\ninherent signal variability and the increased effort required for annotating\ntraining datasets. Hence, existing approaches are trained on limited sets of\nMRI sequences, which might limit their generalizability. To characterize the\nlandscape of MRI abdominal segmentation tools, we present here a comprehensive\nbenchmarking of the three state-of-the-art and open-source models:\nMRSegmentator, MRISegmentator-Abdomen, and TotalSegmentator MRI. Since these\nmodels are trained using labor-intensive manual annotation cycles, we also\nintroduce and evaluate ABDSynth, a SynthSeg-based model purely trained on\nwidely available CT segmentations (no real images). More generally, we assess\naccuracy and generalizability by leveraging three public datasets (not seen by\nany of the evaluated methods during their training), which span all major\nmanufacturers, five MRI sequences, as well as a variety of subject conditions,\nvoxel resolutions, and fields-of-view. Our results reveal that MRSegmentator\nachieves the best performance and is most generalizable. In contrast, ABDSynth\nyields slightly less accurate results, but its relaxed requirements in training\ndata make it an alternative when the annotation budget is limited. The\nevaluation code and datasets are given for future benchmarking at\nhttps://github.com/deepakri201/AbdoBench, along with inference code and weights\nfor ABDSynth.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.17971v2", "cate": "eess.IV", "date": "2025-07-23", "updated": "2025-07-25", "AI": {"title_translation": "用于通用MRI多器官腹部分割的深度学习方法基准测试", "tldr": "对MRI腹部多器官分割的深度学习方法进行了全面基准测试，发现MRSegmentator表现最佳且泛化能力最强，而ABDSynth在标注预算有限时是一个可行的替代方案。", "motivation": "现有深度学习方法在MRI腹部分割中由于固有的信号变异性和训练数据集标注工作量大而面临挑战，且通常在有限的MRI序列上训练，可能限制其泛化能力。因此，需要对MRI腹部分割工具的现状进行全面评估。", "method": "本研究对三种最先进的开源模型（MRSegmentator、MRISegmentator-Abdomen和TotalSegmentator MRI）进行了全面基准测试。此外，引入并评估了ABDSynth，一个纯粹基于广泛可用的CT分割数据（无真实图像）训练的SynthSeg模型。通过利用三个未被评估方法训练过的公共数据集，评估了准确性和泛化能力，这些数据集涵盖了主要制造商、五种MRI序列以及各种受试者条件、体素分辨率和视野。", "result": "结果表明，MRSegmentator取得了最佳性能且泛化能力最强。相比之下，ABDSynth的结果略不准确，但其对训练数据的宽松要求使其在标注预算有限时成为一个可行的替代方案。", "conclusion": "MRSegmentator是MRI多器官腹部分割的最佳深度学习模型，具有卓越的性能和泛化能力。ABDSynth虽然准确性稍逊，但其无需真实MRI标注数据的训练方式，使其在资源受限的情况下具有重要的实用价值。", "translation": "深度学习的最新进展已催生出用于腹部计算机断层扫描（CT）分割的强大自动化工具。然而，由于固有的信号变异性和标注训练数据集所需的大量工作，磁共振成像（MRI）的分割更具挑战性。因此，现有方法在有限的MRI序列集上进行训练，这可能会限制其泛化能力。为了表征MRI腹部分割工具的现状，我们在此对三种最先进的开源模型进行了全面基准测试：MRSegmentator、MRISegmentator-Abdomen和TotalSegmentator MRI。由于这些模型是使用劳动密集型手动标注周期进行训练的，我们还介绍并评估了ABDSynth，一个纯粹基于广泛可用的CT分割数据（无真实图像）训练的SynthSeg模型。更普遍地，我们通过利用三个公共数据集（在任何评估方法的训练过程中均未见过）来评估准确性和泛化能力，这些数据集涵盖了所有主要制造商、五种MRI序列以及各种受试者条件、体素分辨率和视野。我们的结果表明，MRSegmentator实现了最佳性能且泛化能力最强。相比之下，ABDSynth产生的结果略不准确，但其对训练数据的宽松要求使其在标注预算有限时成为一个替代方案。评估代码和数据集可在https://github.com/deepakri201/AbdoBench获取，以供未来基准测试，同时还提供了ABDSynth的推理代码和权重。", "summary": "本研究对用于通用MRI多器官腹部分割的深度学习方法进行了全面的基准测试，评估了MRSegmentator、MRISegmentator-Abdomen和TotalSegmentator MRI三种主流模型。同时，提出并验证了ABDSynth，一个仅利用CT分割数据训练的模型，旨在解决MRI标注成本高昂的问题。通过在多个公共数据集上进行评估，研究发现MRSegmentator表现出最佳的性能和泛化能力，而ABDSynth虽然准确性略低，但在标注预算有限时提供了一个可行的替代方案。", "keywords": "深度学习, MRI分割, 腹部, 基准测试, 多器官", "comments": "该论文通过对现有深度学习模型进行全面的基准测试，清晰地揭示了当前MRI腹部多器官分割领域的性能格局。其创新点在于引入了ABDSynth模型，该模型通过利用CT分割数据进行训练，有效缓解了MRI数据标注成本高昂的瓶颈，为资源有限的研究者和临床应用提供了重要的实用价值。论文还提供了代码和数据集，促进了未来该领域的进一步研究和比较。"}}
{"id": "2507.18834", "title": "Third-Party Assessment of Mobile Performance in the 5G Era", "authors": ["ASM Rizvi", "John Heidemann", "David Plonka"], "categories": ["cs.NI", "cs.PF"], "primary_category": "Subjects:       Networking and Internet Architecture (cs.NI)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18834v1", "summary": "The web experience using mobile devices is important since a significant\nportion of the Internet traffic is initiated from mobile devices. In the era of\n5G, users expect a high-performance data network to stream media content and\nfor other latency-sensitive applications. In this paper, we characterize mobile\nexperience in terms of latency, throughput, and stability measured from a\ncommercial, globally-distributed CDN. Unlike prior work, CDN data provides a\nrelatively neutral, carrier-agnostic perspective, providing a clear view of\nmultiple and international providers. Our analysis of mobile client traffic\nshows mobile users sometimes experience markedly low latency, even as low as 6\nms. However, only the top 5% users regularly experience less than 20 ms of\nminimum latency. While 100 Mb/s throughput is not rare, we show around 60%\nusers observe less than 50 Mb/s throughput. We find the minimum mobile latency\nis generally stable at a specific location which can be an important\ncharacteristic for anomaly detection.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18834v1", "cate": "cs.NI", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "5G时代移动性能的第三方评估", "tldr": "本文通过分析全球分布式CDN数据，从延迟、吞吐量和稳定性方面评估了5G时代移动用户的网络体验，揭示了移动性能的实际表现。", "motivation": "移动设备产生的互联网流量巨大，且5G时代用户期望高性能网络来支持流媒体和低延迟应用，因此评估移动性能至关重要。", "method": "研究通过商业化的全球分布式CDN测量了移动体验的延迟、吞吐量和稳定性。与以往工作不同，CDN数据提供了相对中立、与运营商无关的视角，能清晰地反映多个国际提供商的情况。", "result": "分析显示，移动用户有时会体验到低至6毫秒的显著低延迟，但只有前5%的用户能经常体验到低于20毫秒的最小延迟。虽然100 Mb/s的吞吐量并不少见，但约60%的用户观察到吞吐量低于50 Mb/s。研究还发现，在特定位置，最小移动延迟通常是稳定的。", "conclusion": "本文通过CDN数据全面刻画了5G时代移动用户的网络体验，揭示了实际性能与用户期望之间的差距，并指出最小移动延迟的稳定性可作为异常检测的重要特征。", "translation": "移动设备的网络体验非常重要，因为很大一部分互联网流量都源自移动设备。在5G时代，用户期望高性能数据网络来播放媒体内容和其他对延迟敏感的应用程序。在本文中，我们从商业的全球分布式CDN测量了移动体验的延迟、吞吐量和稳定性。与之前的工作不同，CDN数据提供了相对中立、与运营商无关的视角，清晰地展示了多个国际提供商的情况。我们对移动客户端流量的分析表明，移动用户有时会体验到显著的低延迟，甚至低至6毫秒。然而，只有前5%的用户能经常体验到低于20毫秒的最小延迟。虽然100 Mb/s的吞吐量并不少见，但我们发现约60%的用户观察到吞吐量低于50 Mb/s。我们发现最小移动延迟在特定位置通常是稳定的，这可能是异常检测的一个重要特征。", "summary": "本文基于商业化全球分布式CDN数据，对5G时代移动用户的网络性能进行了深入评估，测量了延迟、吞吐量和稳定性。研究发现，虽然极端低延迟偶尔出现，但多数用户体验到的延迟和吞吐量仍未达到理想水平，例如60%用户吞吐量低于50 Mb/s。同时，研究指出最小移动延迟在特定位置的稳定性可用于异常检测。", "keywords": "5G, 移动性能, CDN, 延迟, 吞吐量", "comments": "该研究通过使用CDN数据，提供了一个相对中立且运营商无关的移动性能评估视角，克服了传统评估方法的局限性。其发现揭示了5G时代移动网络性能的实际差距，并提出了利用延迟稳定性进行异常检测的新思路，具有一定的创新性和实用价值。"}}
{"id": "2311.13729", "title": "Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case", "authors": ["Shashank Gupta", "Xuguang Ai", "Ramakanth Kavuluru"], "categories": ["cs.CL"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      An updated version of this paper has appeared in the proceedings of NLDB 2025 with a different title. The corresonding DOI is in the metadata provided below", "url": "http://arxiv.org/abs/2311.13729v3", "summary": "End-to-end relation extraction (E2ERE) is an important and realistic\napplication of natural language processing (NLP) in biomedicine. In this paper,\nwe aim to compare three prevailing paradigms for E2ERE using a complex dataset\nfocused on rare diseases involving discontinuous and nested entities. We use\nthe RareDis information extraction dataset to evaluate three competing\napproaches (for E2ERE): NER $\\rightarrow$ RE pipelines, joint sequence to\nsequence models, and generative pre-trained transformer (GPT) models. We use\ncomparable state-of-the-art models and best practices for each of these\napproaches and conduct error analyses to assess their failure modes. Our\nfindings reveal that pipeline models are still the best, while\nsequence-to-sequence models are not far behind; GPT models with eight times as\nmany parameters are worse than even sequence-to-sequence models and lose to\npipeline models by over 10 F1 points. Partial matches and discontinuous\nentities caused many NER errors contributing to lower overall E2E performances.\nWe also verify these findings on a second E2ERE dataset for chemical-protein\ninteractions. Although generative LM-based methods are more suitable for\nzero-shot settings, when training data is available, our results show that it\nis better to work with more conventional models trained and tailored for E2ERE.\nMore innovative methods are needed to marry the best of the both worlds from\nsmaller encoder-decoder pipeline models and the larger GPT models to improve\nE2ERE. As of now, we see that well designed pipeline models offer substantial\nperformance gains at a lower cost and carbon footprint for E2ERE. Our\ncontribution is also the first to conduct E2ERE for the RareDis dataset.", "comment": "An updated version of this paper has appeared in the proceedings of\n  NLDB 2025 with a different title. The corresonding DOI is in the metadata\n  provided below", "pdf_url": "http://arxiv.org/pdf/2311.13729v3", "cate": "cs.CL", "date": "2023-11-22", "updated": "2025-07-25", "AI": {"title_translation": "比较管道模型、序列到序列模型和GPT模型在端到端关系抽取中的应用：以罕见病用例进行实验", "tldr": "在罕见病端到端关系抽取（E2ERE）任务中，管道模型表现最佳，优于序列到序列模型和GPT模型。", "motivation": "端到端关系抽取（E2ERE）是生物医学中重要的自然语言处理应用。本文旨在比较三种主流的E2ERE范式，并使用涉及不连续和嵌套实体的复杂罕见病数据集进行评估。", "method": "使用RareDis信息抽取数据集和第二个用于化学-蛋白质相互作用的E2ERE数据集，评估了NER -> RE管道模型、联合序列到序列模型和生成式预训练Transformer (GPT) 模型。采用了可比较的最新模型和最佳实践，并进行了错误分析。", "result": "管道模型仍然表现最佳，序列到序列模型紧随其后。GPT模型（参数量是8倍）性能甚至不如序列到序列模型，且比管道模型低超过10个F1点。部分匹配和不连续实体导致了许多NER错误。这些发现也在第二个数据集上得到了验证。", "conclusion": "当存在训练数据时，为E2ERE训练和定制的传统模型（如管道模型）优于更适合零样本设置的生成式LM方法（如GPT）。精心设计的管道模型能以更低的成本和碳足迹提供显著的性能提升。需要更具创新性的方法来结合小型编码器-解码器管道模型和大型GPT模型的优点。", "translation": "端到端关系抽取（E2ERE）是生物医学中自然语言处理（NLP）的重要且实际应用。在本文中，我们旨在比较三种主流的E2ERE范式，使用一个专注于罕见病的复杂数据集，该数据集涉及不连续和嵌套实体。我们使用RareDis信息抽取数据集来评估三种竞争性方法（用于E2ERE）：NER -> RE管道模型、联合序列到序列模型以及生成式预训练Transformer（GPT）模型。我们对每种方法都使用了可比较的最新模型和最佳实践，并进行了错误分析以评估它们的失效模式。我们的研究结果表明，管道模型仍然是最佳选择，而序列到序列模型紧随其后；GPT模型（参数量是其八倍）甚至比序列到序列模型更差，并且比管道模型低超过10个F1分数点。部分匹配和不连续实体导致了许多NER错误，从而导致整体E2E性能下降。我们还在第二个用于化学-蛋白质相互作用的E2ERE数据集上验证了这些发现。尽管基于生成式LM的方法更适合零样本设置，但当有训练数据可用时，我们的结果表明，使用为E2ERE训练和定制的更传统模型效果更好。需要更具创新性的方法来结合小型编码器-解码器管道模型和大型GPT模型的优点，以改进E2ERE。截至目前，我们认为精心设计的管道模型能以更低的成本和碳足迹为E2ERE提供显著的性能提升。我们的贡献也是首次对RareDis数据集进行E2ERE研究。", "summary": "本文使用罕见病数据集比较了管道模型、序列到序列模型和GPT模型在端到端关系抽取（E2ERE）中的表现。研究发现，管道模型仍是最佳选择，序列到序列模型紧随其后，而大型GPT模型尽管参数更多但性能显著更差。研究表明，在有训练数据的情况下，针对E2ERE定制的传统模型更受欢迎，并强调了需要结合不同模型优点的混合方法。", "keywords": "端到端关系抽取, 管道模型, 序列到序列模型, GPT模型, 罕见病", "comments": "本文对不同E2ERE范式在具有挑战性的数据集上进行了有价值的比较。其主要创新在于，证明了在有训练数据的情况下，更简单、更定制化的管道模型在特定NLP任务上仍能优于更大、更通用的生成模型（如GPT）。这在一定程度上挑战了“越大越好”的范式，并强调了任务特定优化和资源效率（成本、碳足迹）的重要性。对RareDis数据集的E2ERE研究也是一项显著贡献。"}}
{"id": "2507.19154", "title": "Big Data Energy Systems: A Survey of Practices and Associated Challenges", "authors": ["Lunodzo J. Mwinuka", "Massimo Cafaro", "Lucas Pereira", "Hugo Morais"], "categories": ["cs.DB", "cs.DC"], "primary_category": "Subjects:       Databases (cs.DB)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19154v1", "summary": "Energy systems generate vast amounts of data in extremely short time\nintervals, creating challenges for efficient data management. Traditional data\nmanagement methods often struggle with scalability and accessibility, limiting\ntheir usefulness. More advanced solutions, such as NoSQL databases and\ncloud-based platforms, have been adopted to address these issues. Still, even\nthese advanced solutions can encounter bottlenecks, which can impact the\nefficiency of data storage, retrieval, and analysis. This review paper explores\nthe research trends in big data management for energy systems, highlighting the\npractices, opportunities and challenges. Also, the data regulatory demands are\nhighlighted using chosen reference architectures. The review, in particular,\nexplores the limitations of current storage and data integration solutions and\nexamines how new technologies are applied to the energy sector. Novel insights\ninto emerging technologies, including data spaces, various data management\narchitectures, peer-to-peer data management, and blockchains, are provided,\nalong with practical recommendations for achieving enhanced data sharing and\nregulatory compliance.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19154v1", "cate": "cs.DB", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "大数据能源系统：实践与挑战综述", "tldr": "本综述探讨了能源系统大数据管理的挑战、实践、机遇和新兴技术，并提供了增强数据共享和监管合规的建议。", "motivation": "能源系统产生大量数据，传统数据管理方法难以应对可扩展性和可访问性问题。即使是先进解决方案也面临瓶颈，影响数据存储、检索和分析效率。因此，有必要对大数据管理在能源系统中的研究趋势、实践、机遇和挑战进行综述。", "method": "本文是一篇综述性论文，通过探索能源系统大数据管理的研究趋势，分析了实践、机遇和挑战。文章还利用选定的参考架构强调了数据监管需求，并特别探讨了当前存储和数据集成解决方案的局限性，审查了新技术在能源领域的应用。", "result": "本综述探讨了能源系统大数据管理的研究趋势，突出了实践、机遇和挑战。文章强调了数据监管需求，并揭示了当前存储和数据集成解决方案的局限性。同时，文章还展示了数据空间、各种数据管理架构、点对点数据管理和区块链等新兴技术如何应用于能源领域。", "conclusion": "本综述为数据空间、各种数据管理架构、点对点数据管理和区块链等新兴技术提供了新颖的见解，并为实现增强的数据共享和监管合规性提供了实用建议。", "translation": "能源系统在极短的时间间隔内产生大量数据，给高效数据管理带来了挑战。传统数据管理方法通常在可扩展性和可访问性方面存在困难，限制了它们的实用性。为了解决这些问题，已经采用了更先进的解决方案，例如NoSQL数据库和基于云计算的平台。然而，即使是这些先进的解决方案也可能遇到瓶颈，这会影响数据存储、检索和分析的效率。本综述论文探讨了能源系统大数据管理的研究趋势，重点介绍了实践、机遇和挑战。此外，还利用选定的参考架构强调了数据监管要求。本综述特别探讨了当前存储和数据集成解决方案的局限性，并研究了新技术如何应用于能源领域。文章提供了关于数据空间、各种数据管理架构、点对点数据管理和区块链等新兴技术的新颖见解，并为实现增强的数据共享和监管合规性提供了实用建议。", "summary": "本综述性论文深入探讨了能源系统大数据管理的现状、挑战与机遇。鉴于能源数据量的爆炸式增长及传统管理方法的局限性，文章审视了NoSQL数据库和云平台等先进方案的瓶颈，并详细分析了大数据管理在能源领域的实践、挑战和研究趋势。论文还特别关注了数据监管要求，并探讨了数据空间、点对点数据管理和区块链等新兴技术如何应用于能源部门，旨在为提升数据共享和监管合规性提供实用指导和新颖见解。", "keywords": "大数据, 能源系统, 数据管理, 综述, 区块链", "comments": "该论文为能源系统大数据管理领域提供了一个全面的概述，特别强调了从传统方法到新兴技术的演变，并指出了实际应用中的挑战。其创新之处在于对数据空间、点对点数据管理和区块链等前沿技术的探讨，以及对监管合规性的关注。这对于推动能源行业的数据化转型和提升数据利用效率具有重要意义。"}}
{"id": "2507.19062", "title": "From Continuous to Discrete: Cross-Domain Collaborative General Speech Enhancement via Hierarchical Language Models", "authors": ["Zhaoxi Mu", "Rilin Chen", "Andong Li", "Meng Yu", "Xinyu Yang", "Dong Yu"], "categories": ["cs.SD", "eess.AS"], "primary_category": "Subjects:       Sound (cs.SD)", "pdf_link": null, "comments": "Comments:      ACMMM 2025", "url": "http://arxiv.org/abs/2507.19062v1", "summary": "This paper introduces OmniGSE, a novel general speech enhancement (GSE)\nframework designed to mitigate the diverse distortions that speech signals\nencounter in real-world scenarios. These distortions include background noise,\nreverberation, bandwidth limitations, signal clipping, and network packet loss.\nExisting methods typically focus on optimizing for a single type of distortion,\noften struggling to effectively handle the simultaneous presence of multiple\ndistortions in complex scenarios. OmniGSE bridges this gap by integrating the\nstrengths of discriminative and generative approaches through a two-stage\narchitecture that enables cross-domain collaborative optimization. In the first\nstage, continuous features are enhanced using a lightweight channel-split\nNAC-RoFormer. In the second stage, discrete tokens are generated to reconstruct\nhigh-quality speech through language models. Specifically, we designed a\nhierarchical language model structure consisting of a RootLM and multiple\nBranchLMs. The RootLM models general acoustic features across codebook layers,\nwhile the BranchLMs explicitly capture the progressive relationships between\ndifferent codebook levels. Experimental results demonstrate that OmniGSE\nsurpasses existing models across multiple benchmarks, particularly excelling in\nscenarios involving compound distortions. These findings underscore the\nframework's potential for robust and versatile speech enhancement in real-world\napplications.", "comment": "ACMMM 2025", "pdf_url": "http://arxiv.org/pdf/2507.19062v1", "cate": "cs.SD", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "从连续到离散：基于分层语言模型的跨域协同通用语音增强", "tldr": "OmniGSE是一个通用的语音增强框架，通过两阶段架构和分层语言模型，有效处理多种语音失真，在复杂失真场景下表现优异。", "motivation": "现有语音增强方法通常只优化单一类型的失真，难以有效处理真实世界中同时存在的多种复杂失真（如背景噪声、混响、带宽限制、信号削波和网络丢包）。", "method": "论文提出了OmniGSE框架，采用两阶段架构实现跨域协同优化。第一阶段使用轻量级channel-split NAC-RoFormer增强连续特征；第二阶段通过分层语言模型（包括RootLM和多个BranchLMs）生成离散tokens以重建高质量语音。RootLM建模跨码本层的通用声学特征，BranchLMs捕捉不同码本级别间的渐进关系。", "result": "实验结果表明，OmniGSE在多个基准测试中超越现有模型，尤其在复合失真场景下表现出色。", "conclusion": "OmniGSE框架在真实世界应用中展现出强大且多功能的语音增强潜力。", "translation": "这篇论文介绍了OmniGSE，一个新颖的通用语音增强（GSE）框架，旨在减轻语音信号在现实世界中遇到的各种失真。这些失真包括背景噪声、混响、带宽限制、信号削波和网络数据包丢失。现有方法通常侧重于优化单一类型的失真，在复杂场景中难以有效处理多种失真同时存在的情况。OmniGSE通过两阶段架构整合判别式和生成式方法的优势，实现了跨域协同优化，从而弥补了这一空白。在第一阶段，使用轻量级channel-split NAC-RoFormer增强连续特征。在第二阶段，通过语言模型生成离散令牌以重建高质量语音。具体而言，我们设计了一个由RootLM和多个BranchLM组成的分层语言模型结构。RootLM在码本层之间建模通用声学特征，而BranchLM则明确捕捉不同码本级别之间的渐进关系。实验结果表明，OmniGSE在多个基准测试中超越了现有模型，尤其在涉及复合失真的场景中表现出色。这些发现强调了该框架在现实应用中实现鲁棒和多功能语音增强的潜力。", "summary": "OmniGSE是一个新颖的通用语音增强（GSE）框架，旨在解决真实世界中语音信号面临的多种复杂失真问题，如噪声、混响和带宽限制等。与现有单一失真优化方法不同，OmniGSE采用两阶段跨域协同优化架构，第一阶段利用NAC-RoFormer增强连续特征，第二阶段通过独特的分层语言模型（RootLM和BranchLMs）生成离散令牌以重建高质量语音。实验证明，OmniGSE在处理复合失真方面优于现有模型，展现了其在实际应用中的鲁棒性和多功能性。", "keywords": "通用语音增强, 分层语言模型, 跨域协同, 复合失真, OmniGSE", "comments": "该论文提出了一种创新的通用语音增强框架OmniGSE，通过结合判别式和生成式方法，并引入分层语言模型，有效解决了传统方法难以处理的多种复合语音失真问题。其从“连续到离散”的处理范式以及跨域协同优化的设计，是其主要创新点，有望推动语音增强技术在复杂现实环境中的应用。"}}
{"id": "2507.19379", "title": "A non-iterative domain decomposition time integrator for linear wave equations", "authors": ["Tim Buchholz", "Marlis Hochbruck"], "categories": ["math.NA", "cs.NA", "65M12, 35L20, 65M55 (Primary) 65N30, 35L05 (Secondary)"], "primary_category": "Subjects:       Numerical Analysis (math.NA)", "pdf_link": null, "comments": "Comments:      24 pages, 5 figures", "url": "http://arxiv.org/abs/2507.19379v1", "summary": "We propose and analyze a non-iterative domain decomposition integrator for\nthe linear acoustic wave equation. The core idea is to combine an implicit\nCrank-Nicolson step on spatial subdomains with a local prediction step at the\nsubdomain interfaces. This enables parallelization across space while advancing\nsequentially in time, without requiring iterations at each time step. The\nmethod is similar to the methods from Blum, Lisky and Rannacher (1992) or\nDawson and Dupont (1992), which have been designed for parabolic problems. Our\napproach adapts them to the case of the wave equation in a fully discrete\nsetting, using linear finite elements with mass lumping. Compared to explicit\nschemes, our method permits significantly larger time steps and retains high\naccuracy. We prove that the resulting method achieves second-order accuracy in\ntime and global convergence of order $\\mathcal{O}(h + \\tau^2)$ under a CFL-type\ncondition, which depends on the overlap width between subdomains. We conclude\nwith numerical experiments which confirm the theoretical", "comment": "24 pages, 5 figures", "pdf_url": "http://arxiv.org/pdf/2507.19379v1", "cate": "math.NA", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "线性波动方程的非迭代域分解时间积分器", "tldr": "本文提出并分析了一种用于线性声波方程的非迭代域分解积分器，它结合了隐式Crank-Nicolson步和局部预测步，实现了空间并行化，无需迭代即可进行时间推进，并证明了其高精度和收敛性。", "motivation": "与显式方案相比，该方法允许显著更大的时间步长并保持高精度。", "method": "本文提出了一种非迭代域分解积分器，用于线性声波方程。核心思想是将空间子域上的隐式Crank-Nicolson步与子域界面处的局部预测步相结合。该方法在完全离散设置下，使用带质量集总的线性有限元，将Blum、Lisky和Rannacher (1992) 或 Dawson 和 Dupont (1992) 为抛物线问题设计的方法应用于波动方程。", "result": "该方法在时间上达到了二阶精度，并在CFL型条件下实现了$\\\\mathcal{O}(h + \\\\tau^2)$的全局收敛，该条件取决于子域之间的重叠宽度。数值实验证实了理论结果。", "conclusion": "所提出的非迭代域分解时间积分器对于线性波动方程有效且高效，能够允许更大的时间步长并保持高精度，其理论分析和数值实验结果相互印证。", "translation": "我们提出并分析了一种用于线性声波方程的非迭代域分解积分器。核心思想是将空间子域上的隐式Crank-Nicolson步与子域界面处的局部预测步相结合。这使得在时间上顺序推进时可以在空间上并行化，而无需在每个时间步进行迭代。该方法类似于Blum、Lisky和Rannacher (1992) 或 Dawson 和 Dupont (1992) 为抛物线问题设计的方法。我们的方法在完全离散设置下，使用带质量集总的线性有限元，将其应用于波动方程的情况。与显式方案相比，我们的方法允许显著更大的时间步长并保持高精度。我们证明了所得方法在时间上实现了二阶精度，并在取决于子域之间重叠宽度的CFL型条件下实现了\\\\(\\\\mathcal{O}(h + \\\\tau^2)\\\\)的全局收敛。最后，我们通过数值实验证实了理论结果。", "summary": "本文提出了一种用于线性声波方程的非迭代域分解时间积分器。该方法通过结合空间子域上的隐式Crank-Nicolson步和子域界面处的局部预测步，实现了空间并行化并在时间上顺序推进，无需迭代。该方法是现有为抛物线问题设计方法的适应和扩展，适用于波动方程，并使用线性有限元和质量集总。研究证明该方法具有高精度和全局收敛性，并允许比显式方案更大的时间步长。数值实验验证了其理论性能。", "keywords": "域分解, 时间积分器, 线性波动方程, 非迭代, 并行计算", "comments": "该论文的创新点在于将为抛物线问题设计的域分解方法成功地应用于波动方程，并实现了非迭代的时间推进，这对于大规模并行计算具有重要意义。其允许更大时间步长同时保持高精度的特性，提升了计算效率。对收敛性和精度的理论证明以及数值验证增加了其可靠性。"}}
{"id": "2507.18848", "title": "PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole Slide Image Analysis", "authors": ["Beidi Zhao", "SangMook Kim", "Hao Chen", "Chen Zhou", "Zu-hua Gao", "Gang Wang", "Xiaoxiao Li"], "categories": ["cs.CV", "cs.AI"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18848v1", "summary": "Multiple Instance Learning (MIL) has advanced WSI analysis but struggles with\nthe complexity and heterogeneity of WSIs. Existing MIL methods face challenges\nin aggregating diverse patch information into robust WSI representations. While\nViTs and clustering-based approaches show promise, they are computationally\nintensive and fail to capture task-specific and slide-specific variability. To\naddress these limitations, we propose PTCMIL, a novel Prompt Token\nClustering-based ViT for MIL aggregation. By introducing learnable prompt\ntokens into the ViT backbone, PTCMIL unifies clustering and prediction tasks in\nan end-to-end manner. It dynamically aligns clustering with downstream tasks,\nusing projection-based clustering tailored to each WSI, reducing complexity\nwhile preserving patch heterogeneity. Through token merging and prototype-based\npooling, PTCMIL efficiently captures task-relevant patterns. Extensive\nexperiments on eight datasets demonstrate its superior performance in\nclassification and survival analysis tasks, outperforming state-of-the-art\nmethods. Systematic ablation studies confirm its robustness and strong\ninterpretability. The code is released at https://github.com/ubc-tea/PTCMIL.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18848v1", "cate": "cs.CV", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "PTCMIL：基于提示符令牌聚类的多实例学习用于全玻片图像分析", "tldr": "PTCMIL是一种新颖的基于提示符令牌聚类的ViT，用于多实例学习（MIL）聚合，通过统一聚类和预测任务，动态对齐下游任务，并在全玻片图像分析中实现卓越性能。", "motivation": "现有的多实例学习（MIL）方法在聚合多样化的补丁信息以形成鲁棒的全玻片图像（WSI）表示方面面临挑战，且计算密集，未能捕获任务特定和玻片特定的变异性。", "method": "本文提出了PTCMIL，一种基于提示符令牌聚类的ViT，用于MIL聚合。PTCMIL通过在ViT骨干中引入可学习的提示符令牌，以端到端的方式统一聚类和预测任务。它利用为每个WSI量身定制的基于投影的聚类，动态地将聚类与下游任务对齐，在降低复杂性的同时保留了补丁异质性。通过令牌合并和基于原型的池化，PTCMIL有效地捕获了任务相关的模式。", "result": "在八个数据集上的大量实验证明，PTCMIL在分类和生存分析任务中表现出卓越的性能，优于现有最先进的方法。系统消融研究证实了其鲁棒性和强大的可解释性。", "conclusion": "PTCMIL通过其新颖的提示符令牌聚类方法，有效解决了全玻片图像分析中多实例学习的挑战，并在多个任务上取得了SOTA性能，展现了强大的实用性和可解释性。", "translation": "多实例学习（MIL）推动了全玻片图像（WSI）分析的进步，但其在处理WSI的复杂性和异质性方面仍面临挑战。现有的MIL方法在将多样化的补丁信息聚合成鲁棒的WSI表示方面遇到困难。虽然ViT和基于聚类的方法前景广阔，但它们计算密集，未能捕获任务特定和玻片特定的变异性。为了解决这些限制，我们提出了PTCMIL，一种新颖的基于提示符令牌聚类的ViT，用于MIL聚合。通过在ViT骨干中引入可学习的提示符令牌，PTCMIL以端到端的方式统一了聚类和预测任务。它利用为每个WSI量身定制的基于投影的聚类，动态地将聚类与下游任务对齐，在降低复杂性的同时保留了补丁异质性。通过令牌合并和基于原型的池化，PTCMIL有效地捕获了任务相关的模式。在八个数据集上的大量实验证明，它在分类和生存分析任务中表现出卓越的性能，优于现有最先进的方法。系统消融研究证实了其鲁棒性和强大的可解释性。代码已在https://github.com/ubc-tea/PTCMIL发布。", "summary": "本论文提出了PTCMIL，一种新颖的基于提示符令牌聚类的ViT，旨在解决全玻片图像（WSI）分析中多实例学习（MIL）的挑战。PTCMIL通过引入可学习的提示符令牌，将聚类与预测任务进行端到端统一，并利用为每个WSI量身定制的基于投影的聚类来处理补丁异质性。通过令牌合并和原型池化，该方法高效地捕获任务相关模式。实验结果表明，PTCMIL在分类和生存分析任务中均优于现有最先进方法，并具有良好的鲁棒性和可解释性。", "keywords": "多实例学习, 全玻片图像分析, 提示符令牌, 聚类, ViT", "comments": "PTCMIL的创新之处在于其将可学习的提示符令牌引入ViT骨干，并以端到端的方式统一聚类和预测任务，这有效地解决了现有MIL方法在处理WSI复杂性和异质性方面的局限性。其提出的基于投影的聚类和动态对齐下游任务的机制，在降低计算复杂性的同时保持了信息多样性，具有重要的实用价值。此外，其在多个数据集上的卓越性能和强大的可解释性也突出了该方法的潜力。"}}
{"id": "2507.18774", "title": "Bridging Cloud Convenience and Protocol Transparency: A Hybrid Architecture for Ethereum Node Operations on Amazon Managed Blockchain", "authors": ["S M Mostaq Hossain", "Amani Altarawneh", "Maanak Gupta"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "Comments:      11 pages, 5 figures, 6 tables. Conference name is 2025 IEEE International Conference on Service-Oriented System Engineering (SOSE)", "url": "http://arxiv.org/abs/2507.18774v1", "summary": "As blockchain technologies are increasingly adopted in enterprise and\nresearch domains, the need for secure, scalable, and performance-transparent\nnode infrastructure has become critical. While self-hosted Ethereum nodes offer\noperational control, they often lack elasticity and require complex\nmaintenance. This paper presents a hybrid, service-oriented architecture for\ndeploying and monitoring Ethereum full nodes using Amazon Managed Blockchain\n(AMB), integrated with EC2-based observability, IAM-enforced security policies,\nand reproducible automation via the AWS Cloud Development Kit. Our architecture\nsupports end-to-end observability through custom EC2 scripts leveraging Web3.py\nand JSON-RPC, collecting over 1,000 real-time data points-including gas\nutilization, transaction inclusion latency, and mempool dynamics. These metrics\nare visualized and monitored through AWS CloudWatch, enabling service-level\nperformance tracking and anomaly detection. This cloud-native framework\nrestores low-level observability lost in managed environments while maintaining\nthe operational simplicity of managed services. By bridging the simplicity of\nAMB with the transparency required for protocol research and enterprise\nmonitoring, this work delivers one of the first reproducible,\nperformance-instrumented Ethereum deployments on AMB. The proposed hybrid\narchitecture enables secure, observable, and reproducible Ethereum node\noperations in cloud environments, suitable for both research and production\nuse.", "comment": "11 pages, 5 figures, 6 tables. Conference name is 2025 IEEE\n  International Conference on Service-Oriented System Engineering (SOSE)", "pdf_url": "http://arxiv.org/pdf/2507.18774v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "弥合云便利性与协议透明度：亚马逊托管区块链上以太坊节点操作的混合架构", "tldr": "该论文提出了一种混合架构，用于在亚马逊托管区块链上实现可观测、可复制且安全的以太坊节点操作，同时兼顾云服务的便利性和协议级别的透明度。", "motivation": "随着区块链技术在企业和研究领域的日益普及，对安全、可扩展、性能透明的节点基础设施的需求变得至关重要。尽管自托管以太坊节点提供了操作控制，但它们通常缺乏弹性且需要复杂的维护，这促使了对一种既能提供云服务便利性又能保持协议透明度的解决方案的需求。", "method": "本文提出了一种混合的、面向服务的架构，用于使用亚马逊托管区块链（AMB）部署和监控以太坊全节点，并集成了基于EC2的可观测性、IAM强制的安全策略以及通过AWS云开发工具包实现的可复制自动化。该架构通过利用Web3.py和JSON-RPC的自定义EC2脚本支持端到端可观测性，收集实时数据点，并通过AWS CloudWatch进行可视化和监控。", "result": "该架构支持端到端可观测性，能够收集超过1,000个实时数据点，包括Gas利用率、交易包含延迟和内存池动态。这些指标通过AWS CloudWatch进行可视化和监控，实现了服务级性能跟踪和异常检测。该云原生框架恢复了在托管环境中丢失的低级可观测性，同时保持了托管服务的操作简便性。它提供了首批可复制、性能可测量的AMB上以太坊部署之一。", "conclusion": "所提出的混合架构通过弥合AMB的简便性与协议研究和企业监控所需的透明度，在云环境中实现了安全、可观测和可复制的以太坊节点操作，适用于研究和生产用途。", "translation": "随着区块链技术在企业和研究领域的日益普及，对安全、可扩展、性能透明的节点基础设施的需求变得至关重要。虽然自托管以太坊节点提供了操作控制，但它们往往缺乏弹性且需要复杂的维护。本文提出了一种混合的、面向服务的架构，用于使用亚马逊托管区块链（AMB）部署和监控以太坊全节点，并集成了基于EC2的可观测性、IAM强制的安全策略以及通过AWS云开发工具包实现的可复制自动化。我们的架构通过利用Web3.py和JSON-RPC的自定义EC2脚本支持端到端可观测性，收集超过1,000个实时数据点——包括Gas利用率、交易包含延迟和内存池动态。这些指标通过AWS CloudWatch进行可视化和监控，实现了服务级性能跟踪和异常检测。这种云原生框架恢复了在托管环境中丢失的低级可观测性，同时保持了托管服务的操作简便性。通过弥合AMB的简便性与协议研究和企业监控所需的透明度，这项工作提供了首批可复制、性能可测量的AMB上以太坊部署之一。所提出的混合架构能够在云环境中实现安全、可观测和可复制的以太坊节点操作，适用于研究和生产用途。", "summary": "本文介绍了一种创新的混合架构，旨在解决在亚马逊托管区块链（AMB）上操作以太坊节点时，在云服务便利性与协议级透明度之间取得平衡的挑战。该架构结合了AMB的托管优势与基于EC2的深度可观测性、IAM安全策略和AWS CDK自动化，以实现对以太坊全节点的高效部署和监控。通过收集并可视化超过1000个实时性能指标，该方案成功恢复了在托管环境中通常缺失的低级数据洞察力，同时保持了操作的简便性。这项工作为企业和研究机构提供了一个安全、可观测、可复制的以太坊节点操作框架，极大地提升了云端区块链基础设施的实用性和可靠性。", "keywords": "以太坊, 亚马逊托管区块链, 混合架构, 可观测性, 云原生", "comments": "该论文的创新之处在于成功地弥合了云托管服务（如AMB）所提供的操作简便性与以太坊协议研究和企业监控所需的底层透明度之间的鸿沟。通过集成自定义的EC2脚本和AWS原生工具，它解决了托管区块链服务中常见的“黑箱”问题，为用户提供了前所未有的可见性和控制力。其可复制的特性和对关键性能指标的精细监控，对于需要严格性能跟踪和异常检测的生产环境和深入研究场景都具有重要意义。这代表了在云环境中部署和管理区块链节点的一个重要进步。"}}
{"id": "2507.19229", "title": "TrinityDNA: A Bio-Inspired Foundational Model for Efficient Long-Sequence DNA Modeling", "authors": ["Qirong Yang", "Yucheng Guo", "Zicheng Liu", "Yujie Yang", "Qijin Yin", "Siyuan Li", "Shaomin Ji", "Linlin Chao", "Xiaoming Zhang", "Stan Z. Li"], "categories": ["cs.CE", "q-bio.GN"], "primary_category": "Subjects:       Computational Engineering, Finance, and Science (cs.CE)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19229v1", "summary": "The modeling of genomic sequences presents unique challenges due to their\nlength and structural complexity. Traditional sequence models struggle to\ncapture long-range dependencies and biological features inherent in DNA. In\nthis work, we propose TrinityDNA, a novel DNA foundational model designed to\naddress these challenges. The model integrates biologically informed\ncomponents, including Groove Fusion for capturing DNA's structural features and\nGated Reverse Complement (GRC) to handle the inherent symmetry of DNA\nsequences. Additionally, we introduce a multi-scale attention mechanism that\nallows the model to attend to varying levels of sequence dependencies, and an\nevolutionary training strategy that progressively adapts the model to both\nprokaryotic and eukaryotic genomes. TrinityDNA provides a more accurate and\nefficient approach to genomic sequence modeling, offering significant\nimprovements in gene function prediction, regulatory mechanism discovery, and\nother genomics applications. Our model bridges the gap between machine learning\ntechniques and biological insights, paving the way for more effective analysis\nof genomic data. Additionally, we introduced a new DNA long-sequence CDS\nannotation benchmark to make evaluations more comprehensive and oriented toward\npractical applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19229v1", "cate": "cs.CE", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "TrinityDNA：一种用于高效长序列DNA建模的生物启发基础模型", "tldr": "TrinityDNA是一个新的DNA基础模型，它通过整合生物学信息组件、多尺度注意力机制和进化训练策略来解决长序列DNA建模的挑战，提高了基因组序列建模的准确性和效率。", "motivation": "基因组序列建模因其长度和结构复杂性而面临独特挑战。传统序列模型难以捕获DNA固有的长程依赖性和生物学特征。", "method": "本文提出了TrinityDNA，一个新颖的DNA基础模型。该模型整合了生物学知情组件，包括用于捕获DNA结构特征的Groove Fusion和处理DNA序列固有对称性的Gated Reverse Complement (GRC)。此外，模型引入了多尺度注意力机制以处理不同级别的序列依赖性，以及一种逐步适应原核和真核基因组的进化训练策略。", "result": "TrinityDNA提供了一种更准确、更高效的基因组序列建模方法，在基因功能预测、调控机制发现和其他基因组学应用中取得了显著改进。", "conclusion": "TrinityDNA模型弥合了机器学习技术和生物学见解之间的鸿沟，为更有效的基因组数据分析铺平了道路。此外，还引入了一个新的DNA长序列CDS注释基准，以使评估更全面并面向实际应用。", "translation": "基因组序列建模由于其长度和结构复杂性而面临独特的挑战。传统的序列模型难以捕获DNA固有的长程依赖性和生物学特征。在这项工作中，我们提出了TrinityDNA，一个旨在解决这些问题的新型DNA基础模型。该模型整合了生物学知情组件，包括用于捕获DNA结构特征的Groove Fusion和用于处理DNA序列固有对称性的门控反向互补（GRC）。此外，我们引入了一种多尺度注意力机制，使模型能够关注不同级别的序列依赖性，以及一种逐步适应原核和真核基因组的进化训练策略。TrinityDNA为基因组序列建模提供了一种更准确和高效的方法，在基因功能预测、调控机制发现和其他基因组学应用中提供了显著改进。我们的模型弥合了机器学习技术和生物学见解之间的鸿沟，为更有效的基因组数据分析铺平了道路。此外，我们引入了一个新的DNA长序列CDS注释基准，以使评估更全面并面向实际应用。", "summary": "TrinityDNA是一个受生物学启发的DNA基础模型，旨在解决长序列基因组建模中的挑战。它通过整合Groove Fusion、Gated Reverse Complement (GRC)等生物学知情组件、多尺度注意力机制和进化训练策略，显著提高了基因组序列建模的准确性和效率，并在基因功能预测和调控机制发现等应用中表现出色。该模型还引入了新的DNA长序列CDS注释基准，促进了机器学习与生物学见解的结合，为基因组数据分析提供了更有效的方法。", "keywords": "DNA建模, 基因组序列, 基础模型, 生物启发, 长序列", "comments": "TrinityDNA的创新之处在于其生物学启发的设计，特别是Groove Fusion和Gated Reverse Complement (GRC)组件，这些组件直接解决了DNA的结构特征和对称性。多尺度注意力机制和进化训练策略进一步增强了其处理复杂基因组数据的能力。该模型的重要性在于其提高了基因组序列建模的准确性和效率，并为基因组学应用提供了新的工具。引入新的CDS注释基准也显示了其对实际应用和全面评估的关注。"}}
{"id": "2507.18845", "title": "A Truly Subcubic Combinatorial Algorithm for Induced 4-Cycle Detection", "authors": ["Amir Abboud", "Shyan Akmal", "Nick Fischer"], "categories": ["cs.DS", "cs.DM"], "primary_category": "Subjects:       Data Structures and Algorithms (cs.DS)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18845v1", "summary": "We present the first truly subcubic, combinatorial algorithm for detecting an\ninduced $4$-cycle in a graph. The running time is $O(n^{2.84})$ on $n$-node\ngraphs, thus separating the task of detecting induced $4$-cycles from detecting\ntriangles, which requires $n^{3-o(1)}$ time combinatorially under the popular\nBMM hypothesis.\n  Significant work has gone into characterizing the exact time complexity of\ninduced $H$-detection, relative to the complexity of detecting cliques of\nvarious sizes. Prior work identified the question of whether induced $4$-cycle\ndetection is triangle-hard as the only remaining case towards completing the\nlowest level of the classification, dubbing it a \"curious\" case [Dalirrooyfard,\nVassilevska W., FOCS 2022]. Our result can be seen as a negative resolution of\nthis question.\n  Our algorithm deviates from previous techniques in the large body of subgraph\ndetection algorithms and employs the trendy topic of graph decomposition that\nhas hitherto been restricted to more global problems (as in the use of expander\ndecompositions for flow problems) or to shaving subpolynomial factors (as in\nthe application of graph regularity lemmas). While our algorithm is slower than\nthe (non-combinatorial) state-of-the-art $\\tilde{O}(n^{\\omega})$-time algorithm\nbased on polynomial identity testing [Vassilevska W., Wang, Williams, Yu, SODA\n2014], combinatorial advancements often come with other benefits. In\nparticular, we give the first nontrivial deterministic algorithm for detecting\ninduced $4$-cycles.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18845v1", "cate": "cs.DS", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "一个真正的次立方组合算法用于检测诱导4-环", "tldr": "提出了第一个真正的次立方组合算法，用于在图中检测诱导4-环，运行时间为$O(n^{2.84})$，解决了诱导4-环检测是否是三角形难问题。", "motivation": "之前的研究已将诱导4-环检测是否是三角形难的问题确定为完成最低级别分类的唯一剩余情况，被称为“好奇”案例，本研究旨在解决此问题。", "method": "提出了一种新的组合算法，该算法不同于以往的子图检测技术，并采用了图分解这一迄今为止主要用于全局问题或削减次多项式因子的方法。", "result": "首次提出了一个真正的次立方组合算法，用于检测图中的诱导4-环，其运行时间为$O(n^{2.84})$。这使得诱导4-环检测任务与三角形检测任务分离。此外，该算法是第一个非平凡的确定性算法。", "conclusion": "本研究的结果可以看作是对诱导4-环检测是否是三角形难这一问题的否定性解决。", "translation": "我们提出了第一个真正的次立方组合算法，用于在图中检测诱导4-环。该算法在n个节点的图上的运行时间为$O(n^{2.84})$，从而将诱导4-环的检测任务与三角形检测任务分离，后者在流行的BMM假设下需要$n^{3-o(1)}$的组合时间。\n在诱导H-检测的精确时间复杂性方面，相对于不同大小团的检测复杂性，已经进行了大量工作。先前的工作已经确定，诱导4-环检测是否是三角形难的问题是完成分类最低级别的唯一剩余情况，称之为“好奇”案例[Dalirrooyfard, Vassilevska W., FOCS 2022]。我们的结果可以看作是对这个问题的否定性解决。\n我们的算法偏离了大量子图检测算法中先前的技术，并采用了图分解这一热门主题，该主题迄今为止仅限于更全局的问题（如在流问题中使用扩展图分解）或削减次多项式因子（如应用图正则引理）。尽管我们的算法比基于多项式恒等式检验的（非组合）最先进的$\\tilde{O}(n^{\\omega})$时间算法[Vassilevska W., Wang, Williams, Yu, SODA 2014]慢，但组合上的进步通常会带来其他好处。特别是，我们给出了第一个非平凡的确定性算法用于检测诱导4-环。", "summary": "本文提出了首个真正的次立方组合算法，用于在$n$节点图中检测诱导4-环，其运行时间为$O(n^{2.84})$。这一成果将诱导4-环检测与三角形检测任务区分开来，并对“诱导4-环检测是否是三角形难”这一长期存在的“好奇”问题给出了否定性解答。该算法创新性地采用了图分解技术，不同于传统子图检测方法，并且是首个非平凡的确定性算法，尽管其速度慢于非组合的最先进算法。", "keywords": "诱导4-环检测, 组合算法, 图分解, 次立方, 时间复杂度", "comments": "该论文的创新之处在于提出了首个真正的次立方组合算法，解决了长期悬而未决的“诱导4-环检测是否是三角形难”问题，并首次将图分解技术应用于此类子图检测问题。尽管其运行时间不如非组合算法，但作为组合算法的突破以及第一个非平凡的确定性算法，具有重要的理论价值和潜在的实际应用优势。"}}
{"id": "2507.19266", "title": "Overview of 3GPP Release 19 Study on Channel Modeling Enhancements to TR 38.901 for 6G", "authors": ["Hitesh Poddar", "Dimitri Gold", "Daewon Lee", "Nan Zhang", "Gokul Sridharan", "Henrik Asplund", "Mansoor Shaf"], "categories": ["cs.IT", "math.IT"], "primary_category": "Subjects:       Information Theory (cs.IT)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19266v1", "summary": "Channel models are a fundamental component of wireless communication systems,\nproviding critical insights into the physics of radio wave propagation. As\nwireless systems evolve every decade, the development of accurate and\nstandardized channel models becomes increasingly important for the development,\nevaluation and performance assessment of emerging technologies. An effort to\ndevelop a standardized channel model began around 2000 through the Third\nGeneration Partnership Project (3GPP) and the International Telecommunication\nUnion (ITU) with the aim of addressing a broad range of frequencies from sub-1\nGHz to 100 GHz. Prior efforts focused heavily on sub-6 GHz bands and mmWave\nbands, and there exist some gaps in accurately modeling the 7-24 GHz frequency\nrange, a promising candidate band for 6G. To address these gaps, 3GPP approved\na Release (Rel) 19 channel modeling study. This study resulted in several\nenhancements to the channel models, including the ability to accurately model a\nSuburban Macrocell (SMa) scenario, realistic User Terminal (UT) antenna models,\nvariability in the number of clusters, variability in the number of rays per\ncluster, a framework for capturing variability in power among all\npolarizations, near field (NF) propagation, and spatial non-stationarity (SNS)\neffects, all of which may be crucial for future 6G deployments. This paper\npresents the outcomes of this study and provides an overview of the underlying\nrationale, and key discussions that guided the validation, refinement, and\nenhancements of the 3GPP TR 38.901 channel models.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19266v1", "cate": "cs.IT", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "3GPP Release 19 关于 6G TR 38.901 信道建模增强研究概述", "tldr": "本文概述了 3GPP Release 19 针对 6G 关键频段（7-24 GHz）在 TR 38.901 信道模型方面的增强研究成果，旨在弥补现有模型在特定频段和先进传播效应方面的不足。", "motivation": "现有信道模型在准确建模 7-24 GHz 这一有前景的 6G 候选频段方面存在空白，而无线系统每十年演进一次，需要更准确和标准化的信道模型来支持新兴技术的发展、评估和性能分析。为了弥补这些空白，3GPP 批准了 Release 19 信道建模研究。", "method": "3GPP 批准了一项 Release 19 信道建模研究，该研究旨在对 TR 38.901 信道模型进行增强。本文介绍了该研究的成果，并概述了指导验证、完善和增强 3GPP TR 38.901 信道模型的基本原理和关键讨论。", "result": "该研究对信道模型进行了多项增强，包括：能够准确建模郊区宏蜂窝（SMa）场景、真实的终端（UT）天线模型、簇数量的可变性、每个簇中射线数量的可变性、捕获所有极化之间功率可变性的框架、近场（NF）传播以及空间非平稳性（SNS）效应。这些增强对未来的 6G 部署可能至关重要。", "conclusion": "本文展示了 3GPP Release 19 信道建模研究的成果，并阐述了指导 3GPP TR 38.901 信道模型验证、完善和增强的基本原理及关键讨论。这些增强对于未来的 6G 部署至关重要。", "translation": "信道模型是无线通信系统的基本组成部分，提供对无线电波传播物理特性的关键洞察。随着无线系统每十年演进一次，开发准确和标准化的信道模型对于新兴技术的发展、评估和性能评估变得越来越重要。一项旨在开发标准化信道模型的工作于 2000 年左右通过第三代合作伙伴计划（3GPP）和国际电信联盟（ITU）开始，旨在解决从低于 1 GHz 到 100 GHz 的广泛频率范围。之前的努力主要集中在 6 GHz 以下频段和毫米波频段，在准确建模 7-24 GHz 频率范围方面存在一些空白，而这是一个有前景的 6G 候选频段。为了弥补这些空白，3GPP 批准了一项 Release (Rel) 19 信道建模研究。这项研究促成了信道模型的几项增强，包括：能够准确建模郊区宏蜂窝（SMa）场景、真实的终端（UT）天线模型、簇数量的可变性、每个簇中射线数量的可变性、捕获所有极化之间功率可变性的框架、近场（NF）传播以及空间非平稳性（SNS）效应，所有这些对于未来的 6G 部署可能至关重要。本文介绍了这项研究的成果，并概述了指导 3GPP TR 38.901 信道模型验证、完善和增强的基本原理和关键讨论。", "summary": "本文概述了 3GPP Release 19 针对 6G 关键频段（7-24 GHz）的信道建模增强研究。鉴于现有模型在 7-24 GHz 频段的建模空白以及 6G 部署对更准确模型的需求，该研究对 TR 38.901 信道模型进行了多项改进，包括郊区宏蜂窝场景、UT 天线模型、簇和射线可变性、极化功率可变性、近场传播和空间非平稳性效应的建模能力，这些成果对未来的 6G 发展至关重要。", "keywords": "信道模型, 3GPP, 6G, TR 38.901, 7-24 GHz", "comments": "该论文的重要性在于其直接解决了 6G 发展中一个关键的技术挑战：信道建模的准确性，特别是在 7-24 GHz 这一新兴频段。通过对 3GPP TR 38.901 标准的增强，它为未来 6G 系统的设计、评估和部署提供了更坚实的基础，尤其是在考虑近场传播和空间非平稳性等先进效应方面，展现了前瞻性。"}}
{"id": "2507.18801", "title": "Resolving Indirect Calls in Binary Code via Cross-Reference Augmented Graph Neural Networks", "authors": ["Haotian Zhang", "Kun Liu", "Cristian Garces", "Chenke Luo", "Yu Lei", "Jiang Ming"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18801v1", "summary": "Binary code analysis is essential in scenarios where source code is\nunavailable, with extensive applications across various security domains.\nHowever, accurately resolving indirect call targets remains a longstanding\nchallenge in maintaining the integrity of static analysis in binary code. This\ndifficulty arises because the operand of a call instruction (e.g., call rax)\nremains unknown until runtime, resulting in an incomplete inter-procedural\ncontrol flow graph (CFG). Previous approaches have struggled with low accuracy\nand limited scalability. To address these limitations, recent work has\nincreasingly turned to machine learning (ML) to enhance analysis. However, this\nML-driven approach faces two significant obstacles: low-quality callsite-callee\ntraining pairs and inadequate binary code representation, both of which\nundermine the accuracy of ML models. In this paper, we introduce NeuCall, a\nnovel approach for resolving indirect calls using graph neural networks.\nExisting ML models in this area often overlook key elements such as data and\ncode cross-references, which are essential for understanding a program's\ncontrol flow. In contrast, NeuCall augments CFGs with cross-references,\npreserving rich semantic information. Additionally, we leverage advanced\ncompiler-level type analysis to generate high-quality callsite-callee training\npairs, enhancing model precision and reliability. We further design a graph\nneural model that leverages augmented CFGs and relational graph convolutions\nfor accurate target prediction. Evaluated against real-world binaries from\nGitHub and the Arch User Repository on x86_64 architecture, NeuCall achieves an\nF1 score of 95.2%, outperforming state-of-the-art ML-based approaches. These\nresults highlight NeuCall's effectiveness in building precise inter-procedural\nCFGs and its potential to advance downstream binary analysis and security\napplications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18801v1", "cate": "cs.CR", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "通过交叉引用增强图神经网络解析二进制代码中的间接调用", "tldr": "NeuCall是一种利用交叉引用增强图神经网络来准确解析二进制代码中间接调用的新方法，其F1分数达到95.2%，优于现有技术。", "motivation": "在缺乏源代码的情况下，二进制代码分析至关重要，但准确解析间接调用目标仍然是静态分析中的一个长期挑战，因为调用指令的操作数在运行时之前是未知的，导致不完整的程序间控制流图（CFG）。现有方法存在准确性低和可扩展性有限的问题。机器学习方法虽被引入，但面临训练对质量低和二进制代码表示不足的障碍。", "method": "本论文提出了NeuCall，一种利用图神经网络解析间接调用的新方法。NeuCall通过交叉引用（包括数据和代码交叉引用）增强控制流图（CFG），以保留丰富的语义信息。此外，它利用高级编译器级类型分析生成高质量的调用点-被调用方训练对。该方法还设计了一个图神经网络模型，利用增强的CFG和关系图卷积进行准确的目标预测。", "result": "NeuCall在x86_64架构上，针对来自GitHub和Arch用户仓库的真实二进制文件进行评估，取得了95.2%的F1分数，优于最先进的基于机器学习的方法。", "conclusion": "NeuCall在构建精确的程序间CFG方面表现出有效性，并具有推动下游二进制分析和安全应用的潜力。", "translation": "二进制代码分析在源代码不可用的情况下至关重要，在各种安全领域都有广泛应用。然而，准确解析间接调用目标仍然是维护二进制代码静态分析完整性的长期挑战。这种困难的出现是因为调用指令的操作数（例如，call rax）在运行时之前是未知的，导致程序间控制流图（CFG）不完整。以前的方法在准确性低和可扩展性有限方面存在困难。为了解决这些限制，最近的工作越来越多地转向机器学习（ML）来增强分析。然而，这种ML驱动的方法面临两个重大障碍：低质量的调用点-被调用方训练对和不充分的二进制代码表示，这两者都损害了ML模型的准确性。在本文中，我们介绍了NeuCall，一种使用图神经网络解析间接调用的新方法。该领域现有的ML模型通常忽略了数据和代码交叉引用等关键元素，而这些元素对于理解程序的控制流至关重要。相比之下，NeuCall通过交叉引用增强CFG，保留了丰富的语义信息。此外，我们利用高级编译器级类型分析生成高质量的调用点-被调用方训练对，提高了模型的精度和可靠性。我们进一步设计了一个图神经模型，利用增强的CFG和关系图卷积进行准确的目标预测。在x86_64架构上，针对来自GitHub和Arch用户仓库的真实二进制文件进行评估，NeuCall取得了95.2%的F1分数，优于最先进的基于ML的方法。这些结果突出了NeuCall在构建精确的程序间CFG方面的有效性及其推动下游二进制分析和安全应用的潜力。", "summary": "本论文提出了NeuCall，一种利用交叉引用增强图神经网络来解决二进制代码中间接调用解析挑战的新方法。针对现有机器学习方法面临的低质量训练数据和不充分代码表示问题，NeuCall通过将数据和代码交叉引用融入控制流图来丰富语义信息，并利用编译器级类型分析生成高质量训练对。该方法采用关系图卷积进行目标预测，并在真实世界的二进制文件上取得了95.2%的F1分数，优于现有技术，证明了其在构建精确程序间控制流图和推动二进制分析方面的潜力。", "keywords": "二进制代码分析, 间接调用解析, 图神经网络, 交叉引用, 控制流图", "comments": "NeuCall的创新之处在于其结合了交叉引用来增强CFG，并利用高级编译器级类型分析生成高质量的训练数据，有效克服了现有ML方法在二进制代码分析中的两大障碍。这种方法显著提高了间接调用解析的准确性，对于依赖于精确CFG的下游二进制分析和安全应用具有重要意义。"}}
{"id": "2507.19459", "title": "Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization", "authors": ["Pol Francesch Huc", "Emily Bates", "Simone D'Amico"], "categories": ["cs.CV", "cs.LG", "cs.RO"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19459v1", "summary": "The advent of novel view synthesis techniques such as NeRF and 3D Gaussian\nSplatting (3DGS) has enabled learning precise 3D models only from posed\nmonocular images. Although these methods are attractive, they hold two major\nlimitations that prevent their use in space applications: they require poses\nduring training, and have high computational cost at training and inference. To\naddress these limitations, this work contributes: (1) a Convolutional Neural\nNetwork (CNN) based primitive initializer for 3DGS using monocular images; (2)\na pipeline capable of training with noisy or implicit pose estimates; and (3)\nand analysis of initialization variants that reduce the training cost of\nprecise 3D models. A CNN takes a single image as input and outputs a coarse 3D\nmodel represented as an assembly of primitives, along with the target's pose\nrelative to the camera. This assembly of primitives is then used to initialize\n3DGS, significantly reducing the number of training iterations and input images\nneeded -- by at least an order of magnitude. For additional flexibility, the\nCNN component has multiple variants with different pose estimation techniques.\nThis work performs a comparison between these variants, evaluating their\neffectiveness for downstream 3DGS training under noisy or implicit pose\nestimates. The results demonstrate that even with imperfect pose supervision,\nthe pipeline is able to learn high-fidelity 3D representations, opening the\ndoor for the use of novel view synthesis in space applications.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19459v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "通过原始初始化快速学习非合作航天器3D模型", "tldr": "提出一种基于CNN的原始初始化方法，显著降低3DGS在嘈杂姿态估计下训练航天器3D模型的计算成本和数据需求，使其适用于空间应用。", "motivation": "现有的新视图合成技术（如NeRF和3D Gaussian Splatting）在空间应用中存在局限性：训练时需要姿态，且训练和推理的计算成本高昂。", "method": "本文提出了：(1) 一个基于CNN的3DGS原始初始化器，使用单目图像；(2) 一个能够在嘈杂或隐式姿态估计下进行训练的管道；(3) 对初始化变体进行分析，以降低精确3D模型的训练成本。该方法使用CNN从单张图像输出粗糙的3D原始模型和目标姿态，然后用其初始化3DGS，显著减少了训练迭代次数和所需输入图像。", "result": "结果表明，即使在不完美的姿态监督下，该管道也能够学习到高保真度的3D表示。", "conclusion": "本文提出的方法为新视图合成技术在空间应用中的使用铺平了道路。", "translation": "新型视图合成技术（如NeRF和3D Gaussian Splatting，3DGS）的出现使得仅从带姿态的单目图像学习精确3D模型成为可能。尽管这些方法很有吸引力，但它们存在两个主要限制，阻碍了它们在空间应用中的使用：它们在训练期间需要姿态，并且训练和推理的计算成本很高。为了解决这些限制，本工作贡献了：(1) 一个基于卷积神经网络（CNN）的3DGS原始初始化器，使用单目图像；(2) 一个能够在嘈杂或隐式姿态估计下进行训练的管道；(3) 以及对初始化变体的分析，这些变体可以降低精确3D模型的训练成本。一个CNN接收单张图像作为输入，并输出一个表示为原始体组合的粗糙3D模型，以及目标相对于相机的姿态。然后，这个原始体组合用于初始化3DGS，显著减少了所需的训练迭代次数和输入图像数量——至少一个数量级。为了增加灵活性，CNN组件有多种变体，采用不同的姿态估计技术。本工作对这些变体进行了比较，评估了它们在嘈杂或隐式姿态估计下对后续3DGS训练的有效性。结果表明，即使在不完美的姿态监督下，该管道也能够学习到高保真度的3D表示，为新视图合成技术在空间应用中的使用打开了大门。", "summary": "本文提出一种新方法，通过CNN生成的原始模型初始化3D Gaussian Splatting，以解决现有新视图合成技术在空间应用中对姿态的依赖和高计算成本问题。该方法能够在嘈杂或隐式姿态估计下训练高保真度航天器3D模型，显著减少了训练时间和数据需求，从而使这些技术适用于非合作航天器3D建模。", "keywords": "3D模型, 航天器, 新视图合成, 姿态估计, 原始初始化", "comments": "这项工作通过引入原始初始化和处理不完美姿态估计的能力，显著提升了新视图合成技术在实际空间应用中的可行性。其创新点在于将CNN与3DGS结合，解决了训练效率和姿态依赖的关键限制，为航天器3D建模提供了新的高效解决方案。"}}
{"id": "2507.18949", "title": "Adaptive Learning Systems: Personalized Curriculum Design Using LLM-Powered Analytics", "authors": ["Yongjie Li", "Ruilin Nong", "Jianan Liu", "Lucas Evans"], "categories": ["cs.CY", "cs.CL"], "primary_category": "Subjects:       Computers and Society (cs.CY)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18949v1", "summary": "Large language models (LLMs) are revolutionizing the field of education by\nenabling personalized learning experiences tailored to individual student\nneeds. In this paper, we introduce a framework for Adaptive Learning Systems\nthat leverages LLM-powered analytics for personalized curriculum design. This\ninnovative approach uses advanced machine learning to analyze real-time data,\nallowing the system to adapt learning pathways and recommend resources that\nalign with each learner's progress. By continuously assessing students, our\nframework enhances instructional strategies, ensuring that the materials\npresented are relevant and engaging. Experimental results indicate a marked\nimprovement in both learner engagement and knowledge retention when using a\ncustomized curriculum. Evaluations conducted across varied educational\nenvironments demonstrate the framework's flexibility and positive influence on\nlearning outcomes, potentially reshaping conventional educational practices\ninto a more adaptive and student-centered model.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18949v1", "cate": "cs.CY", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "自适应学习系统：利用大型语言模型驱动的分析进行个性化课程设计", "tldr": "本文介绍了一个利用大型语言模型（LLM）驱动的分析来实现个性化课程设计的自适应学习系统框架，实验结果表明该系统能显著提高学习者参与度和知识保留率。", "motivation": "大型语言模型（LLMs）正在通过实现根据个体学生需求量身定制的个性化学习体验，彻底改变教育领域。本研究的动机是引入一个利用LLM驱动的分析进行个性化课程设计的自适应学习系统框架。", "method": "本文引入了一个自适应学习系统框架，该框架利用LLM驱动的分析进行个性化课程设计。这种创新方法使用先进的机器学习来分析实时数据，使系统能够调整学习路径并推荐符合每个学习者进度的资源。通过持续评估学生，该框架增强了教学策略，确保所呈现的材料具有相关性和吸引力。", "result": "实验结果表明，使用定制课程时，学习者的参与度和知识保留率均有显著提高。在不同教育环境中进行的评估证明了该框架的灵活性以及对学习成果的积极影响。", "conclusion": "该自适应学习系统框架能够显著提高学习者的参与度和知识保留率，并有望将传统的教育实践转变为更具适应性和以学生为中心的模型。", "translation": "大型语言模型（LLMs）正通过实现根据个体学生需求量身定制的个性化学习体验，彻底改变教育领域。在本文中，我们介绍了一个自适应学习系统框架，该框架利用大型语言模型驱动的分析进行个性化课程设计。这种创新方法使用先进的机器学习来分析实时数据，使系统能够调整学习路径并推荐符合每个学习者进度的资源。通过持续评估学生，我们的框架增强了教学策略，确保所呈现的材料具有相关性和吸引力。实验结果表明，使用定制课程时，学习者的参与度和知识保留率均有显著提高。在不同教育环境中进行的评估证明了该框架的灵活性以及对学习成果的积极影响，有望将传统的教育实践转变为更具适应性和以学生为中心的模型。", "summary": "本文提出了一个基于大型语言模型（LLM）驱动分析的自适应学习系统框架，旨在实现个性化课程设计。该系统利用机器学习分析实时数据，动态调整学习路径和资源推荐，以适应学生的学习进度。实验结果显示，该框架能显著提升学习者的参与度和知识保留率，并在不同教育环境中表现出良好的适应性，预示着教育模式向个性化和学生中心化转变的潜力。", "keywords": "自适应学习系统, 大型语言模型, 个性化课程设计, 机器学习, 教育技术", "comments": "该论文的创新点在于将LLM驱动的分析应用于自适应学习系统，以实现个性化课程设计。其重要性在于提供了一种可能彻底改变传统教育实践的方法，使其更具适应性和以学生为中心。通过实时数据分析和持续评估，该系统有望显著提高学习效果。论文强调了实验结果的积极性，但未深入探讨具体的算法细节或可能面临的挑战（如数据隐私、模型偏见等）。"}}
{"id": "2312.16903", "title": "Spike No More: Stabilizing the Pre-training of Large Language Models", "authors": ["Sho Takase", "Shun Kiyono", "Sosuke Kobayashi", "Jun Suzuki"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      COLM 2025", "url": "http://arxiv.org/abs/2312.16903v4", "summary": "Loss spikes often occur during pre-training of large language models. The\nspikes degrade the performance of large language models and sometimes ruin the\npre-training. Since the pre-training needs a vast computational budget, we\nshould avoid such spikes. Based on the assumption that the loss spike is caused\nby the sudden growth of the gradient norm, we explore factors to keep the\ngradient norm small through an analysis of the spectral norms of the Jacobian\nmatrices for the sub-layers. Our findings suggest that stabilizing the\npre-training process requires two conditions: small sub-layers and large\nshortcut. We conduct various experiments to empirically verify our theoretical\nanalyses. Experimental results demonstrate that methods satisfying the\nconditions effectively prevent loss spikes during pre-training.", "comment": "COLM 2025", "pdf_url": "http://arxiv.org/pdf/2312.16903v4", "cate": "cs.CL", "date": "2023-12-28", "updated": "2025-07-25", "AI": {"title_translation": "不再出现尖峰：稳定大型语言模型的预训练", "tldr": "本文研究了大型语言模型预训练过程中出现的损失尖峰问题，发现通过减小子层和增大快捷连接可以有效防止损失尖峰，从而稳定预训练过程。", "motivation": "大型语言模型预训练中出现的损失尖峰会降低模型性能，甚至破坏预训练过程，并且预训练需要巨大的计算资源，因此需要避免此类尖峰。", "method": "本文基于损失尖峰是由梯度范数突然增长引起的假设，通过分析子层Jacobian矩阵的谱范数，探索保持梯度范数较小的因素。并通过各种实验验证了理论分析。", "result": "研究发现，稳定预训练过程需要两个条件：小的子层和大的快捷连接。实验结果表明，满足这些条件的方法能有效防止预训练期间的损失尖峰。", "conclusion": "通过减小子层和增大快捷连接，可以有效防止大型语言模型预训练中的损失尖峰，从而稳定预训练过程。", "translation": "大型语言模型预训练过程中经常出现损失尖峰。这些尖峰会降低大型语言模型的性能，有时甚至会毁掉预训练。由于预训练需要巨大的计算预算，我们应该避免此类尖峰。基于损失尖峰是由梯度范数突然增长引起的假设，我们通过分析子层Jacobian矩阵的谱范数，探索了保持梯度范数较小的因素。我们的研究结果表明，稳定预训练过程需要两个条件：小的子层和大的快捷连接。我们进行了各种实验来实证验证我们的理论分析。实验结果表明，满足这些条件的方法能有效防止预训练期间的损失尖峰。", "summary": "本文研究了大型语言模型预训练中常见的损失尖峰问题，该问题会损害模型性能并浪费计算资源。研究假设损失尖峰源于梯度范数的突然增长，并通过分析子层Jacobian矩阵的谱范数，发现减小子层和增大快捷连接是稳定预训练的关键条件。实验验证了这些方法能有效阻止损失尖峰的发生。", "keywords": "损失尖峰, 大型语言模型, 预训练, 梯度范数, 稳定性", "comments": "本文针对大型语言模型预训练中的关键稳定性问题——损失尖峰，提出了理论分析和实证解决方案。其创新点在于将损失尖峰归因于梯度范数的增长，并通过对网络结构（子层大小和快捷连接）的分析给出具体的改进建议。这对于优化LLM的训练效率和稳定性具有重要意义。"}}
{"id": "2504.21019", "title": "Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations", "authors": ["Yinghan Zhou", "Juan Wen", "Wanli Peng", "Yiming Xue", "Ziwei Zhang", "Zhengxian Wu"], "categories": ["cs.CL", "cs.AI"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "Comments:      Accepted by NAACL 2025 main conference", "url": "http://arxiv.org/abs/2504.21019v2", "summary": "The growing popularity of large language models has raised concerns regarding\nthe potential to misuse AI-generated text (AIGT). It becomes increasingly\ncritical to establish an excellent AIGT detection method with high\ngeneralization and robustness. However, existing methods either focus on model\ngeneralization or concentrate on robustness. The unified mechanism, to\nsimultaneously address the challenges of generalization and robustness, is less\nexplored. In this paper, we argue that robustness can be view as a specific\nform of domain shift, and empirically reveal an intrinsic mechanism for model\ngeneralization of AIGT detection task. Then, we proposed a novel AIGT detection\nmethod (DP-Net) via dynamic perturbations introduced by a reinforcement\nlearning with elaborated reward and action. Experimentally, extensive results\nshow that the proposed DP-Net significantly outperforms some state-of-the-art\nAIGT detection methods for generalization capacity in three cross-domain\nscenarios. Meanwhile, the DP-Net achieves best robustness under two text\nadversarial attacks. The code is publicly available at\nhttps://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.", "comment": "Accepted by NAACL 2025 main conference", "pdf_url": "http://arxiv.org/pdf/2504.21019v2", "cate": "cs.CL", "date": "2025-04-22", "updated": "2025-07-25", "AI": {"title_translation": "一石二鸟：通过动态扰动实现通用且鲁棒的AI生成文本检测", "tldr": "DP-Net利用强化学习引入动态扰动，实现了对AI生成文本的通用且鲁棒的检测，在泛化性和鲁棒性方面均优于现有方法。", "motivation": "大型语言模型的普及引发了AI生成文本滥用的担忧，因此迫切需要开发具有高泛化性和鲁棒性的AIGT检测方法。现有方法往往只关注泛化性或鲁棒性，缺乏同时解决这两个挑战的统一机制。", "method": "本文提出鲁棒性可视为一种特定形式的领域偏移，并凭经验揭示了AIGT检测任务中模型泛化性的内在机制。在此基础上，提出了一种新颖的AIGT检测方法DP-Net，该方法通过强化学习引入动态扰动，并设计了精细的奖励和动作机制。", "result": "实验结果表明，所提出的DP-Net在三种跨领域场景下，其泛化能力显著优于一些最先进的AIGT检测方法。同时，DP-Net在两种文本对抗性攻击下也实现了最佳的鲁棒性。", "conclusion": "DP-Net通过动态扰动和强化学习，成功地同时解决了AI生成文本检测的泛化性和鲁棒性挑战，提供了一种有效且性能优越的检测方法。", "translation": "大型语言模型的日益普及引发了对滥用AI生成文本（AIGT）的担忧。建立一种具有高泛化性和鲁棒性的优秀AIGT检测方法变得越来越关键。然而，现有方法要么侧重于模型泛化，要么侧重于鲁棒性。同时解决泛化性和鲁棒性挑战的统一机制探索较少。在本文中，我们认为鲁棒性可以看作是领域偏移的一种特定形式，并凭经验揭示了AIGT检测任务中模型泛化性的内在机制。然后，我们提出了一种新颖的AIGT检测方法（DP-Net），该方法通过强化学习引入动态扰动，并设计了精细的奖励和动作。实验结果广泛表明，所提出的DP-Net在三种跨领域场景下，其泛化能力显著优于一些最先进的AIGT检测方法。同时，DP-Net在两种文本对抗性攻击下也实现了最佳的鲁棒性。代码已公开在 https://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net。", "summary": "本文针对AI生成文本滥用问题，提出了一种名为DP-Net的新型检测方法，旨在同时提升检测的泛化性和鲁棒性。通过将鲁棒性视为领域偏移，并利用强化学习引入动态扰动，DP-Net在跨领域泛化和对抗性攻击鲁棒性方面均显著优于现有SOTA方法，有效解决了AI生成文本检测领域的关键挑战。", "keywords": "AI生成文本检测, 泛化性, 鲁棒性, 动态扰动, 强化学习", "comments": "这篇论文的创新点在于提出了“鲁棒性是领域偏移的一种特殊形式”的观点，并通过强化学习引入动态扰动来同时解决AI生成文本检测的泛化性和鲁棒性问题。这种统一的方法在现有研究中较少探索，具有重要的理论和实践意义。其“一石二鸟”的理念有效地提升了检测系统的实用性。"}}
{"id": "2507.18983", "title": "KASPER: Kolmogorov Arnold Networks for Stock Prediction and Explainable Regimes", "authors": ["Vidhi Oad", "Param Pathak", "Nouhaila Innan", "Shalini D", "Muhammad Shafique"], "categories": ["cs.LG"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      11 pages, 7 figures, 3 tables", "url": "http://arxiv.org/abs/2507.18983v1", "summary": "Forecasting in financial markets remains a significant challenge due to their\nnonlinear and regime-dependent dynamics. Traditional deep learning models, such\nas long short-term memory networks and multilayer perceptrons, often struggle\nto generalize across shifting market conditions, highlighting the need for a\nmore adaptive and interpretable approach. To address this, we introduce\nKolmogorov-Arnold networks for stock prediction and explainable regimes\n(KASPER), a novel framework that integrates regime detection, sparse\nspline-based function modeling, and symbolic rule extraction. The framework\nidentifies hidden market conditions using a Gumbel-Softmax-based mechanism,\nenabling regime-specific forecasting. For each regime, it employs\nKolmogorov-Arnold networks with sparse spline activations to capture intricate\nprice behaviors while maintaining robustness. Interpretability is achieved\nthrough symbolic learning based on Monte Carlo Shapley values, which extracts\nhuman-readable rules tailored to each regime. Applied to real-world financial\ntime series from Yahoo Finance, the model achieves an $R^2$ score of 0.89, a\nSharpe Ratio of 12.02, and a mean squared error as low as 0.0001, outperforming\nexisting methods. This research establishes a new direction for regime-aware,\ntransparent, and robust forecasting in financial markets.", "comment": "11 pages, 7 figures, 3 tables", "pdf_url": "http://arxiv.org/pdf/2507.18983v1", "cate": "cs.LG", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "KASPER：用于股票预测和可解释机制的科尔莫哥洛夫-阿诺德网络", "tldr": "KASPER框架利用科尔莫哥洛夫-阿诺德网络和机制检测，实现了对金融市场更准确和可解释的预测，并优于现有方法。", "motivation": "金融市场预测因其非线性和依赖机制的动态而具有挑战性。传统深度学习模型难以适应市场条件变化，需要更具适应性和可解释性的方法。", "method": "本文引入了KASPER框架，该框架整合了机制检测（使用Gumbel-Softmax机制识别隐藏市场条件）、稀疏样条函数建模（对每种机制采用具有稀疏样条激活的科尔莫哥洛夫-阿诺德网络），以及基于蒙特卡洛Shapley值的符号规则提取以实现可解释性。", "result": "在真实世界金融时间序列上，KASPER模型实现了0.89的$R^2$分数，12.02的夏普比率，以及低至0.0001的均方误差，表现优于现有方法。", "conclusion": "这项研究为金融市场中依赖机制、透明且鲁棒的预测开辟了新方向。", "translation": "金融市场预测因其非线性和依赖机制的动态而仍然是一个重大挑战。传统的深度学习模型，如长短期记忆网络和多层感知机，在不断变化的市场条件下往往难以泛化，这凸显了对更具适应性和可解释性方法的需求。为解决此问题，我们引入了用于股票预测和可解释机制的科尔莫哥洛夫-阿诺德网络（KASPER），这是一个新颖的框架，它集成了机制检测、基于稀疏样条的函数建模和符号规则提取。该框架使用基于Gumbel-Softmax的机制识别隐藏的市场条件，从而实现特定机制的预测。对于每种机制，它采用具有稀疏样条激活的科尔莫哥洛夫-阿诺德网络来捕获复杂的股价行为，同时保持鲁棒性。通过基于蒙特卡洛Shapley值的符号学习实现了可解释性，该学习提取了针对每种机制量身定制的人类可读规则。将该模型应用于来自Yahoo Finance的真实世界金融时间序列数据，该模型实现了0.89的$R^2$分数，12.02的夏普比率，以及低至0.0001的均方误差，表现优于现有方法。这项研究为金融市场中依赖机制、透明且鲁棒的预测开辟了新方向。", "summary": "本文提出了KASPER框架，一个结合了机制检测、科尔莫哥洛夫-阿诺德网络（KANs）和符号规则提取的新型金融市场预测模型。针对金融市场非线性和机制依赖的挑战，KASPER通过Gumbel-Softmax机制识别并适应不同的市场条件，使用稀疏样条激活的KANs捕捉复杂价格行为，并通过符号学习提供可解释的预测规则。在Yahoo Finance的真实数据上，KASPER在$R^2$、夏普比率和MSE等指标上均优于现有方法，为金融预测提供了一种更透明、鲁棒且机制感知的新方法。", "keywords": "金融预测, 科尔莫哥洛夫-阿诺德网络, 机制检测, 可解释人工智能, 股票预测", "comments": "KASPER的创新点在于将机制检测与科尔莫哥洛夫-阿诺德网络结合，并在金融预测领域实现了高可解释性。通过引入稀疏样条激活和符号规则提取，该模型不仅提高了预测精度，还解决了传统深度学习模型在金融市场中缺乏泛化能力和可解释性的痛点。其在真实数据上的优异表现证明了其潜力，为金融AI领域提供了新的研究方向。"}}
{"id": "2507.19032", "title": "How to Copy-Protect Malleable-Puncturable Cryptographic Functionalities Under Arbitrary Challenge Distributions", "authors": ["Alper Çakan", "Vipul Goyal"], "categories": ["cs.CR"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19032v1", "summary": "A quantum copy-protection scheme (Aaronson, CCC 2009) encodes a functionality\ninto a quantum state such that given this state, no efficient adversary can\ncreate two (possibly entangled) quantum states that are both capable of running\nthe functionality. There has been a recent line of works on constructing\nprovably-secure copy-protection schemes for general classes of schemes in the\nplain model, and most recently the recent work of \\c{C}akan and Goyal (IACR\nEprint, 2025) showed how to copy-protect all cryptographically puncturable\nschemes with pseudorandom puncturing points. In this work, we show how to\ncopy-protect even a larger class of schemes. We define a class of cryptographic\nschemes called malleable-puncturable schemes where the only requirement is that\none can create a circuit that is capable of answering inputs at points that are\nunrelated to the challenge in the security game but does not help the adversary\nanswer inputs related to the challenge. This is a flexible generalization of\npuncturable schemes, and can capture a wide range of primitives that was not\nknown how to copy-protect prior to our work. Going further, we show that our\nscheme is secure against arbitrary high min-entropy challenge distributions\nwhereas previous work has only considered schemes that are punctured at\npseudorandom points.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19032v1", "cate": "cs.CR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "如何在任意挑战分布下复制保护可延展可穿刺密码功能", "tldr": "本文提出了一种新的可延展可穿刺方案类别，并展示了如何在任意高最小熵挑战分布下对其进行复制保护，超越了现有工作。", "motivation": "现有量子复制保护方案仅限于在伪随机穿刺点下的可穿刺方案。本文旨在扩展复制保护的范围，使其适用于更广泛的密码原语，并能抵抗更通用的挑战分布。", "method": "1. 定义了一类名为“可延展可穿刺方案”的密码方案。2. 此类方案要求能够创建一个电路，该电路可以回答与安全博弈中挑战无关的输入，但不能帮助攻击者回答与挑战相关的输入。3. 在此定义的基础上，开发了复制保护方案。", "result": "1. 成功定义了可延展可穿刺方案，这是一个对可穿刺方案的灵活推广。2. 该方案能够涵盖以前无法进行复制保护的多种密码原语。3. 证明了该方案在任意高最小熵挑战分布下是安全的，而以往的工作仅考虑了在伪随机点穿刺的方案。", "conclusion": "本文通过引入可延展可穿刺方案并证明其在任意挑战分布下的复制保护能力，显著扩展了量子复制保护的理论和实践范围，为更多密码原语的复制保护提供了可能性。", "translation": "量子复制保护方案（Aaronson, CCC 2009）将功能编码为量子态，使得在给定此状态的情况下，任何高效的攻击者都无法创建两个（可能纠缠的）量子态，而这两个量子态都能运行该功能。最近有一系列工作致力于在普通模型中为通用类别的方案构建可证明安全的复制保护方案，而最近\nc{C}akan和Goyal（IACR Eprint, 2025）的工作展示了如何复制保护所有具有伪随机穿刺点的加密可穿刺方案。在这项工作中，我们展示了如何复制保护更大范围的方案。我们定义了一类名为可延展可穿刺方案的密码方案，其唯一要求是能够创建一个电路，该电路能够回答与安全博弈中挑战无关的输入，但不能帮助攻击者回答与挑战相关的输入。这是对可穿刺方案的灵活推广，可以涵盖在我们的工作之前不知道如何进行复制保护的广泛原语。更进一步，我们表明我们的方案对任意高最小熵挑战分布是安全的，而以前的工作只考虑了在伪随机点穿刺的方案。", "summary": "本文提出了一种新的密码方案类别——可延展可穿刺方案，并展示了如何在任意高最小熵挑战分布下对其进行复制保护。该方案是对现有可穿刺方案的灵活推广，能够复制保护以前无法处理的多种密码原语。与现有仅考虑伪随机穿刺点的工作相比，本研究显著扩展了复制保护的适用范围和安全性保证。", "keywords": "量子复制保护, 可延展可穿刺方案, 任意挑战分布, 密码原语, 安全性", "comments": "本文的主要创新在于引入了“可延展可穿刺方案”这一新的概念，并证明了其在更广泛、更现实的挑战分布（任意高最小熵）下的复制保护能力。这克服了现有研究的局限性，使得更多密码原语能够被复制保护，对量子密码学领域具有重要意义。"}}
{"id": "2507.18779", "title": "Maximizing entropy for power-free languages", "authors": ["Vaughn Climenhaga"], "categories": ["math.DS", "cs.DM", "cs.FL", "math.CO", "37D35, 68R15 (Primary) 37B10 (Secondary)"], "primary_category": "Subjects:       Dynamical Systems (math.DS)", "pdf_link": null, "comments": "Comments:      23 pages, 3 figures", "url": "http://arxiv.org/abs/2507.18779v1", "summary": "A power-free language is characterized by the number of symbols used and a\nlimit on how many times a block of symbols can repeat consecutively. For\ncertain values of these parameters, it is known that the number of legal words\ngrows exponentially fast with respect to length. In the terminology of\ndynamical systems and ergodic theory, this means that the corresponding shift\nspace has positive topological entropy.\n  We prove that in many cases, this shift space has a unique measure of maximal\nentropy. The proof uses a weak analogue of Bowen's specification property. The\nlack of any periodic points in power-free shift spaces stands in striking\ncontrast to other applications of specification-based techniques, where the\nnumber of periodic points often has exponential growth rate given by the\ntopological entropy.", "comment": "23 pages, 3 figures", "pdf_url": "http://arxiv.org/pdf/2507.18779v1", "cate": "math.DS", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "自由幂语言的最大熵", "tldr": "本文证明了在许多情况下，无幂移位空间具有唯一的最大熵测度，其证明使用了Bowen规范性质的弱模拟，并指出与周期点的显著对比。", "motivation": "旨在研究无幂语言的拓扑熵特性，特别是证明在许多情况下存在唯一的最大熵测度。", "method": "证明使用了Bowen规范性质的弱模拟（a weak analogue of Bowen's specification property）。", "result": "证明了在许多情况下，无幂移位空间具有唯一的最大熵测度。同时指出，无幂移位空间中缺乏周期点，这与基于规范性质技术在其他应用中周期点数量呈指数增长的情况形成鲜明对比。", "conclusion": "本文成功证明了无幂移位空间在多数情况下拥有唯一的最大熵测度，并通过弱模拟的规范性质方法实现，这与传统理论中周期点的重要性形成对比。", "translation": "无幂语言的特征在于所使用的符号数量以及符号块连续重复次数的限制。对于这些参数的某些值，已知合法词的数量随长度呈指数级快速增长。在动力系统和遍历理论的术语中，这意味着相应的移位空间具有正拓扑熵。\n我们证明在许多情况下，这个移位空间具有唯一的最大熵测度。该证明使用了Bowen规范性质的弱模拟。无幂移位空间中缺乏任何周期点，这与基于规范性质技术的其他应用形成了鲜明对比，在这些应用中，周期点的数量通常具有由拓扑熵给出的指数增长率。", "summary": "本文研究了无幂语言的拓扑熵，无幂语言通过符号数量和连续重复限制来定义，其合法词数量随长度呈指数增长，表明具有正拓扑熵。作者证明了在许多情况下，这种移位空间具有唯一的最大熵测度，其证明方法是Bowen规范性质的弱模拟。文章还强调了无幂移位空间缺乏周期点，这与依赖周期点数量来反映拓扑熵的其他规范性质应用形成了显著对比。", "keywords": "无幂语言, 最大熵测度, 拓扑熵, Bowen规范性质, 动力系统", "comments": "本文的创新之处在于将Bowen规范性质的弱模拟应用于没有周期点的无幂移位空间，成功证明了最大熵测度的唯一性。这挑战了传统上依赖周期点来理解拓扑熵的视角，揭示了这类特殊系统独特的动力学特性。其重要性在于扩展了遍历理论和动力系统在非周期结构中的应用。"}}
{"id": "2507.18827", "title": "CueBuddy: helping non-native English speakers navigate English-centric STEM education", "authors": ["Pranav Gupta"], "categories": ["cs.CL", "cs.LG"], "primary_category": "Subjects:       Computation and Language (cs.CL)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.18827v1", "summary": "Students across the world in STEM classes, especially in the Global South,\nfall behind their peers who are more fluent in English, despite being at par\nwith them in terms of scientific prerequisites. While many of them are able to\nfollow everyday English at ease, key terms in English stay challenging. In most\ncases, such students have had most of their course prerequisites in a lower\nresource language. Live speech translation to lower resource languages is a\npromising area of research, however, models for speech translation can be too\nexpensive on a large scale and often struggle with technical content. In this\npaper, we describe CueBuddy, which aims to remediate these issues by providing\nreal-time \"lexical cues\" through technical keyword spotting along real-time\nmultilingual glossary lookup to help students stay up to speed with complex\nEnglish jargon without disrupting their concentration on the lecture. We also\ndescribe the limitations and future extensions of our approach.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.18827v1", "cate": "cs.CL", "date": "2025-07-24", "updated": "2025-07-24", "AI": {"title_translation": "CueBuddy：帮助非英语母语者适应以英语为中心的STEM教育", "tldr": "CueBuddy通过实时词汇提示和多语言词汇表查询，帮助非英语母语的STEM学生理解复杂的英语专业术语。", "motivation": "全球STEM课堂中的非英语母语学生，尤其是在全球南方，尽管具备同等的科学先决条件，但由于英语流利度不足，在理解英语专业术语方面面临挑战。现有的实时语音翻译成本高昂且难以处理技术内容。", "method": "本文描述了CueBuddy系统，它通过实时技术关键词识别和实时多语言词汇表查询，提供实时“词汇提示”，旨在帮助学生跟上复杂的英语专业术语，同时不打断他们对讲座的注意力。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "世界各地的STEM课堂学生，尤其是在全球南方，尽管在科学先决条件方面与英语流利的同龄人水平相当，但由于英语流利度不足而落后。虽然他们中的许多人能够轻松理解日常英语，但英语中的关键术语仍然具有挑战性。在大多数情况下，这些学生的大部分课程先决条件都是用资源较少的语言学习的。实时语音翻译到资源较少的语言是一个很有前景的研究领域，然而，语音翻译模型大规模应用可能过于昂贵，并且通常难以处理技术内容。在本文中，我们描述了CueBuddy，它旨在通过实时“词汇提示”来弥补这些问题，通过技术关键词识别和实时多语言词汇表查询来帮助学生跟上复杂的英语专业术语，而不会干扰他们对讲座的注意力。我们还描述了我们方法的局限性和未来扩展。", "summary": "本文介绍了CueBuddy系统，旨在解决非英语母语STEM学生在英语授课环境中理解专业术语的难题。针对现有实时语音翻译成本高昂且处理技术内容能力不足的问题，CueBuddy通过实时识别技术关键词并提供多语言词汇表查询，为学生提供即时“词汇提示”，从而帮助他们掌握复杂的英语行话，同时不影响听课专注度。文章还讨论了该方法的局限性及未来发展方向。", "keywords": "非英语母语者, STEM教育, 词汇提示, 多语言词汇表, 技术术语", "comments": "CueBuddy的创新之处在于其提供实时、非侵入式词汇提示的解决方案，作为全面语音翻译的替代方案，专门针对STEM领域的技术内容。它通过直接解决非英语母语学生在学术交流中的语言障碍，触及了一个重要的教育公平问题。该方法避免了传统翻译的成本和技术内容处理难题，具有实际应用潜力。"}}
{"id": "2503.22498", "title": "Learnable cut flow for high energy physics", "authors": ["Jing Li", "Hao Sun"], "categories": ["cs.LG", "hep-ph"], "primary_category": "Subjects:       Machine Learning (cs.LG)", "pdf_link": null, "comments": "Comments:      31 pages, 26 figures, 8 tables, source code and data are available at GitHub", "url": "http://arxiv.org/abs/2503.22498v2", "summary": "Neural networks have emerged as a powerful paradigm for tasks in high energy\nphysics, yet their opaque training process renders them as a black box. In\ncontrast, the traditional cut flow method offers simplicity and\ninterpretability but requires extensive manual tuning to identify optimal cut\nboundaries. To merge the strengths of both approaches, we propose the Learnable\nCut Flow (LCF), a neural network that transforms the traditional cut selection\ninto a fully differentiable, data-driven process. LCF implements two cut\nstrategies-parallel, where observable distributions are treated independently,\nand sequential, where prior cuts shape subsequent ones-to flexibly determine\noptimal boundaries. Building on this strategy, we introduce the Learnable\nImportance, a metric that quantifies feature importance and adjusts their\ncontributions to the loss accordingly, offering model-driven insights unlike\nad-hoc metrics. To ensure differentiability, a modified loss function replaces\nhard cuts with mask operations, preserving data shape throughout the training\nprocess. LCF is tested on six varied mock datasets and a realistic diboson vs.\nQCD dataset. Results demonstrate that LCF 1. accurately learns cut boundaries\nacross typical feature distributions in both parallel and sequential\nstrategies, 2. assigns higher importance to discriminative features with\nminimal overlap, 3. handles redundant or correlated features robustly, and 4.\nperforms effectively in real-world scenarios. In the diboson dataset, LCF\ninitially underperforms boosted decision trees and multiplayer perceptrons when\nusing all observables. However, pruning less critical features-guided by\nlearned importance-boosts its performance to match or exceed these baselines.\nLCF bridges the gap between traditional cut flow method and modern black-box\nneural networks, delivering actionable insights into the training process and\nfeature importance.", "comment": "31 pages, 26 figures, 8 tables, source code and data are available at\n  GitHub", "pdf_url": "http://arxiv.org/pdf/2503.22498v2", "cate": "cs.LG", "date": "2025-03-28", "updated": "2025-07-25", "AI": {"title_translation": "高能物理中的可学习截断流", "tldr": "神经网络强大但黑箱，传统截断流可解释但需手动调优。本文提出可学习截断流（LCF），一个可微分神经网络，自动学习最优截断边界并量化特征重要性，弥合了传统方法和现代AI的差距。", "motivation": "神经网络在高能物理中表现强大但训练过程不透明，形同“黑箱”。传统截断流方法简单可解释，但需要大量手动调整才能找到最佳截断边界。本文旨在结合两者的优点。", "method": "本文提出了可学习截断流（LCF），一个将传统截断选择转化为完全可微分、数据驱动过程的神经网络。LCF实现了两种截断策略：并行策略（独立处理可观测分布）和序列策略（先前截断影响后续截断），以灵活确定最佳边界。引入了“可学习重要性”来量化特征重要性并调整其对损失的贡献。为确保可微分性，修改后的损失函数用掩码操作取代了硬截断。", "result": "LCF在六个模拟数据集和一个真实的双玻色子与QCD数据集上进行了测试。结果表明：1. LCF能准确学习并行和序列策略下的典型特征分布的截断边界；2. 对判别性强且重叠最小的特征赋予更高重要性；3. 稳健处理冗余或相关特征；4. 在实际场景中表现出色。在双玻色子数据集中，LCF在所有可观测变量下初始表现略逊于梯度提升决策树和多层感知器，但在学习重要性的指导下剪枝掉不关键特征后，其性能提升至与这些基线模型持平或超越。", "conclusion": "LCF弥合了传统截断流方法和现代黑箱神经网络之间的差距，为训练过程和特征重要性提供了可操作的洞察。", "translation": "神经网络已成为高能物理任务中的强大范式，但其不透明的训练过程使其成为一个黑箱。相比之下，传统的截断流（cut flow）方法简单且可解释，但需要大量的手动调整来确定最佳截断边界。为了结合这两种方法的优点，我们提出了可学习截断流（Learnable Cut Flow, LCF），这是一种将传统截断选择转化为完全可微分、数据驱动过程的神经网络。LCF实现了两种截断策略——并行策略（独立处理可观测分布）和序列策略（先前截断影响后续截断）——以灵活确定最佳边界。在此策略基础上，我们引入了可学习重要性（Learnable Importance），这是一种量化特征重要性并相应调整其对损失贡献的指标，与临时指标不同，它提供了模型驱动的洞察。为确保可微分性，修改后的损失函数用掩码操作取代了硬截断，在整个训练过程中保持了数据形状。LCF在六个不同的模拟数据集和一个真实的双玻色子（diboson）与量子色动力学（QCD）数据集上进行了测试。结果表明，LCF 1. 在并行和序列策略中都能准确学习典型特征分布的截断边界，2. 对判别性强且重叠最小的特征赋予更高的重要性，3. 稳健处理冗余或相关特征，4. 在实际场景中表现出色。在双玻色子数据集中，当使用所有可观测变量时，LCF最初表现不如梯度提升决策树（boosted decision trees）和多层感知器（multiplayer perceptrons）。然而，在学习重要性的指导下，修剪掉不那么关键的特征后，其性能提升到与这些基线模型持平或超越。LCF弥合了传统截断流方法和现代黑箱神经网络之间的差距，为训练过程和特征重要性提供了可操作的洞察。", "summary": "本文提出了一种名为可学习截断流（LCF）的新型神经网络，旨在结合传统截断流方法的可解释性与神经网络的强大能力。LCF通过引入并行和序列截断策略以及可学习重要性指标，将手动截断选择转化为可微分、数据驱动的过程。实验结果表明，LCF能有效学习最佳截断边界，量化特征重要性，并能通过特征剪枝提升性能，从而在高能物理任务中提供透明且高效的解决方案。", "keywords": "高能物理, 可学习截断流, 神经网络, 特征重要性, 截断优化", "comments": "LCF的创新之处在于将传统高能物理中依赖人工经验的截断流方法，通过神经网络和可微分技术，转化为自动化、数据驱动且可解释的过程。这不仅提高了效率，还通过“可学习重要性”提供了对模型决策和特征贡献的深入洞察，解决了传统神经网络“黑箱”问题，在高能物理数据分析中具有重要意义。"}}
{"id": "2507.19055", "title": "Virtual local area network over HTTP for launching an insider attack", "authors": ["Yuksel Arslan"], "categories": ["cs.CR", "cs.NI"], "primary_category": "Subjects:       Cryptography and Security (cs.CR)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19055v1", "summary": "Computers and computer networks have become integral to virtually every\naspect of modern life, with the Internet playing an indispensable role.\nOrganizations, businesses, and individuals now store vast amounts of\nproprietary, confidential, and personal data digitally. As such, ensuring the\nsecurity of this data from unauthorized access is critical. Common security\nmeasures, such as firewalls, intrusion detection systems (IDS), intrusion\nprevention systems (IPS), and antivirus software, are constantly evolving to\nsafeguard computer systems and networks. However, these tools primarily focus\non defending against external threats, leaving systems vulnerable to insider\nattacks. Security solutions designed to mitigate risks originating from within\nthe organization are relatively limited and often ineffective. This paper\ndemonstrates how a Local Area Network (LAN) can be covertly exposed to the\nInternet via an insider attack. Specifically, it illustrates how an external\nmachine can gain access to a LAN by exploiting an unused secondary IP address\nof the attacked LAN, effectively bypassing existing security mechanisms by also\nexploiting Hyper Text Transfer Protocol (HTTP). Despite the presence of robust\nexternal protections, such as firewalls and IDS, this form of insider attack\nreveals significant vulnerabilities in the way internal threats are addressed.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19055v1", "cate": "cs.CR", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "基于HTTP的虚拟局域网用于发动内部攻击", "tldr": "本文演示了如何通过利用一个未使用的辅助IP地址和HTTP协议，将局域网（LAN）秘密暴露给互联网，从而使外部机器能够通过内部攻击绕过现有的安全机制。", "motivation": "现有的安全措施主要针对外部威胁，导致系统容易受到内部攻击。针对组织内部风险的安全解决方案相对有限且往往无效。", "method": "本文演示了如何通过内部攻击将局域网（LAN）秘密暴露给互联网。具体来说，它展示了外部机器如何通过利用被攻击局域网未使用的辅助IP地址，并利用HTTP协议，有效绕过现有安全机制来访问局域网。", "result": "尽管存在防火墙和入侵检测系统（IDS）等强大的外部保护，这种形式的内部攻击揭示了在处理内部威胁方面存在的重大漏洞。", "conclusion": "即使有强大的外部保护，内部攻击仍然揭示了在处理内部威胁方面的显著漏洞。", "translation": "计算机和计算机网络已成为现代生活几乎每个方面不可或缺的一部分，其中互联网发挥着不可或缺的作用。组织、企业和个人现在以数字形式存储大量专有、机密和个人数据。因此，确保这些数据免受未经授权的访问至关重要。常见的安全措施，如防火墙、入侵检测系统（IDS）、入侵防御系统（IPS）和防病毒软件，正在不断发展以保护计算机系统和网络。然而，这些工具主要侧重于防御外部威胁，使系统容易受到内部攻击。旨在减轻源自组织内部风险的安全解决方案相对有限且往往无效。本文演示了如何通过内部攻击将局域网（LAN）秘密暴露给互联网。具体来说，它说明了外部机器如何通过利用被攻击局域网未使用的辅助IP地址，并通过利用超文本传输协议（HTTP），有效绕过现有安全机制来访问局域网。尽管存在强大的外部保护，如防火墙和IDS，这种形式的内部攻击揭示了在处理内部威胁方式上的重大漏洞。", "summary": "本文探讨了现有安全措施在防御内部威胁方面的局限性。研究人员演示了一种内部攻击方法，通过利用局域网（LAN）未使用的辅助IP地址和HTTP协议，将LAN秘密暴露给互联网，从而允许外部机器绕过防火墙和IDS等外部安全防御措施。该研究强调了在处理内部威胁方面存在的显著漏洞，即使在有强大外部保护的情况下，内部威胁仍然是一个重要的安全风险。", "keywords": "内部攻击, 局域网, HTTP, 安全漏洞, 网络安全", "comments": "本文通过演示一种具体的内部攻击方法，揭示了当前网络安全防御体系中对内部威胁处理的不足，具有重要的警示意义。其创新之处在于利用了LAN的未使用的辅助IP地址和HTTP协议来绕过传统安全机制。这提醒了组织需要重新审视其内部安全策略，而不仅仅是依赖于外部威胁防御。"}}
{"id": "2507.18971", "title": "Rethinking Dataset Discovery with DataScout", "authors": ["Rachel Lin", "Bhavya Chopra", "Wenjing Lin", "Shreya Shankar", "Madelon Hulsebos", "Aditya G. Parameswaran"], "categories": ["cs.HC"], "primary_category": "Subjects:       Human-Computer Interaction (cs.HC)", "pdf_link": null, "comments": "Comments:      16 pages; 6 figures; 4 tables; To appear at UIST 2025", "url": "http://arxiv.org/abs/2507.18971v1", "summary": "Dataset Search -- the process of finding appropriate datasets for a given\ntask -- remains a critical yet under-explored challenge in data science\nworkflows. Assessing dataset suitability for a task (e.g., training a\nclassification model) is a multi-pronged affair that involves understanding:\ndata characteristics (e.g. granularity, attributes, size), semantics (e.g.,\ndata semantics, creation goals), and relevance to the task at hand. Present-day\ndataset search interfaces are restrictive -- users struggle to convey implicit\npreferences and lack visibility into the search space and result inclusion\ncriteria -- making query iteration challenging. To bridge these gaps, we\nintroduce DataScout to proactively steer users through the process of dataset\ndiscovery via -- (i) AI-assisted query reformulations informed by the\nunderlying search space, (ii) semantic search and filtering based on dataset\ncontent, including attributes (columns) and granularity (rows), and (iii)\ndataset relevance indicators, generated dynamically based on the user-specified\ntask. A within-subjects study with 12 participants comparing DataScout to\nkeyword and semantic dataset search reveals that users uniquely employ\nDataScout's features not only for structured explorations, but also to glean\nfeedback on their search queries and build conceptual models of the search\nspace.", "comment": "16 pages; 6 figures; 4 tables; To appear at UIST 2025", "pdf_url": "http://arxiv.org/pdf/2507.18971v1", "cate": "cs.HC", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "重新思考使用 DataScout 进行数据集发现", "tldr": "DataScout 是一个旨在改进数据集发现过程的系统，它通过AI辅助查询重构、语义搜索和动态相关性指标，帮助用户更好地探索和理解数据集搜索空间，克服了传统搜索界面的限制。", "motivation": "当前的数据集搜索面临诸多挑战：现有界面限制性强，用户难以表达隐含偏好，且缺乏对搜索空间和结果包含标准的可见性，导致查询迭代困难。评估数据集的适用性是一个复杂的多方面问题，涉及数据特征、语义和任务相关性。", "method": "本文引入了 DataScout 系统，通过以下方式主动引导用户进行数据集发现：(i) 基于底层搜索空间的AI辅助查询重构，(ii) 基于数据集内容（包括属性和粒度）的语义搜索和过滤，以及 (iii) 根据用户指定任务动态生成的数据集相关性指标。研究通过一项包含12名参与者的组内研究，将 DataScout 与关键词和语义数据集搜索进行了比较。", "result": "研究发现，用户独特地利用了 DataScout 的功能，不仅用于结构化探索，还用于获取对其搜索查询的反馈，并构建搜索空间的概念模型。", "conclusion": "DataScout 通过其创新的AI辅助查询、语义搜索和动态相关性指标，有效解决了数据集发现过程中的挑战，显著提升了用户理解和探索数据集的能力。", "translation": "数据集搜索——为给定任务寻找合适数据集的过程——在数据科学工作流程中仍然是一个关键但未被充分探索的挑战。评估数据集对任务的适用性（例如，训练分类模型）是一个多方面的问题，涉及理解：数据特征（例如粒度、属性、大小）、语义（例如数据语义、创建目标）以及与手头任务的相关性。当前的（数据集）搜索界面具有限制性——用户难以表达隐含偏好，并且缺乏对搜索空间和结果包含标准的可见性——这使得查询迭代变得困难。为了弥补这些差距，我们引入了 DataScout，通过以下方式主动引导用户完成数据集发现过程：(i) 由底层搜索空间提供信息的AI辅助查询重构，(ii) 基于数据集内容（包括属性（列）和粒度（行））的语义搜索和过滤，以及 (iii) 根据用户指定任务动态生成的数据集相关性指标。一项对12名参与者进行的组内研究，将 DataScout 与关键词和语义数据集搜索进行比较，结果显示用户独特地利用了 DataScout 的功能，不仅用于结构化探索，还用于获取对其搜索查询的反馈，并构建搜索空间的概念模型。", "summary": "本文介绍了 DataScout，一个旨在改进数据集发现过程的系统。针对当前数据集搜索界面存在的限制，DataScout 提供了AI辅助查询重构、基于内容和粒度的语义搜索与过滤，以及动态生成的数据集相关性指标。一项针对12名参与者的研究表明，DataScout 不仅支持用户进行结构化探索，还能帮助他们获取查询反馈并构建搜索空间的概念模型，从而有效解决了数据集适用性评估和搜索迭代的挑战。", "keywords": "数据集发现, DataScout, 语义搜索, AI辅助, 数据科学", "comments": "DataScout 的创新点在于其结合了 AI 辅助的查询重构、语义搜索和动态相关性指标，主动引导用户进行数据集发现，解决了传统搜索界面无法有效表达隐式偏好和缺乏可见性的问题。这项工作对于提升数据科学工作流程中的数据集选择效率和准确性具有重要意义。"}}
{"id": "2507.19199", "title": "Enhancing Diabetic Retinopathy Classification Accuracy through Dual Attention Mechanism in Deep Learning", "authors": ["Abdul Hannan", "Zahid Mahmood", "Rizwan Qureshi", "Hazrat Ali"], "categories": ["eess.IV", "cs.AI", "cs.CV"], "primary_category": "Subjects:       Image and Video Processing (eess.IV)", "pdf_link": null, "comments": "Comments:      submitted to Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization", "url": "http://arxiv.org/abs/2507.19199v1", "summary": "Automatic classification of Diabetic Retinopathy (DR) can assist\nophthalmologists in devising personalized treatment plans, making it a critical\ncomponent of clinical practice. However, imbalanced data distribution in the\ndataset becomes a bottleneck in the generalization of deep learning models\ntrained for DR classification. In this work, we combine global attention block\n(GAB) and category attention block (CAB) into the deep learning model, thus\neffectively overcoming the imbalanced data distribution problem in DR\nclassification. Our proposed approach is based on an attention mechanism-based\ndeep learning model that employs three pre-trained networks, namely,\nMobileNetV3-small, Efficientnet-b0, and DenseNet-169 as the backbone\narchitecture. We evaluate the proposed method on two publicly available\ndatasets of retinal fundoscopy images for DR. Experimental results show that on\nthe APTOS dataset, the DenseNet-169 yielded 83.20% mean accuracy, followed by\nthe MobileNetV3-small and EfficientNet-b0, which yielded 82% and 80%\naccuracies, respectively. On the EYEPACS dataset, the EfficientNet-b0 yielded a\nmean accuracy of 80%, while the DenseNet-169 and MobileNetV3-small yielded\n75.43% and 76.68% accuracies, respectively. In addition, we also compute the\nF1-score of 82.0%, precision of 82.1%, sensitivity of 83.0%, specificity of\n95.5%, and a kappa score of 88.2% for the experiments. Moreover, in our work,\nthe MobileNetV3-small has 1.6 million parameters on the APTOS dataset and 0.90\nmillion parameters on the EYEPACS dataset, which is comparatively less than\nother methods. The proposed approach achieves competitive performance that is\nat par with recently reported works on DR classification.", "comment": "submitted to Computer Methods in Biomechanics and Biomedical\n  Engineering: Imaging & Visualization", "pdf_url": "http://arxiv.org/pdf/2507.19199v1", "cate": "eess.IV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "通过深度学习中的双重注意力机制提高糖尿病视网膜病变分类准确性", "tldr": "本文提出了一种结合全局注意力块和类别注意力块的深度学习模型，以解决糖尿病视网膜病变（DR）分类中数据不平衡问题，并在两个公开数据集上取得了具有竞争力的性能。", "motivation": "糖尿病视网膜病变（DR）的自动分类对眼科医生制定个性化治疗方案至关重要。然而，数据集中不平衡的数据分布成为深度学习模型在DR分类中泛化能力的瓶颈。", "method": "本文将全局注意力块（GAB）和类别注意力块（CAB）结合到深度学习模型中，以有效克服DR分类中的数据不平衡问题。所提出的方法基于注意力机制，并以MobileNetV3-small、Efficientnet-b0和DenseNet-169作为骨干网络进行预训练。", "result": "在APTOS数据集上，DenseNet-169获得了83.20%的平均准确率，MobileNetV3-small和EfficientNet-b0分别为82%和80%。在EYEPACS数据集上，EfficientNet-b0获得了80%的平均准确率，DenseNet-169和MobileNetV3-small分别为75.43%和76.68%。实验还获得了82.0%的F1分数、82.1%的精确度、83.0%的敏感度、95.5%的特异性和88.2%的Kappa分数。MobileNetV3-small在APTOS和EYEPACS数据集上的参数量分别为160万和90万，相对较少。", "conclusion": "所提出的方法在糖尿病视网膜病变分类方面取得了与近期报道工作相当的竞争性性能。", "translation": "糖尿病视网膜病变（DR）的自动分类可以帮助眼科医生制定个性化治疗方案，使其成为临床实践的关键组成部分。然而，数据集中不平衡的数据分布成为用于DR分类的深度学习模型泛化能力的瓶颈。在这项工作中，我们将全局注意力块（GAB）和类别注意力块（CAB）结合到深度学习模型中，从而有效克服DR分类中的数据不平衡问题。我们提出的方法基于一种基于注意力机制的深度学习模型，该模型采用MobileNetV3-small、Efficientnet-b0和DenseNet-169三种预训练网络作为骨干架构。我们评估了在两个公开可用的DR视网膜眼底图像数据集上的所提出方法。实验结果表明，在APTOS数据集上，DenseNet-169取得了83.20%的平均准确率，其次是MobileNetV3-small和EfficientNet-b0，分别取得了82%和80%的准确率。在EYEPACS数据集上，EfficientNet-b0取得了80%的平均准确率，而DenseNet-169和MobileNetV3-small分别取得了75.43%和76.68%的准确率。此外，在实验中，我们还计算了82.0%的F1分数、82.1%的精确度、83.0%的敏感度、95.5%的特异性以及88.2%的Kappa分数。此外，在我们的工作中，MobileNetV3-small在APTOS数据集上拥有160万参数，在EYEPACS数据集上拥有90万参数，这与其他方法相比参数量较少。所提出的方法取得了与近期报道的DR分类工作相当的竞争性性能。", "summary": "本研究旨在通过引入双重注意力机制（全局注意力块和类别注意力块）来提高深度学习模型在糖尿病视网膜病变（DR）分类中的准确性，特别针对数据集中的数据不平衡问题。研究利用MobileNetV3-small、Efficientnet-b0和DenseNet-169作为骨干网络，并在APTOS和EYEPACS两个公开数据集上进行了评估。实验结果表明，该方法取得了具有竞争力的性能，尤其在解决数据不平衡问题上表现出有效性，且部分模型（如MobileNetV3-small）参数量较少。", "keywords": "糖尿病视网膜病变分类, 深度学习, 注意力机制, 数据不平衡, 医学图像", "comments": "本文的创新点在于结合了全局注意力块（GAB）和类别注意力块（CAB）来解决医学图像分类中常见的类不平衡问题，这对于提高模型在实际临床应用中的泛化能力至关重要。其方法具有通用性，可以应用于其他存在数据不平衡的图像分类任务。同时，通过使用轻量级网络（如MobileNetV3-small）并实现竞争性性能，也体现了其实用价值。"}}
{"id": "2507.19209", "title": "Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting with CounterNet", "authors": ["Xiaoyu Zhang", "Zhifeng Bao", "Hai Dong", "Ziwei Wang", "Jiajun Liu"], "categories": ["cs.CV", "cs.MM"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "", "url": "http://arxiv.org/abs/2507.19209v1", "summary": "Autonomous vehicles generate massive volumes of point cloud data, yet only a\nsubset is relevant for specific tasks such as collision detection, traffic\nanalysis, or congestion monitoring. Effectively querying this data is essential\nto enable targeted analytics. In this work, we formalize point cloud querying\nby defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, each\naligned with distinct analytical scenarios. All these queries rely heavily on\naccurate object counts to produce meaningful results, making precise object\ncounting a critical component of query execution. Prior work has focused on\nindexing techniques for 2D video data, assuming detection models provide\naccurate counting information. However, when applied to 3D point cloud data,\nstate-of-the-art detection models often fail to generate reliable object\ncounts, leading to substantial errors in query results. To address this\nlimitation, we propose CounterNet, a heatmap-based network designed for\naccurate object counting in large-scale point cloud data. Rather than focusing\non accurate object localization, CounterNet detects object presence by finding\nobject centers to improve counting accuracy. We further enhance its performance\nwith a feature map partitioning strategy using overlapping regions, enabling\nbetter handling of both small and large objects in complex traffic scenes. To\nadapt to varying frame characteristics, we introduce a per-frame dynamic model\nselection strategy that selects the most effective configuration for each\ninput. Evaluations on three real-world autonomous vehicle datasets show that\nCounterNet improves counting accuracy by 5% to 20% across object categories,\nresulting in more reliable query outcomes across all supported query types.", "comment": null, "pdf_url": "http://arxiv.org/pdf/2507.19209v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "查询自动驾驶车辆点云：通过CounterNet增强3D对象计数", "tldr": "该研究提出CounterNet，一个基于热图的网络，用于准确计数自动驾驶车辆点云中的3D对象，以提高点云查询的可靠性，解决了现有方法在3D数据上计数不准确的问题。", "motivation": "自动驾驶车辆生成大量点云数据，但现有查询方法在3D点云数据上依赖的检测模型在生成可靠的对象计数方面表现不佳，导致查询结果出现重大错误。有效查询这些数据对于有针对性的分析至关重要。", "method": "提出了CounterNet，一个基于热图的网络，专门用于大规模点云数据中的精确对象计数。CounterNet通过寻找对象中心来检测对象存在，以提高计数精度，而不是专注于精确的对象定位。它还采用特征图分区策略，使用重叠区域来处理大小对象。此外，引入了每帧动态模型选择策略，根据帧特性选择最有效的配置。", "result": "在三个真实世界的自动驾驶车辆数据集上进行评估，CounterNet将对象计数精度提高了5%到20%，从而在所有支持的查询类型中实现了更可靠的查询结果。", "conclusion": "通过引入CounterNet及其增强策略，本研究显著提高了自动驾驶车辆点云中3D对象计数的准确性，从而使点云查询（包括检索、计数和聚合）的结果更加可靠和有意义。", "translation": "自动驾驶车辆生成大量的点云数据，然而只有一部分数据与碰撞检测、交通分析或拥堵监控等特定任务相关。有效查询这些数据对于实现有针对性的分析至关重要。在这项工作中，我们通过定义三种核心查询类型来规范点云查询：检索（RETRIEVAL）、计数（COUNT）和聚合（AGGREGATION），每种类型都与不同的分析场景对齐。所有这些查询都严重依赖于准确的对象计数来产生有意义的结果，这使得精确的对象计数成为查询执行的关键组成部分。以前的工作主要集中在2D视频数据的索引技术上，假设检测模型提供了准确的计数信息。然而，当应用于3D点云数据时，最先进的检测模型往往无法生成可靠的对象计数，导致查询结果出现重大错误。为了解决这一限制，我们提出了CounterNet，一个基于热图的网络，旨在对大规模点云数据进行准确的对象计数。CounterNet不是专注于精确的对象定位，而是通过寻找对象中心来检测对象的存在，从而提高计数精度。我们通过使用重叠区域的特征图分区策略进一步增强其性能，从而更好地处理复杂交通场景中的小型和大型对象。为了适应不同的帧特性，我们引入了每帧动态模型选择策略，为每个输入选择最有效的配置。在三个真实世界的自动驾驶车辆数据集上的评估表明，CounterNet将对象类别间的计数精度提高了5%到20%，从而在所有支持的查询类型中产生了更可靠的查询结果。", "summary": "本研究旨在解决自动驾驶车辆点云数据中对象计数不准确导致查询结果错误的问题。作者提出了CounterNet，一个基于热图的网络，通过检测对象中心来提高3D对象计数精度。该网络还采用了重叠区域的特征图分区策略和每帧动态模型选择策略，以适应不同场景和对象大小。实验表明，CounterNet在真实世界数据集中将计数精度提高了5%到20%，显著提升了点云查询的可靠性。", "keywords": "点云查询, 3D对象计数, CounterNet, 自动驾驶, 深度学习", "comments": "CounterNet的创新之处在于其专注于对象计数而非精确本地化，以及引入了特征图分区和动态模型选择策略来优化性能。这对于需要准确对象计数的自动驾驶应用（如交通分析和拥堵监控）具有重要意义，解决了现有2D视频索引技术无法有效应用于3D点云数据的局限性。"}}
{"id": "2507.19481", "title": "HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars", "authors": ["Byungjun Kim", "Shunsuke Saito", "Giljoo Nam", "Tomas Simon", "Jason Saragih", "Hanbyul Joo", "Junxuan Li"], "categories": ["cs.CV"], "primary_category": "Subjects:       Computer Vision and Pattern Recognition (cs.CV)", "pdf_link": null, "comments": "Comments:      ICCV 2025. Project Page: this https URL", "url": "http://arxiv.org/abs/2507.19481v1", "summary": "We present a universal prior model for 3D head avatars with explicit hair\ncompositionality. Existing approaches to build generalizable priors for 3D head\navatars often adopt a holistic modeling approach, treating the face and hair as\nan inseparable entity. This overlooks the inherent compositionality of the\nhuman head, making it difficult for the model to naturally disentangle face and\nhair representations, especially when the dataset is limited. Furthermore, such\nholistic models struggle to support applications like 3D face and hairstyle\nswapping in a flexible and controllable manner. To address these challenges, we\nintroduce a prior model that explicitly accounts for the compositionality of\nface and hair, learning their latent spaces separately. A key enabler of this\napproach is our synthetic hairless data creation pipeline, which removes hair\nfrom studio-captured datasets using estimated hairless geometry and texture\nderived from a diffusion prior. By leveraging a paired dataset of hair and\nhairless captures, we train disentangled prior models for face and hair,\nincorporating compositionality as an inductive bias to facilitate effective\nseparation. Our model's inherent compositionality enables seamless transfer of\nface and hair components between avatars while preserving identity.\nAdditionally, we demonstrate that our model can be fine-tuned in a few-shot\nmanner using monocular captures to create high-fidelity, hair-compositional 3D\nhead avatars for unseen subjects. These capabilities highlight the practical\napplicability of our approach in real-world scenarios, paving the way for\nflexible and expressive 3D avatar generation.", "comment": "ICCV 2025. Project Page: https://bjkim95.github.io/haircup/", "pdf_url": "http://arxiv.org/pdf/2507.19481v1", "cate": "cs.CV", "date": "2025-07-25", "updated": "2025-07-25", "AI": {"title_translation": "HairCUP：用于3D高斯头像的头发组成通用先验模型", "tldr": "HairCUP提出了一种具有显式头发成分的3D头部头像通用先验模型，通过分离面部和头发的潜在空间来解决现有方法中难以解耦和灵活应用的问题。", "motivation": "现有构建3D头部头像通用先验的方法通常采用整体建模方式，将面部和头发视为不可分割的实体，这忽略了人头固有的组成性，导致模型难以自然地解耦面部和头发表示，尤其是在数据集有限的情况下。此外，这些整体模型难以灵活可控地支持3D面部和发型交换等应用。", "method": "我们引入了一个显式考虑面部和头发组成性的先验模型，并分别学习它们的潜在空间。关键在于我们的人造无发数据创建流程，该流程使用从扩散先验中导出的估计无发几何和纹理从工作室捕获的数据集中去除头发。通过利用头发和无发捕获的配对数据集，我们训练了面部和头发的解耦先验模型，将组成性作为归纳偏置以促进有效分离。", "result": "我们的模型固有的组成性使得头像之间可以无缝地传输面部和头发组件，同时保持身份。此外，我们证明了我们的模型可以通过单目捕获进行少样本微调，为未见过的对象创建高保真、具有头发组成性的3D头部头像。", "conclusion": "HairCUP模型通过显式分离面部和头发的表示，克服了现有3D头像建模方法的局限性，实现了灵活的面部和发型组件传输以及高保真头像生成，为实际应用场景中灵活表达的3D头像生成铺平了道路。", "translation": "我们提出了一种用于3D头部头像的通用先验模型，该模型具有明确的头发组成性。现有构建3D头部头像通用先验的方法通常采用整体建模方式，将面部和头发视为不可分割的实体。这忽略了人头固有的组成性，使得模型难以自然地解耦面部和头发表示，尤其是在数据集有限的情况下。此外，这种整体模型难以灵活可控地支持3D面部和发型交换等应用。为了解决这些挑战，我们引入了一个显式考虑面部和头发组成性的先验模型，分别学习它们的潜在空间。实现这一方法的关键是我们的人造无发数据创建流程，该流程使用从扩散先验中导出的估计无发几何和纹理从工作室捕获的数据集中去除头发。通过利用头发和无发捕获的配对数据集，我们训练了面部和头发的解耦先验模型，将组成性作为归纳偏置以促进有效分离。我们模型固有的组成性使得头像之间可以无缝地传输面部和头发组件，同时保持身份。此外，我们证明了我们的模型可以通过单目捕获进行少样本微调，为未见过的对象创建高保真、具有头发组成性的3D头部头像。这些能力突出了我们方法在实际场景中的实际适用性，为灵活和富有表现力的3D头像生成铺平了道路。", "summary": "HairCUP提出了一种新的3D头部头像通用先验模型，通过显式分离面部和头发的潜在空间来解决现有整体建模方法的局限性。该方法利用合成无发数据创建流程和配对数据集训练解耦先验模型，从而实现面部和头发组件的无缝传输以及在少样本条件下生成高保真、具有头发组成性的3D头像，为灵活的3D头像生成提供了实际解决方案。", "keywords": "3D头像, 头发组成性, 通用先验, 解耦表示, 少样本学习", "comments": "这项研究的创新之处在于其显式地将人头头像分解为面部和头发的组成部分，并分别学习它们的潜在空间，解决了传统整体建模在解耦和灵活应用上的难题。通过引入合成无发数据创建流程，该方法巧妙地构建了训练所需的配对数据集，进一步增强了模型的解耦能力。这对于实现更可控、更逼真的3D头像生成具有重要意义，尤其是在虚拟试戴、游戏和元宇宙等领域有广泛的应用前景。"}}
