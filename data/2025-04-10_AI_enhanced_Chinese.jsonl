{"id": "2504.06285", "pdf": "https://arxiv.org/pdf/2504.06285", "abs": "https://arxiv.org/abs/2504.06285", "authors": ["Bryar A. Hassan", "Shko M. Qader", "Alla A. Hassan", "Joan Lu", "Aram M. Ahmed", "Jafar Majidpour", "Tarik A. Rashid"], "title": "Reducing Formal Context Extraction: A Newly Proposed Framework from Big Corpora", "categories": ["cs.CL"], "comment": null, "summary": "Automating the extraction of concept hierarchies from free text is\nadvantageous because manual generation is frequently labor- and\nresource-intensive. Free result, the whole procedure for concept hierarchy\nlearning from free text entails several phases, including sentence-level text\nprocessing, sentence splitting, and tokenization. Lemmatization is after formal\ncontext analysis (FCA) to derive the pairings. Nevertheless, there could be a\nfew uninteresting and incorrect pairings in the formal context. It may take a\nwhile to generate formal context; thus, size reduction formal context is\nnecessary to weed out irrelevant and incorrect pairings to extract the concept\nlattice and hierarchies more quickly. This study aims to propose a framework\nfor reducing formal context in extracting concept hierarchies from free text to\nreduce the ambiguity of the formal context. We achieve this by reducing the\nsize of the formal context using a hybrid of a WordNet-based method and a\nfrequency-based technique. Using 385 samples from the Wikipedia corpus and the\nsuggested framework, tests are carried out to examine the reduced size of\nformal context, leading to concept lattice and concept hierarchy. With the help\nof concept lattice-invariants, the generated formal context lattice is compared\nto the normal one. In contrast to basic ones, the homomorphic between the\nresultant lattices retains up to 98% of the quality of the generating concept\nhierarchies, and the reduced concept lattice receives the structural connection\nof the standard one. Additionally, the new framework is compared to five\nbaseline techniques to calculate the running time on random datasets with\nvarious densities. The findings demonstrate that, in various fill ratios,\nhybrid approaches of the proposed method outperform other indicated competing\nstrategies in concept lattice performance.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4ece\u81ea\u7531\u6587\u672c\u4e2d\u63d0\u53d6\u6982\u5ff5\u5c42\u6b21\u7ed3\u6784\u65f6\u51cf\u5c11\u5f62\u5f0f\u4e0a\u4e0b\u6587\u7684\u89c4\u6a21\u3002", "motivation": "\u624b\u52a8\u751f\u6210\u6982\u5ff5\u5c42\u6b21\u7ed3\u6784\u901a\u5e38\u8017\u65f6\u4e14\u8d44\u6e90\u5bc6\u96c6\uff0c\u800c\u81ea\u52a8\u5316\u63d0\u53d6\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u57fa\u4e8eWordNet\u7684\u65b9\u6cd5\u548c\u57fa\u4e8e\u9891\u7387\u7684\u6280\u672f\uff0c\u51cf\u5c11\u5f62\u5f0f\u4e0a\u4e0b\u6587\u7684\u89c4\u6a21\u3002", "result": "\u63d0\u51fa\u7684\u6df7\u5408\u65b9\u6cd5\u5728\u6982\u5ff5\u683c\u6027\u80fd\u4e0a\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6280\u672f\uff0c\u4fdd\u7559\u4e8698%\u7684\u751f\u6210\u6982\u5ff5\u5c42\u6b21\u7ed3\u6784\u7684\u8d28\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u5f62\u5f0f\u4e0a\u4e0b\u6587\u7684\u6a21\u7cca\u6027\uff0c\u63d0\u5347\u6982\u5ff5\u5c42\u6b21\u7ed3\u6784\u7684\u63d0\u53d6\u6548\u7387\u3002"}}
{"id": "2504.06356", "pdf": "https://arxiv.org/pdf/2504.06356", "abs": "https://arxiv.org/abs/2504.06356", "authors": ["Yifei Yuan", "Zahra Abbasiantaeb", "Yang Deng", "Mohammad Aliannejadi"], "title": "Query Understanding in LLM-based Conversational Information Seeking", "categories": ["cs.CL"], "comment": "WWW'25 Tutorial", "summary": "Query understanding in Conversational Information Seeking (CIS) involves\naccurately interpreting user intent through context-aware interactions. This\nincludes resolving ambiguities, refining queries, and adapting to evolving\ninformation needs. Large Language Models (LLMs) enhance this process by\ninterpreting nuanced language and adapting dynamically, improving the relevance\nand precision of search results in real-time. In this tutorial, we explore\nadvanced techniques to enhance query understanding in LLM-based CIS systems. We\ndelve into LLM-driven methods for developing robust evaluation metrics to\nassess query understanding quality in multi-turn interactions, strategies for\nbuilding more interactive systems, and applications like proactive query\nmanagement and query reformulation. We also discuss key challenges in\nintegrating LLMs for query understanding in conversational search systems and\noutline future research directions. Our goal is to deepen the audience's\nunderstanding of LLM-based conversational query understanding and inspire\ndiscussions to drive ongoing advancements in this field.", "AI": {"task": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u5347\u5bf9\u8bdd\u4fe1\u606f\u68c0\u7d22\uff08CIS\uff09\u4e2d\u7684\u67e5\u8be2\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u4ea4\u4e92\u51c6\u786e\u89e3\u6790\u7528\u6237\u610f\u56fe\uff0c\u89e3\u51b3\u6b67\u4e49\u3001\u4f18\u5316\u67e5\u8be2\u5e76\u9002\u5e94\u52a8\u6001\u4fe1\u606f\u9700\u6c42\uff0c\u4ee5\u63d0\u5347\u641c\u7d22\u7ed3\u679c\u7684\u5b9e\u65f6\u76f8\u5173\u6027\u548c\u7cbe\u786e\u6027\u3002", "method": "\u91c7\u7528LLM\u9a71\u52a8\u7684\u6280\u672f\uff0c\u5f00\u53d1\u9c81\u68d2\u7684\u8bc4\u4ef7\u6307\u6807\uff0c\u6784\u5efa\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u5e76\u5e94\u7528\u4e3b\u52a8\u67e5\u8be2\u7ba1\u7406\u548c\u67e5\u8be2\u91cd\u6784\u7b49\u65b9\u6cd5\u3002", "result": "\u6df1\u5165\u63a2\u8ba8\u4e86LLM\u5728\u5bf9\u8bdd\u641c\u7d22\u7cfb\u7edf\u4e2d\u7684\u67e5\u8be2\u7406\u89e3\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u65e8\u5728\u52a0\u6df1\u5bf9\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u67e5\u8be2\u7406\u89e3\u7684\u8ba4\u8bc6\uff0c\u5e76\u63a8\u52a8\u8be5\u9886\u57df\u7684\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2504.06393", "pdf": "https://arxiv.org/pdf/2504.06393", "abs": "https://arxiv.org/abs/2504.06393", "authors": ["Rebecca M. M. Hicke", "Sil Hamilton", "David Mimno"], "title": "The Zero Body Problem: Probing LLM Use of Sensory Language", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sensory language expresses embodied experiences ranging from taste and sound\nto excitement and stomachache. This language is of interest to scholars from a\nwide range of domains including robotics, narratology, linguistics, and\ncognitive science. In this work, we explore whether language models, which are\nnot embodied, can approximate human use of embodied language. We extend an\nexisting corpus of parallel human and model responses to short story prompts\nwith an additional 18,000 stories generated by 18 popular models. We find that\nall models generate stories that differ significantly from human usage of\nsensory language, but the direction of these differences varies considerably\nbetween model families. Namely, Gemini models use significantly more sensory\nlanguage than humans along most axes whereas most models from the remaining\nfive families use significantly less. Linear probes run on five models suggest\nthat they are capable of identifying sensory language. However, we find\npreliminary evidence suggesting that instruction tuning may discourage usage of\nsensory language. Finally, to support further work, we release our expanded\nstory dataset.", "AI": {"task": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u8fd1\u4f3c\u4eba\u7c7b\u4f7f\u7528\u5177\u8eab\u8bed\u8a00\u7684\u80fd\u529b\u3002", "motivation": "\u5177\u8eab\u8bed\u8a00\uff08\u5982\u611f\u5b98\u8bed\u8a00\uff09\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u673a\u5668\u4eba\u5b66\u3001\u53d9\u4e8b\u5b66\u3001\u8bed\u8a00\u5b66\u3001\u8ba4\u77e5\u79d1\u5b66\uff09\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u5177\u8eab\u4f53\u9a8c\u3002", "method": "\u6269\u5c55\u73b0\u6709\u7684\u4eba\u7c7b\u4e0e\u6a21\u578b\u751f\u6210\u6545\u4e8b\u7684\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u65b0\u589e18,000\u4e2a\u753118\u4e2a\u6d41\u884c\u6a21\u578b\u751f\u6210\u7684\u6545\u4e8b\uff0c\u5e76\u5206\u6790\u5176\u611f\u5b98\u8bed\u8a00\u4f7f\u7528\u5dee\u5f02\u3002", "result": "\u6240\u6709\u6a21\u578b\u751f\u6210\u7684\u6545\u4e8b\u5728\u611f\u5b98\u8bed\u8a00\u4f7f\u7528\u4e0a\u4e0e\u4eba\u7c7b\u663e\u8457\u4e0d\u540c\uff0c\u4f46\u5dee\u5f02\u65b9\u5411\u56e0\u6a21\u578b\u5bb6\u65cf\u800c\u5f02\uff1bGemini\u6a21\u578b\u4f7f\u7528\u66f4\u591a\u611f\u5b98\u8bed\u8a00\uff0c\u5176\u4ed6\u4e94\u7c7b\u6a21\u578b\u4f7f\u7528\u8f83\u5c11\u3002\u7ebf\u6027\u63a2\u6d4b\u8868\u660e\u6a21\u578b\u80fd\u8bc6\u522b\u611f\u5b98\u8bed\u8a00\uff0c\u4f46\u6307\u4ee4\u8c03\u4f18\u53ef\u80fd\u6291\u5236\u5176\u4f7f\u7528\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u611f\u5b98\u8bed\u8a00\u4f7f\u7528\u4e0a\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u6307\u4ee4\u8c03\u4f18\u7684\u5f71\u54cd\uff0c\u5e76\u5229\u7528\u6269\u5c55\u6570\u636e\u96c6\u652f\u6301\u76f8\u5173\u5de5\u4f5c\u3002"}}
{"id": "2504.06426", "pdf": "https://arxiv.org/pdf/2504.06426", "abs": "https://arxiv.org/abs/2504.06426", "authors": ["Hanqing Zeng", "Yinglong Xia", "Zhuokai Zhao", "Gilbert Jiang", "Qiang Zhang", "Jiayi Liu", "Lizhu Zhang", "Xiangjun Fan", "Benyu Zhang"], "title": "S'MoRE: Structural Mixture of Residual Experts for LLM Fine-tuning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Fine-tuning pre-trained large language models (LLMs) presents a dual\nchallenge of balancing parameter efficiency and model capacity. Existing\nmethods like low-rank adaptations (LoRA) are efficient but lack flexibility,\nwhile Mixture-of-Experts (MoE) architectures enhance model capacity at the cost\nof more & under-utilized parameters. To address these limitations, we propose\nStructural Mixture of Residual Experts (S'MoRE), a novel framework that\nseamlessly integrates the efficiency of LoRA with the flexibility of MoE.\nSpecifically, S'MoRE employs hierarchical low-rank decomposition of expert\nweights, yielding residuals of varying orders interconnected in a multi-layer\nstructure. By routing input tokens through sub-trees of residuals, S'MoRE\nemulates the capacity of many experts by instantiating and assembling just a\nfew low-rank matrices. We craft the inter-layer propagation of S'MoRE's\nresiduals as a special type of Graph Neural Network (GNN), and prove that under\nsimilar parameter budget, S'MoRE improves \"structural flexibility\" of\ntraditional MoE (or Mixture-of-LoRA) by exponential order. Comprehensive\ntheoretical analysis and empirical results demonstrate that S'MoRE achieves\nsuperior fine-tuning performance, offering a transformative approach for\nefficient LLM adaptation.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aS'MoRE\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u5e73\u8861\u53c2\u6570\u6548\u7387\u548c\u6a21\u578b\u5bb9\u91cf\uff0c\u7528\u4e8e\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982LoRA\u6548\u7387\u9ad8\u4f46\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u800cMoE\u67b6\u6784\u867d\u589e\u5f3a\u6a21\u578b\u5bb9\u91cf\u5374\u5bfc\u81f4\u53c2\u6570\u5197\u4f59\u548c\u5229\u7528\u7387\u4f4e\u3002", "method": "S'MoRE\u901a\u8fc7\u5206\u5c42\u4f4e\u79e9\u5206\u89e3\u4e13\u5bb6\u6743\u91cd\uff0c\u6784\u5efa\u591a\u5c42\u7ea7\u6b8b\u5dee\u7ed3\u6784\uff0c\u5e76\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5b9e\u73b0\u6b8b\u5dee\u4f20\u64ad\u3002", "result": "\u5728\u76f8\u540c\u53c2\u6570\u9884\u7b97\u4e0b\uff0cS'MoRE\u663e\u8457\u63d0\u5347\u4e86\u4f20\u7edfMoE\u6216Mixture-of-LoRA\u7684\u7ed3\u6784\u7075\u6d3b\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5fae\u8c03\u6027\u80fd\u3002", "conclusion": "S'MoRE\u4e3a\u9ad8\u6548\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.06292", "pdf": "https://arxiv.org/pdf/2504.06292", "abs": "https://arxiv.org/abs/2504.06292", "authors": ["Hongbin Liang", "Hezhe Qiao", "Wei Huang", "Qizhou Wang", "Mingsheng Shang", "Lin Chen"], "title": "Temporal-contextual Event Learning for Pedestrian Crossing Intent Prediction", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted in ICONIP2024", "summary": "Ensuring the safety of vulnerable road users through accurate prediction of\npedestrian crossing intention (PCI) plays a crucial role in the context of\nautonomous and assisted driving. Analyzing the set of observation video frames\nin ego-view has been widely used in most PCI prediction methods to forecast the\ncross intent. However, they struggle to capture the critical events related to\npedestrian behaviour along the temporal dimension due to the high redundancy of\nthe video frames, which results in the sub-optimal performance of PCI\nprediction. Our research addresses the challenge by introducing a novel\napproach called \\underline{T}emporal-\\underline{c}ontextual Event\n\\underline{L}earning (TCL). The TCL is composed of the Temporal Merging Module\n(TMM), which aims to manage the redundancy by clustering the observed video\nframes into multiple key temporal events. Then, the Contextual Attention Block\n(CAB) is employed to adaptively aggregate multiple event features along with\nvisual and non-visual data. By synthesizing the temporal feature extraction and\ncontextual attention on the key information across the critical events, TCL can\nlearn expressive representation for the PCI prediction. Extensive experiments\nare carried out on three widely adopted datasets, including PIE, JAAD-beh, and\nJAAD-all. The results show that TCL substantially surpasses the\nstate-of-the-art methods. Our code can be accessed at\nhttps://github.com/dadaguailhb/TCL.", "AI": {"task": "\u901a\u8fc7\u51c6\u786e\u9884\u6d4b\u884c\u4eba\u8fc7\u8857\u610f\u56fe\uff08PCI\uff09\u6765\u786e\u4fdd\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\u7684\u5b89\u5168\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5206\u6790\u89c6\u9891\u5e27\u65f6\u96be\u4ee5\u6355\u6349\u4e0e\u884c\u4eba\u884c\u4e3a\u76f8\u5173\u7684\u5173\u952e\u65f6\u95f4\u4e8b\u4ef6\uff0c\u5bfc\u81f4PCI\u9884\u6d4b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTemporal-contextual Event Learning\uff08TCL\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u5305\u62ecTemporal Merging Module\uff08TMM\uff09\u548cContextual Attention Block\uff08CAB\uff09\u3002", "result": "\u5728PIE\u3001JAAD-beh\u548cJAAD-all\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTCL\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TCL\u901a\u8fc7\u5408\u6210\u65f6\u95f4\u7279\u5f81\u63d0\u53d6\u548c\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\uff0c\u80fd\u591f\u5b66\u4e60\u5230\u66f4\u5177\u8868\u8fbe\u529b\u7684PCI\u9884\u6d4b\u8868\u793a\u3002"}}
{"id": "2504.06436", "pdf": "https://arxiv.org/pdf/2504.06436", "abs": "https://arxiv.org/abs/2504.06436", "authors": ["Dogus Yuksel", "Mehmet Cem Catalbas", "Bora Oc"], "title": "Language-Dependent Political Bias in AI: A Study of ChatGPT and Gemini", "categories": ["cs.CL", "cs.AI", "cs.ET", "stat.AP"], "comment": "26 pages, 10 figures", "summary": "As leading examples of large language models, ChatGPT and Gemini claim to\nprovide accurate and unbiased information, emphasizing their commitment to\npolitical neutrality and avoidance of personal bias. This research investigates\nthe political tendency of large language models and the existence of\ndifferentiation according to the query language. For this purpose, ChatGPT and\nGemini were subjected to a political axis test using 14 different languages.\nThe findings of the study suggest that these large language models do exhibit\npolitical tendencies, with both models demonstrating liberal and leftist\nbiases. A comparative analysis revealed that Gemini exhibited a more pronounced\nliberal and left-wing tendency compared to ChatGPT. The study also found that\nthese political biases varied depending on the language used for inquiry. The\nstudy delves into the factors that constitute political tendencies and\nlinguistic differentiation, exploring differences in the sources and scope of\neducational data, structural and grammatical features of languages, cultural\nand political contexts, and the model's response to linguistic features. From\nthis standpoint, and an ethical perspective, it is proposed that artificial\nintelligence tools should refrain from asserting a lack of political tendencies\nand neutrality, instead striving for political neutrality and executing user\nqueries by incorporating these tendencies.", "AI": {"task": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT\u548cGemini\uff09\u7684\u653f\u6cbb\u503e\u5411\u53ca\u5176\u5728\u4e0d\u540c\u67e5\u8be2\u8bed\u8a00\u4e2d\u7684\u5dee\u5f02\u3002", "motivation": "\u9a8c\u8bc1\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u5982\u58f0\u79f0\u7684\u90a3\u6837\u4fdd\u6301\u653f\u6cbb\u4e2d\u7acb\u548c\u65e0\u504f\u89c1\u3002", "method": "\u5bf9ChatGPT\u548cGemini\u8fdb\u884c\u653f\u6cbb\u503e\u5411\u6d4b\u8bd5\uff0c\u6db5\u76d614\u79cd\u4e0d\u540c\u8bed\u8a00\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u81ea\u7531\u4e3b\u4e49\u548c\u5de6\u7ffc\u504f\u89c1\uff0c\u4e14Gemini\u7684\u503e\u5411\u66f4\u660e\u663e\uff1b\u653f\u6cbb\u504f\u89c1\u56e0\u67e5\u8be2\u8bed\u8a00\u800c\u5f02\u3002", "conclusion": "\u5efa\u8baeAI\u5de5\u5177\u4e0d\u5e94\u5ba3\u79f0\u65e0\u653f\u6cbb\u503e\u5411\uff0c\u800c\u5e94\u52aa\u529b\u5b9e\u73b0\u653f\u6cbb\u4e2d\u7acb\uff0c\u5e76\u5728\u7528\u6237\u67e5\u8be2\u4e2d\u8003\u8651\u8fd9\u4e9b\u503e\u5411\u3002"}}
{"id": "2504.06298", "pdf": "https://arxiv.org/pdf/2504.06298", "abs": "https://arxiv.org/abs/2504.06298", "authors": ["Ben Crulis", "Cyril De Runz", "Barthelemy Serres", "Gilles Venturini"], "title": "Ternarization of Vision Language Models for use on edge devices", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We propose a process to compress a pre-trained Vision Language Model into a\nternary version of itself instead of training a ternary model from scratch. A\nnew initialization scheme from pre-trained weights based on the k-means\nalgorithm is proposed to reduce the ternarization time. We implement different\ncustom operators for executing the ternary model on the TensorFlow Lite Engine.\nWe compare the original model with its ternary and binary versions in terms of\nmemory consumption, inference speed and perplexity. We find that the ternary\nmodel using our custom ternary matrix multiplication operator provides a good\ncompromise in term of memory usage and perplexity, while having the fastest\ntoken generation speed.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u5c06\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u4e3a\u4e09\u5143\u7248\u672c\u7684\u8fc7\u7a0b\u3002", "motivation": "\u51cf\u5c11\u4ece\u5934\u8bad\u7ec3\u4e09\u5143\u6a21\u578b\u7684\u65f6\u95f4\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u6743\u91cd\u521d\u59cb\u5316\u65b9\u6848\u4f18\u5316\u538b\u7f29\u6548\u7387\u3002", "method": "\u57fa\u4e8ek-means\u7b97\u6cd5\u7684\u65b0\u521d\u59cb\u5316\u65b9\u6848\uff0c\u5e76\u5b9e\u73b0\u81ea\u5b9a\u4e49\u8fd0\u7b97\u7b26\u4ee5\u5728TensorFlow Lite\u5f15\u64ce\u4e0a\u8fd0\u884c\u4e09\u5143\u6a21\u578b\u3002", "result": "\u4e09\u5143\u6a21\u578b\u5728\u5185\u5b58\u5360\u7528\u548c\u56f0\u60d1\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u540c\u65f6\u5177\u6709\u6700\u5feb\u7684\u6807\u8bb0\u751f\u6210\u901f\u5ea6\u3002", "conclusion": "\u4e09\u5143\u6a21\u578b\u901a\u8fc7\u81ea\u5b9a\u4e49\u77e9\u9635\u4e58\u6cd5\u8fd0\u7b97\u7b26\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u6298\u8877\u65b9\u6848\u3002"}}
{"id": "2504.06438", "pdf": "https://arxiv.org/pdf/2504.06438", "abs": "https://arxiv.org/abs/2504.06438", "authors": ["Yuehan Qin", "Shawn Li", "Yi Nian", "Xinyan Velocity Yu", "Yue Zhao", "Xuezhe Ma"], "title": "Don't Let It Hallucinate: Premise Verification via Retrieval-Augmented Logical Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown substantial capacity for generating\nfluent, contextually appropriate responses. However, they can produce\nhallucinated outputs, especially when a user query includes one or more false\npremises-claims that contradict established facts. Such premises can mislead\nLLMs into offering fabricated or misleading details. Existing approaches\ninclude pretraining, fine-tuning, and inference-time techniques that often rely\non access to logits or address hallucinations after they occur. These methods\ntend to be computationally expensive, require extensive training data, or lack\nproactive mechanisms to prevent hallucination before generation, limiting their\nefficiency in real-time applications. We propose a retrieval-based framework\nthat identifies and addresses false premises before generation. Our method\nfirst transforms a user's query into a logical representation, then applies\nretrieval-augmented generation (RAG) to assess the validity of each premise\nusing factual sources. Finally, we incorporate the verification results into\nthe LLM's prompt to maintain factual consistency in the final output.\nExperiments show that this approach effectively reduces hallucinations,\nimproves factual accuracy, and does not require access to model logits or\nlarge-scale fine-tuning.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u751f\u6210\u524d\u8bc6\u522b\u548c\u5904\u7406\u7528\u6237\u67e5\u8be2\u4e2d\u7684\u9519\u8bef\u524d\u63d0\u4ee5\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7b\u89c9\u8f93\u51fa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u63a8\u7406\u65f6\u6280\u672f\uff09\u901a\u5e38\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6216\u7f3a\u4e4f\u4e3b\u52a8\u9884\u9632\u673a\u5236\uff0c\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u5c06\u7528\u6237\u67e5\u8be2\u8f6c\u5316\u4e3a\u903b\u8f91\u8868\u793a\uff0c\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u9a8c\u8bc1\u524d\u63d0\u7684\u771f\u5b9e\u6027\uff0c\u5e76\u5c06\u9a8c\u8bc1\u7ed3\u679c\u878d\u5165\u63d0\u793a\u4e2d\u4ee5\u4fdd\u6301\u8f93\u51fa\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u3001\u63d0\u9ad8\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u4e14\u65e0\u9700\u8bbf\u95ee\u6a21\u578blogits\u6216\u5927\u89c4\u6a21\u5fae\u8c03\u3002", "conclusion": "\u63d0\u51fa\u7684\u68c0\u7d22\u6846\u67b6\u4e3a\u51cf\u5c11LLMs\u5e7b\u89c9\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5b9e\u65f6\u4e14\u65e0\u9700\u590d\u6742\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.06313", "pdf": "https://arxiv.org/pdf/2504.06313", "abs": "https://arxiv.org/abs/2504.06313", "authors": ["Abdulkareem Alsudais"], "title": "Analyzing How Text-to-Image Models Represent Nationalities in Everyday Tasks", "categories": ["cs.CV", "cs.CY"], "comment": null, "summary": "The primary objective of this paper is to investigate how a popular\nText-to-Image (T2I) model represents people from 208 different nationalities\nwhen prompted to generate images of individuals performing typical everyday\ntasks. Two scenarios were developed, and images were generated based on input\nprompts that specified nationalities. The results show that in one scenario,\nthe majority of images, and in the other, a substantial portion, depict\nindividuals wearing traditional attire. This suggests that the model emphasizes\nsuch characteristics even when they are impractical for the given task. A\nstatistically significant relationship was observed between this representation\npattern and the regions associated with the specified countries. This indicates\nthat the issue disproportionately affects certain areas, particularly the\nMiddle East & North Africa and Sub-Saharan Africa. A notable association with\nincome groups was also found. CLIP was used to measure alignment scores between\ngenerated images and various prompts and captions. The findings indicate\nstatistically significant higher scores for images featuring individuals in\ntraditional attire in one scenario. The study also examined revised prompts\n(additional contextual information automatically added to the original input\nprompts) to assess their potential influence on how individuals are represented\nin the generated images, finding that the word \"traditional\" was commonly added\nto revised prompts. These findings provide valuable insights into how T2I\nmodels represent individuals from various countries and highlight potential\nareas for improvement in future models.", "AI": {"task": "\u7814\u7a76\u4e00\u4e2a\u6d41\u884c\u7684\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5982\u4f55\u4ee3\u8868208\u4e2a\u4e0d\u540c\u56fd\u7c4d\u7684\u4eba\u5728\u751f\u6210\u65e5\u5e38\u4efb\u52a1\u56fe\u50cf\u65f6\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u8ba8T2I\u6a21\u578b\u5728\u751f\u6210\u56fe\u50cf\u65f6\u662f\u5426\u8fc7\u5ea6\u5f3a\u8c03\u4f20\u7edf\u670d\u9970\u7b49\u7279\u5f81\uff0c\u4ee5\u53ca\u8fd9\u79cd\u8868\u73b0\u662f\u5426\u4e0e\u5730\u533a\u6216\u6536\u5165\u7fa4\u4f53\u76f8\u5173\u3002", "method": "\u5f00\u53d1\u4e24\u79cd\u573a\u666f\uff0c\u57fa\u4e8e\u8f93\u5165\u63d0\u793a\u751f\u6210\u56fe\u50cf\uff0c\u5e76\u4f7f\u7528CLIP\u6d4b\u91cf\u751f\u6210\u56fe\u50cf\u4e0e\u63d0\u793a\u7684\u5bf9\u9f50\u5206\u6570\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u751f\u6210\u56fe\u50cf\u65f6\u503e\u5411\u4e8e\u5f3a\u8c03\u4f20\u7edf\u670d\u9970\uff0c\u4e14\u8fd9\u79cd\u8868\u73b0\u4e0e\u7279\u5b9a\u5730\u533a\uff08\u5982\u4e2d\u4e1c\u3001\u5317\u975e\u548c\u6492\u54c8\u62c9\u4ee5\u5357\u975e\u6d32\uff09\u548c\u6536\u5165\u7fa4\u4f53\u663e\u8457\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86T2I\u6a21\u578b\u5728\u4ee3\u8868\u4e0d\u540c\u56fd\u7c4d\u4e2a\u4f53\u65f6\u7684\u6f5c\u5728\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6a21\u578b\u6539\u8fdb\u7684\u65b9\u5411\u3002"}}
{"id": "2504.06460", "pdf": "https://arxiv.org/pdf/2504.06460", "abs": "https://arxiv.org/abs/2504.06460", "authors": ["Sai Adith Senthil Kumar", "Hao Yan", "Saipavan Perepa", "Murong Yue", "Ziyu Yao"], "title": "Can LLMs Simulate Personas with Reversed Performance? A Benchmark for Counterfactual Instruction Following", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are now increasingly widely used to simulate\npersonas in virtual environments, leveraging their instruction-following\ncapability. However, we discovered that even state-of-the-art LLMs cannot\nsimulate personas with reversed performance (e.g., student personas with low\nproficiency in educational settings), which impairs the simulation diversity\nand limits the practical applications of the simulated environments. In this\nwork, using mathematical reasoning as a representative scenario, we propose the\nfirst benchmark dataset for evaluating LLMs on simulating personas with\nreversed performance, a capability that we dub \"counterfactual instruction\nfollowing\". We evaluate both open-weight and closed-source LLMs on this task\nand find that LLMs, including the OpenAI o1 reasoning model, all struggle to\nfollow counterfactual instructions for simulating reversedly performing\npersonas. Intersectionally simulating both the performance level and the race\npopulation of a persona worsens the effect even further. These results\nhighlight the challenges of counterfactual instruction following and the need\nfor further research.", "AI": {"task": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6a21\u62df\u53cd\u5411\u6027\u80fd\u4eba\u7269\uff08\u5982\u4f4e\u719f\u7ec3\u5ea6\u5b66\u751f\uff09\u65f6\u7684\u80fd\u529b\u3002", "motivation": "\u53d1\u73b0\u73b0\u6709LLMs\u65e0\u6cd5\u6a21\u62df\u53cd\u5411\u6027\u80fd\u4eba\u7269\uff0c\u9650\u5236\u4e86\u865a\u62df\u73af\u5883\u7684\u591a\u6837\u6027\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u9996\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4ee5\u6570\u5b66\u63a8\u7406\u4e3a\u4ee3\u8868\u573a\u666f\uff0c\u8bc4\u4f30LLMs\u5728\u201c\u53cd\u4e8b\u5b9e\u6307\u4ee4\u9075\u5faa\u201d\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5305\u62ecOpenAI o1\u5728\u5185\u7684LLMs\u5747\u96be\u4ee5\u6a21\u62df\u53cd\u5411\u6027\u80fd\u4eba\u7269\uff0c\u4e14\u6a21\u62df\u4eba\u7269\u79cd\u65cf\u548c\u6027\u80fd\u6c34\u5e73\u65f6\u6548\u679c\u66f4\u5dee\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u6307\u4ee4\u9075\u5faa\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.06330", "pdf": "https://arxiv.org/pdf/2504.06330", "abs": "https://arxiv.org/abs/2504.06330", "authors": ["Hicham Talaoubrid", "Anissa Mokraoui", "Ismail Ben Ayed", "Axel Prouvost", "Sonimith Hang", "Monit Korn", "R\u00e9mi Harvey"], "title": "Analyzing the Impact of Low-Rank Adaptation for Cross-Domain Few-Shot Object Detection in Aerial Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper investigates the application of Low-Rank Adaptation (LoRA) to\nsmall models for cross-domain few-shot object detection in aerial images.\nOriginally designed for large-scale models, LoRA helps mitigate overfitting,\nmaking it a promising approach for resource-constrained settings. We integrate\nLoRA into DiffusionDet, and evaluate its performance on the DOTA and DIOR\ndatasets. Our results show that LoRA applied after an initial fine-tuning\nslightly improves performance in low-shot settings (e.g., 1-shot and 5-shot),\nwhile full fine-tuning remains more effective in higher-shot configurations.\nThese findings highlight LoRA's potential for efficient adaptation in aerial\nobject detection, encouraging further research into parameter-efficient\nfine-tuning strategies for few-shot learning. Our code is available here:\nhttps://github.com/HichTala/LoRA-DiffusionDet.", "AI": {"task": "\u7814\u7a76\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5728\u5c0f\u6a21\u578b\u4e0a\u7684\u5e94\u7528\uff0c\u7528\u4e8e\u8de8\u57df\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u3002", "motivation": "LoRA\u6700\u521d\u8bbe\u8ba1\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u80fd\u7f13\u89e3\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002", "method": "\u5c06LoRA\u96c6\u6210\u5230DiffusionDet\u4e2d\uff0c\u5e76\u5728DOTA\u548cDIOR\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\uff08\u59821-shot\u548c5-shot\uff09\u4e2d\uff0cLoRA\u7565\u5fae\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5728\u9ad8\u6837\u672c\u914d\u7f6e\u4e2d\uff0c\u5b8c\u6574\u5fae\u8c03\u66f4\u6709\u6548\u3002", "conclusion": "LoRA\u5728\u822a\u7a7a\u76ee\u6807\u68c0\u6d4b\u4e2d\u5177\u6709\u9ad8\u6548\u9002\u5e94\u7684\u6f5c\u529b\uff0c\u9f13\u52b1\u8fdb\u4e00\u6b65\u7814\u7a76\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\u3002"}}
{"id": "2504.06465", "pdf": "https://arxiv.org/pdf/2504.06465", "abs": "https://arxiv.org/abs/2504.06465", "authors": ["Ye", "Ma"], "title": "Analyzing Examinee Comments using DistilBERT and Machine Learning to Ensure Quality Control in Exam Content", "categories": ["cs.CL"], "comment": null, "summary": "This study explores using Natural Language Processing (NLP) to analyze\ncandidate comments for identifying problematic test items. We developed and\nvalidated machine learning models that automatically identify relevant negative\nfeedback, evaluated approaches of incorporating psychometric features enhances\nmodel performance, and compared NLP-flagged items with traditionally flagged\nitems. Results demonstrate that candidate feedback provides valuable\ncomplementary information to statistical methods, potentially improving test\nvalidity while reducing manual review burden. This research offers testing\norganizations an efficient mechanism to incorporate direct candidate experience\ninto quality assurance processes.", "AI": {"task": "\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u5206\u6790\u8003\u751f\u8bc4\u8bba\u4ee5\u8bc6\u522b\u6709\u95ee\u9898\u7684\u6d4b\u8bd5\u9898\u76ee\u3002", "motivation": "\u901a\u8fc7\u81ea\u52a8\u8bc6\u522b\u8d1f\u9762\u53cd\u9988\uff0c\u8865\u5145\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\uff0c\u63d0\u9ad8\u6d4b\u8bd5\u6548\u5ea6\u5e76\u51cf\u5c11\u4eba\u5de5\u5ba1\u6838\u8d1f\u62c5\u3002", "method": "\u5f00\u53d1\u548c\u9a8c\u8bc1\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u5fc3\u7406\u6d4b\u91cf\u7279\u5f81\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u6bd4\u8f83NLP\u6807\u8bb0\u4e0e\u4f20\u7edf\u6807\u8bb0\u7684\u9898\u76ee\u3002", "result": "\u8003\u751f\u53cd\u9988\u4e3a\u7edf\u8ba1\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8865\u5145\u4fe1\u606f\uff0c\u53ef\u80fd\u63d0\u5347\u6d4b\u8bd5\u6548\u5ea6\u5e76\u51cf\u5c11\u4eba\u5de5\u5ba1\u6838\u8d1f\u62c5\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6d4b\u8bd5\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u673a\u5236\uff0c\u5c06\u8003\u751f\u76f4\u63a5\u4f53\u9a8c\u7eb3\u5165\u8d28\u91cf\u4fdd\u8bc1\u6d41\u7a0b\u3002"}}
{"id": "2504.06357", "pdf": "https://arxiv.org/pdf/2504.06357", "abs": "https://arxiv.org/abs/2504.06357", "authors": ["Vladimir Golovkin", "Nikolay Nemtsev", "Vasyl Shandyba", "Oleg Udin", "Nikita Kasatkin", "Pavel Kononov", "Anton Afanasiev", "Sergey Ulasen", "Andrei Boiarov"], "title": "From Broadcast to Minimap: Achieving State-of-the-Art SoccerNet Game State Reconstruction", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for presentation at the CVPR 2025 CVsports Workshop", "summary": "Game State Reconstruction (GSR), a critical task in Sports Video\nUnderstanding, involves precise tracking and localization of all individuals on\nthe football field-players, goalkeepers, referees, and others - in real-world\ncoordinates. This capability enables coaches and analysts to derive actionable\ninsights into player movements, team formations, and game dynamics, ultimately\noptimizing training strategies and enhancing competitive advantage. Achieving\naccurate GSR using a single-camera setup is highly challenging due to frequent\ncamera movements, occlusions, and dynamic scene content. In this work, we\npresent a robust end-to-end pipeline for tracking players across an entire\nmatch using a single-camera setup. Our solution integrates a fine-tuned YOLOv5m\nfor object detection, a SegFormer-based camera parameter estimator, and a\nDeepSORT-based tracking framework enhanced with re-identification, orientation\nprediction, and jersey number recognition. By ensuring both spatial accuracy\nand temporal consistency, our method delivers state-of-the-art game state\nreconstruction, securing first place in the SoccerNet Game State Reconstruction\nChallenge 2024 and significantly outperforming competing methods.", "AI": {"task": "\u901a\u8fc7\u5355\u6444\u50cf\u5934\u8bbe\u7f6e\u5b9e\u73b0\u8db3\u7403\u6bd4\u8d5b\u4e2d\u6240\u6709\u4e2a\u4f53\u7684\u7cbe\u786e\u8ddf\u8e2a\u548c\u5b9a\u4f4d\uff08\u6e38\u620f\u72b6\u6001\u91cd\u5efa\uff0cGSR\uff09\u3002", "motivation": "\u4e3a\u6559\u7ec3\u548c\u5206\u6790\u5e08\u63d0\u4f9b\u7403\u5458\u79fb\u52a8\u3001\u961f\u5f62\u548c\u6bd4\u8d5b\u52a8\u6001\u7684\u53ef\u64cd\u4f5c\u89c1\u89e3\uff0c\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\u5e76\u589e\u5f3a\u7ade\u4e89\u4f18\u52bf\u3002", "method": "\u7ed3\u5408\u4e86\u5fae\u8c03\u7684YOLOv5m\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u3001\u57fa\u4e8eSegFormer\u7684\u76f8\u673a\u53c2\u6570\u4f30\u8ba1\u5668\uff0c\u4ee5\u53ca\u589e\u5f3a\u7684DeepSORT\u8ddf\u8e2a\u6846\u67b6\uff08\u5305\u62ec\u91cd\u8bc6\u522b\u3001\u65b9\u5411\u9884\u6d4b\u548c\u7403\u8863\u53f7\u7801\u8bc6\u522b\uff09\u3002", "result": "\u5728SoccerNet Game State Reconstruction Challenge 2024\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u7aef\u5230\u7aef\u7ba1\u9053\u5728\u5355\u6444\u50cf\u5934\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6e38\u620f\u72b6\u6001\u91cd\u5efa\uff0c\u5177\u6709\u7a7a\u95f4\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2504.06529", "pdf": "https://arxiv.org/pdf/2504.06529", "abs": "https://arxiv.org/abs/2504.06529", "authors": ["Khai Phan Tran", "Xue Li"], "title": "CDER: Collaborative Evidence Retrieval for Document-level Relation Extraction", "categories": ["cs.CL"], "comment": "Published at ACIIDS 2024", "summary": "Document-level Relation Extraction (DocRE) involves identifying relations\nbetween entities across multiple sentences in a document. Evidence sentences,\ncrucial for precise entity pair relationships identification, enhance focus on\nessential text segments, improving DocRE performance. However, existing\nevidence retrieval systems often overlook the collaborative nature among\nsemantically similar entity pairs in the same document, hindering the\neffectiveness of the evidence retrieval task. To address this, we propose a\nnovel evidence retrieval framework, namely CDER. CDER employs an attentional\ngraph-based architecture to capture collaborative patterns and incorporates a\ndynamic sub-structure for additional robustness in evidence retrieval.\nExperimental results on the benchmark DocRE dataset show that CDER not only\nexcels in the evidence retrieval task but also enhances overall performance of\nexisting DocRE system.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8bc1\u636e\u68c0\u7d22\u6846\u67b6CDER\uff0c\u7528\u4e8e\u6587\u6863\u7ea7\u5173\u7cfb\u62bd\u53d6\uff08DocRE\uff09\u4e2d\u7684\u8bc1\u636e\u68c0\u7d22\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u8bc1\u636e\u68c0\u7d22\u7cfb\u7edf\u5ffd\u7565\u4e86\u540c\u4e00\u6587\u6863\u4e2d\u8bed\u4e49\u76f8\u4f3c\u5b9e\u4f53\u5bf9\u4e4b\u95f4\u7684\u534f\u4f5c\u6027\uff0c\u5f71\u54cd\u4e86\u8bc1\u636e\u68c0\u7d22\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u56fe\u7ed3\u6784\u67b6\u6784\u6355\u6349\u534f\u4f5c\u6a21\u5f0f\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u5b50\u7ed3\u6784\u4ee5\u589e\u5f3a\u8bc1\u636e\u68c0\u7d22\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u57fa\u51c6DocRE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCDER\u5728\u8bc1\u636e\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u63d0\u5347\u4e86\u73b0\u6709DocRE\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "CDER\u901a\u8fc7\u6355\u6349\u534f\u4f5c\u6a21\u5f0f\u548c\u5f15\u5165\u52a8\u6001\u5b50\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc1\u636e\u68c0\u7d22\u548cDocRE\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2504.06358", "pdf": "https://arxiv.org/pdf/2504.06358", "abs": "https://arxiv.org/abs/2504.06358", "authors": ["Yupeng Cheng", "Zi Pong Lim", "Sarthak Ketanbhai Modi", "Yon Shin Teo", "Yushi Cao", "Shang-Wei Lin"], "title": "Towards Calibration Enhanced Network by Inverse Adversarial Attack", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Test automation has become increasingly important as the complexity of both\ndesign and content in Human Machine Interface (HMI) software continues to grow.\nCurrent standard practice uses Optical Character Recognition (OCR) techniques\nto automatically extract textual information from HMI screens for validation.\nAt present, one of the key challenges faced during the automation of HMI screen\nvalidation is the noise handling for the OCR models. In this paper, we propose\nto utilize adversarial training techniques to enhance OCR models in HMI testing\nscenarios. More specifically, we design a new adversarial attack objective for\nOCR models to discover the decision boundaries in the context of HMI testing.\nWe then adopt adversarial training to optimize the decision boundaries towards\na more robust and accurate OCR model. In addition, we also built an HMI screen\ndataset based on real-world requirements and applied multiple types of\nperturbation onto the clean HMI dataset to provide a more complete coverage for\nthe potential scenarios. We conduct experiments to demonstrate how using\nadversarial training techniques yields more robust OCR models against various\nkinds of noises, while still maintaining high OCR model accuracy. Further\nexperiments even demonstrate that the adversarial training models exhibit a\ncertain degree of robustness against perturbations from other patterns.", "AI": {"task": "\u5229\u7528\u5bf9\u6297\u8bad\u7ec3\u6280\u672f\u589e\u5f3aHMI\u6d4b\u8bd5\u573a\u666f\u4e2d\u7684OCR\u6a21\u578b\u3002", "motivation": "\u968f\u7740HMI\u8f6f\u4ef6\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u6d4b\u8bd5\u81ea\u52a8\u5316\u9700\u6c42\u65e5\u76ca\u91cd\u8981\uff0c\u800cOCR\u6a21\u578b\u5728\u566a\u58f0\u5904\u7406\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u65b0\u7684\u5bf9\u6297\u653b\u51fb\u76ee\u6807\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u4f18\u5316OCR\u6a21\u578b\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u540c\u65f6\u6784\u5efa\u5305\u542b\u591a\u79cd\u6270\u52a8\u7684HMI\u5c4f\u5e55\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5bf9\u6297\u8bad\u7ec3\u6280\u672f\u80fd\u63d0\u5347OCR\u6a21\u578b\u5bf9\u5404\u79cd\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\uff0c\u5e76\u5c55\u73b0\u5bf9\u5176\u4ed6\u6a21\u5f0f\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u5bf9\u6297\u8bad\u7ec3\u6280\u672f\u80fd\u6709\u6548\u589e\u5f3aOCR\u6a21\u578b\u5728HMI\u6d4b\u8bd5\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.06536", "pdf": "https://arxiv.org/pdf/2504.06536", "abs": "https://arxiv.org/abs/2504.06536", "authors": ["Happy Buzaaba", "Alexander Wettig", "David Ifeoluwa Adelani", "Christiane Fellbaum"], "title": "Lugha-Llama: Adapting Large Language Models for African Languages", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have achieved impressive results in a wide range\nof natural language applications. However, they often struggle to recognize\nlow-resource languages, in particular African languages, which are not well\nrepresented in large training corpora. In this paper, we consider how to adapt\nLLMs to low-resource African languages. We find that combining curated data\nfrom African languages with high-quality English educational texts results in a\ntraining mix that substantially improves the model's performance on these\nlanguages. On the challenging IrokoBench dataset, our models consistently\nachieve the best performance amongst similarly sized baselines, particularly on\nknowledge-intensive multiple-choice questions (AfriMMLU). Additionally, on the\ncross-lingual question answering benchmark AfriQA, our models outperform the\nbase model by over 10%. To better understand the role of English data during\ntraining, we translate a subset of 200M tokens into Swahili language and\nperform an analysis which reveals that the content of these data is primarily\nresponsible for the strong performance. We release our models and data to\nencourage future research on African languages.", "AI": {"task": "\u7814\u7a76\u5982\u4f55\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9002\u5e94\u4e8e\u4f4e\u8d44\u6e90\u7684\u975e\u6d32\u8bed\u8a00\u3002", "motivation": "\u975e\u6d32\u8bed\u8a00\u5728\u5927\u578b\u8bad\u7ec3\u8bed\u6599\u5e93\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4LLMs\u5728\u8fd9\u4e9b\u8bed\u8a00\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u975e\u6d32\u8bed\u8a00\u7684\u7cbe\u9009\u6570\u636e\u4e0e\u9ad8\u8d28\u91cf\u7684\u82f1\u8bed\u6559\u80b2\u6587\u672c\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728IrokoBench\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u7279\u522b\u662f\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u591a\u9009\u9898\uff08AfriMMLU\uff09\u4e0a\uff1b\u5728\u8de8\u8bed\u8a00\u95ee\u7b54\u57fa\u51c6AfriQA\u4e0a\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc710%\u3002", "conclusion": "\u82f1\u8bed\u6570\u636e\u7684\u5185\u5bb9\u5bf9\u6a21\u578b\u6027\u80fd\u63d0\u5347\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u7814\u7a76\u9f13\u52b1\u672a\u6765\u5bf9\u975e\u6d32\u8bed\u8a00\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2504.06389", "pdf": "https://arxiv.org/pdf/2504.06389", "abs": "https://arxiv.org/abs/2504.06389", "authors": ["Hritam Basak", "Zhaozheng Yin"], "title": "SemiDAViL: Semi-supervised Domain Adaptation with Vision-Language Guidance for Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted at CVPR 2025", "summary": "Domain Adaptation (DA) and Semi-supervised Learning (SSL) converge in\nSemi-supervised Domain Adaptation (SSDA), where the objective is to transfer\nknowledge from a source domain to a target domain using a combination of\nlimited labeled target samples and abundant unlabeled target data. Although\nintuitive, a simple amalgamation of DA and SSL is suboptimal in semantic\nsegmentation due to two major reasons: (1) previous methods, while able to\nlearn good segmentation boundaries, are prone to confuse classes with similar\nvisual appearance due to limited supervision; and (2) skewed and imbalanced\ntraining data distribution preferring source representation learning whereas\nimpeding from exploring limited information about tailed classes. Language\nguidance can serve as a pivotal semantic bridge, facilitating robust class\ndiscrimination and mitigating visual ambiguities by leveraging the rich\nsemantic relationships encoded in pre-trained language models to enhance\nfeature representations across domains. Therefore, we propose the first\nlanguage-guided SSDA setting for semantic segmentation in this work.\nSpecifically, we harness the semantic generalization capabilities inherent in\nvision-language models (VLMs) to establish a synergistic framework within the\nSSDA paradigm. To address the inherent class-imbalance challenges in\nlong-tailed distributions, we introduce class-balanced segmentation loss\nformulations that effectively regularize the learning process. Through\nextensive experimentation across diverse domain adaptation scenarios, our\napproach demonstrates substantial performance improvements over contemporary\nstate-of-the-art (SoTA) methodologies. Code is available:\n\\href{https://github.com/hritam-98/SemiDAViL}{GitHub}.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u8bed\u8a00\u5f15\u5bfc\u7684\u534a\u76d1\u7763\u57df\u9002\u5e94\uff08SSDA\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfDA\u548cSSL\u7ed3\u5408\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u4e0d\u8db3\uff0c\u5305\u62ec\u7c7b\u522b\u6df7\u6dc6\u548c\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u8bed\u4e49\u6cdb\u5316\u80fd\u529b\uff0c\u8bbe\u8ba1\u7c7b\u5e73\u8861\u5206\u5272\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u591a\u79cd\u57df\u9002\u5e94\u573a\u666f\u4e2d\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8bed\u8a00\u5f15\u5bfc\u7684SSDA\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8bed\u4e49\u5206\u5272\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2504.06560", "pdf": "https://arxiv.org/pdf/2504.06560", "abs": "https://arxiv.org/abs/2504.06560", "authors": ["Lanrui Wang", "Mingyu Zheng", "Hongyin Tang", "Zheng Lin", "Yanan Cao", "Jingang Wang", "Xunliang Cai", "Weiping Wang"], "title": "NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Processing structured tabular data, particularly lengthy tables, constitutes\na fundamental yet challenging task for large language models (LLMs). However,\nexisting long-context benchmarks primarily focus on unstructured text,\nneglecting the challenges of long and complex structured tables. To address\nthis gap, we introduce NeedleInATable (NIAT), a novel task that treats each\ntable cell as a \"needle\" and requires the model to extract the target cell\nunder different queries. Evaluation results of mainstream LLMs on this\nbenchmark show they lack robust long-table comprehension, often relying on\nsuperficial correlations or shortcuts for complex table understanding tasks,\nrevealing significant limitations in processing intricate tabular data. To this\nend, we propose a data synthesis method to enhance models' long-table\ncomprehension capabilities. Experimental results show that our synthesized\ntraining data significantly enhances LLMs' performance on the NIAT task,\noutperforming both long-context LLMs and long-table agent methods. This work\nadvances the evaluation of LLMs' genuine long-structured table comprehension\ncapabilities and paves the way for progress in long-context and table\nunderstanding applications.", "AI": {"task": "\u63d0\u51fa\u5e76\u8bc4\u4f30NeedleInATable\uff08NIAT\uff09\u4efb\u52a1\uff0c\u4ee5\u6d4b\u8bd5\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u957f\u7ed3\u6784\u5316\u8868\u683c\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u975e\u7ed3\u6784\u5316\u6587\u672c\uff0c\u5ffd\u7565\u4e86\u957f\u4e14\u590d\u6742\u7684\u7ed3\u6784\u5316\u8868\u683c\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165NIAT\u4efb\u52a1\uff0c\u5c06\u6bcf\u4e2a\u8868\u683c\u5355\u5143\u683c\u89c6\u4e3a\u201c\u9488\u201d\uff0c\u8981\u6c42\u6a21\u578b\u5728\u4e0d\u540c\u67e5\u8be2\u4e0b\u63d0\u53d6\u76ee\u6807\u5355\u5143\u683c\uff0c\u5e76\u63d0\u51fa\u6570\u636e\u5408\u6210\u65b9\u6cd5\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u957f\u8868\u683c\u7406\u89e3\u80fd\u529b\u3002", "result": "\u4e3b\u6d41LLMs\u5728NIAT\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u63d0\u51fa\u7684\u6570\u636e\u5408\u6210\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63a8\u52a8\u4e86LLMs\u5728\u957f\u7ed3\u6784\u5316\u8868\u683c\u7406\u89e3\u80fd\u529b\u4e0a\u7684\u8bc4\u4f30\uff0c\u5e76\u4e3a\u957f\u4e0a\u4e0b\u6587\u548c\u8868\u683c\u7406\u89e3\u5e94\u7528\u7684\u8fdb\u5c55\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2504.06397", "pdf": "https://arxiv.org/pdf/2504.06397", "abs": "https://arxiv.org/abs/2504.06397", "authors": ["Yufu Wang", "Yu Sun", "Priyanka Patel", "Kostas Daniilidis", "Michael J. Black", "Muhammed Kocabas"], "title": "PromptHMR: Promptable Human Mesh Recovery", "categories": ["cs.CV"], "comment": null, "summary": "Human pose and shape (HPS) estimation presents challenges in diverse\nscenarios such as crowded scenes, person-person interactions, and single-view\nreconstruction. Existing approaches lack mechanisms to incorporate auxiliary\n\"side information\" that could enhance reconstruction accuracy in such\nchallenging scenarios. Furthermore, the most accurate methods rely on cropped\nperson detections and cannot exploit scene context while methods that process\nthe whole image often fail to detect people and are less accurate than methods\nthat use crops. While recent language-based methods explore HPS reasoning\nthrough large language or vision-language models, their metric accuracy is well\nbelow the state of the art. In contrast, we present PromptHMR, a\ntransformer-based promptable method that reformulates HPS estimation through\nspatial and semantic prompts. Our method processes full images to maintain\nscene context and accepts multiple input modalities: spatial prompts like\nbounding boxes and masks, and semantic prompts like language descriptions or\ninteraction labels. PromptHMR demonstrates robust performance across\nchallenging scenarios: estimating people from bounding boxes as small as faces\nin crowded scenes, improving body shape estimation through language\ndescriptions, modeling person-person interactions, and producing temporally\ncoherent motions in videos. Experiments on benchmarks show that PromptHMR\nachieves state-of-the-art performance while offering flexible prompt-based\ncontrol over the HPS estimation process.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u53ef\u63d0\u793a\u65b9\u6cd5PromptHMR\uff0c\u7528\u4e8e\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u63d0\u5347\u4eba\u4f53\u59ff\u6001\u548c\u5f62\u72b6\uff08HPS\uff09\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5229\u7528\u8f85\u52a9\u4fe1\u606f\uff08\u5982\u7a7a\u95f4\u63d0\u793a\u548c\u8bed\u4e49\u63d0\u793a\uff09\u7684\u673a\u5236\uff0c\u4e14\u5728\u590d\u6742\u573a\u666f\uff08\u5982\u62e5\u6324\u573a\u666f\u3001\u4eba\u7269\u4ea4\u4e92\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u7a7a\u95f4\u63d0\u793a\uff08\u5982\u8fb9\u754c\u6846\u548c\u63a9\u7801\uff09\u548c\u8bed\u4e49\u63d0\u793a\uff08\u5982\u8bed\u8a00\u63cf\u8ff0\u6216\u4ea4\u4e92\u6807\u7b7e\uff09\u91cd\u65b0\u5b9a\u4e49HPS\u4f30\u8ba1\uff0c\u540c\u65f6\u5904\u7406\u5b8c\u6574\u56fe\u50cf\u4ee5\u4fdd\u7559\u573a\u666f\u4e0a\u4e0b\u6587\u3002", "result": "PromptHMR\u5728\u62e5\u6324\u573a\u666f\u3001\u4eba\u7269\u4ea4\u4e92\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u9c81\u68d2\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "PromptHMR\u901a\u8fc7\u7075\u6d3b\u7684\u63d0\u793a\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86HPS\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2504.06562", "pdf": "https://arxiv.org/pdf/2504.06562", "abs": "https://arxiv.org/abs/2504.06562", "authors": ["Longguang Zhong", "Fanqi Wan", "Ziyi Yang", "Guosheng Liang", "Tianyuan Shi", "Xiaojun Quan"], "title": "FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion", "categories": ["cs.CL"], "comment": null, "summary": "Heterogeneous model fusion enhances the performance of LLMs by integrating\nthe knowledge and capabilities of multiple structurally diverse models.\nHowever, existing approaches often rely solely on selecting the best output for\neach prompt from source models, which underutilizes their full potential due to\nlimited source knowledge and results in sparse optimization signals. To address\nthis limitation, we propose FuseRL, a novel two-stage framework comprising\nFuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT\nestablishes a robust initialization by integrating the strengths of\nheterogeneous source models through weighted supervised fine-tuning (SFT) on\ndiverse outputs for each prompt. FusePO optimizes weighted preferences based on\nthe outputs of multiple source models to enable superior alignment performance.\nExtensive experiments demonstrate the effectiveness of our framework across\nvarious preference alignment methods, including RLOO, DPO, and SimPO. Using\nLlama-3.1-8B-Instruct as the target model, our approach achieves\nstate-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard\nbenchmarks. Further analysis suggests that FuseSFT regularizes the training\nprocess to reduce overfitting, while FusePO introduces dense and diverse\nsignals for preference optimization.", "AI": {"task": "\u901a\u8fc7\u5f02\u6784\u6a21\u578b\u878d\u5408\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u4ece\u6e90\u6a21\u578b\u4e2d\u9009\u62e9\u6bcf\u4e2a\u63d0\u793a\u7684\u6700\u4f73\u8f93\u51fa\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u6f5c\u529b\uff0c\u5bfc\u81f4\u4f18\u5316\u4fe1\u53f7\u7a00\u758f\u3002", "method": "\u63d0\u51faFuseRL\u6846\u67b6\uff0c\u5305\u542bFuseSFT\u548cFusePO\u4e24\u9636\u6bb5\uff1aFuseSFT\u901a\u8fc7\u52a0\u6743\u76d1\u7763\u5fae\u8c03\u6574\u5408\u5f02\u6784\u6e90\u6a21\u578b\u7684\u4f18\u52bf\uff1bFusePO\u57fa\u4e8e\u591a\u6e90\u6a21\u578b\u8f93\u51fa\u4f18\u5316\u52a0\u6743\u504f\u597d\u3002", "result": "\u5728AlpacaEval-2\u548cArena-Hard\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528Llama-3.1-8B-Instruct\u4f5c\u4e3a\u76ee\u6807\u6a21\u578b\uff0c\u53d6\u5f97\u4e868B LLMs\u4e2d\u7684\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "FuseSFT\u51cf\u5c11\u8fc7\u62df\u5408\uff0cFusePO\u63d0\u4f9b\u5bc6\u96c6\u591a\u6837\u7684\u4f18\u5316\u4fe1\u53f7\uff0c\u5171\u540c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.06432", "pdf": "https://arxiv.org/pdf/2504.06432", "abs": "https://arxiv.org/abs/2504.06432", "authors": ["Rupayan Mallick", "Sibo Dong", "Nataniel Ruiz", "Sarah Adel Bargal"], "title": "D-Feat Occlusions: Diffusion Features for Robustness to Partial Visual Occlusions in Object Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Applications of diffusion models for visual tasks have been quite noteworthy.\nThis paper targets making classification models more robust to occlusions for\nthe task of object recognition by proposing a pipeline that utilizes a frozen\ndiffusion model. Diffusion features have demonstrated success in image\ngeneration and image completion while understanding image context. Occlusion\ncan be posed as an image completion problem by deeming the pixels of the\noccluder to be `missing.' We hypothesize that such features can help\nhallucinate object visual features behind occluding objects, and hence we\npropose using them to enable models to become more occlusion robust. We design\nexperiments to include input-based augmentations as well as feature-based\naugmentations. Input-based augmentations involve finetuning on images where the\noccluder pixels are inpainted, and feature-based augmentations involve\naugmenting classification features with intermediate diffusion features. We\ndemonstrate that our proposed use of diffusion-based features results in models\nthat are more robust to partial object occlusions for both Transformers and\nConvNets on ImageNet with simulated occlusions. We also propose a dataset that\nencompasses real-world occlusions and demonstrate that our method is more\nrobust to partial object occlusions.", "AI": {"task": "\u901a\u8fc7\u5229\u7528\u51bb\u7ed3\u7684\u6269\u6563\u6a21\u578b\uff0c\u63d0\u9ad8\u5206\u7c7b\u6a21\u578b\u5728\u7269\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2d\u5bf9\u906e\u6321\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u56fe\u50cf\u8865\u5168\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u7406\u89e3\u56fe\u50cf\u4e0a\u4e0b\u6587\uff0c\u56e0\u6b64\u53ef\u4ee5\u5c06\u5176\u7279\u5f81\u7528\u4e8e\u89e3\u51b3\u906e\u6321\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8f93\u5165\u589e\u5f3a\u548c\u7279\u5f81\u589e\u5f3a\u7684\u6d41\u7a0b\uff0c\u5305\u62ec\u5bf9\u906e\u6321\u50cf\u7d20\u8fdb\u884c\u4fee\u590d\u7684\u5fae\u8c03\u4ee5\u53ca\u5c06\u6269\u6563\u7279\u5f81\u4e0e\u5206\u7c7b\u7279\u5f81\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u906e\u6321\u7684ImageNet\u6570\u636e\u96c6\u4e0a\u5bf9Transformer\u548cConvNet\u5747\u6709\u6548\uff0c\u4e14\u5728\u771f\u5b9e\u906e\u6321\u6570\u636e\u96c6\u4e0a\u4e5f\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u7279\u5f81\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u90e8\u5206\u7269\u4f53\u906e\u6321\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.06564", "pdf": "https://arxiv.org/pdf/2504.06564", "abs": "https://arxiv.org/abs/2504.06564", "authors": ["Qingcheng Zeng", "Weihao Xuan", "Leyang Cui", "Rob Voigt"], "title": "Do Reasoning Models Show Better Verbalized Calibration?", "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "Large reasoning models (LRMs) have recently shown impressive capabilities in\ncomplex reasoning by leveraging increased test-time computation and exhibiting\nbehaviors akin to human-like deliberation. Despite these advances, it remains\nan open question whether LRMs are better calibrated - particularly in their\nverbalized confidence - compared to instruction-tuned counterparts. In this\npaper, we investigate the calibration properties of LRMs trained via supervised\nfine-tuning distillation on long reasoning traces (henceforth SFT reasoning\nmodels) and outcome-based reinforcement learning for reasoning (henceforth RL\nreasoning models) across diverse domains. Our findings reveal that LRMs\nsignificantly outperform instruction-tuned models on complex reasoning tasks in\nboth accuracy and confidence calibration. In contrast, we find surprising\ntrends in the domain of factuality in particular. On factuality tasks, while\nDeepseek-R1 shows strong calibration behavior, smaller QwQ-32B shows no\nimprovement over instruct models; moreover, SFT reasoning models display worse\ncalibration (greater overconfidence) compared to instruct models. Our results\nprovide evidence for a potentially critical role of reasoning-oriented RL\ntraining in improving LLMs' capacity for generating trustworthy, self-aware\noutputs.", "AI": {"task": "\u7814\u7a76\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6821\u51c6\u6027\u80fd\uff0c\u7279\u522b\u662f\u4e0e\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u76f8\u6bd4\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6821\u51c6\u6027\u80fd\uff08\u5c24\u5176\u662f\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff09\u662f\u5426\u4f18\u4e8e\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u84b8\u998f\uff08SFT\u63a8\u7406\u6a21\u578b\uff09\u548c\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u63a8\u7406\uff08RL\u63a8\u7406\u6a21\u578b\uff09\u8bad\u7ec3LRMs\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "LRMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff0c\u4f46\u5728\u4e8b\u5b9e\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4e00\uff0c\u90e8\u5206\u6a21\u578b\u751a\u81f3\u8868\u73b0\u66f4\u5dee\u3002", "conclusion": "\u63a8\u7406\u5bfc\u5411\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u53ef\u80fd\u5bf9\u63d0\u5347LLM\u751f\u6210\u53ef\u4fe1\u3001\u81ea\u77e5\u7684\u8f93\u51fa\u80fd\u529b\u5177\u6709\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2504.06464", "pdf": "https://arxiv.org/pdf/2504.06464", "abs": "https://arxiv.org/abs/2504.06464", "authors": ["Jos\u00e9 A. Pilartes-Congo", "Matthew Kastl", "Michael J. Starek", "Marina Vicens-Miquel", "Philippe Tissot"], "title": "Implementation of a Zed 2i Stereo Camera for High-Frequency Shoreline Change and Coastal Elevation Monitoring", "categories": ["cs.CV"], "comment": "Published in IGARSS 2023 - 2023 IEEE International Geoscience and\n  Remote Sensing Symposium", "summary": "The increasing population, thus financial interests, in coastal areas have\nincreased the need to monitor coastal elevation and shoreline change. Though\nseveral resources exist to obtain this information, they often lack the\nrequired temporal resolution for short-term monitoring (e.g., every hour). To\naddress this issue, this study implements a low-cost ZED 2i stereo camera\nsystem and close-range photogrammetry to collect images for generating 3D point\nclouds, digital surface models (DSMs) of beach elevation, and georectified\nimagery at a localized scale and high temporal resolution. The main\ncontributions of this study are (i) intrinsic camera calibration, (ii)\ngeorectification and registration of acquired imagery and point cloud, (iii)\ngeneration of the DSM of the beach elevation, and (iv) a comparison of derived\nproducts against those from uncrewed aircraft system structure-from-motion\nphotogrammetry. Preliminary results show that despite its limitations, the ZED\n2i can provide the desired mapping products at localized and high temporal\nscales. The system achieved a mean reprojection error of 0.20 px, a point cloud\nregistration of 27 cm, a vertical error of 37.56 cm relative to ground truth,\nand georectification root mean square errors of 2.67 cm and 2.81 cm for x and\ny.", "AI": {"task": "\u5229\u7528\u4f4e\u6210\u672cZED 2i\u7acb\u4f53\u76f8\u673a\u7cfb\u7edf\u548c\u9ad8\u5206\u8fa8\u7387\u6444\u5f71\u6d4b\u91cf\u6280\u672f\u76d1\u6d4b\u6d77\u5cb8\u7ebf\u53d8\u5316\u548c\u6d77\u5cb8\u9ad8\u7a0b\u3002", "motivation": "\u6cbf\u6d77\u5730\u533a\u4eba\u53e3\u548c\u91d1\u878d\u5229\u76ca\u7684\u589e\u52a0\u5bfc\u81f4\u5bf9\u6d77\u5cb8\u9ad8\u7a0b\u548c\u6d77\u5cb8\u7ebf\u53d8\u5316\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u76d1\u6d4b\u9700\u6c42\u589e\u52a0\u3002", "method": "\u4f7f\u7528ZED 2i\u7acb\u4f53\u76f8\u673a\u7cfb\u7edf\u548c\u8fd1\u8ddd\u79bb\u6444\u5f71\u6d4b\u91cf\u6280\u672f\u751f\u62103D\u70b9\u4e91\u3001\u6570\u5b57\u8868\u9762\u6a21\u578b\uff08DSM\uff09\u548c\u5730\u7406\u6821\u6b63\u56fe\u50cf\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e860.20\u50cf\u7d20\u7684\u5e73\u5747\u91cd\u6295\u5f71\u8bef\u5dee\u300127\u5398\u7c73\u7684\u70b9\u4e91\u914d\u51c6\u300137.56\u5398\u7c73\u7684\u5782\u76f4\u8bef\u5dee\uff0c\u4ee5\u53cax\u548cy\u65b9\u54112.67\u5398\u7c73\u548c2.81\u5398\u7c73\u7684\u5730\u7406\u6821\u6b63\u5747\u65b9\u6839\u8bef\u5dee\u3002", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u5c40\u9650\u6027\uff0cZED 2i\u7cfb\u7edf\u80fd\u591f\u5728\u5c40\u90e8\u548c\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u4e0b\u63d0\u4f9b\u6240\u9700\u7684\u6d4b\u7ed8\u4ea7\u54c1\u3002"}}
{"id": "2504.06577", "pdf": "https://arxiv.org/pdf/2504.06577", "abs": "https://arxiv.org/abs/2504.06577", "authors": ["Pedro Cisneros-Velarde"], "title": "Bypassing Safety Guardrails in LLMs Using Humor", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this paper, we show it is possible to bypass the safety guardrails of\nlarge language models (LLMs) through a humorous prompt including the unsafe\nrequest. In particular, our method does not edit the unsafe request and follows\na fixed template -- it is simple to implement and does not need additional LLMs\nto craft prompts. Extensive experiments show the effectiveness of our method\nacross different LLMs. We also show that both removing and adding more humor to\nour method can reduce its effectiveness -- excessive humor possibly distracts\nthe LLM from fulfilling its unsafe request. Thus, we argue that LLM\njailbreaking occurs when there is a proper balance between focus on the unsafe\nrequest and presence of humor.", "AI": {"task": "\u901a\u8fc7\u5e7d\u9ed8\u63d0\u793a\u7ed5\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u9632\u62a4\u673a\u5236\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u4e0d\u4fee\u6539\u4e0d\u5b89\u5168\u8bf7\u6c42\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u56fa\u5b9a\u6a21\u677f\u7684\u5e7d\u9ed8\u63d0\u793a\u7ed5\u8fc7LLMs\u7684\u5b89\u5168\u9632\u62a4\u3002", "method": "\u4f7f\u7528\u56fa\u5b9a\u6a21\u677f\u7684\u5e7d\u9ed8\u63d0\u793a\uff0c\u65e0\u9700\u989d\u5916LLMs\u8f85\u52a9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540cLLMs\u4e2d\u6709\u6548\uff0c\u4e14\u5e7d\u9ed8\u7a0b\u5ea6\u8fc7\u9ad8\u6216\u8fc7\u4f4e\u5747\u4f1a\u964d\u4f4e\u6548\u679c\u3002", "conclusion": "LLMs\u7684\u8d8a\u72f1\u9700\u8981\u5728\u5173\u6ce8\u4e0d\u5b89\u5168\u8bf7\u6c42\u548c\u5e7d\u9ed8\u4e4b\u95f4\u627e\u5230\u9002\u5f53\u5e73\u8861\u3002"}}
{"id": "2504.06486", "pdf": "https://arxiv.org/pdf/2504.06486", "abs": "https://arxiv.org/abs/2504.06486", "authors": ["Samuel Stevens", "S M Rayeed", "Jenna Kline"], "title": "Mind the Gap: Evaluating Vision Systems in Small Data Applications", "categories": ["cs.CV"], "comment": "4 pages (main text), 5 figures", "summary": "The practical application of AI tools for specific computer vision tasks\nrelies on the \"small-data regime\" of hundreds to thousands of labeled samples.\nThis small-data regime is vital for applications requiring expensive expert\nannotations, such as ecological monitoring, medical diagnostics or industrial\nquality control. We find, however, that computer vision research has ignored\nthe small data regime as evaluations increasingly focus on zero- and few-shot\nlearning. We use the Natural World Tasks (NeWT) benchmark to compare\nmulti-modal large language models (MLLMs) and vision-only methods across\nvarying training set sizes. MLLMs exhibit early performance plateaus, while\nvision-only methods improve throughout the small-data regime, with performance\ngaps widening beyond 10 training examples. We provide the first comprehensive\ncomparison between these approaches in small-data contexts and advocate for\nexplicit small-data evaluations in AI research to better bridge theoretical\nadvances with practical deployments.", "AI": {"task": "\u6bd4\u8f83\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u548c\u7eaf\u89c6\u89c9\u65b9\u6cd5\u5728\u5c0f\u6570\u636e\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u5ffd\u89c6\u4e86\u5c0f\u6570\u636e\u573a\u666f\u7684\u91cd\u8981\u6027\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\uff08\u5982\u751f\u6001\u76d1\u6d4b\u3001\u533b\u7597\u8bca\u65ad\u6216\u5de5\u4e1a\u8d28\u91cf\u63a7\u5236\uff09\u4f9d\u8d56\u5c0f\u6570\u636e\u3002", "method": "\u4f7f\u7528Natural World Tasks\uff08NeWT\uff09\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u4e0d\u540c\u8bad\u7ec3\u96c6\u89c4\u6a21\u4e0b\u6bd4\u8f83MLLMs\u548c\u7eaf\u89c6\u89c9\u65b9\u6cd5\u3002", "result": "MLLMs\u5728\u5c0f\u6570\u636e\u573a\u666f\u4e0b\u6027\u80fd\u8f83\u65e9\u505c\u6ede\uff0c\u800c\u7eaf\u89c6\u89c9\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\uff0c\u5c24\u5176\u5728\u8d85\u8fc710\u4e2a\u8bad\u7ec3\u6837\u672c\u65f6\u5dee\u8ddd\u663e\u8457\u6269\u5927\u3002", "conclusion": "\u547c\u5401\u5728AI\u7814\u7a76\u4e2d\u660e\u786e\u5c0f\u6570\u636e\u573a\u666f\u7684\u8bc4\u4f30\uff0c\u4ee5\u66f4\u597d\u5730\u8fde\u63a5\u7406\u8bba\u8fdb\u5c55\u4e0e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2504.06600", "pdf": "https://arxiv.org/pdf/2504.06600", "abs": "https://arxiv.org/abs/2504.06600", "authors": ["William De Michele", "Abel Armas Cervantes", "Lea Frermann"], "title": "Automated Business Process Analysis: An LLM-Based Approach to Value Assessment", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": null, "summary": "Business processes are fundamental to organizational operations, yet their\noptimization remains challenging due to the timeconsuming nature of manual\nprocess analysis. Our paper harnesses Large Language Models (LLMs) to automate\nvalue-added analysis, a qualitative process analysis technique that aims to\nidentify steps in the process that do not deliver value. To date, this\ntechnique is predominantly manual, time-consuming, and subjective. Our method\noffers a more principled approach which operates in two phases: first,\ndecomposing high-level activities into detailed steps to enable granular\nanalysis, and second, performing a value-added analysis to classify each step\naccording to Lean principles. This approach enables systematic identification\nof waste while maintaining the semantic understanding necessary for qualitative\nanalysis. We develop our approach using 50 business process models, for which\nwe collect and publish manual ground-truth labels. Our evaluation, comparing\nzero-shot baselines with more structured prompts reveals (a) a consistent\nbenefit of structured prompting and (b) promising performance for both tasks.\nWe discuss the potential for LLMs to augment human expertise in qualitative\nprocess analysis while reducing the time and subjectivity inherent in manual\napproaches.", "AI": {"task": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5316\u4e1a\u52a1\u8fc7\u7a0b\u4e2d\u7684\u589e\u503c\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u7684\u624b\u5de5\u589e\u503c\u5206\u6790\u65b9\u6cd5\u8017\u65f6\u4e14\u4e3b\u89c2\uff0c\u96be\u4ee5\u9ad8\u6548\u4f18\u5316\u4e1a\u52a1\u6d41\u7a0b\u3002", "method": "\u5206\u4e24\u9636\u6bb5\u8fdb\u884c\uff1a\u9996\u5148\u5c06\u9ad8\u7ea7\u6d3b\u52a8\u5206\u89e3\u4e3a\u8be6\u7ec6\u6b65\u9aa4\uff0c\u7136\u540e\u57fa\u4e8eLean\u539f\u5219\u5bf9\u6bcf\u4e2a\u6b65\u9aa4\u8fdb\u884c\u589e\u503c\u5206\u7c7b\u3002", "result": "\u572850\u4e2a\u4e1a\u52a1\u6d41\u7a0b\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u7ed3\u6784\u5316\u63d0\u793a\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u96f6\u6837\u672c\u57fa\u7ebf\uff0c\u4e14\u6574\u4f53\u6027\u80fd\u826f\u597d\u3002", "conclusion": "LLMs\u53ef\u4ee5\u8f85\u52a9\u4eba\u7c7b\u4e13\u5bb6\u8fdb\u884c\u5b9a\u6027\u5206\u6790\uff0c\u51cf\u5c11\u65f6\u95f4\u548c\u4e3b\u89c2\u6027\uff0c\u63d0\u5347\u6548\u7387\u3002"}}
{"id": "2504.06504", "pdf": "https://arxiv.org/pdf/2504.06504", "abs": "https://arxiv.org/abs/2504.06504", "authors": ["Xiaohang Yang", "Qing Wang", "Jiahao Yang", "Gregory Slabaugh", "Shanxin Yuan"], "title": "STaR: Seamless Spatial-Temporal Aware Motion Retargeting with Penetration and Consistency Constraints", "categories": ["cs.CV"], "comment": "12 pages, 9 figures;", "summary": "Motion retargeting seeks to faithfully replicate the spatio-temporal motion\ncharacteristics of a source character onto a target character with a different\nbody shape. Apart from motion semantics preservation, ensuring geometric\nplausibility and maintaining temporal consistency are also crucial for\neffective motion retargeting. However, many existing methods prioritize either\ngeometric plausibility or temporal consistency. Neglecting geometric\nplausibility results in interpenetration while neglecting temporal consistency\nleads to motion jitter. In this paper, we propose a novel sequence-to-sequence\nmodel for seamless Spatial-Temporal aware motion Retargeting (STaR), with\npenetration and consistency constraints. STaR consists of two modules: (1) a\nspatial module that incorporates dense shape representation and a novel limb\npenetration constraint to ensure geometric plausibility while preserving motion\nsemantics, and (2) a temporal module that utilizes a temporal transformer and a\nnovel temporal consistency constraint to predict the entire motion sequence at\nonce while enforcing multi-level trajectory smoothness. The seamless\ncombination of the two modules helps us achieve a good balance between the\nsemantic, geometric, and temporal targets. Extensive experiments on the Mixamo\nand ScanRet datasets demonstrate that our method produces plausible and\ncoherent motions while significantly reducing interpenetration rates compared\nwith other approaches.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578bSTaR\uff0c\u7528\u4e8e\u5b9e\u73b0\u7a7a\u95f4-\u65f6\u95f4\u611f\u77e5\u7684\u8fd0\u52a8\u91cd\u5b9a\u5411\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u53ea\u5173\u6ce8\u51e0\u4f55\u5408\u7406\u6027\u6216\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u8fd0\u52a8\u91cd\u5b9a\u5411\u4e2d\u53ef\u80fd\u51fa\u73b0\u7a7f\u900f\u6216\u6296\u52a8\u95ee\u9898\u3002", "method": "STaR\u6a21\u578b\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a\u7a7a\u95f4\u6a21\u5757\uff08\u5229\u7528\u5bc6\u96c6\u5f62\u72b6\u8868\u793a\u548c\u80a2\u4f53\u7a7f\u900f\u7ea6\u675f\uff09\u548c\u65f6\u95f4\u6a21\u5757\uff08\u5229\u7528\u65f6\u95f4\u53d8\u6362\u5668\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7ea6\u675f\uff09\u3002", "result": "\u5728Mixamo\u548cScanRet\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u5408\u7406\u4e14\u8fde\u8d2f\u7684\u8fd0\u52a8\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u7a7f\u900f\u7387\u3002", "conclusion": "STaR\u6a21\u578b\u5728\u8bed\u4e49\u3001\u51e0\u4f55\u548c\u65f6\u95f4\u76ee\u6807\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.06650", "pdf": "https://arxiv.org/pdf/2504.06650", "abs": "https://arxiv.org/abs/2504.06650", "authors": ["Zijian Wang", "Chang Xu"], "title": "ThoughtProbe: Classifier-Guided Thought Space Exploration Leveraging LLM Intrinsic Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Pre-trained large language models (LLMs) have been demonstrated to possess\nintrinsic reasoning capabilities that can emerge naturally when expanding the\nresponse space. However, the neural representation mechanisms underlying these\nintrinsic capabilities and approaches for their optimal utilization remain\ninadequately understood. In this work, we make the key discovery that a simple\nlinear classifier can effectively detect intrinsic reasoning capabilities in\nLLMs' activation space, particularly within specific representation types and\nnetwork layers. Based on this finding, we propose a classifier-guided search\nframework that strategically explore a tree-structured response space. In each\nnode expansion, the classifier serves as a scoring and ranking mechanism that\nefficiently allocates computational resources by identifying and prioritizing\nmore thoughtful reasoning directions for continuation. After completing the\ntree expansion, we collect answers from all branches to form a candidate answer\npool. We propose a branch-aggregation selection method that marginalizes over\nall supporting branches by aggregating their thoughtfulness scores, thereby\nidentifying the optimal answer from the pool. Experimental results show that\nour framework's comprehensive exploration not only covers valid reasoning\nchains but also effectively identifies them, achieving significant improvements\nacross multiple arithmetic reasoning benchmarks.", "AI": {"task": "\u63a2\u7d22\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u5185\u5728\u63a8\u7406\u80fd\u529b\u7684\u795e\u7ecf\u8868\u5f81\u673a\u5236\u53ca\u5176\u6700\u4f18\u5229\u7528\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5c55\u73b0\u51fa\u81ea\u7136\u6d8c\u73b0\u7684\u5185\u5728\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u795e\u7ecf\u8868\u5f81\u673a\u5236\u53ca\u5982\u4f55\u6700\u4f18\u5229\u7528\u8fd9\u4e9b\u80fd\u529b\u4ecd\u672a\u88ab\u5145\u5206\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u5206\u7c7b\u5668\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u7279\u5b9a\u8868\u5f81\u7c7b\u578b\u548c\u7f51\u7edc\u5c42\u6765\u6307\u5bfc\u6811\u72b6\u54cd\u5e94\u7a7a\u95f4\u7684\u63a2\u7d22\uff0c\u5e76\u7ed3\u5408\u5206\u652f\u805a\u5408\u9009\u62e9\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6709\u6548\u8986\u76d6\u5e76\u8bc6\u522b\u6709\u6548\u63a8\u7406\u94fe\uff0c\u5728\u591a\u4e2a\u7b97\u672f\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u5229\u7528LLMs\u7684\u5185\u5728\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u548c\u65b9\u6cd5\u3002"}}
{"id": "2504.06521", "pdf": "https://arxiv.org/pdf/2504.06521", "abs": "https://arxiv.org/abs/2504.06521", "authors": ["Songze Li", "Tonghua Su", "Xu-Yao Zhang", "Qixing Xu", "Zhongjie Wang"], "title": "DUKAE: DUal-level Knowledge Accumulation and Ensemble for Pre-Trained Model-Based Continual Learning", "categories": ["cs.CV"], "comment": null, "summary": "Pre-trained model-based continual learning (PTMCL) has garnered growing\nattention, as it enables more rapid acquisition of new knowledge by leveraging\nthe extensive foundational understanding inherent in pre-trained model (PTM).\nMost existing PTMCL methods use Parameter-Efficient Fine-Tuning (PEFT) to learn\nnew knowledge while consolidating existing memory. However, they often face\nsome challenges. A major challenge lies in the misalignment of classification\nheads, as the classification head of each task is trained within a distinct\nfeature space, leading to inconsistent decision boundaries across tasks and,\nconsequently, increased forgetting. Another critical limitation stems from the\nrestricted feature-level knowledge accumulation, with feature learning\ntypically restricted to the initial task only, which constrains the model's\nrepresentation capabilities. To address these issues, we propose a method named\nDUal-level Knowledge Accumulation and Ensemble (DUKAE) that leverages both\nfeature-level and decision-level knowledge accumulation by aligning\nclassification heads into a unified feature space through Gaussian distribution\nsampling and introducing an adaptive expertise ensemble to fuse knowledge\nacross feature subspaces.Extensive experiments on CIFAR-100, ImageNet-R,\nCUB-200, and Cars-196 datasets demonstrate the superior performance of our\napproach.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aDUKAE\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7279\u5f81\u7ea7\u548c\u51b3\u7b56\u7ea7\u77e5\u8bc6\u79ef\u7d2f\u89e3\u51b3\u9884\u8bad\u7ec3\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5206\u7c7b\u5934\u4e0d\u5bf9\u9f50\u548c\u7279\u5f81\u7ea7\u77e5\u8bc6\u79ef\u7d2f\u53d7\u9650\u95ee\u9898\u3002", "motivation": "\u73b0\u6709PTMCL\u65b9\u6cd5\u5728\u5206\u7c7b\u5934\u4e0d\u5bf9\u9f50\u548c\u7279\u5f81\u7ea7\u77e5\u8bc6\u79ef\u7d2f\u53d7\u9650\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5bfc\u81f4\u51b3\u7b56\u8fb9\u754c\u4e0d\u4e00\u81f4\u548c\u9057\u5fd8\u589e\u52a0\u3002", "method": "DUKAE\u901a\u8fc7\u9ad8\u65af\u5206\u5e03\u91c7\u6837\u5c06\u5206\u7c7b\u5934\u5bf9\u9f50\u5230\u7edf\u4e00\u7279\u5f81\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u4e13\u5bb6\u96c6\u6210\u878d\u5408\u7279\u5f81\u5b50\u7a7a\u95f4\u77e5\u8bc6\u3002", "result": "\u5728CIFAR-100\u3001ImageNet-R\u3001CUB-200\u548cCars-196\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "DUKAE\u6709\u6548\u89e3\u51b3\u4e86\u5206\u7c7b\u5934\u4e0d\u5bf9\u9f50\u548c\u7279\u5f81\u7ea7\u77e5\u8bc6\u79ef\u7d2f\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6301\u7eed\u5b66\u4e60\u6027\u80fd\u3002"}}
{"id": "2504.06664", "pdf": "https://arxiv.org/pdf/2504.06664", "abs": "https://arxiv.org/abs/2504.06664", "authors": ["Zhilin Wang", "Yafu Li", "Xiaoye Qu", "Yu Cheng"], "title": "SEE: Continual Fine-tuning with Sequential Ensemble of Experts", "categories": ["cs.CL", "cs.LG"], "comment": "9pages", "summary": "Continual fine-tuning of large language models (LLMs) suffers from\ncatastrophic forgetting. Rehearsal-based methods mitigate this problem by\nretaining a small set of old data. Nevertheless, they still suffer inevitable\nperformance loss. Although training separate experts for each task can help\nprevent forgetting, effectively assembling them remains a challenge. Some\napproaches use routers to assign tasks to experts, but in continual learning,\nthey often require retraining for optimal performance. To address these\nchallenges, we introduce the Sequential Ensemble of Experts (SEE) framework.\nSEE removes the need for an additional router, allowing each expert to\nindependently decide whether a query should be handled. The framework employs\ndistributed routing, and during continual fine-tuning, SEE only requires the\ntraining of new experts for incoming tasks rather than retraining the entire\nsystem. Experiments reveal that the SEE outperforms prior approaches, including\nmulti-task learning, in continual fine-tuning. It also demonstrates remarkable\ngeneralization ability, as the expert can effectively identify\nout-of-distribution queries, which can then be directed to a more generalized\nmodel for resolution. This work highlights the promising potential of\nintegrating routing and response mechanisms within each expert, paving the way\nfor the future of distributed model ensembling.", "AI": {"task": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6301\u7eed\u5fae\u8c03\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982\u57fa\u4e8e\u6392\u7ec3\u7684\u65b9\u6cd5\u548c\u4e13\u5bb6\u5206\u79bb\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u635f\u5931\u6216\u8def\u7531\u6311\u6218\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faSequential Ensemble of Experts (SEE)\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u8def\u7531\u548c\u72ec\u7acb\u4e13\u5bb6\u51b3\u7b56\u907f\u514d\u989d\u5916\u8def\u7531\u5668\u7684\u9700\u6c42\u3002", "result": "SEE\u5728\u6301\u7eed\u5fae\u8c03\u4e2d\u4f18\u4e8e\u591a\u4efb\u52a1\u5b66\u4e60\u7b49\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SEE\u4e3a\u5206\u5e03\u5f0f\u6a21\u578b\u96c6\u6210\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u7ed3\u5408\u8def\u7531\u548c\u54cd\u5e94\u673a\u5236\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2504.06527", "pdf": "https://arxiv.org/pdf/2504.06527", "abs": "https://arxiv.org/abs/2504.06527", "authors": ["Xinyu Liu", "Xiaoguang Lin", "Xiang Liu", "Yong Yang", "Hongqian Wang", "Qilong Sun"], "title": "TSP-OCS: A Time-Series Prediction for Optimal Camera Selection in Multi-Viewpoint Surgical Video Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recording the open surgery process is essential for educational and medical\nevaluation purposes; however, traditional single-camera methods often face\nchallenges such as occlusions caused by the surgeon's head and body, as well as\nlimitations due to fixed camera angles, which reduce comprehensibility of the\nvideo content. This study addresses these limitations by employing a\nmulti-viewpoint camera recording system, capturing the surgical procedure from\nsix different angles to mitigate occlusions. We propose a fully supervised\nlearning-based time series prediction method to choose the best shot sequences\nfrom multiple simultaneously recorded video streams, ensuring optimal\nviewpoints at each moment. Our time series prediction model forecasts future\ncamera selections by extracting and fusing visual and semantic features from\nsurgical videos using pre-trained models. These features are processed by a\ntemporal prediction network with TimeBlocks to capture sequential dependencies.\nA linear embedding layer reduces dimensionality, and a Softmax classifier\nselects the optimal camera view based on the highest probability. In our\nexperiments, we created five groups of open thyroidectomy videos, each with\nsimultaneous recordings from six different angles. The results demonstrate that\nour method achieves competitive accuracy compared to traditional supervised\nmethods, even when predicting over longer time horizons. Furthermore, our\napproach outperforms state-of-the-art time series prediction techniques on our\ndataset. This manuscript makes a unique contribution by presenting an\ninnovative framework that advances surgical video analysis techniques, with\nsignificant implications for improving surgical education and patient safety.", "AI": {"task": "\u901a\u8fc7\u591a\u89c6\u89d2\u6444\u50cf\u7cfb\u7edf\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ece\u516d\u4e2a\u4e0d\u540c\u89d2\u5ea6\u5f55\u5236\u624b\u672f\u8fc7\u7a0b\u5e76\u9009\u62e9\u6700\u4f73\u89c6\u89d2\u5e8f\u5217\u3002", "motivation": "\u4f20\u7edf\u5355\u6444\u50cf\u5934\u65b9\u6cd5\u5b58\u5728\u906e\u6321\u548c\u56fa\u5b9a\u89c6\u89d2\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u89c6\u9891\u5185\u5bb9\u7684\u53ef\u7406\u89e3\u6027\u3002", "method": "\u91c7\u7528\u591a\u89c6\u89d2\u6444\u50cf\u7cfb\u7edf\uff0c\u7ed3\u5408\u5168\u76d1\u7763\u5b66\u4e60\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u63d0\u53d6\u5e76\u878d\u5408\u89c6\u89c9\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u901a\u8fc7\u65f6\u95f4\u9884\u6d4b\u7f51\u7edc\u9009\u62e9\u6700\u4f73\u89c6\u89d2\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u8f83\u957f\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u5185\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6280\u672f\u3002", "conclusion": "\u63d0\u51fa\u7684\u521b\u65b0\u6846\u67b6\u63d0\u5347\u4e86\u624b\u672f\u89c6\u9891\u5206\u6790\u6280\u672f\uff0c\u5bf9\u624b\u672f\u6559\u80b2\u548c\u60a3\u8005\u5b89\u5168\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2504.06669", "pdf": "https://arxiv.org/pdf/2504.06669", "abs": "https://arxiv.org/abs/2504.06669", "authors": ["Heather Lent", "Erick Galinkin", "Yiyi Chen", "Jens Myrup Pedersen", "Leon Derczynski", "Johannes Bjerva"], "title": "NLP Security and Ethics, in the Wild", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to TACL", "summary": "As NLP models are used by a growing number of end-users, an area of\nincreasing importance is NLP Security (NLPSec): assessing the vulnerability of\nmodels to malicious attacks and developing comprehensive countermeasures\nagainst them. While work at the intersection of NLP and cybersecurity has the\npotential to create safer NLP for all, accidental oversights can result in\ntangible harm (e.g., breaches of privacy or proliferation of malicious models).\nIn this emerging field, however, the research ethics of NLP have not yet faced\nmany of the long-standing conundrums pertinent to cybersecurity, until now. We\nthus examine contemporary works across NLPSec, and explore their engagement\nwith cybersecurity's ethical norms. We identify trends across the literature,\nultimately finding alarming gaps on topics like harm minimization and\nresponsible disclosure. To alleviate these concerns, we provide concrete\nrecommendations to help NLP researchers navigate this space more ethically,\nbridging the gap between traditional cybersecurity and NLP ethics, which we\nframe as ``white hat NLP''. The goal of this work is to help cultivate an\nintentional culture of ethical research for those working in NLP Security.", "AI": {"task": "\u63a2\u8ba8NLP\u5b89\u5168\uff08NLPSec\uff09\u9886\u57df\u7684\u7814\u7a76\u4f26\u7406\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u968f\u7740NLP\u6a21\u578b\u7528\u6237\u589e\u591a\uff0c\u5176\u5b89\u5168\u6027\u95ee\u9898\uff08\u5982\u6076\u610f\u653b\u51fb\uff09\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5728\u4f26\u7406\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u53ef\u80fd\u5bfc\u81f4\u9690\u79c1\u6cc4\u9732\u7b49\u5371\u5bb3\u3002", "method": "\u5206\u6790\u5f53\u4ee3NLPSec\u6587\u732e\uff0c\u63a2\u8ba8\u5176\u4e0e\u7f51\u7edc\u5b89\u5168\u4f26\u7406\u89c4\u8303\u7684\u5173\u8054\uff0c\u5e76\u8bc6\u522b\u7814\u7a76\u4e2d\u7684\u4f26\u7406\u7f3a\u53e3\u3002", "result": "\u53d1\u73b0NLPSec\u5728\u4f24\u5bb3\u6700\u5c0f\u5316\u548c\u8d1f\u8d23\u4efb\u62ab\u9732\u7b49\u4e3b\u9898\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "\u63d0\u51fa\u5177\u4f53\u5efa\u8bae\uff0c\u63a8\u52a8NLP\u5b89\u5168\u7814\u7a76\u66f4\u7b26\u5408\u4f26\u7406\uff0c\u5021\u5bfc\u201c\u767d\u5e3dNLP\u201d\u6587\u5316\u3002"}}
{"id": "2504.06544", "pdf": "https://arxiv.org/pdf/2504.06544", "abs": "https://arxiv.org/abs/2504.06544", "authors": ["Weiwei Xing", "Yue Cheng", "Hongzhu Yi", "Xiaohui Gao", "Xiang Wei", "Xiaoyu Guo", "Yuming Zhang", "Xinyu Pang"], "title": "LCGC: Learning from Consistency Gradient Conflicting for Class-Imbalanced Semi-Supervised Debiasing", "categories": ["cs.CV"], "comment": "This paper has been accepted by AAAI 2025", "summary": "Classifiers often learn to be biased corresponding to the class-imbalanced\ndataset, especially under the semi-supervised learning (SSL) set. While\nprevious work tries to appropriately re-balance the classifiers by subtracting\na class-irrelevant image's logit, but lacks a firm theoretical basis. We\ntheoretically analyze why exploiting a baseline image can refine pseudo-labels\nand prove that the black image is the best choice. We also indicated that as\nthe training process deepens, the pseudo-labels before and after refinement\nbecome closer. Based on this observation, we propose a debiasing scheme dubbed\nLCGC, which Learning from Consistency Gradient Conflicting, by encouraging\nbiased class predictions during training. We intentionally update the\npseudo-labels whose gradient conflicts with the debiased logits, representing\nthe optimization direction offered by the over-imbalanced classifier\npredictions. Then, we debiased the predictions by subtracting the baseline\nimage logits during testing. Extensive experiments demonstrate that LCGC can\nsignificantly improve the prediction accuracy of existing CISSL models on\npublic benchmarks.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aLCGC\u7684\u53bb\u504f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u4e00\u81f4\u6027\u68af\u5ea6\u51b2\u7a81\u6765\u4f18\u5316\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u4f2a\u6807\u7b7e\u3002", "motivation": "\u89e3\u51b3\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u4e2d\u7531\u4e8e\u7c7b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u5bfc\u81f4\u7684\u5206\u7c7b\u5668\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u586b\u8865\u4e4b\u524d\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u7684\u7a7a\u767d\u3002", "method": "\u7406\u8bba\u5206\u6790\u57fa\u7ebf\u56fe\u50cf\u5bf9\u4f2a\u6807\u7b7e\u7684\u4f18\u5316\u4f5c\u7528\uff0c\u63d0\u51faLCGC\u65b9\u6cd5\uff0c\u901a\u8fc7\u9f13\u52b1\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u504f\u5dee\u7c7b\u9884\u6d4b\u6765\u66f4\u65b0\u4f2a\u6807\u7b7e\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u51cf\u53bb\u57fa\u7ebf\u56fe\u50cf\u7684\u5bf9\u6570\u6982\u7387\u3002", "result": "LCGC\u663e\u8457\u63d0\u9ad8\u4e86\u73b0\u6709CISSL\u6a21\u578b\u5728\u516c\u5171\u57fa\u51c6\u4e0a\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "LCGC\u662f\u4e00\u79cd\u6709\u6548\u7684\u53bb\u504f\u65b9\u6cd5\uff0c\u80fd\u591f\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u63d0\u5347\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2504.06792", "pdf": "https://arxiv.org/pdf/2504.06792", "abs": "https://arxiv.org/abs/2504.06792", "authors": ["Zican Dong", "Han Peng", "Peiyu Liu", "Wayne Xin Zhao", "Dong Wu", "Feng Xiao", "Zhifeng Wang"], "title": "Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Mixture-of-Experts (MoE) models achieve a favorable trade-off between\nperformance and inference efficiency by activating only a subset of experts.\nHowever, the memory overhead of storing all experts remains a major limitation,\nespecially in large-scale MoE models such as DeepSeek-R1 (671B). In this study,\nwe investigate domain specialization and expert redundancy in large-scale MoE\nmodels and uncover a consistent behavior we term few-shot expert localization,\nwith only a few demonstrations, the model consistently activates a sparse and\nstable subset of experts. Building on this observation, we propose a simple yet\neffective pruning framework, EASY-EP, that leverages a few domain-specific\ndemonstrations to identify and retain only the most relevant experts. EASY-EP\ncomprises two key components: output-aware expert importance assessment and\nexpert-level token contribution estimation. The former evaluates the importance\nof each expert for the current token by considering the gating scores and\nmagnitudes of the outputs of activated experts, while the latter assesses the\ncontribution of tokens based on representation similarities after and before\nrouted experts. Experiments show that our method can achieve comparable\nperformances and $2.99\\times$ throughput under the same memory budget with full\nDeepSeek-R1 with only half the experts. Our code is available at\nhttps://github.com/RUCAIBox/EASYEP.", "AI": {"task": "\u7814\u7a76\u5927\u89c4\u6a21Mixture-of-Experts\uff08MoE\uff09\u6a21\u578b\u4e2d\u7684\u9886\u57df\u4e13\u4e1a\u5316\u548c\u4e13\u5bb6\u5197\u4f59\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u526a\u679d\u6846\u67b6EASY-EP\u3002", "motivation": "MoE\u6a21\u578b\u5728\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\uff0c\u4f46\u5b58\u50a8\u6240\u6709\u4e13\u5bb6\u7684\u5185\u5b58\u5f00\u9500\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u9650\u5236\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21MoE\u6a21\u578b\u4e2d\u3002", "method": "\u63d0\u51faEASY-EP\u6846\u67b6\uff0c\u901a\u8fc7\u8f93\u51fa\u611f\u77e5\u7684\u4e13\u5bb6\u91cd\u8981\u6027\u8bc4\u4f30\u548c\u4e13\u5bb6\u7ea7\u4ee4\u724c\u8d21\u732e\u4f30\u8ba1\uff0c\u5229\u7528\u5c11\u91cf\u9886\u57df\u7279\u5b9a\u6f14\u793a\u8bc6\u522b\u5e76\u4fdd\u7559\u6700\u76f8\u5173\u7684\u4e13\u5bb6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEASY-EP\u5728\u76f8\u540c\u5185\u5b58\u9884\u7b97\u4e0b\uff0c\u4ec5\u4f7f\u7528\u4e00\u534a\u4e13\u5bb6\u5373\u53ef\u8fbe\u5230\u4e0e\u5b8c\u6574DeepSeek-R1\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e14\u541e\u5410\u91cf\u63d0\u9ad82.99\u500d\u3002", "conclusion": "EASY-EP\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u526a\u679d\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u5185\u5b58\u5f00\u9500\u5e76\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.06572", "pdf": "https://arxiv.org/pdf/2504.06572", "abs": "https://arxiv.org/abs/2504.06572", "authors": ["Shaocong Long", "Qianyu Zhou", "Xikun Jiang", "Chenhao Ying", "Lizhuang Ma", "Yuan Luo"], "title": "Domain Generalization via Discrete Codebook Learning", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025", "summary": "Domain generalization (DG) strives to address distribution shifts across\ndiverse environments to enhance model's generalizability. Current DG approaches\nare confined to acquiring robust representations with continuous features,\nspecifically training at the pixel level. However, this DG paradigm may\nstruggle to mitigate distribution gaps in dealing with a large space of\ncontinuous features, rendering it susceptible to pixel details that exhibit\nspurious correlations or noise. In this paper, we first theoretically\ndemonstrate that the domain gaps in continuous representation learning can be\nreduced by the discretization process. Based on this inspiring finding, we\nintroduce a novel learning paradigm for DG, termed Discrete Domain\nGeneralization (DDG). DDG proposes to use a codebook to quantize the feature\nmap into discrete codewords, aligning semantic-equivalent information in a\nshared discrete representation space that prioritizes semantic-level\ninformation over pixel-level intricacies. By learning at the semantic level,\nDDG diminishes the number of latent features, optimizing the utilization of the\nrepresentation space and alleviating the risks associated with the wide-ranging\nspace of continuous features. Extensive experiments across widely employed\nbenchmarks in DG demonstrate DDG's superior performance compared to\nstate-of-the-art approaches, underscoring its potential to reduce the\ndistribution gaps and enhance the model's generalizability.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u79bb\u6563\u57df\u6cdb\u5316\uff08DDG\uff09\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u79bb\u6563\u5316\u8fc7\u7a0b\u51cf\u5c11\u8fde\u7eed\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u57df\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u57df\u6cdb\u5316\u65b9\u6cd5\u5728\u5904\u7406\u8fde\u7eed\u7279\u5f81\u65f6\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u51cf\u5c11\u5206\u5e03\u5dee\u8ddd\uff0c\u5bb9\u6613\u53d7\u5230\u50cf\u7d20\u7ec6\u8282\u4e2d\u7684\u865a\u5047\u76f8\u5173\u6027\u6216\u566a\u58f0\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u7801\u672c\u5c06\u7279\u5f81\u56fe\u91cf\u5316\u4e3a\u79bb\u6563\u7801\u5b57\uff0c\u5728\u5171\u4eab\u7684\u79bb\u6563\u8868\u793a\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u8bed\u4e49\u7b49\u4ef7\u4fe1\u606f\uff0c\u4f18\u5148\u8003\u8651\u8bed\u4e49\u7ea7\u4fe1\u606f\u800c\u975e\u50cf\u7d20\u7ea7\u7ec6\u8282\u3002", "result": "\u5728\u591a\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57df\u6cdb\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDDG\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "DDG\u901a\u8fc7\u79bb\u6563\u5316\u5b66\u4e60\u51cf\u5c11\u4e86\u5206\u5e03\u5dee\u8ddd\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2504.06816", "pdf": "https://arxiv.org/pdf/2504.06816", "abs": "https://arxiv.org/abs/2504.06816", "authors": ["Karol Mikula", "Mariana Sarkociov\u00e1 Reme\u0161\u00edkov\u00e1"], "title": "A Graph Diffusion Algorithm for Lexical Similarity Evaluation", "categories": ["cs.CL", "2020: 00A69, 05C90, 91F20"], "comment": "28 pages", "summary": "In this paper, we present an algorithm for evaluating lexical similarity\nbetween a given language and several reference language clusters. As an input,\nwe have a list of concepts and the corresponding translations in all considered\nlanguages. Moreover, each reference language is assigned to one of $c$ language\nclusters. For each of the concepts, the algorithm computes the distance between\neach pair of translations. Based on these distances, it constructs a weighted\ndirected graph, where every vertex represents a language. After, it solves a\ngraph diffusion equation with a Dirichlet boundary condition, where the unknown\nis a map from the vertex set to $\\mathbb{R}^c$. The resulting coordinates are\nvalues from the interval $[0,1]$ and they can be interpreted as probabilities\nof belonging to each of the clusters or as a lexical similarity distribution\nwith respect to the reference clusters. The distances between translations are\ncalculated using phonetic transcriptions and a modification of the\nDamerau-Levenshtein distance. The algorithm can be useful in analyzing\nrelationships between languages spoken in multilingual territories with a lot\nof mutual influences. We demonstrate this by presenting a case study regarding\nvarious European languages.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u8bc4\u4f30\u7ed9\u5b9a\u8bed\u8a00\u4e0e\u591a\u4e2a\u53c2\u8003\u8bed\u8a00\u7c07\u4e4b\u95f4\u8bcd\u6c47\u76f8\u4f3c\u6027\u7684\u7b97\u6cd5\u3002", "motivation": "\u5206\u6790\u591a\u8bed\u8a00\u5730\u533a\u4e2d\u76f8\u4e92\u5f71\u54cd\u7684\u8bed\u8a00\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u52a0\u6743\u6709\u5411\u56fe\u5e76\u6c42\u89e3\u5e26\u6709Dirichlet\u8fb9\u754c\u6761\u4ef6\u7684\u56fe\u6269\u6563\u65b9\u7a0b\uff0c\u8ba1\u7b97\u8bed\u8a00\u95f4\u7684\u8bcd\u6c47\u76f8\u4f3c\u6027\u5206\u5e03\u3002", "result": "\u7b97\u6cd5\u80fd\u591f\u751f\u6210\u8bed\u8a00\u5c5e\u4e8e\u5404\u53c2\u8003\u7c07\u7684\u6982\u7387\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u6b27\u6d32\u8bed\u8a00\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u53ef\u7528\u4e8e\u5206\u6790\u591a\u8bed\u8a00\u5730\u533a\u4e2d\u8bed\u8a00\u95f4\u7684\u8bcd\u6c47\u76f8\u4f3c\u6027\u548c\u76f8\u4e92\u5f71\u54cd\u3002"}}
{"id": "2504.06578", "pdf": "https://arxiv.org/pdf/2504.06578", "abs": "https://arxiv.org/abs/2504.06578", "authors": ["Rahul Singh Maharjan", "Marta Romeo", "Angelo Cangelosi"], "title": "Attributes-aware Visual Emotion Representation Learning", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "9 pages, 3 figures", "summary": "Visual emotion analysis or recognition has gained considerable attention due\nto the growing interest in understanding how images can convey rich semantics\nand evoke emotions in human perception. However, visual emotion analysis poses\ndistinctive challenges compared to traditional vision tasks, especially due to\nthe intricate relationship between general visual features and the different\naffective states they evoke, known as the affective gap. Researchers have used\ndeep representation learning methods to address this challenge of extracting\ngeneralized features from entire images. However, most existing methods\noverlook the importance of specific emotional attributes such as brightness,\ncolorfulness, scene understanding, and facial expressions. Through this paper,\nwe introduce A4Net, a deep representation network to bridge the affective gap\nby leveraging four key attributes: brightness (Attribute 1), colorfulness\n(Attribute 2), scene context (Attribute 3), and facial expressions (Attribute\n4). By fusing and jointly training all aspects of attribute recognition and\nvisual emotion analysis, A4Net aims to provide a better insight into emotional\ncontent in images. Experimental results show the effectiveness of A4Net,\nshowcasing competitive performance compared to state-of-the-art methods across\ndiverse visual emotion datasets. Furthermore, visualizations of activation maps\ngenerated by A4Net offer insights into its ability to generalize across\ndifferent visual emotion datasets.", "AI": {"task": "\u63d0\u51faA4Net\uff0c\u4e00\u79cd\u6df1\u5ea6\u8868\u793a\u7f51\u7edc\uff0c\u901a\u8fc7\u5229\u7528\u4eae\u5ea6\u3001\u8272\u5f69\u4e30\u5bcc\u5ea6\u3001\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u9762\u90e8\u8868\u60c5\u56db\u4e2a\u5173\u952e\u5c5e\u6027\u6765\u5f25\u5408\u60c5\u611f\u9e3f\u6c9f\u3002", "motivation": "\u89c6\u89c9\u60c5\u611f\u5206\u6790\u56e0\u56fe\u50cf\u4f20\u8fbe\u4e30\u5bcc\u8bed\u4e49\u548c\u5f15\u53d1\u4eba\u7c7b\u60c5\u611f\u7684\u5174\u8da3\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u7279\u5b9a\u60c5\u611f\u5c5e\u6027\u7684\u91cd\u8981\u6027\u3002", "method": "A4Net\u901a\u8fc7\u878d\u5408\u548c\u8054\u5408\u8bad\u7ec3\u56db\u4e2a\u5173\u952e\u5c5e\u6027\u7684\u8bc6\u522b\u4e0e\u89c6\u89c9\u60c5\u611f\u5206\u6790\uff0c\u63d0\u53d6\u5e7f\u4e49\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aA4Net\u5728\u591a\u4e2a\u89c6\u89c9\u60c5\u611f\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6fc0\u6d3b\u56fe\u53ef\u89c6\u5316\u5c55\u793a\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "A4Net\u901a\u8fc7\u591a\u5c5e\u6027\u8054\u5408\u8bad\u7ec3\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u60c5\u611f\u5206\u6790\u7684\u6027\u80fd\u3002"}}
{"id": "2504.06821", "pdf": "https://arxiv.org/pdf/2504.06821", "abs": "https://arxiv.org/abs/2504.06821", "authors": ["Zora Zhiruo Wang", "Apurva Gandhi", "Graham Neubig", "Daniel Fried"], "title": "Inducing Programmatic Skills for Agentic Tasks", "categories": ["cs.CL"], "comment": null, "summary": "To succeed in common digital tasks such as web navigation, agents must carry\nout a variety of specialized tasks such as searching for products or planning a\ntravel route. To tackle these tasks, agents can bootstrap themselves by\nlearning task-specific skills online through interaction with the web\nenvironment. In this work, we demonstrate that programs are an effective\nrepresentation for skills. We propose agent skill induction (ASI), which allows\nagents to adapt themselves by inducing, verifying, and utilizing program-based\nskills on the fly. We start with an evaluation on the WebArena agent benchmark\nand show that ASI outperforms the static baseline agent and its text-skill\ncounterpart by 23.5% and 11.3% in success rate, mainly thanks to the\nprogrammatic verification guarantee during the induction phase. ASI also\nimproves efficiency by reducing 10.7-15.3% of the steps over baselines, by\ncomposing primitive actions (e.g., click) into higher-level skills (e.g.,\nsearch product). We then highlight the efficacy of ASI in remaining efficient\nand accurate under scaled-up web activities. Finally, we examine the\ngeneralizability of induced skills when transferring between websites, and find\nthat ASI can effectively reuse common skills, while also updating incompatible\nskills to versatile website changes.", "AI": {"task": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u7a0b\u5e8f\u5316\u6280\u80fd\u8868\u793a\u63d0\u5347\u4ee3\u7406\u5728\u6570\u5b57\u4efb\u52a1\uff08\u5982\u7f51\u9875\u5bfc\u822a\uff09\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4ee3\u7406\u9700\u8981\u5b66\u4e60\u7279\u5b9a\u4efb\u52a1\u6280\u80fd\u4ee5\u9ad8\u6548\u5b8c\u6210\u6570\u5b57\u4efb\u52a1\uff0c\u7a0b\u5e8f\u5316\u6280\u80fd\u8868\u793a\u53ef\u80fd\u6bd4\u9759\u6001\u6216\u6587\u672c\u6280\u80fd\u66f4\u6709\u6548\u3002", "method": "\u63d0\u51fa\u4ee3\u7406\u6280\u80fd\u5f52\u7eb3\uff08ASI\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u5f52\u7eb3\u3001\u9a8c\u8bc1\u548c\u5229\u7528\u7a0b\u5e8f\u5316\u6280\u80fd\u6765\u63d0\u5347\u4ee3\u7406\u80fd\u529b\u3002", "result": "ASI\u5728WebArena\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u7387\u548c\u6548\u7387\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u9002\u5e94\u4e0d\u540c\u7f51\u7ad9\u7684\u6280\u80fd\u8fc1\u79fb\u3002", "conclusion": "\u7a0b\u5e8f\u5316\u6280\u80fd\u8868\u793a\u662f\u63d0\u5347\u4ee3\u7406\u5728\u6570\u5b57\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u6709\u6548\u65b9\u6cd5\uff0cASI\u5c55\u793a\u4e86\u5176\u9ad8\u6548\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2504.06580", "pdf": "https://arxiv.org/pdf/2504.06580", "abs": "https://arxiv.org/abs/2504.06580", "authors": ["Joochan Kim", "Minjoon Jung", "Byoung-Tak Zhang"], "title": "Exploring Ordinal Bias in Action Recognition for Instructional Videos", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to SCSL @ ICLR 2025", "summary": "Action recognition models have achieved promising results in understanding\ninstructional videos. However, they often rely on dominant, dataset-specific\naction sequences rather than true video comprehension, a problem that we define\nas ordinal bias. To address this issue, we propose two effective video\nmanipulation methods: Action Masking, which masks frames of frequently\nco-occurring actions, and Sequence Shuffling, which randomizes the order of\naction segments. Through comprehensive experiments, we demonstrate that current\nmodels exhibit significant performance drops when confronted with nonstandard\naction sequences, underscoring their vulnerability to ordinal bias. Our\nfindings emphasize the importance of rethinking evaluation strategies and\ndeveloping models capable of generalizing beyond fixed action patterns in\ndiverse instructional videos.", "AI": {"task": "\u89e3\u51b3\u52a8\u4f5c\u8bc6\u522b\u6a21\u578b\u5728\u7406\u89e3\u6559\u5b66\u89c6\u9891\u65f6\u4f9d\u8d56\u56fa\u5b9a\u52a8\u4f5c\u5e8f\u5217\u800c\u975e\u771f\u5b9e\u89c6\u9891\u7406\u89e3\u7684\u95ee\u9898\uff08\u5e8f\u6570\u504f\u5dee\uff09\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u7406\u89e3\u6559\u5b66\u89c6\u9891\u65f6\u503e\u5411\u4e8e\u4f9d\u8d56\u6570\u636e\u96c6\u7279\u5b9a\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u800c\u975e\u771f\u6b63\u7684\u89c6\u9891\u5185\u5bb9\u7406\u89e3\uff0c\u5bfc\u81f4\u5e8f\u6570\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u89c6\u9891\u5904\u7406\u65b9\u6cd5\uff1a\u52a8\u4f5c\u63a9\u7801\uff08\u5c4f\u853d\u9891\u7e41\u5171\u73b0\u52a8\u4f5c\u7684\u5e27\uff09\u548c\u5e8f\u5217\u968f\u673a\u5316\uff08\u6253\u4e71\u52a8\u4f5c\u7247\u6bb5\u7684\u987a\u5e8f\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6a21\u578b\u5728\u9762\u5bf9\u975e\u6807\u51c6\u52a8\u4f5c\u5e8f\u5217\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u51f8\u663e\u4e86\u5176\u5bf9\u5e8f\u6570\u504f\u5dee\u7684\u8106\u5f31\u6027\u3002", "conclusion": "\u5f3a\u8c03\u4e86\u91cd\u65b0\u8bc4\u4f30\u7b56\u7565\u548c\u5f00\u53d1\u80fd\u591f\u8d85\u8d8a\u56fa\u5b9a\u52a8\u4f5c\u6a21\u5f0f\u7684\u6a21\u578b\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u6559\u5b66\u89c6\u9891\u3002"}}
{"id": "2504.06823", "pdf": "https://arxiv.org/pdf/2504.06823", "abs": "https://arxiv.org/abs/2504.06823", "authors": ["Xiaotian Ye", "Mengqi Zhang", "Shu Wu"], "title": "Open Problems and a Hypothetical Path Forward in LLM Knowledge Paradigms", "categories": ["cs.CL"], "comment": "Blog post preprint, work in progress", "summary": "Knowledge is fundamental to the overall capabilities of Large Language Models\n(LLMs). The knowledge paradigm of a model, which dictates how it encodes and\nutilizes knowledge, significantly affects its performance. Despite the\ncontinuous development of LLMs under existing knowledge paradigms, issues\nwithin these frameworks continue to constrain model potential.\n  This blog post highlight three critical open problems limiting model\ncapabilities: (1) challenges in knowledge updating for LLMs, (2) the failure of\nreverse knowledge generalization (the reversal curse), and (3) conflicts in\ninternal knowledge. We review recent progress made in addressing these issues\nand discuss potential general solutions. Based on observations in these areas,\nwe propose a hypothetical paradigm based on Contextual Knowledge Scaling, and\nfurther outline implementation pathways that remain feasible within\ncontemporary techniques. Evidence suggests this approach holds potential to\naddress current shortcomings, serving as our vision for future model paradigms.\n  This blog post aims to provide researchers with a brief overview of progress\nin LLM knowledge systems, while provide inspiration for the development of\nnext-generation model architectures.", "AI": {"task": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u77e5\u8bc6\u8303\u5f0f\u7684\u5c40\u9650\u6027\u53ca\u5176\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u8303\u5f0f\u9650\u5236\u4e86LLM\u7684\u6f5c\u529b\uff0c\u4e9f\u9700\u89e3\u51b3\u77e5\u8bc6\u66f4\u65b0\u3001\u53cd\u5411\u77e5\u8bc6\u6cdb\u5316\uff08\u9006\u8f6c\u8bc5\u5492\uff09\u548c\u5185\u90e8\u77e5\u8bc6\u51b2\u7a81\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e0a\u4e0b\u6587\u77e5\u8bc6\u6269\u5c55\uff08Contextual Knowledge Scaling\uff09\u7684\u5047\u8bbe\u8303\u5f0f\uff0c\u5e76\u8ba8\u8bba\u5176\u5b9e\u73b0\u8def\u5f84\u3002", "result": "\u8be5\u8303\u5f0f\u6709\u671b\u89e3\u51b3\u5f53\u524dLLM\u77e5\u8bc6\u7cfb\u7edf\u7684\u4e0d\u8db3\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u6a21\u578b\u67b6\u6784\u63d0\u4f9b\u7075\u611f\u3002", "conclusion": "\u4e3a\u7814\u7a76\u8005\u63d0\u4f9bLLM\u77e5\u8bc6\u7cfb\u7edf\u7684\u8fdb\u5c55\u6982\u8ff0\uff0c\u5e76\u542f\u53d1\u672a\u6765\u6a21\u578b\u67b6\u6784\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.06606", "pdf": "https://arxiv.org/pdf/2504.06606", "abs": "https://arxiv.org/abs/2504.06606", "authors": ["Minghe Gao", "Xuqi Liu", "Zhongqi Yue", "Yang Wu", "Shuang Chen", "Juncheng Li", "Siliang Tang", "Fei Wu", "Tat-Seng Chua", "Yueting Zhuang"], "title": "Benchmarking Multimodal CoT Reward Model Stepwise by Visual Program", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in reward signal usage for Large Language Models (LLMs)\nare remarkable. However, significant challenges exist when transitioning reward\nsignal to the multimodal domain, including labor-intensive annotations,\nover-reliance on one-step rewards, and inadequate evaluation. To address these\nissues, we propose SVIP, a novel approach to train a step-level\nmulti-dimensional Chain-of-Thought~(CoT) reward model automatically. It\ngenerates code for solving visual tasks and transforms the analysis of code\nblocks into the evaluation of CoT step as training samples. Then, we train\nSVIP-Reward model using a multi-head attention mechanism called TriAtt-CoT. The\nadvantages of SVIP-Reward are evident throughout the entire process of MLLM. We\nalso introduce a benchmark for CoT reward model training and testing.\nExperimental results demonstrate that SVIP-Reward improves MLLM performance\nacross training and inference-time scaling, yielding better results on\nbenchmarks while reducing hallucinations and enhancing reasoning ability.", "AI": {"task": "\u63d0\u51faSVIP\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u591a\u7ef4\u5ea6\u3001\u6b65\u9aa4\u7ea7\u7684Chain-of-Thought\uff08CoT\uff09\u5956\u52b1\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u9886\u57df\u4e2d\u7684\u5956\u52b1\u4fe1\u53f7\u95ee\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u9886\u57df\u4e2d\u5956\u52b1\u4fe1\u53f7\u7684\u5e94\u7528\u5b58\u5728\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u5bf9\u5355\u6b65\u5956\u52b1\u4f9d\u8d56\u8fc7\u5ea6\u4ee5\u53ca\u8bc4\u4f30\u4e0d\u8db3\u7b49\u6311\u6218\u3002", "method": "\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u89c6\u89c9\u4efb\u52a1\u4ee3\u7801\u5e76\u5c06\u4ee3\u7801\u5757\u5206\u6790\u8f6c\u5316\u4e3aCoT\u6b65\u9aa4\u8bc4\u4f30\uff0c\u5229\u7528TriAtt-CoT\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u8bad\u7ec3SVIP-Reward\u6a21\u578b\u3002", "result": "SVIP-Reward\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u8868\u73b0\u4f18\u5f02\uff0c\u51cf\u5c11\u4e86\u5e7b\u89c9\u5e76\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "SVIP\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7528\u4e8eCoT\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u548c\u6d4b\u8bd5\u7684\u57fa\u51c6\u3002"}}
{"id": "2504.06843", "pdf": "https://arxiv.org/pdf/2504.06843", "abs": "https://arxiv.org/abs/2504.06843", "authors": ["Angela Lopez-Cardona", "Sebastian Idesis", "Ioannis Arapakis"], "title": "Integrating Cognitive Processing Signals into Language Models: A Review of Advances, Applications and Future Directions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, the integration of cognitive neuroscience in Natural Language\nProcessing (NLP) has gained significant attention. This article provides a\ncritical and timely overview of recent advancements in leveraging cognitive\nsignals, particularly Eye-tracking (ET) signals, to enhance Language Models\n(LMs) and Multimodal Large Language Models (MLLMs). By incorporating\nuser-centric cognitive signals, these approaches address key challenges,\nincluding data scarcity and the environmental costs of training large-scale\nmodels. Cognitive signals enable efficient data augmentation, faster\nconvergence, and improved human alignment. The review emphasises the potential\nof ET data in tasks like Visual Question Answering (VQA) and mitigating\nhallucinations in MLLMs, and concludes by discussing emerging challenges and\nresearch trends.", "AI": {"task": "\u7efc\u8ff0\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u4fe1\u53f7\uff08\u5c24\u5176\u662f\u773c\u52a8\u8ffd\u8e2a\u4fe1\u53f7\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u8bad\u7ec3\u5927\u89c4\u6a21\u6a21\u578b\u7684\u73af\u5883\u6210\u672c\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6548\u7387\u548c\u4eba\u7c7b\u5bf9\u9f50\u6027\u3002", "method": "\u901a\u8fc7\u6574\u5408\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8ba4\u77e5\u4fe1\u53f7\uff08\u5982\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\uff09\u6765\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u8ba4\u77e5\u4fe1\u53f7\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u589e\u5f3a\u3001\u66f4\u5feb\u6536\u655b\u548c\u66f4\u597d\u7684\u4eba\u7c7b\u5bf9\u9f50\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u95ee\u7b54\u548c\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u63a2\u8ba8\u4e86\u65b0\u5174\u6311\u6218\u548c\u7814\u7a76\u8d8b\u52bf\uff0c\u5f3a\u8c03\u4e86\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u5728\u672a\u6765\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.06607", "pdf": "https://arxiv.org/pdf/2504.06607", "abs": "https://arxiv.org/abs/2504.06607", "authors": ["Onkar Krishna", "Hiroki Ohashi"], "title": "Visually Similar Pair Alignment for Robust Cross-Domain Object Detection", "categories": ["cs.CV"], "comment": "15 pages, Journal paper submission", "summary": "Domain gaps between training data (source) and real-world environments\n(target) often degrade the performance of object detection models. Most\nexisting methods aim to bridge this gap by aligning features across source and\ntarget domains but often fail to account for visual differences, such as color\nor orientation, in alignment pairs. This limitation leads to less effective\ndomain adaptation, as the model struggles to manage both domain-specific shifts\n(e.g., fog) and visual variations simultaneously. In this work, we demonstrate\nfor the first time, using a custom-built dataset, that aligning visually\nsimilar pairs significantly improves domain adaptation. Based on this insight,\nwe propose a novel memory-based system to enhance domain alignment. This system\nstores precomputed features of foreground objects and background areas from the\nsource domain, which are periodically updated during training. By retrieving\nvisually similar source features for alignment with target foreground and\nbackground features, the model effectively addresses domain-specific\ndifferences while reducing the impact of visual variations. Extensive\nexperiments across diverse domain shift scenarios validate our method's\neffectiveness, achieving 53.1 mAP on Foggy Cityscapes and 62.3 on Sim10k,\nsurpassing prior state-of-the-art methods by 1.2 and 4.1 mAP, respectively.", "AI": {"task": "\u901a\u8fc7\u89c6\u89c9\u76f8\u4f3c\u6027\u5bf9\u9f50\u6e90\u57df\u548c\u76ee\u6807\u57df\u7684\u7279\u5f81\uff0c\u4ee5\u6539\u8fdb\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u7684\u57df\u9002\u5e94\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u57df\u7279\u5f81\u5bf9\u9f50\u65f6\u672a\u80fd\u5145\u5206\u8003\u8651\u89c6\u89c9\u5dee\u5f02\uff08\u5982\u989c\u8272\u6216\u65b9\u5411\uff09\uff0c\u5bfc\u81f4\u57df\u9002\u5e94\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u7cfb\u7edf\uff0c\u5b58\u50a8\u6e90\u57df\u7684\u524d\u666f\u548c\u80cc\u666f\u7279\u5f81\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u52a8\u6001\u66f4\u65b0\uff0c\u901a\u8fc7\u68c0\u7d22\u89c6\u89c9\u76f8\u4f3c\u7684\u7279\u5f81\u5bf9\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "\u5728Foggy Cityscapes\u548cSim10k\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523053.1\u548c62.3 mAP\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u89c6\u89c9\u76f8\u4f3c\u6027\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u57df\u9002\u5e94\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5904\u7406\u89c6\u89c9\u5dee\u5f02\u548c\u57df\u7279\u5f02\u6027\u53d8\u5316\u65f6\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2504.06868", "pdf": "https://arxiv.org/pdf/2504.06868", "abs": "https://arxiv.org/abs/2504.06868", "authors": ["Seungwon Lim", "Seungbeen Lee", "Dongjun Min", "Youngjae Yu"], "title": "Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Artificial agents are increasingly central to complex interactions and\ndecision-making tasks, yet aligning their behaviors with desired human values\nremains an open challenge. In this work, we investigate how human-like\npersonality traits influence agent behavior and performance within text-based\ninteractive environments. We introduce PANDA: PersonalityAdapted Neural\nDecision Agents, a novel method for projecting human personality traits onto\nagents to guide their behavior. To induce personality in a text-based game\nagent, (i) we train a personality classifier to identify what personality type\nthe agent's actions exhibit, and (ii) we integrate the personality profiles\ndirectly into the agent's policy-learning pipeline. By deploying agents\nembodying 16 distinct personality types across 25 text-based games and\nanalyzing their trajectories, we demonstrate that an agent's action decisions\ncan be guided toward specific personality profiles. Moreover, certain\npersonality types, such as those characterized by higher levels of Openness,\ndisplay marked advantages in performance. These findings underscore the promise\nof personality-adapted agents for fostering more aligned, effective, and\nhuman-centric decision-making in interactive environments.", "AI": {"task": "\u7814\u7a76\u4eba\u7c7b\u6027\u683c\u7279\u5f81\u5982\u4f55\u5f71\u54cd\u6587\u672c\u4ea4\u4e92\u73af\u5883\u4e2d\u667a\u80fd\u4ee3\u7406\u7684\u884c\u4e3a\u4e0e\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u5de5\u4ee3\u7406\u5728\u590d\u6742\u4ea4\u4e92\u548c\u51b3\u7b56\u4efb\u52a1\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u5176\u884c\u4e3a\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faPANDA\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4eba\u7c7b\u6027\u683c\u7279\u5f81\u6620\u5c04\u5230\u4ee3\u7406\u4e0a\u6307\u5bfc\u5176\u884c\u4e3a\uff0c\u5305\u62ec\u8bad\u7ec3\u6027\u683c\u5206\u7c7b\u5668\u548c\u5c06\u6027\u683c\u7279\u5f81\u6574\u5408\u5230\u7b56\u7565\u5b66\u4e60\u6d41\u7a0b\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ee3\u7406\u7684\u884c\u4e3a\u53ef\u4ee5\u88ab\u5f15\u5bfc\u81f3\u7279\u5b9a\u6027\u683c\u7279\u5f81\uff0c\u4e14\u67d0\u4e9b\u6027\u683c\u7c7b\u578b\uff08\u5982\u5f00\u653e\u6027\u8f83\u9ad8\uff09\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u6027\u683c\u9002\u914d\u4ee3\u7406\u5728\u4fc3\u8fdb\u66f4\u5bf9\u9f50\u3001\u6709\u6548\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u51b3\u7b56\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2504.06608", "pdf": "https://arxiv.org/pdf/2504.06608", "abs": "https://arxiv.org/abs/2504.06608", "authors": ["Jiajun Chen", "Hongpeng Yin", "Yifu Yang"], "title": "A Cross-Domain Few-Shot Learning Method Based on Domain Knowledge Mapping", "categories": ["cs.CV"], "comment": null, "summary": "In task-based few-shot learning paradigms, it is commonly assumed that\ndifferent tasks are independently and identically distributed (i.i.d.).\nHowever, in real-world scenarios, the distribution encountered in few-shot\nlearning can significantly differ from the distribution of existing data. Thus,\nhow to effectively leverage existing data knowledge to enable models to quickly\nadapt to class variations under non-i.i.d. assumptions has emerged as a key\nresearch challenge. To address this challenge, this paper proposes a new\ncross-domain few-shot learning approach based on domain knowledge mapping,\napplied consistently throughout the pre-training, training, and testing phases.\nIn the pre-training phase, our method integrates self-supervised and supervised\nlosses by maximizing mutual information, thereby mitigating mode collapse.\nDuring the training phase, the domain knowledge mapping layer collaborates with\na domain classifier to learn both domain mapping capabilities and the ability\nto assess domain adaptation difficulty. Finally, this approach is applied\nduring the testing phase, rapidly adapting to domain variations through\nmeta-training tasks on support sets, consequently enhancing the model's\ncapability to transfer domain knowledge effectively. Experimental validation\nconducted across six datasets from diverse domains demonstrates the\neffectiveness of the proposed method.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u57df\u77e5\u8bc6\u6620\u5c04\u7684\u8de8\u57df\u5c0f\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u975e\u72ec\u7acb\u540c\u5206\u5e03\u5047\u8bbe\u4e0b\u7684\u4efb\u52a1\u9002\u5e94\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u5c0f\u6837\u672c\u5b66\u4e60\u7684\u5206\u5e03\u53ef\u80fd\u4e0e\u73b0\u6709\u6570\u636e\u5206\u5e03\u663e\u8457\u4e0d\u540c\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528\u73b0\u6709\u6570\u636e\u77e5\u8bc6\u5feb\u901f\u9002\u5e94\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7684\u4efb\u52a1\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u5728\u9884\u8bad\u7ec3\u3001\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u4e00\u81f4\u5e94\u7528\u57df\u77e5\u8bc6\u6620\u5c04\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u548c\u76d1\u7763\u635f\u5931\uff0c\u5e76\u5f15\u5165\u57df\u5206\u7c7b\u5668\u5b66\u4e60\u57df\u6620\u5c04\u80fd\u529b\u548c\u8bc4\u4f30\u57df\u9002\u5e94\u96be\u5ea6\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u57df\u77e5\u8bc6\u6620\u5c04\u548c\u5143\u8bad\u7ec3\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8de8\u57df\u5c0f\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2504.06910", "pdf": "https://arxiv.org/pdf/2504.06910", "abs": "https://arxiv.org/abs/2504.06910", "authors": ["Sheng Lu", "Ilia Kuznetsov", "Iryna Gurevych"], "title": "Identifying Aspects in Peer Reviews", "categories": ["cs.CL"], "comment": null, "summary": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspect sets from review forms and guidelines of major NLP venues, yet\ndata-driven methods for aspect identification are largely underexplored. To\naddress this gap, our work takes a bottom-up approach: we propose an\noperational definition of aspect and develop a data-driven schema for deriving\nfine-grained aspects from a corpus of peer reviews. We introduce a dataset of\npeer reviews augmented with aspects and show how it can be used for\ncommunity-level review analysis. We further show how the choice of aspects can\nimpact downstream applications, such as LLM-generated review detection. Our\nresults lay a foundation for a principled and data-driven investigation of\nreview aspects, and pave the path for new applications of NLP to support peer\nreview.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u4ece\u540c\u884c\u8bc4\u5ba1\u4e2d\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7684\u8bc4\u5ba1\u65b9\u9762\uff0c\u4ee5\u652f\u6301\u6807\u51c6\u5316\u8bc4\u5ba1\u8fc7\u7a0b\u3002", "motivation": "\u540c\u884c\u8bc4\u5ba1\u662f\u5b66\u672f\u51fa\u7248\u7684\u6838\u5fc3\uff0c\u4f46\u65e5\u76ca\u589e\u957f\u7684\u6295\u7a3f\u91cf\u4f7f\u5176\u538b\u529b\u589e\u5927\uff0c\u9700\u8981\u8ba1\u7b97\u65b9\u6cd5\u7684\u652f\u6301\u3002\u8bc4\u5ba1\u65b9\u9762\uff08\u5982\u65b0\u9896\u6027\uff09\u53cd\u6620\u4e86\u7814\u7a76\u793e\u533a\u7684\u4ef7\u503c\u89c2\uff0c\u6807\u51c6\u5316\u8bc4\u5ba1\u8fc7\u7a0b\u53ef\u4ee5\u63d0\u9ad8\u8d28\u91cf\u63a7\u5236\u5e76\u652f\u6301\u8ba1\u7b97\u8f85\u52a9\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u6cd5\uff0c\u5b9a\u4e49\u8bc4\u5ba1\u65b9\u9762\u7684\u64cd\u4f5c\u5316\u6982\u5ff5\uff0c\u5e76\u4ece\u8bc4\u5ba1\u8bed\u6599\u5e93\u4e2d\u63d0\u53d6\u7ec6\u7c92\u5ea6\u65b9\u9762\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u5e26\u6709\u6807\u6ce8\u65b9\u9762\u7684\u8bc4\u5ba1\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u793e\u533a\u7ea7\u8bc4\u5ba1\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u5c55\u793a\u4e86\u8bc4\u5ba1\u65b9\u9762\u7684\u9009\u62e9\u5bf9\u4e0b\u6e38\u5e94\u7528\uff08\u5982LLM\u751f\u6210\u8bc4\u5ba1\u68c0\u6d4b\uff09\u7684\u5f71\u54cd\uff0c\u4e3a\u8bc4\u5ba1\u65b9\u9762\u7684\u6570\u636e\u9a71\u52a8\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u4e3aNLP\u5728\u652f\u6301\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u65b0\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u63d0\u4f9b\u4e86\u8bc4\u5ba1\u65b9\u9762\u7684\u539f\u5219\u6027\u548c\u6570\u636e\u9a71\u52a8\u7814\u7a76\u65b9\u6cd5\u3002"}}
{"id": "2504.06618", "pdf": "https://arxiv.org/pdf/2504.06618", "abs": "https://arxiv.org/abs/2504.06618", "authors": ["Zijun Lin", "M Ganesh Kumar", "Cheston Tan"], "title": "Human-like compositional learning of visually-grounded concepts using synthetic environments", "categories": ["cs.CV"], "comment": null, "summary": "The compositional structure of language enables humans to decompose complex\nphrases and map them to novel visual concepts, showcasing flexible\nintelligence. While several algorithms exhibit compositionality, they fail to\nelucidate how humans learn to compose concept classes and ground visual cues\nthrough trial and error. To investigate this multi-modal learning challenge, we\ndesigned a 3D synthetic environment in which an agent learns, via\nreinforcement, to navigate to a target specified by a natural language\ninstruction. These instructions comprise nouns, attributes, and critically,\ndeterminers, prepositions, or both. The vast array of word combinations\nheightens the compositional complexity of the visual grounding task, as\nnavigating to a blue cube above red spheres is not rewarded when the\ninstruction specifies navigating to \"some blue cubes below the red sphere\". We\nfirst demonstrate that reinforcement learning agents can ground determiner\nconcepts to visual targets but struggle with more complex prepositional\nconcepts. Second, we show that curriculum learning, a strategy humans employ,\nenhances concept learning efficiency, reducing the required training episodes\nby 15% in determiner environments and enabling agents to easily learn\nprepositional concepts. Finally, we establish that agents trained on determiner\nor prepositional concepts can decompose held-out test instructions and rapidly\nadapt their navigation policies to unseen visual object combinations.\nLeveraging synthetic environments, our findings demonstrate that multi-modal\nreinforcement learning agents can achieve compositional understanding of\ncomplex concept classes and highlight the efficacy of human-like learning\nstrategies in improving artificial systems' learning efficiency.", "AI": {"task": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u8ba9\u667a\u80fd\u4f53\u5728\u5408\u6210\u73af\u5883\u4e2d\u5b66\u4e60\u5e76\u7406\u89e3\u590d\u6742\u7684\u8bed\u8a00\u6307\u4ee4\u4e0e\u89c6\u89c9\u6982\u5ff5\u7684\u7ec4\u5408\u3002", "motivation": "\u63a2\u7d22\u4eba\u7c7b\u5982\u4f55\u901a\u8fc7\u8bd5\u9519\u5b66\u4e60\u7ec4\u5408\u6982\u5ff5\u7c7b\u522b\u5e76\u6620\u5c04\u89c6\u89c9\u7ebf\u7d22\uff0c\u4ee5\u53ca\u5982\u4f55\u5c06\u8fd9\u79cd\u80fd\u529b\u5e94\u7528\u4e8e\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a3D\u5408\u6210\u73af\u5883\uff0c\u667a\u80fd\u4f53\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5bfc\u822a\u81f3\u7531\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6307\u5b9a\u7684\u76ee\u6807\uff0c\u6307\u4ee4\u5305\u542b\u540d\u8bcd\u3001\u5c5e\u6027\u3001\u9650\u5b9a\u8bcd\u548c\u4ecb\u8bcd\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u80fd\u591f\u7406\u89e3\u9650\u5b9a\u8bcd\u6982\u5ff5\uff0c\u4f46\u5728\u4ecb\u8bcd\u6982\u5ff5\u4e0a\u8868\u73b0\u8f83\u5dee\uff1b\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u8bad\u7ec3\u6b21\u6570\u3002", "conclusion": "\u591a\u6a21\u6001\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u80fd\u591f\u5b9e\u73b0\u590d\u6742\u6982\u5ff5\u7c7b\u522b\u7684\u7ec4\u5408\u7406\u89e3\uff0c\u4eba\u7c7b\u7c7b\u4f3c\u7684\u5b66\u4e60\u7b56\u7565\u53ef\u663e\u8457\u63d0\u5347\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u7684\u5b66\u4e60\u6548\u7387\u3002"}}
{"id": "2504.06917", "pdf": "https://arxiv.org/pdf/2504.06917", "abs": "https://arxiv.org/abs/2504.06917", "authors": ["Ming Liu", "Massimo Poesio"], "title": "Data Augmentation for Fake Reviews Detection in Multiple Languages and Multiple Domains", "categories": ["cs.CL"], "comment": "32 pages, 15 figures", "summary": "With the growth of the Internet, buying habits have changed, and customers\nhave become more dependent on the online opinions of other customers to guide\ntheir purchases. Identifying fake reviews thus became an important area for\nNatural Language Processing (NLP) research. However, developing\nhigh-performance NLP models depends on the availability of large amounts of\ntraining data, which are often not available for low-resource languages or\ndomains. In this research, we used large language models to generate datasets\nto train fake review detectors. Our approach was used to generate fake reviews\nin different domains (book reviews, restaurant reviews, and hotel reviews) and\ndifferent languages (English and Chinese). Our results demonstrate that our\ndata augmentation techniques result in improved performance at fake review\ndetection for all domains and languages. The accuracy of our fake review\ndetection model can be improved by 0.3 percentage points on DeRev TEST, 10.9\npercentage points on Amazon TEST, 8.3 percentage points on Yelp TEST and 7.2\npercentage points on DianPing TEST using the augmented datasets.", "AI": {"task": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6570\u636e\u96c6\u4ee5\u8bad\u7ec3\u865a\u5047\u8bc4\u8bba\u68c0\u6d4b\u5668\u3002", "motivation": "\u968f\u7740\u4e92\u8054\u7f51\u7684\u53d1\u5c55\uff0c\u865a\u5047\u8bc4\u8bba\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4f46\u4f4e\u8d44\u6e90\u8bed\u8a00\u6216\u9886\u57df\u7f3a\u4e4f\u8db3\u591f\u7684\u8bad\u7ec3\u6570\u636e\u3002", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e0d\u540c\u9886\u57df\uff08\u4e66\u7c4d\u3001\u9910\u5385\u3001\u9152\u5e97\uff09\u548c\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u4e2d\u6587\uff09\u7684\u865a\u5047\u8bc4\u8bba\u6570\u636e\u96c6\u3002", "result": "\u6570\u636e\u589e\u5f3a\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u865a\u5047\u8bc4\u8bba\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u5404\u6d4b\u8bd5\u96c6\u7684\u51c6\u786e\u7387\u5747\u6709\u63d0\u5347\uff08\u5982DeRev TEST\u63d0\u53470.3\u4e2a\u767e\u5206\u70b9\uff0cAmazon TEST\u63d0\u534710.9\u4e2a\u767e\u5206\u70b9\u7b49\uff09\u3002", "conclusion": "\u901a\u8fc7\u751f\u6210\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u865a\u5047\u8bc4\u8bba\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u9886\u57df\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2504.06620", "pdf": "https://arxiv.org/pdf/2504.06620", "abs": "https://arxiv.org/abs/2504.06620", "authors": ["Yi Zhang", "Xiaoyang Huang", "Yishun Dou", "Yue Shi", "Rui Shi", "Ye Chen", "Bingbing Ni", "Wenjun Zhang"], "title": "InstantSticker: Realistic Decal Blending via Disentangled Object Reconstruction", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2025", "summary": "We present InstantSticker, a disentangled reconstruction pipeline based on\nImage-Based Lighting (IBL), which focuses on highly realistic decal blending,\nsimulates stickers attached to the reconstructed surface, and allows for\ninstant editing and real-time rendering. To achieve stereoscopic impression of\nthe decal, we introduce shadow factor into IBL, which can be adaptively\noptimized during training. This allows the shadow brightness of surfaces to be\naccurately decomposed rather than baked into the diffuse color, ensuring that\nthe edited texture exhibits authentic shading. To address the issues of warping\nand blurriness in previous methods, we apply As-Rigid-As-Possible (ARAP)\nparameterization to pre-unfold a specified area of the mesh and use the local\nUV mapping combined with a neural texture map to enhance the ability to express\nhigh-frequency details in that area. For instant editing, we utilize the Disney\nBRDF model, explicitly defining material colors with 3-channel diffuse albedo.\nThis enables instant replacement of albedo RGB values during the editing\nprocess, avoiding the prolonged optimization required in previous approaches.\nIn our experiment, we introduce the Ratio Variance Warping (RVW) metric to\nevaluate the local geometric warping of the decal area. Extensive experimental\nresults demonstrate that our method surpasses previous decal blending methods\nin terms of editing quality, editing speed and rendering speed, achieving the\nstate-of-the-art.", "AI": {"task": "\u63d0\u51faInstantSticker\uff0c\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u7167\u660e\uff08IBL\uff09\u7684\u89e3\u8026\u91cd\u5efa\u6d41\u7a0b\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u771f\u5b9e\u611f\u7684\u8d34\u56fe\u6df7\u5408\u3001\u6a21\u62df\u8d34\u7eb8\u9644\u7740\u6548\u679c\uff0c\u5e76\u652f\u6301\u5373\u65f6\u7f16\u8f91\u548c\u5b9e\u65f6\u6e32\u67d3\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4e2d\u8d34\u56fe\u6df7\u5408\u7684\u9634\u5f71\u3001\u53d8\u5f62\u548c\u6a21\u7cca\u95ee\u9898\uff0c\u63d0\u5347\u7f16\u8f91\u548c\u6e32\u67d3\u6548\u7387\u3002", "method": "\u5f15\u5165\u9634\u5f71\u56e0\u5b50\u4f18\u5316IBL\uff0c\u4f7f\u7528ARAP\u53c2\u6570\u5316\u9884\u5904\u7406\u7f51\u683c\uff0c\u7ed3\u5408\u5c40\u90e8UV\u6620\u5c04\u548c\u795e\u7ecf\u7eb9\u7406\u56fe\uff0c\u91c7\u7528Disney BRDF\u6a21\u578b\u5b9e\u73b0\u5373\u65f6\u7f16\u8f91\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cInstantSticker\u5728\u7f16\u8f91\u8d28\u91cf\u3001\u901f\u5ea6\u548c\u6e32\u67d3\u901f\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u65b0\u6c34\u5e73\u3002", "conclusion": "InstantSticker\u901a\u8fc7\u521b\u65b0\u6280\u672f\u89e3\u51b3\u4e86\u8d34\u56fe\u6df7\u5408\u7684\u5173\u952e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u8d34\u56fe\u7f16\u8f91\u548c\u6e32\u67d3\u3002"}}
{"id": "2504.06947", "pdf": "https://arxiv.org/pdf/2504.06947", "abs": "https://arxiv.org/abs/2504.06947", "authors": ["Natalia Loukachevitch", "Natalia Tkachenko", "Anna Lapanitsyna", "Mikhail Tikhomirov", "Nicolay Rusnachenko"], "title": "RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts", "categories": ["cs.CL", "I.2.7"], "comment": "RuOpinionNE-2024 represent a proceeding of RuSentNE-2023. It\n  contributes with extraction and evaluation of factual statements that support\n  the assigned sentiment", "summary": "In this paper, we introduce the Dialogue Evaluation shared task on extraction\nof structured opinions from Russian news texts. The task of the contest is to\nextract opinion tuples for a given sentence; the tuples are composed of a\nsentiment holder, its target, an expression and sentiment from the holder to\nthe target. In total, the task received more than 100 submissions. The\nparticipants experimented mainly with large language models in zero-shot,\nfew-shot and fine-tuning formats. The best result on the test set was obtained\nwith fine-tuning of a large language model. We also compared 30 prompts and 11\nopen source language models with 3-32 billion parameters in the 1-shot and\n10-shot settings and found the best models and prompts.", "AI": {"task": "\u4ece\u4fc4\u8bed\u65b0\u95fb\u6587\u672c\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u89c2\u70b9\u7684\u5bf9\u8bdd\u8bc4\u4f30\u5171\u4eab\u4efb\u52a1\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u4ece\u4fc4\u8bed\u65b0\u95fb\u6587\u672c\u4e2d\u63d0\u53d6\u60c5\u611f\u6301\u6709\u8005\u3001\u76ee\u6807\u3001\u8868\u8fbe\u548c\u60c5\u611f\u7684\u89c2\u70b9\u5143\u7ec4\u3002", "method": "\u53c2\u4e0e\u8005\u4e3b\u8981\u5c1d\u8bd5\u4e86\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5bf930\u79cd\u63d0\u793a\u548c11\u79cd\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u5728\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u4f73\u7ed3\u679c\uff0c\u5e76\u786e\u5b9a\u4e86\u6700\u4f73\u6a21\u578b\u548c\u63d0\u793a\u3002", "conclusion": "\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4fc4\u8bed\u65b0\u95fb\u6587\u672c\u89c2\u70b9\u63d0\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2504.06627", "pdf": "https://arxiv.org/pdf/2504.06627", "abs": "https://arxiv.org/abs/2504.06627", "authors": ["Ludvig Dill\u00e9n", "Per-Erik Forss\u00e9n", "Johan Edstedt"], "title": "FACT: Multinomial Misalignment Classification for Point Cloud Registration", "categories": ["cs.CV", "cs.LG", "I.4.5; I.4.8; I.2.9; I.2.10"], "comment": "Accepted at SCIA 2025 (the Scandinavian Conference on Image Analysis\n  2025)", "summary": "We present FACT, a method for predicting alignment quality (i.e.,\nregistration error) of registered lidar point cloud pairs. This is useful e.g.\nfor quality assurance of large, automatically registered 3D models. FACT\nextracts local features from a registered pair and processes them with a point\ntransformer-based network to predict a misalignment class. We generalize prior\nwork that study binary alignment classification of registration errors, by\nrecasting it as multinomial misalignment classification. To achieve this, we\nintroduce a custom regression-by-classification loss function that combines the\ncross-entropy and Wasserstein losses, and demonstrate that it outperforms both\ndirect regression and prior binary classification. FACT successfully classifies\npoint-cloud pairs registered with both the classical ICP and GeoTransformer,\nwhile other choices, such as standard point-cloud-quality metrics and\nregistration residuals are shown to be poor choices for predicting\nmisalignment. On a synthetically perturbed point-cloud task introduced by the\nCorAl method, we show that FACT achieves substantially better performance than\nCorAl. Finally, we demonstrate how FACT can assist experts in correcting\nmisaligned point-cloud maps. Our code is available at\nhttps://github.com/LudvigDillen/FACT_for_PCMC.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aFACT\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u5df2\u6ce8\u518c\u6fc0\u5149\u96f7\u8fbe\u70b9\u4e91\u5bf9\u7684\u914d\u51c6\u8d28\u91cf\uff08\u5373\u914d\u51c6\u8bef\u5dee\uff09\u3002", "motivation": "\u4e3a\u5927\u89c4\u6a21\u81ea\u52a8\u914d\u51c6\u76843D\u6a21\u578b\u63d0\u4f9b\u8d28\u91cf\u4fdd\u8bc1\u3002", "method": "FACT\u4ece\u914d\u51c6\u7684\u70b9\u4e91\u5bf9\u4e2d\u63d0\u53d6\u5c40\u90e8\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u70b9\u53d8\u6362\u5668\u7684\u7f51\u7edc\u5904\u7406\u8fd9\u4e9b\u7279\u5f81\u4ee5\u9884\u6d4b\u914d\u51c6\u8bef\u5dee\u7c7b\u522b\u3002", "result": "FACT\u5728\u914d\u51c6\u8bef\u5dee\u7684\u591a\u7c7b\u522b\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u76f4\u63a5\u56de\u5f52\u548c\u5148\u524d\u7684\u4e8c\u5143\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u5728\u5408\u6210\u6270\u52a8\u70b9\u4e91\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eCorAl\u65b9\u6cd5\u3002", "conclusion": "FACT\u4e0d\u4ec5\u80fd\u6210\u529f\u5206\u7c7b\u4e0d\u540c\u914d\u51c6\u65b9\u6cd5\u7684\u70b9\u4e91\u5bf9\uff0c\u8fd8\u80fd\u8f85\u52a9\u4e13\u5bb6\u7ea0\u6b63\u914d\u51c6\u9519\u8bef\u7684\u70b9\u4e91\u5730\u56fe\u3002"}}
{"id": "2504.06969", "pdf": "https://arxiv.org/pdf/2504.06969", "abs": "https://arxiv.org/abs/2504.06969", "authors": ["Lilian Ngweta", "Kiran Kate", "Jason Tsay", "Yara Rizk"], "title": "Towards LLMs Robustness to Changes in Prompt Format Styles", "categories": ["cs.CL"], "comment": "NAACL Student Research Workshop (SRW) 2025", "summary": "Large language models (LLMs) have gained popularity in recent years for their\nutility in various applications. However, they are sensitive to non-semantic\nchanges in prompt formats, where small changes in the prompt format can lead to\nsignificant performance fluctuations. In the literature, this problem is\ncommonly referred to as prompt brittleness. Previous research on prompt\nengineering has focused mainly on developing techniques for identifying the\noptimal prompt for specific tasks. Some studies have also explored the issue of\nprompt brittleness and proposed methods to quantify performance variations;\nhowever, no simple solution has been found to address this challenge. We\npropose Mixture of Formats (MOF), a simple and efficient technique for\naddressing prompt brittleness in LLMs by diversifying the styles used in the\nprompt few-shot examples. MOF was inspired by computer vision techniques that\nutilize diverse style datasets to prevent models from associating specific\nstyles with the target variable. Empirical results show that our proposed\ntechnique reduces style-induced prompt brittleness in various LLMs while also\nenhancing overall performance across prompt variations and different datasets.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aMixture of Formats (MOF)\u7684\u7b80\u5355\u9ad8\u6548\u6280\u672f\uff0c\u4ee5\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u4e2d\u63d0\u793a\u8106\u5f31\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u63d0\u793a\u683c\u5f0f\u7684\u975e\u8bed\u4e49\u53d8\u5316\u654f\u611f\uff0c\u5bfc\u81f4\u6027\u80fd\u6ce2\u52a8\uff0c\u73b0\u6709\u7814\u7a76\u672a\u80fd\u63d0\u4f9b\u7b80\u5355\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u591a\u6837\u5316\u63d0\u793afew-shot\u793a\u4f8b\u7684\u98ce\u683c\uff0c\u501f\u9274\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u591a\u6837\u98ce\u683c\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMOF\u51cf\u5c11\u4e86\u98ce\u683c\u5f15\u8d77\u7684\u63d0\u793a\u8106\u5f31\u6027\uff0c\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u63d0\u793a\u53d8\u5316\u548c\u6570\u636e\u96c6\u4e0a\u7684\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "MOF\u662f\u4e00\u79cd\u6709\u6548\u7684\u6280\u672f\uff0c\u80fd\u591f\u7f13\u89e3LLMs\u4e2d\u7684\u63d0\u793a\u8106\u5f31\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.06629", "pdf": "https://arxiv.org/pdf/2504.06629", "abs": "https://arxiv.org/abs/2504.06629", "authors": ["MinKyu Lee", "Sangeek Hyun", "Woojin Jun", "Hyunjun Kim", "Jiwoo Chung", "Jae-Pil Heo"], "title": "Rethinking LayerNorm in Image Restoration Transformers", "categories": ["cs.CV"], "comment": null, "summary": "This work investigates abnormal feature behaviors observed in image\nrestoration (IR) Transformers. Specifically, we identify two critical issues:\nfeature entropy becoming excessively small and feature magnitudes diverging up\nto a million-fold scale. We pinpoint the root cause to the per-token\nnormalization aspect of conventional LayerNorm, which disrupts essential\nspatial correlations and internal feature statistics. To address this, we\npropose a simple normalization strategy tailored for IR Transformers. Our\napproach applies normalization across the entire spatio-channel dimension,\neffectively preserving spatial correlations. Additionally, we introduce an\ninput-adaptive rescaling method that aligns feature statistics to the unique\nstatistical requirements of each input. Experimental results verify that this\ncombined strategy effectively resolves feature divergence, significantly\nenhancing both the stability and performance of IR Transformers across various\nIR tasks.", "AI": {"task": "\u7814\u7a76\u56fe\u50cf\u6062\u590d\uff08IR\uff09Transformer\u4e2d\u5f02\u5e38\u7279\u5f81\u884c\u4e3a\u7684\u95ee\u9898\u3002", "motivation": "\u53d1\u73b0\u4f20\u7edfLayerNorm\u7684\u9010\u4ee4\u724c\u5f52\u4e00\u5316\u4f1a\u7834\u574f\u7a7a\u95f4\u76f8\u5173\u6027\u548c\u5185\u90e8\u7279\u5f81\u7edf\u8ba1\uff0c\u5bfc\u81f4\u7279\u5f81\u71b5\u8fc7\u5c0f\u548c\u7279\u5f81\u5e45\u5ea6\u6781\u5ea6\u53d1\u6563\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9IR Transformer\u7684\u5f52\u4e00\u5316\u7b56\u7565\uff0c\u8de8\u6574\u4e2a\u7a7a\u95f4-\u901a\u9053\u7ef4\u5ea6\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u5e76\u5f15\u5165\u8f93\u5165\u81ea\u9002\u5e94\u7684\u91cd\u65b0\u7f29\u653e\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7279\u5f81\u53d1\u6563\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86IR Transformer\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f52\u4e00\u5316\u7b56\u7565\u548c\u81ea\u9002\u5e94\u65b9\u6cd5\u80fd\u591f\u4f18\u5316IR Transformer\u7684\u7279\u5f81\u5904\u7406\uff0c\u9002\u7528\u4e8e\u591a\u79cdIR\u4efb\u52a1\u3002"}}
{"id": "2504.07022", "pdf": "https://arxiv.org/pdf/2504.07022", "abs": "https://arxiv.org/abs/2504.07022", "authors": ["Chad Melton", "Alex Sorokine", "Steve Peterson"], "title": "Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety", "categories": ["cs.CL"], "comment": "14 pages, 3 Figures, 3 tables", "summary": "Applications of generative Large Language Models LLMs are rapidly expanding\nacross various domains, promising significant improvements in workflow\nefficiency and information retrieval. However, their implementation in\nspecialized, high-stakes domains such as hazardous materials transportation is\nchallenging due to accuracy and reliability concerns. This study evaluates the\nperformance of three fine-tuned generative models, ChatGPT, Google's Vertex AI,\nand ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in\nretrieving regulatory information essential for hazardous material\ntransportation compliance in the United States. Utilizing approximately 40\npublicly available federal and state regulatory documents, we developed 100\nrealistic queries relevant to route planning and permitting requirements.\nResponses were qualitatively rated based on accuracy, detail, and relevance,\ncomplemented by quantitative assessments of semantic similarity between model\noutputs. Results demonstrated that the RAG-augmented LLaMA models significantly\noutperformed Vertex AI and ChatGPT, providing more detailed and generally\naccurate information, despite occasional inconsistencies. This research\nintroduces the first known application of RAG in transportation safety,\nemphasizing the need for domain-specific fine-tuning and rigorous evaluation\nmethodologies to ensure reliability and minimize the risk of inaccuracies in\nhigh-stakes environments.", "AI": {"task": "\u8bc4\u4f30\u4e09\u79cd\u751f\u6210\u6a21\u578b\u5728\u5371\u9669\u6750\u6599\u8fd0\u8f93\u5408\u89c4\u6027\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u9762\u4e34\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u6311\u6218\uff0c\u9700\u8981\u9a8c\u8bc1\u5176\u5728\u5371\u9669\u6750\u6599\u8fd0\u8f93\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u7ea640\u4efd\u516c\u5f00\u7684\u8054\u90a6\u548c\u5dde\u6cd5\u89c4\u6587\u6863\uff0c\u751f\u6210100\u4e2a\u76f8\u5173\u67e5\u8be2\uff0c\u5bf9ChatGPT\u3001Vertex AI\u548cRAG\u589e\u5f3a\u7684LLaMA\u6a21\u578b\u8fdb\u884c\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u3002", "result": "RAG\u589e\u5f3a\u7684LLaMA\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u7ec6\u8282\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5c3d\u7ba1\u5076\u5c14\u5b58\u5728\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u9886\u57df\u7279\u5b9a\u7684\u5fae\u8c03\u548c\u4e25\u683c\u8bc4\u4f30\u65b9\u6cd5\u5bf9\u786e\u4fdd\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u6a21\u578b\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.06632", "pdf": "https://arxiv.org/pdf/2504.06632", "abs": "https://arxiv.org/abs/2504.06632", "authors": ["Yifan Gao", "Zihang Lin", "Chuanbin Liu", "Min Zhou", "Tiezheng Ge", "Bo Zheng", "Hongtao Xie"], "title": "PosterMaker: Towards High-Quality Product Poster Generation with Accurate Text Rendering", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025. Project Page: https://poster-maker.github.io", "summary": "Product posters, which integrate subject, scene, and text, are crucial\npromotional tools for attracting customers. Creating such posters using modern\nimage generation methods is valuable, while the main challenge lies in\naccurately rendering text, especially for complex writing systems like Chinese,\nwhich contains over 10,000 individual characters. In this work, we identify the\nkey to precise text rendering as constructing a character-discriminative visual\nfeature as a control signal. Based on this insight, we propose a robust\ncharacter-wise representation as control and we develop TextRenderNet, which\nachieves a high text rendering accuracy of over 90%. Another challenge in\nposter generation is maintaining the fidelity of user-specific products. We\naddress this by introducing SceneGenNet, an inpainting-based model, and propose\nsubject fidelity feedback learning to further enhance fidelity. Based on\nTextRenderNet and SceneGenNet, we present PosterMaker, an end-to-end generation\nframework. To optimize PosterMaker efficiently, we implement a two-stage\ntraining strategy that decouples text rendering and background generation\nlearning. Experimental results show that PosterMaker outperforms existing\nbaselines by a remarkable margin, which demonstrates its effectiveness.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u4ea7\u54c1\u6d77\u62a5\u751f\u6210\u6846\u67b6PosterMaker\uff0c\u89e3\u51b3\u6587\u672c\u6e32\u67d3\u548c\u4ea7\u54c1\u4fdd\u771f\u5ea6\u7684\u6311\u6218\u3002", "motivation": "\u4ea7\u54c1\u6d77\u62a5\u4f5c\u4e3a\u91cd\u8981\u4fc3\u9500\u5de5\u5177\uff0c\u73b0\u6709\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5728\u590d\u6742\u6587\u672c\uff08\u5982\u4e2d\u6587\uff09\u6e32\u67d3\u548c\u7528\u6237\u7279\u5b9a\u4ea7\u54c1\u4fdd\u771f\u5ea6\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faTextRenderNet\uff08\u5b57\u7b26\u7ea7\u8868\u793a\u63a7\u5236\uff09\u548cSceneGenNet\uff08\u57fa\u4e8e\u4fee\u590d\u7684\u6a21\u578b\uff09\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u4f18\u5316PosterMaker\u3002", "result": "PosterMaker\u6587\u672c\u6e32\u67d3\u51c6\u786e\u7387\u8d85\u8fc790%\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "PosterMaker\u901a\u8fc7\u5b57\u7b26\u7ea7\u63a7\u5236\u548c\u4ea7\u54c1\u4fdd\u771f\u5ea6\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6d77\u62a5\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2504.07024", "pdf": "https://arxiv.org/pdf/2504.07024", "abs": "https://arxiv.org/abs/2504.07024", "authors": ["Alessio Tosolini", "Claire Bowern"], "title": "Data Augmentation and Hyperparameter Tuning for Low-Resource MFA", "categories": ["cs.CL"], "comment": null, "summary": "A continued issue for those working with computational tools and endangered\nand under-resourced languages is the lower accuracy of results for languages\nwith smaller amounts of data. We attempt to ameliorate this issue by using data\naugmentation methods to increase corpus size, comparing augmentation to\nhyperparameter tuning for multilingual forced alignment. Unlike text\naugmentation methods, audio augmentation does not lead to substantially\nincreased performance. Hyperparameter tuning, on the other hand, results in\nsubstantial improvement without (for this amount of data) infeasible additional\ntraining time. For languages with small to medium amounts of training data,\nthis is a workable alternative to adapting models from high-resource languages.", "AI": {"task": "\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u5f3a\u5236\u5bf9\u9f50\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u5de5\u5177\u5728\u5904\u7406\u4f4e\u8d44\u6e90\u548c\u6fd2\u5371\u8bed\u8a00\u65f6\u56e0\u6570\u636e\u91cf\u5c0f\u800c\u5bfc\u81f4\u7684\u51c6\u786e\u6027\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u6bd4\u8f83\u6570\u636e\u589e\u5f3a\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u5728\u5f3a\u5236\u5bf9\u9f50\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002", "result": "\u97f3\u9891\u6570\u636e\u589e\u5f3a\u6548\u679c\u6709\u9650\uff0c\u800c\u8d85\u53c2\u6570\u8c03\u4f18\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u4e14\u8bad\u7ec3\u65f6\u95f4\u53ef\u884c\u3002", "conclusion": "\u5bf9\u4e8e\u4e2d\u5c0f\u89c4\u6a21\u6570\u636e\u7684\u8bed\u8a00\uff0c\u8d85\u53c2\u6570\u8c03\u4f18\u662f\u6bd4\u9ad8\u8d44\u6e90\u8bed\u8a00\u6a21\u578b\u8fc1\u79fb\u66f4\u53ef\u884c\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2504.06634", "pdf": "https://arxiv.org/pdf/2504.06634", "abs": "https://arxiv.org/abs/2504.06634", "authors": ["Junyoung Kim", "Youngrok Kim", "Siyeol Jung", "Donghyun Min"], "title": "Crafting Query-Aware Selective Attention for Single Image Super-Resolution", "categories": ["cs.CV"], "comment": "10 pages, 5 figures, 4 tables", "summary": "Single Image Super-Resolution (SISR) reconstructs high-resolution images from\nlow-resolution inputs, enhancing image details. While Vision Transformer\n(ViT)-based models improve SISR by capturing long-range dependencies, they\nsuffer from quadratic computational costs or employ selective attention\nmechanisms that do not explicitly focus on query-relevant regions. Despite\nthese advancements, prior work has overlooked how selective attention\nmechanisms should be effectively designed for SISR. We propose SSCAN, which\ndynamically selects the most relevant key-value windows based on query\nsimilarity, ensuring focused feature extraction while maintaining efficiency.\nIn contrast to prior approaches that apply attention globally or heuristically,\nour method introduces a query-aware window selection strategy that better\naligns attention computation with important image regions. By incorporating\nfixed-sized windows, SSCAN reduces memory usage and enforces linear\ntoken-to-token complexity, making it scalable for large images. Our experiments\ndemonstrate that SSCAN outperforms existing attention-based SISR methods,\nachieving up to 0.14 dB PSNR improvement on urban datasets, guaranteeing both\ncomputational efficiency and reconstruction quality in SISR.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u9009\u62e9\u5173\u952e\u503c\u7a97\u53e3\u7684\u6ce8\u610f\u529b\u673a\u5236\uff08SSCAN\uff09\u4ee5\u6539\u8fdb\u5355\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08SISR\uff09\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eViT\u7684SISR\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u6ce8\u610f\u529b\u673a\u5236\u672a\u660e\u786e\u5173\u6ce8\u67e5\u8be2\u76f8\u5173\u533a\u57df\u7684\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u9009\u62e9\u6027\u6ce8\u610f\u529b\u673a\u5236\u8bbe\u8ba1\u7684\u6709\u6548\u63a2\u7d22\u3002", "method": "\u63d0\u51faSSCAN\uff0c\u901a\u8fc7\u57fa\u4e8e\u67e5\u8be2\u76f8\u4f3c\u6027\u52a8\u6001\u9009\u62e9\u5173\u952e\u503c\u7a97\u53e3\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u805a\u7126\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u91c7\u7528\u56fa\u5b9a\u5927\u5c0f\u7a97\u53e3\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "SSCAN\u5728SISR\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cPSNR\u63d0\u5347\u6700\u9ad8\u8fbe0.14 dB\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "SSCAN\u901a\u8fc7\u67e5\u8be2\u611f\u77e5\u7684\u7a97\u53e3\u9009\u62e9\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86SISR\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2504.07053", "pdf": "https://arxiv.org/pdf/2504.07053", "abs": "https://arxiv.org/abs/2504.07053", "authors": ["Liang-Hsuan Tseng", "Yi-Chang Chen", "Kuan-Yi Lee", "Da-Shan Shiu", "Hung-yi Lee"], "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Preprint. Work in progress", "summary": "Large Language Models (LLMs) excel in text-based natural language processing\ntasks but remain constrained by their reliance on textual inputs and outputs.\nTo enable more natural human-LLM interaction, recent progress have focused on\nderiving a spoken language model (SLM) that can not only listen but also\ngenerate speech. To achieve this, a promising direction is to conduct\nspeech-text joint modeling. However, recent SLM still lag behind text LLM due\nto the modality mismatch. One significant mismatch can be the sequence lengths\nbetween speech and text tokens. To address this, we introduce Text-Aligned\nSpeech Tokenization and Embedding (TASTE), a method that directly addresses the\nmodality gap by aligning speech token with the corresponding text transcription\nduring the tokenization stage. We propose a method that can achieve this\nthrough the special aggregation mechanism and with speech reconstruction as the\ntraining objective. We conduct extensive experiments and show that TASTE can\npreserve essential paralinguistic information while dramatically reducing the\ntoken sequence length. Furthermore, by leveraging TASTE, we can adapt\ntext-based LLMs into effective SLMs with parameter-efficient fine-tuning\ntechniques such as Low-Rank Adaptation (LoRA). Experimental results on\nbenchmark tasks, including SALMON and StoryCloze, demonstrate that TASTE-based\nSLMs perform similarly to previous full-finetuning methods. To our knowledge,\nTASTE is the first end-to-end approach that utilizes a reconstruction objective\nto automatically learn a text-aligned speech tokenization and embedding\nsuitable for spoken language modeling. Our demo, code, and models are publicly\navailable at https://github.com/mtkresearch/TASTE-SpokenLM.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aTASTE\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8bed\u97f3\u548c\u6587\u672c\u6a21\u6001\u4e4b\u95f4\u7684\u5e8f\u5217\u957f\u5ea6\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u8bed\u97f3-\u6587\u672c\u8054\u5408\u5efa\u6a21\u3002", "motivation": "\u63d0\u5347\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7684\u6027\u80fd\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u6587\u672c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4ee5\u652f\u6301\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u901a\u8fc7\u6587\u672c\u5bf9\u9f50\u7684\u8bed\u97f3\u6807\u8bb0\u5316\u548c\u5d4c\u5165\uff08TASTE\uff09\uff0c\u7ed3\u5408\u7279\u6b8a\u805a\u5408\u673a\u5236\u548c\u8bed\u97f3\u91cd\u5efa\u8bad\u7ec3\u76ee\u6807\uff0c\u51cf\u5c11\u8bed\u97f3\u6807\u8bb0\u5e8f\u5217\u957f\u5ea6\u5e76\u4fdd\u7559\u5173\u952e\u526f\u8bed\u8a00\u4fe1\u606f\u3002", "result": "TASTE\u663e\u8457\u51cf\u5c11\u4e86\u8bed\u97f3\u6807\u8bb0\u5e8f\u5217\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5173\u952e\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff08\u5982LoRA\uff09\u5c06\u6587\u672cLLM\u9002\u914d\u4e3a\u6709\u6548\u7684SLM\u3002", "conclusion": "TASTE\u662f\u9996\u4e2a\u5229\u7528\u91cd\u5efa\u76ee\u6807\u81ea\u52a8\u5b66\u4e60\u6587\u672c\u5bf9\u9f50\u8bed\u97f3\u6807\u8bb0\u5316\u548c\u5d4c\u5165\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4e0e\u5168\u5fae\u8c03\u65b9\u6cd5\u76f8\u5f53\u3002"}}
{"id": "2504.06638", "pdf": "https://arxiv.org/pdf/2504.06638", "abs": "https://arxiv.org/abs/2504.06638", "authors": ["Hu Cui", "Tessai Hayama"], "title": "HGMamba: Enhancing 3D Human Pose Estimation with a HyperGCN-Mamba Network", "categories": ["cs.CV"], "comment": "accepted by IJCNN2025", "summary": "3D human pose lifting is a promising research area that leverages estimated\nand ground-truth 2D human pose data for training. While existing approaches\nprimarily aim to enhance the performance of estimated 2D poses, they often\nstruggle when applied to ground-truth 2D pose data. We observe that achieving\naccurate 3D pose reconstruction from ground-truth 2D poses requires precise\nmodeling of local pose structures, alongside the ability to extract robust\nglobal spatio-temporal features. To address these challenges, we propose a\nnovel Hyper-GCN and Shuffle Mamba (HGMamba) block, which processes input data\nthrough two parallel streams: Hyper-GCN and Shuffle-Mamba. The Hyper-GCN stream\nmodels the human body structure as hypergraphs with varying levels of\ngranularity to effectively capture local joint dependencies. Meanwhile, the\nShuffle Mamba stream leverages a state space model to perform spatio-temporal\nscanning across all joints, enabling the establishment of global dependencies.\nBy adaptively fusing these two representations, HGMamba achieves strong global\nfeature modeling while excelling at local structure modeling. We stack multiple\nHGMamba blocks to create three variants of our model, allowing users to select\nthe most suitable configuration based on the desired speed-accuracy trade-off.\nExtensive evaluations on the Human3.6M and MPI-INF-3DHP benchmark datasets\ndemonstrate the effectiveness of our approach. HGMamba-B achieves\nstate-of-the-art results, with P1 errors of 38.65 mm and 14.33 mm on the\nrespective datasets. Code and models are available:\nhttps://github.com/HuCui2022/HGMamba", "AI": {"task": "\u5229\u7528\u4f30\u8ba1\u548c\u771f\u5b9e2D\u4eba\u4f53\u59ff\u6001\u6570\u636e\u8fdb\u884c3D\u4eba\u4f53\u59ff\u6001\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5e94\u7528\u4e8e\u771f\u5b9e2D\u59ff\u6001\u6570\u636e\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u7cbe\u786e\u5efa\u6a21\u5c40\u90e8\u59ff\u6001\u7ed3\u6784\u548c\u63d0\u53d6\u5168\u5c40\u65f6\u7a7a\u7279\u5f81\u3002", "method": "\u63d0\u51faHyper-GCN\u548cShuffle Mamba\uff08HGMamba\uff09\u5757\uff0c\u901a\u8fc7\u5e76\u884c\u5904\u7406\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u5e76\u81ea\u9002\u5e94\u878d\u5408\u3002", "result": "\u5728Human3.6M\u548cMPI-INF-3DHP\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\uff0cP1\u8bef\u5dee\u5206\u522b\u4e3a38.65 mm\u548c14.33 mm\u3002", "conclusion": "HGMamba\u5728\u5168\u5c40\u7279\u5f81\u5efa\u6a21\u548c\u5c40\u90e8\u7ed3\u6784\u5efa\u6a21\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u591a\u79cd\u914d\u7f6e\u4ee5\u6ee1\u8db3\u901f\u5ea6-\u7cbe\u5ea6\u6743\u8861\u3002"}}
{"id": "2504.07069", "pdf": "https://arxiv.org/pdf/2504.07069", "abs": "https://arxiv.org/abs/2504.07069", "authors": ["Bibek Paudel", "Alexander Lyzhov", "Preetam Joshi", "Puneet Anand"], "title": "HalluciNot: Hallucination Detection Through Context and Common Knowledge Verification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces a comprehensive system for detecting hallucinations in\nlarge language model (LLM) outputs in enterprise settings. We present a novel\ntaxonomy of LLM responses specific to hallucination in enterprise applications,\ncategorizing them into context-based, common knowledge, enterprise-specific,\nand innocuous statements. Our hallucination detection model HDM-2 validates LLM\nresponses with respect to both context and generally known facts (common\nknowledge). It provides both hallucination scores and word-level annotations,\nenabling precise identification of problematic content. To evaluate it on\ncontext-based and common-knowledge hallucinations, we introduce a new dataset\nHDMBench. Experimental results demonstrate that HDM-2 out-performs existing\napproaches across RagTruth, TruthfulQA, and HDMBench datasets. This work\naddresses the specific challenges of enterprise deployment, including\ncomputational efficiency, domain specialization, and fine-grained error\nidentification. Our evaluation dataset, model weights, and inference code are\npublicly available.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u4f01\u4e1a\u73af\u5883\u4e2d\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f93\u51fa\u4e2d\u5e7b\u89c9\u7684\u7efc\u5408\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u90e8\u7f72\u4e2dLLM\u8f93\u51fa\u5e7b\u89c9\u7684\u7279\u5b9a\u6311\u6218\uff0c\u5305\u62ec\u8ba1\u7b97\u6548\u7387\u3001\u9886\u57df\u4e13\u4e1a\u5316\u548c\u7ec6\u7c92\u5ea6\u9519\u8bef\u8bc6\u522b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684LLM\u54cd\u5e94\u5206\u7c7b\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86\u5e7b\u89c9\u68c0\u6d4b\u6a21\u578bHDM-2\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u548c\u5e38\u8bc6\u9a8c\u8bc1\u54cd\u5e94\u3002", "result": "HDM-2\u5728RagTruth\u3001TruthfulQA\u548cHDMBench\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4f01\u4e1a\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u516c\u5f00\u3002"}}
{"id": "2504.06647", "pdf": "https://arxiv.org/pdf/2504.06647", "abs": "https://arxiv.org/abs/2504.06647", "authors": ["Nan Peng", "Xun Zhou", "Mingming Wang", "Guisong Chen", "Songming Chen"], "title": "Uni-PrevPredMap: Extending PrevPredMap to a Unified Framework of Prior-Informed Modeling for Online Vectorized HD Map Construction", "categories": ["cs.CV"], "comment": null, "summary": "Safety constitutes a foundational imperative for autonomous driving systems,\nnecessitating the maximal incorporation of accessible external prior\ninformation. This study establishes that temporal perception buffers and\ncost-efficient maps inherently form complementary prior sources for online\nvectorized high-definition (HD) map construction. We present Uni-PrevPredMap, a\nunified prior-informed framework that systematically integrates two synergistic\ninformation sources: previous predictions and simulated outdated HD maps. The\nframework introduces two core innovations: a tile-indexed 3D vectorized global\nmap processor enabling efficient refreshment, storage, and retrieval of 3D\nvectorized priors; a tri-mode operational optimization paradigm ensuring\nconsistency across prior-free, map-absent, and map-prior scenarios while\nmitigating reliance on idealized map fidelity assumptions. Uni-PrevPredMap\nachieves state-of-the-art performance in map-free scenarios across established\nonline vectorized HD map construction benchmarks. When provided with simulated\noutdated HD maps, the framework exhibits robust capabilities in error-resilient\nprior fusion, empirically confirming the synergistic complementarity between\nprevious predictions and simulated outdated HD maps. Code will be available at\nhttps://github.com/pnnnnnnn/Uni-PrevPredMap.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u5148\u9a8c\u4fe1\u606f\u6846\u67b6Uni-PrevPredMap\uff0c\u7528\u4e8e\u5728\u7ebf\u77e2\u91cf\u9ad8\u6e05\u5730\u56fe\u6784\u5efa\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9700\u8981\u6700\u5927\u5316\u5229\u7528\u5916\u90e8\u5148\u9a8c\u4fe1\u606f\u4ee5\u786e\u4fdd\u5b89\u5168\u6027\u3002", "method": "\u7ed3\u5408\u65f6\u95f4\u611f\u77e5\u7f13\u51b2\u533a\u548c\u6210\u672c\u6548\u76ca\u5730\u56fe\u4f5c\u4e3a\u4e92\u8865\u5148\u9a8c\u6e90\uff0c\u5f15\u5165\u5168\u5c40\u5730\u56fe\u5904\u7406\u5668\u548c\u4e09\u6a21\u5f0f\u64cd\u4f5c\u4f18\u5316\u8303\u5f0f\u3002", "result": "\u5728\u65e0\u5730\u56fe\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u6a21\u62df\u8fc7\u65f6\u5730\u56fe\u4e2d\u5c55\u793a\u51fa\u9c81\u68d2\u7684\u5148\u9a8c\u878d\u5408\u80fd\u529b\u3002", "conclusion": "\u8bc1\u5b9e\u4e86\u5148\u524d\u9884\u6d4b\u4e0e\u6a21\u62df\u8fc7\u65f6\u5730\u56fe\u7684\u534f\u540c\u4e92\u8865\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5730\u56fe\u6784\u5efa\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.07070", "pdf": "https://arxiv.org/pdf/2504.07070", "abs": "https://arxiv.org/abs/2504.07070", "authors": ["Zhouhang Xie", "Junda Wu", "Yiran Shen", "Yu Xia", "Xintong Li", "Aaron Chang", "Ryan Rossi", "Sachin Kumar", "Bodhisattwa Prasad Majumder", "Jingbo Shang", "Prithviraj Ammanabrolu", "Julian McAuley"], "title": "A Survey on Personalized and Pluralistic Preference Alignment in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Personalized preference alignment for large language models (LLMs), the\nprocess of tailoring LLMs to individual users' preferences, is an emerging\nresearch direction spanning the area of NLP and personalization. In this\nsurvey, we present an analysis of works on personalized alignment and modeling\nfor LLMs. We introduce a taxonomy of preference alignment techniques, including\ntraining time, inference time, and additionally, user-modeling based methods.\nWe provide analysis and discussion on the strengths and limitations of each\ngroup of techniques and then cover evaluation, benchmarks, as well as open\nproblems in the field.", "AI": {"task": "\u603b\u7ed3\u548c\u5206\u6790\u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2a\u6027\u5316\u504f\u597d\u5bf9\u9f50\u7684\u7814\u7a76\u5de5\u4f5c\u3002", "motivation": "\u4e2a\u6027\u5316\u504f\u597d\u5bf9\u9f50\u662fNLP\u548c\u4e2a\u6027\u5316\u9886\u57df\u7684\u65b0\u5174\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u4f7fLLMs\u66f4\u597d\u5730\u9002\u5e94\u7528\u6237\u7684\u4e2a\u6027\u5316\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u504f\u597d\u5bf9\u9f50\u6280\u672f\u7684\u5206\u7c7b\u6cd5\uff0c\u5305\u62ec\u8bad\u7ec3\u65f6\u3001\u63a8\u7406\u65f6\u548c\u57fa\u4e8e\u7528\u6237\u5efa\u6a21\u7684\u65b9\u6cd5\uff0c\u5e76\u5bf9\u6bcf\u79cd\u6280\u672f\u7684\u4f18\u7f3a\u70b9\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "\u63d0\u4f9b\u4e86\u5bf9\u73b0\u6709\u6280\u672f\u7684\u8bc4\u4f30\u3001\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u53ca\u8be5\u9886\u57df\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3aLLMs\u4e2a\u6027\u5316\u504f\u597d\u5bf9\u9f50\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u5206\u6790\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.06666", "pdf": "https://arxiv.org/pdf/2504.06666", "abs": "https://arxiv.org/abs/2504.06666", "authors": ["Ruotian Peng", "Haiying He", "Yake Wei", "Yandong Wen", "Di Hu"], "title": "Patch Matters: Training-free Fine-grained Image Caption Enhancement via Local Perception", "categories": ["cs.CV"], "comment": null, "summary": "High-quality image captions play a crucial role in improving the performance\nof cross-modal applications such as text-to-image generation, text-to-video\ngeneration, and text-image retrieval. To generate long-form, high-quality\ncaptions, many recent studies have employed multimodal large language models\n(MLLMs). However, current MLLMs often produce captions that lack fine-grained\ndetails or suffer from hallucinations, a challenge that persists in both\nopen-source and closed-source models. Inspired by Feature-Integration theory,\nwhich suggests that attention must focus on specific regions to integrate\nvisual information effectively, we propose a \\textbf{divide-then-aggregate}\nstrategy. Our method first divides the image into semantic and spatial patches\nto extract fine-grained details, enhancing the model's local perception of the\nimage. These local details are then hierarchically aggregated to generate a\ncomprehensive global description. To address hallucinations and inconsistencies\nin the generated captions, we apply a semantic-level filtering process during\nhierarchical aggregation. This training-free pipeline can be applied to both\nopen-source models (LLaVA-1.5, LLaVA-1.6, Mini-Gemini) and closed-source models\n(Claude-3.5-Sonnet, GPT-4o, GLM-4V-Plus). Extensive experiments demonstrate\nthat our method generates more detailed, reliable captions, advancing\nmultimodal description generation without requiring model retraining. The\nsource code are available at https://github.com/GeWu-Lab/Patch-Matters", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u201c\u5206\u800c\u6cbb\u4e4b\u201d\u7b56\u7565\uff0c\u901a\u8fc7\u8bed\u4e49\u548c\u7a7a\u95f4\u5206\u5757\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u957f\u6587\u672c\u56fe\u50cf\u63cf\u8ff0\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u63cf\u8ff0\u7f3a\u4e4f\u7ec6\u8282\u6216\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5c06\u56fe\u50cf\u5206\u4e3a\u8bed\u4e49\u548c\u7a7a\u95f4\u5757\u63d0\u53d6\u7ec6\u8282\uff0c\u5206\u5c42\u805a\u5408\u751f\u6210\u5168\u5c40\u63cf\u8ff0\uff0c\u5e76\u5e94\u7528\u8bed\u4e49\u7ea7\u8fc7\u6ee4\u51cf\u5c11\u5e7b\u89c9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u8be6\u7ec6\u3001\u53ef\u9760\u7684\u63cf\u8ff0\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63cf\u8ff0\u751f\u6210\u7684\u8d28\u91cf\uff0c\u9002\u7528\u4e8e\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u3002"}}
{"id": "2504.07072", "pdf": "https://arxiv.org/pdf/2504.07072", "abs": "https://arxiv.org/abs/2504.07072", "authors": ["Israfel Salazar", "Manuel Fern\u00e1ndez Burda", "Shayekh Bin Islam", "Arshia Soltani Moakhar", "Shivalika Singh", "Fabian Farestam", "Angelika Romanou", "Danylo Boiko", "Dipika Khullar", "Mike Zhang", "Dominik Krzemi\u0144ski", "Jekaterina Novikova", "Lu\u00edsa Shimabucoro", "Joseph Marvin Imperial", "Rishabh Maheshwary", "Sharad Duwal", "Alfonso Amayuelas", "Swati Rajwal", "Jebish Purbey", "Ahmed Ruby", "Nicholas Popovi\u010d", "Marek Suppa", "Azmine Toushik Wasi", "Ram Mohan Rao Kadiyala", "Olga Tsymboi", "Maksim Kostritsya", "Bardia Soltani Moakhar", "Gabriel da Costa Merlin", "Ot\u00e1vio Ferracioli Coletti", "Maral Jabbari Shiviari", "MohammadAmin farahani fard", "Silvia Fernandez", "Mar\u00eda Grandury", "Dmitry Abulkhanov", "Drishti Sharma", "Andre Guarnier De Mitri", "Leticia Bossatto Marchezi", "Johan Obando-Ceron", "Nazar Kohut", "Beyza Ermis", "Desmond Elliott", "Enzo Ferrante", "Sara Hooker", "Marzieh Fadaee"], "title": "Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The evaluation of vision-language models (VLMs) has mainly relied on\nEnglish-language benchmarks, leaving significant gaps in both multilingual and\nmulticultural coverage. While multilingual benchmarks have expanded, both in\nsize and languages, many rely on translations of English datasets, failing to\ncapture cultural nuances. In this work, we propose Kaleidoscope, as the most\ncomprehensive exam benchmark to date for the multilingual evaluation of\nvision-language models. Kaleidoscope is a large-scale, in-language multimodal\nbenchmark designed to evaluate VLMs across diverse languages and visual inputs.\nKaleidoscope covers 18 languages and 14 different subjects, amounting to a\ntotal of 20,911 multiple-choice questions. Built through an open science\ncollaboration with a diverse group of researchers worldwide, Kaleidoscope\nensures linguistic and cultural authenticity. We evaluate top-performing\nmultilingual vision-language models and find that they perform poorly on\nlow-resource languages and in complex multimodal scenarios. Our results\nhighlight the need for progress on culturally inclusive multimodal evaluation\nframeworks.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6Kaleidoscope\uff0c\u4ee5\u586b\u8865\u73b0\u6709\u57fa\u51c6\u5728\u591a\u8bed\u8a00\u548c\u6587\u5316\u591a\u6837\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u82f1\u8bed\uff0c\u4e14\u591a\u8bed\u8a00\u57fa\u51c6\u591a\u57fa\u4e8e\u82f1\u8bed\u6570\u636e\u96c6\u7684\u7ffb\u8bd1\uff0c\u672a\u80fd\u6355\u6349\u6587\u5316\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5f00\u653e\u79d1\u5b66\u5408\u4f5c\u6784\u5efaKaleidoscope\uff0c\u5305\u542b18\u79cd\u8bed\u8a00\u548c14\u4e2a\u4e3b\u9898\u768420,911\u9053\u591a\u9009\u9898\uff0c\u786e\u4fdd\u8bed\u8a00\u548c\u6587\u5316\u771f\u5b9e\u6027\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u5f53\u524d\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u590d\u6742\u591a\u6a21\u6001\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u53d1\u5c55\u66f4\u5177\u6587\u5316\u5305\u5bb9\u6027\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2504.06672", "pdf": "https://arxiv.org/pdf/2504.06672", "abs": "https://arxiv.org/abs/2504.06672", "authors": ["Elia Peruzzo", "Dejia Xu", "Xingqian Xu", "Humphrey Shi", "Nicu Sebe"], "title": "RAGME: Retrieval Augmented Video Generation for Enhanced Motion Realism", "categories": ["cs.CV"], "comment": "Code available at: https://github.com/helia95/ragme", "summary": "Video generation is experiencing rapid growth, driven by advances in\ndiffusion models and the development of better and larger datasets. However,\nproducing high-quality videos remains challenging due to the high-dimensional\ndata and the complexity of the task. Recent efforts have primarily focused on\nenhancing visual quality and addressing temporal inconsistencies, such as\nflickering. Despite progress in these areas, the generated videos often fall\nshort in terms of motion complexity and physical plausibility, with many\noutputs either appearing static or exhibiting unrealistic motion. In this work,\nwe propose a framework to improve the realism of motion in generated videos,\nexploring a complementary direction to much of the existing literature.\nSpecifically, we advocate for the incorporation of a retrieval mechanism during\nthe generation phase. The retrieved videos act as grounding signals, providing\nthe model with demonstrations of how the objects move. Our pipeline is designed\nto apply to any text-to-video diffusion model, conditioning a pretrained model\non the retrieved samples with minimal fine-tuning. We demonstrate the\nsuperiority of our approach through established metrics, recently proposed\nbenchmarks, and qualitative results, and we highlight additional applications\nof the framework.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u673a\u5236\u63d0\u5347\u751f\u6210\u89c6\u9891\u4e2d\u8fd0\u52a8\u7684\u771f\u5b9e\u611f\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u5728\u8fd0\u52a8\u590d\u6742\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u751f\u6210\u7684\u89c6\u9891\u5e38\u663e\u5f97\u9759\u6001\u6216\u8fd0\u52a8\u4e0d\u771f\u5b9e\u3002", "method": "\u5728\u751f\u6210\u9636\u6bb5\u5f15\u5165\u68c0\u7d22\u673a\u5236\uff0c\u5229\u7528\u68c0\u7d22\u5230\u7684\u89c6\u9891\u4f5c\u4e3a\u57fa\u7840\u4fe1\u53f7\uff0c\u6307\u5bfc\u6a21\u578b\u751f\u6210\u66f4\u771f\u5b9e\u7684\u8fd0\u52a8\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5305\u62ec\u5b9a\u91cf\u6307\u6807\u3001\u65b0\u63d0\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u5b9a\u6027\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u8fd0\u52a8\u771f\u5b9e\u611f\uff0c\u8fd8\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.07080", "pdf": "https://arxiv.org/pdf/2504.07080", "abs": "https://arxiv.org/abs/2504.07080", "authors": ["Atharva Pandey", "Kshitij Dubey", "Rahul Sharma", "Amit Sharma"], "title": "DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite great performance on Olympiad-level reasoning problems, frontier\nlarge language models can still struggle on high school math when presented\nwith novel problems outside standard benchmarks. Going beyond final accuracy,\nwe propose a deductive consistency metric to analyze chain-of-thought output\nfrom language models (LMs).Formally, deductive reasoning involves two subtasks:\nunderstanding a set of input premises and inferring the conclusions that follow\nfrom them. The proposed metric studies LMs' performance on these subtasks, with\nthe goal of explaining LMs' reasoning errors on novel problems: how well do LMs\nunderstand input premises with increasing context lengths, and how well can\nthey infer conclusions over multiple reasoning hops? Since existing benchmarks\nmay be memorized, we develop a pipeline to evaluate LMs' deductive consistency\non novel, perturbed versions of benchmark problems. On novel grade school math\nproblems (GSM-8k), we find that LMs are fairly robust to increasing number of\ninput premises, but suffer significant accuracy decay as the number of\nreasoning hops is increased. Interestingly, these errors are masked in the\noriginal benchmark as all models achieve near 100% accuracy. As we increase the\nnumber of solution steps using a synthetic dataset, prediction over multiple\nhops still remains the major source of error compared to understanding input\npremises. Other factors, such as shifts in language style or natural\npropagation of early errors do not explain the trends. Our analysis provides a\nnew view to characterize LM reasoning -- as computations over a window of input\npremises and reasoning hops -- that can provide unified evaluation across\nproblem domains.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u6f14\u7ece\u4e00\u81f4\u6027\u6307\u6807\uff0c\u7528\u4e8e\u5206\u6790\u8bed\u8a00\u6a21\u578b\u5728\u94fe\u5f0f\u601d\u7ef4\u8f93\u51fa\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5965\u6797\u5339\u514b\u7ea7\u63a8\u7406\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9ad8\u4e2d\u6570\u5b66\u7684\u65b0\u9896\u95ee\u9898\u4e0a\u4ecd\u6709\u56f0\u96be\uff0c\u9700\u8981\u8d85\u8d8a\u6700\u7ec8\u51c6\u786e\u7387\u7684\u5206\u6790\u3002", "method": "\u5f00\u53d1\u4e00\u4e2a\u8bc4\u4f30\u7ba1\u9053\uff0c\u6d4b\u8bd5\u8bed\u8a00\u6a21\u578b\u5728\u8f93\u5165\u524d\u63d0\u7406\u89e3\u548c\u591a\u6b65\u63a8\u7406\u7ed3\u8bba\u63a8\u65ad\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u57fa\u51c6\u95ee\u9898\u53d8\u4f53\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u5bf9\u8f93\u5165\u524d\u63d0\u6570\u91cf\u589e\u52a0\u8868\u73b0\u7a33\u5065\uff0c\u4f46\u968f\u7740\u63a8\u7406\u6b65\u6570\u589e\u52a0\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\uff0c\u9519\u8bef\u4e3b\u8981\u6e90\u4e8e\u591a\u6b65\u63a8\u7406\u800c\u975e\u524d\u63d0\u7406\u89e3\u3002", "conclusion": "\u901a\u8fc7\u8f93\u5165\u524d\u63d0\u7a97\u53e3\u548c\u63a8\u7406\u6b65\u6570\u7684\u8ba1\u7b97\u89c6\u89d2\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u7edf\u4e00\u7684\u8de8\u9886\u57df\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2504.06675", "pdf": "https://arxiv.org/pdf/2504.06675", "abs": "https://arxiv.org/abs/2504.06675", "authors": ["Qingtao Yu", "Jaskirat Singh", "Zhaoyuan Yang", "Peter Henry Tu", "Jing Zhang", "Hongdong Li", "Richard Hartley", "Dylan Campbell"], "title": "Probability Density Geodesics in Image Diffusion Latent Space", "categories": ["cs.CV"], "comment": "CVPR2025", "summary": "Diffusion models indirectly estimate the probability density over a data\nspace, which can be used to study its structure. In this work, we show that\ngeodesics can be computed in diffusion latent space, where the norm induced by\nthe spatially-varying inner product is inversely proportional to the\nprobability density. In this formulation, a path that traverses a high density\n(that is, probable) region of image latent space is shorter than the equivalent\npath through a low density region. We present algorithms for solving the\nassociated initial and boundary value problems and show how to compute the\nprobability density along the path and the geodesic distance between two\npoints. Using these techniques, we analyze how closely video clips approximate\ngeodesics in a pre-trained image diffusion space. Finally, we demonstrate how\nthese techniques can be applied to training-free image sequence interpolation\nand extrapolation, given a pre-trained image diffusion model.", "AI": {"task": "\u5728\u6269\u6563\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u6d4b\u5730\u7ebf\uff0c\u5e76\u5206\u6790\u89c6\u9891\u7247\u6bb5\u5728\u8be5\u7a7a\u95f4\u4e2d\u7684\u8fd1\u4f3c\u7a0b\u5ea6\u3002", "motivation": "\u901a\u8fc7\u6269\u6563\u6a21\u578b\u95f4\u63a5\u4f30\u8ba1\u6570\u636e\u7a7a\u95f4\u7684\u6982\u7387\u5bc6\u5ea6\uff0c\u7814\u7a76\u5176\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u7b97\u6cd5\u89e3\u51b3\u521d\u59cb\u548c\u8fb9\u754c\u503c\u95ee\u9898\uff0c\u8ba1\u7b97\u8def\u5f84\u4e0a\u7684\u6982\u7387\u5bc6\u5ea6\u548c\u6d4b\u5730\u7ebf\u8ddd\u79bb\u3002", "result": "\u5c55\u793a\u4e86\u5982\u4f55\u5e94\u7528\u8fd9\u4e9b\u6280\u672f\u8fdb\u884c\u65e0\u8bad\u7ec3\u7684\u56fe\u50cf\u5e8f\u5217\u63d2\u503c\u548c\u5916\u63a8\u3002", "conclusion": "\u6269\u6563\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u6d4b\u5730\u7ebf\u8ba1\u7b97\u4e3a\u56fe\u50cf\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2504.07081", "pdf": "https://arxiv.org/pdf/2504.07081", "abs": "https://arxiv.org/abs/2504.07081", "authors": ["Gabriel Grand", "Joshua B. Tenenbaum", "Vikash K. Mansinghka", "Alexander K. Lew", "Jacob Andreas"], "title": "Self-Steering Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While test-time reasoning enables language models to tackle complex tasks,\nsearching or planning in natural language can be slow, costly, and error-prone.\nBut even when LMs struggle to emulate the precise reasoning steps needed to\nsolve a problem, they often excel at describing its abstract structure--both\nhow to verify solutions and how to search for them. This paper introduces\nDisCIPL, a method for \"self-steering\" LMs where a Planner model generates a\ntask-specific inference program that is executed by a population of Follower\nmodels. Our approach equips LMs with the ability to write recursive search\nprocedures that guide LM inference, enabling new forms of verifiable and\nefficient reasoning. When instantiated with a small Follower (e.g.,\nLlama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models,\nincluding GPT-4o and o1, on challenging constrained generation tasks. In\ndecoupling planning from execution, our work opens up a design space of\nhighly-parallelized Monte Carlo inference strategies that outperform standard\nbest-of-N sampling, require no finetuning, and can be implemented automatically\nby existing LMs.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aDisCIPL\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7Planner\u6a21\u578b\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u7684\u63a8\u7406\u7a0b\u5e8f\uff0c\u7531Follower\u6a21\u578b\u6267\u884c\uff0c\u4ee5\u5b9e\u73b0\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u5f15\u5bfc\u63a8\u7406\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u4e2d\u8fdb\u884c\u641c\u7d22\u6216\u89c4\u5212\u65f6\u901f\u5ea6\u6162\u3001\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u4f46\u64c5\u957f\u63cf\u8ff0\u95ee\u9898\u7684\u62bd\u8c61\u7ed3\u6784\u3002", "method": "DisCIPL\u65b9\u6cd5\u901a\u8fc7Planner\u6a21\u578b\u751f\u6210\u9012\u5f52\u641c\u7d22\u7a0b\u5e8f\uff0c\u6307\u5bfcFollower\u6a21\u578b\u8fdb\u884c\u63a8\u7406\uff0c\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u4e14\u9ad8\u6548\u7684\u63a8\u7406\u3002", "result": "\u5728\u5c0f\u578bFollower\u6a21\u578b\uff08\u5982Llama-3.2-1B\uff09\u4e0a\uff0cDisCIPL\u8868\u73b0\u4f18\u4e8e\u6216\u5339\u914d\u5927\u578b\u6a21\u578b\uff08\u5982GPT-4o\u548co1\uff09\u3002", "conclusion": "DisCIPL\u901a\u8fc7\u5206\u79bb\u89c4\u5212\u4e0e\u6267\u884c\uff0c\u5f00\u8f9f\u4e86\u9ad8\u6548\u5e76\u884c\u8499\u7279\u5361\u6d1b\u63a8\u7406\u7b56\u7565\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u65e0\u9700\u5fae\u8c03\u4e14\u53ef\u7531\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5b9e\u73b0\u3002"}}
{"id": "2504.06680", "pdf": "https://arxiv.org/pdf/2504.06680", "abs": "https://arxiv.org/abs/2504.06680", "authors": ["Christoph Balada", "Aida Romano-Martinez", "Vincent ten Cate", "Katharina Geschke", "Jonas Tesarz", "Paul Cla\u00dfen", "Alexander K. Schuster", "Dativa Tibyampansha", "Karl-Patrik Kresoja", "Philipp S. Wild", "Sheraz Ahmed", "Andreas Dengel"], "title": "Deep Learning for Cardiovascular Risk Assessment: Proxy Features from Carotid Sonography as Predictors of Arterial Damage", "categories": ["cs.CV"], "comment": null, "summary": "In this study, hypertension is utilized as an indicator of individual\nvascular damage. This damage can be identified through machine learning\ntechniques, providing an early risk marker for potential major cardiovascular\nevents and offering valuable insights into the overall arterial condition of\nindividual patients. To this end, the VideoMAE deep learning model, originally\ndeveloped for video classification, was adapted by finetuning for application\nin the domain of ultrasound imaging. The model was trained and tested using a\ndataset comprising over 31,000 carotid sonography videos sourced from the\nGutenberg Health Study (15,010 participants), one of the largest prospective\npopulation health studies. This adaptation facilitates the classification of\nindividuals as hypertensive or non-hypertensive (75.7% validation accuracy),\nfunctioning as a proxy for detecting visual arterial damage. We demonstrate\nthat our machine learning model effectively captures visual features that\nprovide valuable insights into an individual's overall cardiovascular health.", "AI": {"task": "\u5229\u7528\u9ad8\u8840\u538b\u4f5c\u4e3a\u4e2a\u4f53\u8840\u7ba1\u635f\u4f24\u7684\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6280\u672f\u8bc6\u522b\u8fd9\u79cd\u635f\u4f24\u3002", "motivation": "\u63d0\u4f9b\u65e9\u671f\u5fc3\u8840\u7ba1\u4e8b\u4ef6\u98ce\u9669\u6807\u5fd7\u7269\uff0c\u5e76\u4e3a\u4e2a\u4f53\u60a3\u8005\u7684\u52a8\u8109\u72b6\u51b5\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002", "method": "\u901a\u8fc7\u5fae\u8c03VideoMAE\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u8d85\u58f0\u6210\u50cf\u9886\u57df\uff0c\u4f7f\u7528\u6765\u81eaGutenberg\u5065\u5eb7\u7814\u7a76\u768431,000\u591a\u4e2a\u9888\u52a8\u8109\u8d85\u58f0\u89c6\u9891\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523075.7%\u7684\u51c6\u786e\u7387\uff0c\u80fd\u591f\u6709\u6548\u5206\u7c7b\u9ad8\u8840\u538b\u4e0e\u975e\u9ad8\u8840\u538b\u60a3\u8005\uff0c\u4f5c\u4e3a\u68c0\u6d4b\u89c6\u89c9\u52a8\u8109\u635f\u4f24\u7684\u4ee3\u7406\u6307\u6807\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u6355\u6349\u89c6\u89c9\u7279\u5f81\uff0c\u4e3a\u4e2a\u4f53\u5fc3\u8840\u7ba1\u5065\u5eb7\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2504.07087", "pdf": "https://arxiv.org/pdf/2504.07087", "abs": "https://arxiv.org/abs/2504.07087", "authors": ["Elan Markowitz", "Krupa Galiya", "Greg Ver Steeg", "Aram Galstyan"], "title": "KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on Textualized Knowledge Graphs", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "To be presented at NAACL-HLT, KnowledgeNLP Workshop (2025)", "summary": "Knowledge graphs have emerged as a popular method for injecting up-to-date,\nfactual knowledge into large language models (LLMs). This is typically achieved\nby converting the knowledge graph into text that the LLM can process in\ncontext. While multiple methods of encoding knowledge graphs have been\nproposed, the impact of this textualization process on LLM performance remains\nunder-explored. We introduce KG-LLM-Bench, a comprehensive and extensible\nbenchmark spanning five knowledge graph understanding tasks, and evaluate how\ndifferent encoding strategies affect performance across various base models.\nOur extensive experiments with seven language models and five textualization\nstrategies provide insights for optimizing LLM performance on KG reasoning\ntasks.", "AI": {"task": "\u7814\u7a76\u77e5\u8bc6\u56fe\u8c31\u6587\u672c\u5316\u8fc7\u7a0b\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31\u662f\u5411\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6ce8\u5165\u6700\u65b0\u4e8b\u5b9e\u77e5\u8bc6\u7684\u6d41\u884c\u65b9\u6cd5\uff0c\u4f46\u5176\u6587\u672c\u5316\u8fc7\u7a0b\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f15\u5165KG-LLM-Bench\u57fa\u51c6\uff0c\u6db5\u76d6\u4e94\u79cd\u77e5\u8bc6\u56fe\u8c31\u7406\u89e3\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e0d\u540c\u7f16\u7801\u7b56\u7565\u5bf9\u591a\u79cd\u57fa\u7840\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u4e03\u79cd\u8bed\u8a00\u6a21\u578b\u548c\u4e94\u79cd\u6587\u672c\u5316\u7b56\u7565\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u4e3a\u4f18\u5316\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4efb\u52a1\u7684LLM\u6027\u80fd\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u4e3a\u77e5\u8bc6\u56fe\u8c31\u6587\u672c\u5316\u7b56\u7565\u7684\u9009\u62e9\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2504.06716", "pdf": "https://arxiv.org/pdf/2504.06716", "abs": "https://arxiv.org/abs/2504.06716", "authors": ["Anil Armagan", "Albert Sa\u00e0-Garriga", "Bruno Manganelli", "Kyuwon Kim", "M. Kerim Yucel"], "title": "GSta: Efficient Training Scheme with Siestaed Gaussians for Monocular 3D Scene Reconstruction", "categories": ["cs.CV"], "comment": "9 pages. In submission to an IEEE conference", "summary": "Gaussian Splatting (GS) is a popular approach for 3D reconstruction, mostly\ndue to its ability to converge reasonably fast, faithfully represent the scene\nand render (novel) views in a fast fashion. However, it suffers from large\nstorage and memory requirements, and its training speed still lags behind the\nhash-grid based radiance field approaches (e.g. Instant-NGP), which makes it\nespecially difficult to deploy them in robotics scenarios, where 3D\nreconstruction is crucial for accurate operation. In this paper, we propose\nGSta that dynamically identifies Gaussians that have converged well during\ntraining, based on their positional and color gradient norms. By forcing such\nGaussians into a siesta and stopping their updates (freezing) during training,\nwe improve training speed with competitive accuracy compared to state of the\nart. We also propose an early stopping mechanism based on the PSNR values\ncomputed on a subset of training images. Combined with other improvements, such\nas integrating a learning rate scheduler, GSta achieves an improved Pareto\nfront in convergence speed, memory and storage requirements, while preserving\nquality. We also show that GSta can improve other methods and complement\northogonal approaches in efficiency improvement; once combined with Trick-GS,\nGSta achieves up to 5x faster training, 16x smaller disk size compared to\nvanilla GS, while having comparable accuracy and consuming only half the peak\nmemory. More visualisations are available at\nhttps://anilarmagan.github.io/SRUK-GSta.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aGSta\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u8bad\u7ec3\u4e2d\u6536\u655b\u826f\u597d\u7684\u9ad8\u65af\u5206\u5e03\u5e76\u51bb\u7ed3\u5176\u66f4\u65b0\uff0c\u4ee5\u63d0\u9ad83D\u91cd\u5efa\u7684\u8bad\u7ec3\u901f\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u9ad8\u65af\u6cfc\u6e85\uff08GS\uff09\u57283D\u91cd\u5efa\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u5b58\u50a8\u548c\u5185\u5b58\u9700\u6c42\u5927\u3001\u8bad\u7ec3\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5728\u673a\u5668\u4eba\u573a\u666f\u4e2d\u96be\u4ee5\u90e8\u7f72\u3002", "method": "\u57fa\u4e8e\u4f4d\u7f6e\u548c\u989c\u8272\u68af\u5ea6\u8303\u6570\u52a8\u6001\u8bc6\u522b\u6536\u655b\u7684\u9ad8\u65af\u5206\u5e03\u5e76\u51bb\u7ed3\u5176\u66f4\u65b0\uff0c\u7ed3\u5408\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u548c\u65e9\u671f\u505c\u6b62\u673a\u5236\u3002", "result": "GSta\u5728\u8bad\u7ec3\u901f\u5ea6\u3001\u5185\u5b58\u548c\u5b58\u50a8\u9700\u6c42\u4e0a\u663e\u8457\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u8d28\u91cf\uff0c\u4e0e\u5176\u4ed6\u65b9\u6cd5\u7ed3\u5408\u65f6\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "GSta\u6709\u6548\u89e3\u51b3\u4e86GS\u7684\u6548\u7387\u548c\u90e8\u7f72\u95ee\u9898\uff0c\u4e3a3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.07096", "pdf": "https://arxiv.org/pdf/2504.07096", "abs": "https://arxiv.org/abs/2504.07096", "authors": ["Jiacheng Liu", "Taylor Blanton", "Yanai Elazar", "Sewon Min", "YenSung Chen", "Arnavi Chheda-Kothary", "Huy Tran", "Byron Bischoff", "Eric Marsh", "Michael Schmitz", "Cassidy Trier", "Aaron Sarnat", "Jenna James", "Jon Borchardt", "Bailey Kuehl", "Evie Cheng", "Karen Farley", "Sruthi Sreeram", "Taira Anderson", "David Albright", "Carissa Schoenick", "Luca Soldaini", "Dirk Groeneveld", "Rock Yuren Pang", "Pang Wei Koh", "Noah A. Smith", "Sophie Lebrecht", "Yejin Choi", "Hannaneh Hajishirzi", "Ali Farhadi", "Jesse Dodge"], "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens", "categories": ["cs.CL"], "comment": "Under submission at ACL 2025 demo track", "summary": "We present OLMoTrace, the first system that traces the outputs of language\nmodels back to their full, multi-trillion-token training data in real time.\nOLMoTrace finds and shows verbatim matches between segments of language model\noutput and documents in the training text corpora. Powered by an extended\nversion of infini-gram (Liu et al., 2024), our system returns tracing results\nwithin a few seconds. OLMoTrace can help users understand the behavior of\nlanguage models through the lens of their training data. We showcase how it can\nbe used to explore fact checking, hallucination, and the creativity of language\nmodels. OLMoTrace is publicly available and fully open-source.", "AI": {"task": "\u5f00\u53d1\u4e00\u4e2a\u5b9e\u65f6\u8ffd\u8e2a\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u5230\u5176\u8bad\u7ec3\u6570\u636e\u7684\u7cfb\u7edfOLMoTrace\u3002", "motivation": "\u5e2e\u52a9\u7528\u6237\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u7684\u89c6\u89d2\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u5305\u62ec\u4e8b\u5b9e\u6838\u67e5\u3001\u5e7b\u89c9\u548c\u521b\u9020\u529b\u7b49\u65b9\u9762\u3002", "method": "\u57fa\u4e8e\u6269\u5c55\u7248\u7684infini-gram\u6280\u672f\uff0c\u5b9e\u65f6\u627e\u5230\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4e0e\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u9010\u5b57\u5339\u914d\u3002", "result": "\u7cfb\u7edf\u80fd\u5728\u51e0\u79d2\u5185\u8fd4\u56de\u8ffd\u8e2a\u7ed3\u679c\uff0c\u5e76\u516c\u5f00\u53ef\u7528\u3001\u5b8c\u5168\u5f00\u6e90\u3002", "conclusion": "OLMoTrace\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2504.06719", "pdf": "https://arxiv.org/pdf/2504.06719", "abs": "https://arxiv.org/abs/2504.06719", "authors": ["Pedro Hermosilla", "Christian Stippel", "Leon Sick"], "title": "Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPR 2025", "summary": "Self-supervised learning has transformed 2D computer vision by enabling\nmodels trained on large, unannotated datasets to provide versatile\noff-the-shelf features that perform similarly to models trained with labels.\nHowever, in 3D scene understanding, self-supervised methods are typically only\nused as a weight initialization step for task-specific fine-tuning, limiting\ntheir utility for general-purpose feature extraction. This paper addresses this\nshortcoming by proposing a robust evaluation protocol specifically designed to\nassess the quality of self-supervised features for 3D scene understanding. Our\nprotocol uses multi-resolution feature sampling of hierarchical models to\ncreate rich point-level representations that capture the semantic capabilities\nof the model and, hence, are suitable for evaluation with linear probing and\nnearest-neighbor methods. Furthermore, we introduce the first self-supervised\nmodel that performs similarly to supervised models when only off-the-shelf\nfeatures are used in a linear probing setup. In particular, our model is\ntrained natively in 3D with a novel self-supervised approach based on a Masked\nScene Modeling objective, which reconstructs deep features of masked patches in\na bottom-up manner and is specifically tailored to hierarchical 3D models. Our\nexperiments not only demonstrate that our method achieves competitive\nperformance to supervised models, but also surpasses existing self-supervised\napproaches by a large margin. The model and training code can be found at our\nGithub repository (https://github.com/phermosilla/msm).", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u8bc4\u4f30\u81ea\u76d1\u7763\u5b66\u4e60\u57283D\u573a\u666f\u7406\u89e3\u4e2d\u7279\u5f81\u8d28\u91cf\u7684\u534f\u8bae\uff0c\u5e76\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8eMasked Scene Modeling\u76ee\u6807\u7684\u81ea\u76d1\u7763\u6a21\u578b\u3002", "motivation": "\u5f53\u524d3D\u573a\u666f\u7406\u89e3\u4e2d\uff0c\u81ea\u76d1\u7763\u65b9\u6cd5\u4ec5\u7528\u4e8e\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\u521d\u59cb\u5316\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u7279\u5f81\u63d0\u53d6\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u591a\u5206\u8fa8\u7387\u7279\u5f81\u91c7\u6837\u534f\u8bae\u8bc4\u4f30\u7279\u5f81\u8d28\u91cf\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8eMasked Scene Modeling\u7684\u81ea\u76d1\u7763\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u4ec5\u4f7f\u7528\u73b0\u6210\u7279\u5f81\u7684\u7ebf\u6027\u63a2\u6d4b\u8bbe\u7f6e\u4e2d\u8868\u73b0\u63a5\u8fd1\u76d1\u7763\u6a21\u578b\uff0c\u4e14\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u534f\u8bae\u548c\u6a21\u578b\u4e3a3D\u573a\u666f\u7406\u89e3\u4e2d\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002"}}
{"id": "2311.12047", "pdf": "https://arxiv.org/pdf/2311.12047", "abs": "https://arxiv.org/abs/2311.12047", "authors": ["Jiali Cheng", "Hadi Amiri"], "title": "MultiDelete for Multimodal Machine Unlearning", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "ECCV 2024", "summary": "Machine Unlearning removes specific knowledge about training data samples\nfrom an already trained model. It has significant practical benefits, such as\npurging private, inaccurate, or outdated information from trained models\nwithout the need for complete re-training. Unlearning within a multimodal\nsetting presents unique challenges due to the complex dependencies between\ndifferent data modalities and the expensive cost of training on large\nmultimodal datasets and architectures. This paper presents the first machine\nunlearning approach for multimodal data and models, titled MultiDelete, which\nis designed to decouple associations between unimodal data points during\nunlearning without losing the overall representation strength of the trained\nmodel. MultiDelete advocates for three key properties for effective multimodal\nunlearning: (a): modality decoupling, which effectively decouples the\nassociation between individual unimodal data points marked for deletion,\nrendering them as unrelated data points, (b): multimodal knowledge retention,\nwhich retains the multimodal representation post-unlearning, and (c): unimodal\nknowledge retention, which retains the unimodal representation postunlearning.\nMultiDelete is efficient to train and is not constrained by using a strongly\nconvex loss -- a common restriction among existing baselines. Experiments on\ntwo architectures and four datasets, including image-text and graph-text\ndatasets, show that MultiDelete gains an average improvement of 17.6 points\nover best performing baseline in unlearning multimodal samples, can maintain\nthe multimodal and unimodal knowledge of the original model post unlearning,\nand can provide better protection to unlearned data against adversarial\nattacks.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aMultiDelete\u7684\u591a\u6a21\u6001\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5df2\u8bad\u7ec3\u6a21\u578b\u4e2d\u79fb\u9664\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u6837\u672c\u7684\u77e5\u8bc6\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u73af\u5883\u4e0b\u673a\u5668\u9057\u5fd8\u7684\u72ec\u7279\u6311\u6218\uff0c\u5982\u6570\u636e\u6a21\u6001\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u6027\u548c\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u53ca\u67b6\u6784\u7684\u9ad8\u8bad\u7ec3\u6210\u672c\u3002", "method": "MultiDelete\u901a\u8fc7\u6a21\u6001\u89e3\u8026\u3001\u591a\u6a21\u6001\u77e5\u8bc6\u4fdd\u7559\u548c\u5355\u6a21\u6001\u77e5\u8bc6\u4fdd\u7559\u4e09\u4e2a\u5173\u952e\u5c5e\u6027\uff0c\u6709\u6548\u89e3\u8026\u5f85\u5220\u9664\u5355\u6a21\u6001\u6570\u636e\u70b9\u7684\u5173\u8054\u3002", "result": "\u5728\u4e24\u79cd\u67b6\u6784\u548c\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMultiDelete\u5728\u9057\u5fd8\u591a\u6a21\u6001\u6837\u672c\u4e0a\u5e73\u5747\u63d0\u534717.6\u5206\uff0c\u5e76\u80fd\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u7684\u591a\u6a21\u6001\u548c\u5355\u6a21\u6001\u77e5\u8bc6\u3002", "conclusion": "MultiDelete\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u4e0d\u53d7\u5f3a\u51f8\u635f\u5931\u9650\u5236\u7684\u591a\u6a21\u6001\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u4fdd\u62a4\u9057\u5fd8\u6570\u636e\u514d\u53d7\u5bf9\u6297\u653b\u51fb\u3002"}}
{"id": "2504.06738", "pdf": "https://arxiv.org/pdf/2504.06738", "abs": "https://arxiv.org/abs/2504.06738", "authors": ["Wenfeng Feng", "Guoying Sun"], "title": "EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through an Encoder-Decoder Architecture", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel\narchitecture designed to mitigate the attention sink phenomenon observed in\nVision Transformer models. Attention sink occurs when an excessive amount of\nattention is allocated to the [CLS] token, distorting the model's ability to\neffectively process image patches. To address this, we introduce a\nlayer-aligned encoder-decoder architecture, where the encoder utilizes\nself-attention to process image patches, while the decoder uses cross-attention\nto focus on the [CLS] token. Unlike traditional encoder-decoder framework,\nwhere the decoder depends solely on high-level encoder representations, EDIT\nallows the decoder to extract information starting from low-level features,\nprogressively refining the representation layer by layer. EDIT is naturally\ninterpretable demonstrated through sequential attention maps, illustrating the\nrefined, layer-by-layer focus on key image features. Experiments on ImageNet-1k\nand ImageNet-21k, along with transfer learning tasks, show that EDIT achieves\nconsistent performance improvements over DeiT3 models. These results highlight\nthe effectiveness of EDIT's design in addressing attention sink and improving\nvisual feature extraction.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aEDIT\u7684\u65b0\u578b\u67b6\u6784\uff0c\u7528\u4e8e\u7f13\u89e3Vision Transformer\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u4e0b\u6c89\u73b0\u8c61\u3002", "motivation": "\u6ce8\u610f\u529b\u4e0b\u6c89\u73b0\u8c61\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u5173\u6ce8[CLS]\u6807\u8bb0\uff0c\u5f71\u54cd\u56fe\u50cf\u5757\u7684\u6709\u6548\u5904\u7406\u3002", "method": "\u91c7\u7528\u5c42\u5bf9\u9f50\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u7f16\u7801\u5668\u4f7f\u7528\u81ea\u6ce8\u610f\u529b\u5904\u7406\u56fe\u50cf\u5757\uff0c\u89e3\u7801\u5668\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5173\u6ce8[CLS]\u6807\u8bb0\uff0c\u5e76\u4ece\u4f4e\u5c42\u7279\u5f81\u9010\u6b65\u63d0\u53d6\u4fe1\u606f\u3002", "result": "\u5728ImageNet-1k\u548cImageNet-21k\u7b49\u4efb\u52a1\u4e0a\uff0cEDIT\u6027\u80fd\u4f18\u4e8eDeiT3\u6a21\u578b\u3002", "conclusion": "EDIT\u7684\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u6ce8\u610f\u529b\u4e0b\u6c89\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002"}}
{"id": "2504.05804", "pdf": "https://arxiv.org/pdf/2504.05804", "abs": "https://arxiv.org/abs/2504.05804", "authors": ["Yiming Tang", "Yi Fan", "Chenxiao Yu", "Tiankai Yang", "Yue Zhao", "Xiyang Hu"], "title": "StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "The integration of large language models (LLMs) into information retrieval\nsystems introduces new attack surfaces, particularly for adversarial ranking\nmanipulations. We present StealthRank, a novel adversarial ranking attack that\nmanipulates LLM-driven product recommendation systems while maintaining textual\nfluency and stealth. Unlike existing methods that often introduce detectable\nanomalies, StealthRank employs an energy-based optimization framework combined\nwith Langevin dynamics to generate StealthRank Prompts (SRPs)-adversarial text\nsequences embedded within product descriptions that subtly yet effectively\ninfluence LLM ranking mechanisms. We evaluate StealthRank across multiple LLMs,\ndemonstrating its ability to covertly boost the ranking of target products\nwhile avoiding explicit manipulation traces that can be easily detected. Our\nresults show that StealthRank consistently outperforms state-of-the-art\nadversarial ranking baselines in both effectiveness and stealth, highlighting\ncritical vulnerabilities in LLM-driven recommendation systems.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aStealthRank\u7684\u65b0\u578b\u5bf9\u6297\u6027\u6392\u540d\u653b\u51fb\u65b9\u6cd5\uff0c\u7528\u4e8e\u64cd\u7eb5\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ea7\u54c1\u63a8\u8350\u7cfb\u7edf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u4e2d\u7684\u96c6\u6210\u5f15\u5165\u4e86\u65b0\u7684\u653b\u51fb\u9762\uff0c\u5c24\u5176\u662f\u5bf9\u6297\u6027\u6392\u540d\u64cd\u7eb5\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u4f18\u5316\u6846\u67b6\u7ed3\u5408Langevin\u52a8\u529b\u5b66\uff0c\u751f\u6210\u9690\u853d\u7684\u5bf9\u6297\u6027\u6587\u672c\u5e8f\u5217\uff08SRPs\uff09\uff0c\u5d4c\u5165\u4ea7\u54c1\u63cf\u8ff0\u4e2d\u4ee5\u5f71\u54cdLLM\u7684\u6392\u540d\u673a\u5236\u3002", "result": "StealthRank\u5728\u591a\u79cdLLM\u4e0a\u8bc4\u4f30\uff0c\u80fd\u591f\u9690\u853d\u5730\u63d0\u5347\u76ee\u6807\u4ea7\u54c1\u6392\u540d\uff0c\u540c\u65f6\u907f\u514d\u88ab\u68c0\u6d4b\u5230\u660e\u663e\u7684\u64cd\u7eb5\u75d5\u8ff9\u3002", "conclusion": "StealthRank\u5728\u6548\u679c\u548c\u9690\u853d\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u5bf9\u6297\u6027\u6392\u540d\u57fa\u7ebf\uff0c\u63ed\u793a\u4e86LLM\u9a71\u52a8\u63a8\u8350\u7cfb\u7edf\u7684\u5173\u952e\u6f0f\u6d1e\u3002"}}
{"id": "2504.06740", "pdf": "https://arxiv.org/pdf/2504.06740", "abs": "https://arxiv.org/abs/2504.06740", "authors": ["Ylli Sadikaj", "Hongkuan Zhou", "Lavdim Halilaj", "Stefan Schmid", "Steffen Staab", "Claudia Plant"], "title": "MultiADS: Defect-aware Supervision for Multi-type Anomaly Detection and Segmentation in Zero-Shot Learning", "categories": ["cs.CV"], "comment": null, "summary": "Precise optical inspection in industrial applications is crucial for\nminimizing scrap rates and reducing the associated costs. Besides merely\ndetecting if a product is anomalous or not, it is crucial to know the distinct\ntype of defect, such as a bent, cut, or scratch. The ability to recognize the\n\"exact\" defect type enables automated treatments of the anomalies in modern\nproduction lines. Current methods are limited to solely detecting whether a\nproduct is defective or not without providing any insights on the defect type,\nnevertheless detecting and identifying multiple defects. We propose MultiADS, a\nzero-shot learning approach, able to perform Multi-type Anomaly Detection and\nSegmentation. The architecture of MultiADS comprises CLIP and extra linear\nlayers to align the visual- and textual representation in a joint feature\nspace. To the best of our knowledge, our proposal, is the first approach to\nperform a multi-type anomaly segmentation task in zero-shot learning. Contrary\nto the other baselines, our approach i) generates specific anomaly masks for\neach distinct defect type, ii) learns to distinguish defect types, and iii)\nsimultaneously identifies multiple defect types present in an anomalous\nproduct. Additionally, our approach outperforms zero/few-shot learning SoTA\nmethods on image-level and pixel-level anomaly detection and segmentation tasks\non five commonly used datasets: MVTec-AD, Visa, MPDD, MAD and Real-IAD.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aMultiADS\u7684\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u7c7b\u578b\u5f02\u5e38\u68c0\u6d4b\u548c\u5206\u5272\u3002", "motivation": "\u5de5\u4e1a\u5e94\u7528\u4e2d\u7cbe\u786e\u7684\u5149\u5b66\u68c0\u6d4b\u5bf9\u51cf\u5c11\u5e9f\u54c1\u7387\u548c\u6210\u672c\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u80fd\u68c0\u6d4b\u4ea7\u54c1\u662f\u5426\u6709\u7f3a\u9677\uff0c\u65e0\u6cd5\u8bc6\u522b\u5177\u4f53\u7f3a\u9677\u7c7b\u578b\u3002", "method": "MultiADS\u7ed3\u5408CLIP\u548c\u989d\u5916\u7ebf\u6027\u5c42\uff0c\u5728\u8054\u5408\u7279\u5f81\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u89c6\u89c9\u548c\u6587\u672c\u8868\u793a\u3002", "result": "MultiADS\u80fd\u591f\u4e3a\u6bcf\u79cd\u7f3a\u9677\u7c7b\u578b\u751f\u6210\u7279\u5b9a\u5f02\u5e38\u63a9\u7801\uff0c\u533a\u5206\u7f3a\u9677\u7c7b\u578b\uff0c\u5e76\u540c\u65f6\u8bc6\u522b\u591a\u4e2a\u7f3a\u9677\u7c7b\u578b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u96f6/\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "MultiADS\u662f\u9996\u4e2a\u5728\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u5b9e\u73b0\u591a\u7c7b\u578b\u5f02\u5e38\u5206\u5272\u7684\u65b9\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2504.06020", "pdf": "https://arxiv.org/pdf/2504.06020", "abs": "https://arxiv.org/abs/2504.06020", "authors": ["Liyuan Mao", "Haoran Xu", "Amy Zhang", "Weinan Zhang", "Chenjia Bai"], "title": "Information-Theoretic Reward Decomposition for Generalizable RLHF", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Work done during internships at Institute of Artificial Intelligence\n  (TeleAI), China Telecom", "summary": "A generalizable reward model is crucial in Reinforcement Learning from Human\nFeedback (RLHF) as it enables correctly evaluating unseen prompt-response\npairs. However, existing reward models lack this ability, as they are typically\ntrained by increasing the reward gap between chosen and rejected responses,\nwhile overlooking the prompts that the responses are conditioned on.\nConsequently, when the trained reward model is evaluated on prompt-response\npairs that lie outside the data distribution, neglecting the effect of prompts\nmay result in poor generalization of the reward model. To address this issue,\nwe decompose the reward value into two independent components: prompt-free\nreward and prompt-related reward. Prompt-free reward represents the evaluation\nthat is determined only by responses, while the prompt-related reward reflects\nthe reward that derives from both the prompt and the response. We extract these\ntwo components from an information-theoretic perspective, which requires no\nextra models. Subsequently, we propose a new reward learning algorithm by\nprioritizing data samples based on their prompt-free reward values. Through toy\nexamples, we demonstrate that the extracted prompt-free and prompt-related\nrewards effectively characterize two parts of the reward model. Further,\nstandard evaluations show that our method improves both the alignment\nperformance and the generalization capability of the reward model.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5956\u52b1\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u5956\u52b1\u503c\u4e3a\u63d0\u793a\u65e0\u5173\u548c\u63d0\u793a\u76f8\u5173\u4e24\u90e8\u5206\uff0c\u4ee5\u63d0\u5347\u5956\u52b1\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u5728\u8bc4\u4f30\u672a\u89c1\u8fc7\u7684\u63d0\u793a-\u54cd\u5e94\u5bf9\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u5ffd\u89c6\u4e86\u63d0\u793a\u5bf9\u5956\u52b1\u7684\u5f71\u54cd\u3002", "method": "\u5c06\u5956\u52b1\u503c\u5206\u89e3\u4e3a\u63d0\u793a\u65e0\u5173\u5956\u52b1\u548c\u63d0\u793a\u76f8\u5173\u5956\u52b1\uff0c\u5e76\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u63d0\u53d6\u8fd9\u4e24\u90e8\u5206\uff0c\u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u65e0\u5173\u5956\u52b1\u503c\u4f18\u5148\u5904\u7406\u6570\u636e\u6837\u672c\u7684\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8868\u5f81\u5956\u52b1\u6a21\u578b\u7684\u4e24\u90e8\u5206\uff0c\u5e76\u63d0\u5347\u5bf9\u9f50\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u89e3\u51b3\u4e86\u5956\u52b1\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.06741", "pdf": "https://arxiv.org/pdf/2504.06741", "abs": "https://arxiv.org/abs/2504.06741", "authors": ["Constantin Ulrich", "Tassilo Wald", "Fabian Isensee", "Klaus H. Maier-Hein"], "title": "Large Scale Supervised Pretraining For Traumatic Brain Injury Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The segmentation of lesions in Moderate to Severe Traumatic Brain Injury\n(msTBI) presents a significant challenge in neuroimaging due to the diverse\ncharacteristics of these lesions, which vary in size, shape, and distribution\nacross brain regions and tissue types. This heterogeneity complicates\ntraditional image processing techniques, resulting in critical errors in tasks\nsuch as image registration and brain parcellation. To address these challenges,\nthe AIMS-TBI Segmentation Challenge 2024 aims to advance innovative\nsegmentation algorithms specifically designed for T1-weighted MRI data, the\nmost widely utilized imaging modality in clinical practice. Our proposed\nsolution leverages a large-scale multi-dataset supervised pretraining approach\ninspired by the MultiTalent method. We train a Resenc L network on a\ncomprehensive collection of datasets covering various anatomical and\npathological structures, which equips the model with a robust understanding of\nbrain anatomy and pathology. Following this, the model is fine-tuned on\nmsTBI-specific data to optimize its performance for the unique characteristics\nof T1-weighted MRI scans and outperforms the baseline without pretraining up to\n2 Dice points.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u9488\u5bf9T1\u52a0\u6743MRI\u6570\u636e\u7684\u521b\u65b0\u5206\u5272\u7b97\u6cd5\uff0c\u7528\u4e8e\u4e2d\u91cd\u5ea6\u521b\u4f24\u6027\u8111\u635f\u4f24\uff08msTBI\uff09\u75c5\u7076\u7684\u5206\u5272\u3002", "motivation": "\u7531\u4e8emsTBI\u75c5\u7076\u5728\u5927\u5c0f\u3001\u5f62\u72b6\u548c\u5206\u5e03\u4e0a\u7684\u591a\u6837\u6027\uff0c\u4f20\u7edf\u56fe\u50cf\u5904\u7406\u6280\u672f\u5b58\u5728\u4e25\u91cd\u8bef\u5dee\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5927\u89c4\u6a21\u591a\u6570\u636e\u96c6\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff08MultiTalent\u65b9\u6cd5\uff09\uff0c\u5728\u6db5\u76d6\u591a\u79cd\u89e3\u5256\u548c\u75c5\u7406\u7ed3\u6784\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3Resenc L\u7f51\u7edc\uff0c\u968f\u540e\u5728msTBI\u7279\u5b9a\u6570\u636e\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u6a21\u578b\u5728T1\u52a0\u6743MRI\u626b\u63cf\u4e0a\u7684\u6027\u80fd\u4f18\u4e8e\u672a\u9884\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\uff0cDice\u5206\u6570\u63d0\u5347\u9ad8\u8fbe2\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86msTBI\u75c5\u7076\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.06269", "pdf": "https://arxiv.org/pdf/2504.06269", "abs": "https://arxiv.org/abs/2504.06269", "authors": ["Yin Wu", "Zhengxuan Zhang", "Fuling Wang", "Yuyu Luo", "Hui Xiong", "Nan Tang"], "title": "EXCLAIM: An Explainable Cross-Modal Agentic System for Misinformation Detection with Hierarchical Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "15 pages, 2 figures", "summary": "Misinformation continues to pose a significant challenge in today's\ninformation ecosystem, profoundly shaping public perception and behavior. Among\nits various manifestations, Out-of-Context (OOC) misinformation is particularly\nobscure, as it distorts meaning by pairing authentic images with misleading\ntextual narratives. Existing methods for detecting OOC misinformation\npredominantly rely on coarse-grained similarity metrics between image-text\npairs, which often fail to capture subtle inconsistencies or provide meaningful\nexplainability. While multi-modal large language models (MLLMs) demonstrate\nremarkable capabilities in visual reasoning and explanation generation, they\nhave not yet demonstrated the capacity to address complex, fine-grained, and\ncross-modal distinctions necessary for robust OOC detection. To overcome these\nlimitations, we introduce EXCLAIM, a retrieval-based framework designed to\nleverage external knowledge through multi-granularity index of multi-modal\nevents and entities. Our approach integrates multi-granularity contextual\nanalysis with a multi-agent reasoning architecture to systematically evaluate\nthe consistency and integrity of multi-modal news content. Comprehensive\nexperiments validate the effectiveness and resilience of EXCLAIM, demonstrating\nits ability to detect OOC misinformation with 4.3% higher accuracy compared to\nstate-of-the-art approaches, while offering explainable and actionable\ninsights.", "AI": {"task": "\u68c0\u6d4b\u548c\u89e3\u91ca\u4e0a\u4e0b\u6587\u4e0d\u7b26\uff08OOC\uff09\u7684\u865a\u5047\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u7684OOC\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7c92\u5ea6\u7684\u56fe\u50cf-\u6587\u672c\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u7684\u4e0d\u4e00\u81f4\u6027\u6216\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51faEXCLAIM\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u7d22\u5f15\u548c\u591a\u667a\u80fd\u4f53\u63a8\u7406\u67b6\u6784\uff0c\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\u8fdb\u884c\u591a\u6a21\u6001\u5185\u5bb9\u7684\u4e00\u81f4\u6027\u5206\u6790\u3002", "result": "EXCLAIM\u5728\u68c0\u6d4bOOC\u865a\u5047\u4fe1\u606f\u65f6\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u9ad84.3%\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6d1e\u5bdf\u3002", "conclusion": "EXCLAIM\u6846\u67b6\u5728\u68c0\u6d4bOOC\u865a\u5047\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u5185\u5bb9\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.06742", "pdf": "https://arxiv.org/pdf/2504.06742", "abs": "https://arxiv.org/abs/2504.06742", "authors": ["Alexandra Ertl", "Shuhan Xiao", "Stefan Denner", "Robin Peretzke", "David Zimmerer", "Peter Neher", "Fabian Isensee", "Klaus Maier-Hein"], "title": "nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection", "categories": ["cs.CV"], "comment": null, "summary": "Landmark detection plays a crucial role in medical imaging tasks that rely on\nprecise spatial localization, including specific applications in diagnosis,\ntreatment planning, image registration, and surgical navigation. However,\nmanual annotation is labor-intensive and requires expert knowledge. While deep\nlearning shows promise in automating this task, progress is hindered by limited\npublic datasets, inconsistent benchmarks, and non-standardized baselines,\nrestricting reproducibility, fair comparisons, and model generalizability.This\nwork introduces nnLandmark, a self-configuring deep learning framework for 3D\nmedical landmark detection, adapting nnU-Net to perform heatmap-based\nregression. By leveraging nnU-Net's automated configuration, nnLandmark\neliminates the need for manual parameter tuning, offering out-of-the-box\nusability. It achieves state-of-the-art accuracy across two public datasets,\nwith a mean radial error (MRE) of 1.5 mm on the Mandibular Molar Landmark (MML)\ndental CT dataset and 1.2 mm for anatomical fiducials on a brain MRI dataset\n(AFIDs), where nnLandmark aligns with the inter-rater variability of 1.5 mm.\nWith its strong generalization, reproducibility, and ease of deployment,\nnnLandmark establishes a reliable baseline for 3D landmark detection,\nsupporting research in anatomical localization and clinical workflows that\ndepend on precise landmark identification. The code will be available soon.", "AI": {"task": "\u5f00\u53d1\u4e00\u4e2a\u81ea\u914d\u7f6e\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6nnLandmark\uff0c\u7528\u4e8e3D\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u6807\u5fd7\u70b9\u68c0\u6d4b\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u4e2d\u6807\u5fd7\u70b9\u68c0\u6d4b\u5bf9\u8bca\u65ad\u3001\u6cbb\u7597\u8ba1\u5212\u7b49\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u624b\u52a8\u6807\u6ce8\u8017\u65f6\u4e14\u4f9d\u8d56\u4e13\u5bb6\u77e5\u8bc6\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u4e0d\u8db3\u3001\u57fa\u51c6\u4e0d\u7edf\u4e00\u7b49\u95ee\u9898\u3002", "method": "\u57fa\u4e8ennU-Net\u7684\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u91c7\u7528\u70ed\u56fe\u56de\u5f52\u65b9\u6cd5\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u53c2\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7cbe\u5ea6\uff08MML\u6570\u636e\u96c6MRE\u4e3a1.5 mm\uff0cAFIDs\u6570\u636e\u96c6MRE\u4e3a1.2 mm\uff09\u3002", "conclusion": "nnLandmark\u5177\u6709\u5f3a\u6cdb\u5316\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u6613\u90e8\u7f72\u6027\uff0c\u4e3a3D\u6807\u5fd7\u70b9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u51c6\u3002"}}
{"id": "2504.06271", "pdf": "https://arxiv.org/pdf/2504.06271", "abs": "https://arxiv.org/abs/2504.06271", "authors": ["Yikuan Xia", "Jiazun Chen", "Yirui Zhan", "Suifeng Zhao", "Weipeng Jiang", "Chaorui Zhang", "Wei Han", "Bo Bai", "Jun Gao"], "title": "ER-RAG: Enhance RAG with ER-Based Unified Modeling of Heterogeneous Data Sources", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel in question-answering (QA) tasks, and\nretrieval-augmented generation (RAG) enhances their precision by incorporating\nexternal evidence from diverse sources like web pages, databases, and knowledge\ngraphs. However, current RAG methods rely on agent-specific strategies for\nindividual data sources, posing challenges low-resource or black-box\nenvironments and complicates operations when evidence is fragmented across\nsources. To address these limitations, we propose ER-RAG, a framework that\nunifies evidence integration across heterogeneous data sources using the\nEntity-Relationship (ER) model. ER-RAG standardizes entity retrieval and\nrelationship querying through ER-based APIs with GET and JOIN operations. It\nemploys a two-stage generation process: first, a preference optimization module\nselects optimal sources; second, another module constructs API chains based on\nsource schemas. This unified approach allows efficient fine-tuning and seamless\nintegration across diverse data sources. ER-RAG demonstrated its effectiveness\nby winning all three tracks of the 2024 KDDCup CRAG Challenge, achieving\nperformance on par with commercial RAG pipelines using an 8B LLM backbone. It\noutperformed hybrid competitors by 3.1% in LLM score and accelerated retrieval\nby 5.5X.", "AI": {"task": "\u63d0\u51faER-RAG\u6846\u67b6\uff0c\u7edf\u4e00\u5f02\u6784\u6570\u636e\u6e90\u7684\u8bc1\u636e\u6574\u5408\uff0c\u63d0\u5347\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u6548\u7387\u3002", "motivation": "\u5f53\u524dRAG\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u6570\u636e\u6e90\u7684\u7b56\u7565\uff0c\u5728\u4f4e\u8d44\u6e90\u6216\u9ed1\u76d2\u73af\u5883\u4e2d\u5b58\u5728\u6311\u6218\uff0c\u4e14\u8bc1\u636e\u5206\u6563\u65f6\u64cd\u4f5c\u590d\u6742\u3002", "method": "\u4f7f\u7528\u5b9e\u4f53-\u5173\u7cfb\uff08ER\uff09\u6a21\u578b\u6807\u51c6\u5316\u5b9e\u4f53\u68c0\u7d22\u548c\u5173\u7cfb\u67e5\u8be2\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff1a\u4f18\u9009\u6570\u636e\u6e90\u5e76\u6784\u5efaAPI\u94fe\u3002", "result": "ER-RAG\u57282024 KDDCup CRAG Challenge\u4e2d\u83b7\u80dc\uff0c\u6027\u80fd\u4e0e\u5546\u4e1aRAG\u7ba1\u9053\u76f8\u5f53\uff0cLLM\u8bc4\u5206\u63d0\u9ad83.1%\uff0c\u68c0\u7d22\u901f\u5ea6\u63d0\u53475.5\u500d\u3002", "conclusion": "ER-RAG\u901a\u8fc7\u7edf\u4e00\u5f02\u6784\u6570\u636e\u6e90\u7684\u8bc1\u636e\u6574\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2504.06751", "pdf": "https://arxiv.org/pdf/2504.06751", "abs": "https://arxiv.org/abs/2504.06751", "authors": ["Leszek Luchowski", "Dariusz Pojda"], "title": "Visualisation of a multidimensional point cloud as a 3D swarm of avatars", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "The article presents an innovative approach to the visualisation of\nmultidimensional data, using icons inspired by Chernoff faces. The approach\nmerges classical projection techniques with the assignment of particular data\ndimensions to mimic features, capitalizing on the natural ability of the human\nbrain to interpret facial expressions. The technique is implemented as a plugin\nto the dpVision open-source image handling platform. The plugin allows the data\nto be interactively explored in the form of a swarm of \"totems\" whose position\nin hyperspace as well as facial features represent various aspects of the data.\nSample visualisations, based on synthetic test data as well as the vinhoverde\n15-dimensional database on Portuguese wines, confirm the usefulness of our\napproach to the analysis of complex data structures.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eChernoff\u9762\u5b54\u542f\u53d1\u7684\u56fe\u6807\u7684\u591a\u7ef4\u6570\u636e\u53ef\u89c6\u5316\u521b\u65b0\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528\u4eba\u8111\u81ea\u7136\u89e3\u8bfb\u9762\u90e8\u8868\u60c5\u7684\u80fd\u529b\uff0c\u5c06\u6570\u636e\u7ef4\u5ea6\u6620\u5c04\u5230\u9762\u90e8\u7279\u5f81\u4e0a\uff0c\u4ee5\u63d0\u5347\u591a\u7ef4\u6570\u636e\u7684\u53ef\u89c6\u5316\u6548\u679c\u3002", "method": "\u7ed3\u5408\u7ecf\u5178\u6295\u5f71\u6280\u672f\uff0c\u5c06\u6570\u636e\u7ef4\u5ea6\u5206\u914d\u5230\u9762\u90e8\u7279\u5f81\uff0c\u5e76\u5b9e\u73b0\u4e3adpVision\u5f00\u6e90\u56fe\u50cf\u5904\u7406\u5e73\u53f0\u7684\u63d2\u4ef6\u3002", "result": "\u901a\u8fc7\u5408\u6210\u6d4b\u8bd5\u6570\u636e\u548c\u8461\u8404\u7259\u8461\u8404\u9152\u768415\u7ef4\u6570\u636e\u5e93\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u6570\u636e\u7ed3\u6784\u7684\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u53ef\u89c6\u5316\u5de5\u5177\u3002"}}
{"id": "2504.06273", "pdf": "https://arxiv.org/pdf/2504.06273", "abs": "https://arxiv.org/abs/2504.06273", "authors": ["Jiaming Luo", "Weiyi Luo", "Guoqing Sun", "Mengchen Zhu", "Haifeng Tang", "Kunyao Lan", "Mengyue Wu", "Kenny Q. Zhu"], "title": "A Diverse and Effective Retrieval-Based Debt Collection System with Expert Knowledge", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Accepted by NAACL 2025, Industry Track", "summary": "Designing effective debt collection systems is crucial for improving\noperational efficiency and reducing costs in the financial industry. However,\nthe challenges of maintaining script diversity, contextual relevance, and\ncoherence make this task particularly difficult. This paper presents a debt\ncollection system based on real debtor-collector data from a major commercial\nbank. We construct a script library from real-world debt collection\nconversations, and propose a two-stage retrieval based response system for\ncontextual relevance. Experimental results show that our system improves script\ndiversity, enhances response relevance, and achieves practical deployment\nefficiency through knowledge distillation. This work offers a scalable and\nautomated solution, providing valuable insights for advancing debt collection\npractices in real-world applications.", "AI": {"task": "\u8bbe\u8ba1\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u503a\u52a1\u4eba\u4e0e\u50ac\u6536\u5458\u6570\u636e\u7684\u503a\u52a1\u50ac\u6536\u7cfb\u7edf\uff0c\u4ee5\u63d0\u9ad8\u811a\u672c\u591a\u6837\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002", "motivation": "\u503a\u52a1\u50ac\u6536\u7cfb\u7edf\u7684\u8bbe\u8ba1\u5bf9\u63d0\u9ad8\u91d1\u878d\u884c\u4e1a\u7684\u8fd0\u8425\u6548\u7387\u548c\u964d\u4f4e\u6210\u672c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u811a\u672c\u591a\u6837\u6027\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u548c\u4e00\u81f4\u6027\u7684\u7ef4\u62a4\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6784\u5efa\u771f\u5b9e\u503a\u52a1\u50ac\u6536\u5bf9\u8bdd\u7684\u811a\u672c\u5e93\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u4e24\u9636\u6bb5\u68c0\u7d22\u7684\u54cd\u5e94\u7cfb\u7edf\u4ee5\u786e\u4fdd\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u63d0\u9ad8\u4e86\u811a\u672c\u591a\u6837\u6027\u3001\u589e\u5f3a\u4e86\u54cd\u5e94\u76f8\u5173\u6027\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u503a\u52a1\u50ac\u6536\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2504.06752", "pdf": "https://arxiv.org/pdf/2504.06752", "abs": "https://arxiv.org/abs/2504.06752", "authors": ["Rishbuh Parihar", "Vaibhav Agrawal", "Sachidanand VS", "R. Venkatesh Babu"], "title": "Compass Control: Multi Object Orientation Control for Text-to-Image Generation", "categories": ["cs.CV"], "comment": "https://rishubhpar.github.io/compasscontrol", "summary": "Existing approaches for controlling text-to-image diffusion models, while\npowerful, do not allow for explicit 3D object-centric control, such as precise\ncontrol of object orientation. In this work, we address the problem of\nmulti-object orientation control in text-to-image diffusion models. This\nenables the generation of diverse multi-object scenes with precise orientation\ncontrol for each object. The key idea is to condition the diffusion model with\na set of orientation-aware \\textbf{compass} tokens, one for each object, along\nwith text tokens. A light-weight encoder network predicts these compass tokens\ntaking object orientation as the input. The model is trained on a synthetic\ndataset of procedurally generated scenes, each containing one or two 3D assets\non a plain background. However, direct training this framework results in poor\norientation control as well as leads to entanglement among objects. To mitigate\nthis, we intervene in the generation process and constrain the cross-attention\nmaps of each compass token to its corresponding object regions. The trained\nmodel is able to achieve precise orientation control for a) complex objects not\nseen during training and b) multi-object scenes with more than two objects,\nindicating strong generalization capabilities. Further, when combined with\npersonalization methods, our method precisely controls the orientation of the\nnew object in diverse contexts. Our method achieves state-of-the-art\norientation control and text alignment, quantified with extensive evaluations\nand a user study.", "AI": {"task": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u591a\u5bf9\u8c61\u65b9\u5411\u63a7\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u73b0\u5bf93D\u5bf9\u8c61\u65b9\u5411\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u9650\u5236\u4e86\u751f\u6210\u591a\u6837\u5316\u573a\u666f\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u65b9\u5411\u611f\u77e5\u7684\u201c\u6307\u5357\u9488\u201d\u6807\u8bb0\u548c\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u7f51\u7edc\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u7ea6\u675f\uff0c\u5b9e\u73b0\u5bf9\u5bf9\u8c61\u65b9\u5411\u7684\u7cbe\u786e\u63a7\u5236\u3002", "result": "\u6a21\u578b\u80fd\u591f\u5bf9\u672a\u89c1\u8fc7\u7684\u590d\u6742\u5bf9\u8c61\u548c\u591a\u5bf9\u8c61\u573a\u666f\u5b9e\u73b0\u7cbe\u786e\u65b9\u5411\u63a7\u5236\uff0c\u5e76\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65b9\u5411\u63a7\u5236\u548c\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u901a\u8fc7\u5e7f\u6cdb\u8bc4\u4f30\u548c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.06303", "pdf": "https://arxiv.org/pdf/2504.06303", "abs": "https://arxiv.org/abs/2504.06303", "authors": ["Dang Nguyen", "Chenhao Tan"], "title": "On the Effectiveness and Generalization of Race Representations for Debiasing High-Stakes Decisions", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "21 pages, 15 figures, 14 tables", "summary": "Understanding and mitigating biases is critical for the adoption of large\nlanguage models (LLMs) in high-stakes decision-making. We introduce Admissions\nand Hiring, decision tasks with hypothetical applicant profiles where a\nperson's race can be inferred from their name, as simplified test beds for\nracial bias. We show that Gemma 2B Instruct and LLaMA 3.2 3B Instruct exhibit\nstrong biases. Gemma grants admission to 26% more White than Black applicants,\nand LLaMA hires 60% more Asian than White applicants. We demonstrate that these\nbiases are resistant to prompt engineering: multiple prompting strategies all\nfail to promote fairness. In contrast, using distributed alignment search, we\ncan identify \"race subspaces\" within model activations and intervene on them to\ndebias model decisions. Averaging the representation across all races within\nthe subspaces reduces Gemma's bias by 37-57%. Finally, we examine the\ngeneralizability of Gemma's race subspaces, and find limited evidence for\ngeneralization, where changing the prompt format can affect the race\nrepresentation. Our work suggests mechanistic approaches may provide a\npromising venue for improving the fairness of LLMs, but a universal race\nrepresentation remains elusive.", "AI": {"task": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u62db\u751f\u548c\u62db\u8058\u51b3\u7b56\u4e2d\u7684\u79cd\u65cf\u504f\u89c1\u53ca\u5176\u7f13\u89e3\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3\u548c\u51cf\u8f7b\u504f\u89c1\u5bf9\u4e8eLLMs\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u5047\u8bbe\u7684\u7533\u8bf7\u4eba\u6863\u6848\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u5bf9\u9f50\u641c\u7d22\u8bc6\u522b\u5e76\u5e72\u9884\u6a21\u578b\u6fc0\u6d3b\u4e2d\u7684\u201c\u79cd\u65cf\u5b50\u7a7a\u95f4\u201d\u3002", "result": "Gemma\u548cLLaMA\u6a21\u578b\u8868\u73b0\u51fa\u663e\u8457\u504f\u89c1\uff0c\u4f46\u901a\u8fc7\u5e72\u9884\u79cd\u65cf\u5b50\u7a7a\u95f4\uff0cGemma\u7684\u504f\u89c1\u51cf\u5c11\u4e8637-57%\u3002", "conclusion": "\u673a\u5236\u6027\u65b9\u6cd5\u53ef\u80fd\u6539\u5584LLMs\u7684\u516c\u5e73\u6027\uff0c\u4f46\u901a\u7528\u7684\u79cd\u65cf\u8868\u5f81\u5c1a\u672a\u5b9e\u73b0\u3002"}}
{"id": "2504.06755", "pdf": "https://arxiv.org/pdf/2504.06755", "abs": "https://arxiv.org/abs/2504.06755", "authors": ["Li Yu", "Zhihui Li", "Jimin Xiao", "Moncef Gabbouj"], "title": "FANeRV: Frequency Separation and Augmentation based Neural Representation for Video", "categories": ["cs.CV"], "comment": null, "summary": "Neural representations for video (NeRV) have gained considerable attention\nfor their strong performance across various video tasks. However, existing NeRV\nmethods often struggle to capture fine spatial details, resulting in vague\nreconstructions. In this paper, we present a Frequency Separation and\nAugmentation based Neural Representation for video (FANeRV), which addresses\nthese limitations with its core Wavelet Frequency Upgrade Block.This block\nexplicitly separates input frames into high and low-frequency components using\ndiscrete wavelet transform, followed by targeted enhancement using specialized\nmodules. Finally, a specially designed gated network effectively fuses these\nfrequency components for optimal reconstruction. Additionally, convolutional\nresidual enhancement blocks are integrated into the later stages of the network\nto balance parameter distribution and improve the restoration of high-frequency\ndetails. Experimental results demonstrate that FANeRV significantly improves\nreconstruction performance and excels in multiple tasks, including video\ncompression, inpainting, and interpolation, outperforming existing NeRV\nmethods.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u5206\u79bb\u548c\u589e\u5f3a\u7684\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff08FANeRV\uff09\u4ee5\u6539\u8fdb\u89c6\u9891\u91cd\u5efa\u6027\u80fd\u3002", "motivation": "\u73b0\u6709NeRV\u65b9\u6cd5\u5728\u6355\u6349\u7a7a\u95f4\u7ec6\u8282\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u5bfc\u81f4\u91cd\u5efa\u7ed3\u679c\u6a21\u7cca\u3002", "method": "\u4f7f\u7528\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\u5206\u79bb\u9ad8\u4f4e\u9891\u6210\u5206\uff0c\u5e76\u901a\u8fc7\u4e13\u95e8\u6a21\u5757\u589e\u5f3a\uff0c\u6700\u540e\u901a\u8fc7\u95e8\u63a7\u7f51\u7edc\u878d\u5408\uff1b\u5f15\u5165\u5377\u79ef\u6b8b\u5dee\u589e\u5f3a\u5757\u4f18\u5316\u9ad8\u9891\u7ec6\u8282\u6062\u590d\u3002", "result": "FANeRV\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u6027\u80fd\uff0c\u5728\u89c6\u9891\u538b\u7f29\u3001\u4fee\u590d\u548c\u63d2\u503c\u7b49\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709NeRV\u65b9\u6cd5\u3002", "conclusion": "FANeRV\u901a\u8fc7\u9891\u7387\u5206\u79bb\u548c\u589e\u5f3a\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u89c6\u9891\u91cd\u5efa\u6548\u679c\u3002"}}
{"id": "2504.06407", "pdf": "https://arxiv.org/pdf/2504.06407", "abs": "https://arxiv.org/abs/2504.06407", "authors": ["Jiali Cheng", "Hadi Amiri"], "title": "Understanding Machine Unlearning Through the Lens of Mode Connectivity", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Machine Unlearning aims to remove undesired information from trained models\nwithout requiring full retraining from scratch. Despite recent advancements,\ntheir underlying loss landscapes and optimization dynamics received less\nattention. In this paper, we investigate and analyze machine unlearning through\nthe lens of mode connectivity - the phenomenon where independently trained\nmodels can be connected by smooth low-loss paths in the parameter space. We\ndefine and study mode connectivity in unlearning across a range of overlooked\nconditions, including connections between different unlearning methods, models\ntrained with and without curriculum learning, and models optimized with\nfirst-order and secondorder techniques. Our findings show distinct patterns of\nfluctuation of different evaluation metrics along the curve, as well as the\nmechanistic (dis)similarity between unlearning methods. To the best of our\nknowledge, this is the first study on mode connectivity in the context of\nmachine unlearning.", "AI": {"task": "\u7814\u7a76\u673a\u5668\u9057\u5fd8\u4e2d\u7684\u6a21\u5f0f\u8fde\u901a\u6027\u53ca\u5176\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u9057\u5fd8\u4e2d\u6a21\u5f0f\u8fde\u901a\u6027\u7684\u73b0\u8c61\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u5bf9\u635f\u5931\u666f\u89c2\u548c\u4f18\u5316\u52a8\u6001\u7684\u5ffd\u89c6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u9057\u5fd8\u65b9\u6cd5\u3001\u8bfe\u7a0b\u5b66\u4e60\u6a21\u578b\u4ee5\u53ca\u4e00\u9636\u548c\u4e8c\u9636\u4f18\u5316\u6280\u672f\u4e4b\u95f4\u7684\u6a21\u5f0f\u8fde\u901a\u6027\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u8bc4\u4f30\u6307\u6807\u6cbf\u66f2\u7ebf\u7684\u6ce2\u52a8\u6a21\u5f0f\uff0c\u4ee5\u53ca\u9057\u5fd8\u65b9\u6cd5\u4e4b\u95f4\u7684\u673a\u5236\u76f8\u4f3c\u6027\u548c\u5dee\u5f02\u6027\u3002", "conclusion": "\u9996\u6b21\u5728\u673a\u5668\u9057\u5fd8\u80cc\u666f\u4e0b\u7814\u7a76\u4e86\u6a21\u5f0f\u8fde\u901a\u6027\uff0c\u63ed\u793a\u4e86\u5176\u72ec\u7279\u7684\u884c\u4e3a\u548c\u673a\u5236\u3002"}}
{"id": "2504.06777", "pdf": "https://arxiv.org/pdf/2504.06777", "abs": "https://arxiv.org/abs/2504.06777", "authors": ["Xi Tao", "Liyan Lin"], "title": "End2end-ALARA: Approaching the ALARA Law in CT Imaging with End-to-end Learning", "categories": ["cs.CV"], "comment": null, "summary": "Computed tomography (CT) examination poses radiation injury to patient. A\nconsensus performing CT imaging is to make the radiation dose as low as\nreasonably achievable, i.e. the ALARA law. In this paper, we propose an\nend-to-end learning framework, named End2end-ALARA, that jointly optimizes dose\nmodulation and image reconstruction to meet the goal of ALARA in CT imaging.\nEnd2end-ALARA works by building a dose modulation module and an image\nreconstruction module, connecting these modules with a differentiable\nsimulation function, and optimizing the them with a constrained hinge loss\nfunction. The objective is to minimize radiation dose subject to a prescribed\nimage quality (IQ) index. The results show that End2end-ALARA is able to preset\npersonalized dose levels to gain a stable IQ level across patients, which may\nfacilitate image-based diagnosis and downstream model training. Moreover,\ncompared to fixed-dose and conventional dose modulation strategies,\nEnd2end-ALARA consumes lower dose to reach the same IQ level. Our study sheds\nlight on a way of realizing the ALARA law in CT imaging.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aEnd2end-ALARA\u7684\u7aef\u5230\u7aef\u5b66\u4e60\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u5242\u91cf\u8c03\u5236\u548c\u56fe\u50cf\u91cd\u5efa\uff0c\u4ee5\u5b9e\u73b0CT\u6210\u50cf\u4e2d\u7684ALARA\u76ee\u6807\u3002", "motivation": "CT\u68c0\u67e5\u5bf9\u60a3\u8005\u6709\u8f90\u5c04\u4f24\u5bb3\uff0cALARA\u539f\u5219\u8981\u6c42\u5c3d\u53ef\u80fd\u964d\u4f4e\u8f90\u5c04\u5242\u91cf\u3002", "method": "\u6784\u5efa\u5242\u91cf\u8c03\u5236\u6a21\u5757\u548c\u56fe\u50cf\u91cd\u5efa\u6a21\u5757\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u6a21\u62df\u51fd\u6570\u8fde\u63a5\uff0c\u5e76\u4f7f\u7528\u7ea6\u675f\u94f0\u94fe\u635f\u5931\u51fd\u6570\u4f18\u5316\u3002", "result": "End2end-ALARA\u80fd\u4e2a\u6027\u5316\u9884\u8bbe\u5242\u91cf\u6c34\u5e73\uff0c\u7a33\u5b9a\u56fe\u50cf\u8d28\u91cf\uff0c\u4e14\u6bd4\u56fa\u5b9a\u5242\u91cf\u548c\u4f20\u7edf\u5242\u91cf\u8c03\u5236\u7b56\u7565\u66f4\u8282\u7701\u5242\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aCT\u6210\u50cf\u4e2d\u5b9e\u73b0ALARA\u539f\u5219\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u65b9\u6cd5\u3002"}}
{"id": "2504.06514", "pdf": "https://arxiv.org/pdf/2504.06514", "abs": "https://arxiv.org/abs/2504.06514", "authors": ["Chenrui Fan", "Ming Li", "Lichao Sun", "Tianyi Zhou"], "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.", "AI": {"task": "\u7814\u7a76\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7f3a\u5931\u524d\u63d0\uff08MiP\uff09\u95ee\u9898\u4e0a\u7684\u54cd\u5e94\u957f\u5ea6\u548c\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u53d1\u73b0\u63a8\u7406\u578bLLMs\u5728MiP\u95ee\u9898\u4e0a\u8868\u73b0\u51fa\u5197\u4f59\u548c\u4f4e\u6548\u7684\u601d\u8003\uff08MiP-Overthinking\uff09\uff0c\u8fdd\u80cc\u4e86\u201c\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u5b9a\u5f8b\u201d\uff0c\u4e14\u7f3a\u4e4f\u6279\u5224\u6027\u601d\u7ef4\u3002", "method": "\u901a\u8fc7\u5728\u591a\u6570\u636e\u96c6\u4e0a\u89c2\u5bdfMiP\u573a\u666f\u4e0b\u7684LLMs\u8868\u73b0\uff0c\u5206\u6790\u63a8\u7406\u957f\u5ea6\u3001\u8fc7\u5ea6\u601d\u8003\u6a21\u5f0f\u53ca\u6279\u5224\u6027\u601d\u7ef4\u4f4d\u7f6e\uff0c\u5e76\u8fdb\u884c\u84b8\u998f\u5b9e\u9a8c\u3002", "result": "\u63a8\u7406\u578bLLMs\u5728MiP\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u975e\u63a8\u7406\u578bLLMs\u8868\u73b0\u66f4\u597d\uff1b\u8fc7\u5ea6\u601d\u8003\u53ef\u901a\u8fc7\u84b8\u998f\u4f20\u64ad\u3002", "conclusion": "\u5f53\u524d\u63a8\u7406\u578bLLMs\u7684\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\uff0c\u9700\u6539\u8fdb\u4ee5\u9f13\u52b1\u9ad8\u6548\u601d\u8003\uff0c\u907f\u514d\u8fc7\u5ea6\u601d\u8003\u6ee5\u7528\u3002"}}
{"id": "2504.06781", "pdf": "https://arxiv.org/pdf/2504.06781", "abs": "https://arxiv.org/abs/2504.06781", "authors": ["Reiji Saito", "Kazuhiro Hotta"], "title": "Domain Generalization through Attenuation of Domain-Specific Information", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 Workshops", "summary": "In this paper, we propose a new evaluation metric called Domain Independence\n(DI) and Attenuation of Domain-Specific Information (ADSI) which is\nspecifically designed for domain-generalized semantic segmentation in\nautomotive images. DI measures the presence of domain-specific information: a\nlower DI value indicates strong domain dependence, while a higher DI value\nsuggests greater domain independence. This makes it roughly where\ndomain-specific information exists and up to which frequency range it is\npresent. As a result, it becomes possible to effectively suppress only the\nregions in the image that contain domain-specific information, enabling feature\nextraction independent of the domain. ADSI uses a Butterworth filter to remove\nthe low-frequency components of images that contain inherent domain-specific\ninformation such as sensor characteristics and lighting conditions. However,\nsince low-frequency components also contain important information such as\ncolor, we should not remove them completely. Thus, a scalar value (ranging from\n0 to 1) is multiplied by the low-frequency components to retain essential\ninformation. This helps the model learn more domain-independent features. In\nexperiments, GTA5 (synthetic dataset) was used as training images, and a\nreal-world dataset was used for evaluation, and the proposed method\noutperformed conventional approaches. Similarly, in experiments that the\nCityscapes (real-world dataset) was used for training and various environment\ndatasets such as rain and nighttime were used for evaluation, the proposed\nmethod demonstrated its robustness under nighttime conditions.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6307\u6807DI\u548cADSI\uff0c\u7528\u4e8e\u6c7d\u8f66\u56fe\u50cf\u4e2d\u9886\u57df\u6cdb\u5316\u7684\u8bed\u4e49\u5206\u5272\u3002", "motivation": "\u89e3\u51b3\u9886\u57df\u6cdb\u5316\u8bed\u4e49\u5206\u5272\u4e2d\u9886\u57df\u7279\u5b9a\u4fe1\u606f\u7684\u6291\u5236\u95ee\u9898\uff0c\u4ee5\u63d0\u53d6\u66f4\u72ec\u7acb\u7684\u7279\u5f81\u3002", "method": "DI\u8861\u91cf\u9886\u57df\u7279\u5b9a\u4fe1\u606f\u7684\u5b58\u5728\uff0cADSI\u4f7f\u7528Butterworth\u6ee4\u6ce2\u5668\u53bb\u9664\u4f4e\u9891\u7387\u7684\u9886\u57df\u7279\u5b9a\u4fe1\u606f\u3002", "result": "\u5728GTA5\u548cCityscapes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u5728\u591c\u95f4\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684DI\u548cADSI\u80fd\u6709\u6548\u6291\u5236\u9886\u57df\u7279\u5b9a\u4fe1\u606f\uff0c\u63d0\u5347\u6a21\u578b\u5728\u9886\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2504.06575", "pdf": "https://arxiv.org/pdf/2504.06575", "abs": "https://arxiv.org/abs/2504.06575", "authors": ["Li An", "Yujian Liu", "Yepeng Liu", "Yang Zhang", "Yuheng Bu", "Shiyu Chang"], "title": "Defending LLM Watermarking Against Spoofing Attacks with Contrastive Representation Learning", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Watermarking has emerged as a promising technique for detecting texts\ngenerated by LLMs. Current research has primarily focused on three design\ncriteria: high quality of the watermarked text, high detectability, and\nrobustness against removal attack. However, the security against spoofing\nattacks remains relatively understudied. For example, a piggyback attack can\nmaliciously alter the meaning of watermarked text-transforming it into hate\nspeech-while preserving the original watermark, thereby damaging the reputation\nof the LLM provider. We identify two core challenges that make defending\nagainst spoofing difficult: (1) the need for watermarks to be both sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving edits, and\n(2) the contradiction between the need to detect global semantic shifts and the\nlocal, auto-regressive nature of most watermarking schemes. To address these\nchallenges, we propose a semantic-aware watermarking algorithm that post-hoc\nembeds watermarks into a given target text while preserving its original\nmeaning. Our method introduces a semantic mapping model, which guides the\ngeneration of a green-red token list, contrastively trained to be sensitive to\nsemantic-distorting changes and insensitive to semantic-preserving changes.\nExperiments on two standard benchmarks demonstrate strong robustness against\nremoval attacks and security against spoofing attacks, including sentiment\nreversal and toxic content insertion, while maintaining high watermark\ndetectability. Our approach offers a significant step toward more secure and\nsemantically aware watermarking for LLMs. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/contrastive-watermark.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u7684\u6c34\u5370\u7b97\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u4fdd\u62a4\u7531LLM\u751f\u6210\u7684\u6587\u672c\u3002", "motivation": "\u5f53\u524d\u6c34\u5370\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u8d28\u91cf\u3001\u53ef\u68c0\u6d4b\u6027\u548c\u6297\u79fb\u9664\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u5bf9\u9632\u4f2a\u9020\u653b\u51fb\u7684\u5b89\u5168\u6027\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540e\u5904\u7406\u5d4c\u5165\u6c34\u5370\u7684\u7b97\u6cd5\uff0c\u7ed3\u5408\u8bed\u4e49\u6620\u5c04\u6a21\u578b\u751f\u6210\u7ea2\u7eff\u4ee4\u724c\u5217\u8868\uff0c\u901a\u8fc7\u5bf9\u6bd4\u8bad\u7ec3\u4f7f\u5176\u5bf9\u8bed\u4e49\u7834\u574f\u6027\u4fee\u6539\u654f\u611f\uff0c\u5bf9\u8bed\u4e49\u4fdd\u7559\u6027\u4fee\u6539\u4e0d\u654f\u611f\u3002", "result": "\u5728\u4e24\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5bf9\u79fb\u9664\u653b\u51fb\u7684\u5f3a\u9c81\u68d2\u6027\u548c\u5bf9\u4f2a\u9020\u653b\u51fb\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6c34\u5370\u68c0\u6d4b\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLM\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u4e14\u8bed\u4e49\u611f\u77e5\u7684\u6c34\u5370\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9632\u4f2a\u9020\u653b\u51fb\u7684\u80fd\u529b\u3002"}}
{"id": "2504.06785", "pdf": "https://arxiv.org/pdf/2504.06785", "abs": "https://arxiv.org/abs/2504.06785", "authors": ["Shuoshuo Xu", "Kai Zhao", "James Loney", "Zili Li", "Andrea Visentin"], "title": "Zero-Shot Image-Based Large Language Model Approach to Road Pavement Monitoring", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Effective and rapid evaluation of pavement surface condition is critical for\nprioritizing maintenance, ensuring transportation safety, and minimizing\nvehicle wear and tear. While conventional manual inspections suffer from\nsubjectivity, existing machine learning-based methods are constrained by their\nreliance on large and high-quality labeled datasets, which require significant\nresources and limit adaptability across varied road conditions. The\nrevolutionary advancements in Large Language Models (LLMs) present significant\npotential for overcoming these challenges. In this study, we propose an\ninnovative automated zero-shot learning approach that leverages the image\nrecognition and natural language understanding capabilities of LLMs to assess\nroad conditions effectively. Multiple LLM-based assessment models were\ndeveloped, employing prompt engineering strategies aligned with the Pavement\nSurface Condition Index (PSCI) standards. These models' accuracy and\nreliability were evaluated against official PSCI results, with an optimized\nmodel ultimately selected. Extensive tests benchmarked the optimized model\nagainst evaluations from various levels experts using Google Street View road\nimages. The results reveal that the LLM-based approach can effectively assess\nroad conditions, with the optimized model -employing comprehensive and\nstructured prompt engineering strategies -outperforming simpler configurations\nby achieving high accuracy and consistency, even surpassing expert evaluations.\nMoreover, successfully applying the optimized model to Google Street View\nimages demonstrates its potential for future city-scale deployments. These\nfindings highlight the transformative potential of LLMs in automating road\ndamage evaluations and underscore the pivotal role of detailed prompt\nengineering in achieving reliable assessments.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u8def\u9762\u72b6\u51b5\u3002", "motivation": "\u4f20\u7edf\u624b\u52a8\u68c0\u6d4b\u5b58\u5728\u4e3b\u89c2\u6027\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u9002\u5e94\u6027\u6709\u9650\uff0cLLMs\u7684\u8fdb\u6b65\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "method": "\u5f00\u53d1\u591a\u79cd\u57fa\u4e8eLLMs\u7684\u8bc4\u4f30\u6a21\u578b\uff0c\u91c7\u7528\u4e0ePSCI\u6807\u51c6\u5bf9\u9f50\u7684\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u9009\u62e9\u6700\u4f73\u6a21\u578b\u3002", "result": "\u4f18\u5316\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u4e13\u5bb6\u8bc4\u4f30\uff0c\u4e14\u5728Google\u8857\u666f\u56fe\u50cf\u4e2d\u6210\u529f\u5e94\u7528\u3002", "conclusion": "LLMs\u5728\u81ea\u52a8\u5316\u8def\u9762\u635f\u4f24\u8bc4\u4f30\u4e2d\u5177\u6709\u53d8\u9769\u6f5c\u529b\uff0c\u8be6\u7ec6\u63d0\u793a\u5de5\u7a0b\u662f\u5b9e\u73b0\u53ef\u9760\u8bc4\u4f30\u7684\u5173\u952e\u3002"}}
{"id": "2504.06611", "pdf": "https://arxiv.org/pdf/2504.06611", "abs": "https://arxiv.org/abs/2504.06611", "authors": ["Chrisantha Fernando", "Dylan Banarse", "Simon Osindero"], "title": "Wanting to be Understood", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper explores an intrinsic motivation for mutual awareness,\nhypothesizing that humans possess a fundamental drive to understand \\textit{and\nto be understood} even in the absence of extrinsic rewards. Through simulations\nof the perceptual crossing paradigm, we explore the effect of various internal\nreward functions in reinforcement learning agents. The drive to understand is\nimplemented as an active inference type artificial curiosity reward, whereas\nthe drive to be understood is implemented through intrinsic rewards for\nimitation, influence/impressionability, and sub-reaction time anticipation of\nthe other. Results indicate that while artificial curiosity alone does not lead\nto a preference for social interaction, rewards emphasizing reciprocal\nunderstanding successfully drive agents to prioritize interaction. We\ndemonstrate that this intrinsic motivation can facilitate cooperation in tasks\nwhere only one agent receives extrinsic reward for the behaviour of the other.", "AI": {"task": "\u63a2\u7d22\u5185\u5728\u52a8\u673a\u5bf9\u76f8\u4e92\u610f\u8bc6\u7684\u5f71\u54cd\uff0c\u5047\u8bbe\u4eba\u7c7b\u5728\u7f3a\u4e4f\u5916\u5728\u5956\u52b1\u65f6\u4ecd\u5177\u6709\u7406\u89e3\u548c\u88ab\u7406\u89e3\u7684\u57fa\u672c\u9a71\u52a8\u529b\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u5728\u65e0\u5916\u5728\u5956\u52b1\u60c5\u51b5\u4e0b\u5bf9\u7406\u89e3\u548c\u88ab\u7406\u89e3\u7684\u5185\u5728\u9700\u6c42\uff0c\u53ca\u5176\u5bf9\u793e\u4ea4\u4e92\u52a8\u548c\u5408\u4f5c\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u611f\u77e5\u4ea4\u53c9\u8303\u5f0f\uff0c\u7814\u7a76\u4e0d\u540c\u5185\u90e8\u5956\u52b1\u51fd\u6570\u5bf9\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u4e3b\u52a8\u63a8\u7406\u578b\u4eba\u5de5\u597d\u5947\u5fc3\u5956\u52b1\u548c\u6a21\u4eff\u3001\u5f71\u54cd\u529b/\u6613\u611f\u6027\u7b49\u5185\u5728\u5956\u52b1\u3002", "result": "\u4eba\u5de5\u597d\u5947\u5fc3\u5355\u72ec\u4e0d\u8db3\u4ee5\u9a71\u52a8\u793e\u4ea4\u4e92\u52a8\u504f\u597d\uff0c\u4f46\u5f3a\u8c03\u76f8\u4e92\u7406\u89e3\u7684\u5956\u52b1\u80fd\u4fc3\u4f7f\u4ee3\u7406\u4f18\u5148\u4e92\u52a8\uff0c\u5e76\u5728\u5355\u65b9\u83b7\u5f97\u5916\u5728\u5956\u52b1\u7684\u4efb\u52a1\u4e2d\u4fc3\u8fdb\u5408\u4f5c\u3002", "conclusion": "\u5185\u5728\u52a8\u673a\uff08\u7406\u89e3\u548c\u88ab\u7406\u89e3\uff09\u80fd\u6709\u6548\u4fc3\u8fdb\u793e\u4ea4\u4e92\u52a8\u548c\u5408\u4f5c\uff0c\u5c24\u5176\u5728\u7f3a\u4e4f\u5916\u5728\u5956\u52b1\u7684\u73af\u5883\u4e2d\u3002"}}
{"id": "2504.06800", "pdf": "https://arxiv.org/pdf/2504.06800", "abs": "https://arxiv.org/abs/2504.06800", "authors": ["Danielle Cohen", "Hila Chefer", "Lior Wolf"], "title": "A Meaningful Perturbation Metric for Evaluating Explainability Methods", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) have demonstrated remarkable success, yet their\nwide adoption is often hindered by their opaque decision-making. To address\nthis, attribution methods have been proposed to assign relevance values to each\npart of the input. However, different methods often produce entirely different\nrelevance maps, necessitating the development of standardized metrics to\nevaluate them. Typically, such evaluation is performed through perturbation,\nwherein high- or low-relevance regions of the input image are manipulated to\nexamine the change in prediction. In this work, we introduce a novel approach,\nwhich harnesses image generation models to perform targeted perturbation.\nSpecifically, we focus on inpainting only the high-relevance pixels of an input\nimage to modify the model's predictions while preserving image fidelity. This\nis in contrast to existing approaches, which often produce out-of-distribution\nmodifications, leading to unreliable results. Through extensive experiments, we\ndemonstrate the effectiveness of our approach in generating meaningful rankings\nacross a wide range of models and attribution methods. Crucially, we establish\nthat the ranking produced by our metric exhibits significantly higher\ncorrelation with human preferences compared to existing approaches,\nunderscoring its potential for enhancing interpretability in DNNs.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u56fe\u50cf\u751f\u6210\u6a21\u578b\u8fdb\u884c\u9488\u5bf9\u6027\u6270\u52a8\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u8bc4\u4f30\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u7684\u5f52\u56e0\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5f52\u56e0\u65b9\u6cd5\u751f\u6210\u7684\u663e\u8457\u6027\u56fe\u5dee\u5f02\u5927\uff0c\u4e14\u4f20\u7edf\u6270\u52a8\u65b9\u6cd5\u5e38\u5bfc\u81f4\u5206\u5e03\u5916\u4fee\u6539\uff0c\u7ed3\u679c\u4e0d\u53ef\u9760\u3002", "method": "\u901a\u8fc7\u56fe\u50cf\u4fee\u590d\u6280\u672f\u4ec5\u6270\u52a8\u9ad8\u663e\u8457\u6027\u50cf\u7d20\uff0c\u4ee5\u6539\u53d8\u6a21\u578b\u9884\u6d4b\u5e76\u4fdd\u6301\u56fe\u50cf\u4fdd\u771f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u6709\u610f\u4e49\u7684\u5f52\u56e0\u65b9\u6cd5\u6392\u540d\uff0c\u4e14\u4e0e\u4eba\u7c7b\u504f\u597d\u76f8\u5173\u6027\u663e\u8457\u9ad8\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u63d0\u5347DNNs\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.06658", "pdf": "https://arxiv.org/pdf/2504.06658", "abs": "https://arxiv.org/abs/2504.06658", "authors": ["Xiaohua Feng", "Yuyuan Li", "Chengye Wang", "Junlin Liu", "Li Zhang", "Chaochao Chen"], "title": "A Neuro-inspired Interpretation of Unlearning in Large Language Models through Sample-level Unlearning Difficulty", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "16 pages", "summary": "Driven by privacy protection laws and regulations, unlearning in Large\nLanguage Models (LLMs) is gaining increasing attention. However, current\nresearch often neglects the interpretability of the unlearning process,\nparticularly concerning sample-level unlearning difficulty. Existing studies\ntypically assume a uniform unlearning difficulty across samples. This\nsimplification risks attributing the performance of unlearning algorithms to\nsample selection rather than the algorithm's design, potentially steering the\ndevelopment of LLM unlearning in the wrong direction. Thus, we investigate the\nrelationship between LLM unlearning and sample characteristics, with a focus on\nunlearning difficulty. Drawing inspiration from neuroscience, we propose a\nMemory Removal Difficulty ($\\mathrm{MRD}$) metric to quantify sample-level\nunlearning difficulty. Using $\\mathrm{MRD}$, we analyze the characteristics of\nhard-to-unlearn versus easy-to-unlearn samples. Furthermore, we propose an\n$\\mathrm{MRD}$-based weighted sampling method to optimize existing unlearning\nalgorithms, which prioritizes easily forgettable samples, thereby improving\nunlearning efficiency and effectiveness. We validate the proposed metric and\nmethod using public benchmarks and datasets, with results confirming its\neffectiveness.", "AI": {"task": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u9057\u5fd8\u5b66\u4e60\u4e0e\u6837\u672c\u7279\u5f81\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u79fb\u9664\u96be\u5ea6\uff08MRD\uff09\u7684\u52a0\u6743\u91c7\u6837\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u5ffd\u89c6\u4e86\u89e3\u91ca\u6027\uff0c\u5c24\u5176\u662f\u6837\u672c\u7ea7\u9057\u5fd8\u96be\u5ea6\uff0c\u53ef\u80fd\u5bfc\u81f4\u7b97\u6cd5\u6027\u80fd\u88ab\u9519\u8bef\u5f52\u56e0\u4e8e\u6837\u672c\u9009\u62e9\u800c\u975e\u7b97\u6cd5\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faMRD\u6307\u6807\u91cf\u5316\u6837\u672c\u7ea7\u9057\u5fd8\u96be\u5ea6\uff0c\u5e76\u57fa\u4e8eMRD\u8bbe\u8ba1\u52a0\u6743\u91c7\u6837\u65b9\u6cd5\u4f18\u5316\u73b0\u6709\u9057\u5fd8\u7b97\u6cd5\u3002", "result": "\u901a\u8fc7\u516c\u5f00\u57fa\u51c6\u548c\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86MRD\u6307\u6807\u548c\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MRD\u6307\u6807\u548c\u52a0\u6743\u91c7\u6837\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u9057\u5fd8\u5b66\u4e60\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2504.06801", "pdf": "https://arxiv.org/pdf/2504.06801", "abs": "https://arxiv.org/abs/2504.06801", "authors": ["Rishubh Parihar", "Srinjay Sarkar", "Sarthak Vora", "Jogendra Kundu", "R. Venkatesh Babu"], "title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection", "categories": ["cs.CV"], "comment": "https://rishubhpar.github.io/monoplace3D", "summary": "Current monocular 3D detectors are held back by the limited diversity and\nscale of real-world datasets. While data augmentation certainly helps, it's\nparticularly difficult to generate realistic scene-aware augmented data for\noutdoor settings. Most current approaches to synthetic data generation focus on\nrealistic object appearance through improved rendering techniques. However, we\nshow that where and how objects are positioned is just as crucial for training\neffective 3D monocular detectors. The key obstacle lies in automatically\ndetermining realistic object placement parameters - including position,\ndimensions, and directional alignment when introducing synthetic objects into\nactual scenes. To address this, we introduce MonoPlace3D, a novel system that\nconsiders the 3D scene content to create realistic augmentations. Specifically,\ngiven a background scene, MonoPlace3D learns a distribution over plausible 3D\nbounding boxes. Subsequently, we render realistic objects and place them\naccording to the locations sampled from the learned distribution. Our\ncomprehensive evaluation on two standard datasets KITTI and NuScenes,\ndemonstrates that MonoPlace3D significantly improves the accuracy of multiple\nexisting monocular 3D detectors while being highly data efficient.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aMonoPlace3D\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u771f\u5b9e\u573a\u666f\u611f\u77e5\u7684\u5408\u6210\u6570\u636e\u4ee5\u589e\u5f3a\u5355\u76ee3D\u68c0\u6d4b\u5668\u7684\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u5355\u76ee3D\u68c0\u6d4b\u5668\u53d7\u9650\u4e8e\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u548c\u89c4\u6a21\uff0c\u800c\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u771f\u5b9e\u573a\u666f\u611f\u77e5\u7684\u6237\u5916\u6570\u636e\u3002", "method": "MonoPlace3D\u901a\u8fc7\u5b66\u4e60\u573a\u666f\u5185\u5bb9\u751f\u6210\u5408\u7406\u76843D\u8fb9\u754c\u6846\u5206\u5e03\uff0c\u5e76\u5728\u5b66\u4e60\u5230\u7684\u5206\u5e03\u4e2d\u91c7\u6837\u4f4d\u7f6e\u4ee5\u653e\u7f6e\u5408\u6210\u5bf9\u8c61\u3002", "result": "\u5728KITTI\u548cNuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMonoPlace3D\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u5355\u76ee3D\u68c0\u6d4b\u5668\u7684\u51c6\u786e\u6027\uff0c\u4e14\u6570\u636e\u6548\u7387\u9ad8\u3002", "conclusion": "MonoPlace3D\u901a\u8fc7\u751f\u6210\u771f\u5b9e\u573a\u666f\u611f\u77e5\u7684\u5408\u6210\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u76ee3D\u68c0\u6d4b\u5668\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002"}}
{"id": "2504.06659", "pdf": "https://arxiv.org/pdf/2504.06659", "abs": "https://arxiv.org/abs/2504.06659", "authors": ["Xiaohua Feng", "Yuyuan Li", "Huwei Ji", "Jiaming Zhang", "Li Zhang", "Tianyu Du", "Chaochao Chen"], "title": "Bridging the Gap Between Preference Alignment and Machine Unlearning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages", "summary": "Despite advances in Preference Alignment (PA) for Large Language Models\n(LLMs), mainstream methods like Reinforcement Learning with Human Feedback\n(RLHF) face notable challenges. These approaches require high-quality datasets\nof positive preference examples, which are costly to obtain and computationally\nintensive due to training instability, limiting their use in low-resource\nscenarios. LLM unlearning technique presents a promising alternative, by\ndirectly removing the influence of negative examples. However, current research\nhas primarily focused on empirical validation, lacking systematic quantitative\nanalysis. To bridge this gap, we propose a framework to explore the\nrelationship between PA and LLM unlearning. Specifically, we introduce a\nbi-level optimization-based method to quantify the impact of unlearning\nspecific negative examples on PA performance. Our analysis reveals that not all\nnegative examples contribute equally to alignment improvement when unlearned,\nand the effect varies significantly across examples. Building on this insight,\nwe pose a crucial question: how can we optimally select and weight negative\nexamples for unlearning to maximize PA performance? To answer this, we propose\na framework called Unlearning to Align (U2A), which leverages bi-level\noptimization to efficiently select and unlearn examples for optimal PA\nperformance. We validate the proposed method through extensive experiments,\nwith results confirming its effectiveness.", "AI": {"task": "\u63a2\u7d22\u504f\u597d\u5bf9\u9f50\uff08PA\uff09\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9057\u5fd8\u6280\u672f\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53cc\u5c42\u4f18\u5316\u7684\u65b9\u6cd5\uff08U2A\uff09\u6765\u4f18\u5316\u8d1f\u4f8b\u9009\u62e9\u4ee5\u63d0\u9ad8PA\u6027\u80fd\u3002", "motivation": "\u4e3b\u6d41\u65b9\u6cd5\u5982RLHF\u9700\u8981\u9ad8\u8d28\u91cf\u7684\u6b63\u504f\u597d\u6570\u636e\u96c6\uff0c\u6210\u672c\u9ad8\u4e14\u8ba1\u7b97\u5bc6\u96c6\uff0c\u800cLLM\u9057\u5fd8\u6280\u672f\u867d\u80fd\u76f4\u63a5\u79fb\u9664\u8d1f\u4f8b\u5f71\u54cd\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5b9a\u91cf\u5206\u6790\u3002", "method": "\u63d0\u51fa\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u91cf\u5316\u9057\u5fd8\u7279\u5b9a\u8d1f\u4f8b\u5bf9PA\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u8bbe\u8ba1U2A\u65b9\u6cd5\u4f18\u5316\u8d1f\u4f8b\u9009\u62e9\u548c\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cU2A\u80fd\u6709\u6548\u9009\u62e9\u5e76\u9057\u5fd8\u8d1f\u4f8b\uff0c\u663e\u8457\u63d0\u5347PA\u6027\u80fd\u3002", "conclusion": "U2A\u4e3a\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u504f\u597d\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.06803", "pdf": "https://arxiv.org/pdf/2504.06803", "abs": "https://arxiv.org/abs/2504.06803", "authors": ["Wangbo Zhao", "Yizeng Han", "Jiasheng Tang", "Kai Wang", "Hao Luo", "Yibing Song", "Gao Huang", "Fan Wang", "Yang You"], "title": "DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation", "categories": ["cs.CV"], "comment": "Extended journal version for ICLR. arXiv admin note: substantial text\n  overlap with arXiv:2410.03456", "summary": "Diffusion Transformer (DiT), an emerging diffusion model for visual\ngeneration, has demonstrated superior performance but suffers from substantial\ncomputational costs. Our investigations reveal that these costs primarily stem\nfrom the \\emph{static} inference paradigm, which inevitably introduces\nredundant computation in certain \\emph{diffusion timesteps} and \\emph{spatial\nregions}. To overcome this inefficiency, we propose \\textbf{Dy}namic\n\\textbf{Di}ffusion \\textbf{T}ransformer (DyDiT), an architecture that\n\\emph{dynamically} adjusts its computation along both \\emph{timestep} and\n\\emph{spatial} dimensions. Specifically, we introduce a \\emph{Timestep-wise\nDynamic Width} (TDW) approach that adapts model width conditioned on the\ngeneration timesteps. In addition, we design a \\emph{Spatial-wise Dynamic\nToken} (SDT) strategy to avoid redundant computation at unnecessary spatial\nlocations. TDW and SDT can be seamlessly integrated into DiT and significantly\naccelerates the generation process. Building on these designs, we further\nenhance DyDiT in three key aspects. First, DyDiT is integrated seamlessly with\nflow matching-based generation, enhancing its versatility. Furthermore, we\nenhance DyDiT to tackle more complex visual generation tasks, including video\ngeneration and text-to-image generation, thereby broadening its real-world\napplications. Finally, to address the high cost of full fine-tuning and\ndemocratize technology access, we investigate the feasibility of training DyDiT\nin a parameter-efficient manner and introduce timestep-based dynamic LoRA\n(TD-LoRA). Extensive experiments on diverse visual generation models, including\nDiT, SiT, Latte, and FLUX, demonstrate the effectiveness of DyDiT.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u91cf\u7684\u6269\u6563\u53d8\u6362\u5668\uff08DyDiT\uff09\uff0c\u4ee5\u51cf\u5c11Diffusion Transformer\uff08DiT\uff09\u5728\u89c6\u89c9\u751f\u6210\u4e2d\u7684\u5197\u4f59\u8ba1\u7b97\u3002", "motivation": "DiT\u5728\u89c6\u89c9\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u9759\u6001\u63a8\u7406\u8303\u5f0f\u5728\u6269\u6563\u65f6\u95f4\u6b65\u548c\u7a7a\u95f4\u533a\u57df\u5f15\u5165\u7684\u5197\u4f59\u8ba1\u7b97\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6269\u6563\u53d8\u6362\u5668\uff08DyDiT\uff09\uff0c\u5305\u62ec\u65f6\u95f4\u6b65\u52a8\u6001\u5bbd\u5ea6\uff08TDW\uff09\u548c\u7a7a\u95f4\u52a8\u6001\u4ee4\u724c\uff08SDT\uff09\u7b56\u7565\uff0c\u5e76\u96c6\u6210\u6d41\u5339\u914d\u751f\u6210\u548c\u53c2\u6570\u9ad8\u6548\u8bad\u7ec3\u65b9\u6cd5\uff08TD-LoRA\uff09\u3002", "result": "DyDiT\u663e\u8457\u52a0\u901f\u751f\u6210\u8fc7\u7a0b\uff0c\u5e76\u5728\u89c6\u9891\u751f\u6210\u548c\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7b49\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DyDiT\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u91cf\uff0c\u6709\u6548\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u6269\u5c55\u4e86DiT\u7684\u5e94\u7528\u8303\u56f4\uff0c\u540c\u65f6\u652f\u6301\u53c2\u6570\u9ad8\u6548\u8bad\u7ec3\u3002"}}
{"id": "2504.06704", "pdf": "https://arxiv.org/pdf/2504.06704", "abs": "https://arxiv.org/abs/2504.06704", "authors": ["Yoshihiro Yamada"], "title": "CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Transformers have driven remarkable breakthroughs in natural language\nprocessing and computer vision, yet their standard attention mechanism still\nimposes O(N^2) complexity, hindering scalability to longer sequences. We\nintroduce Circular-convolutional ATtention (CAT), a Fourier-based approach that\nefficiently applies circular convolutions to reduce complexity without\nsacrificing representational power. CAT achieves O(NlogN) computations,\nrequires fewer learnable parameters by streamlining fully-connected layers, and\nintroduces no heavier operations, resulting in consistent accuracy improvements\nand about a 10% speedup in naive PyTorch implementations on large-scale\nbenchmarks such as ImageNet-1k and WikiText-103. Grounded in an\nengineering-isomorphism framework, CAT's design not only offers practical\nefficiency and ease of implementation but also provides insights to guide the\ndevelopment of next-generation, high-performance Transformer architectures.\nFinally, our ablation studies highlight the key conditions underlying CAT's\nsuccess, shedding light on broader principles for scalable attention\nmechanisms.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u5faa\u73af\u5377\u79ef\u6ce8\u610f\u529b\u673a\u5236\uff08CAT\uff09\uff0c\u4ee5\u964d\u4f4eTransformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u590d\u6742\u5ea6\u3002", "motivation": "\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u7684O(N^2)\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5176\u5728\u957f\u5e8f\u5217\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u5faa\u73af\u5377\u79ef\u65b9\u6cd5\uff0c\u5c06\u590d\u6742\u5ea6\u964d\u81f3O(NlogN)\uff0c\u5e76\u51cf\u5c11\u53ef\u5b66\u4e60\u53c2\u6570\u3002", "result": "\u5728ImageNet-1k\u548cWikiText-103\u7b49\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u73b0\u4e86\u7ea610%\u7684\u901f\u5ea6\u63d0\u5347\u548c\u4e00\u81f4\u7684\u7cbe\u5ea6\u6539\u8fdb\u3002", "conclusion": "CAT\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6548\u7387\u548c\u6613\u5b9e\u73b0\u6027\uff0c\u8fd8\u4e3a\u4e0b\u4e00\u4ee3\u9ad8\u6027\u80fdTransformer\u67b6\u6784\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2504.06811", "pdf": "https://arxiv.org/pdf/2504.06811", "abs": "https://arxiv.org/abs/2504.06811", "authors": ["Abhinav Roy", "Bhavesh Gyanchandani", "Aditya Oza"], "title": "Hybrid CNN with Chebyshev Polynomial Expansion for Medical Image Analysis", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Lung cancer remains one of the leading causes of cancer-related mortality\nworldwide, with early and accurate diagnosis playing a pivotal role in\nimproving patient outcomes. Automated detection of pulmonary nodules in\ncomputed tomography (CT) scans is a challenging task due to variability in\nnodule size, shape, texture, and location. Traditional Convolutional Neural\nNetworks (CNNs) have shown considerable promise in medical image analysis;\nhowever, their limited ability to capture fine-grained spatial-spectral\nvariations restricts their performance in complex diagnostic scenarios. In this\nstudy, we propose a novel hybrid deep learning architecture that incorporates\nChebyshev polynomial expansions into CNN layers to enhance expressive power and\nimprove the representation of underlying anatomical structures. The proposed\nChebyshev-CNN leverages the orthogonality and recursive properties of Chebyshev\npolynomials to extract high-frequency features and approximate complex\nnonlinear functions with greater fidelity. The model is trained and evaluated\non benchmark lung cancer imaging datasets, including LUNA16 and LIDC-IDRI,\nachieving superior performance in classifying pulmonary nodules as benign or\nmalignant. Quantitative results demonstrate significant improvements in\naccuracy, sensitivity, and specificity compared to traditional CNN-based\napproaches. This integration of polynomial-based spectral approximation within\ndeep learning provides a robust framework for enhancing automated medical\ndiagnostics and holds potential for broader applications in clinical decision\nsupport systems.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u6269\u5c55\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u63d0\u9ad8\u80ba\u90e8\u7ed3\u8282\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u65e9\u671f\u548c\u51c6\u786e\u7684\u80ba\u764c\u8bca\u65ad\u5bf9\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u800c\u4f20\u7edfCNN\u5728\u6355\u6349\u590d\u6742\u7a7a\u95f4-\u5149\u8c31\u53d8\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cdChebyshev-CNN\u67b6\u6784\uff0c\u5229\u7528\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u7684\u6b63\u4ea4\u6027\u548c\u9012\u5f52\u7279\u6027\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u5728LUNA16\u548cLIDC-IDRI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfCNN\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3001\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u533b\u5b66\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u5e76\u5177\u6709\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.06766", "pdf": "https://arxiv.org/pdf/2504.06766", "abs": "https://arxiv.org/abs/2504.06766", "authors": ["Yuxin Wang", "Yiran Guo", "Yining Zheng", "Zhangyue Yin", "Shuo Chen", "Jie Yang", "Jiajun Chen", "Xuanjing Huang", "Xipeng Qiu"], "title": "FamilyTool: A Multi-hop Personalized Tool Use Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The integration of tool learning with Large Language Models (LLMs) has\nexpanded their capabilities in handling complex tasks by leveraging external\ntools. However, existing benchmarks for tool learning inadequately address\ncritical real-world personalized scenarios, particularly those requiring\nmulti-hop reasoning and inductive knowledge adaptation in dynamic environments.\nTo bridge this gap, we introduce FamilyTool, a novel benchmark grounded in a\nfamily-based knowledge graph (KG) that simulates personalized, multi-hop tool\nuse scenarios. FamilyTool challenges LLMs with queries spanning 1 to 3\nrelational hops (e.g., inferring familial connections and preferences) and\nincorporates an inductive KG setting where models must adapt to unseen user\npreferences and relationships without re-training, a common limitation in prior\napproaches that compromises generalization. We further propose KGETool: a\nsimple KG-augmented evaluation pipeline to systematically assess LLMs' tool use\nability in these settings. Experiments reveal significant performance gaps in\nstate-of-the-art LLMs, with accuracy dropping sharply as hop complexity\nincreases and inductive scenarios exposing severe generalization deficits.\nThese findings underscore the limitations of current LLMs in handling\npersonalized, evolving real-world contexts and highlight the urgent need for\nadvancements in tool-learning frameworks. FamilyTool serves as a critical\nresource for evaluating and advancing LLM agents' reasoning, adaptability, and\nscalability in complex, dynamic environments. Code and dataset are available at\nGithub.", "AI": {"task": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e2a\u6027\u5316\u3001\u591a\u8df3\u63a8\u7406\u548c\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5de5\u5177\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u5b66\u4e60\u57fa\u51c6\u672a\u80fd\u5145\u5206\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u4e2a\u6027\u5316\u573a\u666f\uff0c\u7279\u522b\u662f\u9700\u8981\u591a\u8df3\u63a8\u7406\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5f52\u7eb3\u77e5\u8bc6\u9002\u5e94\u7684\u573a\u666f\u3002", "method": "\u5f15\u5165FamilyTool\u57fa\u51c6\uff0c\u57fa\u4e8e\u5bb6\u5ead\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u6a21\u62df\u4e2a\u6027\u5316\u591a\u8df3\u5de5\u5177\u4f7f\u7528\u573a\u666f\uff0c\u5e76\u63d0\u51faKGETool\u8bc4\u4f30\u7ba1\u9053\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524dLLMs\u5728\u590d\u6742\u8df3\u6570\u548c\u5f52\u7eb3\u573a\u666f\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u66b4\u9732\u4e86\u6cdb\u5316\u80fd\u529b\u7684\u4e0d\u8db3\u3002", "conclusion": "FamilyTool\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347LLMs\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u63a8\u7406\u3001\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u63d0\u4f9b\u4e86\u5173\u952e\u8d44\u6e90\u3002"}}
{"id": "2504.06815", "pdf": "https://arxiv.org/pdf/2504.06815", "abs": "https://arxiv.org/abs/2504.06815", "authors": ["Hanxiao Sun", "YuPeng Gao", "Jin Xie", "Jian Yang", "Beibei Wang"], "title": "SVG-IR: Spatially-Varying Gaussian Splatting for Inverse Rendering", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing 3D assets from images, known as inverse rendering (IR),\nremains a challenging task due to its ill-posed nature. 3D Gaussian Splatting\n(3DGS) has demonstrated impressive capabilities for novel view synthesis (NVS)\ntasks. Methods apply it to relighting by separating radiance into BRDF\nparameters and lighting, yet produce inferior relighting quality with artifacts\nand unnatural indirect illumination due to the limited capability of each\nGaussian, which has constant material parameters and normal, alongside the\nabsence of physical constraints for indirect lighting. In this paper, we\npresent a novel framework called Spatially-vayring Gaussian Inverse Rendering\n(SVG-IR), aimed at enhancing both NVS and relighting quality. To this end, we\npropose a new representation-Spatially-varying Gaussian (SVG)-that allows\nper-Gaussian spatially varying parameters. This enhanced representation is\ncomplemented by a SVG splatting scheme akin to vertex/fragment shading in\ntraditional graphics pipelines. Furthermore, we integrate a physically-based\nindirect lighting model, enabling more realistic relighting. The proposed\nSVG-IR framework significantly improves rendering quality, outperforming\nstate-of-the-art NeRF-based methods by 2.5 dB in peak signal-to-noise ratio\n(PSNR) and surpassing existing Gaussian-based techniques by 3.5 dB in\nrelighting tasks, all while maintaining a real-time rendering speed.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aSVG-IR\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u9006\u6e32\u67d3\u4efb\u52a1\u4e2d\u7684\u65b0\u89c6\u89d2\u5408\u6210\u548c\u91cd\u5149\u7167\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u91cd\u5149\u7167\u4efb\u52a1\u4e2d\u56e0\u9ad8\u65af\u6cfc\u6e85\u7684\u5c40\u9650\u6027\uff08\u5982\u6052\u5b9a\u6750\u8d28\u53c2\u6570\u548c\u6cd5\u7ebf\uff09\u4ee5\u53ca\u7f3a\u4e4f\u7269\u7406\u7ea6\u675f\u7684\u95f4\u63a5\u5149\u7167\uff0c\u5bfc\u81f4\u8d28\u91cf\u4e0d\u4f73\u548c\u4eba\u5de5\u75d5\u8ff9\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4\u53d8\u5316\u9ad8\u65af\uff08SVG\uff09\u8868\u793a\u548cSVG\u6cfc\u6e85\u65b9\u6848\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u7269\u7406\u7684\u95f4\u63a5\u5149\u7167\u6a21\u578b\u3002", "result": "SVG-IR\u5728PSNR\u4e0a\u4f18\u4e8e\u73b0\u6709NeRF\u65b9\u6cd52.5 dB\uff0c\u5728\u91cd\u5149\u7167\u4efb\u52a1\u4e2d\u4f18\u4e8e\u9ad8\u65af\u65b9\u6cd53.5 dB\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\u3002", "conclusion": "SVG-IR\u901a\u8fc7\u589e\u5f3a\u8868\u793a\u548c\u7269\u7406\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9006\u6e32\u67d3\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2504.06949", "pdf": "https://arxiv.org/pdf/2504.06949", "abs": "https://arxiv.org/abs/2504.06949", "authors": ["Zhixuan Lin", "Johan Obando-Ceron", "Xu Owen He", "Aaron Courville"], "title": "Adaptive Computation Pruning for the Forgetting Transformer", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint. Under review", "summary": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate\ninto softmax attention and has shown consistently better or on-par performance\ncompared to the standard RoPE-based Transformer. Notably, many attention heads\nin FoX tend to forget quickly, causing their output at each timestep to rely\nprimarily on the local context. Based on this observation, we propose Adaptive\nComputation Pruning (ACP) for FoX, a method that dynamically prunes\ncomputations involving input-output dependencies that are strongly decayed by\nthe forget gate. This is achieved using a dynamically set pruning threshold\nthat ensures that the pruned attention weights remain negligible. We apply ACP\nto language model pretraining with FoX and show it consistently reduces the\nnumber of FLOPs in softmax attention by around 70% across different model sizes\nand context lengths, resulting in a roughly 10% to 35% improvement in training\nthroughput. Furthermore, longer context lengths yield greater computational\nsavings. All these speed improvements are achieved without any performance\ndegradation. We also perform several analyses to provide deeper insights into\nour method, such as examining the pruning patterns and analyzing the\ndistribution of FLOP savings across different attention heads. Our code is\navailable at https://github.com/zhixuan-lin/arctic-fox.", "AI": {"task": "\u63d0\u51faAdaptive Computation Pruning (ACP)\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u526a\u679dForgetting Transformer (FoX)\u4e2d\u56e0\u9057\u5fd8\u95e8\u800c\u8870\u51cf\u7684\u8f93\u5165-\u8f93\u51fa\u4f9d\u8d56\u8ba1\u7b97\u3002", "motivation": "\u89c2\u5bdf\u5230FoX\u4e2d\u8bb8\u591a\u6ce8\u610f\u529b\u5934\u5feb\u901f\u9057\u5fd8\uff0c\u5bfc\u81f4\u8f93\u51fa\u4e3b\u8981\u4f9d\u8d56\u5c40\u90e8\u4e0a\u4e0b\u6587\uff0c\u4ece\u800c\u63d0\u51fa\u52a8\u6001\u526a\u679d\u4ee5\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u8bbe\u7f6e\u7684\u526a\u679d\u9608\u503c\uff0c\u526a\u679d\u88ab\u9057\u5fd8\u95e8\u5f3a\u70c8\u8870\u51cf\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u786e\u4fdd\u526a\u679d\u540e\u7684\u6743\u91cd\u53ef\u5ffd\u7565\u3002", "result": "\u5728\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\uff0cACP\u5c06softmax\u6ce8\u610f\u529b\u7684FLOPs\u51cf\u5c11\u7ea670%\uff0c\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u534710%\u81f335%\uff0c\u4e14\u4e0d\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "ACP\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u91cf\u5e76\u63d0\u5347\u6548\u7387\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u957f\u4e0a\u4e0b\u6587\u573a\u666f\uff0c\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.06827", "pdf": "https://arxiv.org/pdf/2504.06827", "abs": "https://arxiv.org/abs/2504.06827", "authors": ["Can Zhang", "Gim Hee Lee"], "title": "IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments", "categories": ["cs.CV"], "comment": null, "summary": "This work presents IAAO, a novel framework that builds an explicit 3D model\nfor intelligent agents to gain understanding of articulated objects in their\nenvironment through interaction. Unlike prior methods that rely on\ntask-specific networks and assumptions about movable parts, our IAAO leverages\nlarge foundation models to estimate interactive affordances and part\narticulations in three stages. We first build hierarchical features and label\nfields for each object state using 3D Gaussian Splatting (3DGS) by distilling\nmask features and view-consistent labels from multi-view images. We then\nperform object- and part-level queries on the 3D Gaussian primitives to\nidentify static and articulated elements, estimating global transformations and\nlocal articulation parameters along with affordances. Finally, scenes from\ndifferent states are merged and refined based on the estimated transformations,\nenabling robust affordance-based interaction and manipulation of objects.\nExperimental results demonstrate the effectiveness of our method.", "AI": {"task": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIAAO\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u4e92\u4e3a\u667a\u80fd\u4f53\u6784\u5efa\u660e\u786e\u76843D\u6a21\u578b\u4ee5\u7406\u89e3\u73af\u5883\u4e2d\u7684\u94f0\u63a5\u7269\u4f53\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4efb\u52a1\u7279\u5b9a\u7f51\u7edc\u548c\u5bf9\u53ef\u79fb\u52a8\u90e8\u5206\u7684\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528\u5927\u578b\u57fa\u7840\u6a21\u578b\u5206\u4e09\u4e2a\u9636\u6bb5\u4f30\u8ba1\u4ea4\u4e92\u53ef\u7528\u6027\u548c\u90e8\u4ef6\u94f0\u63a5\uff1a1) \u4f7f\u75283D\u9ad8\u65af\u6cfc\u6e85\u6784\u5efa\u5c42\u6b21\u7279\u5f81\u548c\u6807\u7b7e\u573a\uff1b2) \u901a\u8fc7\u67e5\u8be23D\u9ad8\u65af\u57fa\u5143\u8bc6\u522b\u9759\u6001\u548c\u94f0\u63a5\u5143\u7d20\uff1b3) \u5408\u5e76\u548c\u4f18\u5316\u4e0d\u540c\u72b6\u6001\u7684\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "IAAO\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u57fa\u4e8e\u53ef\u7528\u6027\u7684\u9c81\u68d2\u4ea4\u4e92\u548c\u7269\u4f53\u64cd\u4f5c\u3002"}}
{"id": "2504.06963", "pdf": "https://arxiv.org/pdf/2504.06963", "abs": "https://arxiv.org/abs/2504.06963", "authors": ["Vladimir Bataev"], "title": "RNN-Transducer-based Losses for Speech Recognition on Noisy Targets", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": "Final Project Report, Bachelor's Degree in Computer Science,\n  University of London, March 2024", "summary": "Training speech recognition systems on noisy transcripts is a significant\nchallenge in industrial pipelines, where datasets are enormous and ensuring\naccurate transcription for every instance is difficult. In this work, we\nintroduce novel loss functions to mitigate the impact of transcription errors\nin RNN-Transducer models. Our Star-Transducer loss addresses deletion errors by\nincorporating \"skip frame\" transitions in the loss lattice, restoring over 90%\nof the system's performance compared to models trained with accurate\ntranscripts. The Bypass-Transducer loss uses \"skip token\" transitions to tackle\ninsertion errors, recovering more than 60% of the quality. Finally, the\nTarget-Robust Transducer loss merges these approaches, offering robust\nperformance against arbitrary errors. Experimental results demonstrate that the\nTarget-Robust Transducer loss significantly improves RNN-T performance on noisy\ndata by restoring over 70% of the quality compared to well-transcribed data.", "AI": {"task": "\u63d0\u51fa\u65b0\u7684\u635f\u5931\u51fd\u6570\u4ee5\u51cf\u5c11RNN-Transducer\u6a21\u578b\u4e2d\u8f6c\u5f55\u9519\u8bef\u7684\u5f71\u54cd\u3002", "motivation": "\u5de5\u4e1a\u7ea7\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u4e2d\uff0c\u6570\u636e\u96c6\u5e9e\u5927\u4e14\u96be\u4ee5\u786e\u4fdd\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u8f6c\u5f55\u51c6\u786e\u6027\uff0c\u8bad\u7ec3\u566a\u58f0\u8f6c\u5f55\u6570\u636e\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u5f15\u5165\u4e09\u79cd\u635f\u5931\u51fd\u6570\uff1aStar-Transducer\uff08\u5904\u7406\u5220\u9664\u9519\u8bef\uff09\u3001Bypass-Transducer\uff08\u5904\u7406\u63d2\u5165\u9519\u8bef\uff09\u548cTarget-Robust Transducer\uff08\u7efc\u5408\u5904\u7406\u4efb\u610f\u9519\u8bef\uff09\u3002", "result": "Star-Transducer\u6062\u590d90%\u6027\u80fd\uff0cBypass-Transducer\u6062\u590d60%\u8d28\u91cf\uff0cTarget-Robust Transducer\u6062\u590d70%\u8d28\u91cf\u3002", "conclusion": "Target-Robust Transducer\u663e\u8457\u63d0\u5347\u4e86RNN-T\u5728\u566a\u58f0\u6570\u636e\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2504.06835", "pdf": "https://arxiv.org/pdf/2504.06835", "abs": "https://arxiv.org/abs/2504.06835", "authors": ["Ziyi Wang", "Haoran Wu", "Yiming Rong", "Deyang Jiang", "Yixin Zhang", "Yunlong Zhao", "Shuang Xu", "Bo XU"], "title": "LVC: A Lightweight Compression Framework for Enhancing VLMs in Long Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Long video understanding is a complex task that requires both spatial detail\nand temporal awareness. While Vision-Language Models (VLMs) obtain frame-level\nunderstanding capabilities through multi-frame input, they suffer from\ninformation loss due to the sparse sampling strategy. In contrast, Video Large\nLanguage Models (Video-LLMs) capture temporal relationships within visual\nfeatures but are limited by the scarcity of high-quality video-text datasets.\nTo transfer long video understanding capabilities to VLMs with minimal data and\ncomputational cost, we propose Lightweight Video Compression (LVC), a novel\nmethod featuring the Query-Attention Video Compression mechanism, which\neffectively tackles the sparse sampling problem in VLMs. By training only the\nalignment layer with 10k short video-text pairs, LVC significantly enhances the\ntemporal reasoning abilities of VLMs. Extensive experiments show that LVC\nprovides consistent performance improvements across various models, including\nthe InternVL2 series and Phi-3.5-Vision. Notably, the InternVL2-40B-LVC\nachieves scores of 68.2 and 65.9 on the long video understanding benchmarks\nMLVU and Video-MME, respectively, with relative improvements of 14.6% and 7.7%.\nThe enhanced models and code will be publicly available soon.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u9891\u538b\u7f29\u65b9\u6cd5\uff08LVC\uff09\uff0c\u4ee5\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u7a00\u758f\u91c7\u6837\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u56e0\u7a00\u758f\u91c7\u6837\u7b56\u7565\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\uff0c\u800c\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08Video-LLMs\uff09\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u89c6\u9891-\u6587\u672c\u6570\u636e\u96c6\u7684\u7a00\u7f3a\u6027\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u89c6\u9891\u538b\u7f29\uff08LVC\uff09\u65b9\u6cd5\uff0c\u91c7\u7528\u67e5\u8be2\u6ce8\u610f\u529b\u89c6\u9891\u538b\u7f29\u673a\u5236\uff0c\u4ec5\u9700\u8bad\u7ec3\u5bf9\u9f50\u5c42\u548c\u5c11\u91cf\u6570\u636e\uff0810k\u77ed\u89c6\u9891-\u6587\u672c\u5bf9\uff09\u3002", "result": "LVC\u663e\u8457\u63d0\u5347\u4e86VLMs\u7684\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5728MLVU\u548cVideo-MME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u53d6\u5f9768.2\u548c65.9\u5206\uff0c\u76f8\u5bf9\u63d0\u534714.6%\u548c7.7%\u3002", "conclusion": "LVC\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u65b9\u6cd5\uff0c\u53ef\u663e\u8457\u589e\u5f3aVLMs\u7684\u957f\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u5176\u6a21\u578b\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2504.07046", "pdf": "https://arxiv.org/pdf/2504.07046", "abs": "https://arxiv.org/abs/2504.07046", "authors": ["Jifang Wang", "Xue Yang", "Longyue Wang", "Zhenran Xu", "Yiyu Wang", "Yaowei Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "title": "A Unified Agentic Framework for Evaluating Conditional Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": "Work in progress. GitHub:\n  https://github.com/HITsz-TMG/Agentic-CIGEval", "summary": "Conditional image generation has gained significant attention for its ability\nto personalize content. However, the field faces challenges in developing\ntask-agnostic, reliable, and explainable evaluation metrics. This paper\nintroduces CIGEval, a unified agentic framework for comprehensive evaluation of\nconditional image generation tasks. CIGEval utilizes large multimodal models\n(LMMs) as its core, integrating a multi-functional toolbox and establishing a\nfine-grained evaluation framework. Additionally, we synthesize evaluation\ntrajectories for fine-tuning, empowering smaller LMMs to autonomously select\nappropriate tools and conduct nuanced analyses based on tool outputs.\nExperiments across seven prominent conditional image generation tasks\ndemonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625\nwith human assessments, closely matching the inter-annotator correlation of\n0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K\ntraining trajectories, CIGEval surpasses the previous GPT-4o-based\nstate-of-the-art method. Case studies on GPT-4o image generation highlight\nCIGEval's capability in identifying subtle issues related to subject\nconsistency and adherence to control guidance, indicating its great potential\nfor automating evaluation of image generation tasks with human-level\nreliability.", "AI": {"task": "\u63d0\u51faCIGEval\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u9886\u57df\u7f3a\u4e4f\u4efb\u52a1\u65e0\u5173\u3001\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6307\u6807\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u4f5c\u4e3a\u6838\u5fc3\uff0c\u6574\u5408\u591a\u529f\u80fd\u5de5\u5177\u7bb1\u5e76\u5efa\u7acb\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u540c\u65f6\u5408\u6210\u8bc4\u4f30\u8f68\u8ff9\u4ee5\u5fae\u8c03\u5c0f\u578bLMMs\u3002", "result": "CIGEval\uff08GPT-4o\u7248\u672c\uff09\u5728\u4e03\u4e2a\u4efb\u52a1\u4e2d\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u76f8\u5173\u7cfb\u6570\u8fbe0.4625\uff0c\u63a5\u8fd1\u4eba\u7c7b\u8bc4\u4f30\u8005\u95f4\u7684\u76f8\u5173\u7cfb\u65700.47\uff1b\u4f7f\u75287B\u5f00\u6e90LMMs\u65f6\uff0c\u4ec5\u97002.3K\u8bad\u7ec3\u8f68\u8ff9\u5373\u8d85\u8d8a\u57fa\u4e8eGPT-4o\u7684\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "CIGEval\u5728\u8bc6\u522b\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u7ec6\u5fae\u95ee\u9898\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5176\u5728\u81ea\u52a8\u5316\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4eba\u7c7b\u7ea7\u53ef\u9760\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.06836", "pdf": "https://arxiv.org/pdf/2504.06836", "abs": "https://arxiv.org/abs/2504.06836", "authors": ["Jakub Maciej Wi\u015bniewski", "Anders Nymark Christensen", "Mary Le Ngo", "Martin Gr\u00f8nneb\u00e6k Tolsgaard", "Chun Kit Wong"], "title": "Determining Fetal Orientations From Blind Sweep Ultrasound Video", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Cognitive demands of fetal ultrasound examinations pose unique challenges\namong clinicians. With the goal of providing an assistive tool, we developed an\nautomated pipeline for predicting fetal orientation from ultrasound videos\nacquired following a simple blind sweep protocol. Leveraging on a pre-trained\nhead detection and segmentation model, this is achieved by first determining\nthe fetal presentation (cephalic or breech) with a template matching approach,\nfollowed by the fetal lie (facing left or right) by analyzing the spatial\ndistribution of segmented brain anatomies. Evaluation on a dataset of\nthird-trimester ultrasound scans demonstrated the promising accuracy of our\npipeline. This work distinguishes itself by introducing automated fetal lie\nprediction and by proposing an assistive paradigm that augments sonographer\nexpertise rather than replacing it. Future research will focus on enhancing\nacquisition efficiency, and exploring real-time clinical integration to improve\nworkflow and support for obstetric clinicians.", "AI": {"task": "\u5f00\u53d1\u4e00\u4e2a\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u7528\u4e8e\u4ece\u9075\u5faa\u7b80\u5355\u76f2\u626b\u534f\u8bae\u7684\u8d85\u58f0\u89c6\u9891\u4e2d\u9884\u6d4b\u80ce\u513f\u65b9\u4f4d\u3002", "motivation": "\u80ce\u513f\u8d85\u58f0\u68c0\u67e5\u7684\u8ba4\u77e5\u9700\u6c42\u5bf9\u4e34\u5e8a\u533b\u751f\u63d0\u51fa\u4e86\u72ec\u7279\u6311\u6218\uff0c\u9700\u8981\u8f85\u52a9\u5de5\u5177\u6765\u51cf\u8f7b\u8d1f\u62c5\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5934\u90e8\u68c0\u6d4b\u548c\u5206\u5272\u6a21\u578b\uff0c\u5148\u901a\u8fc7\u6a21\u677f\u5339\u914d\u786e\u5b9a\u80ce\u513f\u5148\u9732\uff08\u5934\u4f4d\u6216\u81c0\u4f4d\uff09\uff0c\u518d\u901a\u8fc7\u5206\u6790\u5206\u5272\u51fa\u7684\u8111\u90e8\u89e3\u5256\u7ed3\u6784\u7684\u7a7a\u95f4\u5206\u5e03\u9884\u6d4b\u80ce\u513f\u671d\u5411\uff08\u5de6\u6216\u53f3\uff09\u3002", "result": "\u5728\u7b2c\u4e09\u5b55\u671f\u8d85\u58f0\u626b\u63cf\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6d41\u7a0b\u5177\u6709\u8f83\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5f15\u5165\u81ea\u52a8\u5316\u80ce\u513f\u671d\u5411\u9884\u6d4b\u548c\u8f85\u52a9\u8303\u5f0f\uff0c\u589e\u5f3a\u4e86\u8d85\u58f0\u533b\u5e08\u7684\u4e13\u4e1a\u80fd\u529b\u800c\u975e\u66ff\u4ee3\u4ed6\u4eec\u3002\u672a\u6765\u7814\u7a76\u5c06\u805a\u7126\u4e8e\u63d0\u9ad8\u91c7\u96c6\u6548\u7387\u548c\u5b9e\u65f6\u4e34\u5e8a\u96c6\u6210\u3002"}}
{"id": "2504.07079", "pdf": "https://arxiv.org/pdf/2504.07079", "abs": "https://arxiv.org/abs/2504.07079", "authors": ["Boyuan Zheng", "Michael Y. Fatemi", "Xiaolong Jin", "Zora Zhiruo Wang", "Apurva Gandhi", "Yueqi Song", "Yu Gu", "Jayanth Srinivasa", "Gaowen Liu", "Graham Neubig", "Yu Su"], "title": "SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "To survive and thrive in complex environments, humans have evolved\nsophisticated self-improvement mechanisms through environment exploration,\nhierarchical abstraction of experiences into reuseable skills, and\ncollaborative construction of an ever-growing skill repertoire. Despite recent\nadvancements, autonomous web agents still lack crucial self-improvement\ncapabilities, struggling with procedural knowledge abstraction, refining\nskills, and skill composition. In this work, we introduce SkillWeaver, a\nskill-centric framework enabling agents to self-improve by autonomously\nsynthesizing reusable skills as APIs. Given a new website, the agent\nautonomously discovers skills, executes them for practice, and distills\npractice experiences into robust APIs. Iterative exploration continually\nexpands a library of lightweight, plug-and-play APIs, significantly enhancing\nthe agent's capabilities. Experiments on WebArena and real-world websites\ndemonstrate the efficacy of SkillWeaver, achieving relative success rate\nimprovements of 31.8% and 39.8%, respectively. Additionally, APIs synthesized\nby strong agents substantially enhance weaker agents through transferable\nskills, yielding improvements of up to 54.3% on WebArena. These results\ndemonstrate the effectiveness of honing diverse website interactions into APIs,\nwhich can be seamlessly shared among various web agents.", "AI": {"task": "\u63d0\u51faSkillWeaver\u6846\u67b6\uff0c\u4f7f\u81ea\u4e3b\u7f51\u7edc\u4ee3\u7406\u80fd\u591f\u901a\u8fc7\u5408\u6210\u53ef\u91cd\u7528\u6280\u80fdAPI\u5b9e\u73b0\u81ea\u6211\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u7f51\u7edc\u4ee3\u7406\u7f3a\u4e4f\u5173\u952e\u81ea\u6211\u63d0\u5347\u80fd\u529b\uff0c\u5982\u7a0b\u5e8f\u6027\u77e5\u8bc6\u62bd\u8c61\u3001\u6280\u80fd\u7cbe\u70bc\u548c\u6280\u80fd\u7ec4\u5408\u3002", "method": "SkillWeaver\u6846\u67b6\u901a\u8fc7\u81ea\u4e3b\u53d1\u73b0\u6280\u80fd\u3001\u6267\u884c\u7ec3\u4e60\u5e76\u5c06\u7ecf\u9a8c\u63d0\u70bc\u4e3a\u8f7b\u91cf\u7ea7API\uff0c\u5b9e\u73b0\u6280\u80fd\u5e93\u7684\u6301\u7eed\u6269\u5c55\u3002", "result": "\u5728WebArena\u548c\u771f\u5b9e\u7f51\u7ad9\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSkillWeaver\u5206\u522b\u63d0\u5347\u4e8631.8%\u548c39.8%\u7684\u6210\u529f\u7387\uff0c\u4e14API\u5171\u4eab\u4f7f\u5f31\u4ee3\u7406\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe54.3%\u3002", "conclusion": "SkillWeaver\u901a\u8fc7\u5c06\u591a\u6837\u7f51\u7ad9\u4ea4\u4e92\u63d0\u70bc\u4e3a\u53ef\u5171\u4eabAPI\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u4ee3\u7406\u7684\u80fd\u529b\u548c\u534f\u4f5c\u6548\u7387\u3002"}}
{"id": "2504.06838", "pdf": "https://arxiv.org/pdf/2504.06838", "abs": "https://arxiv.org/abs/2504.06838", "authors": ["Seonghwan Park", "Jaehyeon Jeong", "Yongjun Kim", "Jaeho Lee", "Namhoon Lee"], "title": "ZIP: An Efficient Zeroth-order Prompt Tuning for Black-box Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": "ICLR 2025", "summary": "Recent studies have introduced various approaches for prompt-tuning black-box\nvision-language models, referred to as black-box prompt-tuning (BBPT). While\nBBPT has demonstrated considerable potential, it is often found that many\nexisting methods require an excessive number of queries (i.e., function\nevaluations), which poses a significant challenge in real-world scenarios where\nthe number of allowed queries is limited. To tackle this issue, we propose\nZeroth-order Intrinsic-dimensional Prompt-tuning (ZIP), a novel approach that\nenables efficient and robust prompt optimization in a purely black-box setting.\nThe key idea of ZIP is to reduce the problem dimensionality and the variance of\nzeroth-order gradient estimates, such that the training is done fast with far\nless queries. We achieve this by re-parameterizing prompts in low-rank\nrepresentations and designing intrinsic-dimensional clipping of estimated\ngradients. We evaluate ZIP on 13+ vision-language tasks in standard benchmarks\nand show that it achieves an average improvement of approximately 6% in\nfew-shot accuracy and 48% in query efficiency compared to the best-performing\nalternative BBPT methods, establishing a new state of the art. Our ablation\nanalysis further shows that the proposed clipping mechanism is robust and\nnearly optimal, without the need to manually select the clipping threshold,\nmatching the result of expensive hyperparameter search.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aZIP\u7684\u9ad8\u6548\u9ed1\u76d2\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u67e5\u8be2\u6b21\u6570\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9ed1\u76d2\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u9700\u8981\u8fc7\u591a\u67e5\u8be2\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u4f4e\u79e9\u8868\u793a\u91cd\u65b0\u53c2\u6570\u5316\u63d0\u793a\uff0c\u5e76\u8bbe\u8ba1\u5185\u5728\u7ef4\u5ea6\u68af\u5ea6\u88c1\u526a\u3002", "result": "\u572813+\u4e2a\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\uff0cZIP\u5e73\u5747\u5c11\u6837\u672c\u51c6\u786e\u7387\u63d0\u53476%\uff0c\u67e5\u8be2\u6548\u7387\u63d0\u534748%\u3002", "conclusion": "ZIP\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u9ed1\u76d2\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u8d85\u53c2\u6570\u3002"}}
{"id": "2504.07086", "pdf": "https://arxiv.org/pdf/2504.07086", "abs": "https://arxiv.org/abs/2504.07086", "authors": ["Andreas Hochlehnert", "Hardik Bhatnagar", "Vishaal Udandarao", "Samuel Albanie", "Ameya Prabhu", "Matthias Bethge"], "title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility", "categories": ["cs.LG", "cs.CL"], "comment": "Technical Report", "summary": "Reasoning has emerged as the next major frontier for language models (LMs),\nwith rapid advances from both academic and industrial labs. However, this\nprogress often outpaces methodological rigor, with many evaluations relying on\nbenchmarking practices that lack transparency, robustness, or statistical\ngrounding. In this work, we conduct a comprehensive empirical study and find\nthat current mathematical reasoning benchmarks are highly sensitive to subtle\nimplementation choices - including decoding parameters, random seeds, prompt\nformatting, and even hardware and software-framework configurations.\nPerformance gains reported in recent studies frequently hinge on unclear\ncomparisons or unreported sources of variance. To address these issues, we\npropose a standardized evaluation framework with clearly defined best practices\nand reporting standards. Using this framework, we reassess recent methods and\nfind that reinforcement learning (RL) approaches yield only modest improvements\n- far below prior claims - and are prone to overfitting, especially on\nsmall-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT)\nmethods show consistently stronger generalization. To foster reproducibility,\nwe release all code, prompts, and model outputs, for reasoning benchmarks,\nestablishing more rigorous foundations for future work.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5f53\u524d\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u7a33\u5065\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u7f3a\u4e4f\u65b9\u6cd5\u8bba\u7684\u4e25\u8c28\u6027\uff0c\u8bb8\u591a\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u5b9e\u73b0\u7ec6\u8282\u9ad8\u5ea6\u654f\u611f\uff0c\u5bfc\u81f4\u6027\u80fd\u63d0\u5347\u7684\u62a5\u9053\u53ef\u80fd\u4e0d\u53ef\u9760\u3002", "method": "\u901a\u8fc7\u5168\u9762\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u5b9e\u73b0\u9009\u62e9\u7684\u654f\u611f\u6027\uff0c\u5e76\u63d0\u51fa\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u6700\u4f73\u5b9e\u8df5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08RL\uff09\u7684\u6539\u8fdb\u6709\u9650\u4e14\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u800c\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff08SFT\uff09\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03\u4ee3\u7801\u3001\u63d0\u793a\u548c\u6a21\u578b\u8f93\u51fa\uff0c\u4e3a\u672a\u6765\u7684\u63a8\u7406\u7814\u7a76\u5efa\u7acb\u4e86\u66f4\u4e25\u8c28\u7684\u57fa\u7840\u3002"}}
{"id": "2504.06841", "pdf": "https://arxiv.org/pdf/2504.06841", "abs": "https://arxiv.org/abs/2504.06841", "authors": ["Tom Simon", "William Mocaer", "Pierrick Tranouez", "Clement Chatelain", "Thierry Paquet"], "title": "Classifying the Unknown: In-Context Learning for Open-Vocabulary Text and Symbol Recognition", "categories": ["cs.CV"], "comment": "Submitted to ICDAR 2025", "summary": "We introduce Rosetta, a multimodal model that leverages Multimodal In-Context\nLearning (MICL) to classify sequences of novel script patterns in documents by\nleveraging minimal examples, thus eliminating the need for explicit retraining.\nTo enhance contextual learning, we designed a dataset generation process that\nensures varying degrees of contextual informativeness, improving the model's\nadaptability in leveraging context across different scenarios. A key strength\nof our method is the use of a Context-Aware Tokenizer (CAT), which enables\nopen-vocabulary classification. This allows the model to classify text and\nsymbol patterns across an unlimited range of classes, extending its\nclassification capabilities beyond the scope of its training alphabet of\npatterns. As a result, it unlocks applications such as the recognition of new\nalphabets and languages. Experiments on synthetic datasets demonstrate the\npotential of Rosetta to successfully classify Out-Of-Distribution visual\npatterns and diverse sets of alphabets and scripts, including but not limited\nto Chinese, Greek, Russian, French, Spanish, and Japanese.", "AI": {"task": "\u5229\u7528\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08MICL\uff09\u5bf9\u6587\u6863\u4e2d\u7684\u65b0\u811a\u672c\u6a21\u5f0f\u5e8f\u5217\u8fdb\u884c\u5206\u7c7b\uff0c\u65e0\u9700\u663e\u5f0f\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u901a\u8fc7\u6700\u5c0f\u793a\u4f8b\u5b9e\u73b0\u5bf9\u65b0\u578b\u811a\u672c\u6a21\u5f0f\u7684\u9ad8\u6548\u5206\u7c7b\uff0c\u6269\u5c55\u6a21\u578b\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u8bbe\u8ba1\u4e86\u6570\u636e\u96c6\u751f\u6210\u8fc7\u7a0b\u4ee5\u589e\u5f3a\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u5e76\u91c7\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u5206\u8bcd\u5668\uff08CAT\uff09\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRosetta\u80fd\u591f\u6210\u529f\u5206\u7c7b\u8d85\u51fa\u8bad\u7ec3\u8303\u56f4\u7684\u89c6\u89c9\u6a21\u5f0f\u548c\u591a\u79cd\u5b57\u6bcd\u53ca\u811a\u672c\u3002", "conclusion": "Rosetta\u5c55\u793a\u4e86\u5728\u591a\u8bed\u8a00\u548c\u65b0\u578b\u811a\u672c\u5206\u7c7b\u4e2d\u7684\u6f5c\u529b\uff0c\u6269\u5c55\u4e86\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2504.07089", "pdf": "https://arxiv.org/pdf/2504.07089", "abs": "https://arxiv.org/abs/2504.07089", "authors": ["Yiting Lu", "Jiakang Yuan", "Zhen Li", "Shitian Zhao", "Qi Qin", "Xinyue Li", "Le Zhuo", "Licheng Wen", "Dongyang Liu", "Yuewen Cao", "Xiangchao Yan", "Xin Li", "Botian Shi", "Tao Chen", "Zhibo Chen", "Lei Bai", "Bo Zhang", "Peng Gao"], "title": "OmniCaptioner: One Captioner to Rule Them All", "categories": ["cs.CV", "cs.CL"], "comment": "More visualizations on Homepage:\n  https://alpha-innovator.github.io/OmniCaptioner-project-page and Official\n  code: https://github.com/Alpha-Innovator/OmniCaptioner", "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities.", "AI": {"task": "\u63d0\u51faOmniCaptioner\uff0c\u4e00\u4e2a\u901a\u7528\u7684\u89c6\u89c9\u63cf\u8ff0\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u8de8\u591a\u79cd\u89c6\u89c9\u9886\u57df\u7684\u7ec6\u7c92\u5ea6\u6587\u672c\u63cf\u8ff0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u56fe\u50cf\u7c7b\u578b\uff08\u5982\u81ea\u7136\u56fe\u50cf\u6216\u51e0\u4f55\u89c6\u89c9\uff09\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u4f4e\u5c42\u6b21\u50cf\u7d20\u4fe1\u606f\u8f6c\u6362\u4e3a\u8bed\u4e49\u4e30\u5bcc\u7684\u6587\u672c\u8868\u793a\uff0c\u5f25\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u5c55\u793a\u4e86\u4e09\u4e2a\u5173\u952e\u4f18\u52bf\uff1a\u589e\u5f3a\u89c6\u89c9\u63a8\u7406\u3001\u6539\u8fdb\u56fe\u50cf\u751f\u6210\u548c\u9ad8\u6548\u76d1\u7763\u5fae\u8c03\u3002", "conclusion": "OmniCaptioner\u7684\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u4e3a\u5f25\u5408\u8bed\u8a00\u4e0e\u89c6\u89c9\u6a21\u6001\u4e4b\u95f4\u7684\u5dee\u8ddd\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2504.06856", "pdf": "https://arxiv.org/pdf/2504.06856", "abs": "https://arxiv.org/abs/2504.06856", "authors": ["Mishan Aliev", "Dmitry Baranchuk", "Kirill Struminsky"], "title": "CasTex: Cascaded Text-to-Texture Synthesis via Explicit Texture Maps and Physically-Based Shading", "categories": ["cs.CV"], "comment": "Preprint, work in progress", "summary": "This work investigates text-to-texture synthesis using diffusion models to\ngenerate physically-based texture maps. We aim to achieve realistic model\nappearances under varying lighting conditions. A prominent solution for the\ntask is score distillation sampling. It allows recovering a complex texture\nusing gradient guidance given a differentiable rasterization and shading\npipeline. However, in practice, the aforementioned solution in conjunction with\nthe widespread latent diffusion models produces severe visual artifacts and\nrequires additional regularization such as implicit texture parameterization.\nAs a more direct alternative, we propose an approach using cascaded diffusion\nmodels for texture synthesis (CasTex). In our setup, score distillation\nsampling yields high-quality textures out-of-the box. In particular, we were\nable to omit implicit texture parameterization in favor of an explicit\nparameterization to improve the procedure. In the experiments, we show that our\napproach significantly outperforms state-of-the-art optimization-based\nsolutions on public texture synthesis benchmarks.", "AI": {"task": "\u7814\u7a76\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u5230\u7eb9\u7406\u5408\u6210\u65b9\u6cd5\uff0c\u4ee5\u751f\u6210\u7269\u7406\u57fa\u7840\u7684\u7eb9\u7406\u8d34\u56fe\u3002", "motivation": "\u65e8\u5728\u5b9e\u73b0\u6a21\u578b\u5728\u4e0d\u540c\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u771f\u5b9e\u5916\u89c2\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u7ea7\u8054\u6269\u6563\u6a21\u578b\uff08CasTex\uff09\u7684\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4e86\u4f20\u7edf\u7684\u9690\u5f0f\u7eb9\u7406\u53c2\u6570\u5316\uff0c\u91c7\u7528\u663e\u5f0f\u53c2\u6570\u5316\u4ee5\u63d0\u9ad8\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u516c\u5171\u7eb9\u7406\u5408\u6210\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "CasTex\u65b9\u6cd5\u80fd\u591f\u76f4\u63a5\u751f\u6210\u9ad8\u8d28\u91cf\u7eb9\u7406\uff0c\u65e0\u9700\u989d\u5916\u7684\u9690\u5f0f\u7eb9\u7406\u53c2\u6570\u5316\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2504.07097", "pdf": "https://arxiv.org/pdf/2504.07097", "abs": "https://arxiv.org/abs/2504.07097", "authors": ["Nikhil Shivakumar Nayak", "Krishnateja Killamsetty", "Ligong Han", "Abhishek Bhandwaldar", "Prateek Chanda", "Kai Xu", "Hao Wang", "Aldo Pareja", "Oleg Silkin", "Mustafa Eyceoz", "Akash Srivastava"], "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "math.PR", "stat.ML", "68T50", "I.2.0; G.3"], "comment": "25 pages, 13 figures, 6 tables", "summary": "Continual learning in large language models (LLMs) is prone to catastrophic\nforgetting, where adapting to new tasks significantly degrades performance on\npreviously learned ones. Existing methods typically rely on low-rank,\nparameter-efficient updates that limit the model's expressivity and introduce\nadditional parameters per task, leading to scalability issues. To address these\nlimitations, we propose a novel continual full fine-tuning approach leveraging\nadaptive singular value decomposition (SVD). Our method dynamically identifies\ntask-specific low-rank parameter subspaces and constrains updates to be\northogonal to critical directions associated with prior tasks, thus effectively\nminimizing interference without additional parameter overhead or storing\nprevious task gradients. We evaluate our approach extensively on standard\ncontinual learning benchmarks using both encoder-decoder (T5-Large) and\ndecoder-only (LLaMA-2 7B) models, spanning diverse tasks including\nclassification, generation, and reasoning. Empirically, our method achieves\nstate-of-the-art results, up to 7% higher average accuracy than recent\nbaselines like O-LoRA, and notably maintains the model's general linguistic\ncapabilities, instruction-following accuracy, and safety throughout the\ncontinual learning process by reducing forgetting to near-negligible levels.\nOur adaptive SVD framework effectively balances model plasticity and knowledge\nretention, providing a practical, theoretically grounded, and computationally\nscalable solution for continual learning scenarios in large language models.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u7684\u6301\u7eed\u5168\u5fae\u8c03\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u9762\u4e34\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u4f4e\u79e9\u3001\u53c2\u6570\u9ad8\u6548\u7684\u66f4\u65b0\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u5e76\u5f15\u5165\u4e86\u989d\u5916\u7684\u4efb\u52a1\u53c2\u6570\uff0c\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u8bc6\u522b\u4efb\u52a1\u7279\u5b9a\u7684\u4f4e\u79e9\u53c2\u6570\u5b50\u7a7a\u95f4\uff0c\u5e76\u5c06\u66f4\u65b0\u7ea6\u675f\u5728\u4e0e\u5148\u524d\u4efb\u52a1\u76f8\u5173\u7684\u5173\u952e\u65b9\u5411\u4e0a\u6b63\u4ea4\uff0c\u4ece\u800c\u6700\u5c0f\u5316\u5e72\u6270\u3002", "result": "\u5728\u6807\u51c6\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982O-LoRA\uff09\u9ad8\u51fa7%\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u9057\u5fd8\u73b0\u8c61\u3002", "conclusion": "\u81ea\u9002\u5e94SVD\u6846\u67b6\u5728\u6a21\u578b\u53ef\u5851\u6027\u548c\u77e5\u8bc6\u4fdd\u7559\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u8ba1\u7b97\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.06861", "pdf": "https://arxiv.org/pdf/2504.06861", "abs": "https://arxiv.org/abs/2504.06861", "authors": ["Diljeet Jagpal", "Xi Chen", "Vinay P. Namboodiri"], "title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025", "summary": "Zero-shot, training-free, image-based text-to-video generation is an emerging\narea that aims to generate videos using existing image-based diffusion models.\nCurrent methods in this space require specific architectural changes to image\ngeneration models, which limit their adaptability and scalability. In contrast\nto such methods, we provide a model-agnostic approach. We use intersections in\ndiffusion trajectories, working only with the latent values. We could not\nobtain localized frame-wise coherence and diversity using only the intersection\nof trajectories. Thus, we instead use a grid-based approach. An in-context\ntrained LLM is used to generate coherent frame-wise prompts; another is used to\nidentify differences between frames. Based on these, we obtain a CLIP-based\nattention mask that controls the timing of switching the prompts for each grid\ncell. Earlier switching results in higher variance, while later switching\nresults in more coherence. Therefore, our approach can ensure appropriate\ncontrol between coherence and variance for the frames. Our approach results in\nstate-of-the-art performance while being more flexible when working with\ndiverse image-generation models. The empirical analysis using quantitative\nmetrics and user studies confirms our model's superior temporal consistency,\nvisual fidelity and user satisfaction, thus providing a novel way to obtain\ntraining-free, image-based text-to-video generation.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u96f6\u6837\u672c\u3001\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u56fe\u50cf\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6709\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u89c6\u9891\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u9700\u8981\u5bf9\u56fe\u50cf\u751f\u6210\u6a21\u578b\u8fdb\u884c\u7279\u5b9a\u67b6\u6784\u4fee\u6539\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u548c\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u8f68\u8ff9\u7684\u4ea4\u96c6\u548c\u57fa\u4e8e\u7f51\u683c\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u8bad\u7ec3\u7684LLM\u751f\u6210\u8fde\u8d2f\u7684\u5e27\u63d0\u793a\u548c\u5e27\u95f4\u5dee\u5f02\uff0c\u901a\u8fc7CLIP\u6ce8\u610f\u529b\u63a9\u7801\u63a7\u5236\u63d0\u793a\u5207\u6362\u65f6\u673a\u3002", "result": "\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u66f4\u597d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u89c6\u89c9\u4fdd\u771f\u5ea6\u53ca\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u57fa\u4e8e\u56fe\u50cf\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u8fde\u8d2f\u6027\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2504.06863", "pdf": "https://arxiv.org/pdf/2504.06863", "abs": "https://arxiv.org/abs/2504.06863", "authors": ["Chang Nie", "Yiqing Xu", "Guangming Wang", "Zhe Liu", "Yanzi Miao", "Hesheng Wang"], "title": "MovSAM: A Single-image Moving Object Segmentation Framework Based on Deep Thinking", "categories": ["cs.CV"], "comment": null, "summary": "Moving object segmentation plays a vital role in understanding dynamic visual\nenvironments. While existing methods rely on multi-frame image sequences to\nidentify moving objects, single-image MOS is critical for applications like\nmotion intention prediction and handling camera frame drops. However,\nsegmenting moving objects from a single image remains challenging for existing\nmethods due to the absence of temporal cues. To address this gap, we propose\nMovSAM, the first framework for single-image moving object segmentation. MovSAM\nleverages a Multimodal Large Language Model (MLLM) enhanced with\nChain-of-Thought (CoT) prompting to search the moving object and generate text\nprompts based on deep thinking for segmentation. These prompts are cross-fused\nwith visual features from the Segment Anything Model (SAM) and a\nVision-Language Model (VLM), enabling logic-driven moving object segmentation.\nThe segmentation results then undergo a deep thinking refinement loop, allowing\nMovSAM to iteratively improve its understanding of the scene context and\ninter-object relationships with logical reasoning. This innovative approach\nenables MovSAM to segment moving objects in single images by considering scene\nunderstanding. We implement MovSAM in the real world to validate its practical\napplication and effectiveness for autonomous driving scenarios where the\nmulti-frame methods fail. Furthermore, despite the inherent advantage of\nmulti-frame methods in utilizing temporal information, MovSAM achieves\nstate-of-the-art performance across public MOS benchmarks, reaching 92.5\\% on\nJ\\&F. Our implementation will be available at\nhttps://github.com/IRMVLab/MovSAM.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aMovSAM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5355\u56fe\u50cf\u8fd0\u52a8\u7269\u4f53\u5206\u5272\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u65f6\u95f4\u7ebf\u7d22\u800c\u96be\u4ee5\u4ece\u5355\u56fe\u50cf\u4e2d\u5206\u5272\u8fd0\u52a8\u7269\u4f53\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8fd0\u52a8\u610f\u56fe\u9884\u6d4b\u548c\u76f8\u673a\u5e27\u4e22\u5931\u5904\u7406\u7b49\u5e94\u7528\u4e2d\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u548c\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63d0\u793a\u751f\u6210\u6587\u672c\u63d0\u793a\uff0c\u4e0eSegment Anything Model\uff08SAM\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u89c6\u89c9\u7279\u5f81\u4ea4\u53c9\u878d\u5408\uff0c\u5b9e\u73b0\u903b\u8f91\u9a71\u52a8\u7684\u8fd0\u52a8\u7269\u4f53\u5206\u5272\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u601d\u8003\u5faa\u73af\u8fed\u4ee3\u4f18\u5316\u5206\u5272\u7ed3\u679c\u3002", "result": "\u5728\u516c\u5171MOS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523092.5%\u7684J&F\u5206\u6570\uff0c\u8868\u73b0\u4f18\u4e8e\u591a\u5e27\u65b9\u6cd5\u3002", "conclusion": "MovSAM\u901a\u8fc7\u573a\u666f\u7406\u89e3\u548c\u903b\u8f91\u63a8\u7406\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5355\u56fe\u50cf\u8fd0\u52a8\u7269\u4f53\u5206\u5272\uff0c\u5e76\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.06881", "pdf": "https://arxiv.org/pdf/2504.06881", "abs": "https://arxiv.org/abs/2504.06881", "authors": ["Mingbo Li", "Liying Liu", "Ye Luo"], "title": "Compound and Parallel Modes of Tropical Convolutional Neural Networks", "categories": ["cs.CV", "cs.AI", "I.2.6"], "comment": "28 pages, 5 figures", "summary": "Convolutional neural networks have become increasingly deep and complex,\nleading to higher computational costs. While tropical convolutional neural\nnetworks (TCNNs) reduce multiplications, they underperform compared to standard\nCNNs. To address this, we propose two new variants - compound TCNN (cTCNN) and\nparallel TCNN (pTCNN)-that use combinations of tropical min-plus and max-plus\nkernels to replace traditional convolution kernels. This reduces\nmultiplications and balances efficiency with performance. Experiments on\nvarious datasets show that cTCNN and pTCNN match or exceed the performance of\nother CNN methods. Combining these with conventional CNNs in deeper\narchitectures also improves performance. We are further exploring simplified\nTCNN architectures that reduce parameters and multiplications with minimal\naccuracy loss, aiming for efficient and effective models.", "AI": {"task": "\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u70ed\u5e26\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u53d8\u4f53\uff08cTCNN\u548cpTCNN\uff09\uff0c\u4ee5\u66ff\u4ee3\u4f20\u7edf\u5377\u79ef\u6838\uff0c\u51cf\u5c11\u4e58\u6cd5\u8fd0\u7b97\u5e76\u5e73\u8861\u6548\u7387\u4e0e\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u70ed\u5e26\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08TCNNs\uff09\u867d\u51cf\u5c11\u4e58\u6cd5\u8fd0\u7b97\u4f46\u6027\u80fd\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u70ed\u5e26min-plus\u548cmax-plus\u6838\uff0c\u8bbe\u8ba1cTCNN\u548cpTCNN\uff0c\u5e76\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "cTCNN\u548cpTCNN\u5728\u6027\u80fd\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u5176\u4ed6CNN\u65b9\u6cd5\uff0c\u4e14\u5728\u6df1\u5c42\u67b6\u6784\u4e2d\u7ed3\u5408\u4f20\u7edfCNNs\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "cTCNN\u548cpTCNN\u662f\u9ad8\u6548\u4e14\u6709\u6548\u7684\u6a21\u578b\uff0c\u672a\u6765\u5c06\u8fdb\u4e00\u6b65\u63a2\u7d22\u7b80\u5316\u67b6\u6784\u4ee5\u51cf\u5c11\u53c2\u6570\u548c\u4e58\u6cd5\u8fd0\u7b97\u3002"}}
{"id": "2504.06895", "pdf": "https://arxiv.org/pdf/2504.06895", "abs": "https://arxiv.org/abs/2504.06895", "authors": ["Dingkun Yan", "Xinrui Wang", "Yusuke Iwasawa", "Yutaka Matsuo", "Suguru Saito", "Jiaxian Guo"], "title": "ColorizeDiffusion v2: Enhancing Reference-based Sketch Colorization Through Separating Utilities", "categories": ["cs.CV"], "comment": null, "summary": "Reference-based sketch colorization methods have garnered significant\nattention due to their potential applications in the animation production\nindustry. However, most existing methods are trained with image triplets of\nsketch, reference, and ground truth that are semantically and spatially\nwell-aligned, while real-world references and sketches often exhibit\nsubstantial misalignment. This mismatch in data distribution between training\nand inference leads to overfitting, consequently resulting in spatial artifacts\nand significant degradation in overall colorization quality, limiting potential\napplications of current methods for general purposes. To address this\nlimitation, we conduct an in-depth analysis of the \\textbf{carrier}, defined as\nthe latent representation facilitating information transfer from reference to\nsketch. Based on this analysis, we propose a novel workflow that dynamically\nadapts the carrier to optimize distinct aspects of colorization. Specifically,\nfor spatially misaligned artifacts, we introduce a split cross-attention\nmechanism with spatial masks, enabling region-specific reference injection\nwithin the diffusion process. To mitigate semantic neglect of sketches, we\nemploy dedicated background and style encoders to transfer detailed reference\ninformation in the latent feature space, achieving enhanced spatial control and\nricher detail synthesis. Furthermore, we propose character-mask merging and\nbackground bleaching as preprocessing steps to improve foreground-background\nintegration and background generation. Extensive qualitative and quantitative\nevaluations, including a user study, demonstrate the superior performance of\nour proposed method compared to existing approaches. An ablation study further\nvalidates the efficacy of each proposed component.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u53c2\u8003\u7684\u8349\u56fe\u7740\u8272\u65b9\u6cd5\uff0c\u89e3\u51b3\u8bad\u7ec3\u4e0e\u63a8\u7406\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\u8bed\u4e49\u548c\u7a7a\u95f4\u5bf9\u9f50\u7684\u56fe\u50cf\u4e09\u5143\u7ec4\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u53c2\u8003\u56fe\u548c\u8349\u56fe\u5f80\u5f80\u5b58\u5728\u663e\u8457\u4e0d\u5bf9\u9f50\uff0c\u5bfc\u81f4\u7740\u8272\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8f7d\u4f53\uff08\u4fe1\u606f\u4f20\u9012\u7684\u6f5c\u5728\u8868\u793a\uff09\uff0c\u63d0\u51fa\u52a8\u6001\u9002\u5e94\u8f7d\u4f53\u7684\u5de5\u4f5c\u6d41\uff0c\u5305\u62ec\u5206\u5272\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u3001\u4e13\u7528\u7f16\u7801\u5668\u548c\u9884\u5904\u7406\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7528\u6237\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u4f18\u5316\u8f7d\u4f53\u548c\u5f15\u5165\u65b0\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8349\u56fe\u7740\u8272\u7684\u8d28\u91cf\u548c\u9002\u7528\u6027\u3002"}}
{"id": "2504.06897", "pdf": "https://arxiv.org/pdf/2504.06897", "abs": "https://arxiv.org/abs/2504.06897", "authors": ["Jiawei Mao", "Yuhan Wang", "Yucheng Tang", "Daguang Xu", "Kang Wang", "Yang Yang", "Zongwei Zhou", "Yuyin Zhou"], "title": "MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "12 pages, 8 figures, The project page can be accessed via\n  https://jwmao1.github.io/MedSegFactory_web", "summary": "This paper presents MedSegFactory, a versatile medical synthesis framework\nthat generates high-quality paired medical images and segmentation masks across\nmodalities and tasks. It aims to serve as an unlimited data repository,\nsupplying image-mask pairs to enhance existing segmentation tools. The core of\nMedSegFactory is a dual-stream diffusion model, where one stream synthesizes\nmedical images and the other generates corresponding segmentation masks. To\nensure precise alignment between image-mask pairs, we introduce Joint\nCross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic\ncross-conditioning between streams. This bidirectional interaction allows both\nrepresentations to guide each other's generation, enhancing consistency between\ngenerated pairs. MedSegFactory unlocks on-demand generation of paired medical\nimages and segmentation masks through user-defined prompts that specify the\ntarget labels, imaging modalities, anatomical regions, and pathological\nconditions, facilitating scalable and high-quality data generation. This new\nparadigm of medical image synthesis enables seamless integration into diverse\nmedical imaging workflows, enhancing both efficiency and accuracy. Extensive\nexperiments show that MedSegFactory generates data of superior quality and\nusability, achieving competitive or state-of-the-art performance in 2D and 3D\nsegmentation tasks while addressing data scarcity and regulatory constraints.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aMedSegFactory\u7684\u591a\u529f\u80fd\u533b\u5b66\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u914d\u5bf9\u7684\u533b\u5b66\u56fe\u50cf\u548c\u5206\u5272\u63a9\u7801\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u76d1\u7ba1\u9650\u5236\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u73b0\u6709\u5206\u5272\u5de5\u5177\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u53cc\u6d41\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u8054\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\uff08JCA\uff09\u673a\u5236\uff0c\u5b9e\u73b0\u56fe\u50cf\u548c\u63a9\u7801\u7684\u534f\u540c\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMedSegFactory\u751f\u6210\u7684\u6570\u636e\u8d28\u91cf\u548c\u53ef\u7528\u6027\u4f18\u8d8a\uff0c\u57282D\u548c3D\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MedSegFactory\u4e3a\u533b\u5b66\u56fe\u50cf\u5408\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.06908", "pdf": "https://arxiv.org/pdf/2504.06908", "abs": "https://arxiv.org/abs/2504.06908", "authors": ["Emmanuelle Bourigault", "Amir Jamaludin", "Abdullah Hamdi"], "title": "UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "preprint", "summary": "In medical imaging, the primary challenge is collecting large-scale labeled\ndata due to privacy concerns, logistics, and high labeling costs. In this work,\nwe present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset\nof body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D\nimages) and more than 1.37 billion 2D segmentation masks of 72 organs, all\nbased on the UK Biobank MRI dataset. We utilize automatic labeling, introduce\nan automated label cleaning pipeline with organ-specific filters, and manually\nannotate a subset of 300 MRIs with 11 abdominal classes to validate the quality\n(referred to as UKBOB-manual). This approach allows for scaling up the dataset\ncollection while maintaining confidence in the labels. We further confirm the\nvalidity of the labels by demonstrating zero-shot generalization of trained\nmodels on the filtered UKBOB to other small labeled datasets from similar\ndomains (e.g., abdominal MRI). To further mitigate the effect of noisy labels,\nwe propose a novel method called Entropy Test-time Adaptation (ETTA) to refine\nthe segmentation output. We use UKBOB to train a foundation model, Swin-BOB,\nfor 3D medical image segmentation based on the Swin-UNetr architecture,\nachieving state-of-the-art results in several benchmarks in 3D medical imaging,\nincluding the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the\nBTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained\nmodels and the code are available at https://emmanuelleb985.github.io/ukbob ,\nand the filtered labels will be made available with the UK Biobank.", "AI": {"task": "\u6784\u5efa\u5e76\u9a8c\u8bc1UKBOB\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6807\u7b7e\u6e05\u6d17\u65b9\u6cd5\uff08ETTA\uff09\uff0c\u5e76\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8eSwin-UNetr\u67b6\u6784\u7684\u57fa\u7840\u6a21\u578bSwin-BOB\u7528\u4e8e3D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u5f71\u50cf\u9886\u57df\u56e0\u9690\u79c1\u3001\u7269\u6d41\u548c\u9ad8\u6807\u6ce8\u6210\u672c\u5bfc\u81f4\u7684\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u81ea\u52a8\u6807\u6ce8\u548c\u6807\u7b7e\u6e05\u6d17\u6d41\u7a0b\uff08\u5305\u62ec\u5668\u5b98\u7279\u5b9a\u8fc7\u6ee4\u5668\u548c\u624b\u52a8\u9a8c\u8bc1\uff09\uff0c\u63d0\u51faETTA\u65b9\u6cd5\u4f18\u5316\u5206\u5272\u8f93\u51fa\uff0c\u5e76\u57fa\u4e8eSwin-UNetr\u8bad\u7ec3Swin-BOB\u6a21\u578b\u3002", "result": "UKBOB\u6210\u4e3a\u6700\u5927\u7684\u5668\u5b98\u6807\u6ce8\u6570\u636e\u96c6\uff0cSwin-BOB\u5728\u591a\u4e2a3D\u533b\u5b66\u5f71\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff08\u5982BRATS\u548cBTCV\uff09\u3002", "conclusion": "UKBOB\u6570\u636e\u96c6\u548cSwin-BOB\u6a21\u578b\u4e3a\u533b\u5b66\u5f71\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u5f00\u6e90\u6570\u636e\u548c\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2504.06920", "pdf": "https://arxiv.org/pdf/2504.06920", "abs": "https://arxiv.org/abs/2504.06920", "authors": ["Masquil El\u00edas", "Mar\u00ed Roger", "Ehret Thibaud", "Meinhardt-Llopis Enric", "Mus\u00e9 Pablo", "Facciolo Gabriele"], "title": "S-EO: A Large-Scale Dataset for Geometry-Aware Shadow Detection in Remote Sensing Applications", "categories": ["cs.CV"], "comment": "Accepted at Earthvision 2025 (CVPR Workshop)", "summary": "We introduce the S-EO dataset: a large-scale, high-resolution dataset,\ndesigned to advance geometry-aware shadow detection. Collected from diverse\npublic-domain sources, including challenge datasets and government providers\nsuch as USGS, our dataset comprises 702 georeferenced tiles across the USA,\neach covering 500x500 m. Each tile includes multi-date, multi-angle WorldView-3\npansharpened RGB images, panchromatic images, and a ground-truth DSM of the\narea obtained from LiDAR scans. For each image, we provide a shadow mask\nderived from geometry and sun position, a vegetation mask based on the NDVI\nindex, and a bundle-adjusted RPC model. With approximately 20,000 images, the\nS-EO dataset establishes a new public resource for shadow detection in remote\nsensing imagery and its applications to 3D reconstruction. To demonstrate the\ndataset's impact, we train and evaluate a shadow detector, showcasing its\nability to generalize, even to aerial images. Finally, we extend EO-NeRF - a\nstate-of-the-art NeRF approach for satellite imagery - to leverage our shadow\npredictions for improved 3D reconstructions.", "AI": {"task": "\u63d0\u51faS-EO\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u51e0\u4f55\u611f\u77e5\u7684\u9634\u5f71\u68c0\u6d4b\uff0c\u5e76\u5c55\u793a\u5176\u57283D\u91cd\u5efa\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u4e3a\u9065\u611f\u5f71\u50cf\u4e2d\u7684\u9634\u5f71\u68c0\u6d4b\u53ca\u5176\u57283D\u91cd\u5efa\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u65b0\u7684\u516c\u5f00\u8d44\u6e90\u3002", "method": "\u6536\u96c6\u591a\u6e90\u6570\u636e\uff08\u5982WorldView-3\u5f71\u50cf\u548cLiDAR DSM\uff09\uff0c\u751f\u6210\u9634\u5f71\u548c\u690d\u88ab\u63a9\u819c\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u9634\u5f71\u68c0\u6d4b\u5668\u9a8c\u8bc1\u6570\u636e\u96c6\u6548\u679c\u3002", "result": "S-EO\u6570\u636e\u96c6\u5305\u542b\u7ea620,000\u5f20\u56fe\u50cf\uff0c\u652f\u6301\u9634\u5f71\u68c0\u6d4b\u548c3D\u91cd\u5efa\u4efb\u52a1\u3002", "conclusion": "S-EO\u6570\u636e\u96c6\u4e3a\u9634\u5f71\u68c0\u6d4b\u548c3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u8d44\u6e90\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.06925", "pdf": "https://arxiv.org/pdf/2504.06925", "abs": "https://arxiv.org/abs/2504.06925", "authors": ["Sergio Romero-Tapiador", "Ruben Tolosana", "Blanca Lacruz-Pleguezuelos", "Laura Judith Marcos Zambrano", "Guadalupe X. Baz\u00e1n", "Isabel Espinosa-Salinas", "Julian Fierrez", "Javier Ortega-Garcia", "Enrique Carrillo de Santa Pau", "Aythami Morales"], "title": "Are Vision-Language Models Ready for Dietary Assessment? Exploring the Next Frontier in AI-Powered Food Image Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at IEEE/CVF Computer Vision and Pattern Recognition\n  Conference workshops 2025 (CVPRw) 10 pages, 4 figures, 2 tables", "summary": "Automatic dietary assessment based on food images remains a challenge,\nrequiring precise food detection, segmentation, and classification.\nVision-Language Models (VLMs) offer new possibilities by integrating visual and\ntextual reasoning. In this study, we evaluate six state-of-the-art VLMs\n(ChatGPT, Gemini, Claude, Moondream, DeepSeek, and LLaVA), analyzing their\ncapabilities in food recognition at different levels. For the experimental\nframework, we introduce the FoodNExTDB, a unique food image database that\ncontains 9,263 expert-labeled images across 10 categories (e.g., \"protein\nsource\"), 62 subcategories (e.g., \"poultry\"), and 9 cooking styles (e.g.,\n\"grilled\"). In total, FoodNExTDB includes 50k nutritional labels generated by\nseven experts who manually annotated all images in the database. Also, we\npropose a novel evaluation metric, Expert-Weighted Recall (EWR), that accounts\nfor the inter-annotator variability. Results show that closed-source models\noutperform open-source ones, achieving over 90% EWR in recognizing food\nproducts in images containing a single product. Despite their potential,\ncurrent VLMs face challenges in fine-grained food recognition, particularly in\ndistinguishing subtle differences in cooking styles and visually similar food\nitems, which limits their reliability for automatic dietary assessment. The\nFoodNExTDB database is publicly available at\nhttps://github.com/AI4Food/FoodNExtDB.", "AI": {"task": "\u8bc4\u4f30\u516d\u79cd\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u98df\u54c1\u8bc6\u522b\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u81ea\u52a8\u996e\u98df\u8bc4\u4f30\u57fa\u4e8e\u98df\u54c1\u56fe\u50cf\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u7cbe\u786e\u7684\u98df\u54c1\u68c0\u6d4b\u3001\u5206\u5272\u548c\u5206\u7c7b\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u548c\u6587\u672c\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u5f15\u5165FoodNExTDB\u6570\u636e\u5e93\uff0c\u5305\u542b9,263\u5f20\u4e13\u5bb6\u6807\u6ce8\u7684\u56fe\u50cf\uff0c\u6db5\u76d610\u4e2a\u7c7b\u522b\u300162\u4e2a\u5b50\u7c7b\u522b\u548c9\u79cd\u70f9\u996a\u98ce\u683c\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u6307\u6807Expert-Weighted Recall (EWR)\u3002", "result": "\u95ed\u6e90\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b\uff0c\u5728\u5355\u4ea7\u54c1\u56fe\u50cf\u4e2dEWR\u8d85\u8fc790%\uff0c\u4f46\u5f53\u524dVLMs\u5728\u7ec6\u7c92\u5ea6\u98df\u54c1\u8bc6\u522b\uff08\u5982\u70f9\u996a\u98ce\u683c\u548c\u89c6\u89c9\u76f8\u4f3c\u98df\u54c1\u7684\u533a\u5206\uff09\u4e0a\u4ecd\u9762\u4e34\u6311\u6218\u3002", "conclusion": "\u5c3d\u7ba1VLMs\u5728\u98df\u54c1\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4e0a\u7684\u5c40\u9650\u6027\u9650\u5236\u4e86\u5176\u5728\u81ea\u52a8\u996e\u98df\u8bc4\u4f30\u4e2d\u7684\u53ef\u9760\u6027\u3002FoodNExTDB\u6570\u636e\u5e93\u5df2\u516c\u5f00\u3002"}}
{"id": "2504.06950", "pdf": "https://arxiv.org/pdf/2504.06950", "abs": "https://arxiv.org/abs/2504.06950", "authors": ["Sachin Kumar Danisetty", "Alexandros Graikos", "Srikar Yellapragada", "Dimitris Samaras"], "title": "PathSegDiff: Pathology Segmentation using Diffusion model representations", "categories": ["cs.CV"], "comment": null, "summary": "Image segmentation is crucial in many computational pathology pipelines,\nincluding accurate disease diagnosis, subtyping, outcome, and survivability\nprediction. The common approach for training a segmentation model relies on a\npre-trained feature extractor and a dataset of paired image and mask\nannotations. These are used to train a lightweight prediction model that\ntranslates features into per-pixel classes. The choice of the feature extractor\nis central to the performance of the final segmentation model, and recent\nliterature has focused on finding tasks to pre-train the feature extractor. In\nthis paper, we propose PathSegDiff, a novel approach for histopathology image\nsegmentation that leverages Latent Diffusion Models (LDMs) as pre-trained\nfeatured extractors. Our method utilizes a pathology-specific LDM, guided by a\nself-supervised encoder, to extract rich semantic information from H\\&E stained\nhistopathology images. We employ a simple, fully convolutional network to\nprocess the features extracted from the LDM and generate segmentation masks.\nOur experiments demonstrate significant improvements over traditional methods\non the BCSS and GlaS datasets, highlighting the effectiveness of\ndomain-specific diffusion pre-training in capturing intricate tissue structures\nand enhancing segmentation accuracy in histopathology images.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u7684\u75c5\u7406\u56fe\u50cf\u5206\u5272\u65b9\u6cd5PathSegDiff\u3002", "motivation": "\u4f20\u7edf\u5206\u5272\u6a21\u578b\u4f9d\u8d56\u4e8e\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u800c\u73b0\u6709\u7814\u7a76\u5173\u6ce8\u5982\u4f55\u901a\u8fc7\u4efb\u52a1\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u672c\u6587\u63d0\u51fa\u5229\u7528\u75c5\u7406\u7279\u5f02\u6027LDM\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u75c5\u7406\u7279\u5f02\u6027LDM\u7ed3\u5408\u81ea\u76d1\u7763\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5168\u5377\u79ef\u7f51\u7edc\u751f\u6210\u5206\u5272\u63a9\u7801\u3002", "result": "\u5728BCSS\u548cGlaS\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u9886\u57df\u7279\u5b9a\u6269\u6563\u9884\u8bad\u7ec3\u7684\u6709\u6548\u6027\u3002", "conclusion": "PathSegDiff\u901a\u8fc7LDM\u63d0\u53d6\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u75c5\u7406\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2504.06957", "pdf": "https://arxiv.org/pdf/2504.06957", "abs": "https://arxiv.org/abs/2504.06957", "authors": ["Marco Acerbis", "Nata\u0161a Sladoje", "Joakim Lindblad"], "title": "A Comparison of Deep Learning Methods for Cell Detection in Digital Cytology", "categories": ["cs.CV"], "comment": "14 pages, 6 figures, SCIA2025", "summary": "Accurate and efficient cell detection is crucial in many biomedical image\nanalysis tasks. We evaluate the performance of several Deep Learning (DL)\nmethods for cell detection in Papanicolaou-stained cytological Whole Slide\nImages (WSIs), focusing on accuracy of predictions and computational\nefficiency. We examine recentoff-the-shelf algorithms as well as\ncustom-designed detectors, applying them to two datasets: the CNSeg Dataset and\nthe Oral Cancer (OC) Dataset. Our comparison includes well-established\nsegmentation methods such as StarDist, Cellpose, and the Segment Anything Model\n2 (SAM2), alongside centroid-based Fully Convolutional Regression Network\n(FCRN) approaches. We introduce a suitable evaluation metric to assess the\naccuracy of predictions based on the distance from ground truth positions. We\nalso explore the impact of dataset size and data augmentation techniques on\nmodel performance. Results show that centroid-based methods, particularly the\nImproved Fully Convolutional Regression Network (IFCRN) method, outperform\nsegmentation-based methods in terms of both detection accuracy and\ncomputational efficiency. This study highlights the potential of centroid-based\ndetectors as a preferred option for cell detection in resource-limited\nenvironments, offering faster processing times and lower GPU memory usage\nwithout compromising accuracy.", "AI": {"task": "\u8bc4\u4f30\u51e0\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728Papanicolaou\u67d3\u8272\u7ec6\u80de\u5b66\u5168\u5207\u7247\u56fe\u50cf\uff08WSIs\uff09\u4e2d\u7684\u7ec6\u80de\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7ec6\u80de\u68c0\u6d4b\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7387\u7684\u65b9\u6cd5\u3002", "method": "\u6bd4\u8f83\u4e86\u73b0\u6210\u7684\u7b97\u6cd5\u548c\u5b9a\u5236\u8bbe\u8ba1\u7684\u68c0\u6d4b\u5668\uff0c\u5305\u62ecStarDist\u3001Cellpose\u3001SAM2\u548c\u57fa\u4e8e\u8d28\u5fc3\u7684FCRN\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ddd\u79bb\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u57fa\u4e8e\u8d28\u5fc3\u7684\u65b9\u6cd5\uff08\u5982IFCRN\uff09\u5728\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u5206\u5272\u7684\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u8d28\u5fc3\u7684\u68c0\u6d4b\u5668\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u5728\u4e0d\u727a\u7272\u7cbe\u5ea6\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u66f4\u5feb\u7684\u5904\u7406\u901f\u5ea6\u548c\u66f4\u4f4e\u7684GPU\u5185\u5b58\u6d88\u8017\u3002"}}
{"id": "2504.06958", "pdf": "https://arxiv.org/pdf/2504.06958", "abs": "https://arxiv.org/abs/2504.06958", "authors": ["Xinhao Li", "Ziang Yan", "Desen Meng", "Lu Dong", "Xiangyu Zeng", "Yinan He", "Yali Wang", "Yu Qiao", "Yi Wang", "Limin Wang"], "title": "VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in reinforcement learning have significantly advanced the\nreasoning capabilities of multimodal large language models (MLLMs). While\napproaches such as Group Relative Policy Optimization (GRPO) and rule-based\nreward mechanisms demonstrate promise in text and image domains, their\napplication to video understanding remains limited. This paper presents a\nsystematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video\nMLLMs, aiming to enhance spatio-temporal perception while maintaining general\ncapabilities. Our experiments reveal that RFT is highly data-efficient for\ntask-specific improvements. Through multi-task RFT on spatio-temporal\nperception objectives with limited samples, we develop VideoChat-R1, a powerful\nvideo MLLM that achieves state-of-the-art performance on spatio-temporal\nperception tasks without sacrificing chat ability, while exhibiting emerging\nspatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1\nboosts performance several-fold in tasks like temporal grounding (+31.8) and\nobject tracking (+31.2). Additionally, it significantly improves on general QA\nbenchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9).\nOur findings underscore the potential of RFT for specialized task enhancement\nof Video MLLMs. We hope our work offers valuable insights for future RL\nresearch in video MLLMs.", "AI": {"task": "\u63a2\u7d22\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u4e0eGRPO\u5728\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u589e\u5f3a\u65f6\u7a7a\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982GRPO\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u673a\u5236\uff09\u5728\u6587\u672c\u548c\u56fe\u50cf\u9886\u57df\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5e94\u7528\u6709\u9650\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1RFT\u65b9\u6cd5\uff0c\u7ed3\u5408GRPO\uff0c\u5bf9\u89c6\u9891MLLMs\u8fdb\u884c\u5fae\u8c03\uff0c\u4e13\u6ce8\u4e8e\u65f6\u7a7a\u611f\u77e5\u76ee\u6807\u3002", "result": "\u5f00\u53d1\u7684VideoChat-R1\u5728\u65f6\u7a7a\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff08\u5982\u65f6\u95f4\u5b9a\u4f4d\u63d0\u534731.8\uff0c\u76ee\u6807\u8ddf\u8e2a\u63d0\u534731.2\uff09\uff0c\u540c\u65f6\u5728\u901a\u7528QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "RFT\u5728\u89c6\u9891MLLMs\u7684\u4e13\u7528\u4efb\u52a1\u589e\u5f3a\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u89c6\u9891MLLMs\u7684\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2504.06962", "pdf": "https://arxiv.org/pdf/2504.06962", "abs": "https://arxiv.org/abs/2504.06962", "authors": ["Thomas Kerdreux", "Alexandre Tuel", "Quentin Febvre", "Alexis Mouche", "Bertrand Chapron"], "title": "Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at CVPR Workshop : The First Workshop on Foundation and\n  Large Vision Models in Remote Sensing", "summary": "Self-supervised learning (SSL) has enabled the development of vision\nfoundation models for Earth Observation (EO), demonstrating strong\ntransferability across diverse remote sensing tasks. While prior work has\nfocused on network architectures and training strategies, the role of dataset\ncuration, especially in balancing and diversifying pre-training datasets,\nremains underexplored. In EO, this challenge is amplified by the redundancy and\nheavy-tailed distributions common in satellite imagery, which can lead to\nbiased representations and inefficient training.\n  In this work, we propose a dynamic dataset pruning strategy designed to\nimprove SSL pre-training by maximizing dataset diversity and balance. Our\nmethod iteratively refines the training set without requiring a pre-existing\nfeature extractor, making it well-suited for domains where curated datasets are\nlimited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode\n(WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by\nocean observations. We train models from scratch on the entire Sentinel-1 WV\narchive spanning 10 years. Across three downstream tasks, our results show that\ndynamic pruning improves both computational efficiency and representation\nquality, leading to stronger transferability.\n  We also release the weights of Nereus-SAR-1, the first model in the Nereus\nfamily, a series of foundation models for ocean observation and analysis using\nSAR imagery, at github.com/galeio-research/nereus-sar-models/.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u6570\u636e\u96c6\u526a\u679d\u7b56\u7565\uff0c\u4ee5\u6539\u8fdb\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u5728\u9065\u611f\u4efb\u52a1\u4e2d\u7684\u9884\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u9065\u611f\u6570\u636e\u4e2d\u5e38\u89c1\u7684\u5197\u4f59\u548c\u957f\u5c3e\u5206\u5e03\u53ef\u80fd\u5bfc\u81f4\u504f\u7f6e\u8868\u793a\u548c\u4f4e\u6548\u8bad\u7ec3\uff0c\u800c\u73b0\u6709\u7814\u7a76\u5bf9\u6570\u636e\u96c6\u5e73\u8861\u548c\u591a\u6837\u5316\u7684\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\u7684\u52a8\u6001\u6570\u636e\u96c6\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8bad\u7ec3\u96c6\u4ee5\u63d0\u9ad8\u591a\u6837\u6027\u548c\u5e73\u8861\u6027\u3002", "result": "\u5728Sentinel-1 WV SAR\u6863\u6848\u4e0a\u9a8c\u8bc1\uff0c\u52a8\u6001\u526a\u679d\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8868\u793a\u8d28\u91cf\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u52a8\u6001\u526a\u679d\u7b56\u7565\u5728\u9065\u611f\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u9996\u4e2aSAR\u6d77\u6d0b\u89c2\u6d4b\u57fa\u7840\u6a21\u578bNereus-SAR-1\u3002"}}
{"id": "2504.06965", "pdf": "https://arxiv.org/pdf/2504.06965", "abs": "https://arxiv.org/abs/2504.06965", "authors": ["Teng Xiao", "Qi Hu", "Qingsong Yan", "Wei Liu", "Zhiwei Ye", "Fei Deng"], "title": "A Deep Single Image Rectification Approach for Pan-Tilt-Zoom Cameras", "categories": ["cs.CV"], "comment": "Accepted to ICME 2025", "summary": "Pan-Tilt-Zoom (PTZ) cameras with wide-angle lenses are widely used in\nsurveillance but often require image rectification due to their inherent\nnonlinear distortions. Current deep learning approaches typically struggle to\nmaintain fine-grained geometric details, resulting in inaccurate rectification.\nThis paper presents a Forward Distortion and Backward Warping Network\n(FDBW-Net), a novel framework for wide-angle image rectification. It begins by\nusing a forward distortion model to synthesize barrel-distorted images,\nreducing pixel redundancy and preventing blur. The network employs a pyramid\ncontext encoder with attention mechanisms to generate backward warping flows\ncontaining geometric details. Then, a multi-scale decoder is used to restore\ndistorted features and output rectified images. FDBW-Net's performance is\nvalidated on diverse datasets: public benchmarks, AirSim-rendered PTZ camera\nimagery, and real-scene PTZ camera datasets. It demonstrates that FDBW-Net\nachieves SOTA performance in distortion rectification, boosting the\nadaptability of PTZ cameras for practical visual applications.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u6846\u67b6FDBW-Net\uff0c\u7528\u4e8e\u5e7f\u89d2\u56fe\u50cf\u6821\u6b63\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5e7f\u89d2\u56fe\u50cf\u6821\u6b63\u4e2d\u96be\u4ee5\u4fdd\u6301\u7ec6\u7c92\u5ea6\u51e0\u4f55\u7ec6\u8282\uff0c\u5bfc\u81f4\u6821\u6b63\u4e0d\u51c6\u786e\u3002", "method": "\u4f7f\u7528\u524d\u5411\u7578\u53d8\u6a21\u578b\u5408\u6210\u6876\u5f62\u7578\u53d8\u56fe\u50cf\uff0c\u7ed3\u5408\u91d1\u5b57\u5854\u4e0a\u4e0b\u6587\u7f16\u7801\u5668\u548c\u6ce8\u610f\u529b\u673a\u5236\u751f\u6210\u53cd\u5411\u53d8\u5f62\u6d41\uff0c\u5e76\u901a\u8fc7\u591a\u5c3a\u5ea6\u89e3\u7801\u5668\u6062\u590d\u56fe\u50cf\u3002", "result": "FDBW-Net\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8868\u73b0\u51faSOTA\u6027\u80fd\u3002", "conclusion": "FDBW-Net\u63d0\u5347\u4e86PTZ\u76f8\u673a\u5728\u5b9e\u9645\u89c6\u89c9\u5e94\u7528\u4e2d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2504.06978", "pdf": "https://arxiv.org/pdf/2504.06978", "abs": "https://arxiv.org/abs/2504.06978", "authors": ["Daiwei Zhang", "Joaquin Gajardo", "Tomislav Medic", "Isinsu Katircioglu", "Mike Boss", "Norbert Kirchgessner", "Achim Walter", "Lukas Roth"], "title": "Wheat3DGS: In-field 3D Reconstruction, Instance Segmentation and Phenotyping of Wheat Heads with Gaussian Splatting", "categories": ["cs.CV"], "comment": "Copyright 2025 IEEE. This is the author's version of the work. It is\n  posted here for your personal use. Not for redistribution. The definitive\n  version is published in the 2025 IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition Workshops (CVPRW)", "summary": "Automated extraction of plant morphological traits is crucial for supporting\ncrop breeding and agricultural management through high-throughput field\nphenotyping (HTFP). Solutions based on multi-view RGB images are attractive due\nto their scalability and affordability, enabling volumetric measurements that\n2D approaches cannot directly capture. While advanced methods like Neural\nRadiance Fields (NeRFs) have shown promise, their application has been limited\nto counting or extracting traits from only a few plants or organs. Furthermore,\naccurately measuring complex structures like individual wheat heads-essential\nfor studying crop yields-remains particularly challenging due to occlusions and\nthe dense arrangement of crop canopies in field conditions. The recent\ndevelopment of 3D Gaussian Splatting (3DGS) offers a promising alternative for\nHTFP due to its high-quality reconstructions and explicit point-based\nrepresentation. In this paper, we present Wheat3DGS, a novel approach that\nleverages 3DGS and the Segment Anything Model (SAM) for precise 3D instance\nsegmentation and morphological measurement of hundreds of wheat heads\nautomatically, representing the first application of 3DGS to HTFP. We validate\nthe accuracy of wheat head extraction against high-resolution laser scan data,\nobtaining per-instance mean absolute percentage errors of 15.1%, 18.3%, and\n40.2% for length, width, and volume. We provide additional comparisons to\nNeRF-based approaches and traditional Muti-View Stereo (MVS), demonstrating\nsuperior results. Our approach enables rapid, non-destructive measurements of\nkey yield-related traits at scale, with significant implications for\naccelerating crop breeding and improving our understanding of wheat\ndevelopment.", "AI": {"task": "\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u548cSegment Anything Model\uff08SAM\uff09\u5b9e\u73b0\u5c0f\u9ea6\u7a57\u7684\u7cbe\u786e3D\u5b9e\u4f8b\u5206\u5272\u548c\u5f62\u6001\u6d4b\u91cf\u3002", "motivation": "\u9ad8\u541e\u5410\u91cf\u7530\u95f4\u8868\u578b\u5206\u6790\uff08HTFP\uff09\u9700\u8981\u81ea\u52a8\u5316\u63d0\u53d6\u690d\u7269\u5f62\u6001\u7279\u5f81\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982NeRF\uff09\u5728\u5904\u7406\u590d\u6742\u7ed3\u6784\uff08\u5982\u5c0f\u9ea6\u7a57\uff09\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u54083DGS\u548cSAM\uff0c\u5f00\u53d1\u4e86Wheat3DGS\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6d4b\u91cf\u5c0f\u9ea6\u7a57\u7684\u5f62\u6001\u7279\u5f81\u3002", "result": "\u4e0e\u9ad8\u5206\u8fa8\u7387\u6fc0\u5149\u626b\u63cf\u6570\u636e\u76f8\u6bd4\uff0c\u5c0f\u9ea6\u7a57\u7684\u957f\u5ea6\u3001\u5bbd\u5ea6\u548c\u4f53\u79ef\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\u5206\u522b\u4e3a15.1%\u300118.3%\u548c40.2%\uff0c\u4f18\u4e8eNeRF\u548c\u4f20\u7edfMVS\u65b9\u6cd5\u3002", "conclusion": "Wheat3DGS\u65b9\u6cd5\u80fd\u591f\u5feb\u901f\u3001\u65e0\u635f\u5730\u5927\u89c4\u6a21\u6d4b\u91cf\u5173\u952e\u4ea7\u91cf\u76f8\u5173\u6027\u72b6\uff0c\u5bf9\u52a0\u901f\u4f5c\u7269\u80b2\u79cd\u548c\u5c0f\u9ea6\u53d1\u80b2\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2504.06982", "pdf": "https://arxiv.org/pdf/2504.06982", "abs": "https://arxiv.org/abs/2504.06982", "authors": ["Yuhang Yang", "Fengqi Liu", "Yixing Lu", "Qin Zhao", "Pingyu Wu", "Wei Zhai", "Ran Yi", "Yang Cao", "Lizhuang Ma", "Zheng-Jun Zha", "Junting Dong"], "title": "SIGMAN:Scaling 3D Human Gaussian Generation with Millions of Assets", "categories": ["cs.CV"], "comment": "project page:https://yyvhang.github.io/SIGMAN_3D/", "summary": "3D human digitization has long been a highly pursued yet challenging task.\nExisting methods aim to generate high-quality 3D digital humans from single or\nmultiple views, but remain primarily constrained by current paradigms and the\nscarcity of 3D human assets. Specifically, recent approaches fall into several\nparadigms: optimization-based and feed-forward (both single-view regression and\nmulti-view generation with reconstruction). However, they are limited by slow\nspeed, low quality, cascade reasoning, and ambiguity in mapping low-dimensional\nplanes to high-dimensional space due to occlusion and invisibility,\nrespectively. Furthermore, existing 3D human assets remain small-scale,\ninsufficient for large-scale training. To address these challenges, we propose\na latent space generation paradigm for 3D human digitization, which involves\ncompressing multi-view images into Gaussians via a UV-structured VAE, along\nwith DiT-based conditional generation, we transform the ill-posed\nlow-to-high-dimensional mapping problem into a learnable distribution shift,\nwhich also supports end-to-end inference. In addition, we employ the multi-view\noptimization approach combined with synthetic data to construct the HGS-1M\ndataset, which contains $1$ million 3D Gaussian assets to support the\nlarge-scale training. Experimental results demonstrate that our paradigm,\npowered by large-scale training, produces high-quality 3D human Gaussians with\nintricate textures, facial details, and loose clothing deformation.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u751f\u6210\u8303\u5f0f\u76843D\u4eba\u4f53\u6570\u5b57\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u901f\u5ea6\u3001\u8d28\u91cf\u548c\u6570\u636e\u89c4\u6a21\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u73b0\u67093D\u4eba\u4f53\u6570\u5b57\u5316\u65b9\u6cd5\u53d7\u9650\u4e8e\u8303\u5f0f\u3001\u6570\u636e\u7a00\u7f3a\u4ee5\u53ca\u4f4e\u7ef4\u5230\u9ad8\u7ef4\u6620\u5c04\u7684\u6a21\u7cca\u6027\uff0c\u5bfc\u81f4\u901f\u5ea6\u6162\u3001\u8d28\u91cf\u4f4e\u3002", "method": "\u91c7\u7528UV\u7ed3\u6784\u5316\u7684VAE\u5c06\u591a\u89c6\u56fe\u56fe\u50cf\u538b\u7f29\u4e3a\u9ad8\u65af\u5206\u5e03\uff0c\u7ed3\u5408DiT\u6761\u4ef6\u751f\u6210\uff0c\u5c06\u4f4e\u7ef4\u5230\u9ad8\u7ef4\u6620\u5c04\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5b66\u4e60\u7684\u5206\u5e03\u504f\u79fb\uff0c\u5e76\u652f\u6301\u7aef\u5230\u7aef\u63a8\u7406\u3002\u540c\u65f6\u6784\u5efaHGS-1M\u6570\u636e\u96c6\u652f\u6301\u5927\u89c4\u6a21\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u7ec6\u8282\u4e30\u5bcc\u76843D\u4eba\u4f53\u9ad8\u65af\u6a21\u578b\uff0c\u5305\u62ec\u590d\u6742\u7eb9\u7406\u3001\u9762\u90e8\u7ec6\u8282\u548c\u5bbd\u677e\u8863\u7269\u53d8\u5f62\u3002", "conclusion": "\u63d0\u51fa\u7684\u6f5c\u5728\u7a7a\u95f4\u751f\u6210\u8303\u5f0f\u7ed3\u5408\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u4eba\u4f53\u6570\u5b57\u5316\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2504.07008", "pdf": "https://arxiv.org/pdf/2504.07008", "abs": "https://arxiv.org/abs/2504.07008", "authors": ["Jonas Loos", "Lorenz Linhardt"], "title": "Latent Diffusion U-Net Representations Contain Positional Embeddings and Anomalies", "categories": ["cs.CV"], "comment": "ICLR 2025 Workshop on Deep Generative Models: Theory, Principle, and\n  Efficacy", "summary": "Diffusion models have demonstrated remarkable capabilities in synthesizing\nrealistic images, spurring interest in using their representations for various\ndownstream tasks. To better understand the robustness of these representations,\nwe analyze popular Stable Diffusion models using representational similarity\nand norms. Our findings reveal three phenomena: (1) the presence of a learned\npositional embedding in intermediate representations, (2) high-similarity\ncorner artifacts, and (3) anomalous high-norm artifacts. These findings\nunderscore the need to further investigate the properties of diffusion model\nrepresentations before considering them for downstream tasks that require\nrobust features. Project page:\nhttps://jonasloos.github.io/sd-representation-anomalies", "AI": {"task": "\u5206\u6790Stable Diffusion\u6a21\u578b\u7684\u8868\u793a\u7279\u6027\u53ca\u5176\u9c81\u68d2\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u903c\u771f\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8868\u793a\u7279\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u9700\u8981\u4e86\u89e3\u5176\u9c81\u68d2\u6027\u4ee5\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u8868\u793a\u76f8\u4f3c\u6027\u548c\u8303\u6570\u5206\u6790\u6d41\u884c\u7684Stable Diffusion\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u4e09\u79cd\u73b0\u8c61\uff1a(1)\u4e2d\u95f4\u8868\u793a\u4e2d\u5b58\u5728\u5b66\u4e60\u7684\u4f4d\u7f6e\u5d4c\u5165\uff0c(2)\u9ad8\u76f8\u4f3c\u6027\u89d2\u70b9\u4f2a\u5f71\uff0c(3)\u5f02\u5e38\u9ad8\u8303\u6570\u4f2a\u5f71\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u7684\u8868\u793a\u7279\u6027\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\uff0c\u4ee5\u786e\u4fdd\u5176\u9002\u7528\u4e8e\u9700\u8981\u9c81\u68d2\u7279\u5f81\u7684\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2504.07025", "pdf": "https://arxiv.org/pdf/2504.07025", "abs": "https://arxiv.org/abs/2504.07025", "authors": ["Bojian Wu", "Yifan Peng", "Ruizhen Hu", "Xiaowei Zhou"], "title": "Glossy Object Reconstruction with Cost-effective Polarized Acquisition", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 as highlight", "summary": "The challenge of image-based 3D reconstruction for glossy objects lies in\nseparating diffuse and specular components on glossy surfaces from captured\nimages, a task complicated by the ambiguity in discerning lighting conditions\nand material properties using RGB data alone. While state-of-the-art methods\nrely on tailored and/or high-end equipment for data acquisition, which can be\ncumbersome and time-consuming, this work introduces a scalable\npolarization-aided approach that employs cost-effective acquisition tools. By\nattaching a linear polarizer to readily available RGB cameras, multi-view\npolarization images can be captured without the need for advance calibration or\nprecise measurements of the polarizer angle, substantially reducing system\nconstruction costs. The proposed approach represents polarimetric BRDF, Stokes\nvectors, and polarization states of object surfaces as neural implicit fields.\nThese fields, combined with the polarizer angle, are retrieved by optimizing\nthe rendering loss of input polarized images. By leveraging fundamental\nphysical principles for the implicit representation of polarization rendering,\nour method demonstrates superiority over existing techniques through\nexperiments in public datasets and real captured images on both reconstruction\nand novel view synthesis.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u504f\u632f\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u4f4e\u6210\u672c\u8bbe\u5907\u6355\u83b7\u7684\u591a\u89c6\u89d2\u504f\u632f\u56fe\u50cf\u4e2d\u5206\u79bb\u6f2b\u53cd\u5c04\u548c\u955c\u9762\u53cd\u5c04\u6210\u5206\uff0c\u5b9e\u73b0\u5149\u6cfd\u7269\u4f53\u76843D\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4e14\u590d\u6742\u7684\u8bbe\u5907\uff0c\u800c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f4e\u6210\u672c\u5de5\u5177\u548c\u504f\u632f\u6280\u672f\u7b80\u5316\u6570\u636e\u91c7\u96c6\u8fc7\u7a0b\uff0c\u964d\u4f4e\u7cfb\u7edf\u6784\u5efa\u6210\u672c\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u6027\u504f\u632f\u5668\u8f85\u52a9\u4e0b\u6355\u83b7\u591a\u89c6\u89d2\u504f\u632f\u56fe\u50cf\uff0c\u5229\u7528\u795e\u7ecf\u9690\u5f0f\u573a\u8868\u793a\u504f\u632fBRDF\u3001\u65af\u6258\u514b\u65af\u77e2\u91cf\u548c\u8868\u9762\u504f\u632f\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u6e32\u67d3\u635f\u5931\u6062\u590d\u8fd9\u4e9b\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u516c\u5171\u6570\u636e\u96c6\u548c\u5b9e\u9645\u6355\u83b7\u56fe\u50cf\u4e0a\u7684\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u504f\u632f\u7269\u7406\u539f\u7406\u548c\u795e\u7ecf\u9690\u5f0f\u8868\u793a\uff0c\u8be5\u65b9\u6cd5\u4e3a\u5149\u6cfd\u7269\u4f53\u76843D\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.07029", "pdf": "https://arxiv.org/pdf/2504.07029", "abs": "https://arxiv.org/abs/2504.07029", "authors": ["Ran Zhang", "Xuanhua He", "Ke Cao", "Liu Liu", "Li Zhang", "Man Zhou", "Jie Zhang"], "title": "Distilling Textual Priors from LLM to Efficient Image Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Multi-modality image fusion aims to synthesize a single, comprehensive image\nfrom multiple source inputs. Traditional approaches, such as CNNs and GANs,\noffer efficiency but struggle to handle low-quality or complex inputs. Recent\nadvances in text-guided methods leverage large model priors to overcome these\nlimitations, but at the cost of significant computational overhead, both in\nmemory and inference time. To address this challenge, we propose a novel\nframework for distilling large model priors, eliminating the need for text\nguidance during inference while dramatically reducing model size. Our framework\nutilizes a teacher-student architecture, where the teacher network incorporates\nlarge model priors and transfers this knowledge to a smaller student network\nvia a tailored distillation process. Additionally, we introduce spatial-channel\ncross-fusion module to enhance the model's ability to leverage textual priors\nacross both spatial and channel dimensions. Our method achieves a favorable\ntrade-off between computational efficiency and fusion quality. The distilled\nnetwork, requiring only 10\\% of the parameters and inference time of the\nteacher network, retains 90\\% of its performance and outperforms existing SOTA\nmethods. Extensive experiments demonstrate the effectiveness of our approach.\nThe implementation will be made publicly available as an open-source resource.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u84b8\u998f\u5927\u578b\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ee5\u5728\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u4e2d\u5b9e\u73b0\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982CNNs\u548cGANs\uff09\u5728\u5904\u7406\u4f4e\u8d28\u91cf\u6216\u590d\u6742\u8f93\u5165\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u57fa\u4e8e\u6587\u672c\u5f15\u5bfc\u7684\u65b9\u6cd5\u867d\u7136\u6709\u6548\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\u3002", "method": "\u91c7\u7528\u6559\u5e08-\u5b66\u751f\u67b6\u6784\uff0c\u901a\u8fc7\u5b9a\u5236\u84b8\u998f\u8fc7\u7a0b\u5c06\u5927\u578b\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u8f6c\u79fb\u5230\u5c0f\u578b\u5b66\u751f\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u7a7a\u95f4-\u901a\u9053\u4ea4\u53c9\u878d\u5408\u6a21\u5757\u589e\u5f3a\u6587\u672c\u5148\u9a8c\u5229\u7528\u3002", "result": "\u84b8\u998f\u540e\u7684\u7f51\u7edc\u4ec5\u9700\u6559\u5e08\u7f51\u7edc10%\u7684\u53c2\u6570\u548c\u63a8\u7406\u65f6\u95f4\uff0c\u4fdd\u755990%\u6027\u80fd\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u878d\u5408\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5c06\u5f00\u6e90\u5b9e\u73b0\u3002"}}
{"id": "2504.07046", "pdf": "https://arxiv.org/pdf/2504.07046", "abs": "https://arxiv.org/abs/2504.07046", "authors": ["Jifang Wang", "Xue Yang", "Longyue Wang", "Zhenran Xu", "Yiyu Wang", "Yaowei Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "title": "A Unified Agentic Framework for Evaluating Conditional Image Generation", "categories": ["cs.CV", "cs.CL"], "comment": "Work in progress. GitHub:\n  https://github.com/HITsz-TMG/Agentic-CIGEval", "summary": "Conditional image generation has gained significant attention for its ability\nto personalize content. However, the field faces challenges in developing\ntask-agnostic, reliable, and explainable evaluation metrics. This paper\nintroduces CIGEval, a unified agentic framework for comprehensive evaluation of\nconditional image generation tasks. CIGEval utilizes large multimodal models\n(LMMs) as its core, integrating a multi-functional toolbox and establishing a\nfine-grained evaluation framework. Additionally, we synthesize evaluation\ntrajectories for fine-tuning, empowering smaller LMMs to autonomously select\nappropriate tools and conduct nuanced analyses based on tool outputs.\nExperiments across seven prominent conditional image generation tasks\ndemonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625\nwith human assessments, closely matching the inter-annotator correlation of\n0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K\ntraining trajectories, CIGEval surpasses the previous GPT-4o-based\nstate-of-the-art method. Case studies on GPT-4o image generation highlight\nCIGEval's capability in identifying subtle issues related to subject\nconsistency and adherence to control guidance, indicating its great potential\nfor automating evaluation of image generation tasks with human-level\nreliability.", "AI": {"task": "\u63d0\u51faCIGEval\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u9886\u57df\u7f3a\u4e4f\u4efb\u52a1\u65e0\u5173\u3001\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6307\u6807\u3002", "method": "\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u4f5c\u4e3a\u6838\u5fc3\uff0c\u6574\u5408\u591a\u529f\u80fd\u5de5\u5177\u7bb1\uff0c\u5efa\u7acb\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u8bc4\u4f30\u8f68\u8ff9\u5fae\u8c03\u5c0f\u578bLMMs\u3002", "result": "CIGEval\uff08GPT-4o\u7248\u672c\uff09\u5728\u4e03\u9879\u4efb\u52a1\u4e2d\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u76f8\u5173\u6027\u8fbe\u52300.4625\uff0c\u63a5\u8fd1\u6807\u6ce8\u8005\u95f4\u76f8\u5173\u60270.47\uff1b\u4f7f\u75287B\u5f00\u6e90LMMs\u548c2.3K\u8bad\u7ec3\u8f68\u8ff9\u65f6\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CIGEval\u5728\u8bc6\u522b\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u7ec6\u5fae\u95ee\u9898\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u5907\u81ea\u52a8\u5316\u8bc4\u4f30\u7684\u6f5c\u529b\uff0c\u53ef\u9760\u6027\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u3002"}}
{"id": "2504.07060", "pdf": "https://arxiv.org/pdf/2504.07060", "abs": "https://arxiv.org/abs/2504.07060", "authors": ["Ruoyu Chen", "Hua Zhang", "Jingzhi Li", "Li Liu", "Zhen Huang", "Xiaochun Cao"], "title": "Generalized Semantic Contrastive Learning via Embedding Side Information for Few-Shot Object Detection", "categories": ["cs.CV"], "comment": "Accepted by T-PAMI (IEEE Transactions on Pattern Analysis and Machine\n  Intelligence)", "summary": "The objective of few-shot object detection (FSOD) is to detect novel objects\nwith few training samples. The core challenge of this task is how to construct\na generalized feature space for novel categories with limited data on the basis\nof the base category space, which could adapt the learned detection model to\nunknown scenarios. However, limited by insufficient samples for novel\ncategories, two issues still exist: (1) the features of the novel category are\neasily implicitly represented by the features of the base category, leading to\ninseparable classifier boundaries, (2) novel categories with fewer data are not\nenough to fully represent the distribution, where the model fine-tuning is\nprone to overfitting. To address these issues, we introduce the side\ninformation to alleviate the negative influences derived from the feature space\nand sample viewpoints and formulate a novel generalized feature representation\nlearning method for FSOD. Specifically, we first utilize embedding side\ninformation to construct a knowledge matrix to quantify the semantic\nrelationship between the base and novel categories. Then, to strengthen the\ndiscrimination between semantically similar categories, we further develop\ncontextual semantic supervised contrastive learning which embeds side\ninformation. Furthermore, to prevent overfitting problems caused by sparse\nsamples, a side-information guided region-aware masked module is introduced to\naugment the diversity of samples, which finds and abandons biased information\nthat discriminates between similar categories via counterfactual explanation,\nand refines the discriminative representation space further. Extensive\nexperiments using ResNet and ViT backbones on PASCAL VOC, MS COCO, LVIS V1,\nFSOD-1K, and FSVOD-500 benchmarks demonstrate that our model outperforms the\nprevious state-of-the-art methods, significantly improving the ability of FSOD\nin most shots/splits.", "AI": {"task": "\u89e3\u51b3\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\uff08FSOD\uff09\u4e2d\u56e0\u6837\u672c\u4e0d\u8db3\u5bfc\u81f4\u7684\u5206\u7c7b\u8fb9\u754c\u4e0d\u6e05\u6670\u548c\u6a21\u578b\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u5728\u5c11\u6837\u672c\u60c5\u51b5\u4e0b\uff0c\u65b0\u7c7b\u522b\u7684\u7279\u5f81\u5bb9\u6613\u88ab\u57fa\u7c7b\u7279\u5f81\u9690\u542b\u8868\u793a\uff0c\u4e14\u6570\u636e\u4e0d\u8db3\u5bfc\u81f4\u6a21\u578b\u5bb9\u6613\u8fc7\u62df\u5408\u3002", "method": "\u5f15\u5165\u8f85\u52a9\u4fe1\u606f\u6784\u5efa\u77e5\u8bc6\u77e9\u9635\u91cf\u5316\u57fa\u7c7b\u4e0e\u65b0\u7c7b\u7684\u8bed\u4e49\u5173\u7cfb\uff0c\u5f00\u53d1\u4e0a\u4e0b\u6587\u8bed\u4e49\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8f85\u52a9\u4fe1\u606f\u7684\u533a\u57df\u611f\u77e5\u63a9\u7801\u6a21\u5757\u589e\u5f3a\u6837\u672c\u591a\u6837\u6027\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u8f85\u52a9\u4fe1\u606f\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.07061", "pdf": "https://arxiv.org/pdf/2504.07061", "abs": "https://arxiv.org/abs/2504.07061", "authors": ["Shi Pan", "Jianan Chen", "Maria Secrier"], "title": "Teaching pathology foundation models to accurately predict gene expression with parameter efficient knowledge transfer", "categories": ["cs.CV"], "comment": null, "summary": "Gene expression profiling provides critical insights into cellular\nheterogeneity, biological processes and disease mechanisms. There has been an\nincreasing interest in computational approaches that can predict gene\nexpression directly from digitalized histopathology images. While image\nfoundation models have shown promise in a variety of pathology downstream\nanalysis, their performances on gene-expression prediction are still limited.\nExplicitly incorporating information from the transcriptomic models can help\nimage models to address domain shift, yet the fine-tuning and alignment of\nfoundation models can be expensive. In the work, we propose Parameter Efficient\nKnowledge trAnsfer (PEKA), a novel framework that leverages Block-Affine\nAdaptation and integrates knowledge distillation and structure alignment losses\nfor cross-modal knowledge transfer. We evaluated PEKA for gene expression\nprediction using multiple spatial transcriptomics datasets (comprising 206,123\nimage tiles with matched gene expression profiles) that encompassed various\ntypes of tissue. PEKA achieved at least 5\\% performance improvement over\nbaseline foundation models while also outperforming alternative\nparameter-efficient fine-tuning strategies. We will release the code, datasets\nand aligned models after peer-review to facilitate broader adoption and further\ndevelopment for parameter efficient model alignment.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aPEKA\u7684\u53c2\u6570\u9ad8\u6548\u77e5\u8bc6\u8f6c\u79fb\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u9884\u6d4b\u57fa\u56e0\u8868\u8fbe\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u56fe\u50cf\u7684\u57fa\u7840\u6a21\u578b\u5728\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u4e14\u8de8\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u7684\u5fae\u8c03\u548c\u5bf9\u9f50\u6210\u672c\u8f83\u9ad8\u3002", "method": "\u7ed3\u5408Block-Affine Adaptation\u3001\u77e5\u8bc6\u84b8\u998f\u548c\u7ed3\u6784\u5bf9\u9f50\u635f\u5931\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u3002", "result": "\u5728\u591a\u4e2a\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u6570\u636e\u96c6\u4e0a\uff0cPEKA\u6bd4\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u63d0\u5347\u81f3\u5c115%\uff0c\u4e14\u4f18\u4e8e\u5176\u4ed6\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\u3002", "conclusion": "PEKA\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u65b9\u6cd5\uff0c\u672a\u6765\u5c06\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2504.07078", "pdf": "https://arxiv.org/pdf/2504.07078", "abs": "https://arxiv.org/abs/2504.07078", "authors": ["Meien Li", "Mark Stamp"], "title": "Detecting AI-generated Artwork", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The high efficiency and quality of artwork generated by Artificial\nIntelligence (AI) has created new concerns and challenges for human artists. In\nparticular, recent improvements in generative AI have made it difficult for\npeople to distinguish between human-generated and AI-generated art. In this\nresearch, we consider the potential utility of various types of Machine\nLearning (ML) and Deep Learning (DL) models in distinguishing AI-generated\nartwork from human-generated artwork. We focus on three challenging artistic\nstyles, namely, baroque, cubism, and expressionism. The learning models we test\nare Logistic Regression (LR), Support Vector Machine (SVM), Multilayer\nPerceptron (MLP), and Convolutional Neural Network (CNN). Our best experimental\nresults yield a multiclass accuracy of 0.8208 over six classes, and an\nimpressive accuracy of 0.9758 for the binary classification problem of\ndistinguishing AI-generated from human-generated art.", "AI": {"task": "\u7814\u7a76\u5982\u4f55\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u533a\u5206AI\u751f\u6210\u4e0e\u4eba\u7c7b\u751f\u6210\u7684\u827a\u672f\u4f5c\u54c1\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u827a\u672f\u7684\u9ad8\u6548\u6027\u548c\u8d28\u91cf\u63d0\u5347\uff0c\u533a\u5206AI\u4e0e\u4eba\u7c7b\u4f5c\u54c1\u53d8\u5f97\u56f0\u96be\uff0c\u8fd9\u5f15\u53d1\u4e86\u65b0\u7684\u6311\u6218\u548c\u5173\u6ce8\u3002", "method": "\u6d4b\u8bd5\u4e86\u903b\u8f91\u56de\u5f52\uff08LR\uff09\u3001\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u3001\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u5df4\u6d1b\u514b\u3001\u7acb\u4f53\u4e3b\u4e49\u548c\u8868\u73b0\u4e3b\u4e49\u4e09\u79cd\u827a\u672f\u98ce\u683c\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u591a\u5206\u7c7b\u95ee\u9898\u51c6\u786e\u7387\u4e3a0.8208\uff0c\u4e8c\u5206\u7c7b\u95ee\u9898\uff08\u533a\u5206AI\u4e0e\u4eba\u7c7b\u4f5c\u54c1\uff09\u51c6\u786e\u7387\u8fbe0.9758\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u533a\u5206AI\u4e0e\u4eba\u7c7b\u827a\u672f\u4f5c\u54c1\u65b9\u9762\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u5c24\u5176\u662fCNN\u5728\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.07083", "pdf": "https://arxiv.org/pdf/2504.07083", "abs": "https://arxiv.org/abs/2504.07083", "authors": ["Mengchen Zhang", "Tong Wu", "Jing Tan", "Ziwei Liu", "Gordon Wetzstein", "Dahua Lin"], "title": "GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography", "categories": ["cs.CV"], "comment": null, "summary": "Camera trajectory design plays a crucial role in video production, serving as\na fundamental tool for conveying directorial intent and enhancing visual\nstorytelling. In cinematography, Directors of Photography meticulously craft\ncamera movements to achieve expressive and intentional framing. However,\nexisting methods for camera trajectory generation remain limited: Traditional\napproaches rely on geometric optimization or handcrafted procedural systems,\nwhile recent learning-based methods often inherit structural biases or lack\ntextual alignment, constraining creative synthesis. In this work, we introduce\nan auto-regressive model inspired by the expertise of Directors of Photography\nto generate artistic and expressive camera trajectories. We first introduce\nDataDoP, a large-scale multi-modal dataset containing 29K real-world shots with\nfree-moving camera trajectories, depth maps, and detailed captions in specific\nmovements, interaction with the scene, and directorial intent. Thanks to the\ncomprehensive and diverse database, we further train an auto-regressive,\ndecoder-only Transformer for high-quality, context-aware camera movement\ngeneration based on text guidance and RGBD inputs, named GenDoP. Extensive\nexperiments demonstrate that compared to existing methods, GenDoP offers better\ncontrollability, finer-grained trajectory adjustments, and higher motion\nstability. We believe our approach establishes a new standard for\nlearning-based cinematography, paving the way for future advancements in camera\ncontrol and filmmaking. Our project website:\nhttps://kszpxxzmc.github.io/GenDoP/.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u5f15\u5bfc\u548cRGBD\u8f93\u5165\u7684\u81ea\u52a8\u56de\u5f52\u6a21\u578b\uff08GenDoP\uff09\uff0c\u7528\u4e8e\u751f\u6210\u827a\u672f\u6027\u548c\u8868\u73b0\u529b\u7684\u76f8\u673a\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u76f8\u673a\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u51e0\u4f55\u4f18\u5316\u6216\u624b\u5de5\u7cfb\u7edf\uff0c\u800c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u5b58\u5728\u7ed3\u6784\u504f\u5dee\u6216\u7f3a\u4e4f\u6587\u672c\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u521b\u610f\u5408\u6210\u3002", "method": "\u9996\u5148\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6DataDoP\uff0c\u5305\u542b29K\u771f\u5b9e\u955c\u5934\uff1b\u968f\u540e\u8bad\u7ec3\u4e86\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u81ea\u52a8\u56de\u5f52\u89e3\u7801\u5668\u6a21\u578bGenDoP\u3002", "result": "GenDoP\u5728\u53ef\u63a7\u6027\u3001\u8f68\u8ff9\u8c03\u6574\u7cbe\u7ec6\u5ea6\u548c\u8fd0\u52a8\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u5b66\u4e60\u7684\u7535\u5f71\u6444\u5f71\u8bbe\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u63a8\u52a8\u4e86\u76f8\u673a\u63a7\u5236\u548c\u7535\u5f71\u5236\u4f5c\u7684\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2504.07089", "pdf": "https://arxiv.org/pdf/2504.07089", "abs": "https://arxiv.org/abs/2504.07089", "authors": ["Yiting Lu", "Jiakang Yuan", "Zhen Li", "Shitian Zhao", "Qi Qin", "Xinyue Li", "Le Zhuo", "Licheng Wen", "Dongyang Liu", "Yuewen Cao", "Xiangchao Yan", "Xin Li", "Botian Shi", "Tao Chen", "Zhibo Chen", "Lei Bai", "Bo Zhang", "Peng Gao"], "title": "OmniCaptioner: One Captioner to Rule Them All", "categories": ["cs.CV", "cs.CL"], "comment": "More visualizations on Homepage:\n  https://alpha-innovator.github.io/OmniCaptioner-project-page and Official\n  code: https://github.com/Alpha-Innovator/OmniCaptioner", "summary": "We propose OmniCaptioner, a versatile visual captioning framework for\ngenerating fine-grained textual descriptions across a wide variety of visual\ndomains. Unlike prior methods limited to specific image types (e.g., natural\nimages or geometric visuals), our framework provides a unified solution for\ncaptioning natural images, visual text (e.g., posters, UIs, textbooks), and\nstructured visuals (e.g., documents, tables, charts). By converting low-level\npixel information into semantically rich textual representations, our framework\nbridges the gap between visual and textual modalities. Our results highlight\nthree key advantages: (i) Enhanced Visual Reasoning with LLMs, where\nlong-context captions of visual modalities empower LLMs, particularly the\nDeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii)\nImproved Image Generation, where detailed captions improve tasks like\ntext-to-image generation and image transformation; and (iii) Efficient\nSupervised Fine-Tuning (SFT), which enables faster convergence with less data.\nWe believe the versatility and adaptability of OmniCaptioner can offer a new\nperspective for bridging the gap between language and visual modalities.", "AI": {"task": "\u63d0\u51faOmniCaptioner\uff0c\u4e00\u4e2a\u901a\u7528\u7684\u89c6\u89c9\u63cf\u8ff0\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u591a\u79cd\u89c6\u89c9\u9886\u57df\u751f\u6210\u7ec6\u7c92\u5ea6\u7684\u6587\u672c\u63cf\u8ff0\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u56fe\u50cf\u7c7b\u578b\uff08\u5982\u81ea\u7136\u56fe\u50cf\u6216\u51e0\u4f55\u89c6\u89c9\uff09\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u4f4e\u5c42\u6b21\u50cf\u7d20\u4fe1\u606f\u8f6c\u6362\u4e3a\u8bed\u4e49\u4e30\u5bcc\u7684\u6587\u672c\u8868\u793a\uff0c\u5f25\u5408\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u5c55\u793a\u4e86\u4e09\u4e2a\u5173\u952e\u4f18\u52bf\uff1a\u589e\u5f3a\u89c6\u89c9\u63a8\u7406\u3001\u6539\u8fdb\u56fe\u50cf\u751f\u6210\u548c\u9ad8\u6548\u76d1\u7763\u5fae\u8c03\u3002", "conclusion": "OmniCaptioner\u7684\u901a\u7528\u6027\u548c\u9002\u5e94\u6027\u4e3a\u5f25\u5408\u8bed\u8a00\u4e0e\u89c6\u89c9\u6a21\u6001\u4e4b\u95f4\u7684\u5dee\u8ddd\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2504.07092", "pdf": "https://arxiv.org/pdf/2504.07092", "abs": "https://arxiv.org/abs/2504.07092", "authors": ["Alexander Rubinstein", "Ameya Prabhu", "Matthias Bethge", "Seong Joon Oh"], "title": "Are We Done with Object-Centric Learning?", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Object-centric learning (OCL) seeks to learn representations that only encode\nan object, isolated from other objects or background cues in a scene. This\napproach underpins various aims, including out-of-distribution (OOD)\ngeneralization, sample-efficient composition, and modeling of structured\nenvironments. Most research has focused on developing unsupervised mechanisms\nthat separate objects into discrete slots in the representation space,\nevaluated using unsupervised object discovery. However, with recent\nsample-efficient segmentation models, we can separate objects in the pixel\nspace and encode them independently. This achieves remarkable zero-shot\nperformance on OOD object discovery benchmarks, is scalable to foundation\nmodels, and can handle a variable number of slots out-of-the-box. Hence, the\ngoal of OCL methods to obtain object-centric representations has been largely\nachieved. Despite this progress, a key question remains: How does the ability\nto separate objects within a scene contribute to broader OCL objectives, such\nas OOD generalization? We address this by investigating the OOD generalization\nchallenge caused by spurious background cues through the lens of OCL. We\npropose a novel, training-free probe called $\\textbf{Object-Centric\nClassification with Applied Masks (OCCAM)}$, demonstrating that\nsegmentation-based encoding of individual objects significantly outperforms\nslot-based OCL methods. However, challenges in real-world applications remain.\nWe provide the toolbox for the OCL community to use scalable object-centric\nrepresentations, and focus on practical applications and fundamental questions,\nsuch as understanding object perception in human cognition. Our code is\navailable $\\href{https://github.com/AlexanderRubinstein/OCCAM}{here}$.", "AI": {"task": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u5b66\u4e60\uff08OCL\uff09\u5b9e\u73b0\u5bf9\u8c61\u5206\u79bb\u53ca\u5176\u5bf9\u5206\u5e03\u5916\uff08OOD\uff09\u6cdb\u5316\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u5bf9\u8c61\u5206\u79bb\u80fd\u529b\u5bf9OCL\u76ee\u6807\uff08\u5982OOD\u6cdb\u5316\uff09\u7684\u8d21\u732e\uff0c\u5e76\u89e3\u51b3\u7531\u865a\u5047\u80cc\u666f\u7ebf\u7d22\u5f15\u8d77\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5OCCAM\uff0c\u57fa\u4e8e\u5206\u5272\u7684\u72ec\u7acb\u5bf9\u8c61\u7f16\u7801\uff0c\u5e76\u4e0e\u57fa\u4e8e\u69fd\u7684OCL\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5206\u5272\u7f16\u7801\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u69fd\u57faOCL\u65b9\u6cd5\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u5b58\u5728\u6311\u6218\u3002", "conclusion": "\u4e3aOCL\u793e\u533a\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5bf9\u8c61\u4e2d\u5fc3\u8868\u793a\u5de5\u5177\uff0c\u5e76\u5173\u6ce8\u5b9e\u9645\u5e94\u7528\u548c\u57fa\u7840\u95ee\u9898\uff08\u5982\u4eba\u7c7b\u8ba4\u77e5\u4e2d\u7684\u5bf9\u8c61\u611f\u77e5\uff09\u3002"}}
{"id": "2504.07093", "pdf": "https://arxiv.org/pdf/2504.07093", "abs": "https://arxiv.org/abs/2504.07093", "authors": ["Gene Chou", "Wenqi Xian", "Guandao Yang", "Mohamed Abdelfattah", "Bharath Hariharan", "Noah Snavely", "Ning Yu", "Paul Debevec"], "title": "FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution", "categories": ["cs.CV"], "comment": null, "summary": "A versatile video depth estimation model should (1) be accurate and\nconsistent across frames, (2) produce high-resolution depth maps, and (3)\nsupport real-time streaming. We propose FlashDepth, a method that satisfies all\nthree requirements, performing depth estimation on a 2044x1148 streaming video\nat 24 FPS. We show that, with careful modifications to pretrained single-image\ndepth models, these capabilities are enabled with relatively little data and\ntraining. We evaluate our approach across multiple unseen datasets against\nstate-of-the-art depth models, and find that ours outperforms them in terms of\nboundary sharpness and speed by a significant margin, while maintaining\ncompetitive accuracy. We hope our model will enable various applications that\nrequire high-resolution depth, such as video editing, and online\ndecision-making, such as robotics.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u5206\u8fa8\u7387\u4e14\u652f\u6301\u5b9e\u65f6\u89c6\u9891\u6d41\u7684\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578bFlashDepth\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u5728\u89c6\u9891\u6d41\u5904\u7406\u4e2d\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u5206\u8fa8\u7387\u548c\u5b9e\u65f6\u6027\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u5bf9\u9884\u8bad\u7ec3\u7684\u5355\u56fe\u50cf\u6df1\u5ea6\u6a21\u578b\u8fdb\u884c\u6539\u8fdb\uff0c\u7ed3\u5408\u5c11\u91cf\u6570\u636e\u548c\u8bad\u7ec3\uff0c\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u672a\u89c1\u6570\u636e\u96c6\u4e0a\uff0cFlashDepth\u5728\u8fb9\u754c\u6e05\u6670\u5ea6\u548c\u901f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7cbe\u5ea6\u3002", "conclusion": "FlashDepth\u4e3a\u89c6\u9891\u7f16\u8f91\u548c\u673a\u5668\u4eba\u7b49\u9700\u8981\u9ad8\u5206\u8fa8\u7387\u6df1\u5ea6\u4fe1\u606f\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2311.12047", "pdf": "https://arxiv.org/pdf/2311.12047", "abs": "https://arxiv.org/abs/2311.12047", "authors": ["Jiali Cheng", "Hadi Amiri"], "title": "MultiDelete for Multimodal Machine Unlearning", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "ECCV 2024", "summary": "Machine Unlearning removes specific knowledge about training data samples\nfrom an already trained model. It has significant practical benefits, such as\npurging private, inaccurate, or outdated information from trained models\nwithout the need for complete re-training. Unlearning within a multimodal\nsetting presents unique challenges due to the complex dependencies between\ndifferent data modalities and the expensive cost of training on large\nmultimodal datasets and architectures. This paper presents the first machine\nunlearning approach for multimodal data and models, titled MultiDelete, which\nis designed to decouple associations between unimodal data points during\nunlearning without losing the overall representation strength of the trained\nmodel. MultiDelete advocates for three key properties for effective multimodal\nunlearning: (a): modality decoupling, which effectively decouples the\nassociation between individual unimodal data points marked for deletion,\nrendering them as unrelated data points, (b): multimodal knowledge retention,\nwhich retains the multimodal representation post-unlearning, and (c): unimodal\nknowledge retention, which retains the unimodal representation postunlearning.\nMultiDelete is efficient to train and is not constrained by using a strongly\nconvex loss -- a common restriction among existing baselines. Experiments on\ntwo architectures and four datasets, including image-text and graph-text\ndatasets, show that MultiDelete gains an average improvement of 17.6 points\nover best performing baseline in unlearning multimodal samples, can maintain\nthe multimodal and unimodal knowledge of the original model post unlearning,\nand can provide better protection to unlearned data against adversarial\nattacks.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aMultiDelete\u7684\u591a\u6a21\u6001\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5df2\u8bad\u7ec3\u6a21\u578b\u4e2d\u79fb\u9664\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u6837\u672c\u7684\u77e5\u8bc6\u3002", "motivation": "\u591a\u6a21\u6001\u73af\u5883\u4e0b\u7684\u673a\u5668\u9057\u5fd8\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5982\u6570\u636e\u6a21\u6001\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u6027\u548c\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u7684\u9ad8\u8bad\u7ec3\u6210\u672c\u3002", "method": "MultiDelete\u901a\u8fc7\u6a21\u6001\u89e3\u8026\u3001\u591a\u6a21\u6001\u77e5\u8bc6\u4fdd\u7559\u548c\u5355\u6a21\u6001\u77e5\u8bc6\u4fdd\u7559\u4e09\u4e2a\u5173\u952e\u7279\u6027\uff0c\u6709\u6548\u89e3\u8026\u5f85\u5220\u9664\u5355\u6a21\u6001\u6570\u636e\u70b9\u4e4b\u95f4\u7684\u5173\u8054\u3002", "result": "\u5728\u4e24\u79cd\u67b6\u6784\u548c\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMultiDelete\u5728\u591a\u6a21\u6001\u6837\u672c\u9057\u5fd8\u4e0a\u5e73\u5747\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u9ad817.6\u5206\uff0c\u5e76\u80fd\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u7684\u591a\u6a21\u6001\u548c\u5355\u6a21\u6001\u77e5\u8bc6\u3002", "conclusion": "MultiDelete\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u4fdd\u62a4\u9057\u5fd8\u6570\u636e\u514d\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u7559\u6a21\u578b\u7684\u77e5\u8bc6\u8868\u793a\u3002"}}
{"id": "2504.06299", "pdf": "https://arxiv.org/pdf/2504.06299", "abs": "https://arxiv.org/abs/2504.06299", "authors": ["Jonas Br\u00e4ndli", "Maurice Schneeberger", "Lisa Herzog", "Loran Avci", "Nordin Dari", "Martin H\u00e4ansel", "Hakim Baazaoui", "Pascal B\u00fchler", "Susanne Wegener", "Beate Sick"], "title": "Going beyond explainability in multi-modal stroke outcome prediction models", "categories": ["eess.IV", "cs.CV", "cs.LG", "stat.AP"], "comment": null, "summary": "Aim: This study aims to enhance interpretability and explainability of\nmulti-modal prediction models integrating imaging and tabular patient data.\n  Methods: We adapt the xAI methods Grad-CAM and Occlusion to multi-modal,\npartly interpretable deep transformation models (dTMs). DTMs combine\nstatistical and deep learning approaches to simultaneously achieve\nstate-of-the-art prediction performance and interpretable parameter estimates,\nsuch as odds ratios for tabular features. Based on brain imaging and tabular\ndata from 407 stroke patients, we trained dTMs to predict functional outcome\nthree months after stroke. We evaluated the models using different\ndiscriminatory metrics. The adapted xAI methods were used to generated\nexplanation maps for identification of relevant image features and error\nanalysis.\n  Results: The dTMs achieve state-of-the-art prediction performance, with area\nunder the curve (AUC) values close to 0.8. The most important tabular\npredictors of functional outcome are functional independence before stroke and\nNIHSS on admission, a neurological score indicating stroke severity.\nExplanation maps calculated from brain imaging dTMs for functional outcome\nhighlighted critical brain regions such as the frontal lobe, which is known to\nbe linked to age which in turn increases the risk for unfavorable outcomes.\nSimilarity plots of the explanation maps revealed distinct patterns which give\ninsight into stroke pathophysiology, support developing novel predictors of\nstroke outcome and enable to identify false predictions.\n  Conclusion: By adapting methods for explanation maps to dTMs, we enhanced the\nexplainability of multi-modal and partly interpretable prediction models. The\nresulting explanation maps facilitate error analysis and support hypothesis\ngeneration regarding the significance of specific image regions in outcome\nprediction.", "AI": {"task": "\u589e\u5f3a\u591a\u6a21\u6001\u9884\u6d4b\u6a21\u578b\uff08\u7ed3\u5408\u5f71\u50cf\u548c\u8868\u683c\u60a3\u8005\u6570\u636e\uff09\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u7edf\u8ba1\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u9884\u6d4b\u548c\u53ef\u89e3\u91ca\u53c2\u6570\u4f30\u8ba1\uff0c\u540c\u65f6\u5229\u7528xAI\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528Grad-CAM\u548cOcclusion\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df1\u5ea6\u8f6c\u6362\u6a21\u578b\uff08dTMs\uff09\uff0c\u57fa\u4e8e407\u540d\u4e2d\u98ce\u60a3\u8005\u7684\u5f71\u50cf\u548c\u8868\u683c\u6570\u636e\u8bad\u7ec3\u6a21\u578b\uff0c\u9884\u6d4b\u4e2d\u98ce\u540e\u4e09\u4e2a\u6708\u7684\u529f\u80fd\u7ed3\u679c\u3002", "result": "dTMs\u5b9e\u73b0\u4e86\u63a5\u8fd10.8\u7684AUC\u503c\uff0c\u5173\u952e\u8868\u683c\u9884\u6d4b\u56e0\u5b50\u4e3a\u4e2d\u98ce\u524d\u7684\u529f\u80fd\u72ec\u7acb\u6027\u548c\u5165\u9662\u65f6\u7684NIHSS\u8bc4\u5206\uff1b\u5f71\u50cf\u89e3\u91ca\u56fe\u7a81\u51fa\u4e86\u4e0e\u4e0d\u826f\u7ed3\u679c\u76f8\u5173\u7684\u5173\u952e\u8111\u533a\uff08\u5982\u989d\u53f6\uff09\u3002", "conclusion": "\u901a\u8fc7\u5c06\u89e3\u91ca\u56fe\u65b9\u6cd5\u5e94\u7528\u4e8edTMs\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u652f\u6301\u9519\u8bef\u5206\u6790\u548c\u5047\u8bbe\u751f\u6210\u3002"}}
{"id": "2504.06301", "pdf": "https://arxiv.org/pdf/2504.06301", "abs": "https://arxiv.org/abs/2504.06301", "authors": ["Mohsen Jenadeleh", "Jon Sneyers", "Panqi Jia", "Shima Mohammadi", "Joao Ascenso", "Dietmar Saupe"], "title": "Subjective Visual Quality Assessment for High-Fidelity Learning-Based Image Compression", "categories": ["eess.IV", "cs.CV"], "comment": "7 pages, 5 figures, 3 tables, submitted to QoMEX 2025", "summary": "Learning-based image compression methods have recently emerged as promising\nalternatives to traditional codecs, offering improved rate-distortion\nperformance and perceptual quality. JPEG AI represents the latest standardized\nframework in this domain, leveraging deep neural networks for high-fidelity\nimage reconstruction. In this study, we present a comprehensive subjective\nvisual quality assessment of JPEG AI-compressed images using the JPEG AIC-3\nmethodology, which quantifies perceptual differences in terms of Just\nNoticeable Difference (JND) units. We generated a dataset of 50 compressed\nimages with fine-grained distortion levels from five diverse sources. A\nlarge-scale crowdsourced experiment collected 96,200 triplet responses from 459\nparticipants. We reconstructed JND-based quality scales using a unified model\nbased on boosted and plain triplet comparisons. Additionally, we evaluated the\nalignment of multiple objective image quality metrics with human perception in\nthe high-fidelity range. The CVVDP metric achieved the overall highest\nperformance; however, most metrics including CVVDP were overly optimistic in\npredicting the quality of JPEG AI-compressed images. These findings emphasize\nthe necessity for rigorous subjective evaluations in the development and\nbenchmarking of modern image codecs, particularly in the high-fidelity range.\nAnother technical contribution is the introduction of the well-known\nMeng-Rosenthal-Rubin statistical test to the field of Quality of Experience\nresearch. This test can reliably assess the significance of difference in\nperformance of quality metrics in terms of correlation between metrics and\nground truth. The complete dataset, including all subjective scores, is\npublicly available at https://github.com/jpeg-aic/dataset-JPEG-AI-SDR25.", "AI": {"task": "\u5bf9JPEG AI\u538b\u7f29\u56fe\u50cf\u8fdb\u884c\u4e3b\u89c2\u89c6\u89c9\u8d28\u91cf\u8bc4\u4f30\uff0c\u5e76\u4f7f\u7528JPEG AIC-3\u65b9\u6cd5\u91cf\u5316\u611f\u77e5\u5dee\u5f02\u3002", "motivation": "\u57fa\u4e8e\u5b66\u4e60\u7684\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u611f\u77e5\u8d28\u91cf\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7f16\u89e3\u7801\u5668\uff0cJPEG AI\u662f\u8fd9\u4e00\u9886\u57df\u7684\u6700\u65b0\u6807\u51c6\u5316\u6846\u67b6\uff0c\u9700\u8981\u5bf9\u5176\u538b\u7f29\u56fe\u50cf\u7684\u8d28\u91cf\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\u3002", "method": "\u751f\u621050\u5f20\u538b\u7f29\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u4f17\u5305\u5b9e\u9a8c\uff0c\u6536\u96c696,200\u7ec4\u4e09\u5143\u7ec4\u54cd\u5e94\uff0c\u5e76\u4f7f\u7528\u7edf\u4e00\u6a21\u578b\u91cd\u5efa\u57fa\u4e8eJND\u7684\u8d28\u91cf\u5c3a\u5ea6\u3002", "result": "CVVDP\u6307\u6807\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u5927\u591a\u6570\u6307\u6807\u5bf9JPEG AI\u538b\u7f29\u56fe\u50cf\u7684\u8d28\u91cf\u9884\u6d4b\u8fc7\u4e8e\u4e50\u89c2\u3002", "conclusion": "\u5f3a\u8c03\u5728\u9ad8\u4fdd\u771f\u8303\u56f4\u5185\u5bf9\u73b0\u4ee3\u56fe\u50cf\u7f16\u89e3\u7801\u5668\u8fdb\u884c\u4e25\u683c\u4e3b\u89c2\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u5f15\u5165Meng-Rosenthal-Rubin\u7edf\u8ba1\u6d4b\u8bd5\u7528\u4e8e\u8d28\u91cf\u4f53\u9a8c\u7814\u7a76\u3002"}}
{"id": "2504.06304", "pdf": "https://arxiv.org/pdf/2504.06304", "abs": "https://arxiv.org/abs/2504.06304", "authors": ["Matvei Popov", "Aymen Kallala", "Anirudha Ramesh", "Narimane Hennouni", "Shivesh Khaitan", "Rick Gentry", "Alain-Sam Cohen"], "title": "Leveraging State Space Models in Long Range Genomics", "categories": ["q-bio.GN", "cs.CV", "cs.LG"], "comment": "Accepted at ICLR 2025 (LMRL) - Project page:\n  https://anirudharamesh.github.io/iclr-long-range-genomics/", "summary": "Long-range dependencies are critical for understanding genomic structure and\nfunction, yet most conventional methods struggle with them. Widely adopted\ntransformer-based models, while excelling at short-context tasks, are limited\nby the attention module's quadratic computational complexity and inability to\nextrapolate to sequences longer than those seen in training. In this work, we\nexplore State Space Models (SSMs) as a promising alternative by benchmarking\ntwo SSM-inspired architectures, Caduceus and Hawk, on long-range genomics\nmodeling tasks under conditions parallel to a 50M parameter transformer\nbaseline. We discover that SSMs match transformer performance and exhibit\nimpressive zero-shot extrapolation across multiple tasks, handling contexts 10\nto 100 times longer than those seen during training, indicating more\ngeneralizable representations better suited for modeling the long and complex\nhuman genome. Moreover, we demonstrate that these models can efficiently\nprocess sequences of 1M tokens on a single GPU, allowing for modeling entire\ngenomic regions at once, even in labs with limited compute. Our findings\nestablish SSMs as efficient and scalable for long-context genomic analysis.", "AI": {"task": "\u63a2\u7d22\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u5728\u957f\u8ddd\u79bb\u57fa\u56e0\u7ec4\u5efa\u6a21\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u800cTransformer\u6a21\u578b\u7531\u4e8e\u8ba1\u7b97\u590d\u6742\u6027\u548c\u65e0\u6cd5\u5916\u63a8\u957f\u5e8f\u5217\u800c\u53d7\u9650\u3002", "method": "\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u4e24\u79cdSSM\u67b6\u6784\uff08Caduceus\u548cHawk\uff09\uff0c\u572850M\u53c2\u6570Transformer\u57fa\u7ebf\u7684\u6761\u4ef6\u4e0b\u8fdb\u884c\u957f\u8ddd\u79bb\u57fa\u56e0\u7ec4\u5efa\u6a21\u4efb\u52a1\u3002", "result": "SSMs\u5728\u6027\u80fd\u4e0a\u4e0eTransformer\u76f8\u5f53\uff0c\u5e76\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u96f6\u6837\u672c\u5916\u63a8\u80fd\u529b\uff0c\u80fd\u5904\u7406\u6bd4\u8bad\u7ec3\u65f6\u957f10\u5230100\u500d\u7684\u5e8f\u5217\u3002", "conclusion": "SSMs\u5728\u957f\u8ddd\u79bb\u57fa\u56e0\u7ec4\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u5408\u5904\u7406\u590d\u6742\u7684\u4eba\u7c7b\u57fa\u56e0\u7ec4\u3002"}}
{"id": "2504.06385", "pdf": "https://arxiv.org/pdf/2504.06385", "abs": "https://arxiv.org/abs/2504.06385", "authors": ["Paul Roetzer", "Florian Bernard"], "title": "Fast Globally Optimal and Geometrically Consistent 3D Shape Matching", "categories": ["cs.GR", "cs.CV"], "comment": "8 pages main paper", "summary": "Geometric consistency, i.e. the preservation of neighbourhoods, is a natural\nand strong prior in 3D shape matching. Geometrically consistent matchings are\ncrucial for many downstream applications, such as texture transfer or\nstatistical shape modelling. Yet, in practice, geometric consistency is often\noverlooked, or only achieved under severely limiting assumptions (e.g. a good\ninitialisation). In this work, we propose a novel formalism for computing\nglobally optimal and geometrically consistent matchings between 3D shapes which\nis scalable in practice. Our key idea is to represent the surface of the source\nshape as a collection of cyclic paths, which are then consistently matched to\nthe target shape. Mathematically, we construct a hyper product graph (between\nsource and target shape), and then cast 3D shape matching as a minimum-cost\ncirculation flow problem in this hyper graph, which yields global geometrically\nconsistent matchings between both shapes. We empirically show that our\nformalism is efficiently solvable and that it leads to high-quality results.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u8ba1\u7b97\u5168\u5c40\u6700\u4f18\u4e14\u51e0\u4f55\u4e00\u81f4\u7684\u4e09\u7ef4\u5f62\u72b6\u5339\u914d\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u51e0\u4f55\u4e00\u81f4\u6027\uff08\u5373\u90bb\u57df\u7684\u4fdd\u6301\uff09\u662f\u4e09\u7ef4\u5f62\u72b6\u5339\u914d\u4e2d\u7684\u91cd\u8981\u5148\u9a8c\uff0c\u4f46\u5b9e\u8df5\u4e2d\u5e38\u88ab\u5ffd\u89c6\u6216\u4ec5\u80fd\u5728\u4e25\u683c\u5047\u8bbe\u4e0b\u5b9e\u73b0\u3002", "method": "\u5c06\u6e90\u5f62\u72b6\u8868\u9762\u8868\u793a\u4e3a\u5faa\u73af\u8def\u5f84\u96c6\u5408\uff0c\u5e76\u5728\u8d85\u4e58\u79ef\u56fe\u4e2d\u5c06\u5176\u4e0e\u76ee\u6807\u5f62\u72b6\u5339\u914d\uff0c\u8f6c\u5316\u4e3a\u6700\u5c0f\u6210\u672c\u5faa\u73af\u6d41\u95ee\u9898\u3002", "result": "\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u6709\u9ad8\u6548\u53ef\u89e3\u6027\uff0c\u5e76\u80fd\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u5339\u914d\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e09\u7ef4\u5f62\u72b6\u5339\u914d\u63d0\u4f9b\u4e86\u4e00\u79cd\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.06407", "pdf": "https://arxiv.org/pdf/2504.06407", "abs": "https://arxiv.org/abs/2504.06407", "authors": ["Jiali Cheng", "Hadi Amiri"], "title": "Understanding Machine Unlearning Through the Lens of Mode Connectivity", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Machine Unlearning aims to remove undesired information from trained models\nwithout requiring full retraining from scratch. Despite recent advancements,\ntheir underlying loss landscapes and optimization dynamics received less\nattention. In this paper, we investigate and analyze machine unlearning through\nthe lens of mode connectivity - the phenomenon where independently trained\nmodels can be connected by smooth low-loss paths in the parameter space. We\ndefine and study mode connectivity in unlearning across a range of overlooked\nconditions, including connections between different unlearning methods, models\ntrained with and without curriculum learning, and models optimized with\nfirst-order and secondorder techniques. Our findings show distinct patterns of\nfluctuation of different evaluation metrics along the curve, as well as the\nmechanistic (dis)similarity between unlearning methods. To the best of our\nknowledge, this is the first study on mode connectivity in the context of\nmachine unlearning.", "AI": {"task": "\u7814\u7a76\u673a\u5668\u9057\u5fd8\u4e2d\u7684\u6a21\u5f0f\u8fde\u901a\u6027\u53ca\u5176\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u9057\u5fd8\u8fc7\u7a0b\u4e2d\u635f\u5931\u666f\u89c2\u548c\u4f18\u5316\u52a8\u6001\u7684\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u65b9\u9762\uff0c\u7279\u522b\u662f\u6a21\u5f0f\u8fde\u901a\u6027\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u9057\u5fd8\u65b9\u6cd5\u3001\u8bfe\u7a0b\u5b66\u4e60\u6a21\u578b\u4ee5\u53ca\u4e00\u9636\u548c\u4e8c\u9636\u4f18\u5316\u6280\u672f\u4e0b\u7684\u6a21\u5f0f\u8fde\u901a\u6027\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u8bc4\u4f30\u6307\u6807\u6cbf\u66f2\u7ebf\u7684\u6ce2\u52a8\u6a21\u5f0f\u4ee5\u53ca\u9057\u5fd8\u65b9\u6cd5\u4e4b\u95f4\u7684\u673a\u5236\uff08\u4e0d\uff09\u76f8\u4f3c\u6027\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5728\u673a\u5668\u9057\u5fd8\u80cc\u666f\u4e0b\u7814\u7a76\u6a21\u5f0f\u8fde\u901a\u6027\uff0c\u63ed\u793a\u4e86\u5176\u72ec\u7279\u7684\u884c\u4e3a\u548c\u6f5c\u5728\u673a\u5236\u3002"}}
{"id": "2504.06410", "pdf": "https://arxiv.org/pdf/2504.06410", "abs": "https://arxiv.org/abs/2504.06410", "authors": ["Huzaifa Arif", "Keerthiram Murugesan", "Payel Das", "Alex Gittens", "Pin-Yu Chen"], "title": "PEEL the Layers and Find Yourself: Revisiting Inference-time Data Leakage for Residual Neural Networks", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": null, "summary": "This paper explores inference-time data leakage risks of deep neural networks\n(NNs), where a curious and honest model service provider is interested in\nretrieving users' private data inputs solely based on the model inference\nresults. Particularly, we revisit residual NNs due to their popularity in\ncomputer vision and our hypothesis that residual blocks are a primary cause of\ndata leakage owing to the use of skip connections. By formulating\ninference-time data leakage as a constrained optimization problem, we propose a\nnovel backward feature inversion method, \\textbf{PEEL}, which can effectively\nrecover block-wise input features from the intermediate output of residual NNs.\nThe surprising results in high-quality input data recovery can be explained by\nthe intuition that the output from these residual blocks can be considered as a\nnoisy version of the input and thus the output retains sufficient information\nfor input recovery. We demonstrate the effectiveness of our layer-by-layer\nfeature inversion method on facial image datasets and pre-trained classifiers.\nOur results show that PEEL outperforms the state-of-the-art recovery methods by\nan order of magnitude when evaluated by mean squared error (MSE). The code is\navailable at\n\\href{https://github.com/Huzaifa-Arif/PEEL}{https://github.com/Huzaifa-Arif/PEEL}", "AI": {"task": "\u63a2\u7d22\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08NNs\uff09\u5728\u63a8\u7406\u65f6\u7684\u6570\u636e\u6cc4\u6f0f\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u53cd\u5411\u7279\u5f81\u53cd\u8f6c\u65b9\u6cd5PEEL\u6765\u6062\u590d\u6b8b\u5dee\u7f51\u7edc\u7684\u8f93\u5165\u6570\u636e\u3002", "motivation": "\u7814\u7a76\u6a21\u578b\u670d\u52a1\u63d0\u4f9b\u5546\u4ec5\u57fa\u4e8e\u6a21\u578b\u63a8\u7406\u7ed3\u679c\u68c0\u7d22\u7528\u6237\u79c1\u6709\u6570\u636e\u7684\u98ce\u9669\uff0c\u7279\u522b\u662f\u6b8b\u5dee\u7f51\u7edc\u56e0\u5176\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u548c\u8df3\u8dc3\u8fde\u63a5\u53ef\u80fd\u5bfc\u81f4\u6570\u636e\u6cc4\u6f0f\u3002", "method": "\u5c06\u63a8\u7406\u65f6\u6570\u636e\u6cc4\u6f0f\u95ee\u9898\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51faPEEL\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u5c42\u7279\u5f81\u53cd\u8f6c\u6062\u590d\u6b8b\u5dee\u7f51\u7edc\u7684\u8f93\u5165\u7279\u5f81\u3002", "result": "PEEL\u5728\u9762\u90e8\u56fe\u50cf\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u5206\u7c7b\u5668\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5176\u6062\u590d\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08MSE\u6307\u6807\u63d0\u5347\u4e00\u4e2a\u6570\u91cf\u7ea7\uff09\u3002", "conclusion": "\u6b8b\u5dee\u7f51\u7edc\u7684\u4e2d\u95f4\u8f93\u51fa\u4fdd\u7559\u4e86\u8db3\u591f\u7684\u4fe1\u606f\u7528\u4e8e\u8f93\u5165\u6062\u590d\uff0cPEEL\u65b9\u6cd5\u5728\u6570\u636e\u6cc4\u6f0f\u98ce\u9669\u7814\u7a76\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2504.06422", "pdf": "https://arxiv.org/pdf/2504.06422", "abs": "https://arxiv.org/abs/2504.06422", "authors": ["Adam McArthur", "Stephanie Wichuk", "Stephen Burnside", "Andrew Kirby", "Alexander Scammon", "Damian Sol", "Abhilash Hareendranathan", "Jacob L. Jaremko"], "title": "Retuve: Automated Multi-Modality Analysis of Hip Dysplasia with Open Source AI", "categories": ["eess.IV", "cs.CV"], "comment": "12 pages, 8 figures, submitted to Software Impacts", "summary": "Developmental dysplasia of the hip (DDH) poses significant diagnostic\nchallenges, hindering timely intervention. Current screening methodologies lack\nstandardization, and AI-driven studies suffer from reproducibility issues due\nto limited data and code availability. To address these limitations, we\nintroduce Retuve, an open-source framework for multi-modality DDH analysis,\nencompassing both ultrasound (US) and X-ray imaging. Retuve provides a complete\nand reproducible workflow, offering open datasets comprising expert-annotated\nUS and X-ray images, pre-trained models with training code and weights, and a\nuser-friendly Python Application Programming Interface (API). The framework\nintegrates segmentation and landmark detection models, enabling automated\nmeasurement of key diagnostic parameters such as the alpha angle and acetabular\nindex. By adhering to open-source principles, Retuve promotes transparency,\ncollaboration, and accessibility in DDH research. This initiative has the\npotential to democratize DDH screening, facilitate early diagnosis, and\nultimately improve patient outcomes by enabling widespread screening and early\nintervention. The GitHub repository/code can be found here:\nhttps://github.com/radoss-org/retuve", "AI": {"task": "\u5f00\u53d1\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6Retuve\uff0c\u7528\u4e8e\u591a\u6a21\u6001\uff08\u8d85\u58f0\u548cX\u5c04\u7ebf\uff09\u7684\u53d1\u80b2\u6027\u9acb\u5173\u8282\u53d1\u80b2\u4e0d\u826f\uff08DDH\uff09\u5206\u6790\u3002", "motivation": "\u5f53\u524dDDH\u8bca\u65ad\u65b9\u6cd5\u7f3a\u4e4f\u6807\u51c6\u5316\uff0cAI\u7814\u7a76\u56e0\u6570\u636e\u548c\u4ee3\u7801\u53ef\u7528\u6027\u4e0d\u8db3\u800c\u5b58\u5728\u53ef\u91cd\u590d\u6027\u95ee\u9898\u3002", "method": "Retuve\u63d0\u4f9b\u5b8c\u6574\u4e14\u53ef\u91cd\u590d\u7684\u5de5\u4f5c\u6d41\uff0c\u5305\u62ec\u5f00\u653e\u6570\u636e\u96c6\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u8bad\u7ec3\u4ee3\u7801\u548c\u6743\u91cd\uff0c\u4ee5\u53ca\u7528\u6237\u53cb\u597d\u7684Python API\u3002", "result": "\u6846\u67b6\u6574\u5408\u4e86\u5206\u5272\u548c\u6807\u5fd7\u70b9\u68c0\u6d4b\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5173\u952e\u8bca\u65ad\u53c2\u6570\uff08\u5982\u03b1\u89d2\u548c\u9acb\u81fc\u6307\u6570\uff09\u7684\u81ea\u52a8\u5316\u6d4b\u91cf\u3002", "conclusion": "Retuve\u901a\u8fc7\u5f00\u6e90\u539f\u5219\u4fc3\u8fdbDDH\u7814\u7a76\u7684\u900f\u660e\u6027\u3001\u534f\u4f5c\u6027\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u6709\u671b\u666e\u53caDDH\u7b5b\u67e5\u3001\u4fc3\u8fdb\u65e9\u671f\u8bca\u65ad\u5e76\u6539\u5584\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2504.06463", "pdf": "https://arxiv.org/pdf/2504.06463", "abs": "https://arxiv.org/abs/2504.06463", "authors": ["Yashil Sukurdeep", "Fausto Navarro", "Tam\u00e1s Budav\u00e1ri"], "title": "AstroClearNet: Deep image prior for multi-frame astronomical image restoration", "categories": ["astro-ph.IM", "cs.CV"], "comment": null, "summary": "Recovering high-fidelity images of the night sky from blurred observations is\na fundamental problem in astronomy, where traditional methods typically fall\nshort. In ground-based astronomy, combining multiple exposures to enhance\nsignal-to-noise ratios is further complicated by variations in the point-spread\nfunction caused by atmospheric turbulence. In this work, we present a\nself-supervised multi-frame method, based on deep image priors, for denoising,\ndeblurring, and coadding ground-based exposures. Central to our approach is a\ncarefully designed convolutional neural network that integrates information\nacross multiple observations and enforces physically motivated constraints. We\ndemonstrate the method's potential by processing Hyper Suprime-Cam exposures,\nyielding promising preliminary results with sharper restored images.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u7684\u81ea\u76d1\u7763\u591a\u5e27\u65b9\u6cd5\uff0c\u7528\u4e8e\u5730\u9762\u5929\u6587\u89c2\u6d4b\u7684\u53bb\u566a\u3001\u53bb\u6a21\u7cca\u548c\u53e0\u52a0\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u6062\u590d\u6a21\u7cca\u89c2\u6d4b\u7684\u9ad8\u4fdd\u771f\u56fe\u50cf\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14\u5730\u9762\u5929\u6587\u89c2\u6d4b\u4e2d\u591a\u5e27\u53e0\u52a0\u56e0\u5927\u6c14\u6e4d\u6d41\u5bfc\u81f4\u7684\u70b9\u6269\u6563\u51fd\u6570\u53d8\u5316\u800c\u590d\u6742\u5316\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u6574\u5408\u591a\u89c2\u6d4b\u4fe1\u606f\u5e76\u65bd\u52a0\u7269\u7406\u7ea6\u675f\u3002", "result": "\u901a\u8fc7\u5904\u7406Hyper Suprime-Cam\u89c2\u6d4b\u6570\u636e\uff0c\u83b7\u5f97\u4e86\u66f4\u6e05\u6670\u7684\u6062\u590d\u56fe\u50cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5929\u6587\u56fe\u50cf\u5904\u7406\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\u3002"}}
{"id": "2504.06479", "pdf": "https://arxiv.org/pdf/2504.06479", "abs": "https://arxiv.org/abs/2504.06479", "authors": ["Julian Nubert", "Turcan Tuna", "Jonas Frey", "Cesar Cadena", "Katherine J. Kuchenbecker", "Shehryar Khattak", "Marco Hutter"], "title": "Holistic Fusion: Task- and Setup-Agnostic Robot Localization and State Estimation with Factor Graphs", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY"], "comment": "21 pages, 25 figures, 9 tables, journal submission", "summary": "Seamless operation of mobile robots in challenging environments requires\nlow-latency local motion estimation (e.g., dynamic maneuvers) and accurate\nglobal localization (e.g., wayfinding). While most existing sensor-fusion\napproaches are designed for specific scenarios, this work introduces a flexible\nopen-source solution for task- and setup-agnostic multimodal sensor fusion that\nis distinguished by its generality and usability. Holistic Fusion formulates\nsensor fusion as a combined estimation problem of i) the local and global robot\nstate and ii) a (theoretically unlimited) number of dynamic context variables,\nincluding automatic alignment of reference frames; this formulation fits\ncountless real-world applications without any conceptual modifications. The\nproposed factor-graph solution enables the direct fusion of an arbitrary number\nof absolute, local, and landmark measurements expressed with respect to\ndifferent reference frames by explicitly including them as states in the\noptimization and modeling their evolution as random walks. Moreover, local\nsmoothness and consistency receive particular attention to prevent jumps in the\nrobot state belief. HF enables low-latency and smooth online state estimation\non typical robot hardware while simultaneously providing low-drift global\nlocalization at the IMU measurement rate. The efficacy of this released\nframework is demonstrated in five real-world scenarios on three robotic\nplatforms, each with distinct task requirements.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7075\u6d3b\u7684\u5f00\u6e90\u591a\u6a21\u6001\u4f20\u611f\u5668\u878d\u5408\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u4efb\u52a1\u548c\u8bbe\u7f6e\u65e0\u5173\u7684\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u573a\u666f\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u7075\u6d3b\u6027\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u91c7\u7528\u56e0\u5b50\u56fe\u65b9\u6cd5\uff0c\u5c06\u4f20\u611f\u5668\u878d\u5408\u5efa\u6a21\u4e3a\u5c40\u90e8\u548c\u5168\u5c40\u673a\u5668\u4eba\u72b6\u6001\u4ee5\u53ca\u52a8\u6001\u4e0a\u4e0b\u6587\u53d8\u91cf\u7684\u8054\u5408\u4f30\u8ba1\u95ee\u9898\uff0c\u652f\u6301\u4efb\u610f\u6570\u91cf\u7684\u7edd\u5bf9\u3001\u5c40\u90e8\u548c\u5730\u6807\u6d4b\u91cf\u3002", "result": "HF\u6846\u67b6\u5728\u5178\u578b\u673a\u5668\u4eba\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u3001\u5e73\u6ed1\u7684\u5728\u7ebf\u72b6\u6001\u4f30\u8ba1\uff0c\u540c\u65f6\u4ee5IMU\u6d4b\u91cf\u901f\u7387\u63d0\u4f9b\u4f4e\u6f02\u79fb\u7684\u5168\u5c40\u5b9a\u4f4d\u3002", "conclusion": "HF\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\u548c\u7075\u6d3b\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u573a\u666f\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.06553", "pdf": "https://arxiv.org/pdf/2504.06553", "abs": "https://arxiv.org/abs/2504.06553", "authors": ["Yun Chang", "Leonor Fermoselle", "Duy Ta", "Bernadette Bucher", "Luca Carlone", "Jiuguang Wang"], "title": "ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "While recent work in scene reconstruction and understanding has made strides\nin grounding natural language to physical 3D environments, it is still\nchallenging to ground abstract, high-level instructions to a 3D scene.\nHigh-level instructions might not explicitly invoke semantic elements in the\nscene, and even the process of breaking a high-level task into a set of more\nconcrete subtasks, a process called hierarchical task analysis, is\nenvironment-dependent. In this work, we propose ASHiTA, the first framework\nthat generates a task hierarchy grounded to a 3D scene graph by breaking down\nhigh-level tasks into grounded subtasks. ASHiTA alternates LLM-assisted\nhierarchical task analysis, to generate the task breakdown, with task-driven 3D\nscene graph construction to generate a suitable representation of the\nenvironment. Our experiments show that ASHiTA performs significantly better\nthan LLM baselines in breaking down high-level tasks into environment-dependent\nsubtasks and is additionally able to achieve grounding performance comparable\nto state-of-the-art methods.", "AI": {"task": "\u63d0\u51faASHITA\u6846\u67b6\uff0c\u5c06\u9ad8\u7ea7\u4efb\u52a1\u5206\u89e3\u4e3a\u57fa\u4e8e3D\u573a\u666f\u56fe\u7684\u5b50\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u96be\u4ee5\u5c06\u62bd\u8c61\u9ad8\u7ea7\u6307\u4ee4\u4e0e3D\u573a\u666f\u5173\u8054\uff0c\u4e14\u4efb\u52a1\u5206\u89e3\u4f9d\u8d56\u73af\u5883\u3002", "method": "ASHITA\u6846\u67b6\u7ed3\u5408LLM\u8f85\u52a9\u7684\u4efb\u52a1\u5206\u89e3\u548c\u4efb\u52a1\u9a71\u52a8\u76843D\u573a\u666f\u56fe\u6784\u5efa\u3002", "result": "ASHITA\u5728\u4efb\u52a1\u5206\u89e3\u548c\u573a\u666f\u5173\u8054\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8eLLM\u57fa\u7ebf\uff0c\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "ASHITA\u4e3a\u9ad8\u7ea7\u4efb\u52a1\u4e0e3D\u573a\u666f\u7684\u5173\u8054\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.06604", "pdf": "https://arxiv.org/pdf/2504.06604", "abs": "https://arxiv.org/abs/2504.06604", "authors": ["Jaehong Chung", "Wei Cai", "Tapan Mukerji"], "title": "Image registration of 2D optical thin sections in a 3D porous medium: Application to a Berea sandstone digital rock image", "categories": ["physics.geo-ph", "cs.CV"], "comment": null, "summary": "This study proposes a systematic image registration approach to align 2D\noptical thin-section images within a 3D digital rock volume. Using template\nimage matching with differential evolution optimization, we identify the most\nsimilar 2D plane in 3D. The method is validated on a synthetic porous medium,\nachieving exact registration, and applied to Berea sandstone, where it achieves\na structural similarity index (SSIM) of 0.990. With the registered images, we\nexplore upscaling properties based on paired multimodal images, focusing on\npore characteristics and effective elastic moduli. The thin-section image\nreveals 50 % more porosity and submicron pores than the registered CT plane. In\naddition, bulk and shear moduli from thin sections are 25 % and 30 % lower,\nrespectively, than those derived from CT images. Beyond numerical comparisons,\nthin sections provide additional geological insights, including cementation,\nmineral phases, and weathering effects, which are not clear in CT images. This\nstudy demonstrates the potential of multimodal image registration to improve\ncomputed rock properties in digital rock physics by integrating complementary\nimaging modalities.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\uff0c\u7528\u4e8e\u57283D\u6570\u5b57\u5ca9\u77f3\u4f53\u79ef\u4e2d\u5bf9\u9f502D\u5149\u5b66\u8584\u7247\u56fe\u50cf\u3002", "motivation": "\u901a\u8fc7\u591a\u6a21\u6001\u56fe\u50cf\u914d\u51c6\uff0c\u6574\u5408\u4e92\u8865\u7684\u6210\u50cf\u6a21\u6001\uff0c\u6539\u8fdb\u6570\u5b57\u5ca9\u77f3\u7269\u7406\u5b66\u4e2d\u8ba1\u7b97\u7684\u5ca9\u77f3\u5c5e\u6027\u3002", "method": "\u4f7f\u7528\u6a21\u677f\u56fe\u50cf\u5339\u914d\u7ed3\u5408\u5dee\u5206\u8fdb\u5316\u4f18\u5316\uff0c\u8bc6\u522b3D\u4e2d\u6700\u76f8\u4f3c\u76842D\u5e73\u9762\u3002", "result": "\u5728\u5408\u6210\u591a\u5b54\u4ecb\u8d28\u4e2d\u5b9e\u73b0\u7cbe\u786e\u914d\u51c6\uff0c\u5e76\u5728Berea\u7802\u5ca9\u4e2d\u8fbe\u52300.990\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\u6307\u6570\uff08SSIM\uff09\u3002\u8584\u7247\u56fe\u50cf\u6bd4\u914d\u51c6\u7684CT\u5e73\u9762\u591a\u63ed\u793a50%\u7684\u5b54\u9699\u5ea6\u548c\u4e9a\u5fae\u7c73\u5b54\u9699\uff0c\u4e14\u5f39\u6027\u6a21\u91cf\u66f4\u4f4e\u3002", "conclusion": "\u591a\u6a21\u6001\u56fe\u50cf\u914d\u51c6\u5728\u6570\u5b57\u5ca9\u77f3\u7269\u7406\u5b66\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u6574\u5408\u4e92\u8865\u6210\u50cf\u6a21\u6001\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u5730\u8d28\u4fe1\u606f\u3002"}}
{"id": "2504.06610", "pdf": "https://arxiv.org/pdf/2504.06610", "abs": "https://arxiv.org/abs/2504.06610", "authors": ["Sumeyye Meryem Tasyurek", "Tugce Kiziltepe", "Hacer Yalim Keles"], "title": "Disentangle and Regularize: Sign Language Production with Articulator-Based Disentanglement and Channel-Aware Regularization", "categories": ["cs.LG", "cs.CV"], "comment": "11 pages, 4 figures, 1 table", "summary": "In this work, we propose a simple gloss-free, transformer-based sign language\nproduction (SLP) framework that directly maps spoken-language text to sign pose\nsequences. We first train a pose autoencoder that encodes sign poses into a\ncompact latent space using an articulator-based disentanglement strategy, where\nfeatures corresponding to the face, right hand, left hand, and body are modeled\nseparately to promote structured and interpretable representation learning.\nNext, a non-autoregressive transformer decoder is trained to predict these\nlatent representations from sentence-level text embeddings. To guide this\nprocess, we apply channel-aware regularization by aligning predicted latent\ndistributions with priors extracted from the ground-truth encodings using a\nKL-divergence loss. The contribution of each channel to the loss is weighted\naccording to its associated articulator region, enabling the model to account\nfor the relative importance of different articulators during training. Our\napproach does not rely on gloss supervision or pretrained models, and achieves\nstate-of-the-art results on the PHOENIX14T dataset using only a modest training\nset.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u3001\u57fa\u4e8eTransformer\u7684\u624b\u8bed\u751f\u6210\u6846\u67b6\uff0c\u76f4\u63a5\u5c06\u53e3\u8bed\u6587\u672c\u6620\u5c04\u5230\u624b\u8bed\u59ff\u52bf\u5e8f\u5217\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u624b\u8bed\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u751f\u6210\u59ff\u52bf\u7684\u7ed3\u6784\u5316\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u59ff\u52bf\u81ea\u7f16\u7801\u5668\u5c06\u624b\u8bed\u59ff\u52bf\u7f16\u7801\u5230\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u91c7\u7528\u975e\u81ea\u56de\u5f52Transformer\u89e3\u7801\u5668\u4ece\u6587\u672c\u5d4c\u5165\u9884\u6d4b\u6f5c\u5728\u8868\u793a\uff0c\u7ed3\u5408\u901a\u9053\u611f\u77e5\u6b63\u5219\u5316\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5728PHOENIX14T\u6570\u636e\u96c6\u4e0a\u4ec5\u7528\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u5373\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u6807\u6ce8\u6216\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e3a\u624b\u8bed\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.06677", "pdf": "https://arxiv.org/pdf/2504.06677", "abs": "https://arxiv.org/abs/2504.06677", "authors": ["Alexandre Banks", "Richard Cook", "Septimiu E. Salcudean"], "title": "Setup-Invariant Augmented Reality for Teaching by Demonstration with Surgical Robots", "categories": ["cs.RO", "cs.CV", "cs.HC", "cs.SY", "eess.SY", "I.4.9; J.3.2; J.2.7"], "comment": "12 pages, 10 figures; Open-source code, see\n  https://github.com/AlexandreBanks6/dV-STEAR_Public.git; Supplementary movies,\n  see https://github.com/AlexandreBanks6/dVSTEAR_Supplemental_Files.git", "summary": "Augmented reality (AR) is an effective tool in robotic surgery education as\nit combines exploratory learning with three-dimensional guidance. However,\nexisting AR systems require expert supervision and do not account for\ndifferences in the mentor and mentee robot configurations. To enable novices to\ntrain outside the operating room while receiving expert-informed guidance, we\npresent dV-STEAR: an open-source system that plays back task-aligned expert\ndemonstrations without assuming identical setup joint positions between expert\nand novice. Pose estimation was rigorously quantified, showing a registration\nerror of 3.86 (SD=2.01)mm. In a user study (N=24), dV-STEAR significantly\nimproved novice performance on tasks from the Fundamentals of Laparoscopic\nSurgery. In a single-handed ring-over-wire task, dV-STEAR increased completion\nspeed (p=0.03) and reduced collision time (p=0.01) compared to dry-lab training\nalone. During a pick-and-place task, it improved success rates (p=0.004).\nAcross both tasks, participants using dV-STEAR exhibited significantly more\nbalanced hand use and reported lower frustration levels. This work presents a\nnovel educational tool implemented on the da Vinci Research Kit, demonstrates\nits effectiveness in teaching novices, and builds the foundation for further AR\nintegration into robot-assisted surgery.", "AI": {"task": "\u5f00\u53d1\u4e00\u4e2a\u5f00\u6e90\u7cfb\u7edf\uff08dV-STEAR\uff09\uff0c\u7528\u4e8e\u5728\u673a\u5668\u4eba\u624b\u672f\u6559\u80b2\u4e2d\u63d0\u4f9b\u4e13\u5bb6\u6307\u5bfc\u7684\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u7684AR\u7cfb\u7edf\u9700\u8981\u4e13\u5bb6\u76d1\u7763\uff0c\u4e14\u672a\u8003\u8651\u5bfc\u5e08\u548c\u5b66\u5458\u673a\u5668\u4eba\u914d\u7f6e\u7684\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u65b0\u624b\u7684\u81ea\u4e3b\u8bad\u7ec3\u3002", "method": "dV-STEAR\u901a\u8fc7\u56de\u653e\u4efb\u52a1\u5bf9\u9f50\u7684\u4e13\u5bb6\u6f14\u793a\uff0c\u4e0d\u8981\u6c42\u4e13\u5bb6\u548c\u65b0\u624b\u7684\u8bbe\u7f6e\u5173\u8282\u4f4d\u7f6e\u76f8\u540c\uff0c\u5e76\u91cf\u5316\u4e86\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002", "result": "dV-STEAR\u663e\u8457\u63d0\u9ad8\u4e86\u65b0\u624b\u5728\u8179\u8154\u955c\u624b\u672f\u57fa\u7840\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u62ec\u5b8c\u6210\u901f\u5ea6\u3001\u78b0\u649e\u65f6\u95f4\u548c\u6210\u529f\u7387\uff0c\u540c\u65f6\u6539\u5584\u4e86\u624b\u90e8\u5e73\u8861\u4f7f\u7528\u548c\u964d\u4f4e\u4e86\u632b\u8d25\u611f\u3002", "conclusion": "dV-STEAR\u662f\u4e00\u79cd\u6709\u6548\u7684\u6559\u80b2\u5de5\u5177\uff0c\u4e3aAR\u5728\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u4e2d\u7684\u8fdb\u4e00\u6b65\u96c6\u6210\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.06704", "pdf": "https://arxiv.org/pdf/2504.06704", "abs": "https://arxiv.org/abs/2504.06704", "authors": ["Yoshihiro Yamada"], "title": "CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Transformers have driven remarkable breakthroughs in natural language\nprocessing and computer vision, yet their standard attention mechanism still\nimposes O(N^2) complexity, hindering scalability to longer sequences. We\nintroduce Circular-convolutional ATtention (CAT), a Fourier-based approach that\nefficiently applies circular convolutions to reduce complexity without\nsacrificing representational power. CAT achieves O(NlogN) computations,\nrequires fewer learnable parameters by streamlining fully-connected layers, and\nintroduces no heavier operations, resulting in consistent accuracy improvements\nand about a 10% speedup in naive PyTorch implementations on large-scale\nbenchmarks such as ImageNet-1k and WikiText-103. Grounded in an\nengineering-isomorphism framework, CAT's design not only offers practical\nefficiency and ease of implementation but also provides insights to guide the\ndevelopment of next-generation, high-performance Transformer architectures.\nFinally, our ablation studies highlight the key conditions underlying CAT's\nsuccess, shedding light on broader principles for scalable attention\nmechanisms.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u5faa\u73af\u5377\u79ef\u6ce8\u610f\u529b\u673a\u5236\uff08CAT\uff09\uff0c\u4ee5\u964d\u4f4eTransformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u590d\u6742\u5ea6\u3002", "motivation": "\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u7684O(N^2)\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5176\u5728\u957f\u5e8f\u5217\u4e0a\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u901a\u8fc7\u5085\u91cc\u53f6\u53d8\u6362\u5e94\u7528\u5faa\u73af\u5377\u79ef\uff0c\u5c06\u590d\u6742\u5ea6\u964d\u81f3O(NlogN)\uff0c\u5e76\u51cf\u5c11\u53ef\u5b66\u4e60\u53c2\u6570\u3002", "result": "\u5728ImageNet-1k\u548cWikiText-103\u7b49\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCAT\u5b9e\u73b0\u4e86\u7ea610%\u7684\u901f\u5ea6\u63d0\u5347\u548c\u4e00\u81f4\u7684\u7cbe\u5ea6\u6539\u8fdb\u3002", "conclusion": "CAT\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6548\u7387\u548c\u6613\u5b9e\u73b0\u6027\uff0c\u8fd8\u4e3a\u4e0b\u4e00\u4ee3\u9ad8\u6027\u80fdTransformer\u67b6\u6784\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2504.06767", "pdf": "https://arxiv.org/pdf/2504.06767", "abs": "https://arxiv.org/abs/2504.06767", "authors": ["Paolo Angella", "Luca Balbi", "Fabrizio Ferrando", "Paolo Traverso", "Rosario Varriale", "Vito Paolo Pastore", "Matteo Santacesaria"], "title": "DIMA: DIffusing Motion Artifacts for unsupervised correction in brain MRI images", "categories": ["eess.IV", "cs.CV"], "comment": "7 pages, 5 figures, 7 tables", "summary": "Motion artifacts remain a significant challenge in Magnetic Resonance Imaging\n(MRI), compromising diagnostic quality and potentially leading to misdiagnosis\nor repeated scans. Existing deep learning approaches for motion artifact\ncorrection typically require paired motion-free and motion-affected images for\ntraining, which are rarely available in clinical settings. To overcome this\nrequirement, we present DIMA (DIffusing Motion Artifacts), a novel framework\nthat leverages diffusion models to enable unsupervised motion artifact\ncorrection in brain MRI. Our two-phase approach first trains a diffusion model\non unpaired motion-affected images to learn the distribution of motion\nartifacts. This model then generates realistic motion artifacts on clean\nimages, creating paired datasets suitable for supervised training of correction\nnetworks. Unlike existing methods, DIMA operates without requiring k-space\nmanipulation or detailed knowledge of MRI sequence parameters, making it\nadaptable across different scanning protocols and hardware. Comprehensive\nevaluations across multiple datasets and anatomical planes demonstrate that our\nmethod achieves comparable performance to state-of-the-art supervised\napproaches while offering superior generalizability to real clinical data. DIMA\nrepresents a significant advancement in making motion artifact correction more\naccessible for routine clinical use, potentially reducing the need for repeat\nscans and improving diagnostic accuracy.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aDIMA\u7684\u65e0\u76d1\u7763\u8fd0\u52a8\u4f2a\u5f71\u6821\u6b63\u6846\u67b6\uff0c\u7528\u4e8e\u8111\u90e8MRI\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u6210\u5bf9\u7684\u8fd0\u52a8\u4f2a\u5f71\u548c\u65e0\u4f2a\u5f71\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u5229\u7528\u6269\u6563\u6a21\u578b\u5206\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9996\u5148\u5728\u672a\u914d\u5bf9\u7684\u8fd0\u52a8\u4f2a\u5f71\u56fe\u50cf\u4e0a\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u4f2a\u5f71\uff1b\u7136\u540e\u5229\u7528\u751f\u6210\u7684\u4f2a\u5f71\u56fe\u50cf\u8bad\u7ec3\u6821\u6b63\u7f51\u7edc\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u89e3\u5256\u5e73\u9762\u4e0a\uff0cDIMA\u7684\u6027\u80fd\u4e0e\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DIMA\u4e3a\u4e34\u5e8a\u5e38\u89c4\u4f7f\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6613\u5b9e\u73b0\u7684\u8fd0\u52a8\u4f2a\u5f71\u6821\u6b63\u65b9\u6cd5\uff0c\u6709\u671b\u51cf\u5c11\u91cd\u590d\u626b\u63cf\u5e76\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002"}}
{"id": "2504.06866", "pdf": "https://arxiv.org/pdf/2504.06866", "abs": "https://arxiv.org/abs/2504.06866", "authors": ["Seunghyeok Back", "Joosoon Lee", "Kangmin Kim", "Heeseon Rho", "Geonhyup Lee", "Raeyoung Kang", "Sangbeom Lee", "Sangjun Noh", "Youngjin Lee", "Taeyeop Lee", "Kyoobin Lee"], "title": "GraspClutter6D: A Large-scale Real-world Dataset for Robust Perception and Grasping in Cluttered Scenes", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Robust grasping in cluttered environments remains an open challenge in\nrobotics. While benchmark datasets have significantly advanced deep learning\nmethods, they mainly focus on simplistic scenes with light occlusion and\ninsufficient diversity, limiting their applicability to practical scenarios. We\npresent GraspClutter6D, a large-scale real-world grasping dataset featuring:\n(1) 1,000 highly cluttered scenes with dense arrangements (14.1 objects/scene,\n62.6\\% occlusion), (2) comprehensive coverage across 200 objects in 75\nenvironment configurations (bins, shelves, and tables) captured using four\nRGB-D cameras from multiple viewpoints, and (3) rich annotations including 736K\n6D object poses and 9.3B feasible robotic grasps for 52K RGB-D images. We\nbenchmark state-of-the-art segmentation, object pose estimation, and grasping\ndetection methods to provide key insights into challenges in cluttered\nenvironments. Additionally, we validate the dataset's effectiveness as a\ntraining resource, demonstrating that grasping networks trained on\nGraspClutter6D significantly outperform those trained on existing datasets in\nboth simulation and real-world experiments. The dataset, toolkit, and\nannotation tools are publicly available on our project website:\nhttps://sites.google.com/view/graspclutter6d.", "AI": {"task": "\u63d0\u51fa\u5e76\u8bc4\u4f30GraspClutter6D\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u6293\u53d6\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u8fc7\u4e8e\u7b80\u5355\uff0c\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u906e\u6321\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u6784\u5efa\u5305\u542b1000\u4e2a\u9ad8\u5ea6\u6742\u4e71\u573a\u666f\u3001200\u4e2a\u7269\u4f53\u548c75\u79cd\u73af\u5883\u914d\u7f6e\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5e76\u6807\u6ce8736K 6D\u7269\u4f53\u4f4d\u59ff\u548c9.3B\u53ef\u884c\u6293\u53d6\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\uff0c\u57fa\u4e8eGraspClutter6D\u8bad\u7ec3\u7684\u6293\u53d6\u7f51\u7edc\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u3002", "conclusion": "GraspClutter6D\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u6293\u53d6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2504.06884", "pdf": "https://arxiv.org/pdf/2504.06884", "abs": "https://arxiv.org/abs/2504.06884", "authors": ["Wuyang Liu", "Yi Chai", "Yongpeng Yan", "Yanzhen Ren"], "title": "Audio-visual Event Localization on Portrait Mode Short Videos", "categories": ["cs.MM", "cs.AI", "cs.CV"], "comment": null, "summary": "Audio-visual event localization (AVEL) plays a critical role in multimodal\nscene understanding. While existing datasets for AVEL predominantly comprise\nlandscape-oriented long videos with clean and simple audio context, short\nvideos have become the primary format of online video content due to the the\nproliferation of smartphones. Short videos are characterized by\nportrait-oriented framing and layered audio compositions (e.g., overlapping\nsound effects, voiceovers, and music), which brings unique challenges\nunaddressed by conventional methods. To this end, we introduce AVE-PM, the\nfirst AVEL dataset specifically designed for portrait mode short videos,\ncomprising 25,335 clips that span 86 fine-grained categories with frame-level\nannotations. Beyond dataset creation, our empirical analysis shows that\nstate-of-the-art AVEL methods suffer an average 18.66% performance drop during\ncross-mode evaluation. Further analysis reveals two key challenges of different\nvideo formats: 1) spatial bias from portrait-oriented framing introduces\ndistinct domain priors, and 2) noisy audio composition compromise the\nreliability of audio modality. To address these issues, we investigate optimal\npreprocessing recipes and the impact of background music for AVEL on portrait\nmode videos. Experiments show that these methods can still benefit from\ntailored preprocessing and specialized model design, thus achieving improved\nperformance. This work provides both a foundational benchmark and actionable\ninsights for advancing AVEL research in the era of mobile-centric video\ncontent. Dataset and code will be released.", "AI": {"task": "\u7814\u7a76\u97f3\u9891-\u89c6\u89c9\u4e8b\u4ef6\u5b9a\u4f4d\uff08AVEL\uff09\u5728\u8096\u50cf\u6a21\u5f0f\u77ed\u89c6\u9891\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u73b0\u6709AVEL\u6570\u636e\u96c6\u4e3b\u8981\u9488\u5bf9\u6a2a\u5411\u957f\u89c6\u9891\uff0c\u800c\u77ed\u89c6\u9891\u5df2\u6210\u4e3a\u4e3b\u6d41\u5185\u5bb9\u5f62\u5f0f\uff0c\u5176\u72ec\u7279\u7684\u7eb5\u5411\u6784\u56fe\u548c\u590d\u6742\u97f3\u9891\u80cc\u666f\u5e26\u6765\u4e86\u65b0\u6311\u6218\u3002", "method": "\u5f15\u5165AVE-PM\u6570\u636e\u96c6\uff0c\u5305\u542b25,335\u4e2a\u7eb5\u5411\u77ed\u89c6\u9891\u7247\u6bb5\uff0c\u5e76\u63d0\u51fa\u9488\u5bf9\u6027\u7684\u9884\u5904\u7406\u65b9\u6cd5\u548c\u6a21\u578b\u8bbe\u8ba1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u6a21\u5f0f\u8bc4\u4f30\u4e2d\u6027\u80fd\u4e0b\u964d18.66%\uff0c\u4f46\u901a\u8fc7\u4f18\u5316\u9884\u5904\u7406\u548c\u6a21\u578b\u8bbe\u8ba1\u53ef\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u79fb\u52a8\u89c6\u9891\u65f6\u4ee3\u7684AVEL\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u5b9e\u7528\u5efa\u8bae\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2504.06915", "pdf": "https://arxiv.org/pdf/2504.06915", "abs": "https://arxiv.org/abs/2504.06915", "authors": ["Miro Miranda", "Francisco Mena", "Andreas Dengel"], "title": "An Analysis of Temporal Dropout in Earth Observation Time Series for Regression Tasks", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted at Symposium on Intelligent Data Analysis (IDA 2025)", "summary": "Missing instances in time series data impose a significant challenge to deep\nlearning models, particularly in regression tasks. In the Earth Observation\nfield, satellite failure or cloud occlusion frequently results in missing\ntime-steps, introducing uncertainties in the predicted output and causing a\ndecline in predictive performance. While many studies address missing\ntime-steps through data augmentation to improve model robustness, the\nuncertainty arising at the input level is commonly overlooked. To address this\ngap, we introduce Monte Carlo Temporal Dropout (MC-TD), a method that\nexplicitly accounts for input-level uncertainty by randomly dropping time-steps\nduring inference using a predefined dropout ratio, thereby simulating the\neffect of missing data. To bypass the need for costly searches for the optimal\ndropout ratio, we extend this approach with Monte Carlo Concrete Temporal\nDropout (MC-ConcTD), a method that learns the optimal dropout distribution\ndirectly. Both MC-TD and MC-ConcTD are applied during inference, leveraging\nMonte Carlo sampling for uncertainty quantification. Experiments on three EO\ntime-series datasets demonstrate that MC-ConcTD improves predictive performance\nand uncertainty calibration compared to existing approaches. Additionally, we\nhighlight the advantages of adaptive dropout tuning over manual selection,\nmaking uncertainty quantification more robust and accessible for EO\napplications.", "AI": {"task": "\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7f3a\u5931\u5b9e\u4f8b\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u5f71\u54cd\u95ee\u9898\u3002", "motivation": "\u536b\u661f\u6545\u969c\u6216\u4e91\u906e\u6321\u5bfc\u81f4\u7684\u65f6\u95f4\u6b65\u7f3a\u5931\u4f1a\u5f15\u5165\u9884\u6d4b\u8f93\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u5e76\u964d\u4f4e\u6027\u80fd\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u5ffd\u7565\u8f93\u5165\u7ea7\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u8499\u7279\u5361\u6d1b\u65f6\u95f4\u4e22\u5f03\uff08MC-TD\uff09\u548c\u8499\u7279\u5361\u6d1b\u5177\u4f53\u65f6\u95f4\u4e22\u5f03\uff08MC-ConcTD\uff09\uff0c\u524d\u8005\u968f\u673a\u4e22\u5f03\u65f6\u95f4\u6b65\u4ee5\u6a21\u62df\u7f3a\u5931\u6570\u636e\uff0c\u540e\u8005\u5b66\u4e60\u6700\u4f18\u4e22\u5f03\u5206\u5e03\u3002", "result": "\u5728\u4e09\u4e2aEO\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\uff0cMC-ConcTD\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u6548\u679c\u3002", "conclusion": "\u81ea\u9002\u5e94\u4e22\u5f03\u8c03\u4f18\u4f18\u4e8e\u624b\u52a8\u9009\u62e9\uff0c\u4f7f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u66f4\u7a33\u5065\u4e14\u9002\u7528\u4e8eEO\u5e94\u7528\u3002"}}
{"id": "2504.06921", "pdf": "https://arxiv.org/pdf/2504.06921", "abs": "https://arxiv.org/abs/2504.06921", "authors": ["Anisa V. Prasad", "Tejas Sudharshan Mathai", "Pritam Mukherjee", "Jianfei Liu", "Ronald M. Summers"], "title": "Leveraging Anatomical Priors for Automated Pancreas Segmentation on Abdominal CT", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Published at SPIE Medical Imaging 2025", "summary": "An accurate segmentation of the pancreas on CT is crucial to identify\npancreatic pathologies and extract imaging-based biomarkers. However, prior\nresearch on pancreas segmentation has primarily focused on modifying the\nsegmentation model architecture or utilizing pre- and post-processing\ntechniques. In this article, we investigate the utility of anatomical priors to\nenhance the segmentation performance of the pancreas. Two 3D full-resolution\nnnU-Net models were trained, one with 8 refined labels from the public PANORAMA\ndataset, and another that combined them with labels derived from the public\nTotalSegmentator (TS) tool. The addition of anatomical priors resulted in a 6\\%\nincrease in Dice score ($p < .001$) and a 36.5 mm decrease in Hausdorff\ndistance for pancreas segmentation ($p < .001$). Moreover, the pancreas was\nalways detected when anatomy priors were used, whereas there were 8 instances\nof failed detections without their use. The use of anatomy priors shows promise\nfor pancreas segmentation and subsequent derivation of imaging biomarkers.", "AI": {"task": "\u5229\u7528\u89e3\u5256\u5b66\u5148\u9a8c\u77e5\u8bc6\u63d0\u5347\u80f0\u817aCT\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u80f0\u817aCT\u5206\u5272\u5bf9\u75c5\u7406\u8bc6\u522b\u548c\u5f71\u50cf\u751f\u7269\u6807\u5fd7\u7269\u63d0\u53d6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u6a21\u578b\u67b6\u6784\u6216\u9884\u5904\u7406\u6280\u672f\uff0c\u89e3\u5256\u5b66\u5148\u9a8c\u77e5\u8bc6\u7684\u6548\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8bad\u7ec3\u4e24\u4e2a3D\u5168\u5206\u8fa8\u7387nnU-Net\u6a21\u578b\uff0c\u4e00\u4e2a\u4f7f\u7528PANORAMA\u6570\u636e\u96c6\u76848\u4e2a\u7cbe\u7ec6\u6807\u7b7e\uff0c\u53e6\u4e00\u4e2a\u7ed3\u5408PANORAMA\u548cTotalSegmentator\u5de5\u5177\u7684\u6807\u7b7e\u3002", "result": "\u89e3\u5256\u5b66\u5148\u9a8c\u77e5\u8bc6\u4f7fDice\u5206\u6570\u63d0\u53476%\uff08p < .001\uff09\uff0cHausdorff\u8ddd\u79bb\u51cf\u5c1136.5 mm\uff08p < .001\uff09\uff0c\u4e14\u907f\u514d\u4e868\u6b21\u68c0\u6d4b\u5931\u8d25\u3002", "conclusion": "\u89e3\u5256\u5b66\u5148\u9a8c\u77e5\u8bc6\u663e\u8457\u63d0\u5347\u80f0\u817a\u5206\u5272\u6027\u80fd\uff0c\u5bf9\u5f71\u50cf\u751f\u7269\u6807\u5fd7\u7269\u63d0\u53d6\u6709\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2504.06924", "pdf": "https://arxiv.org/pdf/2504.06924", "abs": "https://arxiv.org/abs/2504.06924", "authors": ["Tejas Sudharshan Mathai", "Benjamin Hou", "Ronald M. Summers"], "title": "Longitudinal Assessment of Lung Lesion Burden in CT", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Published at SPIE Medical Imaging 2025", "summary": "In the U.S., lung cancer is the second major cause of death. Early detection\nof suspicious lung nodules is crucial for patient treatment planning,\nmanagement, and improving outcomes. Many approaches for lung nodule\nsegmentation and volumetric analysis have been proposed, but few have looked at\nlongitudinal changes in total lung tumor burden. In this work, we trained two\n3D models (nnUNet) with and without anatomical priors to automatically segment\nlung lesions and quantified total lesion burden for each patient. The 3D model\nwithout priors significantly outperformed ($p < .001$) the model trained with\nanatomy priors. For detecting clinically significant lesions $>$ 1cm, a\nprecision of 71.3\\%, sensitivity of 68.4\\%, and F1-score of 69.8\\% was\nachieved. For segmentation, a Dice score of 77.1 $\\pm$ 20.3 and Hausdorff\ndistance error of 11.7 $\\pm$ 24.1 mm was obtained. The median lesion burden was\n6.4 cc (IQR: 2.1, 18.1) and the median volume difference between manual and\nautomated measurements was 0.02 cc (IQR: -2.8, 1.2). Agreements were also\nevaluated with linear regression and Bland-Altman plots. The proposed approach\ncan produce a personalized evaluation of the total tumor burden for a patient\nand facilitate interval change tracking over time.", "AI": {"task": "\u5229\u75283D\u6a21\u578b\uff08nnUNet\uff09\u81ea\u52a8\u5206\u5272\u80ba\u90e8\u75c5\u53d8\u5e76\u91cf\u5316\u60a3\u8005\u7684\u603b\u75c5\u53d8\u8d1f\u62c5\u3002", "motivation": "\u80ba\u764c\u662f\u7f8e\u56fd\u7b2c\u4e8c\u5927\u6b7b\u4ea1\u539f\u56e0\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u6cbb\u7597\u8ba1\u5212\u548c\u6539\u5584\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7814\u7a76\u8f83\u5c11\u5173\u6ce8\u80ba\u90e8\u80bf\u7624\u8d1f\u62c5\u7684\u7eb5\u5411\u53d8\u5316\u3002", "method": "\u8bad\u7ec3\u4e24\u79cd3D\u6a21\u578b\uff08\u5e26\u548c\u4e0d\u5e26\u89e3\u5256\u5b66\u5148\u9a8c\u7684nnUNet\uff09\uff0c\u81ea\u52a8\u5206\u5272\u80ba\u90e8\u75c5\u53d8\u5e76\u91cf\u5316\u603b\u75c5\u53d8\u8d1f\u62c5\u3002", "result": "\u65e0\u5148\u9a8c\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u5e26\u5148\u9a8c\u6a21\u578b\uff08p < .001\uff09\uff1b\u68c0\u6d4b\u5927\u4e8e1cm\u75c5\u53d8\u7684\u7cbe\u5ea6\u4e3a71.3%\uff0c\u7075\u654f\u5ea6\u4e3a68.4%\uff0cF1\u5206\u6570\u4e3a69.8%\uff1b\u5206\u5272Dice\u5206\u6570\u4e3a77.1\u00b120.3\uff0cHausdorff\u8ddd\u79bb\u8bef\u5dee\u4e3a11.7\u00b124.1 mm\uff1b\u4e2d\u4f4d\u75c5\u53d8\u8d1f\u62c5\u4e3a6.4 cc\uff0c\u624b\u52a8\u4e0e\u81ea\u52a8\u6d4b\u91cf\u4f53\u79ef\u5dee\u5f02\u4e2d\u4f4d\u6570\u4e3a0.02 cc\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u4e2a\u6027\u5316\u8bc4\u4f30\u60a3\u8005\u603b\u80bf\u7624\u8d1f\u62c5\uff0c\u5e76\u4fbf\u4e8e\u968f\u65f6\u95f4\u8ddf\u8e2a\u53d8\u5316\u3002"}}
{"id": "2504.06961", "pdf": "https://arxiv.org/pdf/2504.06961", "abs": "https://arxiv.org/abs/2504.06961", "authors": ["Yu Qi", "Yuanchen Ju", "Tianming Wei", "Chi Chu", "Lawson L. S. Wong", "Huazhe Xu"], "title": "Two by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted to CVPR 2025 (Conference on Computer Vision and Pattern\n  Recognition)", "summary": "3D assembly tasks, such as furniture assembly and component fitting, play a\ncrucial role in daily life and represent essential capabilities for future home\nrobots. Existing benchmarks and datasets predominantly focus on assembling\ngeometric fragments or factory parts, which fall short in addressing the\ncomplexities of everyday object interactions and assemblies. To bridge this\ngap, we present 2BY2, a large-scale annotated dataset for daily pairwise\nobjects assembly, covering 18 fine-grained tasks that reflect real-life\nscenarios, such as plugging into sockets, arranging flowers in vases, and\ninserting bread into toasters. 2BY2 dataset includes 1,034 instances and 517\npairwise objects with pose and symmetry annotations, requiring approaches that\nalign geometric shapes while accounting for functional and spatial\nrelationships between objects. Leveraging the 2BY2 dataset, we propose a\ntwo-step SE(3) pose estimation method with equivariant features for assembly\nconstraints. Compared to previous shape assembly methods, our approach achieves\nstate-of-the-art performance across all 18 tasks in the 2BY2 dataset.\nAdditionally, robot experiments further validate the reliability and\ngeneralization ability of our method for complex 3D assembly tasks.", "AI": {"task": "\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u7528\u4e8e\u65e5\u5e38\u6210\u5bf9\u7269\u4f533D\u7ec4\u88c5\u7684\u4e24\u6b65SE(3)\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u51e0\u4f55\u788e\u7247\u6216\u5de5\u5382\u96f6\u4ef6\u7684\u7ec4\u88c5\uff0c\u672a\u80fd\u5145\u5206\u89e3\u51b3\u65e5\u5e38\u7269\u4f53\u4ea4\u4e92\u548c\u7ec4\u88c5\u7684\u590d\u6742\u6027\u3002", "method": "\u5229\u75282BY2\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u7b49\u53d8\u7279\u5f81\u7684\u4e24\u6b65SE(3)\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002", "result": "\u5728\u6240\u670918\u4e2a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "2BY2\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4e3a\u590d\u67423D\u7ec4\u88c5\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.06994", "pdf": "https://arxiv.org/pdf/2504.06994", "abs": "https://arxiv.org/abs/2504.06994", "authors": ["Omar Alama", "Avigyan Bhattacharya", "Haoyang He", "Seungchan Kim", "Yuheng Qiu", "Wenshan Wang", "Cherie Ho", "Nikhil Keetha", "Sebastian Scherer"], "title": "RayFronts: Open-Set Semantic Ray Frontiers for Online Scene Understanding and Exploration", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Open-set semantic mapping is crucial for open-world robots. Current mapping\napproaches either are limited by the depth range or only map beyond-range\nentities in constrained settings, where overall they fail to combine\nwithin-range and beyond-range observations. Furthermore, these methods make a\ntrade-off between fine-grained semantics and efficiency. We introduce\nRayFronts, a unified representation that enables both dense and beyond-range\nefficient semantic mapping. RayFronts encodes task-agnostic open-set semantics\nto both in-range voxels and beyond-range rays encoded at map boundaries,\nempowering the robot to reduce search volumes significantly and make informed\ndecisions both within & beyond sensory range, while running at 8.84 Hz on an\nOrin AGX. Benchmarking the within-range semantics shows that RayFronts's\nfine-grained image encoding provides 1.34x zero-shot 3D semantic segmentation\nperformance while improving throughput by 16.5x. Traditionally, online mapping\nperformance is entangled with other system components, complicating evaluation.\nWe propose a planner-agnostic evaluation framework that captures the utility\nfor online beyond-range search and exploration, and show RayFronts reduces\nsearch volume 2.2x more efficiently than the closest online baselines.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u8868\u793a\u65b9\u6cd5RayFronts\uff0c\u7528\u4e8e\u5b9e\u73b0\u5bc6\u96c6\u548c\u8d85\u8303\u56f4\u7684\u9ad8\u6548\u8bed\u4e49\u5efa\u56fe\u3002", "motivation": "\u73b0\u6709\u5efa\u56fe\u65b9\u6cd5\u5728\u6df1\u5ea6\u8303\u56f4\u6216\u8d85\u8303\u56f4\u5b9e\u4f53\u5efa\u56fe\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u4e14\u65e0\u6cd5\u7ed3\u5408\u8303\u56f4\u5185\u548c\u8303\u56f4\u5916\u89c2\u5bdf\uff0c\u540c\u65f6\u5728\u7ec6\u7c92\u5ea6\u8bed\u4e49\u548c\u6548\u7387\u4e4b\u95f4\u505a\u51fa\u6743\u8861\u3002", "method": "\u5f15\u5165RayFronts\uff0c\u4e00\u79cd\u7edf\u4e00\u8868\u793a\u65b9\u6cd5\uff0c\u7f16\u7801\u4efb\u52a1\u65e0\u5173\u7684\u5f00\u653e\u96c6\u8bed\u4e49\uff0c\u6db5\u76d6\u8303\u56f4\u5185\u4f53\u7d20\u548c\u8303\u56f4\u5916\u5c04\u7ebf\uff0c\u663e\u8457\u51cf\u5c11\u641c\u7d22\u91cf\u5e76\u652f\u6301\u9ad8\u6548\u51b3\u7b56\u3002", "result": "RayFronts\u57283D\u8bed\u4e49\u5206\u5272\u6027\u80fd\u4e0a\u63d0\u53471.34\u500d\uff0c\u541e\u5410\u91cf\u63d0\u9ad816.5\u500d\uff0c\u641c\u7d22\u91cf\u6548\u7387\u63d0\u53472.2\u500d\u3002", "conclusion": "RayFronts\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u5f00\u653e\u96c6\u8bed\u4e49\u5efa\u56fe\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u51b3\u7b56\u548c\u63a2\u7d22\u3002"}}
{"id": "2504.07072", "pdf": "https://arxiv.org/pdf/2504.07072", "abs": "https://arxiv.org/abs/2504.07072", "authors": ["Israfel Salazar", "Manuel Fern\u00e1ndez Burda", "Shayekh Bin Islam", "Arshia Soltani Moakhar", "Shivalika Singh", "Fabian Farestam", "Angelika Romanou", "Danylo Boiko", "Dipika Khullar", "Mike Zhang", "Dominik Krzemi\u0144ski", "Jekaterina Novikova", "Lu\u00edsa Shimabucoro", "Joseph Marvin Imperial", "Rishabh Maheshwary", "Sharad Duwal", "Alfonso Amayuelas", "Swati Rajwal", "Jebish Purbey", "Ahmed Ruby", "Nicholas Popovi\u010d", "Marek Suppa", "Azmine Toushik Wasi", "Ram Mohan Rao Kadiyala", "Olga Tsymboi", "Maksim Kostritsya", "Bardia Soltani Moakhar", "Gabriel da Costa Merlin", "Ot\u00e1vio Ferracioli Coletti", "Maral Jabbari Shiviari", "MohammadAmin farahani fard", "Silvia Fernandez", "Mar\u00eda Grandury", "Dmitry Abulkhanov", "Drishti Sharma", "Andre Guarnier De Mitri", "Leticia Bossatto Marchezi", "Johan Obando-Ceron", "Nazar Kohut", "Beyza Ermis", "Desmond Elliott", "Enzo Ferrante", "Sara Hooker", "Marzieh Fadaee"], "title": "Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "The evaluation of vision-language models (VLMs) has mainly relied on\nEnglish-language benchmarks, leaving significant gaps in both multilingual and\nmulticultural coverage. While multilingual benchmarks have expanded, both in\nsize and languages, many rely on translations of English datasets, failing to\ncapture cultural nuances. In this work, we propose Kaleidoscope, as the most\ncomprehensive exam benchmark to date for the multilingual evaluation of\nvision-language models. Kaleidoscope is a large-scale, in-language multimodal\nbenchmark designed to evaluate VLMs across diverse languages and visual inputs.\nKaleidoscope covers 18 languages and 14 different subjects, amounting to a\ntotal of 20,911 multiple-choice questions. Built through an open science\ncollaboration with a diverse group of researchers worldwide, Kaleidoscope\nensures linguistic and cultural authenticity. We evaluate top-performing\nmultilingual vision-language models and find that they perform poorly on\nlow-resource languages and in complex multimodal scenarios. Our results\nhighlight the need for progress on culturally inclusive multimodal evaluation\nframeworks.", "AI": {"task": "\u63d0\u51faKaleidoscope\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u82f1\u8bed\u57fa\u51c6\uff0c\u7f3a\u4e4f\u591a\u8bed\u8a00\u548c\u591a\u6587\u5316\u8986\u76d6\uff0c\u4e14\u8bb8\u591a\u591a\u8bed\u8a00\u57fa\u51c6\u4ec5\u57fa\u4e8e\u82f1\u8bed\u6570\u636e\u96c6\u7684\u7ffb\u8bd1\uff0c\u65e0\u6cd5\u6355\u6349\u6587\u5316\u7ec6\u5fae\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u5168\u7403\u7814\u7a76\u8005\u7684\u5f00\u653e\u79d1\u5b66\u5408\u4f5c\uff0c\u6784\u5efa\u6db5\u76d618\u79cd\u8bed\u8a00\u548c14\u4e2a\u4e3b\u9898\u768420,911\u9053\u591a\u9009\u9898\u7684\u591a\u6a21\u6001\u57fa\u51c6Kaleidoscope\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u5f53\u524d\u8868\u73b0\u6700\u4f73\u7684\u591a\u8bed\u8a00\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u590d\u6742\u591a\u6a21\u6001\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u5f3a\u8c03\u9700\u8981\u53d1\u5c55\u66f4\u5177\u6587\u5316\u5305\u5bb9\u6027\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2504.07079", "pdf": "https://arxiv.org/pdf/2504.07079", "abs": "https://arxiv.org/abs/2504.07079", "authors": ["Boyuan Zheng", "Michael Y. Fatemi", "Xiaolong Jin", "Zora Zhiruo Wang", "Apurva Gandhi", "Yueqi Song", "Yu Gu", "Jayanth Srinivasa", "Gaowen Liu", "Graham Neubig", "Yu Su"], "title": "SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "To survive and thrive in complex environments, humans have evolved\nsophisticated self-improvement mechanisms through environment exploration,\nhierarchical abstraction of experiences into reuseable skills, and\ncollaborative construction of an ever-growing skill repertoire. Despite recent\nadvancements, autonomous web agents still lack crucial self-improvement\ncapabilities, struggling with procedural knowledge abstraction, refining\nskills, and skill composition. In this work, we introduce SkillWeaver, a\nskill-centric framework enabling agents to self-improve by autonomously\nsynthesizing reusable skills as APIs. Given a new website, the agent\nautonomously discovers skills, executes them for practice, and distills\npractice experiences into robust APIs. Iterative exploration continually\nexpands a library of lightweight, plug-and-play APIs, significantly enhancing\nthe agent's capabilities. Experiments on WebArena and real-world websites\ndemonstrate the efficacy of SkillWeaver, achieving relative success rate\nimprovements of 31.8% and 39.8%, respectively. Additionally, APIs synthesized\nby strong agents substantially enhance weaker agents through transferable\nskills, yielding improvements of up to 54.3% on WebArena. These results\ndemonstrate the effectiveness of honing diverse website interactions into APIs,\nwhich can be seamlessly shared among various web agents.", "AI": {"task": "\u63d0\u51faSkillWeaver\u6846\u67b6\uff0c\u4f7f\u81ea\u4e3b\u7f51\u7edc\u4ee3\u7406\u80fd\u591f\u901a\u8fc7\u5408\u6210\u53ef\u91cd\u7528\u6280\u80fdAPI\u5b9e\u73b0\u81ea\u6211\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u7f51\u7edc\u4ee3\u7406\u7f3a\u4e4f\u5173\u952e\u81ea\u6211\u63d0\u5347\u80fd\u529b\uff0c\u5982\u7a0b\u5e8f\u6027\u77e5\u8bc6\u62bd\u8c61\u3001\u6280\u80fd\u63d0\u70bc\u548c\u7ec4\u5408\u3002", "method": "SkillWeaver\u6846\u67b6\u901a\u8fc7\u81ea\u4e3b\u53d1\u73b0\u6280\u80fd\u3001\u6267\u884c\u5b9e\u8df5\u5e76\u5c06\u7ecf\u9a8c\u63d0\u70bc\u4e3a\u8f7b\u91cf\u7ea7API\uff0c\u6301\u7eed\u6269\u5c55\u6280\u80fd\u5e93\u3002", "result": "\u5728WebArena\u548c\u771f\u5b9e\u7f51\u7ad9\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cSkillWeaver\u5206\u522b\u63d0\u9ad8\u4e8631.8%\u548c39.8%\u7684\u6210\u529f\u7387\uff0c\u4e14\u5f3a\u4ee3\u7406\u5408\u6210\u7684API\u53ef\u63d0\u5347\u5f31\u4ee3\u7406\u6027\u80fd\u8fbe54.3%\u3002", "conclusion": "SkillWeaver\u901a\u8fc7\u5c06\u591a\u6837\u5316\u7f51\u7ad9\u4ea4\u4e92\u63d0\u70bc\u4e3a\u53ef\u5171\u4eabAPI\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f51\u7edc\u4ee3\u7406\u7684\u80fd\u529b\u548c\u534f\u4f5c\u6548\u7387\u3002"}}
