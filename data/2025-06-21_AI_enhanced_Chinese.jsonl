{"id": "2506.14913", "title": "Winter Soldier: Backdooring Language Models at Pre-Training with Indirect Data Poisoning", "authors": ["Wassim Bouaziz", "Mathurin Videau", "Nicolas Usunier", "El-Mahdi El-Mhamdi"], "summary": "The pre-training of large language models (LLMs) relies on massive text\ndatasets sourced from diverse and difficult-to-curate origins. Although\nmembership inference attacks and hidden canaries have been explored to trace\ndata usage, such methods rely on memorization of training data, which LM\nproviders try to limit. In this work, we demonstrate that indirect data\npoisoning (where the targeted behavior is absent from training data) is not\nonly feasible but also allow to effectively protect a dataset and trace its\nuse. Using gradient-based optimization prompt-tuning, we make a model learn\narbitrary secret sequences: secret responses to secret prompts that are absent\nfrom the training corpus. We validate our approach on language models\npre-trained from scratch and show that less than 0.005% of poisoned tokens are\nsufficient to covertly make a LM learn a secret and detect it with extremely\nhigh confidence ($p < 10^{-55}$) with a theoretically certifiable scheme.\nCrucially, this occurs without performance degradation (on LM benchmarks) and\ndespite secrets never appearing in the training set.", "comment": "18 pages, 12 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.14913v1", "AI": {"title_translation": "冬日战士：通过间接数据投毒在预训练阶段对语言模型进行后门攻击", "tldr": "研究表明，在LLM预训练中，通过间接数据投毒可以有效保护数据集并追踪其使用，即使目标行为不在训练数据中。", "motivation": "现有数据使用追踪方法（如成员推理攻击、隐藏金丝雀）依赖于模型对训练数据的记忆，而语言模型提供商试图限制这种记忆。因此，需要一种不依赖于记忆的新方法来追踪数据使用。", "method": "本文提出了一种间接数据投毒方法。具体通过梯度优化提示调优，使模型学习任意秘密序列（即对秘密提示的秘密响应，这些秘密提示和响应均未出现在训练语料库中）。该方法在从头预训练的语言模型上进行了验证。", "result": "研究表明，仅需不到0.005%的投毒令牌，就能隐蔽地使语言模型学习秘密并以极高置信度（p < 10^-55）检测到，且不导致模型性能下降。此外，秘密从未出现在训练集中。", "conclusion": "间接数据投毒不仅可行，而且能够有效保护数据集并追踪其使用，即使目标行为在训练数据中不存在。", "translation": "大型语言模型（LLMs）的预训练依赖于从多样且难以管理来源获取的海量文本数据集。尽管已探索了成员推理攻击和隐藏金丝雀等方法来追踪数据使用，但这些方法依赖于训练数据的记忆，而语言模型提供商试图限制这种记忆。在这项工作中，我们证明了间接数据投毒（目标行为在训练数据中不存在）不仅可行，而且能够有效保护数据集并追踪其使用。通过基于梯度的优化提示调优，我们使模型学习任意秘密序列：对秘密提示的秘密响应，这些秘密提示在训练语料库中不存在。我们在从头开始预训练的语言模型上验证了我们的方法，并表明不到0.005%的投毒令牌足以隐蔽地使语言模型学习秘密，并通过理论上可认证的方案以极高置信度（p < 10^-55）进行检测。至关重要的是，这在不降低性能（在语言模型基准测试上）的情况下发生，尽管秘密从未出现在训练集中。", "summary": "本文提出了一种新颖的间接数据投毒方法，用于在大型语言模型（LLMs）的预训练阶段保护数据集并追踪其使用。与依赖模型记忆的现有方法不同，该方法通过梯度优化提示调优，使模型学习对秘密提示的秘密响应，而这些秘密序列并未出现在训练数据中。实验证明，极少量（<0.005%）的投毒令牌即可有效且隐蔽地使模型学习并检测这些秘密，且不影响模型性能，为数据溯源和保护提供了新的途径。", "keywords": "间接数据投毒, 语言模型, 预训练, 数据溯源, 后门攻击", "comments": "这项工作具有创新性，因为它提出了一种不依赖于模型记忆的间接数据投毒方法来追踪LLM数据使用，克服了现有方法的局限性。其重要性在于为数据集所有者提供了在LLM预训练过程中保护知识产权和追踪数据流的有效手段，同时保持了模型性能。"}}
{"id": "2506.14944", "title": "Fair Data Exchange with Constant-Time Proofs", "authors": ["Majid Khabbazian"], "summary": "The Fair Data Exchange (FDE) protocol introduced at CCS 2024 offers atomic\npay-per-file transfers with constant-size proofs, but its prover and verifier\nruntimes still scale linearly with the file length n. We collapse these costs\nto essentially constant by viewing the file as a rate-1 Reed-Solomon (RS)\ncodeword, extending it to a lower-rate RS code with constant redundancy,\nencrypting this extended vector, and then proving correctness for only a small\nrandom subset of the resulting ciphertexts; RS decoding repairs any corrupted\nsymbols with negligible failure probability. Our protocol preserves full\nclient- and server-fairness, and adds only a tunable communication redundancy\noverhead.\n  Finally, we patch the elliptic-curve mismatch in the Bitcoin instantiation of\nFDE with a compact zk-SNARK, enabling the entire exchange to run off-chain and\nfalling back to just two on-chain transactions when channels are unavailable.", "comment": "13 pages, 0 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.14944v1", "AI": {"title_translation": "具有常数时间证明的公平数据交换", "tldr": "通过结合Reed-Solomon编码和随机抽样证明，将公平数据交换（FDE）协议的证明者和验证者运行时间从线性缩减到接近常数时间，并支持比特币链下交易。", "motivation": "现有的公平数据交换（FDE）协议虽然提供了原子按文件付费传输和常数大小的证明，但其证明者和验证者的运行时间仍与文件长度呈线性关系。", "method": "1. 将文件视为速率为1的Reed-Solomon（RS）码字。2. 将其扩展为具有常数冗余的低速率RS码。3. 加密扩展后的向量。4. 仅对结果密文的一个小随机子集进行正确性证明。5. 利用RS解码修复任何损坏的符号。6. 使用紧凑的zk-SNARK修补FDE在比特币实例化中的椭圆曲线不匹配问题，从而实现整个交换的链下运行，并在通道不可用时回退到仅两次链上交易。", "result": "1. 证明者和验证者的运行成本缩减到接近常数时间。2. 保留了完整的客户端和服务器公平性。3. 只增加了可调的通信冗余开销。4. 实现了整个交换的链下运行，并能在通道不可用时回退到仅两次链上交易。", "conclusion": "通过引入RS编码和随机抽样证明，该协议显著优化了FDE的性能，使其运行时间达到常数级别，同时保持公平性并支持高效的比特币链下交易。", "translation": "CCS 2024上引入的公平数据交换（FDE）协议提供原子按文件付费传输和常数大小的证明，但其证明者和验证者运行时间仍与文件长度n呈线性关系。我们通过将文件视为速率为1的Reed-Solomon (RS) 码字，将其扩展为具有常数冗余的低速率RS码，加密此扩展向量，然后仅对结果密文的一个小随机子集证明正确性，将这些成本缩减到基本常数；RS解码以可忽略的失败概率修复任何损坏的符号。我们的协议保留了完整的客户端和服务器公平性，并且只增加了可调的通信冗余开销。最后，我们使用紧凑的zk-SNARK修补了FDE在比特币实例化中的椭圆曲线不匹配问题，从而使整个交换可以在链下运行，并在通道不可用时回退到仅两次链上交易。", "summary": "本文提出了一种改进的公平数据交换（FDE）协议，解决了现有FDE协议中证明者和验证者运行时间随文件长度线性增长的问题。通过将文件视为Reed-Solomon码字并进行扩展和加密，然后仅对密文的随机子集进行证明，实现了接近常数时间的运行成本。该协议保持了客户端和服务器的公平性，并引入了可调的通信冗余。此外，还通过zk-SNARK解决了FDE在比特币实例化中的兼容性问题，支持高效的链下交易。", "keywords": "公平数据交换, 常数时间证明, Reed-Solomon编码, zk-SNARK, 链下交易", "comments": "这篇论文通过引入Reed-Solomon编码和随机抽样证明，对公平数据交换协议进行了重要的性能优化，将线性时间复杂度降低到常数时间，这对于处理大型文件的数据交换具有显著的实际意义。此外，对比特币链下交易的支持也增强了其实用性。"}}
{"id": "2506.14964", "title": "Narrowing the Gap between TEEs Threat Model and Deployment Strategies", "authors": ["Filip Rezabek", "Jonathan Passerat-Palmbach", "Moe Mahhouk", "Frieder Erdmann", "Andrew Miller"], "summary": "Confidential Virtual Machines (CVMs) provide isolation guarantees for data in\nuse, but their threat model does not include physical level protection and\nside-channel attacks. Therefore, current deployments rely on trusted cloud\nproviders to host the CVMs' underlying infrastructure. However, TEE\nattestations do not provide information about the operator hosting a CVM.\nWithout knowing whether a Trusted Execution Environment (TEE) runs within a\nprovider's infrastructure, a user cannot accurately assess the risks of\nphysical attacks. We observe a misalignment in the threat model where the\nworkloads are protected against other tenants but do not offer end-to-end\nsecurity assurances to external users without relying on cloud providers. The\nattestation should be extended to bind the CVM with the provider. A possible\nsolution can rely on the Protected Platform Identifier (PPID), a unique CPU\nidentifier. However, the implementation details of various TEE manufacturers,\nattestation flows, and providers vary. This makes verification of attestations,\nease of migration, and building applications without relying on a trusted party\nchallenging, highlighting a key limitation that must be addressed for the\nadoption of CVMs. We discuss two points focusing on hardening and extensions of\nTEEs' attestation.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.14964v1", "AI": {"title_translation": "缩小TEE威胁模型与部署策略之间的差距", "tldr": "本文指出，机密虚拟机（CVM）的威胁模型与实际部署之间存在差距，即CVM的认证不包含提供商信息，导致用户无法评估物理攻击风险。文章讨论了通过扩展认证来解决这一问题，并提出了基于受保护平台标识符（PPID）的可能解决方案，强调了需要解决的局限性。", "motivation": "机密虚拟机（CVMs）虽然为使用中的数据提供了隔离保证，但其威胁模型不包括物理层保护和侧信道攻击。当前部署依赖于受信任的云提供商，但TEE认证不提供有关托管CVM的操作者的信息。这导致用户无法准确评估物理攻击的风险，凸显了威胁模型与部署策略之间的不一致性。", "method": "作者观察到威胁模型与部署的错位，并提出应扩展认证以将CVM与提供商绑定。一个可能的解决方案是依赖受保护平台标识符（PPID）。文章讨论了加强和扩展TEE认证的两个重点。", "result": "Not mentioned in abstract", "conclusion": "TEE认证的当前实现使得验证、迁移和在不依赖受信任方的情况下构建应用程序变得具有挑战性，这是CVMs普及必须解决的关键限制。", "translation": "机密虚拟机（CVMs）为使用中的数据提供了隔离保证，但其威胁模型不包括物理层保护和侧信道攻击。因此，当前的部署依赖于受信任的云提供商来托管CVM底层基础设施。然而，TEE认证不提供有关托管CVM的操作者的信息。在不知道可信执行环境（TEE）是否在提供商基础设施内运行的情况下，用户无法准确评估物理攻击的风险。我们观察到威胁模型存在错位，即工作负载受到其他租户的保护，但在不依赖云提供商的情况下，无法向外部用户提供端到端安全保证。认证应扩展以将CVM与提供商绑定。一个可能的解决方案可以依赖于受保护平台标识符（PPID），一个独特的CPU标识符。然而，各种TEE制造商、认证流程和提供商的实现细节各不相同。这使得认证的验证、迁移的便利性以及在不依赖受信任方的情况下构建应用程序变得具有挑战性，突出了CVMs普及必须解决的关键限制。我们讨论了两个重点，侧重于加强和扩展TEE的认证。", "summary": "本文探讨了机密虚拟机（CVMs）在威胁模型与部署策略之间存在的差距。尽管CVMs提供数据隔离，但其威胁模型未涵盖物理层保护和侧信道攻击。目前的TEE认证不包含提供商信息，导致用户难以评估物理攻击风险。文章指出，这种威胁模型的错位使得CVMs无法为外部用户提供端到端安全保障，除非依赖云提供商。为解决此问题，作者建议扩展认证以绑定CVM与提供商，并提出利用受保护平台标识符（PPID）作为潜在方案。文中强调了当前实现中认证验证、迁移和构建无信任第三方应用的挑战，并讨论了加强和扩展TEE认证的必要性。", "keywords": "机密虚拟机, TEE, 威胁模型, 认证, PPID", "comments": "本文识别并讨论了可信执行环境（TEE）和机密虚拟机（CVM）部署中的一个关键安全差距，即认证信息不足以消除对云提供商的信任。提出通过扩展认证和利用PPID来解决这一问题，指出了一个重要的研究方向，对于CVMs的广泛采用具有重要意义。其创新之处在于明确指出了现有威胁模型与实际部署之间的不一致性，并提出了具体的改进方向。"}}
{"id": "2506.15018", "title": "Private Continual Counting of Unbounded Streams", "authors": ["Ben Jacobsen", "Kassem Fawaz"], "summary": "We study the problem of differentially private continual counting in the\nunbounded setting where the input size $n$ is not known in advance. Current\nstate-of-the-art algorithms based on optimal instantiations of the matrix\nmechanism cannot be directly applied here because their privacy guarantees only\nhold when key parameters are tuned to $n$. Using the common `doubling trick'\navoids knowledge of $n$ but leads to suboptimal and non-smooth error. We solve\nthis problem by introducing novel matrix factorizations based on logarithmic\nperturbations of the function $\\frac{1}{\\sqrt{1-z}}$ studied in prior works,\nwhich may be of independent interest. The resulting algorithm has smooth error,\nand for any $\\alpha > 0$ and $t\\leq n$ it is able to privately estimate the sum\nof the first $t$ data points with $O(\\log^{2+2\\alpha}(t))$ variance. It\nrequires $O(t)$ space and amortized $O(\\log t)$ time per round, compared to\n$O(\\log(n)\\log(t))$ variance, $O(n)$ space and $O(n \\log n)$ pre-processing\ntime for the nearly-optimal bounded-input algorithm of Henzinger et al. (SODA\n2023). Empirically, we find that our algorithm's performance is also comparable\nto theirs in absolute terms: our variance is less than $1.5\\times$ theirs for\n$t$ as large as $2^{24}$.", "comment": "12 pages, 2 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15018v1", "AI": {"title_translation": "无界流的私有持续计数", "tldr": "本文提出了一种用于无界数据流的差分隐私持续计数新算法，通过引入新颖的矩阵分解，解决了现有方法需要预知输入大小的限制，并实现了平滑误差和高效的性能。", "motivation": "当前最先进的差分隐私持续计数算法（基于矩阵机制）需要事先知道输入大小 $n$ 才能保证隐私。虽然“加倍技巧”可以避免这种预知，但会导致次优和非平滑的误差。本文的动机是解决无界流设置下差分隐私持续计数的问题。", "method": "通过引入基于先前工作中研究的函数 $\\frac{1}{\\sqrt{1-z}}$ 的对数扰动的新型矩阵分解。", "result": "所提出的算法具有平滑误差，对于任何 $\\alpha > 0$ 和 $t \\leq n$，它能够以 $O(\\log^{2+2\\alpha}(t))$ 的方差私密地估计前 $t$ 个数据点的总和。它需要 $O(t)$ 的空间和每轮摊销 $O(\\log t)$ 的时间。与Henzinger等人的近似最优有界输入算法相比，其性能更优或相当（该算法需要 $O(\\log(n)\\log(t))$ 的方差、$O(n)$ 的空间和 $O(n \\log n)$ 的预处理时间）。经验上，对于高达 $2^{24}$ 的 $t$，我们的算法方差小于他们的1.5倍。", "conclusion": "本文提出了一种用于无界流差分隐私持续计数的新算法，该算法通过提供平滑误差和具有竞争力的性能，克服了现有方法的局限性，且无需事先了解输入大小。", "translation": "我们研究了无界设置下差分隐私持续计数的问题，其中输入大小 $n$ 是事先未知的。当前最先进的基于矩阵机制最优实例的算法不能直接应用于此处，因为它们的隐私保证只有在关键参数调整到 $n$ 时才成立。使用常见的“加倍技巧”可以避免对 $n$ 的了解，但会导致次优和非平滑的误差。我们通过引入基于先前工作中研究的函数 $\\frac{1}{\\sqrt{1-z}}$ 的对数扰动的新型矩阵分解来解决这个问题，这可能具有独立的兴趣。由此产生的算法具有平滑误差，对于任何 $\\alpha > 0$ 和 $t \\leq n$，它能够以 $O(\\log^{2+2\\alpha}(t))$ 的方差私密地估计前 $t$ 个数据点的总和。它需要 $O(t)$ 的空间和每轮摊销 $O(\\log t)$ 的时间，而 Henzinger 等人（SODA 2023）的近似最优有界输入算法需要 $O(\\log(n)\\log(t))$ 的方差、$O(n)$ 的空间和 $O(n \\log n)$ 的预处理时间。经验上，我们发现我们的算法的性能在绝对值上也与他们的相当：对于高达 $2^{24}$ 的 $t$，我们的方差小于他们的 1.5 倍。", "summary": "本文针对无界数据流的差分隐私持续计数问题，提出了一个新颖的解决方案。现有基于矩阵机制的算法因依赖预先已知的输入大小而无法直接应用于此场景，而简单的“加倍技巧”则会导致次优且非平滑的误差。为解决此问题，作者引入了基于先前研究函数对数扰动的新型矩阵分解。所提出的算法实现了平滑误差，能够以 $O(\\log^{2+2\\alpha}(t))$ 的方差私密估计前 $t$ 个数据点的总和，并具有 $O(t)$ 的空间复杂度和每轮摊销 $O(\\log t)$ 的时间复杂度。与现有有界输入算法相比，该算法在资源使用和经验性能上表现出优越性或可比性，即使对于非常大的流长度也如此。", "keywords": "差分隐私, 持续计数, 无界流, 矩阵机制, 对数扰动", "comments": "本文为差分隐私领域中的一个挑战性问题（无界数据流的持续计数）提供了一种创新方法。其关键贡献在于新颖的矩阵分解，这使得算法能够在无需预知流长度的情况下实现平滑误差和高效性能。这与需要此类知识或产生次优结果的现有方法相比，是一个显著的改进。实验结果进一步验证了该算法的实际应用价值。"}}
{"id": "2506.14866", "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents", "authors": ["Thomas Kuntz", "Agatha Duzan", "Hao Zhao", "Francesco Croce", "Zico Kolter", "Nicolas Flammarion", "Maksym Andriushchenko"], "summary": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.14866v1", "AI": {"title_translation": "OS-Harm：衡量计算机使用代理安全性的基准", "tldr": "OS-Harm是一个新的基准测试，用于评估基于LLM的计算机使用代理的安全性，涵盖了用户滥用、提示注入和模型不当行为等有害类别，并发现当前模型存在安全漏洞。", "motivation": "尽管基于LLM的计算机使用代理越来越受欢迎，但它们的安全性却被大大忽视，而评估和理解其潜在的有害行为对于广泛采用至关重要。本文旨在弥补这一空白。", "method": "引入了OS-Harm，这是一个建立在OSWorld环境之上的新基准，用于衡量计算机使用代理的安全性。该基准旨在测试模型在三种危害类别中的表现：故意用户滥用、提示注入攻击和模型不当行为。为此，创建了150个涵盖多种安全违规类型（如骚扰、版权侵犯、虚假信息、数据泄露等）的任务，并要求代理与各种操作系统应用程序交互。此外，提出了一种自动化评估器来评估代理的准确性和安全性，该评估器与人工标注具有高度一致性。使用一系列前沿模型（如o4-mini、Claude 3.7 Sonnet、Gemini 2.5 Pro）评估了计算机使用代理。", "result": "所有模型都倾向于直接遵守许多故意的滥用查询，相对容易受到静态提示注入的影响，并且偶尔会执行不安全的操作。", "conclusion": "OS-Harm基准测试揭示了当前计算机使用代理在安全性方面存在显著的漏洞，特别是在处理用户滥用、提示注入和潜在的不安全行为方面，这表明在广泛部署这些系统之前，需要进一步改进其安全机制。", "translation": "计算机使用代理是基于LLM的代理，它们可以通过处理屏幕截图或辅助功能树直接与图形用户界面交互。尽管这些系统越来越受欢迎，但它们的安全性却被大大忽视，尽管评估和理解其潜在的有害行为对于广泛采用至关重要。为了弥补这一空白，我们引入了OS-Harm，一个用于衡量计算机使用代理安全性的新基准。OS-Harm建立在OSWorld环境之上，旨在测试模型在三种危害类别中的表现：故意用户滥用、提示注入攻击和模型不当行为。为了涵盖这些情况，我们创建了150个任务，这些任务涵盖了多种类型的安全违规（骚扰、版权侵犯、虚假信息、数据泄露等），并要求代理与各种操作系统应用程序（电子邮件客户端、代码编辑器、浏览器等）进行交互。此外，我们提出了一种自动化评估器来评估代理的准确性和安全性，该评估器与人工标注具有高度一致性（F1分数分别为0.76和0.79）。我们评估了基于一系列前沿模型（如o4-mini、Claude 3.7 Sonnet、Gemini 2.5 Pro）的计算机使用代理，并提供了对其安全性的见解。特别是，所有模型都倾向于直接遵守许多故意的滥用查询，相对容易受到静态提示注入的影响，并且偶尔会执行不安全的操作。OS-Harm基准测试可在https://github.com/tml-epfl/os-harm上获取。", "summary": "本文提出了OS-Harm，这是一个新颖的基准测试，旨在解决LLM驱动的计算机使用代理在安全性评估方面的空白。该基准基于OSWorld环境，包含150个任务，涵盖故意用户滥用、提示注入攻击和模型不当行为等三类潜在危害，涉及多种安全违规类型和OS应用程序交互。研究还开发了一个与人类标注高度一致的自动化评估器。对包括o4-mini、Claude 3.7 Sonnet和Gemini 2.5 Pro在内的前沿模型进行的评估显示，这些模型普遍存在安全漏洞，例如容易遵守滥用查询、易受提示注入攻击以及偶尔执行不安全操作。", "keywords": "计算机使用代理, 安全性, 基准, LLM, OS-Harm", "comments": "本文的创新之处在于提出了一个专门针对计算机使用代理安全性的综合基准OS-Harm，并引入了自动化评估器，填补了该领域的重要空白。其重要性在于强调了当前LLM代理在实际应用中存在的安全风险，为未来代理的设计和改进提供了关键见解。该基准的发布为研究人员提供了评估和提升代理安全性的工具。"}}
{"id": "2506.15066", "title": "ChatModel: Automating Reference Model Design and Verification with LLMs", "authors": ["Jianmin Ye", "Tianyang Liu", "Qi Tian", "Shengchu Su", "Zhe Jiang", "Xi Wang"], "summary": "As the complexity of integrated circuit designs continues to escalate, the\nfunctional verification becomes increasingly challenging. Reference models,\ncritical for accelerating the verification process, are themselves becoming\nmore intricate and time-consuming to develop. Despite the promise shown by\nlarge language models (LLMs) in code programming, effectively generating\ncomplex reference models remains a significant hurdle. To address these\nchallenges, we introduce ChatModel, the first LLM-aided agile reference model\ngeneration and verification platform. ChatModel streamlines the transition from\ndesign specifications to fully functional reference models by integrating\ndesign standardization and hierarchical agile modeling. Employing a\nbuilding-block generation strategy, it not only enhances the design\ncapabilities of LLMs for reference models but also significantly boosts\nverification efficiency. We evaluated ChatModel on 300 designs of varying\ncomplexity, demonstrating substantial improvements in both efficiency and\nquality of reference model generation. ChatModel achieved a peak performance\nimprovement of 55.02% compared to alternative methods, with notable\nenhancements in generation stability, and delivered a 9.18x increase in its\ncapacity to produce reference model designs. Furthermore, it accelerated the\niterative process of reference model design and validation by an average of\n5.90x compared to traditional approaches. These results highlight the potential\nof ChatModel to significantly advance the automation of reference model\ngeneration and validation.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15066v1", "AI": {"title_translation": "ChatModel：利用大型语言模型自动化参考模型设计与验证", "tldr": "ChatModel是一个利用大型语言模型（LLMs）自动化集成电路设计中参考模型生成和验证的平台，显著提升了效率和质量。", "motivation": "随着集成电路设计复杂性不断增加，功能验证变得更具挑战性，而参考模型的开发也日益复杂和耗时。尽管大型语言模型在代码编程方面表现出潜力，但有效生成复杂的参考模型仍是一个重大难题。", "method": "本文提出了ChatModel，首个由大型语言模型辅助的敏捷参考模型生成与验证平台。ChatModel通过整合设计标准化和分层敏捷建模，简化了从设计规范到功能齐全参考模型的转换。它采用积木式生成策略，提升了大型语言模型设计参考模型的能力并显著提高了验证效率。", "result": "ChatModel在300个不同复杂度的设计上进行了评估，结果显示其在参考模型生成效率和质量上都有显著提升。与替代方法相比，ChatModel的峰值性能提升了55.02%，生成稳定性显著增强，生产参考模型设计的能力提高了9.18倍。此外，它将参考模型设计和验证的迭代过程平均加速了5.90倍。", "conclusion": "ChatModel的评估结果表明，它在显著推动参考模型生成和验证自动化方面具有巨大潜力。", "translation": "随着集成电路设计的复杂性持续升级，功能验证变得越来越具挑战性。参考模型对于加速验证过程至关重要，但其本身也变得越来越复杂且耗时。尽管大型语言模型（LLM）在代码编程方面展现出希望，但有效生成复杂的参考模型仍然是一个重大障碍。为了应对这些挑战，我们引入了ChatModel，这是第一个由LLM辅助的敏捷参考模型生成和验证平台。ChatModel通过整合设计标准化和分层敏捷建模，简化了从设计规范到功能齐全参考模型的转换。它采用积木式生成策略，不仅增强了LLM设计参考模型的能力，还显著提高了验证效率。我们对ChatModel在300个不同复杂度的设计上进行了评估，结果表明其在参考模型生成效率和质量方面均有显著提升。与替代方法相比，ChatModel的峰值性能提升了55.02%，生成稳定性显著增强，并且其生产参考模型设计的能力提升了9.18倍。此外，与传统方法相比，它将参考模型设计和验证的迭代过程平均加速了5.90倍。这些结果突显了ChatModel在显著推进参考模型生成和验证自动化方面的潜力。", "summary": "ChatModel是一个创新平台，它利用大型语言模型（LLMs）来自动化集成电路设计中复杂参考模型的生成和验证。该平台通过整合设计标准化和分层敏捷建模，以及采用积木式生成策略，解决了传统参考模型开发耗时且复杂的难题。实验结果表明，ChatModel显著提高了参考模型生成的效率和质量，性能提升高达55.02%，生成能力增加9.18倍，并加速了迭代过程。这展示了LLMs在推动集成电路设计自动化方面的巨大潜力。", "keywords": "参考模型, 大型语言模型, 功能验证, 自动化, 集成电路设计", "comments": "ChatModel的创新之处在于它是首个将大型语言模型应用于集成电路参考模型敏捷生成和验证的平台。通过引入设计标准化和积木式生成策略，它有效克服了LLM在生成复杂工程代码方面的局限性。其显著的性能提升和效率加速，预示着该技术在未来集成电路设计自动化领域的重要应用前景。"}}
{"id": "2506.14771", "title": "Empirical Studies of Large Scale Environment Scanning by Consumer Electronics", "authors": ["Mengyuan Wang", "Yang Liu", "Haopeng Wang", "Haiwei Dong", "Abdulmotaleb El Saddik"], "summary": "This paper presents an empirical evaluation of the Matterport Pro3, a\nconsumer-grade 3D scanning device, for large-scale environment reconstruction.\nWe conduct detailed scanning (1,099 scanning points) of a six-floor building\n(17,567 square meters) and assess the device's effectiveness, limitations, and\nperformance enhancements in diverse scenarios. Challenges encountered during\nthe scanning are addressed through proposed solutions, while we also explore\nadvanced methods to overcome them more effectively. Comparative analysis with\nanother consumer-grade device (iPhone) highlights the Pro3's balance between\ncost-effectiveness and performance. The Matterport Pro3 achieves a denser point\ncloud with 1,877,324 points compared to the iPhone's 506,961 points and higher\nalignment accuracy with an RMSE of 0.0118 meters. The cloud-to-cloud (C2C)\naverage distance error between the two point cloud models is 0.0408 meters,\nwith a standard deviation of 0.0715 meters. The study demonstrates the Pro3's\nability to generate high-quality 3D models suitable for large-scale\napplications, leveraging features such as LiDAR and advanced alignment\ntechniques.", "comment": "Accepted by IEEE Consumer Electronics Magazine", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14771v1", "AI": {"title_translation": "消费电子产品大规模环境扫描的实证研究", "tldr": "本文对消费级3D扫描设备Matterport Pro3在大规模环境重建中的性能进行了实证评估，通过与iPhone的对比，发现Pro3在成本效益和性能之间取得了良好平衡，能生成高质量的3D模型。", "motivation": "评估Matterport Pro3消费级3D扫描设备在大规模环境重建中的有效性、局限性和性能增强。", "method": "对一个六层建筑（17,567平方米）进行了详细扫描（1,099个扫描点），并评估了设备在不同场景中的表现。提出了解决扫描挑战的方案，并探索了更有效的先进方法。与另一款消费级设备（iPhone）进行了比较分析。", "result": "Matterport Pro3生成了更密集的点云（1,877,324点），而iPhone生成了506,961点，并且Pro3具有更高的对齐精度，RMSE为0.0118米。两个点云模型之间的云到云（C2C）平均距离误差为0.0408米，标准偏差为0.0715米。该研究表明，Pro3能够生成适用于大规模应用的高质量3D模型。", "conclusion": "Matterport Pro3在成本效益和性能之间取得了良好平衡，能够生成适用于大规模应用的高质量3D模型。", "translation": "本文对消费级3D扫描设备Matterport Pro3在大规模环境重建中的应用进行了实证评估。我们对一座六层建筑（17,567平方米）进行了详细扫描（1,099个扫描点），并评估了该设备在不同场景下的有效性、局限性和性能增强。文中提出了解决扫描过程中遇到的挑战的方案，同时我们还探索了更有效地克服这些挑战的先进方法。与另一款消费级设备（iPhone）进行的比较分析突出了Pro3在成本效益和性能之间的平衡。Matterport Pro3生成了更密集的点云（1,877,324点），而iPhone生成了506,961点，并且Pro3具有更高的对齐精度，RMSE为0.0118米。两个点云模型之间的云到云（C2C）平均距离误差为0.0408米，标准偏差为0.0715米。该研究表明，Pro3能够利用LiDAR和先进的对齐技术等功能，生成适用于大规模应用的高质量3D模型。", "summary": "本文对Matterport Pro3消费级3D扫描设备在大规模环境重建中的性能进行了实证评估。研究通过扫描一座六层建筑，详细分析了设备的有效性、局限性及性能提升。通过与iPhone的对比，结果显示Pro3在点云密度和对齐精度上表现更优，证明其在成本效益和性能之间取得了良好平衡，能够生成适用于大规模应用的高质量3D模型。", "keywords": "Matterport Pro3, 3D扫描, 大规模环境重建, 消费级设备, 点云", "comments": "这项研究通过实际大规模环境扫描，提供了Matterport Pro3的详细性能评估，填补了消费级3D扫描设备在大规模应用中性能数据的空白。其创新之处在于对真实世界大规模场景的实证研究，并提供了量化比较数据。这对于需要进行大规模3D建模的专业人士或研究人员具有重要参考价值。"}}
{"id": "2506.14855", "title": "Feedback-MPPI: Fast Sampling-Based MPC via Rollout Differentiation -- Adios low-level controllers", "authors": ["Tommaso Belvedere", "Michael Ziegltrum", "Giulio Turrisi", "Valerio Modugno"], "summary": "Model Predictive Path Integral control is a powerful sampling-based approach\nsuitable for complex robotic tasks due to its flexibility in handling nonlinear\ndynamics and non-convex costs. However, its applicability in real-time,\nhighfrequency robotic control scenarios is limited by computational demands.\nThis paper introduces Feedback-MPPI (F-MPPI), a novel framework that augments\nstandard MPPI by computing local linear feedback gains derived from sensitivity\nanalysis inspired by Riccati-based feedback used in gradient-based MPC. These\ngains allow for rapid closed-loop corrections around the current state without\nrequiring full re-optimization at each timestep. We demonstrate the\neffectiveness of F-MPPI through simulations and real-world experiments on two\nrobotic platforms: a quadrupedal robot performing dynamic locomotion on uneven\nterrain and a quadrotor executing aggressive maneuvers with onboard\ncomputation. Results illustrate that incorporating local feedback significantly\nimproves control performance and stability, enabling robust, high-frequency\noperation suitable for complex robotic systems.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14855v1", "AI": {"title_translation": "Feedback-MPPI：通过展开微分实现快速基于采样的MPC——告别低级控制器", "tldr": "Feedback-MPPI (F-MPPI) 是一种新颖的基于采样的模型预测控制方法，它通过计算局部线性反馈增益来提高性能和稳定性，从而实现复杂机器人系统的高频操作。", "motivation": "模型预测路径积分控制（MPPI）在处理非线性动力学和非凸成本方面具有灵活性，适用于复杂机器人任务，但其计算需求限制了其在实时、高频机器人控制场景中的应用。", "method": "本文提出了Feedback-MPPI (F-MPPI)，通过计算源自灵敏度分析的局部线性反馈增益来增强标准MPPI，这些增益类似于基于Riccati的梯度MPC中使用的反馈。这些增益允许围绕当前状态进行快速闭环校正，而无需在每个时间步进行完全重新优化。", "result": "F-MPPI在模拟和真实世界的实验中（在四足机器人和四旋翼飞行器上）展示了其有效性。结果表明，引入局部反馈显著提高了控制性能和稳定性，实现了适用于复杂机器人系统的鲁棒、高频操作。", "conclusion": "Feedback-MPPI通过结合局部线性反馈增益，显著提升了采样型模型预测控制的性能和稳定性，使其能够实现复杂机器人系统的高频和鲁棒操作。", "translation": "模型预测路径积分控制（MPPI）是一种强大的基于采样的方法，由于其在处理非线性动力学和非凸成本方面的灵活性，适用于复杂的机器人任务。然而，其在实时、高频机器人控制场景中的适用性受到计算需求的限制。本文介绍了Feedback-MPPI（F-MPPI），这是一种新颖的框架，通过计算源自灵敏度分析的局部线性反馈增益来增强标准MPPI，这些增益的灵感来自于基于Riccati的梯度MPC中使用的反馈。这些增益允许在当前状态周围进行快速闭环校正，而无需在每个时间步进行完全重新优化。我们通过在两个机器人平台上的模拟和真实世界实验证明了F-MPPI的有效性：一个在崎岖地形上进行动态运动的四足机器人和一个使用板载计算执行激进机动的四旋翼飞行器。结果表明，结合局部反馈显著提高了控制性能和稳定性，实现了适用于复杂机器人系统的鲁棒、高频操作。", "summary": "本文介绍了Feedback-MPPI（F-MPPI），这是一种增强型模型预测路径积分控制（MPPI）框架，旨在解决MPPI在高频实时机器人控制中面临的计算挑战。F-MPPI通过引入基于灵敏度分析的局部线性反馈增益，实现了快速闭环校正，从而避免了每一步的完全重新优化。在四足机器人和四旋翼飞行器上的仿真和真实实验表明，F-MPPI显著提升了控制性能和稳定性，使其能够为复杂机器人系统提供鲁棒的高频操作。", "keywords": "Feedback-MPPI, 模型预测控制, 采样方法, 机器人控制, 局部反馈", "comments": "本文的创新点在于将局部线性反馈增益引入到采样型MPPI中，有效地解决了其在实时高频应用中的计算瓶颈。这种方法通过避免每一步的完全重新优化，显著提高了控制效率和鲁棒性，对于需要快速响应和稳定性的复杂机器人系统具有重要意义。"}}
{"id": "2506.14775", "title": "See What I Mean? CUE: A Cognitive Model of Understanding Explanations", "authors": ["Tobias Labarta", "Nhi Hoang", "Katharina Weitz", "Wojciech Samek", "Sebastian Lapuschkin", "Leander Weber"], "summary": "As machine learning systems increasingly inform critical decisions, the need\nfor human-understandable explanations grows. Current evaluations of Explainable\nAI (XAI) often prioritize technical fidelity over cognitive accessibility which\ncritically affects users, in particular those with visual impairments. We\npropose CUE, a model for Cognitive Understanding of Explanations, linking\nexplanation properties to cognitive sub-processes: legibility (perception),\nreadability (comprehension), and interpretability (interpretation). In a study\n(N=455) testing heatmaps with varying colormaps (BWR, Cividis, Coolwarm), we\nfound comparable task performance but lower confidence/effort for visually\nimpaired users. Unlike expected, these gaps were not mitigated and sometimes\nworsened by accessibility-focused color maps like Cividis. These results\nchallenge assumptions about perceptual optimization and support the need for\nadaptive XAI interfaces. They also validate CUE by demonstrating that altering\nexplanation legibility affects understandability. We contribute: (1) a\nformalized cognitive model for explanation understanding, (2) an integrated\ndefinition of human-centered explanation properties, and (3) empirical evidence\nmotivating accessible, user-tailored XAI.", "comment": "10 pages, 5 figures (main text), 4 tables, 455-participant user study", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14775v1", "AI": {"title_translation": "“懂我意思吗？CUE：一个理解解释的认知模型”", "tldr": "CUE是一个新的认知模型，用于理解XAI解释。一项研究发现，无障碍色图并未改善视障用户的解释理解，反而有时会恶化，这表明需要适应性XAI界面。", "motivation": "随着机器学习系统越来越多地影响关键决策，对人类可理解解释的需求日益增长。当前的解释性AI（XAI）评估通常优先考虑技术保真度而非认知可及性，这严重影响用户，特别是视障用户。", "method": "提出CUE（解释认知理解模型），该模型将解释属性与认知子过程（易读性、可读性、可解释性）联系起来。通过一项针对455名参与者的研究，测试了不同颜色映射（BWR、Cividis、Coolwarm）的热图，以评估其对理解的影响。", "result": "研究发现，视障用户在任务表现上与其他人相似，但自信心/努力程度较低。与预期相反，这些差距并未通过可访问性色图（如Cividis）得到缓解，有时甚至恶化。结果表明改变解释的易读性会影响可理解性，并挑战了对感知优化的假设。", "conclusion": "本文提出了一个形式化的解释理解认知模型，并集成了以人为中心的解释属性定义。经验证据表明需要可访问、用户定制的XAI。研究结果支持了CUE模型，并强调了自适应XAI界面的重要性。", "translation": "随着机器学习系统越来越多地为关键决策提供信息，对人类可理解解释的需求日益增长。当前对可解释人工智能（XAI）的评估通常优先考虑技术保真度而非认知可及性，这严重影响用户，特别是视障用户。我们提出了CUE，一个解释认知理解模型，将解释属性与认知子过程联系起来：易读性（感知）、可读性（理解）和可解释性（解读）。在一项测试不同颜色映射（BWR、Cividis、Coolwarm）热图的研究（N=455）中，我们发现任务表现相当，但视障用户的信心/努力程度较低。与预期不同的是，这些差距并未通过可访问性聚焦的颜色映射（如Cividis）得到缓解，有时甚至恶化。这些结果挑战了关于感知优化的假设，并支持了对自适应XAI界面的需求。它们还通过证明改变解释的易读性会影响可理解性来验证CUE。我们的贡献包括：(1) 一个形式化的解释理解认知模型，(2) 以人为中心的解释属性的综合定义，以及 (3) 激励可访问、用户定制XAI的经验证据。", "summary": "本文提出了CUE，一个用于解释认知理解的模型，旨在解决当前解释性AI（XAI）评估中认知可及性不足的问题，尤其关注视障用户。通过一项455人的研究，测试了不同颜色映射的热图，发现视障用户在任务表现上与其他人相当，但信心和努力度较低，且无障碍色图未能有效缓解这些问题，有时甚至加剧。研究结果挑战了感知优化的假设，并验证了CUE模型，强调了自适应XAI界面的必要性。文章贡献了一个形式化的认知模型、人本解释属性定义以及支持可访问、用户定制XAI的实证证据。", "keywords": "解释性AI, 认知模型, 可及性, 视障用户, 热图", "comments": "这篇论文通过提出CUE认知模型，创新性地将解释属性与认知过程联系起来，填补了当前XAI评估中对认知可及性关注不足的空白。其针对视障用户的实证研究揭示了无障碍色图的局限性，挑战了现有假设，对于推动XAI向真正以人为本的方向发展具有重要意义。该研究强调了自适应XAI界面的必要性，为未来研究提供了明确方向。"}}
{"id": "2506.14826", "title": "Collaborative Interest-aware Graph Learning for Group Identification", "authors": ["Rui Zhao", "Beihong Jin", "Beibei Li", "Yiyuan Zheng"], "summary": "With the popularity of social media, an increasing number of users are\njoining group activities on online social platforms. This elicits the\nrequirement of group identification (GI), which is to recommend groups to\nusers. We reveal that users are influenced by both group-level and item-level\ninterests, and these dual-level interests have a collaborative evolution\nrelationship: joining a group expands the user's item interests, further\nprompting the user to join new groups. Ultimately, the two interests tend to\nalign dynamically. However, existing GI methods fail to fully model this\ncollaborative evolution relationship, ignoring the enhancement of group-level\ninterests on item-level interests, and suffering from false-negative samples\nwhen aligning cross-level interests. In order to fully model the collaborative\nevolution relationship between dual-level user interests, we propose CI4GI, a\nCollaborative Interest-aware model for Group Identification. Specifically, we\ndesign an interest enhancement strategy that identifies additional interests of\nusers from the items interacted with by the groups they have joined as a\nsupplement to item-level interests. In addition, we adopt the distance between\ninterest distributions of two users to optimize the identification of negative\nsamples for a user, mitigating the interference of false-negative samples\nduring cross-level interests alignment. The results of experiments on three\nreal-world datasets demonstrate that CI4GI significantly outperforms\nstate-of-the-art models.", "comment": "accepted by ECML PKDD 2025", "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.14826v1", "AI": {"title_translation": "协作兴趣感知图学习用于群组识别", "tldr": "本文提出CI4GI模型，通过建模用户双层兴趣的协作演化关系并优化负样本识别，显著提升了群组识别的性能。", "motivation": "随着社交媒体的普及，在线社交平台上用户参与群组活动的需求日益增长，这催生了群组识别（GI）的需求。现有GI方法未能充分建模用户群组级和项目级双层兴趣的协作演化关系，忽略了群组级兴趣对项目级兴趣的增强作用，并且在对齐跨层兴趣时存在假阴性样本问题。", "method": "为了充分建模用户双层兴趣的协作演化关系，本文提出了CI4GI（Collaborative Interest-aware model for Group Identification）模型。具体来说，CI4GI设计了一种兴趣增强策略，从用户已加入群组交互过的项目识别出额外的用户兴趣，作为项目级兴趣的补充。此外，它还采用两个用户兴趣分布之间的距离来优化用户负样本的识别，从而减轻跨层兴趣对齐过程中假阴性样本的干扰。", "result": "在三个真实世界数据集上的实验结果表明，CI4GI显著优于现有最先进的模型。", "conclusion": "本文提出的CI4GI模型通过有效建模用户双层兴趣的协作演化关系，并通过兴趣增强和优化负样本识别的策略，成功解决了现有群组识别方法的局限性，实现了卓越的性能。", "translation": "随着社交媒体的普及，越来越多的用户正在在线社交平台上参与群组活动。这催生了群组识别（GI）的需求，即向用户推荐群组。我们发现用户受到群组级和项目级兴趣的影响，并且这些双层兴趣具有协作演化关系：加入一个群组会扩展用户的项目兴趣，进一步促使用户加入新的群组。最终，这两种兴趣倾向于动态对齐。然而，现有的GI方法未能充分建模这种协作演化关系，忽略了群组级兴趣对项目级兴趣的增强作用，并且在对齐跨层兴趣时存在假阴性样本问题。为了充分建模用户双层兴趣之间的协作演化关系，我们提出了CI4GI，一个协作兴趣感知群组识别模型。具体来说，我们设计了一种兴趣增强策略，从用户已加入群组交互过的项目中识别出额外的用户兴趣，作为项目级兴趣的补充。此外，我们采用两个用户兴趣分布之间的距离来优化用户负样本的识别，从而减轻跨层兴趣对齐过程中假阴性样本的干扰。在三个真实世界数据集上的实验结果表明，CI4GI显著优于现有最先进的模型。", "summary": "本研究针对群组识别（GI）问题，提出了一种名为CI4GI的协作兴趣感知模型。该模型旨在解决现有GI方法未能充分建模用户群组级和项目级双层兴趣协作演化关系，以及在跨层兴趣对齐中存在假阴性样本的问题。CI4GI通过设计兴趣增强策略来补充项目级兴趣，并利用用户兴趣分布距离优化负样本识别。实验结果表明，该模型在真实数据集上显著优于现有先进方法。", "keywords": "群组识别, 双层兴趣, 协作演化, 图学习, 假阴性样本", "comments": "本文的创新点在于揭示并建模了用户群组级和项目级双层兴趣的协作演化关系，这为群组识别提供了一个新颖的视角。通过提出的兴趣增强策略和负样本优化方法，有效解决了现有模型在兴趣建模和样本对齐方面的局限性，对提升群组推荐的准确性具有重要意义。该研究方法严谨，并取得了显著的实验效果。"}}
{"id": "2506.14987", "title": "CNN-Enabled Scheduling for Probabilistic Real-Time Guarantees in Industrial URLLC", "authors": ["Eman Alqudah", "Ashfaq Khokhar"], "summary": "Ensuring packet-level communication quality is vital for ultra-reliable,\nlow-latency communications (URLLC) in large-scale industrial wireless networks.\nWe enhance the Local Deadline Partition (LDP) algorithm by introducing a\nCNN-based dynamic priority prediction mechanism for improved interference\ncoordination in multi-cell, multi-channel networks. Unlike LDP's static\npriorities, our approach uses a Convolutional Neural Network and graph coloring\nto adaptively assign link priorities based on real-time traffic, transmission\nopportunities, and network conditions. Assuming that first training phase is\nperformed offline, our approach introduced minimal overhead, while enabling\nmore efficient resource allocation, boosting network capacity, SINR, and\nschedulability. Simulation results show SINR gains of up to 113\\%, 94\\%, and\n49\\% over LDP across three network configurations, highlighting its\neffectiveness for complex URLLC scenarios.", "comment": "This paper has been submitted to IEEEGLOBE2025 on April 15, 2025", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.14987v1", "AI": {"title_translation": "工业URLLC中基于CNN的概率实时保证调度", "tldr": "本文提出一种基于CNN的动态优先级预测机制，以增强LDP算法在工业URLLC网络中的干扰协调能力，显著提升了网络性能。", "motivation": "在大规模工业无线网络中，确保超可靠低延迟通信（URLLC）的包级通信质量至关重要。", "method": "通过引入基于CNN的动态优先级预测机制来增强局部截止日期分区（LDP）算法，以改善多小区、多信道网络中的干扰协调。该方法利用卷积神经网络和图着色，根据实时流量、传输机会和网络条件自适应地分配链路优先级。离线训练阶段后，该方法引入了最小的开销。", "result": "模拟结果显示，在三种网络配置下，相对于LDP算法，信噪比（SINR）增益分别高达113%、94%和49%。这表明该方法能够实现更高效的资源分配，提升网络容量、SINR和可调度性。", "conclusion": "该基于CNN的调度方法有效提升了复杂URLLC场景下的网络性能，特别是在信噪比方面表现出色。", "translation": "确保包级通信质量对于大规模工业无线网络中的超可靠低延迟通信（URLLC）至关重要。我们通过引入基于CNN的动态优先级预测机制，增强了局部截止日期分区（LDP）算法，以改善多小区、多信道网络中的干扰协调。与LDP的静态优先级不同，我们的方法使用卷积神经网络和图着色，根据实时流量、传输机会和网络条件自适应地分配链路优先级。假设第一阶段训练离线进行，我们的方法引入了最小的开销，同时实现了更高效的资源分配，提升了网络容量、信噪比（SINR）和可调度性。模拟结果显示，在三种网络配置下，相对于LDP，信噪比（SINR）增益分别高达113%、94%和49%，突出了其在复杂URLLC场景中的有效性。", "summary": "本文针对大规模工业URLLC网络中的通信质量挑战，提出了一种改进的局部截止日期分区（LDP）算法。该算法引入基于卷积神经网络（CNN）的动态优先级预测机制和图着色技术，以自适应地协调多小区、多信道环境下的干扰。与传统的静态优先级LDP不同，新方法能根据实时网络条件动态分配链路优先级，且仅产生最小开销。模拟结果证实，该方法显著提升了网络容量、可调度性，并在信噪比方面取得了高达113%的显著增益，证明了其在复杂URLLC场景中的有效性。", "keywords": "URLLC, CNN, 调度, 动态优先级, 干扰协调", "comments": "本文创新性地将CNN引入到URLLC调度中，通过动态优先级预测有效解决了多小区干扰问题，显著提升了网络性能，对于工业物联网的可靠通信具有重要意义。"}}
{"id": "2506.15045", "title": "An Integrated Sensing and Communication System for Time-Sensitive Targets with Random Arrivals", "authors": ["Homa Nikbakht", "Yonina C. Eldar", "H. Vincent Poor"], "summary": "In 6G networks, integrated sensing and communication (ISAC) is envisioned as\na key technology that enables wireless systems to perform joint sensing and\ncommunication using shared hardware, antennas and spectrum. ISAC designs\nfacilitate emerging applications such as smart cities and autonomous driving.\nSuch applications also demand ultra-reliable and low-latency communication\n(URLLC). Thus, an ISAC-enabled URLLC system can prioritize time-sensitive\ntargets and ensure information delivery under strict latency and reliability\nconstraints. We propose a bi-static MIMO ISAC system to detect the arrival of\nURLLC messages and prioritize their delivery. In this system, a base station\n(BS) communicates with a user equipment (UE) and a sensing receiver (SR) is\ndeployed to collect echo signals reflected from a target of interest. The BS\nregularly transmits messages of enhanced mobile broadband (eMBB) services to\nthe UE. During each eMBB transmission, if the SR senses the presence of a\ntarget of interest, it immediately triggers the transmission of an additional\nURLLC message. To reinforce URLLC transmissions, we propose a dirty-paper\ncoding (DPC)-based technique that mitigates the interference of both eMBB and\nsensing signals. To decode the eMBB message, we consider two approaches for\nhandling the URLLC interference: treating interference as noise and successive\ninterference cancellation. For this system, we formulate the\nrate-reliability-detection trade-off in the finite blocklength (FBL) regime by\nevaluating the communication rate of the eMBB transmissions, the reliability of\nthe URLLC transmissions and the probability of the target detection. Our\nnumerical analysis show that our proposed DPC-based ISAC scheme significantly\noutperforms power-sharing and traditional time-sharing schemes. In particular,\nit achieves higher eMBB transmission rate while satisfying both URLLC and\nsensing constraints.", "comment": "This work has been submitted to the IEEE for possible publication", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15045v1", "AI": {"title_translation": "用于随机到达时间敏感目标的集成感知与通信系统", "tldr": "本文提出了一种基于脏纸编码（DPC）的集成感知与通信（ISAC）系统，用于处理随机到达的时间敏感目标，通过数值分析表明其在实现更高eMBB速率的同时满足URLLC和感知约束，优于传统的功率共享和时分共享方案。", "motivation": "在6G网络中，集成感知与通信（ISAC）被设想为一项关键技术，能够实现无线系统的联合感知和通信。智能城市和自动驾驶等新兴应用对超可靠和低延迟通信（URLLC）提出了高要求。因此，需要一个支持ISAC的URLLC系统来优先处理时间敏感目标，并在严格的延迟和可靠性约束下确保信息传输。", "method": "本文提出了一种双基地MIMO ISAC系统，用于检测URLLC消息的到达并优先传输。在该系统中，基站（BS）与用户设备（UE）通信，同时部署感知接收器（SR）收集目标反射的回波信号。当SR感知到目标存在时，立即触发额外URLLC消息的传输。为增强URLLC传输，提出了一种基于脏纸编码（DPC）的技术来减轻eMBB和感知信号的干扰。在解码eMBB消息时，考虑了将干扰视为噪声和连续干扰消除两种URLLC干扰处理方法。该系统在有限块长（FBL）状态下，通过评估eMBB通信速率、URLLC可靠性和目标检测概率，制定了速率-可靠性-检测的权衡。", "result": "数值分析表明，所提出的基于DPC的ISAC方案显著优于功率共享和传统时分共享方案。具体而言，它在满足URLLC和感知约束的同时，实现了更高的增强型移动宽带（eMBB）传输速率。", "conclusion": "本文提出的基于脏纸编码（DPC）的集成感知与通信（ISAC）方案，有效地集成了感知和通信功能，用于处理时间敏感目标。该方案在满足超可靠和低延迟通信（URLLC）与感知要求的同时，提高了增强型移动宽带（eMBB）的传输速率，使其成为未来6G网络中一个有前景的解决方案。", "translation": "在6G网络中，集成感知与通信（ISAC）被设想为一项关键技术，使无线系统能够使用共享的硬件、天线和频谱执行联合感知和通信。ISAC设计促进了智能城市和自动驾驶等新兴应用。此类应用还要求超可靠和低延迟通信（URLLC）。因此，支持ISAC的URLLC系统可以优先处理时间敏感目标，并确保在严格的延迟和可靠性约束下传输信息。我们提出了一种双基地MIMO ISAC系统来检测URLLC消息的到达并优先传输它们。在该系统中，基站（BS）与用户设备（UE）通信，并部署一个感知接收器（SR）来收集从感兴趣目标反射的回波信号。BS定期向UE传输增强型移动宽带（eMBB）服务的消息。在每次eMBB传输期间，如果SR感知到感兴趣目标的存在，它会立即触发额外URLLC消息的传输。为了加强URLLC传输，我们提出了一种基于脏纸编码（DPC）的技术，该技术可以减轻eMBB和感知信号的干扰。为了解码eMBB消息，我们考虑了两种处理URLLC干扰的方法：将干扰视为噪声和连续干扰消除。对于该系统，我们通过评估eMBB传输的通信速率、URLLC传输的可靠性以及目标检测概率，在有限块长（FBL）状态下制定了速率-可靠性-检测的权衡。我们的数值分析表明，我们提出的基于DPC的ISAC方案显著优于功率共享和传统时分共享方案。特别是，它在满足URLLC和感知约束的同时，实现了更高的eMBB传输速率。", "summary": "本文提出了一种双基地MIMO集成感知与通信（ISAC）系统，利用脏纸编码（DPC）技术增强6G网络中时间敏感目标的超可靠和低延迟通信（URLLC）。该系统通过感知事件触发URLLC消息的优先传输，并有效管理增强型移动宽带（eMBB）和感知信号之间的干扰。数值分析结果表明，该基于DPC的方案优于传统的功率共享和时分共享方法，能在满足严格URLLC和感知约束的同时，实现更高的eMBB传输速率。", "keywords": "集成感知与通信（ISAC）, 超可靠低延迟通信（URLLC）, 脏纸编码（DPC）, 有限块长（FBL）, 6G", "comments": "该论文提出了一种创新的基于脏纸编码（DPC）的方法来解决ISAC系统中的干扰问题，这对于集成URLLC和感知功能至关重要。其优势在于证明了其性能优于传统的资源分配方法，凸显了其在未来需要高数据速率（eMBB）和严格可靠性/低延迟（URLLC）的动态环境中的6G应用潜力。有限块长分析也是对URLLC的一个相关贡献。"}}
{"id": "2506.14791", "title": "SemIRNet: A Semantic Irony Recognition Network for Multimodal Sarcasm Detection", "authors": ["Jingxuan Zhou", "Yuehao Wu", "Yibo Zhang", "Yeyubei Zhang", "Yunchong Liu", "Bolin Huang", "Chunhong Yuan"], "summary": "Aiming at the problem of difficulty in accurately identifying graphical\nimplicit correlations in multimodal irony detection tasks, this paper proposes\na Semantic Irony Recognition Network (SemIRNet). The model contains three main\ninnovations: (1) The ConceptNet knowledge base is introduced for the first time\nto acquire conceptual knowledge, which enhances the model's common-sense\nreasoning ability; (2) Two cross-modal semantic similarity detection modules at\nthe word level and sample level are designed to model graphic-textual\ncorrelations at different granularities; and (3) A contrastive learning loss\nfunction is introduced to optimize the spatial distribution of the sample\nfeatures, which improves the separability of positive and negative samples.\nExperiments on a publicly available multimodal irony detection benchmark\ndataset show that the accuracy and F1 value of this model are improved by 1.64%\nand 2.88% to 88.87% and 86.33%, respectively, compared with the existing\noptimal methods. Further ablation experiments verify the important role of\nknowledge fusion and semantic similarity detection in improving the model\nperformance.", "comment": "5 pages, 3 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14791v1", "AI": {"title_translation": "SemIRNet：一种用于多模态讽刺检测的语义反讽识别网络", "tldr": "本文提出了一种名为 SemIRNet 的语义反讽识别网络，通过引入 ConceptNet 知识库、设计跨模态语义相似性检测模块和对比学习损失函数，解决了多模态反讽检测中图形隐含关联难以准确识别的问题，并在公开数据集上取得了显著的性能提升。", "motivation": "现有方法在多模态反讽检测任务中，难以准确识别图形隐含关联，导致检测效果不佳。", "method": "本文提出了 SemIRNet（语义反讽识别网络）。该模型包含三项主要创新：1) 首次引入 ConceptNet 知识库获取概念知识，增强模型常识推理能力；2) 设计了词级和样本级两种跨模态语义相似性检测模块，以不同粒度建模图文关联；3) 引入对比学习损失函数，优化样本特征空间分布，提高正负样本可分离性。", "result": "在公开多模态反讽检测基准数据集上，SemIRNet 的准确率和 F1 值分别达到 88.87% 和 86.33%，比现有最优方法分别提高了 1.64% 和 2.88%。消融实验验证了知识融合和语义相似性检测对模型性能提升的重要作用。", "conclusion": "SemIRNet 通过引入外部知识、设计多粒度跨模态交互以及优化特征空间，有效解决了多模态反讽检测中图形隐含关联识别的难题，显著提升了模型的性能。", "translation": "针对多模态反讽检测任务中图形隐含关联难以准确识别的问题，本文提出了一种语义反讽识别网络（SemIRNet）。该模型包含三项主要创新：(1) 首次引入 ConceptNet 知识库获取概念知识，增强模型的常识推理能力；(2) 设计了词级和样本级两种跨模态语义相似性检测模块，以不同粒度建模图文关联；(3) 引入对比学习损失函数，优化样本特征的空间分布，提高正负样本的可分离性。在公开的多模态反讽检测基准数据集上进行的实验表明，与现有最优方法相比，该模型的准确率和 F1 值分别提高了 1.64% 和 2.88%，达到 88.87% 和 86.33%。进一步的消融实验验证了知识融合和语义相似性检测在提高模型性能方面的重要作用。", "summary": "本文提出了一种名为 SemIRNet 的语义反讽识别网络，旨在解决多模态反讽检测中图形隐含关联识别的难题。该网络通过引入 ConceptNet 知识库增强常识推理，设计了词级和样本级的跨模态语义相似性检测模块以建模图文关联，并采用对比学习损失函数优化特征空间。实验结果表明，SemIRNet 在公开数据集上的准确率和 F1 值均优于现有最佳方法，验证了其创新点对性能提升的有效性。", "keywords": "多模态讽刺检测, 语义反讽识别网络, ConceptNet, 跨模态相似性, 对比学习", "comments": "该论文的创新点在于首次将 ConceptNet 知识库引入多模态反讽检测，以增强模型的常识推理能力，这为处理隐性语义关联提供了新的视角。此外，多粒度的跨模态语义相似性检测和对比学习的应用，也进一步提升了模型捕捉复杂反讽现象的能力。其方法具有通用性，可能适用于其他需要跨模态语义理解的任务。"}}
{"id": "2506.15006", "title": "Scaling Intelligence: Designing Data Centers for Next-Gen Language Models", "authors": ["Jesmin Jahan Tithi", "Hanjiang Wu", "Avishaii Abuhatzera", "Fabrizio Petrini"], "summary": "The explosive growth of Large Language Models (LLMs) - such as GPT-4 with 1.8\ntrillion parameters - demands a radical rethinking of data center architecture\nto ensure scalability, efficiency, and cost-effectiveness. Our work provides a\ncomprehensive co-design framework that jointly explores FLOPS, HBM bandwidth\nand capacity, multiple network topologies (two-tier vs. FullFlat optical), the\nsize of the scale-out domain, and popular parallelism/optimization strategies\nused in LLMs. We introduce and evaluate FullFlat network architectures, which\nprovide uniform high-bandwidth, low-latency connectivity between all nodes, and\ndemonstrate their transformative impact on performance and scalability. Through\ndetailed sensitivity analyses, we quantify the benefits of overlapping compute\nand communication, leveraging hardware-accelerated collectives, wider scale-out\ndomains, and larger memory capacity. Our study spans both sparse (mixture of\nexperts) and dense transformer-based LLMs, revealing how system design choices\naffect Model FLOPS Utilization (MFU = Model flops per token x Observed tokens\nper sec / Peak flops of the hardware) and overall throughput. For the co-design\nstudy, we extended and validated a performance modeling tool capable of\npredicting LLM runtime within 10% of real-world measurements. Our findings\noffer actionable insights and a practical roadmap for designing AI data centers\nthat can efficiently support trillion-parameter models, reduce optimization\ncomplexity, and sustain the rapid evolution of AI capabilities.", "comment": "14 pages, submitted to SC25 for review", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15006v1", "AI": {"title_translation": "扩展智能：为下一代语言模型设计数据中心", "tldr": "大型语言模型（LLMs）的快速发展需要数据中心架构的根本性变革。本文提出了一个全面的协同设计框架，探索了计算、内存、网络拓扑和并行策略。研究引入并评估了FullFlat网络架构，展示了其对性能和可扩展性的变革性影响，并量化了各种设计选择的益处。研究结果为设计高效支持万亿参数模型的AI数据中心提供了见解和路线图。", "motivation": "大型语言模型（LLMs）的爆炸式增长（例如拥有1.8万亿参数的GPT-4）要求对数据中心架构进行彻底的重新思考，以确保可扩展性、效率和成本效益。", "method": "本文提出了一个全面的协同设计框架，联合探索了FLOPS、HBM带宽和容量、多种网络拓扑（两层 vs. FullFlat 光学）、横向扩展域的大小以及LLMs中常用的并行/优化策略。研究引入并评估了FullFlat网络架构。此外，还扩展并验证了一个能够预测LLM运行时且误差在10%以内的性能建模工具。", "result": "FullFlat网络架构在所有节点之间提供了统一的高带宽、低延迟连接，并对性能和可扩展性产生了变革性影响。研究量化了计算与通信重叠、利用硬件加速集合、更宽的横向扩展域和更大内存容量的益处。研究揭示了系统设计选择如何影响稀疏和密集基于Transformer的LLM的模型FLOPS利用率（MFU）和整体吞吐量。扩展并验证的性能建模工具能够预测LLM运行时，其误差在实际测量值的10%以内。", "conclusion": "研究结果为设计能够高效支持万亿参数模型、降低优化复杂性并维持AI能力快速发展的AI数据中心提供了可操作的见解和实用的路线图。", "translation": "大型语言模型（LLMs）的爆炸式增长——例如拥有1.8万亿参数的GPT-4——要求对数据中心架构进行彻底的重新思考，以确保可扩展性、效率和成本效益。我们的工作提供了一个全面的协同设计框架，共同探索了FLOPS、HBM带宽和容量、多种网络拓扑（两层 vs. FullFlat 光学）、横向扩展域的大小以及LLMs中常用的并行/优化策略。我们引入并评估了FullFlat网络架构，该架构在所有节点之间提供统一的高带宽、低延迟连接，并展示了它们对性能和可扩展性的变革性影响。通过详细的敏感性分析，我们量化了计算与通信重叠、利用硬件加速集合、更宽的横向扩展域和更大内存容量的益处。我们的研究涵盖了稀疏（专家混合）和密集基于Transformer的LLM，揭示了系统设计选择如何影响模型FLOPS利用率（MFU = 每令牌模型浮点运算数 x 每秒观测到的令牌数 / 硬件峰值浮点运算数）和整体吞吐量。为了进行协同设计研究，我们扩展并验证了一个性能建模工具，该工具能够预测LLM运行时，其误差在实际测量值的10%以内。我们的研究结果为设计能够高效支持万亿参数模型、降低优化复杂性并维持AI能力快速发展的AI数据中心提供了可操作的见解和实用的路线图。", "summary": "本文针对大型语言模型（LLMs）快速增长带来的数据中心架构挑战，提出了一个协同设计框架。该框架综合考虑了计算、内存、网络拓扑（包括引入FullFlat网络）和并行策略。研究展示了FullFlat网络在性能和可扩展性上的显著优势，并量化了不同设计选择对LLM效率的影响。此外，论文还提出了一个经过验证的性能建模工具，为未来能够支持万亿参数模型的AI数据中心设计提供了实用路线图。", "keywords": "大型语言模型, 数据中心架构, FullFlat网络, 可扩展性, 性能建模", "comments": "这篇论文通过其全面的协同设计方法，为下一代AI数据中心，特别是针对大型语言模型的独特需求，提供了创新性的解决方案。FullFlat网络架构的引入和评估尤为突出，因为它有可能显著提升性能和可扩展性。性能建模工具的验证增加了实际价值，为预测和优化LLM在实际场景中的运行时间提供了具体方法。对稀疏和密集LLM的兼顾使得研究结果具有广泛的适用性。"}}
{"id": "2506.14936", "title": "CALM: Contextual Analog Logic with Multimodality", "authors": ["Maxwell J. Jacobson", "Corey J. Maley", "Yexiang Xue"], "summary": "In this work, we introduce Contextual Analog Logic with Multimodality (CALM).\nCALM unites symbolic reasoning with neural generation, enabling systems to make\ncontext-sensitive decisions grounded in real-world multi-modal data.\n  Background: Classic bivalent logic systems cannot capture the nuance of human\ndecision-making. They also require human grounding in multi-modal environments,\nwhich can be ad-hoc, rigid, and brittle. Neural networks are good at extracting\nrich contextual information from multi-modal data, but lack interpretable\nstructures for reasoning.\n  Objectives: CALM aims to bridge the gap between logic and neural perception,\ncreating an analog logic that can reason over multi-modal inputs. Without this\nintegration, AI systems remain either brittle or unstructured, unable to\ngeneralize robustly to real-world tasks. In CALM, symbolic predicates evaluate\nto analog truth values computed by neural networks and constrained search.\n  Methods: CALM represents each predicate using a domain tree, which\niteratively refines its analog truth value when the contextual groundings of\nits entities are determined. The iterative refinement is predicted by neural\nnetworks capable of capturing multi-modal information and is filtered through a\nsymbolic reasoning module to ensure constraint satisfaction.\n  Results: In fill-in-the-blank object placement tasks, CALM achieved 92.2%\naccuracy, outperforming classical logic (86.3%) and LLM (59.4%) baselines. It\nalso demonstrated spatial heatmap generation aligned with logical constraints\nand delicate human preferences, as shown by a human study.\n  Conclusions: CALM demonstrates the potential to reason with logic structure\nwhile aligning with preferences in multi-modal environments. It lays the\nfoundation for next-gen AI systems that require the precision and\ninterpretation of logic and the multimodal information processing of neural\nnetworks.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.14936v1", "AI": {"title_translation": "CALM：多模态上下文模拟逻辑", "tldr": "CALM是一种结合了符号推理和神经网络生成的新型模拟逻辑系统，能够在多模态数据中进行上下文敏感的决策，并在对象放置任务中表现出色。", "motivation": "经典二值逻辑系统无法捕捉人类决策的细微差别，且在多模态环境中需要人工接地，缺乏灵活性和鲁棒性。神经网络虽然善于从多模态数据中提取上下文信息，但缺乏可解释的推理结构。CALM旨在弥合逻辑和神经感知之间的鸿沟，创建一个能够对多模态输入进行推理的模拟逻辑，以解决现有AI系统在现实世界任务中脆弱或缺乏结构的问题。", "method": "CALM使用域树来表示每个谓词，当其实体的上下文基础确定后，域树会迭代地细化其模拟真值。这种迭代细化由能够捕获多模态信息的神经网络预测，并通过符号推理模块进行过滤，以确保满足约束条件。", "result": "在填空式对象放置任务中，CALM达到了92.2%的准确率，优于经典逻辑（86.3%）和LLM（59.4%）基线。一项人类研究表明，它还能生成符合逻辑约束和精妙人类偏好的空间热图。", "conclusion": "CALM展示了在多模态环境中通过逻辑结构进行推理并与人类偏好保持一致的潜力。它为下一代AI系统奠定了基础，这些系统需要逻辑的精确性和可解释性以及神经网络的多模态信息处理能力。", "translation": "在这项工作中，我们介绍了多模态上下文模拟逻辑（CALM）。CALM将符号推理与神经生成相结合，使系统能够根据真实世界的多模态数据做出上下文敏感的决策。\n背景：经典的二值逻辑系统无法捕捉人类决策的细微差别。它们还需要在多模态环境中进行人工接地，这可能是临时性的、僵硬的和脆弱的。神经网络擅长从多模态数据中提取丰富的上下文信息，但缺乏可解释的推理结构。\n目标：CALM旨在弥合逻辑和神经感知之间的鸿沟，创建一种能够对多模态输入进行推理的模拟逻辑。如果没有这种集成，AI系统将仍然脆弱或缺乏结构，无法稳健地推广到现实世界任务。在CALM中，符号谓词计算出由神经网络计算并受约束搜索的模拟真值。\n方法：CALM使用域树表示每个谓词，当其实体的上下文基础确定后，域树会迭代地细化其模拟真值。这种迭代细化由能够捕获多模态信息的神经网络预测，并通过符号推理模块进行过滤，以确保满足约束条件。\n结果：在填空式对象放置任务中，CALM取得了92.2%的准确率，优于经典逻辑（86.3%）和LLM（59.4%）基线。它还展示了与逻辑约束和精妙人类偏好对齐的空间热图生成，正如一项人类研究所显示的那样。\n结论：CALM展示了在多模态环境中通过逻辑结构进行推理同时与偏好对齐的潜力。它为下一代AI系统奠定了基础，这些系统需要逻辑的精确性和解释性以及神经网络的多模态信息处理能力。", "summary": "CALM（多模态上下文模拟逻辑）是一种新型AI系统，旨在结合符号推理和神经网络的优势。它通过神经网络计算并由约束搜索过滤的模拟真值来评估符号谓词，从而实现对多模态输入的上下文敏感推理。该方法通过域树迭代细化谓词的模拟真值，并在对象放置任务中表现出优于传统逻辑和大型语言模型的准确性（92.2%），同时能生成符合人类偏好的空间热图。CALM为需要逻辑精度和多模态处理能力的下一代AI系统奠定了基础。", "keywords": "上下文模拟逻辑, 多模态, 符号推理, 神经网络, 对象放置", "comments": "CALM的创新之处在于其成功地将可解释的符号逻辑与强大的多模态神经网络结合起来，解决了现有AI系统在处理复杂现实世界任务时面临的鲁棒性和可解释性不足的问题。通过引入“模拟真值”和迭代细化机制，它能够捕捉人类决策的细微差别，这是传统二值逻辑难以实现的。其在对象放置任务中的显著性能提升，特别是超越LLM基线，凸显了其在实际应用中的潜力。该研究为构建兼具逻辑推理能力和多模态感知能力的下一代AI系统提供了有价值的方向。"}}
{"id": "2506.14933", "title": "Explain First, Trust Later: LLM-Augmented Explanations for Graph-Based Crypto Anomaly Detection", "authors": ["Adriana Watson"], "summary": "The decentralized finance (DeFi) community has grown rapidly in recent years,\npushed forward by cryptocurrency enthusiasts interested in the vast untapped\npotential of new markets. The surge in popularity of cryptocurrency has ushered\nin a new era of financial crime. Unfortunately, the novelty of the technology\nmakes the task of catching and prosecuting offenders particularly challenging.\nThus, it is necessary to implement automated detection tools related to\npolicies to address the growing criminality in the cryptocurrency realm.", "comment": "6 pages, 4 figures. Code available at:\n  https://github.com/awatson246/crypto-anomaly-detection-policy", "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.14933v1", "AI": {"title_translation": "先解释，后信任：基于图的加密异常检测的LLM增强解释", "tldr": "去中心化金融（DeFi）和加密货币的快速发展带来了金融犯罪的新时代。由于技术的新颖性，抓捕和起诉犯罪分子变得非常困难。因此，迫切需要实施自动化检测工具来应对加密货币领域日益增长的犯罪活动。", "motivation": "去中心化金融（DeFi）社区和加密货币的迅速发展导致了金融犯罪的新时代。由于技术的新颖性，抓捕和起诉犯罪分子变得特别具有挑战性。", "method": "Not mentioned in abstract", "result": "Not mentioned in abstract", "conclusion": "为了应对加密货币领域日益增长的犯罪活动，有必要实施与政策相关的自动化检测工具。", "translation": "近年来，去中心化金融（DeFi）社区发展迅速，得益于对新市场巨大未开发潜力感兴趣的加密货币爱好者。加密货币的普及迎来了金融犯罪的新时代。不幸的是，这项技术的新颖性使得抓捕和起诉犯罪分子尤其具有挑战性。因此，有必要实施与政策相关的自动化检测工具，以应对加密货币领域日益增长的犯罪活动。", "summary": "去中心化金融和加密货币的迅速扩张导致了金融犯罪的增加。由于该技术的新颖性，识别和起诉犯罪分子变得复杂。因此，迫切需要结合政策框架的自动化检测工具，以打击加密货币生态系统中不断升级的犯罪活动。", "keywords": "加密货币, 金融犯罪, 异常检测, 去中心化金融, 自动化检测", "comments": "该论文切入了一个及时且关键的问题：快速发展的去中心化金融领域的金融犯罪。标题暗示了一种创新方法，即使用LLM增强解释进行基于图的异常检测，这可以增强检测系统的信任度和透明度，考虑到加密交易的复杂性和新颖性，这是一个至关重要的方面。然而，摘要只提出了问题，并未提供关于具体方法或其创新点的详细信息。"}}
{"id": "2506.15532", "title": "Controller Synthesis for Parametric Timed Games", "authors": ["Mikael Bisgaard Dahlsen-Jensen", "Baptiste Fievet", "Laure Petrucci", "Jaco van de Pol"], "summary": "We present a (semi)-algorithm to compute winning strategies for parametric\ntimed games. Previous algorithms only synthesized constraints on the clock\nparameters for which the game is winning. A new definition of (winning)\nstrategies is proposed, and ways to compute them. A transformation of these\nstrategies to (parametric) timed automata allows for building a controller\nenforcing them. The feasibility of the method is demonstrated by an\nimplementation and experiments for the Production Cell case study.", "comment": "This is the full version of the paper under the same title accepted\n  to QEST+FORMATS 2025. 29 pages", "cate": "cs.FL", "url": "http://arxiv.org/abs/2506.15532v1", "AI": {"title_translation": "参数化定时博弈的控制器综合", "tldr": "本文提出了一种计算参数化定时博弈中获胜策略的（半）算法，并将其转化为参数化定时自动机以构建控制器。", "motivation": "以前的算法只能综合出博弈获胜的时钟参数约束，而不能直接合成控制器。本文旨在提出一种新的策略定义和计算方法，并将其转化为可执行的控制器。", "method": "本文提出了一种（半）算法来计算参数化定时博弈的获胜策略。该方法包含对（获胜）策略的新定义及其计算方法。这些策略进一步被转换为（参数化）定时自动机，从而能够构建一个强制执行这些策略的控制器。", "result": "该方法的有效性通过在生产单元案例研究中的实现和实验得到了证明。", "conclusion": "本文提出并验证了一种新的方法，用于计算参数化定时博弈的获胜策略，并能够将这些策略转化为实际的控制器，从而超越了以往仅生成参数约束的方法。", "translation": "我们提出了一种（半）算法来计算参数化定时博弈的获胜策略。以前的算法只综合出博弈获胜的时钟参数约束。本文提出了一种（获胜）策略的新定义以及计算它们的方法。将这些策略转换为（参数化）定时自动机，可以构建一个强制执行它们的控制器。该方法的可行性通过在生产单元案例研究中的实现和实验得到了证明。", "summary": "本文提出了一种用于计算参数化定时博弈获胜策略的（半）算法，并引入了策略的新定义和计算方法。通过将这些策略转换为参数化定时自动机，可以构建一个实施这些策略的控制器。该方法已通过在生产单元案例研究中的实现和实验证明了其可行性。", "keywords": "参数化定时博弈, 控制器综合, 获胜策略, 定时自动机, 半算法", "comments": "本文的创新之处在于提出了新的获胜策略定义和计算方法，并能够将这些策略直接转化为可执行的控制器，而不仅仅是输出参数约束。这对于实际系统控制器的综合具有重要意义，尤其是在实时系统和混合系统领域。"}}
{"id": "2506.14851", "title": "Efficient Serving of LLM Applications with Probabilistic Demand Modeling", "authors": ["Yifei Liu", "Zuo Gan", "Zhenghao Gan", "Weiye Wang", "Chen Chen", "Yizhou Shan", "Xusheng Chen", "Zhenhua Han", "Yifei Zhu", "Shixuan Sun", "Minyi Guo"], "summary": "Applications based on Large Language Models (LLMs) contains a series of tasks\nto address real-world problems with boosted capability, which have dynamic\ndemand volumes on diverse backends. Existing serving systems treat the resource\ndemands of LLM applications as a blackbox, compromising end-to-end efficiency\ndue to improper queuing order and backend warm up latency. We find that the\nresource demands of LLM applications can be modeled in a general and accurate\nmanner with Probabilistic Demand Graph (PDGraph). We then propose Hermes, which\nleverages PDGraph for efficient serving of LLM applications. Confronting\nprobabilistic demand description, Hermes applies the Gittins policy to\ndetermine the scheduling order that can minimize the average application\ncompletion time. It also uses the PDGraph model to help prewarm cold backends\nat proper moments. Experiments with diverse LLM applications confirm that\nHermes can effectively improve the application serving efficiency, reducing the\naverage completion time by over 70% and the P95 completion time by over 80%.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.14851v1", "AI": {"title_translation": "基于概率需求建模的大型语言模型应用高效服务", "tldr": "Hermes通过概率需求图（PDGraph）和Gittins策略，显著提高了LLM应用的服务效率，将平均完成时间缩短了70%以上。", "motivation": "现有LLM服务系统将资源需求视为黑盒，导致队列顺序不当和后端预热延迟，从而损害了端到端效率。", "method": "本文提出了Hermes系统，它利用概率需求图（PDGraph）来建模LLM应用的资源需求。Hermes应用Gittins策略来确定调度顺序，以最小化平均应用完成时间，并利用PDGraph模型在适当的时机预热冷后端。", "result": "实验表明，Hermes能有效提高LLM应用的服务效率，将平均完成时间缩短70%以上，P95完成时间缩短80%以上。", "conclusion": "通过概率需求建模和优化的调度策略，Hermes显著提升了LLM应用的服务效率，解决了现有系统中的效率瓶颈。", "translation": "基于大型语言模型（LLM）的应用包含一系列任务，以增强的能力解决现实世界问题，这些应用对不同后端的需求量是动态的。现有的服务系统将LLM应用的资源需求视为黑盒，由于不当的排队顺序和后端预热延迟，损害了端到端效率。我们发现LLM应用的资源需求可以通过概率需求图（PDGraph）以通用且准确的方式建模。然后，我们提出了Hermes，它利用PDGraph来高效服务LLM应用。面对概率需求描述，Hermes应用Gittins策略来确定调度顺序，从而最小化平均应用完成时间。它还使用PDGraph模型来帮助在适当的时机预热冷后端。对各种LLM应用的实验证实，Hermes可以有效提高应用服务效率，将平均完成时间缩短70%以上，P95完成时间缩短80%以上。", "summary": "本文提出了一种名为Hermes的系统，旨在解决现有LLM应用服务系统中因资源需求处理不当导致的效率低下问题。通过引入概率需求图（PDGraph）来准确建模LLM应用的动态资源需求，Hermes利用Gittins策略优化调度顺序以最小化平均完成时间，并利用PDGraph进行后端预热。实验结果表明，Hermes显著提升了服务效率，平均完成时间减少了70%以上，P95完成时间减少了80%以上。", "keywords": "LLM服务, 概率需求建模, Hermes, 调度, 效率", "comments": "该论文的创新点在于提出了使用概率需求图（PDGraph）来精确建模LLM应用的动态资源需求，并结合Gittins策略进行调度优化和智能后端预热。这解决了传统LLM服务系统中将需求视为黑盒所导致的效率瓶颈，为LLM应用的生产部署提供了显著的性能提升。"}}
{"id": "2506.14863", "title": "Preparing for the Intelligence Explosion", "authors": ["William MacAskill", "Fin Moorhouse"], "summary": "AI that can accelerate research could drive a century of technological\nprogress over just a few years. During such a period, new technological or\npolitical developments will raise consequential and hard-to-reverse decisions,\nin rapid succession. We call these developments grand challenges. These\nchallenges include new weapons of mass destruction, AI-enabled autocracies,\nraces to grab offworld resources, and digital beings worthy of moral\nconsideration, as well as opportunities to dramatically improve quality of life\nand collective decision-making. We argue that these challenges cannot always be\ndelegated to future AI systems, and suggest things we can do today to\nmeaningfully improve our prospects. AGI preparedness is therefore not just\nabout ensuring that advanced AI systems are aligned: we should be preparing,\nnow, for the disorienting range of developments an intelligence explosion would\nbring.", "comment": "61 pages", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.14863v1", "AI": {"title_translation": "为智能爆炸做准备", "tldr": "人工智能的快速发展（智能爆炸）将带来一系列重大的、难以逆转的“巨大挑战”，需要我们现在就做好准备，而不仅仅是确保AI对齐。", "motivation": "本文的动机是指出，能够加速研究的人工智能可能在几年内带来一个世纪的技术进步，这将迅速引发一系列重大且难以逆转的决策，即“巨大挑战”。这些挑战不能总是委托给未来的AI系统，因此需要当前就采取行动。", "method": "本文论证了这些挑战不能总是委托给未来的AI系统，并提出了我们今天可以做些什么来有意义地改善前景，强调了当前采取准备措施的必要性。", "result": "本文定义了智能爆炸可能带来的“巨大挑战”，包括大规模杀伤性武器、AI驱动的独裁政权、对外星资源的争夺、需要道德考量的数字生命，以及改善生活质量和集体决策的机会。研究结果表明，AGI的准备工作不仅仅是确保高级AI系统对齐，更应为智能爆炸可能带来的一系列令人迷失的发展做好准备。", "conclusion": "结论是，AGI的准备工作不仅仅是为了确保高级AI系统对齐，我们现在就应该为智能爆炸可能带来的一系列令人迷失的发展做好准备。", "translation": "能够加速研究的人工智能可以在短短几年内推动一个世纪的技术进步。在此期间，新的技术或政治发展将接踵而至，引发重大且难以逆转的决策。我们称这些发展为巨大挑战。这些挑战包括新型大规模杀伤性武器、人工智能驱动的独裁政权、对外星资源的争夺，以及值得道德考量的数字生命，同时也带来了显著改善生活质量和集体决策的机会。我们认为，这些挑战不能总是委托给未来的AI系统，并提出了我们今天可以做些什么来有意义地改善前景。因此，AGI的准备工作不仅仅是确保高级AI系统对齐：我们现在就应该为智能爆炸可能带来的一系列令人迷失的发展做好准备。", "summary": "本文探讨了人工智能加速研究可能带来的“智能爆炸”现象，指出其将在短时间内引发巨大的技术和政治变革，并带来一系列“巨大挑战”。这些挑战包括新型武器、AI独裁、资源争夺以及数字生命伦理等，同时也有改善人类生活的机会。作者强调，这些挑战无法完全委托给未来的AI系统，因此，AGI的准备工作应超越单纯的AI对齐，而是需要现在就为智能爆炸可能带来的广泛且令人迷失的发展做好全面准备。", "keywords": "智能爆炸, AI准备, 巨大挑战, 技术进步, AI对齐", "comments": "这篇论文的创新之处在于，它将人工智能的准备工作从传统的“对齐”问题扩展到更广泛的社会、政治和伦理层面，强调了智能爆炸可能带来的多维度“巨大挑战”。其重要性在于提醒我们，AI的未来影响远超技术范畴，需要跨学科、多方位的提前规划和应对，具有很强的现实指导意义。"}}
{"id": "2506.15004", "title": "Mixed Traffic: A Perspective from Long Duration Autonomy", "authors": ["Filippos Tzortzoglou", "Logan E. Beaver"], "summary": "The rapid adoption of autonomous vehicle has established mixed traffic\nenvironments, comprising both autonomous and human-driven vehicles (HDVs), as\nessential components of next-generation mobility systems. Along these lines,\nconnectivity between autonomous vehicles and infrastructure (V2I) is also a\nsignificant factor that can effectively support higher-level decision-making.\nAt the same time, the integration of V2I within mixed traffic environments\nremains a timely and challenging problem. In this paper, we present a\nlong-duration autonomy controller for connected and automated vehicles (CAVs)\noperating in such environments, with a focus on intersections where right turns\non red are permitted. We begin by deriving the optimal control policy for CAVs\nunder free-flow traffic. Next, we analyze crossing time constraints imposed by\nsmart traffic lights and map these constraints to controller bounds using\nControl Barrier Functions (CBFs), with the aim to drive a CAV to cross the\nintersection on time. We also introduce criteria for identifying, in real-time,\nfeasible crossing intervals for each CAV. To ensure safety for the CAVs, we\npresent model-agnostic safety guarantees, and demonstrate their compatibility\nwith both CAVs and HDVs. Ultimately, the final control actions are enforced\nthrough a combination of CBF constraints, constraining CAVs to traverse the\nintersection within the designated time intervals while respecting other\nvehicles. Finally, we guarantee that our control policy yields always a\nfeasible solution and validate the proposed approach through extensive\nsimulations in MATLAB.", "comment": "14 pages, 12 figures", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15004v1", "AI": {"title_translation": "混合交通：长时程自主性的视角", "tldr": "本文提出了一种用于混合交通环境中联网自动驾驶汽车（CAVs）的长时程自主控制器，特别关注允许右转的交叉路口，并利用控制障碍函数（CBFs）和实时识别的可行穿越间隔来确保安全和及时通行。", "motivation": "自动驾驶汽车的迅速普及使得混合交通环境（包括自动驾驶汽车和人工驾驶汽车）成为下一代移动系统的关键组成部分。同时，车辆与基础设施（V2I）的连接是支持更高层决策的重要因素。然而，在混合交通环境中整合V2I仍然是一个及时且具有挑战性的问题。", "method": "本文提出了一种用于联网自动驾驶汽车（CAVs）的长时程自主控制器，专注于允许红灯右转的交叉路口。首先，推导了自由流交通下CAVs的最优控制策略。其次，分析了智能交通灯施加的交叉时间约束，并使用控制障碍函数（CBFs）将这些约束映射到控制器边界，以确保CAV及时通过交叉路口。同时，引入了实时识别每个CAV可行穿越间隔的标准。为了确保CAVs的安全，提出了与模型无关的安全保障，并证明了它们与CAVs和HDVs的兼容性。最终，通过CBF约束的组合来执行最终的控制动作，限制CAVs在指定时间间隔内通过交叉路口，同时尊重其他车辆。", "result": "研究结果表明，所提出的控制策略始终能够产生可行的解决方案。该方法通过在MATLAB中进行大量仿真得到了验证。", "conclusion": "本文提出的长时程自主控制器及其基于CBF的控制策略，能够有效解决混合交通环境中联网自动驾驶汽车在交叉路口的安全和及时通行问题，并保证了解决方案的可行性。", "translation": "自动驾驶汽车的迅速普及使得混合交通环境（包括自动驾驶汽车和人工驾驶汽车）成为下一代移动系统的关键组成部分。与此同步，自动驾驶汽车与基础设施（V2I）之间的连接也是一个重要因素，能够有效支持更高层次的决策。与此同时，在混合交通环境中整合V2I仍然是一个及时且具有挑战性的问题。在本文中，我们提出了一种用于在此类环境中运行的联网自动驾驶汽车（CAVs）的长时程自主控制器，重点关注允许红灯右转的交叉路口。我们首先推导了自由流交通下CAVs的最优控制策略。接下来，我们分析了智能交通灯施加的交叉时间约束，并使用控制障碍函数（CBFs）将这些约束映射到控制器边界，旨在驱动CAV及时通过交叉路口。我们还引入了实时识别每个CAV可行穿越间隔的标准。为了确保CAVs的安全，我们提出了与模型无关的安全保障，并证明了它们与CAVs和HDVs的兼容性。最终，通过CBF约束的组合来执行最终的控制动作，限制CAVs在指定时间间隔内通过交叉路口，同时尊重其他车辆。最后，我们保证我们的控制策略始终能够产生可行的解决方案，并通过MATLAB中的大量仿真验证了所提出的方法。", "summary": "本文针对混合交通环境中联网自动驾驶汽车（CAVs）在允许红灯右转的交叉路口通行问题，提出了一种长时程自主控制器。该控制器首先推导了CAVs的最优控制策略，并利用控制障碍函数（CBFs）将智能交通灯施加的交叉时间约束映射到控制器边界，以确保CAV及时通过。研究还引入了实时识别可行穿越间隔的准则，并提供了与模型无关的安全保障，适用于CAVs和人工驾驶车辆。最终的控制动作通过CBF约束来执行，确保CAVs在指定时间段内安全通过交叉路口。该方法经过MATLAB仿真验证，证明了其控制策略能始终产生可行解。", "keywords": "混合交通, 自动驾驶汽车, 控制障碍函数, 交叉路口, 长时程自主性", "comments": "本文的创新点在于提出了一个结合长时程自主性和控制障碍函数（CBFs）的控制器，以解决混合交通环境中联网自动驾驶汽车在交叉路口的安全和及时通行问题。特别是在允许红灯右转的场景下，其对复杂交通流的建模和实时约束处理具有重要意义。所提出的模型无关安全保障和可行性保证增强了其实用性，为未来智能交通系统的发展提供了有价值的参考。"}}
{"id": "2506.14906", "title": "Demonstrating Superresolution in Radar Range Estimation Using a Denoising Autoencoder", "authors": ["Robert Czupryniak", "Abhishek Chakraborty", "Andrew N. Jordan", "John C. Howell"], "summary": "We apply machine learning methods to demonstrate range superresolution in\nremote sensing radar detection. Specifically, we implement a denoising\nautoencoder to estimate the distance between two equal intensity scatterers in\nthe subwavelength regime. The machine learning models are trained on waveforms\nsubject to a bandlimit constraint such that ranges much smaller than the\ninverse bandlimit are optimized in their precision. The autoencoder achieves\neffective dimensionality reduction, with the bottleneck layer exhibiting a\nstrong and consistent correlation with the true scatterer separation. We\nconfirm reproducibility across different training sessions and network\ninitializations by analyzing the scaled encoder outputs and their robustness to\nnoise. We investigate the behavior of the bottleneck layer for the following\ntypes of pulses: a traditional sinc pulse, a bandlimited triangle-type pulse,\nand a theoretically near-optimal pulse created from a spherical Bessel function\nbasis. The Bessel signal performs best, followed by the triangle wave, with the\nsinc signal performing worst, highlighting the crucial role of signal design in\nthe success of machine-learning-based range resolution.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.14906v1", "AI": {"title_translation": "使用去噪自编码器实现雷达测距中的超分辨", "tldr": "利用去噪自编码器在雷达测距中实现超分辨，并发现信号设计对机器学习方法的成功至关重要。", "motivation": "在遥感雷达探测中实现距离超分辨。", "method": "应用机器学习方法，具体实现了去噪自编码器来估计亚波长区域内两个等强度散射体之间的距离。模型在受带限约束的波形上进行训练，以优化远小于逆带限的距离精度。研究了三种脉冲类型（传统sinc脉冲、带限三角型脉冲和基于球贝塞尔函数基的理论上接近最优脉冲）对瓶颈层行为的影响。", "result": "自编码器实现了有效的降维，其瓶颈层与真实的散射体分离度表现出强大且一致的相关性。实验结果在不同训练会话和网络初始化中具有可重现性，并且对噪声具有鲁棒性。贝塞尔信号表现最佳，其次是三角波，sinc信号表现最差。", "conclusion": "信号设计在基于机器学习的距离分辨率成功中起着关键作用。", "translation": "我们将机器学习方法应用于遥感雷达探测，以实现距离超分辨。具体来说，我们实现了一个去噪自编码器，用于估计亚波长区域内两个等强度散射体之间的距离。机器学习模型在受带限约束的波形上进行训练，从而优化了远小于逆带限的距离精度。该自编码器实现了有效的降维，其瓶颈层与真实的散射体分离度表现出强大且一致的相关性。通过分析缩放后的编码器输出及其对噪声的鲁棒性，我们确认了不同训练会话和网络初始化之间的可重现性。我们研究了瓶颈层对以下类型脉冲的行为：传统的sinc脉冲、带限三角型脉冲以及由球贝塞尔函数基创建的理论上接近最优的脉冲。贝塞尔信号表现最佳，其次是三角波，sinc信号表现最差，这突出了信号设计在基于机器学习的距离分辨率成功中的关键作用。", "summary": "本研究利用去噪自编码器在遥感雷达探测中实现了距离超分辨。模型通过训练在带限波形上，优化了亚波长区域内散射体的距离估计精度。结果显示自编码器的瓶颈层与散射体距离高度相关，且方法具有良好的可重现性和抗噪性。此外，研究发现基于球贝塞尔函数的信号表现最优，强调了信号设计对机器学习雷达测距性能的关键影响。", "keywords": "雷达测距, 超分辨, 去噪自编码器, 机器学习, 信号设计", "comments": "该论文创新性地将去噪自编码器应用于雷达测距的超分辨问题，展示了机器学习在突破传统雷达分辨率限制方面的潜力。其亮点在于不仅实现了超分辨，还深入探讨了不同脉冲信号对性能的影响，揭示了信号设计的重要性，为未来雷达系统和机器学习算法的协同优化提供了宝贵的见解。"}}
{"id": "2506.14774", "title": "MedSyn: Enhancing Diagnostics with Human-AI Collaboration", "authors": ["Burcu Sayin", "Ipek Baris Schlicht", "Ngoc Vo Hong", "Sara Allievi", "Jacopo Staiano", "Pasquale Minervini", "Andrea Passerini"], "summary": "Clinical decision-making is inherently complex, often influenced by cognitive\nbiases, incomplete information, and case ambiguity. Large Language Models\n(LLMs) have shown promise as tools for supporting clinical decision-making, yet\ntheir typical one-shot or limited-interaction usage may overlook the\ncomplexities of real-world medical practice. In this work, we propose a hybrid\nhuman-AI framework, MedSyn, where physicians and LLMs engage in multi-step,\ninteractive dialogues to refine diagnoses and treatment decisions. Unlike\nstatic decision-support tools, MedSyn enables dynamic exchanges, allowing\nphysicians to challenge LLM suggestions while the LLM highlights alternative\nperspectives. Through simulated physician-LLM interactions, we assess the\npotential of open-source LLMs as physician assistants. Results show open-source\nLLMs are promising as physician assistants in the real world. Future work will\ninvolve real physician interactions to further validate MedSyn's usefulness in\ndiagnostic accuracy and patient outcomes.", "comment": "Accepted to the Trustworthy and Collaborative Artificial Intelligence\n  Workshop 2025 (TCAI 2025) in the 4th International Conference Series on\n  Hybrid Human-Artificial Intelligence (HHAI 2025)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14774v1", "AI": {"title_translation": "MedSyn：通过人机协作增强诊断", "tldr": "MedSyn是一个人机混合框架，通过多步骤、交互式对话让医生和大型语言模型（LLM）协作，以改进诊断和治疗决策，并评估了开源LLM作为医生助手的潜力。", "motivation": "临床决策过程复杂，常受认知偏差、信息不完整和病例模糊性影响。尽管大型语言模型（LLMs）在支持临床决策方面显示出潜力，但其通常的单次或有限交互使用方式可能忽视真实世界医疗实践的复杂性。", "method": "本研究提出了一个名为MedSyn的人机混合框架，医生和LLM通过多步骤、交互式对话来完善诊断和治疗决策。与静态决策支持工具不同，MedSyn支持动态交流，允许医生质疑LLM的建议，同时LLM也能提出替代观点。研究通过模拟医生与LLM的交互来评估开源LLM作为医生助手的潜力。", "result": "结果表明，开源LLM在真实世界中作为医生助手具有前景。", "conclusion": "开源LLM作为医生助手具有前景，MedSyn框架通过动态、交互式的人机协作，有望提高诊断准确性和改善患者预后，未来的工作将涉及真实的医生交互以进一步验证其效用。", "translation": "临床决策本身就很复杂，常常受到认知偏差、信息不完整和病例模糊性的影响。大型语言模型（LLMs）已显示出作为支持临床决策工具的潜力，但其典型的单次或有限交互使用方式可能忽视真实世界医疗实践的复杂性。在这项工作中，我们提出了一个混合人机框架MedSyn，医生和LLM在其中进行多步骤、交互式对话，以完善诊断和治疗决策。与静态决策支持工具不同，MedSyn实现了动态交流，允许医生质疑LLM的建议，同时LLM也能突出替代观点。通过模拟医生与LLM的交互，我们评估了开源LLM作为医生助手的潜力。结果表明，开源LLM在真实世界中作为医生助手具有前景。未来的工作将涉及真实的医生交互，以进一步验证MedSyn在诊断准确性和患者结局方面的有用性。", "summary": "MedSyn是一个创新的人机协作框架，旨在通过医生与大型语言模型（LLMs）之间的多步骤、交互式对话来提升临床诊断和治疗决策的准确性。该框架克服了传统LLM在医疗应用中交互不足的局限性，实现了医生对LLM建议的质疑以及LLM提供替代观点的动态交流。初步的模拟交互评估显示，开源LLMs作为医生助手具有良好前景，未来将进一步通过真实医生交互验证其在提高诊断准确性和改善患者预后方面的实际效用。", "keywords": "人机协作, 大型语言模型, 临床诊断, 决策支持, 交互式系统", "comments": "MedSyn的创新之处在于其强调人机之间的动态、多步骤交互，而非传统的静态或单次决策支持。这更符合真实世界的医疗实践，允许医生和AI相互学习和完善。其重要性在于为LLM在医疗领域的应用提供了一条更实用、更值得信赖的路径。未来通过真实医生交互的验证将是关键。"}}
{"id": "2506.14973", "title": "Thinking in Directivity: Speech Large Language Model for Multi-Talker Directional Speech Recognition", "authors": ["Jiamin Xie", "Ju Lin", "Yiteng Huang", "Tyler Vuong", "Zhaojiang Lin", "Zhaojun Yang", "Peng Su", "Prashant Rawat", "Sangeeta Srivastava", "Ming Sun", "Florian Metze"], "summary": "Recent studies have demonstrated that prompting large language models (LLM)\nwith audio encodings enables effective speech recognition capabilities.\nHowever, the ability of Speech LLMs to comprehend and process multi-channel\naudio with spatial cues remains a relatively uninvestigated area of research.\nIn this work, we present directional-SpeechLlama, a novel approach that\nleverages the microphone array of smart glasses to achieve directional speech\nrecognition, source localization, and bystander cross-talk suppression. To\nenhance the model's ability to understand directivity, we propose two key\ntechniques: serialized directional output training (S-DOT) and contrastive\ndirection data augmentation (CDDA). Experimental results show that our proposed\ndirectional-SpeechLlama effectively captures the relationship between textual\ncues and spatial audio, yielding strong performance in both speech recognition\nand source localization tasks.", "comment": "Accepted to Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.14973v1", "AI": {"title_translation": "面向指向性的思考：用于多说话人定向语音识别的语音大语言模型", "tldr": "该研究提出了一种名为 directional-SpeechLlama 的语音大语言模型，利用智能眼镜的麦克风阵列实现定向语音识别、声源定位和旁观者串扰抑制，并通过S-DOT和CDDA技术提升了模型的指向性理解能力，在实验中表现出强大的性能。", "motivation": "当前语音大语言模型（Speech LLM）在语音识别方面已展现出有效能力，但其理解和处理带有空间线索的多通道音频的能力，尤其是在多说话人场景下的定向处理，仍是一个相对未被充分研究的领域。", "method": "本文提出了一种名为 directional-SpeechLlama 的新方法，它利用智能眼镜的麦克风阵列来实现定向语音识别、声源定位和旁观者串扰抑制。为了增强模型理解指向性的能力，该研究提出了两种关键技术：序列化定向输出训练（S-DOT）和对比方向数据增强（CDDA）。", "result": "实验结果表明，所提出的 directional-SpeechLlama 能够有效地捕获文本线索与空间音频之间的关系，并在语音识别和声源定位任务中均表现出强大的性能。", "conclusion": "directional-SpeechLlama 有效地利用了智能眼镜的空间音频信息，显著提升了语音大语言模型在多说话人定向语音识别和声源定位方面的能力。", "translation": "最近的研究表明，通过音频编码提示大型语言模型（LLM）可以实现有效的语音识别能力。然而，语音LLM理解和处理带有空间线索的多通道音频的能力仍然是一个相对未被研究的领域。在这项工作中，我们提出了directional-SpeechLlama，这是一种利用智能眼镜的麦克风阵列实现定向语音识别、声源定位和旁观者串扰抑制的新颖方法。为了增强模型理解指向性的能力，我们提出了两个关键技术：序列化定向输出训练（S-DOT）和对比方向数据增强（CDDA）。实验结果表明，我们提出的directional-SpeechLlama有效地捕获了文本线索与空间音频之间的关系，在语音识别和声源定位任务中均表现出强大的性能。", "summary": "本文介绍了一种名为 directional-SpeechLlama 的新型语音大语言模型，旨在解决多说话人定向语音识别、声源定位和旁观者串扰抑制问题。该方法创新性地利用智能眼镜的麦克风阵列，并通过序列化定向输出训练（S-DOT）和对比方向数据增强（CDDA）两种关键技术来增强模型对指向性的理解。实验证明，directional-SpeechLlama 能有效关联文本线索与空间音频，在语音识别和声源定位任务中均展现出卓越性能。", "keywords": "语音大语言模型, 定向语音识别, 声源定位, 智能眼镜, 多说话人", "comments": "该论文解决了语音大语言模型在处理空间音频和多说话人场景中的一个重要空白。利用智能眼镜作为硬件平台，并结合S-DOT和CDDA等特定训练技术来融入指向性信息，是其创新之处。这为在真实世界环境中开发更鲁棒、实用的多说话人语音识别系统提供了新的方向和潜力。"}}
{"id": "2506.15183", "title": "You Only Render Once: Enhancing Energy and Computation Efficiency of Mobile Virtual Reality", "authors": ["Xingyu Chen", "Xinmin Fang", "Shuting Zhang", "Xinyu Zhang", "Liang He", "Zhengxiong Li"], "summary": "Mobile Virtual Reality (VR) is essential to achieving convenient and\nimmersive human-computer interaction and realizing emerging applications such\nas Metaverse. However, existing VR technologies require two separate renderings\nof binocular images, causing a significant bottleneck for mobile devices with\nlimited computing capability and power supply. This paper proposes an approach\nto rendering optimization for mobile VR called EffVR. By utilizing the\nper-pixel attribute, EffVR can generate binocular VR images from the monocular\nimage through genuinely one rendering, saving half the computation over\nconventional approaches. Our evaluation indicates that, compared with the\nstate-of-art, EffVRcan save 27% power consumption on average while achieving\nhigh binocular image quality (0.9679 SSIM and 34.09 PSNR) in mobile VR\napplications. Additionally, EffVR can increase the frame rate by 115.2%. These\nresults corroborate EffVRsuperior computation/energy-saving performance, paving\nthe road to a sustainable mobile VR. The source code, demo video, android app,\nand more are released anonymously at https://yoro-vr.github.io/", "comment": null, "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.15183v1", "AI": {"title_translation": "你只需渲染一次：提升移动虚拟现实的能效和计算效率", "tldr": "EffVR是一种新的移动VR渲染优化方法，通过一次渲染生成双目图像，相比传统方法可节省50%计算量，平均节省27%功耗，帧率提升115.2%，同时保持高图像质量，旨在解决移动VR设备的计算和能耗瓶颈。", "motivation": "现有的VR技术需要对双目图像进行两次单独渲染，这对于计算能力和电源受限的移动设备来说是一个显著的瓶颈。", "method": "本文提出了一种名为EffVR的移动VR渲染优化方法。通过利用逐像素属性，EffVR能够通过一次渲染从单目图像生成双目VR图像，相比传统方法节省一半的计算量。", "result": "与现有技术相比，EffVR平均节省27%的功耗，同时在移动VR应用中实现了高双目图像质量（0.9679 SSIM和34.09 PSNR）。此外，EffVR可以将帧率提高115.2%。", "conclusion": "这些结果证实了EffVR卓越的计算/节能性能，为可持续的移动VR铺平了道路。", "translation": "移动虚拟现实（VR）对于实现便捷沉浸式人机交互和实现元宇宙等新兴应用至关重要。然而，现有VR技术需要对双目图像进行两次单独渲染，这对于计算能力和电源受限的移动设备来说是一个显著的瓶颈。本文提出了一种名为EffVR的移动VR渲染优化方法。通过利用逐像素属性，EffVR能够通过一次渲染从单目图像生成双目VR图像，相比传统方法节省一半的计算量。我们的评估表明，与现有技术相比，EffVR平均可节省27%的功耗，同时在移动VR应用中实现了高双目图像质量（0.9679 SSIM和34.09 PSNR）。此外，EffVR可以将帧率提高115.2%。这些结果证实了EffVR卓越的计算/节能性能，为可持续的移动VR铺平了道路。源代码、演示视频、安卓应用等已匿名发布在https://yoro-vr.github.io/", "summary": "本文提出了一种名为EffVR的移动VR渲染优化方法，旨在解决现有技术中双目图像两次渲染导致的计算和能耗瓶颈。EffVR利用逐像素属性，通过一次渲染从单目图像生成双目VR图像，从而节省了50%的计算量。实验结果表明，EffVR与现有技术相比，平均节省27%的功耗，帧率提升115.2%，同时保持了高图像质量（SSIM 0.9679，PSNR 34.09）。这些优势使得EffVR在计算和能效方面表现卓越，为移动VR的可持续发展提供了新的途径。", "keywords": "移动VR, 渲染优化, 能效, 计算效率, EffVR", "comments": "这项研究通过创新的“一次渲染”方法，有效地解决了移动VR设备长期存在的计算和能耗瓶颈。其核心创新在于利用逐像素属性从单目图像生成双目图像，这直接将渲染计算量减半。显著的功耗节省和帧率提升对于提升移动VR的用户体验和推动其普及具有重要意义。该方法的普适性和实际应用潜力是其主要亮点。"}}
{"id": "2506.14974", "title": "Parallel Complexity of Depth-First-Search and Maximal path", "authors": ["Archit Chauhan", "Samir Datta", "M. Praveen"], "summary": "Constructing a Depth First Search (DFS) tree is a fundamental graph problem,\nwhose parallel complexity is still not settled. Reif showed parallel\nintractability of lex-first DFS. In contrast, randomized parallel algorithms\n(and more recently, deterministic quasipolynomial parallel algorithms) are\nknown for constructing a DFS tree in general (di)graphs. However a\ndeterministic parallel algorithm for DFS in general graphs remains an elusive\ngoal. Working towards this, a series of works gave deterministic NC algorithms\nfor DFS in planar graphs and digraphs. We further extend these results to more\ngeneral graph classes, by providing NC algorithms for (di)graphs of bounded\ngenus, and for undirected H-minor-free graphs where H is a fixed graph with at\nmost one crossing. For the case of (di)graphs of bounded tree-width, we further\nimprove the complexity to a Logspace bound. Constructing a maximal path is a\nsimpler problem (that reduces to DFS) for which no deterministic parallel\nbounds are known for general graphs. For planar graphs a bound of O(log n)\nparallel time on a CREW PRAM (thus in NC2) is known. We improve this bound to\nLogspace.", "comment": null, "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.14974v1", "AI": {"title_translation": "深度优先搜索和最大路径的并行复杂度", "tldr": "本文为特定图类（如有限亏格图、H-minor-free图和有限树宽图）的深度优先搜索（DFS）和最大路径问题提供了改进的确定性并行算法和复杂度界限。", "motivation": "深度优先搜索（DFS）树的并行复杂度问题尚未完全解决，特别是对于一般图的确定性并行DFS算法仍然缺乏。此外，构建最大路径在一般图上也没有已知的确定性并行界限。", "method": "本文通过提供NC算法，将DFS的确定性并行算法扩展到有界亏格的有向/无向图和无向H-minor-free图。对于有界树宽的有向/无向图，进一步将DFS的复杂度改进到Logspace。对于最大路径问题，将平面图的并行复杂度从NC2改进到Logspace。", "result": "1. 为有界亏格的有向/无向图和无向H-minor-free图提供了DFS的NC算法。2. 将有界树宽的有向/无向图的DFS复杂度改进到Logspace。3. 将平面图上构建最大路径的并行复杂度从NC2改进到Logspace。", "conclusion": "本文在特定图类（如有限亏格图、H-minor-free图和有限树宽图）上，通过提供确定性NC算法和Logspace复杂度界限，在解决DFS和最大路径问题的并行复杂度方面取得了显著进展。", "translation": "构建深度优先搜索（DFS）树是一个基本的图问题，其并行复杂度仍未解决。Reif证明了词典优先DFS的并行不可处理性。相比之下，已知有随机并行算法（以及最近的确定性拟多项式并行算法）用于在一般（有向）图中构建DFS树。然而，在一般图中实现DFS的确定性并行算法仍然是一个难以实现的目标。为此，一系列工作为平面图和有向图中的DFS提供了确定性NC算法。我们通过为有界亏格的（有向）图和无向H-minor-free图（其中H是具有至多一个交叉的固定图）提供NC算法，进一步扩展了这些结果。对于有界树宽的（有向）图，我们将复杂度进一步改进到Logspace界限。构建最大路径是一个更简单的问题（可以归结为DFS），对于一般图，其确定性并行界限尚不明确。对于平面图，已知在CREW PRAM上的并行时间为O(log n)（因此在NC2中）。我们将此界限改进到Logspace。", "summary": "本文研究了深度优先搜索（DFS）和最大路径问题的并行复杂度。针对DFS，作者将现有确定性NC算法的适用范围扩展到更普遍的图类，包括有界亏格图和某些H-minor-free图，并为有界树宽图将复杂度提升至Logspace。对于最大路径问题，文章也将平面图的并行复杂度从NC2改进到了Logspace。这些工作为在特定图类中实现确定性并行算法迈出了重要一步。", "keywords": "深度优先搜索, 并行复杂度, 最大路径, NC算法, Logspace", "comments": "本文在解决DFS和最大路径这两个基本图问题的确定性并行复杂度方面取得了重要进展。其创新点在于将现有NC算法推广到更广的图类，并对特定图类的复杂度进行了Logspace级别的改进，这对于并行算法理论和图算法实践都具有重要意义。"}}
{"id": "2506.15120", "title": "Advancing Loss Functions in Recommender Systems: A Comparative Study with a Rényi Divergence-Based Solution", "authors": ["Shengjia Zhang", "Jiawei Chen", "Changdong Li", "Sheng Zhou", "Qihao Shi", "Yan Feng", "Chun Chen", "Can Wang"], "summary": "Loss functions play a pivotal role in optimizing recommendation models. Among\nvarious loss functions, Softmax Loss (SL) and Cosine Contrastive Loss (CCL) are\nparticularly effective. Their theoretical connections and differences warrant\nin-depth exploration. This work conducts comprehensive analyses of these\nlosses, yielding significant insights: 1) Common strengths -- both can be\nviewed as augmentations of traditional losses with Distributional Robust\nOptimization (DRO), enhancing robustness to distributional shifts; 2)\nRespective limitations -- stemming from their use of different distribution\ndistance metrics in DRO optimization, SL exhibits high sensitivity to false\nnegative instances, whereas CCL suffers from low data utilization. To address\nthese limitations, this work proposes a new loss function, DrRL, which\ngeneralizes SL and CCL by leveraging R\\'enyi-divergence in DRO optimization.\nDrRL incorporates the advantageous structures of both SL and CCL, and can be\ndemonstrated to effectively mitigate their limitations. Extensive experiments\nhave been conducted to validate the superiority of DrRL on both recommendation\naccuracy and robustness.", "comment": "AAAI 2025", "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.15120v1", "AI": {"title_translation": "推荐系统中损失函数的改进：基于雷尼散度的解决方案的比较研究", "tldr": "本文分析了推荐系统中常用的Softmax Loss (SL)和Cosine Contrastive Loss (CCL)的优缺点，并提出了一种新的基于雷尼散度的损失函数DrRL，该函数结合了SL和CCL的优点，有效解决了它们的局限性，提升了推荐系统的准确性和鲁棒性。", "motivation": "损失函数在推荐模型优化中至关重要。Softmax Loss (SL) 和 Cosine Contrastive Loss (CCL) 是有效的，但它们的理论联系和差异需要深入探索。SL对假负例敏感，CCL数据利用率低，这些局限性促使研究者寻找更好的解决方案。", "method": "本文首先对Softmax Loss (SL) 和 Cosine Contrastive Loss (CCL) 进行了全面分析，发现它们都可视为传统损失与分布鲁棒优化 (DRO) 的结合。然后，针对它们各自的局限性（SL对假负例敏感，CCL数据利用率低），提出了一种新的损失函数DrRL。DrRL通过在DRO优化中利用雷尼散度来泛化SL和CCL，并结合了两者的优点。", "result": "研究发现SL和CCL都可视为传统损失与分布鲁棒优化 (DRO) 的增强，增强了对分布变化的鲁棒性。SL对假负例敏感，CCL数据利用率低。提出的DrRL损失函数结合了SL和CCL的优点，并有效缓解了它们的局限性。广泛的实验证明DrRL在推荐准确性和鲁棒性方面均优于现有方法。", "conclusion": "DrRL作为一种基于雷尼散度的通用损失函数，成功地结合了SL和CCL的优势，并有效解决了它们各自的局限性，显著提升了推荐系统的性能和鲁棒性。", "translation": "损失函数在优化推荐模型中扮演着关键角色。在各种损失函数中，Softmax Loss (SL) 和 Cosine Contrastive Loss (CCL) 特别有效。它们的理论联系和差异值得深入探索。这项工作对这些损失函数进行了全面分析，得出了重要的见解：1) 共同优势——两者都可以看作是传统损失通过分布鲁棒优化 (DRO) 的增强，从而增强了对分布变化的鲁棒性；2) 各自的局限性——源于它们在DRO优化中使用了不同的分布距离度量，SL对假负例实例表现出高度敏感性，而CCL则存在数据利用率低的问题。为了解决这些局限性，这项工作提出了一种新的损失函数DrRL，它通过在DRO优化中利用雷尼散度来泛化SL和CCL。DrRL结合了SL和CCL的优势结构，并被证明可以有效缓解它们的局限性。已经进行了广泛的实验来验证DrRL在推荐准确性和鲁棒性方面的优越性。", "summary": "本文深入分析了推荐系统中常用的Softmax Loss (SL) 和 Cosine Contrastive Loss (CCL)，发现它们虽能增强鲁棒性，但也存在SL对假负例敏感、CCL数据利用率低的问题。为克服这些限制，研究提出了一种新的通用损失函数DrRL，该函数通过在分布鲁棒优化中引入雷尼散度，融合了SL和CCL的优点。实验证明DrRL在推荐准确性和鲁棒性上均表现出卓越性能。", "keywords": "推荐系统, 损失函数, Softmax Loss, Cosine Contrastive Loss, 雷尼散度, 分布鲁棒优化", "comments": "这篇论文的创新点在于提出了一个通用的基于雷尼散度的损失函数DrRL，它成功地整合了SL和CCL的优点，并有效解决了它们在推荐系统中的固有局限性。通过将SL和CCL统一到分布鲁棒优化的框架下进行分析，揭示了它们共同的优势和各自的问题根源，提供了深刻的理论见解。DrRL的提出为推荐系统损失函数的设计提供了新的思路，有望提升模型的泛化能力和鲁棒性。"}}
{"id": "2506.14984", "title": "Extending Spike-Timing Dependent Plasticity to Learning Synaptic Delays", "authors": ["Marissa Dominijanni", "Alexander Ororbia", "Kenneth W. Regan"], "summary": "Synaptic delays play a crucial role in biological neuronal networks, where\ntheir modulation has been observed in mammalian learning processes. In the\nrealm of neuromorphic computing, although spiking neural networks (SNNs) aim to\nemulate biology more closely than traditional artificial neural networks do,\nsynaptic delays are rarely incorporated into their simulation. We introduce a\nnovel learning rule for simultaneously learning synaptic connection strengths\nand delays, by extending spike-timing dependent plasticity (STDP), a Hebbian\nmethod commonly used for learning synaptic weights. We validate our approach by\nextending a widely-used SNN model for classification trained with unsupervised\nlearning. Then we demonstrate the effectiveness of our new method by comparing\nit against another existing methods for co-learning synaptic weights and delays\nas well as against STDP without synaptic delays. Results demonstrate that our\nproposed method consistently achieves superior performance across a variety of\ntest scenarios. Furthermore, our experimental results yield insight into the\ninterplay between synaptic efficacy and delay.", "comment": "Repository containing the source code used to generate the results is\n  available at: https://github.com/mdominijanni/dsstdp-results", "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.14984v1", "AI": {"title_translation": "将脉冲时间依赖可塑性扩展到学习突触延迟", "tldr": "本文提出了一种新的学习规则，通过扩展STDP来同时学习SNN中的突触连接强度和延迟，并在多种场景下取得了优越的性能。", "motivation": "生物神经网络中的突触延迟在学习过程中起着关键作用，但在神经形态计算中的脉冲神经网络（SNNs）中却很少被纳入模拟。", "method": "本文提出了一种新的学习规则，通过扩展脉冲时间依赖可塑性（STDP）来同时学习突触连接强度和突触延迟。该方法通过扩展一个广泛使用的SNN分类模型并进行无监督学习进行验证，并与现有共学习突触权重和延迟的方法以及不包含突触延迟的STDP方法进行比较。", "result": "提出的方法在各种测试场景中始终表现出卓越的性能。此外，实验结果揭示了突触效能和延迟之间的相互作用。", "conclusion": "本文成功地将STDP扩展到同时学习突触连接强度和延迟，并在SNN中取得了显著的性能提升，表明突触延迟的纳入对神经形态计算至关重要。", "translation": "突触延迟在生物神经网络中发挥着至关重要的作用，在哺乳动物的学习过程中观察到它们的调节。在神经形态计算领域，尽管脉冲神经网络（SNNs）旨在比传统人工神经网络更紧密地模拟生物学，但突触延迟很少被纳入其模拟中。我们引入了一种新颖的学习规则，通过扩展脉冲时间依赖可塑性（STDP）（一种常用于学习突触权重的赫布方法），来同时学习突触连接强度和延迟。我们通过扩展一个广泛使用的、通过无监督学习训练的SNN分类模型来验证我们的方法。然后，我们通过将其与另一种现有的共学习突触权重和延迟的方法以及不包含突触延迟的STDP进行比较，展示了我们新方法的有效性。结果表明，我们提出的方法在各种测试场景中始终表现出卓越的性能。此外，我们的实验结果揭示了突触效能和延迟之间的相互作用。", "summary": "本文提出了一种新颖的学习规则，通过扩展传统的脉冲时间依赖可塑性（STDP），首次实现了在脉冲神经网络（SNNs）中同时学习突触连接强度和突触延迟。鉴于突触延迟在生物学习中的重要性及其在现有SNN模型中被忽视的现状，该研究旨在弥补这一空白。通过在SNN分类模型上的实验验证，并与现有方法及不考虑延迟的STDP进行对比，结果显示所提出的方法在多种测试场景下均取得了优越的性能，并为突触效能和延迟的相互作用提供了新的见解。", "keywords": "脉冲时间依赖可塑性, 突触延迟, 脉冲神经网络, 神经形态计算, 学习规则", "comments": "这篇论文的创新点在于将STDP扩展到不仅学习突触权重，还同时学习突触延迟，这更符合生物学现实。其重要性在于弥补了当前SNNs在模拟生物真实性方面的不足，有望提升神经形态计算模型的性能和解释性。"}}
{"id": "2506.14900", "title": "Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings", "authors": ["Imane Guellil", "Salomé Andres", "Atul Anand", "Bruce Guthrie", "Huayu Zhang", "Abul Hasan", "Honghan Wu", "Beatrice Alex"], "summary": "In this work, we present a manually annotated corpus for Adverse Event (AE)\nextraction from discharge summaries of elderly patients, a population often\nunderrepresented in clinical NLP resources. The dataset includes 14 clinically\nsignificant AEs-such as falls, delirium, and intracranial haemorrhage, along\nwith contextual attributes like negation, diagnosis type, and in-hospital\noccurrence. Uniquely, the annotation schema supports both discontinuous and\noverlapping entities, addressing challenges rarely tackled in prior work. We\nevaluate multiple models using FlairNLP across three annotation granularities:\nfine-grained, coarse-grained, and coarse-grained with negation. While\ntransformer-based models (e.g., BERT-cased) achieve strong performance on\ndocument-level coarse-grained extraction (F1 = 0.943), performance drops\nnotably for fine-grained entity-level tasks (e.g., F1 = 0.675), particularly\nfor rare events and complex attributes. These results demonstrate that despite\nhigh-level scores, significant challenges remain in detecting underrepresented\nAEs and capturing nuanced clinical language. Developed within a Trusted\nResearch Environment (TRE), the dataset is available upon request via DataLoch\nand serves as a robust benchmark for evaluating AE extraction methods and\nsupporting future cross-dataset generalisation.", "comment": "Accepted and will be published at ACL2025 (main conference)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14900v1", "AI": {"title_translation": "出院总结中不良事件提取：新数据集、标注方案和初步发现", "tldr": "本研究提出了一个用于从老年患者出院总结中提取不良事件（AE）的手动标注数据集，并评估了模型的性能，发现粗粒度提取表现良好，但细粒度任务仍面临挑战。", "motivation": "老年患者在临床自然语言处理（NLP）资源中往往代表性不足，且现有研究很少解决不连续和重叠实体等挑战。本研究旨在通过创建新数据集来解决在检测代表性不足的不良事件和捕获细微临床语言方面的重大挑战。", "method": "研究构建了一个手动标注的语料库，用于从老年患者的出院总结中提取不良事件（AE）。该数据集包含14种临床上重要的AE及上下文属性，且标注方案独特地支持不连续和重叠实体。研究使用FlairNLP评估了多种模型，涵盖细粒度、粗粒度以及带否定的粗粒度三种标注粒度。", "result": "基于Transformer的模型（如BERT-cased）在文档级粗粒度提取上表现出色（F1 = 0.943）。然而，在细粒度实体级任务上性能显著下降（例如F1 = 0.675），特别是在处理罕见事件和复杂属性时。", "conclusion": "尽管在粗粒度提取上表现良好，但在检测代表性不足的不良事件和捕获细微的临床语言方面仍然存在重大挑战。该数据集可作为评估不良事件提取方法和支持未来跨数据集泛化的强大基准。", "translation": "在这项工作中，我们提出了一个手动标注的语料库，用于从老年患者的出院总结中提取不良事件（AE），老年患者是临床NLP资源中经常代表性不足的人群。该数据集包括14个临床上重要的不良事件，如跌倒、谵妄和颅内出血，以及上下文属性，如否定、诊断类型和住院期间发生。独特的是，该标注方案支持不连续和重叠的实体，解决了以往工作中很少处理的挑战。我们使用FlairNLP在三种标注粒度下评估了多个模型：细粒度、粗粒度以及带否定的粗粒度。虽然基于Transformer的模型（例如BERT-cased）在文档级粗粒度提取上取得了强大的性能（F1 = 0.943），但在细粒度实体级任务上性能显著下降（例如F1 = 0.675），特别是对于罕见事件和复杂属性。这些结果表明，尽管得分很高，但在检测代表性不足的不良事件和捕获细微的临床语言方面仍存在重大挑战。该数据集在可信研究环境（TRE）中开发，可通过DataLoch按需提供，并作为评估不良事件提取方法和支持未来跨数据集泛化的强大基准。", "summary": "本研究提出了一个用于从老年患者出院总结中提取不良事件（AE）的手动标注数据集，填补了临床NLP资源中老年患者代表性不足的空白。该数据集包含14种临床AE及上下文属性，并采用了支持不连续和重叠实体的独特标注方案。研究评估了FlairNLP模型在不同粒度下的性能，发现Transformer模型在粗粒度提取上表现良好（F1=0.943），但在细粒度任务（F1=0.675）上性能显著下降，尤其对于罕见事件和复杂属性。这表明在检测代表性不足的AE和处理复杂临床语言方面仍存在挑战。该数据集可作为AE提取方法的强大基准。", "keywords": "不良事件提取, 临床NLP, 数据集, 出院总结, 老年患者", "comments": "该论文的创新之处在于创建了一个针对老年患者出院总结的全新手动标注不良事件数据集，填补了该人群在临床NLP资源中的空白。其独特的标注方案支持不连续和重叠实体，解决了以往研究中较少触及的复杂性问题。该研究的重要性在于为不良事件提取提供了一个急需的基准，特别是对于弱势群体，并揭示了细粒度临床NLP中尚存的挑战。局限性在于模型在细粒度任务和罕见事件上的性能显著下降，表明了该问题本身的复杂性。"}}
{"id": "2506.14780", "title": "Faster Computation of Entropic Optimal Transport via Stable Low Frequency Modes", "authors": ["Reda Chhaibi", "Serge Gratton", "Samuel Vaiter"], "summary": "In this paper, we propose an accelerated version for the Sinkhorn algorithm,\nwhich is the reference method for computing the solution to Entropic Optimal\nTransport.\n  Its main draw-back is the exponential slow-down of convergence as the\nregularization weakens $\\varepsilon \\rightarrow 0$.\n  Thanks to spectral insights on the behavior of the Hessian, we propose to\nmitigate the problem via an original spectral warm-start strategy. This leads\nto faster convergence compared to the reference method, as also demonstrated in\nour numerical experiments.", "comment": "22 pages, 5 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14780v1", "AI": {"title_translation": "熵最优传输的稳定低频模式加速计算", "tldr": "提出一种基于谱分析的预热策略，加速Sinkhorn算法计算熵最优传输，解决其在正则化弱化时收敛慢的问题。", "motivation": "Sinkhorn算法是计算熵最优传输的参考方法，但其主要缺点是当正则化项ε趋于0时，收敛速度呈指数级下降。", "method": "通过对Hessian矩阵行为的谱分析，提出一种原创的谱预热策略来缓解收敛慢的问题。", "result": "数值实验表明，与参考方法相比，该方法收敛速度更快。", "conclusion": "通过谱预热策略可以有效加速熵最优传输的Sinkhorn算法，尤其是在正则化项较弱的情况下。", "translation": "在本文中，我们提出了一种加速版的Sinkhorn算法，该算法是计算熵最优传输解的参考方法。其主要缺点是当正则化ε趋于0时，收敛速度呈指数级下降。通过对Hessian行为的谱洞察，我们提出通过一种原创的谱预热策略来缓解这个问题。这导致了比参考方法更快的收敛速度，这也在我们的数值实验中得到了证明。", "summary": "本文提出一种加速Sinkhorn算法计算熵最优传输的方法，旨在解决该算法在正则化项减弱时收敛速度指数级下降的问题。通过对Hessian矩阵行为的谱分析，研究人员开发了一种新颖的谱预热策略，并在数值实验中验证了其比现有参考方法更快的收敛性能。", "keywords": "熵最优传输, Sinkhorn算法, 谱预热, 加速计算, 收敛性", "comments": "该研究通过引入谱预热策略，有效解决了Sinkhorn算法在特定条件下的收敛瓶颈，为熵最优传输的实际应用提供了更高效的计算工具。其创新点在于将谱分析应用于算法的加速，具有理论和实践意义。"}}
{"id": "2506.14864", "title": "pycnet-audio: A Python package to support bioacoustics data processing", "authors": ["Zachary J. Ruff", "Damon B. Lesmeister"], "summary": "Passive acoustic monitoring is an emerging approach in wildlife research that\nleverages recent improvements in purpose-made automated recording units (ARUs).\nThe general approach is to deploy ARUs in the field to record on a programmed\nschedule for extended periods (weeks or months), after which the audio data are\nretrieved. These data must then be processed, typically either by measuring or\nanalyzing characteristics of the audio itself (e.g. calculating acoustic\nindices), or by searching for some signal of interest within the recordings,\ne.g. vocalizations or other sounds produced by some target species,\nanthropogenic or environmental noise, etc. In the latter case, some method is\nrequired to locate the signal(s) of interest within the audio. While very small\ndatasets can simply be searched manually, even modest projects can produce\naudio datasets on the order of 105 hours of recordings, making manual review\nimpractical and necessitating some form of automated detection. pycnet-audio\n(Ruff 2024) is intended to provide a practical processing workflow for acoustic\ndata, built around the PNW-Cnet model, which was initially developed by the\nU.S. Forest Service to support population monitoring of northern spotted owls\n(Strix occidentalis caurina) and other forest owls (Lesmeister and Jenkins\n2022; Ruff et al. 2020). PNW-Cnet has been expanded to detect vocalizations of\nca. 80 forest wildlife species and numerous forms of anthropogenic and\nenvironmental noise (Ruff et al. 2021, 2023).", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.14864v1", "AI": {"title_translation": "pycnet-audio：一个支持生物声学数据处理的Python包", "tldr": "pycnet-audio是一个基于PNW-Cnet模型的Python包，用于自动化处理大型生物声学数据集，检测野生动物叫声和噪音。", "motivation": "被动声学监测产生的生物声学数据集规模庞大，手动处理不切实际，因此需要自动化解决方案。", "method": "pycnet-audio是一个Python包，它围绕PNW-Cnet模型构建。PNW-Cnet最初由美国林务局开发，用于监测斑点猫头鹰和其他森林猫头鹰，现已扩展到检测约80种森林野生动物的叫声以及各种人为和环境噪音。该包提供了一个实用的声学数据处理工作流程，包括测量音频特征或定位特定信号。", "result": "pycnet-audio为处理大型生物声学数据集提供了一个自动化且实用的工作流程，利用PNW-Cnet模型检测广泛的野生动物叫声和环境/人为噪音的能力。", "conclusion": "pycnet-audio通过自动化处理大量的生物声学数据，为野生动物研究提供了一个关键工具，使被动声学监测更加高效和可扩展。", "translation": "被动声学监测是野生动物研究中一种新兴方法，它利用了专用自动化录音设备（ARUs）的最新改进。通常的做法是在野外部署ARU，按预设时间表进行数周或数月的长时间录音，然后检索音频数据。这些数据随后必须进行处理，通常是通过测量或分析音频本身的特征（例如计算声学指数），或者在录音中搜索感兴趣的信号，例如某些目标物种产生的叫声或其他声音、人为或环境噪音等。在后一种情况下，需要某种方法来定位音频中感兴趣的信号。虽然非常小的数据集可以简单地手动搜索，但即使是适度的项目也能产生大约10^5小时的录音音频数据集，这使得手动审查不切实际，并需要某种形式的自动化检测。pycnet-audio（Ruff 2024）旨在为声学数据提供一个实用的处理工作流程，该工作流程围绕PNW-Cnet模型构建，PNW-Cnet模型最初由美国林务局开发，用于支持北部斑点猫头鹰（Strix occidentalis caurina）和其他森林猫头鹰的种群监测（Lesmeister和Jenkins 2022；Ruff et al. 2020）。PNW-Cnet已扩展到检测约80种森林野生动物的叫声以及多种形式的人为和环境噪音（Ruff et al. 2021, 2023）。", "summary": "pycnet-audio是一个Python包，旨在为生物声学数据处理提供实用工作流程。鉴于被动声学监测产生的大量数据使得手动分析不切实际，该包基于PNW-Cnet模型实现自动化检测。PNW-Cnet最初用于监测森林猫头鹰，现已扩展到识别约80种森林野生动物的叫声以及多种人为和环境噪音。该工具通过自动化处理，有效解决了生物声学数据分析的规模化挑战。", "keywords": "生物声学, 被动声学监测, Python包, 自动化检测, PNW-Cnet", "comments": "该软件包通过自动化数据处理，解决了生物声学领域的一个重大挑战，这对于大规模被动声学监测至关重要。它建立在PNW-Cnet模型的基础上，该模型经过了广泛的开发和扩展，表明其具有鲁棒性和广泛的适用性。其创新之处在于提供了一个实用、易于访问的Python软件包，简化了以前劳动密集型的过程。"}}
{"id": "2506.15028", "title": "Systems-Theoretic and Data-Driven Security Analysis in ML-enabled Medical Devices", "authors": ["Gargi Mitra", "Mohammadreza Hallajiyan", "Inji Kim", "Athish Pranav Dharmalingam", "Mohammed Elnawawy", "Shahrear Iqbal", "Karthik Pattabiraman", "Homa Alemzadeh"], "summary": "The integration of AI/ML into medical devices is rapidly transforming\nhealthcare by enhancing diagnostic and treatment facilities. However, this\nadvancement also introduces serious cybersecurity risks due to the use of\ncomplex and often opaque models, extensive interconnectivity, interoperability\nwith third-party peripheral devices, Internet connectivity, and vulnerabilities\nin the underlying technologies. These factors contribute to a broad attack\nsurface and make threat prevention, detection, and mitigation challenging.\nGiven the highly safety-critical nature of these devices, a cyberattack on\nthese devices can cause the ML models to mispredict, thereby posing significant\nsafety risks to patients. Therefore, ensuring the security of these devices\nfrom the time of design is essential. This paper underscores the urgency of\naddressing the cybersecurity challenges in ML-enabled medical devices at the\npre-market phase. We begin by analyzing publicly available data on device\nrecalls and adverse events, and known vulnerabilities, to understand the threat\nlandscape of AI/ML-enabled medical devices and their repercussions on patient\nsafety. Building on this analysis, we introduce a suite of tools and techniques\ndesigned by us to assist security analysts in conducting comprehensive\npremarket risk assessments. Our work aims to empower manufacturers to embed\ncybersecurity as a core design principle in AI/ML-enabled medical devices,\nthereby making them safe for patients.", "comment": "32 pages, 6 figures, 6 tables", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15028v1", "AI": {"title_translation": "机器学习医疗设备中的系统理论与数据驱动安全分析", "tldr": "本研究旨在通过分析公开数据和引入一套工具，帮助安全分析师在上市前对机器学习医疗设备进行全面的网络安全风险评估，以确保患者安全。", "motivation": "AI/ML集成到医疗设备中带来了显著的医疗改进，但也引入了严重的安全风险，如模型不透明、互联性广、第三方设备互操作性、互联网连接以及底层技术漏洞，这些都扩大了攻击面。鉴于这些设备的高度安全关键性，网络攻击可能导致ML模型误判，从而对患者构成重大安全风险。因此，从设计阶段确保这些设备的安全性至关重要，尤其是在上市前阶段解决网络安全挑战具有紧迫性。", "method": "研究首先分析了公开可用的设备召回、不良事件数据和已知漏洞，以了解AI/ML医疗设备的威胁态势及其对患者安全的影响。在此分析的基础上，引入了一套自行设计的工具和技术，旨在协助安全分析师进行全面的上市前风险评估。", "result": "本研究旨在协助安全分析师进行全面的上市前风险评估，并赋能制造商将网络安全作为AI/ML医疗设备的核心设计原则，从而确保设备对患者是安全的。", "conclusion": "确保机器学习医疗设备从设计阶段就具备安全性至关重要，尤其是在上市前阶段解决网络安全挑战具有紧迫性，以保障患者安全。", "translation": "AI/ML集成到医疗设备中正在通过增强诊断和治疗设施迅速改变医疗保健。然而，这种进步也由于复杂且通常不透明的模型、广泛的互联性、与第三方外围设备的互操作性、互联网连接以及底层技术的漏洞而引入了严重的安全风险。这些因素导致了广泛的攻击面，并使威胁预防、检测和缓解变得具有挑战性。鉴于这些设备的高度安全关键性，对这些设备的网络攻击可能导致ML模型误判，从而对患者构成重大安全风险。因此，从设计阶段确保这些设备的安全性至关重要。本文强调了在上市前阶段解决ML医疗设备网络安全挑战的紧迫性。我们首先分析了公开可用的设备召回和不良事件数据以及已知漏洞，以了解AI/ML医疗设备的威胁态势及其对患者安全的影响。在此分析的基础上，我们引入了一套自行设计的工具和技术，旨在协助安全分析师进行全面的上市前风险评估。我们的工作旨在赋能制造商将网络安全作为AI/ML医疗设备的核心设计原则，从而使它们对患者是安全的。", "summary": "本文探讨了AI/ML集成医疗设备带来的网络安全挑战。由于模型复杂性、广泛互联性和潜在漏洞，这些设备面临严重威胁，可能导致模型误判并危及患者安全。研究强调了在上市前阶段解决这些问题的紧迫性，通过分析公开数据来理解威胁格局，并提出一套工具和技术，以协助安全分析师进行全面的上市前风险评估。最终目标是帮助制造商将网络安全融入设备设计，确保患者安全。", "keywords": "机器学习医疗设备, 网络安全, 风险评估, 患者安全, 上市前分析", "comments": "本论文的创新之处在于其结合了系统理论和数据驱动的方法来解决机器学习医疗设备的网络安全问题，并特别强调了在产品上市前阶段进行风险评估的重要性。这对于保障患者安全和推动医疗技术健康发展具有重要意义。通过分析公开数据和提供一套工具，该研究为制造商和安全分析师提供了一个实用的框架。"}}
{"id": "2506.15084", "title": "An Empirical Study of Bugs in Data Visualization Libraries", "authors": ["Weiqi Lu", "Yongqiang Tian", "Xiaohan Zhong", "Haoyang Ma", "Zhenyang Xu", "Shing-Chi Cheung", "Chengnian Sun"], "summary": "Data visualization (DataViz) libraries play a crucial role in presentation,\ndata analysis, and application development, underscoring the importance of\ntheir accuracy in transforming data into visual representations. Incorrect\nvisualizations can adversely impact user experience, distort information\nconveyance, and influence user perception and decision-making processes. Visual\nbugs in these libraries can be particularly insidious as they may not cause\nobvious errors like crashes, but instead mislead users of the underlying data\ngraphically, resulting in wrong decision making. Consequently, a good\nunderstanding of the unique characteristics of bugs in DataViz libraries is\nessential for researchers and developers to detect and fix bugs in DataViz\nlibraries.\n  This study presents the first comprehensive analysis of bugs in DataViz\nlibraries, examining 564 bugs collected from five widely-used libraries. Our\nstudy systematically analyzes their symptoms and root causes, and provides a\ndetailed taxonomy. We found that incorrect/inaccurate plots are pervasive in\nDataViz libraries and incorrect graphic computation is the major root cause,\nwhich necessitates further automated testing methods for DataViz libraries.\nMoreover, we identified eight key steps to trigger such bugs and two test\noracles specific to DataViz libraries, which may inspire future research in\ndesigning effective automated testing techniques. Furthermore, with the recent\nadvancements in Vision Language Models (VLMs), we explored the feasibility of\napplying these models to detect incorrect/inaccurate plots. The results show\nthat the effectiveness of VLMs in bug detection varies from 29% to 57%,\ndepending on the prompts, and adding more information in prompts does not\nnecessarily increase the effectiveness. More findings can be found in our\nmanuscript.", "comment": "Proc. ACM Softw. Eng. 2, FSE", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15084v1", "AI": {"title_translation": "数据可视化库中缺陷的实证研究", "tldr": "对数据可视化库中的缺陷进行了首次全面分析，研究了564个缺陷的症状和根本原因，并提出了一个详细的分类法，发现不准确的图表普遍存在且计算错误是主要原因。还探索了视觉语言模型在缺陷检测中的应用。", "motivation": "数据可视化库在数据呈现、分析和应用开发中至关重要，但其中的视觉缺陷可能不易察觉，却会导致错误的信息传达和决策。因此，理解数据可视化库中缺陷的独特特征对于研究人员和开发人员检测和修复缺陷至关重要。", "method": "本研究对从五个广泛使用的库中收集的564个数据可视化库缺陷进行了首次全面分析。系统地分析了它们的症状和根本原因，并提供了一个详细的分类法。此外，还确定了触发此类缺陷的八个关键步骤和两个特定于数据可视化库的测试预言机。最后，探索了视觉语言模型（VLMs）在检测不正确/不准确图表中的可行性。", "result": "研究发现，不正确/不准确的图表在数据可视化库中普遍存在，并且不正确的图形计算是主要的根本原因，这表明需要进一步的自动化测试方法。此外，确定了触发此类缺陷的八个关键步骤和两个特定于数据可视化库的测试预言机。探索VLMs在缺陷检测中的有效性从29%到57%不等，取决于提示，并且在提示中添加更多信息不一定会提高有效性。", "conclusion": "数据可视化库中不准确的图表普遍存在，主要原因是图形计算错误，需要开发更有效的自动化测试技术。视觉语言模型在缺陷检测方面有一定潜力，但其有效性受提示影响较大。", "translation": "数据可视化（DataViz）库在呈现、数据分析和应用程序开发中扮演着关键角色，突显了其将数据转换为视觉表示的准确性的重要性。不正确的可视化可能会对用户体验产生不利影响，扭曲信息传达，并影响用户感知和决策过程。这些库中的视觉缺陷可能特别隐蔽，因为它们可能不会导致像崩溃那样的明显错误，而是以图形方式误导用户对底层数据的理解，从而导致错误的决策。因此，深入了解DataViz库中缺陷的独特特征对于研究人员和开发人员检测和修复DataViz库中的缺陷至关重要。\n本研究首次对DataViz库中的缺陷进行了全面分析，检查了从五个广泛使用的库中收集的564个缺陷。我们的研究系统地分析了它们的症状和根本原因，并提供了一个详细的分类法。我们发现不正确/不准确的图表在DataViz库中普遍存在，不正确的图形计算是主要根本原因，这需要进一步的数据可视化库自动化测试方法。此外，我们确定了触发此类缺陷的八个关键步骤和两个特定于DataViz库的测试预言机，这可能启发未来设计有效自动化测试技术的研究。此外，随着视觉语言模型（VLMs）的最新进展，我们探索了应用这些模型检测不正确/不准确图表的可行性。结果表明，VLMs在缺陷检测中的有效性从29%到57%不等，取决于提示，并且在提示中添加更多信息不一定会提高有效性。更多发现可在我们的手稿中找到。", "summary": "本研究首次对数据可视化库中的缺陷进行了全面深入的实证分析。通过检查来自五个流行库的564个缺陷，研究系统地分析了它们的症状、根本原因并构建了详细的分类法。主要发现包括不准确图表的普遍性以及图形计算错误是核心原因，这强调了开发自动化测试方法的必要性。此外，研究还识别了触发缺陷的关键步骤和特定的测试预言机，并初步探索了视觉语言模型在检测不准确图表方面的应用潜力，发现其有效性受提示影响。", "keywords": "数据可视化, 软件缺陷, 实证研究, 自动化测试, 视觉语言模型", "comments": "这项研究首次对数据可视化库中的缺陷进行了系统性的大规模分析，填补了该领域的一个空白。其创新之处在于提供了详细的缺陷分类法、识别了缺陷触发步骤和测试预言机，为未来的自动化测试方法设计奠定了基础。同时，初步探索了VLMs在缺陷检测中的应用，尽管效果有限，但也为后续研究提供了方向。这项工作对于提高数据可视化工具的可靠性和用户决策的准确性具有重要意义。"}}
{"id": "2506.15207", "title": "Multi-Agent Reinforcement Learning for Autonomous Multi-Satellite Earth Observation: A Realistic Case Study", "authors": ["Mohamad A. Hady", "Siyi Hu", "Mahardhika Pratama", "Jimmy Cao", "Ryszard Kowalczyk"], "summary": "The exponential growth of Low Earth Orbit (LEO) satellites has revolutionised\nEarth Observation (EO) missions, addressing challenges in climate monitoring,\ndisaster management, and more. However, autonomous coordination in\nmulti-satellite systems remains a fundamental challenge. Traditional\noptimisation approaches struggle to handle the real-time decision-making\ndemands of dynamic EO missions, necessitating the use of Reinforcement Learning\n(RL) and Multi-Agent Reinforcement Learning (MARL). In this paper, we\ninvestigate RL-based autonomous EO mission planning by modelling\nsingle-satellite operations and extending to multi-satellite constellations\nusing MARL frameworks. We address key challenges, including energy and data\nstorage limitations, uncertainties in satellite observations, and the\ncomplexities of decentralised coordination under partial observability. By\nleveraging a near-realistic satellite simulation environment, we evaluate the\ntraining stability and performance of state-of-the-art MARL algorithms,\nincluding PPO, IPPO, MAPPO, and HAPPO. Our results demonstrate that MARL can\neffectively balance imaging and resource management while addressing\nnon-stationarity and reward interdependency in multi-satellite coordination.\nThe insights gained from this study provide a foundation for autonomous\nsatellite operations, offering practical guidelines for improving policy\nlearning in decentralised EO missions.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15207v1", "AI": {"title_translation": "多智能体强化学习在自主多卫星地球观测中的应用：一个现实案例研究", "tldr": "本文研究了多智能体强化学习（MARL）在自主多卫星地球观测任务规划中的应用，解决了资源限制和不确定性等挑战，并在准现实仿真环境中验证了MARL算法的有效性，为自主卫星操作提供了实践指导。", "motivation": "低地球轨道（LEO）卫星的增长推动了地球观测任务，但多卫星系统的自主协调仍是基本挑战。传统优化方法难以应对动态EO任务的实时决策需求，因此需要强化学习（RL）和多智能体强化学习（MARL）。", "method": "通过建模单卫星操作并使用MARL框架扩展到多卫星星座，研究基于RL的自主EO任务规划。解决了能源和数据存储限制、卫星观测不确定性以及部分可观测下分散协调的复杂性。在准现实卫星仿真环境中，评估了PPO、IPPO、MAPPO和HAPPO等MARL算法的训练稳定性和性能。", "result": "MARL能够有效平衡成像和资源管理，同时解决多卫星协调中的非平稳性和奖励相互依赖问题。", "conclusion": "本研究的见解为自主卫星操作奠定了基础，并为改进分散式EO任务中的策略学习提供了实践指导。", "translation": "低地球轨道（LEO）卫星的指数级增长彻底改变了地球观测（EO）任务，解决了气候监测、灾害管理等方面的挑战。然而，多卫星系统中的自主协调仍然是一个基本挑战。传统的优化方法难以处理动态EO任务的实时决策需求，因此需要使用强化学习（RL）和多智能体强化学习（MARL）。在本文中，我们通过建模单卫星操作并使用MARL框架扩展到多卫星星座来研究基于RL的自主EO任务规划。我们解决了关键挑战，包括能源和数据存储限制、卫星观测的不确定性以及部分可观测下分散协调的复杂性。通过利用一个接近现实的卫星仿真环境，我们评估了包括PPO、IPPO、MAPPO和HAPPO在内的最先进MARL算法的训练稳定性和性能。我们的结果表明，MARL可以有效地平衡成像和资源管理，同时解决多卫星协调中的非平稳性和奖励相互依赖问题。本研究获得的见解为自主卫星操作奠定了基础，为改进分散式EO任务中的策略学习提供了实践指导。", "summary": "本文探讨了多智能体强化学习（MARL）在自主多卫星地球观测任务规划中的应用。研究人员通过建模单卫星操作并扩展到多卫星星座，解决了能源、数据存储限制及不确定性等挑战。在准现实仿真环境中，评估了PPO、IPPO、MAPPO和HAPPO等先进MARL算法，结果表明MARL能有效平衡成像与资源管理，并处理多卫星协调中的复杂性。本研究为自主卫星操作和分散式地球观测任务的策略学习提供了实用指导。", "keywords": "多智能体强化学习, 地球观测, 卫星协调, 自主操作, 强化学习", "comments": "该研究的创新之处在于将多智能体强化学习应用于复杂的自主多卫星地球观测任务，并考虑了现实世界中的多项限制和挑战，如资源限制和部分可观测性。其重要性在于为未来自主卫星操作提供了坚实的理论和实践基础，特别是在应对动态和分布式任务方面。"}}
{"id": "2506.14928", "title": "On the solvable-unsolvable transition due to noise-induced chaos in digital memcomputing", "authors": ["Dyk Chung Nguyen", "Thomas Chetaille", "Yuan-Hang Zhang", "Yuriy V. Pershin", "Massimiliano Di Ventra"], "summary": "Digital memcomputing machines (DMMs) have been designed to solve complex\ncombinatorial optimization problems. Since DMMs are fundamentally classical\ndynamical systems, their ordinary differential equations (ODEs) can be\nefficiently simulated on modern computers. This provides a unique platform to\nstudy their performance under various conditions. An aspect that has received\nlittle attention so far is how their performance is affected by the numerical\nerrors in the solution of their ODEs and the physical noise they would be\nnaturally subject to if built in hardware. Here, we analyze these two aspects\nin detail by varying the integration time step (numerical noise) and adding\nstochastic perturbations (physical noise) into the equations of DMMs. We are\nparticularly interested in understanding how noise induces a chaotic transition\nthat marks the shift from successful problem-solving to failure in these\nsystems. Our study includes an analysis of power spectra and Lyapunov exponents\ndepending on the noise strength. The results reveal a correlation between the\ninstance solvability and the sign of the ensemble averaged mean largest\nLyapunov exponent. Interestingly, we find a regime in which DMMs with positive\nmean largest Lyapunov exponents still exhibit solvability. Furthermore, the\npower spectra provide additional information about our system by distinguishing\nbetween regular behavior (peaks) and chaotic behavior (broadband spectrum).\nTherefore, power spectra could be utilized to control whether a DMM operates in\nthe optimal dynamical regime. Overall, we find that the qualitative effects of\nnumerical and physical noise are mostly similar, despite their fundamentally\ndifferent origin.", "comment": null, "cate": "nlin.CD", "url": "http://arxiv.org/abs/2506.14928v1", "AI": {"title_translation": "数字忆阻计算中噪声诱导混沌导致的可解-不可解转变", "tldr": "本研究探讨了数字忆阻计算机器（DMMs）中噪声如何导致混沌转变，从而影响问题解决能力。研究发现，可解性与李雅普诺夫指数的符号相关，并且功率谱可用于控制DMMs的最佳运行状态，同时数值噪声和物理噪声的影响在性质上相似。", "motivation": "数字忆阻计算机器（DMMs）被设计用于解决复杂的组合优化问题。然而，DMMs作为经典动力系统，其性能受数值误差和物理噪声的影响，这方面此前受到的关注较少。本研究旨在详细分析这些因素如何影响DMMs的性能，特别是噪声如何诱导混沌转变，从而导致问题解决的成功与失败之间的转变。", "method": "研究通过改变积分时间步长（数值噪声）和在DMMs方程中添加随机扰动（物理噪声）来分析噪声的影响。研究内容包括分析功率谱和李雅普诺夫指数，并将其与噪声强度关联起来。", "result": "结果显示，实例可解性与系综平均最大李雅普诺夫指数的符号之间存在相关性。有趣的是，研究发现一个DMMs即使具有正的平均最大李雅普诺夫指数，仍然表现出可解性的区域。此外，功率谱通过区分规则行为（峰值）和混沌行为（宽带谱）提供了额外信息。数值噪声和物理噪声的定性影响大多相似，尽管它们的来源根本不同。", "conclusion": "噪声可以在数字忆阻计算机器（DMMs）中诱导混沌转变，从而导致问题解决的失败。然而，在某些情况下，即使李雅普诺夫指数为正，DMMs仍能保持可解性。功率谱可以作为控制DMMs在最佳动态范围内运行的工具。", "translation": "数字忆阻计算机器（DMMs）被设计用于解决复杂的组合优化问题。由于DMMs本质上是经典的动力系统，它们的常微分方程（ODEs）可以在现代计算机上高效模拟。这提供了一个独特的平台来研究它们在各种条件下的性能。迄今为止，一个很少受到关注的方面是其性能如何受到其ODEs求解中的数值误差以及如果构建为硬件时它们自然会受到的物理噪声的影响。在这里，我们通过改变积分时间步长（数值噪声）和在DMMs方程中添加随机扰动（物理噪声）来详细分析这两个方面。我们特别感兴趣的是理解噪声如何诱导混沌转变，从而标志着这些系统中从成功解决问题到失败的转变。我们的研究包括分析功率谱和李雅普诺夫指数，具体取决于噪声强度。结果揭示了实例可解性与系综平均最大李雅普诺夫指数的符号之间的相关性。有趣的是，我们发现一个DMMs即使具有正的平均最大李雅普诺夫指数，仍然表现出可解性的区域。此外，功率谱通过区分规则行为（峰值）和混沌行为（宽带谱）提供了关于我们系统的额外信息。因此，功率谱可以用于控制DMM是否在最佳动态范围内运行。总的来说，我们发现数值噪声和物理噪声的定性影响大多相似，尽管它们的来源根本不同。", "summary": "本论文研究了数值误差和物理噪声对数字忆阻计算机器（DMMs）性能的影响，重点关注噪声如何诱导从成功问题解决到失败的混沌转变。通过改变积分时间步长和添加随机扰动，作者分析了功率谱和李雅普诺夫指数。研究发现，实例可解性与系综平均最大李雅普诺夫指数的符号相关，并且存在DMMs在李雅普诺夫指数为正时仍可解的区域。功率谱能够区分规则行为和混沌行为，表明其可用于优化DMMs的操作。研究还指出，数值噪声和物理噪声的定性影响基本相似。", "keywords": "数字忆阻计算, 噪声诱导混沌, 可解性, 李雅普诺夫指数, 功率谱", "comments": "本论文创新性地探讨了噪声对数字忆阻计算机器（DMMs）性能的影响，并将其与混沌理论（通过李雅普诺夫指数和功率谱）联系起来。这为理解DMMs的运行极限以及未来硬件的鲁棒性设计提供了重要见解。发现DMMs在某些情况下即使李雅普诺夫指数为正也能保持可解性，这一点尤其值得关注。"}}
{"id": "2506.14857", "title": "Towards Perception-based Collision Avoidance for UAVs when Guiding the Visually Impaired", "authors": ["Suman Raj", "Swapnil Padhi", "Ruchi Bhoot", "Prince Modi", "Yogesh Simmhan"], "summary": "Autonomous navigation by drones using onboard sensors combined with machine\nlearning and computer vision algorithms is impacting a number of domains,\nincluding agriculture, logistics, and disaster management. In this paper, we\nexamine the use of drones for assisting visually impaired people (VIPs) in\nnavigating through outdoor urban environments. Specifically, we present a\nperception-based path planning system for local planning around the\nneighborhood of the VIP, integrated with a global planner based on GPS and maps\nfor coarse planning. We represent the problem using a geometric formulation and\npropose a multi DNN based framework for obstacle avoidance of the UAV as well\nas the VIP. Our evaluations conducted on a drone human system in a university\ncampus environment verifies the feasibility of our algorithms in three\nscenarios; when the VIP walks on a footpath, near parked vehicles, and in a\ncrowded street.", "comment": "16 pages, 7 figures; Accepted as Late-Breaking Results at the\n  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\n  2023", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14857v1", "AI": {"title_translation": "面向视觉障碍者引导的无人机基于感知的避障研究", "tldr": "该论文提出了一种基于感知的路径规划系统，用于无人机在室外城市环境中协助视障人士导航和避障，并通过实验验证了其可行性。", "motivation": "无人机结合机载传感器、机器学习和计算机视觉算法在多个领域展现潜力。本研究旨在探索无人机在室外城市环境中协助视障人士（VIP）导航的应用。", "method": "该系统采用基于感知的局部路径规划，结合基于GPS和地图的全局规划。问题通过几何公式表示，并提出了一个基于多DNN的框架，用于无人机和视障人士的避障。", "result": "在大学校园环境中对无人机-人类系统进行的评估验证了该算法在三种场景下的可行性：视障人士在人行道上行走、在停放车辆附近行走以及在拥挤街道上行走。", "conclusion": "该研究验证了所提出的基于感知的无人机避障系统在协助视障人士导航方面的可行性。", "translation": "无人机利用机载传感器结合机器学习和计算机视觉算法进行自主导航，正在影响农业、物流和灾害管理等多个领域。在本文中，我们研究了无人机在室外城市环境中协助视障人士（VIP）导航的应用。具体来说，我们提出了一种基于感知的路径规划系统，用于VIP附近区域的局部规划，并与基于GPS和地图的全局规划器集成，用于粗略规划。我们使用几何公式来表示该问题，并提出了一个基于多DNN的框架，用于无人机以及VIP的避障。我们在大学校园环境中对无人机-人类系统进行的评估验证了我们算法在三种场景下的可行性：当VIP在人行道上行走时、在停放车辆附近行走时以及在拥挤街道上行走时。", "summary": "本研究探讨了无人机在室外城市环境中辅助视障人士导航的应用。论文提出了一种结合全局（GPS/地图）和局部（基于感知）规划的路径规划系统，并采用几何公式和多DNN框架实现无人机及视障人士的避障。实验结果表明，该系统在多种城市场景下均能有效运行，验证了其可行性。", "keywords": "无人机避障, 视障人士导航, 路径规划, 深度学习, 计算机视觉", "comments": "本文提出了一种新颖且具有社会意义的应用场景，即利用无人机辅助视障人士导航。其创新点在于结合了全局和局部路径规划，并引入多DNN框架进行双重避障（无人机和视障人士）。该研究具有重要的实际应用潜力，为残障辅助技术提供了新的思路。"}}
{"id": "2506.14777", "title": "WebXAII: an open-source web framework to study human-XAI interaction", "authors": ["Jules Leguy", "Pierre-Antoine Jean", "Felipe Torres Figueroa", "Sébastien Harispe"], "summary": "This article introduces WebXAII, an open-source web framework designed to\nfacilitate research on human interaction with eXplainable Artificial\nIntelligence (XAI) systems. The field of XAI is rapidly expanding, driven by\nthe growing societal implications of the widespread adoption of AI (and in\nparticular machine learning) across diverse applications. Researchers who study\nthe interaction between humans and XAI techniques typically develop ad hoc\ninterfaces in order to conduct their studies. These interfaces are usually not\nshared alongside the results of the studies, which limits their reusability and\nthe reproducibility of experiments. In response, we design and implement\nWebXAII, a web-based platform that can embody full experimental protocols,\nmeaning that it can present all aspects of the experiment to human participants\nand record their responses. The experimental protocols are translated into a\ncomposite architecture of generic views and modules, which offers a lot of\nflexibility. The architecture is defined in a structured configuration file, so\nthat protocols can be implemented with minimal programming skills. We\ndemonstrate that WebXAII can effectively embody relevant protocols, by\nreproducing the protocol of a state-of-the-art study of the literature. The\nframework is available at https://github.com/PAJEAN/WebXAII.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14777v1", "AI": {"title_translation": "WebXAII：一个用于研究人机可解释人工智能交互的开源Web框架", "tldr": "WebXAII是一个开源Web框架，旨在促进人机可解释人工智能（XAI）系统交互的研究，通过提供可重用和可复现的实验协议平台，解决了现有研究中接口非共享和可复用性低的问题。", "motivation": "可解释人工智能（XAI）领域发展迅速，AI（特别是机器学习）的广泛应用带来了日益增长的社会影响。研究人机XAI交互的学者通常开发一次性接口进行研究，这些接口通常不与研究结果一起共享，限制了其可重用性和实验的可复现性。", "method": "作者设计并实现了WebXAII，一个基于Web的平台，能够承载完整的实验协议，向参与者展示实验的所有方面并记录其响应。实验协议被转化为通用视图和模块的复合架构，提供了很大的灵活性。该架构在结构化配置文件中定义，只需最少的编程技能即可实现协议。", "result": "作者通过复现一篇最先进文献研究的协议，证明WebXAII可以有效地承载相关协议。", "conclusion": "Not mentioned in abstract", "translation": "本文介绍了WebXAII，一个旨在促进人机可解释人工智能（XAI）系统交互研究的开源Web框架。XAI领域正在迅速扩展，这得益于人工智能（特别是机器学习）在各种应用中广泛采用所带来的日益增长的社会影响。研究人机XAI技术交互的研究人员通常开发临时接口来开展研究。这些接口通常不与研究结果一起共享，这限制了它们的可重用性和实验的可复现性。为此，我们设计并实现了WebXAII，一个基于Web的平台，可以承载完整的实验协议，这意味着它可以向人类参与者展示实验的所有方面并记录他们的反应。实验协议被转化为通用视图和模块的复合架构，提供了很大的灵活性。该架构在结构化配置文件中定义，因此只需最少的编程技能即可实现协议。我们通过复制一篇最先进的文献研究协议，证明WebXAII可以有效地承载相关协议。该框架可在https://github.com/PAJEAN/WebXAII获取。", "summary": "WebXAII是一个开源Web框架，旨在解决人机可解释人工智能（XAI）交互研究中实验接口缺乏共享和可复用性的问题。该平台提供了一个灵活的、基于配置文件的架构，能够承载完整的实验协议并记录参与者响应，从而提高研究的可复现性。作者通过复现现有研究协议证明了其有效性。", "keywords": "WebXAII, 可解释人工智能, 人机交互, 开源框架, 实验复现性", "comments": "WebXAII的创新之处在于它提供了一个标准化的、可重用的Web框架，用于人机XAI交互研究，解决了现有方法中实验接口难以共享和复现的问题。这对于促进XAI领域的研究进步和提高实验结果的可靠性具有重要意义。其基于配置文件的设计降低了编程门槛，使得更广泛的研究人员能够方便地进行实验设计和执行。"}}
{"id": "2506.14836", "title": "Detecting Narrative Shifts through Persistent Structures: A Topological Analysis of Media Discourse", "authors": ["Mark M. Bailey", "Mark I. Heiligman"], "summary": "How can we detect when global events fundamentally reshape public discourse?\nThis study introduces a topological framework for identifying structural change\nin media narratives using persistent homology. Drawing on international news\narticles surrounding major events - including the Russian invasion of Ukraine\n(Feb 2022), the murder of George Floyd (May 2020), the U.S. Capitol\ninsurrection (Jan 2021), and the Hamas-led invasion of Israel (Oct 2023) - we\nconstruct daily co-occurrence graphs of noun phrases to trace evolving\ndiscourse. Each graph is embedded and transformed into a persistence diagram\nvia a Vietoris-Rips filtration. We then compute Wasserstein distances and\npersistence entropies across homological dimensions to capture semantic\ndisruption and narrative volatility over time. Our results show that major\ngeopolitical and social events align with sharp spikes in both H0 (connected\ncomponents) and H1 (loops), indicating sudden reorganization in narrative\nstructure and coherence. Cross-correlation analyses reveal a typical lag\npattern in which changes to component-level structure (H0) precede higher-order\nmotif shifts (H1), suggesting a bottom-up cascade of semantic change. An\nexception occurs during the Russian invasion of Ukraine, where H1 entropy leads\nH0, possibly reflecting top-down narrative framing before local discourse\nadjusts. Persistence entropy further distinguishes tightly focused from diffuse\nnarrative regimes. These findings demonstrate that persistent homology offers a\nmathematically principled, unsupervised method for detecting inflection points\nand directional shifts in public attention - without requiring prior knowledge\nof specific events. This topological approach advances computational social\nscience by enabling real-time detection of semantic restructuring during\ncrises, protests, and information shocks.", "comment": "23 pages", "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.14836v1", "AI": {"title_translation": "通过持久结构检测叙事转变：媒体话语的拓扑分析", "tldr": "本研究利用持久同调的拓扑框架，通过分析新闻文章中的名词短语共现图，检测重大事件如何重塑媒体叙事，发现叙事转变与拓扑指标的剧烈变化相关。", "motivation": "该研究旨在解决如何检测全球事件何时从根本上重塑公共话语的问题。", "method": "本研究引入了一个拓扑框架，利用持久同调来识别媒体叙事中的结构变化。研究人员构建了每日名词短语共现图，并通过Vietoris-Rips过滤将其嵌入并转换为持久图。然后计算了同调维度上的Wasserstein距离和持久熵，以捕捉语义中断和叙事波动。", "result": "结果显示，重大的地缘政治和社会事件与H0（连接分量）和H1（循环）的急剧峰值一致，表明叙事结构和连贯性发生突然重组。交叉关联分析揭示，分量级结构（H0）的变化通常先于高阶主题转变（H1），这表明语义变化是自下而上的级联。俄罗斯入侵乌克兰期间是一个例外，H1熵领先H0，可能反映了在局部话语调整之前存在自上而下的叙事框架。持久熵还能区分紧密聚焦和扩散的叙事模式。", "conclusion": "研究结果表明，持久同调提供了一种数学上严谨、无监督的方法，用于检测公共注意力中的拐点和方向性转变，而无需事先了解特定事件。这种拓扑方法通过实现危机、抗议和信息冲击期间语义重组的实时检测，推动了计算社会科学的发展。", "translation": "我们如何才能检测到全球事件何时从根本上重塑公共话语？本研究引入了一个拓扑框架，利用持久同调来识别媒体叙事中的结构变化。我们以围绕重大事件（包括2022年2月的俄罗斯入侵乌克兰、2020年5月的乔治·弗洛伊德谋杀案、2021年1月的美国国会暴动和2023年10月的哈马斯入侵以色列）的国际新闻文章为基础，构建了每日名词短语共现图，以追踪不断演变的话语。每个图都被嵌入并通过Vietoris-Rips过滤转换为持久图。然后，我们计算同调维度上的Wasserstein距离和持久熵，以捕捉语义中断和叙事随时间推移的波动性。我们的结果显示，重大的地缘政治和社会事件与H0（连接分量）和H1（循环）的急剧峰值一致，表明叙事结构和连贯性发生突然重组。交叉关联分析揭示了一种典型的滞后模式，其中分量级结构（H0）的变化先于高阶主题转变（H1），这表明语义变化是自下而上的级联。俄罗斯入侵乌克兰期间发生了一个例外，H1熵领先H0，这可能反映了在局部话语调整之前存在自上而下的叙事框架。持久熵进一步区分了紧密聚焦和扩散的叙事模式。这些发现表明，持久同调提供了一种数学上严谨、无监督的方法，用于检测公共注意力中的拐点和方向性转变——而无需事先了解特定事件。这种拓扑方法通过实现危机、抗议和信息冲击期间语义重组的实时检测，推动了计算社会科学的发展。", "summary": "本研究提出一种基于持久同调的拓扑框架，用于检测重大全球事件如何重塑公共话语中的媒体叙事。通过分析俄罗斯入侵乌克兰、乔治·弗洛伊德谋杀案、美国国会暴动和哈马斯入侵以色列等事件相关的国际新闻文章，研究人员构建了每日名词短语共现图，并利用拓扑数据分析方法（如Vietoris-Rips过滤、Wasserstein距离和持久熵）来量化叙事结构的变化。结果表明，重大事件与叙事结构（H0和H1）的突然重组相关，且通常伴随自下而上的语义变化级联。该方法提供了一种无需先验知识的无监督手段，用于实时检测公共注意力中的转折点和方向性转变，对计算社会科学领域具有重要意义。", "keywords": "持久同调, 叙事转变, 媒体话语, 拓扑数据分析, 语义重组", "comments": "这项研究的创新之处在于将拓扑数据分析，特别是持久同调，应用于媒体话语分析，提供了一种数学上严谨且无监督的方法来检测叙事转变。其重要性在于能够实时识别公共注意力中的关键转折点，无需依赖预设事件标签，这在危机管理和信息分析中具有巨大潜力。该方法揭示了叙事变化的层次结构，并能区分不同的叙事模式，为计算社会科学提供了一个强大的新工具。"}}
{"id": "2506.15011", "title": "GCN-Driven Reinforcement Learning for Probabilistic Real-Time Guarantees in Industrial URLLC", "authors": ["Eman Alqudah", "Ashfaq Khokhar"], "summary": "Ensuring packet-level communication quality is vital for ultra-reliable,\nlow-latency communications (URLLC) in large-scale industrial wireless networks.\nWe enhance the Local Deadline Partition (LDP) algorithm by introducing a Graph\nConvolutional Network (GCN) integrated with a Deep Q-Network (DQN)\nreinforcement learning framework for improved interference coordination in\nmulti-cell, multi-channel networks. Unlike LDP's static priorities, our\napproach dynamically learns link priorities based on real-time traffic demand,\nnetwork topology, remaining transmission opportunities, and interference\npatterns. The GCN captures spatial dependencies, while the DQN enables adaptive\nscheduling decisions through reward-guided exploration. Simulation results show\nthat our GCN-DQN model achieves mean SINR improvements of 179.6\\%, 197.4\\%, and\n175.2\\% over LDP across three network configurations. Additionally, the GCN-DQN\nmodel demonstrates mean SINR improvements of 31.5\\%, 53.0\\%, and 84.7\\% over\nour previous CNN-based approach across the same configurations. These results\nunderscore the effectiveness of our GCN-DQN model in addressing complex URLLC\nrequirements with minimal overhead and superior network performance.", "comment": "This paper has been submitted to IEEE MASS 2025 on May 7, 2025", "cate": "cs.NI", "url": "http://arxiv.org/abs/2506.15011v1", "AI": {"title_translation": "GCN驱动的强化学习在工业URLLC中实现概率实时保证", "tldr": "该研究提出了一种基于GCN-DQN的强化学习方法，通过动态学习链路优先级来增强LDP算法，以改善多蜂窝、多信道网络中的干扰协调，并在URLLC场景下显著提升了SINR性能。", "motivation": "在大规模工业无线网络中，确保超可靠、低延迟通信（URLLC）的包级通信质量至关重要。", "method": "该研究通过引入一个集成深度Q网络（DQN）强化学习框架的图卷积网络（GCN）来增强局部截止日期划分（LDP）算法，以改善多蜂窝、多信道网络中的干扰协调。与LDP的静态优先级不同，该方法根据实时流量需求、网络拓扑、剩余传输机会和干扰模式动态学习链路优先级。GCN捕获空间依赖性，而DQN通过奖励引导的探索实现自适应调度决策。", "result": "仿真结果表明，在三种网络配置下，GCN-DQN模型相较于LDP算法，平均SINR分别提升了179.6%、197.4%和175.2%。此外，GCN-DQN模型相较于之前基于CNN的方法，在相同配置下平均SINR分别提升了31.5%、53.0%和84.7%。", "conclusion": "这些结果强调了GCN-DQN模型在解决复杂URLLC要求方面，以最小的开销和卓越的网络性能所展现的有效性。", "translation": "确保包级通信质量对于大规模工业无线网络中的超可靠、低延迟通信（URLLC）至关重要。我们通过引入一个集成深度Q网络（DQN）强化学习框架的图卷积网络（GCN）来增强局部截止日期划分（LDP）算法，以改善多蜂窝、多信道网络中的干扰协调。与LDP的静态优先级不同，我们的方法根据实时流量需求、网络拓扑、剩余传输机会和干扰模式动态学习链路优先级。GCN捕获空间依赖性，而DQN通过奖励引导的探索实现自适应调度决策。仿真结果表明，我们的GCN-DQN模型在三种网络配置下，相较于LDP分别实现了179.6%、197.4%和175.2%的平均SINR提升。此外，GCN-DQN模型在相同配置下，相较于我们之前基于CNN的方法，平均SINR分别提升了31.5%、53.0%和84.7%。这些结果强调了我们的GCN-DQN模型在解决复杂URLLC要求方面，以最小的开销和卓越的网络性能所展现的有效性。", "summary": "本研究提出了一种结合图卷积网络（GCN）和深度Q网络（DQN）的强化学习框架，以增强局部截止日期划分（LDP）算法，用于工业URLLC中的干扰协调。该方法能够根据实时网络状况动态学习链路优先级，有效捕获空间依赖性并进行自适应调度。实验结果表明，GCN-DQN模型在平均SINR方面显著优于LDP算法和先前的CNN方法，证明了其在满足URLLC严格要求方面的有效性和高性能。", "keywords": "URLLC, 强化学习, GCN, DQN, 干扰协调", "comments": "该论文的创新点在于将GCN与DQN结合，用于工业URLLC中的干扰协调，并动态学习链路优先级，而非依赖静态设定。GCN的应用有效地捕获了复杂的空间依赖性，而DQN则实现了自适应决策。这种结合显著提升了网络性能，特别是在SINR方面，对于需要高可靠性和低延迟的工业应用具有重要意义。该方法通过智能调度优化了资源利用，降低了开销，为未来URLLC网络设计提供了有价值的思路。"}}
{"id": "2506.15052", "title": "MIMO Systems Aided by Microwave Linear Analog Computers: Capacity-Achieving Architectures with Reduced Circuit Complexity", "authors": ["Matteo Nerini", "Bruno Clerckx"], "summary": "To meet the demands of future wireless networks, antenna arrays must scale\nfrom massive multiple-input multiple-output (MIMO) to gigantic MIMO, involving\neven larger numbers of antennas. To address the hardware and computational cost\nof gigantic MIMO, several strategies are available that shift processing from\nthe digital to the analog domain. Among them, microwave linear analog computers\n(MiLACs) offer a compelling solution by enabling fully analog beamforming\nthrough reconfigurable microwave networks. Prior work has focused on\nfully-connected MiLACs, whose ports are all interconnected to each other via\ntunable impedance components. Although such MiLACs are capacity-achieving,\ntheir circuit complexity, given by the number of required impedance components,\nscales quadratically with the number of antennas, limiting their practicality.\nTo solve this issue, in this paper, we propose a graph theoretical model of\nMiLAC facilitating the systematic design of lower-complexity MiLAC\narchitectures. Leveraging this model, we propose stem-connected MiLACs as a\nfamily of MiLAC architectures maintaining capacity-achieving performance while\ndrastically reducing the circuit complexity. Besides, we optimize\nstem-connected MiLACs with a closed-form capacity-achieving solution. Our\ntheoretical analysis, confirmed by numerical simulations, shows that\nstem-connected MiLACs are capacity-achieving, but with circuit complexity that\nscales linearly with the number of antennas, enabling high-performance,\nscalable, gigantic MIMO.", "comment": "Submitted to IEEE for publication", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15052v1", "AI": {"title_translation": "微波线性模拟计算机辅助的MIMO系统：具有降低电路复杂度的容量实现架构", "tldr": "本文提出了一种名为“干连接MiLAC”的新型MIMO架构，它在实现容量的同时，将电路复杂度从平方级降低到线性级，从而实现可扩展的巨型MIMO。", "motivation": "为了应对未来无线网络中巨型MIMO（G-MIMO）系统面临的硬件和计算成本问题，现有全连接MiLAC虽然能实现容量，但其电路复杂度与天线数量呈平方关系，限制了其实用性。", "method": "本文提出了一个MiLAC的图论模型，以系统性地设计低复杂度MiLAC架构。基于此模型，提出了“干连接MiLAC”架构家族，并对其进行了优化，得到了闭式容量实现解决方案。", "result": "理论分析和数值模拟证实，“干连接MiLAC”能够实现容量，但其电路复杂度与天线数量呈线性关系。", "conclusion": "“干连接MiLAC”架构能够实现高性能、可扩展的巨型MIMO系统，因为它在实现容量的同时显著降低了电路复杂度。", "translation": "为了满足未来无线网络的需求，天线阵列必须从大规模多输入多输出（MIMO）扩展到巨型MIMO，涉及更多的天线数量。为了解决巨型MIMO的硬件和计算成本问题，有几种可用的策略将处理从数字域转移到模拟域。其中，微波线性模拟计算机（MiLACs）通过可重构微波网络实现全模拟波束成形，提供了一个引人注目的解决方案。先前的工作主要集中于全连接MiLACs，其所有端口通过可调阻抗组件相互连接。尽管此类MiLACs可以实现容量，但其电路复杂度（由所需阻抗组件的数量决定）与天线数量呈平方关系，限制了其实用性。为了解决这个问题，在本文中，我们提出了一个MiLAC的图论模型，以促进低复杂度MiLAC架构的系统设计。利用该模型，我们提出了干连接MiLACs作为一类MiLAC架构，它在保持容量实现性能的同时，显著降低了电路复杂度。此外，我们通过一个闭式容量实现解决方案优化了干连接MiLACs。我们的理论分析和数值模拟证实，干连接MiLACs能够实现容量，但其电路复杂度与天线数量呈线性关系，从而实现高性能、可扩展的巨型MIMO。", "summary": "本文针对巨型MIMO系统的高硬件和计算成本问题，特别是现有全连接微波线性模拟计算机（MiLAC）的电路复杂度随天线数量呈平方增长的局限性，提出了一种基于图论模型的新型“干连接MiLAC”架构。该架构在保持容量实现性能的同时，将电路复杂度降低到与天线数量呈线性关系，并通过闭式解决方案进行优化。理论分析和数值模拟验证了其在实现高性能、可扩展巨型MIMO方面的潜力。", "keywords": "巨型MIMO, 微波线性模拟计算机, 电路复杂度, 容量实现, 干连接MiLAC", "comments": "本文的创新点在于提出了“干连接MiLAC”这一新型架构和相应的图论模型，有效解决了巨型MIMO系统中模拟波束成形器电路复杂度过高的问题。通过将复杂度从平方级降低到线性级，极大地提升了MiLAC在未来大规模MIMO系统中的实用性和可扩展性，对于推动模拟域信号处理在无线通信中的应用具有重要意义。"}}
{"id": "2506.14805", "title": "Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?", "authors": ["Yang Yao", "Lingyu Li", "Jiaxin Song", "Chiyu Chen", "Zhenqi He", "Yixu Wang", "Xin Wang", "Tianle Gu", "Jie Li", "Yan Teng", "Yingchun Wang"], "summary": "As Multimodal Large Language Models (MLLMs) continue to evolve, their\ncognitive and reasoning capabilities have seen remarkable progress. However,\nchallenges in visual fine-grained perception and commonsense causal inference\npersist. This paper introduces Argus Inspection, a multimodal benchmark with\ntwo levels of difficulty, emphasizing detailed visual recognition while\nincorporating real-world commonsense understanding to evaluate causal reasoning\nabilities. Expanding on it, we present the Eye of Panoptes framework, which\nintegrates a binary parametric Sigmoid metric with an indicator function,\nenabling a more holistic evaluation of MLLMs' responses in opinion-based\nreasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the\nhighest performance in visual fine-grained reasoning reaches only 0.46,\nhighlighting considerable potential for enhancement. Our research offers\nvaluable perspectives for the continued refinement of MLLMs.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14805v1", "AI": {"title_translation": "Argus 检查：多模态大型语言模型是否拥有潘诺普忒斯之眼？", "tldr": "多模态大型语言模型（MLLM）在视觉细粒度感知和常识因果推理方面仍面临挑战。本文引入了Argus Inspection基准和Eye of Panoptes评估框架，发现当前主流MLLM在此类任务上表现不佳，表明有很大的改进空间。", "motivation": "尽管多模态大型语言模型（MLLM）在认知和推理能力方面取得了显著进展，但在视觉细粒度感知和常识因果推理方面仍然存在挑战。本研究旨在评估MLLM在这些方面的能力。", "method": "本文引入了Argus Inspection，一个具有两个难度级别的多模态基准，旨在评估详细的视觉识别和结合现实世界常识的因果推理能力。在此基础上，还提出了Eye of Panoptes框架，该框架整合了一个二元参数Sigmoid度量与一个指示函数，以更全面地评估MLLM在基于意见的推理任务中的响应。", "result": "对26个主流多模态大型语言模型（MLLM）进行的实验表明，在视觉细粒度推理方面的最高性能仅达到0.46，这突出显示了巨大的提升潜力。", "conclusion": "本研究为多模态大型语言模型（MLLM）的持续改进提供了宝贵的视角。", "translation": "随着多模态大型语言模型（MLLM）的不断发展，它们的认知和推理能力取得了显著进步。然而，视觉细粒度感知和常识因果推理方面的挑战依然存在。本文介绍了Argus Inspection，一个具有两个难度级别的多模态基准，强调详细的视觉识别，同时结合现实世界常识理解来评估因果推理能力。在此基础上，我们提出了Eye of Panoptes框架，该框架整合了一个二元参数Sigmoid度量与一个指示函数，从而能够更全面地评估MLLM在基于意见的推理任务中的响应。对26个主流MLLM进行的实验表明，视觉细粒度推理的最高性能仅达到0.46，这突出显示了巨大的提升潜力。我们的研究为MLLM的持续完善提供了宝贵的视角。", "summary": "本论文介绍了Argus Inspection，一个用于评估多模态大型语言模型（MLLM）视觉细粒度感知和常识因果推理能力的新型多模态基准。同时，还提出了Eye of Panoptes框架，结合创新的评估指标，以更全面地衡量MLLM在基于意见的推理任务中的表现。通过对26个主流MLLM的实验，研究发现它们在视觉细粒度推理方面的表现仍有显著提升空间，最高性能仅为0.46。这项工作为MLLM的未来发展和改进提供了重要的见解。", "keywords": "多模态大型语言模型, 基准测试, 视觉感知, 因果推理", "comments": "这篇论文通过引入新的基准（Argus Inspection）和评估框架（Eye of Panoptes），有效地解决了多模态大型语言模型（MLLM）在视觉细粒度感知和常识因果推理方面的评估难题。其创新之处在于结合了不同难度级别的视觉识别和现实世界常识理解，并提出了更全面的评估指标。实验结果清晰地揭示了当前主流MLLM的局限性，为未来MLLM的研究和优化指明了方向，具有重要的实践指导意义。"}}
{"id": "2506.14990", "title": "MEAL: A Benchmark for Continual Multi-Agent Reinforcement Learning", "authors": ["Tristan Tomilin", "Luka van den Boogaard", "Samuel Garcin", "Bram Grooten", "Meng Fang", "Mykola Pechenizkiy"], "summary": "Benchmarks play a crucial role in the development and analysis of\nreinforcement learning (RL) algorithms, with environment availability strongly\nimpacting research. One particularly underexplored intersection is continual\nlearning (CL) in cooperative multi-agent settings. To remedy this, we introduce\nMEAL (Multi-agent Environments for Adaptive Learning), the first benchmark\ntailored for continual multi-agent reinforcement learning (CMARL). Existing CL\nbenchmarks run environments on the CPU, leading to computational bottlenecks\nand limiting the length of task sequences. MEAL leverages JAX for GPU\nacceleration, enabling continual learning across sequences of 100 tasks on a\nstandard desktop PC in a few hours. We show that naively combining popular CL\nand MARL methods yields strong performance on simple environments, but fails to\nscale to more complex settings requiring sustained coordination and adaptation.\nOur ablation study identifies architectural and algorithmic features critical\nfor CMARL on MEAL.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.14990v1", "AI": {"title_translation": "MEAL：一个持续多智能体强化学习的基准", "tldr": "引入了MEAL，首个专为持续多智能体强化学习设计的基准，通过JAX实现GPU加速，揭示了现有方法在复杂持续多智能体任务中的局限性。", "motivation": "现有强化学习基准缺乏对合作多智能体持续学习的探索，导致该领域研究受限。", "method": "引入了MEAL（Multi-agent Environments for Adaptive Learning），这是第一个专为持续多智能体强化学习（CMARL）设计的基准。MEAL利用JAX进行GPU加速，克服了现有CPU基准的计算瓶颈，使得在标准PC上几小时内就能在100个任务序列上进行持续学习。", "result": "研究表明，简单地结合流行的持续学习和多智能体强化学习方法在简单环境中表现良好，但在需要持续协调和适应的复杂环境中表现不佳。消融研究确定了对MEAL上CMARL至关重要的架构和算法特征。", "conclusion": "MEAL基准揭示了当前持续多智能体强化学习方法在复杂任务中的不足，并为未来研究指明了方向，强调了特定架构和算法特征的重要性。", "translation": "基准在强化学习（RL）算法的开发和分析中发挥着关键作用，环境的可用性强烈影响着研究。一个特别未被充分探索的交叉点是合作多智能体设置中的持续学习（CL）。为了弥补这一点，我们引入了MEAL（多智能体自适应学习环境），这是第一个专为持续多智能体强化学习（CMARL）量身定制的基准。现有的CL基准在CPU上运行环境，导致计算瓶颈并限制了任务序列的长度。MEAL利用JAX进行GPU加速，使得在标准台式电脑上几小时内就能在100个任务序列上进行持续学习。我们展示了简单地结合流行的CL和MARL方法在简单环境中表现出强大的性能，但无法扩展到需要持续协调和适应的更复杂设置。我们的消融研究确定了对MEAL上CMARL至关重要的架构和算法特征。", "summary": "本文介绍了MEAL，一个专门为持续多智能体强化学习（CMARL）设计的基准。为了解决现有基准在计算效率上的限制，MEAL利用JAX实现GPU加速，使得研究人员能够在更长的任务序列上进行高效的持续学习实验。通过MEAL，作者发现简单结合现有持续学习和多智能体强化学习方法在复杂合作任务中表现不佳，并识别出对CMARL至关重要的架构和算法特性。", "keywords": "持续多智能体强化学习, 基准, JAX, GPU加速, 持续学习", "comments": "MEAL基准的创新之处在于其首次专门针对持续多智能体强化学习领域，并通过利用JAX实现GPU加速，显著提升了实验效率，解决了现有CPU基准的计算瓶颈。这对于推动CMARL领域的研究至关重要，因为它允许研究人员探索更长的任务序列和更复杂的场景。该工作还揭示了当前方法的局限性，为未来算法设计提供了宝贵的见解。"}}
{"id": "2506.15263", "title": "Minimizing Structural Vibrations via Guided Flow Matching Design Optimization", "authors": ["Jan van Delden", "Julius Schultz", "Sebastian Rothe", "Christian Libner", "Sabine C. Langer", "Timo Lüddecke"], "summary": "Structural vibrations are a source of unwanted noise in engineering systems\nlike cars, trains or airplanes. Minimizing these vibrations is crucial for\nimproving passenger comfort. This work presents a novel design optimization\napproach based on guided flow matching for reducing vibrations by placing\nbeadings (indentations) in plate-like structures. Our method integrates a\ngenerative flow matching model and a surrogate model trained to predict\nstructural vibrations. During the generation process, the flow matching model\npushes towards manufacturability while the surrogate model pushes to\nlow-vibration solutions. The flow matching model and its training data\nimplicitly define the design space, enabling a broader exploration of potential\nsolutions as no optimization of manually-defined design parameters is required.\nWe apply our method to a range of differentiable optimization objectives,\nincluding direct optimization of specific eigenfrequencies through careful\nconstruction of the objective function. Results demonstrate that our method\ngenerates diverse and manufacturable plate designs with reduced structural\nvibrations compared to designs from random search, a criterion-based design\nheuristic and genetic optimization. The code and data are available from\nhttps://github.com/ecker-lab/Optimizing_Vibrating_Plates.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.15263v1", "AI": {"title_translation": "通过引导流匹配设计优化最小化结构振动", "tldr": "本文提出了一种基于引导流匹配的新型设计优化方法，用于通过在板状结构中放置压痕来减少结构振动，生成了多样化且可制造的低振动设计。", "motivation": "结构振动是汽车、火车或飞机等工程系统中不必要的噪音源。最小化这些振动对于提高乘客舒适度至关重要。", "method": "本文提出了一种基于引导流匹配的新型设计优化方法，用于通过在板状结构中放置压痕（压纹）来减少振动。该方法整合了一个生成流匹配模型和一个预测结构振动的代理模型。在生成过程中，流匹配模型趋向于可制造性，而代理模型趋向于低振动解决方案。流匹配模型及其训练数据隐式定义了设计空间，无需优化手动定义的设计参数，从而实现更广泛的潜在解决方案探索。", "result": "结果表明，与随机搜索、基于准则的设计启发式方法和遗传优化相比，该方法生成了多样化且可制造的板设计，并显著降低了结构振动。该方法还应用于一系列可微分优化目标，包括通过仔细构建目标函数直接优化特定特征频率。", "conclusion": "通过引导流匹配设计优化，可以有效地生成多样化、可制造且具有显著降低结构振动的板状结构设计，从而改善工程系统的乘客舒适度。", "translation": "结构振动是汽车、火车或飞机等工程系统中不必要的噪音源。最小化这些振动对于提高乘客舒适度至关重要。本文提出了一种基于引导流匹配的新型设计优化方法，用于通过在板状结构中放置压痕（压纹）来减少振动。我们的方法整合了一个生成流匹配模型和一个预测结构振动的代理模型。在生成过程中，流匹配模型趋向于可制造性，而代理模型趋向于低振动解决方案。流匹配模型及其训练数据隐式定义了设计空间，无需优化手动定义的设计参数，从而实现更广泛的潜在解决方案探索。我们将我们的方法应用于一系列可微分优化目标，包括通过仔细构建目标函数直接优化特定特征频率。结果表明，与随机搜索、基于准则的设计启发式方法和遗传优化相比，我们的方法生成了多样化且可制造的板设计，并显著降低了结构振动。代码和数据可在 https://github.com/ecker-lab/Optimizing_Vibrating_Plates 获取。", "summary": "本文介绍了一种基于引导流匹配的新型设计优化方法，旨在通过在板状结构中引入压痕来最小化结构振动。该方法结合了生成流匹配模型和预测振动的代理模型，以在确保可制造性的同时实现低振动设计。通过隐式定义设计空间，该方法能够进行广泛的探索，并已成功应用于优化特定特征频率。实验结果表明，与现有方法相比，该方法能够生成多样化、可制造且振动显著降低的板设计。", "keywords": "结构振动, 流匹配, 设计优化, 压痕, 代理模型", "comments": "该论文的创新之处在于将引导流匹配模型与代理模型相结合，用于结构振动优化，同时兼顾了可制造性。其重要性体现在无需手动定义设计参数即可探索广泛的设计空间，并成功生成了优于传统方法的低振动设计，对于提高工程系统中的乘客舒适度具有实际应用价值。"}}
{"id": "2506.15543", "title": "Learning Algorithms in the Limit", "authors": ["Hristo Papazov", "Nicolas Flammarion"], "summary": "This paper studies the problem of learning computable functions in the limit\nby extending Gold's inductive inference framework to incorporate\n\\textit{computational observations} and \\textit{restricted input sources}.\nComplimentary to the traditional Input-Output Observations, we introduce\nTime-Bound Observations, and Policy-Trajectory Observations to study the\nlearnability of general recursive functions under more realistic constraints.\nWhile input-output observations do not suffice for learning the class of\ngeneral recursive functions in the limit, we overcome this learning barrier by\nimposing computational complexity constraints or supplementing with approximate\ntime-bound observations. Further, we build a formal framework around\nobservations of \\textit{computational agents} and show that learning computable\nfunctions from policy trajectories reduces to learning rational functions from\ninput and output, thereby revealing interesting connections to finite-state\ntransducer inference. On the negative side, we show that computable or\npolynomial-mass characteristic sets cannot exist for the class of linear-time\ncomputable functions even for policy-trajectory observations.", "comment": "Accepted at COLT 2025. This version matches the proceedings version", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15543v1", "AI": {"title_translation": "极限学习算法", "tldr": "本文通过引入计算观测和受限输入源，扩展了Gold的归纳推理框架，研究了在更现实约束下可计算函数的学习能力，并揭示了正反两方面结果。", "motivation": "该研究旨在通过扩展Gold的归纳推理框架，并引入计算观测和受限输入源，来研究在更现实约束下可计算函数的学习问题。传统的输入-输出观测不足以学习通用递归函数，因此需要新的观测类型（如时间界限观测和策略轨迹观测）来克服这一学习障碍。", "method": "该研究通过以下方法进行：1. 扩展Gold的归纳推理框架，纳入计算观测和受限输入源。2. 引入时间界限观测（Time-Bound Observations）和策略轨迹观测（Policy-Trajectory Observations），以研究在更现实约束下通用递归函数的学习能力。3. 通过施加计算复杂度约束或补充近似时间界限观测来克服输入-输出观测不足以学习通用递归函数的限制。4. 围绕计算代理的观测构建一个形式框架。", "result": "研究结果包括：1. 尽管输入-输出观测不足以极限学习通用递归函数类，但通过施加计算复杂度约束或补充近似时间界限观测可以克服这一学习障碍。2. 从策略轨迹中学习可计算函数可以简化为从输入和输出中学习有理函数，这揭示了与有限状态传感器推理的有趣联系。3. 负面结果是，即使对于策略轨迹观测，线性时间可计算函数类也不存在可计算或多项式质量的特征集。", "conclusion": "本文通过引入时间界限观测和策略轨迹观测等新的计算观测类型，扩展了Gold的归纳推理框架，以研究在更现实约束下可计算函数的学习能力。研究表明，通过新的观测类型可以克服传统输入-输出观测的局限性，实现了通用递归函数的学习，并揭示了与有限状态传感器推理的联系，但也发现了某些函数类在特定观测下学习的限制。", "translation": "本文通过将Gold的归纳推理框架扩展到包含计算观测和受限输入源，研究了极限学习可计算函数的问题。作为传统输入-输出观测的补充，我们引入了时间界限观测和策略轨迹观测，以在更现实的约束下研究通用递归函数的学习能力。虽然输入-输出观测不足以极限学习通用递归函数类，但我们通过施加计算复杂度约束或补充近似时间界限观测来克服了这一学习障碍。此外，我们围绕计算代理的观测构建了一个形式框架，并表明从策略轨迹中学习可计算函数可以简化为从输入和输出中学习有理函数，从而揭示了与有限状态传感器推理的有趣联系。从负面来看，我们表明即使对于策略轨迹观测，线性时间可计算函数类也不存在可计算或多项式质量的特征集。", "summary": "本论文通过扩展Gold的归纳推理框架，并引入时间界限观测和策略轨迹观测等新型计算观测，研究了在更现实约束下可计算函数的极限学习问题。研究发现，这些新观测方式能克服传统输入-输出观测的局限，使通用递归函数得以学习，并揭示了与有限状态传感器推理的关联。然而，论文也指出，对于线性时间可计算函数，即使在策略轨迹观测下，也无法存在可计算或多项式质量的特征集。", "keywords": "极限学习, 归纳推理, 计算观测, 递归函数, 学习能力", "comments": "该论文通过引入时间界限观测和策略轨迹观测等新颖的计算观测类型，创新性地扩展了Gold的归纳推理框架，使得在更现实的计算约束下学习可计算函数成为可能。这一方法突破了传统输入-输出观测的局限性，对理解复杂函数的可学习性具有重要意义。然而，论文也揭示了某些函数类在特定观测下学习的内在限制，特别是线性时间可计算函数不存在可计算或多项式质量的特征集，这为未来的研究指明了方向。"}}
{"id": "2506.14852", "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching", "authors": ["Qizheng Zhang", "Michael Wornow", "Kunle Olukotun"], "summary": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures.", "comment": "23 pages", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.14852v1", "AI": {"title_translation": "通过测试时计划缓存实现LLM代理的成本效益服务", "tldr": "提出了一种名为“代理计划缓存”的新方法，通过提取、存储、适应和重用结构化计划模板来降低LLM代理服务的成本，平均可降低46.62%的成本。", "motivation": "基于LLM的代理应用程序由于广泛的规划和推理需求而产生高昂成本。现有LLM缓存技术（如上下文缓存和语义缓存）主要为聊天机器人设计，不足以满足代理应用程序的需求，因为它们的输出依赖于外部数据或环境上下文。", "method": "提出了一种新颖的“代理计划缓存”方法，该方法从代理应用程序的规划阶段提取、存储、适应和重用结构化计划模板，以跨语义相似任务降低服务成本。与传统语义缓存不同，该系统在测试时从已完成的代理执行中提取计划模板，使用关键词提取匹配新请求与缓存计划，并利用轻量级模型将这些模板与上下文适应为特定任务的计划。", "result": "在多个真实世界代理应用程序上的评估显示，该系统平均可将成本降低46.62%，同时保持性能。", "conclusion": "代理计划缓存为LLM代理服务提供了一种更高效的解决方案，显著降低了成本并保持了性能，补充了现有的LLM服务基础设施。", "translation": "基于LLM的代理应用程序在复杂工作流程中展现出越来越卓越的能力，但由于广泛的规划和推理需求而产生高昂成本。现有的LLM缓存技术（如上下文缓存和语义缓存）主要为聊天机器人服务设计，不足以满足代理应用程序的需求，因为它们的输出依赖于外部数据或环境上下文。我们提出了一种新颖的方法——代理计划缓存，它从代理应用程序规划阶段中提取、存储、适应和重用结构化计划模板，以跨语义相似任务降低服务成本。与传统的语义缓存不同，我们的系统在测试时从已完成的代理执行中提取计划模板，采用关键词提取来将新请求与缓存计划匹配，并利用轻量级模型将这些模板与上下文适应为特定任务的计划。对多个真实世界代理应用程序的评估表明，我们的系统平均可将成本降低46.62%，同时保持性能，为LLM代理服务提供了一种更高效的解决方案，补充了现有的LLM服务基础设施。", "summary": "本文旨在解决LLM代理应用程序因大量规划而产生的高昂成本问题。研究提出了一种名为“代理计划缓存”的新颖方法，该方法在测试时从代理执行中提取、存储、适应和重用结构化计划模板。该系统利用关键词匹配和轻量级模型来适应新请求的计划。评估结果显示，该方法平均可降低46.62%的成本，同时保持性能，为现有LLM服务基础设施提供了一个高效的补充方案。", "keywords": "LLM代理, 计划缓存, 成本效益, 代理应用程序, 测试时缓存", "comments": "这项研究的创新之处在于，它专门为LLM代理设计了缓存机制，通过缓存结构化计划而非简单的上下文或语义嵌入，解决了代理工作流的独特挑战。测试时提取和利用轻量级模型进行适应是其高效和实用性的关键。其重要性在于使LLM代理在经济上更具可行性，从而促进更广泛的部署。"}}
{"id": "2506.14922", "title": "FORTRESS: Frontier Risk Evaluation for National Security and Public Safety", "authors": ["Christina Q. Knight", "Kaustubh Deshpande", "Ved Sirdeshmukh", "Meher Mankikar", "Scale Red Team", "SEAL Research Team", "Julian Michael"], "summary": "The rapid advancement of large language models (LLMs) introduces dual-use\ncapabilities that could both threaten and bolster national security and public\nsafety (NSPS). Models implement safeguards to protect against potential misuse\nrelevant to NSPS and allow for benign users to receive helpful information.\nHowever, current benchmarks often fail to test safeguard robustness to\npotential NSPS risks in an objective, robust way. We introduce FORTRESS: 500\nexpert-crafted adversarial prompts with instance-based rubrics of 4-7 binary\nquestions for automated evaluation across 3 domains (unclassified information\nonly): Chemical, Biological, Radiological, Nuclear and Explosive (CBRNE),\nPolitical Violence & Terrorism, and Criminal & Financial Illicit Activities,\nwith 10 total subcategories across these domains. Each prompt-rubric pair has a\ncorresponding benign version to test for model over-refusals. This evaluation\nof frontier LLMs' safeguard robustness reveals varying trade-offs between\npotential risks and model usefulness: Claude-3.5-Sonnet demonstrates a low\naverage risk score (ARS) (14.09 out of 100) but the highest over-refusal score\n(ORS) (21.8 out of 100), while Gemini 2.5 Pro shows low over-refusal (1.4) but\na high average potential risk (66.29). Deepseek-R1 has the highest ARS at\n78.05, but the lowest ORS at only 0.06. Models such as o1 display a more even\ntrade-off between potential risks and over-refusals (with an ARS of 21.69 and\nORS of 5.2). To provide policymakers and researchers with a clear understanding\nof models' potential risks, we publicly release FORTRESS at\nhttps://huggingface.co/datasets/ScaleAI/fortress_public. We also maintain a\nprivate set for evaluation.", "comment": "12 pages, 7 figures, submitted to NeurIPS", "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.14922v1", "AI": {"title_translation": "堡垒：国家安全和公共安全前沿风险评估", "tldr": "引入FORTRESS基准，用于评估大型语言模型（LLM）在国家安全和公共安全方面的潜在风险和过度拒绝。", "motivation": "大型语言模型（LLMs）的快速发展带来了可能威胁或增强国家安全和公共安全（NSPS）的双重用途能力。现有基准未能客观、稳健地测试LLM安全措施对潜在NSPS风险的鲁棒性。", "method": "引入FORTRESS，一个包含500个专家制作的对抗性提示的基准，并附带4-7个二元问题的实例评估标准，用于在3个领域（CBRNE、政治暴力与恐怖主义、犯罪与金融非法活动）的10个子类别中进行自动化评估。每个对抗性提示-评估对都有一个对应的良性版本，以测试模型的过度拒绝。", "result": "对前沿LLM安全措施鲁棒性的评估显示，潜在风险和模型有用性之间存在不同的权衡：Claude-3.5-Sonnet平均风险得分低但过度拒绝得分最高；Gemini 2.5 Pro过度拒绝低但平均潜在风险高；Deepseek-R1平均风险得分最高但过度拒绝得分最低；o1等模型在潜在风险和过度拒绝之间表现出更均衡的权衡。FORTRESS已公开。", "conclusion": "评估揭示了前沿LLM在潜在风险和模型有用性之间存在不同的安全措施权衡。FORTRESS基准旨在为决策者和研究人员提供对模型潜在风险的清晰理解。", "translation": "大型语言模型（LLM）的快速发展带来了双重用途能力，既可能威胁也可能增强国家安全和公共安全（NSPS）。模型实施了安全措施，以防止与NSPS相关的潜在滥用，并允许良性用户接收有用的信息。然而，当前的基准往往无法客观、稳健地测试安全措施对潜在NSPS风险的鲁棒性。我们引入了FORTRESS：500个专家精心制作的对抗性提示，带有4-7个二元问题的基于实例的评估标准，用于在3个领域（仅限非机密信息）：化学、生物、放射性、核和爆炸物（CBRNE）、政治暴力与恐怖主义、犯罪与金融非法活动，以及这些领域内的10个子类别中进行自动化评估。每个提示-评估对都有一个相应的良性版本，用于测试模型的过度拒绝。对前沿LLM安全措施鲁棒性的评估揭示了潜在风险和模型有用性之间存在不同的权衡：Claude-3.5-Sonnet的平均风险得分（ARS）较低（100分中14.09分），但过度拒绝得分（ORS）最高（100分中21.8分），而Gemini 2.5 Pro的过度拒绝较低（1.4分），但平均潜在风险较高（66.29分）。Deepseek-R1的ARS最高，为78.05分，但ORS最低，仅为0.06分。像o1这样的模型在潜在风险和过度拒绝之间表现出更均衡的权衡（ARS为21.69分，ORS为5.2分）。为了让决策者和研究人员清晰了解模型的潜在风险，我们公开了FORTRESS，网址为https://huggingface.co/datasets/ScaleAI/fortress_public。我们还保留了一套私人数据集用于评估。", "summary": "本研究引入了FORTRESS，一个针对大型语言模型（LLM）国家安全和公共安全（NSPS）风险的评估基准。该基准包含500个专家制作的对抗性提示，涵盖CBRNE、政治暴力与恐怖主义、犯罪与金融非法活动三大领域，并设计了对应的良性版本以测试模型过度拒绝。评估结果揭示了不同LLM在潜在风险和模型有用性之间存在的权衡，例如Claude-3.5-Sonnet风险低但过度拒绝高，而Gemini 2.5 Pro过度拒绝低但风险高。FORTRESS已公开，旨在帮助决策者和研究人员理解LLM的潜在风险。", "keywords": "大型语言模型, 国家安全, 公共安全, 风险评估, 对抗性提示", "comments": "FORTRESS基准的创新性在于其专家制作的对抗性提示和实例评估标准，以及对过度拒绝的考虑，这比现有基准更能客观、稳健地评估LLM在NSPS领域的风险。其公开性对于促进LLM安全研究和负责任的AI开发具有重要意义。"}}
{"id": "2506.15026", "title": "Algorithmic Approaches to Enhance Safety in Autonomous Vehicles: Minimizing Lane Changes and Merging", "authors": ["Seyed Moein Abtahi", "Akramul Azim"], "summary": "The rapid advancements in autonomous vehicle (AV) technology promise enhanced\nsafety and operational efficiency. However, frequent lane changes and merging\nmaneuvers continue to pose significant safety risks and disrupt traffic flow.\nThis paper introduces the Minimizing Lane Change Algorithm (MLCA), a\nstate-machine-based approach designed to reduce unnecessary lane changes,\nthereby enhancing both traffic safety and efficiency. The MLCA algorithm\nprioritizes maintaining lane stability unless safety-critical conditions\nnecessitate a lane change. The algorithm's effectiveness was evaluated through\nsimulations conducted on the SUMO platform, comparing its performance against\nestablished models, including LC2017 and MOBIL. Results demonstrate substantial\nreductions in lane changes and collisions, leading to smoother traffic flow and\nimproved safety metrics. Additionally, the study highlights the MLCA's\nadaptability to various traffic densities and roadway configurations,\nshowcasing its potential for wide-scale deployment in real-world AV systems.\nFuture work aims to validate these findings in more complex scenarios using the\nCARLA simulator, which will enable the testing of the algorithm under more\ndynamic and high-fidelity conditions, such as urban traffic environments with\ndiverse road users. Moreover, the integration of cybersecurity measures for\nvehicle-to-vehicle (V2V) communication will be explored to ensure robust and\nsecure data exchange, further enhancing the reliability and safety of AV\noperations. This research contributes to the broader goal of developing\nintelligent traffic systems that optimize both individual vehicle performance\nand overall traffic network efficiency.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15026v1", "AI": {"title_translation": "增强自动驾驶汽车安全性的算法方法：最小化变道与合流", "tldr": "本文提出了最小化变道算法 (MLCA)，旨在减少自动驾驶汽车不必要的变道，从而提高交通安全和效率，并通过仿真进行了验证。", "motivation": "尽管自动驾驶汽车技术取得了快速发展，但频繁的变道和合流操作仍会带来显著的安全风险并扰乱交通流。", "method": "本文引入了最小化变道算法 (MLCA)，这是一种基于状态机的方法，旨在优先保持车道稳定性，除非安全关键条件需要变道。该算法的有效性通过在 SUMO 平台上进行的仿真进行评估，并将其性能与包括 LC2017 和 MOBIL 在内的现有模型进行比较。", "result": "仿真结果表明，MLCA 显著减少了变道和碰撞，从而使交通流更平稳，安全指标得到改善。此外，研究还强调了 MLCA 对各种交通密度和道路配置的适应性。", "conclusion": "MLCA 算法通过最小化自动驾驶汽车不必要的变道，有效提高了交通安全和效率。它具有大规模部署的潜力，并有助于开发智能交通系统。未来的工作包括在更复杂的场景中进行验证以及集成网络安全措施。", "translation": "自动驾驶汽车 (AV) 技术的快速发展有望提高安全性和运营效率。然而，频繁的变道和合流操作仍然带来显著的安全风险并扰乱交通流。本文引入了最小化变道算法 (MLCA)，这是一种基于状态机的方法，旨在减少不必要的变道，从而提高交通安全和效率。MLCA 算法优先保持车道稳定性，除非安全关键条件需要变道。该算法的有效性通过在 SUMO 平台上进行的仿真进行评估，并将其性能与包括 LC2017 和 MOBIL 在内的现有模型进行比较。结果表明，变道和碰撞显著减少，从而使交通流更平稳，安全指标得到改善。此外，研究强调了 MLCA 对各种交通密度和道路配置的适应性，展示了其在现实世界 AV 系统中大规模部署的潜力。未来的工作旨在利用 CARLA 模拟器在更复杂的场景中验证这些发现，这将能够在更动态和高保真条件下测试算法，例如具有多样化道路使用者的城市交通环境。此外，将探索车辆到车辆 (V2V) 通信的网络安全措施的集成，以确保强大和安全的数据交换，进一步提高 AV 运行的可靠性和安全性。这项研究有助于实现开发智能交通系统的更广泛目标，该系统可优化单个车辆性能和整体交通网络效率。", "summary": "本文针对自动驾驶汽车频繁变道和合流带来的安全风险及交通中断问题，提出了一种基于状态机的最小化变道算法 (MLCA)。该算法优先保持车道稳定性，并经 SUMO 平台仿真验证，与现有模型（如 LC2017 和 MOBIL）相比，显著减少了变道和碰撞，从而改善了交通流并提高了安全指标。MLCA 对不同交通条件和道路配置的良好适应性，预示其在实际自动驾驶系统中的广泛应用潜力，有助于构建更安全高效的智能交通系统。", "keywords": "自动驾驶汽车, 变道算法, 交通安全, SUMO 仿真, MLCA", "comments": "该论文为自动驾驶中不必要的变道这一关键安全和效率问题提供了一种实用的算法解决方案（MLCA）。其基于状态机的方法直观易懂，并且在 SUMO 上与现有模型进行的仿真结果为其有效性提供了良好的基线。文中提到对各种交通密度和未来使用 CARLA 及 V2V 网络安全措施的适应性，突显了对实际部署和鲁棒性的全面考虑，使其成为智能交通系统的重要贡献。"}}
{"id": "2506.14985", "title": "Metasurfaces-Integrated Doubly-Dispersive MIMO: Channel Modeling and Optimization", "authors": ["Kuranage Roche Rayan Ranasinghe", "Hyeon Seok Rou", "Iván Alexander Morales Sandoval", "Giuseppe Thadeu Freitas de Abreu", "George C. Alexandropoulos"], "summary": "The doubly-dispersive (DD) channel structure has played a pivotal role in\nwireless communications, particularly in high-mobility scenarios and integrated\nsensing and communications (ISAC), due to its ability to capture the key fading\neffects experienced by a transmitted signal as it propagates through a dynamic\nmedium. However, extending the DD framework to multiple-input multiple-output\n(MIMO) systems, especially in environments artificially enhanced by\nreconfigurable intelligent surfaces (RISs) and stacked intelligent metasurfaces\n(SIM), remains a challenging open problem. In this chapter, a novel\nmetasurfaces-parametrized DD (MPDD) channel model that integrates an arbitrary\nnumber of RISs, while also incorporating SIM at both the transmitter and\nreceiver is introduced. Next, the application of this model to some key\nwaveforms optimized for DD environments -- namely orthogonal frequency division\nmultiplexing (OFDM), orthogonal time frequency space (OTFS), and affine\nfrequency division multiplexing (AFDM) -- is discussed. Finally, the\nprogrammability of the proposed model is highlighted through an illustrative\napplication, demonstrating its potential for enhancing waveform performance in\nSIM-assisted wireless systems.", "comment": "Author's version of chapter from forthcoming book \"Reconfigurable\n  Metasurfaces for Wireless Communications: Architectures, Modeling, and\n  Optimization\"", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.14985v1", "AI": {"title_translation": "超表面集成双色散MIMO：信道建模与优化", "tldr": "本文介绍了一种新颖的超表面参数化双色散（MPDD）信道模型，用于集成可重构智能表面（RIS）和堆叠智能超表面（SIM）的多输入多输出（MIMO）系统，并探讨了其在优化波形方面的应用潜力。", "motivation": "将双色散（DD）信道框架扩展到多输入多输出（MIMO）系统，特别是在可重构智能表面（RISs）和堆叠智能超表面（SIM）人工增强的环境中，仍然是一个具有挑战性的开放问题。而DD信道结构在高移动性场景和集成感知与通信（ISAC）中扮演着关键角色。", "method": "提出了一种新颖的超表面参数化双色散（MPDD）信道模型，该模型集成了任意数量的RIS，并在发射器和接收器端都包含了SIM。探讨了该模型在正交频分复用（OFDM）、正交时频空间（OTFS）和仿射频分复用（AFDM）等为DD环境优化的关键波形上的应用。通过一个说明性应用展示了模型的可编程性。", "result": "提出了一个能够集成RIS并包含SIM的MPDD信道模型。该模型适用于正交频分复用（OFDM）、正交时频空间（OTFS）和仿射频分复用（AFDM）等DD优化波形。模型的可编程性被突出，并展示了其在SIM辅助无线系统中增强波形性能的潜力。", "conclusion": "本文引入了一种新颖的超表面参数化双色散（MPDD）信道模型，解决了将双色散框架扩展到集成超表面的MIMO系统中的挑战，并展示了其可编程性以及在增强SIM辅助无线系统波形性能方面的潜力。", "translation": "双色散（DD）信道结构在无线通信中发挥了关键作用，特别是在高移动性场景和集成感知与通信（ISAC）中，因为它能够捕捉传输信号在动态介质中传播时所经历的关键衰落效应。然而，将DD框架扩展到多输入多输出（MIMO）系统，尤其是在可重构智能表面（RISs）和堆叠智能超表面（SIM）人工增强的环境中，仍然是一个具有挑战性的开放问题。本章介绍了一种新颖的超表面参数化DD（MPDD）信道模型，该模型集成了任意数量的RIS，同时在发射器和接收器端都包含了SIM。接下来，讨论了该模型在一些为DD环境优化的关键波形（即正交频分复用（OFDM）、正交时频空间（OTFS）和仿射频分复用（AFDM））上的应用。最后，通过一个说明性应用突出了所提出模型的可编程性，展示了其在SIM辅助无线系统中增强波形性能的潜力。", "summary": "本文旨在解决将双色散（DD）信道建模扩展到由可重构智能表面（RISs）和堆叠智能超表面（SIM）增强的多输入多输出（MIMO）系统所面临的挑战。为此，论文引入了一种新颖的超表面参数化DD（MPDD）信道模型，该模型能够集成RISs并包含SIM。文中讨论了该模型在多种DD优化波形（如OFDM、OTFS、AFDM）上的应用，并强调了其可编程性，展示了其在SIM辅助无线系统中提升波形性能的潜力。", "keywords": "超表面, 双色散MIMO, 信道建模, RIS, SIM", "comments": "该论文通过将超表面集成到双色散MIMO信道建模中，解决了先进无线通信领域的一个重要挑战。MPDD模型的引入具有创新性，为在动态和智能表面辅助环境中优化波形性能提供了一个框架，这对于未来的高移动性和ISAC应用至关重要。"}}
{"id": "2506.14781", "title": "Two-dimensional Parallel Tempering for Constrained Optimization", "authors": ["Corentin Delacour", "M Mahmudul Hasan Sajeeb", "Joao P. Hespanha", "Kerem Y. Camsari"], "summary": "Sampling Boltzmann probability distributions plays a key role in machine\nlearning and optimization, motivating the design of hardware accelerators such\nas Ising machines. While the Ising model can in principle encode arbitrary\noptimization problems, practical implementations are often hindered by soft\nconstraints that either slow down mixing when too strong, or fail to enforce\nfeasibility when too weak. We introduce a two-dimensional extension of the\npowerful parallel tempering algorithm (PT) that addresses this challenge by\nadding a second dimension of replicas interpolating the penalty strengths. This\nscheme ensures constraint satisfaction in the final replicas, analogous to\nlow-energy states at low temperature. The resulting two-dimensional parallel\ntempering algorithm (2D-PT) improves mixing in heavily constrained replicas and\neliminates the need to explicitly tune the penalty strength. In a\nrepresentative example of graph sparsification with copy constraints, 2D-PT\nachieves near-ideal mixing, with Kullback-Leibler divergence decaying as\nO(1/t). When applied to sparsified Wishart instances, 2D-PT yields orders of\nmagnitude speedup over conventional PT with the same number of replicas. The\nmethod applies broadly to constrained Ising problems and can be deployed on\nexisting Ising machines.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14781v1", "AI": {"title_translation": "用于约束优化的二维并行退火算法", "tldr": "本文提出二维并行退火算法（2D-PT），通过增加第二个副本维度来处理强约束优化问题，有效提高混合效率并消除罚函数强度调优需求，实现显著加速。", "motivation": "采样玻尔兹曼概率分布在机器学习和优化中至关重要，但伊辛机等硬件加速器在处理软约束问题时面临挑战：过强的约束会减慢混合，过弱则无法保证可行性。", "method": "引入了强大的并行退火算法（PT）的二维扩展（2D-PT），通过增加第二个副本维度来插值罚函数强度，从而解决上述挑战。", "result": "2D-PT显著改善了强约束副本中的混合效率，并消除了显式调整罚函数强度的需要。在图稀疏化等示例中，2D-PT实现了接近理想的混合（KL散度以O(1/t)衰减），并在稀疏化Wishart实例上比传统PT（相同副本数）实现了数量级的加速。", "conclusion": "该方法广泛适用于约束伊辛问题，并可部署在现有伊辛机上。", "translation": "采样玻尔兹曼概率分布在机器学习和优化中扮演着关键角色，这促使了伊辛机等硬件加速器的设计。尽管伊辛模型原则上可以编码任意优化问题，但实际实现常受软约束的阻碍，这些约束在过强时会减慢混合，过弱时则无法强制执行可行性。我们引入了一种强大的并行退火算法（PT）的二维扩展，通过增加第二个副本维度来插值罚函数强度，从而解决了这一挑战。该方案确保了最终副本中的约束满足，类似于低温下的低能量状态。由此产生的二维并行退火算法（2D-PT）改善了强约束副本中的混合效率，并消除了显式调整罚函数强度的需要。在一个具有复制约束的图稀疏化代表性示例中，2D-PT实现了接近理想的混合，库尔巴克-莱布勒散度以O(1/t)衰减。当应用于稀疏化Wishart实例时，2D-PT比相同副本数的传统PT实现了数量级的加速。该方法广泛适用于约束伊辛问题，并可部署在现有伊辛机上。", "summary": "本文针对机器学习和优化中玻尔兹曼概率分布采样面临的软约束挑战，提出了一种二维并行退火算法（2D-PT）。该算法通过引入第二个副本维度来插值罚函数强度，有效解决了强约束下混合效率低和罚函数强度调优困难的问题。实验证明，2D-PT显著提高了混合效率，并在特定约束优化问题上实现了数量级的加速，展现了其在处理约束伊辛问题上的普适性和部署潜力。", "keywords": "并行退火, 约束优化, 伊辛模型, 混合效率, 罚函数强度", "comments": "该论文的创新点在于将并行退火算法扩展至二维，巧妙地通过第二个副本维度解决了强约束优化中罚函数强度难以调优和混合效率低下的核心问题。这不仅提高了算法的实用性和性能，特别是对于伊辛机等硬件平台，而且为处理复杂约束优化问题提供了新的有效范式。"}}
{"id": "2506.15456", "title": "Factorized RVQ-GAN For Disentangled Speech Tokenization", "authors": ["Sameer Khurana", "Dominik Klement", "Antoine Laurent", "Dominik Bobos", "Juraj Novosad", "Peter Gazdik", "Ellen Zhang", "Zili Huang", "Amir Hussein", "Ricard Marxer", "Yoshiki Masuyama", "Ryo Aihara", "Chiori Hori", "Francois G. Germain", "Gordon Wichern", "Jonathan Le Roux"], "summary": "We propose Hierarchical Audio Codec (HAC), a unified neural speech codec that\nfactorizes its bottleneck into three linguistic levels-acoustic, phonetic, and\nlexical-within a single model. HAC leverages two knowledge distillation\nobjectives: one from a pre-trained speech encoder (HuBERT) for phoneme-level\nstructure, and another from a text-based encoder (LaBSE) for lexical cues.\nExperiments on English and multilingual data show that HAC's factorized\nbottleneck yields disentangled token sets: one aligns with phonemes, while\nanother captures word-level semantics. Quantitative evaluations confirm that\nHAC tokens preserve naturalness and provide interpretable linguistic\ninformation, outperforming single-level baselines in both disentanglement and\nreconstruction quality. These findings underscore HAC's potential as a unified\ndiscrete speech representation, bridging acoustic detail and lexical meaning\nfor downstream speech generation and understanding tasks.", "comment": "Accepted to Interspeech 2025", "cate": "eess.AS", "url": "http://arxiv.org/abs/2506.15456v1", "AI": {"title_translation": "用于解耦语音分词的分解式RVQ-GAN", "tldr": "该论文提出了分层音频编码器（HAC），一个统一的神经语音编码器，将语音瓶颈分解为声学、语音和词汇三个语言级别，并利用知识蒸馏实现解耦的语音表示，在解耦和重建质量上均优于基线模型。", "motivation": "旨在开发一种统一的神经语音编码器，能够将其瓶颈分解为声学、语音和词汇三个语言级别，以桥接声学细节和词汇意义，服务于下游语音生成和理解任务。", "method": "论文提出了分层音频编码器（HAC），这是一种统一的神经语音编码器，在一个模型中将其瓶颈分解为声学、语音和词汇三个语言级别。HAC利用两个知识蒸馏目标：一个来自预训练的语音编码器（HuBERT），用于学习音素级别的结构；另一个来自基于文本的编码器（LaBSE），用于学习词汇线索。", "result": "在英语和多语言数据上的实验表明，HAC的分解瓶颈产生了解耦的标记集：一个与音素对齐，另一个捕获词级语义。量化评估证实HAC标记保留了自然度并提供了可解释的语言信息，在解耦和重建质量方面均优于单级基线模型。", "conclusion": "HAC作为一种统一的离散语音表示具有潜力，能够桥接声学细节和词汇意义，用于下游语音生成和理解任务。", "translation": "我们提出了分层音频编码器（HAC），这是一种统一的神经语音编码器，在一个模型中将其瓶颈分解为声学、语音和词汇三个语言级别。HAC利用两个知识蒸馏目标：一个来自预训练的语音编码器（HuBERT），用于学习音素级别的结构；另一个来自基于文本的编码器（LaBSE），用于学习词汇线索。在英语和多语言数据上的实验表明，HAC的分解瓶颈产生了解耦的标记集：一个与音素对齐，另一个捕获词级语义。量化评估证实HAC标记保留了自然度并提供了可解释的语言信息，在解耦和重建质量方面均优于单级基线模型。这些发现强调了HAC作为一种统一的离散语音表示的潜力，能够桥接声学细节和词汇意义，用于下游语音生成和理解任务。", "summary": "分层音频编码器（HAC）是一种新颖的神经语音编码器，它利用知识蒸馏（来自HuBERT和LaBSE）将语音分解为声学、语音和词汇级别的标记。实验证明，HAC生成的标记集具有良好的解耦性，一个与音素对齐，另一个捕获词级语义，并且在保持自然度和提供可解释的语言信息方面优于现有基线，同时在解耦和重建质量上表现出色，为下游语音任务提供了统一的离散语音表示。", "keywords": "分层音频编码器, 语音分词, 解耦, 知识蒸馏, 语言级别", "comments": "该论文的创新之处在于提出了分层音频编码器（HAC），将语音表示分解为声学、语音和词汇三个语言级别，并通过多源知识蒸馏实现各级别信息的解耦。这种方法为生成和理解任务提供了更具解释性和解耦性的语音表示，具有重要的研究价值和应用潜力。"}}
{"id": "2506.14834", "title": "Deploying and Evaluating Multiple Deep Learning Models on Edge Devices for Diabetic Retinopathy Detection", "authors": ["Akwasi Asare", "Dennis Agyemanh Nana Gookyi", "Derrick Boateng", "Fortunatus Aabangbio Wulnye"], "summary": "Diabetic Retinopathy (DR), a leading cause of vision impairment in\nindividuals with diabetes, affects approximately 34.6% of diabetes patients\nglobally, with the number of cases projected to reach 242 million by 2045.\nTraditional DR diagnosis relies on the manual examination of retinal fundus\nimages, which is both time-consuming and resource intensive. This study\npresents a novel solution using Edge Impulse to deploy multiple deep learning\nmodels for real-time DR detection on edge devices. A robust dataset of over\n3,662 retinal fundus images, sourced from the Kaggle EyePACS dataset, was\ncurated, and enhanced through preprocessing techniques, including augmentation\nand normalization. Using TensorFlow, various Convolutional Neural Networks\n(CNNs), such as MobileNet, ShuffleNet, SqueezeNet, and a custom Deep Neural\nNetwork (DNN), were designed, trained, and optimized for edge deployment. The\nmodels were converted to TensorFlowLite and quantized to 8-bit integers to\nreduce their size and enhance inference speed, with minimal trade-offs in\naccuracy. Performance evaluations across different edge hardware platforms,\nincluding smartphones and microcontrollers, highlighted key metrics such as\ninference speed, accuracy, precision, and resource utilization. MobileNet\nachieved an accuracy of 96.45%, while SqueezeNet demonstrated strong real-time\nperformance with a small model size of 176 KB and latency of just 17 ms on GPU.\nShuffleNet and the custom DNN achieved moderate accuracy but excelled in\nresource efficiency, making them suitable for lower-end devices. This\nintegration of edge AI technology into healthcare presents a scalable,\ncost-effective solution for early DR detection, providing timely and accurate\ndiagnosis, especially in resource-constrained and remote healthcare settings.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14834v1", "AI": {"title_translation": "在边缘设备上部署和评估多个深度学习模型用于糖尿病视网膜病变检测", "tldr": "本研究提出了一种在边缘设备上部署和评估多个深度学习模型（如MobileNet、SqueezeNet、ShuffleNet和自定义DNN）以实时检测糖尿病视网膜病变（DR）的解决方案，实现了高精度和资源效率，特别适用于资源受限的环境。", "motivation": "传统的糖尿病视网膜病变（DR）诊断依赖于人工检查视网膜眼底图像，这既耗时又资源密集，且DR是全球糖尿病患者视力障碍的主要原因，病例数量预计将大幅增加。因此，需要一个可扩展、经济高效的早期DR检测解决方案。", "method": "研究使用Edge Impulse在边缘设备上部署多个深度学习模型。从Kaggle EyePACS数据集中策划了3,662多张视网膜眼底图像，并通过数据增强和归一化进行预处理。使用TensorFlow设计、训练和优化了多种卷积神经网络（CNNs），包括MobileNet、ShuffleNet、SqueezeNet和自定义深度神经网络（DNN）。模型被转换为TensorFlowLite并量化为8位整数，以减小模型大小并提高推理速度。最后，在智能手机和微控制器等不同边缘硬件平台上评估了模型的性能。", "result": "MobileNet实现了96.45%的准确率。SqueezeNet在GPU上展现出强大的实时性能，模型大小仅为176 KB，延迟为17 ms。ShuffleNet和自定义DNN实现了中等准确率，但在资源效率方面表现出色，适用于低端设备。性能评估涵盖了推理速度、准确率、精确度和资源利用率等关键指标。", "conclusion": "将边缘AI技术整合到医疗保健中，为早期糖尿病视网膜病变检测提供了一个可扩展、经济高效的解决方案，能够提供及时准确的诊断，尤其适用于资源受限和偏远医疗环境。", "translation": "糖尿病视网膜病变（DR）是糖尿病患者视力障碍的主要原因，全球约34.6%的糖尿病患者受其影响，预计到2045年病例数将达到2.42亿。传统的DR诊断依赖于视网膜眼底图像的人工检查，这既耗时又资源密集。本研究提出了一种新颖的解决方案，利用Edge Impulse在边缘设备上部署多个深度学习模型，实现实时DR检测。研究从Kaggle EyePACS数据集中整理了超过3,662张视网膜眼底图像的强大数据集，并通过数据增强和归一化等预处理技术进行了增强。使用TensorFlow设计、训练和优化了各种卷积神经网络（CNNs），例如MobileNet、ShuffleNet、SqueezeNet和自定义深度神经网络（DNN），以适应边缘部署。模型被转换为TensorFlowLite并量化为8位整数，以减小模型大小并提高推理速度，同时最大限度地减少准确性方面的权衡。在包括智能手机和微控制器在内的不同边缘硬件平台上进行的性能评估，突出了推理速度、准确性、精确性和资源利用率等关键指标。MobileNet实现了96.45%的准确率，而SqueezeNet展示了强大的实时性能，模型大小仅为176 KB，在GPU上的延迟仅为17毫秒。ShuffleNet和自定义DNN实现了中等准确率，但在资源效率方面表现出色，使其适用于低端设备。将边缘AI技术整合到医疗保健中，为早期DR检测提供了一个可扩展、经济高效的解决方案，提供了及时准确的诊断，尤其是在资源受限和偏远医疗环境中。", "summary": "本研究提出了一种创新的边缘AI解决方案，用于实时检测糖尿病视网膜病变（DR）。通过利用Edge Impulse，研究人员在边缘设备上部署并评估了多个深度学习模型（包括MobileNet、ShuffleNet、SqueezeNet和自定义DNN）。这些模型在经过预处理的Kaggle EyePACS数据集上进行训练和优化，并转换为TensorFlowLite进行边缘部署。结果显示，MobileNet实现了高精度（96.45%），SqueezeNet在小模型尺寸和低延迟下表现出强大的实时性能，而ShuffleNet和自定义DNN则在资源效率方面表现出色，适用于低端设备。这项工作为在资源受限环境中实现可扩展且经济高效的早期DR诊断提供了可行途径。", "keywords": "糖尿病视网膜病变, 深度学习, 边缘计算, 实时检测, 卷积神经网络", "comments": "这项研究的创新点在于将多个深度学习模型部署到边缘设备上进行糖尿病视网膜病变检测，并针对边缘计算的特点进行了模型优化（如TensorFlowLite转换和8位量化）。这对于资源受限和偏远地区的医疗保健具有重要意义，因为它提供了一个可扩展、成本效益高的早期诊断方案。研究详细评估了不同模型在不同边缘硬件上的性能，提供了实用的部署指导。"}}
{"id": "2506.15290", "title": "Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models", "authors": ["Andela Ilic", "Jiaxi Jiang", "Paul Streli", "Xintong Liu", "Christian Holz"], "summary": "Motion capture using sparse inertial sensors has shown great promise due to\nits portability and lack of occlusion issues compared to camera-based tracking.\nExisting approaches typically assume that IMU sensors are tightly attached to\nthe human body. However, this assumption often does not hold in real-world\nscenarios. In this paper, we present a new task of full-body human pose\nestimation using sparse, loosely attached IMU sensors. To solve this task, we\nsimulate IMU recordings from an existing garment-aware human motion dataset. We\ndeveloped transformer-based diffusion models to synthesize loose IMU data and\nestimate human poses based on this challenging loose IMU data. In addition, we\nshow that incorporating garment-related parameters while training the model on\nsimulated loose data effectively maintains expressiveness and enhances the\nability to capture variations introduced by looser or tighter garments.\nExperiments show that our proposed diffusion methods trained on simulated and\nsynthetic data outperformed the state-of-the-art methods quantitatively and\nqualitatively, opening up a promising direction for future research.", "comment": "Accepted by IJCAI 2025", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.15290v1", "AI": {"title_translation": "基于宽松稀疏惯性传感器的服装感知扩散模型人体运动捕捉", "tldr": "该研究提出了一种利用宽松稀疏惯性传感器进行全身人体姿态估计的新方法，通过模拟数据和基于Transformer的扩散模型，并结合服装参数，在处理传感器松动带来的变化方面优于现有SOTA方法。", "motivation": "现有基于惯性测量单元（IMU）的运动捕捉方法通常假定传感器紧密附着于人体，但在实际场景中这一假设往往不成立，导致无法有效处理传感器松动的情况。", "method": "研究团队通过现有服装感知人体运动数据集模拟IMU记录，并开发了基于Transformer的扩散模型来合成松散的IMU数据，并基于此数据估计人体姿态。此外，在模型训练中融入服装相关参数，以增强其捕捉由服装松紧引入的变化的能力。", "result": "实验表明，该文提出的在模拟和合成数据上训练的扩散方法，在定量和定性上均优于现有的最先进方法。", "conclusion": "该研究为未来基于宽松稀疏惯性传感器的人体运动捕捉开辟了一个有前景的方向。", "translation": "使用稀疏惯性传感器进行运动捕捉因其便携性和无遮挡问题（相较于基于摄像头的追踪）而显示出巨大潜力。现有方法通常假设惯性测量单元（IMU）传感器紧密附着于人体。然而，在实际场景中，这一假设往往不成立。在本文中，我们提出了一项新的任务：使用稀疏、松散附着的IMU传感器进行全身人体姿态估计。为了解决这项任务，我们从现有的服装感知人体运动数据集中模拟了IMU记录。我们开发了基于Transformer的扩散模型，用于合成松散的IMU数据，并基于这些具有挑战性的松散IMU数据估计人体姿态。此外，我们发现，在模拟松散数据上训练模型时，结合服装相关参数能有效保持表达能力，并增强捕捉由更宽松或更紧密服装引入的变化的能力。实验表明，我们提出的在模拟和合成数据上训练的扩散方法在定量和定性上均优于现有最先进方法，为未来的研究开辟了一个有前景的方向。", "summary": "本研究提出了一种利用稀疏、松散惯性传感器进行全身人体姿态估计的新任务。针对现有IMU运动捕捉方法传感器需紧密附着的问题，作者通过模拟服装感知的人体运动数据，并开发了基于Transformer的扩散模型来处理松散的IMU数据。该模型通过结合服装相关参数，有效捕捉了由服装松紧引起的变化。实验结果表明，该方法在性能上超越了现有最先进技术，为未来相关研究提供了新的方向。", "keywords": "人体运动捕捉, 惯性传感器, 扩散模型, 姿态估计, 服装感知", "comments": "本文的创新点在于提出了一个处理宽松稀疏惯性传感器数据进行人体姿态估计的新任务，并有效地利用了基于Transformer的扩散模型来解决这一挑战。特别是，将服装相关参数纳入模型训练，显著提升了模型对实际场景中传感器松动和服装变化鲁棒性，这是一个非常重要的进步。该方法为解决IMU运动捕捉在现实应用中的局限性提供了有前景的解决方案。"}}
{"id": "2506.15097", "title": "Efficient space reduction techniques by optimized majority rules for the Kemeny aggregation problem", "authors": ["Xuan Kien Phung", "Sylvie Hamel"], "summary": "The Kemeny aggregation problem consists of computing the consensus rankings\nof an election with respect to the Kemeny-Young voting method. These aggregated\nrankings are the geometric medians as well as the maximum likelihood estimators\nin the Mallows model of the rankings in the election under the Kendall-tau\ndistance which counts the number of pairwise disagreements. The problem admits\nfundamental applications in various domains such as computational social\nchoice, machine learning, operations research, and biology but its\ncomputational complexity is unfortunately expensive. In this paper, we\nestablish optimized quantitative extensions of the well-known 3/4-majority rule\nof Betzler et al. and the Major Order Theorem of Hamel and Milosz for the\nKemeny aggregation problem. By taking into account the extra information\navailable in the problem such as the number of candidates and by considering an\nadditional optimization of certain piecewise linear functions in one variable,\nour results achieve significantly more refined space reduction techniques as\nillustrated by experimental results on real and synthetic data without\nincreasing the time complexity of the algorithms.", "comment": null, "cate": "cs.DS", "url": "http://arxiv.org/abs/2506.15097v1", "AI": {"title_translation": "Kemeny聚合问题中基于优化多数规则的有效空间削减技术", "tldr": "本文针对Kemeny聚合问题，通过优化多数规则，提出了更高效的空间削减技术，且不增加时间复杂度。", "motivation": "Kemeny聚合问题在计算社会选择、机器学习、运筹学和生物学等多个领域有重要应用，但其计算复杂度很高，因此需要更高效的解决方案。", "method": "本文通过优化Betzler等人提出的3/4多数规则和Hamel与Milosz的多数顺序定理，并结合候选人数量等额外信息，以及对分段线性函数进行优化，实现了空间削减。", "result": "实验结果表明，该方法在不增加算法时间复杂度的情况下，在真实和合成数据上实现了显著更精细的空间削减技术。", "conclusion": "通过优化多数规则和利用额外信息，可以有效提升Kemeny聚合问题的空间削减效率，同时保持时间复杂度不变。", "translation": "Kemeny聚合问题在于计算选举中关于Kemeny-Young投票方法的共识排名。这些聚合排名是Mallows模型中选举排名的几何中位数和最大似然估计，衡量标准是Kendall-tau距离，该距离计算成对分歧的数量。该问题在计算社会选择、机器学习、运筹学和生物学等各个领域都有基本应用，但其计算复杂度不幸地很高。在本文中，我们针对Kemeny聚合问题，建立了Betzler等人著名的3/4多数规则和Hamel与Milosz的多数顺序定理的优化定量扩展。通过考虑问题中可用的额外信息，例如候选人数量，并考虑对一个变量中的某些分段线性函数进行额外优化，我们的结果实现了显著更精细的空间削减技术，这通过在真实和合成数据上的实验结果得到证明，且没有增加算法的时间复杂度。", "summary": "本文针对计算复杂度较高的Kemeny聚合问题，提出了一种新的空间削减技术。该技术通过优化现有多数规则并结合额外信息（如候选人数量）以及对分段线性函数进行优化，实现了比现有方法更精细的空间削减，且在实验中证明了其有效性，同时不增加算法的时间复杂度。", "keywords": "Kemeny聚合, 多数规则, 空间削减, 计算社会选择, Kendall-tau距离", "comments": "本文通过优化现有多数规则并引入额外信息和函数优化，有效解决了Kemeny聚合问题的计算效率瓶颈，其创新性在于在不增加时间复杂度的情况下显著提升了空间削减效果，对计算社会选择等领域具有重要意义。"}}
{"id": "2506.15267", "title": "Next-User Retrieval: Enhancing Cold-Start Recommendations via Generative Next-User Modeling", "authors": ["Yu-Ting Lan", "Yang Huo", "Yi Shen", "Xiao Yang", "Zuotao Liu"], "summary": "The item cold-start problem is critical for online recommendation systems, as\nthe success of this phase determines whether high-quality new items can\ntransition to popular ones, receive essential feedback to inspire creators, and\nthus lead to the long-term retention of creators. However, modern\nrecommendation systems still struggle to address item cold-start challenges due\nto the heavy reliance on item and historical interactions, which are\nnon-trivial for cold-start items lacking sufficient exposure and feedback.\nLookalike algorithms provide a promising solution by extending feedback for new\nitems based on lookalike users. Traditional lookalike algorithms face such\nlimitations: (1) failing to effectively model the lookalike users and further\nimprove recommendations with the existing rule- or model-based methods; and (2)\nstruggling to utilize the interaction signals and incorporate diverse features\nin modern recommendation systems.\n  Inspired by lookalike algorithms, we propose Next-User Retrieval, a novel\nframework for enhancing cold-start recommendations via generative next-user\nmodeling. Specifically, we employ a transformer-based model to capture the\nunidirectional relationships among recently interacted users and utilize these\nsequences to generate the next potential user who is most likely to interact\nwith the item. The additional item features are also integrated as prefix\nprompt embeddings to assist the next-user generation. The effectiveness of\nNext-User Retrieval is evaluated through both offline experiments and online\nA/B tests. Our method achieves significant improvements with increases of\n0.0142% in daily active users and +0.1144% in publications in Douyin,\nshowcasing its practical applicability and scalability.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.15267v1", "AI": {"title_translation": "下一用户检索：通过生成式下一用户建模增强冷启动推荐", "tldr": "该研究提出了一种名为“下一用户检索”的新框架，通过生成式下一用户建模来解决推荐系统中的物品冷启动问题，并在抖音上取得了显著效果。", "motivation": "物品冷启动问题对于在线推荐系统至关重要，因为其成功决定了新物品能否流行并激励创作者。然而，现代推荐系统因严重依赖物品和历史交互而难以处理冷启动物品。传统的“相似用户”算法也面临无法有效建模相似用户和利用多样化特征的局限性。", "method": "该论文提出了“下一用户检索”框架，通过生成式下一用户建模来增强冷启动推荐。具体方法是采用基于Transformer的模型来捕获最近交互用户之间的单向关系，并利用这些序列生成最有可能与物品交互的下一个潜在用户。此外，还将额外的物品特征作为前缀提示嵌入集成，以辅助下一用户生成。", "result": "通过离线实验和在线A/B测试，该方法在抖音上实现了显著改进，日活跃用户增加了0.0142%，发布量增加了0.1144%，展示了其实用性和可扩展性。", "conclusion": "该研究提出了一种新颖的“下一用户检索”框架，通过生成式下一用户建模有效解决了推荐系统中的物品冷启动问题，并在真实世界应用中展现出显著的性能提升。", "translation": "物品冷启动问题对于在线推荐系统至关重要，因为此阶段的成功决定了高质量的新物品能否转变为流行物品，获得必要的反馈以激励创作者，从而实现创作者的长期留存。然而，现代推荐系统仍然难以解决物品冷启动挑战，因为它们严重依赖物品和历史交互，这对于缺乏足够曝光和反馈的冷启动物品来说并非易事。“相似用户”算法通过基于相似用户扩展新物品的反馈提供了一个有前景的解决方案。传统的相似用户算法面临以下局限性：（1）未能有效建模相似用户并利用现有基于规则或模型的方法进一步改进推荐；（2）难以利用交互信号并整合现代推荐系统中的多样化特征。受相似用户算法的启发，我们提出了“下一用户检索”——一种通过生成式下一用户建模来增强冷启动推荐的新颖框架。具体来说，我们采用基于Transformer的模型来捕获最近交互用户之间的单向关系，并利用这些序列生成最有可能与该物品交互的下一个潜在用户。额外的物品特征也被整合为前缀提示嵌入，以辅助下一用户生成。“下一用户检索”的有效性通过离线实验和在线A/B测试进行了评估。我们的方法在抖音上取得了显著改进，日活跃用户增加了0.0142%，发布量增加了0.1144%，展示了其实用性和可扩展性。", "summary": "该论文提出了一种名为“下一用户检索”（Next-User Retrieval）的新颖框架，旨在通过生成式下一用户建模来解决在线推荐系统中的物品冷启动问题。该方法利用基于Transformer的模型捕获最近交互用户之间的单向关系，并预测最有可能与新物品交互的下一个用户，同时整合物品特征作为提示。通过离线实验和在抖音上的在线A/B测试，该框架在日活跃用户和发布量方面取得了显著提升，证明了其在实际应用中的有效性和可扩展性。", "keywords": "冷启动推荐, 生成式建模, Transformer, 相似用户, 用户检索", "comments": "创新点在于将生成式模型应用于“相似用户”概念，通过预测“下一个用户”来解决冷启动问题，而非传统的相似度匹配。在真实大规模平台（抖音）上的在线A/B测试结果提供了强有力的实践证据，表明其具有很高的实用价值和可扩展性。"}}
{"id": "2506.15222", "title": "The Pitfalls and Potentials of Adding Gene-invariance to Optimal Mixing", "authors": ["Anton Bouter", "Dirk Thierens", "Peter A. N. Bosman"], "summary": "Optimal Mixing (OM) is a variation operator that integrates local search with\ngenetic recombination. EAs with OM are capable of state-of-the-art optimization\nin discrete spaces, offering significant advantages over classic\nrecombination-based EAs. This success is partly due to high selection pressure\nthat drives rapid convergence. However, this can also negatively impact\npopulation diversity, complicating the solving of hierarchical problems, which\nfeature multiple layers of complexity. While there have been attempts to\naddress this issue, these solutions are often complicated and prone to bias. To\novercome this, we propose a solution inspired by the Gene Invariant Genetic\nAlgorithm (GIGA), which preserves gene frequencies in the population throughout\nthe process. This technique is tailored to and integrated with the Gene-pool\nOptimal Mixing Evolutionary Algorithm (GOMEA), resulting in GI-GOMEA. The\nsimple, yet elegant changes are found to have striking potential: GI-GOMEA\noutperforms GOMEA on a range of well-known problems, even when these problems\nare adjusted for pitfalls - biases in much-used benchmark problems that can be\neasily exploited by maintaining gene invariance. Perhaps even more notably,\nGI-GOMEA is also found to be effective at solving hierarchical problems,\nincluding newly introduced asymmetric hierarchical trap functions.", "comment": null, "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.15222v1", "AI": {"title_translation": "将基因不变性引入最优混合的陷阱与潜力", "tldr": "最优混合（OM）算法在离散空间优化方面表现出色，但其高选择压力会损害种群多样性，难以解决分层问题。本文提出GI-GOMEA，通过引入基因不变性来解决此问题。实验证明GI-GOMEA在多种问题上优于GOMEA，尤其在解决分层问题方面表现出色。", "motivation": "最优混合（OM）算法虽然在离散空间优化中表现先进，但其高选择压力会导致种群多样性下降，从而难以有效解决具有多层复杂性的分层问题。现有解决此问题的方法往往复杂且容易产生偏差。", "method": "本文提出了一种受基因不变遗传算法（GIGA）启发的解决方案，该方案在整个过程中保持种群中的基因频率。这项技术经过专门调整并与基因池最优混合进化算法（GOMEA）相结合，形成了GI-GOMEA。", "result": "GI-GOMEA 在一系列知名问题上表现优于 GOMEA，即使这些问题经过调整以消除基准测试中常见的偏置。更值得注意的是，GI-GOMEA 在解决分层问题（包括新引入的非对称分层陷阱函数）方面也表现出有效性。", "conclusion": "通过将基因不变性（受GIGA启发）简单而优雅地整合到GOMEA中，GI-GOMEA不仅提升了算法在多种问题上的性能，还显著增强了其解决复杂分层问题的能力，同时避免了现有解决方案的复杂性和偏置问题。", "translation": "最优混合（OM）是一种将局部搜索与基因重组相结合的变异算子。采用OM的进化算法（EAs）能够在离散空间中实现最先进的优化，与经典的基于重组的EAs相比具有显著优势。这一成功部分归因于驱动快速收敛的高选择压力。然而，这也会对种群多样性产生负面影响，从而使解决具有多层复杂性的分层问题变得复杂。尽管已经有一些尝试来解决这个问题，但这些解决方案通常复杂且容易产生偏置。为了克服这一点，我们提出了一种受基因不变遗传算法（GIGA）启发的解决方案，该方案在整个过程中保留种群中的基因频率。这项技术经过专门调整并与基因池最优混合进化算法（GOMEA）相结合，形成了GI-GOMEA。结果发现，这些简单而优雅的改变具有惊人的潜力：GI-GOMEA 在一系列知名问题上优于 GOMEA，即使这些问题经过调整以消除可能被维持基因不变性轻易利用的基准问题中的偏置。或许更值得注意的是，GI-GOMEA 还被发现能够有效解决分层问题，包括新引入的非对称分层陷阱函数。", "summary": "最优混合（OM）进化算法在离散空间优化中表现出色，但高选择压力导致多样性不足，难以处理分层问题。为解决此缺陷，本文提出GI-GOMEA，它将基因不变性（受GIGA启发）整合到GOMEA中，以保持基因频率。实验证明，GI-GOMEA在多个基准问题上性能优于GOMEA，并能有效解决分层问题，包括新型非对称分层陷阱函数，显示出其简单而强大的潜力。", "keywords": "最优混合, 基因不变性, 进化算法, 分层问题, GOMEA", "comments": "该论文的创新点在于将基因不变性这一简洁有效的机制融入到GOMEA中，巧妙地解决了OM算法因高选择压力导致的多样性不足问题。这种改进不仅提升了算法在通用问题上的性能，更重要的是，它显著增强了算法解决复杂分层问题的能力，并且有效规避了现有解决方案的复杂性和偏置风险，为进化算法的设计提供了新的思路和方向。"}}
{"id": "2506.14901", "title": "Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction", "authors": ["Marija Šakota", "Robert West"], "summary": "Many recent approaches to structured NLP tasks use an autoregressive language\nmodel $M$ to map unstructured input text $x$ to output text $y$ representing\nstructured objects (such as tuples, lists, trees, code, etc.), where the\ndesired output structure is enforced via constrained decoding. During training,\nthese approaches do not require the model to be aware of the constraints, which\nare merely implicit in the training outputs $y$. This is advantageous as it\nallows for dynamic constraints without requiring retraining, but can lead to\nlow-quality output during constrained decoding at test time. We overcome this\nproblem with Boosted Constrained Decoding (BoostCD), which combines constrained\nand unconstrained decoding in two phases: Phase 1 decodes from the base model\n$M$ twice, in constrained and unconstrained mode, obtaining two weak\npredictions. In phase 2, a learned autoregressive boosted model combines the\ntwo weak predictions into one final prediction. The mistakes made by the base\nmodel with vs. without constraints tend to be complementary, which the boosted\nmodel learns to exploit for improved performance. We demonstrate the power of\nBoostCD by applying it to closed information extraction. Our model, BoostIE,\noutperforms prior approaches both in and out of distribution, addressing\nseveral common errors identified in those approaches.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14901v1", "AI": {"title_translation": "结合受限与非受限解码通过提升：BoostCD及其在信息抽取中的应用", "tldr": "BoostCD通过结合受限和非受限解码来提高结构化NLP任务的性能，特别是在信息抽取中表现优异。", "motivation": "现有的结构化自然语言处理（NLP）任务方法在训练时未考虑约束条件，导致测试时受限解码的输出质量低下，尽管其允许动态约束而无需重新训练。", "method": "本文提出了Boosted Constrained Decoding (BoostCD)，它分两阶段结合了受限和非受限解码。第一阶段，基础模型M在受限和非受限模式下分别进行解码，得到两个弱预测。第二阶段，一个学习到的自回归提升模型将这两个弱预测结合成一个最终预测，利用了基础模型在不同约束条件下犯错的互补性。", "result": "将BoostCD应用于封闭信息抽取任务的BoostIE模型，在分布内和分布外均优于现有方法，并解决了这些方法中识别出的几个常见错误。", "conclusion": "BoostCD通过结合受限和非受限解码的互补优势，有效提高了结构化NLP任务的性能，特别是在信息抽取领域展现出显著效果。", "translation": "许多最近的结构化自然语言处理任务方法使用自回归语言模型M将非结构化输入文本x映射到表示结构化对象（如元组、列表、树、代码等）的输出文本y，其中所需的输出结构通过受限解码强制执行。在训练期间，这些方法不要求模型感知约束，约束仅隐含在训练输出y中。这具有优势，因为它允许动态约束而无需重新训练，但可能导致测试时受限解码期间输出质量低下。我们通过提升受限解码（BoostCD）克服了这个问题，它分两个阶段结合了受限和非受限解码：第一阶段，从基础模型M进行两次解码，分别在受限和非受限模式下，获得两个弱预测。在第二阶段，一个学习到的自回归提升模型将两个弱预测结合成一个最终预测。基础模型在有约束和无约束情况下犯的错误往往是互补的，提升模型学习利用这一点来提高性能。我们通过将其应用于封闭信息抽取来展示BoostCD的强大功能。我们的模型BoostIE在分布内和分布外均优于现有方法，并解决了这些方法中识别出的几个常见错误。", "summary": "本文提出了Boosted Constrained Decoding (BoostCD)，旨在解决结构化NLP任务中因训练时未考虑约束而导致的测试时受限解码输出质量低下的问题。BoostCD通过两阶段过程结合了受限和非受限解码的弱预测，一个提升模型学习利用两者错误的互补性。实验表明，应用于信息抽取的BoostIE模型在分布内和分布外均超越了现有方法，有效纠正了常见错误。", "keywords": "受限解码, 非受限解码, 提升学习, 信息抽取, 结构化预测", "comments": "这篇论文的创新点在于提出了一种新颖的解码策略BoostCD，通过结合受限和非受限解码的互补优势来提高结构化NLP任务的性能。它解决了当前自回归模型在处理动态约束时输出质量下降的关键问题，具有重要的实践意义。其两阶段提升模型的设计巧妙地利用了不同解码模式下的错误互补性，为未来的结构化预测任务提供了新的思路。"}}
{"id": "2506.14785", "title": "Moment-enhanced shallow water equations for non-slip boundary conditions", "authors": ["Shiping Zhou", "Juntao Huang", "Andrew J. Christlieb"], "summary": "The shallow water equations often assume a constant velocity profile along\nthe vertical axis. However, this assumption does not hold in many practical\napplications. To better approximate the vertical velocity distribution, models\nsuch as the shallow water moment expansion models have been proposed.\nNevertheless, under non-slip bottom boundary conditions, both the standard\nshallow water equation and its moment-enhanced models struggle to accurately\ncapture the vertical velocity profile due to the stiff source terms. In this\nwork, we propose modified shallow water equations and corresponding\nmoment-enhanced models that perform well under both non-slip and slip boundary\nconditions. The primary difference between the modified and original models\nlies in the treatment of the source term, which allows our modified moment\nexpansion models to be readily generalized, while maintaining compatibility\nwith our previous analysis on the hyperbolicity of the model. To assess the\nperformance of both the standard and modified moment expansion models, we\nconduct a comprehensive numerical comparison with the incompressible\nNavier--Stokes equations -- a comparison that is absent from existing\nliterature.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14785v1", "AI": {"title_translation": "非滑移边界条件下的动量增强浅水方程", "tldr": "本文提出了改进的浅水方程和动量增强模型，以在非滑移和滑移边界条件下更好地捕捉垂直速度分布，并通过与Navier-Stokes方程进行数值比较来评估其性能。", "motivation": "在许多实际应用中，浅水方程假设的恒定垂直速度剖面不成立。现有的浅水方程和动量增强模型在非滑移底部边界条件下，由于刚性源项，难以准确捕捉垂直速度剖面。", "method": "本文提出了修改后的浅水方程和相应的动量增强模型。主要区别在于对源项的处理，这使得修改后的模型易于推广，并与之前关于模型双曲性的分析保持兼容性。此外，还与不可压缩Navier-Stokes方程进行了全面的数值比较。", "result": "修改后的浅水方程和动量增强模型在非滑移和滑移边界条件下均表现良好。与现有文献中缺失的Navier-Stokes方程的数值比较已完成。", "conclusion": "本文成功开发了在非滑移和滑移边界条件下均能有效捕捉垂直速度剖面的改进浅水方程和动量增强模型，并通过全面的数值比较验证了其性能。", "translation": "浅水方程通常假设沿垂直轴的速度剖面是恒定的。然而，在许多实际应用中，这一假设并不成立。为了更好地近似垂直速度分布，已经提出了诸如浅水动量展开模型等模型。然而，在非滑移底部边界条件下，无论是标准浅水方程还是其动量增强模型，由于刚性源项，都难以准确捕捉垂直速度剖面。在这项工作中，我们提出了修改后的浅水方程和相应的动量增强模型，它们在非滑移和滑移边界条件下均表现良好。修改后的模型与原始模型的主要区别在于对源项的处理，这使得我们修改后的动量展开模型易于推广，同时与我们之前关于模型双曲性的分析保持兼容性。为了评估标准和修改后的动量展开模型的性能，我们与不可压缩Navier-Stokes方程进行了全面的数值比较——这种比较在现有文献中是缺失的。", "summary": "本文针对现有浅水方程及其动量增强模型在非滑移边界条件下垂直速度剖面捕捉不准确的问题，提出了改进的浅水方程和动量增强模型。通过修改源项的处理方式，新模型在非滑移和滑移边界条件下均表现出良好性能，并且易于推广。作者首次与不可压缩Navier-Stokes方程进行了全面的数值比较，填补了现有文献的空白。", "keywords": "浅水方程, 非滑移边界条件, 动量增强模型, 源项处理, 数值比较", "comments": "本文的创新点在于提出了对浅水方程源项的修改方法，使其在非滑移边界条件下能更准确地描述流体行为。此外，与Navier-Stokes方程的全面数值比较是现有文献中缺失的重要工作，这增强了研究的严谨性和模型的可靠性。这项工作对于需要精确模拟非滑移边界条件下浅水流动的应用具有重要意义。"}}
{"id": "2506.15000", "title": "A Comparative Evaluation of Deep Learning Models for Speech Enhancement in Real-World Noisy Environments", "authors": ["Md Jahangir Alam Khondkar", "Ajan Ahmed", "Masudul Haider Imtiaz", "Stephanie Schuckers"], "summary": "Speech enhancement, particularly denoising, is vital in improving the\nintelligibility and quality of speech signals for real-world applications,\nespecially in noisy environments. While prior research has introduced various\ndeep learning models for this purpose, many struggle to balance noise\nsuppression, perceptual quality, and speaker-specific feature preservation,\nleaving a critical research gap in their comparative performance evaluation.\nThis study benchmarks three state-of-the-art models Wave-U-Net, CMGAN, and\nU-Net, on diverse datasets such as SpEAR, VPQAD, and Clarkson datasets. These\nmodels were chosen due to their relevance in the literature and code\naccessibility. The evaluation reveals that U-Net achieves high noise\nsuppression with SNR improvements of +71.96% on SpEAR, +64.83% on VPQAD, and\n+364.2% on the Clarkson dataset. CMGAN outperforms in perceptual quality,\nattaining the highest PESQ scores of 4.04 on SpEAR and 1.46 on VPQAD, making it\nwell-suited for applications prioritizing natural and intelligible speech.\nWave-U-Net balances these attributes with improvements in speaker-specific\nfeature retention, evidenced by VeriSpeak score gains of +10.84% on SpEAR and\n+27.38% on VPQAD. This research indicates how advanced methods can optimize\ntrade-offs between noise suppression, perceptual quality, and speaker\nrecognition. The findings may contribute to advancing voice biometrics,\nforensic audio analysis, telecommunication, and speaker verification in\nchallenging acoustic conditions.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15000v1", "AI": {"title_translation": "深度学习模型在真实世界噪声环境下语音增强的比较评估", "tldr": "本研究比较了Wave-U-Net、CMGAN和U-Net三种深度学习模型在真实世界噪声环境下语音增强的表现，发现U-Net在噪声抑制方面表现突出，CMGAN在感知质量方面更优，而Wave-U-Net在保留说话人特定特征方面有优势。", "motivation": "语音增强，特别是去噪，对于提高语音信号在嘈杂环境中的可懂度和质量至关重要。尽管已有多种深度学习模型用于此目的，但许多模型难以平衡噪声抑制、感知质量和说话人特定特征保留，因此在它们的比较性能评估方面存在关键研究空白。", "method": "本研究在SpEAR、VPQAD和Clarkson等不同数据集上，对Wave-U-Net、CMGAN和U-Net三种最先进的深度学习模型进行了基准测试。这些模型因其在文献中的相关性和代码可访问性而被选择。", "result": "评估结果显示，U-Net在噪声抑制方面表现出色，在SpEAR数据集上SNR提高了+71.96%，在VPQAD上提高了+64.83%，在Clarkson数据集上提高了+364.2%。CMGAN在感知质量方面表现更优，在SpEAR上PESQ分数达到4.04，在VPQAD上达到1.46。Wave-U-Net在保留说话人特定特征方面取得了平衡，在SpEAR上VeriSpeak分数提高了+10.84%，在VPQAD上提高了+27.38%。", "conclusion": "本研究表明先进方法可以优化噪声抑制、感知质量和说话人识别之间的权衡。研究结果可能有助于推进语音生物识别、法医音频分析、电信和说话人验证在挑战性声学条件下的发展。", "translation": "语音增强，特别是去噪，对于提高真实世界应用中，尤其是在嘈杂环境下的语音信号的可懂度和质量至关重要。虽然之前的研究已经为此目的引入了各种深度学习模型，但许多模型难以平衡噪声抑制、感知质量和说话人特定特征保留，这在它们的比较性能评估中留下了一个关键的研究空白。本研究在SpEAR、VPQAD和Clarkson等不同数据集上，对Wave-U-Net、CMGAN和U-Net三种最先进的模型进行了基准测试。选择这些模型是由于它们在文献中的相关性和代码可访问性。评估结果显示，U-Net在噪声抑制方面表现出色，在SpEAR数据集上SNR提高了+71.96%，在VPQAD上提高了+64.83%，在Clarkson数据集上提高了+364.2%。CMGAN在感知质量方面表现更优，在SpEAR上PESQ分数达到4.04，在VPQAD上达到1.46，使其非常适合优先考虑自然和清晰语音的应用。Wave-U-Net通过提高说话人特定特征保留能力来平衡这些属性，这在SpEAR上VeriSpeak分数提高了+10.84%，在VPQAD上提高了+27.38%的证据中得到体现。这项研究表明先进方法如何优化噪声抑制、感知质量和说话人识别之间的权衡。研究结果可能有助于推进语音生物识别、法医音频分析、电信和说话人验证在挑战性声学条件下的发展。", "summary": "本研究对Wave-U-Net、CMGAN和U-Net三种先进的深度学习语音增强模型在真实世界噪声环境中的性能进行了比较评估。研究发现，U-Net在噪声抑制方面表现最佳，CMGAN在感知质量方面表现优异，而Wave-U-Net在保留说话人特定特征方面具有优势。这项工作揭示了如何通过先进方法优化噪声抑制、感知质量和说话人识别之间的权衡，并对语音生物识别、法医音频分析等领域具有潜在贡献。", "keywords": "语音增强, 深度学习, 噪声抑制, 感知质量, 说话人识别", "comments": "这项研究的创新之处在于对当前主流的深度学习语音增强模型进行了系统的比较评估，并量化了它们在噪声抑制、感知质量和说话人特征保留方面的权衡。其重要性在于为实际应用中选择合适的语音增强模型提供了数据支持和指导，特别是在语音生物识别和电信等领域。这项研究的局限性可能在于其评估的数据集范围，以及是否涵盖了所有最新的模型变体。"}}
{"id": "2506.15034", "title": "MECHA: Multithreaded and Efficient Cryptographic Hardware Access", "authors": ["Pratama Derry", "Laksmono Agus Mahardika Ari", "Iqbal Muhammad", "Howon Kim"], "summary": "This paper presents a multithread and efficient cryptographic hardware access\n(MECHA) for efficient and fast cryptographic operations that eliminates the\nneed for context switching. Utilizing a UNIX domain socket, MECHA manages\nmultiple requests from multiple applications simultaneously, resulting in\nfaster processing and improved efficiency. We comprise several key components,\nincluding the Server thread, Client thread, Transceiver thread, and a pair of\nSender and Receiver queues. MECHA design is portable and can be used with any\ncommunication protocol, with experimental results demonstrating a 83% increase\nin the speed of concurrent cryptographic requests compared to conventional\ninterface design. MECHA architecture has significant potential in the field of\nsecure communication applications ranging from cloud computing to the IoT,\noffering a faster and more efficient solution for managing multiple\ncryptographic operation requests concurrently.", "comment": "4 Page", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15034v1", "AI": {"title_translation": "MECHA: 多线程高效密码硬件访问", "tldr": "MECHA是一个多线程系统，通过UNIX域套接字管理并发密码请求，消除上下文切换，实现83%的速度提升。", "motivation": "为了实现高效快速的密码操作，并消除处理来自多个应用程序的多个请求时上下文切换的需要。", "method": "MECHA是一个多线程且高效的密码硬件访问系统，它利用UNIX域套接字同时管理来自多个应用程序的多个请求。其组成部分包括服务器线程、客户端线程、收发器线程以及一对发送器和接收器队列。MECHA的设计具有可移植性，可以与任何通信协议一起使用。", "result": "实验结果表明，与传统接口设计相比，并发密码请求的速度提高了83%。", "conclusion": "MECHA为同时管理多个密码操作请求提供了一个更快、更高效的解决方案，在从云计算到物联网的安全通信应用领域具有巨大潜力。", "translation": "本文提出了一种多线程高效密码硬件访问（MECHA）系统，用于高效快速的密码操作，消除了上下文切换的需要。MECHA利用UNIX域套接字，同时管理来自多个应用程序的多个请求，从而实现更快的处理速度和更高的效率。我们由几个关键组件组成，包括服务器线程、客户端线程、收发器线程以及一对发送器和接收器队列。MECHA设计具有可移植性，可与任何通信协议一起使用，实验结果表明，与传统接口设计相比，并发密码请求的速度提高了83%。MECHA架构在从云计算到物联网的安全通信应用领域具有巨大潜力，为同时管理多个密码操作请求提供了更快、更高效的解决方案。", "summary": "MECHA是一种多线程高效密码硬件访问系统，通过利用UNIX域套接字同时管理来自多个应用程序的并发密码请求，从而避免上下文切换。该系统由服务器、客户端和收发器线程以及队列组成，具有良好的可移植性。实验证明，MECHA将并发密码请求的速度提高了83%，在云计算和物联网等安全通信应用中具有广阔前景。", "keywords": "密码硬件访问, 多线程, UNIX域套接字, 并发请求, 安全通信", "comments": "MECHA的创新点在于其多线程设计和UNIX域套接字的使用，有效消除了上下文切换，显著提升了并发密码操作的效率。其可移植性也增加了其实用价值。该方法为处理高并发密码请求提供了一个高效的替代方案，对于需要高性能安全通信的领域（如云服务和物联网）具有重要意义。"}}
{"id": "2506.15088", "title": "Program Feature-based Fuzzing Benchmarking", "authors": ["Miao Miao"], "summary": "Fuzzing is a powerful software testing technique renowned for its\neffectiveness in identifying software vulnerabilities. Traditional fuzzing\nevaluations typically focus on overall fuzzer performance across a set of\ntarget programs, yet few benchmarks consider how fine-grained program features\ninfluence fuzzing effectiveness. To bridge this gap, we introduce a novel\nbenchmark designed to generate programs with configurable, fine-grained program\nfeatures to enhance fuzzing evaluations. We reviewed 25 recent grey-box fuzzing\nstudies, extracting 7 program features related to control-flow and data-flow\nthat can impact fuzzer performance. Using these features, we generated a\nbenchmark consisting of 153 programs controlled by 10 fine-grained configurable\nparameters. We evaluated 11 popular fuzzers using this benchmark. The results\nindicate that fuzzer performance varies significantly based on the program\nfeatures and their strengths, highlighting the importance of incorporating\nprogram characteristics into fuzzing evaluations.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15088v1", "AI": {"title_translation": "基于程序特征的模糊测试基准", "tldr": "本文提出了一个新的基准测试方法，用于评估模糊测试工具，该方法通过生成具有可配置细粒度程序特征的程序来研究程序特征对模糊测试有效性的影响。研究发现，模糊测试工具的性能会因程序特征及其强度而显著变化。", "motivation": "传统的模糊测试评估通常侧重于模糊测试工具在目标程序集上的整体性能，但很少有基准考虑细粒度的程序特征如何影响模糊测试的有效性。为了弥补这一空白，本文引入了一个新颖的基准测试。", "method": "作者回顾了25项近期灰盒模糊测试研究，提取了7个与控制流和数据流相关的程序特征，这些特征可能影响模糊测试工具的性能。利用这些特征，生成了一个包含153个程序、由10个细粒度可配置参数控制的基准。作者使用该基准评估了11个流行的模糊测试工具。", "result": "结果表明，模糊测试工具的性能根据程序特征及其强度而显著变化。", "conclusion": "模糊测试工具的性能会因程序特征及其强度而显著变化，这突出了在模糊测试评估中考虑程序特性的重要性。", "translation": "模糊测试是一种强大的软件测试技术，以其在识别软件漏洞方面的有效性而闻名。传统的模糊测试评估通常侧重于模糊测试工具在目标程序集上的整体性能，但很少有基准考虑细粒度的程序特征如何影响模糊测试的有效性。为了弥补这一空白，我们引入了一个新颖的基准，旨在生成具有可配置、细粒度程序特征的程序，以增强模糊测试评估。我们回顾了25项近期灰盒模糊测试研究，提取了7个与控制流和数据流相关的程序特征，这些特征可能影响模糊测试工具的性能。利用这些特征，我们生成了一个由10个细粒度可配置参数控制的153个程序组成的基准。我们使用该基准评估了11个流行的模糊测试工具。结果表明，模糊测试工具的性能根据程序特征及其强度而显著变化，这突出了在模糊测试评估中纳入程序特性的重要性。", "summary": "本文提出了一个新颖的基准测试，用于评估模糊测试工具，该基准通过生成具有可配置细粒度程序特征的程序来研究程序特征如何影响模糊测试的有效性。通过回顾25项研究并提取7个程序特征，作者构建了一个包含153个程序的基准，并使用它评估了11个流行的模糊测试工具。研究发现，模糊测试工具的性能显著依赖于程序特征及其强度，强调了在模糊测试评估中考虑程序特性的重要性。", "keywords": "模糊测试, 基准测试, 程序特征, 软件漏洞, 性能评估", "comments": "这项工作通过引入一个基于可配置程序特征的模糊测试基准，创新性地解决了传统模糊测试评估中缺乏对程序细粒度特性考虑的问题。其重要性在于，它提供了一个更精细的工具来理解和比较模糊测试工具的性能，超越了简单的整体性能评估。这有助于未来模糊测试工具的开发和改进，使其能更好地适应不同程序特性。"}}
{"id": "2506.15672", "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via Swarm Intelligence", "authors": ["Yao Zhang", "Chenyang Lin", "Shijie Tang", "Haokun Chen", "Shijie Zhou", "Yunpu Ma", "Volker Tresp"], "summary": "The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/.", "comment": "41 pages", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15672v1", "AI": {"title_translation": "SwarmAgentic：迈向基于群体智能的全自动化智能体系统生成", "tldr": "SwarmAgentic是一个受粒子群优化启发的框架，它能从零开始全自动化生成智能体系统，并优化其功能和协作，在开放式任务中显著优于现有基线。", "motivation": "现有智能体系统生成框架缺乏完全的自主性，无法从零开始生成智能体、自我优化智能体功能以及进行协作，这限制了它们的适应性和可扩展性。", "method": "我们提出了SwarmAgentic框架，它通过语言驱动的探索，从零开始构建智能体系统，并联合优化智能体功能和协作。受粒子群优化（PSO）的启发，SwarmAgentic维护一个候选系统种群，并通过反馈引导的更新来演化它们，以实现对系统级结构的有效搜索。", "result": "SwarmAgentic在六个真实世界的、开放式和探索性任务（包括高级规划、系统级协调和创造性推理）上进行了评估，仅给定任务描述和目标函数，它就超越了所有基线，在TravelPlanner基准测试中，相对于ADAS实现了+261.8%的相对改进。", "conclusion": "SwarmAgentic框架通过将群体智能与全自动化多智能体系统生成相结合，标志着可扩展和自主智能体系统设计迈出了重要一步，证明了在结构不受约束的任务中全自动化的有效性。", "translation": "大型语言模型的快速发展推动了智能体系统在决策、协调和任务执行方面的进步。然而，现有的智能体系统生成框架缺乏完全的自主性，缺少从零开始的智能体生成、自我优化的智能体功能和协作，这限制了它们的适应性和可扩展性。我们提出了SwarmAgentic，一个用于全自动化智能体系统生成的框架，它从零开始构建智能体系统，并通过语言驱动的探索，将智能体功能和协作作为相互依赖的组件进行联合优化。为了实现对系统级结构的有效搜索，SwarmAgentic维护一个候选系统种群，并通过反馈引导的更新来演化它们，这借鉴了粒子群优化（PSO）的灵感。我们在六个真实世界的、开放式和探索性任务上评估了我们的方法，这些任务涉及高级规划、系统级协调和创造性推理。仅给定任务描述和目标函数，SwarmAgentic就超越了所有基线，在TravelPlanner基准测试中，相对于ADAS实现了+261.8%的相对改进，突出了全自动化在结构不受约束的任务中的有效性。这个框架标志着可扩展和自主智能体系统设计迈出了重要一步，将群体智能与全自动化系统多智能体生成联系起来。我们的代码已在https://yaoz720.github.io/SwarmAgentic/公开。", "summary": "SwarmAgentic是一个创新的框架，旨在通过受粒子群优化启发的群体智能，实现智能体系统的全自动化生成。它克服了现有框架在从零生成、自我优化和协作方面的不足，能够从任务描述和目标函数出发，构建并优化智能体功能和协作。在多个开放式任务中，SwarmAgentic表现出色，显著超越了所有基线，证明了其在实现可扩展和自主智能体系统设计方面的巨大潜力。", "keywords": "智能体系统, 群体智能, 全自动化, 大语言模型, 粒子群优化", "comments": "该论文提出了一种新颖的方法，将群体智能（特别是受PSO启发）引入到智能体系统的全自动化生成中，解决了现有框架自主性不足的问题。其创新点在于从零开始构建系统并联合优化智能体功能和协作。在多个复杂任务上的显著性能提升凸显了其重要性，为未来可扩展的自主智能体系统设计提供了新的方向。"}}
{"id": "2506.14865", "title": "Efficient and Real-Time Motion Planning for Robotics Using Projection-Based Optimization", "authors": ["Xuemin Chi", "Hakan Girgin", "Tobias Löw", "Yangyang Xie", "Teng Xue", "Jihao Huang", "Cheng Hu", "Zhitao Liu", "Sylvain Calinon"], "summary": "Generating motions for robots interacting with objects of various shapes is a\ncomplex challenge, further complicated by the robot geometry and multiple\ndesired behaviors. While current robot programming tools (such as inverse\nkinematics, collision avoidance, and manipulation planning) often treat these\nproblems as constrained optimization, many existing solvers focus on specific\nproblem domains or do not exploit geometric constraints effectively. We propose\nan efficient first-order method, Augmented Lagrangian Spectral Projected\nGradient Descent (ALSPG), which leverages geometric projections via Euclidean\nprojections, Minkowski sums, and basis functions. We show that by using\ngeometric constraints rather than full constraints and gradients, ALSPG\nsignificantly improves real-time performance. Compared to second-order methods\nlike iLQR, ALSPG remains competitive in the unconstrained case. We validate our\nmethod through toy examples and extensive simulations, and demonstrate its\neffectiveness on a 7-axis Franka robot, a 6-axis P-Rob robot and a 1:10 scale\ncar in real-world experiments. Source codes, experimental data and videos are\navailable on the project webpage: https://sites.google.com/view/alspg-oc", "comment": "submitted to IROS 2025", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14865v1", "AI": {"title_translation": "基于投影优化的机器人高效实时运动规划", "tldr": "提出一种基于几何投影的高效一阶优化方法ALSPG，用于机器人实时运动规划，性能优于现有方法。", "motivation": "生成机器人与各种形状物体交互的运动是一个复杂的挑战，机器人几何形状和多个期望行为使其进一步复杂化。现有机器人编程工具（如逆运动学、碰撞避免和操作规划）通常将这些问题视为约束优化，但许多现有求解器专注于特定问题领域或未能有效利用几何约束，导致效率不足。", "method": "本文提出了一种高效的一阶方法——增广拉格朗日谱投影梯度下降（ALSPG）。该方法利用欧几里得投影、闵可夫斯基和以及基函数进行几何投影，并通过使用几何约束而非完整约束和梯度来显著提高实时性能。", "result": "ALSPG显著提升了实时性能，并且在无约束情况下与iLQR等二阶方法相比仍具竞争力。该方法通过玩具示例、大量仿真以及在7轴Franka机器人、6轴P-Rob机器人和1:10比例汽车上的真实世界实验得到了验证。", "conclusion": "ALSPG是一种有效且高效的机器人运动规划方法，能够显著提升实时性能，并通过利用几何投影优于现有方法，为机器人与复杂环境交互提供了新的解决方案。", "translation": "生成机器人与各种形状物体交互的运动是一个复杂的挑战，机器人几何形状和多个期望行为使其进一步复杂化。虽然当前的机器人编程工具（如逆运动学、碰撞避免和操作规划）通常将这些问题视为约束优化，但许多现有求解器专注于特定问题领域或未能有效利用几何约束。我们提出了一种高效的一阶方法，即增广拉格朗日谱投影梯度下降（ALSPG），该方法通过欧几里得投影、闵可夫斯基和以及基函数来利用几何投影。我们表明，通过使用几何约束而非完整约束和梯度，ALSPG显著提高了实时性能。与iLQR等二阶方法相比，ALSPG在无约束情况下仍具有竞争力。我们通过玩具示例和大量仿真验证了我们的方法，并在7轴Franka机器人、6轴P-Rob机器人和1:10比例汽车上进行了真实世界实验，证明了其有效性。源代码、实验数据和视频可在项目网页上获取：https://sites.google.com/view/alspg-oc", "summary": "本文提出了一种名为增广拉格朗日谱投影梯度下降（ALSPG）的高效一阶优化方法，用于解决机器人复杂运动规划中的实时性问题。ALSPG通过利用几何投影（如欧几里得投影和闵可夫斯基和）和基函数来有效处理几何约束，从而显著提高了实时性能。实验结果表明，与现有方法相比，ALSPG在机器人运动规划中表现出优越的效率和竞争力，尤其适用于需要实时响应的场景。", "keywords": "运动规划, 机器人, 优化, 投影, 实时", "comments": "本文创新性地提出了一种基于几何投影的一阶优化方法ALSPG，有效解决了机器人运动规划中的实时性挑战。通过利用几何约束而非传统完整约束和梯度，显著提升了算法效率，使其在实际机器人应用中具有重要意义。该方法的普适性和在多种机器人平台上的验证增强了其可靠性。"}}
{"id": "2506.14799", "title": "Analyzing Character Representation in Media Content using Multimodal Foundation Model: Effectiveness and Trust", "authors": ["Evdoxia Taka", "Debadyuti Bhattacharya", "Joanne Garde-Hansen", "Sanjay Sharma", "Tanaya Guha"], "summary": "Recent advances in AI has enabled automated analysis of complex media content\nat scale and generate actionable insights regarding character representation\nalong such dimensions as gender and age. Past work focused on quantifying\nrepresentation from audio/video/text using various ML models, but without\nhaving the audience in the loop. We ask, even if character distribution along\ndemographic dimensions are available, how useful are they to the general\npublic? Do they actually trust the numbers generated by AI models? Our work\naddresses these questions through a user study, while proposing a new AI-based\ncharacter representation and visualization tool. Our tool based on the\nContrastive Language Image Pretraining (CLIP) foundation model to analyze\nvisual screen data to quantify character representation across dimensions of\nage and gender. We also designed effective visualizations suitable for\npresenting such analytics to lay audience. Next, we conducted a user study to\nseek empirical evidence on the usefulness and trustworthiness of the\nAI-generated results for carefully chosen movies presented in the form of our\nvisualizations. We note that participants were able to understand the analytics\nfrom our visualization, and deemed the tool `overall useful'. Participants also\nindicated a need for more detailed visualizations to include more demographic\ncategories and contextual information of the characters. Participants' trust in\nAI-based gender and age models is seen to be moderate to low, although they\nwere not against the use of AI in this context. Our tool including code,\nbenchmarking, and data from the user study can be found here:\nhttps://anonymous.4open.science/r/Character-Representation-Media-FF7B", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14799v1", "AI": {"title_translation": "使用多模态基础模型分析媒体内容中的角色表征：有效性和信任", "tldr": "本研究通过用户研究评估了使用多模态基础模型（CLIP）分析媒体内容中角色（性别和年龄）表征的AI工具的有效性和信任度。结果显示工具被认为是“总体有用”的，但用户对AI模型的信任度中等偏低，并需要更详细的可视化。", "motivation": "过去的工作侧重于量化媒体内容中的角色表征，但没有考虑受众的参与。本研究旨在解决两个核心问题：AI生成的角色人口统计数据对公众有多大用处？公众是否真正信任AI模型生成的数据？", "method": "研究提出了一种新的基于AI的角色表征和可视化工具，该工具利用Contrastive Language Image Pretraining (CLIP) 基础模型分析视觉屏幕数据，以量化角色在年龄和性别维度上的表征。研究设计了适合普通观众的可视化效果。随后，通过用户研究，使用精心选择的电影和提出的可视化形式，收集了AI生成结果的有用性和可信度的实证证据。", "result": "参与者能够理解可视化分析结果，并认为该工具“总体有用”。参与者还指出需要更详细的可视化，以包含更多人口统计类别和角色上下文信息。参与者对基于AI的性别和年龄模型的信任度被认为是中等偏低，尽管他们不反对在此上下文中使用AI。", "conclusion": "尽管AI工具在分析媒体内容中的角色表征方面被认为是“总体有用”且易于理解，但公众对AI模型的信任度仍有待提高，并且需要更详细、更具上下文信息的可视化来满足用户需求。", "translation": "AI的最新进展使得大规模自动化分析复杂媒体内容并生成关于角色表征（如性别和年龄维度）的可操作见解成为可能。过去的工作侧重于使用各种机器学习模型从音频/视频/文本中量化表征，但没有让受众参与进来。我们不禁要问，即使角色在人口统计维度上的分布是可用的，它们对公众有多大用处？他们真的信任AI模型生成的数据吗？我们的工作通过一项用户研究解决了这些问题，同时提出了一种新的基于AI的角色表征和可视化工具。我们的工具基于对比语言图像预训练（CLIP）基础模型，用于分析视觉屏幕数据，以量化角色在年龄和性别维度上的表征。我们还设计了适合向普通受众呈现此类分析的有效可视化。接下来，我们进行了一项用户研究，旨在为以我们可视化形式呈现的精心选择的电影中AI生成结果的有用性和可信度寻求实证证据。我们注意到参与者能够理解我们可视化中的分析结果，并认为该工具“总体有用”。参与者还表示需要更详细的可视化，以包含更多人口统计类别和角色的上下文信息。参与者对基于AI的性别和年龄模型的信任度被认为是中等到低，尽管他们不反对在此上下文中使用AI。我们的工具，包括代码、基准测试和用户研究数据，可以在这里找到：https://anonymous.4open.science/r/Character-Representation-Media-FF7B", "summary": "本研究提出并评估了一个基于CLIP多模态基础模型的AI工具，用于分析媒体内容中角色（性别和年龄）的表征，并通过用户研究考察其有效性和用户信任度。研究发现，该工具的可视化易于理解，被用户评为“总体有用”，但用户对AI模型的信任度处于中低水平，并期望更详细、包含更多人口统计和上下文信息的可视化。", "keywords": "角色表征, 多模态模型, 用户研究, 有效性, 信任", "comments": "该论文的创新之处在于将用户纳入AI模型评估的循环中，特别是关注AI在媒体内容分析中生成的人物表征数据的“有用性”和“信任度”。这超越了单纯的技术量化，触及了AI应用中的社会接受度和人机交互的关键问题。其局限性在于用户对AI的信任度仍不高，且对可视化细节有更高要求，表明未来工作需在提升模型透明度和结果呈现方式上做更多努力。"}}
{"id": "2506.15168", "title": "Algorithmic resolution of crowd-sourced moderation on X in polarized settings across countries", "authors": ["Paul Bouchaud", "Pedro Ramaciotti"], "summary": "Social platforms increasingly transition from expert fact-checking to\ncrowd-sourced moderation, with X pioneering this shift through its Community\nNotes system, enabling users to collaboratively moderate misleading content. To\nresolve conflicting moderation, Community Notes learns a latent ideological\ndimension and selects notes garnering cross-partisan support. As this system,\ndesigned for and evaluated in the United States, is now deployed worldwide, we\nevaluate its operation across diverse polarization contexts. We analyze 1.9\nmillion moderation notes with 135 million ratings from 1.2 million users,\ncross-referencing ideological scaling data across 13 countries. Our results\nshow X's Community Notes effectively captures each country's main polarizing\ndimension but fails by design to moderate the most polarizing content, posing\npotential risks to civic discourse and electoral processes.", "comment": "46 pages", "cate": "cs.SI", "url": "http://arxiv.org/abs/2506.15168v1", "AI": {"title_translation": "X平台在各国两极分化背景下众包审核的算法解析", "tldr": "X平台众包审核系统（Community Notes）虽能捕捉各国极化维度，但无法有效审核最极化的内容，对公共讨论构成风险。", "motivation": "社交平台正日益从专家事实核查转向众包审核，X平台通过其社区笔记系统率先实现了这一转变，该系统已在全球部署，因此需要评估其在不同极化背景下的运作情况。", "method": "分析了来自120万用户的190万条审核笔记和1.35亿条评分，并交叉引用了13个国家的意识形态量表数据。", "result": "X平台的Community Notes系统能有效捕捉每个国家的主要极化维度，但由于设计原因未能审核最具极化性的内容。", "conclusion": "X平台的Community Notes系统在处理最具极化性的内容方面存在固有的局限性，这可能对公民话语和选举过程构成潜在风险。", "translation": "社交平台正日益从专家事实核查转向众包审核，X平台通过其社区笔记系统率先实现了这一转变，该系统使用户能够协作审核误导性内容。为了解决相互冲突的审核意见，社区笔记学习了一个潜在的意识形态维度，并选择获得跨党派支持的笔记。鉴于该系统在美国设计和评估后现已在全球部署，我们评估了其在不同极化背景下的运作情况。我们分析了来自120万用户的199万条审核笔记和1.35亿条评分，并交叉引用了13个国家的意识形态量表数据。我们的结果显示，X平台的社区笔记系统能够有效捕捉每个国家的主要极化维度，但由于设计原因未能审核最具极化性的内容，这可能对公民话语和选举过程构成潜在风险。", "summary": "本研究评估了X平台在全球范围内部署的众包审核系统Community Notes。该系统旨在通过学习意识形态维度来解决冲突的审核意见。研究分析了来自13个国家的数百万条审核笔记和评分数据，发现Community Notes能够有效识别各国的主要极化维度，但其设计缺陷使其无法有效审核最具极化性的内容，从而对公共讨论和选举过程构成风险。", "keywords": "众包审核, X平台, 社区笔记, 极化, 内容审核", "comments": "该研究揭示了X平台众包审核系统在国际部署中面临的挑战，特别是其在处理高度极化内容时的固有局限性。这对于理解大型社交平台内容审核机制的局限性及其对全球公民话语的影响具有重要意义。"}}
{"id": "2506.14937", "title": "Determinação Automática de Limiar de Detecção de Ataques em Redes de Computadores Utilizando Autoencoders", "authors": ["Luan Gonçalves Miranda", "Pedro Ivo da Cruz", "Murilo Bellezoni Loiola"], "summary": "Currently, digital security mechanisms like Anomaly Detection Systems using\nAutoencoders (AE) show great potential for bypassing problems intrinsic to the\ndata, such as data imbalance. Because AE use a non-trivial and nonstandardized\nseparation threshold to classify the extracted reconstruction error, the\ndefinition of this threshold directly impacts the performance of the detection\nprocess. Thus, this work proposes the automatic definition of this threshold\nusing some machine learning algorithms. For this, three algorithms were\nevaluated: the K-Nearst Neighbors, the K-Means and the Support Vector Machine.", "comment": "This work was accepted at SBrT 2022 (Brazilian Symposium on\n  Telecommunications and Signal Processing), though it was not included in the\n  official proceedings. in Portuguese language", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14937v1", "AI": {"title_translation": "使用自编码器自动确定计算机网络攻击检测阈值", "tldr": "该论文提出使用机器学习算法自动定义基于自编码器的异常检测系统中至关重要的分离阈值，以提高检测性能。", "motivation": "当前使用自编码器（AE）的异常检测系统在处理数据不平衡等固有问题方面潜力巨大，但由于其非标准化的分离阈值直接影响检测性能，因此需要一种自动定义该阈值的方法。", "method": "本研究提出使用机器学习算法自动定义自编码器（AE）的检测阈值。为此，评估了三种算法：K近邻（K-Nearest Neighbors）、K均值（K-Means）和支持向量机（Support Vector Machine）。", "result": "未在摘要中提及", "conclusion": "未在摘要中提及", "translation": "目前，使用自编码器（AE）的异常检测系统等数字安全机制在规避数据固有的问题（如数据不平衡）方面显示出巨大潜力。由于自编码器使用非平凡且非标准化的分离阈值来分类提取的重建误差，因此该阈值的定义直接影响检测过程的性能。因此，这项工作提出使用一些机器学习算法自动定义此阈值。为此，评估了三种算法：K近邻、K均值和支持向量机。", "summary": "本论文旨在解决基于自编码器的异常检测系统中关键的分离阈值定义问题，该问题直接影响检测性能。文章提出了一种利用机器学习算法自动确定此阈值的方法，并评估了K近邻、K均值和支持向量机这三种算法的适用性。", "keywords": "自编码器, 异常检测, 阈值, 机器学习, 网络安全", "comments": "该论文着眼于解决自编码器异常检测系统部署中的一个实际挑战，即阈值的自动设置。通过尝试自动化这一关键参数，它有望提高此类系统的鲁棒性和易用性，尤其是在数据不平衡的场景中。其创新点在于应用机器学习算法来动态设定此阈值。"}}
{"id": "2506.15073", "title": "Linear and Numerical SDoF Bounds of Active RIS-Assisted MIMO Wiretap Interference Channel", "authors": ["Su Linfan", "Miao Yuhang", "Song Yuxuan", "Zheng Shuo", "Zhang Tong", "Xu Yinfei", "Wang Shuai", "Li Na"], "summary": "The multiple-input multiple-output (MIMO) wiretap interference channel (IC)\nserves as a canonical model for information-theoretic security, where a\nmultiple-antenna eavesdropper attempts to intercept communications in a\ntwo-user MIMO IC system. The secure degrees-of-freedom (SDoF) of an active\nreconfigurable intelligent surface (RIS)-assisted MIMO wiretap IC is with\npractical interests but remains unexplored. In this paper, we establish both\nsum-SDoF lower and upper bounds through linear beamforming conditions and\nnumerical methods. Specifically, our proposed lower bound is derived from\ntransmission scheme design and corresponding solutions to the sum-SDoF\nmaximization problem, formulated by linear integer programming. The solutions\nto this optimization problem addresses RIS element allocation for leakage and\ninterference cancellation. The proposed upper bound is obtained by solving a\nnuclear norm minimization problem, leveraging the fact that nuclear norm serves\nas a convex relaxation of the rank function. For symmetry antenna\nconfigurations, we derive a closed-form lower bound. Extensive numerical\nsimulations show that our proposed lower and upper bounds coincide across many\nantenna configurations, and our proposed lower bound outperforms the existing\nbenchmark.", "comment": "11 pages, 2 figures, submitted to IEEE OJCOMS", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15073v1", "AI": {"title_translation": "有源RIS辅助MIMO窃听干扰信道的线性和数值SDoF界限", "tldr": "本文研究了有源RIS辅助MIMO窃听干扰信道的安全自由度（SDoF），通过线性和数值方法建立了和SDoF的上下界，并证明了其有效性。", "motivation": "有源可重构智能表面（RIS）辅助的多输入多输出（MIMO）窃听干扰信道的安全自由度（SDoF）具有实际意义但尚未被探索。", "method": "通过线性波束成形条件和数值方法建立了和SDoF的上下界。具体地，下界通过传输方案设计和线性整数规划公式化的和SDoF最大化问题的解导出，该解解决了RIS元件分配以消除泄漏和干扰。上界通过解决核范数最小化问题获得。对于对称天线配置，推导了闭式下界。", "result": "提出的下界和上界在许多天线配置下吻合，且提出的下界优于现有基准。", "conclusion": "本文成功建立了有源RIS辅助MIMO窃听干扰信道的和SDoF上下界，并通过仿真验证了其有效性和优越性。", "translation": "多输入多输出（MIMO）窃听干扰信道（IC）是信息论安全性的一个典型模型，其中多天线窃听者试图截获双用户MIMO IC系统中的通信。有源可重构智能表面（RIS）辅助的MIMO窃听IC的安全自由度（SDoF）具有实际意义但尚未被探索。在本文中，我们通过线性波束成形条件和数值方法建立了总SDoF的下界和上界。具体来说，我们提出的下界是通过传输方案设计和线性整数规划公式化的总SDoF最大化问题的相应解导出的。该优化问题的解解决了RIS元件分配以消除泄漏和干扰。提出的上界通过解决核范数最小化问题获得，利用了核范数作为秩函数的凸松弛。对于对称天线配置，我们推导了闭式下界。大量的数值模拟表明，我们提出的下界和上界在许多天线配置下吻合，并且我们提出的下界优于现有基准。", "summary": "本文研究了有源RIS辅助MIMO窃听干扰信道的安全自由度（SDoF）。作者首次通过线性波束成形和数值方法建立了和SDoF的上下界。下界通过线性整数规划解决的传输方案设计获得，旨在优化RIS元件分配以消除泄漏和干扰。上界则通过核范数最小化问题导出。仿真结果表明，所提出的界限在多种天线配置下高度吻合，并且所提出的下界性能优于现有方法。", "keywords": "SDoF, 有源RIS, MIMO窃听信道, 上下界, 核范数最小化", "comments": "本文首次探讨了有源RIS辅助MIMO窃听干扰信道的SDoF，填补了该领域的空白。其创新之处在于结合了线性整数规划和核范数最小化来推导SDoF的上下界，并解决了RIS元件的优化分配问题。研究结果对提升无线通信安全具有重要意义。"}}
{"id": "2506.14816", "title": "A Hybrid ConvNeXt-EfficientNet AI Solution for Precise Falcon Disease Detection", "authors": ["Alavikunhu Panthakkan", "Zubair Medammal", "S M Anzar", "Fatma Taher", "Hussain Al-Ahmad"], "summary": "Falconry, a revered tradition involving the training and hunting with\nfalcons, requires meticulous health surveillance to ensure the health and\nsafety of these prized birds, particularly in hunting scenarios. This paper\npresents an innovative method employing a hybrid of ConvNeXt and EfficientNet\nAI models for the classification of falcon diseases. The study focuses on\naccurately identifying three conditions: Normal, Liver Disease and\n'Aspergillosis'. A substantial dataset was utilized for training and validating\nthe model, with an emphasis on key performance metrics such as accuracy,\nprecision, recall, and F1-score. Extensive testing and analysis have shown that\nour concatenated AI model outperforms traditional diagnostic methods and\nindividual model architectures. The successful implementation of this hybrid AI\nmodel marks a significant step forward in precise falcon disease detection and\npaves the way for future developments in AI-powered avian healthcare solutions.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14816v1", "AI": {"title_translation": "一种用于精确猎鹰疾病检测的混合ConvNeXt-EfficientNet人工智能解决方案", "tldr": "该研究提出了一种结合ConvNeXt和EfficientNet的混合AI模型，用于精确检测猎鹰的正常、肝病和曲霉病。该模型在性能上优于传统方法和单一模型，为AI驱动的禽类医疗保健发展奠定了基础。", "motivation": "为了确保猎鹰的健康和安全，尤其是在狩猎场景中，需要对这些珍贵鸟类进行细致的健康监测。", "method": "本研究提出了一种创新的方法，采用ConvNeXt和EfficientNet AI模型的混合体对猎鹰疾病进行分类。研究重点是准确识别三种情况：正常、肝病和曲霉病。利用大量数据集对模型进行训练和验证，并强调准确性、精确度、召回率和F1分数等关键性能指标。", "result": "广泛的测试和分析表明，所提出的混合AI模型优于传统的诊断方法和单独的模型架构。", "conclusion": "这种混合AI模型的成功实施标志着猎鹰疾病精确检测方面迈出了重要一步，并为未来AI驱动的禽类医疗保健解决方案的发展铺平了道路。", "translation": "猎鹰捕猎是一项受人尊敬的传统，涉及训练和使用猎鹰进行狩猎，需要细致的健康监测，以确保这些珍贵鸟类的健康和安全，特别是在狩猎场景中。本文提出了一种创新的方法，采用ConvNeXt和EfficientNet AI模型的混合体对猎鹰疾病进行分类。研究重点是准确识别三种情况：正常、肝病和曲霉病。利用大量数据集对模型进行训练和验证，并强调准确性、精确度、召回率和F1分数等关键性能指标。广泛的测试和分析表明，我们提出的混合AI模型优于传统的诊断方法和单独的模型架构。这种混合AI模型的成功实施标志着猎鹰疾病精确检测方面迈出了重要一步，并为未来AI驱动的禽类医疗保健解决方案的发展铺平了道路。", "summary": "本研究提出了一种结合ConvNeXt和EfficientNet的混合人工智能模型，旨在精确检测猎鹰的正常、肝病和曲霉病。该模型利用大量数据集进行训练和验证，并在准确性、精确度、召回率和F1分数等指标上表现出色，超越了传统的诊断方法和单一模型架构。这项工作为AI在禽类医疗保健领域的应用开辟了新途径。", "keywords": "猎鹰疾病检测, ConvNeXt, EfficientNet, 混合AI模型, 禽类医疗保健", "comments": "该论文的创新点在于结合了ConvNeXt和EfficientNet两种先进的深度学习模型，形成混合架构以提高疾病检测的准确性。其重要性在于为猎鹰这种特殊且珍贵的动物提供了更精确、高效的疾病诊断方案，有望推动AI在动物医疗保健领域的实际应用。"}}
{"id": "2506.15316", "title": "J3DAI: A tiny DNN-Based Edge AI Accelerator for 3D-Stacked CMOS Image Sensor", "authors": ["Benoit Tain", "Raphael Millet", "Romain Lemaire", "Michal Szczepanski", "Laurent Alacoque", "Emmanuel Pluchart", "Sylvain Choisnet", "Rohit Prasad", "Jerome Chossat", "Pascal Pierunek", "Pascal Vivet", "Sebastien Thuries"], "summary": "This paper presents J3DAI, a tiny deep neural network-based hardware\naccelerator for a 3-layer 3D-stacked CMOS image sensor featuring an artificial\nintelligence (AI) chip integrating a Deep Neural Network (DNN)-based\naccelerator. The DNN accelerator is designed to efficiently perform neural\nnetwork tasks such as image classification and segmentation. This paper focuses\non the digital system of J3DAI, highlighting its Performance-Power-Area (PPA)\ncharacteristics and showcasing advanced edge AI capabilities on a CMOS image\nsensor. To support hardware, we utilized the Aidge comprehensive software\nframework, which enables the programming of both the host processor and the DNN\naccelerator. Aidge supports post-training quantization, significantly reducing\nmemory footprint and computational complexity, making it crucial for deploying\nmodels on resource-constrained hardware like J3DAI. Our experimental results\ndemonstrate the versatility and efficiency of this innovative design in the\nfield of edge AI, showcasing its potential to handle both simple and\ncomputationally intensive tasks. Future work will focus on further optimizing\nthe architecture and exploring new applications to fully leverage the\ncapabilities of J3DAI. As edge AI continues to grow in importance, innovations\nlike J3DAI will play a crucial role in enabling real-time, low-latency, and\nenergy-efficient AI processing at the edge.", "comment": "Preprint from ISLPED 2025. 979-8-3315-2710-5/25/$31.00\n  \\c{opyright}2025 IEEE", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15316v1", "AI": {"title_translation": "J3DAI：一种用于3D堆叠CMOS图像传感器的微型基于DNN的边缘AI加速器", "tldr": "J3DAI是一种用于3D堆叠CMOS图像传感器的微型DNN边缘AI加速器，具有出色的性能、功耗和面积特性，并通过Aidge软件框架支持量化，可高效处理边缘AI任务。", "motivation": "随着边缘AI重要性的日益增长，需要实现实时、低延迟和高能效的AI处理。本研究旨在开发一种微型、高效的硬件加速器来满足这些需求。", "method": "本文提出J3DAI，一个基于DNN的硬件加速器，集成在3层3D堆叠CMOS图像传感器中。设计重点关注其数字系统，并利用Aidge综合软件框架进行编程和后训练量化，以减少内存占用和计算复杂性。", "result": "J3DAI在性能、功耗和面积(PPA)方面表现出色，展示了先进的边缘AI能力。实验结果证明了其在边缘AI领域的多功能性和效率，能够处理简单和计算密集型任务。", "conclusion": "J3DAI作为一种创新的边缘AI加速器设计，在实现实时、低延迟和高能效的边缘AI处理方面具有关键作用和巨大潜力。", "translation": "本文介绍了J3DAI，这是一种基于深度神经网络的微型硬件加速器，专为三层3D堆叠CMOS图像传感器设计，该传感器集成了一个基于深度神经网络（DNN）加速器的人工智能（AI）芯片。该DNN加速器旨在高效执行图像分类和分割等神经网络任务。本文重点介绍J3DAI的数字系统，强调其性能-功耗-面积（PPA）特性，并展示了CMOS图像传感器上的先进边缘AI能力。为了支持硬件，我们使用了Aidge综合软件框架，该框架能够对主机处理器和DNN加速器进行编程。Aidge支持训练后量化，显著减少内存占用和计算复杂性，这对于在J3DAI等资源受限硬件上部署模型至关重要。我们的实验结果证明了这种创新设计在边缘AI领域的多功能性和效率，展示了其处理简单和计算密集型任务的潜力。未来的工作将侧重于进一步优化架构并探索新的应用，以充分利用J3DAI的能力。随着边缘AI重要性的持续增长，J3DAI等创新将在实现边缘实时、低延迟和高能效的AI处理方面发挥关键作用。", "summary": "本文介绍了J3DAI，这是一种专为3D堆叠CMOS图像传感器设计的微型DNN边缘AI加速器。它旨在高效执行图像分类和分割等任务，并优化了性能、功耗和面积。通过结合Aidge软件框架实现后训练量化，J3DAI在资源受限的边缘设备上展现了处理简单及计算密集型任务的多功能性和效率。", "keywords": "边缘AI, DNN加速器, 3D堆叠CMOS, 后训练量化, 图像传感器", "comments": "J3DAI的创新之处在于将微型DNN加速器直接集成到3D堆叠CMOS图像传感器中，实现了高度集成的边缘AI解决方案。其结合Aidge软件框架进行后训练量化，有效解决了边缘设备资源受限的挑战。该设计在性能、功耗和面积方面的优化，使其在实时、低延迟和高能效的边缘AI应用中具有重要意义。"}}
{"id": "2506.15050", "title": "Truncated Proximal Policy Optimization", "authors": ["Tiantian Fan", "Lingjun Liu", "Yu Yue", "Jiaze Chen", "Chengyi Wang", "Qiying Yu", "Chi Zhang", "Zhiqi Lin", "Ruofei Zhu", "Yufeng Yuan", "Xiaochen Zuo", "Bole Ma", "Mofan Zhang", "Gaohong Liu", "Ru Zhang", "Haotian Zhou", "Cong Xie", "Ruidong Zhu", "Zhi Zhang", "Xin Liu", "Mingxuan Wang", "Lin Yan", "Yonghui Wu"], "summary": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated\nexceptional reasoning capabilities across scientific and professional tasks by\ngenerating long chains-of-thought (CoT). As a crucial component for developing\nthese reasoning models, reinforcement learning (RL), exemplified by Proximal\nPolicy Optimization (PPO) and its variants, allows models to learn through\ntrial and error. However, PPO can be time-consuming due to its inherent\non-policy nature, which is further exacerbated by increasing response lengths.\nIn this work, we propose Truncated Proximal Policy Optimization (T-PPO), a\nnovel extension to PPO that improves training efficiency by streamlining policy\nupdate and length-restricted response generation. T-PPO mitigates the issue of\nlow hardware utilization, an inherent drawback of fully synchronized\nlong-generation procedures, where resources often sit idle during the waiting\nperiods for complete rollouts. Our contributions are two-folds. First, we\npropose Extended Generalized Advantage Estimation (EGAE) for advantage\nestimation derived from incomplete responses while maintaining the integrity of\npolicy learning. Second, we devise a computationally optimized mechanism that\nallows for the independent optimization of the policy and value models. By\nselectively filtering prompt and truncated tokens, this mechanism reduces\nredundant computations and accelerates the training process without sacrificing\nconvergence performance. We demonstrate the effectiveness and efficacy of T-PPO\non AIME 2024 with a 32B base model. The experimental results show that T-PPO\nimproves the training efficiency of reasoning LLMs by up to 2.5x and\noutperforms its existing competitors.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15050v1", "AI": {"title_translation": "截断近端策略优化", "tldr": "T-PPO是一种新的PPO变体，通过流线化策略更新和限制长度的响应生成，显著提高了大型语言模型（LLMs）的训练效率，最高可达2.5倍。", "motivation": "大型语言模型（LLMs）在推理任务中表现出色，但基于PPO的强化学习训练因其在线特性和响应长度增加而耗时。现有方法在处理长生成过程时，存在硬件利用率低的问题（资源闲置）。", "method": "本文提出了截断近端策略优化（T-PPO）。主要贡献有两点：1. 提出扩展广义优势估计（EGAE），用于从不完整响应中进行优势估计，同时保持策略学习的完整性。2. 设计了一种计算优化的机制，允许策略模型和价值模型独立优化，通过选择性过滤提示和截断的token，减少冗余计算并加速训练过程，同时不牺牲收敛性能。", "result": "在AIME 2024数据集上，使用32B基础模型进行实验，T-PPO将推理LLMs的训练效率提高了2.5倍，并且优于现有竞争对手。", "conclusion": "T-PPO通过引入EGAE和优化的独立模型优化机制，有效解决了PPO在长响应LLMs训练中的效率问题，显著提升了训练速度并保持了性能。", "translation": "最近，测试时扩展的大型语言模型（LLMs）通过生成长链式思考（CoT）在科学和专业任务中展示了卓越的推理能力。作为开发这些推理模型的关键组成部分，以近端策略优化（PPO）及其变体为代表的强化学习（RL）允许模型通过试错学习。然而，PPO由于其固有的在线性质可能非常耗时，而响应长度的增加进一步加剧了这一问题。在这项工作中，我们提出了截断近端策略优化（T-PPO），这是PPO的一种新颖扩展，通过简化策略更新和限制长度的响应生成来提高训练效率。T-PPO缓解了硬件利用率低的问题，这是完全同步长生成过程固有的缺点，在等待完整rollout期间资源经常处于闲置状态。我们的贡献是双重的。首先，我们提出了扩展广义优势估计（EGAE），用于从不完整响应中进行优势估计，同时保持策略学习的完整性。其次，我们设计了一种计算优化的机制，允许策略和价值模型独立优化。通过选择性过滤提示和截断的token，该机制减少了冗余计算，并在不牺牲收敛性能的情况下加速了训练过程。我们在AIME 2024上使用32B基础模型展示了T-PPO的有效性和功效。实验结果表明，T-PPO将推理LLMs的训练效率提高了2.5倍，并优于现有竞争对手。", "summary": "本文提出了一种名为截断近端策略优化（T-PPO）的新型PPO扩展，旨在解决大型语言模型（LLMs）在长链式思考生成中，PPO训练效率低下的问题。T-PPO通过引入扩展广义优势估计（EGAE）来处理不完整响应，并设计了独立的策略和价值模型优化机制，有效减少了冗余计算和硬件空闲时间。实验证明，T-PPO显著提高了推理LLMs的训练效率，最高可达2.5倍，并超越了现有方法。", "keywords": "强化学习, 大型语言模型, 近端策略优化, 训练效率, 截断", "comments": "T-PPO在解决LLMs长序列训练中PPO效率低下的问题上具有创新性。它通过引入针对不完整响应的优势估计和独立模型优化，有效提升了训练速度，这对于大型模型训练成本高昂的现状具有重要意义。该方法有望加速LLMs在复杂推理任务上的RLHF（Reinforcement Learning from Human Feedback）过程。"}}
{"id": "2506.15405", "title": "Simulation of parametrized cardiac electrophysiology in three dimensions using physics-informed neural networks", "authors": ["Roshan Antony Gomez", "Julien Stöcker", "Barış Cansız", "Michael Kaliske"], "summary": "Physics-informed neural networks (PINNs) are extensively used to represent\nvarious physical systems across multiple scientific domains. The same can be\nsaid for cardiac electrophysiology, wherein fully-connected neural networks\n(FCNNs) have been employed to predict the evolution of an action potential in a\n2D space following the two-parameter phenomenological Aliev-Panfilov (AP)\nmodel. In this paper, the training behaviour of PINNs is investigated to\ndetermine optimal hyperparameters to predict the electrophysiological activity\nof the myocardium in 3D according to the AP model, with the inclusion of\nboundary and material parameters. An FCNN architecture is employed with the\ngoverning partial differential equations in their strong form, which are scaled\nconsistently with normalization of network inputs. The finite element (FE)\nmethod is used to generate training data for the network. Numerical examples\nwith varying spatial dimensions and parameterizations are generated using the\ntrained models. The network predicted fields for both the action potential and\nthe recovery variable are compared with the respective FE simulations. Network\nlosses are weighed with individual scalar values. Their effect on training and\nprediction is studied to arrive at a method of controlling losses during\ntraining.", "comment": null, "cate": "cs.CE", "url": "http://arxiv.org/abs/2506.15405v1", "AI": {"title_translation": "基于物理信息神经网络的三维参数化心脏电生理模拟", "tldr": "本文研究了物理信息神经网络（PINNs）在三维心脏电生理模拟中的训练行为和超参数优化，并与有限元模拟进行比较。", "motivation": "现有的心脏电生理预测多在二维空间进行，本文旨在将物理信息神经网络（PINNs）应用于三维心脏电生理活动预测，并优化其训练行为和超参数，以纳入边界和材料参数。", "method": "采用基于强形式偏微分方程的全连接神经网络（FCNN）架构，并通过有限元（FE）方法生成训练数据。研究了PINNs的训练行为，以确定预测三维心肌电生理活动（基于Aliev-Panfilov模型）的最佳超参数。通过对网络损失进行加权来研究其对训练和预测的影响，以期找到一种在训练期间控制损失的方法。", "result": "训练后的模型能够生成具有不同空间维度和参数化的数值示例。网络预测的动作电位和恢复变量场与各自的有限元模拟结果进行了比较。研究了加权损失对训练和预测的影响，并提出了一种控制训练期间损失的方法。", "conclusion": "本文通过对物理信息神经网络（PINNs）训练行为的研究，成功实现了三维心脏电生理活动的模拟，并提出了一种控制训练损失的方法，其预测结果与有限元模拟结果进行了有效比较。", "translation": "物理信息神经网络（PINNs）被广泛用于表示多个科学领域中的各种物理系统。心脏电生理学领域亦是如此，其中全连接神经网络（FCNNs）已被用于根据双参数现象学Aliev-Panfilov（AP）模型预测二维空间中动作电位的演变。在本文中，研究了PINNs的训练行为，以确定预测三维心肌电生理活动（根据AP模型，并包含边界和材料参数）的最佳超参数。采用了一种FCNN架构，其中包含了强形式的控制偏微分方程，并与网络输入的归一化保持一致缩放。有限元（FE）方法用于为网络生成训练数据。使用训练好的模型生成了具有不同空间维度和参数化的数值示例。将动作电位和恢复变量的网络预测场与各自的有限元模拟进行了比较。网络损失通过单独的标量值进行加权。研究了它们对训练和预测的影响，以期找到一种在训练期间控制损失的方法。", "summary": "本文研究了物理信息神经网络（PINNs）在三维心脏电生理模拟中的应用。研究人员利用全连接神经网络（FCNNs）和强形式偏微分方程，并结合有限元（FE）方法生成的训练数据，预测了心肌在Aliev-Panfilov（AP）模型下的三维电生理活动，并考虑了边界和材料参数。通过比较PINN预测结果与FE模拟，并研究了加权损失对训练和预测的影响，该研究旨在优化PINNs在复杂生物物理系统中的应用，并提出一种控制训练损失的方法。", "keywords": "心脏电生理学, 物理信息神经网络, 三维模拟, Aliev-Panfilov模型, 有限元方法", "comments": "本文将物理信息神经网络应用于复杂的三维心脏电生理模拟，这是一个重要的进步，因为它扩展了PINNs在生物医学领域的应用范围，并解决了从2D到3D的挑战。通过结合强形式偏微分方程和有限元数据生成训练，以及研究损失加权，该研究为提高PINNs在复杂系统模拟中的准确性和稳定性提供了见解。"}}
{"id": "2506.14981", "title": "Zarr-Based Chunk-Level Cumulative Sums in Reduced Dimensions", "authors": ["Hailiang Zhang", "Dieu My T. Nguyen", "Christine Smit", "Mahabal Hegde"], "summary": "Data analysis on massive multi-dimensional data, such as high-resolution\nlarge-region time averaging or area averaging for geospatial data, often\ninvolves calculations over a significant number of data points. While\nperforming calculations in scalable and flexible distributed or cloud\nenvironments is a viable option, a full scan of large data volumes still serves\nas a computationally intensive bottleneck, leading to significant cost. This\npaper introduces a generic and comprehensive method to address these\ncomputational challenges. This method generates a small, size-tunable\nsupplementary dataset that stores the cumulative sums along specific subset\ndimensions on top of the raw data. This minor addition unlocks rapid and cheap\nhigh-resolution large-region data analysis, making calculations over large\nnumbers of data points feasible with small instances or even microservices in\nthe cloud. This method is general-purpose, but is particularly well-suited for\ndata stored in chunked, cloud-optimized formats and for services running in\ndistributed or cloud environments. We present a Zarr extension proposal to\nintegrate the specifications of this method and facilitate its straightforward\nimplementation in general-purpose software applications. Benchmark tests\ndemonstrate that this method, implemented in Amazon Web services (AWS),\nsignificantly outperforms the brute-force approach used in on-premises\nservices. With just 5% supplemental storage, this method achieves a performance\nthat is 3-4 orders of magnitude (~10,000 times) faster than the brute-force\napproach, while incurring significantly reduced computational costs.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.14981v1", "AI": {"title_translation": "基于Zarr的降维分块级累积和", "tldr": "本文提出了一种基于Zarr的通用方法，通过生成一个小的辅助数据集来存储特定维度上的累积和，从而显著加速大规模多维数据的计算分析，比暴力方法快10,000倍，且成本更低。", "motivation": "大规模多维数据分析（如高分辨率大区域时间或面积平均）涉及大量数据点计算，在分布式或云环境中，对大数据量进行完整扫描是计算密集型瓶颈，导致高昂成本。", "method": "本文提出一种通用且全面的方法：生成一个小的、大小可调的辅助数据集，用于在原始数据之上存储沿特定子集维度的累积和。该方法特别适用于以分块、云优化格式存储的数据和在分布式或云环境中运行的服务。提出了一个Zarr扩展提案以集成该方法的规范。", "result": "在Amazon Web Services (AWS) 中实施的基准测试表明，该方法显著优于本地服务中使用的暴力方法。仅需5%的额外存储，该方法就能实现比暴力方法快3-4个数量级（约10,000倍）的性能，同时显著降低计算成本。", "conclusion": "通过在原始数据之上添加一个小的辅助数据集来存储特定维度上的累积和，可以极大地加速大规模多维数据的分析，显著降低计算成本，并使其在小型实例或微服务上可行。", "translation": "对海量多维数据进行数据分析，例如地理空间数据的高分辨率大区域时间平均或面积平均，通常涉及大量数据点的计算。虽然在可扩展和灵活的分布式或云环境中执行计算是一个可行的选择，但对大数据量进行全面扫描仍然是一个计算密集型瓶颈，导致显著的成本。本文介绍了一种通用且全面的方法来解决这些计算挑战。该方法在原始数据之上生成一个小的、大小可调的辅助数据集，用于存储沿特定子集维度的累积和。这一小小的补充解锁了快速且廉价的高分辨率大区域数据分析，使得在云中使用小型实例甚至微服务对大量数据点进行计算成为可能。该方法是通用的，但特别适用于以分块、云优化格式存储的数据以及在分布式或云环境中运行的服务。我们提出了一个Zarr扩展提案，以整合该方法的规范，并促进其在通用软件应用程序中的直接实现。基准测试表明，在Amazon Web Services (AWS) 中实施的该方法，显著优于本地服务中使用的暴力方法。仅需5%的辅助存储，该方法就能实现比暴力方法快3-4个数量级（约10,000倍）的性能，同时显著降低计算成本。", "summary": "本文针对大规模多维数据分析中全扫描导致的计算瓶颈和高成本问题，提出了一种基于Zarr的通用方法。该方法通过创建小的辅助数据集存储特定维度上的累积和，从而实现对大量数据点的快速、低成本分析。基准测试显示，与传统暴力方法相比，该方法仅需5%额外存储即可将性能提升10,000倍，并大幅降低计算成本，特别适用于云环境中的分块数据。", "keywords": "Zarr, 累积和, 多维数据, 云计算, 性能优化", "comments": "该论文的创新点在于提出了一种通用的、基于Zarr的辅助数据集方法，通过预计算和存储累积和来避免大规模数据全扫描，显著提升了多维数据分析的效率和成本效益。其重要性体现在为处理云环境中海量数据提供了高效的解决方案，并且提出了Zarr扩展提案，有利于其在通用软件中的推广和应用。该方法对大数据分析、地球空间数据处理等领域具有重要意义。"}}
{"id": "2506.14997", "title": "Hypothesis Testing for Quantifying LLM-Human Misalignment in Multiple Choice Settings", "authors": ["Harbin Hong", "Sebastian Caldas", "Liu Leqi"], "summary": "As Large Language Models (LLMs) increasingly appear in social science\nresearch (e.g., economics and marketing), it becomes crucial to assess how well\nthese models replicate human behavior. In this work, using hypothesis testing,\nwe present a quantitative framework to assess the misalignment between\nLLM-simulated and actual human behaviors in multiple-choice survey settings.\nThis framework allows us to determine in a principled way whether a specific\nlanguage model can effectively simulate human opinions, decision-making, and\ngeneral behaviors represented through multiple-choice options. We applied this\nframework to a popular language model for simulating people's opinions in\nvarious public surveys and found that this model is ill-suited for simulating\nthe tested sub-populations (e.g., across different races, ages, and incomes)\nfor contentious questions. This raises questions about the alignment of this\nlanguage model with the tested populations, highlighting the need for new\npractices in using LLMs for social science studies beyond naive simulations of\nhuman subjects.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.14997v1", "AI": {"title_translation": "假设检验用于量化多项选择场景中LLM与人类行为的不一致性", "tldr": "本研究提出了一个基于假设检验的定量框架，用于评估多项选择场景中LLM模拟行为与实际人类行为之间的不一致性。研究发现，一个流行的LLM不适合模拟针对有争议问题的特定子人群（如不同种族、年龄和收入）的观点。", "motivation": "随着大型语言模型（LLM）在社会科学研究中日益普及，评估这些模型复制人类行为的程度变得至关重要。", "method": "提出了一个基于假设检验的定量框架，用于评估LLM模拟行为与实际人类行为在多项选择调查环境中的不一致性。该框架旨在以有原则的方式确定特定LLM是否能有效模拟人类观点、决策和行为。", "result": "将该框架应用于一个流行的语言模型，发现该模型不适合模拟针对有争议问题的受测子人群（例如，跨越不同种族、年龄和收入）的观点。", "conclusion": "测试的LLM与受测人群之间存在不一致性，强调了在社会科学研究中使用LLM时，需要超越简单模拟人类受试者的新实践。", "translation": "随着大型语言模型（LLM）越来越多地出现在社会科学研究中（例如经济学和市场营销），评估这些模型复制人类行为的程度变得至关重要。在这项工作中，我们利用假设检验，提出了一个量化框架，用于评估多项选择调查环境中LLM模拟行为与实际人类行为之间的不一致性。该框架使我们能够以一种有原则的方式确定特定的语言模型是否能有效模拟通过多项选择选项所代表的人类观点、决策和一般行为。我们将此框架应用于一个流行的语言模型，用于模拟各种公共调查中人们的观点，发现该模型不适合模拟针对有争议问题的受测子人群（例如，跨越不同种族、年龄和收入）。这引发了关于该语言模型与受测人群之间一致性的疑问，强调了在社会科学研究中使用LLM时需要采用新的实践，而不仅仅是简单地模拟人类受试者。", "summary": "本论文引入了一个基于假设检验的定量框架，用于评估大型语言模型（LLM）模拟行为与实际人类行为在多项选择调查中的不一致性。通过将该框架应用于一个流行的LLM，研究发现其不适合模拟针对有争议问题的不同子人群的观点，从而强调了在社会科学研究中使用LLM时需要更先进的方法。", "keywords": "LLM, 人类行为, 不一致性, 假设检验, 社会科学", "comments": "这篇论文解决了LLM在社会科学研究中可靠性的一个关键问题。利用假设检验提供了一种严谨的定量方法。研究发现LLM可能无法准确模拟针对有争议问题的不同子人群，这一点非常重要，突出了其局限性以及超越简单模拟的更复杂应用的需求。"}}
{"id": "2506.15082", "title": "Make Your AUV Adaptive: An Environment-Aware Reinforcement Learning Framework For Underwater Tasks", "authors": ["Yimian Ding", "Jingzehua Xu", "Guanwen Xie", "Shuai Zhang", "Yi Li"], "summary": "This study presents a novel environment-aware reinforcement learning (RL)\nframework designed to augment the operational capabilities of autonomous\nunderwater vehicles (AUVs) in underwater environments. Departing from\ntraditional RL architectures, the proposed framework integrates an\nenvironment-aware network module that dynamically captures flow field data,\neffectively embedding this critical environmental information into the state\nspace. This integration facilitates real-time environmental adaptation,\nsignificantly enhancing the AUV's situational awareness and decision-making\ncapabilities. Furthermore, the framework incorporates AUV structure\ncharacteristics into the optimization process, employing a large language model\n(LLM)-based iterative refinement mechanism that leverages both environmental\nconditions and training outcomes to optimize task performance. Comprehensive\nexperimental evaluations demonstrate the framework's superior performance,\nrobustness and adaptability.", "comment": "This paper has been accepted by IROS 2025", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15082v1", "AI": {"title_translation": "让你的AUV适应环境：一种用于水下任务的环境感知强化学习框架", "tldr": "本研究提出了一种新颖的环境感知强化学习框架，通过整合环境感知网络模块和基于LLM的迭代优化机制，显著增强了AUV在水下环境中的适应性、态势感知和决策能力。", "motivation": "提高自主水下航行器（AUV）在水下环境中的操作能力，解决传统强化学习架构在环境适应性方面的不足。", "method": "本框架整合了一个环境感知网络模块，动态捕获流场数据并将其嵌入到状态空间中，以实现实时环境适应。此外，该框架将AUV结构特性纳入优化过程，并采用基于大型语言模型（LLM）的迭代优化机制，利用环境条件和训练结果来优化任务性能。", "result": "综合实验评估表明，该框架具有卓越的性能、鲁棒性和适应性。", "conclusion": "该环境感知强化学习框架能够显著提升AUV在复杂水下环境中的适应性、态势感知和决策能力，并展现出优异的性能和鲁棒性。", "translation": "本研究提出了一种新颖的环境感知强化学习（RL）框架，旨在增强自主水下航行器（AUV）在水下环境中的操作能力。与传统RL架构不同，所提出的框架集成了一个环境感知网络模块，该模块动态捕获流场数据，有效地将这些关键环境信息嵌入到状态空间中。这种集成促进了实时环境适应，显著增强了AUV的态势感知和决策能力。此外，该框架将AUV结构特性纳入优化过程，采用基于大型语言模型（LLM）的迭代优化机制，利用环境条件和训练结果来优化任务性能。综合实验评估表明，该框架具有卓越的性能、鲁棒性和适应性。", "summary": "本研究提出了一种创新的环境感知强化学习框架，旨在提升自主水下航行器（AUV）在水下环境中的性能。该框架通过集成一个环境感知网络模块，将流场数据动态嵌入状态空间，增强AUV的实时环境适应性、态势感知和决策能力。同时，它将AUV结构特性纳入优化，并利用基于大型语言模型（LLM）的迭代机制优化任务表现。实验结果验证了该框架在性能、鲁棒性和适应性方面的优越性。", "keywords": "AUV, 强化学习, 环境感知, 大型语言模型, 水下任务", "comments": "该研究的创新之处在于将环境感知网络模块和基于LLM的迭代优化机制引入强化学习框架，有效解决了AUV在复杂水下环境中的适应性问题。通过动态捕获流场数据并结合AUV结构特性进行优化，显著提升了AUV的智能水平和决策能力。这项工作为AUV的自主导航和任务执行提供了有前景的新方向。"}}
{"id": "2506.14992", "title": "Secure Time-Modulated Intelligent Reflecting Surface via Generative Flow Networks", "authors": ["Zhihao Tao", "Athina P. Petropulu"], "summary": "We propose a novel directional modulation (DM) design for OFDM transmitters\naided by a time-modulated intelligent reflecting surface (TM-IRS). The TM-IRS\nis configured to preserve the integrity of transmitted signals toward multiple\nlegitimate users while scrambling the signal in all other directions. Existing\nTM-IRS design methods typically target a single user direction and follow\npredefined rule-based procedures, making them unsuitable for multi-user\nscenarios. Here, we propose a generative AI-based approach to design good sets\nof TM-IRS parameters out of a set of all possible quantized ranges of\nparameters. The design objective is to maximize the sum rate across the\nauthorized directions. We model the TM-IRS parameter selection as a\ndeterministic Markov decision process (MDP), where each terminal state\ncorresponds to a specific configuration of TM-IRS parameters. GFlowNets are\nemployed to learn a stochastic policy that samples TM-IRS parameter sets with\nprobability proportional to their associated sum rate reward. Experimental\nresults demonstrate that the proposed method effectively enhances the security\nof the TM-IRS-aided OFDM systems with multi-users. Also, despite the vast size\nof the TM-IRS configuration space, the GFlowNet is able to converge after\ntraining on fewer than 0.000001% of all possible configurations, demonstrating\nremarkable efficiency compared to exhaustive combinatorial search.\nImplementation code is available at https://github.com/ZhihaoTao/GFN4TM-RIS to\nfacilitate reproducibility.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.14992v1", "AI": {"title_translation": "通过生成流网络实现安全时变智能反射面", "tldr": "本文提出一种基于GFlowNets的生成式AI方法，用于设计多用户时变智能反射面（TM-IRS）参数，以增强OFDM系统的安全性。", "motivation": "现有TM-IRS设计方法通常针对单个用户方向，且遵循预定义的规则，不适用于多用户场景。", "method": "提出一种基于生成式AI的方法来设计TM-IRS参数。将TM-IRS参数选择建模为确定性马尔可夫决策过程（MDP），并使用GFlowNets学习随机策略，以采样与总和速率奖励成比例的TM-IRS参数集。", "result": "所提出的方法有效增强了多用户TM-IRS辅助OFDM系统的安全性。GFlowNet在极小部分的配置空间上训练后即可收敛，效率显著高于穷举组合搜索。", "conclusion": "通过GFlowNets可以高效且有效地为多用户TM-IRS系统设计参数，显著提升系统安全性。", "translation": "我们提出了一种新颖的定向调制（DM）设计，用于由时变智能反射面（TM-IRS）辅助的OFDM发射机。TM-IRS被配置为保持向多个合法用户传输信号的完整性，同时在所有其他方向上扰乱信号。现有的TM-IRS设计方法通常针对单个用户方向并遵循预定义的基于规则的程序，这使得它们不适用于多用户场景。在此，我们提出一种基于生成式AI的方法，从所有可能的量化参数范围中设计出良好的TM-IRS参数集。设计目标是最大化授权方向上的总和速率。我们将TM-IRS参数选择建模为确定性马尔可夫决策过程（MDP），其中每个终端状态对应于TM-IRS参数的特定配置。GFlowNets被用来学习一个随机策略，该策略以与相关总和速率奖励成比例的概率采样TM-IRS参数集。实验结果表明，所提出的方法有效增强了多用户TM-IRS辅助OFDM系统的安全性。此外，尽管TM-IRS配置空间巨大，GFlowNet在训练不到所有可能配置的0.000001%后就能收敛，与穷举组合搜索相比，显示出显著的效率。实现代码可在https://github.com/ZhihaoTao/GFN4TM-RIS获取，以方便重现。", "summary": "本文提出一种新颖的基于生成流网络（GFlowNets）的AI方法，用于设计多用户时变智能反射面（TM-IRS）的参数。该方法将TM-IRS参数选择建模为马尔可夫决策过程，旨在最大化授权方向上的总和速率，并在保持合法用户信号完整性的同时扰乱其他方向的信号。实验证明，该方法能有效提升多用户TM-IRS辅助OFDM系统的安全性，且在巨大的配置空间中展现出卓越的效率。", "keywords": "时变智能反射面, 生成流网络, 定向调制, 多用户, 安全通信", "comments": "本文的创新之处在于将生成流网络（GFlowNets）引入到时变智能反射面（TM-IRS）的设计中，解决了传统方法在多用户场景下的局限性。通过将参数选择建模为MDP，并利用GFlowNets进行高效探索，显著提升了TM-IRS系统的安全性和设计效率，对于未来无线通信安全领域具有重要意义。"}}
{"id": "2506.14782", "title": "Integrating Dynamical Systems Learning with Foundational Models: A Meta-Evolutionary AI Framework for Clinical Trials", "authors": ["Joseph Geraci", "Bessi Qorri", "Christian Cumbaa", "Mike Tsay", "Paul Leonczyk", "Luca Pani"], "summary": "Artificial intelligence (AI) has evolved into an ecosystem of specialized\n\"species,\" each with unique strengths. We analyze two: DeepSeek-V3, a\n671-billion-parameter Mixture of Experts large language model (LLM)\nexemplifying scale-driven generality, and NetraAI, a dynamical system-based\nframework engineered for stability and interpretability on small clinical trial\ndatasets. We formalize NetraAI's foundations, combining contraction mappings,\ninformation geometry, and evolutionary algorithms to identify predictive\npatient cohorts. Features are embedded in a metric space and iteratively\ncontracted toward stable attractors that define latent subgroups. A\npseudo-temporal embedding and long-range memory enable exploration of\nhigher-order feature interactions, while an internal evolutionary loop selects\ncompact, explainable 2-4-variable bundles (\"Personas\").\n  To guide discovery, we introduce an LLM Strategist as a meta-evolutionary\nlayer that observes Persona outputs, prioritizes promising variables, injects\ndomain knowledge, and assesses robustness. This two-tier architecture mirrors\nthe human scientific process: NetraAI as experimentalist, the LLM as theorist,\nforming a self-improving loop.\n  In case studies (schizophrenia, depression, pancreatic cancer), NetraAI\nuncovered small, high-effect-size subpopulations that transformed weak baseline\nmodels (AUC ~0.50-0.68) into near-perfect classifiers using only a few\nfeatures. We position NetraAI at the intersection of dynamical systems,\ninformation geometry, and evolutionary learning, aligned with emerging\nconcept-level reasoning paradigms such as LeCun's Joint Embedding Predictive\nArchitecture (JEPA). By prioritizing reliable, explainable knowledge, NetraAI\noffers a new generation of adaptive, self-reflective AI to accelerate clinical\ndiscovery.", "comment": "27 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14782v1", "AI": {"title_translation": "将动力系统学习与基础模型相结合：一个用于临床试验的元进化AI框架", "tldr": "该研究提出了一个名为NetraAI的AI框架，结合动力系统、信息几何和进化算法，并利用大型语言模型作为元进化层，在小规模临床试验数据上发现可解释的高效应子人群，显著提升了预测模型性能。", "motivation": "论文指出AI生态系统中有不同优势的“物种”，并分析了大型语言模型（DeepSeek-V3）的通用性以及基于动力系统的NetraAI在小规模临床试验数据集上的稳定性和可解释性。其动机在于在临床试验中发现可解释、高效应子的人群，以加速临床发现。", "method": "本文提出一个两层AI架构。底层是NetraAI，它结合收缩映射、信息几何和进化算法来识别预测性患者队列。特征被嵌入度量空间并迭代收缩，形成定义潜在子群的稳定吸引子。利用伪时间嵌入和长程记忆探索高阶特征交互，并通过内部进化循环选择紧凑、可解释的2-4变量组合（“Persona”）。上层是LLM Strategist（大型语言模型策略师），作为元进化层，观察Persona输出，优先选择有前景的变量，注入领域知识并评估稳健性。这个架构模仿人类科学过程，NetraAI作为实验者，LLM作为理论家，形成一个自我改进的循环。", "result": "在精神分裂症、抑郁症和胰腺癌的案例研究中，NetraAI发现了小规模、高效应子的人群，这些人群将弱基线模型（AUC约0.50-0.68）转化为几乎完美的分类器，且仅使用了少量特征。", "conclusion": "NetraAI提供了一种新一代的自适应、自反思AI，通过优先考虑可靠、可解释的知识来加速临床发现，并将其定位在动力系统、信息几何和进化学习的交叉点上，与新兴的概念级推理范式（如LeCun的JEPA）对齐。", "translation": "人工智能（AI）已演变为一个由专业“物种”组成的生态系统，每个物种都具有独特的优势。我们分析了其中两种：DeepSeek-V3，一个拥有6710亿参数的混合专家大型语言模型（LLM），体现了规模驱动的通用性；以及NetraAI，一个基于动力系统框架，旨在小规模临床试验数据集上实现稳定性和可解释性。我们形式化了NetraAI的基础，结合了收缩映射、信息几何和进化算法来识别预测性患者队列。特征被嵌入到一个度量空间中，并迭代收缩向定义潜在子群的稳定吸引子。伪时间嵌入和长程记忆能够探索高阶特征交互，而内部进化循环则选择紧凑、可解释的2-4变量组合（“Persona”）。\n为了指导发现，我们引入了LLM策略师作为元进化层，它观察Persona输出，优先选择有前景的变量，注入领域知识并评估稳健性。这种两层架构模仿了人类的科学过程：NetraAI作为实验者，LLM作为理论家，形成一个自我改进的循环。\n在案例研究（精神分裂症、抑郁症、胰腺癌）中，NetraAI发现了小规模、高效应子的人群，这些人群将弱基线模型（AUC约0.50-0.68）转化为几乎完美的分类器，且仅使用了少量特征。我们将NetraAI定位在动力系统、信息几何和进化学习的交叉点上，与新兴的概念级推理范式如LeCun的联合嵌入预测架构（JEPA）对齐。通过优先考虑可靠、可解释的知识，NetraAI提供了一种新一代的自适应、自反思AI，以加速临床发现。", "summary": "本文提出了一种名为NetraAI的元进化AI框架，用于临床试验。该框架结合了动力系统学习、信息几何和进化算法，以在小规模临床试验数据中发现可解释的预测性患者亚群。NetraAI通过迭代收缩特征到稳定吸引子来定义潜在子群，并选择紧凑的变量组合（Persona）。一个大型语言模型（LLM）作为元进化策略师，指导NetraAI的发现过程，形成一个自我改进的两层架构。在精神分裂症、抑郁症和胰腺癌的案例研究中，NetraAI显著提升了模型性能，将弱基线模型转化为高精度分类器，证明了其在加速临床发现方面的潜力。", "keywords": "动力系统学习, 基础模型, 元进化AI, 临床试验, 可解释性AI", "comments": "这篇论文的创新之处在于其独特的两层元进化AI框架，将传统的动力系统学习与现代大型语言模型相结合，模仿了人类的科学发现过程。NetraAI专注于在小数据量下实现稳定性和可解释性，这对于临床试验数据分析至关重要。大型语言模型作为策略师的引入，为自动化科学发现提供了新的范式，特别是通过注入领域知识和评估稳健性。其在真实世界临床数据上的显著效果（将弱模型转化为近乎完美分类器）凸显了其实用价值和重要性。该框架对可解释性和可靠知识的强调，使其在医疗健康等高风险领域具有巨大潜力。"}}
{"id": "2506.14844", "title": "Improving Prostate Gland Segmenting Using Transformer based Architectures", "authors": ["Shatha Abudalou"], "summary": "Inter reader variability and cross site domain shift challenge the automatic\nsegmentation of prostate anatomy using T2 weighted MRI images. This study\ninvestigates whether transformer models can retain precision amid such\nheterogeneity. We compare the performance of UNETR and SwinUNETR in prostate\ngland segmentation against our previous 3D UNet model [1], based on 546 MRI\n(T2weighted) volumes annotated by two independent experts. Three training\nstrategies were analyzed: single cohort dataset, 5 fold cross validated mixed\ncohort, and gland size based dataset. Hyperparameters were tuned by Optuna. The\ntest set, from an independent population of readers, served as the evaluation\nendpoint (Dice Similarity Coefficient). In single reader training, SwinUNETR\nachieved an average dice score of 0.816 for Reader#1 and 0.860 for Reader#2,\nwhile UNETR scored 0.8 and 0.833 for Readers #1 and #2, respectively, compared\nto the baseline UNets 0.825 for Reader #1 and 0.851 for Reader #2. SwinUNETR\nhad an average dice score of 0.8583 for Reader#1 and 0.867 for Reader#2 in\ncross-validated mixed training. For the gland size-based dataset, SwinUNETR\nachieved an average dice score of 0.902 for Reader#1 subset and 0.894 for\nReader#2, using the five-fold mixed training strategy (Reader#1, n=53;\nReader#2, n=87) at larger gland size-based subsets, where UNETR performed\npoorly. Our findings demonstrate that global and shifted-window self-attention\neffectively reduces label noise and class imbalance sensitivity, resulting in\nimprovements in the Dice score over CNNs by up to five points while maintaining\ncomputational efficiency. This contributes to the high robustness of SwinUNETR\nfor clinical deployment.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14844v1", "AI": {"title_translation": "使用基于Transformer的架构改进前列腺体分割", "tldr": "本研究比较了UNETR和SwinUNETR在T2加权MRI图像上前列腺体分割中的性能，发现SwinUNETR在减少标签噪声和类别不平衡敏感性方面表现出色，并提高了Dice分数。", "motivation": "由于读者间差异和跨站点域偏移，使用T2加权MRI图像自动分割前列腺解剖结构面临挑战。本研究旨在探究Transformer模型能否在此类异质性中保持精度。", "method": "本研究比较了UNETR和SwinUNETR在546个由两名独立专家标注的MRI（T2加权）体积上前列腺体分割的性能，并与先前的3D UNet模型进行对比。分析了三种训练策略：单队列数据集、五折交叉验证混合队列和基于腺体大小的数据集。使用Optuna调整超参数，并以来自独立人群的测试集作为评估终点（Dice相似系数）。", "result": "在单读者训练中，SwinUNETR在读者#1和读者#2上的平均Dice分数分别为0.816和0.860，而UNETR分别为0.8和0.833，基线UNet分别为0.825和0.851。在交叉验证混合训练中，SwinUNETR在读者#1和读者#2上的平均Dice分数分别为0.8583和0.867。在基于腺体大小的数据集上，SwinUNETR在较大腺体大小的子集（读者#1，n=53；读者#2，n=87）上，使用五折混合训练策略，分别达到了0.902和0.894的平均Dice分数，而UNETR表现不佳。", "conclusion": "研究结果表明，全局和移位窗口自注意力能有效减少标签噪声和类别不平衡敏感性，使Dice分数比CNN提高多达五个百分点，同时保持计算效率。这有助于SwinUNETR在临床部署中展现出高度鲁棒性。", "translation": "读者间差异和跨站点域偏移对使用T2加权MRI图像自动分割前列腺解剖结构提出了挑战。本研究旨在探讨Transformer模型在此类异质性中能否保持精度。我们比较了UNETR和SwinUNETR在前列腺体分割中的性能，并与我们之前的3D UNet模型[1]进行对比，数据基于两名独立专家标注的546个MRI（T2加权）体积。分析了三种训练策略：单队列数据集、五折交叉验证混合队列和基于腺体大小的数据集。使用Optuna调整超参数。测试集来自独立的读者群体，作为评估终点（Dice相似系数）。在单读者训练中，SwinUNETR在读者#1和读者#2上的平均Dice分数分别为0.816和0.860，而UNETR在读者#1和读者#2上的平均Dice分数分别为0.8和0.833，相比之下，基线UNet在读者#1和读者#2上的平均Dice分数分别为0.825和0.851。在交叉验证混合训练中，SwinUNETR在读者#1和读者#2上的平均Dice分数分别为0.8583和0.867。对于基于腺体大小的数据集，SwinUNETR在较大腺体大小的子集（读者#1，n=53；读者#2，n=87）上，使用五折混合训练策略，在读者#1子集上达到了0.902的平均Dice分数，在读者#2子集上达到了0.894，而UNETR在该情况下表现不佳。我们的研究结果表明，全局和移位窗口自注意力能有效减少标签噪声和类别不平衡敏感性，使Dice分数比CNN提高多达五个百分点，同时保持计算效率。这有助于SwinUNETR在临床部署中展现出高度鲁棒性。", "summary": "本研究旨在解决前列腺MRI图像自动分割中存在的读者间差异和跨站点域偏移问题。通过比较UNETR和SwinUNETR与传统3D UNet模型在546个T2加权MRI体积上的性能，并分析了单队列、交叉验证混合以及基于腺体大小的三种训练策略。结果显示，SwinUNETR在各种训练设置下，尤其是在处理较大腺体时，表现出更优越的分割精度（Dice分数），并有效降低了对标签噪声和类别不平衡的敏感性，相较于CNN模型，Dice分数提升高达五个百分点，且保持计算效率。这表明SwinUNETR具有在临床应用中部署的高鲁棒性。", "keywords": "前列腺分割, Transformer, UNETR, SwinUNETR, MRI", "comments": "本研究的创新点在于将Transformer架构（UNETR和SwinUNETR）应用于前列腺T2加权MRI图像分割，并深入探讨了它们在处理数据异质性（如读者间差异和腺体大小变化）方面的能力。SwinUNETR的优异表现，特别是其对标签噪声和类别不平衡的鲁棒性，以及Dice分数相较于CNN的显著提升，凸显了其在临床部署中的巨大潜力。这项工作为医学图像分割领域引入了更先进的深度学习模型，并为解决实际临床挑战提供了有效的解决方案。"}}
{"id": "2506.15312", "title": "One-shot Face Sketch Synthesis in the Wild via Generative Diffusion Prior and Instruction Tuning", "authors": ["Han Wu", "Junyao Li", "Kangbo Zhao", "Sen Zhang", "Yukai Shi", "Liang Lin"], "summary": "Face sketch synthesis is a technique aimed at converting face photos into\nsketches. Existing face sketch synthesis research mainly relies on training\nwith numerous photo-sketch sample pairs from existing datasets. However, these\nlarge-scale discriminative learning methods will have to face problems such as\ndata scarcity and high human labor costs. Once the training data becomes\nscarce, their generative performance significantly degrades. In this paper, we\npropose a one-shot face sketch synthesis method based on diffusion models. We\noptimize text instructions on a diffusion model using face photo-sketch image\npairs. Then, the instructions derived through gradient-based optimization are\nused for inference. To simulate real-world scenarios more accurately and\nevaluate method effectiveness more comprehensively, we introduce a new\nbenchmark named One-shot Face Sketch Dataset (OS-Sketch). The benchmark\nconsists of 400 pairs of face photo-sketch images, including sketches with\ndifferent styles and photos with different backgrounds, ages, sexes,\nexpressions, illumination, etc. For a solid out-of-distribution evaluation, we\nselect only one pair of images for training at each time, with the rest used\nfor inference. Extensive experiments demonstrate that the proposed method can\nconvert various photos into realistic and highly consistent sketches in a\none-shot context. Compared to other methods, our approach offers greater\nconvenience and broader applicability. The dataset will be available at:\nhttps://github.com/HanWu3125/OS-Sketch", "comment": "We propose a novel framework for face sketch synthesis, where merely\n  a single pair of samples suffices to enable in-the-wild face sketch synthesis", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.15312v1", "AI": {"title_translation": "通过生成扩散先验和指令微调实现野外条件下的一次性人脸草图合成", "tldr": "本文提出了一种基于扩散模型和指令微调的一次性人脸草图合成方法，旨在解决传统方法中数据稀缺的问题，并引入了一个新的数据集用于评估。", "motivation": "现有的人脸草图合成研究主要依赖于大量照片-草图样本对进行训练，导致数据稀缺和人力成本高昂的问题。一旦训练数据稀缺，其生成性能会显著下降。", "method": "本文提出了一种基于扩散模型的一次性人脸草图合成方法。该方法通过人脸照片-草图图像对优化扩散模型上的文本指令，然后利用通过基于梯度的优化获得的指令进行推理。为了更准确地模拟真实世界场景并更全面地评估方法有效性，本文引入了一个名为“一次性人脸草图数据集”（OS-Sketch）的新基准，该基准包含400对人脸照片-草图图像。为了进行可靠的域外评估，每次只选择一对图像进行训练，其余用于推理。", "result": "所提出的方法能够在一次性环境下将各种照片转换为逼真且高度一致的草图。与其他方法相比，我们的方法提供了更大的便利性和更广泛的适用性。", "conclusion": "本文成功提出了一种基于生成扩散先验和指令微调的一次性人脸草图合成方法，有效解决了传统方法中数据稀缺的挑战。通过引入新的OS-Sketch数据集并进行广泛实验，证明了该方法在野外条件下生成逼真且一致草图的有效性，展现了其在实际应用中的便利性和广泛适用性。", "translation": "人脸草图合成是一种旨在将人脸照片转换为草图的技术。现有的人脸草图合成研究主要依赖于从现有数据集中获取的大量照片-草图样本对进行训练。然而，这些大规模判别学习方法将不得不面对数据稀缺和高昂的人力成本等问题。一旦训练数据变得稀缺，它们的生成性能会显著下降。在本文中，我们提出了一种基于扩散模型的一次性人脸草图合成方法。我们使用人脸照片-草图图像对来优化扩散模型上的文本指令。然后，利用通过基于梯度的优化获得的指令进行推理。为了更准确地模拟真实世界场景并更全面地评估方法有效性，我们引入了一个名为“一次性人脸草图数据集”（OS-Sketch）的新基准。该基准包含400对人脸照片-草图图像，其中包括不同风格的草图和不同背景、年龄、性别、表情、光照等的照片。为了进行可靠的域外评估，我们每次只选择一对图像进行训练，其余用于推理。大量实验表明，所提出的方法能够在一次性环境下将各种照片转换为逼真且高度一致的草图。与其他方法相比，我们的方法提供了更大的便利性和更广泛的适用性。数据集将在此处提供：https://github.com/HanWu3125/OS-Sketch", "summary": "本文提出了一种新颖的一次性人脸草图合成方法，该方法利用生成扩散模型和指令微调，旨在解决传统大规模判别学习方法面临的数据稀缺问题。该方法通过少量照片-草图对优化扩散模型上的文本指令，然后将这些优化后的指令用于推理。为了在多样化的真实世界条件下严格评估其性能，本文引入了一个新的基准数据集OS-Sketch，包含400对多样化的照片-草图对。实验证明，所提出的方法仅使用单个训练对即可有效从各种照片生成逼真且一致的草图，与现有方法相比，展现出卓越的便利性和更广泛的适用性。", "keywords": "人脸草图合成, 一次性学习, 扩散模型, 指令微调, OS-Sketch", "comments": "该论文的创新点在于将扩散模型应用于人脸草图合成领域的一次性学习，有效解决了现有方法在数据稀缺情况下的性能瓶颈。同时，引入新的OS-Sketch基准数据集，用于在“野外”场景下进行一次性评估，也为该领域做出了重要贡献，提升了研究的实用性和评估的全面性。"}}
{"id": "2506.15284", "title": "Multi-Interest Recommendation: A Survey", "authors": ["Zihao Li", "Qiang Chen", "Lixin Zou", "Aixin Sun", "Chenliang Li"], "summary": "Existing recommendation methods often struggle to model users' multifaceted\npreferences due to the diversity and volatility of user behavior, as well as\nthe inherent uncertainty and ambiguity of item attributes in practical\nscenarios. Multi-interest recommendation addresses this challenge by extracting\nmultiple interest representations from users' historical interactions, enabling\nfine-grained preference modeling and more accurate recommendations. It has\ndrawn broad interest in recommendation research. However, current\nrecommendation surveys have either specialized in frontier recommendation\nmethods or delved into specific tasks and downstream applications. In this\nwork, we systematically review the progress, solutions, challenges, and future\ndirections of multi-interest recommendation by answering the following three\nquestions: (1) Why is multi-interest modeling significantly important for\nrecommendation? (2) What aspects are focused on by multi-interest modeling in\nrecommendation? and (3) How can multi-interest modeling be applied, along with\nthe technical details of the representative modules? We hope that this survey\nestablishes a fundamental framework and delivers a preliminary overview for\nresearchers interested in this field and committed to further exploration. The\nimplementation of multi-interest recommendation summarized in this survey is\nmaintained at https://github.com/WHUIR/Multi-Interest-Recommendation-A-Survey.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.15284v1", "AI": {"title_translation": "多兴趣推荐：一项综述", "tldr": "本综述系统地回顾了多兴趣推荐的进展、解决方案、挑战和未来方向，旨在解决现有推荐方法在建模用户多样化偏好方面的不足。", "motivation": "现有推荐方法难以对用户多方面的偏好进行建模，因为用户行为多样且不稳定，且物品属性存在不确定性和模糊性。多兴趣推荐通过提取用户历史交互中的多个兴趣表示来解决这一挑战。当前的推荐综述缺乏对多兴趣推荐的系统回顾，因此本工作旨在填补这一空白。", "method": "本综述通过回答三个关键问题来系统地回顾多兴趣推荐：1）为什么多兴趣建模对推荐至关重要？2）多兴趣建模在推荐中关注哪些方面？3）多兴趣建模如何应用，以及代表性模块的技术细节？", "result": "本综述系统地回顾了多兴趣推荐的进展、解决方案、挑战和未来方向。它为对该领域感兴趣的研究人员建立了一个基本框架并提供了初步概述。", "conclusion": "作者希望本综述能为对多兴趣推荐领域感兴趣并致力于进一步探索的研究人员建立一个基本框架并提供初步概述。", "translation": "现有推荐方法由于用户行为的多样性和波动性，以及实际场景中物品属性固有的不确定性和模糊性，往往难以对用户多方面的偏好进行建模。多兴趣推荐通过从用户的历史交互中提取多个兴趣表示来解决这一挑战，从而实现细粒度的偏好建模和更准确的推荐。它在推荐研究中引起了广泛关注。然而，当前的推荐综述要么专注于前沿推荐方法，要么深入探讨特定任务和下游应用。在这项工作中，我们通过回答以下三个问题，系统地回顾了多兴趣推荐的进展、解决方案、挑战和未来方向：（1）为什么多兴趣建模对推荐至关重要？（2）多兴趣建模在推荐中关注哪些方面？（3）多兴趣建模如何应用，以及代表性模块的技术细节？我们希望这项综述能为对该领域感兴趣并致力于进一步探索的研究人员建立一个基本框架并提供初步概述。本综述中总结的多兴趣推荐的实现维护在 https://github.com/WHUIR/Multi-Interest-Recommendation-A-Survey。", "summary": "本论文对多兴趣推荐进行了系统综述，该领域因其能够通过从用户历史交互中提取多个兴趣表示来建模多样化用户偏好而受到关注。与以往专注于特定方法或任务的综述不同，本文全面回顾了多兴趣推荐的进展、解决方案、挑战和未来方向。它回答了关于其重要性、关注点和应用细节的关键问题，旨在为研究人员提供一个基础框架和概览。", "keywords": "多兴趣推荐, 用户偏好, 综述, 推荐系统", "comments": "这项综述具有重要意义，因为它系统地组织了快速发展的多兴趣推荐领域，这对于解决复杂的用户偏好至关重要。通过围绕关键问题构建综述，它为研究人员提供了一个清晰易懂的资源。随附的GitHub仓库也增加了实用价值。"}}
{"id": "2506.15602", "title": "Estimate Hitting Time by Hitting Probability for Elitist Evolutionary Algorithms", "authors": ["Jun He", "Siang Yew Chong", "Xin Yao"], "summary": "Drift analysis is a powerful tool for analyzing the time complexity of\nevolutionary algorithms. However, it requires manual construction of drift\nfunctions to bound hitting time for each specific algorithm and problem. To\naddress this limitation, general linear drift functions were introduced for\nelitist evolutionary algorithms. But calculating linear bound coefficients\neffectively remains a problem. This paper proposes a new method called drift\nanalysis of hitting probability to compute these coefficients. Each coefficient\nis interpreted as a bound on the hitting probability of a fitness level,\ntransforming the task of estimating hitting time into estimating hitting\nprobability. A novel drift analysis method is then developed to estimate\nhitting probability, where paths are introduced to handle multimodal fitness\nlandscapes. Explicit expressions are constructed to compute hitting\nprobability, significantly simplifying the estimation process. One advantage of\nthe proposed method is its ability to estimate both the lower and upper bounds\nof hitting time and to compare the performance of two algorithms in terms of\nhitting time. To demonstrate this application, two algorithms for the knapsack\nproblem, each incorporating feasibility rules and greedy repair respectively,\nare compared. The analysis indicates that neither constraint handling technique\nconsistently outperforms the other.", "comment": null, "cate": "cs.NE", "url": "http://arxiv.org/abs/2506.15602v1", "AI": {"title_translation": "估计精英进化算法的击中时间通过击中概率", "tldr": "本文提出了一种通过估计击中概率来计算精英进化算法击中时间界限的新漂移分析方法，并能处理多模态适应度景观。", "motivation": "漂移分析在分析进化算法时间复杂度时需要手动构建漂移函数，而通用线性漂移函数虽然被引入，但有效计算其线性界限系数仍是难题。", "method": "本文提出了一种名为“击中概率漂移分析”的新方法来计算线性界限系数。该方法将每个系数解释为适应度水平击中概率的界限，从而将估计击中时间转化为估计击中概率。为估计击中概率，开发了一种新的漂移分析方法，其中引入了路径以处理多模态适应度景观，并构建了显式表达式来简化计算。", "result": "该方法能够估计击中时间的下限和上限，并能比较两种算法在击中时间方面的性能。在背包问题上的应用表明，两种约束处理技术（可行性规则和贪婪修复）没有一个能始终优于另一个。", "conclusion": "本文通过将击中时间估计转化为击中概率估计，并引入路径处理多模态景观，显著简化了精英进化算法的击中时间估计过程，并为算法性能比较提供了新工具。", "translation": "漂移分析是分析进化算法时间复杂度的强大工具。然而，它需要为每个特定的算法和问题手动构建漂移函数来界定击中时间。为了解决这个限制，引入了精英进化算法的通用线性漂移函数。但有效计算线性界限系数仍然是一个问题。本文提出了一种新的方法，称为击中概率漂移分析，来计算这些系数。每个系数被解释为适应度水平击中概率的界限，将估计击中时间的任务转化为估计击中概率。然后开发了一种新的漂移分析方法来估计击中概率，其中引入了路径来处理多模态适应度景观。构建了显式表达式来计算击中概率，显著简化了估计过程。所提出方法的一个优点是它能够估计击中时间的下限和上限，并比较两种算法在击中时间方面的性能。为了演示这个应用，比较了两种解决背包问题的算法，它们分别结合了可行性规则和贪婪修复。分析表明，这两种约束处理技术都没有一个能始终优于另一个。", "summary": "本文提出了一种新的“击中概率漂移分析”方法，用于估计精英进化算法的击中时间。该方法将击中时间的估计问题转化为击中概率的估计，并引入路径来处理多模态适应度景观，从而简化了线性界限系数的计算。新方法能够估计击中时间的上下限，并可用于比较不同算法的性能，例如在背包问题上比较了两种约束处理技术。", "keywords": "漂移分析, 击中时间, 击中概率, 精英进化算法, 多模态适应度景观", "comments": "本文创新性地将击中时间估计问题转化为击中概率估计，并引入了“路径”概念来处理多模态适应度景观，这对于漂移分析在更复杂问题上的应用具有重要意义。通过提供显式表达式，简化了计算过程。此外，其能够比较算法性能的特性也增加了其实用价值。"}}
{"id": "2506.14912", "title": "CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision", "authors": ["Dyah Adila", "Shuai Zhang", "Boran Han", "Bonan Min", "Yuyang Wang"], "summary": "The integration of contextual information has significantly enhanced the\nperformance of large language models (LLMs) on knowledge-intensive tasks.\nHowever, existing methods often overlook a critical challenge: the credibility\nof context documents can vary widely, potentially leading to the propagation of\nunreliable information. In this paper, we introduce CrEst, a novel weakly\nsupervised framework for assessing the credibility of context documents during\nLLM inference--without requiring manual annotations. Our approach is grounded\nin the insight that credible documents tend to exhibit higher semantic\ncoherence with other credible documents, enabling automated credibility\nestimation through inter-document agreement. To incorporate credibility into\nLLM inference, we propose two integration strategies: a black-box approach for\nmodels without access to internal weights or activations, and a white-box\nmethod that directly modifies attention mechanisms. Extensive experiments\nacross three model architectures and five datasets demonstrate that CrEst\nconsistently outperforms strong baselines, achieving up to a 26.86% improvement\nin accuracy and a 3.49% increase in F1 score. Further analysis shows that CrEst\nmaintains robust performance even under high-noise conditions.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14912v1", "AI": {"title_translation": "CrEst：基于弱监督的大语言模型上下文可信度评估", "tldr": "CrEst是一个弱监督框架，用于在大语言模型推理时评估上下文文档的可信度，通过文档间语义一致性实现，并在多个模型和数据集上表现优异。", "motivation": "现有LLM集成上下文信息的方法忽略了上下文文档可信度差异大的问题，这可能导致不可靠信息的传播。", "method": "引入CrEst，一个新型弱监督框架，无需手动标注即可评估上下文文档的可信度。其核心思想是可信文档倾向于与其他可信文档具有更高的语义一致性，通过文档间一致性进行自动化评估。提出了两种集成策略：黑盒方法（不访问内部权重/激活）和白盒方法（直接修改注意力机制）。", "result": "在三种模型架构和五个数据集上的大量实验表明，CrEst持续优于强基线，准确率提高高达26.86%，F1分数提高3.49%。即使在高噪声条件下，CrEst也能保持鲁棒性能。", "conclusion": "CrEst有效解决了LLM中上下文文档可信度评估的挑战，显著提升了模型性能和信息可靠性。", "translation": "上下文信息的整合显著增强了大型语言模型（LLMs）在知识密集型任务上的性能。然而，现有方法常常忽视一个关键挑战：上下文文档的可信度可能差异很大，从而可能导致不可靠信息的传播。在本文中，我们引入了CrEst，一个新颖的弱监督框架，用于在LLM推理过程中评估上下文文档的可信度——无需手动标注。我们的方法基于一个洞察：可信文档倾向于与其他可信文档表现出更高的语义一致性，从而可以通过文档间的一致性实现自动化可信度评估。为了将可信度整合到LLM推理中，我们提出了两种集成策略：一种是针对无法访问内部权重或激活的模型设计的黑盒方法，另一种是直接修改注意力机制的白盒方法。在三种模型架构和五个数据集上进行的广泛实验表明，CrEst持续优于强大的基线模型，准确率提高了26.86%，F1分数提高了3.49%。进一步分析显示，即使在高噪声条件下，CrEst也能保持鲁棒性能。", "summary": "本文提出了CrEst，一个用于在大语言模型（LLMs）推理阶段评估上下文文档可信度的新型弱监督框架。它利用可信文档间语义一致性高的特点进行自动化评估，无需人工标注。CrEst提供了黑盒和白盒两种集成策略，并在多项实验中表现出显著优于现有基线的性能，提高了LLM在知识密集型任务中处理不可靠信息的能力。", "keywords": "大语言模型, 上下文可信度, 弱监督, 语义一致性, CrEst", "comments": "CrEst的创新之处在于提出了一个无需人工标注的弱监督方法来评估LLM上下文的可信度，解决了现有方法忽视的关键问题。通过利用文档间的语义一致性来判断可信度，并提供两种灵活的集成策略，使其具有广泛的适用性。在高噪声条件下的鲁棒性也增加了其实用价值。"}}
{"id": "2506.14788", "title": "Energy-consistent dynamic fracture phase field models: unilateral constraints and finite element simulations", "authors": ["Md Mamun Miah", "Ryuhei Wakida", "Masato Kimura"], "summary": "Phase field models have emerged as a powerful and flexible framework for\nsimulating complex interface-driven phenomena across a wide range of scientific\nand engineering applications. In fracture mechanics, the phase field\napproach--formulated as a gradient flow of the Griffith fracture energy with\nAmbrosio-Tortorelli regularization--has gained significant attention for its\nability to capture complex crack topologies. In this study, we propose a\ndynamic fracture phase field model (DF-PFM) based on the elastodynamic wave\nequation. We further extend this framework by incorporating a unilateral\ncontact condition, yielding a refined model suitable for simulating fault\nrupture under high pressure. For both models, we rigorously derive energy\ndissipation identities under mixed boundary conditions, ensuring energy\nconsistency of the formulations. To validate the proposed approach, we conduct\nnumerical experiments using linear implicit time discretization and finite\nelement methods. Our simulations demonstrate that the unilateral contact\ncondition is essential for accurately capturing shear-dominated crack\npropagation and preventing non-physical interpenetration, especially under\nhigh-compression loading scenarios relevant to seismic faulting.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14788v1", "AI": {"title_translation": "能量一致的动态断裂相场模型：单边约束和有限元模拟", "tldr": "本研究提出了一个基于弹性动力波方程的动态断裂相场模型，并通过引入单边接触条件对其进行了扩展，以模拟高压下的断层破裂。模型能量一致性得到严格推导，数值模拟验证了单边接触条件对于准确捕捉剪切主导裂纹扩展和防止非物理渗透的重要性。", "motivation": "相场模型在断裂力学中因其捕捉复杂裂纹拓扑的能力而受到关注。本研究旨在提出并验证一个能量一致的动态断裂相场模型，特别是通过引入单边接触条件来提高其在高压下模拟断层破裂的准确性。", "method": "本研究提出了一个基于弹性动力波方程的动态断裂相场模型（DF-PFM），并通过引入单边接触条件对其进行扩展。对于这两种模型，严格推导了混合边界条件下的能量耗散恒等式，确保了公式的能量一致性。通过线性隐式时间离散化和有限元方法进行数值实验验证。", "result": "数值模拟表明，单边接触条件对于准确捕捉剪切主导的裂纹扩展和防止非物理渗透至关重要，尤其是在与地震断层相关的高压缩载荷情景下。", "conclusion": "单边接触条件对于动态断裂相场模型在高压和剪切主导裂纹扩展场景下的准确模拟是必不可少的，它能有效防止非物理渗透。", "translation": "相场模型已成为模拟各种科学和工程应用中复杂界面驱动现象的强大而灵活的框架。在断裂力学中，相场方法——作为Griffith断裂能的梯度流，并采用Ambrosio-Tortorelli正则化——因其捕捉复杂裂纹拓扑的能力而受到广泛关注。在本研究中，我们提出了一个基于弹性动力波方程的动态断裂相场模型（DF-PFM）。我们通过引入单边接触条件进一步扩展了该框架，从而得到了一个适用于模拟高压下断层破裂的精细模型。对于这两种模型，我们严格推导了混合边界条件下的能量耗散恒等式，确保了公式的能量一致性。为了验证所提出的方法，我们使用线性隐式时间离散化和有限元方法进行了数值实验。我们的模拟表明，单边接触条件对于准确捕捉剪切主导的裂纹扩展和防止非物理渗透至关重要，尤其是在与地震断层相关的高压缩载荷情景下。", "summary": "本研究提出了一个能量一致的动态断裂相场模型（DF-PFM），该模型基于弹性动力波方程，并创新性地引入了单边接触条件以模拟高压下的断层破裂。论文严格推导了模型的能量耗散恒等式，并通过有限元数值模拟验证了单边接触条件在准确捕捉剪切主导裂纹扩展和防止非物理渗透方面的关键作用。", "keywords": "相场模型, 动态断裂, 单边约束, 有限元, 能量一致性", "comments": "该论文的创新点在于将单边接触条件引入动态断裂相场模型，并严格证明了模型的能量一致性。这对于模拟高压下（如地震断层）的复杂裂纹行为具有重要意义，尤其是在处理剪切主导裂纹和防止非物理渗透方面，提高了模型的物理准确性。"}}
{"id": "2506.15029", "title": "An accurate and revised version of optical character recognition-based speech synthesis using LabVIEW", "authors": ["Prateek Mehta", "Anasuya Patil"], "summary": "Knowledge extraction through sound is a distinctive property. Visually\nimpaired individuals often rely solely on Braille books and audio recordings\nprovided by NGOs. Due to limitations in these approaches, blind individuals\noften cannot access books of their choice. Speech is a more effective mode of\ncommunication than text for blind and visually impaired persons, as they can\neasily respond to sounds. This paper presents the development of an accurate,\nreliable, cost-effective, and user-friendly optical character recognition\n(OCR)-based speech synthesis system. The OCR-based system has been implemented\nusing Laboratory Virtual Instrument Engineering Workbench (LabVIEW).", "comment": "9 pages, 9 figures", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15029v1", "AI": {"title_translation": "基于LabVIEW的准确改进版光学字符识别语音合成", "tldr": "本文开发了一个基于光学字符识别（OCR）和LabVIEW的语音合成系统，旨在为视障人士提供一种准确、可靠、经济高效且用户友好的信息获取方式。", "motivation": "视障人士获取书籍和信息受限，现有方法如盲文书和录音存在局限性，语音是更有效的交流方式，因此需要开发一种更便捷的系统。", "method": "本文开发了一个基于光学字符识别（OCR）的语音合成系统，并使用LabVIEW（Laboratory Virtual Instrument Engineering Workbench）实现了该系统。", "result": "开发了一个准确、可靠、经济高效且用户友好的基于OCR的语音合成系统。", "conclusion": "该研究成功开发了一个基于OCR和LabVIEW的语音合成系统，旨在为视障人士提供更便捷、经济的信息获取方式。", "translation": "通过声音提取知识是一种独特的特性。视障人士通常仅依赖盲文书籍和非政府组织提供的录音。由于这些方法的局限性，盲人往往无法获取他们选择的书籍。对于盲人和视力受损者来说，语音比文本是更有效的交流模式，因为他们可以轻松地对声音做出反应。本文提出了一种准确、可靠、经济高效且用户友好的光学字符识别（OCR）语音合成系统的开发。该基于OCR的系统已使用LabVIEW（Laboratory Virtual Instrument Engineering Workbench）实现。", "summary": "本文介绍了一种基于光学字符识别（OCR）的语音合成系统的开发。该系统利用LabVIEW实现，旨在为视障人士提供一个准确、可靠、经济高效且用户友好的信息获取工具，以克服传统盲文书籍和录音的局限性，使他们能够更便捷地访问所需书籍。", "keywords": "光学字符识别, 语音合成, LabVIEW, 视障人士, 信息获取", "comments": "该论文提出了一种实用的解决方案，旨在解决视障人士获取信息的问题，具有重要的社会意义。其创新之处在于结合了OCR技术和LabVIEW平台，提供了一个经济高效且用户友好的系统。"}}
{"id": "2506.15043", "title": "Advanced Prediction of Hypersonic Missile Trajectories with CNN-LSTM-GRU Architectures", "authors": ["Amir Hossein Baradaran"], "summary": "Advancements in the defense industry are paramount for ensuring the safety\nand security of nations, providing robust protection against emerging threats.\nAmong these threats, hypersonic missiles pose a significant challenge due to\ntheir extreme speeds and maneuverability, making accurate trajectory prediction\na critical necessity for effective countermeasures. This paper addresses this\nchallenge by employing a novel hybrid deep learning approach, integrating\nConvolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks,\nand Gated Recurrent Units (GRUs). By leveraging the strengths of these\narchitectures, the proposed method successfully predicts the complex\ntrajectories of hypersonic missiles with high accuracy, offering a significant\ncontribution to defense strategies and missile interception technologies. This\nresearch demonstrates the potential of advanced machine learning techniques in\nenhancing the predictive capabilities of defense systems.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15043v1", "AI": {"title_translation": "采用CNN-LSTM-GRU架构的高超音速导弹轨迹高级预测", "tldr": "本研究提出了一种结合CNN、LSTM和GRU的混合深度学习方法，以高精度预测高超音速导弹的复杂轨迹，为防御策略和导弹拦截技术做出贡献。", "motivation": "高超音速导弹因其极高的速度和机动性对国家安全构成重大挑战，因此准确的轨迹预测对于有效的反制措施至关重要。", "method": "采用了一种新颖的混合深度学习方法，该方法集成了卷积神经网络（CNNs）、长短期记忆（LSTM）网络和门控循环单元（GRUs）。", "result": "所提出的方法成功地以高精度预测了高超音速导弹的复杂轨迹。", "conclusion": "这项研究展示了先进机器学习技术在增强防御系统预测能力方面的潜力，并为防御策略和导弹拦截技术做出了重要贡献。", "translation": "国防工业的进步对于确保国家安全至关重要，为应对新兴威胁提供强有力的保护。在这些威胁中，高超音速导弹因其极高的速度和机动性而构成了严峻挑战，因此准确的轨迹预测对于有效的反制措施至关重要。本文通过采用一种新颖的混合深度学习方法来解决这一挑战，该方法集成了卷积神经网络（CNNs）、长短期记忆（LSTM）网络和门控循环单元（GRUs）。通过利用这些架构的优势，所提出的方法成功地以高精度预测了高超音速导弹的复杂轨迹，为防御策略和导弹拦截技术做出了重要贡献。这项研究展示了先进机器学习技术在增强防御系统预测能力方面的潜力。", "summary": "本文提出了一种新颖的混合深度学习模型，结合了卷积神经网络（CNNs）、长短期记忆（LSTM）网络和门控循环单元（GRUs），旨在解决高超音速导弹轨迹预测的挑战。该方法能够高精度预测复杂的高超音速导弹轨迹，从而增强防御系统的预测能力，并对防御策略和导弹拦截技术做出重要贡献。", "keywords": "高超音速导弹, 轨迹预测, 深度学习, CNN-LSTM-GRU", "comments": "这项研究的创新之处在于将CNN、LSTM和GRU这三种深度学习架构有效地结合起来，形成一个混合模型，专门用于解决高超音速导弹轨迹预测这一复杂且关键的问题。其重要性在于为国家防御系统提供了更精确的预测能力，有助于开发更有效的反制措施和拦截技术。"}}
{"id": "2506.15098", "title": "Enhancement Report Approval Prediction: A Comparative Study of Large Language Models", "authors": ["Haosheng Zuo", "Feifei Niu", "Chuanyi Li"], "summary": "Enhancement reports (ERs) serve as a critical communication channel between\nusers and developers, capturing valuable suggestions for software improvement.\nHowever, manually processing these reports is resource-intensive, leading to\ndelays and potential loss of valuable insights. To address this challenge,\nenhancement report approval prediction (ERAP) has emerged as a research focus,\nleveraging machine learning techniques to automate decision-making. While\ntraditional approaches have employed feature-based classifiers and deep\nlearning models, recent advancements in large language models (LLM) present new\nopportunities for enhancing prediction accuracy. This study systematically\nevaluates 18 LLM variants (including BERT, RoBERTa, DeBERTa-v3, ELECTRA, and\nXLNet for encoder models; GPT-3.5-turbo, GPT-4o-mini, Llama 3.1 8B, Llama 3.1\n8B Instruct and DeepSeek-V3 for decoder models) against traditional methods\n(CNN/LSTM-BERT/GloVe). Our experiments reveal two key insights: (1)\nIncorporating creator profiles increases unfine-tuned decoder-only models'\noverall accuracy by 10.8 percent though it may introduce bias; (2) LoRA\nfine-tuned Llama 3.1 8B Instruct further improve performance, reaching 79\npercent accuracy and significantly enhancing recall for approved reports (76.1\npercent vs. LSTM-GLOVE's 64.1 percent), outperforming traditional methods by 5\npercent under strict chronological evaluation and effectively addressing class\nimbalance issues. These findings establish LLM as a superior solution for ERAP,\ndemonstrating their potential to streamline software maintenance workflows and\nimprove decision-making in real-world development environments. We also\ninvestigated and summarized the ER cases where the large models underperformed,\nproviding valuable directions for future research.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15098v1", "AI": {"title_translation": "增强报告批准预测：大型语言模型的比较研究", "tldr": "本研究比较了大型语言模型（LLMs）与传统方法在增强报告批准预测（ERAP）方面的性能，发现LLMs特别是经过LoRA微调的Llama 3.1 8B Instruct模型表现更优，能有效提高预测准确性和召回率。", "motivation": "手动处理增强报告（ERs）耗费资源且效率低下，导致延迟和潜在的洞察力损失。为解决此问题，需要自动化增强报告批准决策，而大型语言模型（LLM）的最新进展为提高预测准确性提供了新机会。", "method": "本研究系统地评估了18种LLM变体（包括BERT、RoBERTa、DeBERTa-v3、ELECTRA、XLNet等编码器模型；GPT-3.5-turbo、GPT-4o-mini、Llama 3.1 8B、Llama 3.1 8B Instruct和DeepSeek-V3等解码器模型），并将其与传统方法（CNN/LSTM-BERT/GloVe）进行比较。实验中还探讨了引入创建者资料和LoRA微调对模型性能的影响，并在严格的时间顺序评估下进行测试，以解决类不平衡问题。", "result": "1. 引入创建者资料可使未微调的仅解码器模型的整体准确率提高10.8%，尽管可能引入偏差。2. 经过LoRA微调的Llama 3.1 8B Instruct模型进一步提升了性能，达到79%的准确率，并显著提高了批准报告的召回率（76.1% vs. LSTM-GLOVE的64.1%），在严格的时间顺序评估下比传统方法高出5%，并有效解决了类不平衡问题。研究还总结了大型模型表现不佳的ER案例。", "conclusion": "大型语言模型（LLMs）被证明是增强报告批准预测（ERAP）的优越解决方案，展示了其在简化软件维护工作流程和改进实际开发环境中决策的潜力。", "translation": "增强报告（ERs）是用户和开发者之间重要的沟通渠道，捕获有价值的软件改进建议。然而，手动处理这些报告是资源密集型的，导致延迟和潜在的有价值洞察力的损失。为了应对这一挑战，增强报告批准预测（ERAP）已成为一个研究焦点，它利用机器学习技术自动化决策。虽然传统方法采用了基于特征的分类器和深度学习模型，但大型语言模型（LLM）的最新进展为提高预测准确性带来了新的机会。本研究系统地评估了18种LLM变体（包括BERT、RoBERTa、DeBERTa-v3、ELECTRA和XLNet等编码器模型；GPT-3.5-turbo、GPT-4o-mini、Llama 3.1 8B、Llama 3.1 8B Instruct和DeepSeek-V3等解码器模型），并将其与传统方法（CNN/LSTM-BERT/GloVe）进行比较。我们的实验揭示了两个关键见解：（1）引入创建者资料可使未微调的仅解码器模型的整体准确率提高10.8%，尽管可能引入偏差；（2）经过LoRA微调的Llama 3.1 8B Instruct模型进一步提升了性能，达到79%的准确率，并显著提高了批准报告的召回率（76.1% vs. LSTM-GLOVE的64.1%），在严格的时间顺序评估下比传统方法高出5%，并有效解决了类不平衡问题。这些发现确立了LLM作为ERAP的优越解决方案，展示了它们简化软件维护工作流程和改进实际开发环境中决策的潜力。我们还调查并总结了大型模型表现不佳的ER案例，为未来的研究提供了宝贵的方向。", "summary": "本研究旨在通过比较大型语言模型（LLMs）与传统机器学习方法，提升增强报告（ERs）的自动批准预测准确性。研究评估了18种LLM变体（包括编码器和解码器模型）与CNN/LSTM等传统方法的性能。结果表明，引入创建者资料可提高未微调解码器模型的准确率，而LoRA微调的Llama 3.1 8B Instruct模型表现最佳，达到79%的准确率和76.1%的批准报告召回率，显著优于传统方法。研究证实LLMs是ERAP的更优解决方案，有助于优化软件维护流程，并为未来研究提供了方向。", "keywords": "大型语言模型, 增强报告批准预测, 比较研究, Llama 3.1, 软件维护", "comments": "该研究创新性地将多种大型语言模型应用于增强报告批准预测这一实际软件工程问题，并进行了全面的比较研究。其重要性在于证明了LLMs在自动化决策和提高效率方面的巨大潜力，特别是在处理非结构化文本数据方面。引入创建者资料和LoRA微调的策略也为LLM在特定领域的应用提供了有价值的经验。研究还指出了LLMs表现不佳的案例，为未来的改进指明了方向，具有很强的实践指导意义。"}}
{"id": "2506.14968", "title": "FEAST: A Flexible Mealtime-Assistance System Towards In-the-Wild Personalization", "authors": ["Rajat Kumar Jenamani", "Tom Silver", "Ben Dodson", "Shiqin Tong", "Anthony Song", "Yuting Yang", "Ziang Liu", "Benjamin Howe", "Aimee Whitneck", "Tapomayukh Bhattacharjee"], "summary": "Physical caregiving robots hold promise for improving the quality of life of\nmillions worldwide who require assistance with feeding. However, in-home meal\nassistance remains challenging due to the diversity of activities (e.g.,\neating, drinking, mouth wiping), contexts (e.g., socializing, watching TV),\nfood items, and user preferences that arise during deployment. In this work, we\npropose FEAST, a flexible mealtime-assistance system that can be personalized\nin-the-wild to meet the unique needs of individual care recipients. Developed\nin collaboration with two community researchers and informed by a formative\nstudy with a diverse group of care recipients, our system is guided by three\nkey tenets for in-the-wild personalization: adaptability, transparency, and\nsafety. FEAST embodies these principles through: (i) modular hardware that\nenables switching between assisted feeding, drinking, and mouth-wiping, (ii)\ndiverse interaction methods, including a web interface, head gestures, and\nphysical buttons, to accommodate diverse functional abilities and preferences,\nand (iii) parameterized behavior trees that can be safely and transparently\nadapted using a large language model. We evaluate our system based on the\npersonalization requirements identified in our formative study, demonstrating\nthat FEAST offers a wide range of transparent and safe adaptations and\noutperforms a state-of-the-art baseline limited to fixed customizations. To\ndemonstrate real-world applicability, we conduct an in-home user study with two\ncare recipients (who are community researchers), feeding them three meals each\nacross three diverse scenarios. We further assess FEAST's ecological validity\nby evaluating with an Occupational Therapist previously unfamiliar with the\nsystem. In all cases, users successfully personalize FEAST to meet their\nindividual needs and preferences. Website: https://emprise.cs.cornell.edu/feast", "comment": "RSS 2025 - Outstanding Paper Award & Outstanding Systems Paper Award\n  Finalist", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14968v1", "AI": {"title_translation": "FEAST: 一种面向野外个性化的柔性用餐辅助系统", "tldr": "FEAST是一种灵活的用餐辅助机器人系统，可以根据个人需求进行现场个性化，并通过模块化硬件、多样化交互和参数化行为树实现适应性、透明度和安全性。", "motivation": "物理护理机器人对需要喂食帮助的人具有改善生活质量的潜力，但家庭用餐辅助面临挑战，因为活动、环境、食物和用户偏好的多样性。", "method": "提出FEAST系统，旨在实现“野外”个性化，满足个体护理接受者的独特需求。系统开发与社区研究者合作，并基于对护理接受者的形成性研究。其设计遵循适应性、透明度和安全性三个原则，通过模块化硬件（喂食、饮水、擦嘴）、多样化交互方法（网页、头部姿势、物理按钮）和使用大型语言模型安全透明地调整的参数化行为树来实现。系统通过形成性研究中确定的个性化需求进行评估，并进行了家庭用户研究和职业治疗师评估。", "result": "FEAST提供了广泛的透明和安全适应性，并且优于仅限于固定定制的现有基线系统。在所有案例中，用户都成功地个性化FEAST以满足其个人需求和偏好。", "conclusion": "FEAST系统通过其灵活的、可个性化的设计，成功地在家庭环境中为护理接受者提供用餐辅助，证明了其在实际应用中的有效性和生态学效度。", "translation": "物理护理机器人有望改善全球数百万需要喂食帮助的人的生活质量。然而，由于部署过程中出现的活动（例如，进食、饮水、擦嘴）、环境（例如，社交、看电视）、食物种类和用户偏好的多样性，家庭用餐辅助仍然充满挑战。在这项工作中，我们提出了FEAST，一种灵活的用餐辅助系统，可以在“野外”进行个性化，以满足个体护理接受者的独特需求。该系统是与两位社区研究人员合作开发的，并由对不同护理接受者群体进行的形成性研究提供信息，其设计遵循“野外”个性化的三个关键原则：适应性、透明性和安全性。FEAST通过以下方式体现这些原则：（i）模块化硬件，能够实现在辅助喂食、饮水和擦嘴之间切换；（ii）多样化的交互方法，包括网页界面、头部姿势和物理按钮，以适应不同的功能能力和偏好；（iii）参数化行为树，可以使用大型语言模型安全透明地进行调整。我们根据形成性研究中确定的个性化要求对系统进行了评估，结果表明FEAST提供了广泛的透明和安全适应性，并且优于仅限于固定定制的现有基线系统。为了展示实际应用性，我们与两名护理接受者（他们是社区研究人员）进行了家庭用户研究，在三种不同场景下为他们提供了三餐。我们通过对一位以前不熟悉该系统的职业治疗师进行评估，进一步评估了FEAST的生态学效度。在所有情况下，用户都成功地个性化FEAST以满足其个人需求和偏好。", "summary": "本文介绍了FEAST，一个灵活的用餐辅助系统，旨在解决家庭护理中多样化的个性化需求。该系统通过模块化硬件、多种交互方式和基于大型语言模型的参数化行为树，实现了在“野外”的适应性、透明度和安全性。实验证明，FEAST在个性化能力上优于现有基线，并成功满足了不同护理接受者的独特需求，展现了其在实际应用中的潜力。", "keywords": "用餐辅助, 机器人护理, 个性化, 适应性, 行为树", "comments": "FEAST的创新之处在于其“野外个性化”的能力，这对于实际家庭护理环境中的多样性至关重要。通过结合模块化设计、多样化交互和LLM驱动的行为适应，该系统提供了一个实用的解决方案，超越了传统固定定制的限制。其与社区研究者的合作和对生态学效度的关注也增强了其现实意义和潜在影响力。"}}
{"id": "2506.14809", "title": "Impact of a Deployed LLM Survey Creation Tool through the IS Success Model", "authors": ["Peng Jiang", "Vinicius Cezar Monteiro de Lira", "Antonio Maiorino"], "summary": "Surveys are a cornerstone of Information Systems (IS) research, yet creating\nhigh-quality surveys remains labor-intensive, requiring both domain expertise\nand methodological rigor. With the evolution of large language models (LLMs),\nnew opportunities emerge to automate survey generation. This paper presents the\nreal-world deployment of an LLM-powered system designed to accelerate data\ncollection while maintaining survey quality. Deploying such systems in\nproduction introduces real-world complexity, including diverse user needs and\nquality control. We evaluate the system using the DeLone and McLean IS Success\nModel to understand how generative AI can reshape a core IS method. This study\nmakes three key contributions. To our knowledge, this is the first application\nof the IS Success Model to a generative AI system for survey creation. In\naddition, we propose a hybrid evaluation framework combining automated and\nhuman assessments. Finally, we implement safeguards that mitigate\npost-deployment risks and support responsible integration into IS workflows.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14809v1", "AI": {"title_translation": "通过IS成功模型评估已部署的LLM问卷创建工具的影响", "tldr": "本文评估了一个已部署的LLM驱动的问卷创建工具，使用IS成功模型，并提出了混合评估框架和风险缓解措施。", "motivation": "问卷创建耗时且需要专业知识，而大型语言模型（LLM）提供了自动化生成问卷的新机会。本文旨在评估一个LLM驱动的系统在实际部署中如何加速数据收集并保持问卷质量，同时应对现实世界的复杂性。", "method": "本文部署了一个LLM驱动的问卷创建系统，并使用DeLone和McLean IS成功模型对其进行评估。研究还提出了一个结合自动化和人工评估的混合评估框架，并实施了减轻部署后风险的保障措施。", "result": "据作者所知，这是首次将IS成功模型应用于生成式AI问卷创建系统。此外，研究提出了一个混合评估框架，并实施了旨在减轻部署后风险的保障措施。", "conclusion": "部署LLM驱动的问卷创建工具能够加速数据收集，并通过IS成功模型进行评估，同时需要结合混合评估和风险缓解措施以实现负责任的集成到IS工作流程中。", "translation": "问卷是信息系统（IS）研究的基石，然而创建高质量的问卷仍然是劳动密集型的，既需要领域专业知识，也需要方法论的严谨性。随着大型语言模型（LLM）的发展，自动化问卷生成的新机会应运而生。本文介绍了一个LLM驱动系统的实际部署，该系统旨在加速数据收集，同时保持问卷质量。在生产环境中部署此类系统会带来现实世界的复杂性，包括多样化的用户需求和质量控制。我们使用DeLone和McLean IS成功模型评估了该系统，以了解生成式AI如何重塑核心IS方法。本研究做出了三项关键贡献。据我们所知，这是IS成功模型首次应用于生成式AI问卷创建系统。此外，我们提出了一个结合自动化和人工评估的混合评估框架。最后，我们实施了保障措施，以减轻部署后风险并支持负责任地集成到IS工作流程中。", "summary": "本文探讨了大型语言模型（LLM）在自动化问卷创建方面的潜力，并介绍了一个已部署的LLM驱动问卷系统。该研究利用DeLone和McLean IS成功模型评估了该系统，旨在理解生成式AI如何影响信息系统（IS）的核心方法。主要贡献包括首次将IS成功模型应用于生成式AI问卷系统，提出了一种结合自动化和人工评估的混合评估框架，以及实施了降低部署风险的保障措施。", "keywords": "大型语言模型, 问卷创建, IS成功模型, 生成式AI, 混合评估", "comments": "本文的创新之处在于首次将经典的IS成功模型应用于评估生成式AI系统，这为理解AI在IS领域的实际影响提供了一个新的视角。提出的混合评估框架和风险缓解措施对于未来AI工具的负责任部署具有重要的实践指导意义，强调了在实际应用中对用户需求和质量控制的关注。"}}
{"id": "2506.15397", "title": "Learn to Vaccinate: Combining Structure Learning and Effective Vaccination for Epidemic and Outbreak Control", "authors": ["Sepehr Elahi", "Paula Mürmann", "Patrick Thiran"], "summary": "The Susceptible-Infected-Susceptible (SIS) model is a widely used model for\nthe spread of information and infectious diseases, particularly non-immunizing\nones, on a graph. Given a highly contagious disease, a natural question is how\nto best vaccinate individuals to minimize the disease's extinction time. While\nprevious works showed that the problem of optimal vaccination is closely linked\nto the NP-hard Spectral Radius Minimization (SRM) problem, they assumed that\nthe graph is known, which is often not the case in practice. In this work, we\nconsider the problem of minimizing the extinction time of an outbreak modeled\nby an SIS model where the graph on which the disease spreads is unknown and\nonly the infection states of the vertices are observed. To this end, we split\nthe problem into two: learning the graph and determining effective vaccination\nstrategies. We propose a novel inclusion-exclusion-based learning algorithm\nand, unlike previous approaches, establish its sample complexity for graph\nrecovery. We then detail an optimal algorithm for the SRM problem and prove\nthat its running time is polynomial in the number of vertices for graphs with\nbounded treewidth. This is complemented by an efficient and effective\npolynomial-time greedy heuristic for any graph. Finally, we present experiments\non synthetic and real-world data that numerically validate our learning and\nvaccination algorithms.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15397v1", "AI": {"title_translation": "学会接种：结合结构学习和有效接种以控制流行病和疫情", "tldr": "本文提出了一种在未知传播图上通过学习图结构和优化疫苗接种策略来最小化SIS模型疫情消亡时间的方法，并提供了理论保证和实验验证。", "motivation": "传统的疫苗接种优化问题（最小化疾病消亡时间）假设传播图是已知的，但实际情况中往往无法获得完整的图信息。因此，本研究旨在解决在图结构未知且仅能观察到顶点感染状态的情况下，如何有效接种以最小化疫情消亡时间的问题。", "method": "作者将问题分解为两部分：图学习和确定有效的疫苗接种策略。他们提出了一种新颖的基于包含-排除的学习算法，并首次建立了其图恢复的样本复杂度。此外，还详细介绍了一种针对谱半径最小化（SRM）问题的最优算法，并证明了其在有界树宽图上的运行时间是顶点数量的多项式时间。对于任意图，则提供了一种高效且有效的多项式时间贪婪启发式算法。", "result": "本文建立了图恢复学习算法的样本复杂度，并证明了最优SRM算法在有界树宽图上的多项式时间运行效率。通过在合成数据和真实世界数据上的实验，数值验证了所提出的学习和疫苗接种算法的有效性。", "conclusion": "本研究成功地解决了在传播图未知的情况下，如何通过结合图结构学习和优化疫苗接种策略来有效控制流行病的问题，并提供了理论依据和实验支持。", "translation": "易感-感染-易感（SIS）模型是一种广泛用于图上传播信息和传染病（特别是非免疫性疾病）的模型。对于一种高度传染性的疾病，一个自然的问题是如何最好地接种个体以最小化疾病的消亡时间。虽然以前的工作表明最优疫苗接种问题与NP难的谱半径最小化（SRM）问题密切相关，但它们都假设图是已知的，这在实践中往往并非如此。在这项工作中，我们考虑了在疾病传播图未知且仅能观察到顶点感染状态的情况下，最小化由SIS模型建模的疫情消亡时间的问题。为此，我们将问题分为两部分：学习图和确定有效的疫苗接种策略。我们提出了一种新颖的基于包含-排除的学习算法，并且与以前的方法不同，我们建立了其图恢复的样本复杂度。然后，我们详细介绍了一种用于SRM问题的最优算法，并证明了其在有界树宽图上的运行时间是顶点数量的多项式时间。此外，还为任何图提供了一种高效且有效的多项式时间贪婪启发式算法。最后，我们在合成数据和真实世界数据上进行了实验，数值验证了我们的学习和疫苗接种算法。", "summary": "该研究针对SIS模型下未知传播图的疫情控制问题，提出了一种结合图结构学习和疫苗接种策略优化的新方法。它将问题分解为图学习和疫苗接种两部分，引入了基于包含-排除的图学习算法并分析了其样本复杂度。同时，提供了一种针对谱半径最小化（SRM）问题的最优算法（对于有界树宽图）和一种通用的贪婪启发式算法。实验结果验证了所提算法在控制疫情方面的有效性。", "keywords": "疫情控制, SIS模型, 图学习, 疫苗接种, 谱半径最小化", "comments": "本文的创新点在于解决了实际应用中常见的图结构未知问题，并为图学习算法提供了理论上的样本复杂度保证。它将图学习与疫苗接种优化相结合，为疫情控制提供了更实用的解决方案。特别是对有界树宽图给出了多项式时间的最优SRM算法，这在理论上具有重要意义。同时，提供的贪婪启发式算法也保证了对任意图的实用性。"}}
{"id": "2506.15127", "title": "New Constructions of Full Flag Codes Based on Partial Spreads", "authors": ["Xiang Han", "Xinran Li", "Gang Wang"], "summary": "Flag codes are a class of multishot network codes comprising sequences of\nnested subspaces (flags) within the vector space $\\mathbb{F}_q^n$, where $q$ is\na prime power. In this paper, we propose a family of constructions for full\nflag codes based on partial spreads. The distances of this family include\nmaximum distance (optimum distance flag codes), second-maximum distance\n(quasi-optimum distance flag codes), as well as other feasible values. The\nstructure of these flag codes resembles that of a \\textquotedblleft sandwich\",\nconsisting of one layer of companion matrix and two layers of partial spreads.\nFurthermore, we present an efficient decoding algorithm for these codes.", "comment": "24 pages", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15127v1", "AI": {"title_translation": "基于部分扩充的全旗码的新构造", "tldr": "本文提出了一系列基于部分扩充的全旗码构造方法，其距离包括最大距离和次最大距离，并提供了一种高效的解码算法。", "motivation": "旗码是多发射网络编码中的一类，本文旨在为全旗码提出新的构造方法。", "method": "本文提出了一系列基于部分扩充的全旗码构造方法。这些旗码的结构类似于“三明治”，由一层伴随矩阵和两层部分扩充组成。此外，还提出了一种高效的解码算法。", "result": "所构造的旗码的距离包括最大距离（最优距离旗码）、次最大距离（准最优距离旗码）以及其他可行值。", "conclusion": "本文成功提出了基于部分扩充的全旗码新构造，并为其提供了高效的解码算法，这些旗码具有多种距离特性。", "translation": "旗码是一类多发射网络编码，包含向量空间 $\\mathbb{F}_q^n$ 中嵌套子空间（旗）的序列，其中 $q$ 是素数幂。在本文中，我们提出了一系列基于部分扩充的全旗码构造方法。该族的距离包括最大距离（最优距离旗码）、次最大距离（准最优距离旗码）以及其他可行值。这些旗码的结构类似于“三明治”，由一层伴随矩阵和两层部分扩充组成。此外，我们还提出了一种高效的解码算法。", "summary": "本文提出了一种基于部分扩充的全旗码的新构造方法。这些旗码的距离可以达到最大距离和次最大距离，其结构类似于“三明治”，并且提供了一种高效的解码算法。", "keywords": "旗码, 部分扩充, 网络编码, 解码算法, 最大距离", "comments": "这项研究为旗码的构造提供了新的视角，特别是其“三明治”结构和高效解码算法的提出，可能对网络编码领域具有重要意义。"}}
{"id": "2506.14823", "title": "ViLLa: A Neuro-Symbolic approach for Animal Monitoring", "authors": ["Harsha Koduri"], "summary": "Monitoring animal populations in natural environments requires systems that\ncan interpret both visual data and human language queries. This work introduces\nViLLa (Vision-Language-Logic Approach), a neuro-symbolic framework designed for\ninterpretable animal monitoring. ViLLa integrates three core components: a\nvisual detection module for identifying animals and their spatial locations in\nimages, a language parser for understanding natural language queries, and a\nsymbolic reasoning layer that applies logic-based inference to answer those\nqueries. Given an image and a question such as \"How many dogs are in the\nscene?\" or \"Where is the buffalo?\", the system grounds visual detections into\nsymbolic facts and uses predefined rules to compute accurate answers related to\ncount, presence, and location. Unlike end-to-end black-box models, ViLLa\nseparates perception, understanding, and reasoning, offering modularity and\ntransparency. The system was evaluated on a range of animal imagery tasks and\ndemonstrates the ability to bridge visual content with structured,\nhuman-interpretable queries.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14823v1", "AI": {"title_translation": "ViLLa：一种用于动物监测的神经符号方法", "tldr": "ViLLa是一个神经符号框架，通过结合视觉检测、语言解析和符号推理，实现可解释的动物监测，能够回答关于图像中动物数量、存在和位置的自然语言查询。", "motivation": "在自然环境中监测动物种群需要能够解释视觉数据和人类语言查询的系统。", "method": "ViLLa（Vision-Language-Logic Approach）是一个神经符号框架，整合了三个核心组件：用于识别图像中动物及其空间位置的视觉检测模块、用于理解自然语言查询的语言解析器，以及应用基于逻辑推理来回答这些查询的符号推理层。系统将视觉检测结果转化为符号事实，并使用预定义规则计算与计数、存在和位置相关的答案。", "result": "该系统在各种动物图像任务上进行了评估，并展示了将视觉内容与结构化、人类可解释查询连接起来的能力。", "conclusion": "ViLLa通过分离感知、理解和推理，提供了模块化和透明度，实现了可解释的动物监测，优于端到端黑盒模型。", "translation": "在自然环境中监测动物种群需要能够解释视觉数据和人类语言查询的系统。这项工作引入了ViLLa（视觉-语言-逻辑方法），一个为可解释动物监测设计的神经符号框架。ViLLa集成了三个核心组件：一个用于识别图像中动物及其空间位置的视觉检测模块，一个用于理解自然语言查询的语言解析器，以及一个应用基于逻辑推理来回答这些查询的符号推理层。给定一幅图像和一个问题，例如“场景中有多少只狗？”或“水牛在哪里？”，系统将视觉检测结果转化为符号事实，并使用预定义规则计算与计数、存在和位置相关的准确答案。与端到端黑盒模型不同，ViLLa将感知、理解和推理分开，提供了模块化和透明度。该系统在各种动物图像任务上进行了评估，并展示了将视觉内容与结构化、人类可解释查询连接起来的能力。", "summary": "ViLLa是一个创新的神经符号框架，专为可解释的动物监测而设计。它通过整合视觉检测、自然语言理解和符号推理，能够处理图像并回答关于动物计数、存在和位置的自然语言查询。与传统的黑盒模型不同，ViLLa强调模块化和透明度，通过将感知、理解和推理分离，实现了视觉内容与人类可解释查询的有效桥接。", "keywords": "动物监测, 神经符号, 视觉-语言, 符号推理, 可解释AI", "comments": "ViLLa的创新之处在于其神经符号方法，它结合了深度学习的感知能力和符号逻辑的推理能力，实现了可解释且透明的系统。这对于需要高可信度和可追溯性的动物监测等应用至关重要，弥补了传统端到端黑盒模型在解释性方面的不足。其模块化设计也增强了系统的灵活性和可维护性。"}}
{"id": "2506.15440", "title": "Acore-CIM: build accurate and reliable mixed-signal CIM cores with RISC-V controlled self-calibration", "authors": ["Omar Numan", "Gaurav Singh", "Kazybek Adam", "Jelin Leslin", "Aleksi Korsman", "Otto Simola", "Marko Kosunen", "Jussi Ryynänen", "Martin Andraud"], "summary": "Developing accurate and reliable Compute-In-Memory (CIM) architectures is\nbecoming a key research focus to accelerate Artificial Intelligence (AI) tasks\non hardware, particularly Deep Neural Networks (DNNs). In that regard, there\nhas been significant interest in analog and mixed-signal CIM architectures\naimed at increasing the efficiency of data storage and computation to handle\nthe massive amount of data needed by DNNs. Specifically, resistive mixed-signal\nCIM cores are pushed by recent progresses in emerging Non-Volatile Memory\n(eNVM) solutions. Yet, mixed-signal CIM computing cores still face several\nintegration and reliability challenges that hinder their large-scale adoption\ninto end-to-end AI computing systems. In terms of integration, resistive and\neNVM-based CIM cores need to be integrated with a control processor to realize\nend-to-end AI acceleration. Moreover, SRAM-based CIM architectures are still\nmore efficient and easier to program than their eNVM counterparts. In terms of\nreliability, analog circuits are more susceptible to variations, leading to\ncomputation errors and degraded accuracy. This work addresses these two\nchallenges by proposing a self-calibrated mixed-signal CIM accelerator SoC,\nfabricated in 22-nm FDSOI technology. The integration is facilitated by (1) the\nCIM architecture, combining the density and ease of SRAM-based weight storage\nwith multi-bit computation using linear resistors, and (2) an open-source\nprogramming and testing strategy for CIM systems. The accuracy and reliability\nare enabled through an automated RISC-V controlled on-chip calibration,\nallowing us to improve the compute SNR by 25 to 45% across multiple columns to\nreach 18-24 dB. To showcase further integration possibilities, we show how our\nproof-of-concept SoC can be extended to recent high-density linear resistor\ntechnologies for enhanced computing performance.", "comment": "This work has been submitted to the IEEE for possible publication. 12\n  pages, 10 figures, 2 tables", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15440v1", "AI": {"title_translation": "Acore-CIM：构建基于RISC-V控制自校准的精确可靠混合信号CIM核", "tldr": "本文提出Acore-CIM，一种基于22纳米FDSOI技术、RISC-V控制自校准的混合信号CIM加速器SoC，旨在解决混合信号CIM在集成和可靠性方面的挑战，显著提升计算信噪比。", "motivation": "混合信号存内计算（CIM）架构在加速AI任务（特别是深度神经网络）方面具有巨大潜力，但其大规模应用面临集成和可靠性挑战，如需要与控制处理器集成、模拟电路易受变异影响导致计算错误等。", "method": "本文提出一个在22纳米FDSOI技术下制造的自校准混合信号CIM加速器SoC。通过结合SRAM基权重存储的密度和线性电阻的多位计算来促进集成，并采用开源编程和测试策略。通过自动化RISC-V控制的片上校准实现精度和可靠性。", "result": "该方法将多个列的计算信噪比（SNR）提高了25%到45%，达到18-24 dB。", "conclusion": "Acore-CIM成功解决了混合信号CIM核的集成和可靠性挑战，通过RISC-V控制的自校准显著提升了计算精度，为端到端AI计算系统的大规模部署奠定了基础。", "translation": "开发精确可靠的存内计算（CIM）架构正成为加速硬件上人工智能（AI）任务，特别是深度神经网络（DNNs）的关键研究重点。在这方面，模拟和混合信号CIM架构引起了极大的兴趣，旨在提高数据存储和计算的效率，以处理DNN所需的大量数据。具体而言，新兴非易失性存储器（eNVM）解决方案的最新进展推动了电阻式混合信号CIM核的发展。然而，混合信号CIM计算核仍然面临若干集成和可靠性挑战，阻碍了它们大规模应用于端到端AI计算系统。在集成方面，电阻式和基于eNVM的CIM核需要与控制处理器集成以实现端到端AI加速。此外，基于SRAM的CIM架构仍然比其eNVM对应物更高效且更容易编程。在可靠性方面，模拟电路更容易受到变异的影响，导致计算错误和精度下降。这项工作通过提出一个在22纳米FDSOI技术中制造的自校准混合信号CIM加速器SoC来解决这两个挑战。集成通过（1）结合SRAM基权重存储的密度和易用性与使用线性电阻的多位计算的CIM架构，以及（2）用于CIM系统的开源编程和测试策略来促进。精度和可靠性通过自动化RISC-V控制的片上校准实现，使我们能够将多个列的计算信噪比（SNR）提高25%到45%，达到18-24 dB。为了展示进一步的集成可能性，我们展示了我们的概念验证SoC如何扩展到最近的高密度线性电阻技术，以增强计算性能。", "summary": "本文针对混合信号存内计算（CIM）在AI加速应用中面临的集成和可靠性挑战，提出了一种名为Acore-CIM的自校准混合信号CIM加速器SoC。该SoC结合了SRAM基存储与线性电阻多位计算的优势，并通过RISC-V控制的片上自校准机制，显著提升了计算精度和可靠性，将计算信噪比提高了25-45%至18-24 dB，为CIM系统的大规模部署提供了解决方案。", "keywords": "存内计算, 混合信号, 自校准, RISC-V, AI加速", "comments": "本文的创新之处在于提出了一种结合SRAM和线性电阻的混合信号CIM架构，并引入了RISC-V控制的片上自校准机制，有效解决了混合信号CIM在集成和可靠性方面的核心挑战。该工作通过量化指标展示了精度提升，并考虑了开源策略和未来扩展性，对推动CIM技术在AI硬件加速中的实际应用具有重要意义。"}}
{"id": "2506.15196", "title": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges", "authors": ["Xianliang Yang", "Ling Zhang", "Haolong Qian", "Lei Song", "Jiang Bian"], "summary": "Heuristic algorithms play a vital role in solving combinatorial optimization\n(CO) problems, yet traditional designs depend heavily on manual expertise and\nstruggle to generalize across diverse instances. We introduce\n\\textbf{HeurAgenix}, a two-stage hyper-heuristic framework powered by large\nlanguage models (LLMs) that first evolves heuristics and then selects among\nthem automatically. In the heuristic evolution phase, HeurAgenix leverages an\nLLM to compare seed heuristic solutions with higher-quality solutions and\nextract reusable evolution strategies. During problem solving, it dynamically\npicks the most promising heuristic for each problem state, guided by the LLM's\nperception ability. For flexibility, this selector can be either a\nstate-of-the-art LLM or a fine-tuned lightweight model with lower inference\ncost. To mitigate the scarcity of reliable supervision caused by CO complexity,\nwe fine-tune the lightweight heuristic selector with a dual-reward mechanism\nthat jointly exploits singals from selection preferences and state perception,\nenabling robust selection under noisy annotations. Extensive experiments on\ncanonical benchmarks show that HeurAgenix not only outperforms existing\nLLM-based hyper-heuristics but also matches or exceeds specialized solvers.\nCode is available at https://github.com/microsoft/HeurAgenix.", "comment": "27 pages,9 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15196v1", "AI": {"title_translation": "HeurAgenix: 利用大型语言模型解决复杂组合优化挑战", "tldr": "HeurAgenix是一个两阶段的超启发式框架，利用大型语言模型（LLM）来进化启发式算法并自动选择最佳算法，在组合优化问题上表现优于现有方法并媲美专业求解器。", "motivation": "传统的启发式算法设计严重依赖人工专业知识，并且难以泛化到不同的实例，这促使研究者寻找一种更通用、更自动化的解决方案来解决组合优化（CO）问题。", "method": "HeurAgenix是一个两阶段的超启发式框架，由大型语言模型（LLM）驱动。在启发式进化阶段，它利用LLM比较种子启发式解与高质量解，并提取可重用的进化策略。在问题解决阶段，它根据LLM的感知能力动态选择最有前景的启发式算法。选择器可以是LLM或经过微调的轻量级模型。为解决可靠监督数据稀缺问题，轻量级启发式选择器通过双重奖励机制进行微调，该机制结合了选择偏好和状态感知的信号，以在嘈杂的标注下实现鲁棒选择。", "result": "广泛的实验表明，HeurAgenix不仅超越了现有的基于LLM的超启发式算法，而且与专用求解器持平或表现更优。", "conclusion": "HeurAgenix通过其LLM驱动的两阶段超启发式框架，成功地解决了传统启发式算法在泛化性和自动化方面的挑战，并在复杂组合优化问题上取得了卓越的性能。", "translation": "启发式算法在解决组合优化（CO）问题中发挥着至关重要的作用，但传统设计严重依赖人工专业知识，并且难以泛化到不同的实例。我们引入了 HeurAgenix，一个由大型语言模型（LLM）驱动的两阶段超启发式框架，它首先进化启发式算法，然后自动在它们之间进行选择。在启发式进化阶段，HeurAgenix 利用 LLM 比较种子启发式解与更高质量的解，并提取可重用的进化策略。在问题解决过程中，它根据 LLM 的感知能力动态选择每个问题状态最有前景的启发式算法。为了灵活性，该选择器可以是最先进的 LLM，也可以是推理成本较低的经过微调的轻量级模型。为了缓解 CO 复杂性导致的可靠监督稀缺问题，我们使用双重奖励机制微调了轻量级启发式选择器，该机制共同利用了选择偏好和状态感知的信号，从而在嘈杂的标注下实现鲁棒选择。在经典基准上的大量实验表明，HeurAgenix 不仅优于现有的基于 LLM 的超启发式算法，而且与专用求解器持平或表现更优。代码可在 https://github.com/microsoft/HeurAgenix 获取。", "summary": "HeurAgenix是一个创新的两阶段超启发式框架，旨在利用大型语言模型（LLM）解决复杂的组合优化（CO）问题。针对传统启发式算法依赖人工且泛化性差的痛点，HeurAgenix在第一阶段利用LLM进化启发式策略，在第二阶段根据LLM的感知能力动态选择最佳启发式算法。为提高鲁棒性，轻量级选择器采用双重奖励机制进行微调。实验证明，HeurAgenix在CO基准测试中表现优于其他LLM超启发式方法，并能与专业求解器媲美。", "keywords": "组合优化, 大型语言模型, 超启发式, 启发式进化, 动态选择", "comments": "HeurAgenix的创新之处在于其将LLM引入超启发式框架，实现了启发式算法的自动化进化和动态选择，克服了传统方法对人工经验的过度依赖和泛化能力不足的问题。其双重奖励机制在解决CO问题中监督数据稀缺的挑战方面具有重要意义。该研究为利用LLM解决复杂优化问题开辟了新路径，具有较高的实用价值和研究潜力。"}}
{"id": "2506.15114", "title": "Parallel Data Object Creation: Towards Scalable Metadata Management in High-Performance I/O Library", "authors": ["Youjia Li", "Robert Latham", "Robert Ross", "Ankit Agrawal", "Alok Choudhary", "Wei-Keng Liao"], "summary": "High-level I/O libraries, such as HDF5 and PnetCDF, are commonly used by\nlarge-scale scientific applications to perform I/O tasks in parallel. These I/O\nlibraries store the metadata such as data types and dimensionality along with\nthe raw data in the same files. While these libraries are well-optimized for\nconcurrent access to the raw data, they are designed neither to handle a large\nnumber of data objects efficiently nor to create different data objects\nindependently by multiple processes, as they require applications to call data\nobject creation APIs collectively with consistent metadata among all processes.\nApplications that process data gathered from remote sensors, such as particle\ncollision experiments in high-energy physics, may generate data of different\nsizes from different sensors and desire to store them as separate data objects.\nFor such applications, the I/O library's requirement on collective data object\ncreation can become very expensive, as the cost of metadata consistency check\nincreases with the metadata volume as well as the number of processes. To\naddress this limitation, using PnetCDF as an experimental platform, we\ninvestigate solutions in this paper that abide the netCDF file format, as well\nas propose a new file header format that enables independent data object\ncreation. The proposed file header consists of two sections, an index table and\na list of metadata blocks. The index table contains the reference to the\nmetadata blocks and each block stores metadata of objects that can be created\ncollectively or independently. The new design achieves a scalable performance,\ncutting data object creation times by up to 582x when running on 4096 MPI\nprocesses to create 5,684,800 data objects in parallel. Additionally, the new\nmethod reduces the memory footprints, with each process requiring an amount of\nmemory space inversely proportional to the number of processes.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15114v1", "AI": {"title_translation": "并行数据对象创建：迈向高性能I/O库中可伸缩元数据管理", "tldr": "当前高性能I/O库在创建大量数据对象时，由于集体元数据一致性检查，难以实现可伸缩的独立创建。本文提出了一种针对PnetCDF的新文件头格式，实现了独立对象创建，显著提升了性能并减少了内存占用。", "motivation": "高级I/O库（如HDF5和PnetCDF）并非旨在高效处理大量数据对象或由多个进程独立创建不同数据对象，因为它们要求应用程序集体调用API并保持元数据一致性。这对于处理来自不同传感器（如高能物理）的异构数据应用程序而言，由于元数据一致性检查成本随元数据量和进程数增加而急剧上升，变得非常昂贵。", "method": "本文以PnetCDF为实验平台，研究了符合netCDF文件格式的解决方案，并提出了一种新的文件头格式。该文件头由索引表和元数据块列表组成，索引表引用元数据块，每个块存储可集体或独立创建的对象的元数据。", "result": "新设计实现了可伸缩的性能，在4096个MPI进程上并行创建5,684,800个数据对象时，数据对象创建时间缩短了582倍。此外，新方法减少了内存占用，每个进程所需的内存空间与进程数量成反比。", "conclusion": "所提出的新文件头设计显著提高了高性能I/O库中数据对象创建的可伸缩性和效率，特别适用于需要独立对象创建的场景，同时还减少了内存消耗。", "translation": "高级I/O库，如HDF5和PnetCDF，通常被大型科学应用用于并行执行I/O任务。这些I/O库将数据类型和维度等元数据与原始数据一起存储在同一文件中。虽然这些库针对原始数据的并发访问进行了优化，但它们并非旨在高效处理大量数据对象，也无法由多个进程独立创建不同的数据对象，因为它们要求应用程序集体调用数据对象创建API，并要求所有进程之间元数据保持一致。处理来自远程传感器（如高能物理中的粒子碰撞实验）收集的数据的应用程序，可能会从不同传感器生成不同大小的数据，并希望将它们作为单独的数据对象存储。对于此类应用程序，I/O库对集体数据对象创建的要求可能变得非常昂贵，因为元数据一致性检查的成本随元数据量和进程数量的增加而增加。为了解决这一限制，本文以PnetCDF作为实验平台，研究了符合netCDF文件格式的解决方案，并提出了一种新的文件头格式，该格式支持独立的数据对象创建。所提出的文件头由两部分组成：索引表和元数据块列表。索引表包含对元数据块的引用，每个块存储可以集体或独立创建的对象的元数据。新设计实现了可伸缩的性能，在4096个MPI进程上并行创建5,684,800个数据对象时，数据对象创建时间缩短了582倍。此外，新方法减少了内存占用，每个进程所需的内存空间与进程数量成反比。", "summary": "本文旨在解决高性能I/O库（如HDF5、PnetCDF）在并行创建大量数据对象时，因昂贵的集体元数据一致性检查而导致的扩展性限制。作者以PnetCDF为基础，提出了一种包含索引表和元数据块的新型文件头格式，以实现独立的数据对象创建，同时兼容netCDF文件格式。新设计显著提升了性能，在4096个MPI进程上将数据对象创建时间缩短了高达582倍，并有效降低了内存占用。", "keywords": "高性能I/O, 元数据管理, 并行数据对象创建, PnetCDF, 可伸缩性", "comments": "该论文为高性能I/O库提供了一项重要的架构改进，特别解决了并行数据对象创建中元数据管理的关键瓶颈。其创新之处在于提出了允许独立对象创建的文件头格式，从而摆脱了耗时的集体一致性检查。所展示的性能提升（高达582倍加速）和内存占用减少，突显了其对于处理异构数据的大规模科学应用的实际重要性。"}}
{"id": "2506.15113", "title": "Transit for All: Mapping Equitable Bike2Subway Connection using Region Representation Learning", "authors": ["Min Namgung", "JangHyeon Lee", "Fangyi Ding", "Yao-Yi Chiang"], "summary": "Ensuring equitable public transit access remains challenging, particularly in\ndensely populated cities like New York City (NYC), where low-income and\nminority communities often face limited transit accessibility. Bike-sharing\nsystems (BSS) can bridge these equity gaps by providing affordable first- and\nlast-mile connections. However, strategically expanding BSS into underserved\nneighborhoods is difficult due to uncertain bike-sharing demand at newly\nplanned (\"cold-start\") station locations and limitations in traditional\naccessibility metrics that may overlook realistic bike usage potential. We\nintroduce Transit for All (TFA), a spatial computing framework designed to\nguide the equitable expansion of BSS through three components: (1)\nspatially-informed bike-sharing demand prediction at cold-start stations using\nregion representation learning that integrates multimodal geospatial data, (2)\ncomprehensive transit accessibility assessment leveraging our novel weighted\nPublic Transport Accessibility Level (wPTAL) by combining predicted\nbike-sharing demand with conventional transit accessibility metrics, and (3)\nstrategic recommendations for new bike station placements that consider\npotential ridership and equity enhancement. Using NYC as a case study, we\nidentify transit accessibility gaps that disproportionately impact low-income\nand minority communities in historically underserved neighborhoods. Our results\nshow that strategically placing new stations guided by wPTAL notably reduces\ndisparities in transit access related to economic and demographic factors. From\nour study, we demonstrate that TFA provides practical guidance for urban\nplanners to promote equitable transit and enhance the quality of life in\nunderserved urban communities.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.15113v1", "AI": {"title_translation": "全民交通：利用区域表征学习绘制公平的自行车到地铁连接", "tldr": "本文提出了“全民交通”（TFA）框架，通过区域表征学习预测自行车共享需求并引入加权公共交通可达性水平（wPTAL），以指导自行车共享系统在服务不足社区的公平扩展，并在纽约市案例研究中展示了其减少交通不平等的有效性。", "motivation": "在纽约等人口稠密城市，低收入和少数族裔社区面临公共交通可达性有限的问题。自行车共享系统可以弥补这一公平差距，但战略性地将其扩展到服务不足的社区面临挑战，因为新设站点的需求不确定以及传统可达性指标可能忽略实际自行车使用潜力。", "method": "本文介绍了“全民交通”（TFA）空间计算框架，旨在通过三个组成部分指导自行车共享系统的公平扩展：1) 利用集成多模态地理空间数据的区域表征学习，对“冷启动”站点的自行车共享需求进行空间预测；2) 结合预测的自行车共享需求和传统交通可达性指标，利用新型加权公共交通可达性水平（wPTAL）进行全面的交通可达性评估；3) 考虑潜在乘客量和公平性提升，为新自行车站点提供战略性选址建议。", "result": "以纽约市为例，研究识别出对历史上服务不足的低收入和少数族裔社区产生不成比例影响的交通可达性差距。结果显示，在wPTAL指导下战略性地放置新站点显著减少了与经济和人口因素相关的交通可达性差异。", "conclusion": "本研究表明，“全民交通”（TFA）为城市规划者提供了实用指导，以促进公平交通并提高服务不足城市社区的生活质量。", "translation": "确保公平的公共交通可达性仍然具有挑战性，特别是在纽约市等人口稠密的城市，低收入和少数族裔社区往往面临有限的交通可达性。自行车共享系统（BSS）可以通过提供负担得起的首英里和末英里连接来弥合这些公平差距。然而，由于新规划的（“冷启动”）站点位置的自行车共享需求不确定以及传统可达性指标可能忽略实际自行车使用潜力，将BSS战略性地扩展到服务不足的社区很困难。我们引入了“全民交通”（TFA），这是一个空间计算框架，旨在通过三个组成部分指导BSS的公平扩展：(1) 使用集成多模态地理空间数据的区域表征学习，对冷启动站点的自行车共享需求进行空间预测；(2) 通过结合预测的自行车共享需求和传统交通可达性指标，利用我们新颖的加权公共交通可达性水平（wPTAL）进行全面的交通可达性评估；(3) 考虑潜在乘客量和公平性提升，为新自行车站点提供战略性建议。以纽约市作为案例研究，我们识别出对历史上服务不足的低收入和少数族裔社区产生不成比例影响的交通可达性差距。我们的结果表明，在wPTAL指导下战略性地放置新站点显著减少了与经济和人口因素相关的交通可达性差异。从我们的研究中，我们证明了TFA为城市规划者提供了实用指导，以促进公平交通并提高服务不足城市社区的生活质量。", "summary": "本文提出了一种名为“全民交通”（TFA）的空间计算框架，旨在解决城市中公共交通可达性不公平的问题，尤其是在低收入和少数族裔社区。该框架通过整合区域表征学习和多模态地理空间数据，预测新自行车共享站点的需求，并引入加权公共交通可达性水平（wPTAL）来评估和提升交通可达性。TFA还提供了战略性站点选址建议，以最大化潜在乘客量和公平性。在纽约市的案例研究表明，TFA能够有效识别并减少交通可达性差距，为城市规划者提供了促进公平交通和改善社区生活质量的实用工具。", "keywords": "自行车共享系统, 交通可达性, 区域表征学习, 公平交通, 城市规划", "comments": "该论文的创新之处在于其提出的“全民交通”（TFA）框架，特别是引入了区域表征学习来解决“冷启动”站点的需求预测不确定性，以及开发了wPTAL来更全面地评估交通可达性。这对于解决城市交通公平性问题具有重要意义，因为它提供了一个数据驱动的方法来指导自行车共享系统的战略性扩展，从而弥合服务不足社区的交通差距。该研究的实际应用价值在于其为城市规划者提供了具体的工具和指导，以促进更公平的城市交通系统。"}}
{"id": "2506.15105", "title": "Skew-Induced Insertion Loss Deviation (SILD) and FOM_SILD: Metrics for Quantifying P/N Skew Effects in High-Speed Channels", "authors": ["David Nozadze", "Zurab Kiguradze", "Amendra Koul", "Mike Sapozhnikov"], "summary": "The rise of AI workloads and growing data center demands have driven the need\nfor ultra-high-speed interconnects exceeding 200 Gb/s. As unit intervals (UI)\nshrink, even a few picoseconds of P/N skew can degrade serializer-deserializer\n(SerDes) performance. Traditional methods for quantifying skew fall short in\ncapturing its impact. We introduce two new metrics: 1) Skew-Induced Insertion\nLoss Deviation (SILD) and 2) its complementary Figure of Merit (FOM_SILD),\nanalytically developed to assess P/N skew effects. Measured S-parameters\nconfirm FOM_SILD reciprocity, while simulations of 224G PAM4 SerDes show strong\ncorrelation with bit error rate (BER) trends. This approach offers a robust\nframework for analyzing skew in next-generation ultra-high-speed interconnects.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15105v1", "AI": {"title_translation": "偏斜引起的插入损耗偏差 (SILD) 和 FOM_SILD：用于量化高速通道中 P/N 偏斜效应的指标", "tldr": "随着高速互连速度提升，P/N 偏斜对 SerDes 性能影响显著。本文提出了 SILD 和 FOM_SILD 两个新指标来量化这种偏斜效应，并通过测量和仿真验证了其有效性。", "motivation": "AI 工作负载和数据中心需求的增长推动了对超过 200 Gb/s 的超高速互连的需求。随着单位间隔 (UI) 的缩小，即使是几皮秒的 P/N 偏斜也会降低串行器-解串器 (SerDes) 的性能。传统的量化偏斜的方法未能充分捕捉其影响。", "method": "本文引入了两个新的分析性开发的指标：1) 偏斜引起的插入损耗偏差 (SILD) 和 2) 其互补的品质因数 (FOM_SILD)，用于评估 P/N 偏斜效应。通过测量 S 参数确认了 FOM_SILD 的互易性，并通过 224G PAM4 SerDes 仿真显示与误码率 (BER) 趋势的强相关性。", "result": "测量的 S 参数证实了 FOM_SILD 的互易性。224G PAM4 SerDes 的仿真显示 FOM_SILD 与误码率 (BER) 趋势有很强的相关性。", "conclusion": "本文提出的方法为分析下一代超高速互连中的偏斜提供了一个强大的框架。", "translation": "人工智能工作负载的兴起和数据中心需求的增长推动了对超过 200 Gb/s 的超高速互连的需求。随着单位间隔 (UI) 的缩小，即使是几皮秒的 P/N 偏斜也会降低串行器-解串器 (SerDes) 的性能。传统的量化偏斜的方法未能充分捕捉其影响。我们引入了两个新指标：1) 偏斜引起的插入损耗偏差 (SILD) 和 2) 其互补的品质因数 (FOM_SILD)，它们经过分析性开发，用于评估 P/N 偏斜效应。测量的 S 参数证实了 FOM_SILD 的互易性，而 224G PAM4 SerDes 的仿真显示与误码率 (BER) 趋势有很强的相关性。这种方法为分析下一代超高速互连中的偏斜提供了一个强大的框架。", "summary": "鉴于超高速互连中 P/N 偏斜对 SerDes 性能的显著影响，本文提出了 SILD 和 FOM_SILD 两个新指标来量化这种偏斜效应。通过 S 参数测量验证了 FOM_SILD 的互易性，并通过 224G PAM4 SerDes 仿真展示了其与误码率的强相关性，为未来高速通道的偏斜分析提供了有效工具。", "keywords": "P/N 偏斜, 插入损耗偏差, SILD, FOM_SILD, 高速互连, SerDes", "comments": "这篇论文通过引入 SILD 和 FOM_SILD 两个新颖的指标，解决了传统方法在量化高速通道中 P/N 偏斜效应方面的不足。其创新性在于提供了分析性开发的量化工具，并得到了实测和仿真数据的支持，对于下一代超高速互连的设计和性能优化具有重要指导意义。"}}
{"id": "2506.15125", "title": "Fiber Signal Denoising Algorithm using Hybrid Deep Learning Networks", "authors": ["Linlin Wang", "Wei Wang", "Dezhao Wang", "Shanwen Wang"], "summary": "With the applicability of optical fiber-based distributed acoustic sensing\n(DAS) systems, effective signal processing and analysis approaches are needed\nto promote its popularization in the field of intelligent transportation\nsystems (ITS). This paper presents a signal denoising algorithm using a hybrid\ndeep-learning network (HDLNet). Without annotated data and time-consuming\nlabeling, this self-supervised network runs in parallel, combining an\nautoencoder for denoising (DAE) and a long short-term memory (LSTM) for\nsequential processing. Additionally, a line-by-line matching algorithm for\nvehicle detection and tracking is introduced, thus realizing the complete\nprocessing of fiber signal denoising and feature extraction. Experiments were\ncarried out on a self-established real highway tunnel dataset, showing that our\nproposed hybrid network yields more satisfactory denoising performance than\nSpatial-domain DAE.", "comment": "15 pages, 10 figures", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15125v1", "AI": {"title_translation": "基于混合深度学习网络的光纤信号去噪算法", "tldr": "本文提出了一种用于光纤分布式声波传感（DAS）系统信号去噪的混合深度学习网络（HDLNet），该网络结合了去噪自编码器（DAE）和长短期记忆网络（LSTM），无需标注数据，并引入了车辆检测和跟踪算法，实验证明其去噪性能优于传统方法。", "motivation": "为了促进光纤分布式声波传感（DAS）系统在智能交通系统（ITS）领域的普及，需要有效的信号处理和分析方法，特别是解决信号去噪问题。", "method": "本文提出了一种混合深度学习网络（HDLNet）用于信号去噪。该网络是自监督的，无需标注数据，并并行运行。它结合了用于去噪的自编码器（DAE）和用于序列处理的长短期记忆网络（LSTM）。此外，还引入了一种逐行匹配算法，用于车辆检测和跟踪，从而实现了光纤信号去噪和特征提取的完整处理。", "result": "在自建的真实高速公路隧道数据集上进行的实验表明，所提出的混合网络比空间域DAE产生了更令人满意的去噪性能。", "conclusion": "本文提出的基于混合深度学习网络的信号去噪算法，在无需标注数据的情况下，能够有效提升光纤分布式声波传感系统的信号去噪性能，并能实现车辆检测和跟踪，有助于推动DAS系统在智能交通领域的应用。", "translation": "随着光纤分布式声波传感（DAS）系统适用性的提高，需要有效的信号处理和分析方法来促进其在智能交通系统（ITS）领域的普及。本文提出了一种使用混合深度学习网络（HDLNet）的信号去噪算法。这个自监督网络无需标注数据和耗时的标记，它并行运行，结合了用于去噪的自编码器（DAE）和用于序列处理的长短期记忆（LSTM）。此外，还引入了一种逐行匹配算法，用于车辆检测和跟踪，从而实现了光纤信号去噪和特征提取的完整处理。实验在自建的真实高速公路隧道数据集上进行，结果表明我们提出的混合网络比空间域DAE产生了更令人满意的去噪性能。", "summary": "本文针对光纤分布式声波传感（DAS）系统在智能交通系统（ITS）中的应用需求，提出了一种基于混合深度学习网络（HDLNet）的信号去噪算法。该自监督网络结合了去噪自编码器（DAE）和长短期记忆网络（LSTM），无需标注数据即可并行处理。此外，该方法还集成了逐行匹配算法，用于车辆检测和跟踪，实现了从去噪到特征提取的完整流程。实验结果表明，该混合网络在真实高速公路隧道数据集上的去噪性能优于空间域DAE。", "keywords": "光纤传感, 信号去噪, 深度学习, 混合网络, 智能交通系统", "comments": "该论文的创新点在于提出了一个无需标注数据的自监督混合深度学习网络（HDLNet）用于光纤信号去噪，并结合了车辆检测与跟踪功能，实现了端到端的信号处理。这对于实际应用中数据标注困难的场景具有重要意义，提升了DAS系统在智能交通领域的实用性。"}}
{"id": "2506.14783", "title": "ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification", "authors": ["Mohamed Masry", "Mohamed Amen", "Mohamed Elzyat", "Mohamed Hamed", "Norhan Magdy", "Maram Khaled"], "summary": "Decoding natural language from brain activity using non-invasive\nelectroencephalography (EEG) remains a significant challenge in neuroscience\nand machine learning, particularly for open-vocabulary scenarios where\ntraditional methods struggle with noise and variability. Previous studies have\nachieved high accuracy on small-closed vocabularies, but it still struggles on\nopen vocabularies. In this study, we propose ETS, a framework that integrates\nEEG with synchronized eye-tracking data to address two critical tasks: (1)\nopen-vocabulary text generation and (2) sentiment classification of perceived\nlanguage. Our model achieves a superior performance on BLEU and Rouge score for\nEEG-To-Text decoding and up to 10% F1 score on EEG-based ternary sentiment\nclassification, which significantly outperforms supervised baselines.\nFurthermore, we show that our proposed model can handle data from various\nsubjects and sources, showing great potential for high performance open\nvocabulary eeg-to-text system.", "comment": "Graduation project report submitted at Faculty of Computer Science\n  and Artificial Intelligence, Helwan University", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14783v1", "AI": {"title_translation": "ETS：开放词汇脑电图到文本解码与情感分类", "tldr": "ETS是一个结合EEG和眼动追踪数据的框架，旨在解决开放词汇的脑电图到文本解码和情感分类任务，并在两项任务上均取得了优于基线模型的性能。", "motivation": "从脑电活动中解码自然语言，特别是在开放词汇场景下，仍然是神经科学和机器学习领域的一个重大挑战。传统方法在处理开放词汇的噪声和变异性时表现不佳，而现有研究在小规模封闭词汇上表现良好，但在开放词汇方面仍面临困难。", "method": "本研究提出了ETS框架，该框架将脑电图（EEG）与同步的眼动追踪数据相结合，以解决开放词汇文本生成和感知语言的情感分类这两个关键任务。", "result": "该模型在EEG到文本解码任务上取得了优越的BLEU和Rouge分数，并在基于EEG的三元情感分类任务上，F1分数提高了高达10%，显著优于监督基线模型。此外，该模型能够处理来自不同受试者和来源的数据。", "conclusion": "该提出的模型在开放词汇脑电图到文本解码和情感分类任务上表现出色，且能够处理多源数据，显示出其在高性能开放词汇脑电图到文本系统方面的巨大潜力。", "translation": "从非侵入性脑电图（EEG）中解码自然语言仍然是神经科学和机器学习领域的一个重大挑战，特别是在开放词汇场景下，传统方法在噪声和变异性方面存在困难。以往的研究在小规模封闭词汇上取得了高准确率，但在开放词汇上仍然面临挑战。在本研究中，我们提出了ETS，这是一个将脑电图与同步眼动追踪数据相结合的框架，旨在解决两个关键任务：（1）开放词汇文本生成和（2）感知语言的情感分类。我们的模型在EEG到文本解码任务上取得了优越的BLEU和Rouge分数，并在基于EEG的三元情感分类任务上，F1分数提高了高达10%，这显著优于监督基线模型。此外，我们展示了我们提出的模型可以处理来自不同受试者和来源的数据，显示出高性能开放词汇EEG到文本系统的巨大潜力。", "summary": "本研究提出了ETS框架，旨在解决从非侵入性脑电图（EEG）解码自然语言的开放词汇挑战。ETS框架整合了EEG与同步眼动追踪数据，用于开放词汇文本生成和感知语言的情感分类。该模型在EEG到文本解码任务上取得了优异的BLEU和Rouge分数，并在EEG情感分类任务上实现了高达10%的F1分数提升，显著优于现有基线模型。研究还表明，ETS能够处理来自不同受试者和来源的数据，展现了其在构建高性能开放词汇EEG到文本系统方面的巨大潜力。", "keywords": "脑电图, 文本解码, 情感分类, 开放词汇, 眼动追踪", "comments": "这项研究的创新之处在于结合了EEG和眼动追踪数据来解决开放词汇的脑电图到文本解码和情感分类问题，这在以往的研究中是一个难点。其在两项任务上均显著优于基线模型的性能，特别是其处理不同受试者和来源数据的能力，显示了该模型具有良好的泛化性和实际应用潜力。这为非侵入性脑机接口技术的发展提供了新的方向和可能性。"}}
{"id": "2506.14877", "title": "Beyond Universality: Cultural Diversity in Music and Its Implications for Sound Design and Sonification", "authors": ["Rubén García-Benito"], "summary": "The Audio Mostly (AM) conference has long been a platform for exploring the\nintersection of sound, technology, and culture. Despite growing interest in\nsonic cultures, discussions on the role of cultural diversity in sound design\nand sonification remain limited. This paper investigates the implicit biases\nand gaps within the discourse on music and sound aesthetics, challenging the\nnotion of music as a 'universal language'. Through a historical and\ncross-cultural analysis of musicology and ethnomusicology, the profound\ninfluence of cultural context on auditory perception and aesthetic appraisal is\nhighlighted. By drawing parallels between historical music practices and\ncontemporary sound design, the paper advocates for a more inclusive approach\nthat recognizes the diversity of sonic traditions. Using music as a case study,\nwe underscore broader implications for sound design and sonification,\nemphasizing the need to integrate cultural perspectives into auditory design\npractices. A reevaluation of existing frameworks in sound design and\nsonification is proposed, emphasizing the necessity of culturally informed\npractices that resonate with global audiences. Ultimately, embracing cultural\ndiversity in sound design is suggested to lead to richer, more meaningful\nauditory experiences and to foster greater inclusivity within the field.", "comment": "12 pages, 1 figure. Long paper accepted for publication at the Audio\n  Mostly & ICAD Joint Conference (AM.ICAD 2025). To appear in the ACM\n  International Conference Proceedings Series (ICPS)", "cate": "physics.soc-ph", "url": "http://arxiv.org/abs/2506.14877v1", "AI": {"title_translation": "超越普适性：音乐中的文化多样性及其对声音设计和声音化应用的影响", "tldr": "本文探讨了音乐中的文化多样性，挑战了音乐作为“通用语言”的观念，并主张在声音设计和声音化中融入文化视角，以实现更具包容性和意义的听觉体验。", "motivation": "尽管对声音文化日益增长的兴趣，但声音设计和声音化中文化多样性的作用讨论仍然有限。本文旨在调查音乐和声音美学讨论中隐含的偏见和空白，挑战音乐作为“通用语言”的观念。", "method": "本文通过对音乐学和民族音乐学的历史和跨文化分析，突出了文化背景对听觉感知和审美评价的深刻影响。通过将历史音乐实践与当代声音设计进行比较，并以音乐作为案例研究。", "result": "分析强调了文化背景对听觉感知和审美评价的深刻影响。研究结果强调了将文化视角整合到听觉设计实践中的必要性，并指出了对声音设计和声音化更广泛的影响。", "conclusion": "在声音设计中拥抱文化多样性将带来更丰富、更有意义的听觉体验，并促进该领域更大的包容性。因此，需要重新评估现有框架，强调文化知情的实践。", "translation": "Audio Mostly (AM) 会议长期以来一直是探索声音、技术和文化交叉点的平台。尽管人们对声音文化的兴趣日益增长，但关于文化多样性在声音设计和声音化中作用的讨论仍然有限。本文调查了音乐和声音美学讨论中隐含的偏见和空白，挑战了音乐作为“通用语言”的观念。通过对音乐学和民族音乐学的历史和跨文化分析，突出了文化背景对听觉感知和审美评价的深刻影响。通过将历史音乐实践与当代声音设计进行比较，本文主张采用更具包容性的方法，承认声音传统的多样性。我们以音乐作为案例研究，强调了对声音设计和声音化的更广泛影响，强调了将文化视角整合到听觉设计实践中的必要性。本文提出对声音设计和声音化现有框架进行重新评估，强调文化知情实践的必要性，以与全球受众产生共鸣。最终，建议在声音设计中拥抱文化多样性将带来更丰富、更有意义的听觉体验，并促进该领域更大的包容性。", "summary": "本文探讨了声音设计和声音化中文化多样性讨论的局限性，挑战了音乐作为普适语言的观念。通过对音乐学和民族音乐学的历史及跨文化分析，论文强调了文化背景对听觉感知和审美评价的深远影响。它主张在声音设计中采纳更具包容性的方法，整合多样化的声音传统，以期创造更丰富、更有意义的听觉体验，并促进领域内的更大包容性。", "keywords": "文化多样性, 声音设计, 声音化, 音乐学, 民族音乐学", "comments": "本文的创新之处在于其敢于挑战“音乐是通用语言”的普遍观念，并将其批判性视角延伸至声音设计和声音化领域。其重要性在于，它为创建更具文化敏感性和包容性的听觉产品和体验提供了理论基础，这在全球化背景下尤为重要。论文强调了文化背景对听觉感知的影响，为未来声音设计实践提供了新的思考方向。"}}
{"id": "2506.14909", "title": "Foundation Artificial Intelligence Models for Health Recognition Using Face Photographs (FAHR-Face)", "authors": ["Fridolin Haugg", "Grace Lee", "John He", "Leonard Nürnberg", "Dennis Bontempi", "Danielle S. Bitterman", "Paul Catalano", "Vasco Prudente", "Dmitrii Glubokov", "Andrew Warrington", "Suraj Pai", "Dirk De Ruysscher", "Christian Guthier", "Benjamin H. Kann", "Vadim N. Gladyshev", "Hugo JWL Aerts", "Raymond H. Mak"], "summary": "Background: Facial appearance offers a noninvasive window into health. We\nbuilt FAHR-Face, a foundation model trained on >40 million facial images and\nfine-tuned it for two distinct tasks: biological age estimation (FAHR-FaceAge)\nand survival risk prediction (FAHR-FaceSurvival).\n  Methods: FAHR-FaceAge underwent a two-stage, age-balanced fine-tuning on\n749,935 public images; FAHR-FaceSurvival was fine-tuned on 34,389 photos of\ncancer patients. Model robustness (cosmetic surgery, makeup, pose, lighting)\nand independence (saliency mapping) was tested extensively. Both models were\nclinically tested in two independent cancer patient datasets with survival\nanalyzed by multivariable Cox models and adjusted for clinical prognostic\nfactors.\n  Findings: For age estimation, FAHR-FaceAge had the lowest mean absolute error\nof 5.1 years on public datasets, outperforming benchmark models and maintaining\naccuracy across the full human lifespan. In cancer patients, FAHR-FaceAge\noutperformed a prior facial age estimation model in survival prognostication.\nFAHR-FaceSurvival demonstrated robust prediction of mortality, and the\nhighest-risk quartile had more than triple the mortality of the lowest\n(adjusted hazard ratio 3.22; P<0.001). These findings were validated in the\nindependent cohort and both models showed generalizability across age, sex,\nrace and cancer subgroups. The two algorithms provided distinct, complementary\nprognostic information; saliency mapping revealed each model relied on distinct\nfacial regions. The combination of FAHR-FaceAge and FAHR-FaceSurvival improved\nprognostic accuracy.\n  Interpretation: A single foundation model can generate inexpensive, scalable\nfacial biomarkers that capture both biological ageing and disease-related\nmortality risk. The foundation model enabled effective training using\nrelatively small clinical datasets.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14909v1", "AI": {"title_translation": "用于健康识别的基于人脸照片的基石人工智能模型 (FAHR-Face)", "tldr": "FAHR-Face是一个基于4000多万张面部图像训练的基石人工智能模型，可用于精确的生物年龄估计和生存风险预测，提供可扩展的健康生物标志物。", "motivation": "面部外观为评估健康提供了一个非侵入性的途径。本研究旨在构建一个名为FAHR-Face的基石模型，用于通过面部照片进行健康识别，具体任务包括生物年龄估计和生存风险预测。", "method": "FAHR-Face模型在超过4000万张面部图像上进行训练。随后，它被微调以执行两项任务：FAHR-FaceAge（在749,935张公共图像上进行两阶段、年龄平衡的微调）和FAHR-FaceSurvival（在34,389张癌症患者照片上进行微调）。模型对整形手术、化妆、姿势和光照的鲁棒性以及独立性（通过显著性映射）进行了广泛测试。这两个模型在两个独立的癌症患者数据集中进行了临床测试，通过多变量Cox模型分析生存情况，并根据临床预后因素进行了调整。", "result": "在年龄估计方面，FAHR-FaceAge在公共数据集上的平均绝对误差最低，为5.1年，优于基准模型，并在整个人类寿命范围内保持准确性。在癌症患者中，FAHR-FaceAge在生存预后方面优于先前的面部年龄估计模型。FAHR-FaceSurvival能够可靠预测死亡率，最高风险四分位组的死亡率是最低风险四分位组的三倍多（调整后的风险比3.22；P<0.001）。这些发现得到了独立队列的验证，并且两个模型在年龄、性别、种族和癌症亚组中均显示出泛化能力。两种算法提供了独特且互补的预后信息，且显著性映射显示每个模型依赖于不同的面部区域。FAHR-FaceAge和FAHR-FaceSurvival的结合提高了预后准确性。", "conclusion": "一个单一的基石模型能够生成廉价且可扩展的面部生物标志物，用于捕捉生物老化和疾病相关的死亡风险。该基石模型使得即使使用相对较小的临床数据集也能进行有效的训练。", "translation": "背景：面部外观为健康提供了一个非侵入性的窗口。我们构建了FAHR-Face，一个在超过4000万张面部图像上训练的基石模型，并对其进行了微调，以完成两项不同的任务：生物年龄估计（FAHR-FaceAge）和生存风险预测（FAHR-FaceSurvival）。\n方法：FAHR-FaceAge在749,935张公共图像上进行了两阶段、年龄平衡的微调；FAHR-FaceSurvival在34,389张癌症患者照片上进行了微调。模型鲁棒性（整形手术、化妆、姿势、光照）和独立性（显著性映射）得到了广泛测试。这两个模型都在两个独立的癌症患者数据集中进行了临床测试，并通过多变量Cox模型分析了生存情况，并根据临床预后因素进行了调整。\n发现：在年龄估计方面，FAHR-FaceAge在公共数据集上的平均绝对误差最低，为5.1年，优于基准模型，并在整个人类寿命范围内保持准确性。在癌症患者中，FAHR-FaceAge在生存预后方面优于先前的面部年龄估计模型。FAHR-FaceSurvival显示出对死亡率的强大预测能力，最高风险四分位数组的死亡率是最低风险四分位数组的三倍多（调整后的风险比3.22；P<0.001）。这些发现得到了独立队列的验证，并且这两个模型在年龄、性别、种族和癌症亚组中都显示出泛化能力。这两种算法提供了独特、互补的预后信息；显著性映射显示每个模型依赖于不同的面部区域。FAHR-FaceAge和FAHR-FaceSurvival的结合提高了预后准确性。\n解释：一个单一的基石模型可以生成廉价、可扩展的面部生物标志物，捕捉生物老化和疾病相关的死亡风险。这个基石模型使得使用相对较小的临床数据集进行有效训练成为可能。", "summary": "本文介绍了FAHR-Face，一个在超过4000万张面部图像上训练的基石人工智能模型，旨在进行健康识别。该模型被微调为FAHR-FaceAge用于生物年龄估计，以及FAHR-FaceSurvival用于癌症患者生存预测。FAHR-FaceAge在年龄估计方面实现了5.1年的平均绝对误差，并在年龄估计和生存预后方面均优于基准模型。FAHR-FaceSurvival准确预测了死亡率，最高风险四分位组的死亡风险是最低风险四分位组的三倍多。两个模型均表现出鲁棒性和泛化能力，并提供了互补的预后信息，它们的结合提高了准确性。研究得出结论，单一的基石模型可以创建经济有效、可扩展的面部生物标志物，用于评估老化和疾病风险，即使临床数据有限也能有效训练。", "keywords": "基石模型, 面部生物标志物, 年龄估计, 生存预测, 健康识别", "comments": "这篇论文通过利用大规模基石模型从面部图像进行健康识别，提出了一种创新方法。能够将单一模型微调以完成不同任务（年龄估计和生存预测），并且仅使用相对较小的临床数据集，是其显著的优势，凸显了基石模型的强大能力。通过显著性映射展示不同面部区域提供互补预后信息也具有启发性。其提供廉价且可扩展的生物标志物的潜力具有重要的影响力。"}}
{"id": "2506.15684", "title": "Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards", "authors": ["Qingming Liu", "Zhen Liu", "Dinghuai Zhang", "Kui Jia"], "summary": "Generating high-quality and photorealistic 3D assets remains a longstanding\nchallenge in 3D vision and computer graphics. Although state-of-the-art\ngenerative models, such as diffusion models, have made significant progress in\n3D generation, they often fall short of human-designed content due to limited\nability to follow instructions, align with human preferences, or produce\nrealistic textures, geometries, and physical attributes. In this paper, we\nintroduce Nabla-R2D3, a highly effective and sample-efficient reinforcement\nlearning alignment framework for 3D-native diffusion models using 2D rewards.\nBuilt upon the recently proposed Nabla-GFlowNet method, which matches the score\nfunction to reward gradients in a principled manner for reward finetuning, our\nNabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D\nreward signals. Extensive experiments show that, unlike vanilla finetuning\nbaselines which either struggle to converge or suffer from reward hacking,\nNabla-R2D3 consistently achieves higher rewards and reduced prior forgetting\nwithin a few finetuning steps.", "comment": "Technical Report (21 pages, 21 figures)", "cate": "cs.GR", "url": "http://arxiv.org/abs/2506.15684v1", "AI": {"title_translation": "Nabla-R2D3：基于2D奖励的有效高效3D扩散对齐", "tldr": "Nabla-R2D3是一个使用2D奖励对3D扩散模型进行微调的强化学习对齐框架，解决了3D生成中指令遵循、人类偏好和真实感的问题，并取得了更好的收敛性和更少的遗忘。", "motivation": "高质量和逼真的3D资产生成仍然是3D视觉和计算机图形学的长期挑战。尽管先进的生成模型（如扩散模型）在3D生成方面取得了显著进展，但它们在遵循指令、与人类偏好对齐或生成逼真纹理、几何形状和物理属性方面的能力有限，因此常常达不到人类设计内容的水平。", "method": "本文引入了Nabla-R2D3，一个高效且样本有效的强化学习对齐框架，用于使用2D奖励信号对3D原生扩散模型进行对齐。它建立在最近提出的Nabla-GFlowNet方法之上，该方法以一种原则性的方式将分数函数与奖励梯度匹配以进行奖励微调。", "result": "广泛的实验表明，与香草微调基线（它们要么难以收敛，要么遭受奖励欺骗）不同，Nabla-R2D3在几个微调步骤内始终实现更高的奖励并减少先验遗忘。", "conclusion": "Nabla-R2D3是一种有效且高效的3D扩散模型对齐方法，通过利用2D奖励信号，解决了现有3D生成模型在遵循指令、对齐人类偏好和生成逼真内容方面的不足，并且在实验中表现出优于传统微调基线的性能。", "translation": "生成高质量和逼真的3D资产仍然是3D视觉和计算机图形学中的一个长期挑战。尽管最先进的生成模型（如扩散模型）在3D生成方面取得了显著进展，但由于遵循指令、与人类偏好对齐或生成逼真纹理、几何形状和物理属性的能力有限，它们往往达不到人类设计内容的水平。在本文中，我们引入了Nabla-R2D3，一个高度有效且样本高效的强化学习对齐框架，用于使用2D奖励的3D原生扩散模型。我们的Nabla-R2D3建立在最近提出的Nabla-GFlowNet方法之上，该方法以一种原则性的方式将分数函数与奖励梯度匹配以进行奖励微调，从而能够有效地适应3D扩散模型，仅使用2D奖励信号。广泛的实验表明，与香草微调基线（它们要么难以收敛，要么遭受奖励欺骗）不同，Nabla-R2D3在几个微调步骤内始终实现更高的奖励并减少先验遗忘。", "summary": "Nabla-R2D3是一个用于3D原生扩散模型的强化学习对齐框架，它利用2D奖励信号来解决3D生成中存在的指令遵循、人类偏好对齐和真实感不足的问题。该方法基于Nabla-GFlowNet，能有效地对3D扩散模型进行微调。实验证明，Nabla-R2D3相比传统微调方法，能更快地达到更高奖励并减少遗忘。", "keywords": "3D扩散模型, 强化学习, 2D奖励, 模型对齐, Nabla-R2D3", "comments": "该论文的创新点在于提出了Nabla-R2D3，一个利用2D奖励信号对3D扩散模型进行对齐的强化学习框架。这种方法有效地解决了3D生成中模型难以遵循指令和生成逼真内容的问题。其基于Nabla-GFlowNet的原理性设计，以及在样本效率和避免奖励欺骗方面的表现，显示了其在3D内容生成领域的潜力。"}}
{"id": "2506.15576", "title": "DiscRec: Disentangled Semantic-Collaborative Modeling for Generative Recommendation", "authors": ["Chang Liu", "Yimeng Bai", "Xiaoyan Zhao", "Yang Zhang", "Fuli Feng", "Wenge Rong"], "summary": "Generative recommendation is emerging as a powerful paradigm that directly\ngenerates item predictions, moving beyond traditional matching-based\napproaches. However, current methods face two key challenges: token-item\nmisalignment, where uniform token-level modeling ignores item-level granularity\nthat is critical for collaborative signal learning, and semantic-collaborative\nsignal entanglement, where collaborative and semantic signals exhibit distinct\ndistributions yet are fused in a unified embedding space, leading to\nconflicting optimization objectives that limit the recommendation performance.\n  To address these issues, we propose DiscRec, a novel framework that enables\nDisentangled Semantic-Collaborative signal modeling with flexible fusion for\ngenerative Recommendation.First, DiscRec introduces item-level position\nembeddings, assigned based on indices within each semantic ID, enabling\nexplicit modeling of item structure in input token sequences.Second, DiscRec\nemploys a dual-branch module to disentangle the two signals at the embedding\nlayer: a semantic branch encodes semantic signals using original token\nembeddings, while a collaborative branch applies localized attention restricted\nto tokens within the same item to effectively capture collaborative signals. A\ngating mechanism subsequently fuses both branches while preserving the model's\nability to model sequential dependencies. Extensive experiments on four\nreal-world datasets demonstrate that DiscRec effectively decouples these\nsignals and consistently outperforms state-of-the-art baselines. Our codes are\navailable on https://github.com/Ten-Mao/DiscRec.", "comment": null, "cate": "cs.IR", "url": "http://arxiv.org/abs/2506.15576v1", "AI": {"title_translation": "DiscRec: 生成式推荐中解耦语义-协同建模", "tldr": "DiscRec是一种新的生成式推荐框架，通过解耦语义和协同信号并灵活融合，解决了现有方法中令牌-物品错位和信号纠缠的问题，提高了推荐性能。", "motivation": "现有生成式推荐方法面临两个挑战：1. 令牌-物品错位，即统一的令牌级建模忽略了对协同信号学习至关重要的物品级粒度；2. 语义-协同信号纠缠，即协同和语义信号分布不同却在统一嵌入空间中融合，导致优化目标冲突，限制了推荐性能。", "method": "本文提出了DiscRec框架，实现生成式推荐中解耦的语义-协同信号建模与灵活融合。首先，DiscRec引入物品级位置嵌入，基于每个语义ID内的索引分配，显式建模输入令牌序列中的物品结构。其次，DiscRec采用双分支模块在嵌入层解耦两种信号：语义分支使用原始令牌嵌入编码语义信号；协同分支应用局部注意力，仅限于同一物品内的令牌，有效捕获协同信号。最后，使用门控机制融合两个分支，同时保留模型建模序列依赖的能力。", "result": "在四个真实世界数据集上的广泛实验表明，DiscRec有效解耦了这些信号，并持续优于最先进的基线方法。", "conclusion": "DiscRec通过解耦语义和协同信号，并采用灵活的融合机制，成功解决了生成式推荐中的关键挑战，显著提升了推荐性能。", "translation": "生成式推荐正在成为一种强大的范式，它直接生成物品预测，超越了传统的基于匹配的方法。然而，当前方法面临两个关键挑战：令牌-物品错位，即统一的令牌级建模忽略了对协同信号学习至关重要的物品级粒度；以及语义-协同信号纠缠，即协同和语义信号表现出不同的分布，却在统一的嵌入空间中融合，导致优化目标冲突，从而限制了推荐性能。\n为了解决这些问题，我们提出了DiscRec，一个新颖的框架，它实现了生成式推荐中解耦的语义-协同信号建模与灵活融合。首先，DiscRec引入了物品级位置嵌入，根据每个语义ID内的索引进行分配，从而能够在输入令牌序列中显式建模物品结构。其次，DiscRec采用双分支模块在嵌入层解耦这两种信号：一个语义分支使用原始令牌嵌入编码语义信号，而一个协同分支应用局部注意力，仅限于同一物品内的令牌，以有效捕获协同信号。随后，一个门控机制融合了两个分支，同时保留了模型建模序列依赖的能力。在四个真实世界数据集上的广泛实验表明，DiscRec有效地解耦了这些信号，并持续优于最先进的基线方法。我们的代码可在https://github.com/Ten-Mao/DiscRec 上获取。", "summary": "本文提出了DiscRec，一个用于生成式推荐的新颖框架，旨在解决现有方法中令牌-物品错位和语义-协同信号纠缠的问题。DiscRec通过引入物品级位置嵌入来建模物品结构，并采用双分支模块（语义分支和协同分支）在嵌入层解耦两种信号，最后通过门控机制灵活融合。实验结果表明，DiscRec能有效解耦信号并显著优于现有基线。", "keywords": "生成式推荐, 语义-协同建模, 解耦, 令牌-物品错位, DiscRec", "comments": "DiscRec的创新之处在于其对语义和协同信号的解耦处理，这解决了传统生成式推荐模型中存在的信号冲突问题。通过引入物品级位置嵌入和双分支模块，DiscRec能够更精细地处理不同粒度的信息，从而提升了推荐的准确性和鲁棒性。这种解耦与灵活融合的范式对于理解和改进复杂推荐系统具有重要意义。"}}
{"id": "2506.14951", "title": "Flat Channels to Infinity in Neural Loss Landscapes", "authors": ["Flavio Martinelli", "Alexander Van Meegen", "Berfin Şimşek", "Wulfram Gerstner", "Johanni Brea"], "summary": "The loss landscapes of neural networks contain minima and saddle points that\nmay be connected in flat regions or appear in isolation. We identify and\ncharacterize a special structure in the loss landscape: channels along which\nthe loss decreases extremely slowly, while the output weights of at least two\nneurons, $a_i$ and $a_j$, diverge to $\\pm$infinity, and their input weight\nvectors, $\\mathbf{w_i}$ and $\\mathbf{w_j}$, become equal to each other. At\nconvergence, the two neurons implement a gated linear unit:\n$a_i\\sigma(\\mathbf{w_i} \\cdot \\mathbf{x}) + a_j\\sigma(\\mathbf{w_j} \\cdot\n\\mathbf{x}) \\rightarrow \\sigma(\\mathbf{w} \\cdot \\mathbf{x}) + (\\mathbf{v} \\cdot\n\\mathbf{x}) \\sigma'(\\mathbf{w} \\cdot \\mathbf{x})$. Geometrically, these\nchannels to infinity are asymptotically parallel to symmetry-induced lines of\ncritical points. Gradient flow solvers, and related optimization methods like\nSGD or ADAM, reach the channels with high probability in diverse regression\nsettings, but without careful inspection they look like flat local minima with\nfinite parameter values. Our characterization provides a comprehensive picture\nof these quasi-flat regions in terms of gradient dynamics, geometry, and\nfunctional interpretation. The emergence of gated linear units at the end of\nthe channels highlights a surprising aspect of the computational capabilities\nof fully connected layers.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14951v1", "AI": {"title_translation": "神经网络损失景观中的平坦通道至无穷大", "tldr": "神经网络损失景观中存在一种特殊的“平坦通道”，其中损失缓慢下降，特定神经元权重趋于无穷，最终形成门控线性单元，这在优化过程中常被误认为是平坦局部最小值。", "motivation": "论文旨在识别和描述神经网络损失景观中一种特殊的结构——“通道”，并解释其在优化过程中的表现和功能。", "method": "通过识别并特征化这种特殊结构，描述其梯度动态、几何形状和功能解释。具体来说，观察到输出权重发散到无穷大，输入权重向量趋于相等，以及最终形成门控线性单元。", "result": "识别出损失景观中“通道”结构：损失缓慢下降，同时至少两个神经元 ($a_i$, $a_j$) 的输出权重发散到 $\\pm$ 无穷大，其输入权重向量 ($\\mathbf{w_i}$, $\\mathbf{w_j}$) 变得相等。在收敛时，这两个神经元实现了一个门控线性单元。几何上，这些通道渐近平行于对称诱导的临界点线。梯度流求解器及相关优化方法（如SGD或ADAM）在高概率下会到达这些通道，但容易被误认为是具有有限参数值的平坦局部最小值。", "conclusion": "论文提供了对神经网络损失景观中这些准平坦区域的全面描述，包括梯度动态、几何形状和功能解释。门控线性单元的出现揭示了全连接层计算能力的意外方面。", "translation": "神经网络的损失景观包含可能在平坦区域中连接或独立出现的最小值和鞍点。我们识别并描述了损失景观中的一种特殊结构：沿着该结构，损失下降极其缓慢，同时至少两个神经元 $a_i$ 和 $a_j$ 的输出权重发散到 $\\pm$ 无穷大，并且它们的输入权重向量 $\\mathbf{w_i}$ 和 $\\mathbf{w_j}$ 变得彼此相等。在收敛时，这两个神经元实现了一个门控线性单元：$a_i\\sigma(\\mathbf{w_i} \\cdot \\mathbf{x}) + a_j\\sigma(\\mathbf{w_j} \\cdot \\mathbf{x}) \\rightarrow \\sigma(\\mathbf{w} \\cdot \\mathbf{x}) + (\\mathbf{v} \\cdot \\mathbf{x}) \\sigma'(\\mathbf{w} \\cdot \\mathbf{x})$。从几何角度看，这些通向无穷大的通道渐近平行于对称引起的临界点线。梯度流求解器以及SGD或ADAM等相关优化方法，在各种回归设置中以高概率到达这些通道，但如果不仔细检查，它们看起来就像具有有限参数值的平坦局部最小值。我们的特征化从梯度动态、几何形状和功能解释方面提供了这些准平坦区域的全面图景。通道末端门控线性单元的出现突出了全连接层计算能力的一个令人惊讶的方面。", "summary": "本文识别并特征化了神经网络损失景观中的一种特殊结构，称为“平坦通道至无穷大”。这些通道表现为损失缓慢下降，同时特定神经元的输出权重发散而输入权重向量趋于相等，最终形成门控线性单元。研究指出，优化器如SGD和ADAM在高概率下会进入这些通道，但其外观常被误认为是有限参数的平坦局部最小值。论文从梯度动态、几何和功能角度全面解释了这些准平坦区域及其对全连接层计算能力的启示。", "keywords": "神经网络, 损失景观, 平坦通道, 门控线性单元, 优化", "comments": "这篇论文揭示了神经网络损失景观中一种此前未被充分理解的复杂行为模式，即“平坦通道至无穷大”。其创新之处在于详细描述了这种特殊结构的梯度动态、几何特性和功能解释，特别是指出了它如何导致特定神经元权重发散并最终形成门控线性单元。这对于理解神经网络的优化过程、收敛行为以及全连接层的实际计算能力具有重要意义。它也提醒研究者在分析所谓的“平坦局部最小值”时需要更深入的检查，因为其中可能隐藏着权重趋于无穷的通道。"}}
{"id": "2506.14927", "title": "MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance", "authors": ["Joseph J. Peper", "Wenzhao Qiu", "Ali Payani", "Lu Wang"], "summary": "Natural language processing evaluation has made significant progress, largely\ndriven by the proliferation of powerful large language mod-els (LLMs). New\nevaluation benchmarks are of increasing priority as the reasoning capabilities\nof LLMs are expanding at a rapid pace. In particular, while multi-document (MD)\nreasoning is an area of extreme relevance given LLM capabilities in handling\nlonger-context inputs, few benchmarks exist to rigorously examine model\nbehavior in this setting. Moreover, the multi-document setting is historically\nchallenging for benchmark creation due to the expensive cost of annotating long\ninputs. In this work, we introduce MDBench, a new dataset for evaluating LLMs\non the task of multi-document reasoning. Notably, MDBench is created through a\nnovel synthetic generation process, allowing us to controllably and efficiently\ngenerate challenging document sets and the corresponding question-answer (QA)\nexamples. Our novel technique operates on condensed structured seed knowledge,\nmodifying it through LLM-assisted edits to induce MD-specific reasoning\nchallenges. We then convert this structured knowledge into a natural text\nsurface form, generating a document set and corresponding QA example. We\nanalyze the behavior of popular LLMs and prompting techniques, finding that\nMDBENCH poses significant challenges for all methods, even with relatively\nshort document sets. We also see our knowledge-guided generation technique (1)\nallows us to readily perform targeted analysis of MD-specific reasoning\ncapabilities and (2) can be adapted quickly to account for new challenges and\nfuture modeling improvements.", "comment": "ACL 2025 Findings", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14927v1", "AI": {"title_translation": "MDBench：一个知识引导生成的合成多文档推理基准", "tldr": "MDBench是一个新的多文档推理基准，通过新颖的合成生成过程创建，旨在评估大型语言模型（LLMs）的多文档推理能力，并发现现有LLMs在此基准上表现不佳。", "motivation": "随着大型语言模型（LLMs）推理能力的快速发展，对新的评估基准的需求日益增加。特别是，尽管多文档（MD）推理对于LLM处理长上下文输入的能力至关重要，但现有严格评估模型在此设置下行为的基准很少。此外，由于长输入标注成本高昂，多文档基准的创建历来具有挑战性。", "method": "本研究引入了MDBench，一个用于评估LLM多文档推理任务的新数据集。MDBench通过一种新颖的合成生成过程创建，该过程允许可控且高效地生成具有挑战性的文档集和相应的问答（QA）示例。该技术基于浓缩的结构化种子知识，通过LLM辅助编辑进行修改以引入MD特定的推理挑战，然后将结构化知识转换为自然文本形式，生成文档集和相应的QA示例。", "result": "分析发现，即使是相对较短的文档集，MDBench对所有流行的大型语言模型和提示技术都构成了重大挑战。研究还发现，知识引导的生成技术（1）可以方便地对MD特定推理能力进行有针对性的分析，并且（2）可以快速适应新的挑战和未来的模型改进。", "conclusion": "MDBench作为一个新颖的合成多文档推理基准，成功揭示了当前大型语言模型在多文档推理方面的不足，并提供了一种高效、可控的基准创建方法，支持对模型能力的深入分析和未来发展。", "translation": "自然语言处理评估取得了显著进展，这主要得益于强大的大型语言模型（LLMs）的普及。随着LLM推理能力快速扩展，新的评估基准变得越来越重要。特别是，鉴于LLM处理长上下文输入的能力，多文档（MD）推理是一个极其相关的领域，但很少有基准能够严格检查模型在此设置下的行为。此外，由于标注长输入的成本高昂，多文档设置历来对基准创建构成挑战。在这项工作中，我们引入了MDBench，一个用于评估LLM在多文档推理任务上的新数据集。值得注意的是，MDBench是通过一种新颖的合成生成过程创建的，这使我们能够可控且高效地生成具有挑战性的文档集和相应的问答（QA）示例。我们的新颖技术基于浓缩的结构化种子知识，通过LLM辅助编辑对其进行修改，以引入MD特定的推理挑战。然后，我们将这种结构化知识转换为自然文本表面形式，生成文档集和相应的QA示例。我们分析了流行LLM和提示技术的行为，发现MDBench对所有方法都构成了重大挑战，即使文档集相对较短。我们还发现，我们的知识引导生成技术（1）使我们能够轻松地对MD特定推理能力进行有针对性的分析，并且（2）可以快速适应新的挑战和未来的建模改进。", "summary": "本文介绍了MDBench，一个用于评估大型语言模型（LLMs）多文档推理能力的新型合成基准。MDBench通过知识引导的合成生成过程创建，有效解决了传统多文档基准构建成本高昂的问题，并能生成具有挑战性的文档集和问答对。实验结果表明，MDBench对现有LLMs构成了显著挑战，突显了其在多文档推理方面的局限性。此外，该生成方法支持对特定推理能力的深入分析，并易于适应未来模型改进。", "keywords": "多文档推理, LLM评估, 合成基准, 知识引导, MDBench", "comments": "MDBench的创新之处在于其采用合成生成过程来创建多文档推理基准，这有效解决了人工标注成本高昂的痛点。通过知识引导和LLM辅助编辑，该方法能够高效且可控地生成具有挑战性的数据，为评估LLM在复杂长上下文推理能力方面提供了重要工具。其重要性体现在推动LLM在多文档理解和推理领域的进一步发展，并为未来模型改进提供了明确的评估方向。"}}
{"id": "2506.14792", "title": "Fast automated adjoints for spectral PDE solvers", "authors": ["Calum S. Skene", "Keaton J. Burns"], "summary": "We present a general and automated approach for computing model gradients for\nPDE solvers built on sparse spectral methods, and implement this capability in\nthe widely used open-source Dedalus framework. We apply reverse-mode automatic\ndifferentiation to symbolic graph representations of PDEs, efficiently\nconstructing adjoint solvers that retain the speed and memory efficiency of\nthis important class of modern numerical methods. This approach enables users\nto compute gradients and perform optimization for a wide range of\ntime-dependent and nonlinear systems without writing additional code. The\nframework supports a broad class of equations, geometries, and boundary\nconditions, and runs efficiently in parallel using MPI. We demonstrate the\nflexibility and capabilities of this system using canonical problems from the\nliterature, showing both strong performance and practical utility for a wide\nvariety of inverse problems. By integrating automatic adjoints into a flexible\nhigh-level solver, our approach enables researchers to perform gradient-based\noptimization and sensitivity analyses in spectral simulations with ease and\nefficiency.", "comment": "17 pages, 6 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14792v1", "AI": {"title_translation": "谱偏微分方程求解器快速自动化伴随", "tldr": "本文提出了一种为基于稀疏谱方法的偏微分方程（PDE）求解器计算模型梯度的通用自动化方法，并在Dedalus框架中实现，通过逆向自动微分构建伴随求解器，从而在无需额外编码的情况下，高效地支持广泛的时间相关和非线性系统的梯度计算和优化。", "motivation": "传统上，为偏微分方程（PDE）求解器计算模型梯度和执行优化需要大量额外编码。本文的动机在于提供一种通用且自动化的方法，使研究人员能够轻松高效地进行基于梯度的优化和敏感性分析，从而无需手动编写伴随代码。", "method": "本文提出了一种通用且自动化的方法，用于计算基于稀疏谱方法的偏微分方程求解器的模型梯度。具体而言，该方法将逆向自动微分应用于偏微分方程的符号图表示，从而高效构建伴随求解器。该能力已在开源的Dedalus框架中实现，并支持广泛的方程、几何形状和边界条件，且能使用MPI高效并行运行。", "result": "该系统在文献中的典型问题上展示了强大的性能和实际效用，适用于各种逆问题。通过将自动伴随集成到灵活的高级求解器中，该方法使得研究人员能够轻松高效地在谱模拟中执行基于梯度的优化和敏感性分析。", "conclusion": "通过将自动化伴随功能集成到灵活的高级求解器中，本文提出的方法极大地简化了研究人员在谱模拟中执行基于梯度优化和敏感性分析的过程，显著提高了计算效率和易用性。", "translation": "我们提出了一种通用且自动化的方法，用于计算基于稀疏谱方法的偏微分方程（PDE）求解器的模型梯度，并在广泛使用的开源Dedalus框架中实现了这一功能。我们将逆向自动微分应用于偏微分方程的符号图表示，高效地构建了伴随求解器，保留了这类重要现代数值方法的速度和内存效率。这种方法使用户无需编写额外代码，即可为各种时间相关和非线性系统计算梯度并执行优化。该框架支持广泛的方程、几何形状和边界条件，并能使用MPI高效并行运行。我们利用文献中的典型问题展示了该系统的灵活性和能力，展示了其在各种逆问题中的强大性能和实际效用。通过将自动伴随集成到灵活的高级求解器中，我们的方法使研究人员能够轻松高效地在谱模拟中执行基于梯度的优化和敏感性分析。", "summary": "本文提出了一种在开源Dedalus框架中实现PDE求解器模型梯度计算的通用自动化方法。该方法利用逆向自动微分处理PDE的符号图表示，构建高效的伴随求解器，以支持谱方法的速度和内存效率。它使得用户无需额外编码即可对广泛的时间相关和非线性系统进行梯度计算和优化，并在并行环境下表现出色，为各种逆问题提供了强大的实用工具，极大地便利了谱模拟中的梯度优化和敏感性分析。", "keywords": "自动微分, 伴随求解器, 谱方法, 偏微分方程, 梯度计算", "comments": "本文的创新点在于将逆向自动微分技术与谱方法结合，实现了PDE求解器模型梯度的自动化计算，并将其集成到流行的Dedalus框架中。这一方法解决了传统上梯度计算需要大量手动编码的痛点，显著提高了研究人员进行基于梯度优化和敏感性分析的效率和便利性。其重要性体现在能够支持广泛的方程、几何和边界条件，并能高效并行运行，在各种逆问题中展现出强大的实用价值。抽象中未提及明显局限性。"}}
{"id": "2506.15154", "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning", "authors": ["Anuradha Chopra", "Abhinaba Roy", "Dorien Herremans"], "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.", "comment": "14 pages, 2 figures, Accepted to AIMC 2025", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15154v1", "AI": {"title_translation": "SonicVerse：面向音乐特征感知字幕生成的多任务学习", "tldr": "SonicVerse是一个多任务音乐字幕生成模型，它通过整合辅助音乐特征检测任务来生成详细的音乐描述，并在实验中证明了其有效性。", "motivation": "准确反映音乐作品特征的详细字幕可以丰富音乐数据库，并推动音乐AI领域的研究。", "method": "本文提出了一个名为SonicVerse的多任务音乐字幕生成模型。该模型将字幕生成与辅助音乐特征检测任务（如音调检测、人声检测等）相结合，以直接捕捉低级声学细节和高级音乐属性。其关键贡献是一种基于投影的架构，将音频输入转换为语言标记，同时通过专用的辅助头检测音乐特征。这些辅助头的输出也被投影到语言标记中，以增强字幕输入。该框架不仅能为短音乐片段生成丰富、描述性的字幕，还能通过使用大型语言模型链接输出来为较长的音乐作品生成详细的时间感知描述。为了训练模型，作者使用模块化音乐特征提取器MIMFLEX扩展了MusicBench数据集，并用音乐特征进行标注，从而得到了配对的音频、字幕和音乐特征数据。", "result": "实验结果表明，以这种方式整合特征可以提高生成字幕的质量和细节。", "conclusion": "通过将音乐特征检测整合到多任务学习框架中，SonicVerse模型能够生成更丰富、更详细的音乐字幕，并为长音乐片段提供时间感知的描述。", "translation": "详细准确反映音乐作品特征的字幕可以丰富音乐数据库，并推动音乐AI领域的研究。本文介绍了一个多任务音乐字幕生成模型SonicVerse，它将字幕生成与辅助音乐特征检测任务（如音调检测、人声检测等）相结合，以直接捕捉低级声学细节和高级音乐属性。其关键贡献是一种基于投影的架构，将音频输入转换为语言标记，同时通过专用的辅助头检测音乐特征。这些辅助头的输出也被投影到语言标记中，以增强字幕输入。该框架不仅能为短音乐片段生成丰富、描述性的字幕，还能通过使用大型语言模型链接输出来为较长的音乐作品生成详细的时间感知描述。为了训练该模型，我们通过使用模块化音乐特征提取器MIMFLEX标注音乐特征，扩展了MusicBench数据集，从而得到了配对的音频、字幕和音乐特征数据。实验结果表明，以这种方式整合特征可以提高生成字幕的质量和细节。", "summary": "SonicVerse是一个创新的多任务音乐字幕生成模型，旨在通过整合音调和人声等辅助音乐特征检测任务来生成详细的音乐描述。它采用独特的投影架构，将音频和检测到的特征转换为语言标记，以增强字幕生成。该模型不仅能为短片段提供丰富描述，还能结合大型语言模型为长音乐作品生成时间感知的详细描述。通过扩展MusicBench数据集并进行实验，研究表明整合音乐特征显著提高了生成字幕的质量和细节。", "keywords": "音乐字幕生成, 多任务学习, 音乐特征检测, SonicVerse, 音频AI", "comments": "SonicVerse的创新之处在于其多任务学习方法，将音乐特征检测与字幕生成相结合，通过投影架构有效地融合了低级声学细节和高级音乐属性。这种方法不仅提升了字幕的质量，还为长音乐片段的描述提供了解决方案。该研究对于音乐信息检索和音乐AI领域具有重要意义，尤其是在丰富音乐元数据和推动内容理解方面。"}}
{"id": "2506.15070", "title": "Toward a Lightweight, Scalable, and Parallel Secure Encryption Engine", "authors": ["Rasha Karakchi", "Rye Stahle-Smith", "Nishant Chinnasami", "Tiffany Yu"], "summary": "The exponential growth of Internet of Things (IoT) applications has\nintensified the demand for efficient, high-throughput, and energy-efficient\ndata processing at the edge. Conventional CPU-centric encryption methods suffer\nfrom performance bottlenecks and excessive data movement, especially in\nlatency-sensitive and resource-constrained environments. In this paper, we\npresent SPiME, a lightweight, scalable, and FPGA-compatible Secure\nProcessor-in-Memory Encryption architecture that integrates the Advanced\nEncryption Standard (AES-128) directly into a Processing-in-Memory (PiM)\nframework. SPiME is designed as a modular array of parallel PiM units, each\ncombining an AES core with a minimal control unit to enable distributed\nin-place encryption with minimal overhead. The architecture is fully\nimplemented in Verilog and tested on multiple AMD UltraScale and UltraScale+\nFPGAs. Evaluation results show that SPiME can scale beyond 4,000 parallel units\nwhile maintaining less than 5\\% utilization of key FPGA resources on high-end\ndevices. It delivers over 25~Gbps in sustained encryption throughput with\npredictable, low-latency performance. The design's portability,\nconfigurability, and resource efficiency make it a compelling solution for\nsecure edge computing, embedded cryptographic systems, and customizable\nhardware accelerators.", "comment": "This is submitted to the ACM/IEEE Symposium on Edge Computing (SEC\n  2025)", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15070v1", "AI": {"title_translation": "迈向轻量级、可扩展、并行的安全加密引擎", "tldr": "SPiME是一种轻量级、可扩展的FPGA兼容内存内安全加密架构，解决了边缘计算中传统加密方法的性能瓶颈。", "motivation": "物联网应用快速增长，对边缘高效、高吞吐量、节能的数据处理需求增加。传统CPU加密方法在延迟敏感和资源受限环境中存在性能瓶颈和数据移动过多问题。", "method": "本文提出SPiME，一种轻量级、可扩展、FPGA兼容的内存内安全处理器加密架构。它将AES-128直接集成到内存内处理(PiM)框架中，设计为并行PiM单元的模块化阵列，每个单元结合AES核心和最小控制单元，实现分布式就地加密。该架构用Verilog实现，并在AMD UltraScale和UltraScale+ FPGA上测试。", "result": "SPiME可扩展至4,000多个并行单元，同时高端设备的关键FPGA资源利用率低于5%。它提供超过25 Gbps的持续加密吞吐量，并具有可预测的低延迟性能。", "conclusion": "SPiME设计的便携性、可配置性和资源效率使其成为安全边缘计算、嵌入式密码系统和可定制硬件加速器的有吸引力的解决方案。", "translation": "物联网 (IoT) 应用的指数级增长加剧了对边缘高效、高吞吐量和节能数据处理的需求。传统的以 CPU 为中心的加密方法存在性能瓶颈和过多的数据移动，特别是在延迟敏感和资源受限的环境中。在本文中，我们提出了 SPiME，一种轻量级、可扩展且与 FPGA 兼容的安全内存处理器加密架构，它将高级加密标准 (AES-128) 直接集成到内存处理 (PiM) 框架中。SPiME 被设计为并行 PiM 单元的模块化阵列，每个单元将一个 AES 核心与一个最小控制单元相结合，以实现开销最小的分布式就地加密。该架构完全采用 Verilog 实现，并在多个 AMD UltraScale 和 UltraScale+ FPGA 上进行了测试。评估结果表明，SPiME 可以扩展到 4,000 多个并行单元，同时在高端设备上保持不到 5% 的关键 FPGA 资源利用率。它以可预测的低延迟性能提供超过 25 Gbps 的持续加密吞吐量。该设计的可移植性、可配置性和资源效率使其成为安全边缘计算、嵌入式加密系统和可定制硬件加速器的引人注目解决方案。", "summary": "本文提出SPiME，一种轻量级、可扩展的FPGA兼容内存内安全加密架构，旨在解决物联网边缘计算中传统CPU加密方法的性能瓶颈和高数据移动问题。SPiME将AES-128集成到内存内处理框架中，作为并行PiM单元的模块化阵列，实现分布式就地加密。在FPGA上的评估显示，SPiME具有出色的可扩展性（超过4,000个单元）、低资源利用率（<5%）和高吞吐量（>25 Gbps），为边缘计算和嵌入式系统提供了高效安全的加密解决方案。", "keywords": "内存内处理, FPGA, AES-128, 边缘计算, 安全加密", "comments": "这篇论文的创新点在于提出了SPiME架构，将AES-128直接集成到内存内处理(PiM)框架中，有效解决了传统CPU加密在边缘设备上的性能和资源限制。其模块化、并行化的设计理念以及在FPGA上的成功实现，展示了其在提供高吞吐量和低延迟加密方面的潜力，对于推动安全边缘计算领域的发展具有重要意义。"}}
{"id": "2506.15135", "title": "Towards Bug-Free Distributed Go Programs", "authors": ["Zhengqun Koo"], "summary": "Programmers of distributed systems need to reason about concurrency to avoid\nraces. However, reasoning about concurrency is difficult, and unexpected races\nshow up as bugs. Data race detection in shared memory systems is well-studied\n(dynamic data race detection [13], behavioral types [15], dynamic race\ndetection [31]). Similar to how a data race consists of reads and writes not\nrelated by happens-before at a shared memory location, a communication race\nconsists of receives and sends not related by happens-before on a shared\nchannel. Communication races are problematic: a receiver expects a specific\nmessage from a specific sender, but with a communication race, the receiver can\nreceive a message meant for another receiver, or not receive anything at all.\nIn this work, we describe a verification framework that can prove the absence\nof communication races for distributed programs that use a subset of the Go\nprogramming language, where synchronization is mainly achieved via message\npassing. We statically reason about how a distributed program executes, using a\nhappens-before order, extended to buffered and unbuffered channels.", "comment": "Version 1. B.Comp. Dissertation", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15135v1", "AI": {"title_translation": "迈向无错误分布式Go程序", "tldr": "本文提出一个验证框架，用于静态证明使用Go语言子集的分布式程序中不存在通信竞争，通过扩展happens-before顺序来分析消息传递同步。", "motivation": "分布式系统程序员需要处理并发以避免竞争条件，但推理并发性很困难，意外的竞争会导致错误。共享内存系统中的数据竞争已得到充分研究，但通道上的通信竞争（接收和发送之间没有happens-before关系）类似且同样有问题，可能导致接收到错误消息或根本没有消息。", "method": "本文描述了一个验证框架，该框架可以证明使用Go语言子集的分布式程序中不存在通信竞争，其中同步主要通过消息传递实现。该框架通过静态推理分布式程序的执行方式，并使用扩展到缓冲和非缓冲通道的happens-before顺序。", "result": "该验证框架能够证明分布式Go程序中不存在通信竞争。", "conclusion": "本文成功开发了一个验证框架，能够静态地证明基于Go语言子集且依赖消息传递进行同步的分布式程序中通信竞争的缺失。", "translation": "分布式系统的程序员需要推理并发以避免竞争。然而，推理并发性很困难，并且意外的竞争表现为错误。共享内存系统中的数据竞争检测已得到充分研究（动态数据竞争检测 [13]、行为类型 [15]、动态竞争检测 [31]）。类似于数据竞争由共享内存位置上不通过happens-before关系关联的读写组成，通信竞争由共享通道上不通过happens-before关系关联的接收和发送组成。通信竞争是有问题的：接收方期望从特定发送方接收特定消息，但存在通信竞争时，接收方可能会收到本应发给另一个接收方的消息，或者根本没有收到任何消息。在这项工作中，我们描述了一个验证框架，该框架可以证明使用Go编程语言子集（其中同步主要通过消息传递实现）的分布式程序中不存在通信竞争。我们使用扩展到缓冲和非缓冲通道的happens-before顺序，静态地推理分布式程序的执行方式。", "summary": "本文提出了一个针对Go语言子集分布式程序的验证框架，旨在解决通信竞争问题。与共享内存系统中的数据竞争类似，通信竞争发生在共享通道上，当接收和发送操作之间缺乏happens-before关系时，可能导致消息传递错误。该框架通过静态分析程序的执行，并扩展happens-before顺序以涵盖Go的缓冲和非缓冲通道，从而能够证明程序中通信竞争的缺失，有助于构建无错误的分布式系统。", "keywords": "分布式系统, Go语言, 通信竞争, happens-before, 静态验证", "comments": "这篇论文的创新点在于将“happens-before”顺序的概念扩展到Go语言的缓冲和非缓冲通道，并将其应用于分布式程序中的通信竞争检测。这项工作的重要性在于为Go语言编写的分布式系统提供了一种静态验证通信竞争缺失的方法，这对于提高分布式程序的可靠性和健然性至关重要。其局限性可能在于其仅适用于Go语言的一个子集，并且静态分析的复杂性和可扩展性可能是一个挑战。"}}
{"id": "2506.14975", "title": "Time-Optimized Safe Navigation in Unstructured Environments through Learning Based Depth Completion", "authors": ["Jeffrey Mao", "Raghuram Cauligi Srinivas", "Steven Nogar", "Giuseppe Loianno"], "summary": "Quadrotors hold significant promise for several applications such as\nagriculture, search and rescue, and infrastructure inspection. Achieving\nautonomous operation requires systems to navigate safely through complex and\nunfamiliar environments. This level of autonomy is particularly challenging due\nto the complexity of such environments and the need for real-time decision\nmaking especially for platforms constrained by size, weight, and power (SWaP),\nwhich limits flight time and precludes the use of bulky sensors like Light\nDetection and Ranging (LiDAR) for mapping. Furthermore, computing globally\noptimal, collision-free paths and translating them into time-optimized, safe\ntrajectories in real time adds significant computational complexity. To address\nthese challenges, we present a fully onboard, real-time navigation system that\nrelies solely on lightweight onboard sensors. Our system constructs a dense 3D\nmap of the environment using a novel visual depth estimation approach that\nfuses stereo and monocular learning-based depth, yielding longer-range, denser,\nand less noisy depth maps than conventional stereo methods. Building on this\nmap, we introduce a novel planning and trajectory generation framework capable\nof rapidly computing time-optimal global trajectories. As the map is\nincrementally updated with new depth information, our system continuously\nrefines the trajectory to maintain safety and optimality. Both our planner and\ntrajectory generator outperforms state-of-the-art methods in terms of\ncomputational efficiency and guarantee obstacle-free trajectories. We validate\nour system through robust autonomous flight experiments in diverse indoor and\noutdoor environments, demonstrating its effectiveness for safe navigation in\npreviously unknown settings.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.14975v1", "AI": {"title_translation": "基于学习深度补全的非结构化环境时间优化安全导航", "tldr": "该论文提出了一种全机载、实时导航系统，通过融合立体和单目学习深度估计来构建密集的3D地图，并结合新的规划和轨迹生成框架，使四旋翼飞行器能够在非结构化环境中安全、时间优化地导航。", "motivation": "四旋翼飞行器在农业、搜救和基础设施检查等应用中具有巨大潜力，但其自主操作面临挑战，尤其是在复杂和未知环境中实现实时决策，且受限于尺寸、重量和功率(SWaP)，无法使用笨重传感器如LiDAR。此外，实时计算全局最优、无碰撞路径并转换为时间优化的安全轨迹也增加了计算复杂性。", "method": "该系统采用全机载、实时导航方法，仅依赖轻量级机载传感器。它通过一种新颖的视觉深度估计方法构建密集的3D地图，该方法融合了立体和单目学习深度，生成比传统立体方法更远距离、更密集、噪声更少的深度图。在此地图基础上，引入了一种新的规划和轨迹生成框架，能够快速计算时间最优的全局轨迹，并随地图更新连续优化轨迹以保持安全性和最优性。", "result": "该系统的规划器和轨迹生成器在计算效率方面优于现有最先进的方法，并能保证无障碍轨迹。通过在多样化的室内外环境中的鲁棒自主飞行实验验证了其有效性。", "conclusion": "该系统能够有效实现四旋翼飞行器在未知环境中安全导航，克服了传统方法在传感器和计算方面的限制。", "translation": "四旋翼飞行器在农业、搜救和基础设施检查等多个应用中具有重要前景。实现自主操作要求系统能够在复杂和陌生的环境中安全导航。这种程度的自主性尤其具有挑战性，因为这些环境的复杂性以及实时决策的需求，特别是对于受尺寸、重量和功率（SWaP）限制的平台，这限制了飞行时间并排除了使用笨重传感器如激光雷达（LiDAR）进行地图构建。此外，实时计算全局最优、无碰撞路径并将其转换为时间优化的安全轨迹增加了显著的计算复杂性。为了解决这些挑战，我们提出了一种完全机载的实时导航系统，该系统仅依赖轻量级机载传感器。我们的系统利用一种新颖的视觉深度估计方法构建环境的密集3D地图，该方法融合了立体和单目学习深度，生成的深度图比传统立体方法具有更长距离、更密集且噪声更低的特点。在此地图基础上，我们引入了一种新颖的规划和轨迹生成框架，能够快速计算时间最优的全局轨迹。随着地图通过新的深度信息增量更新，我们的系统不断优化轨迹以保持安全性和最优性。我们的规划器和轨迹生成器在计算效率方面均优于现有最先进的方法，并保证无障碍轨迹。我们通过在多样化的室内外环境中的鲁棒自主飞行实验验证了我们的系统，展示了其在先前未知环境中安全导航的有效性。", "summary": "该论文介绍了一种用于四旋翼飞行器在非结构化环境中进行时间优化安全导航的完全机载实时系统。该系统通过融合立体和单目学习深度估计，利用轻量级传感器构建高密度、低噪声的3D环境地图。在此基础上，开发了一种新颖的规划和轨迹生成框架，能够快速计算并持续优化时间最优的无障碍轨迹。实验证明，该系统在计算效率和导航安全性方面均优于现有技术，适用于未知环境。", "keywords": "四旋翼飞行器, 深度补全, 安全导航, 时间优化, 非结构化环境", "comments": "该论文的创新点在于结合了学习型深度估计（融合立体和单目）与实时规划和轨迹生成，解决了小型SWaP受限平台在复杂非结构化环境中自主导航的难题。其重要性体现在为资源受限的自主系统提供了一种高效、鲁棒的解决方案，尤其适用于需要高自主性和安全性的应用场景。该研究对未来小型无人机在复杂环境中的应用具有重要推动作用。"}}
{"id": "2506.14820", "title": "Navigating High-Dimensional Backstage: A Guide for Exploring Literature for the Reliable Use of Dimensionality Reduction", "authors": ["Hyeon Jeon", "Hyunwook Lee", "Yun-Hsin Kuo", "Taehyun Yang", "Daniel Archambault", "Sungahn Ko", "Takanori Fujiwara", "Kwan-Liu Ma", "Jinwook Seo"], "summary": "Visual analytics using dimensionality reduction (DR) can easily be unreliable\nfor various reasons, e.g., inherent distortions in representing the original\ndata. The literature has thus proposed a wide range of methodologies to make\nDR-based visual analytics reliable. However, the diversity and extensiveness of\nthe literature can leave novice analysts and researchers uncertain about where\nto begin and proceed. To address this problem, we propose a guide for reading\npapers for reliable visual analytics with DR. Relying on the previous\nclassification of the relevant literature, our guide helps both practitioners\nto (1) assess their current DR expertise and (2) identify papers that will\nfurther enhance their understanding. Interview studies with three experts in DR\nand data visualizations validate the significance, comprehensiveness, and\nusefulness of our guide.", "comment": "EG/VGTC EuroVis 2025 Short paper", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14820v1", "AI": {"title_translation": "驾驭高维幕后：一份探索可靠使用降维的文献指南", "tldr": "本研究提出了一份指南，帮助分析师通过查阅相关文献，可靠地使用降维技术进行可视化分析。", "motivation": "使用降维（DR）的可视化分析可能不可靠，且相关文献的多样性和广泛性使新手分析师和研究人员不知所措。", "method": "本文提出了一份阅读可靠降维可视化分析文献的指南，该指南基于现有文献分类，帮助实践者评估降维专业知识并识别相关论文。通过对三位专家进行访谈研究，验证了该指南的有效性。", "result": "对三位降维和数据可视化专家的访谈研究验证了该指南的重要性、全面性和实用性。", "conclusion": "该指南能帮助实践者评估其当前的降维专业知识，并识别能进一步提升理解的相关论文。", "translation": "使用降维（DR）进行可视化分析可能因各种原因而不可靠，例如表示原始数据时固有的失真。因此，文献中提出了多种方法来使基于DR的可视化分析变得可靠。然而，文献的多样性和广泛性可能使新手分析师和研究人员不确定从何开始和如何进行。为了解决这个问题，我们提出了一份关于阅读用于可靠DR可视化分析的论文的指南。我们的指南依靠先前对相关文献的分类，帮助实践者（1）评估他们当前的DR专业知识，以及（2）识别能进一步提高他们理解的论文。对三位DR和数据可视化专家的访谈研究验证了我们指南的重要性、全面性和实用性。", "summary": "本文提出了一份指南，旨在帮助新手分析师和研究人员理解和应用可靠的降维可视化分析技术。该指南基于现有文献分类，协助用户评估其降维专业知识并找到相关论文。通过专家访谈，验证了该指南的重要性和实用性。", "keywords": "降维, 可视化分析, 文献指南, 可靠性, 专家访谈", "comments": "该论文通过提供一份文献指南，解决了新手在探索复杂且多样化的降维可视化分析文献时面临的挑战，具有实际应用价值。"}}
{"id": "2506.15464", "title": "Spectral Contraction of Boundary-Weighted Filters on delta-Hyperbolic Graphs", "authors": ["Le Vu Anh", "Mehmet Dik", "Nguyen Viet Anh"], "summary": "Hierarchical graphs often exhibit tree-like branching patterns, a structural\nproperty that challenges the design of traditional graph filters. We introduce\na boundary-weighted operator that rescales each edge according to how far its\nendpoints drift toward the graph's Gromov boundary. Using Busemann functions on\ndelta-hyperbolic networks, we prove a closed-form upper bound on the operator's\nspectral norm: every signal loses a curvature-controlled fraction of its energy\nat each pass. The result delivers a parameter-free, lightweight filter whose\nstability follows directly from geometric first principles, offering a new\nanalytic tool for graph signal processing on data with dense or hidden\nhierarchical structure.", "comment": "5 pages, 5 figures", "cate": "math.MG", "url": "http://arxiv.org/abs/2506.15464v1", "AI": {"title_translation": "δ-双曲图上边界加权滤波器的谱收缩", "tldr": "本文引入了一种边界加权算子，用于处理具有层级结构的图，并证明了其谱范数的闭式上限，从而提供了一种参数无关且轻量级的图滤波器。", "motivation": "传统图滤波器在处理具有树状分支模式的层级图时面临设计挑战。", "method": "引入了一种边界加权算子，该算子根据边的端点向图的格罗莫夫边界漂移的距离来重新缩放每条边。利用δ-双曲网络上的Busemann函数，证明了该算子谱范数的闭式上限。", "result": "证明了该算子谱范数的闭式上限：每个信号在每次通过时都会损失其能量中由曲率控制的一部分。", "conclusion": "该结果提供了一种参数无关、轻量级的滤波器，其稳定性直接源于几何第一性原理，为处理具有密集或隐藏层级结构的数据的图信号处理提供了一种新的分析工具。", "translation": "层级图通常表现出树状分支模式，这种结构属性对传统图滤波器的设计提出了挑战。我们引入了一种边界加权算子，该算子根据每条边的端点向图的格罗莫夫边界漂移的距离来重新缩放每条边。利用δ-双曲网络上的Busemann函数，我们证明了该算子谱范数的闭式上限：每个信号在每次通过时都会损失其能量中由曲率控制的一部分。该结果提供了一种参数无关、轻量级的滤波器，其稳定性直接源于几何第一性原理，为处理具有密集或隐藏层级结构的数据的图信号处理提供了一种新的分析工具。", "summary": "本文针对传统图滤波器在处理层级图时遇到的挑战，提出了一种新的边界加权算子。该算子通过根据边端点到格罗莫夫边界的距离来重新缩放边。研究证明了该算子谱范数的闭式上限，表明信号能量在每次滤波时都会以曲率相关的方式衰减。这项工作提供了一种基于几何原理的、参数无关的轻量级图滤波器，为处理具有复杂层级结构的数据提供了新的分析工具。", "keywords": "边界加权滤波器, δ-双曲图, 谱收缩, 图信号处理, Busemann函数", "comments": "该论文的创新点在于引入了基于图几何（Gromov边界和Busemann函数）的边界加权算子，并从几何第一性原理出发证明了其谱收缩性质，这为图信号处理提供了一种理论基础强、参数无关的新型滤波器。其重要性体现在为处理具有复杂层级结构的数据提供了有效的分析工具，尤其适用于传统方法难以应对的场景。"}}
{"id": "2506.15167", "title": "LLM Agent for Hyper-Parameter Optimization", "authors": ["Wanzhe Wang", "Jianqiu Peng", "Menghao Hu", "Weihuang Zhong", "Tong Zhang", "Shuai Wang", "Yixin Zhang", "Mingjie Shao", "Wanli Ni"], "summary": "Hyper-parameters are essential and critical for the performance of\ncommunication algorithms. However, current hyper-parameters tuning methods for\nwarm-start particles swarm optimization with cross and mutation (WS-PSO-CM)\nalgortihm for radio map-enabled unmanned aerial vehicle (UAV) trajectory and\ncommunication are primarily heuristic-based, exhibiting low levels of\nautomation and unsatisfactory performance. In this paper, we design an large\nlanguage model (LLM) agent for automatic hyper-parameters-tuning, where an\niterative framework and model context protocol (MCP) are applied. In\nparticular, the LLM agent is first setup via a profile, which specifies the\nmission, background, and output format. Then, the LLM agent is driven by the\nprompt requirement, and iteratively invokes WS-PSO-CM algorithm for\nexploration. Finally, the LLM agent autonomously terminates the loop and\nreturns a set of hyper-parameters. Our experiment results show that the minimal\nsum-rate achieved by hyper-parameters generated via our LLM agent is\nsignificantly higher than those by both human heuristics and random generation\nmethods. This indicates that an LLM agent with PSO knowledge and WS-PSO-CM\nalgorithm background is useful in finding high-performance hyper-parameters.", "comment": "6 pages, 6 figures", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15167v1", "AI": {"title_translation": "用于超参数优化的LLM智能体", "tldr": "本文提出了一种基于LLM的智能体，用于自动优化通信算法的超参数，并通过实验证明其性能显著优于传统方法。", "motivation": "通信算法的超参数对性能至关重要，但现有针对WS-PSO-CM算法的超参数调优方法主要基于启发式，自动化程度低且性能不佳。", "method": "设计了一个基于大型语言模型（LLM）的智能体，用于自动超参数调优。该LLM智能体采用迭代框架和模型上下文协议（MCP）。具体实现包括：通过配置文件设置LLM智能体的任务、背景和输出格式；LLM智能体根据提示要求，迭代调用WS-PSO-CM算法进行探索；最终自主终止循环并返回一组超参数。", "result": "实验结果表明，通过LLM智能体生成的超参数所实现的最小和速率显著高于人类启发式和随机生成方法。", "conclusion": "具有PSO知识和WS-PSO-CM算法背景的LLM智能体在寻找高性能超参数方面非常有用。", "translation": "超参数对于通信算法的性能至关重要。然而，目前针对射频地图使能的无人机（UAV）轨迹和通信的暖启动粒子群优化与交叉变异（WS-PSO-CM）算法的超参数调优方法主要基于启发式，自动化程度低且性能不尽如人意。在本文中，我们设计了一个用于自动超参数调优的大型语言模型（LLM）智能体，其中应用了迭代框架和模型上下文协议（MCP）。具体而言，LLM智能体首先通过配置文件进行设置，该文件指定了任务、背景和输出格式。然后，LLM智能体由提示要求驱动，并迭代调用WS-PSO-CM算法进行探索。最后，LLM智能体自主终止循环并返回一组超参数。我们的实验结果表明，通过LLM智能体生成的超参数所实现的最小和速率显著高于人类启发式和随机生成方法。这表明，具有PSO知识和WS-PSO-CM算法背景的LLM智能体在寻找高性能超参数方面非常有用。", "summary": "本文提出了一种基于大型语言模型（LLM）的智能体，用于自动优化通信算法中的超参数，特别是针对无人机轨迹和通信的WS-PSO-CM算法。针对现有启发式方法自动化程度低、性能不佳的问题，该LLM智能体通过迭代框架和模型上下文协议（MCP）进行超参数调优。实验结果表明，与人类启发式和随机生成方法相比，该LLM智能体能够生成性能显著更优的超参数，证明了LLM智能体在高效超参数优化方面的潜力。", "keywords": "LLM智能体, 超参数优化, WS-PSO-CM算法, 无人机通信, 自动化调优", "comments": "该论文的创新点在于将LLM智能体应用于传统的超参数优化领域，特别是针对复杂的通信算法。它通过引入迭代框架和模型上下文协议，提高了超参数调优的自动化程度和效率。研究结果表明LLM在结合特定领域知识（如PSO和WS-PSO-CM算法）后，能够超越传统启发式方法，为复杂系统参数优化提供了一种新的、有前景的范式。"}}
{"id": "2506.14825", "title": "GraphGSOcc: Semantic and Geometric Graph Transformer for 3D Gaussian Splating-based Occupancy Prediction", "authors": ["Ke Song", "Yunhe Wu", "Chunchit Siu", "Huiyuan Xiong"], "summary": "Addressing the task of 3D semantic occupancy prediction for autonomous\ndriving, we tackle two key issues in existing 3D Gaussian Splating (3DGS)\nmethods: (1) unified feature aggregation neglecting semantic correlations among\nsimilar categories and across regions, and (2) boundary ambiguities caused by\nthe lack of geometric constraints in MLP iterative optimization. We propose the\nGraphGSOcc model, a novel framework that combines semantic and geometric graph\nTransformer for 3D Gaussian Splating-based Occupancy Prediction. We propose the\nDual Gaussians Graph Attenntion, which dynamically constructs dual graph\nstructures: a geometric graph adaptively calculating KNN search radii based on\nGaussian poses, enabling large-scale Gaussians to aggregate features from\nbroader neighborhoods while compact Gaussians focus on local geometric\nconsistency; a semantic graph retaining top-M highly correlated nodes via\ncosine similarity to explicitly encode semantic relationships within and across\ninstances. Coupled with the Multi-scale Graph Attention framework, fine-grained\nattention at lower layers optimizes boundary details, while coarse-grained\nattention at higher layers models object-level topology. Experiments on the\nSurroundOcc dataset achieve an mIoU of 24.10%, reducing GPU memory to 6.1 GB,\ndemonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to\nGaussianWorld", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14825v1", "AI": {"title_translation": "GraphGSOcc：基于3D高斯泼溅的语义和几何图Transformer用于占用预测", "tldr": "GraphGSOcc提出了一种结合语义和几何图Transformer的新框架，用于解决3D高斯泼溅方法在3D语义占用预测中存在的特征聚合不足和边界模糊问题，并在SurroundOcc数据集上取得了改进的性能和更低的内存消耗。", "motivation": "现有的3D高斯泼溅（3DGS）方法在自动驾驶的3D语义占用预测任务中存在两个关键问题：1）统一的特征聚合忽略了相似类别之间和跨区域的语义关联；2）由于MLP迭代优化中缺乏几何约束，导致边界模糊。", "method": "本文提出了GraphGSOcc模型，这是一个结合语义和几何图Transformer的新框架。它引入了双高斯图注意力（Dual Gaussians Graph Attention），动态构建几何图（基于高斯姿态自适应计算KNN搜索半径）和语义图（通过余弦相似度保留前M个高度相关的节点）。此外，该模型还结合了多尺度图注意力框架，在低层优化边界细节，在高层建模对象级拓扑。", "result": "在SurroundOcc数据集上，GraphGSOcc实现了24.10%的mIoU，并将GPU内存减少到6.1 GB，与GaussianWorld相比，mIoU提高了1.97%，内存减少了13.7%。", "conclusion": "GraphGSOcc通过结合语义和几何图Transformer，有效解决了3DGS方法在3D语义占用预测中的特征聚合和边界模糊问题，显著提高了性能并降低了内存消耗。", "translation": "针对自动驾驶中的3D语义占用预测任务，我们解决了现有3D高斯泼溅（3DGS）方法中的两个关键问题：（1）统一的特征聚合忽略了相似类别之间和跨区域的语义关联，以及（2）由于MLP迭代优化中缺乏几何约束导致的边界模糊。我们提出了GraphGSOcc模型，一个结合语义和几何图Transformer的3D高斯泼溅占用预测新框架。我们提出了双高斯图注意力，它动态构建双图结构：一个几何图，根据高斯姿态自适应计算KNN搜索半径，使大规模高斯能够从更广阔的邻域聚合特征，而紧凑高斯则专注于局部几何一致性；一个语义图，通过余弦相似度保留前M个高度相关的节点，以明确编码实例内部和实例之间的语义关系。结合多尺度图注意力框架，低层的细粒度注意力优化边界细节，而高层的粗粒度注意力建模对象级拓扑。在SurroundOcc数据集上的实验结果表明，mIoU达到24.10%，GPU内存减少到6.1 GB，与GaussianWorld相比，mIoU提高了1.97%，内存减少了13.7%。", "summary": "GraphGSOcc是一种新颖的框架，用于解决自动驾驶中3D高斯泼溅（3DGS）方法在3D语义占用预测中遇到的特征聚合不足和边界模糊问题。该模型提出了双高斯图注意力机制，通过动态构建几何图和语义图来分别处理几何约束和语义关联。结合多尺度图注意力，GraphGSOcc在SurroundOcc数据集上显著提升了mIoU并降低了GPU内存消耗，验证了其在3D场景理解方面的有效性。", "keywords": "3D高斯泼溅, 占用预测, 图Transformer, 语义图, 几何图", "comments": "GraphGSOcc的创新性在于将图Transformer引入3D高斯泼溅，通过双图结构（几何图和语义图）有效地解决了现有方法在特征聚合和边界模糊方面的不足。多尺度图注意力框架进一步提升了模型的细节处理能力和全局拓扑建模能力。该方法在性能提升的同时显著降低了内存消耗，这对于资源受限的自动驾驶应用具有重要意义。"}}
{"id": "2506.15601", "title": "CXL-GPU: Pushing GPU Memory Boundaries with the Integration of CXL Technologies", "authors": ["Donghyun Gouk", "Seungkwan Kang", "Seungjun Lee", "Jiseon Kim", "Kyungkuk Nam", "Eojin Ryu", "Sangwon Lee", "Dongpyung Kim", "Junhyeok Jang", "Hanyeoreum Bae", "Myoungsoo Jung"], "summary": "This work introduces a GPU storage expansion solution utilizing CXL,\nfeaturing a novel GPU system design with multiple CXL root ports for\nintegrating diverse storage media (DRAMs and/or SSDs). We developed and\nsiliconized a custom CXL controller integrated at the hardware RTL level,\nachieving two-digit nanosecond roundtrip latency, the first in the field. This\nstudy also includes speculative read and deterministic store mechanisms to\nefficiently manage read and write operations to hide the endpoint's backend\nmedia latency variation. Performance evaluations reveal our approach\nsignificantly outperforms existing methods, marking a substantial advancement\nin GPU storage technology.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15601v1", "AI": {"title_translation": "CXL-GPU：通过集成CXL技术突破GPU内存限制", "tldr": "提出一种基于CXL的GPU存储扩展方案，通过定制CXL控制器和优化机制，实现低延迟并显著提升性能。", "motivation": "现有GPU内存存在边界限制，需要一种解决方案来扩展GPU存储能力。", "method": "引入CXL技术实现GPU存储扩展，设计新型GPU系统，包含多个CXL根端口以集成DRAM和/或SSD。开发并硅化定制CXL控制器，集成在硬件RTL层面，并实现了两位数纳秒级往返延迟。采用推测性读取和确定性存储机制来管理读写操作，以隐藏后端媒体延迟变化。", "result": "定制CXL控制器实现了两位数纳秒级的往返延迟，是该领域首次达到。性能评估显示，该方法显著优于现有方法。", "conclusion": "该研究通过CXL集成和定制控制器，在GPU存储技术上取得了实质性进展，成功扩展了GPU内存边界并提升了性能。", "translation": "这项工作介绍了一种利用CXL的GPU存储扩展解决方案，其特点是采用新颖的GPU系统设计，具有多个CXL根端口，用于集成各种存储介质（DRAM和/或SSD）。我们开发并硅化了一个定制的CXL控制器，该控制器在硬件RTL级别集成，实现了两位数的纳秒级往返延迟，这是该领域的首次。本研究还包括推测性读取和确定性存储机制，以有效管理读写操作，从而隐藏端点后端介质的延迟变化。性能评估表明，我们的方法显著优于现有方法，标志着GPU存储技术取得了实质性进展。", "summary": "该论文提出并实现了CXL-GPU，一个利用CXL技术扩展GPU存储的解决方案。通过设计带有多个CXL根端口的GPU系统，并开发定制的硬件RTL级CXL控制器，实现了创纪录的两位数纳秒级往返延迟。此外，结合推测性读取和确定性存储机制，有效隐藏了存储介质的延迟变化。性能评估表明，该方案显著优于现有技术，代表了GPU存储领域的重要进步。", "keywords": "CXL, GPU存储, 内存扩展, 低延迟, 硬件RTL", "comments": "这项工作的创新之处在于首次在硬件RTL层面实现了两位数纳秒级的CXL控制器往返延迟，这对于高性能计算至关重要。通过集成多样存储介质和优化读写机制，有效地突破了传统GPU内存的限制，对未来大规模数据处理和AI应用具有重要意义。"}}
{"id": "2506.15155", "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving", "authors": ["Jiale Xu", "Rui Zhang", "Yi Xiong", "Cong Guo", "Zihan Liu", "Yangjie Zhou", "Weiming Hu", "Hao Wu", "Changxu Shao", "Ziqing Wang", "Yongjie Yuan", "Junping Zhao", "Minyi Guo", "Jingwen Leng"], "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15155v1", "AI": {"title_translation": "eLLM：用于高效LLM服务的弹性内存管理框架", "tldr": "eLLM是一个受操作系统内存膨胀机制启发的弹性内存管理框架，通过统一运行时内存和KV缓存管理、引入虚拟张量抽象、弹性内存机制和轻量级调度策略，显著提高了大型语言模型服务的吞吐量和批处理大小。", "motivation": "现代LLM服务系统对运行时内存和KV缓存采用不同的抽象级别进行处理，这种双层方法导致内存管理隔离，在动态工作负载下内存利用率低下，可能导致吞吐量下降近20%。", "method": "eLLM框架包含三个核心组件：1) 虚拟张量抽象，将张量的虚拟地址空间与物理GPU内存解耦，创建统一灵活的内存池；2) 弹性内存机制，通过运行时内存的膨胀和收缩动态调整内存分配，利用CPU内存作为可扩展缓冲区；3) 轻量级调度策略，采用SLO感知的策略来优化内存利用率并平衡性能权衡。", "result": "eLLM显著优于现有最先进的系统，实现了2.32倍更高的解码吞吐量，并支持128K-token输入的3倍更大批处理大小。", "conclusion": "eLLM通过其创新的弹性内存管理框架，有效解决了LLM服务中的内存利用率低下问题，显著提升了吞吐量和批处理能力，为LLM的高效部署提供了新的解决方案。", "translation": "大型语言模型正越来越多地部署在数据中心。服务这些模型需要仔细的内存管理，因为它们的内存使用包括静态权重、动态激活和键值缓存。虽然静态权重是恒定且可预测的，但动态组件（如激活和KV缓存）在运行时频繁变化，给高效内存管理带来了巨大挑战。现代LLM服务系统通常在不同的抽象级别处理运行时内存和KV缓存：运行时内存管理依赖于静态张量抽象，而KV缓存则利用在张量抽象之上构建的基于页表的虚拟化层。这种虚拟化动态管理KV缓存以减轻内存碎片化。然而，这种双层方法从根本上隔离了运行时内存和KV缓存管理，导致在动态工作负载下内存利用率不佳，这可能导致吞吐量下降近20%。\n为了解决这些限制，我们提出了eLLM，一个受操作系统中经典内存膨胀机制启发的弹性内存管理框架。eLLM的核心组件包括：(1) 虚拟张量抽象，将张量的虚拟地址空间与物理GPU内存解耦，创建一个统一且灵活的内存池；(2) 弹性内存机制，通过运行时内存的膨胀和收缩动态调整内存分配，利用CPU内存作为可扩展缓冲区；以及(3) 轻量级调度策略，采用SLO感知的策略来优化内存利用率并有效平衡在严格SLO约束下的性能权衡。全面的评估表明，eLLM显著优于现有最先进的系统，实现了2.32倍更高的解码吞吐量，并支持128K-token输入的3倍更大批处理大小。", "summary": "eLLM是一个新颖的弹性内存管理框架，旨在解决大型语言模型（LLM）服务中由于运行时内存和KV缓存管理隔离导致的内存利用率低下问题。该框架引入了虚拟张量抽象以统一内存池，弹性内存机制利用CPU内存作为可扩展缓冲区进行动态分配，并采用SLO感知的轻量级调度策略。实验结果表明，eLLM相较于现有系统，可将解码吞吐量提高2.32倍，并支持3倍更大的批处理大小，从而显著提升LLM服务的效率和性能。", "keywords": "LLM服务, 内存管理, 弹性内存, 虚拟张量, KV缓存", "comments": "eLLM的创新之处在于其借鉴了操作系统中经典的内存膨胀机制，并将其应用于LLM的内存管理，通过统一的虚拟张量抽象和弹性内存机制，有效解决了传统LLM服务中内存碎片化和利用率低下的问题。其将CPU内存作为GPU内存的有效扩展，以及SLO感知的调度策略，都体现了对实际部署场景的深刻理解和优化。这项工作对于提升LLM在数据中心部署的效率和降低运营成本具有重要意义。"}}
{"id": "2506.15278", "title": "Not Even Nice Work If You Can Get It; A Longitudinal Study of Uber's Algorithmic Pay and Pricing", "authors": ["Reuben Binns", "Jake Stein", "Siddhartha Datta", "Max Van Kleek", "Nigel Shadbolt"], "summary": "Ride-sharing platforms like Uber market themselves as enabling `flexibility'\nfor their workforce, meaning that drivers are expected to anticipate when and\nwhere the algorithm will allocate them jobs, and how well remunerated those\njobs will be. In this work we describe our process of participatory action\nresearch with drivers and trade union organisers, culminating in a\nparticipatory audit of Uber's algorithmic pay and work allocation, before and\nafter the introduction of dynamic pricing. Through longitudinal analysis of 1.5\nmillion trips from 258 drivers in the UK, we find that after dynamic pricing,\npay has decreased, Uber's cut has increased, job allocation and pay is less\npredictable, inequality between drivers is increased, and drivers spend more\ntime waiting for jobs. In addition to these findings, we provide methodological\nand theoretical contributions to algorithm auditing, gig work, and the emerging\npractice of worker data science.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.15278v1", "AI": {"title_translation": "即使能得到也不是好工作；一项关于优步算法薪酬和定价的纵向研究", "tldr": "一项对优步算法薪酬和定价的纵向研究发现，在引入动态定价后，司机的收入减少，优步的抽成增加，工作分配和薪酬变得更不可预测，司机间的不平等加剧，且等待工作的时间更长。", "motivation": "优步等网约车平台声称能为劳动力提供“灵活性”，但司机需要预测算法何时何地分配工作以及报酬高低。本研究旨在通过参与式行动研究，调查优步算法薪酬和工作分配的实际影响，尤其是在引入动态定价之后。", "method": "研究团队与司机和工会组织者进行了参与式行动研究，并对优步的算法薪酬和工作分配进行了参与式审计。通过对英国258名司机150万次行程的纵向分析，比较了动态定价引入前后的情况。", "result": "在引入动态定价后，司机的收入下降，优步的抽成增加，工作分配和薪酬的可预测性降低，司机之间的不平等加剧，并且司机等待工作的时间更长。", "conclusion": "本研究通过对优步动态定价的纵向分析，揭示了其对司机收入、工作稳定性及公平性的负面影响。此外，本研究对算法审计、零工经济和工人数据科学的新兴实践做出了方法论和理论贡献。", "translation": "优步等网约车平台声称能为其劳动力提供“灵活性”，这意味着司机需要预测算法何时何地会给他们分配工作，以及这些工作的报酬会如何。在这项工作中，我们描述了与司机和工会组织者进行参与式行动研究的过程，最终在引入动态定价前后，对优步的算法薪酬和工作分配进行了参与式审计。通过对英国258名司机的150万次行程进行纵向分析，我们发现，在动态定价之后，薪酬下降了，优步的抽成增加了，工作分配和薪酬变得更不可预测，司机之间的不平等加剧了，并且司机花在等待工作上的时间更长了。除了这些发现，我们还为算法审计、零工经济和工人数据科学的新兴实践提供了方法论和理论贡献。", "summary": "本研究通过对英国258名优步司机150万次行程的纵向分析，揭示了优步引入动态定价后对司机的影响。研究发现，动态定价导致司机收入下降、优步抽成增加、工作分配和薪酬可预测性降低、司机间不平等加剧以及等待时间增加。该研究采用了与司机和工会组织者合作的参与式行动研究方法，并为算法审计、零工经济和工人数据科学领域做出了贡献。", "keywords": "优步, 算法薪酬, 动态定价, 零工经济, 纵向研究", "comments": "这项研究通过结合参与式行动研究和大规模数据分析，深入揭示了优步动态定价对司机收入和工作条件的负面影响，这在理解零工经济中算法权力方面具有重要意义。其对算法审计和工人数据科学的方法论贡献也极具创新性。"}}
{"id": "2506.15106", "title": "Local Differential Privacy for Distributed Stochastic Aggregative Optimization with Guaranteed Optimality", "authors": ["Ziqin Chen", "Yongqiang Wang"], "summary": "Distributed aggregative optimization underpins many cooperative optimization\nand multi-agent control systems, where each agent's objective function depends\nboth on its local optimization variable and an aggregate of all agents'\noptimization variables. Existing distributed aggregative optimization\napproaches typically require access to accurate gradients of the objective\nfunctions, which, however, are often hard to obtain in real-world applications.\nFor example, in machine learning, gradients are commonly contaminated by two\nmain sources of noise: the randomness inherent in sampled data, and the\nadditional variability introduced by mini-batch computations. In addition to\nthe issue of relying on accurate gradients, existing distributed aggregative\noptimization approaches require agents to share explicit information, which\ncould breach the privacy of participating agents. We propose an algorithm that\ncan solve both problems with existing distributed aggregative optimization\napproaches: not only can the proposed algorithm guarantee mean-square\nconvergence to an exact optimal solution when the gradients are subject to\nnoise, it also simultaneously ensures rigorous differential privacy, with the\ncumulative privacy budget guaranteed to be finite even when the number of\niterations tends to infinity. To the best of our knowledge, this is the first\nalgorithm able to guarantee both accurate convergence and rigorous differential\nprivacy in distributed aggregative optimization. Besides characterizing the\nconvergence rates under nonconvex/convex/strongly convex conditions, we also\nrigorously quantify the cost of differential privacy in terms of convergence\nrates. Experimental results on personalized machine learning using benchmark\ndatasets confirm the efficacy of the proposed algorithm.", "comment": "21 pages, 6 figures", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15106v1", "AI": {"title_translation": "局部差分隐私用于保证最优性的分布式随机聚合优化", "tldr": "本文提出了一种新算法，解决了分布式聚合优化中梯度噪声和隐私泄露的问题，首次实现在保证严格差分隐私的同时，对含噪声梯度实现精确收敛。", "motivation": "现有的分布式聚合优化方法依赖于难以获取的准确梯度（常受采样数据和mini-batch计算噪声污染），且要求代理间共享显式信息，存在隐私泄露风险。", "method": "本文提出了一种新算法，该算法能在梯度受噪声影响时，保证均方收敛到精确最优解，并同时确保严格的差分隐私，即使迭代次数趋于无穷大，累积隐私预算也保证有限。该算法还分析了在非凸/凸/强凸条件下的收敛速率，并量化了差分隐私对收敛速率的代价。", "result": "所提出的算法在梯度受噪声影响时，能够保证均方收敛到精确最优解，并同时确保严格的差分隐私，且累积隐私预算有限。实验结果在个性化机器学习基准数据集上证实了该算法的有效性。", "conclusion": "本文首次提出了一种算法，能够在分布式聚合优化中同时保证精确收敛和严格差分隐私，解决了现有方法的梯度噪声和隐私泄露问题，并量化了隐私代价。", "translation": "分布式聚合优化是许多协作优化和多智能体控制系统的基础，其中每个智能体的目标函数既取决于其局部优化变量，也取决于所有智能体优化变量的聚合。现有的分布式聚合优化方法通常需要访问目标函数的精确梯度，然而，在实际应用中这往往难以获得。例如，在机器学习中，梯度通常会受到两种主要噪声源的污染：采样数据固有的随机性，以及小批量计算引入的额外变异性。除了依赖精确梯度的问题外，现有的分布式聚合优化方法要求智能体共享明确信息，这可能侵犯参与智能体的隐私。我们提出了一种算法，可以同时解决现有分布式聚合优化方法的这两个问题：所提出的算法不仅可以在梯度受噪声影响时保证均方收敛到精确最优解，而且还同时确保严格的差分隐私，即使迭代次数趋于无穷大，累积隐私预算也保证是有限的。据我们所知，这是第一个能够在分布式聚合优化中同时保证精确收敛和严格差分隐私的算法。除了描述在非凸/凸/强凸条件下的收敛速率外，我们还严格量化了差分隐私在收敛速率方面的代价。在个性化机器学习中使用基准数据集进行的实验结果证实了所提出算法的有效性。", "summary": "本文针对分布式聚合优化中梯度噪声和隐私泄露两大挑战，提出了一种创新的算法。该算法首次实现了在梯度存在噪声的情况下，仍能保证均方收敛到精确最优解，并同时提供严格的差分隐私保护，确保即使在无限迭代下累积隐私预算也有限。研究还分析了算法在不同凸性条件下的收敛速率，并量化了隐私保护带来的收敛代价，通过个性化机器学习实验验证了其有效性。", "keywords": "分布式聚合优化, 局部差分隐私, 随机梯度, 优化算法, 隐私保护", "comments": "这项研究具有重要的创新性，它首次成功地将差分隐私保护引入到存在噪声梯度的分布式聚合优化中，解决了长期存在的隐私和准确性之间的权衡问题。其理论分析严谨，不仅证明了收敛性，还量化了隐私代价，为实际应用提供了坚实的理论基础。该工作对于需要保护数据隐私的分布式机器学习和多智能体系统具有重要意义。"}}
{"id": "2506.15136", "title": "Out-of-Band Modality Synergy Based Multi-User Beam Prediction and Proactive BS Selection with Zero Pilot Overhead", "authors": ["Kehui Li", "Binggui Zhou", "Jiajia Guo", "Feifei Gao", "Guanghua Yang", "Shaodan Ma"], "summary": "Multi-user millimeter-wave communication relies on narrow beams and dense\ncell deployments to ensure reliable connectivity. However, tracking optimal\nbeams for multiple mobile users across multiple base stations (BSs) results in\nsignificant signaling overhead. Recent works have explored the capability of\nout-of-band (OOB) modalities in obtaining spatial characteristics of wireless\nchannels and reducing pilot overhead in single-BS single-user/multi-user\nsystems. However, applying OOB modalities for multi-BS selection towards dense\ncell deployments leads to high coordination overhead, i.e, excessive computing\noverhead and high latency in data exchange. How to leverage OOB modalities to\neliminate pilot overhead and achieve efficient multi-BS coordination in\nmulti-BS systems remains largely unexplored. In this paper, we propose a novel\nOOB modality synergy (OMS) based mobility management scheme to realize\nmulti-user beam prediction and proactive BS selection by synergizing two OOB\nmodalities, i.e., vision and location. Specifically, mobile users are initially\nidentified via spatial alignment of visual sensing and location feedback, and\nthen tracked according to the temporal correlation in image sequence.\nSubsequently, a binary encoding map based gain and beam prediction network\n(BEM-GBPN) is designed to predict beamforming gains and optimal beams for\nmobile users at each BS, such that a central unit can control the BSs to\nperform user handoff and beam switching. Simulation results indicate that the\nproposed OMS-based mobility management scheme enhances beam prediction and BS\nselection accuracy and enables users to achieve 91% transmission rates of the\noptimal with zero pilot overhead and significantly improve multi-BS\ncoordination efficiency compared to existing methods.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15136v1", "AI": {"title_translation": "频外模态协同的多用户波束预测和主动基站选择，零导频开销", "tldr": "该论文提出一种基于频外模态协同（OMS）的移动管理方案，通过视觉和定位协同实现多用户波束预测和主动基站选择，无需导频开销，并显著提高多基站协调效率。", "motivation": "毫米波多用户通信中，跟踪多移动用户在多基站间的最佳波束会产生显著的信令开销。现有频外模态研究主要集中在单基站系统，将其应用于多基站选择会导致高协调开销（计算和延迟）。因此，如何利用频外模态消除导频开销并实现高效多基站协调是未探索的问题。", "method": "本文提出一种新颖的基于频外模态协同（OMS）的移动管理方案，通过协同视觉和定位两种频外模态，实现多用户波束预测和主动基站选择。具体而言，首先通过视觉感知和定位反馈的空间对齐识别移动用户，然后根据图像序列的时间相关性进行跟踪。随后，设计一个基于二进制编码图的增益和波束预测网络（BEM-GBPN），预测每个基站处移动用户的波束赋形增益和最佳波束，从而使中心单元控制基站执行用户切换和波束切换。", "result": "仿真结果表明，所提出的OMS方案提高了波束预测和基站选择的准确性，使传输速率达到最优的91%，且无需导频开销，并显著提高了多基站协调效率。", "conclusion": "本文提出的OMS方案通过协同视觉和定位模态，有效解决了毫米波多用户多基站通信中的导频开销和协调效率问题，实现了高效的波束预测和主动基站选择。", "translation": "多用户毫米波通信依赖窄波束和密集小区部署来确保可靠连接。然而，跟踪多个移动用户在多个基站（BS）之间的最佳波束会导致显著的信令开销。最近的研究探索了频外（OOB）模态在获取无线信道空间特性和降低单基站单用户/多用户系统中导频开销的能力。然而，将OOB模态应用于密集小区部署的多基站选择会导致高协调开销，即过高的计算开销和数据交换的高延迟。如何利用OOB模态消除导频开销并实现多基站系统中高效的多基站协调，仍 largely 未被探索。在本文中，我们提出了一种新颖的基于OOB模态协同（OMS）的移动管理方案，通过协同视觉和定位两种OOB模态，实现多用户波束预测和主动基站选择。具体来说，移动用户最初通过视觉感知和定位反馈的空间对齐进行识别，然后根据图像序列的时间相关性进行跟踪。随后，设计了一个基于二进制编码图的增益和波束预测网络（BEM-GBPN），用于预测每个基站处移动用户的波束赋形增益和最佳波束，从而使中心单元能够控制基站执行用户切换和波束切换。仿真结果表明，所提出的基于OMS的移动管理方案提高了波束预测和基站选择的准确性，并使用户在零导频开销的情况下达到最佳传输速率的91%，与现有方法相比，显著提高了多基站协调效率。", "summary": "本文提出了一种基于频外模态协同（OMS）的移动管理方案，旨在解决毫米波多用户多基站通信中因波束跟踪和基站选择带来的高信令开销和协调开销问题。该方案通过协同视觉和定位两种频外模态，首先识别并跟踪移动用户，然后利用设计的BEM-GBPN网络预测波束赋形增益和最佳波束，实现多用户波束预测和主动基站选择。仿真结果表明，该方案在零导频开销下显著提升了波束预测和基站选择精度，并大幅提高了多基站协调效率，使传输速率达到最优的91%。", "keywords": "频外模态协同, 波束预测, 基站选择, 毫米波通信, 零导频开销", "comments": "该论文的创新点在于首次将频外模态协同应用于多基站系统中的波束预测和主动基站选择，有效解决了传统方法中存在的导频开销和多基站协调效率低的问题。通过结合视觉和定位信息，提供了一种无需额外无线信令开销的解决方案，对于未来毫米波大规模MIMO和6G通信具有重要意义。其提出的BEM-GBPN网络设计也体现了深度学习在无线通信中的应用潜力。"}}
{"id": "2506.14784", "title": "Predicting Onflow Parameters Using Transfer Learning for Domain and Task Adaptation", "authors": ["Emre Yilmaz", "Philipp Bekemeyer"], "summary": "Determining onflow parameters is crucial from the perspectives of wind tunnel\ntesting and regular flight and wind turbine operations. These parameters have\ntraditionally been predicted via direct measurements which might lead to\nchallenges in case of sensor faults. Alternatively, a data-driven prediction\nmodel based on surface pressure data can be used to determine these parameters.\nIt is essential that such predictors achieve close to real-time learning as\ndictated by practical applications such as monitoring wind tunnel operations or\nlearning the variations in aerodynamic performance of aerospace and wind energy\nsystems. To overcome the challenges caused by changes in the data distribution\nas well as in adapting to a new prediction task, we propose a transfer learning\nmethodology to predict the onflow parameters, specifically angle of attack and\nonflow speed. It requires first training a convolutional neural network\n(ConvNet) model offline for the core prediction task, then freezing the weights\nof this model except the selected layers preceding the output node, and finally\nexecuting transfer learning by retraining these layers. A demonstration of this\napproach is provided using steady CFD analysis data for an airfoil for i)\ndomain adaptation where transfer learning is performed with data from a target\ndomain having different data distribution than the source domain and ii) task\nadaptation where the prediction task is changed. Further exploration on the\ninfluence of noisy data, performance on an extended domain, and trade studies\nvarying sampling sizes and architectures are provided. Results successfully\ndemonstrate the potential of the approach for adaptation to changing data\ndistribution, domain extension, and task update while the application for noisy\ndata is concluded to be not as effective.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14784v1", "AI": {"title_translation": "使用迁移学习进行域和任务适应的迎流参数预测", "tldr": "本文提出了一种基于迁移学习的方法，利用卷积神经网络预测迎流参数（攻角和迎流速度），以适应数据分布变化、领域扩展和任务更新，克服了传统方法和数据驱动模型面临的挑战。", "motivation": "确定迎流参数对于风洞测试、常规飞行和风力涡轮机运行至关重要。传统上通过直接测量进行预测，但可能面临传感器故障挑战。替代地，基于表面压力数据的预测模型可用于确定这些参数，但需要实现接近实时学习以适应实际应用中数据分布的变化和新预测任务的适应性。", "method": "本文提出了一种迁移学习方法来预测迎流参数（攻角和迎流速度）。该方法首先离线训练一个卷积神经网络（ConvNet）模型用于核心预测任务，然后冻结该模型除输出节点前选定层之外的所有权重，最后通过重新训练这些层执行迁移学习。通过对翼型稳态CFD分析数据进行演示，包括域适应（目标域数据分布与源域不同）和任务适应（预测任务改变）。", "result": "研究结果成功证明了该方法在适应数据分布变化、域扩展和任务更新方面的潜力。然而，对于噪声数据的应用被认为效果不佳。", "conclusion": "本文提出的迁移学习方法能够有效预测迎流参数，并成功适应数据分布变化、领域扩展和任务更新。尽管在处理噪声数据方面仍有局限，但该方法为克服传统预测方法的挑战提供了一条有前景的路径。", "translation": "确定迎流参数对于风洞测试、常规飞行和风力涡轮机运行至关重要。这些参数传统上通过直接测量进行预测，这可能在传感器故障情况下导致挑战。或者，可以使用基于表面压力数据的数据驱动预测模型来确定这些参数。至关重要的是，此类预测器应实现接近实时学习，正如监测风洞操作或学习航空航天和风能系统气动性能变化等实际应用所要求的那样。为了克服数据分布变化以及适应新预测任务所带来的挑战，我们提出了一种迁移学习方法来预测迎流参数，特别是攻角和迎流速度。它首先需要离线训练一个卷积神经网络（ConvNet）模型用于核心预测任务，然后冻结该模型除输出节点前选定层之外的权重，最后通过重新训练这些层执行迁移学习。通过对翼型稳态CFD分析数据进行了该方法的演示，包括 i) 域适应，其中迁移学习是在目标域数据分布与源域不同的情况下进行的，以及 ii) 任务适应，其中预测任务发生了变化。进一步探讨了噪声数据的影响、在扩展域上的性能以及不同采样大小和架构的权衡研究。结果成功证明了该方法在适应不断变化的数据分布、域扩展和任务更新方面的潜力，而对于噪声数据的应用被认为效果不佳。", "summary": "本文提出了一种新颖的迁移学习方法，用于利用卷积神经网络预测迎流参数，如攻角和迎流速度。该方法旨在解决传统直接测量和现有数据驱动模型在传感器故障、数据分布变化及任务适应性方面的挑战。通过离线训练ConvNet模型，然后冻结大部分权重并仅重新训练特定层，实现了域适应和任务适应。实验结果表明，该方法在处理数据分布变化、领域扩展和任务更新方面表现出色，但在噪声数据处理方面效果不佳。", "keywords": "迁移学习, 迎流参数, 卷积神经网络, 域适应, 任务适应", "comments": "该论文提出了一种创新的迁移学习方法来解决迎流参数预测中的域和任务适应问题，这对于实时监测和系统适应性至关重要。其核心在于利用ConvNet进行特征提取并通过微调适应新场景，避免了从头开始训练的计算成本。然而，对噪声数据处理效果不佳是其局限性，未来研究可聚焦于提升其在复杂、非理想数据环境下的鲁棒性。"}}
{"id": "2506.14914", "title": "Recursive Variational Autoencoders for 3D Blood Vessel Generative Modeling", "authors": ["Paula Feldman", "Miguel Fainstein", "Viviana Siless", "Claudio Delrieux", "Emmanuel Iarussi"], "summary": "Anatomical trees play an important role in clinical diagnosis and treatment\nplanning. Yet, accurately representing these structures poses significant\nchallenges owing to their intricate and varied topology and geometry. Most\nexisting methods to synthesize vasculature are rule based, and despite\nproviding some degree of control and variation in the structures produced, they\nfail to capture the diversity and complexity of actual anatomical data. We\ndeveloped a Recursive variational Neural Network (RvNN) that fully exploits the\nhierarchical organization of the vessel and learns a low-dimensional manifold\nencoding branch connectivity along with geometry features describing the target\nsurface. After training, the RvNN latent space can be sampled to generate new\nvessel geometries. By leveraging the power of generative neural networks, we\ngenerate 3D models of blood vessels that are both accurate and diverse, which\nis crucial for medical and surgical training, hemodynamic simulations, and many\nother purposes. These results closely resemble real data, achieving high\nsimilarity in vessel radii, length, and tortuosity across various datasets,\nincluding those with aneurysms. To the best of our knowledge, this work is the\nfirst to utilize this technique for synthesizing blood vessels.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14914v1", "AI": {"title_translation": "用于3D血管生成建模的递归变分自编码器", "tldr": "现有的基于规则的血管合成方法缺乏多样性。本文提出了一种递归变分神经网络（RvNN），通过学习血管的层次结构和低维流形，生成准确且多样化的3D血管模型。", "motivation": "解剖树在临床诊断和治疗计划中至关重要，但由于其复杂的拓扑结构和几何形状，准确表示它们具有挑战性。大多数现有血管合成的基于规则的方法无法捕捉实际解剖数据的多样性和复杂性。", "method": "开发了一种递归变分神经网络（RvNN），该网络充分利用血管的层次结构，并学习一个编码分支连接和几何特征的低维流形。训练后，可以对RvNN的潜在空间进行采样以生成新的血管几何形状。", "result": "生成了既准确又多样化的3D血管模型。这些结果与真实数据高度相似，在各种数据集（包括动脉瘤数据集）中，血管半径、长度和弯曲度均达到了高相似性。", "conclusion": "开发的RvNN成功生成了准确且多样化的3D血管模型，这对于医疗和外科培训、血流动力学模拟以及许多其他目的至关重要，并且据作者所知，这是首次将该技术用于合成血管。", "translation": "解剖树在临床诊断和治疗计划中发挥着重要作用。然而，由于其复杂多变的拓扑结构和几何形状，准确表示这些结构带来了巨大的挑战。大多数现有的血管合成方法都是基于规则的，尽管它们在生成结构方面提供了一定程度的控制和变异，但未能捕捉到实际解剖数据的多样性和复杂性。我们开发了一种递归变分神经网络（RvNN），它充分利用了血管的层次结构，学习了一个低维流形，编码了分支连接以及描述目标表面的几何特征。训练后，可以对RvNN的潜在空间进行采样以生成新的血管几何形状。通过利用生成式神经网络的力量，我们生成了既准确又多样化的3D血管模型，这对于医疗和外科培训、血流动力学模拟以及许多其他目的至关重要。这些结果与真实数据非常相似，在各种数据集（包括动脉瘤数据集）中，血管半径、长度和弯曲度都达到了高度相似。据我们所知，这项工作是首次利用该技术合成血管。", "summary": "本文提出了一种递归变分神经网络（RvNN），用于生成准确且多样化的3D血管模型。针对现有基于规则方法在捕捉数据复杂性方面的局限性，RvNN通过学习血管的层次结构及其低维流形来合成新的几何形状。生成的模型与真实解剖数据高度相似，在关键特征上表现出高一致性，对医疗应用具有重要价值。", "keywords": "3D血管, 生成建模, 递归变分自编码器, 解剖树, 神经网络", "comments": "该论文的创新之处在于首次将递归变分神经网络（RvNN）应用于利用血管的层次结构进行生成建模。这种方法解决了现有基于规则方法在生成多样化和准确解剖结构方面的关键限制，对于各种医疗应用至关重要。其捕捉复杂性和多样性的能力标志着在现实解剖数据生成方面迈出了重要一步。"}}
{"id": "2506.15571", "title": "MicroRicci: A Greedy and Local Ricci Flow Solver for Self-Tuning Mesh Smoothing", "authors": ["Le Vu Anh", "Nguyen Viet Anh", "Mehmet Dik", "Tu Nguyen Thi Ngoc"], "summary": "Real-time mesh smoothing at scale remains a formidable challenge: classical\nRicci-flow solvers demand costly global updates, while greedy heuristics suffer\nfrom slow convergence or brittle tuning. We present MicroRicci, the first truly\nself-tuning, local Ricci-flow solver that borrows ideas from coding theory and\npacks them into just 1K + 200 parameters. Its primary core is a greedy\nsyndrome-decoding step that pinpoints and corrects the largest curvature error\nin O(E) time, augmented by two tiny neural modules that adaptively choose\nvertices and step sizes on the fly. On a diverse set of 110 SJTU-TMQA meshes,\nMicroRicci slashes iteration counts from 950+=140 to 400+=80 (2.4x speedup),\ntightens curvature spread from 0.19 to 0.185, and achieves a remarkable\nUV-distortion-to-MOS correlation of r = -0.93. It adds only 0.25 ms per\niteration (0.80 to 1.05 ms), yielding an end-to-end 1.8x runtime acceleration\nover state-of-the-art methods. MicroRicci's combination of linear-time updates,\nautomatic hyperparameter adaptation, and high-quality geometric and perceptual\nresults makes it well suited for real-time, resource-limited applications in\ngraphics, simulation, and related fields.", "comment": "9 pages, 8 figures, 4 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15571v1", "AI": {"title_translation": "MicroRicci: 一种用于自适应网格平滑的贪婪局部Ricci流求解器", "tldr": "MicroRicci是一种自适应、局部Ricci流求解器，通过结合编码理论和小型神经网络模块，实现了实时网格平滑的显著加速和质量提升。", "motivation": "实时大规模网格平滑是一个巨大挑战，因为经典的Ricci流求解器需要昂贵的全局更新，而贪婪启发式方法收敛缓慢或难以调整。", "method": "MicroRicci是第一个真正的自适应、局部Ricci流求解器，它借鉴了编码理论的思想，并包含1K + 200个参数。其核心是一个贪婪的综合症解码步骤，能够在O(E)时间内精确识别并纠正最大的曲率误差，并辅以两个小型神经网络模块，用于动态自适应地选择顶点和步长。", "result": "在110个SJTU-TMQA网格数据集上，MicroRicci将迭代次数从950+140次减少到400+80次（提速2.4倍），将曲率扩散从0.19收紧到0.185，并实现了UV畸变与MOS之间r=-0.93的显著相关性。每次迭代仅增加0.25毫秒（从0.80毫秒到1.05毫秒），相对于最先进的方法，端到端运行时加速了1.8倍。", "conclusion": "MicroRicci结合了线性时间更新、自动超参数适应以及高质量的几何和感知结果，使其非常适合图形、仿真和相关领域中的实时、资源受限应用。", "translation": "大规模实时网格平滑仍然是一个艰巨的挑战：经典的Ricci流求解器需要昂贵的全局更新，而贪婪启发式方法则存在收敛缓慢或调整脆弱的问题。我们提出了MicroRicci，这是第一个真正自适应的局部Ricci流求解器，它借鉴了编码理论的思想，并将其打包到仅1K + 200个参数中。其主要核心是一个贪婪的综合症解码步骤，能够在O(E)时间内精确识别并纠正最大的曲率误差，并辅以两个小型神经网络模块，用于动态自适应地选择顶点和步长。在110个SJTU-TMQA网格的多种数据集上，MicroRicci将迭代次数从950±140次削减到400±80次（2.4倍加速），将曲率扩散从0.19收紧到0.185，并实现了显著的UV畸变与MOS相关性r = -0.93。每次迭代仅增加0.25毫秒（从0.80毫秒到1.05毫秒），相对于最先进的方法，端到端运行时加速了1.8倍。MicroRicci结合了线性时间更新、自动超参数适应以及高质量的几何和感知结果，使其非常适合图形、仿真和相关领域中的实时、资源受限应用。", "summary": "MicroRicci是一种新型的自适应、局部Ricci流求解器，专为解决大规模实时网格平滑的挑战而设计。它通过结合编码理论的贪婪综合症解码步骤（O(E)时间纠正曲率误差）和两个小型神经网络模块（自适应选择顶点和步长）来实现。实验结果表明，MicroRicci在迭代次数、曲率质量和UV畸变相关性方面均优于现有方法，并提供了显著的运行时加速，使其适用于实时、资源受限的图形和仿真应用。", "keywords": "网格平滑, Ricci流, 自适应, 实时, 编码理论", "comments": "MicroRicci的创新之处在于将编码理论与神经网络相结合，实现了Ricci流求解器的自适应局部更新，克服了传统方法全局更新成本高和贪婪启发式方法难以调优的缺点。其线性时间更新和自动超参数适应能力对于实时应用至关重要，展示了在计算几何领域结合多学科思想的潜力。"}}
{"id": "2506.15379", "title": "Tractable Graph Structures in EFX Orientation", "authors": ["Václav Blažej", "Sushmita Gupta", "M. S. Ramanujan", "Peter Strulo"], "summary": "Since its introduction, envy-freeness up to any good (EFX) has become a\nfundamental solution concept in fair division of indivisible goods. Its\nexistence remains elusive -- even for four agents with additive utility\nfunctions, it is unknown whether an EFX allocation always exists.\nUnsurprisingly, restricted settings to delineate tractable and intractable\ncases have been explored. Christadolou, Fiat et al.[EC'23] introduced the\nnotion of EFX-orientation, where the agents form the vertices of a graph and\nthe items correspond to edges, and an agent values only the items that are\nincident to it. The goal is to allocate items to one of the adjacent agents\nwhile satisfying the EFX condition.\n  Building on the work of Zeng and Mehta'24, which established a sharp\ncomplexity threshold based on the structure of the underlying graph --\npolynomial-time solvability for bipartite graphs and NP-hardness for graphs\nwith chromatic number at least three -- we further explore the algorithmic\nlandscape of EFX-orientation using parameterized graph algorithms.\n  Specifically, we show that bipartiteness is a surprisingly stringent\ncondition for tractability: EFX orientation is NP-complete even when the\nvaluations are symmetric, binary and the graph is at most two edge-removals\naway from being bipartite. Moreover, introducing a single non-binary value\nmakes the problem NP-hard even when the graph is only one edge removal away\nfrom being bipartite. We further perform a parameterized analysis to examine\nstructures of the underlying graph that enable tractability. In particular, we\nshow that the problem is solvable in linear time on graphs whose treewidth is\nbounded by a constant and that the complexity of an instance is closely tied to\nthe sizes of acyclic connected components on its one-valued edges.", "comment": null, "cate": "cs.GT", "url": "http://arxiv.org/abs/2506.15379v1", "AI": {"title_translation": "EFX定向中的易处理图结构", "tldr": "本文深入探讨了EFX定向问题的算法复杂性，发现即使图结构略微偏离二分图，问题仍是NP完全的，但对于树宽有界图，问题可在线性时间内解决。", "motivation": "EFX（Envy-Freeness up to any good）作为不可分物品公平分配中的一个基本解决方案概念，其存在性仍不确定，即使对于具有加性效用函数的四个代理，也未知是否存在EFX分配。因此，探索受限设置以划分可处理和不可处理的情况成为必要。Christadolou、Fiat等人引入了EFX定向的概念，而Zeng和Mehta'24的工作已经根据底层图的结构建立了尖锐的复杂性阈值。本文在此基础上，旨在进一步探索EFX定向的算法前景。", "method": "本文在Zeng和Mehta'24的工作基础上，使用参数化图算法进一步探索EFX定向的算法前景。具体而言，通过分析图结构（例如，与二分图的距离、树宽）和估值类型（对称、二进制、非二进制）对问题复杂性的影响，进行参数化分析。", "result": "研究表明，二分性是可处理性的一个极其严格的条件：即使估值是对称、二进制的，并且图距离二分图最多只有两次边移除，EFX定向问题仍然是NP完全的。此外，引入单个非二进制值会使问题变得NP难，即使图距离二分图仅一次边移除。研究还发现，该问题在树宽受常数限制的图上可在线性时间内解决，并且实例的复杂性与单值边上无环连通分量的大小密切相关。", "conclusion": "EFX定向问题在图结构上对可处理性条件非常敏感，即使是微小的结构变化或估值类型的引入，都可能导致问题从可处理变为NP完全或NP难。然而，对于具有特定结构属性（如树宽有界）的图，问题仍可有效解决。", "translation": "自引入以来，EFX（Envy-Freeness up to any good）已成为不可分物品公平分配中的一个基本解决方案概念。它的存在仍然难以捉摸——即使对于具有加性效用函数的四个代理，也未知是否存在EFX分配。不出所料，人们已经探索了限制性设置以划定可处理和不可处理的情况。Christadolou、Fiat等人[EC'23]引入了EFX定向的概念，其中代理构成图的顶点，物品对应于边，代理只重视与其相邻的物品。目标是将物品分配给其中一个相邻代理，同时满足EFX条件。\n在Zeng和Mehta'24的工作基础上，该工作根据底层图的结构建立了尖锐的复杂性阈值——二分图的多项式时间可解性以及色数至少为三的图的NP难性——我们使用参数化图算法进一步探索EFX定向的算法前景。\n具体而言，我们表明二分性是可处理性的一个令人惊讶的严格条件：即使估值是对称的、二进制的，并且图距离二分图最多只有两次边移除，EFX定向仍然是NP完全的。此外，引入单个非二进制值会使问题变得NP难，即使图距离二分图仅一次边移除。我们进一步进行参数化分析，以检查底层图的结构如何实现可处理性。特别是，我们表明该问题在树宽受常数限制的图上可在线性时间内解决，并且实例的复杂性与单值边上无环连通分量的大小密切相关。", "summary": "本研究深入探讨了EFX定向问题的算法复杂性，该问题是公平分配不可分物品中的一个核心概念。研究发现，即使在估值受限且图结构仅轻微偏离二分图的情况下，EFX定向问题仍然是NP完全或NP难的，这表明二分性是可处理性的一个非常严格的条件。然而，对于树宽有界的图，该问题可以在线性时间内解决，且问题的复杂性与单值边上无环连通分量的大小密切相关。这为理解EFX定向在不同图结构下的可处理性提供了新的见解。", "keywords": "EFX, 公平分配, 图算法, 复杂性, NP完全性, 树宽", "comments": "本文通过参数化图算法，对EFX定向这一重要公平分配问题进行了深入的复杂性分析。其创新之处在于揭示了二分性对于问题可处理性的严格性，即使是微小的结构扰动或估值变化也能导致NP硬度。同时，通过引入树宽等图参数，为在特定图结构下实现高效算法提供了理论基础，这对于理解和解决实际公平分配问题具有重要意义。"}}
{"id": "2506.15064", "title": "HiPreNets: High-Precision Neural Networks through Progressive Training", "authors": ["Ethan Mulle", "Wei Kang", "Qi Gong"], "summary": "Deep neural networks are powerful tools for solving nonlinear problems in\nscience and engineering, but training highly accurate models becomes\nchallenging as problem complexity increases. Non-convex optimization and\nnumerous hyperparameters to tune make performance improvement difficult, and\ntraditional approaches often prioritize minimizing mean squared error (MSE)\nwhile overlooking $L^{\\infty}$ error, which is the critical focus in many\napplications. To address these challenges, we present a progressive framework\nfor training and tuning high-precision neural networks (HiPreNets). Our\napproach refines a previously explored staged training technique for neural\nnetworks that improves an existing fully connected neural network by\nsequentially learning its prediction residuals using additional networks,\nleading to improved overall accuracy. We discuss how to take advantage of the\nstructure of the residuals to guide the choice of loss function, number of\nparameters to use, and ways to introduce adaptive data sampling techniques. We\nvalidate our framework's effectiveness through several benchmark problems.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15064v1", "AI": {"title_translation": "HiPreNets: 通过渐进式训练实现高精度神经网络", "tldr": "HiPreNets提出了一种渐进式训练框架，通过顺序学习预测残差来提高神经网络的精度，并关注L∞误差，解决了复杂问题中训练高精度模型的挑战。", "motivation": "深度神经网络在解决科学和工程中的非线性问题时面临挑战，尤其是在问题复杂性增加时，训练高精度模型变得困难。非凸优化和大量超参数调整使得性能提升不易，且传统方法常优先考虑最小化均方误差（MSE）而忽略在许多应用中至关重要的L∞误差。", "method": "我们提出了一种用于训练和调整高精度神经网络（HiPreNets）的渐进式框架。该方法改进了先前探索的神经网络分阶段训练技术，通过使用额外的网络顺序学习现有全连接神经网络的预测残差，从而提高整体精度。我们还讨论了如何利用残差结构来指导损失函数、参数数量的选择以及引入自适应数据采样技术。", "result": "通过几个基准问题验证了我们框架的有效性。", "conclusion": "HiPreNets框架通过渐进式训练和残差学习，能够有效提高神经网络的精度，尤其是在关注L∞误差的应用中。", "translation": "深度神经网络是解决科学和工程中非线性问题的强大工具，但随着问题复杂性的增加，训练高精度模型变得具有挑战性。非凸优化和大量需要调整的超参数使得性能提升困难，并且传统方法通常优先考虑最小化均方误差（MSE），而忽略了在许多应用中至关重要的L∞误差。为了应对这些挑战，我们提出了一种用于训练和调整高精度神经网络（HiPreNets）的渐进式框架。我们的方法改进了先前探索的神经网络分阶段训练技术，通过使用额外的网络顺序学习现有全连接神经网络的预测残差，从而提高整体精度。我们讨论了如何利用残差的结构来指导损失函数、所用参数数量的选择以及引入自适应数据采样技术。我们通过几个基准问题验证了我们框架的有效性。", "summary": "本文提出HiPreNets，一种用于训练高精度神经网络的渐进式框架。针对复杂非线性问题中训练高精度模型以及传统方法忽视L∞误差的挑战，HiPreNets通过改进分阶段训练技术，顺序学习预测残差以提高整体精度。该框架还探讨了如何利用残差结构指导损失函数选择、参数量设定和自适应数据采样，并通过基准问题验证了其有效性。", "keywords": "高精度神经网络, 渐进式训练, 残差学习, L∞误差, 分阶段训练", "comments": "HiPreNets的创新点在于其渐进式训练框架，通过顺序学习预测残差来提高神经网络的精度，并明确关注L∞误差，这在许多高精度应用中至关重要。这种分阶段训练的方法为解决复杂问题中高精度模型训练的挑战提供了一个有前景的途径。"}}
{"id": "2506.14949", "title": "From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?", "authors": ["Shadman Sakib", "Oishy Fatema Akhand", "Ajwad Abrar"], "summary": "While Machine Learning (ML) and Deep Learning (DL) models have been widely\nused for diabetes prediction, the use of Large Language Models (LLMs) for\nstructured numerical data is still not well explored. In this study, we test\nthe effectiveness of LLMs in predicting diabetes using zero-shot, one-shot, and\nthree-shot prompting methods. We conduct an empirical analysis using the Pima\nIndian Diabetes Database (PIDD). We evaluate six LLMs, including four\nopen-source models: Gemma-2-27B, Mistral-7B, Llama-3.1-8B, and Llama-3.2-2B. We\nalso test two proprietary models: GPT-4o and Gemini Flash 2.0. In addition, we\ncompare their performance with three traditional machine learning models:\nRandom Forest, Logistic Regression, and Support Vector Machine (SVM). We use\naccuracy, precision, recall, and F1-score as evaluation metrics. Our results\nshow that proprietary LLMs perform better than open-source ones, with GPT-4o\nand Gemma-2-27B achieving the highest accuracy in few-shot settings. Notably,\nGemma-2-27B also outperforms the traditional ML models in terms of F1-score.\nHowever, there are still issues such as performance variation across prompting\nstrategies and the need for domain-specific fine-tuning. This study shows that\nLLMs can be useful for medical prediction tasks and encourages future work on\nprompt engineering and hybrid approaches to improve healthcare predictions.", "comment": "Accepted in 1st IEEE QPAIN 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.14949v1", "AI": {"title_translation": "从聊天到检查：大型语言模型能否辅助糖尿病预测？", "tldr": "研究发现大型语言模型（LLMs）可用于糖尿病预测，特别是专有模型和Gemma-2-27B表现优于传统机器学习模型，但仍需优化提示策略和领域微调。", "motivation": "尽管机器学习（ML）和深度学习（DL）模型已广泛用于糖尿病预测，但大型语言模型（LLMs）在结构化数值数据上的应用尚未得到充分探索。", "method": "本研究使用Pima Indian Diabetes Database (PIDD) 数据集，通过零样本、单样本和三样本提示方法测试了六种大型语言模型（包括Gemma-2-27B、Mistral-7B、Llama-3.1-8B、Llama-3.2-2B、GPT-4o和Gemini Flash 2.0）在糖尿病预测中的有效性。同时，将它们的性能与随机森林、逻辑回归和支持向量机三种传统机器学习模型进行比较，并使用准确率、精确率、召回率和F1分数作为评估指标。", "result": "结果显示，专有大型语言模型表现优于开源模型，其中GPT-4o和Gemma-2-27B在少样本设置中取得了最高的准确率。值得注意的是，Gemma-2-27B在F1分数方面也优于传统机器学习模型。然而，研究也发现存在提示策略之间性能差异以及需要进行领域特定微调等问题。", "conclusion": "本研究表明大型语言模型可用于医疗预测任务，并鼓励未来在提示工程和混合方法方面进行进一步研究，以改进医疗预测。", "translation": "尽管机器学习（ML）和深度学习（DL）模型已广泛用于糖尿病预测，但大型语言模型（LLMs）在结构化数值数据上的应用尚未得到充分探索。在本研究中，我们测试了LLMs使用零样本、单样本和三样本提示方法预测糖尿病的有效性。我们使用Pima Indian Diabetes Database (PIDD) 进行了实证分析。我们评估了六种LLMs，包括四种开源模型：Gemma-2-27B、Mistral-7B、Llama-3.1-8B和Llama-3.2-2B。我们还测试了两种专有模型：GPT-4o和Gemini Flash 2.0。此外，我们将其性能与三种传统机器学习模型（随机森林、逻辑回归和支持向量机（SVM））进行了比较。我们使用准确率、精确率、召回率和F1分数作为评估指标。我们的结果表明，专有LLMs的表现优于开源LLMs，其中GPT-4o和Gemma-2-27B在少样本设置中取得了最高的准确率。值得注意的是，Gemma-2-27B在F1分数方面也优于传统ML模型。然而，仍然存在诸如提示策略之间性能差异以及需要进行领域特定微调等问题。这项研究表明LLMs可用于医疗预测任务，并鼓励未来在提示工程和混合方法方面进行进一步工作，以改进医疗预测。", "summary": "本研究探讨了大型语言模型（LLMs）在糖尿病预测中的应用潜力，填补了LLMs在结构化数值数据应用方面的空白。通过对Pima Indian Diabetes Database (PIDD) 数据集进行零样本、单样本和三样本提示实验，评估了六种LLMs（包括开源和专有模型）并与传统机器学习模型进行了比较。结果显示，专有LLMs表现更优，其中GPT-4o和Gemma-2-27B在准确率上表现突出，Gemma-2-27B在F1分数上甚至超越了传统模型。研究指出，尽管LLMs在医疗预测方面具有前景，但仍需解决提示策略差异和领域特定微调等挑战。", "keywords": "大型语言模型, 糖尿病预测, 少样本学习, Pima Indian Diabetes Database, 医疗AI", "comments": "这项研究具有创新性，因为它首次系统地探索了大型语言模型在结构化数值数据（如医疗诊断）中的应用，而此前LLMs主要集中在文本处理。其重要性在于揭示了LLMs在糖尿病预测任务上的潜力，尤其是某些LLMs甚至能超越传统机器学习模型，为医疗AI领域开辟了新的研究方向。然而，研究也指出了局限性，如提示策略的敏感性和对领域特定微调的需求，这为未来的研究提供了明确的方向。"}}
{"id": "2506.14800", "title": "A micromorphic-based artificial diffusion method for stabilized finite element approximation of convection-diffusion problems", "authors": ["Soheil Firooz", "B. Daya Reddy", "Paul Steinmann"], "summary": "We present a novel artificial diffusion method to circumvent the\ninstabilities associated with the standard finite element approximation of\nconvection-diffusion equations. Motivated by the micromorphic approach, we\nintroduce an auxiliary variable, which is related to the gradient of the field\nof interest, and which leads to a coupled problem. Conditions for\nwell-posedness of the resulting formulation are established. We carry out a\ncomprehensive numerical study to compare the proposed methodology against some\nwell-established approaches in one- and two-dimensional settings. The proposed\nmethod outperforms established approaches in general in approximating\naccurately the solutions to pertinent and challenging problems.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14800v1", "AI": {"title_translation": "一种基于微观形态学的人工扩散方法，用于对流-扩散问题的稳定有限元逼近", "tldr": "该研究提出了一种基于微观形态学的新型人工扩散方法，用于稳定对流-扩散问题的有限元逼近，并证明其性能优于现有方法。", "motivation": "规避与对流-扩散方程标准有限元逼近相关的 H 不稳定性。", "method": "受微观形态学方法的启发，引入了一个与关注场梯度相关的辅助变量，从而形成一个耦合问题。建立了所得公式的适定性条件，并进行了全面的数值研究以比较其与现有方法的性能。", "result": "所提出的方法在准确逼近相关和挑战性问题的解决方案方面通常优于已建立的方法。", "conclusion": "该新型人工扩散方法能有效稳定对流-扩散问题的有限元逼近，并且在准确性方面表现出优于现有方法的性能。", "translation": "我们提出了一种新颖的人工扩散方法，以规避与对流-扩散方程标准有限元逼近相关的 H 不稳定性。受微观形态学方法的启发，我们引入了一个辅助变量，该变量与关注场的梯度相关，并导致一个耦合问题。建立了所得公式的适定性条件。我们进行了一项全面的数值研究，以比较所提出的方法与一些在一维和二维设置中已建立的方法。所提出的方法在准确逼近相关和挑战性问题的解决方案方面通常优于已建立的方法。", "summary": "本文提出了一种新颖的人工扩散方法，该方法受微观形态学启发，旨在解决对流-扩散方程标准有限元逼近中的不稳定性问题。通过引入一个与关注场梯度相关的辅助变量，形成了一个耦合问题，并建立了其适定性条件。全面的数值研究表明，与现有方法相比，该方法在准确逼近相关和挑战性问题的解方面通常表现更优。", "keywords": "人工扩散, 有限元方法, 对流-扩散, 微观形态学, 稳定性", "comments": "该论文提出了一种创新性的方法，通过引入基于微观形态学的辅助变量来稳定对流-扩散方程的有限元逼近，这在数值方法领域具有重要意义。其在准确性方面超越现有方法的表现，预示着该方法在解决实际问题中的巨大潜力。"}}
{"id": "2506.15514", "title": "Exploiting Music Source Separation for Automatic Lyrics Transcription with Whisper", "authors": ["Jaza Syed", "Ivan Meresman Higgs", "Ondřej Cífka", "Mark Sandler"], "summary": "Automatic lyrics transcription (ALT) remains a challenging task in the field\nof music information retrieval, despite great advances in automatic speech\nrecognition (ASR) brought about by transformer-based architectures in recent\nyears. One of the major challenges in ALT is the high amplitude of interfering\naudio signals relative to conventional ASR due to musical accompaniment. Recent\nadvances in music source separation have enabled automatic extraction of\nhigh-quality separated vocals, which could potentially improve ALT performance.\nHowever, the effect of source separation has not been systematically\ninvestigated in order to establish best practices for its use. This work\nexamines the impact of source separation on ALT using Whisper, a\nstate-of-the-art open source ASR model. We evaluate Whisper's performance on\noriginal audio, separated vocals, and vocal stems across short-form and\nlong-form transcription tasks. For short-form, we suggest a concatenation\nmethod that results in a consistent reduction in Word Error Rate (WER). For\nlong-form, we propose an algorithm using source separation as a vocal activity\ndetector to derive segment boundaries, which results in a consistent reduction\nin WER relative to Whisper's native long-form algorithm. Our approach achieves\nstate-of-the-art results for an open source system on the Jam-ALT long-form ALT\nbenchmark, without any training or fine-tuning. We also publish MUSDB-ALT, the\nfirst dataset of long-form lyric transcripts following the Jam-ALT guidelines\nfor which vocal stems are publicly available.", "comment": "Accepted at 2025 ICME Workshop AI for Music", "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15514v1", "AI": {"title_translation": "利用音乐源分离技术结合 Whisper 实现自动歌词转录", "tldr": "本文探讨了利用音乐源分离技术改进基于 Whisper 的自动歌词转录（ALT），结果显示在短形式和长形式任务中均能持续降低词错误率（WER），并在 Jam-ALT 基准测试中为开源系统取得了最先进的结果。", "motivation": "自动歌词转录（ALT）因音乐伴奏的强烈干扰而面临巨大挑战。尽管自动语音识别（ASR）取得了进展，但源分离对 ALT 性能的影响尚未被系统性研究，以建立最佳实践。", "method": "本研究系统地调查了音乐源分离对使用最先进的开源 ASR 模型 Whisper 进行自动歌词转录（ALT）的影响。研究评估了 Whisper 在原始音频、分离人声和人声干音上的性能，涵盖短形式和长形式转录任务。对于短形式，提出了一种串联方法；对于长形式，提出了一种利用源分离作为人声活动检测器来确定分段边界的算法。此外，还发布了 MUSDB-ALT 数据集。", "result": "对于短形式转录任务，所提出的串联方法能持续降低词错误率（WER）。对于长形式转录任务，利用源分离作为人声活动检测器的算法相对于 Whisper 的原生算法能持续降低 WER。该方法在 Jam-ALT 长形式 ALT 基准测试中，为开源系统取得了最先进的结果，且无需任何训练或微调。同时，发布了首个遵循 Jam-ALT 指南且人声干音公开可用的长形式歌词转录数据集 MUSDB-ALT。", "conclusion": "系统地将音乐源分离与 Whisper 结合使用可显著提升自动歌词转录（ALT）性能，为开源系统在基准测试中无需额外训练即可达到最先进水平，并为源分离在 ALT 中的应用提供了最佳实践。", "translation": "自动歌词转录（ALT）在音乐信息检索领域仍然是一个具有挑战性的任务，尽管近年来基于 Transformer 的架构在自动语音识别（ASR）方面取得了巨大进展。ALT 的主要挑战之一是，与传统的 ASR 相比，音乐伴奏引起的干扰音频信号的振幅很高。音乐源分离的最新进展使得高质量分离人声的自动提取成为可能，这可能改善 ALT 的性能。然而，源分离的效果尚未被系统地研究，以建立其使用的最佳实践。这项工作利用最先进的开源 ASR 模型 Whisper，研究了源分离对 ALT 的影响。我们在短形式和长形式转录任务中评估了 Whisper 在原始音频、分离人声和人声干音上的性能。对于短形式，我们提出了一种连接方法，该方法能持续降低词错误率（WER）。对于长形式，我们提出了一种算法，利用源分离作为人声活动检测器来确定分段边界，这相对于 Whisper 的原生长形式算法能持续降低 WER。我们的方法在 Jam-ALT 长形式 ALT 基准测试中，为开源系统取得了最先进的结果，且无需任何训练或微调。我们还发布了 MUSDB-ALT，这是第一个遵循 Jam-ALT 指南且人声干音公开可用的长形式歌词转录数据集。", "summary": "本文探讨了音乐源分离技术对自动歌词转录（ALT）的影响，并结合了先进的 ASR 模型 Whisper。针对 ALT 中音乐干扰大的挑战，研究系统评估了 Whisper 在分离人声上的表现，涵盖短形式和长形式任务。论文提出了一种用于短形式转录的串联方法，以及一种基于源分离的人声活动检测算法用于长形式转录，两者均能持续降低词错误率（WER）。该方法在 Jam-ALT 基准测试中，为开源系统取得了最先进的结果，且无需微调。此外，研究还发布了新的长形式数据集 MUSDB-ALT。", "keywords": "音乐源分离, 自动歌词转录, Whisper, 词错误率, 人声活动检测", "comments": "本文系统地研究了将音乐源分离与强大的 ASR 模型 Whisper 结合应用于自动歌词转录（ALT）的方法，这是一个实用且重要的进展。其主要创新在于提出了具体的方法（短形式的串联法，长形式的 VAD 法）以有效利用分离后的人声，从而建立了最佳实践。在无需微调的情况下，为开源系统在基准测试中取得最先进结果，这具有重要意义，表明了该方法的即时适用性和鲁棒性。新数据集（MUSDB-ALT）的发布进一步促进了研究社区的未来工作。"}}
{"id": "2506.15075", "title": "CWGAN-GP Augmented CAE for Jamming Detection in 5G-NR in Non-IID Datasets", "authors": ["Samhita Kuili", "Mohammadreza Amini", "Burak Kantarci"], "summary": "In the ever-expanding domain of 5G-NR wireless cellular networks,\nover-the-air jamming attacks are prevalent as security attacks, compromising\nthe quality of the received signal. We simulate a jamming environment by\nincorporating additive white Gaussian noise (AWGN) into the real-world In-phase\nand Quadrature (I/Q) OFDM datasets. A Convolutional Autoencoder (CAE) is\nexploited to implement a jamming detection over various characteristics such as\nheterogenous I/Q datasets; extracting relevant information on Synchronization\nSignal Blocks (SSBs), and fewer SSB observations with notable class imbalance.\nGiven the characteristics of datasets, balanced datasets are acquired by\nemploying a Conv1D conditional Wasserstein Generative Adversarial\nNetwork-Gradient Penalty(CWGAN-GP) on both majority and minority SSB\nobservations. Additionally, we compare the performance and detection ability of\nthe proposed CAE model on augmented datasets with benchmark models:\nConvolutional Denoising Autoencoder (CDAE) and Convolutional Sparse Autoencoder\n(CSAE). Despite the complexity of data heterogeneity involved across all\ndatasets, CAE depicts the robustness in detection performance of jammed signal\nby achieving average values of 97.33% precision, 91.33% recall, 94.08%\nF1-score, and 94.35% accuracy over CDAE and CSAE.", "comment": "6 pages, 5 figures, Accepted to IEEE International Symposium on\n  Personal, Indoor and Mobile Radio Communications (PIMRC) 2025", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15075v1", "AI": {"title_translation": "CWGAN-GP增强型CAE用于5G-NR非独立同分布数据集中的干扰检测", "tldr": "该研究提出使用CWGAN-GP增强卷积自编码器（CAE）来检测5G-NR非独立同分布数据集中的干扰，并展现了其鲁棒性。", "motivation": "在5G-NR无线蜂窝网络中，空中干扰攻击普遍存在，会损害接收信号的质量，因此需要有效的检测方法。", "method": "研究通过向实际的同相/正交（I/Q）OFDM数据集中添加高斯白噪声（AWGN）来模拟干扰环境。利用卷积自编码器（CAE）对各种特性（如异构I/Q数据集、同步信号块（SSB）的相关信息提取以及少数SSB观测值存在的显著类别不平衡）进行干扰检测。通过在多数和少数SSB观测值上应用Conv1D条件Wasserstein生成对抗网络-梯度惩罚（CWGAN-GP）来获取平衡数据集。此外，将所提出的CAE模型在增强数据集上的性能和检测能力与基准模型（卷积去噪自编码器（CDAE）和卷积稀疏自编码器（CSAE））进行了比较。", "result": "尽管所有数据集都涉及复杂的数据异构性，CAE在干扰信号检测性能上表现出鲁棒性，平均精度达到97.33%，召回率91.33%，F1分数94.08%，准确率94.35%，优于CDAE和CSAE。", "conclusion": "该CAE模型在干扰信号检测性能上表现出鲁棒性，即使在复杂和异构的数据集中也能有效工作，并优于基准模型。", "translation": "在不断扩展的5G-NR无线蜂窝网络领域中，空中干扰攻击作为安全威胁普遍存在，损害了接收信号的质量。我们通过将附加高斯白噪声（AWGN）引入真实的同相/正交（I/Q）OFDM数据集来模拟干扰环境。利用卷积自编码器（CAE）对各种特性（如异构I/Q数据集；同步信号块（SSB）的相关信息提取；以及少数SSB观测值存在的显著类别不平衡）进行干扰检测。考虑到数据集的特性，通过在多数和少数SSB观测值上应用Conv1D条件Wasserstein生成对抗网络-梯度惩罚（CWGAN-GP）来获取平衡数据集。此外，我们将所提出的CAE模型在增强数据集上的性能和检测能力与基准模型：卷积去噪自编码器（CDAE）和卷积稀疏自编码器（CSAE）进行了比较。尽管所有数据集都涉及复杂的数据异构性，CAE在干扰信号检测性能上表现出鲁棒性，平均精度达到97.33%，召回率91.33%，F1分数94.08%，准确率94.35%，优于CDAE和CSAE。", "summary": "该论文旨在解决5G-NR网络中的干扰检测问题。研究通过在实际I/Q OFDM数据中加入AWGN来模拟干扰环境，并提出使用卷积自编码器（CAE）进行检测。为了处理异构和类别不平衡的同步信号块（SSB）数据集，论文引入了CWGAN-GP来平衡数据。实验结果表明，所提出的CAE模型在增强数据集上的检测性能（包括精度、召回率、F1分数和准确率）显著优于卷积去噪自编码器（CDAE）和卷积稀疏自编码器（CSAE），证明了其在复杂数据环境下的鲁棒性。", "keywords": "5G-NR, 干扰检测, 卷积自编码器, CWGAN-GP, 非独立同分布数据集", "comments": "该论文的创新点在于利用CWGAN-GP来处理非独立同分布（Non-IID）和不平衡数据集，从而增强了CAE模型在5G-NR干扰检测中的鲁棒性。这对于提高5G-NR网络的安全性具有重要的实际意义。"}}
{"id": "2506.15172", "title": "Advanced approach for Agile/Scrum Process: RetroAI++", "authors": ["Maria Spichkova", "Kevin Iwan", "Madeleine Zwart", "Hina Lee", "Yuwon Yoon", "Xiaohan Qin"], "summary": "In Agile/Scrum software development, sprint planning and retrospective\nanalysis are the key elements of project management. The aim of our work is to\nsupport software developers in these activities. In this paper, we present our\nprototype tool RetroAI++, based on emerging intelligent technologies. In our\nRetroAI++ prototype, we aim to automate and refine the practical application of\nAgile/Scrum processes within Sprint Planning and Retrospectives. Leveraging AI\ninsights, our prototype aims to automate and refine the many processes involved\nin the Sprint Planning, Development and Retrospective stages of Agile/Scrum\ndevelopment projects, offering intelligent suggestions for sprint organisation\nas well as meaningful insights for retrospective reflection.", "comment": "Preprint. Accepted to the 29th International Conference on\n  Knowledge-Based and Intelligent Information & Engineering Systems (KES 2025).\n  Final version to be published by Elsevier (In Press)", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15172v1", "AI": {"title_translation": "敏捷/Scrum过程的先进方法：RetroAI++", "tldr": "本文介绍了一个名为RetroAI++的原型工具，它利用人工智能技术自动化和优化敏捷/Scrum开发中的冲刺规划和回顾活动，为开发者提供智能建议和有意义的见解。", "motivation": "敏捷/Scrum软件开发中的冲刺规划和回顾分析是项目管理的关键要素，本文旨在支持软件开发者在这些活动中。", "method": "本文提出了一个名为RetroAI++的原型工具，该工具基于新兴智能技术，旨在利用AI洞察力自动化和优化敏捷/Scrum开发项目中冲刺规划、开发和回顾阶段的许多流程，为冲刺组织提供智能建议，并为回顾反思提供有意义的见解。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "在敏捷/Scrum软件开发中，冲刺规划和回顾分析是项目管理的关键要素。我们工作的目标是支持软件开发人员进行这些活动。在本文中，我们提出了基于新兴智能技术的原型工具RetroAI++。在我们的RetroAI++原型中，我们旨在自动化和完善冲刺规划和回顾中敏捷/Scrum流程的实际应用。利用AI洞察力，我们的原型旨在自动化和完善敏捷/Scrum开发项目冲刺规划、开发和回顾阶段所涉及的许多流程，为冲刺组织提供智能建议，并为回顾反思提供有意义的见解。", "summary": "本文介绍了一个名为RetroAI++的AI原型工具，旨在通过自动化和优化冲刺规划、开发和回顾阶段的流程，支持敏捷/Scrum软件开发人员。该工具利用人工智能提供智能建议和有意义的洞察，以改进敏捷/Scrum实践。", "keywords": "敏捷开发, Scrum, 人工智能, 冲刺规划, 回顾", "comments": "RetroAI++的创新之处在于将人工智能应用于敏捷/Scrum流程中的冲刺规划和回顾，旨在提高效率和提供更深入的见解。其重要性在于，它有望减轻开发人员在这些关键项目管理活动中的负担，并通过智能建议提升决策质量。然而，抽象中未提及实际应用效果或验证结果，因此其有效性仍需进一步的实证支持。"}}
{"id": "2506.15009", "title": "Six-DoF Hand-Based Teleoperation for Omnidirectional Aerial Robots", "authors": ["Jinjie Li", "Jiaxuan Li", "Kotaro Kaneko", "Liming Shu", "Moju Zhao"], "summary": "Omnidirectional aerial robots offer full 6-DoF independent control over\nposition and orientation, making them popular for aerial manipulation. Although\nadvancements in robotic autonomy, operating by human remains essential in\ncomplex aerial environments. Existing teleoperation approaches for multirotors\nfail to fully leverage the additional DoFs provided by omnidirectional\nrotation. Additionally, the dexterity of human fingers should be exploited for\nmore engaged interaction. In this work, we propose an aerial teleoperation\nsystem that brings the omnidirectionality of human hands into the unbounded\naerial workspace. Our system includes two motion-tracking marker sets -- one on\nthe shoulder and one on the hand -- along with a data glove to capture hand\ngestures. Using these inputs, we design four interaction modes for different\ntasks, including Spherical Mode and Cartesian Mode for long-range moving as\nwell as Operation Mode and Locking Mode for precise manipulation, where the\nhand gestures are utilized for seamless mode switching. We evaluate our system\non a valve-turning task in real world, demonstrating how each mode contributes\nto effective aerial manipulation. This interaction framework bridges human\ndexterity with aerial robotics, paving the way for enhanced teleoperated aerial\nmanipulation in unstructured environments.", "comment": "7 pages, 9 figures. This work has been accepted to IROS 2025. The\n  video will be released soon", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15009v1", "AI": {"title_translation": "六自由度手部全向空中机器人遥操作", "tldr": "本文提出了一种基于手部运动和手势的六自由度全向空中机器人遥操作系统，通过四种交互模式和手势切换，实现了对机器人位置和姿态的精确控制与空中操纵。", "motivation": "尽管机器人自主性有所进步，但在复杂的空中环境中，人类操作仍然至关重要。现有的多旋翼遥操作方法未能充分利用全向旋转提供的额外自由度，也未充分利用人类手指的灵活性进行更深入的交互。", "method": "该系统包含两个运动跟踪标记集（一个在肩部，一个在手部）以及一个数据手套，用于捕捉手势。基于这些输入，设计了四种交互模式：用于长距离移动的球形模式和笛卡尔模式，以及用于精确操纵的操作模式和锁定模式，其中手势用于无缝模式切换。", "result": "该系统在现实世界的阀门转动任务中进行了评估，结果表明每种模式都有助于有效的空中操纵。", "conclusion": "该交互框架将人类的灵活性与空中机器人技术相结合，为在非结构化环境中增强遥操作空中操纵铺平了道路。", "translation": "全向空中机器人对位置和姿态提供完整的六自由度独立控制，使其在空中操纵中广受欢迎。尽管机器人自主性取得了进步，但在复杂的空中环境中，人类操作仍然至关重要。现有的多旋翼遥操作方法未能充分利用全向旋转提供的额外自由度。此外，应利用人类手指的灵活性进行更深入的交互。在这项工作中，我们提出了一种空中遥操作系统，将人类手的全向性带入无限的空中工作空间。我们的系统包括两组运动跟踪标记——一个在肩部，一个在手部——以及一个数据手套来捕捉手势。利用这些输入，我们为不同的任务设计了四种交互模式，包括用于长距离移动的球形模式和笛卡尔模式，以及用于精确操纵的操作模式和锁定模式，其中手势用于无缝模式切换。我们在现实世界的阀门转动任务中评估了我们的系统，展示了每种模式如何有助于有效的空中操纵。这个交互框架将人类的灵活性与空中机器人技术相结合，为在非结构化环境中增强遥操作空中操纵铺平了道路。", "summary": "本文提出了一种新颖的六自由度手部遥操作系统，用于控制全向空中机器人。该系统利用肩部和手部的运动跟踪标记以及数据手套捕获手势，设计了四种针对不同任务的交互模式（包括长距离移动和精确操纵模式）。手势用于模式的无缝切换。通过在阀门转动任务中的实际评估，证明了该系统能有效利用人类的灵活性，实现对全向空中机器人的高效空中操纵。", "keywords": "遥操作, 全向空中机器人, 六自由度, 手势控制, 空中操纵", "comments": "该论文的创新点在于充分利用了人类手部的全向性和手指的灵活性，通过创新的多模式交互设计，弥补了现有遥操作系统在利用全向机器人额外自由度方面的不足。它为在复杂、非结构化环境中进行高精度空中操纵提供了新的解决方案，具有重要的实际应用前景。"}}
{"id": "2506.14829", "title": "The Hardness of Achieving Impact in AI for Social Impact Research: A Ground-Level View of Challenges & Opportunities", "authors": ["Aditya Majumdar", "Wenbo Zhang", "Kashvi Prawal", "Amulya Yadav"], "summary": "In an attempt to tackle the UN SDGs, AI for Social Impact (AI4SI) projects\nfocus on harnessing AI to address societal issues in areas such as healthcare,\nsocial justice, etc. Unfortunately, despite growing interest in AI4SI,\nachieving tangible, on-the-ground impact remains a significant challenge. For\nexample, identifying and engaging motivated collaborators who are willing to\nco-design and deploy AI based solutions in real-world settings is often\ndifficult. Even when such partnerships are established, many AI4SI projects\n\"fail\" to progress beyond the proof-of-concept stage, and hence, are unable to\ntransition to at-scale production-level solutions. Furthermore, the unique\nchallenges faced by AI4SI researchers are not always fully recognized within\nthe broader AI community, where such work is sometimes viewed as primarily\napplied and not aligning with the traditional criteria for novelty emphasized\nin core AI venues. This paper attempts to shine a light on the diverse\nchallenges faced in AI4SI research by diagnosing a multitude of factors that\nprevent AI4SI partnerships from achieving real-world impact on the ground.\nDrawing on semi-structured interviews with six leading AI4SI researchers -\ncomplemented by the authors' own lived experiences in conducting AI4SI research\n- this paper attempts to understand the day-to-day difficulties faced in\ndeveloping and deploying socially impactful AI solutions. Through thematic\nanalysis, we identify structural and organizational, communication,\ncollaboration, and operational challenges as key barriers to deployment. While\nthere are no easy fixes, we synthesize best practices and actionable strategies\ndrawn from these interviews and our own work in this space. In doing so, we\nhope this paper serves as a practical reference guide for AI4SI researchers and\npartner organizations seeking to engage more effectively in socially impactful\nAI collaborations.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14829v1", "AI": {"title_translation": "人工智能社会影响研究中实现影响的困难：挑战与机遇的基层视角", "tldr": "尽管人们对人工智能社会影响（AI4SI）兴趣日益增长，但实现实际影响仍然是一个重大挑战。本文通过对领先研究人员的采访和作者的经验，识别了部署AI4SI解决方案的关键障碍，并提出了最佳实践和可操作的策略。", "motivation": "尽管人们对人工智能社会影响（AI4SI）项目日益增长的兴趣，但在实际中实现可衡量的影响仍然是一个重大挑战。AI4SI项目经常难以超越概念验证阶段，并且AI4SI研究人员面临的独特挑战在更广泛的AI社区中未被充分认识。本文旨在揭示这些挑战，并诊断阻碍AI4SI合作实现实际影响的因素。", "method": "本研究通过对六位领先的AI4SI研究人员进行半结构化访谈，并结合作者自身进行AI4SI研究的经验。通过主题分析，识别了部署的关键障碍。", "result": "研究识别出结构和组织、沟通、协作以及操作方面的挑战是AI4SI解决方案部署的关键障碍。同时，论文也综合了从访谈和作者自身工作中得出的最佳实践和可操作策略。", "conclusion": "本文旨在为AI4SI研究人员和合作组织提供一份实用的参考指南，帮助他们更有效地参与到具有社会影响的人工智能合作中。", "translation": "为了应对联合国可持续发展目标，人工智能社会影响（AI4SI）项目致力于利用人工智能解决医疗、社会正义等领域的社会问题。不幸的是，尽管对AI4SI的兴趣日益增长，但实现切实的、实际的影响仍然是一个重大挑战。例如，识别并吸引积极的合作者，他们愿意在真实世界中共同设计和部署基于人工智能的解决方案，通常很困难。即使建立了这种伙伴关系，许多AI4SI项目也“未能”超越概念验证阶段，因此无法过渡到大规模的生产级解决方案。此外，AI4SI研究人员面临的独特挑战在更广泛的人工智能社区中并不总是得到充分认可，在该社区中，此类工作有时被视为主要是应用性的，不符合核心人工智能场所强调的新颖性传统标准。本文试图通过诊断阻止AI4SI伙伴关系在实地实现实际影响的多种因素，来揭示AI4SI研究中面临的各种挑战。本文借鉴了对六位领先AI4SI研究人员的半结构化访谈——并辅以作者自身进行AI4SI研究的亲身经验——试图理解在开发和部署具有社会影响力的人工智能解决方案时面临的日常困难。通过主题分析，我们识别出结构和组织、沟通、协作以及操作方面的挑战是部署的关键障碍。虽然没有简单的解决方案，但我们综合了从这些访谈和我们在此领域的工作中得出的最佳实践和可操作策略。通过这样做，我们希望本文能为寻求更有效地参与具有社会影响力的人工智能合作的AI4SI研究人员和合作组织提供一份实用的参考指南。", "summary": "本文探讨了人工智能社会影响（AI4SI）研究中实现实际影响的困难。通过对六位领先AI4SI研究人员的半结构化访谈以及作者的亲身经验，研究识别出结构和组织、沟通、协作以及操作等方面的挑战是阻碍AI4SI解决方案部署的关键障碍。论文还综合了最佳实践和可操作策略，旨在为AI4SI研究人员和合作伙伴提供实用指导，以促进更有效的社会影响力AI合作。", "keywords": "AI for Social Impact, 挑战, 影响, 部署, 合作", "comments": "这篇论文通过深入的访谈和作者的亲身经验，揭示了人工智能社会影响（AI4SI）领域所面临的实际挑战，这对于理解和克服AI4SI项目从概念验证到实际部署的障碍至关重要。其创新之处在于提供了“基层视角”，并综合了可操作的策略，对于促进AI4SI的实际影响具有重要指导意义。"}}
{"id": "2506.15176", "title": "In-Context Learning for Gradient-Free Receiver Adaptation: Principles, Applications, and Theory", "authors": ["Matteo Zecchin", "Tomer Raviv", "Dileep Kalathil", "Krishna Narayanan", "Nir Shlezinger", "Osvaldo Simeone"], "summary": "In recent years, deep learning has facilitated the creation of wireless\nreceivers capable of functioning effectively in conditions that challenge\ntraditional model-based designs. Leveraging programmable hardware\narchitectures, deep learning-based receivers offer the potential to dynamically\nadapt to varying channel environments. However, current adaptation strategies,\nincluding joint training, hypernetwork-based methods, and meta-learning, either\ndemonstrate limited flexibility or necessitate explicit optimization through\ngradient descent. This paper presents gradient-free adaptation techniques\nrooted in the emerging paradigm of in-context learning (ICL). We review\narchitectural frameworks for ICL based on Transformer models and structured\nstate-space models (SSMs), alongside theoretical insights into how sequence\nmodels effectively learn adaptation from contextual information. Further, we\nexplore the application of ICL to cell-free massive MIMO networks, providing\nboth theoretical analyses and empirical evidence. Our findings indicate that\nICL represents a principled and efficient approach to real-time receiver\nadaptation using pilot signals and auxiliary contextual information-without\nrequiring online retraining.", "comment": null, "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15176v1", "AI": {"title_translation": "上下文学习在无梯度接收机自适应中的应用：原理、应用与理论", "tldr": "本文提出使用上下文学习（ICL）实现无线接收机的无梯度实时自适应，展示了其高效性，且无需在线再训练，并探讨了基于Transformer和SSM的架构。", "motivation": "当前基于深度学习的无线接收机自适应策略（如联合训练、超网络方法和元学习）要么灵活性有限，要么需要通过梯度下降进行显式优化。", "method": "本文提出了基于上下文学习（ICL）的无梯度自适应技术。论文回顾了基于Transformer模型和结构化状态空间模型（SSM）的ICL架构框架，并提供了关于序列模型如何从上下文信息中有效学习自适应的理论见解。此外，还探讨了ICL在无蜂窝大规模MIMO网络中的应用。", "result": "研究结果表明，ICL代表了一种原则性且高效的方法，可以在不进行在线再训练的情况下，利用导频信号和辅助上下文信息进行实时接收机自适应。论文为ICL在无蜂窝大规模MIMO网络中的应用提供了理论分析和实证证据。", "conclusion": "上下文学习为实时无线接收机自适应提供了一种原则性且高效的解决方案，克服了传统基于梯度方法的局限性。", "translation": "近年来，深度学习促进了无线接收机的创建，使其能够在传统基于模型设计面临挑战的条件下有效运行。利用可编程硬件架构，基于深度学习的接收机提供了动态适应不同信道环境的潜力。然而，当前的自适应策略，包括联合训练、基于超网络的方法和元学习，要么灵活性有限，要么需要通过梯度下降进行显式优化。本文提出了植根于新兴的上下文学习（ICL）范式的无梯度自适应技术。我们回顾了基于Transformer模型和结构化状态空间模型（SSM）的ICL架构框架，以及关于序列模型如何有效地从上下文信息中学习自适应的理论见解。此外，我们探讨了ICL在无蜂窝大规模MIMO网络中的应用，提供了理论分析和实证证据。我们的研究结果表明，ICL代表了一种原则性且高效的方法，可以在不进行在线再训练的情况下，利用导频信号和辅助上下文信息进行实时接收机自适应。", "summary": "本文提出了一种基于上下文学习（ICL）的无线接收机无梯度自适应技术，旨在克服现有深度学习方法在灵活性和对梯度优化依赖方面的局限性。论文回顾了基于Transformer和SSM的ICL架构，并提供了序列模型如何利用上下文信息进行有效自适应的理论见解。通过在无蜂窝大规模MIMO网络中的应用，研究表明ICL是一种原则性且高效的实时接收机自适应方法，无需在线再训练。", "keywords": "上下文学习, 接收机自适应, 无梯度, 无线通信, Transformer, SSM", "comments": "该论文具有创新性，因为它将新兴的上下文学习范式应用于无线接收机自适应，解决了实时、无梯度自适应的重大挑战。其强调避免在线再训练的特性使其在动态无线环境中具有高度实用性。跨不同架构和网络类型的理论见解和实证证据进一步增强了其贡献。"}}
{"id": "2506.14827", "title": "DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning", "authors": ["Yifeng Gao", "Yifan Ding", "Hongyu Su", "Juncheng Li", "Yunhan Zhao", "Lin Luo", "Zixing Chen", "Li Wang", "Xin Wang", "Yixu Wang", "Xingjun Ma", "Yu-Gang Jiang"], "summary": "As AI-generated video becomes increasingly pervasive across media platforms,\nthe ability to reliably distinguish synthetic content from authentic footage\nhas become both urgent and essential. Existing approaches have primarily\ntreated this challenge as a binary classification task, offering limited\ninsight into where or why a model identifies a video as AI-generated. However,\nthe core challenge extends beyond simply detecting subtle artifacts; it\nrequires providing fine-grained, persuasive evidence that can convince auditors\nand end-users alike. To address this critical gap, we introduce DAVID-X, the\nfirst dataset to pair AI-generated videos with detailed defect-level,\ntemporal-spatial annotations and written rationales. Leveraging these rich\nannotations, we present DAVID-XR1, a video-language model designed to deliver\nan interpretable chain of visual reasoning-including defect categorization,\ntemporal-spatial localization, and natural language explanations. This approach\nfundamentally transforms AI-generated video detection from an opaque black-box\ndecision into a transparent and verifiable diagnostic process. We demonstrate\nthat a general-purpose backbone, fine-tuned on our compact dataset and enhanced\nwith chain-of-thought distillation, achieves strong generalization across a\nvariety of generators and generation modes. Our results highlight the promise\nof explainable detection methods for trustworthy identification of AI-generated\nvideo content.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14827v1", "AI": {"title_translation": "DAVID-XR1：通过可解释推理检测AI生成视频", "tldr": "DAVID-XR1引入了首个带有详细缺陷注释的AI生成视频数据集DAVID-X，并提出了一个视频-语言模型，通过提供可解释的视觉推理（包括缺陷分类、时空定位和自然语言解释），将AI生成视频检测从不透明的黑盒决策转变为透明可验证的诊断过程，实现了强大的泛化能力。", "motivation": "随着AI生成视频日益普及，可靠区分合成内容和真实素材变得紧迫且必要。现有方法多为二元分类，缺乏对AI生成视频被识别原因的深入洞察，无法提供细粒度、有说服力的证据来取信审计员和最终用户。", "method": "引入了DAVID-X数据集，这是首个将AI生成视频与详细的缺陷级别、时空注释和书面解释配对的数据集。在此基础上，提出了DAVID-XR1，一个视频-语言模型，旨在提供可解释的视觉推理链，包括缺陷分类、时空定位和自然语言解释。该方法通过在紧凑数据集上微调通用骨干网络并结合思维链蒸馏来实现。", "result": "DAVID-XR1模型在各种生成器和生成模式下均表现出强大的泛化能力。", "conclusion": "可解释的检测方法在可信识别AI生成视频内容方面展现出巨大潜力。", "translation": "随着AI生成视频在媒体平台上日益普及，可靠地区分合成内容和真实素材的能力变得既紧迫又必不可少。现有方法主要将此挑战视为二元分类任务，对模型在何处或为何将视频识别为AI生成提供有限的洞察。然而，核心挑战不仅仅是检测细微的伪影；它需要提供细粒度、有说服力的证据，能够说服审计员和最终用户。为解决这一关键空白，我们引入了DAVID-X，这是第一个将AI生成视频与详细的缺陷级别、时空注释和书面解释配对的数据集。利用这些丰富的注释，我们提出了DAVID-XR1，一个视频-语言模型，旨在提供可解释的视觉推理链——包括缺陷分类、时空定位和自然语言解释。这种方法从根本上将AI生成视频检测从不透明的黑盒决策转变为透明且可验证的诊断过程。我们证明，一个在我们的紧凑数据集上进行微调并结合思维链蒸馏的通用骨干网络，在各种生成器和生成模式下都能实现强大的泛化能力。我们的结果突出了可解释检测方法在可信识别AI生成视频内容方面的潜力。", "summary": "该论文引入了DAVID-XR1，一个视频-语言模型，及其配套的DAVID-X数据集，旨在解决AI生成视频检测中缺乏可解释性的问题。与现有黑盒方法不同，DAVID-XR1通过提供详细的视觉推理，包括缺陷定位和自然语言解释，将检测过程转变为透明可验证的诊断过程。该模型在DAVID-X数据集上训练，并结合思维链蒸馏，在多种生成器和生成模式下均表现出强大的泛化能力，凸显了可解释方法在可信AI视频识别方面的潜力。", "keywords": "AI生成视频, 可解释AI, 视频检测, 深度伪造, 视觉推理", "comments": "这篇论文解决了当前AI生成内容检测的一个关键局限性：缺乏可解释性。通过引入一个带有详细注释的新数据集和一个能够提供视觉推理和自然语言解释的模型，它显著推动了该领域向更透明和可信的检测方向发展。论文关注“为什么”和“在哪里”视频是合成的，是其关键创新点。"}}
{"id": "2506.15613", "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and Instruction Annotation", "authors": ["Miryeong Kwon", "Donghyun Gouk", "Junhyeok Jang", "Jinwoo Baek", "Hyunwoo You", "Sangyoon Ji", "Hongjoo Jung", "Junseok Moon", "Seungkwan Kang", "Seungjun Lee", "Myoungsoo Jung"], "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence.", "comment": null, "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15613v1", "AI": {"title_translation": "从块到字节：通过CXL内存协议和指令注解改造PCIe SSD", "tldr": "本文探讨了如何利用CXL将PCIe SSD块存储转换为可扩展的字节寻址工作内存，并通过原型和注解机制实现了显著的性能提升。", "motivation": "本研究旨在探索如何将基于PCIe的块存储转换为可扩展的字节寻址工作内存，以适应CXL的以内存为中心的模型，并解决其面临的挑战。", "method": "作者通过强调可缓存性作为关键促成因素，并提倡使用Type 3端点设备（称为CXL-SSD）来适应CXL的内存模型。他们在一个定制的FPGA平台上原型化了一个CXL-SSD，并提出了“确定性”和“可缓冲性”的注解机制，以在保持数据持久性的同时提升性能。", "result": "模拟评估表明，CXL-SSD比基于PCIe的内存扩展器性能提升10.9倍，通过注解增强进一步将延迟降低5.4倍。在高局部性工作负载中，由于高效的片上缓存，CXL-SSD的性能接近DRAM。", "conclusion": "这项工作突出了将块存储集成到CXL生态系统中的可行性，并为未来的内存-存储融合奠定了基础。", "translation": "本文探讨了计算快速连接（CXL）如何将基于PCIe的块存储转换为可扩展的字节寻址工作内存。我们通过强调可缓存性作为关键促成因素，并倡导使用Type 3端点设备（称为CXL-SSD），解决了将块存储适应CXL以内存为中心的模型所面临的挑战。为了验证我们的方法，我们在定制的FPGA平台上原型化了一个CXL-SSD，并提出了“确定性”和“可缓冲性”的注解机制，以在保持数据持久性的同时提升性能。我们的基于模拟的评估表明，CXL-SSD比基于PCIe的内存扩展器性能提升10.9倍，并通过注解增强进一步将延迟降低5.4倍。在高局部性工作负载中，由于高效的片上缓存，CXL-SSD的性能接近DRAM。这项工作突出了将块存储集成到CXL生态系统中的可行性，并为未来的内存-存储融合奠定了基础。", "summary": "本文研究了通过CXL协议将PCIe SSD块存储转换为字节寻址内存的可行性。研究提出并原型化了CXL-SSD设备，并引入了“确定性”和“可缓冲性”两种注解机制以优化性能。实验结果表明，CXL-SSD比现有方案性能显著提升，并在高局部性场景下接近DRAM表现，为内存与存储的融合开辟了新途径。", "keywords": "CXL, PCIe SSD, 字节寻址, 内存-存储融合, 缓存", "comments": "这项工作具有重要的创新性，它提出了一种将传统块存储设备（SSD）无缝集成到新兴CXL内存生态系统中的方法。通过强调可缓存性和引入新的注解机制，作者有效地解决了块存储向字节寻址内存转换的挑战，并展示了显著的性能提升，为未来的异构内存系统设计和内存-存储融合提供了坚实的基础。"}}
{"id": "2506.15225", "title": "Joint Computation Offloading and Resource Allocation for Uncertain Maritime MEC via Cooperation of UAVs and Vessels", "authors": ["Jiahao You", "Ziye Jia", "Chao Dong", "Qihui Wu", "Zhu Han"], "summary": "The computation demands from the maritime Internet of Things (MIoT) increase\nrapidly in recent years, and the unmanned aerial vehicles (UAVs) and vessels\nbased multi-access edge computing (MEC) can fulfill these MIoT requirements.\nHowever, the uncertain maritime tasks present significant challenges of\ninefficient computation offloading and resource allocation. In this paper, we\nfocus on the maritime computation offloading and resource allocation through\nthe cooperation of UAVs and vessels, with consideration of uncertain tasks.\nSpecifically, we propose a cooperative MEC framework for computation offloading\nand resource allocation, including MIoT devices, UAVs and vessels. Then, we\nformulate the optimization problem to minimize the total execution time. As for\nthe uncertain MIoT tasks, we leverage Lyapunov optimization to tackle the\nunpredictable task arrivals and varying computational resource availability. By\nconverting the long-term constraints into short-term constraints, we obtain a\nset of small-scale optimization problems. Further, considering the\nheterogeneity of actions and resources of UAVs and vessels, we reformulate the\nsmall-scale optimization problem into a Markov game (MG). Moreover, a\nheterogeneous-agent soft actor-critic is proposed to sequentially update\nvarious neural networks and effectively solve the MG problem. Finally,\nsimulations are conducted to verify the effectiveness in addressing\ncomputational offloading and resource allocation.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15225v1", "AI": {"title_translation": "考虑不确定性的无人机与船舶协同海上MEC联合计算卸载与资源分配", "tldr": "针对海上物联网MEC中的不确定任务，提出无人机与船舶协同的计算卸载与资源分配框架，并利用Lyapunov优化和异构智能体SAC算法解决优化问题，旨在最小化总执行时间。", "motivation": "近年来，海上物联网（MIoT）的计算需求迅速增长，而现有MEC系统在处理不确定的海上任务时，面临计算卸载和资源分配效率低下的挑战。", "method": "本文提出了一个包含MIoT设备、无人机和船舶的协同MEC框架。将优化问题表述为最小化总执行时间。利用Lyapunov优化处理不确定的MIoT任务，将长期约束转换为短期小规模优化问题。考虑到无人机和船舶的异构性，将小规模优化问题重新表述为马尔可夫博弈（MG），并提出异构智能体软Actor-Critic算法来解决该MG问题。", "result": "仿真结果验证了所提方法在解决计算卸载和资源分配方面的有效性。", "conclusion": "提出的协同MEC框架和优化算法能够有效解决不确定海上MEC环境下的计算卸载和资源分配问题。", "translation": "近年来，海上物联网（MIoT）的计算需求迅速增长，基于无人机（UAV）和船舶的多接入边缘计算（MEC）可以满足这些MIoT需求。然而，不确定的海上任务给低效的计算卸载和资源分配带来了巨大挑战。在本文中，我们重点关注通过无人机和船舶的协同，并考虑不确定任务的海上计算卸载和资源分配。具体来说，我们提出了一个用于计算卸载和资源分配的协同MEC框架，包括MIoT设备、无人机和船舶。然后，我们制定了最小化总执行时间的优化问题。对于不确定的MIoT任务，我们利用Lyapunov优化来处理不可预测的任务到达和变化的计算资源可用性。通过将长期约束转换为短期约束，我们得到了一系列小规模优化问题。此外，考虑到无人机和船舶动作和资源的异构性，我们将小规模优化问题重新表述为马尔可夫博弈（MG）。此外，提出了一种异构智能体软Actor-Critic算法，以序贯更新各种神经网络并有效解决MG问题。最后，通过仿真验证了其在解决计算卸载和资源分配方面的有效性。", "summary": "本文针对海上物联网（MIoT）中不确定的计算任务带来的计算卸载和资源分配挑战，提出了一种无人机与船舶协同的多接入边缘计算（MEC）框架。该研究将最小化总执行时间作为优化目标，并利用Lyapunov优化处理任务的不确定性。进一步，考虑到无人机和船舶的异构性，将问题建模为马尔可夫博弈，并通过异构智能体软Actor-Critic算法进行求解。仿真结果验证了所提方法的有效性。", "keywords": "海上MEC, 计算卸载, 资源分配, 无人机, 船舶, 不确定性, Lyapunov优化, 马尔可夫博弈", "comments": "本文创新性地结合了无人机和船舶的协同，以应对海上MIoT中不确定任务的计算卸载和资源分配挑战。通过引入Lyapunov优化和强化学习（异构智能体SAC）来处理动态和不确定的环境，提高了解决方案的适应性和鲁棒性。这种方法对于提升海上通信和计算效率具有重要意义。"}}
{"id": "2506.15418", "title": "RISC-V for HPC: An update of where we are and main action points", "authors": ["Nick Brown"], "summary": "This extended abstract is submitted on behalf of the RISC-V HPC SIG who have\nbeen undertaking an analysis to explore the current state and limitations of\nthe RISC-V ecosystem for HPC. Whilst it is right to celebrate that there has\nbeen great progress made in recent years, we also highlight limitations and\nwhere effort should be focussed.", "comment": "Extended abstract accepted to the RISC-V Summit Europe 2025", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15418v1", "AI": {"title_translation": "RISC-V在HPC中的应用：现状更新与主要行动点", "tldr": "RISC-V HPC SIG分析了RISC-V在HPC领域的现状和局限性，指出了进展和需要关注的重点。", "motivation": "探索RISC-V生态系统在高性能计算（HPC）领域的当前状态和局限性，并指出需要重点关注的领域。", "method": "该论文是代表RISC-V HPC SIG提交的扩展摘要，该小组进行了一项分析。具体的分析方法未在摘要中提及。", "result": "RISC-V在HPC领域近年来取得了巨大进展，但也存在局限性。", "conclusion": "应将精力集中在解决已识别的局限性上。", "translation": "这份扩展摘要代表RISC-V HPC SIG提交，该小组一直在进行一项分析，以探讨RISC-V生态系统在高性能计算（HPC）领域的当前状态和局限性。虽然近年来取得了巨大进展值得庆祝，但我们也强调了局限性以及需要重点关注的领域。", "summary": "这份扩展摘要由RISC-V HPC SIG提交，评估了RISC-V生态系统在高性能计算（HPC）领域的当前状态和局限性。它肯定了所取得的显著进展，同时也指出了现有局限性以及需要集中精力发展的领域。", "keywords": "RISC-V, HPC, 生态系统, 局限性, 进展", "comments": "该论文提供了RISC-V生态系统在HPC领域的最新情况，这对于指导这一有前途架构的未来发展非常重要。它侧重于对当前状态和未来需求的实际评估。"}}
{"id": "2506.15572", "title": "Misinformation by Omission: The Need for More Environmental Transparency in AI", "authors": ["Sasha Luccioni", "Boris Gamazaychikov", "Theo Alves da Costa", "Emma Strubell"], "summary": "In recent years, Artificial Intelligence (AI) models have grown in size and\ncomplexity, driving greater demand for computational power and natural\nresources. In parallel to this trend, transparency around the costs and impacts\nof these models has decreased, meaning that the users of these technologies\nhave little to no information about their resource demands and subsequent\nimpacts on the environment. Despite this dearth of adequate data, escalating\ndemand for figures quantifying AI's environmental impacts has led to numerous\ninstances of misinformation evolving from inaccurate or de-contextualized\nbest-effort estimates of greenhouse gas emissions. In this article, we explore\npervasive myths and misconceptions shaping public understanding of AI's\nenvironmental impacts, tracing their origins and their spread in both the media\nand scientific publications. We discuss the importance of data transparency in\nclarifying misconceptions and mitigating these harms, and conclude with a set\nof recommendations for how AI developers and policymakers can leverage this\ninformation to mitigate negative impacts in the future.", "comment": null, "cate": "cs.CY", "url": "http://arxiv.org/abs/2506.15572v1", "AI": {"title_translation": "遗漏导致的错误信息：AI中需要更多环境透明度", "tldr": "AI模型对环境影响的信息缺乏透明度导致了错误信息，本文探讨这些错误信息的起源和传播，并提出通过数据透明度来缓解负面影响的建议。", "motivation": "近年来AI模型规模和复杂性不断增长，导致计算能力和自然资源需求增加。然而，这些模型的成本和影响的透明度却在下降，使得用户对AI的资源需求及其对环境的影响知之甚少。这种数据匮乏导致了关于AI环境影响的错误信息，因此需要探讨并解决。", "method": "本文探讨了影响公众对AI环境影响理解的普遍神话和误解，追溯了它们在媒体和科学出版物中的起源和传播。文章讨论了数据透明度在澄清误解和减轻危害方面的重要性。", "result": "研究发现，由于缺乏足够的数据，对AI环境影响的量化需求导致了许多源于不准确或脱离语境的最佳估计的温室气体排放错误信息。", "conclusion": "文章强调了数据透明度在澄清误解和减轻AI负面环境影响方面的重要性，并为AI开发者和政策制定者提供了利用这些信息来减轻未来负面影响的建议。", "translation": "近年来，人工智能（AI）模型的规模和复杂性不断增长，推动了对计算能力和自然资源的需求。与此同时，这些模型的成本和影响的透明度却在下降，这意味着这些技术的使用者对其资源需求以及随之对环境产生的影响知之甚少。尽管缺乏足够的数据，但对量化AI环境影响数据的需求不断升级，导致了许多源于不准确或脱离语境的最佳估计的温室气体排放错误信息。在本文中，我们探讨了影响公众对AI环境影响理解的普遍神话和误解，追溯了它们在媒体和科学出版物中的起源和传播。我们讨论了数据透明度在澄清误解和减轻这些危害方面的重要性，并以一套建议作为结论，说明AI开发者和政策制定者如何利用这些信息来减轻未来的负面影响。", "summary": "本文指出，随着AI模型规模的增长，其环境影响的透明度却在下降，导致了大量基于不准确估计的错误信息。文章探讨了这些普遍的误解及其传播途径，并强调了数据透明度在纠正错误信息和减轻AI环境负面影响中的关键作用。最后，文章为AI开发者和政策制定者提供了具体的建议，以促进未来的环境可持续性。", "keywords": "AI环境影响, 数据透明度, 错误信息, 计算能力, 资源消耗", "comments": "本文揭示了一个日益重要的问题：AI快速发展带来的环境成本及其信息不对称。其创新点在于明确指出“遗漏”是错误信息的来源，并强调数据透明度作为解决方案。重要性体现在呼吁行业和政策制定者共同关注并解决AI的环境足迹，具有现实指导意义。"}}
{"id": "2506.15124", "title": "A Force Feedback Exoskeleton for Teleoperation Using Magnetorheological Clutches", "authors": ["Zhongyuan Kong", "Lei Li", "Erwin Ang Tien Yew", "Zirui Chen", "Wenbo Li", "Shiwu Zhang", "Jian Yang", "Shuaishuai Sun"], "summary": "This paper proposes an upper-limb exoskeleton teleoperation system based on\nmagnetorheological (MR) clutches, aiming to improve operational accuracy and\nenhance the immersive experience during lunar sampling tasks. Conventional\nexoskeleton teleoperation systems commonly employ active force feedback\nsolutions, such as servo motors, which typically suffer from high system\ncomplexity and increased energy consumption. Furthermore, force feedback\ndevices utilizing motors and gear reducers generally compromise backdrivability\nand pose safety risks to operators due to active force output. To address these\nlimitations, we propose a semi-active force feedback strategy based on MR\nclutches. Dynamic magnetic field control enables precise adjustment of joint\nstiffness and damping, thereby providing smooth and high-resolution force\nfeedback. The designed MR clutch exhibits outstanding performance across key\nmetrics, achieving a torque-to-mass ratio (TMR) of 93.6 Nm/kg, a\ntorque-to-volume ratio (TVR) of 4.05 x 10^5 Nm/m^3, and a torque-to-power ratio\n(TPR) of 4.15 Nm/W. Notably, the TMR represents an improvement of approximately\n246% over a representative design in prior work. Experimental results validate\nthe system's capability to deliver high-fidelity force feedback. Overall, the\nproposed system presents a promising solution for deep-space teleoperation with\nstrong potential for real-world deployment in future missions.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15124v1", "AI": {"title_translation": "一种基于磁流变离合器的遥操作力反馈外骨骼", "tldr": "该研究提出了一种基于磁流变（MR）离合器的上肢外骨骼遥操作系统，旨在提高月球采样任务中的操作精度和沉浸感，并解决了传统主动力反馈系统复杂性高、能耗大、回驱性差及安全风险等问题。", "motivation": "传统外骨骼遥操作系统通常采用伺服电机等主动力反馈方案，但存在系统复杂性高、能耗大、回驱性差以及对操作者存在安全风险等局限性。为解决这些问题，本文提出了一种新的解决方案。", "method": "本文提出了一种基于磁流变（MR）离合器的半主动力反馈策略。通过动态磁场控制，实现关节刚度和阻尼的精确调节，从而提供平滑、高分辨率的力反馈。", "result": "设计的MR离合器在关键指标上表现出色，扭矩质量比（TMR）达到93.6 Nm/kg，扭矩体积比（TVR）达到4.05 x 10^5 Nm/m^3，扭矩功率比（TPR）达到4.15 Nm/W。其中，TMR相较于现有代表性设计提升了约246%。实验结果验证了系统提供高保真力反馈的能力。", "conclusion": "所提出的系统为深空遥操作提供了一个有前景的解决方案，在未来任务中具有强大的实际部署潜力。", "translation": "本文提出了一种基于磁流变（MR）离合器的上肢外骨骼遥操作系统，旨在提高月球采样任务中的操作精度和沉浸式体验。传统的力反馈外骨骼遥操作系统通常采用伺服电机等主动力反馈解决方案，这些方案通常存在系统复杂性高和能耗增加的问题。此外，利用电机和减速器的力反馈装置通常会损害回驱性，并由于主动力输出而对操作者构成安全风险。为了解决这些限制，我们提出了一种基于MR离合器的半主动力反馈策略。动态磁场控制能够精确调节关节刚度和阻尼，从而提供平滑、高分辨率的力反馈。所设计的MR离合器在关键指标上表现出色，实现了93.6 Nm/kg的扭矩质量比（TMR）、4.05 x 10^5 Nm/m^3的扭矩体积比（TVR）和4.15 Nm/W的扭矩功率比（TPR）。值得注意的是，TMR比现有工作中具有代表性的设计提高了约246%。实验结果验证了系统提供高保真力反馈的能力。总的来说，所提出的系统为深空遥操作提供了一个有前景的解决方案，在未来任务中具有强大的实际部署潜力。", "summary": "本论文提出了一种基于磁流变（MR）离合器的上肢外骨骼遥操作系统，旨在解决传统主动力反馈系统在复杂性、能耗、回驱性和安全性方面的不足。该系统采用半主动力反馈策略，通过精确控制磁场来调节关节刚度和阻尼，从而实现高分辨率的力反馈。实验结果表明，所设计的MR离合器性能优异，特别是在扭矩质量比方面有显著提升。该系统被认为是深空遥操作领域一个有前景的解决方案。", "keywords": "磁流变离合器, 力反馈, 外骨骼, 遥操作, 半主动", "comments": "本文的创新之处在于引入磁流变（MR）离合器作为半主动力反馈机制，有效克服了传统主动力反馈系统在复杂性、能耗和安全性方面的固有缺陷。MR离合器的高性能指标，特别是其显著提升的扭矩质量比，表明了该技术在轻量化和高效力反馈方面的潜力。这对于深空遥操作等对设备性能要求极高的应用场景具有重要意义。"}}
{"id": "2506.15148", "title": "Probabilistic Trajectory GOSPA: A Metric for Uncertainty-Aware Multi-Object Tracking Performance Evaluation", "authors": ["Yuxuan Xia", "Ángel F. García-Fernández", "Johan Karlsson", "Yu Ge", "Lennart Svensson", "Ting Yuan"], "summary": "This paper presents a generalization of the trajectory general optimal\nsub-pattern assignment (GOSPA) metric for evaluating multi-object tracking\nalgorithms that provide trajectory estimates with track-level uncertainties.\nThis metric builds on the recently introduced probabilistic GOSPA metric to\naccount for both the existence and state estimation uncertainties of individual\nobject states. Similar to trajectory GOSPA (TGOSPA), it can be formulated as a\nmultidimensional assignment problem, and its linear programming\nrelaxation--also a valid metric--is computable in polynomial time.\nAdditionally, this metric retains the interpretability of TGOSPA, and we show\nthat its decomposition yields intuitive costs terms associated to expected\nlocalization error and existence probability mismatch error for properly\ndetected objects, expected missed and false detection error, and track switch\nerror. The effectiveness of the proposed metric is demonstrated through a\nsimulation study.", "comment": "7 pages, 4 figures", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15148v1", "AI": {"title_translation": "概率轨迹GOSPA：一种用于不确定性感知多目标跟踪性能评估的度量", "tldr": "本文提出了一种概率轨迹GOSPA（PTGOSPA）度量，用于评估考虑轨迹级不确定性的多目标跟踪算法，并通过仿真研究证明了其有效性。", "motivation": "现有度量可能未能充分考虑多目标跟踪算法中轨迹估计的轨迹级不确定性，包括个体目标状态的存在性和状态估计不确定性。因此，需要一种新的度量来解决这个问题。", "method": "本文提出了一种概率轨迹GOSPA（PTGOSPA）度量，它是轨迹GOSPA（TGOSPA）的泛化，并基于最近引入的概率GOSPA度量构建。该度量能够解释单个目标状态的存在性和状态估计不确定性。它可以被表述为一个多维分配问题，并且其线性规划松弛（也是一个有效的度量）可以在多项式时间内计算。该度量还保留了TGOSPA的可解释性，其分解产生了与正确检测目标的预期定位误差和存在概率不匹配误差、预期漏检和误检误差以及轨迹切换误差相关的直观成本项。", "result": "通过仿真研究证明了所提出的概率轨迹GOSPA度量的有效性。该度量的分解产生了直观的成本项，包括预期定位误差、存在概率不匹配误差、预期漏检和误检误差以及轨迹切换误差。", "conclusion": "本文成功提出了一种名为概率轨迹GOSPA的新度量，用于评估具有轨迹级不确定性的多目标跟踪算法的性能。该度量具有可计算性、可解释性，并通过仿真证明了其有效性，能够全面地评估跟踪性能。", "translation": "本文提出了一种轨迹通用最优子模式分配（GOSPA）度量的泛化形式，用于评估提供具有轨迹级不确定性轨迹估计的多目标跟踪算法。该度量建立在最近引入的概率GOSPA度量的基础上，以解释个体目标状态的存在性和状态估计不确定性。与轨迹GOSPA（TGOSPA）类似，它可以被表述为一个多维分配问题，并且其线性规划松弛——也是一个有效的度量——可以在多项式时间内计算。此外，该度量保留了TGOSPA的可解释性，我们证明其分解产生了与正确检测目标的预期定位误差和存在概率不匹配误差、预期漏检和误检误差以及轨迹切换误差相关的直观成本项。所提出度量的有效性通过仿真研究得到证明。", "summary": "本文引入了一种名为概率轨迹GOSPA（PTGOSPA）的新度量，用于评估考虑轨迹级不确定性的多目标跟踪（MOT）算法。PTGOSPA是轨迹GOSPA的泛化，并基于概率GOSPA构建，能够同时考虑目标的存在性和状态估计不确定性。该度量可被公式化为多维分配问题，其线性规划松弛可在多项式时间内计算，并保留了TGOSPA的可解释性。PTGOSPA的分解提供了直观的成本项，包括定位误差、存在概率不匹配误差、漏检/误检误差和轨迹切换误差。通过仿真研究验证了该度量的有效性。", "keywords": "多目标跟踪, GOSPA, 轨迹评估, 不确定性, 性能度量", "comments": "该论文提出了一种创新的、针对不确定性感知的多目标跟踪性能评估度量。其重要性在于，它弥补了现有评估方法在处理轨迹级不确定性方面的不足，通过引入概率GOSPA的概念，使得评估结果更贴近实际应用中的不确定性场景。该度量不仅可计算，而且具有良好的可解释性，其分解的成本项能为算法改进提供明确的方向。仿真研究的验证增加了其可信度。"}}
{"id": "2506.14786", "title": "PIPE: Physics-Informed Position Encoding for Alignment of Satellite Images and Time Series", "authors": ["Haobo Li", "Eunseo Jung", "Zixin Chen", "Zhaowei Wang", "Yueya Wang", "Huamin Qu", "Alexis Kai Hon Lau"], "summary": "Multimodal time series forecasting is foundational in various fields, such as\nutilizing satellite imagery and numerical data for predicting typhoons in\nclimate science. However, existing multimodal approaches primarily focus on\nutilizing text data to help time series forecasting, leaving the visual data in\nexisting time series datasets untouched. Furthermore, it is challenging for\nmodels to effectively capture the physical information embedded in visual data,\nsuch as satellite imagery's temporal and geospatial context, which extends\nbeyond images themselves. To address this gap, we propose physics-informed\npositional encoding (PIPE), a lightweight method that embeds physical\ninformation into vision language models (VLMs). PIPE introduces two key\ninnovations: (1) a physics-informed positional indexing scheme for mapping\nphysics to positional IDs, and (2) a variant-frequency positional encoding\nmechanism for encoding frequency information of physical variables and\nsequential order of tokens within the embedding space. By preserving both the\nphysical information and sequential order information, PIPE significantly\nimproves multimodal alignment and forecasting accuracy. Through the experiments\non the most representative and the largest open-sourced satellite image\ndataset, PIPE achieves state-of-the-art performance in both deep learning\nforecasting and climate domain methods, demonstrating superiority across\nbenchmarks, including a 12% improvement in typhoon intensity forecasting over\nprior works. Our code is provided in the supplementary material.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14786v1", "AI": {"title_translation": "PIPE：用于卫星图像和时间序列对齐的物理信息位置编码", "tldr": "PIPE提出了一种轻量级方法，通过物理信息位置编码将物理信息嵌入到视觉语言模型中，显著提高了卫星图像和时间序列的多模态对齐和预测精度，并在台风强度预测中取得了最先进的性能。", "motivation": "现有的多模态时间序列预测方法主要侧重于利用文本数据，而忽略了视觉数据中蕴含的物理信息（如卫星图像的时空上下文），这导致模型难以有效捕捉这些信息。", "method": "我们提出了物理信息位置编码（PIPE），这是一种轻量级方法，用于将物理信息嵌入视觉语言模型（VLMs）。PIPE引入了两项关键创新：1）一种将物理信息映射到位置ID的物理信息位置索引方案；2）一种变频位置编码机制，用于编码物理变量的频率信息和嵌入空间中标记的顺序。", "result": "通过保留物理信息和顺序信息，PIPE显著改善了多模态对齐和预测精度。在最具代表性和最大的开源卫星图像数据集上的实验表明，PIPE在深度学习预测和气候领域方法中都达到了最先进的性能，在各项基准测试中表现出卓越性，包括在台风强度预测方面比先前工作提高了12%。", "conclusion": "PIPE通过将物理信息有效地编码到视觉语言模型中，解决了现有方法在处理视觉数据物理信息方面的不足，显著提高了多模态对齐和时间序列预测的准确性，尤其在气候科学领域展现出强大的应用潜力。", "translation": "多模态时间序列预测在各个领域都是基础性的，例如利用卫星图像和数值数据预测气候科学中的台风。然而，现有的多模态方法主要侧重于利用文本数据来帮助时间序列预测，而现有时间序列数据集中的视觉数据则未被触及。此外，模型难以有效捕捉视觉数据中嵌入的物理信息，例如卫星图像的时空上下文，这些信息超出了图像本身。为了解决这一差距，我们提出了物理信息位置编码（PIPE），这是一种将物理信息嵌入视觉语言模型（VLMs）的轻量级方法。PIPE引入了两项关键创新：（1）一种用于将物理信息映射到位置ID的物理信息位置索引方案，以及（2）一种变频位置编码机制，用于编码物理变量的频率信息和嵌入空间中标记的顺序。通过同时保留物理信息和顺序信息，PIPE显著提高了多模态对齐和预测精度。通过在最具代表性和最大的开源卫星图像数据集上进行的实验，PIPE在深度学习预测和气候领域方法中均达到了最先进的性能，在各项基准测试中表现出卓越性，包括在台风强度预测方面比先前工作提高了12%。我们的代码在补充材料中提供。", "summary": "PIPE提出了一种新颖的物理信息位置编码方法，旨在解决现有模型在多模态时间序列预测中忽视视觉数据中物理信息的问题。该方法通过引入物理信息位置索引和变频位置编码机制，有效地将卫星图像中的时空物理信息嵌入到视觉语言模型中。实验证明，PIPE在卫星图像数据集上实现了最先进的性能，显著提高了多模态对齐和预测精度，尤其在台风强度预测方面取得了12%的提升。", "keywords": "物理信息位置编码, 卫星图像, 时间序列预测, 多模态对齐, 台风强度预测", "comments": "该论文的创新点在于提出了PIPE，一种将物理信息有效融入视觉语言模型的位置编码方法，填补了现有模型在处理视觉数据物理信息方面的空白。其重要性体现在显著提升了多模态对齐和时间序列预测的准确性，尤其在气候科学（如台风预测）中表现出强大的应用潜力，取得了显著的性能提升。该方法轻量级且通用，有望推广到其他需要融合视觉与物理信息的多模态任务。"}}
{"id": "2506.14970", "title": "NeuroMoE: A Transformer-Based Mixture-of-Experts Framework for Multi-Modal Neurological Disorder Classification", "authors": ["Wajih Hassan Raza", "Aamir Bader Shah", "Yu Wen", "Yidan Shen", "Juan Diego Martinez Lemus", "Mya Caryn Schiess", "Timothy Michael Ellmore", "Renjie Hu", "Xin Fu"], "summary": "The integration of multi-modal Magnetic Resonance Imaging (MRI) and clinical\ndata holds great promise for enhancing the diagnosis of neurological disorders\n(NDs) in real-world clinical settings. Deep Learning (DL) has recently emerged\nas a powerful tool for extracting meaningful patterns from medical data to aid\nin diagnosis. However, existing DL approaches struggle to effectively leverage\nmulti-modal MRI and clinical data, leading to suboptimal performance.\n  To address this challenge, we utilize a unique, proprietary multi-modal\nclinical dataset curated for ND research. Based on this dataset, we propose a\nnovel transformer-based Mixture-of-Experts (MoE) framework for ND\nclassification, leveraging multiple MRI modalities-anatomical (aMRI), Diffusion\nTensor Imaging (DTI), and functional (fMRI)-alongside clinical assessments. Our\nframework employs transformer encoders to capture spatial relationships within\nvolumetric MRI data while utilizing modality-specific experts for targeted\nfeature extraction. A gating mechanism with adaptive fusion dynamically\nintegrates expert outputs, ensuring optimal predictive performance.\nComprehensive experiments and comparisons with multiple baselines demonstrate\nthat our multi-modal approach significantly enhances diagnostic accuracy,\nparticularly in distinguishing overlapping disease states. Our framework\nachieves a validation accuracy of 82.47\\%, outperforming baseline methods by\nover 10\\%, highlighting its potential to improve ND diagnosis by applying\nmulti-modal learning to real-world clinical data.", "comment": "Accepted at the 47th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.14970v1", "AI": {"title_translation": "NeuroMoE：一个基于Transformer的专家混合框架，用于多模态神经疾病分类", "tldr": "NeuroMoE是一个基于Transformer的专家混合（MoE）框架，它利用多模态MRI和临床数据来提高神经疾病的诊断准确性。", "motivation": "现有深度学习方法在有效利用多模态MRI和临床数据进行神经疾病诊断方面存在局限性，导致性能不佳。", "method": "本文提出了NeuroMoE框架，该框架基于专有的多模态临床数据集。它利用Transformer编码器捕获体积MRI数据中的空间关系，并使用模态特定专家进行有针对性的特征提取。一个具有自适应融合的门控机制动态整合专家输出，以优化预测性能。该框架结合了解剖（aMRI）、弥散张量成像（DTI）和功能（fMRI）MRI数据以及临床评估。", "result": "实验表明，该多模态方法显著提高了诊断准确性，特别是在区分重叠疾病状态方面。该框架实现了82.47%的验证准确率，比基线方法高出10%以上。", "conclusion": "NeuroMoE框架通过将多模态学习应用于真实世界临床数据，显著提高了神经疾病的诊断能力，展现了其巨大的临床应用潜力。", "translation": "多模态磁共振成像（MRI）与临床数据的整合，对于在真实临床环境中增强神经系统疾病（NDs）的诊断具有巨大前景。深度学习（DL）最近已成为一种强大的工具，能够从医疗数据中提取有意义的模式以辅助诊断。然而，现有的DL方法难以有效利用多模态MRI和临床数据，导致性能不佳。\n为了解决这一挑战，我们利用了一个独特的、专有的多模态临床数据集，该数据集是为ND研究而整理的。基于该数据集，我们提出了一个新颖的基于Transformer的专家混合（MoE）框架，用于ND分类，该框架利用多种MRI模态——解剖（aMRI）、弥散张量成像（DTI）和功能（fMRI）——以及临床评估。我们的框架采用Transformer编码器来捕获体积MRI数据中的空间关系，同时利用模态特定专家进行有针对性的特征提取。具有自适应融合的门控机制动态整合专家输出，确保最佳的预测性能。与多个基线的全面实验和比较表明，我们的多模态方法显著提高了诊断准确性，特别是在区分重叠疾病状态方面。我们的框架实现了82.47%的验证准确率，比基线方法高出10%以上，突出了其通过将多模态学习应用于真实世界临床数据来改善ND诊断的潜力。", "summary": "本文提出NeuroMoE，一个基于Transformer和专家混合（MoE）的新型框架，用于多模态神经疾病分类。该框架利用解剖、弥散张量和功能MRI以及临床数据，通过Transformer编码器和模态特定专家提取特征，并使用自适应门控机制融合信息。实验证明，NeuroMoE在神经疾病诊断中表现出色，验证准确率达82.47%，显著优于现有基线方法，尤其在区分相似疾病状态上。", "keywords": "神经疾病分类, 多模态学习, Transformer, 专家混合, 磁共振成像", "comments": "该论文的创新点在于将Transformer架构与专家混合（MoE）模型相结合，有效地处理和融合多模态医疗数据。这种方法能够更精细地捕获不同模态间的复杂关系，并针对性地提取特征，从而显著提升了神经疾病诊断的准确性，尤其是在区分病症相似的疾病时。其在真实世界临床数据集上的优异表现，展现了巨大的临床应用潜力。"}}
{"id": "2506.15121", "title": "Generative thermodynamic computing", "authors": ["Stephen Whitelam"], "summary": "We introduce a generative modeling framework for thermodynamic computing, in\nwhich structured data is synthesized from noise by the natural time evolution\nof a physical system governed by Langevin dynamics. While conventional\ndiffusion models use neural networks to perform denoising, here the information\nneeded to generate structure from noise is encoded by the dynamics of a\nthermodynamic system. Training proceeds by maximizing the probability with\nwhich the computer generates the reverse of a noising trajectory, which ensures\nthat the computer generates data with minimal heat emission. We demonstrate\nthis framework within a digital simulation of a thermodynamic computer. If\nrealized in analog hardware, such a system would function as a generative model\nthat produces structured samples without the need for artificially-injected\nnoise or active control of denoising.", "comment": null, "cate": "cond-mat.stat-mech", "url": "http://arxiv.org/abs/2506.15121v1", "AI": {"title_translation": "生成式热力学计算", "tldr": "本文提出了一种生成建模框架，通过受朗之万动力学支配的物理系统的自然时间演化从噪声中合成结构化数据，旨在实现最小热量排放，并在模拟硬件中无需人工噪声或主动控制即可运行。", "motivation": "传统扩散模型依赖神经网络进行去噪，而本文提出一种替代方案，将生成结构所需的信息编码在热力学系统的动力学中，旨在实现更节能的生成模型，特别是在模拟硬件中。", "method": "该框架通过受朗之万动力学支配的物理系统的自然时间演化，从噪声中合成结构化数据。训练过程通过最大化计算机生成去噪轨迹逆向的概率进行，以确保最小热量排放。该框架在热力学计算机的数字模拟中进行了演示。", "result": "该框架在热力学计算机的数字模拟中得到了验证。如果以模拟硬件实现，这样的系统将作为一个生成模型运行，无需人工注入噪声或主动控制去噪即可生成结构化样本。", "conclusion": "本文介绍了一种基于热力学原理的新型生成建模框架，该框架能够以最小热量排放从噪声中合成结构化数据。这种方法为节能生成模型提供了一条潜在途径，特别是在模拟硬件中实现时。", "translation": "我们引入了一种用于热力学计算的生成建模框架，其中结构化数据通过受朗之万动力学支配的物理系统的自然时间演化从噪声中合成。虽然传统的扩散模型使用神经网络进行去噪，但这里将从噪声生成结构所需的信息编码在热力学系统的动力学中。训练通过最大化计算机生成去噪轨迹逆向的概率进行，这确保了计算机以最小的热量排放生成数据。我们在热力学计算机的数字模拟中演示了该框架。如果以模拟硬件实现，这样的系统将作为一个生成模型运行，无需人工注入噪声或主动控制去噪即可生成结构化样本。", "summary": "本文提出了一种名为“生成式热力学计算”的新型生成建模框架。与依赖神经网络去噪的传统扩散模型不同，该方法通过热力学系统的自然时间演化和朗之万动力学，直接将从噪声中生成结构化数据所需的信息编码其中。训练过程通过确保计算机生成去噪轨迹的逆向，以实现最小热量排放。该框架已通过数字模拟演示，如果能在模拟硬件中实现，它将无需人工注入噪声或主动去噪控制即可生成结构化样本，从而提供一种潜在的更节能的生成模型。", "keywords": "生成建模, 热力学计算, 朗之万动力学, 扩散模型, 热量排放", "comments": "该论文通过利用热力学原理和物理系统动力学而非神经网络，为生成建模提供了一种创新方法。其主要创新在于将生成信息编码在系统的自然演化中，旨在实现能源效率（最小热量排放）和在模拟硬件中潜在的自主操作。这可能为基于硬件的生成式人工智能开辟新途径，尽管其实际实现和可扩展性仍有待探索。"}}
{"id": "2506.15001", "title": "Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings", "authors": ["Ignacio Sastre", "Aiala Rosá"], "summary": "In this work, we observe an interesting phenomenon: it is possible to\ngenerate reversible sentence embeddings that allow an LLM to reconstruct the\noriginal text exactly, without modifying the model's weights. This is achieved\nby introducing a special memory token, whose embedding is optimized through\ntraining on a fixed sequence. When prompted with this embedding, the model\nreconstructs the fixed sequence exactly. We evaluate this phenomenon across\nEnglish and Spanish datasets, sequences of up to approximately 240 tokens, and\nmodel scales ranging from 100M to 8B parameters. Notably, Llama 3.1 8B\nsuccessfully reconstructs all tested sequences. Our findings highlight an\ninteresting capability of LLMs and suggest potential applications in\nmemory-based retrieval, compression, and controlled text generation.", "comment": "This paper will be presented at The First Workshop on Large Language\n  Model Memorization (L2M2) at ACL 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15001v1", "AI": {"title_translation": "记忆令牌：大型语言模型可以生成可逆的句子嵌入", "tldr": "大型语言模型（LLM）可以通过特殊训练的“记忆令牌”精确重建原始文本，而无需修改模型权重。", "motivation": "论文观察到一个有趣的现象：可以生成可逆的句子嵌入，使LLM能够精确重建原始文本，且无需修改模型权重。", "method": "通过引入一个特殊的记忆令牌，其嵌入通过对固定序列的训练进行优化。当模型被此嵌入提示时，它能精确重建该固定序列。", "result": "该现象在英语和西班牙语数据集、长达约240个令牌的序列以及1亿到80亿参数的模型规模上进行了评估。Llama 3.1 8B成功重建了所有测试序列。", "conclusion": "本研究的发现突出了大型语言模型的一个有趣能力，并暗示了在基于记忆的检索、压缩和受控文本生成方面的潜在应用。", "translation": "在这项工作中，我们观察到一个有趣的现象：可以生成可逆的句子嵌入，使大型语言模型（LLM）能够精确地重建原始文本，而无需修改模型的权重。这是通过引入一个特殊的记忆令牌来实现的，其嵌入通过对固定序列的训练进行优化。当使用此嵌入进行提示时，模型会精确地重建固定序列。我们评估了英语和西班牙语数据集、长达约240个令牌的序列以及从1亿到80亿参数的模型规模上的这种现象。值得注意的是，Llama 3.1 8B成功重建了所有测试序列。我们的发现突出了大型语言模型的一个有趣能力，并暗示了在基于记忆的检索、压缩和受控文本生成方面的潜在应用。", "summary": "本研究揭示了一种新现象：大型语言模型（LLMs）无需修改自身权重，即可通过特殊的“记忆令牌”生成可逆的句子嵌入，从而精确重建原始文本。通过优化记忆令牌的嵌入并对其进行固定序列训练，模型在接收此嵌入时能准确复原该序列。实验验证了该方法在英语和西班牙语数据集、长达240个令牌以及100M至8B参数规模的LLMs上的有效性，其中Llama 3.1 8B模型表现尤为出色。这一发现不仅揭示了LLMs的新能力，也预示着其在记忆检索、文本压缩和受控生成等领域的潜在应用。", "keywords": "记忆令牌, 可逆嵌入, 大型语言模型, 文本重建, 句子嵌入", "comments": "这项工作通过引入“记忆令牌”实现了LLMs生成可逆句子嵌入，使得模型能够在不修改自身权重的情况下精确重建原始文本，这是其主要创新点。这种能力对于基于记忆的检索、文本压缩和受控文本生成具有重要意义，展示了LLMs超越传统生成能力的潜力。"}}
{"id": "2506.14807", "title": "An explicit computational approach for a three-dimensional system of nonlinear elastodynamic sine-Gordon problem", "authors": ["Eric Ngondiep"], "summary": "This paper proposes an explicit computational method for solving a\nthree-dimensional system of nonlinear elastodynamic sine-Gordon equations\nsubject to appropriate initial and boundary conditions. The time derivative is\napproximated by interpolation technique whereas the finite element approach is\nused to approximate the space derivatives. The developed numerical scheme is\nso-called, high-order explicit computational technique. The new algorithm\nefficiently treats the time derivative term and provides a suitable time step\nrestriction for stability and convergence. Under this time step limitation,\nboth stability and error estimates of the proposed approach are deeply analyzed\nusing a constructed strong norm. The theoretical studies indicate that the\ndeveloped approach is temporal second-order convergent and spatially\nthird-order accurate. Some numerical examples are carried out to confirm the\ntheory, to validate the computational efficiency and to demonstrate the\npractical applicability of the new computational technique.", "comment": "21 pages, 20 figures, 6 tables", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14807v1", "AI": {"title_translation": "三维非线性弹性动力学正弦-戈登问题的显式计算方法", "tldr": "本文提出了一种求解三维非线性弹性动力学正弦-戈登方程组的高阶显式计算方法，并分析了其稳定性和收敛性，通过数值例子验证了其有效性。", "motivation": "本文旨在为三维非线性弹性动力学正弦-戈登方程组提供一种高效且精确的显式计算方法。", "method": "本文提出了一种高阶显式计算技术。时间导数通过插值技术进行近似，而空间导数则采用有限元方法近似。该算法能有效处理时间导数项，并为稳定性和收敛性提供了合适的步长限制。通过构建的强范数对该方法的稳定性和误差估计进行了深入分析。", "result": "理论研究表明，所开发的方案在时间上是二阶收敛的，在空间上是三阶精确的。通过数值例子证实了理论，验证了计算效率，并展示了新计算技术的实际适用性。", "conclusion": "本文提出的高阶显式计算技术是一种有效、稳定且精确的数值方案，适用于求解三维非线性弹性动力学正弦-戈登问题。", "translation": "本文提出了一种求解受适当初始条件和边界条件约束的三维非线性弹性动力学正弦-戈登方程组的显式计算方法。时间导数通过插值技术近似，而空间导数则使用有限元方法近似。所开发的数值方案被称为高阶显式计算技术。新算法有效地处理了时间导数项，并为稳定性和收敛性提供了合适的步长限制。在此步长限制下，使用构建的强范数深入分析了所提出方法的稳定性和误差估计。理论研究表明，所开发的方法具有时间二阶收敛性和空间三阶精度。通过一些数值例子来证实理论，验证计算效率并证明新计算技术的实际适用性。", "summary": "本文提出了一种针对三维非线性弹性动力学正弦-戈登方程组的高阶显式计算方法。该方法结合了插值技术处理时间导数和有限元方法处理空间导数。理论分析表明，该方法在时间上是二阶收敛的，在空间上是三阶精确的，并在严格的步长限制下保证了稳定性和收敛性。数值实验验证了其理论预测、计算效率和实际应用价值。", "keywords": "显式计算方法, 弹性动力学, 正弦-戈登方程, 有限元方法, 高阶精度", "comments": "该论文的创新点在于开发了一种结合插值技术处理时间导数和有限元方法处理空间导数的高阶显式计算技术，并提供了严格的理论保证（稳定性、收敛性、精度阶数）。其重要性在于为复杂的非线性弹性动力学问题提供了一种高效且精确的数值求解工具。"}}
{"id": "2506.15530", "title": "Diff-TONE: Timestep Optimization for iNstrument Editing in Text-to-Music Diffusion Models", "authors": ["Teysir Baoueb", "Xiaoyu Bie", "Xi Wang", "Gaël Richard"], "summary": "Breakthroughs in text-to-music generation models are transforming the\ncreative landscape, equipping musicians with innovative tools for composition\nand experimentation like never before. However, controlling the generation\nprocess to achieve a specific desired outcome remains a significant challenge.\nEven a minor change in the text prompt, combined with the same random seed, can\ndrastically alter the generated piece. In this paper, we explore the\napplication of existing text-to-music diffusion models for instrument editing.\nSpecifically, for an existing audio track, we aim to leverage a pretrained\ntext-to-music diffusion model to edit the instrument while preserving the\nunderlying content. Based on the insight that the model first focuses on the\noverall structure or content of the audio, then adds instrument information,\nand finally refines the quality, we show that selecting a well-chosen\nintermediate timestep, identified through an instrument classifier, yields a\nbalance between preserving the original piece's content and achieving the\ndesired timbre. Our method does not require additional training of the\ntext-to-music diffusion model, nor does it compromise the generation process's\nspeed.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15530v1", "AI": {"title_translation": "Diff-TONE：文本到音乐扩散模型中乐器编辑的时间步优化", "tldr": "本文提出Diff-TONE，一种无需额外训练的方法，通过优化文本到音乐扩散模型中的中间时间步，实现乐器编辑，同时保留原始音频内容，并平衡内容保留与音色需求。", "motivation": "现有文本到音乐生成模型在控制生成过程以达到特定期望结果方面存在挑战，即使微小提示词变化也会显著改变生成结果。本文旨在解决如何利用预训练模型对现有音频进行乐器编辑，同时保留底层内容。", "method": "基于模型首先关注音频整体结构/内容，然后添加乐器信息，最后细化质量的洞察，通过乐器分类器识别并选择一个合适的中间时间步。该方法无需额外训练文本到音乐扩散模型，也不影响生成速度。", "result": "选择一个经过良好选择的中间时间步（通过乐器分类器识别）可以在保留原始作品内容和实现所需音色之间取得平衡。", "conclusion": "通过优化中间时间步，可以在不额外训练和不影响速度的前提下，利用现有文本到音乐扩散模型进行有效的乐器编辑，同时平衡内容保留与音色需求。", "translation": "文本到音乐生成模型的突破正在改变创意格局，为音乐家提供了前所未有的创新工具，用于创作和实验。然而，控制生成过程以实现特定的期望结果仍然是一个重大挑战。即使文本提示中的微小变化，结合相同的随机种子，也可能极大地改变生成的作品。在本文中，我们探索了现有文本到音乐扩散模型在乐器编辑方面的应用。具体来说，对于现有音频轨道，我们旨在利用预训练的文本到音乐扩散模型来编辑乐器，同时保留底层内容。基于模型首先关注音频的整体结构或内容，然后添加乐器信息，最后细化质量的洞察，我们展示了通过乐器分类器识别并选择一个精心选择的中间时间步，可以在保留原始作品内容和实现所需音色之间取得平衡。我们的方法不需要对文本到音乐扩散模型进行额外的训练，也不会影响生成过程的速度。", "summary": "本文提出Diff-TONE，一种利用现有文本到音乐扩散模型进行乐器编辑的方法。通过观察模型生成过程（先结构后乐器再细化），并结合乐器分类器选择最佳中间时间步，该方法能够在不额外训练和不牺牲速度的前提下，实现对现有音频的乐器修改，同时有效平衡原始内容保留与目标音色转换。", "keywords": "文本到音乐, 扩散模型, 乐器编辑, 时间步优化, 音色控制", "comments": "该研究的创新之处在于利用扩散模型生成过程的内在特性（从结构到细节）进行优化，提出了一种无需额外训练的乐器编辑方法。这对于利用现有预训练模型进行精细控制具有重要意义，降低了应用门槛并保持了效率。其亮点在于通过“时间步优化”这一巧妙机制，解决了在保留内容的同时进行特定属性（乐器）编辑的难题。"}}
{"id": "2506.15093", "title": "Flexible Hardware-Enabled Guarantees for AI Compute", "authors": ["James Petrie", "Onni Aarne", "Nora Ammann", "David Dalrymple"], "summary": "As artificial intelligence systems become increasingly powerful, they pose\ngrowing risks to international security, creating urgent coordination\nchallenges that current governance approaches struggle to address without\ncompromising sensitive information or national security. We propose flexible\nhardware-enabled guarantees (flexHEGs), that could be integrated with AI\naccelerators to enable trustworthy, privacy-preserving verification and\nenforcement of claims about AI development. FlexHEGs consist of an auditable\nguarantee processor that monitors accelerator usage and a secure enclosure\nproviding physical tamper protection. The system would be fully open source\nwith flexible, updateable verification capabilities. FlexHEGs could enable\ndiverse governance mechanisms including privacy-preserving model evaluations,\ncontrolled deployment, compute limits for training, and automated safety\nprotocol enforcement. In this first part of a three part series, we provide a\ncomprehensive introduction of the flexHEG system, including an overview of the\ngovernance and security capabilities it offers, its potential development and\nadoption paths, and the remaining challenges and limitations it faces. While\ntechnically challenging, flexHEGs offer an approach to address emerging\nregulatory and international security challenges in frontier AI development.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15093v1", "AI": {"title_translation": "AI计算的灵活硬件保障", "tldr": "本文提出灵活硬件保障 (flexHEGs)，这是一种可集成到AI加速器中的系统，旨在实现可信、隐私保护的AI开发验证和执行，以应对日益增长的国际安全挑战。", "motivation": "随着人工智能系统日益强大，它们对国际安全构成越来越大的风险，带来了紧迫的协调挑战。当前的治理方法在不损害敏感信息或国家安全的情况下，难以有效应对这些挑战。", "method": "本文提出灵活硬件保障 (flexHEGs) 系统，该系统可集成到AI加速器中。flexHEGs由一个可审计的保障处理器组成，用于监控加速器使用情况，以及一个提供物理防篡改保护的安全外壳。该系统将完全开源，并具有灵活、可更新的验证能力。", "result": "flexHEGs能够实现可信、隐私保护的AI开发验证和执行，并支持多样化的治理机制，包括隐私保护模型评估、受控部署、训练计算限制和自动化安全协议执行。", "conclusion": "尽管技术上具有挑战性，但灵活硬件保障 (flexHEGs) 为解决前沿AI开发中出现的监管和国际安全挑战提供了一种方法。", "translation": "随着人工智能系统日益强大，它们对国际安全构成越来越大的风险，带来了紧迫的协调挑战，而当前的治理方法在不损害敏感信息或国家安全的情况下，难以有效应对。我们提出灵活硬件保障 (flexHEGs)，可将其集成到AI加速器中，以实现关于AI开发的可信、隐私保护的验证和执行。FlexHEGs由一个可审计的保障处理器组成，用于监控加速器使用情况，以及一个提供物理防篡改保护的安全外壳。该系统将完全开源，并具有灵活、可更新的验证能力。FlexHEGs可以实现多样化的治理机制，包括隐私保护模型评估、受控部署、训练计算限制和自动化安全协议执行。作为三部曲系列的第一部分，我们全面介绍了flexHEG系统，包括其提供的治理和安全能力概述、其潜在的开发和采用路径，以及其面临的剩余挑战和局限性。尽管技术上具有挑战性，但flexHEGs为解决前沿AI开发中出现的监管和国际安全挑战提供了一种方法。", "summary": "本文提出灵活硬件保障 (flexHEGs) 系统，旨在应对强大AI系统带来的国际安全风险和治理挑战。flexHEGs通过集成可审计的保障处理器和安全外壳到AI加速器中，实现对AI开发的可信、隐私保护的验证和执行。该系统开源且具有灵活的验证能力，能够支持多种治理机制，如隐私保护评估和计算限制。研究认为，flexHEGs为解决前沿AI的监管和安全挑战提供了一种创新途径。", "keywords": "AI安全, 硬件保障, 灵活治理, 隐私保护, 国际安全", "comments": "这项研究提出了一种创新且及时的方法，通过硬件层面的保障来解决AI发展中的国际安全和治理挑战。其强调的开源性和灵活性是其关键优势，有助于建立信任并适应不断变化的监管需求。然而，正如文中提到的，实现这一目标面临显著的技术挑战和潜在的采纳障碍，因为这需要硬件和软件层面的深度集成与协作。"}}
{"id": "2506.15227", "title": "Large Language Models for Unit Testing: A Systematic Literature Review", "authors": ["Quanjun Zhang", "Chunrong Fang", "Siqi Gu", "Ye Shang", "Zhenyu Chen", "Liang Xiao"], "summary": "Unit testing is a fundamental practice in modern software engineering, with\nthe aim of ensuring the correctness, maintainability, and reliability of\nindividual software components. Very recently, with the advances in Large\nLanguage Models (LLMs), a rapidly growing body of research has leveraged LLMs\nto automate various unit testing tasks, demonstrating remarkable performance\nand significantly reducing manual effort. However, due to ongoing explorations\nin the LLM-based unit testing field, it is challenging for researchers to\nunderstand existing achievements, open challenges, and future opportunities.\nThis paper presents the first systematic literature review on the application\nof LLMs in unit testing until March 2025. We analyze \\numpaper{} relevant\npapers from the perspectives of both unit testing and LLMs. We first categorize\nexisting unit testing tasks that benefit from LLMs, e.g., test generation and\noracle generation. We then discuss several critical aspects of integrating LLMs\ninto unit testing research, including model usage, adaptation strategies, and\nhybrid approaches. We further summarize key challenges that remain unresolved\nand outline promising directions to guide future research in this area.\nOverall, our paper provides a systematic overview of the research landscape to\nthe unit testing community, helping researchers gain a comprehensive\nunderstanding of achievements and promote future research. Our artifacts are\npublicly available at the GitHub repository:\nhttps://github.com/iSEngLab/AwesomeLLM4UT.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15227v1", "AI": {"title_translation": "大型语言模型在单元测试中的应用：一项系统性文献综述", "tldr": "对LLM在单元测试中应用的首次系统性综述。", "motivation": "由于大型语言模型（LLMs）在单元测试领域的应用仍在探索中，研究人员难以理解现有成就、开放挑战和未来机遇。", "method": "本文对截至2025年3月大型语言模型在单元测试中的应用进行了首次系统性文献综述。作者从单元测试和LLM的角度分析了相关论文，对受益于LLM的现有单元测试任务进行分类（例如测试生成和预言生成），讨论了将LLM整合到单元测试研究中的关键方面（包括模型使用、适应策略和混合方法），并总结了未解决的关键挑战并概述了指导未来研究的有前景的方向。", "result": "本综述将受益于LLM的单元测试任务进行了分类（如测试生成和预言生成），讨论了LLM集成到单元测试研究中的关键方面（包括模型使用、适应策略和混合方法），并总结了仍未解决的关键挑战，勾勒出指导未来研究的有前景方向。", "conclusion": "本文为单元测试社区提供了研究现状的系统性概述，帮助研究人员全面理解现有成就并促进未来的研究。", "translation": "单元测试是现代软件工程中的一项基本实践，旨在确保单个软件组件的正确性、可维护性和可靠性。最近，随着大型语言模型（LLMs）的进步，越来越多的研究利用LLMs自动化各种单元测试任务，展示出卓越的性能并显著减少了人工工作量。然而，由于基于LLM的单元测试领域仍在持续探索中，研究人员难以理解现有成就、开放挑战和未来机遇。本文首次对截至2025年3月LLMs在单元测试中的应用进行了系统性文献综述。我们从单元测试和LLMs的角度分析了相关论文。我们首先对受益于LLMs的现有单元测试任务进行分类，例如测试生成和预言生成。然后，我们讨论了将LLMs整合到单元测试研究中的几个关键方面，包括模型使用、适应策略和混合方法。我们进一步总结了仍未解决的关键挑战，并概述了指导该领域未来研究的有前景方向。总的来说，我们的论文为单元测试社区提供了研究现状的系统性概述，帮助研究人员全面理解现有成就并促进未来的研究。我们的研究成果可在GitHub仓库公开获取：https://github.com/iSEngLab/AwesomeLLM4UT。", "summary": "本文是首篇针对大型语言模型（LLMs）在单元测试中应用的系统性文献综述。鉴于LLM在此领域的快速发展，该综述旨在帮助研究人员理解现有进展、挑战和未来机遇。文章分类了LLM受益的单元测试任务（如测试生成和预言生成），探讨了LLM集成（模型使用、适应策略、混合方法）的关键方面，并总结了未解决的挑战和未来研究方向。该综述为单元测试社区提供了全面的研究概览。", "keywords": "大型语言模型, 单元测试, 系统性文献综述, 软件工程, LLMs", "comments": "该论文具有重要意义，因为它是在大型语言模型应用于单元测试这一快速发展领域中的首次系统性文献综述。它为研究人员提供了一个急需的结构化概览，其对任务的分类、集成策略的讨论以及对挑战和未来方向的识别，对于指导该领域的研究至关重要。研究成果的公开可用性也增加了其价值。"}}
{"id": "2506.15012", "title": "Context Matters: Learning Generalizable Rewards via Calibrated Features", "authors": ["Alexandra Forsey-Smerek", "Julie Shah", "Andreea Bobu"], "summary": "A key challenge in reward learning from human input is that desired agent\nbehavior often changes based on context. Traditional methods typically treat\neach new context as a separate task with its own reward function. For example,\nif a previously ignored stove becomes too hot to be around, the robot must\nlearn a new reward from scratch, even though the underlying preference for\nprioritizing safety over efficiency remains unchanged. We observe that context\ninfluences not the underlying preference itself, but rather the\n$\\textit{saliency}$--or importance--of reward features. For instance, stove\nheat affects the importance of the robot's proximity, yet the human's safety\npreference stays the same. Existing multi-task and meta IRL methods learn\ncontext-dependent representations $\\textit{implicitly}$--without distinguishing\nbetween preferences and feature importance--resulting in substantial data\nrequirements. Instead, we propose $\\textit{explicitly}$ modeling\ncontext-invariant preferences separately from context-dependent feature\nsaliency, creating modular reward representations that adapt to new contexts.\nTo achieve this, we introduce $\\textit{calibrated features}$--representations\nthat capture contextual effects on feature saliency--and present specialized\npaired comparison queries that isolate saliency from preference for efficient\nlearning. Experiments with simulated users show our method significantly\nimproves sample efficiency, requiring 10x fewer preference queries than\nbaselines to achieve equivalent reward accuracy, with up to 15% better\nperformance in low-data regimes (5-10 queries). An in-person user study (N=12)\ndemonstrates that participants can effectively teach their unique personal\ncontextual preferences using our method, enabling more adaptable and\npersonalized reward learning.", "comment": "30 pages, 21 figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15012v1", "AI": {"title_translation": "上下文很重要：通过校准特征学习可泛化的奖励", "tldr": "传统奖励学习在不同上下文下效率低下，因为它们混淆了不变的偏好和上下文相关的特征显著性。本文提出显式分离这两者，通过“校准特征”和专门查询来高效学习，显著提高数据效率并实现个性化奖励学习。", "motivation": "传统奖励学习方法在面对上下文变化时，将每个新上下文视为独立任务，需要从头学习奖励函数，效率低下。这是因为它们没有区分不变的底层偏好和随上下文变化的特征显著性，导致数据需求量大且难以泛化。", "method": "提出显式地将上下文不变的偏好与上下文相关的特征显著性分开建模，创建模块化的奖励表示。引入“校准特征”来捕捉上下文对特征显著性的影响，并设计专门的成对比较查询，以有效地将显著性与偏好分离进行学习。", "result": "与基线方法相比，该方法在模拟用户实验中显著提高了样本效率，达到相同奖励精度所需的偏好查询次数减少了10倍，在低数据量（5-10次查询）情况下性能提升高达15%。真人用户研究（N=12）表明，参与者可以有效地教授其独特的个人上下文偏好，从而实现更具适应性和个性化的奖励学习。", "conclusion": "通过显式分离上下文不变偏好和上下文依赖特征显著性，并引入校准特征，可以大幅提高奖励学习的样本效率和泛化能力，实现更适应和个性化的奖励学习。", "translation": "从人类输入中学习奖励的一个关键挑战是，期望的智能体行为常常会根据上下文而变化。传统方法通常将每个新上下文视为一个独立的任务，并拥有自己的奖励函数。例如，如果一个之前被忽略的炉子变得太热而无法靠近，机器人必须从头开始学习一个新的奖励，尽管优先考虑安全而非效率的底层偏好保持不变。我们观察到，上下文影响的不是底层偏好本身，而是奖励特征的“显著性”——即重要性。例如，炉子热度影响机器人靠近程度的重要性，但人类的安全偏好保持不变。现有的多任务和元IRL方法“隐式”学习上下文相关的表示——不区分偏好和特征重要性——导致大量的数据需求。相反，我们建议“显式”地将上下文不变的偏好与上下文相关的特征显著性分开建模，创建适应新上下文的模块化奖励表示。为了实现这一点，我们引入了“校准特征”——捕捉上下文对特征显著性影响的表示——并提出了专门的成对比较查询，以有效地将显著性与偏好分离进行学习。模拟用户实验表明，我们的方法显著提高了样本效率，达到相同奖励精度所需的偏好查询次数比基线方法少10倍，在低数据量（5-10次查询）情况下性能提升高达15%。一项真人用户研究（N=12）表明，参与者可以利用我们的方法有效地教授他们独特的个人上下文偏好，从而实现更具适应性和个性化的奖励学习。", "summary": "本文提出了一种新颖的奖励学习方法，旨在解决传统方法在上下文变化时效率低下的问题。作者观察到，上下文影响的是奖励特征的显著性而非底层偏好。因此，他们建议显式地将上下文不变的偏好与上下文相关的特征显著性分开建模，并引入“校准特征”和专门的查询方式。实验结果表明，该方法显著提高了样本效率，并能有效学习个性化的上下文偏好，使得奖励学习更具适应性和泛化能力。", "keywords": "奖励学习, 上下文, 特征显著性, 可泛化性, 样本效率", "comments": "这篇论文的创新点在于明确区分了奖励学习中的“偏好”和“特征显著性”这两个概念，并提出了一种模块化的方法来分别处理它们。这种分离使得奖励模型在面对新上下文时具有更好的泛化能力和数据效率。特别是在低数据量情境下的显著性能提升，以及通过真人用户研究验证其个性化学习能力，都凸显了其实用价值和潜在影响力。"}}
{"id": "2506.14948", "title": "Structured Moral Reasoning in Language Models: A Value-Grounded Evaluation Framework", "authors": ["Mohna Chakraborty", "Lu Wang", "David Jurgens"], "summary": "Large language models (LLMs) are increasingly deployed in domains requiring\nmoral understanding, yet their reasoning often remains shallow, and misaligned\nwith human reasoning. Unlike humans, whose moral reasoning integrates\ncontextual trade-offs, value systems, and ethical theories, LLMs often rely on\nsurface patterns, leading to biased decisions in morally and ethically complex\nscenarios. To address this gap, we present a value-grounded framework for\nevaluating and distilling structured moral reasoning in LLMs. We benchmark 12\nopen-source models across four moral datasets using a taxonomy of prompts\ngrounded in value systems, ethical theories, and cognitive reasoning\nstrategies. Our evaluation is guided by four questions: (1) Does reasoning\nimprove LLM decision-making over direct prompting? (2) Which types of\nvalue/ethical frameworks most effectively guide LLM reasoning? (3) Which\ncognitive reasoning strategies lead to better moral performance? (4) Can\nsmall-sized LLMs acquire moral competence through distillation? We find that\nprompting with explicit moral structure consistently improves accuracy and\ncoherence, with first-principles reasoning and Schwartz's + care-ethics\nscaffolds yielding the strongest gains. Furthermore, our supervised\ndistillation approach transfers moral competence from large to small models\nwithout additional inference cost. Together, our results offer a scalable path\ntoward interpretable and value-grounded models.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.14948v1", "AI": {"title_translation": "语言模型中的结构化道德推理：一个基于价值的评估框架", "tldr": "本文提出了一个基于价值的框架，用于评估和蒸馏大型语言模型中的结构化道德推理，并发现明确的道德结构提示和监督蒸馏可以显著提高模型的道德表现和可解释性。", "motivation": "大型语言模型（LLMs）在需要道德理解的领域中部署时，其推理能力往往是肤浅的，且与人类推理不一致，经常依赖表面模式导致有偏见的决策。", "method": "提出了一个基于价值的框架来评估和蒸馏LLMs中的结构化道德推理。使用基于价值系统、伦理理论和认知推理策略的提示分类法，在四个道德数据集上对12个开源模型进行了基准测试。通过监督蒸馏方法将道德能力从大型模型转移到小型模型。", "result": "明确的道德结构提示一致性地提高了准确性和连贯性；第一性原理推理和Schwartz的+关怀伦理支架带来了最显著的提升；监督蒸馏方法可以在不增加额外推理成本的情况下，将道德能力从大型模型转移到小型模型。", "conclusion": "他们的结果为实现可解释且基于价值的模型提供了一条可扩展的路径。", "translation": "大型语言模型（LLM）越来越多地部署在需要道德理解的领域，但其推理往往是肤浅的，且与人类推理不一致。与人类的道德推理整合了情境权衡、价值系统和伦理理论不同，LLM通常依赖表面模式，导致在道德和伦理复杂场景中做出有偏见的决策。为了解决这一差距，我们提出了一个基于价值的框架，用于评估和蒸馏LLM中的结构化道德推理。我们使用一个基于价值系统、伦理理论和认知推理策略的提示分类法，在四个道德数据集上对12个开源模型进行了基准测试。我们的评估由四个问题指导：（1）推理是否比直接提示更能改善LLM的决策？（2）哪种类型的价值/伦理框架最有效地指导LLM推理？（3）哪种认知推理策略能带来更好的道德表现？（4）小型LLM能否通过蒸馏获得道德能力？我们发现，使用明确的道德结构进行提示一致性地提高了准确性和连贯性，其中第一性原理推理和Schwartz的+关怀伦理支架带来了最显著的提升。此外，我们的监督蒸馏方法可以在不增加额外推理成本的情况下，将道德能力从大型模型转移到小型模型。总而言之，我们的结果为实现可解释且基于价值的模型提供了一条可扩展的路径。", "summary": "本文针对大型语言模型在道德理解方面的局限性，提出了一个基于价值的评估框架，以提升其结构化道德推理能力。研究通过对12个开源模型进行基准测试，并探索不同提示策略和蒸馏方法，发现明确的道德结构提示（特别是第一性原理和Schwartz的+关怀伦理支架）能显著提高模型的道德表现。同时，监督蒸馏技术成功地将道德能力从大型模型迁移到小型模型，为开发可解释且基于价值的语言模型提供了可行方案。", "keywords": "道德推理, 语言模型, 价值系统, 评估框架, 知识蒸馏", "comments": "这篇论文的创新点在于提出了一个“基于价值的评估框架”，专门针对LLM的道德推理进行结构化评估和蒸馏。它不仅指出了LLM道德推理的现有问题（肤浅、不一致），还提供了具体的解决方案，例如通过明确的道德结构提示和监督蒸馏来提升性能。特别值得注意的是，它证明了即使是小型模型也能通过蒸馏获得道德能力，这对于实际应用和部署具有重要意义，降低了成本并提高了可扩展性。"}}
{"id": "2506.15233", "title": "New Bounds and Constructions for Variable Packet-Error Coding", "authors": ["Xiangliang Kong", "Xin Wang", "Ron M. Roth", "Itzhak Tamo"], "summary": "In this paper, we consider the problem of variable packet-error coding, which\nemerges in network communication scenarios where a source transmits information\nto a destination through multiple disjoint paths. The objective is to design\ncodes with dynamic error-correcting capabilities that adapt to varying numbers\nof errors. Specifically, we first provide several bounds on the\nrate--distortion trade-off for general variable packet-error coding schemes.\nThen, we present two explicit constructions of variable packet-error coding\nschemes. The first construction uses higher-order MDS codes and provides a\ncoding scheme that achieves a better rate--distortion trade-off compared to\nknown results for general parameter regimes. The second construction is based\non a variant of the repetition code and yields a coding scheme with an optimal\nrate--distortion trade-off, with respect to our bound, for certain parameter\nregimes.", "comment": "20 pages, 3 figures, part of the work in this paper has been accepted\n  for presentation at the 2025 IEEE International Symposium on Information\n  Theory (ISIT)", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15233v1", "AI": {"title_translation": "可变分组差错编码的新界限和构造", "tldr": "本文研究了可变分组差错编码问题，提出了新的速率-失真权衡界限，并给出了两种显式构造方案，分别在不同参数范围内实现了更好的或最优的性能。", "motivation": "在网络通信场景中，信息通过多条不相交路径从源传输到目的地，需要设计具有动态纠错能力的代码以适应不同数量的错误。", "method": "首先，为一般的可变分组差错编码方案提供了几个速率-失真权衡的界限。然后，提出了两种显式的可变分组差错编码方案构造：第一种使用高阶MDS码；第二种基于重复码的变体。", "result": "第一种构造方案在一般参数范围内实现了比已知结果更好的速率-失真权衡。第二种构造方案在某些参数范围内实现了相对于所提出界限的最优速率-失真权衡。", "conclusion": "本文为可变分组差错编码提供了新的理论界限和实用的构造方法，显著提升了在网络通信中动态纠错码的性能。", "translation": "在本文中，我们考虑了可变分组差错编码问题，该问题出现在源通过多条不相交路径向目的地传输信息的网络通信场景中。目标是设计具有动态纠错能力的代码，以适应不同数量的错误。具体来说，我们首先为一般的可变分组差错编码方案提供了几个关于速率-失真权衡的界限。然后，我们提出了两种显式的可变分组差错编码方案构造。第一个构造使用高阶MDS码，并提供了一种编码方案，与已知结果相比，在一般参数范围内实现了更好的速率-失真权衡。第二个构造基于重复码的一种变体，并在某些参数范围内相对于我们的界限实现了最优的速率-失真权衡。", "summary": "本文研究了网络通信中的可变分组差错编码问题，旨在设计能适应动态错误数量的编码。作者首先提出了通用可变分组差错编码方案的速率-失真权衡界限，随后介绍了两种显式构造方案。其中一种利用高阶MDS码，在一般参数下实现了更优的速率-失真权衡；另一种基于重复码变体，在特定参数下达到了最优的速率-失真权衡。", "keywords": "可变分组差错编码, 速率-失真权衡, MDS码, 重复码, 网络通信", "comments": "本文在可变分组差错编码领域做出了重要贡献，不仅提供了新的理论界限，还提出了两种实用的构造方法。特别是，第二种构造方案在特定条件下实现了最优性能，这对于实际网络通信中的动态纠错具有重要意义。其创新性在于对速率-失真权衡的深入分析和基于MDS码与重复码变体的有效构造。"}}
{"id": "2506.14831", "title": "Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review", "authors": ["Céline Finet", "Stephane Da Silva Martins", "Jean-Bernard Hayet", "Ioannis Karamouzas", "Javad Amirian", "Sylvie Le Hégarat-Mascle", "Julien Pettré", "Emanuel Aldea"], "summary": "With the emergence of powerful data-driven methods in human trajectory\nprediction (HTP), gaining a finer understanding of multi-agent interactions\nlies within hand's reach, with important implications in areas such as\nautonomous navigation and crowd modeling. This survey reviews some of the most\nrecent advancements in deep learning-based multi-agent trajectory prediction,\nfocusing on studies published between 2020 and 2024. We categorize the existing\nmethods based on their architectural design, their input representations, and\ntheir overall prediction strategies, placing a particular emphasis on models\nevaluated using the ETH/UCY benchmark. Furthermore, we highlight key challenges\nand future research directions in the field of multi-agent HTP.", "comment": "30 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14831v1", "AI": {"title_translation": "多智能体人类轨迹预测的最新进展：一项综合综述", "tldr": "该综述全面回顾了2020年至2024年间多智能体人类轨迹预测领域基于深度学习的最新进展，并指出了挑战和未来方向。", "motivation": "随着强大的数据驱动方法在人类轨迹预测（HTP）中的出现，更好地理解多智能体交互变得可能，这在自主导航和人群建模等领域具有重要意义。因此，有必要对该领域的最新进展进行全面综述。", "method": "本综述回顾了2020年至2024年间发表的基于深度学习的多智能体轨迹预测的最新进展。研究方法根据其架构设计、输入表示和整体预测策略对现有方法进行分类，并特别关注使用ETH/UCY基准进行评估的模型。", "result": "该综述对多智能体人类轨迹预测领域基于深度学习的现有方法进行了系统分类，并强调了该领域的关键挑战和未来研究方向。", "conclusion": "本综述全面总结了多智能体人类轨迹预测的最新进展，并为未来的研究提供了方向。", "translation": "随着人类轨迹预测（HTP）领域强大的数据驱动方法的出现，更深入地理解多智能体交互已触手可及，这在自主导航和人群建模等领域具有重要意义。本综述回顾了基于深度学习的多智能体轨迹预测的一些最新进展，重点关注2020年至2024年间发表的研究。我们根据现有方法的架构设计、输入表示和整体预测策略对其进行分类，并特别强调使用ETH/UCY基准进行评估的模型。此外，我们还强调了多智能体HTP领域的关键挑战和未来研究方向。", "summary": "本文对2020年至2024年间多智能体人类轨迹预测领域基于深度学习的最新进展进行了全面综述。该综述根据架构、输入表示和预测策略对现有方法进行分类，并重点关注使用ETH/UCY基准评估的模型。此外，文章还指出了该领域的关键挑战和未来研究方向，对于自主导航和人群建模等应用具有重要意义。", "keywords": "多智能体, 人类轨迹预测, 深度学习, 综述, 交互", "comments": "这篇综述论文通过系统分类和总结2020-2024年的最新进展，为多智能体人类轨迹预测领域的研究人员提供了宝贵的资源。其创新性在于对方法的结构化分类以及对未来挑战和方向的明确指出，对于推动该领域的发展具有重要指导意义。"}}
{"id": "2506.15634", "title": "SR-NCL: an Area-/Energy-Efficient Resilient NCL Architecture Based on Selective Redundancy", "authors": ["Hasnain A. Ziad", "Alexander C. Bodoh", "Ashiq A. Sakib"], "summary": "Duplication-based redundancy schemes have proven to be effective in designing\nfully-resilient Quasi-delay Insensitive (QDI) asynchronous circuits. The\ncomplete resiliency, however, is accompanied by significant energy, latency,\nand area overhead. This paper presents a novel error-tolerant Null Convention\nLogic (NCL) architecture based on selective redundancy. Results demonstrate the\nefficacy of the proposed method in terms of area and energy utilization as\ncompared to existing duplication-based NCL designs, targeting an image\nprocessing application.", "comment": "5 pages. Accepted for publication in the Proceedings of IEEE ISCAS\n  2025", "cate": "cs.AR", "url": "http://arxiv.org/abs/2506.15634v1", "AI": {"title_translation": "SR-NCL: 一种基于选择性冗余的面积/能效弹性NCL架构", "tldr": "本文提出了一种基于选择性冗余的SR-NCL架构，旨在为准延迟不敏感异步电路提供错误容忍性，同时显著降低传统基于复制冗余方案带来的面积和能耗开销。", "motivation": "现有的基于复制的冗余方案在设计完全弹性的准延迟不敏感（QDI）异步电路时，会带来显著的能耗、延迟和面积开销，因此需要一种更高效的错误容忍架构。", "method": "本文提出了一种基于选择性冗余的新型错误容忍空约定逻辑（NCL）架构。", "result": "结果表明，与现有的基于复制的NCL设计相比，所提出的方法在面积和能耗利用方面更有效，并以图像处理应用为例进行了验证。", "conclusion": "基于选择性冗余的SR-NCL架构能有效降低弹性QDI异步电路的面积和能耗开销，优于传统的复制冗余方案。", "translation": "基于复制的冗余方案已被证明在设计完全弹性的准延迟不敏感（QDI）异步电路方面是有效的。然而，这种完全的弹性伴随着显著的能耗、延迟和面积开销。本文提出了一种基于选择性冗余的新型错误容忍空约定逻辑（NCL）架构。结果表明，与现有的基于复制的NCL设计相比，所提出的方法在面积和能耗利用方面是有效的，并以图像处理应用为例进行了验证。", "summary": "本文提出了一种名为SR-NCL的新型空约定逻辑（NCL）架构，该架构基于选择性冗余，旨在解决传统基于复制的冗余方案在准延迟不敏感（QDI）异步电路中带来的高能耗、延迟和面积开销问题。实验结果表明，与现有设计相比，SR-NCL在面积和能耗效率方面表现出显著优势，特别是在图像处理应用中。", "keywords": "选择性冗余, NCL, 异步电路, 弹性, 面积能耗效率", "comments": "该论文提出了一种创新的选择性冗余方法，有效解决了传统复制冗余在异步电路中导致的资源开销问题，对于设计高效、弹性的QDI电路具有重要意义。"}}
{"id": "2506.15377", "title": "Efficient and Generalizable Environmental Understanding for Visual Navigation", "authors": ["Ruoyu Wang", "Xinshu Li", "Chen Wang", "Lina Yao"], "summary": "Visual Navigation is a core task in Embodied AI, enabling agents to navigate\ncomplex environments toward given objectives. Across diverse settings within\nNavigation tasks, many necessitate the modelling of sequential data accumulated\nfrom preceding time steps. While existing methods perform well, they typically\nprocess all historical observations simultaneously, overlooking the internal\nassociation structure within the data, which may limit the potential for\nfurther improvements in task performance. We address this by examining the\nunique characteristics of Navigation tasks through the lens of causality,\nintroducing a causal framework to highlight the limitations of conventional\nsequential methods. Leveraging this insight, we propose Causality-Aware\nNavigation (CAN), which incorporates a Causal Understanding Module to enhance\nthe agent's environmental understanding capability. Empirical evaluations show\nthat our approach consistently outperforms baselines across various tasks and\nsimulation environments. Extensive ablations studies attribute these gains to\nthe Causal Understanding Module, which generalizes effectively in both\nReinforcement and Supervised Learning settings without computational overhead.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15377v1", "AI": {"title_translation": "视觉导航中高效且可泛化的环境理解", "tldr": "本文提出了一种名为因果感知导航（CAN）的新框架，通过引入因果理解模块来增强代理在视觉导航任务中的环境理解能力，该方法在多种任务和仿真环境中均优于现有基线。", "motivation": "现有视觉导航方法在处理历史观察数据时通常同时处理所有数据，忽视了数据内部的关联结构，这可能限制了任务性能的进一步提升。本文旨在通过因果关系视角解决这一问题，以突出传统序列方法的局限性。", "method": "本文引入了一个因果框架来分析导航任务的独特特性，并提出了一种名为因果感知导航（CAN）的方法。CAN包含一个因果理解模块（Causal Understanding Module），旨在增强代理的环境理解能力。该模块在强化学习和监督学习设置中均能有效泛化，且没有计算开销。", "result": "实验评估表明，本文提出的方法在各种任务和仿真环境中均持续优于现有基线。广泛的消融研究表明，这些性能提升归因于因果理解模块，该模块在强化学习和监督学习设置中均能有效泛化，且没有计算开销。", "conclusion": "通过引入因果理解模块，本文提出的因果感知导航（CAN）框架能够有效提升代理在视觉导航任务中的环境理解能力，并在多种复杂环境中展现出卓越的泛化性和性能，证明了因果关系在处理序列数据中的重要性。", "translation": "视觉导航是具身AI中的核心任务，它使代理能够导航复杂的环境以达到给定目标。在导航任务的各种设置中，许多任务需要对前一时间步积累的序列数据进行建模。虽然现有方法表现良好，但它们通常同时处理所有历史观察数据，忽略了数据内部的关联结构，这可能限制了任务性能的进一步提升。我们通过从因果关系的角度审视导航任务的独特特征来解决这个问题，引入一个因果框架来突出传统序列方法的局限性。利用这一洞察，我们提出了因果感知导航（CAN），它包含一个因果理解模块，以增强代理的环境理解能力。实证评估表明，我们的方法在各种任务和仿真环境中均持续优于基线。广泛的消融研究将这些增益归因于因果理解模块，该模块在强化学习和监督学习设置中均能有效泛化，且没有计算开销。", "summary": "本文针对视觉导航任务中现有方法忽视历史观察数据内部关联结构的问题，提出了一个基于因果框架的新方法——因果感知导航（CAN）。CAN通过引入一个因果理解模块，显著提升了代理的环境理解能力。实验结果表明，该方法在多种任务和仿真环境中均优于基线，并且因果理解模块在强化学习和监督学习设置中均展现出良好的泛化能力和计算效率。", "keywords": "视觉导航, 因果关系, 环境理解, 具身AI, 序列数据", "comments": "本文的创新之处在于将因果关系引入视觉导航任务，通过构建因果框架来解决现有方法对序列数据内部关联结构处理不足的问题。提出的因果理解模块不仅提升了导航性能，还在强化学习和监督学习环境下表现出良好的泛化性，且无额外计算开销，这对于具身AI领域具有重要意义。该研究为未来处理序列数据和构建更鲁棒的导航系统提供了新的视角。"}}
{"id": "2506.15437", "title": "Exploring Fast Fourier Transforms on the Tenstorrent Wormhole", "authors": ["Nick Brown", "Jake Davies", "Felix LeClair"], "summary": "Whilst numerous areas of computing have adopted the RISC-V Instruction Set\nArchitecture (ISA) wholesale in recent years, it is yet to become widespread in\nHPC. RISC-V accelerators offer a compelling option where the HPC community can\nbenefit from the specialisation offered by the open nature of the standard but\nwithout the extensive ecosystem changes required when adopting RISC-V CPUs. In\nthis paper we explore porting the Cooley-Tukey Fast Fourier Transform (FFT)\nalgorithm to the Tenstorrent Wormhole PCIe RISC-V based accelerator. Built upon\nTenstorrent's Tensix architecture, this technology decouples the movement of\ndata from compute, potentially offering increased control to the programmer.\nExploring different optimisation techniques to address the bottlenecks inherent\nin data movement, we demonstrate that for a 2D FFT whilst the Wormhole n300 is\nslower than a server-grade 24-core Xeon Platinum CPU, the Wormhole draws around\n8 times less power and consumes around 2.8 times less energy than the CPU when\ncomputing the Fourier transform.", "comment": "Author accepted version of paper submitted to RISC-V for HPC ISC\n  workshop 2025", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15437v1", "AI": {"title_translation": "在 Tenstorrent Wormhole 上探索快速傅里叶变换", "tldr": "本文探讨了将Cooley-Tukey快速傅里叶变换（FFT）算法移植到基于Tenstorrent Wormhole PCIe RISC-V的加速器上。研究发现，虽然Wormhole在计算2D FFT时比服务器级CPU慢，但其功耗降低了约8倍，能耗降低了约2.8倍。", "motivation": "近年来，RISC-V指令集架构（ISA）在计算的许多领域得到了广泛应用，但在高性能计算（HPC）领域尚未普及。RISC-V加速器为HPC社区提供了一个有吸引力的选择，可以从开放标准的专业化中受益，而无需在采用RISC-V CPU时进行大规模生态系统更改。", "method": "本文探索了将Cooley-Tukey快速傅里叶变换（FFT）算法移植到基于Tenstorrent Wormhole PCIe RISC-V的加速器上。该技术基于Tenstorrent的Tensix架构，将数据移动与计算解耦。论文中探索了不同的优化技术以解决数据移动固有的瓶颈。", "result": "对于2D FFT，Wormhole n300比服务器级24核Xeon Platinum CPU慢，但Wormhole在计算傅里叶变换时功耗降低了约8倍，能耗降低了约2.8倍。", "conclusion": "虽然Tenstorrent Wormhole RISC-V加速器在性能上可能不如高端CPU，但它在计算快速傅里叶变换时展现出显著的功耗和能耗优势，证明了RISC-V加速器在HPC中节能的潜力。", "translation": "尽管近年来许多计算领域已全面采用RISC-V指令集架构（ISA），但它尚未在高性能计算（HPC）领域普及。RISC-V加速器提供了一个引人注目的选择，HPC社区可以从开放标准的专业化中受益，而无需在采用RISC-V CPU时进行广泛的生态系统更改。在本文中，我们探索了将Cooley-Tukey快速傅里叶变换（FFT）算法移植到基于Tenstorrent Wormhole PCIe RISC-V的加速器上。该技术建立在Tenstorrent的Tensix架构之上，将数据移动与计算解耦，可能为程序员提供更大的控制权。通过探索不同的优化技术来解决数据移动中固有的瓶颈，我们证明，对于2D FFT，虽然Wormhole n300比服务器级24核Xeon Platinum CPU慢，但Wormhole在计算傅里叶变换时功耗降低了约8倍，能耗降低了约2.8倍。", "summary": "本文研究了将Cooley-Tukey快速傅里叶变换（FFT）算法移植到Tenstorrent Wormhole PCIe RISC-V加速器上，旨在探索RISC-V加速器在高性能计算（HPC）中的潜力。该加速器基于Tenstorrent的Tensix架构，其特点是将数据移动与计算解耦。研究通过优化数据移动瓶颈，展示了Wormhole在执行2D FFT时，尽管速度慢于服务器级Intel Xeon CPU，但在功耗和能耗方面分别实现了约8倍和2.8倍的显著降低。", "keywords": "快速傅里叶变换, RISC-V, Tenstorrent Wormhole, 高性能计算, 能效", "comments": "本文的创新点在于将FFT算法移植到RISC-V加速器上，并特别关注了Tenstorrent Wormhole架构中数据移动与计算解耦的特性。其重要性在于揭示了RISC-V加速器在HPC领域实现显著能效的潜力，即使在性能上可能不如传统CPU。主要局限性是其计算速度仍慢于高端服务器CPU。"}}
{"id": "2506.14817", "title": "Next-Generation Conflict Forecasting: Unleashing Predictive Patterns through Spatiotemporal Learning", "authors": ["Simon P. von der Maase"], "summary": "Forecasting violent conflict at high spatial and temporal resolution remains\na central challenge for both researchers and policymakers. This study presents\na novel neural network architecture for forecasting three distinct types of\nviolence -- state-based, non-state, and one-sided -- at the subnational\n(priogrid-month) level, up to 36 months in advance. The model jointly performs\nclassification and regression tasks, producing both probabilistic estimates and\nexpected magnitudes of future events. It achieves state-of-the-art performance\nacross all tasks and generates approximate predictive posterior distributions\nto quantify forecast uncertainty.\n  The architecture is built on a Monte Carlo Dropout Long Short-Term Memory\n(LSTM) U-Net, integrating convolutional layers to capture spatial dependencies\nwith recurrent structures to model temporal dynamics. Unlike many existing\napproaches, it requires no manual feature engineering and relies solely on\nhistorical conflict data. This design enables the model to autonomously learn\ncomplex spatiotemporal patterns underlying violent conflict.\n  Beyond achieving state-of-the-art predictive performance, the model is also\nhighly extensible: it can readily integrate additional data sources and jointly\nforecast auxiliary variables. These capabilities make it a promising tool for\nearly warning systems, humanitarian response planning, and evidence-based\npeacebuilding initiatives.", "comment": "33 pages, 9 figures, 3 tables. Presented at workshops hosted by PRIO,\n  AFK (German Association for Peace and Conflict Studies), CCEW (Bundeswehr\n  University Munich), Uppsala University, SODAS (University of Copenhagen) and\n  in briefings with UN agencies including UNIDIR, OCHA, and FAO", "cate": "stat.OT", "url": "http://arxiv.org/abs/2506.14817v1", "AI": {"title_translation": "下一代冲突预测：通过时空学习释放预测模式", "tldr": "一项新颖的神经网络模型，利用时空学习，无需手动特征工程，在次国家层面以高分辨率预测暴力冲突，实现了最先进的性能并量化了预测不确定性。", "motivation": "以高空间和时间分辨率预测暴力冲突对研究人员和决策者来说仍是一个核心挑战。", "method": "本研究提出了一种新颖的神经网络架构，用于预测三种不同类型的暴力（国家间、非国家和单方面）冲突，预测粒度达到次国家（priogrid-month）级别，提前期长达36个月。该模型联合执行分类和回归任务，生成未来事件的概率估计和预期量级。其架构基于蒙特卡洛Dropout长短期记忆（LSTM）U-Net，集成了卷积层以捕捉空间依赖性，并结合循环结构来建模时间动态。该模型无需手动特征工程，仅依赖历史冲突数据，能够自主学习暴力冲突背后的复杂时空模式。", "result": "该模型在所有任务中均实现了最先进的性能，并生成近似的预测后验分布以量化预测不确定性。除了达到最先进的预测性能外，该模型还具有高度可扩展性。", "conclusion": "该模型可以轻松集成额外的数据源并联合预测辅助变量。这些能力使其成为早期预警系统、人道主义响应规划和循证和平建设倡议的一个有前景的工具。", "translation": "以高空间和时间分辨率预测暴力冲突对研究人员和决策者来说仍然是一个核心挑战。本研究提出了一种新颖的神经网络架构，用于预测三种不同类型的暴力——国家间、非国家和单方面——在次国家（priogrid-month）级别，提前长达36个月。该模型联合执行分类和回归任务，生成未来事件的概率估计和预期量级。它在所有任务中均实现了最先进的性能，并生成近似的预测后验分布以量化预测不确定性。该架构建立在蒙特卡洛Dropout长短期记忆（LSTM）U-Net之上，集成了卷积层以捕捉空间依赖性，并结合循环结构来建模时间动态。与许多现有方法不同，它不需要手动特征工程，仅依赖历史冲突数据。这种设计使模型能够自主学习暴力冲突背后的复杂时空模式。除了实现最先进的预测性能外，该模型还具有高度可扩展性：它可以轻松集成额外的数据源并联合预测辅助变量。这些能力使其成为早期预警系统、人道主义响应规划和循证和平建设倡议的一个有前景的工具。", "summary": "本研究提出了一种新颖的神经网络架构，通过蒙特卡洛Dropout LSTM U-Net，结合卷积层和循环结构，实现了对次国家级别暴力冲突的高时空分辨率预测，预测提前期长达36个月。该模型无需手动特征工程，仅利用历史冲突数据自主学习复杂的时空模式，并在分类和回归任务中均达到最先进的性能，同时能量化预测不确定性。其高度可扩展性使其在早期预警和和平建设领域具有重要应用潜力。", "keywords": "冲突预测, 神经网络, 时空学习, LSTM U-Net, 暴力冲突", "comments": "这篇论文创新性地将蒙特卡洛Dropout LSTM U-Net应用于冲突预测，解决了传统方法需要手动特征工程的痛点，并通过联合分类和回归任务以及量化不确定性提升了预测的实用性。其在次国家层面的高分辨率预测对政策制定者具有重要价值。"}}
{"id": "2506.15191", "title": "Islanding Strategy for Smart Grids Oriented to Resilience Enhancement and Its Power Supply Range Optimization", "authors": ["Yanhong Luo", "Wenchao Meng", "Xi Zhu", "Andreas Elombo", "Hu Rong", "Bing Xie", "Tianwen Zhang"], "summary": "With the increasing prevalence of distributed generators, islanded operation\nbased on distributed generation is considered a vital means to enhance the\nreliability and resilience of smart grids. This paper investigates the main\nfactors in islanding partition of smart grids and establishes a mathematical\nmodel for islanding division. A method to determine the maximum power supply\nrange of distributed energy resources (DERs) based on the reachability matrix\nand power circle algorithm is proposed to improve computational efficiency. A\ndynamic programming method based on breadth-first search (BFS) is used to solve\nthe islanding partition scheme, and a region correction method is applied to\nmodify the maximum power supply area by considering controllable loads and\nprioritizing critical load restoration, thereby enhancing system resilience.\nFinally, simulation results verify the effectiveness of the proposed algorithm\nin improving smart grid resilience.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15191v1", "AI": {"title_translation": "智能电网面向韧性提升的孤岛策略及其供电范围优化", "tldr": "本文提出了一种智能电网孤岛策略，通过优化分布式电源供电范围和考虑可控负荷，增强电网韧性。", "motivation": "随着分布式发电的日益普及，基于分布式发电的孤岛运行被认为是提高智能电网可靠性和韧性的重要手段。", "method": "研究了智能电网孤岛划分的主要因素并建立了数学模型。提出了一种基于可达矩阵和功率圆算法的分布式能源最大供电范围确定方法，以提高计算效率。采用基于广度优先搜索（BFS）的动态规划方法求解孤岛划分方案，并应用区域校正方法，通过考虑可控负荷和优先恢复关键负荷来修改最大供电区域。", "result": "仿真结果验证了所提算法在提高智能电网韧性方面的有效性。", "conclusion": "所提出的孤岛策略和优化方法能够有效提高智能电网的韧性。", "translation": "随着分布式发电机的日益普及，基于分布式发电的孤岛运行被认为是提高智能电网可靠性和韧性的重要手段。本文研究了智能电网孤岛划分的主要因素，并建立了孤岛划分的数学模型。提出了一种基于可达矩阵和功率圆算法的分布式能源（DERs）最大供电范围确定方法，以提高计算效率。采用基于广度优先搜索（BFS）的动态规划方法求解孤岛划分方案，并应用区域校正方法，通过考虑可控负荷和优先恢复关键负荷来修改最大供电区域，从而增强系统韧性。最后，仿真结果验证了所提算法在提高智能电网韧性方面的有效性。", "summary": "本文旨在通过优化孤岛策略来提升智能电网的韧性。研究了孤岛划分的关键因素并建立了数学模型，提出了一种结合可达矩阵和功率圆算法的分布式能源最大供电范围确定方法以提高效率。同时，利用动态规划和广度优先搜索求解孤岛划分方案，并通过区域校正方法考虑可控负荷和关键负荷的优先恢复，进一步增强系统韧性。仿真结果证实了该算法的有效性。", "keywords": "智能电网, 孤岛策略, 韧性提升, 分布式发电, 供电范围优化", "comments": "本文的创新点在于结合了可达矩阵和功率圆算法来优化分布式能源的供电范围，并通过区域校正方法考虑了可控负荷和关键负荷的优先恢复，这对于实际智能电网的韧性提升具有重要意义。该研究为智能电网在故障或极端事件下的可靠运行提供了新的思路和方法。"}}
{"id": "2506.15235", "title": "Enhancing eLoran Timing Accuracy via Machine Learning with Meteorological and Terrain Data", "authors": ["Taewon Kang", "Seunghyeon Park", "Pyo-Woong Son", "Jiwon Seo"], "summary": "The vulnerabilities of global navigation satellite systems (GNSS) to signal\ninterference have increased the demand for complementary positioning,\nnavigation, and timing (PNT) systems. To address this, South Korea has decided\nto deploy an enhanced long-range navigation (eLoran) system as a complementary\nPNT solution. Similar to GNSS, eLoran provides highly accurate timing\ninformation, which is essential for applications such as telecommunications,\nfinancial systems, and power distribution. However, the primary sources of\nerror for GNSS and eLoran differ. For eLoran, the main source of error is\nsignal propagation delay over land, known as the additional secondary factor\n(ASF). This delay, influenced by ground conductivity and weather conditions\nalong the signal path, is challenging to predict and mitigate. In this paper,\nwe measure the time difference (TD) between GPS and eLoran using a time\ninterval counter and analyze the correlations between eLoran/GPS TD and eleven\nmeteorological factors. Accurate estimation of eLoran/GPS TD could enable\neLoran to achieve timing accuracy comparable to that of GPS. We propose two\nestimation models for eLoran/GPS TD and compare their performance with existing\nTD estimation methods. The proposed WLR-AGRNN model captures the linear\nrelationships between meteorological factors and eLoran/GPS TD using weighted\nlinear regression (WLR) and models nonlinear relationships between outputs from\nexpert networks through an anisotropic general regression neural network\n(AGRNN). The model incorporates terrain elevation to appropriately weight\nmeteorological data, as elevation influences signal propagation delay.\nExperimental results based on four months of data demonstrate that the\nWLR-AGRNN model outperforms other models, highlighting its effectiveness in\nimproving eLoran/GPS TD estimation accuracy.", "comment": "Submitted to IEEE Access", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15235v1", "AI": {"title_translation": "基于机器学习结合气象和地形数据提高eLoran授时精度", "tldr": "本文提出WLR-AGRNN模型，结合气象和地形数据，利用机器学习方法有效提高了eLoran系统的授时精度，使其接近GPS水平。", "motivation": "全球导航卫星系统（GNSS）易受信号干扰，导致对互补定位、导航和授时（PNT）系统（如eLoran）的需求增加。eLoran能提供高精度授时信息，但其主要误差源是陆地信号传播延迟（ASF），受地面电导率和天气条件影响，难以预测和缓解。准确估计eLoran/GPS时间差(TD)是提高eLoran授时精度到GPS水平的关键。", "method": "研究测量了GPS和eLoran之间的时间差(TD)，并分析了eLoran/GPS TD与十一个气象因素之间的相关性。提出并比较了两种eLoran/GPS TD估算模型与现有方法。其中，WLR-AGRNN模型利用加权线性回归(WLR)捕捉气象因素与eLoran/GPS TD之间的线性关系，并通过各向异性广义回归神经网络(AGRNN)建模专家网络输出之间的非线性关系。该模型还结合了地形高程数据，以适当加权气象数据，因为高程会影响信号传播延迟。", "result": "基于四个月的数据进行的实验结果表明，WLR-AGRNN模型优于其他现有模型，有效提高了eLoran/GPS TD估算精度。", "conclusion": "WLR-AGRNN模型在提高eLoran/GPS TD估算精度方面表现出显著效果，有助于eLoran达到与GPS相当的授时精度。", "translation": "全球导航卫星系统（GNSS）对信号干扰的脆弱性增加了对互补定位、导航和授时（PNT）系统的需求。为解决此问题，韩国决定部署增强型远程导航（eLoran）系统作为互补PNT解决方案。与GNSS类似，eLoran提供高精度授时信息，这对于电信、金融系统和电力分配等应用至关重要。然而，GNSS和eLoran的主要误差来源不同。对于eLoran，主要的误差来源是陆地上的信号传播延迟，称为附加二次因子（ASF）。这种延迟受地面电导率和信号路径上的天气条件影响，难以预测和缓解。在本文中，我们使用时间间隔计数器测量GPS和eLoran之间的时间差（TD），并分析eLoran/GPS TD与十一个气象因素之间的相关性。准确估算eLoran/GPS TD可以使eLoran达到与GPS相当的授时精度。我们提出了两种eLoran/GPS TD估算模型，并将其性能与现有TD估算方法进行比较。所提出的WLR-AGRNN模型使用加权线性回归（WLR）捕捉气象因素与eLoran/GPS TD之间的线性关系，并通过各向异性广义回归神经网络（AGRNN）建模专家网络输出之间的非线性关系。该模型结合了地形高程以适当加权气象数据，因为高程会影响信号传播延迟。基于四个月数据的实验结果表明，WLR-AGRNN模型优于其他模型，突出了其在提高eLoran/GPS TD估算精度方面的有效性。", "summary": "本文针对eLoran系统因信号传播延迟（ASF）导致的授时误差问题，提出了一种基于机器学习的WLR-AGRNN模型。该模型结合了气象数据和地形高程，通过加权线性回归处理线性关系，并利用各向异性广义回归神经网络处理非线性关系，以准确估算eLoran与GPS之间的时间差(TD)。实验结果表明，该模型显著优于现有方法，有效提升了eLoran的授时精度，使其有望达到GPS水平。", "keywords": "eLoran, 授时精度, 机器学习, 气象数据, 地形高程", "comments": "该研究的创新之处在于将机器学习（特别是WLR和AGRNN）应用于eLoran的ASF误差校正，并巧妙地整合了气象和地形数据，以更全面地捕捉信号传播延迟的影响。这对于提高eLoran作为GNSS互补PNT系统的实用性和可靠性具有重要意义。"}}
{"id": "2506.14787", "title": "Topology-Aware and Highly Generalizable Deep Reinforcement Learning for Efficient Retrieval in Multi-Deep Storage Systems", "authors": ["Funing Li", "Yuan Tian", "Ruben Noortwyck", "Jifeng Zhou", "Liming Kuang", "Robert Schulz"], "summary": "In modern industrial and logistics environments, the rapid expansion of fast\ndelivery services has heightened the demand for storage systems that combine\nhigh efficiency with increased density. Multi-deep autonomous vehicle storage\nand retrieval systems (AVS/RS) present a viable solution for achieving greater\nstorage density. However, these systems encounter significant challenges during\nretrieval operations due to lane blockages. A conventional approach to mitigate\nthis issue involves storing items with homogeneous characteristics in a single\nlane, but this strategy restricts the flexibility and adaptability of\nmulti-deep storage systems.\n  In this study, we propose a deep reinforcement learning-based framework to\naddress the retrieval problem in multi-deep storage systems with heterogeneous\nitem configurations. Each item is associated with a specific due date, and the\nobjective is to minimize total tardiness. To effectively capture the system's\ntopology, we introduce a graph-based state representation that integrates both\nitem attributes and the local topological structure of the multi-deep\nwarehouse. To process this representation, we design a novel neural network\narchitecture that combines a Graph Neural Network (GNN) with a Transformer\nmodel. The GNN encodes topological and item-specific information into\nembeddings for all directly accessible items, while the Transformer maps these\nembeddings into global priority assignments. The Transformer's strong\ngeneralization capability further allows our approach to be applied to storage\nsystems with diverse layouts. Extensive numerical experiments, including\ncomparisons with heuristic methods, demonstrate the superiority of the proposed\nneural network architecture and the effectiveness of the trained agent in\noptimizing retrieval tardiness.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14787v1", "AI": {"title_translation": "拓扑感知和高度可泛化的深度强化学习，用于多深度存储系统中的高效检索", "tldr": "提出一种结合GNN和Transformer的深度强化学习框架，解决多深度存储系统中的检索问题，以最小化总迟到时间，并展示了其优越性和泛化能力。", "motivation": "现代工业和物流环境中，快速配送服务对高效、高密度存储系统需求增加。多深度自动车辆存储和检索系统（AVS/RS）能提高存储密度，但检索操作中存在巷道堵塞问题，传统方法限制了系统灵活性。", "method": "提出一个基于深度强化学习的框架，处理异构物品配置的多深度存储系统中的检索问题，目标是最小化总迟到时间。引入图基状态表示，结合物品属性和局部拓扑结构。设计新的神经网络架构，结合图神经网络（GNN）和Transformer模型。GNN编码拓扑和物品信息，Transformer将嵌入映射到全局优先级分配。", "result": "广泛的数值实验，包括与启发式方法的比较，证明了所提出的神经网络架构的优越性。训练好的智能体在优化检索迟到时间方面是有效的。", "conclusion": "所提出的结合GNN和Transformer的深度强化学习框架，通过其拓扑感知能力和强大的泛化能力，有效解决了多深度存储系统中的检索问题，显著优化了检索迟到时间。", "translation": "在现代工业和物流环境中，快速配送服务的迅速发展提高了对兼具高效率和高密度的存储系统的需求。多深度自主车辆存储和检索系统（AVS/RS）为实现更大的存储密度提供了一个可行的解决方案。然而，这些系统在检索操作中由于巷道堵塞而面临重大挑战。缓解此问题的传统方法是将具有同质特性的物品存储在单个巷道中，但这种策略限制了多深度存储系统的灵活性和适应性。\n在这项研究中，我们提出了一个基于深度强化学习的框架，以解决具有异构物品配置的多深度存储系统中的检索问题。每个物品都关联一个特定的到期日，目标是最小化总迟到时间。为了有效地捕获系统拓扑结构，我们引入了一种基于图的状态表示，该表示集成了物品属性和多深度仓库的局部拓扑结构。为了处理这种表示，我们设计了一种新颖的神经网络架构，它结合了图神经网络（GNN）和Transformer模型。GNN将拓扑和物品特定信息编码为所有可直接访问物品的嵌入，而Transformer将这些嵌入映射到全局优先级分配。Transformer强大的泛化能力进一步使我们的方法能够应用于具有不同布局的存储系统。广泛的数值实验，包括与启发式方法的比较，证明了所提出的神经网络架构的优越性以及训练好的智能体在优化检索迟到时间方面的有效性。", "summary": "本研究提出一种基于深度强化学习的框架，解决多深度存储系统中异构物品的检索问题，旨在最小化总迟到时间。通过引入结合物品属性和局部拓扑结构的图基状态表示，并设计融合GNN和Transformer的新型神经网络架构，该方法能够有效捕获系统拓扑并实现强大的泛化能力。实验结果表明，该方法在优化检索迟到时间方面优于传统启发式方法。", "keywords": "深度强化学习, 多深度存储系统, 检索, 图神经网络, Transformer", "comments": "创新点在于结合GNN和Transformer处理多深度存储系统的复杂拓扑结构和物品属性，实现高效检索并具有强大的泛化能力。重要性体现在其为解决高密度存储系统中的检索效率和灵活性问题提供了新颖且有效的深度学习解决方案。局限性方面，抽象中未明确提及，但通常深度强化学习的训练成本和实际部署的复杂性可能是潜在挑战。"}}
{"id": "2506.15107", "title": "I Know You're Listening: Adaptive Voice for HRI", "authors": ["Paige Tuttösí"], "summary": "While the use of social robots for language teaching has been explored, there\nremains limited work on a task-specific synthesized voices for language\nteaching robots. Given that language is a verbal task, this gap may have severe\nconsequences for the effectiveness of robots for language teaching tasks. We\naddress this lack of L2 teaching robot voices through three contributions: 1.\nWe address the need for a lightweight and expressive robot voice. Using a\nfine-tuned version of Matcha-TTS, we use emoji prompting to create an\nexpressive voice that shows a range of expressivity over time. The voice can\nrun in real time with limited compute resources. Through case studies, we found\nthis voice more expressive, socially appropriate, and suitable for long periods\nof expressive speech, such as storytelling. 2. We explore how to adapt a\nrobot's voice to physical and social ambient environments to deploy our voices\nin various locations. We found that increasing pitch and pitch rate in noisy\nand high-energy environments makes the robot's voice appear more appropriate\nand makes it seem more aware of its current environment. 3. We create an\nEnglish TTS system with improved clarity for L2 listeners using known\nlinguistic properties of vowels that are difficult for these listeners. We used\na data-driven, perception-based approach to understand how L2 speakers use\nduration cues to interpret challenging words with minimal tense (long) and lax\n(short) vowels in English. We found that the duration of vowels strongly\ninfluences the perception for L2 listeners and created an \"L2 clarity mode\" for\nMatcha-TTS that applies a lengthening to tense vowels while leaving lax vowels\nunchanged. Our clarity mode was found to be more respectful, intelligible, and\nencouraging than base Matcha-TTS while reducing transcription errors in these\nchallenging tense/lax minimal pairs.", "comment": "PhD Thesis Simon Fraser University https://summit.sfu.ca/item/39353\n  Read the Room: Adapting a Robot's Voice to Ambient and Social Contexts IROS\n  23 Mmm whatcha say? Uncovering distal and proximal context effects in first\n  and second-language word perception using psychophysical reverse correlation\n  INTERSPEECH 24 Emojivoice: Towards long-term controllable expressivity in\n  robot speech RO-MAN 25", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15107v1", "AI": {"title_translation": "我知道你在听：用于人机交互的自适应语音", "tldr": "为语言教学机器人开发了富有表现力、环境自适应且针对第二语言学习者更清晰的语音系统。", "motivation": "语言教学机器人缺乏任务特定的合成语音研究，这可能严重影响其教学效果。", "method": "1. 基于微调的 Matcha-TTS 和表情符号提示，开发了轻量级、富有表现力的机器人语音。2. 研究了如何根据物理和社会环境调整机器人语音（如调整音高和音高率）。3. 基于数据驱动的感知方法，利用元音语言学特性，为第二语言学习者创建了提高清晰度的英语文本转语音（TTS）系统，特别是“L2清晰模式”。", "result": "1. 所开发的富有表现力的语音更具表现力、社会适宜性，并适合长时间表达性讲话。2. 在嘈杂和高能量环境中提高音高和音高率，使机器人语音更适宜并显得更具环境感知力。3. “L2清晰模式”比基础 Matcha-TTS 更受尊重、更易懂、更具鼓励性，并减少了紧元音/松元音最小对的转录错误。", "conclusion": "本文通过开发富有表现力、环境自适应且针对第二语言学习者提高清晰度的语音系统，成功弥补了语言教学机器人语音的不足，显著提升了机器人在该任务中的有效性。", "translation": "尽管社交机器人在语言教学中的应用已有所探索，但针对语言教学机器人的任务特定合成语音研究仍然有限。鉴于语言是一项口头任务，这一空白可能会严重影响机器人在语言教学任务中的有效性。我们通过三项贡献来解决第二语言教学机器人语音的不足：1. 我们解决了对轻量级且富有表现力的机器人语音的需求。通过微调版的 Matcha-TTS，我们使用表情符号提示来创建一种富有表现力的语音，该语音随着时间的推移展现出多种表达范围。该语音可以在有限的计算资源下实时运行。通过案例研究，我们发现这种语音更具表现力、社会适宜性，并且适合长时间的表达性讲话，例如讲故事。2. 我们探索了如何使机器人的语音适应物理和社会环境，以便在各种地点部署我们的语音。我们发现，在嘈杂和高能量环境中提高音高和音高率，可以使机器人的语音显得更适宜，并使其似乎更了解当前环境。3. 我们创建了一个英语文本转语音（TTS）系统，该系统利用第二语言听众难以区分的已知元音语言特性，提高了对他们的清晰度。我们采用数据驱动、基于感知的方法来理解第二语言使用者如何使用持续时间线索来解释英语中带有最小紧（长）和松（短）元音的挑战性词语。我们发现元音的持续时间强烈影响第二语言听众的感知，并为 Matcha-TTS 创建了一个“L2清晰模式”，该模式对紧元音进行延长，同时保持松元音不变。我们的清晰模式被发现比基础 Matcha-TTS 更受尊重、更易懂、更具鼓励性，同时减少了这些具有挑战性的紧元音/松元音最小对的转录错误。", "summary": "本文旨在解决语言教学机器人缺乏任务特定合成语音的问题。作者提出了三项贡献：首先，开发了一种基于微调 Matcha-TTS 和表情符号提示的轻量级、富有表现力的机器人语音，适用于长时间表达性讲话。其次，研究了如何使机器人语音适应物理和社会环境，发现在嘈杂环境中提高音高和音高率可增强语音的适宜性。最后，为第二语言听众创建了一个改进清晰度的英语文本转语音系统，即“L2清晰模式”，该模式通过调整元音持续时间，提高了语音的易懂性、尊重性，并减少了特定音对的转录错误。", "keywords": "自适应语音, 人机交互, 语言教学, Matcha-TTS, 第二语言清晰度", "comments": "该论文创新性地解决了人机交互中语言教学语音的关键问题。其多方面的研究方法——专注于表达力、环境适应性和针对第二语言学习者的清晰度——极具价值。特别是针对第二语言清晰度的基于数据驱动、感知的方法，即“L2清晰模式”，代表了使机器人语言教师对非母语使用者更有效和用户友好方面迈出了重要一步。"}}
{"id": "2506.15182", "title": "Classification of Multi-Parametric Body MRI Series Using Deep Learning", "authors": ["Boah Kim", "Tejas Sudharshan Mathai", "Kimberly Helm", "Peter A. Pinto", "Ronald M. Summers"], "summary": "Multi-parametric magnetic resonance imaging (mpMRI) exams have various series\ntypes acquired with different imaging protocols. The DICOM headers of these\nseries often have incorrect information due to the sheer diversity of protocols\nand occasional technologist errors. To address this, we present a deep\nlearning-based classification model to classify 8 different body mpMRI series\ntypes so that radiologists read the exams efficiently. Using mpMRI data from\nvarious institutions, multiple deep learning-based classifiers of ResNet,\nEfficientNet, and DenseNet are trained to classify 8 different MRI series, and\ntheir performance is compared. Then, the best-performing classifier is\nidentified, and its classification capability under the setting of different\ntraining data quantities is studied. Also, the model is evaluated on the\nout-of-training-distribution datasets. Moreover, the model is trained using\nmpMRI exams obtained from different scanners in two training strategies, and\nits performance is tested. Experimental results show that the DenseNet-121\nmodel achieves the highest F1-score and accuracy of 0.966 and 0.972 over the\nother classification models with p-value$<$0.05. The model shows greater than\n0.95 accuracy when trained with over 729 studies of the training data, whose\nperformance improves as the training data quantities grew larger. On the\nexternal data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and\n0.810 accuracy for each. These results indicate that in both the internal and\nexternal datasets, the DenseNet-121 model attains high accuracy for the task of\nclassifying 8 body MRI series types.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15182v1", "AI": {"title_translation": "基于深度学习的多参数身体MRI序列分类", "tldr": "该研究提出了一种基于深度学习的模型，用于分类多参数身体MRI序列，以解决DICOM头信息不准确的问题，并提高了放射科医生的工作效率。", "motivation": "多参数磁共振成像（mpMRI）检查的DICOM头信息常因协议多样性或技术人员错误而导致不准确，这影响了放射科医生高效阅读检查。本研究旨在通过深度学习解决此问题。", "method": "研究训练并比较了ResNet、EfficientNet和DenseNet等多种深度学习分类器，用于分类8种不同的身体mpMRI系列类型。选定表现最佳的模型后，进一步研究了其在不同训练数据量下的分类能力，并评估了其在训练分布之外和来自不同扫描仪的数据集上的性能。", "result": "实验结果显示，DenseNet-121模型表现最佳，F1分数和准确率分别为0.966和0.972（p值<0.05）。当训练数据量超过729项时，模型准确率保持在0.95以上，且随数据量增加性能提升。在外部数据集DLDS和CPTAC-UCEC上，模型准确率分别为0.872和0.810。", "conclusion": "DenseNet-121模型在内部和外部数据集上均能高准确率地完成8种身体MRI序列类型的分类任务。", "translation": "多参数磁共振成像（mpMRI）检查有多种系列类型，通过不同的成像方案获取。由于协议的多样性和偶尔的技术人员错误，这些序列的DICOM头信息经常不正确。为了解决这个问题，我们提出了一种基于深度学习的分类模型，用于分类8种不同的身体mpMRI系列类型，以便放射科医生高效地阅读检查。使用来自不同机构的mpMRI数据，训练了多个基于深度学习的分类器，包括ResNet、EfficientNet和DenseNet，以分类8种不同的MRI序列，并比较了它们的性能。然后，识别出性能最佳的分类器，并研究了其在不同训练数据量设置下的分类能力。此外，该模型还在训练分布之外的数据集上进行了评估。此外，该模型使用从不同扫描仪获取的mpMRI检查，采用两种训练策略进行训练，并测试了其性能。实验结果表明，DenseNet-121模型在其他分类模型中实现了最高的F1分数（0.966）和准确率（0.972），p值<0.05。当使用超过729项训练数据进行训练时，该模型显示出大于0.95的准确率，并且随着训练数据量的增加，其性能也随之提高。在DLDS和CPTAC-UCEC数据集的外部数据上，该模型分别获得了0.872和0.810的准确率。这些结果表明，在内部和外部数据集上，DenseNet-121模型在分类8种身体MRI系列类型的任务中均达到了高准确率。", "summary": "本研究提出并评估了一种基于深度学习的分类模型，用于自动识别多参数身体MRI（mpMRI）序列类型，以纠正DICOM头信息中的常见错误并提高放射科医生的工作效率。研究比较了ResNet、EfficientNet和DenseNet等多种深度学习分类器，发现DenseNet-121在内部数据集上表现最佳，F1分数和准确率分别达到0.966和0.972。该模型在不同训练数据量下表现稳定，并在外部数据集上保持了较高的准确性，验证了其在复杂临床环境中的泛化能力。", "keywords": "深度学习, MRI分类, 多参数MRI, DenseNet, DICOM", "comments": "该论文解决了MRI临床实践中DICOM头信息不准确的实际问题，这对于提高放射科工作效率至关重要。通过比较多种先进的深度学习架构，并在不同数据量和外部数据集上进行严格评估，验证了其方法的鲁棒性。DenseNet-121模型在内部和外部数据集上均表现出高准确性，显示出其在实际部署中改善诊断效率和潜在减少错误的巨大潜力。"}}
{"id": "2506.15030", "title": "Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods", "authors": ["Drew Walker", "Swati Rajwal", "Sudeshna Das", "Snigdha Peddireddy", "Abeed Sarker"], "summary": "Social isolation and loneliness, which have been increasing in recent years\nstrongly contribute toward suicide rates. Although social isolation and\nloneliness are not currently recorded within the US National Violent Death\nReporting System's (NVDRS) structured variables, natural language processing\n(NLP) techniques can be used to identify these constructs in law enforcement\nand coroner medical examiner narratives. Using topic modeling to generate\nlexicon development and supervised learning classifiers, we developed\nhigh-quality classifiers (average F1: .86, accuracy: .82). Evaluating over\n300,000 suicides from 2002 to 2020, we identified 1,198 mentioning chronic\nsocial isolation. Decedents had higher odds of chronic social isolation\nclassification if they were men (OR = 1.44; CI: 1.24, 1.69, p<.0001), gay (OR =\n3.68; 1.97, 6.33, p<.0001), or were divorced (OR = 3.34; 2.68, 4.19, p<.0001).\nWe found significant predictors for other social isolation topics of recent or\nimpending divorce, child custody loss, eviction or recent move, and break-up.\nOur methods can improve surveillance and prevention of social isolation and\nloneliness in the United States.", "comment": "22 pages, 2 figures, 5 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15030v1", "AI": {"title_translation": "使用主题建模和文本分类方法识别NVDRS文本叙述中的社会隔离主题", "tldr": "本研究利用自然语言处理技术，通过主题建模和监督学习分类器，从美国国家暴力死亡报告系统（NVDRS）的文本叙述中识别社会隔离主题，并发现与慢性社会隔离相关的显著风险因素，以期改进监测和预防。", "motivation": "近年来，社会隔离和孤独感日益增加，并与自杀率密切相关。尽管美国国家暴力死亡报告系统（NVDRS）的结构化变量目前未记录社会隔离和孤独感，但这些信息可能存在于执法和验尸官的叙述文本中，因此需要开发方法来识别这些关键概念，以改进监测和预防。", "method": "研究使用了自然语言处理（NLP）技术。具体方法包括：1. 使用主题建模（topic modeling）进行词汇开发（lexicon development）。2. 开发监督学习分类器（supervised learning classifiers）。3. 在超过30万份2002年至2020年的自杀案例中评估这些分类器。", "result": "1. 开发了高质量的分类器（平均F1分数：0.86，准确率：0.82）。2. 在2002年至2020年的30多万例自杀事件中，识别出1198例提及慢性社会隔离。3. 发现慢性社会隔离的显著风险因素：男性（OR = 1.44）、同性恋者（OR = 3.68）、离婚者（OR = 3.34）有更高的慢性社会隔离分类几率。4. 发现其他社会隔离主题的显著预测因素，包括近期或即将离婚、失去子女监护权、被驱逐或近期搬家、以及分手。", "conclusion": "本研究开发的方法能够有效地从NVDRS文本叙述中识别社会隔离主题，并揭示了与自杀相关的社会隔离的显著风险因素。这些方法有望改进美国社会隔离和孤独感的监测和预防工作。", "translation": "近年来，社会隔离和孤独感日益加剧，并与自杀率密切相关。尽管美国国家暴力死亡报告系统（NVDRS）的结构化变量目前尚未记录社会隔离和孤独感，但自然语言处理（NLP）技术可用于在执法和验尸官的叙述文本中识别这些概念。通过使用主题建模生成词汇开发并结合监督学习分类器，我们开发了高质量的分类器（平均F1分数：0.86，准确率：0.82）。在评估2002年至2020年超过30万例自杀事件时，我们识别出1198例提及慢性社会隔离。如果死者是男性（OR = 1.44；CI：1.24，1.69，p<.0001）、同性恋者（OR = 3.68；1.97，6.33，p<.0001）或已离婚（OR = 3.34；2.68，4.19，p<.0001），则其被归类为慢性社会隔离的几率更高。我们还发现了其他社会隔离主题的显著预测因素，包括近期或即将离婚、失去子女监护权、被驱逐或近期搬家以及分手。我们的方法可以改善美国社会隔离和孤独感的监测和预防。", "summary": "本研究旨在利用自然语言处理技术，从美国国家暴力死亡报告系统（NVDRS）的文本叙述中识别社会隔离主题，以应对社会隔离和孤独感对自杀率的日益增长的影响。研究团队通过主题建模进行词汇开发并构建了高质量的监督学习分类器（F1: 0.86, 准确率: 0.82）。对2002年至2020年间超过30万例自杀事件的分析显示，有1198例提及慢性社会隔离。研究发现，男性、同性恋者和离婚者被归类为慢性社会隔离的几率更高，同时还识别出近期离婚、失去子女监护权、被驱逐或搬家、以及分手等其他社会隔离主题的预测因素。这些方法有望显著改善美国社会隔离和孤独感的监测和预防。", "keywords": "社会隔离, 自然语言处理, 自杀预防, 主题建模, 文本分类", "comments": "这篇论文的创新点在于将自然语言处理技术应用于非结构化文本数据，以识别与自杀相关的社会隔离主题，填补了传统结构化数据记录的空白。其重要性在于提供了一种有效工具，能够从大量叙述文本中提取关键信息，从而改进对高风险人群的识别和干预。研究结果揭示了与慢性社会隔离相关的特定人口统计学特征和生活事件，为自杀预防策略提供了宝贵的洞察。未来研究可以进一步探索这些分类器在不同人群和地域的泛化能力，以及如何将识别出的主题更直接地整合到公共卫生干预措施中。"}}
{"id": "2506.14812", "title": "Weak TransNet: A Petrov-Galerkin based neural network method for solving elliptic PDEs", "authors": ["Zhihang Xu", "Min Wang", "Zhu Wang"], "summary": "While deep learning has achieved remarkable success in solving partial\ndifferential equations (PDEs), it still faces significant challenges,\nparticularly when the PDE solutions have low regularity or singularities. To\naddress these issues, we propose the Weak TransNet (WTN) method, based on a\nPetrov-Galerkin formulation, for solving elliptic PDEs in this work, though its\nframework may extend to other classes of equations. Specifically, the neural\nfeature space defined by TransNet (Zhang et al., 2023) is used as the trial\nspace, while the test space is composed of radial basis functions. Since the\nsolution is expressed as a linear combination of trial functions, the\ncoefficients can be determined by minimizing the weak PDE residual via least\nsquares. Thus, this approach could help mitigate the challenges of\nnon-convexity and ill-conditioning that often arise in neural network training.\nFurthermore, the WTN method is extended to handle problems whose solutions\nexhibit multiscale features or possess sharp variations. Several numerical\nexperiments are presented to demonstrate the robustness and efficiency of the\nproposed methods.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14812v1", "AI": {"title_translation": "弱TransNet：一种基于Petrov-Galerkin的神经网络方法求解椭圆偏微分方程", "tldr": "提出一种基于Petrov-Galerkin的神经网络方法Weak TransNet (WTN) 求解椭圆PDE，能处理低正则性或奇点问题，并改善训练挑战。", "motivation": "深度学习在解决偏微分方程（PDEs）时面临挑战，特别是在PDE解具有低正则性或奇点时。", "method": "提出Weak TransNet (WTN) 方法，基于Petrov-Galerkin公式求解椭圆PDE。使用TransNet定义的神经特征空间作为试验空间，径向基函数组成测试空间。通过最小二乘法最小化弱PDE残差来确定系数，从而缓解神经网络训练中常见的非凸性和病态问题。该方法还扩展到处理具有多尺度特征或剧烈变化的解。", "result": "多个数值实验表明所提出方法的鲁棒性和效率。", "conclusion": "WTN方法能够有效解决椭圆PDE，特别是对于低正则性、奇点、多尺度或剧烈变化的解，并改善了神经网络训练的挑战。", "translation": "虽然深度学习在解决偏微分方程（PDEs）方面取得了显著成功，但它仍然面临重大挑战，特别是当PDE解具有低正则性或奇点时。为了解决这些问题，我们在这项工作中提出了一种基于Petrov-Galerkin公式的弱TransNet（WTN）方法，用于求解椭圆PDE，尽管其框架可能扩展到其他类型的方程。具体来说，TransNet（Zhang et al., 2023）定义的神经特征空间被用作试验空间，而测试空间由径向基函数组成。由于解表示为试验函数的线性组合，因此可以通过最小二乘法最小化弱PDE残差来确定系数。因此，这种方法可以帮助缓解神经网络训练中经常出现的非凸性和病态问题。此外，WTN方法还扩展到处理解表现出多尺度特征或具有剧烈变化的问题。本文通过几个数值实验来证明所提出方法的鲁棒性和效率。", "summary": "本文提出一种名为Weak TransNet (WTN) 的神经网络方法，它基于Petrov-Galerkin公式，旨在解决椭圆偏微分方程，特别是那些解具有低正则性、奇点、多尺度特征或剧烈变化的情况。WTN方法将TransNet定义的神经特征空间作为试验空间，径向基函数作为测试空间，并通过最小二乘法优化弱PDE残差来确定解的系数，从而有效缓解了传统神经网络训练中常见的非凸性和病态问题。数值实验验证了该方法的鲁棒性和效率。", "keywords": "偏微分方程, 神经网络, Petrov-Galerkin, 弱TransNet, 椭圆PDE", "comments": "该论文的创新点在于结合了Petrov-Galerkin方法与神经网络，特别是利用TransNet作为试验空间和径向基函数作为测试空间，以解决传统深度学习方法在处理低正则性或奇点PDE解时的局限性。通过最小化弱PDE残差来确定系数，有助于克服神经网络训练中的非凸性和病态问题，这对于提高数值方法的稳定性和可靠性非常重要。"}}
{"id": "2506.15548", "title": "Versatile Symbolic Music-for-Music Modeling via Function Alignment", "authors": ["Junyan Jiang", "Daniel Chin", "Liwei Lin", "Xuanjie Liu", "Gus Xia"], "summary": "Many music AI models learn a map between music content and human-defined\nlabels. However, many annotations, such as chords, can be naturally expressed\nwithin the music modality itself, e.g., as sequences of symbolic notes. This\nobservation enables both understanding tasks (e.g., chord recognition) and\nconditional generation tasks (e.g., chord-conditioned melody generation) to be\nunified under a music-for-music sequence modeling paradigm. In this work, we\npropose parameter-efficient solutions for a variety of symbolic music-for-music\ntasks. The high-level idea is that (1) we utilize a pretrained Language Model\n(LM) for both the reference and the target sequence and (2) we link these two\nLMs via a lightweight adapter. Experiments show that our method achieves\nsuperior performance among different tasks such as chord recognition, melody\ngeneration, and drum track generation. All demos, code and model weights are\npublicly available.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15548v1", "AI": {"title_translation": "多功能符号音乐对音乐建模通过功能对齐", "tldr": "本文提出了一种参数高效的方法，通过使用预训练语言模型和轻量级适配器，统一了符号音乐的多种音乐对音乐任务，并在实验中取得了优异性能。", "motivation": "许多音乐AI模型依赖人工定义的标签，但许多注释（如和弦）本身就可以用音乐模态表达。这使得理解和条件生成任务可以统一在“音乐对音乐”序列建模范式下。因此，需要为各种符号音乐对音乐任务提供参数高效的解决方案。", "method": "作者提出了参数高效的解决方案。核心思想是：1) 对参考序列和目标序列都使用预训练语言模型（LM）；2) 通过轻量级适配器连接这两个LM。", "result": "实验表明，该方法在和弦识别、旋律生成和鼓点生成等不同任务中取得了优异性能。", "conclusion": "通过使用预训练语言模型和轻量级适配器，将多种符号音乐对音乐任务统一在一个范式下，并实现了卓越的性能，证明了该方法的通用性和有效性。", "translation": "许多音乐AI模型学习音乐内容与人类定义标签之间的映射。然而，许多注释，例如和弦，可以在音乐模态本身内自然表达，例如作为符号音符序列。这一观察使得理解任务（例如，和弦识别）和条件生成任务（例如，和弦条件旋律生成）能够统一在“音乐对音乐”序列建模范式下。在这项工作中，我们为各种符号音乐对音乐任务提出了参数高效的解决方案。其高层次思想是：(1) 我们对参考序列和目标序列都利用预训练语言模型（LM），以及 (2) 我们通过轻量级适配器连接这两个LM。实验表明，我们的方法在和弦识别、旋律生成和鼓点生成等不同任务中取得了优异性能。所有演示、代码和模型权重均已公开。", "summary": "本文提出了一种通用的符号音乐对音乐建模方法，旨在将音乐理解和生成任务统一在“音乐对音乐”序列建模范式下。该方法利用预训练语言模型处理参考和目标音乐序列，并通过轻量级适配器连接它们，从而实现参数高效。实验证明，该方法在和弦识别、旋律生成和鼓点生成等任务上表现出色，优于现有方法。", "keywords": "符号音乐,音乐对音乐建模,语言模型,适配器,和弦识别", "comments": "这项工作的创新之处在于将多种音乐理解和生成任务统一到“音乐对音乐”的序列建模范式中，并利用预训练语言模型和轻量级适配器实现了参数高效的解决方案。这种方法避免了对大量人工标签的依赖，提高了模型的通用性和效率，对于音乐AI领域具有重要意义。"}}
{"id": "2506.15100", "title": "International Security Applications of Flexible Hardware-Enabled Guarantees", "authors": ["Onni Aarne", "James Petrie"], "summary": "As AI capabilities advance rapidly, flexible hardware-enabled guarantees\n(flexHEGs) offer opportunities to address international security challenges\nthrough comprehensive governance frameworks. This report examines how flexHEGs\ncould enable internationally trustworthy AI governance by establishing\nstandardized designs, robust ecosystem defenses, and clear operational\nparameters for AI-relevant chips. We analyze four critical international\nsecurity applications: limiting proliferation to address malicious use,\nimplementing safety norms to prevent loss of control, managing risks from\nmilitary AI systems, and supporting strategic stability through\nbalance-of-power mechanisms while respecting national sovereignty. The report\nexplores both targeted deployments for specific high-risk facilities and\ncomprehensive deployments covering all AI-relevant compute. We examine two\nprimary governance models: verification-based agreements that enable\ntransparent compliance monitoring, and ruleset-based agreements that\nautomatically enforce international rules through cryptographically-signed\nupdates. Through game-theoretic analysis, we demonstrate that comprehensive\nflexHEG agreements could remain stable under reasonable assumptions about state\npreferences and catastrophic risks. The report addresses critical\nimplementation challenges including technical thresholds for AI-relevant chips,\nmanagement of existing non-flexHEG hardware, and safeguards against abuse of\ngovernance power. While requiring significant international coordination,\nflexHEGs could provide a technical foundation for managing AI risks at the\nscale and speed necessary to address emerging threats to international security\nand stability.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15100v1", "AI": {"title_translation": "灵活硬件保障的国际安全应用", "tldr": "该报告探讨了灵活硬件保障（flexHEGs）如何通过建立标准化设计、强大的生态系统防御和清晰的操作参数，为人工智能相关芯片提供国际可信赖的人工智能治理，以应对国际安全挑战。", "motivation": "随着人工智能能力迅速发展，国际安全面临挑战，需要通过全面的治理框架来管理人工智能风险。", "method": "本报告通过审查flexHEGs如何实现国际可信赖的AI治理，分析了四项关键国际安全应用，探讨了两种主要的治理模型（基于验证的协议和基于规则集的协议），并进行了博弈论分析以证明协议的稳定性。同时，报告还探讨了实施挑战。", "result": "flexHEGs可以建立标准化设计、强大的生态系统防御和清晰的操作参数，从而实现国际可信赖的AI治理。全面的flexHEG协议在合理假设下可以保持稳定。flexHEGs能够为管理AI风险提供技术基础，以应对国际安全和稳定面临的新兴威胁。", "conclusion": "尽管需要大量的国际协调，但灵活硬件保障（flexHEGs）能够为大规模、高速地管理人工智能风险提供技术基础，以应对国际安全和稳定面临的新兴威胁。", "translation": "随着人工智能能力的迅速发展，灵活硬件保障（flexHEGs）提供了通过全面的治理框架应对国际安全挑战的机会。本报告审查了flexHEGs如何通过为人工智能相关芯片建立标准化设计、强大的生态系统防御和清晰的操作参数来促成国际可信赖的人工智能治理。我们分析了四项关键的国际安全应用：限制扩散以应对恶意使用，实施安全规范以防止失控，管理军事人工智能系统的风险，以及通过权力平衡机制支持战略稳定同时尊重国家主权。报告探讨了针对特定高风险设施的定向部署和涵盖所有人工智能相关计算的全面部署。我们研究了两种主要的治理模型：基于验证的协议，可实现透明的合规性监控；以及基于规则集的协议，可通过加密签名的更新自动执行国际规则。通过博弈论分析，我们证明了全面的flexHEG协议在关于国家偏好和灾难性风险的合理假设下可以保持稳定。报告解决了关键的实施挑战，包括人工智能相关芯片的技术阈值、现有非flexHEG硬件的管理以及防止治理权力滥用的保障措施。尽管需要大量的国际协调，但flexHEGs可以为管理人工智能风险提供技术基础，以应对国际安全和稳定面临的新兴威胁。", "summary": "本报告探讨了灵活硬件保障（flexHEGs）在国际安全领域的应用，旨在通过为AI芯片建立标准设计和操作参数，实现国际可信赖的AI治理。文章分析了限制AI扩散、实施安全规范、管理军事AI风险和支持战略稳定等四个关键应用。报告提出了基于验证和基于规则集的两种治理模型，并通过博弈论证明了全面flexHEG协议的稳定性。尽管存在实施挑战，flexHEGs被认为是管理AI风险和维护国际稳定的重要技术基础。", "keywords": "人工智能治理, 灵活硬件保障, 国际安全, 战略稳定, 风险管理", "comments": "该论文提出了灵活硬件保障（flexHEGs）这一创新概念，将其与国际安全和AI治理相结合，具有重要的理论和实践意义。通过引入硬件层面的保障措施，它为AI风险管理提供了一个新的技术视角，尤其是在防止恶意使用和确保战略稳定方面。博弈论分析增加了研究的严谨性。然而，实际部署和国际协调的复杂性是其主要挑战，特别是如何管理现有硬件和防止权力滥用。"}}
{"id": "2506.15453", "title": "Uncovering Intention through LLM-Driven Code Snippet Description Generation", "authors": ["Yusuf Sulistyo Nugroho", "Farah Danisha Salam", "Brittany Reid", "Raula Gaikovina Kula", "Kazumasa Shimari", "Kenichi Matsumoto"], "summary": "Documenting code snippets is essential to pinpoint key areas where both\ndevelopers and users should pay attention. Examples include usage examples and\nother Application Programming Interfaces (APIs), which are especially important\nfor third-party libraries. With the rise of Large Language Models (LLMs), the\nkey goal is to investigate the kinds of description developers commonly use and\nevaluate how well an LLM, in this case Llama, can support description\ngeneration. We use NPM Code Snippets, consisting of 185,412 packages with\n1,024,579 code snippets. From there, we use 400 code snippets (and their\ndescriptions) as samples. First, our manual classification found that the\nmajority of original descriptions (55.5%) highlight example-based usage. This\nfinding emphasizes the importance of clear documentation, as some descriptions\nlacked sufficient detail to convey intent. Second, the LLM correctly identified\nthe majority of original descriptions as \"Example\" (79.75%), which is identical\nto our manual finding, showing a propensity for generalization. Third, compared\nto the originals, the produced description had an average similarity score of\n0.7173, suggesting relevance but room for improvement. Scores below 0.9\nindicate some irrelevance. Our results show that depending on the task of the\ncode snippet, the intention of the document may differ from being instructions\nfor usage, installations, or descriptive learning examples for any user of a\nlibrary.", "comment": "6 pages, 3 figures, 4 tables, conference paper", "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15453v1", "AI": {"title_translation": "通过LLM驱动的代码片段描述生成揭示意图", "tldr": "该研究探讨了大型语言模型（LLM），特别是Llama，在生成代码片段描述方面的能力。结果显示LLM在识别描述类型方面表现良好，但生成描述的相似性仍有提升空间，并且强调了代码文档意图的多样性。", "motivation": "代码片段的文档化对于开发者和用户理解其关键关注点至关重要，尤其对于第三方库。随着大型语言模型（LLM）的兴起，本研究旨在调查开发者常用的描述类型，并评估LLM（如Llama）在支持描述生成方面的效果。", "method": "研究使用了包含185,412个包和1,024,579个代码片段的NPM代码片段数据集。从中抽取了400个代码片段及其描述作为样本。首先进行人工分类以识别原始描述的类型。其次，评估了LLM识别原始描述类型的能力。最后，将LLM生成的描述与原始描述进行相似性比较。", "result": "1. 人工分类发现，大多数原始描述（55.5%）侧重于基于示例的用法。2. LLM（Llama）正确识别了79.75%的原始描述为“示例”，与人工发现一致，显示出泛化倾向。3. 与原始描述相比，LLM生成的描述平均相似性得分为0.7173，表明相关性但仍有改进空间（低于0.9的分数表示存在一定不相关性）。结果表明，根据代码片段的任务，文档的意图可能不同，包括使用说明、安装指南或供库用户学习的描述性示例。", "conclusion": "大型语言模型在识别和生成代码片段描述方面展现出潜力，但仍需在提高生成描述的准确性和完整性方面进行改进。代码文档的意图是多样化的，可能包括使用说明、安装指南或描述性学习示例。", "translation": "文档化代码片段对于开发者和用户明确关键关注点至关重要。示例包括用法示例和其他应用程序编程接口（API），这对于第三方库尤其重要。随着大型语言模型（LLM）的兴起，关键目标是调查开发者常用的描述类型，并评估LLM（在本例中是Llama）在支持描述生成方面的效果。我们使用了NPM代码片段，包含185,412个包和1,024,579个代码片段。从中，我们使用400个代码片段（及其描述）作为样本。首先，我们的人工分类发现，大多数原始描述（55.5%）突出显示了基于示例的用法。这一发现强调了清晰文档的重要性，因为一些描述缺乏足够细节来传达意图。其次，LLM正确地将大多数原始描述识别为“示例”（79.75%），这与我们的人工发现一致，显示出泛化倾向。第三，与原始描述相比，生成的描述的平均相似性得分为0.7173，表明相关性但仍有改进空间。低于0.9的分数表示存在一定不相关性。我们的结果表明，根据代码片段的任务，文档的意图可能不同，可以是使用说明、安装指南，或供任何库用户使用的描述性学习示例。", "summary": "本文探讨了大型语言模型（LLM），特别是Llama，在生成代码片段描述方面的有效性。研究分析了NPM代码片段数据集，并抽样400个片段进行手动分类，发现大多数原始描述侧重于基于示例的用法。随后，评估了Llama模型识别和生成描述的能力，结果显示Llama在识别描述类型方面表现出良好的泛化能力（79.75%正确识别为“示例”）。尽管生成描述与原始描述的平均相似性达到0.7173，表明其相关性，但仍有提升空间，尤其是在完全捕捉代码片段意图的多样性方面，这些意图可能包括使用说明、安装指南或学习示例。", "keywords": "LLM, 代码片段, 描述生成, 文档, Llama", "comments": "该论文解决了代码文档化的实际问题，其创新之处在于将LLM应用于理解和生成代码片段描述。研究发现文档意图的多样性具有重要意义。0.7173的相似性得分揭示了当前LLM在完全捕捉代码片段细微意图方面的局限性，指明了未来研究的方向。"}}
{"id": "2506.15032", "title": "Assigning Multi-Robot Tasks to Multitasking Robots", "authors": ["Winston Smith", "Andrew Boateng", "Taha Shaheen", "Yu Zhang"], "summary": "One simplifying assumption in existing and well-performing task allocation\nmethods is that the robots are single-tasking: each robot operates on a single\ntask at any given time. While this assumption is harmless to make in some\nsituations, it can be inefficient or even infeasible in others. In this paper,\nwe consider assigning multi-robot tasks to multitasking robots. The key\ncontribution is a novel task allocation framework that incorporates the\nconsideration of physical constraints introduced by multitasking. This is in\ncontrast to the existing work where such constraints are largely ignored. After\nformulating the problem, we propose a compilation to weighted MAX-SAT, which\nallows us to leverage existing solvers for a solution. A more efficient greedy\nheuristic is then introduced. For evaluation, we first compare our methods with\na modern baseline that is efficient for single-tasking robots to validate the\nbenefits of multitasking in synthetic domains. Then, using a site-clearing\nscenario in simulation, we further illustrate the complex task interaction\nconsidered by the multitasking robots in our approach to demonstrate its\nperformance. Finally, we demonstrate a physical experiment to show how\nmultitasking enabled by our approach can benefit task efficiency in a realistic\nsetting.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15032v1", "AI": {"title_translation": "为多任务机器人分配多机器人任务", "tldr": "本文提出了一种新的任务分配框架，用于为多任务机器人分配多机器人任务，解决了现有方法忽略多任务处理带来的物理约束的问题，并通过加权MAX-SAT编译和贪婪启发式算法进行求解。", "motivation": "现有任务分配方法通常假设机器人是单任务的，即每个机器人在任何给定时间只执行一个任务。然而，这种假设在某些情况下可能效率低下甚至不可行，特别是在处理多机器人任务时。", "method": "本文提出了一种新颖的任务分配框架，该框架考虑了多任务处理引入的物理约束。首先，将问题表述为加权MAX-SAT问题，利用现有求解器寻找解决方案。随后，引入了一种更高效的贪婪启发式算法。", "result": "通过与现有单任务机器人基线方法的比较，在合成领域验证了多任务处理的优势。在模拟的场地清理场景中，进一步展示了所提出方法中多任务机器人考虑的复杂任务交互及其性能。最后，通过物理实验证明了该方法实现的多任务处理在实际环境中能提高任务效率。", "conclusion": "本文提出了一种考虑物理约束的多任务机器人任务分配框架，并通过实验证明了其在提高任务效率方面的有效性。", "translation": "现有表现良好的任务分配方法中，一个简化的假设是机器人是单任务的：每个机器人在任何给定时间只执行一个任务。虽然这个假设在某些情况下是无害的，但在其他情况下可能效率低下甚至不可行。在本文中，我们考虑为多任务机器人分配多机器人任务。关键贡献是一个新颖的任务分配框架，该框架纳入了多任务处理引入的物理约束的考虑。这与现有工作中很大程度上忽略此类约束形成对比。在问题建模之后，我们提出了一个编译到加权MAX-SAT的方法，这使我们能够利用现有求解器来获得解决方案。然后引入了一种更高效的贪婪启发式算法。为了评估，我们首先将我们的方法与一个对单任务机器人高效的现代基线进行比较，以验证多任务处理在合成领域中的优势。然后，在模拟的场地清理场景中，我们进一步说明了我们方法中多任务机器人考虑的复杂任务交互，以展示其性能。最后，我们通过物理实验展示了我们的方法实现的多任务处理如何在实际环境中提高任务效率。", "summary": "本文针对现有任务分配方法中机器人被假设为单任务的局限性，提出了一种为多任务机器人分配多机器人任务的新型框架。该框架创新性地考虑了多任务处理带来的物理约束，并通过将问题编译为加权MAX-SAT以及引入高效的贪婪启发式算法来求解。实验结果表明，与传统单任务方法相比，该方法在合成领域和实际场景中均能有效提升任务效率。", "keywords": "多任务机器人, 任务分配, 物理约束, MAX-SAT, 贪婪启发式", "comments": "本文的核心创新在于打破了机器人任务分配中普遍存在的单任务假设，并首次系统地将多任务处理引入的物理约束纳入考量。这种对现实世界复杂性的关注，通过结合理论（MAX-SAT）和实用（贪婪启发式）方法，为多机器人系统在更复杂、更高效场景中的应用提供了重要的理论和实践基础。其贡献在于提高了多机器人系统的任务执行效率和灵活性。"}}
{"id": "2506.15008", "title": "Insights Informed Generative AI for Design: Incorporating Real-world Data for Text-to-Image Output", "authors": ["Richa Gupta", "Alexander Htet Kyaw"], "summary": "Generative AI, specifically text-to-image models, have revolutionized\ninterior architectural design by enabling the rapid translation of conceptual\nideas into visual representations from simple text prompts. While generative AI\ncan produce visually appealing images they often lack actionable data for\ndesigners In this work, we propose a novel pipeline that integrates DALL-E 3\nwith a materials dataset to enrich AI-generated designs with sustainability\nmetrics and material usage insights. After the model generates an interior\ndesign image, a post-processing module identifies the top ten materials present\nand pairs them with carbon dioxide equivalent (CO2e) values from a general\nmaterials dictionary. This approach allows designers to immediately evaluate\nenvironmental impacts and refine prompts accordingly. We evaluate the system\nthrough three user tests: (1) no mention of sustainability to the user prior to\nthe prompting process with generative AI, (2) sustainability goals communicated\nto the user before prompting, and (3) sustainability goals communicated along\nwith quantitative CO2e data included in the generative AI outputs. Our\nqualitative and quantitative analyses reveal that the introduction of\nsustainability metrics in the third test leads to more informed design\ndecisions, however, it can also trigger decision fatigue and lower overall\nsatisfaction. Nevertheless, the majority of participants reported incorporating\nsustainability principles into their workflows in the third test, underscoring\nthe potential of integrated metrics to guide more ecologically responsible\npractices. Our findings showcase the importance of balancing design freedom\nwith practical constraints, offering a clear path toward holistic, data-driven\nsolutions in AI-assisted architectural design.", "comment": "15 Pages, 6 figures, CAAD Futures 2025", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15008v1", "AI": {"title_translation": "洞察力驱动的生成式AI设计：将真实世界数据融入文本到图像输出", "tldr": "该研究提出了一种将可持续性指标（如碳排放量）整合到文本到图像生成式AI设计流程中的新方法，以帮助设计师做出更明智的决策，尽管可能导致决策疲劳，但能促进生态负责实践。", "motivation": "尽管生成式AI（特别是文本到图像模型）彻底改变了室内建筑设计，但其生成的图像往往缺乏可操作的数据，例如可持续性指标，设计师需要立即评估环境影响。", "method": "本研究提出了一种新颖的管道，将DALL-E 3与材料数据集集成。在模型生成室内设计图像后，一个后处理模块会识别出前十种存在的材料，并将其与通用材料字典中的二氧化碳当量（CO2e）值配对。该系统通过三项用户测试进行评估：(1) 在提示生成AI之前不提及可持续性，(2) 在提示之前向用户传达可持续性目标，(3) 在生成AI输出中包含可持续性目标和定量的CO2e数据。", "result": "定性与定量分析显示，在第三项测试中引入可持续性指标导致了更明智的设计决策，但同时也可能引发决策疲劳并降低整体满意度。尽管如此，大多数参与者在第三项测试中表示将可持续性原则纳入了他们的工作流程。", "conclusion": "研究结果表明，平衡设计自由与实际约束的重要性，为AI辅助建筑设计中整体、数据驱动的解决方案提供了明确的路径。整合的指标有潜力引导更具生态责任感的实践。", "translation": "生成式人工智能，特别是文本到图像模型，通过将概念想法从简单的文本提示快速转化为视觉表示，彻底改变了室内建筑设计。尽管生成式人工智能可以产生视觉上吸引人的图像，但它们通常缺乏对设计师而言可操作的数据。在这项工作中，我们提出了一种新颖的管道，将DALL-E 3与材料数据集集成，以通过可持续性指标和材料使用洞察来丰富人工智能生成的设计。在模型生成室内设计图像后，一个后处理模块会识别出前十种存在的材料，并将其与通用材料字典中的二氧化碳当量（CO2e）值配对。这种方法允许设计师立即评估环境影响并相应地完善提示。我们通过三项用户测试评估了该系统：(1) 在与生成式人工智能的提示过程之前不向用户提及可持续性，(2) 在提示之前向用户传达可持续性目标，以及 (3) 在生成式人工智能输出中包含可持续性目标和定量的CO2e数据。我们的定性与定量分析显示，在第三项测试中引入可持续性指标导致了更明智的设计决策，但同时也可能引发决策疲劳并降低整体满意度。尽管如此，大多数参与者在第三项测试中表示将可持续性原则纳入了他们的工作流程，强调了集成指标在指导更具生态责任感的实践方面的潜力。我们的研究结果展示了平衡设计自由与实际约束的重要性，为AI辅助建筑设计中整体、数据驱动的解决方案提供了明确的路径。", "summary": "本文提出了一种新颖的管道，旨在通过整合真实世界材料数据和可持续性指标（如CO2e），增强用于室内建筑设计的文本到图像生成式AI（DALL-E 3）。该系统在图像生成后通过后处理模块识别材料并提供其环境影响。用户测试表明，提供CO2e数据能够促进更明智的可持续设计决策，但可能导致决策疲劳。然而，它总体上鼓励了可持续实践的采纳，强调了在AI辅助设计中平衡创意自由与数据驱动约束的重要性。", "keywords": "生成式AI, 可持续设计, 文本到图像, 建筑设计, 材料数据", "comments": "该研究的创新之处在于将实际、可操作的可持续性数据直接整合到生成式AI设计工作流程中，超越了纯粹的美学输出，解决了当前AI工具在提供实用设计洞察方面的不足。其提出的决策疲劳问题也为未来研究提供了重要的考量方向。"}}
{"id": "2506.15412", "title": "Golden Partition Zone: Rethinking Neural Network Partitioning Under Inversion Threats in Collaborative Inference", "authors": ["Rongke Liu", "Youwen Zhu"], "summary": "In collaborative inference, intermediate features transmitted from edge\ndevices can be exploited by adversaries to reconstruct original inputs via\nmodel inversion attacks (MIA). While existing defenses focus on shallow-layer\nprotection, they often incur significant utility loss. A key open question is\nhow to partition the edge-cloud model to maximize resistance to MIA while\nminimizing accuracy degradation. We first show that increasing model depth\nalone does not guarantee resistance. Through theoretical analysis, we\ndemonstrate that representational transitions in neural networks cause sharp\nchanges in conditional entropy $H(x\\mid z)$, with intra-class variance (denoted\n$R_c^2$) and feature dimensionality as critical factors. Experiments on three\nrepresentative deep vision models demonstrate that splitting at the\nrepresentational-transition or decision-level layers increases mean squared\nerror by more than four times compared to shallow splits, indicating\nsignificantly stronger resistance to MIA. Positive label smoothing further\nenhances robustness by compressing $R_c^2$ and improving generalization.\nFinally, we validate the resilience of decision-level features under enhanced\ninversion models and observe that the type of auxiliary data influences both\ntransition boundaries and reconstruction behavior.", "comment": "8 pages, 11 figures, 5 tables", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15412v1", "AI": {"title_translation": "黄金分割区：协作推理中反演威胁下神经网络分区的再思考", "tldr": "本文研究在协作推理中如何划分神经网络以最大化抵抗模型反演攻击同时最小化精度损失。研究发现，在神经网络的表征转换层或决策层进行分割能显著增强对MIA的抵抗力，且正向标签平滑可进一步提升鲁棒性。", "motivation": "在协作推理中，边缘设备传输的中间特征容易被攻击者利用模型反演攻击（MIA）重建原始输入。现有防御方法主要关注浅层保护，但往往导致显著的效用损失。因此，一个关键的开放问题是如何划分边缘-云模型，以在最大化对MIA抵抗力的同时，最小化精度下降。", "method": "首先指出单纯增加模型深度并不能保证抵抗MIA。通过理论分析，证明神经网络中的表征转换会导致条件熵H(x|z)的急剧变化，其中类内方差(Rc^2)和特征维度是关键因素。在三个代表性深度视觉模型上进行实验验证。引入正向标签平滑以增强鲁棒性。在增强的反演模型下验证决策层特征的弹性。", "result": "单纯增加模型深度并不能保证抵抗MIA。在表征转换层或决策层进行分割，与浅层分割相比，均方误差增加了四倍以上，表明对MIA的抵抗力显著增强。正向标签平滑通过压缩Rc^2和改善泛化能力，进一步增强了鲁棒性。辅助数据类型会影响转换边界和重建行为。", "conclusion": "在协作推理中，通过在神经网络的表征转换层或决策层进行分割，能够显著增强对模型反演攻击的抵抗力。此外，正向标签平滑被证明能进一步提升模型的鲁棒性。", "translation": "在协作推理中，边缘设备传输的中间特征可能被攻击者利用，通过模型反演攻击 (MIA) 重建原始输入。虽然现有防御措施侧重于浅层保护，但它们通常会造成显著的效用损失。一个关键的开放问题是，如何划分边缘-云模型以最大化对MIA的抵抗力，同时最小化精度下降。我们首先表明，仅仅增加模型深度并不能保证抵抗力。通过理论分析，我们证明了神经网络中的表征转换导致条件熵 $H(x\text{|}{z})$ 的急剧变化，其中类内方差（表示为 $R_c^2$）和特征维度是关键因素。在三个代表性深度视觉模型上的实验表明，在表征转换层或决策层进行分割，与浅层分割相比，均方误差增加了四倍以上，这表明对MIA的抵抗力显著增强。正向标签平滑通过压缩 $R_c^2$ 并改善泛化能力，进一步增强了鲁棒性。最后，我们在增强的反演模型下验证了决策层特征的弹性，并观察到辅助数据类型会影响转换边界和重建行为。", "summary": "本文探讨了在协作推理中如何有效划分神经网络以防御模型反演攻击（MIA）。研究发现，单纯增加模型深度不足以提供保护。通过理论分析和实验验证，作者指出在神经网络的表征转换层或决策层进行分割可以显著增强对MIA的抵抗力，远超浅层分割的效果。此外，正向标签平滑也被证明能进一步提升模型的鲁棒性。研究还强调了类内方差和特征维度在抵抗MIA中的关键作用，并指出辅助数据类型对重建行为的影响。", "keywords": "协作推理, 模型反演攻击, 神经网络分区, 表征转换, 决策层分割", "comments": "本文提出了一个新颖的“黄金分割区”概念，即在神经网络的表征转换层或决策层进行分割，以有效防御模型反演攻击，这与传统侧重浅层防御的方法不同。其理论分析结合实验验证，揭示了类内方差和特征维度在抵抗MIA中的重要性，并引入了正向标签平滑作为增强鲁棒性的手段，具有重要的理论和实践意义。"}}
{"id": "2506.14832", "title": "ArchShapeNet:An Interpretable 3D-CNN Framework for Evaluating Architectural Shapes", "authors": ["Jun Yin", "Jing Zhong", "Pengyu Zeng", "Peilin Li", "Zixuan Dai", "Miao Zhang", "Shuai Lu"], "summary": "In contemporary architectural design, the growing complexity and diversity of\ndesign demands have made generative plugin tools essential for quickly\nproducing initial concepts and exploring novel 3D forms. However, objectively\nanalyzing the differences between human-designed and machine-generated 3D forms\nremains a challenge, limiting our understanding of their respective strengths\nand hindering the advancement of generative tools.\n  To address this, we built ArchForms-4000, a dataset containing 2,000\narchitect-designed and 2,000 Evomass-generated 3D forms; Proposed ArchShapeNet,\na 3D convolutional neural network tailored for classifying and analyzing\narchitectural forms, incorporating a saliency module to highlight key spatial\nfeatures aligned with architectural reasoning; And conducted comparative\nexperiments showing our model outperforms human experts in distinguishing form\norigins, achieving 94.29% accuracy, 96.2% precision, and 98.51% recall.\n  This study not only highlights the distinctive advantages of human-designed\nforms in spatial organization, proportional harmony, and detail refinement but\nalso provides valuable insights for enhancing generative design tools in the\nfuture.", "comment": "22 pages, 8 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14832v1", "AI": {"title_translation": "ArchShapeNet：一个用于评估建筑形态的可解释3D-CNN框架", "tldr": "ArchShapeNet是一个可解释的3D-CNN框架，用于区分人类设计和机器生成的建筑3D形态，并提供对人类设计优势的洞察。", "motivation": "当代建筑设计中，生成式插件工具虽然对快速生成初步概念和探索新3D形态至关重要，但客观分析人类设计与机器生成3D形态之间差异的挑战，限制了我们对其各自优势的理解，并阻碍了生成工具的进步。", "method": "研究构建了包含2000个人类设计和2000个Evomass生成的3D形态的ArchForms-4000数据集；提出了ArchShapeNet，一个专门用于分类和分析建筑形态的3D卷积神经网络，并融入了显著性模块以突出与建筑推理相符的关键空间特征；并进行了比较实验。", "result": "模型在区分形态来源方面优于人类专家，达到了94.29%的准确率、96.2%的精确率和98.51%的召回率。", "conclusion": "本研究不仅强调了人类设计形态在空间组织、比例协调和细节完善方面的独特优势，还为未来增强生成式设计工具提供了宝贵见解。", "translation": "在当代建筑设计中，设计需求日益增长的复杂性和多样性使得生成式插件工具对于快速产生初始概念和探索新颖的3D形态变得必不可少。然而，客观分析人类设计和机器生成3D形态之间的差异仍然是一个挑战，这限制了我们对其各自优势的理解，并阻碍了生成工具的进步。\n为了解决这个问题，我们构建了ArchForms-4000数据集，其中包含2000个人类建筑师设计的3D形态和2000个Evomass生成的3D形态；提出了ArchShapeNet，一个专为分类和分析建筑形态而定制的3D卷积神经网络，并融入了显著性模块以突出与建筑推理相符的关键空间特征；并进行了比较实验，结果表明我们的模型在区分形态来源方面优于人类专家，达到了94.29%的准确率、96.2%的精确率和98.51%的召回率。\n这项研究不仅突出了人类设计形态在空间组织、比例协调和细节完善方面的独特优势，而且为未来增强生成式设计工具提供了宝贵的见解。", "summary": "本研究旨在解决人类设计与机器生成建筑3D形态差异分析的挑战。为此，作者构建了ArchForms-4000数据集，并提出了ArchShapeNet，一个可解释的3D-CNN模型，用于分类和分析建筑形态。实验结果表明，ArchShapeNet在区分形态来源方面表现优异，甚至超越了人类专家。该研究不仅揭示了人类设计形态的独特优势，也为未来生成式设计工具的改进提供了方向。", "keywords": "建筑形态, 3D-CNN, 生成式设计, 可解释性, ArchShapeNet", "comments": "ArchShapeNet的创新之处在于其可解释的3D-CNN框架，结合了显著性模块，使得模型能够识别与建筑推理相关的关键空间特征，这对于理解模型决策和建筑形态的内在质量至关重要。其超越人类专家的分类性能凸显了其在建筑形态评估领域的潜力。此外，该研究通过量化人类设计形态的优势，为未来生成式AI工具的优化提供了明确的指导方向，具有重要的实践意义。"}}
{"id": "2506.14830", "title": "Optimization of bi-directional gated loop cell based on multi-head attention mechanism for SSD health state classification model", "authors": ["Zhizhao Wen", "Ruoxin Zhang", "Chao Wang"], "summary": "Aiming at the critical role of SSD health state prediction in data\nreliability assurance, this study proposes a hybrid BiGRU-MHA model that\nincorporates a multi-head attention mechanism to enhance the accuracy and\nstability of storage device health classification. The model innovatively\nintegrates temporal feature extraction and key information focusing\ncapabilities. Specifically, it leverages the bidirectional timing modeling\nadvantages of the BiGRU network to capture both forward and backward\ndependencies of SSD degradation features. Simultaneously, the multi-head\nattention mechanism dynamically assigns feature weights, improving the model's\nsensitivity to critical health indicators. Experimental results show that the\nproposed model achieves classification accuracies of 92.70% on the training set\nand 92.44% on the test set, with a minimal performance gap of only 0.26%,\ndemonstrating excellent generalization ability. Further analysis using the\nreceiver operating characteristic (ROC) curve shows an area under the curve\n(AUC) of 0.94 on the test set, confirming the model's robust binary\nclassification performance. This work not only presents a new technical\napproach for SSD health prediction but also addresses the generalization\nbottleneck of traditional models, offering a verifiable method with practical\nvalue for preventive maintenance of industrial-grade storage systems. The\nresults show the model can significantly reduce data loss risks by providing\nearly failure warnings and help optimize maintenance costs, supporting\nintelligent decision-making in building reliable storage systems for cloud\ncomputing data centers and edge storage environments.", "comment": "Source code available; Accepted by 2025 6th International Conference\n  on Electronic Communication and Artificial Intelligence; 5 pages; 7 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14830v1", "AI": {"title_translation": "基于多头注意力机制的双向门控循环单元SSD健康状态分类模型优化", "tldr": "本研究提出了一种结合多头注意力机制的混合BiGRU-MHA模型，用于提高SSD健康状态分类的准确性和稳定性，并在实验中取得了高准确率和良好的泛化能力。", "motivation": "固态硬盘（SSD）健康状态预测在数据可靠性保障中扮演着关键角色，传统模型存在泛化瓶颈，因此需要一种新的技术方法来提高存储设备健康分类的准确性和稳定性。", "method": "本研究提出了一种混合BiGRU-MHA模型。该模型创新性地整合了时间特征提取和关键信息聚焦能力。具体而言，它利用BiGRU网络的双向时序建模优势来捕获SSD退化特征的前向和后向依赖关系，同时，多头注意力机制动态分配特征权重，提高了模型对关键健康指标的敏感性。", "result": "所提出的模型在训练集上实现了92.70%的分类准确率，在测试集上实现了92.44%的分类准确率，性能差距仅为0.26%，表现出卓越的泛化能力。ROC曲线分析显示测试集上的曲线下面积（AUC）为0.94，证实了模型强大的二分类性能。", "conclusion": "本研究不仅为SSD健康预测提供了一种新的技术方法，还解决了传统模型的泛化瓶颈，提供了一种具有实用价值的可验证方法，可用于工业级存储系统的预防性维护。结果表明，该模型通过提供早期故障预警，可以显著降低数据丢失风险，并有助于优化维护成本，支持云计算数据中心和边缘存储环境中构建可靠存储系统的智能决策。", "translation": "针对SSD健康状态预测在数据可靠性保障中的关键作用，本研究提出了一种结合多头注意力机制的混合BiGRU-MHA模型，以提高存储设备健康分类的准确性和稳定性。该模型创新性地整合了时间特征提取和关键信息聚焦能力。具体而言，它利用BiGRU网络的双向时序建模优势来捕获SSD退化特征的前向和后向依赖关系。同时，多头注意力机制动态分配特征权重，提高了模型对关键健康指标的敏感性。实验结果表明，所提出的模型在训练集上实现了92.70%的分类准确率，在测试集上实现了92.44%的分类准确率，性能差距仅为0.26%，表现出卓越的泛化能力。使用受试者工作特征（ROC）曲线的进一步分析显示，测试集上的曲线下面积（AUC）为0.94，证实了模型强大的二分类性能。这项工作不仅为SSD健康预测提供了一种新的技术方法，还解决了传统模型的泛化瓶颈，提供了一种具有实用价值的可验证方法，可用于工业级存储系统的预防性维护。结果表明，该模型通过提供早期故障预警，可以显著降低数据丢失风险，并有助于优化维护成本，支持云计算数据中心和边缘存储环境中构建可靠存储系统的智能决策。", "summary": "本研究提出了一种新颖的混合BiGRU-MHA模型，旨在提高固态硬盘（SSD）健康状态分类的准确性和稳定性。该模型结合了BiGRU网络的时间特征提取能力和多头注意力机制的关键信息聚焦能力，以捕捉SSD退化特征并动态分配权重。实验结果显示，该模型在SSD健康状态分类任务上表现出色，准确率高且泛化能力强，为工业级存储系统的预防性维护提供了有价值的解决方案，有助于降低数据丢失风险和优化维护成本。", "keywords": "SSD健康预测, BiGRU-MHA模型, 多头注意力机制, 分类准确率, 泛化能力", "comments": "该论文提出了一种结合BiGRU和多头注意力机制的混合模型，创新性地将时间序列分析与关键特征权重分配相结合，有效提升了SSD健康状态分类的准确性和泛化能力。其在实际应用中对降低数据丢失风险和优化维护成本具有显著潜力，为工业级存储系统的智能维护提供了可验证的技术支持。"}}
{"id": "2506.15567", "title": "Managing Complex Failure Analysis Workflows with LLM-based Reasoning and Acting Agents", "authors": ["Aline Dobrovsky", "Konstantin Schekotihin", "Christian Burmer"], "summary": "Failure Analysis (FA) is a highly intricate and knowledge-intensive process.\nThe integration of AI components within the computational infrastructure of FA\nlabs has the potential to automate a variety of tasks, including the detection\nof non-conformities in images, the retrieval of analogous cases from diverse\ndata sources, and the generation of reports from annotated images. However, as\nthe number of deployed AI models increases, the challenge lies in orchestrating\nthese components into cohesive and efficient workflows that seamlessly\nintegrate with the FA process.\n  This paper investigates the design and implementation of a Large Language\nModel (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their\nanalysis cases. The LPA integrates LLMs with advanced planning capabilities and\nexternal tool utilization, enabling autonomous processing of complex queries,\nretrieval of relevant data from external systems, and generation of\nhuman-readable responses. Evaluation results demonstrate the agent's\noperational effectiveness and reliability in supporting FA tasks.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15567v1", "AI": {"title_translation": "基于LLM的推理和行动代理管理复杂故障分析工作流", "tldr": "本文提出并实现了一种基于大型语言模型（LLM）的规划代理，用于自动化和管理复杂的故障分析工作流，通过将LLM与规划能力和外部工具集成。", "motivation": "故障分析（FA）是一个高度复杂且知识密集的过程。尽管AI组件能够自动化FA中的各种任务，但随着AI模型数量的增加，如何将这些组件编排成连贯高效的工作流是一个重大挑战。本文旨在解决这一编排挑战，以协助FA工程师。", "method": "本文设计并实现了一个基于大型语言模型（LLM）的规划代理（LPA）。该LPA将LLM与高级规划能力和外部工具利用相结合，使其能够自主处理复杂查询、从外部系统检索相关数据并生成人类可读的响应。", "result": "评估结果表明，该规划代理在支持故障分析（FA）任务方面具有操作有效性和可靠性。", "conclusion": "基于LLM的规划代理通过提供自主处理和数据检索能力，有效支持并改进了复杂的故障分析工作流。", "translation": "故障分析 (FA) 是一个高度复杂且知识密集的过程。在FA实验室的计算基础设施中整合AI组件，有可能自动化各种任务，包括图像中不合格品的检测、从不同数据源检索类似案例以及从带注释的图像生成报告。然而，随着部署的AI模型数量的增加，挑战在于如何将这些组件编排成与FA过程无缝集成的连贯高效的工作流。本文研究了基于大型语言模型 (LLM) 的规划代理 (LPA) 的设计和实现，以协助FA工程师解决他们的分析案例。LPA将LLM与高级规划能力和外部工具利用相结合，能够自主处理复杂查询、从外部系统检索相关数据以及生成人类可读的响应。评估结果表明该代理在支持FA任务方面的操作有效性和可靠性。", "summary": "本文旨在解决复杂故障分析（FA）工作流中多个人工智能组件的编排挑战。作者提出并实现了一种基于大型语言模型（LLM）的规划代理（LPA），该代理整合了LLM的高级规划能力和外部工具利用，能够自主处理FA任务、检索相关数据并生成响应。评估结果证明了该代理在支持FA任务方面的有效性和可靠性。", "keywords": "故障分析, LLM, 规划代理, 工作流管理, AI编排", "comments": "本文的创新之处在于利用LLM作为核心规划代理，协调故障分析等复杂领域内的各种AI组件和外部工具，这超越了简单的任务自动化，实现了工作流管理。这种方法有望显著提高知识密集型流程的效率并减少人工投入。"}}
{"id": "2506.15454", "title": "Parallel Paradigms in Modern HPC: A Comparative Analysis of MPI, OpenMP, and CUDA", "authors": ["Nizar ALHafez", "Ahmad Kurdi"], "summary": "This paper presents a comprehensive comparison of three dominant parallel\nprogramming models in High Performance Computing (HPC): Message Passing\nInterface (MPI), Open Multi-Processing (OpenMP), and Compute Unified Device\nArchitecture (CUDA). Selecting optimal programming approaches for modern\nheterogeneous HPC architectures has become increasingly critical. We\nsystematically analyze these models across multiple dimensions: architectural\nfoundations, performance characteristics, domain-specific suitability,\nprogramming complexity, and recent advancements. We examine each model's\nstrengths, weaknesses, and optimization techniques. Our investigation\ndemonstrates that MPI excels in distributed memory environments with\nnear-linear scalability for communication-intensive applications, but faces\ncommunication overhead challenges. OpenMP provides strong performance and\nusability in shared-memory systems and loop-centric tasks, though it is limited\nby shared memory contention. CUDA offers substantial performance gains for\ndata-parallel GPU workloads, but is restricted to NVIDIA GPUs and requires\nspecialized expertise. Performance evaluations across scientific simulations,\nmachine learning, and data analytics reveal that hybrid approaches combining\ntwo or more models often yield optimal results in heterogeneous environments.\nThe paper also discusses implementation challenges, optimization best\npractices, and emerging trends such as performance portability frameworks,\ntask-based programming, and the convergence of HPC and Big Data. This research\nhelps developers and researchers make informed decisions when selecting\nprogramming models for modern HPC applications, emphasizing that the best\nchoice depends on application requirements, hardware, and development\nconstraints.", "comment": "10 pages", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15454v1", "AI": {"title_translation": "现代HPC中的并行范式：MPI、OpenMP和CUDA的比较分析", "tldr": "论文全面比较了HPC领域的三种主流并行编程模型MPI、OpenMP和CUDA，分析了它们在不同维度上的优缺点及适用场景，并指出在异构环境中采用混合方法常能获得最优性能，旨在帮助开发者选择合适的编程模型。", "motivation": "为现代异构HPC架构选择最优的编程方法变得越来越关键。", "method": "本文对MPI、OpenMP和CUDA这三种并行编程模型进行了系统性的比较分析，分析维度包括：架构基础、性能特征、领域特定适用性、编程复杂性和最新进展。并通过对科学模拟、机器学习和数据分析的性能评估来验证其结论。", "result": "MPI在分布式内存环境中表现出色，适用于通信密集型应用，但面临通信开销。OpenMP在共享内存系统和以循环为中心的任务中性能和可用性强，但受限于共享内存争用。CUDA为数据并行GPU工作负载提供显著性能提升，但仅限于NVIDIA GPU并需要专业知识。性能评估表明，结合两种或更多模型的混合方法在异构环境中通常能产生最佳结果。", "conclusion": "最佳选择取决于应用程序需求、硬件和开发限制。这项研究旨在帮助开发人员和研究人员在为现代HPC应用程序选择编程模型时做出明智的决策。", "translation": "本论文对高性能计算（HPC）中三种主流并行编程模型进行了全面比较：消息传递接口（MPI）、开放多处理（OpenMP）和统一计算设备架构（CUDA）。为现代异构HPC架构选择最优的编程方法变得越来越关键。我们系统地分析了这些模型在多个维度上的表现：架构基础、性能特征、领域特定适用性、编程复杂性以及最新进展。我们研究了每个模型的优点、缺点和优化技术。我们的研究表明，MPI在分布式内存环境中表现出色，对于通信密集型应用程序具有接近线性的可伸缩性，但面临通信开销挑战。OpenMP在共享内存系统和以循环为中心的任务中提供强大的性能和可用性，但受限于共享内存争用。CUDA为数据并行GPU工作负载提供了显著的性能提升，但仅限于NVIDIA GPU并需要专业知识。对科学模拟、机器学习和数据分析的性能评估表明，结合两种或更多模型的混合方法在异构环境中通常能产生最佳结果。论文还讨论了实现挑战、优化最佳实践以及新兴趋势，如性能可移植性框架、基于任务的编程以及HPC与大数据的融合。这项研究有助于开发人员和研究人员在为现代HPC应用程序选择编程模型时做出明智的决策，强调最佳选择取决于应用程序需求、硬件和开发限制。", "summary": "本文对高性能计算中MPI、OpenMP和CUDA三种主流并行编程模型进行了深入的比较分析。研究涵盖了它们的架构基础、性能特征、领域适用性、编程复杂性及最新进展，并评估了它们在科学模拟、机器学习和数据分析中的表现。结果显示，MPI擅长分布式内存环境，OpenMP适用于共享内存系统，CUDA在GPU数据并行中表现优异，而混合方法在异构环境中常能达到最优性能。论文旨在帮助开发者和研究人员根据具体的应用程序需求、硬件条件和开发约束，做出明智的编程模型选择。", "keywords": "HPC, 并行编程, MPI, OpenMP, CUDA", "comments": "这篇论文通过对MPI、OpenMP和CUDA这三种核心并行编程模型的全面比较，为HPC领域的开发者和研究者提供了宝贵的决策指导。其创新之处在于系统性地分析了多维度的特性，并强调了混合编程在异构环境中的重要性，这反映了当前HPC发展的趋势。论文的实用性强，有助于优化复杂并行应用的开发。"}}
{"id": "2506.15047", "title": "Mapping Caregiver Needs to AI Chatbot Design: Strengths and Gaps in Mental Health Support for Alzheimer's and Dementia Caregivers", "authors": ["Jiayue Melissa Shi", "Dong Whi Yoo", "Keran Wang", "Violeta J. Rodriguez", "Ravi Karkar", "Koustuv Saha"], "summary": "Family caregivers of individuals with Alzheimer's Disease and Related\nDementia (AD/ADRD) face significant emotional and logistical challenges that\nplace them at heightened risk for stress, anxiety, and depression. Although\nrecent advances in generative AI -- particularly large language models (LLMs)\n-- offer new opportunities to support mental health, little is known about how\ncaregivers perceive and engage with such technologies. To address this gap, we\ndeveloped Carey, a GPT-4o-based chatbot designed to provide informational and\nemotional support to AD/ADRD caregivers. Using Carey as a technology probe, we\nconducted semi-structured interviews with 16 family caregivers following\nscenario-driven interactions grounded in common caregiving stressors. Through\ninductive coding and reflexive thematic analysis, we surface a systemic\nunderstanding of caregiver needs and expectations across six themes --\non-demand information access, emotional support, safe space for disclosure,\ncrisis management, personalization, and data privacy. For each of these themes,\nwe also identified the nuanced tensions in the caregivers' desires and\nconcerns. We present a mapping of caregiver needs, AI chatbot's strengths,\ngaps, and design recommendations. Our findings offer theoretical and practical\ninsights to inform the design of proactive, trustworthy, and caregiver-centered\nAI systems that better support the evolving mental health needs of AD/ADRD\ncaregivers.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15047v1", "AI": {"title_translation": "护理人员需求与AI聊天机器人设计：阿尔茨海默症和痴呆症护理人员心理健康支持的优势与不足", "tldr": "本研究开发并使用了基于GPT-4o的聊天机器人Carey，通过访谈16位阿尔茨海默症/痴呆症护理人员，识别了他们在心理健康支持方面的需求和期望，并提出了AI聊天机器人的设计建议。", "motivation": "阿尔茨海默症及相关痴呆症(AD/ADRD)患者的家庭护理人员面临巨大的情感和后勤挑战，导致他们面临压力、焦虑和抑郁的高风险。尽管生成式AI（特别是大型语言模型LLMs）提供了支持心理健康的新机会，但对于护理人员如何看待和使用这些技术知之甚少。本研究旨在弥补这一空白。", "method": "研究开发了一个基于GPT-4o的聊天机器人Carey，作为技术探测工具。通过情景驱动的互动，对16位家庭护理人员进行了半结构化访谈，访谈内容基于常见的护理压力源。研究采用归纳编码和反思性主题分析方法，系统地理解了护理人员的需求和期望，并识别了护理人员在愿望和担忧之间的细微张力。", "result": "研究揭示了护理人员需求的六个主题：按需信息获取、情感支持、安全的倾诉空间、危机管理、个性化和数据隐私。针对每个主题，都识别了护理人员愿望和担忧之间的细微张力。研究还呈现了护理人员需求、AI聊天机器人优势、不足和设计建议的映射。", "conclusion": "本研究的发现为设计主动、值得信赖和以护理人员为中心的AI系统提供了理论和实践见解，这些系统能更好地支持AD/ADRD护理人员不断变化的心理健康需求。", "translation": "阿尔茨海默病及相关痴呆症（AD/ADRD）患者的家庭护理人员面临着巨大的情感和后勤挑战，这使他们面临更高的压力、焦虑和抑郁风险。尽管生成式人工智能（特别是大型语言模型LLMs）的最新进展为支持心理健康提供了新机会，但对于护理人员如何看待和使用此类技术知之甚少。为了弥补这一空白，我们开发了Carey，一个基于GPT-4o的聊天机器人，旨在为AD/ADRD护理人员提供信息和情感支持。我们以Carey作为技术探测工具，在基于常见护理压力的情景驱动互动后，对16位家庭护理人员进行了半结构化访谈。通过归纳编码和反思性主题分析，我们系统地理解了护理人员在六个主题上的需求和期望——按需信息获取、情感支持、安全的倾诉空间、危机管理、个性化和数据隐私。对于每个主题，我们还识别了护理人员愿望和担忧之间的细微张力。我们提出了护理人员需求、AI聊天机器人优势、不足以及设计建议的映射。我们的研究结果提供了理论和实践见解，以指导设计主动、值得信赖和以护理人员为中心的AI系统，从而更好地支持AD/ADRD护理人员不断变化的心理健康需求。", "summary": "本研究旨在解决阿尔茨海默症及相关痴呆症(AD/ADRD)护理人员面临的心理健康挑战，并探索AI聊天机器人的支持潜力。通过开发基于GPT-4o的Carey聊天机器人并对16名护理人员进行访谈，研究识别了护理人员在信息获取、情感支持、隐私等六个方面的核心需求和担忧。研究结果明确了AI聊天机器人的优势与不足，并提供了相应的设计建议，旨在为开发更有效、以护理人员为中心的AI心理健康支持系统提供指导。", "keywords": "阿尔茨海默症护理人员, AI聊天机器人, 心理健康支持, GPT-4o, 需求映射", "comments": "这项研究通过将AI聊天机器人作为“技术探测器”来深入理解用户需求，方法新颖且实用。它不仅识别了护理人员的具体需求，还揭示了他们在接受AI支持时存在的微妙矛盾和担忧，这对于未来开发真正以用户为中心、值得信赖的AI心理健康工具至关重要。研究结果为AI在医疗健康领域的应用提供了宝贵的实践指导，特别是在支持弱势群体方面。"}}
{"id": "2506.15192", "title": "Microgrid Operation Control with Adaptable Droop Gains", "authors": ["E. D. Gomez Anccas", "C. A. Hans", "D. Schulz"], "summary": "Modern low-carbon power systems come with many challenges, such as increased\ninverter penetration and increased uncertainty from renewable sources and\nloads. In this context, the microgrid concept is a promising approach, which is\nbased on a segmentation of the grid into independent smaller cells that can run\neither in grid-connected or standalone mode.In microgrids, droop control is\nwidely used for primary control. It enables proportional power sharing,\ndepending on the droop gains. Operation control schemes considering droop\ncontrol often assume fixed droop gains. However, using adaptive droop gains for\ngrid-forming units allow to shape power sharing in presence of fluctuations,\nenhancing flexibility while maintaining a safe microgrid operation,\nparticularly under uncertainty. This work introduces a bilinear formulation for\nmicrogrid operation control that finds optimal power setpoints and droop gains\non a timescale of minutes by solving a finite horizon optimization problem. In\ndetail, a robust minmax model predictive control scheme is designed for a\nstandalone microgrid, comprising a fuel cell, a photovoltaic system and an\nenergy storage. Closed-loop simulations are performed with and without variable\ndroop gains. The results show an increase in renewable utilization of up to 7.5\n% while reducing the power output of the fuel cell by 6 %, when allowing\nvariable droop gains.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15192v1", "AI": {"title_translation": "具有自适应下垂增益的微电网运行控制", "tldr": "本文提出了一种具有自适应下垂增益的双线性微电网运行控制方案，通过鲁棒minmax模型预测控制，提高了可再生能源利用率并降低了燃料电池输出。", "motivation": "现代低碳电力系统面临逆变器渗透率增加和可再生能源与负荷不确定性增加的挑战。微电网是解决这些问题的一种有前景的方法，其中下垂控制被广泛用于初级控制。然而，传统的下垂控制通常假设固定的下垂增益，这限制了在波动存在下塑造功率共享的灵活性。因此，需要一种能够利用自适应下垂增益来增强灵活性并保持安全运行的操作控制方案。", "method": "本文引入了一种用于微电网运行控制的双线性公式，通过解决一个有限时域优化问题，在分钟级时间尺度上寻找最优功率设定点和下垂增益。具体来说，为包含燃料电池、光伏系统和储能的独立微电网设计了一种鲁棒minmax模型预测控制方案。", "result": "在允许可变下垂增益的情况下，可再生能源利用率提高了高达7.5%，同时燃料电池的功率输出降低了6%。", "conclusion": "自适应下垂增益可以显著提高微电网中可再生能源的利用率并优化燃料电池的运行，从而增强微电网在不确定性下的灵活性和运行效率。", "translation": "现代低碳电力系统面临诸多挑战，例如逆变器渗透率增加以及可再生能源和负荷的不确定性增加。在此背景下，微电网概念是一种很有前景的方法，它基于将电网划分为独立的更小单元，这些单元可以以并网或独立模式运行。在微电网中，下垂控制被广泛用于初级控制。它根据下垂增益实现比例功率共享。考虑下垂控制的运行控制方案通常假设固定的下垂增益。然而，对并网单元使用自适应下垂增益可以在存在波动的情况下塑造功率共享，从而在保持微电网安全运行的同时提高灵活性，特别是在不确定性下。这项工作引入了一种用于微电网运行控制的双线性公式，通过解决一个有限时域优化问题，在分钟级时间尺度上寻找最优功率设定点和下垂增益。具体来说，为包含燃料电池、光伏系统和储能的独立微电网设计了一种鲁棒minmax模型预测控制方案。在有和没有可变下垂增益的情况下进行了闭环仿真。结果表明，在允许可变下垂增益的情况下，可再生能源利用率提高了高达7.5%，同时燃料电池的功率输出降低了6%。", "summary": "本文提出了一种针对现代低碳电力系统挑战的微电网运行控制方案，该方案利用自适应下垂增益来提高灵活性和应对不确定性。研究引入了一种双线性公式，并设计了一个鲁棒minmax模型预测控制方案，用于优化独立微电网中的功率设定点和下垂增益。仿真结果表明，与固定下垂增益相比，采用可变下垂增益可使可再生能源利用率提高7.5%，并使燃料电池功率输出降低6%。", "keywords": "微电网, 下垂控制, 自适应增益, 模型预测控制, 可再生能源", "comments": "该论文的创新点在于提出了将自适应下垂增益整合到微电网运行控制中的双线性公式和鲁棒minmax模型预测控制方案。这对于提高微电网在不确定性下的灵活性和可再生能源利用率具有重要意义，克服了传统固定下垂增益的局限性。"}}
{"id": "2506.15273", "title": "Reinforcement Learning-Based Policy Optimisation For Heterogeneous Radio Access", "authors": ["Anup Mishra", "Čedomir Stefanović", "Xiuqiang Xu", "Petar Popovski", "Israel Leyva-Mayorga"], "summary": "Flexible and efficient wireless resource sharing across heterogeneous\nservices is a key objective for future wireless networks. In this context, we\ninvestigate the performance of a system where latency-constrained\ninternet-of-things (IoT) devices coexist with a broadband user. The base\nstation adopts a grant-free access framework to manage resource allocation,\neither through orthogonal radio access network (RAN) slicing or by allowing\nshared access between services. For the IoT users, we propose a reinforcement\nlearning (RL) approach based on double Q-Learning (QL) to optimise their\nrepetition-based transmission strategy, allowing them to adapt to varying\nlevels of interference and meet a predefined latency target. We evaluate the\nsystem's performance in terms of the cumulative distribution function of IoT\nusers' latency, as well as the broadband user's throughput and energy\nefficiency (EE). Our results show that the proposed RL-based access policies\nsignificantly enhance the latency performance of IoT users in both RAN Slicing\nand RAN Sharing scenarios, while preserving desirable broadband throughput and\nEE. Furthermore, the proposed policies enable RAN Sharing to be\nenergy-efficient at low IoT traffic levels, and RAN Slicing to be favourable\nunder high IoT traffic.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15273v1", "AI": {"title_translation": "基于强化学习的异构无线接入策略优化", "tldr": "本文提出一种基于双Q学习的强化学习方法，优化异构网络中IoT设备的传输策略，以提高IoT延迟性能并保持宽带用户性能。", "motivation": "未来无线网络的一个关键目标是实现异构服务间灵活高效的无线资源共享。", "method": "论文研究了一个延迟受限的物联网设备与宽带用户共存的系统。基站采用免授权接入框架管理资源分配，通过正交无线接入网络切片或服务间共享接入。针对物联网用户，提出了一种基于双Q学习（QL）的强化学习（RL）方法，优化其基于重复的传输策略，使其适应不同干扰水平并满足预设的延迟目标。", "result": "所提出的基于RL的接入策略显著增强了物联网用户在RAN切片和RAN共享场景下的延迟性能，同时保持了理想的宽带吞吐量和能效。此外，该策略使得RAN共享在低物联网流量水平下具有能效优势，而RAN切片在高物联网流量下更为有利。", "conclusion": "所提出的基于RL的接入策略能够显著提升物联网用户的延迟性能，同时保持宽带用户的良好吞吐量和能效，并能根据物联网流量水平选择最佳的RAN接入方式。", "translation": "未来无线网络的一个关键目标是实现异构服务间灵活高效的无线资源共享。在此背景下，我们研究了一个延迟受限的物联网（IoT）设备与宽带用户共存的系统性能。基站采用免授权接入框架来管理资源分配，这可以通过正交无线接入网络（RAN）切片或允许服务间共享接入来实现。对于物联网用户，我们提出了一种基于双Q学习（QL）的强化学习（RL）方法，以优化其基于重复的传输策略，使其能够适应不同程度的干扰并满足预设的延迟目标。我们通过物联网用户延迟的累积分布函数以及宽带用户的吞吐量和能效（EE）来评估系统性能。我们的结果表明，所提出的基于RL的接入策略在RAN切片和RAN共享两种场景下都显著增强了物联网用户的延迟性能，同时保持了理想的宽带吞吐量和能效。此外，所提出的策略使得RAN共享在低物联网流量水平下具有能效优势，而RAN切片在高物联网流量下更为有利。", "summary": "本文研究了异构无线网络中物联网设备与宽带用户共存的场景下的资源共享问题。为优化物联网设备的延迟性能，论文提出了一种基于双Q学习的强化学习策略，使其能自适应调整传输以满足延迟目标。实验结果表明，该RL策略在RAN切片和RAN共享模式下均能显著提升物联网延迟，同时保持宽带性能，并能根据物联网流量动态选择最优的接入模式。", "keywords": "强化学习, 异构无线接入, 物联网, 双Q学习, 无线资源共享", "comments": "这篇论文通过引入强化学习（特别是双Q学习）来优化异构网络中物联网设备的传输策略，以解决延迟问题，具有一定的创新性。它不仅关注了物联网设备的性能，也兼顾了宽带用户的吞吐量和能效，体现了对异构网络复杂性的全面考虑。论文还提供了不同流量条件下RAN切片和RAN共享的适用性建议，具有实际指导意义。"}}
{"id": "2506.14789", "title": "AZT1D: A Real-World Dataset for Type 1 Diabetes", "authors": ["Saman Khamesian", "Asiful Arefeen", "Bithika M. Thompson", "Maria Adela Grando", "Hassan Ghasemzadeh"], "summary": "High quality real world datasets are essential for advancing data driven\napproaches in type 1 diabetes (T1D) management, including personalized therapy\ndesign, digital twin systems, and glucose prediction models. However, progress\nin this area has been limited by the scarcity of publicly available datasets\nthat offer detailed and comprehensive patient data. To address this gap, we\npresent AZT1D, a dataset containing data collected from 25 individuals with T1D\non automated insulin delivery (AID) systems. AZT1D includes continuous glucose\nmonitoring (CGM) data, insulin pump and insulin administration data,\ncarbohydrate intake, and device mode (regular, sleep, and exercise) obtained\nover 6 to 8 weeks for each patient. Notably, the dataset provides granular\ndetails on bolus insulin delivery (i.e., total dose, bolus type, correction\nspecific amounts) features that are rarely found in existing datasets. By\noffering rich, naturalistic data, AZT1D supports a wide range of artificial\nintelligence and machine learning applications aimed at improving clinical\ndecision making and individualized care in T1D.", "comment": "4 pages", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14789v1", "AI": {"title_translation": "AZT1D：一个真实世界的1型糖尿病数据集", "tldr": "AZT1D是一个新的真实世界数据集，包含来自25名使用自动胰岛素输送系统（AID）的1型糖尿病患者的详细数据，旨在解决现有公开数据集的稀缺性，支持AI/ML在T1D管理中的应用。", "motivation": "高质量的真实世界数据集对于推进1型糖尿病（T1D）管理中的数据驱动方法至关重要，包括个性化治疗设计、数字孪生系统和血糖预测模型。然而，现有公开数据集的稀缺性限制了该领域的进展。", "method": "我们提出了AZT1D数据集，该数据集收集了来自25名使用自动胰岛素输送系统（AID）的1型糖尿病患者的数据。数据包括连续血糖监测（CGM）数据、胰岛素泵和胰岛素给药数据、碳水化合物摄入量以及设备模式（常规、睡眠和运动），每个患者的数据收集了6到8周。", "result": "AZT1D数据集包含了25名T1D患者的数据，每位患者的数据收集了6至8周。该数据集提供了连续血糖监测（CGM）数据、胰岛素泵和胰岛素给药数据、碳水化合物摄入量以及设备模式。值得注意的是，该数据集提供了现有数据集中罕见的推注胰岛素输送（即总剂量、推注类型、校正具体量）的详细信息。", "conclusion": "AZT1D数据集通过提供丰富的自然数据，支持广泛的人工智能和机器学习应用，旨在改善1型糖尿病的临床决策和个体化护理。", "translation": "高质量的真实世界数据集对于推进1型糖尿病（T1D）管理中的数据驱动方法至关重要，包括个性化治疗设计、数字孪生系统和血糖预测模型。然而，该领域的进展受到公开可用数据集稀缺性的限制，这些数据集提供详细和全面的患者数据。为了弥补这一空白，我们提出了AZT1D，这是一个包含从25名使用自动胰岛素输送（AID）系统的T1D患者收集的数据集。AZT1D包括连续血糖监测（CGM）数据、胰岛素泵和胰岛素给药数据、碳水化合物摄入量以及每位患者6到8周的设备模式（常规、睡眠和运动）。值得注意的是，该数据集提供了推注胰岛素输送（即总剂量、推注类型、校正具体量）的详细信息，这些特征在现有数据集中很少发现。通过提供丰富的、自然的数据，AZT1D支持广泛的人工智能和机器学习应用，旨在改善T1D的临床决策和个体化护理。", "summary": "AZT1D是一个新颖的真实世界数据集，旨在解决1型糖尿病（T1D）领域高质量公开数据集的稀缺问题。该数据集包含了25名使用自动胰岛素输送系统（AID）的T1D患者的详细数据，包括连续血糖监测、胰岛素输送和碳水化合物摄入等。其独特之处在于提供了罕见的推注胰岛素输送的详细信息。AZT1D旨在支持人工智能和机器学习在T1D管理中的应用，以改善临床决策和个性化护理。", "keywords": "1型糖尿病, 真实世界数据, 数据集, 自动胰岛素输送, 机器学习", "comments": "AZT1D数据集的创新之处在于其对推注胰岛素输送的详细记录，这在现有公开数据集中是罕见的。这使得该数据集对于开发更精细、更个性化的T1D管理算法具有重要价值，尤其是在数字孪生和血糖预测模型方面。"}}
{"id": "2506.15228", "title": "ABC: Adaptive BayesNet Structure Learning for Computational Scalable Multi-task Image Compression", "authors": ["Yufeng Zhang", "Wenrui Dai", "Hang Yu", "Shizhan Liu", "Junhui Hou", "Jianguo Li", "Weiyao Lin"], "summary": "Neural Image Compression (NIC) has revolutionized image compression with its\nsuperior rate-distortion performance and multi-task capabilities, supporting\nboth human visual perception and machine vision tasks. However, its widespread\nadoption is hindered by substantial computational demands. While existing\napproaches attempt to address this challenge through module-specific\noptimizations or pre-defined complexity levels, they lack comprehensive control\nover computational complexity. We present ABC (Adaptive BayesNet structure\nlearning for computational scalable multi-task image Compression), a novel,\ncomprehensive framework that achieves computational scalability across all NIC\ncomponents through Bayesian network (BayesNet) structure learning. ABC\nintroduces three key innovations: (i) a heterogeneous bipartite BayesNet\n(inter-node structure) for managing neural backbone computations; (ii) a\nhomogeneous multipartite BayesNet (intra-node structure) for optimizing\nautoregressive unit processing; and (iii) an adaptive control module that\ndynamically adjusts the BayesNet structure based on device capabilities, input\ndata complexity, and downstream task requirements. Experiments demonstrate that\nABC enables full computational scalability with better complexity adaptivity\nand broader complexity control span, while maintaining competitive compression\nperformance. Furthermore, the framework's versatility allows integration with\nvarious NIC architectures that employ BayesNet representations, making it a\nrobust solution for ensuring computational scalability in NIC applications.\nCode is available in https://github.com/worldlife123/cbench_BaSIC.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15228v1", "AI": {"title_translation": "ABC：计算可扩展多任务图像压缩的自适应贝叶斯网络结构学习", "tldr": "ABC是一个新颖的框架，通过贝叶斯网络结构学习，为神经图像压缩（NIC）提供全面的计算可扩展性，同时保持竞争力。", "motivation": "神经图像压缩（NIC）虽然在性能和多任务能力上表现优越，但其高计算需求阻碍了广泛应用。现有方法缺乏对计算复杂度的全面控制。", "method": "本文提出了ABC框架，通过贝叶斯网络（BayesNet）结构学习实现NIC所有组件的计算可扩展性。主要创新包括：(i) 用于管理神经网络骨干计算的异构二分贝叶斯网络；(ii) 用于优化自回归单元处理的同构多方贝叶斯网络；(iii) 根据设备能力、输入数据复杂度和下游任务需求动态调整贝叶斯网络结构的自适应控制模块。", "result": "实验表明，ABC实现了全面的计算可扩展性，具有更好的复杂度适应性和更广的复杂度控制范围，同时保持了有竞争力的压缩性能。", "conclusion": "ABC框架的通用性使其能够与各种采用贝叶斯网络表示的NIC架构集成，是确保NIC应用中计算可扩展性的强大解决方案。", "translation": "神经图像压缩（NIC）以其卓越的速率-失真性能和多任务能力（支持人类视觉感知和机器视觉任务）彻底改变了图像压缩。然而，其广泛采用受到巨大的计算需求的阻碍。尽管现有方法试图通过模块特定优化或预定义复杂度级别来解决这一挑战，但它们缺乏对计算复杂度的全面控制。我们提出了ABC（用于计算可扩展多任务图像压缩的自适应贝叶斯网络结构学习），这是一个新颖、全面的框架，通过贝叶斯网络（BayesNet）结构学习在所有NIC组件中实现计算可扩展性。ABC引入了三项关键创新：(i) 用于管理神经网络骨干计算的异构二分贝叶斯网络（节点间结构）；(ii) 用于优化自回归单元处理的同构多方贝叶斯网络（节点内结构）；以及(iii) 根据设备能力、输入数据复杂度以及下游任务需求动态调整贝叶斯网络结构的自适应控制模块。实验表明，ABC实现了全面的计算可扩展性，具有更好的复杂度适应性和更广的复杂度控制范围，同时保持了有竞争力的压缩性能。此外，该框架的通用性使其能够与各种采用贝叶斯网络表示的NIC架构集成，使其成为确保NIC应用中计算可扩展性的强大解决方案。代码可在https://github.com/worldlife123/cbench_BaSIC 获取。", "summary": "本文提出了ABC框架，旨在解决神经图像压缩（NIC）高计算需求的问题。ABC通过引入异构二分贝叶斯网络、同构多方贝叶斯网络和自适应控制模块，实现了对NIC所有组件的全面计算可扩展性。实验证明，ABC在保持竞争性压缩性能的同时，提供了更好的复杂度适应性和更广的复杂度控制范围，为NIC应用提供了强大的计算可扩展性解决方案。", "keywords": "神经图像压缩,计算可扩展性,贝叶斯网络,多任务,结构学习", "comments": "本文的创新之处在于其通过贝叶斯网络结构学习，为神经图像压缩提供了细粒度和全面的计算复杂度控制，而非仅仅是模块层面的优化。这种自适应的结构调整能力，使其能够根据具体设备和任务需求动态优化计算资源，显著提升了NIC的实用性。其通用性也使其能够集成到现有多种NIC架构中，具有重要的实际应用价值。"}}
{"id": "2506.15612", "title": "A survey of Chernoff and Hoeffding bounds", "authors": ["Alexandros V. Gerbessiotis"], "summary": "This is a survey paper that discusses the original bounds of the seminal\npapers by Chernoff and Hoeffding. Moreover, it includes a variety of derivative\nbounds in a variety of forms. Complete proofs are provided as needed. The\nintent is to provide a repository of reference bounds for the interested\nresearcher.", "comment": null, "cate": "cs.DM", "url": "http://arxiv.org/abs/2506.15612v1", "AI": {"title_translation": "Chernoff 和 Hoeffding 界限的综述", "tldr": "对Chernoff和Hoeffding界限及其衍生形式的综述，为研究者提供参考。", "motivation": "旨在为感兴趣的研究人员提供一个关于Chernoff和Hoeffding界限及其各种衍生形式的参考存储库。", "method": "本文讨论了Chernoff和Hoeffding的原始界限，并包含了多种形式的衍生界限，根据需要提供了完整的证明。", "result": "提供了一个关于Chernoff和Hoeffding界限及其各种衍生形式的全面综述。", "conclusion": "本文旨在为感兴趣的研究人员提供一个参考界限的存储库。", "translation": "这是一篇综述性论文，讨论了Chernoff和Hoeffding的开创性论文中的原始界限。此外，它还包括各种形式的多种衍生界限。根据需要提供了完整的证明。其目的是为感兴趣的研究人员提供一个参考界限的存储库。", "summary": "本文是一篇综述，系统地讨论了Chernoff和Hoeffding的原始界限及其多种衍生形式。论文提供了必要的完整证明，旨在为研究人员提供一个实用的参考界限库。", "keywords": "Chernoff bounds, Hoeffding bounds, Survey, Probability bounds, Statistical bounds", "comments": "这篇综述论文对于需要深入理解和应用Chernoff和Hoeffding界限的研究人员来说非常重要，因为它汇集了原始概念和各种衍生形式，并提供了证明，极大地便利了查阅和学习。"}}
{"id": "2506.15068", "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation", "authors": ["Zongxia Li", "Yapei Chang", "Yuhang Zhou", "Xiyang Wu", "Zichao Liang", "Yoo Yeon Sung", "Jordan Lee Boyd-Graber"], "summary": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15068v1", "AI": {"title_translation": "用于自由形式生成中开放式R1训练的语义感知奖励", "tldr": "PrefBERT为开放式长文本生成提供更好的语义奖励，提升了与人类偏好的一致性。", "motivation": "开放式长文本生成评估面临挑战，现有方法难以区分好坏输出，且常忽略连贯性、风格、相关性，或受预训练数据偏见影响，导致该领域研究不足。", "method": "提出PrefBERT模型，一个评分模型，用于评估GRPO中的开放式长文本生成，并通过区分好坏输出来指导训练。PrefBERT在两个包含多样长文本风格和Likert评分质量的响应评估数据集上进行训练。", "result": "PrefBERT比ROUGE-L和BERTScore等传统指标提供更好的语义奖励反馈，有效支持GRPO。通过LLM-as-a-judge、人工评分和定性分析等综合评估，PrefBERT在多句和段落长度响应上训练后，在不同长篇幅文本中保持可靠性，并与GRPO所需的奖励对齐。人工评估证实，使用PrefBERT作为奖励信号训练策略模型，生成响应更符合人类偏好。", "conclusion": "PrefBERT通过提供语义感知奖励，有效解决了开放式长文本生成评估的挑战，并显著提升了生成模型与人类偏好的一致性。", "translation": "评估开放式长文本生成具有挑战性，因为很难明确区分好坏输出。现有方法通常会遗漏连贯性、风格或相关性等关键方面，或者受到预训练数据的偏见，使得开放式长文本评估成为一个未充分探索的问题。为了解决这一空白，我们提出了PrefBERT，一个评分模型，用于评估GRPO中的开放式长文本生成，并通过对好坏输出提供不同的奖励来指导其训练。PrefBERT在两个包含多样长文本风格和Likert评分质量的响应评估数据集上进行训练，它通过提供比传统指标ROUGE-L和BERTScore更好的语义奖励反馈，有效支持GRPO。通过全面的评估，包括LLM-as-a-judge、人工评分和定性分析，我们表明PrefBERT在多句和段落长度响应上训练后，在不同长篇幅文本中保持可靠性，并与GRPO所需的验证性奖励良好对齐。人工评估证实，使用PrefBERT作为奖励信号来训练策略模型，产生的响应比使用传统指标训练的响应更符合人类偏好。我们的代码可在https://github.com/zli12321/long_form_rl 获取。", "summary": "这篇论文提出了PrefBERT，一个用于评估和指导开放式长文本生成训练的语义感知评分模型。针对现有评估方法在连贯性、风格和相关性上的不足以及预训练数据偏差问题，PrefBERT通过提供优于传统指标的语义奖励反馈来支持GRPO。通过LLM-as-a-judge和人工评估，研究表明PrefBERT能有效提高生成响应与人类偏好的一致性，证明了其在开放式长文本生成评估中的可靠性和有效性。", "keywords": "开放式生成, 语义奖励, PrefBERT, 长文本生成, GRPO", "comments": "这项工作提出PrefBERT，一个新颖的语义感知奖励模型，有效解决了开放式长文本生成评估的难题。其创新点在于通过区分好坏输出来指导训练，并提供了比传统指标更优的语义反馈。通过多样的评估方法，证明了其在对齐人类偏好方面的优越性，对RLHF在长文本生成领域的应用具有重要意义。"}}
{"id": "2506.14814", "title": "Semi-orthogonal Tribonacci Wavelets and Numerical Solutions of Nonlinear Singular BVPs Arising in a Chemical Reaction", "authors": ["Ankita Yadav", "Amit K. Verma"], "summary": "In this article, we introduce a semi-orthogonal tribonacci wavelet and\ndevelop a semi-orthogonal tribonacci wavelet collocation method, offering an\neffective numerical method for solving a class of non-linear singular BVPs.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14814v1", "AI": {"title_translation": "半正交Tribonacci小波及其在化学反应中非线性奇异边值问题数值解的应用", "tldr": "引入半正交Tribonacci小波并开发了其配置方法，用于求解非线性奇异边值问题。", "motivation": "旨在为一类非线性奇异边值问题提供一种有效的数值解法。", "method": "引入了一种半正交Tribonacci小波，并开发了一种半正交Tribonacci小波配置方法。", "result": "提出了一种有效的非线性奇异边值问题数值解法。", "conclusion": "Not mentioned in abstract", "translation": "在本文中，我们引入了一种半正交Tribonacci小波，并开发了一种半正交Tribonacci小波配置方法，为解决一类非线性奇异边值问题提供了一种有效的数值方法。", "summary": "本文引入了一种新型的半正交Tribonacci小波及其配置方法，旨在为解决化学反应中出现的一类非线性奇异边值问题提供一种高效的数值解决方案。", "keywords": "半正交Tribonacci小波, 小波配置方法, 非线性奇异边值问题, 数值解", "comments": "本文的创新之处在于引入了一种新的小波类型——半正交Tribonacci小波，并将其应用于非线性奇异边值问题的数值求解，这为解决这类复杂问题提供了新的工具和方法。"}}
{"id": "2506.15614", "title": "TTSOps: A Closed-Loop Corpus Optimization Framework for Training Multi-Speaker TTS Models from Dark Data", "authors": ["Kentaro Seki", "Shinnosuke Takamichi", "Takaaki Saeki", "Hiroshi Saruwatari"], "summary": "This paper presents TTSOps, a fully automated closed-loop framework for\nconstructing multi-speaker text-to-speech (TTS) systems from noisy, uncurated\nweb-scale speech data, often referred to as ``dark data,'' such as online\nvideos. Conventional TTS training pipelines require well-curated corpora with\nhigh acoustic quality and accurate text-speech alignment, which severely limits\nscalability, speaker diversity, and real-world applicability. While recent\nstudies have proposed acoustic-quality-based data selection techniques, they\noften overlook two critical aspects: (1) the inherent robustness of modern TTS\nmodels to noise, and (2) the potential contribution of perceptually low-quality\nyet informative samples. To address these issues, TTSOps introduces a\ndata-centric training pipeline that integrates three core components: (1)\nautomated data collection from dark data sources, (2) utterance-level dynamic\nselection of data cleansing methods based on training data quality, and (3)\nevaluation-in-the-loop data selection using automatically predicted mean\nopinion scores (MOS) to estimate each utterance's impact on model performance.\nFurthermore, TTSOps jointly optimizes the corpus and the TTS model in a\nclosed-loop framework by dynamically adapting both data selection and data\ncleansing processes to the characteristics of the target TTS model. Extensive\nexperiments on Japanese YouTube data demonstrate that TTSOps outperforms\nconventional acoustic-quality-based baselines in both the naturalness and\nspeaker diversity of synthesized speech.", "comment": null, "cate": "cs.SD", "url": "http://arxiv.org/abs/2506.15614v1", "AI": {"title_translation": "TTSOps：一个用于从暗数据训练多说话人TTS模型的闭环语料库优化框架", "tldr": "TTSOps是一个自动化的闭环框架，用于从嘈杂的“暗数据”构建多说话人文本到语音（TTS）系统，通过动态数据选择和清洗以及基于预测MOS的评估来优化语料库和模型，从而在语音自然度和说话人多样性方面优于传统方法。", "motivation": "传统的TTS训练需要高质量且对齐良好的语料库，这限制了可扩展性、说话人多样性和实际应用。现有的数据选择技术忽略了现代TTS模型对噪声的鲁棒性以及感知质量低但信息丰富的样本的潜在贡献。", "method": "TTSOps引入了一个以数据为中心的训练流程，包含三个核心组件：1) 从暗数据源自动收集数据；2) 基于训练数据质量的语音级动态数据清洗方法选择；3) 使用自动预测的平均意见得分（MOS）进行评估循环数据选择。此外，TTSOps通过动态调整数据选择和数据清洗过程以适应目标TTS模型，在闭环框架中共同优化语料库和TTS模型。", "result": "在日语YouTube数据上的大量实验表明，TTSOps在合成语音的自然度和说话人多样性方面均优于传统的基于声学质量的基线方法。", "conclusion": "Not mentioned in abstract", "translation": "本文提出了TTSOps，一个完全自动化的闭环框架，用于从嘈杂、未经整理的网络规模语音数据（通常称为“暗数据”，如在线视频）构建多说话人文本到语音（TTS）系统。传统的TTS训练流程需要精心整理的、具有高声学质量和准确文本-语音对齐的语料库，这严重限制了可扩展性、说话人多样性和实际适用性。虽然最近的研究提出了基于声学质量的数据选择技术，但它们往往忽视了两个关键方面：(1) 现代TTS模型固有的噪声鲁棒性，以及(2) 感知质量较低但信息丰富的样本的潜在贡献。为了解决这些问题，TTSOps引入了一个以数据为中心的训练流程，该流程集成了三个核心组件：(1) 从暗数据源自动收集数据，(2) 基于训练数据质量的语音级动态数据清洗方法选择，以及(3) 使用自动预测的平均意见得分（MOS）进行评估循环数据选择，以估计每个语音对模型性能的影响。此外，TTSOps通过动态调整数据选择和数据清洗过程以适应目标TTS模型，在闭环框架中共同优化语料库和TTS模型。在日语YouTube数据上的大量实验表明，TTSOps在合成语音的自然度和说话人多样性方面均优于传统的基于声学质量的基线方法。", "summary": "TTSOps是一个创新的闭环框架，旨在解决从嘈杂的“暗数据”构建多说话人TTS系统所面临的挑战。它通过整合自动化数据收集、动态数据清洗选择和基于预测MOS的评估循环数据选择，实现了数据和TTS模型的共同优化。该框架克服了传统方法对高质量语料库的依赖，并充分利用了即使是感知质量较低但信息丰富的样本，从而在合成语音的自然度和说话人多样性方面取得了显著提升。", "keywords": "TTS, 多说话人, 暗数据, 语料库优化, 闭环框架", "comments": "该论文的创新之处在于其提出的TTSOps框架，它解决了传统TTS模型训练中对高质量、精心整理语料库的严格依赖，并通过利用“暗数据”极大地扩展了数据来源。其核心贡献在于引入了一个数据驱动的闭环优化范式，该范式能够动态地选择和清洗数据，并利用预测的MOS来评估数据对模型性能的影响，这使得系统能够从嘈杂数据中学习并生成更自然和多样化的语音。这种方法对于降低TTS系统开发的门槛和提高其在真实世界中的适用性具有重要意义。"}}
{"id": "2506.15102", "title": "EVA-S2PMLP: Secure and Scalable Two-Party MLP via Spatial Transformation", "authors": ["Shizhao Peng", "Shoumo Li", "Tianle Tao"], "summary": "Privacy-preserving neural network training in vertically partitioned\nscenarios is vital for secure collaborative modeling across institutions. This\npaper presents \\textbf{EVA-S2PMLP}, an Efficient, Verifiable, and Accurate\nSecure Two-Party Multi-Layer Perceptron framework that introduces spatial-scale\noptimization for enhanced privacy and performance. To enable reliable\ncomputation under real-number domain, EVA-S2PMLP proposes a secure\ntransformation pipeline that maps scalar inputs to vector and matrix spaces\nwhile preserving correctness. The framework includes a suite of atomic\nprotocols for linear and non-linear secure computations, with modular support\nfor secure activation, matrix-vector operations, and loss evaluation.\nTheoretical analysis confirms the reliability, security, and asymptotic\ncomplexity of each protocol. Extensive experiments show that EVA-S2PMLP\nachieves high inference accuracy and significantly reduced communication\noverhead, with up to $12.3\\times$ improvement over baselines. Evaluation on\nbenchmark datasets demonstrates that the framework maintains model utility\nwhile ensuring strict data confidentiality, making it a practical solution for\nprivacy-preserving neural network training in finance, healthcare, and\ncross-organizational AI applications.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15102v1", "AI": {"title_translation": "EVA-S2PMLP：通过空间变换实现安全可扩展的两方MLP", "tldr": "EVA-S2PMLP是一个高效、可验证、准确的安全两方MLP框架，通过空间变换优化，显著提高了隐私保护神经网络训练的性能和准确性，并大幅降低了通信开销。", "motivation": "在垂直分区场景下，机构间安全协作建模中保护隐私的神经网络训练至关重要。", "method": "提出了EVA-S2PMLP框架，引入空间尺度优化，通过安全的变换管道将标量输入映射到向量和矩阵空间，以在实数域下实现可靠计算。该框架包含一系列用于线性和非线性安全计算的原子协议，支持安全激活、矩阵-向量操作和损失评估。", "result": "实验表明EVA-S2PMLP实现了高推理精度并显著降低了通信开销，比基线提高了高达12.3倍。在基准数据集上的评估证明该框架在确保严格数据保密性的同时保持了模型效用。", "conclusion": "EVA-S2PMLP为金融、医疗保健和跨组织AI应用中的隐私保护神经网络训练提供了一个实用的解决方案。", "translation": "在垂直分区场景下，隐私保护神经网络训练对于机构间的安全协作建模至关重要。本文提出了EVA-S2PMLP，一个高效、可验证、准确的安全两方多层感知器框架，该框架引入了空间尺度优化以增强隐私和性能。为了在实数域下实现可靠计算，EVA-S2PMLP提出了一种安全的变换管道，将标量输入映射到向量和矩阵空间，同时保持正确性。该框架包含一系列用于线性和非线性安全计算的原子协议，模块化支持安全激活、矩阵-向量操作和损失评估。理论分析证实了每个协议的可靠性、安全性和渐近复杂性。广泛的实验表明，EVA-S2PMLP实现了高推理精度并显著降低了通信开销，比基线提高了高达12.3倍。在基准数据集上的评估表明，该框架在确保严格数据保密性的同时保持了模型效用，使其成为金融、医疗保健和跨组织AI应用中隐私保护神经网络训练的实用解决方案。", "summary": "EVA-S2PMLP是一个创新的安全两方多层感知器框架，专为垂直分区场景下的隐私保护神经网络训练设计。它通过引入空间尺度优化和安全的变换管道，将标量输入映射到高维空间，从而在保证数据机密性的同时，显著提升了计算效率和模型精度。实验结果表明，该框架在通信开销方面有显著改进，并保持了良好的模型效用，使其适用于金融、医疗等领域的安全AI应用。", "keywords": "隐私保护, 两方计算, 多层感知器, 空间变换, 安全计算", "comments": "EVA-S2PMLP的创新点在于其空间尺度优化和安全的变换管道，实现了在隐私保护下的高效实数域计算，这对于实际应用具有重要意义。其在通信开销上的显著降低解决了现有隐私保护计算方法的瓶颈，使其成为一个实用的解决方案。"}}
{"id": "2506.15655", "title": "cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree", "authors": ["Yilin Zhang", "Xinran Zhao", "Zora Zhiruo Wang", "Chenyang Yang", "Jiayi Wei", "Tongshuang Wu"], "summary": "Retrieval-Augmented Generation (RAG) has become essential for large-scale\ncode generation, grounding predictions in external code corpora to improve\nactuality. However, a critical yet underexplored aspect of RAG pipelines is\nchunking -- the process of dividing documents into retrievable units. Existing\nline-based chunking heuristics often break semantic structures, splitting\nfunctions or merging unrelated code, which can degrade generation quality. We\npropose chunking via Abstract Syntax Trees (\\ourwork), a structure-aware method\nthat recursively breaks large AST nodes into smaller chunks and merges sibling\nnodes while respecting size limits. This approach generates self-contained,\nsemantically coherent units across programming languages and tasks, improving\nperformance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3\npoints on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.\nOur work highlights the importance of structure-aware chunking for scaling\nretrieval-enhanced code intelligence.", "comment": null, "cate": "cs.SE", "url": "http://arxiv.org/abs/2506.15655v1", "AI": {"title_translation": "cAST：通过抽象语法树进行结构化分块，增强代码检索增强生成", "tldr": "cAST通过使用抽象语法树进行结构化分块，解决了代码检索增强生成中现有分块方法破坏语义结构的问题，显著提升了代码生成性能。", "motivation": "检索增强生成（RAG）在代码生成中至关重要，但现有基于行的分块启发式方法常破坏代码的语义结构，导致生成质量下降。这促使研究者探索更有效的分块策略。", "method": "本文提出了cAST（通过抽象语法树分块），这是一种结构感知方法。它递归地将大型AST节点分解为更小的块，并合并兄弟节点，同时遵守大小限制。这种方法能够生成跨编程语言和任务的自包含、语义连贯的单元。", "result": "该方法在各种代码生成任务中提升了性能。例如，在RepoEval检索中，Recall@5提高了4.3个百分点；在SWE-bench生成中，Pass@1提高了2.67个百分点。", "conclusion": "本研究强调了结构感知分块对于扩展检索增强型代码智能的重要性。", "translation": "检索增强生成（RAG）已成为大规模代码生成的关键，通过将预测基于外部代码语料库来提高实际性。然而，RAG管道中一个关键但未被充分探索的方面是分块——即将文档划分为可检索单元的过程。现有基于行的分块启发式方法常常破坏语义结构，分割函数或合并不相关的代码，这会降低生成质量。我们提出了通过抽象语法树进行分块（cAST），这是一种结构感知方法，它递归地将大型AST节点分解为更小的块，并在遵守大小限制的同时合并兄弟节点。这种方法在不同编程语言和任务中生成自包含、语义连贯的单元，从而提高了各种代码生成任务的性能，例如在RepoEval检索中将Recall@5提高了4.3个百分点，在SWE-bench生成中将Pass@1提高了2.67个百分点。我们的工作强调了结构感知分块对于扩展检索增强型代码智能的重要性。", "summary": "本文提出cAST，一种基于抽象语法树的结构化分块方法，旨在解决代码检索增强生成（RAG）中传统行式分块破坏代码语义结构的问题。cAST通过递归分解和合并AST节点生成语义连贯的代码单元。实验结果表明，cAST显著提升了代码检索和生成的性能，验证了结构感知分块对于代码智能的重要性。", "keywords": "代码生成, 检索增强生成, 抽象语法树, 分块, 语义结构", "comments": "这项工作创新性地将抽象语法树应用于代码分块，解决了传统行式分块的语义断裂问题。其重要性在于通过改进RAG的关键环节，显著提升了代码生成模型的性能和代码智能的扩展潜力。该方法具有跨语言和任务的普适性。"}}
{"id": "2506.15085", "title": "EmojiVoice: Towards long-term controllable expressivity in robot speech", "authors": ["Paige Tuttösí", "Shivam Mehta", "Zachary Syvenky", "Bermet Burkanova", "Gustav Eje Henter", "Angelica Lim"], "summary": "Humans vary their expressivity when speaking for extended periods to maintain\nengagement with their listener. Although social robots tend to be deployed with\n``expressive'' joyful voices, they lack this long-term variation found in human\nspeech. Foundation model text-to-speech systems are beginning to mimic the\nexpressivity in human speech, but they are difficult to deploy offline on\nrobots. We present EmojiVoice, a free, customizable text-to-speech (TTS)\ntoolkit that allows social roboticists to build temporally variable, expressive\nspeech on social robots. We introduce emoji-prompting to allow fine-grained\ncontrol of expressivity on a phase level and use the lightweight Matcha-TTS\nbackbone to generate speech in real-time. We explore three case studies: (1) a\nscripted conversation with a robot assistant, (2) a storytelling robot, and (3)\nan autonomous speech-to-speech interactive agent. We found that using varied\nemoji prompting improved the perception and expressivity of speech over a long\nperiod in a storytelling task, but expressive voice was not preferred in the\nassistant use case.", "comment": "Accepted to RO-MAN 2025, Demo at HRI 2025 :\n  https://dl.acm.org/doi/10.5555/3721488.3721774", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15085v1", "AI": {"title_translation": "EmojiVoice：迈向机器人语音的长期可控表现力", "tldr": "EmojiVoice是一个免费、可定制的文本转语音工具包，通过表情符号提示实现机器人语音的长期可控表现力，并在讲故事任务中有效，但在机器人助手任务中不受青睐。", "motivation": "人类在长时间讲话时会改变表达方式以保持听众的参与度，但社交机器人缺乏这种长期变化。虽然基础模型文本转语音系统开始模仿人类语音的表现力，但它们难以在机器人上离线部署。", "method": "本文提出了EmojiVoice，一个免费、可定制的文本转语音（TTS）工具包，允许社交机器人专家在社交机器人上构建时间可变、富有表现力的语音。该方法引入了表情符号提示（emoji-prompting）以实现对阶段级表现力的细粒度控制，并使用轻量级的Matcha-TTS骨干网络实时生成语音。", "result": "通过三个案例研究发现，在讲故事任务中，使用不同的表情符号提示改善了语音在长时间内的感知和表现力，但在助手用例中，富有表现力的声音并未受到青睐。", "conclusion": "EmojiVoice能够为社交机器人提供长期可控的、富有表现力的语音，尤其在讲故事等任务中表现良好，但其有效性和偏好因具体应用场景而异。", "translation": "人类在长时间讲话时会改变表达方式以保持听众的参与度。尽管社交机器人通常部署有“富有表现力”的快乐声音，但它们缺乏人类语音中发现的这种长期变化。基础模型文本转语音系统开始模仿人类语音的表现力，但它们难以在机器人上离线部署。我们提出了EmojiVoice，一个免费、可定制的文本转语音（TTS）工具包，允许社交机器人专家在社交机器人上构建时间可变、富有表现力的语音。我们引入了表情符号提示（emoji-prompting）以实现对阶段级表现力的细粒度控制，并使用轻量级的Matcha-TTS骨干网络实时生成语音。我们探索了三个案例研究：（1）与机器人助手的脚本对话，（2）一个讲故事机器人，以及（3）一个自主的语音到语音交互代理。我们发现，在讲故事任务中，使用不同的表情符号提示改善了语音在长时间内的感知和表现力，但在助手用例中，富有表现力的声音并未受到青睐。", "summary": "EmojiVoice是一个创新的文本转语音（TTS）工具包，旨在解决社交机器人缺乏长期可变语音表现力的问题。它通过引入表情符号提示实现对语音表现力的细粒度、阶段级控制，并利用轻量级Matcha-TTS骨干网络实现实时语音生成，使其适合离线部署在机器人上。研究通过案例研究展示，EmojiVoice在讲故事任务中显著提升了语音的感知和表现力，但在机器人助手场景中，其表现力并未受到用户偏好。", "keywords": "EmojiVoice, 机器人语音, 表现力, 文本转语音, 表情符号提示", "comments": "该论文提出了一种新颖的方法，通过“表情符号提示”实现机器人语音表现力的细粒度控制，解决了现有基础模型难以离线部署的痛点，具有较高的创新性。其重要性在于为社交机器人提供了更自然、更具吸引力的长期交互能力。然而，研究结果也揭示了其局限性，即表现力的偏好因应用场景而异，这提示未来需要更精细化的上下文感知表现力控制策略。"}}
{"id": "2506.15467", "title": "Towards Weight Distribution-Aware Polar Codes", "authors": ["Mohammad Rowshan", "Vlad-Florin Dragoi"], "summary": "Polar codes are constructed based on the reliability of sub-channels\nresulting from the polarization effect. However, this information-theoretic\nconstruction approach leads to a poor weight distribution. To address this\nissue, pre-transformed polar codes, such as CRC-polar codes and PAC codes, have\nbeen employed. In this paper, we focus on the structure of polar codes without\napplying any pre-transformations and explore methods, guided by the\nweight-contribution partial order, to design polar-like codes with enhanced\nweight distribution, notably without employing any search or optimization\nalgorithms. Numerical results demonstrate improvement over a range of codes\nboth with and without pre-transformation.", "comment": "Accepted and to be presented at IEEE ISIT'25", "cate": "cs.IT", "url": "http://arxiv.org/abs/2506.15467v1", "AI": {"title_translation": "趋向于权重分布感知的极化码", "tldr": "本文提出了一种基于权重贡献偏序设计极化码的方法，无需预变换或优化算法，显著改善了码字的权重分布。", "motivation": "极化码的传统构造方法导致其权重分布较差。", "method": "本文在不应用任何预变换的情况下，通过权重贡献偏序指导，设计具有增强权重分布的类极化码，且不采用任何搜索或优化算法。", "result": "数值结果表明，与一系列有或没有预变换的码相比，所提出的方法取得了改进。", "conclusion": "本文成功设计了一种无需预变换和优化算法即可改善权重分布的极化码。", "translation": "极化码是基于极化效应产生的子信道可靠性来构造的。然而，这种信息论构造方法导致了较差的权重分布。为了解决这个问题，预变换极化码，如CRC-极化码和PAC码，已被采用。在本文中，我们关注不应用任何预变换的极化码结构，并探索在权重贡献偏序指导下的方法，以设计具有增强权重分布的类极化码，值得注意的是，这不使用任何搜索或优化算法。数值结果表明，与一系列有或没有预变换的码相比，取得了改进。", "summary": "本文针对极化码固有的差权重分布问题，提出了一种不依赖预变换或搜索优化算法的新型设计方法。该方法利用权重贡献偏序来指导极化码的结构设计，以提升其权重分布。数值模拟结果验证了该方法在性能上优于现有有无预变换的极化码。", "keywords": "极化码, 权重分布, 权重贡献偏序, 无预变换, 编码设计", "comments": "这项工作具有创新性，因为它提出了一种无需复杂预变换或计算密集型优化算法即可改善极化码权重分布的方法，这可能为极化码的设计开辟新的方向，并降低实现复杂性。"}}
{"id": "2506.14833", "title": "Real-Time, Low-Latency Surveillance Using Entropy-Based Adaptive Buffering and MobileNetV2 on Edge Devices", "authors": ["Poojashree Chandrashekar Pankaj M Sajjanar"], "summary": "This paper describes a high-performance, low-latency video surveillance\nsystem designed for resource-constrained environments. We have proposed a\nformal entropy-based adaptive frame buffering algorithm and integrated that\nwith MobileNetV2 to achieve high throughput with low latency. The system is\ncapable of processing live streams of video with sub-50ms end-to-end inference\nlatency on resource-constrained devices (embedding platforms) such as Raspberry\nPi, Amazon, and NVIDIA Jetson Nano. Our method maintains over 92% detection\naccuracy on standard datasets focused on video surveillance and exhibits\nrobustness to varying lighting, backgrounds, and speeds. A number of\ncomparative and ablation experiments validate the effectiveness of our design.\nFinally, our architecture is scalable, inexpensive, and compliant with stricter\ndata privacy regulations than common surveillance systems, so that the system\ncould coexist in a smart city or embedded security architecture.", "comment": "& pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14833v1", "AI": {"title_translation": "基于熵的自适应缓冲和MobileNetV2在边缘设备上的实时低延迟监控", "tldr": "该论文提出了一种结合熵自适应缓冲和MobileNetV2的实时低延迟视频监控系统，可在资源受限的边缘设备上实现高性能、高精度且符合隐私规范的监控。", "motivation": "在资源受限的环境中实现高性能、低延迟的视频监控。", "method": "提出了一种基于熵的自适应帧缓冲算法，并将其与MobileNetV2集成，以实现高吞吐量和低延迟。通过比较和消融实验验证设计。", "result": "在树莓派、亚马逊和NVIDIA Jetson Nano等资源受限设备上，实现低于50ms的端到端推理延迟。在标准视频监控数据集上保持超过92%的检测精度，并对光照、背景和速度变化表现出鲁棒性。", "conclusion": "该系统架构可扩展、成本低廉，且比常见监控系统更符合严格的数据隐私法规，适用于智慧城市或嵌入式安全架构。", "translation": "本文描述了一种专为资源受限环境设计的高性能、低延迟视频监控系统。我们提出了一种正式的基于熵的自适应帧缓冲算法，并将其与MobileNetV2集成，以实现高吞吐量和低延迟。该系统能够在树莓派、亚马逊和NVIDIA Jetson Nano等资源受限设备（嵌入式平台）上以低于50毫秒的端到端推理延迟处理实时视频流。我们的方法在专注于视频监控的标准数据集上保持超过92%的检测精度，并对不同的光照、背景和速度表现出鲁棒性。大量的比较和消融实验验证了我们设计的有效性。最后，我们的架构具有可扩展性、成本低廉，并且比常见的监控系统更符合严格的数据隐私法规，因此该系统可以与智慧城市或嵌入式安全架构共存。", "summary": "本文介绍了一种针对资源受限环境设计的实时低延迟视频监控系统。该系统结合了新颖的基于熵的自适应帧缓冲算法与MobileNetV2，实现了在边缘设备上亚50毫秒的端到端推理延迟和超过92%的检测精度。该方案具有高鲁棒性、可扩展性、低成本，并符合数据隐私法规，适用于智慧城市等嵌入式安全应用。", "keywords": "实时监控, 低延迟, 边缘计算, MobileNetV2, 熵自适应缓冲", "comments": "该论文的创新点在于结合了熵自适应缓冲和MobileNetV2，解决了资源受限环境下视频监控的延迟和性能问题。其在边缘设备上的低延迟表现和对隐私法规的兼容性使其在实际应用中具有重要意义。"}}
{"id": "2506.15624", "title": "The Effect of State Representation on LLM Agent Behavior in Dynamic Routing Games", "authors": ["Lyle Goodyear", "Rachel Guo", "Ramesh Johari"], "summary": "Large Language Models (LLMs) have shown promise as decision-makers in dynamic\nsettings, but their stateless nature necessitates creating a natural language\nrepresentation of history. We present a unifying framework for systematically\nconstructing natural language \"state\" representations for prompting LLM agents\nin repeated multi-agent games. Previous work on games with LLM agents has taken\nan ad hoc approach to encoding game history, which not only obscures the impact\nof state representation on agents' behavior, but also limits comparability\nbetween studies. Our framework addresses these gaps by characterizing methods\nof state representation along three axes: action informativeness (i.e., the\nextent to which the state representation captures actions played); reward\ninformativeness (i.e., the extent to which the state representation describes\nrewards obtained); and prompting style (or natural language compression, i.e.,\nthe extent to which the full text history is summarized).\n  We apply this framework to a dynamic selfish routing game, chosen because it\nadmits a simple equilibrium both in theory and in human subject experiments\n\\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find\nthat there are key dependencies of LLM agent behavior on the natural language\nstate representation. In particular, we observe that representations which\nprovide agents with (1) summarized, rather than complete, natural language\nrepresentations of past history; (2) information about regrets, rather than raw\npayoffs; and (3) limited information about others' actions lead to behavior\nthat more closely matches game theoretic equilibrium predictions, and with more\nstable game play by the agents. By contrast, other representations can exhibit\neither large deviations from equilibrium, higher variation in dynamic game play\nover time, or both.", "comment": "27 pages, 20 figures", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15624v1", "AI": {"title_translation": "状态表示对动态路由博弈中LLM智能体行为的影响", "tldr": "LLM智能体在动态博弈中的行为受自然语言状态表示的影响，简洁表示和遗憾信息能使其行为更接近理论均衡。", "motivation": "大型语言模型（LLM）在动态环境中作为决策者展现出潜力，但其无状态特性要求创建历史的自然语言表示。以往关于LLM智能体博弈的工作对博弈历史的编码采取了临时性的方法，这不仅模糊了状态表示对智能体行为的影响，也限制了研究之间的可比性。", "method": "本文提出了一个统一框架，用于系统地构建用于提示LLM智能体在重复多智能体博弈中的自然语言“状态”表示。该框架沿行动信息量、奖励信息量和提示风格（或自然语言压缩）三个轴线表征状态表示方法。研究将此框架应用于一个动态自私路由博弈。", "result": "研究发现LLM智能体行为对自然语言状态表示有关键依赖。具体而言，提供(1)总结而非完整的历史表示；(2)遗憾而非原始收益信息；(3)有限的他人行动信息的表示，能使智能体行为更接近博弈论均衡预测，并具有更稳定的博弈过程。其他表示可能导致与均衡的大幅偏离、动态博弈过程中随时间变化的更大波动，或两者兼有。", "conclusion": "状态表示对LLM智能体在动态博弈中的行为有显著影响，特定的简洁和信息丰富的表示能引导其行为更符合理论预测和更稳定。", "translation": "大型语言模型（LLM）在动态环境中作为决策者展现出潜力，但其无状态特性需要创建历史的自然语言表示。我们提出了一个统一框架，用于系统地构建自然语言“状态”表示，以提示LLM智能体在重复多智能体博弈中的行为。以往关于LLM智能体博弈的工作对博弈历史的编码采取了临时性的方法，这不仅模糊了状态表示对智能体行为的影响，也限制了研究之间的可比性。我们的框架通过沿着三个轴线描述状态表示方法来解决这些差距：行动信息量（即状态表示捕获所玩行动的程度）；奖励信息量（即状态表示描述所获得奖励的程度）；以及提示风格（或自然语言压缩，即完整文本历史被总结的程度）。\n我们将此框架应用于一个动态自私路由博弈，选择它是因为它在理论和人类受试者实验中都存在简单的均衡[rapoport_choice_2009]。尽管博弈相对简单，我们发现LLM智能体行为对自然语言状态表示存在关键依赖。特别是，我们观察到，向智能体提供（1）总结而非完整的过去历史自然语言表示；（2）关于遗憾而非原始收益的信息；以及（3）关于他人行动的有限信息的表示，会导致其行为更接近博弈论均衡预测，并且智能体博弈过程更稳定。相比之下，其他表示方式可能表现出与均衡的大幅偏离、动态博弈过程中随时间变化的更大波动，或两者兼有。", "summary": "本文提出了一个统一框架，用于系统地构建LLM智能体在重复多智能体博弈中的自然语言状态表示，以解决现有研究中状态编码的随意性和可比性问题。通过在动态路由博弈中应用该框架，研究发现，简洁的历史总结、遗憾信息以及有限的他人行动信息能够使LLM智能体行为更接近博弈论均衡预测并更稳定。", "keywords": "LLM智能体, 状态表示, 动态路由博弈, 博弈论, 自然语言处理", "comments": "这项研究创新性地提出了一个系统性框架来研究LLM智能体在动态博弈中的状态表示问题，填补了现有研究中缺乏统一方法和可比性的空白。其发现对于指导LLM智能体在复杂决策环境中的设计和应用具有重要意义，表明了输入表示的精细化对LLM行为表现的关键影响。"}}
{"id": "2506.15461", "title": "All is Not Lost: LLM Recovery without Checkpoints", "authors": ["Nikolay Blagoev", "Oğuzhan Ersoy", "Lydia Yiyu Chen"], "summary": "Training LLMs on decentralized and wimpy computation nodes, e.g., multiple\non-spot instances, lowers the training cost and enables model democratization.\nThe inevitable challenge here is the churn of nodes due to failures and the\noperator's scheduling policies, leading to losing a stage - a part of the\nmodel. The conventional approaches to recover from failures are to either use\ncheckpointing, where periodically a copy of the entire model is sent to an\nadditional storage, or redundant computation. These approaches yield\nsignificant communication and/or computation overhead even in non-failure cases\nand scale poorly in settings with large models. In this paper, we propose,\nCheckFree, an efficient recovery method where a failing stage is substituted by\na weighted average of the closest neighboring stages. In contrast to the state\nof the art, CheckFree requires no additional computation or storage. However,\nbecause of the nature of averaging neighbouring stages, it can only recover\nfailures of intermediate stages. We further extend our method to CheckFree+\nwith out-of-order pipeline execution to tolerate crashes of the first and last\nstages. Thanks to out-of-order pipelining, behaviour of those stages is\nmimicked by their neighboring ones, which allows CheckFree+ to recover them by\nsimply copying the weights from the immediate neighbour. To be able to recover\nthe (de)embedding layers, CheckFree+ copies those layers to the neighboring\nstages, which requires relatively small storage overhead. We extensively\nevaluate our method on LLaMa models of model sizes from 124M to 1.5B with\nvarying failure frequencies. In the case of low and medium failure rates\n(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant\ncomputation in terms of convergence in wall-clock time by over 12%. Both of our\nproposals can be run via our code available at:\nhttps://github.com/gensyn-ai/CheckFree.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15461v1", "AI": {"title_translation": "一切尚未失去：LLM 无检查点恢复", "tldr": "提出 CheckFree 和 CheckFree+，无需检查点即可高效恢复 LLM 训练中的节点故障，在收敛时间上优于传统方法。", "motivation": "在去中心化和计算资源有限的节点上训练大型语言模型（LLMs）可以降低成本并促进模型民主化。然而，节点故障导致模型阶段丢失是不可避免的挑战。传统的恢复方法（如检查点或冗余计算）即使在无故障情况下也会产生显著的通信和/或计算开销，并且在大模型设置中扩展性差。因此，本研究旨在提出一种高效的故障恢复方法，以克服现有方法的局限性。", "method": "本文提出了 CheckFree 方法，通过使用最近邻阶段的加权平均来替代故障阶段，无需额外的计算或存储，但仅限于恢复中间阶段的故障。为了解决首尾阶段的故障，进一步提出了 CheckFree+ 方法，该方法采用乱序流水线执行，通过相邻阶段模仿行为来恢复，对于（解）嵌入层，则将其复制到相邻阶段，仅需相对较小的存储开销。", "result": "在对 LLaMa 模型（124M 至 1.5B）进行广泛评估后发现，在低到中等故障率（5-10%）下，CheckFree 和 CheckFree+ 在挂钟时间收敛方面比检查点和冗余计算方法性能提高超过 12%。", "conclusion": "本文提出的 CheckFree 和 CheckFree+ 方法为去中心化 LLM 训练中的节点故障提供了一种高效、无检查点的恢复方案，在特定故障率下显著优于传统的检查点和冗余计算方法，有助于降低训练成本并促进模型民主化。", "translation": "在去中心化和计算资源有限的节点（例如多个现货实例）上训练大型语言模型（LLMs）可以降低训练成本并实现模型民主化。然而，这里不可避免的挑战是由于故障和操作员调度策略导致的节点流失，从而导致模型的一部分——一个阶段——的丢失。传统的故障恢复方法是使用检查点（定期将整个模型的副本发送到额外的存储）或冗余计算。这些方法即使在无故障情况下也会产生显著的通信和/或计算开销，并且在大模型设置中扩展性差。在本文中，我们提出了一种高效的恢复方法 CheckFree，其中故障阶段被最近邻阶段的加权平均值替代。与现有技术不同，CheckFree 不需要额外的计算或存储。但是，由于平均相邻阶段的性质，它只能恢复中间阶段的故障。我们进一步将我们的方法扩展到 CheckFree+，通过乱序流水线执行来容忍首尾阶段的崩溃。由于乱序流水线，这些阶段的行为被它们的相邻阶段模仿，这使得 CheckFree+ 可以通过简单地从直接邻居复制权重来恢复它们。为了能够恢复（解）嵌入层，CheckFree+ 将这些层复制到相邻阶段，这需要相对较小的存储开销。我们对 LLaMa 模型（模型大小从 124M 到 1.5B）进行了广泛评估，并测试了不同的故障频率。在低到中等故障率（5-10%）的情况下，CheckFree 和 CheckFree+ 在挂钟时间收敛方面比检查点和冗余计算方法性能提高超过 12%。我们提出的两种方案都可以通过我们的代码运行，代码地址：https://github.com/gensyn-ai/CheckFree。", "summary": "本文介绍了 CheckFree 和 CheckFree+，这是一种在去中心化环境中，无需依赖昂贵的检查点或冗余计算，即可从节点故障中恢复大型语言模型（LLM）训练的新颖方法。CheckFree 通过对相邻阶段进行加权平均来恢复中间阶段的故障，而 CheckFree+ 则通过乱序流水线执行将此功能扩展到首尾阶段，并对嵌入层进行少量复制。对 LLaMa 模型的实验表明，在低到中等故障率下，CheckFree 和 CheckFree+ 在挂钟时间收敛方面比传统方法快 12% 以上，从而使 LLM 训练更具鲁棒性和成本效益。", "keywords": "LLM恢复, 无检查点, 去中心化训练, 节点故障, CheckFree", "comments": "该论文提出了一种创新的 LLM 故障恢复方法，通过利用相邻阶段信息（加权平均、复制）替代传统的检查点或冗余计算，显著降低了训练成本和开销。这对于在去中心化、资源有限的计算节点上进行 LLM 训练具有重要意义，有助于模型的民主化。然而，其性能优势主要体现在低到中等故障率下，且 CheckFree+ 对（解）嵌入层的处理仍需要少量存储开销，这可能是未来研究可以进一步优化的方向。"}}
{"id": "2506.15384", "title": "Disruption of parkinsonian brain oscillations", "authors": ["Cédric Join", "Jakub Orłowski", "Antoine Chaillet", "Madeleine Lowery", "Hugues Mounier", "Michel Fliess"], "summary": "Deep brain stimulation (DBS) is an advanced surgical treatment for the\nsymptoms of Parkinson's disease (PD), involving electrical stimulation of\nneurons within the basal ganglia region of the brain. DBS is traditionally\ndelivered in an open-loop manner using fixed stimulation parameters, which may\nlead to suboptimal results. In an effort to overcome these limitations, closed\nloop DBS, using pathological subthalamic beta (13--30 Hz) activity as a\nfeedback signal, offers the potential to adapt DBS automatically in response to\nchanges in patient symptoms and side effects. However, clinically implemented\nclosed-loop techniques have been limited to date to simple control algorithms,\ndue to the inherent uncertainties in the dynamics involved. Model-free control,\nwhich has already seen successful applications in the field of bioengineering,\noffers a way to avoid this limitation and provides an alternative method to\napply modern control approach to selective suppression of pathological\noscillations.", "comment": "23rd Internat. Conf. Computational Methods in Systems Biology (CMSB\n  2025), 10-12 september 2025, Lyon, France", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15384v1", "AI": {"title_translation": "帕金森氏症脑振荡的扰动", "tldr": "传统DBS治疗帕金森病效果不佳，现有闭环DBS算法简单。本文提出模型无关控制方法，有望更有效抑制帕金森氏症的病理性脑振荡。", "motivation": "传统的开放式DBS参数固定，可能导致次优结果。现有的闭环DBS技术受限于简单的控制算法，无法有效应对动态不确定性。", "method": "提出采用模型无关控制（model-free control）方法，将其应用于闭环DBS中，以选择性抑制病理性脑振荡。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "深部脑刺激（DBS）是一种针对帕金森病（PD）症状的先进外科治疗方法，涉及对大脑基底神经节区域的神经元进行电刺激。DBS传统上以开环方式使用固定刺激参数进行，这可能导致次优结果。为了克服这些局限性，闭环DBS利用病理性丘脑底核β（13-30 Hz）活动作为反馈信号，有望根据患者症状和副作用的变化自动调整DBS。然而，由于所涉及的动态固有的不确定性，临床实施的闭环技术迄今为止仅限于简单的控制算法。模型无关控制（在生物工程领域已成功应用）提供了一种避免这种局限性的方法，并为应用现代控制方法选择性抑制病理性振荡提供了替代方案。", "summary": "本文指出帕金森病（PD）的传统开环深部脑刺激（DBS）治疗存在局限性，即固定参数可能导致次优结果。尽管闭环DBS利用病理性β波活动作为反馈信号，有望实现自动调整，但目前的临床应用受限于简单的控制算法。为解决这一问题，文章提出采用模型无关控制方法，该方法已在生物工程领域成功应用，有望更有效地选择性抑制帕金森氏症的病理性脑振荡，从而克服现有闭环DBS的局限性。", "keywords": "帕金森病, 深部脑刺激, 闭环控制, 模型无关控制, 脑振荡", "comments": "本文提出将模型无关控制引入闭环DBS领域，以克服现有算法简单、效果次优的局限性，具有一定的创新性。若能成功应用，将有望显著提升帕金森病DBS治疗的精确性和有效性，减少副作用，对神经工程和临床医学领域具有重要意义。"}}
{"id": "2506.15338", "title": "Urban RIS-Assisted HAP Networks: Performance Analysis Using Stochastic Geometry", "authors": ["Islam M. Tanash", "Ayush Kumar Dwivedi", "Taneli Riihonen"], "summary": "This paper studies a high-altitude platform (HAP) network supported by\nreconfigurable intelligent surfaces (RISs). The practical irregular placement\nof HAPs and RISs is modeled using homogeneous Poisson point processes, while\nbuildings that cause blockages in urban areas are modeled as a Boolean scheme\nof rectangles. We introduce a novel approach to characterize the statistical\nchannel based on generalized Beta prime distribution. Analytical expressions\nfor coverage probability and ergodic capacity in an interference-limited system\nare derived and validated through Monte Carlo simulations. The findings show\nnotable performance improvements and reveal the impact of various system\nparameters, including blockages effect which contribute in mitigating\ninterference from the other visible HAPs. This proposed system could enhance\nconnectivity and enable effective data offloading in urban environments.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15338v1", "AI": {"title_translation": "城市RIS辅助HAP网络：基于随机几何的性能分析", "tldr": "本文研究了RIS辅助的高空平台（HAP）网络，使用随机几何模型分析其性能，并推导出覆盖概率和遍历容量的解析表达式，结果显示显著的性能提升。", "motivation": "研究RIS辅助的HAP网络，以增强城市环境中的连接性并实现有效的数据分流。", "method": "1. 使用齐次泊松点过程对HAP和RIS的实际不规则放置进行建模。2. 使用矩形布尔方案对城市区域中的建筑物阻塞进行建模。3. 引入基于广义Beta素数分布的新方法来表征统计信道。4. 推导并验证了干扰受限系统中覆盖概率和遍历容量的解析表达式。5. 通过蒙特卡洛模拟进行验证。", "result": "1. 显示了显著的性能改进。2. 揭示了各种系统参数的影响。3. 发现阻塞效应有助于减轻来自其他可见HAP的干扰。", "conclusion": "所提出的RIS辅助HAP系统可以增强城市环境中的连接性并实现有效的数据分流。", "translation": "本文研究了由可重构智能表面（RIS）支持的高空平台（HAP）网络。HAP和RIS的实际不规则放置使用齐次泊松点过程进行建模，而城市区域中引起阻塞的建筑物则建模为矩形布尔方案。我们引入了一种基于广义Beta素数分布的新颖方法来表征统计信道。在干扰受限系统中，推导了覆盖概率和遍历容量的解析表达式，并通过蒙特卡洛模拟进行了验证。研究结果显示了显著的性能改进，并揭示了各种系统参数的影响，包括有助于减轻来自其他可见HAP干扰的阻塞效应。所提出的系统可以增强城市环境中的连接性并实现有效的数据分流。", "summary": "本文研究了一种由可重构智能表面（RIS）辅助的高空平台（HAP）网络在城市环境中的性能。通过使用齐次泊松点过程和矩形布尔方案对HAP、RIS和建筑物阻塞进行建模，并引入基于广义Beta素数分布的新颖信道表征方法，推导并验证了干扰受限系统中的覆盖概率和遍历容量解析表达式。研究结果表明，该系统能显著提升性能，并揭示了阻塞效应在减轻干扰方面的积极作用，有望增强城市连接性和数据分流能力。", "keywords": "RIS, HAP网络, 随机几何, 覆盖概率, 遍历容量", "comments": "本文的创新之处在于结合了RIS辅助的HAP网络与随机几何建模，特别是引入了广义Beta素数分布来表征统计信道，并考虑了城市环境中建筑物阻塞的实际影响。其重要性在于为未来城市高空通信网络的部署提供了理论分析和性能评估基础，尤其是在提升连接性和数据分流效率方面。"}}
{"id": "2506.14790", "title": "Continuous Evolution Pool: Taming Recurring Concept Drift in Online Time Series Forecasting", "authors": ["Tianxiang Zhan", "Ming Jin", "Yuanpeng He", "Yuxuan Liang", "Yong Deng", "Shirui Pan"], "summary": "Recurring concept drift, a type of concept drift in which previously observed\ndata patterns reappear after some time, is one of the most prevalent types of\nconcept drift in time series. As time progresses, concept drift occurs and\npreviously encountered concepts are forgotten, thereby leading to a decline in\nthe accuracy of online predictions. Existing solutions employ parameter\nupdating techniques to delay forgetting; however, this may result in the loss\nof some previously learned knowledge while neglecting the exploration of\nknowledge retention mechanisms. To retain all conceptual knowledge and fully\nutilize it when the concepts recur, we propose the Continuous Evolution Pool\n(CEP), a pooling mechanism that stores different instances of forecasters for\ndifferent concepts. Our method first selects the forecaster nearest to the test\nsample and then learns the features from its neighboring samples - a process we\nrefer to as the retrieval. If there are insufficient neighboring samples, it\nindicates that a new concept has emerged, and a new model will evolve from the\ncurrent nearest sample to the pool to store the knowledge of the concept.\nSimultaneously, the elimination mechanism will enable outdated knowledge to be\ncleared to ensure the prediction effect of the forecasters. Experiments on\ndifferent architectural models and eight real datasets demonstrate that CEP\neffectively retains the knowledge of different concepts. In the scenario of\nonline forecasting with recurring concepts, CEP significantly enhances the\nprediction results.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14790v1", "AI": {"title_translation": "持续进化池：驯服在线时间序列预测中反复出现的概念漂移", "tldr": "本文提出了持续进化池（CEP），一种池化机制，用于在在线时间序列预测中有效处理反复出现的概念漂移，通过存储和利用不同概念的预测器实例来保留知识并提高预测精度。", "motivation": "反复出现的概念漂移是时间序列中普遍存在的一种概念漂移，导致在线预测准确性下降。现有解决方案通过参数更新来延迟遗忘，但这可能导致先前学习知识的丢失，并忽视知识保留机制的探索。", "method": "本文提出了持续进化池（CEP），这是一种池化机制，用于存储不同概念的预测器实例。该方法首先选择最接近测试样本的预测器，然后从其邻近样本中学习特征（称为检索）。如果邻近样本不足，则表明新概念出现，将从当前最近样本进化出一个新模型加入池中。同时，淘汰机制会清除过时知识以确保预测效果。", "result": "在不同架构模型和八个真实数据集上的实验表明，CEP有效地保留了不同概念的知识。在具有反复出现概念的在线预测场景中，CEP显著提高了预测结果。", "conclusion": "持续进化池（CEP）通过其独特的池化、检索、进化和淘汰机制，成功解决了在线时间序列预测中反复出现的概念漂移问题，有效保留了概念知识并显著提升了预测性能。", "translation": "反复出现的概念漂移是时间序列中一种最普遍的概念漂移类型，其中先前观察到的数据模式在一段时间后再次出现。随着时间的推移，概念漂移发生，先前遇到的概念被遗忘，从而导致在线预测准确性下降。现有解决方案采用参数更新技术来延迟遗忘；然而，这可能导致一些先前学习的知识丢失，同时忽视了知识保留机制的探索。为了保留所有概念知识并在概念再次出现时充分利用它，我们提出了持续进化池（CEP），这是一种池化机制，用于存储不同概念的不同预测器实例。我们的方法首先选择最接近测试样本的预测器，然后从其邻近样本中学习特征——我们将其称为检索。如果邻近样本不足，则表明新概念已经出现，并且一个新的模型将从当前最近的样本进化到池中，以存储该概念的知识。同时，淘汰机制将使过时的知识被清除，以确保预测器的预测效果。在不同架构模型和八个真实数据集上的实验表明，CEP有效地保留了不同概念的知识。在具有反复出现概念的在线预测场景中，CEP显著提高了预测结果。", "summary": "本文针对在线时间序列预测中反复出现的概念漂移问题，提出了一种名为持续进化池（CEP）的新型池化机制。CEP通过存储和管理不同概念的预测器实例，解决了现有方法在知识保留方面的不足。其核心机制包括：检索最匹配的预测器、在新概念出现时进化新模型加入池中，以及淘汰过时知识。实验证明，CEP能够有效保留概念知识，并在处理反复出现的概念时显著提高在线预测的准确性。", "keywords": "反复概念漂移, 在线时间序列预测, 知识保留, 概念进化, 池化机制", "comments": "本文提出的持续进化池（CEP）在处理时间序列数据中反复出现的概念漂移方面具有创新性。通过引入池化机制来存储和管理不同概念的预测器，并结合检索、进化和淘汰策略，有效解决了知识遗忘和知识保留的难题。这对于需要长期稳定预测性能的在线系统具有重要意义。"}}
{"id": "2506.15258", "title": "Privacy-Preserving Chest X-ray Classification in Latent Space with Homomorphically Encrypted Neural Inference", "authors": ["Jonghun Kim", "Gyeongdeok Jo", "Shinyoung Ra", "Hyunjin Park"], "summary": "Medical imaging data contain sensitive patient information requiring strong\nprivacy protection. Many analytical setups require data to be sent to a server\nfor inference purposes. Homomorphic encryption (HE) provides a solution by\nallowing computations to be performed on encrypted data without revealing the\noriginal information. However, HE inference is computationally expensive,\nparticularly for large images (e.g., chest X-rays). In this study, we propose\nan HE inference framework for medical images that uses VQGAN to compress images\ninto latent representations, thereby significantly reducing the computational\nburden while preserving image quality. We approximate the activation functions\nwith lower-degree polynomials to balance the accuracy and efficiency in\ncompliance with HE requirements. We observed that a downsampling factor of\neight for compression achieved an optimal balance between performance and\ncomputational cost. We further adapted the squeeze and excitation module, which\nis known to improve traditional CNNs, to enhance the HE framework. Our method\nwas tested on two chest X-ray datasets for multi-label classification tasks\nusing vanilla CNN backbones. Although HE inference remains relatively slow and\nintroduces minor performance differences compared with unencrypted inference,\nour approach shows strong potential for practical use in medical images", "comment": "11 pages, 5 figures", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15258v1", "AI": {"title_translation": "隐私保护的胸部X射线潜在空间分类，基于同态加密的神经网络推理", "tldr": "本研究提出了一种用于医学图像的同态加密（HE）推理框架，该框架利用VQGAN将图像压缩为潜在表示，以显著降低计算负担，同时保持图像质量，解决了HE在处理大型图像时计算成本高昂的问题。", "motivation": "医学影像数据包含敏感患者信息，需要强大的隐私保护。虽然同态加密（HE）允许在加密数据上进行计算以保护隐私，但其推理过程计算成本高昂，特别是对于胸部X射线等大型图像。", "method": "本研究提出了一种新的HE推理框架。该框架使用VQGAN将图像压缩为潜在表示，以降低计算负担。激活函数通过低阶多项式进行近似，以平衡准确性和效率。研究发现下采样因子为八时，在性能和计算成本之间达到最佳平衡。此外，该框架还适应了挤压和激励模块以增强性能。方法在两个胸部X射线数据集上进行了多标签分类任务测试，使用了普通的CNN骨干网络。", "result": "通过将图像压缩到潜在空间，显著降低了计算负担，同时保持了图像质量。下采样因子为八的压缩实现了性能和计算成本之间的最佳平衡。尽管HE推理相对于未加密推理仍相对较慢并引入了轻微的性能差异，但该方法在医学图像的实际应用中显示出巨大的潜力。", "conclusion": "本研究提出的同态加密推理框架，通过结合VQGAN压缩和激活函数的多项式近似，有效解决了隐私保护医学图像分析中的计算挑战。尽管目前存在速度限制，但该方法在医学图像处理中展现出强大的实用潜力。", "translation": "医学影像数据包含敏感的患者信息，需要强大的隐私保护。许多分析设置需要将数据发送到服务器进行推理。同态加密（HE）通过允许在加密数据上执行计算而不泄露原始信息来提供解决方案。然而，HE推理计算成本高昂，特别是对于大型图像（例如胸部X射线）。在本研究中，我们提出了一种用于医学图像的HE推理框架，该框架使用VQGAN将图像压缩为潜在表示，从而在保持图像质量的同时显著降低计算负担。我们用低阶多项式近似激活函数，以平衡符合HE要求的准确性和效率。我们观察到，压缩的下采样因子为八时，在性能和计算成本之间达到了最佳平衡。我们进一步调整了挤压和激励模块（已知可改进传统CNN），以增强HE框架。我们的方法在两个胸部X射线数据集上进行了多标签分类任务测试，使用了普通的CNN骨干网络。尽管HE推理仍然相对较慢，并且与未加密推理相比引入了轻微的性能差异，但我们的方法在医学图像的实际应用中显示出巨大的潜力。", "summary": "本论文介绍了一种用于隐私保护医学图像分析（特别是胸部X射线分类）的同态加密（HE）推理框架。为解决HE在大型图像上的高计算成本问题，该框架利用VQGAN将图像压缩到潜在空间，并用多项式近似激活函数。该方法在胸部X射线数据集上进行了评估，结果表明在保持图像质量的同时显著降低了计算量，尽管存在一些性能权衡，但其在实际应用中显示出巨大潜力。", "keywords": "隐私保护, 同态加密, 医学影像, 潜在空间, VQGAN", "comments": "该研究的创新之处在于将VQGAN压缩与同态加密相结合，用于医学图像分析，有效克服了HE处理大型数据时的计算瓶颈。挤压和激励模块的引入进一步增强了框架。尽管论文承认当前在速度和轻微性能差异上的局限性，但这项工作为医疗保健领域安全AI的发展指明了一个有前景的方向。"}}
{"id": "2506.15076", "title": "Learning-Time Encoding Shapes Unlearning in LLMs", "authors": ["Ruihan Wu", "Konstantin Garov", "Kamalika Chaudhuri"], "summary": "As large language models (LLMs) are increasingly deployed in the real world,\nthe ability to ``unlearn'', or remove specific pieces of knowledge post hoc,\nhas become essential for a variety of reasons ranging from privacy regulations\nto correcting outdated or harmful content. Prior work has proposed unlearning\nbenchmarks and algorithms, and has typically assumed that the training process\nand the target model are fixed. In this work, we empirically investigate how\nlearning-time choices in knowledge encoding impact the effectiveness of\nunlearning factual knowledge. Our experiments reveal two key findings: (1)\nlearning with paraphrased descriptions improves unlearning performance and (2)\nunlearning individual piece of knowledge from a chunk of text is challenging.\nOur results suggest that learning-time knowledge encoding may play a central\nrole in enabling reliable post-hoc unlearning.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15076v1", "AI": {"title_translation": "学习时编码影响LLM中的遗忘能力", "tldr": "本文实证研究了学习时知识编码如何影响大型语言模型中事实知识遗忘的有效性，发现使用释义描述学习可以提高遗忘性能，但从文本块中遗忘单个知识点很困难。", "motivation": "随着大型语言模型（LLM）在现实世界中得到越来越多的部署，事后移除特定知识（即“遗忘”）的能力已变得至关重要，以应对隐私法规、纠正过时或有害内容等需求。现有工作通常假设训练过程和目标模型是固定的，而本文旨在探究学习时知识编码的选择如何影响事实知识遗忘的有效性。", "method": "通过经验性实验，研究了学习时知识编码的选择如何影响事实知识遗忘的有效性。", "result": "实验揭示了两个关键发现：1) 使用释义描述进行学习可以提高遗忘性能；2) 从文本块中遗忘单个知识点具有挑战性。", "conclusion": "研究结果表明，学习时知识编码可能在实现可靠的事后遗忘中发挥核心作用。", "translation": "随着大型语言模型（LLM）在现实世界中得到越来越多的部署，“遗忘”或事后移除特定知识的能力已变得至关重要，原因从隐私法规到纠正过时或有害内容不等。先前的研究提出了遗忘基准和算法，并且通常假设训练过程和目标模型是固定的。在这项工作中，我们实证研究了学习时知识编码的选择如何影响事实知识遗忘的有效性。我们的实验揭示了两个关键发现：(1) 使用释义描述进行学习可以提高遗忘性能，以及 (2) 从文本块中遗忘单个知识点具有挑战性。我们的结果表明，学习时知识编码可能在实现可靠的事后遗忘中发挥核心作用。", "summary": "本文探讨了大型语言模型中学习时知识编码对事后遗忘能力的影响。研究发现，通过释义描述学习有助于提升遗忘效果，但从文本块中移除单个知识点则较为困难。结果表明，学习时的知识编码方式对LLM的可靠遗忘能力至关重要。", "keywords": "大型语言模型, 遗忘, 知识编码, 学习时, 事实知识", "comments": "这篇论文的创新点在于它从“学习时”的角度而非传统的“遗忘算法”角度来探讨LLM的遗忘问题，揭示了知识编码方式对遗忘效果的潜在影响。这对于设计更易于遗忘的LLM训练范式具有重要指导意义。"}}
{"id": "2506.14916", "title": "Interpolation-based reproducing kernel particle method", "authors": ["Jennifer E. Fromm", "John A. Evans", "J. S. Chen"], "summary": "Meshfree methods, including the reproducing kernel particle method (RKPM),\nhave been widely used within the computational mechanics community to model\nphysical phenomena in materials undergoing large deformations or extreme\ntopology changes. RKPM shape functions and their derivatives cannot be\naccurately integrated with the Gauss-quadrature methods widely employed for the\nfinite element method (FEM) and typically require sophisticated nodal\nintegration techniques, preventing them from easily being implemented in\nexisting FEM software. Interpolation-based methods have been developed to\naddress similar problems with isogeometric and immersed boundary methods,\nallowing these techniques to be implemented within open-source finite element\nsoftware. With interpolation-based methods, background basis functions are\nrepresented as linear combinations of Lagrange polynomial foreground basis\nfunctions defined upon a boundary-conforming foreground mesh. This work extends\nthe applications of interpolation-based methods to implement RKPM within\nopen-source finite element software. Interpolation-based RKPM is applied to\nseveral PDEs, and error convergence rates are equivalent to classic RKPM\nintegrated using high-order Gauss-quadrature schemes. The interpolation-based\nmethod is able to exploit the continuity of the RKPM basis to solve\nhigher-order PDEs, demonstrated through the biharmonic problem. The method is\nextended to multi-material problems through Heaviside enrichment schemes, using\nlocal foreground refinement to reduce geometric integration error and achieve\nhigh-order accuracy. The computational cost of interpolation-based RKPM is\nsimilar to the smoothed gradient nodal integration schemes, offering\nsignificant savings over Gauss-quadrature-based meshfree methods while enabling\neasy implementation within existing finite element software.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14916v1", "AI": {"title_translation": "基于插值的再生核粒子法", "tldr": "提出了一种基于插值的再生核粒子法(RKPM)，解决了传统RKPM难以在现有有限元软件中实现的问题，并实现了高效且高精度的计算。", "motivation": "传统的再生核粒子法(RKPM)在与广泛使用的有限元法(FEM)中的高斯积分方法结合时，其形函数及其导数无法被精确积分，通常需要复杂的节点积分技术，这阻碍了RKPM在现有FEM软件中的便捷实现。", "method": "本文将插值方法应用于再生核粒子法(RKPM)的实现。该方法将背景基函数表示为在边界一致的前景网格上定义的拉格朗日多项式前景基函数的线性组合。它通过Heaviside富集方案扩展到多材料问题，并利用局部前景细化来减少几何积分误差。", "result": "插值再生核粒子法在求解偏微分方程时，其误差收敛率与使用高阶高斯积分方案的经典RKPM相当。该方法能够利用RKPM基函数的连续性来求解高阶偏微分方程，并通过双调和问题得到验证。在多材料问题中，通过Heaviside富集和局部前景细化，实现了高阶精度并减少了几何积分误差。其计算成本与平滑梯度节点积分方案相似，比基于高斯积分的无网格方法显著节省，同时便于在现有有限元软件中实现。", "conclusion": "基于插值的再生核粒子法提供了一种有效且高效的方法，解决了传统RKPM在现有有限元软件中实现困难的问题，并在保持高精度的同时显著降低了计算成本。", "translation": "无网格方法，包括再生核粒子法（RKPM），已广泛应用于计算力学领域，以模拟材料在大变形或极端拓扑变化下的物理现象。RKPM的形函数及其导数无法通过有限元法（FEM）中广泛使用的高斯积分方法进行精确积分，通常需要复杂的节点积分技术，这阻碍了它们在现有FEM软件中的便捷实现。为了解决等几何和浸入边界法中类似的问题，已经开发了基于插值的方法，使得这些技术可以在开源有限元软件中实现。通过基于插值的方法，背景基函数被表示为在边界一致的前景网格上定义的拉格朗日多项式前景基函数的线性组合。这项工作扩展了基于插值方法的应用，以在开源有限元软件中实现RKPM。基于插值的RKPM应用于几个偏微分方程，其误差收敛率与使用高阶高斯积分方案的经典RKPM相当。基于插值的方法能够利用RKPM基函数的连续性来求解高阶偏微分方程，并通过双调和问题得到证明。该方法通过Heaviside富集方案扩展到多材料问题，利用局部前景细化来减少几何积分误差并实现高阶精度。基于插值的RKPM的计算成本与平滑梯度节点积分方案相似，与基于高斯积分的无网格方法相比显著节省，同时便于在现有有限元软件中实现。", "summary": "本文提出了一种基于插值的再生核粒子法（RKPM），旨在解决传统RKPM难以在现有有限元软件中集成的问题。该方法通过将背景基函数表示为拉格朗日多项式前景基函数的线性组合来实现，并能应用于高阶偏微分方程和多材料问题。研究结果表明，该方法在保持与经典RKPM相当的误差收敛率和高精度的同时，显著降低了计算成本，并极大地简化了RKPM在现有有限元软件中的实施。", "keywords": "再生核粒子法, 无网格方法, 插值方法, 有限元方法, 计算力学", "comments": "这项工作通过引入插值方法，有效地解决了再生核粒子法（RKPM）在有限元软件中实施的长期挑战，特别是其积分难题。其创新之处在于将背景基函数与前景网格上的拉格朗日多项式相结合，从而兼容现有FEM框架。该方法的意义在于，它不仅提高了RKPM的实际可用性，使其能够被更广泛的计算力学社区所采用，而且在计算效率和精度方面也表现出色，为处理大变形和复杂拓扑变化中的材料行为提供了更便捷、高效的工具。"}}
{"id": "2506.15112", "title": "PDLRecover: Privacy-preserving Decentralized Model Recovery with Machine Unlearning", "authors": ["Xiangman Li", "Xiaodong Wu", "Jianbing Ni", "Mohamed Mahmoud", "Maazen Alsabaan"], "summary": "Decentralized learning is vulnerable to poison attacks, where malicious\nclients manipulate local updates to degrade global model performance. Existing\ndefenses mainly detect and filter malicious models, aiming to prevent a limited\nnumber of attackers from corrupting the global model. However, restoring an\nalready compromised global model remains a challenge. A direct approach is to\nremove malicious clients and retrain the model using only the benign clients.\nYet, retraining is time-consuming, computationally expensive, and may\ncompromise model consistency and privacy.\n  We propose PDLRecover, a novel method to recover a poisoned global model\nefficiently by leveraging historical model information while preserving\nprivacy. The main challenge lies in protecting shared historical models while\nenabling parameter estimation for model recovery. By exploiting the linearity\nof approximate Hessian matrix computation, we apply secret sharing to protect\nhistorical updates, ensuring local models are not leaked during transmission or\nreconstruction. PDLRecover introduces client-side preparation, periodic\nrecovery updates, and a final exact update to ensure robustness and convergence\nof the recovered model. Periodic updates maintain accurate curvature\ninformation, and the final step ensures high-quality convergence. Experiments\nshow that the recovered global model achieves performance comparable to a fully\nretrained model but with significantly reduced computation and time cost.\nMoreover, PDLRecover effectively prevents leakage of local model parameters,\nensuring both accuracy and privacy in recovery.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15112v1", "AI": {"title_translation": "PDLRecover：一种基于机器学习遗忘的隐私保护去中心化模型恢复方法", "tldr": "PDLRecover通过利用历史模型信息和秘密共享技术，高效且隐私地恢复被中毒的去中心化学习模型。", "motivation": "去中心化学习易受投毒攻击，现有防御主要检测和过滤恶意模型，但恢复已受损的全局模型仍是挑战。直接重训耗时、计算昂贵且可能损害模型一致性和隐私。", "method": "提出PDLRecover。利用近似Hessian矩阵计算的线性特性，应用秘密共享保护历史更新，防止本地模型泄露。引入客户端准备、周期性恢复更新和最终精确更新，以确保恢复模型的鲁棒性和收敛性。", "result": "恢复的全局模型性能与完全重训的模型相当，但显著降低了计算和时间成本。同时，PDLRecover有效防止了本地模型参数的泄露，确保了恢复的准确性和隐私性。", "conclusion": "PDLRecover提供了一种高效、隐私保护的去中心化学习模型恢复方案，解决了模型中毒后的恢复难题，且在性能、成本和隐私方面表现出色。", "translation": "去中心化学习容易受到投毒攻击，恶意客户端会操纵本地更新以降低全局模型性能。现有防御主要检测和过滤恶意模型，旨在防止有限数量的攻击者破坏全局模型。然而，恢复一个已经受损的全局模型仍然是一个挑战。一种直接的方法是移除恶意客户端并仅使用良性客户端重新训练模型。然而，重新训练耗时、计算成本高昂，并可能损害模型一致性和隐私。\n我们提出了PDLRecover，这是一种新颖的方法，通过利用历史模型信息同时保护隐私来有效地恢复被投毒的全局模型。主要挑战在于保护共享的历史模型，同时实现模型恢复的参数估计。通过利用近似Hessian矩阵计算的线性特性，我们应用秘密共享来保护历史更新，确保本地模型在传输或重建过程中不会泄露。PDLRecover引入了客户端准备、周期性恢复更新和最终精确更新，以确保恢复模型的鲁棒性和收敛性。周期性更新保持准确的曲率信息，最后一步确保高质量的收敛。实验表明，恢复的全局模型性能与完全重新训练的模型相当，但计算和时间成本显著降低。此外，PDLRecover有效防止了本地模型参数的泄露，确保了恢复的准确性和隐私性。", "summary": "本文提出了PDLRecover，一种针对去中心化学习中投毒攻击的模型恢复方法。该方法通过利用历史模型信息并结合秘密共享技术来保护隐私，高效地恢复受损的全局模型。PDLRecover通过客户端准备、周期性更新和最终精确更新确保恢复模型的鲁棒性和收敛性。实验证明，PDLRecover在显著降低计算和时间成本的同时，实现了与完全重训模型相当的性能，并有效防止了本地模型参数泄露。", "keywords": "去中心化学习, 模型恢复, 隐私保护, 机器学习遗忘, 投毒攻击", "comments": "这篇论文的创新点在于提出了一个结合了机器学习遗忘和隐私保护机制（秘密共享）的去中心化模型恢复框架。它不仅解决了去中心化学习中模型中毒后难以高效恢复的问题，还兼顾了隐私保护，避免了传统重训方法的成本和隐私风险。其通过利用Hessian矩阵的线性特性来整合秘密共享，是方法上的一个亮点。"}}
{"id": "2506.15648", "title": "deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses", "authors": ["Georgios Androutsopoulos", "Antonio Bianchi"], "summary": "Although Rust ensures memory safety by default, it also permits the use of\nunsafe code, which can introduce memory safety vulnerabilities if misused.\nUnfortunately, existing tools for detecting memory bugs in Rust typically\nexhibit limited detection capabilities, inadequately handle Rust-specific\ntypes, or rely heavily on manual intervention.\n  To address these limitations, we present deepSURF, a tool that integrates\nstatic analysis with Large Language Model (LLM)-guided fuzzing harness\ngeneration to effectively identify memory safety vulnerabilities in Rust\nlibraries, specifically targeting unsafe code. deepSURF introduces a novel\napproach for handling generics by substituting them with custom types and\ngenerating tailored implementations for the required traits, enabling the\nfuzzer to simulate user-defined behaviors within the fuzzed library.\nAdditionally, deepSURF employs LLMs to augment fuzzing harnesses dynamically,\nfacilitating exploration of complex API interactions and significantly\nincreasing the likelihood of exposing memory safety vulnerabilities. We\nevaluated deepSURF on 27 real-world Rust crates, successfully rediscovering 20\nknown memory safety bugs and uncovering 6 previously unknown vulnerabilities,\ndemonstrating clear improvements over state-of-the-art tools.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15648v1", "AI": {"title_translation": "deepSURF：通过模糊测试LLM增强的测试工具检测Rust中的内存安全漏洞", "tldr": "deepSURF结合静态分析和LLM增强的模糊测试，有效检测Rust不安全代码中的内存安全漏洞，并发现了新漏洞。", "motivation": "尽管Rust默认确保内存安全，但其不安全代码块仍可能引入漏洞。现有工具在Rust内存错误检测方面能力有限、对Rust特定类型处理不足或需大量手动干预。", "method": "deepSURF结合静态分析和LLM引导的模糊测试工具生成，专门针对Rust不安全代码。它通过用自定义类型替换泛型并生成所需特性的定制实现来处理泛型，以模拟用户定义行为。此外，它使用LLM动态增强模糊测试工具，促进复杂API交互的探索。", "result": "在27个真实Rust crate上进行了评估，成功重新发现了20个已知内存安全错误，并发现了6个以前未知的漏洞，表明比现有最先进的工具有了明显改进。", "conclusion": "deepSURF通过结合静态分析和LLM增强的模糊测试，显著提高了Rust不安全代码中内存安全漏洞的检测能力，超越了现有工具。", "translation": "尽管Rust默认确保内存安全，但也允许使用不安全代码，如果使用不当，可能会引入内存安全漏洞。不幸的是，现有用于检测Rust中内存错误的工具通常检测能力有限，无法充分处理Rust特有类型，或者严重依赖手动干预。\n为了解决这些限制，我们提出了deepSURF，一个将静态分析与大型语言模型（LLM）引导的模糊测试工具生成相结合的工具，以有效识别Rust库中的内存安全漏洞，特别是针对不安全代码。deepSURF引入了一种处理泛型的新方法，通过将其替换为自定义类型并为所需特性生成定制实现，使模糊测试器能够在被测试库中模拟用户定义的行为。此外，deepSURF利用LLM动态增强模糊测试工具，促进复杂API交互的探索，并显著增加暴露内存安全漏洞的可能性。我们在27个真实世界的Rust crate上评估了deepSURF，成功重新发现了20个已知内存安全错误并发现了6个以前未知的漏洞，证明了比最先进的工具有了明显的改进。", "summary": "deepSURF是一种针对Rust不安全代码的内存安全漏洞检测工具。它通过结合静态分析和LLM引导的模糊测试工具生成来解决现有工具的局限性。deepSURF创新性地处理泛型并利用LLM动态增强模糊测试，从而更有效地探索复杂API交互。在对真实Rust项目的评估中，deepSURF不仅重新发现了大量已知漏洞，还发现了新的未知漏洞，表现出优于现有先进工具的性能。", "keywords": "Rust, 内存安全, 模糊测试, LLM, 漏洞检测", "comments": "deepSURF的创新在于将静态分析与LLM引导的模糊测试相结合，并特别解决了Rust泛型处理的挑战。通过LLM动态增强模糊测试工具，它能够更有效地探索复杂的API交互，这对于发现深层漏洞至关重要。其在实际项目中的优异表现证明了其有效性和实用性，为Rust生态系统的安全提供了重要贡献。"}}
{"id": "2506.15087", "title": "3D Vision-tactile Reconstruction from Infrared and Visible Images for Robotic Fine-grained Tactile Perception", "authors": ["Yuankai Lin", "Xiaofan Lu", "Jiahui Chen", "Hua Yang"], "summary": "To achieve human-like haptic perception in anthropomorphic grippers, the\ncompliant sensing surfaces of vision tactile sensor (VTS) must evolve from\nconventional planar configurations to biomimetically curved topographies with\ncontinuous surface gradients. However, planar VTSs have challenges when\nextended to curved surfaces, including insufficient lighting of surfaces,\nblurring in reconstruction, and complex spatial boundary conditions for surface\nstructures. With an end goal of constructing a human-like fingertip, our\nresearch (i) develops GelSplitter3D by expanding imaging channels with a prism\nand a near-infrared (NIR) camera, (ii) proposes a photometric stereo neural\nnetwork with a CAD-based normal ground truth generation method to calibrate\ntactile geometry, and (iii) devises a normal integration method with boundary\nconstraints of depth prior information to correcting the cumulative error of\nsurface integrals. We demonstrate better tactile sensing performance, a 40$\\%$\nimprovement in normal estimation accuracy, and the benefits of sensor shapes in\ngrasping and manipulation tasks.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15087v1", "AI": {"title_translation": "机器人精细触觉感知的红外与可见光图像三维视觉-触觉重建", "tldr": "本文开发了一种新型三维视觉触觉传感器GelSplitter3D及其重建方法，通过结合红外和可见光图像，显著提高了机器人触觉感知的性能，特别是在曲面传感方面。", "motivation": "为了使拟人化夹具实现类人触觉感知，视觉触觉传感器（VTS）的柔性传感表面需从传统平面配置发展到仿生曲面拓扑结构。然而，平面VTS在扩展到曲面时面临照明不足、重建模糊和复杂空间边界条件等挑战。", "method": "研究开发了GelSplitter3D，通过棱镜和近红外（NIR）相机扩展成像通道；提出了一种基于CAD法线真值生成方法的光度立体神经网络来校准触觉几何；设计了一种具有深度先验信息边界约束的法线积分方法，用于校正表面积分的累积误差。", "result": "实验证明了更好的触觉传感性能，法线估计精度提高了40%，并且传感器形状在抓取和操作任务中展现出优势。", "conclusion": "本研究成功开发了一种用于曲面视觉触觉传感器的三维重建方法，显著提升了机器人精细触觉感知的能力，为构建类人指尖奠定了基础。", "translation": "为了使拟人化夹具实现类人触觉感知，视觉触觉传感器（VTS）的柔性传感表面必须从传统平面配置发展到具有连续表面梯度的仿生曲面拓扑结构。然而，平面VTS在扩展到曲面时面临挑战，包括表面照明不足、重建模糊以及表面结构复杂的空间边界条件。为了构建一个类人指尖的最终目标，我们的研究（i）通过棱镜和近红外（NIR）相机扩展成像通道，开发了GelSplitter3D；（ii）提出了一种基于CAD法线真值生成方法的光度立体神经网络来校准触觉几何；（iii）设计了一种具有深度先验信息边界约束的法线积分方法，用于校正表面积分的累积误差。我们展示了更好的触觉传感性能，法线估计精度提高了40%，以及传感器形状在抓取和操作任务中的优势。", "summary": "本文旨在为机器人精细触觉感知构建类人指尖，解决了传统平面视觉触觉传感器在曲面应用中面临的照明、模糊和边界条件等挑战。为此，研究开发了GelSplitter3D传感器，通过增加近红外成像通道，并提出了基于CAD真值生成的光度立体神经网络进行几何校准，以及利用深度先验信息校正累积误差的法线积分方法。实验结果表明，该方法显著提升了触觉传感性能，法线估计精度提高了40%，并验证了新型传感器形状在抓取和操作任务中的优势。", "keywords": "3D视觉触觉重建, 机器人感知, GelSplitter3D, 光度立体, 触觉传感器", "comments": "本文的创新点在于提出了GelSplitter3D传感器，通过引入近红外成像和结合光度立体神经网络与法线积分方法，有效解决了曲面视觉触觉重建中的关键问题。法线估计精度40%的提升是一个显著的进步，对于实现机器人精细操作和类人触觉感知具有重要意义。该研究为仿生机器人指尖的开发提供了新的思路和技术支撑。"}}
{"id": "2506.15129", "title": "Data Verbalisation: What is Text Doing in a Data Visualisation?", "authors": ["Paul Murrell"], "summary": "This article discusses the role that text elements play in a data\nvisualisation. We argue that there is a need for a simple, coherent explanation\nof text elements similar to the understanding that already exists for non-text\nelements like bars, points, and lines. We explore examples of how text is used\nwithin a data visualisation and use existing knowledge and assessment\ntechniques to evaluate when text is effective and when it is not. The result is\na framework that aims to be easy to understand and easy to apply in order to\nunderstand the purpose and effectiveness of the text elements in any data\nvisualisation.", "comment": "43 pages (including appendix), 20 figures", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15129v1", "AI": {"title_translation": "数据口语化：文本在数据可视化中扮演什么角色？", "tldr": "本文讨论了数据可视化中文本元素的作用，并提出了一个评估文本有效性的框架。", "motivation": "现有数据可视化中对非文本元素的理解已很成熟，但对文本元素的理解缺乏简单、连贯的解释，因此需要探讨文本元素的作用及其有效性。", "method": "通过探索数据可视化中文本使用的例子，并利用现有知识和评估技术来判断文本何时有效、何时无效。", "result": "提出了一个易于理解和应用的框架，旨在帮助理解数据可视化中文本元素的用途和有效性。", "conclusion": "本文的结论是提供了一个用于理解和评估数据可视化中文本元素目的和有效性的框架。", "translation": "本文讨论了文本元素在数据可视化中扮演的角色。我们认为，需要对文本元素进行简单、连贯的解释，类似于对条形图、点和线等非文本元素的现有理解。我们探讨了文本在数据可视化中如何使用的例子，并利用现有知识和评估技术来评估文本何时有效、何时无效。结果是一个旨在易于理解和易于应用的框架，以便理解任何数据可视化中文本元素的用途和有效性。", "summary": "本文探讨了文本元素在数据可视化中的重要作用，指出当前对文本元素的理解不如非文本元素。通过分析文本使用案例并应用现有评估技术，研究提出一个易于理解和应用的框架，以帮助用户理解和评估数据可视化中文本元素的有效性和目的。", "keywords": "数据可视化, 文本元素, 框架, 有效性, 评估", "comments": "本文的创新之处在于填补了数据可视化领域中对文本元素系统性理解的空白。它提供了一个实用的框架，有助于提升数据可视化的清晰度和有效性，对于可视化设计师和研究人员都具有重要意义。"}}
{"id": "2506.14835", "title": "MonoVQD: Monocular 3D Object Detection with Variational Query Denoising and Self-Distillation", "authors": ["Kiet Dang Vu", "Trung Thai Tran", "Duc Dung Nguyen"], "summary": "Precisely localizing 3D objects from a single image constitutes a central\nchallenge in monocular 3D detection. While DETR-like architectures offer a\npowerful paradigm, their direct application in this domain encounters inherent\nlimitations, preventing optimal performance. Our work addresses these\nchallenges by introducing MonoVQD, a novel framework designed to fundamentally\nadvance DETR-based monocular 3D detection. We propose three main contributions.\nFirst, we propose the Mask Separated Self-Attention mechanism that enables the\nintegration of the denoising process into a DETR architecture. This improves\nthe stability of Hungarian matching to achieve a consistent optimization\nobjective. Second, we present the Variational Query Denoising technique to\naddress the gradient vanishing problem of conventional denoising methods, which\nseverely restricts the efficiency of the denoising process. This explicitly\nintroduces stochastic properties to mitigate this fundamental limitation and\nunlock substantial performance gains. Finally, we introduce a sophisticated\nself-distillation strategy, leveraging insights from later decoder layers to\nsynergistically improve query quality in earlier layers, thereby amplifying the\niterative refinement process. Rigorous experimentation demonstrates that\nMonoVQD achieves superior performance on the challenging KITTI monocular\nbenchmark. Highlighting its broad applicability, MonoVQD's core components\nseamlessly integrate into other architectures, delivering significant\nperformance gains even in multi-view 3D detection scenarios on the nuScenes\ndataset and underscoring its robust generalization capabilities.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14835v1", "AI": {"title_translation": "MonoVQD：基于变分查询去噪和自蒸馏的单目3D目标检测", "tldr": "MonoVQD是一个新的DETR-based框架，通过引入掩码分离自注意力、变分查询去噪和自蒸馏策略，显著提升了单目3D目标检测的性能，并在多视图场景中也表现出良好的泛化能力。", "motivation": "在单目3D检测中，从单张图像精确地定位3D物体是一个核心挑战。DETR类架构直接应用于此领域时存在固有限制，阻碍了其达到最佳性能。", "method": "提出了MonoVQD框架，包含三项主要贡献：1. 掩码分离自注意力机制：将去噪过程整合到DETR架构中，提高匈牙利匹配的稳定性，以实现一致的优化目标。2. 变分查询去噪技术：解决传统去噪方法的梯度消失问题，引入随机特性以缓解这一限制并提升性能。3. 自蒸馏策略：利用后期解码器层的洞察力协同提高早期层的查询质量，从而增强迭代细化过程。", "result": "MonoVQD在KITTI单目基准测试中取得了卓越的性能。其核心组件可以无缝集成到其他架构中，即使在nuScenes数据集上的多视图3D检测场景中也能带来显著的性能提升，并突显了其强大的泛化能力。", "conclusion": "MonoVQD通过其创新的掩码分离自注意力机制、变分查询去噪技术和自蒸馏策略，成功克服了DETR类架构在单目3D检测中的局限性，实现了卓越的性能提升和强大的泛化能力。", "translation": "从单张图像精确地定位3D物体构成了单目3D检测中的一个核心挑战。尽管DETR类架构提供了一个强大的范式，但它们在此领域的直接应用遇到了固有的局限性，阻碍了最佳性能的实现。我们的工作通过引入MonoVQD来应对这些挑战，这是一个旨在从根本上推进基于DETR的单目3D检测的新颖框架。我们提出了三项主要贡献。首先，我们提出了掩码分离自注意力机制，该机制能够将去噪过程整合到DETR架构中。这提高了匈牙利匹配的稳定性，以实现一致的优化目标。其次，我们提出了变分查询去噪技术，以解决传统去噪方法的梯度消失问题，该问题严重限制了去噪过程的效率。这明确地引入了随机特性以缓解这一根本限制并解锁显著的性能增益。最后，我们引入了一种复杂的自蒸馏策略，利用后期解码器层的洞察力协同提高早期层的查询质量，从而放大迭代细化过程。严格的实验表明，MonoVQD在具有挑战性的KITTI单目基准测试中取得了卓越的性能。突出其广泛适用性的是，MonoVQD的核心组件可以无缝集成到其他架构中，即使在nuScenes数据集上的多视图3D检测场景中也能带来显著的性能提升，并突显了其强大的泛化能力。", "summary": "MonoVQD是一个新颖的DETR-based框架，旨在解决单目3D目标检测中DETR类架构的性能限制。该框架通过引入掩码分离自注意力机制、变分查询去噪技术和自蒸馏策略，显著提升了3D目标定位的精度和稳定性。实验证明，MonoVQD在KITTI单目基准测试中表现优越，并且其核心组件在多视图3D检测场景（如nuScenes数据集）中也展现出强大的泛化能力和性能增益。", "keywords": "单目3D检测, DETR, 变分查询去噪, 自蒸馏, 目标检测", "comments": "本文的创新之处在于其针对DETR类架构在单目3D检测中固有局限性的系统性解决方案。通过引入掩码分离自注意力机制改善匹配稳定性，变分查询去噪技术解决梯度消失问题，以及自蒸馏策略优化查询质量，MonoVQD从多个维度提升了模型的性能和训练效率。其在多视图场景下的泛化能力也显示了其方法的普适性。"}}
{"id": "2506.15639", "title": "The AI Policy Module: Developing Computer Science Student Competency in AI Ethics and Policy", "authors": ["James Weichert", "Daniel Dunlap", "Mohammed Farghally", "Hoda Eldardiry"], "summary": "As artificial intelligence (AI) further embeds itself into many settings\nacross personal and professional contexts, increasing attention must be paid\nnot only to AI ethics, but also to the governance and regulation of AI\ntechnologies through AI policy. However, the prevailing post-secondary\ncomputing curriculum is currently ill-equipped to prepare future AI\npractitioners to confront increasing demands to implement abstract ethical\nprinciples and normative policy preferences into the design and development of\nAI systems. We believe that familiarity with the 'AI policy landscape' and the\nability to translate ethical principles to practices will in the future\nconstitute an important responsibility for even the most technically-focused AI\nengineers.\n  Toward preparing current computer science (CS) students for these new\nexpectations, we developed an AI Policy Module to introduce discussions of AI\npolicy into the CS curriculum. Building on a successful pilot in fall 2024, in\nthis innovative practice full paper we present an updated and expanded version\nof the module, including a technical assignment on \"AI regulation\". We present\nthe findings from our pilot of the AI Policy Module 2.0, evaluating student\nattitudes towards AI ethics and policy through pre- and post-module surveys.\nFollowing the module, students reported increased concern about the ethical\nimpacts of AI technologies while also expressing greater confidence in their\nabilities to engage in discussions about AI regulation. Finally, we highlight\nthe AI Regulation Assignment as an effective and engaging tool for exploring\nthe limits of AI alignment and emphasizing the role of 'policy' in addressing\nethical challenges.", "comment": "Accepted at IEEE Frontiers in Education (FIE) 2025", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15639v1", "AI": {"title_translation": "人工智能政策模块：培养计算机科学学生在人工智能伦理和政策方面的能力", "tldr": "本文介绍了一个人工智能政策模块，旨在将人工智能政策讨论引入计算机科学课程，以培养学生在人工智能伦理和政策方面的能力。试点结果显示，学生对人工智能伦理影响的关注度提高，并对参与人工智能监管讨论更有信心。", "motivation": "随着人工智能在个人和专业领域的广泛应用，不仅需要关注人工智能伦理，还需要通过人工智能政策来治理和监管人工智能技术。然而，当前的高等计算课程未能充分培养未来的AI从业者，使其能够将抽象的伦理原则和规范性政策偏好融入AI系统的设计和开发中。作者认为，熟悉“人工智能政策格局”并将伦理原则转化为实践的能力将成为未来AI工程师的重要职责。", "method": "为了培养计算机科学学生应对这些新期望，研究人员开发了一个人工智能政策模块，旨在将人工智能政策讨论引入计算机科学课程。该模块在2024年秋季成功试点后，进行了更新和扩展，包括一项关于“人工智能监管”的技术作业。研究人员通过模块前后调查评估了学生对人工智能伦理和政策的态度，从而呈现了人工智能政策模块2.0的试点结果。", "result": "模块结束后，学生报告称对人工智能技术的伦理影响的关注度增加，同时对参与人工智能监管讨论的能力表现出更大的信心。人工智能监管作业被认为是探索人工智能对齐局限性并强调“政策”在解决伦理挑战中作用的有效且引人入胜的工具。", "conclusion": "该人工智能政策模块有效地提高了计算机科学学生对人工智能伦理和政策的认识和参与能力，表明将政策讨论整合到计算课程中是培养未来AI从业者的重要途径。", "translation": "随着人工智能（AI）进一步融入个人和专业背景下的许多环境，不仅要越来越关注人工智能伦理，还要关注通过人工智能政策对人工智能技术进行治理和监管。然而，当前普遍的专上计算课程未能充分准备未来的AI从业者，以应对日益增长的将抽象伦理原则和规范性政策偏好融入AI系统设计和开发的需求。我们认为，熟悉“人工智能政策格局”以及将伦理原则转化为实践的能力，未来将成为即使是最注重技术的AI工程师的重要职责。\n为了培养当前的计算机科学（CS）学生应对这些新期望，我们开发了一个人工智能政策模块，将人工智能政策讨论引入CS课程。基于2024年秋季的成功试点，在这篇创新实践的全文中，我们介绍了该模块的更新和扩展版本，包括一项关于“人工智能监管”的技术作业。我们展示了人工智能政策模块2.0的试点结果，通过模块前和模块后调查评估了学生对人工智能伦理和政策的态度。模块结束后，学生报告称对人工智能技术的伦理影响的关注度增加，同时对参与人工智能监管讨论的能力表现出更大的信心。最后，我们强调人工智能监管作业是探索人工智能对齐局限性并强调“政策”在解决伦理挑战中作用的有效且引人入胜的工具。", "summary": "本文介绍了一个创新的人工智能政策模块，旨在弥补当前计算机科学课程在培养学生应对人工智能伦理和政策挑战方面的不足。该模块通过引入人工智能政策讨论和一项关于“人工智能监管”的技术作业，提升学生对人工智能伦理影响的关注度，并增强他们参与相关讨论的信心。试点结果表明，该模块是培养未来AI从业者在人工智能伦理和政策方面能力的有效工具。", "keywords": "人工智能伦理, 人工智能政策, 计算机科学教育, 课程开发, 学生能力", "comments": "该论文的创新之处在于提出了一个具体且可实施的“人工智能政策模块”，旨在弥补当前计算机科学教育在人工智能伦理和政策方面的空白。其重要性在于认识到AI工程师不仅需要技术能力，还需要具备理解和实践AI伦理与政策的能力。通过实际的模块设计和试点验证，为其他教育机构提供了宝贵的参考，有助于培养更全面、负责任的未来AI从业者。论文还强调了“人工智能监管作业”作为一种有效教学工具的价值。"}}
{"id": "2506.15488", "title": "Minimizing Communication for Parallel Symmetric Tensor Times Same Vector Computation", "authors": ["Hussam Al Daas", "Grey Ballard", "Laura Grigori", "Suraj Kumar", "Kathryn Rouse", "Mathieu Vérité"], "summary": "In this article, we focus on the parallel communication cost of multiplying\nthe same vector along two modes of a $3$-dimensional symmetric tensor. This is\na key computation in the higher-order power method for determining eigenpairs\nof a $3$-dimensional symmetric tensor and in gradient-based methods for\ncomputing a symmetric CP decomposition. We establish communication lower bounds\nthat determine how much data movement is required to perform the specified\ncomputation in parallel. The core idea of the proof relies on extending a key\ngeometric inequality for $3$-dimensional symmetric computations. We demonstrate\nthat the communication lower bounds are tight by presenting an optimal\nalgorithm where the data distribution is a natural extension of the triangle\nblock partition scheme for symmetric matrices to 3-dimensional symmetric\ntensors.", "comment": "19 pages, 1 figure", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15488v1", "AI": {"title_translation": "最小化并行对称张量与相同向量乘法的通信开销", "tldr": "本文研究了并行计算中3维对称张量与相同向量相乘的通信成本，建立了通信下界，并提出了一个达到最优的算法。", "motivation": "将同一个向量沿着一个3维对称张量的两个模进行乘法是高阶幂方法和计算对称CP分解的基于梯度方法中的一个关键计算，因此最小化其并行通信成本对于提高计算效率至关重要。", "method": "建立了并行计算中3维对称张量与相同向量乘法的通信下界，证明核心思想是扩展3维对称计算的关键几何不等式。通过提出一个数据分布是对称矩阵三角形块分区方案自然扩展的最优算法，证明了所建立的通信下界是紧密的。", "result": "建立了并行对称张量与相同向量乘法的通信下界，并提出了一个达到这些下界的最优算法，证明了通信下界的紧密性。", "conclusion": "本文确定了3维对称张量与相同向量乘法在并行计算中的通信下界，并提供了一个通信最优的算法，这对于提升相关数值方法的效率具有重要意义。", "translation": "在本文中，我们关注的是将同一个向量沿着一个3维对称张量的两个模进行乘法的并行通信成本。这是确定3维对称张量特征对的高阶幂方法和计算对称CP分解的基于梯度方法中的一个关键计算。我们建立了通信下界，该下界确定了并行执行指定计算所需的数据移动量。证明的核心思想依赖于扩展3维对称计算的关键几何不等式。我们通过提出一个最优算法来证明通信下界是紧密的，该算法的数据分布是对称矩阵三角形块分区方案向3维对称张量的自然扩展。", "summary": "本文研究了3维对称张量与相同向量相乘的并行通信成本，这是高阶幂方法和对称CP分解中的关键操作。作者通过扩展一个几何不等式，建立了该计算的通信下界，并提出了一个数据分布是对称矩阵三角形块分区方案自然扩展的最优算法，从而证明了所建立的通信下界是紧密的。", "keywords": "张量计算, 并行通信, 对称张量, 通信下界, 最优算法", "comments": "本文通过建立通信下界并提出最优算法，为并行计算中关键的张量乘法操作提供了理论和实践指导。其创新之处在于将对称矩阵的块分区方案扩展到高维张量，对于提升大规模张量计算的效率具有重要意义。"}}
{"id": "2506.15366", "title": "Performative Validity of Recourse Explanations", "authors": ["Gunnar König", "Hidde Fokkema", "Timo Freiesleben", "Celestine Mendler-Dünner", "Ulrike Von Luxburg"], "summary": "When applicants get rejected by an algorithmic decision system, recourse\nexplanations provide actionable suggestions for how to change their input\nfeatures to get a positive evaluation. A crucial yet overlooked phenomenon is\nthat recourse explanations are performative: When many applicants act according\nto their recommendations, their collective behavior may change statistical\nregularities in the data and, once the model is refitted, also the decision\nboundary. Consequently, the recourse algorithm may render its own\nrecommendations invalid, such that applicants who make the effort of\nimplementing their recommendations may be rejected again when they reapply. In\nthis work, we formally characterize the conditions under which recourse\nexplanations remain valid under performativity. A key finding is that recourse\nactions may become invalid if they are influenced by or if they intervene on\nnon-causal variables. Based on our analysis, we caution against the use of\nstandard counterfactual explanations and causal recourse methods, and instead\nadvocate for recourse methods that recommend actions exclusively on causal\nvariables.", "comment": "34 pages, 3 figures, 1 table, Preprint", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.15366v1", "AI": {"title_translation": "补救解释的执行有效性", "tldr": "当许多申请人按照算法决策系统的补救建议行动时，其集体行为可能改变数据中的统计规律，导致建议失效。本研究正式描述了补救解释在执行性下保持有效的条件，并建议仅在因果变量上推荐行动。", "motivation": "当许多申请人根据算法决策系统的补救建议采取行动时，他们的集体行为可能会改变数据中的统计规律，并在模型重新拟合后改变决策边界。这可能导致补救算法使其自身的建议失效，使得付出努力的申请人再次被拒绝，这是一个被忽视但至关重要的现象。", "method": "本研究正式描述了补救解释在执行性下保持有效的条件。", "result": "一个关键发现是，如果补救行动受到非因果变量的影响或干预非因果变量，它们可能会变得无效。", "conclusion": "基于分析，本研究警告不要使用标准的反事实解释和因果补救方法，而是提倡仅在因果变量上推荐行动的补救方法。", "translation": "当申请人被算法决策系统拒绝时，补救解释提供了关于如何改变其输入特征以获得积极评估的可行建议。一个关键但被忽视的现象是，补救解释是具有执行性的：当许多申请人根据其建议采取行动时，他们的集体行为可能会改变数据中的统计规律，一旦模型重新拟合，也会改变决策边界。因此，补救算法可能会使其自身的建议失效，使得那些努力实施其建议的申请人再次申请时可能再次被拒绝。在这项工作中，我们正式描述了补救解释在执行性下保持有效的条件。一个关键发现是，如果补救行动受到非因果变量的影响或干预非因果变量，它们可能会变得无效。基于我们的分析，我们警告不要使用标准的反事实解释和因果补救方法，而是提倡仅在因果变量上推荐行动的补救方法。", "summary": "本研究探讨了算法决策系统中补救解释的执行有效性问题。当大量申请人依据补救建议行动时，其集体行为可能改变底层数据分布和决策边界，从而使补救建议失效。论文正式刻画了补救解释在执行性下保持有效的条件，并发现若补救行动涉及非因果变量，则其可能失效。因此，研究建议避免使用标准反事实解释和现有因果补救方法，转而提倡仅基于因果变量推荐行动的补救方法。", "keywords": "补救解释, 执行性, 因果变量, 算法公平性, 反事实解释", "comments": "这篇论文创新性地指出了算法补救解释中一个关键的“执行性”问题，即大规模的用户反馈行为可能导致模型失效。其重要性在于揭示了现有解释方法（如反事实解释）的潜在缺陷，并提出了针对性的解决方案——聚焦于因果变量。这对于构建更稳健、更公平的AI决策系统具有重要指导意义。"}}
{"id": "2506.15398", "title": "Multi-dimensional evaluation on a rural integrated energy system including solar, wind, biomass and geothermal energy", "authors": ["Ruonan Lia", "Chang Wena", "Mingyu Yan", "Congcong Wu", "Ahmed Lotfy Elrefai", "Xiaotong Zhang", "Sahban Wael Saeed Alnaser"], "summary": "This study focuses on the novel municipal-scale rural integrated energy\nsystem (RIES), which encompasses energy supply and application. By constructing\na seven-dimensional evaluation system including energy efficiency, energy\nsupply, low-carbon sustainability, environmental impact, energy economy, social\nbenefits, and integrated energy system development, this research combines the\nimproved analytic hierarchy process (IAHP) and entropy weight method (EWM) by\nsum of squares of deviations to balance expert experience and data objectivity.\nFurthermore, the cloud model is introduced to handle the fuzziness and\nrandomness in the evaluation. This method can quantify the differences in\nsystem performance before and after the planning implementation. The results\nindicate that after planning, the comprehensive score has increased from 83.12\nto 87.55, the entropy value has decreased from 6.931 to 5.336, indicating\nenhanced system stability. The hyper-entropy has dropped from 3.08 to 2.278,\nreflecting a reduction in uncertainty. The research findings provide a\nscientific basis for the planning optimization, policy-making, and sustainable\ndevelopment of rural integrated energy systems, possessing both theoretical\ninnovation and practical guiding value.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15398v1", "AI": {"title_translation": "包含太阳能、风能、生物质能和地热能的乡村综合能源系统多维度评估", "tldr": "本研究对新型市级乡村综合能源系统进行了七维评估，结合IAHP、EWM和云模型，结果显示规划后系统性能显著提升，为RIES规划优化和可持续发展提供科学依据。", "motivation": "该研究旨在对新型市级乡村综合能源系统（RIES）进行多维度评估，并量化规划实施前后系统性能的差异，以提供科学依据支持其规划优化、政策制定和可持续发展。", "method": "研究构建了一个包含能源效率、能源供应、低碳可持续性、环境影响、能源经济、社会效益和综合能源系统发展七个维度的评估系统。评估方法结合了改进的层次分析法（IAHP）和基于偏差平方和的熵权法（EWM），以平衡专家经验和数据客观性。此外，引入云模型来处理评估中的模糊性和随机性。", "result": "规划实施后，综合得分从83.12提高到87.55；熵值从6.931下降到5.336，表明系统稳定性增强；超熵从3.08下降到2.278，反映不确定性降低。", "conclusion": "研究结果表明，所提出的评估方法能够有效量化规划对乡村综合能源系统性能的提升作用，为系统的规划优化、政策制定和可持续发展提供了具有理论创新和实践指导价值的科学依据。", "translation": "本研究聚焦于新型市级乡村综合能源系统（RIES），该系统涵盖能源供应和应用。通过构建包括能源效率、能源供应、低碳可持续性、环境影响、能源经济、社会效益和综合能源系统发展在内的七维评估体系，本研究结合改进的层次分析法（IAHP）和基于偏差平方和的熵权法（EWM），以平衡专家经验和数据客观性。此外，引入云模型来处理评估中的模糊性和随机性。该方法能够量化规划实施前后系统性能的差异。结果表明，规划后综合得分从83.12提高到87.55，熵值从6.931下降到5.336，表明系统稳定性增强。超熵从3.08下降到2.278，反映不确定性降低。研究结果为乡村综合能源系统的规划优化、政策制定和可持续发展提供了科学依据，兼具理论创新和实践指导价值。", "summary": "本研究对包含太阳能、风能、生物质能和地热能的新型市级乡村综合能源系统（RIES）进行了多维度评估。通过构建七维评估体系，并结合改进的层次分析法、熵权法和云模型，有效量化了规划实施前后系统性能的变化。结果显示，规划后系统综合得分、稳定性和不确定性均得到显著改善。该研究为RIES的规划优化、政策制定和可持续发展提供了重要的科学依据。", "keywords": "乡村综合能源系统, 多维度评估, 层次分析法, 熵权法, 云模型", "comments": "该研究的创新点在于构建了一个全面的七维评估体系，并结合了多种先进的评估方法（IAHP、EWM、云模型），有效地处理了评估中的主观性、客观性、模糊性和随机性。其方法学对于复杂能源系统的综合评估具有借鉴意义，且研究结果为乡村能源系统的可持续发展提供了直接的决策支持，兼具理论深度和实践价值。"}}
{"id": "2506.15463", "title": "Effect of Signal Quantization on Performance Measures of a 1st Order One Dimensional Differential Microphone Array", "authors": ["Shweta Pal", "Arun Kumar", "Monika Agrawal"], "summary": "In practical systems, recorded analog signals must be digitized for\nprocessing, introducing quantization as a critical aspect of data acquisition.\nWhile prior studies have examined quantization effects in various signal\nprocessing contexts, its impact on differential microphone arrays (DMAs),\nparticularly in one-dimensional (1D) first-order configurations, remains\nunexplored. This paper investigates the influence of signal quantization on\nperformance of first-order 1D DMAs across various beampatterns. An analytical\nexpression for quantized beamformed output for a first-order 1D DMA has been\nformulated. The effect of signal quantization has been studied on array\nperformance measures such as the Beampattern, Directivity Factor (DF),\nFront-to-Back Ratio (FBR), and null depth (ND). Simulation results reveal that\nbeampattern shape remains structurally invariant across quantization bit\ndepths, with quantization primarily affecting ND. DF and FBR remain constant\nwith the varying number of quantization bits. Additionally, ND is shown to be\nfrequency-independent; however, it increases with increasing quantization bit\ndepths, enhancing interference suppression. The study also examines the effect\nof steering nulls across the azimuthal range, showing that ND degrades as the\nnull moves closer to the source look direction, indicating reduced interference\nsuppression.", "comment": "5 Pages with 6 figures and 1 table", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15463v1", "AI": {"title_translation": "信号量化对一阶一维差分麦克风阵列性能测量的影响", "tldr": "本文研究了信号量化对一阶一维差分麦克风阵列性能的影响，发现量化主要影响零点深度，而方向性因子和前后比保持不变。", "motivation": "现有研究已探讨量化对各种信号处理的影响，但其对差分麦克风阵列（特别是1D一阶配置）的影响仍未被探索。", "method": "本文建立了一阶一维差分麦克风阵列量化波束形成输出的解析表达式，并通过仿真研究了信号量化对阵列性能指标（如波束图、方向性因子（DF）、前后比（FBR）和零点深度（ND））的影响。", "result": "仿真结果表明，波束图形状在不同量化位深度下结构上保持不变，量化主要影响零点深度（ND）。方向性因子（DF）和前后比（FBR）随量化位数的变化保持不变。ND与频率无关，但随量化位深度的增加而增加，从而增强了干扰抑制。当零点更接近声源方向时，ND会退化，表明干扰抑制降低。", "conclusion": "信号量化对一阶一维差分麦克风阵列的性能有特定影响，主要体现在零点深度上，这对于干扰抑制至关重要。", "translation": "在实际系统中，记录的模拟信号必须进行数字化处理，从而使量化成为数据采集的关键方面。尽管先前的研究已经检查了量化在各种信号处理上下文中的影响，但其对差分麦克风阵列（DMAs），特别是一维（1D）一阶配置的影响仍未被探索。本文研究了信号量化对一阶一维DMAs在各种波束图下的性能影响。本文建立了一阶一维DMA量化波束形成输出的解析表达式。研究了信号量化对阵列性能指标的影响，例如波束图、方向性因子（DF）、前后比（FBR）和零点深度（ND）。仿真结果表明，波束图形状在不同量化位深度下结构上保持不变，量化主要影响ND。DF和FBR随量化位数的变化保持不变。此外，ND被证明与频率无关；然而，它随量化位深度的增加而增加，从而增强了干扰抑制。该研究还检查了在方位角范围内转向零点的影响，表明当零点更接近声源方向时，ND会退化，表明干扰抑制降低。", "summary": "本文探讨了信号量化对一阶一维差分麦克风阵列（DMAs）性能的影响，填补了现有研究的空白。通过建立量化波束形成输出的解析表达式并进行仿真，研究发现量化主要影响阵列的零点深度（ND），而波束图形状、方向性因子（DF）和前后比（FBR）则基本不受影响。研究指出，ND随量化位深度的增加而增强，有助于干扰抑制，但当零点接近声源方向时，ND会减弱。", "keywords": "信号量化, 差分麦克风阵列, 零点深度, 波束形成, 性能测量", "comments": "该研究首次探讨了信号量化对一阶一维差分麦克风阵列性能的影响，具有创新性。其发现量化主要影响零点深度而非整体波束图形状，这对于设计和优化麦克风阵列系统以实现更好的干扰抑制具有重要的指导意义。"}}
{"id": "2506.14793", "title": "Protein Language Model Zero-Shot Fitness Predictions are Improved by Inference-only Dropout", "authors": ["Aditya Ravuri", "Neil D. Lawrence"], "summary": "Protein Language Models (PLMs) such as ESM2 have been shown to be capable of\nzero-shot prediction of critical scalar properties of proteins (fitness). In\nthis work, we show that injecting a dropout layer at inference time between a\nPLM's featurizer/embedding layer and its transformer, and averaging its output\nakin to Monte-Carlo dropout increases zero-shot performance on a subset of the\nProteinGym dataset. This is the case even when the model was not trained with\ndropouts to begin with, and does not require retraining or finetuning of the\nPLM. A dropout of 0.1 seems performant across all models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14793v1", "AI": {"title_translation": "蛋白质语言模型零样本适应性预测通过仅推理时丢弃法得到改善", "tldr": "在蛋白质语言模型（PLM）的推理阶段引入丢弃层并平均其输出，即使模型未经过丢弃训练，也能提高零样本适应性预测性能。", "motivation": "蛋白质语言模型（PLMs）如ESM2已被证明能够对蛋白质的关键标量属性（适应性）进行零样本预测，但本工作旨在探索进一步提高其零样本性能的方法。", "method": "在PLM的特征提取器/嵌入层与其转换器之间注入一个丢弃层，并在推理时平均其输出，类似于蒙特卡洛丢弃法。此方法不需要对PLM进行再训练或微调。", "result": "该方法提高了在ProteinGym数据集子集上的零样本性能。即使模型最初没有使用丢弃进行训练，这种改进也存在。0.1的丢弃率似乎在所有模型中表现良好。", "conclusion": "在蛋白质语言模型推理阶段引入丢弃层并平均输出，即使未经训练，也能有效提升零样本预测性能，且0.1的丢弃率表现最佳。", "translation": "蛋白质语言模型（PLMs）如ESM2已被证明能够对蛋白质的关键标量属性（适应性）进行零样本预测。在这项工作中，我们展示了在PLM的特征提取器/嵌入层与其转换器之间注入一个推理时的丢弃层，并平均其输出（类似于蒙特卡洛丢弃），可以提高在ProteinGym数据集子集上的零样本性能。即使模型最初没有使用丢弃进行训练，这种情况也成立，并且不需要对PLM进行再训练或微调。0.1的丢弃率似乎在所有模型中都表现良好。", "summary": "本研究发现，通过在蛋白质语言模型（PLM）的推理阶段，在特征提取器与转换器之间引入一个丢弃层并平均其输出，可以显著提升零样本蛋白质适应性预测的性能。值得注意的是，这种方法无需对PLM进行任何再训练或微调，即使模型最初未经过丢弃训练也能奏效，其中0.1的丢弃率表现最佳。", "keywords": "蛋白质语言模型, 零样本预测, 适应性, 丢弃法, 推理", "comments": "本文的创新点在于提出了“仅推理时丢弃”（inference-only dropout）的方法，实现了在不修改或重新训练现有蛋白质语言模型的情况下提升其零样本预测能力。这对于利用预训练大型模型进行下游任务具有重要意义，因为它降低了应用门槛和计算成本。"}}
{"id": "2506.15364", "title": "Brain Stroke Classification Using Wavelet Transform and MLP Neural Networks on DWI MRI Images", "authors": ["Mana Mohammadi", "Amirhesam Jafari Rad", "Ashkan Behrouzi"], "summary": "This paper presents a lightweight framework for classifying brain stroke\ntypes from Diffusion-Weighted Imaging (DWI) MRI scans, employing a Multi-Layer\nPerceptron (MLP) neural network with Wavelet Transform for feature extraction.\nAccurate and timely stroke detection is critical for effective treatment and\nimproved patient outcomes in neuroimaging. While Convolutional Neural Networks\n(CNNs) are widely used for medical image analysis, their computational\ncomplexity often hinders deployment in resource-constrained clinical settings.\nIn contrast, our approach combines Wavelet Transform with a compact MLP to\nachieve efficient and accurate stroke classification. Using the \"Brain Stroke\nMRI Images\" dataset, our method yields classification accuracies of 82.0% with\nthe \"db4\" wavelet (level 3 decomposition) and 86.00% with the \"Haar\" wavelet\n(level 2 decomposition). This analysis highlights a balance between diagnostic\naccuracy and computational efficiency, offering a practical solution for\nautomated stroke diagnosis. Future research will focus on enhancing model\nrobustness and integrating additional MRI modalities for comprehensive stroke\nassessment.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15364v1", "AI": {"title_translation": "基于小波变换和MLP神经网络对DWI MRI图像进行脑卒中分类", "tldr": "该研究提出了一种轻量级框架，结合小波变换和MLP神经网络，实现高效准确的脑卒中分类，在DWI MRI图像上分别达到82.0%和86.00%的分类精度。", "motivation": "在神经影像学中，准确及时的卒中检测对于有效的治疗和改善患者预后至关重要。尽管卷积神经网络（CNN）广泛用于医学图像分析，但其计算复杂性常常阻碍在资源受限的临床环境中部署。", "method": "该方法结合小波变换进行特征提取，并使用紧凑型多层感知器（MLP）神经网络对扩散加权成像（DWI）MRI扫描图像进行脑卒中类型分类，旨在实现高效准确的卒中分类。", "result": "使用“脑卒中MRI图像”数据集，该方法在使用“db4”小波（3级分解）时分类准确率达到82.0%，在使用“Haar”小波（2级分解）时分类准确率达到86.00%。", "conclusion": "该分析突出了诊断准确性和计算效率之间的平衡，为自动化卒中诊断提供了一个实用的解决方案。", "translation": "本文提出了一种轻量级框架，利用多层感知器（MLP）神经网络和小波变换进行特征提取，对扩散加权成像（DWI）MRI扫描图像中的脑卒中类型进行分类。准确及时的卒中检测对于神经影像学中有效的治疗和改善患者预后至关重要。尽管卷积神经网络（CNN）广泛用于医学图像分析，但其计算复杂性常常阻碍在资源受限的临床环境中部署。相比之下，我们的方法结合了小波变换和紧凑型MLP，以实现高效准确的卒中分类。使用“脑卒中MRI图像”数据集，我们的方法在使用“db4”小波（3级分解）时分类准确率达到82.0%，在使用“Haar”小波（2级分解）时分类准确率达到86.00%。这项分析突出了诊断准确性和计算效率之间的平衡，为自动化卒中诊断提供了一个实用的解决方案。未来的研究将侧重于增强模型鲁棒性并整合额外的MRI模态以进行全面的卒中评估。", "summary": "本研究提出了一种轻量级框架，利用小波变换进行特征提取，并结合多层感知器（MLP）神经网络，对DWI MRI图像中的脑卒中类型进行分类。该方法旨在解决传统CNN在资源受限临床环境中部署的计算复杂性问题，通过平衡诊断准确性和计算效率，实现了高效准确的卒中分类。在“脑卒中MRI图像”数据集上，使用“db4”小波和“Haar”小波分别达到了82.0%和86.00%的分类准确率，为自动化卒中诊断提供了一个实用方案。", "keywords": "脑卒中分类, 小波变换, MLP神经网络, DWI MRI, 计算效率", "comments": "该论文的创新点在于提出了一个轻量级的脑卒中分类框架，通过结合小波变换和MLP神经网络，在保证一定诊断准确性的同时，显著提高了计算效率，这对于资源受限的临床环境具有重要意义。与主流的CNN方法相比，其简洁性是主要优势。然而，其准确性相较于更复杂的深度学习模型可能仍有提升空间，并且未来研究计划中提到的模型鲁棒性增强和多模态整合是进一步提升其临床应用价值的关键。"}}
{"id": "2506.15081", "title": "Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification", "authors": ["Yaxin Fan", "Peifeng Li", "Qiaoming Zhu"], "summary": "Dialogue discourse parsing aims to identify and analyze discourse relations\nbetween the utterances within dialogues. However, linguistic features in\ndialogues, such as omission and idiom, frequently introduce ambiguities that\nobscure the intended discourse relations, posing significant challenges for\nparsers. To address this issue, we propose a Discourse-aware Clarification\nModule (DCM) to enhance the performance of the dialogue discourse parser. DCM\nemploys two distinct reasoning processes: clarification type reasoning and\ndiscourse goal reasoning. The former analyzes linguistic features, while the\nlatter distinguishes the intended relation from the ambiguous one. Furthermore,\nwe introduce Contribution-aware Preference Optimization (CPO) to mitigate the\nrisk of erroneous clarifications, thereby reducing cascading errors. CPO\nenables the parser to assess the contributions of the clarifications from DCM\nand provide feedback to optimize the DCM, enhancing its adaptability and\nalignment with the parser's requirements. Extensive experiments on the STAC and\nMolweni datasets demonstrate that our approach effectively resolves ambiguities\nand significantly outperforms the state-of-the-art (SOTA) baselines.", "comment": "Accepted by ACL2025(main conference)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15081v1", "AI": {"title_translation": "通过语篇感知的话语澄清改进对话语篇分析", "tldr": "本文提出语篇感知澄清模块（DCM）和贡献感知偏好优化（CPO），以解决对话语篇分析中的模糊性问题，并在STAC和Molweni数据集上显著超越了SOTA基线。", "motivation": "对话中的语言特征，如省略和习语，经常引入模糊性，模糊了预期的语篇关系，这给对话语篇分析带来了重大挑战。", "method": "提出语篇感知澄清模块（DCM）来增强对话语篇分析器的性能。DCM采用澄清类型推理和语篇目标推理两种推理过程。此外，引入贡献感知偏好优化（CPO）来减轻错误澄清的风险，减少级联错误。CPO使分析器能够评估DCM澄清的贡献并提供反馈以优化DCM。", "result": "在STAC和Molweni数据集上的大量实验表明，该方法有效解决了模糊性，并显著优于最先进的（SOTA）基线。", "conclusion": "提出的语篇感知澄清模块（DCM）和贡献感知偏好优化（CPO）方法能有效解决对话语篇分析中的模糊性问题，并显著提升了分析性能，超越了现有最先进水平。", "translation": "对话语篇分析旨在识别和分析对话中话语之间的语篇关系。然而，对话中的语言特征，如省略和习语，经常引入模糊性，模糊了预期的语篇关系，给分析器带来了重大挑战。为了解决这个问题，我们提出了一个语篇感知澄清模块（DCM）来增强对话语篇分析器的性能。DCM采用两种不同的推理过程：澄清类型推理和语篇目标推理。前者分析语言特征，而后者区分预期关系和模糊关系。此外，我们引入了贡献感知偏好优化（CPO）来减轻错误澄清的风险，从而减少级联错误。CPO使分析器能够评估DCM澄清的贡献并提供反馈以优化DCM，增强其适应性和与分析器要求的对齐。在STAC和Molweni数据集上的大量实验表明，我们的方法有效解决了模糊性并显著优于最先进的（SOTA）基线。", "summary": "本文提出了一种新颖的语篇感知澄清模块（DCM）和贡献感知偏好优化（CPO）方法，以解决对话语篇分析中因省略和习语等语言特征引起的模糊性问题。DCM通过两种推理过程识别和区分模糊关系，而CPO则优化DCM的澄清质量，减少错误。实验证明，该方法在STAC和Molweni数据集上有效提升了对话语篇分析的性能，并显著超越了现有SOTA基线。", "keywords": "对话语篇分析, 语篇澄清, 模糊性消解, 贡献感知偏好优化, 自然语言处理", "comments": "本文的创新点在于提出了一个专门的澄清模块（DCM）来处理对话语篇分析中的模糊性，并通过反馈优化机制（CPO）来提高澄清的准确性和适应性，这对于提升对话理解的鲁棒性具有重要意义。"}}
{"id": "2506.14994", "title": "Optimal alignment of Lorentz orientation and generalization to matrix Lie groups", "authors": ["Congzhou M Sha"], "summary": "There exist elegant methods of aligning point clouds in $\\mathbb R^3$.\nUnfortunately, these methods rely on the positive definite property of the\nEuclidean metric, and do not easily extend to the indefinite Minkowski metric.\nIn this paper, we propose two solutions to the following problem: given\ninertial reference frames $A$ and $B$, and given (possibly noisy) measurements\nof a set of 4-vectors $\\{v_i\\}$ made in those reference frames with components\n$\\{v_{A,i}\\}$ and $\\{v_{B,i}\\}$, find the optimal Lorentz transformation\n$\\Lambda$ such that $\\Lambda v_{A,i}=v_{B,i}$. The method we outline is\nconceptually simple and easily extends to alignment problems in other matrix\nLie groups.", "comment": "8 pages, 2 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.14994v1", "AI": {"title_translation": "Lorentz方向的最佳对齐及其在矩阵李群上的推广", "tldr": "本文提出了一种在洛伦兹空间中对齐四向量集的方法，并可推广到其他矩阵李群。", "motivation": "现有的$\\\\mathbb R^3$点云对齐方法依赖于欧几里得度量的正定性，无法轻易扩展到不定闵可夫斯基度量。", "method": "本文提出了两种解决方案，旨在找到最佳洛伦兹变换$\\Lambda$，使得在给定惯性参考系A和B中测量的4向量集合的组件$\\left\\{v_{A,i}\\right\\}$和$\\left\\{v_{B,i}\\right\\}$能够对齐。该方法概念简单，并易于推广到其他矩阵李群中的对齐问题。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "在$\\\\mathbb R^3$中存在优雅的点云对齐方法。不幸的是，这些方法依赖于欧几里得度量的正定性，并且不容易扩展到不定闵可夫斯基度量。在本文中，我们提出了两种解决方案来解决以下问题：给定惯性参考系A和B，以及在这些参考系中对一组4向量$\\left\\{v_i\\right\\}$进行的（可能带噪声的）测量值$\\left\\{v_{A,i}\\right\\}$和$\\left\\{v_{B,i}\\right\\}$，找到最佳洛伦兹变换$\\Lambda$使得$\\Lambda v_{A,i}=v_{B,i}$。我们概述的方法概念简单，并且易于推广到其他矩阵李群中的对齐问题。", "summary": "本文针对现有欧几里得空间点云对齐方法难以应用于不定闵可夫斯基度量的问题，提出了两种解决方案。这些方案旨在找到最佳洛伦兹变换，以对齐在不同惯性参考系中测量的一组四向量。该方法概念简单，并具有推广到其他矩阵李群对齐问题的潜力。", "keywords": "洛伦兹变换, 点云对齐, 闵可夫斯基度量, 矩阵李群, 惯性参考系", "comments": "该论文解决了将欧几里得空间中的点云对齐方法推广到洛伦兹空间这一重要挑战，其创新点在于提出了适用于不定闵可夫斯基度量的方法，并强调了其在其他矩阵李群上的通用性。"}}
{"id": "2506.15117", "title": "CipherMind: The Longest Codebook in the World", "authors": ["Ming Nie", "Zhixiong Yang", "Bingsheng Wei"], "summary": "In recent years, the widespread application of large language models has\ninspired us to consider using inference for communication encryption. We\ntherefore propose CipherMind, which utilizes intermediate results from\ndeterministic fine-tuning of large model inferences as transmission content.\nThe semantic parameters of large models exhibit characteristics like opaque\nunderlying implementations and weak interpretability, thus enabling their use\nas an encryption method for data transmission. This communication paradigm can\nbe applied in scenarios like intra-gateway transmission, and theoretically, it\ncan be implemented using any large model as its foundation.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15117v1", "AI": {"title_translation": "CipherMind：世界上最长的密码本", "tldr": "CipherMind提出了一种新的加密通信方法，通过利用大型语言模型推理的中间结果进行数据传输，适用于网关内传输。", "motivation": "近年来，大型语言模型的广泛应用启发作者考虑使用推理进行通信加密。", "method": "CipherMind利用大型模型确定性微调推理的中间结果作为传输内容。大型模型的语义参数具有底层实现不透明和可解释性弱的特点，因此可作为数据传输的加密方法。", "result": "Not mentioned in abstract", "conclusion": "这种通信范式可应用于网关内传输等场景，理论上可以使用任何大型模型作为其基础实现。", "translation": "近年来，大型语言模型的广泛应用启发我们考虑使用推理进行通信加密。因此，我们提出了CipherMind，它利用大型模型推理的确定性微调的中间结果作为传输内容。大型模型的语义参数表现出底层实现不透明和可解释性弱的特点，从而使其能够作为数据传输的加密方法。这种通信范式可以应用于网关内传输等场景，理论上，它可以使用任何大型模型作为其基础实现。", "summary": "本文提出了CipherMind，一种基于大型语言模型推理的新型加密通信方法。该方法利用大型模型确定性微调的中间结果作为传输内容，利用其语义参数的不透明性和弱可解释性实现数据加密。该范式适用于网关内传输，并可基于任何大型模型实现。", "keywords": "CipherMind, 加密通信, 大型语言模型, 推理, 数据传输", "comments": "CipherMind提出了一种新颖的加密思路，将大型语言模型的推理过程本身作为一种加密机制，利用了其内在的不透明性和复杂性。这种方法在概念上具有创新性，尤其是在特定场景（如网关内传输）下可能提供一种独特的安全层。其重要性在于开辟了利用AI模型进行非传统加密的新途径。然而，抽象中未提及具体的安全性分析、性能评估以及实际应用中的可行性细节，这些将是进一步研究的关键。"}}
{"id": "2506.15096", "title": "DyNaVLM: Zero-Shot Vision-Language Navigation System with Dynamic Viewpoints and Self-Refining Graph Memory", "authors": ["Zihe Ji", "Huangxuan Lin", "Yue Gao"], "summary": "We present DyNaVLM, an end-to-end vision-language navigation framework using\nVision-Language Models (VLM). In contrast to prior methods constrained by fixed\nangular or distance intervals, our system empowers agents to freely select\nnavigation targets via visual-language reasoning. At its core lies a\nself-refining graph memory that 1) stores object locations as executable\ntopological relations, 2) enables cross-robot memory sharing through\ndistributed graph updates, and 3) enhances VLM's decision-making via retrieval\naugmentation. Operating without task-specific training or fine-tuning, DyNaVLM\ndemonstrates high performance on GOAT and ObjectNav benchmarks. Real-world\ntests further validate its robustness and generalization. The system's three\ninnovations: dynamic action space formulation, collaborative graph memory, and\ntraining-free deployment, establish a new paradigm for scalable embodied robot,\nbridging the gap between discrete VLN tasks and continuous real-world\nnavigation.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15096v1", "AI": {"title_translation": "DyNaVLM：具有动态视点和自优化图记忆的零样本视觉-语言导航系统", "tldr": "DyNaVLM是一个端到端、零样本的视觉-语言导航框架，通过动态视点和自优化图记忆，使机器人在没有特定任务训练的情况下，能自由选择导航目标，并在基准测试和实际应用中表现出色。", "motivation": "为了解决现有视觉-语言导航方法受限于固定角度或距离间隔的问题，并使机器人能够通过视觉-语言推理自由选择导航目标，从而弥合离散VLN任务与连续真实世界导航之间的差距。", "method": "DyNaVLM是一个端到端视觉-语言导航框架，其核心是一个自优化图记忆，该记忆：1) 将物体位置存储为可执行的拓扑关系；2) 通过分布式图更新实现跨机器人记忆共享；3) 通过检索增强提升VLM的决策能力。该系统无需特定任务训练或微调即可运行，并采用了动态动作空间制定和协作图记忆。", "result": "DyNaVLM在GOAT和ObjectNav基准测试中表现出高性能。实际测试进一步验证了其鲁棒性和泛化能力。", "conclusion": "DyNaVLM通过其动态动作空间制定、协作图记忆和免训练部署三项创新，为可扩展具身机器人建立了新的范式，成功弥合了离散VLN任务与连续真实世界导航之间的差距。", "translation": "我们提出了DyNaVLM，一个使用视觉-语言模型（VLM）的端到端视觉-语言导航框架。与受限于固定角度或距离间隔的现有方法不同，我们的系统使智能体能够通过视觉-语言推理自由选择导航目标。其核心是一个自优化图记忆，该记忆：1）将物体位置存储为可执行的拓扑关系；2）通过分布式图更新实现跨机器人记忆共享；3）通过检索增强提升VLM的决策能力。DyNaVLM无需特定任务训练或微调即可运行，在GOAT和ObjectNav基准测试中表现出高性能。实际测试进一步验证了其鲁棒性和泛化能力。该系统的三项创新：动态动作空间制定、协作图记忆和免训练部署，为可扩展具身机器人建立了新的范式，弥合了离散VLN任务与连续真实世界导航之间的差距。", "summary": "DyNaVLM是一个创新的视觉-语言导航系统，它通过引入动态视点和自优化图记忆，克服了传统方法在导航目标选择上的限制。该系统利用VLM进行视觉-语言推理，并能将物体位置存储为拓扑关系，支持跨机器人记忆共享，并通过检索增强VLM的决策。DyNaVLM无需特定任务训练或微调即可实现高性能，并在基准测试和实际场景中展现出鲁棒性和泛化能力，为具身机器人导航开辟了新方向。", "keywords": "视觉-语言导航, 零样本, 动态视点, 图记忆, 机器人", "comments": "DyNaVLM的创新之处在于其动态动作空间制定、协作图记忆和无需训练的部署，这三点共同解决了现有视觉-语言导航系统在灵活性和泛化性上的不足。特别是其零样本能力和在真实世界中的验证，表明了其在实际具身机器人应用中的巨大潜力，为将离散VLN任务扩展到连续真实世界导航提供了有效途径。"}}
{"id": "2506.15189", "title": "Accessible Gesture-Driven Augmented Reality Interaction System", "authors": ["Yikan Wang"], "summary": "Augmented reality (AR) offers immersive interaction but remains inaccessible\nfor users with motor impairments or limited dexterity due to reliance on\nprecise input methods. This study proposes a gesture-based interaction system\nfor AR environments, leveraging deep learning to recognize hand and body\ngestures from wearable sensors and cameras, adapting interfaces to user\ncapabilities. The system employs vision transformers (ViTs), temporal\nconvolutional networks (TCNs), and graph attention networks (GATs) for gesture\nprocessing, with federated learning ensuring privacy-preserving model training\nacross diverse users. Reinforcement learning optimizes interface elements like\nmenu layouts and interaction modes. Experiments demonstrate a 20% improvement\nin task completion efficiency and a 25% increase in user satisfaction for\nmotor-impaired users compared to baseline AR systems. This approach enhances AR\naccessibility and scalability. Keywords: Deep learning, Federated learning,\nGesture recognition, Augmented reality, Accessibility, Human-computer\ninteraction", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15189v1", "AI": {"title_translation": "无障碍手势驱动增强现实交互系统", "tldr": "该研究提出了一种基于手势的增强现实交互系统，利用深度学习和联邦学习提高运动障碍用户的可访问性，实验证明能显著提升任务效率和用户满意度。", "motivation": "增强现实（AR）虽然提供沉浸式交互，但由于依赖精确的输入方法，对运动障碍或手部灵活性有限的用户来说仍难以使用。", "method": "该研究提出了一种基于手势的AR交互系统，利用深度学习（包括视觉Transformer、时间卷积网络和图注意力网络）通过可穿戴传感器和摄像头识别手势和身体姿态。系统采用联邦学习进行隐私保护的模型训练，并利用强化学习优化界面元素（如菜单布局和交互模式），以适应用户能力。", "result": "实验表明，与基线AR系统相比，该方法使运动障碍用户的任务完成效率提高了20%，用户满意度提高了25%。", "conclusion": "该方法显著增强了AR的可访问性和可扩展性。", "translation": "增强现实（AR）提供了沉浸式交互，但由于依赖精确的输入方法，对于运动障碍或手部灵活性有限的用户来说仍然难以使用。本研究提出了一种用于AR环境的基于手势的交互系统，利用深度学习从可穿戴传感器和摄像头识别手部和身体手势，并根据用户能力调整界面。该系统采用视觉Transformer（ViTs）、时间卷积网络（TCNs）和图注意力网络（GATs）进行手势处理，并通过联邦学习确保跨不同用户的隐私保护模型训练。强化学习优化了菜单布局和交互模式等界面元素。实验表明，与基线AR系统相比，该方法使运动障碍用户的任务完成效率提高了20%，用户满意度提高了25%。这种方法增强了AR的可访问性和可扩展性。", "summary": "本研究提出了一种创新的手势驱动增强现实交互系统，旨在解决运动障碍用户在AR环境中的可访问性问题。该系统结合了深度学习技术（如ViTs、TCNs、GATs）进行手势识别，并利用联邦学习保障数据隐私，同时通过强化学习动态优化用户界面。实验结果显示，该系统显著提升了运动障碍用户的任务效率和满意度，证明了其在增强AR可访问性和可扩展性方面的潜力。", "keywords": "深度学习, 联邦学习, 手势识别, 增强现实, 可访问性, 人机交互", "comments": "该论文的创新点在于结合了多种先进的深度学习技术（ViTs, TCNs, GATs）进行手势识别，并引入了联邦学习来解决多用户数据隐私问题，同时利用强化学习动态优化用户界面，从而为运动障碍用户提供了更无障碍的AR体验。其重要性体现在拓宽了AR技术的应用范围和用户群体，提升了人机交互的包容性。"}}
{"id": "2506.15400", "title": "The maximum-average subtensor problem: equilibrium and out-of-equilibrium properties", "authors": ["Vittorio Erba", "Nathan Malo Kupferschmid", "Rodrigo Pérez Ortiz", "Lenka Zdeborová"], "summary": "In this paper we introduce and study the Maximum-Average Subtensor ($p$-MAS)\nproblem, in which one wants to find a subtensor of size $k$ of a given random\ntensor of size $N$, both of order $p$, with maximum sum of entries. We are\nmotivated by recent work on the matrix case of the problem in which several\nequilibrium and non-equilibrium properties have been characterized analytically\nin the asymptotic regime $1 \\ll k \\ll N$, and a puzzling phenomenon was\nobserved involving the coexistence of a clustered equilibrium phase and an\nefficient algorithm which produces submatrices in this phase. Here we extend\nprevious results on equilibrium and algorithmic properties for the matrix case\nto the tensor case. We show that the tensor case has a similar equilibrium\nphase diagram as the matrix case, and an overall similar phenomenology for the\nconsidered algorithms. Additionally, we consider out-of-equilibrium landscape\nproperties using Overlap Gap Properties and Franz-Parisi analysis, and discuss\nthe implications or lack-thereof for average-case algorithmic hardness.", "comment": null, "cate": "cond-mat.dis-nn", "url": "http://arxiv.org/abs/2506.15400v1", "AI": {"title_translation": "最大平均子张量问题：平衡态和非平衡态性质", "tldr": "本文引入并研究了最大平均子张量（p-MAS）问题，将其从矩阵情况推广到张量情况，并分析了其平衡态和非平衡态性质，发现与矩阵情况相似的相图和算法现象。", "motivation": "本文的动机是近期关于矩阵情况下该问题的研究，其中分析了多种平衡态和非平衡态性质，并观察到一个令人费解的现象，即聚类平衡态与能有效产生该相中子矩阵的算法共存。", "method": "本文引入并研究了最大平均子张量（p-MAS）问题，旨在找到给定随机张量的最大条目和的子张量。研究方法是将先前关于矩阵情况的平衡态和算法性质的结果推广到张量情况。此外，还使用重叠间隙性质（Overlap Gap Properties）和Franz-Parisi分析来考察非平衡态景观性质。", "result": "研究结果表明，张量情况与矩阵情况具有相似的平衡态相图，并且对于所考虑的算法具有总体相似的现象学。", "conclusion": "本文讨论了平均情况算法硬度的含义或其缺乏。", "translation": "在本文中，我们引入并研究了最大平均子张量（p-MAS）问题，其中目标是找到给定大小为N的随机张量（两者阶数均为p）中大小为k的子张量，使其条目之和最大。我们受到近期关于该问题的矩阵情况研究的启发，在该研究中，在渐近状态$1 \text{ \textless\textless } k \text{ \textless\textless } N$下，分析了多种平衡态和非平衡态性质，并观察到一个令人费解的现象，即聚类平衡态与能有效产生该相中子矩阵的算法共存。在此，我们将先前关于矩阵情况的平衡态和算法性质的结果推广到张量情况。我们发现张量情况与矩阵情况具有相似的平衡态相图，并且对于所考虑的算法具有总体相似的现象学。此外，我们使用重叠间隙性质和Franz-Parisi分析来考察非平衡态景观性质，并讨论了其对平均情况算法硬度的含义或其缺乏。", "summary": "本文引入并研究了最大平均子张量（p-MAS）问题，旨在寻找给定随机张量中具有最大条目和的子张量。该研究将先前在矩阵情况下对平衡态和算法性质的分析推广到更高阶的张量情况。研究发现，张量情况下的平衡态相图和算法现象与矩阵情况相似。此外，通过重叠间隙性质和Franz-Parisi分析，本文还探讨了非平衡态景观性质及其对平均情况算法硬度的影响。", "keywords": "最大平均子张量,平衡态,非平衡态,张量,算法硬度", "comments": "这篇论文的创新点在于将最大平均子问题从矩阵推广到高阶张量，这在理论物理和机器学习等领域具有重要意义。它不仅扩展了现有成果，还通过研究平衡态和非平衡态性质，深入探讨了问题的复杂性，并对算法的平均情况硬度提供了见解。"}}
{"id": "2506.14837", "title": "Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction", "authors": ["Chengzhi Xu", "Yuyang Wang", "Lai Wei", "Lichao Sun", "Weiran Huang"], "summary": "Recently, multimodal large language models (MLLMs) have attracted increasing\nresearch attention due to their powerful visual understanding capabilities.\nWhile they have achieved impressive results on various vision tasks, their\nperformance on chart-to-code generation remains suboptimal. This task requires\nMLLMs to generate executable code that can reproduce a given chart, demanding\nnot only precise visual understanding but also accurate translation of visual\nelements into structured code. Directly prompting MLLMs to perform this complex\ntask often yields unsatisfactory results. To address this challenge, we propose\n{ChartIR}, an iterative refinement method based on structured instruction.\nFirst, we distinguish two tasks: visual understanding and code translation. To\naccomplish the visual understanding component, we design two types of\nstructured instructions: description and difference. The description\ninstruction captures the visual elements of the reference chart, while the\ndifference instruction characterizes the discrepancies between the reference\nchart and the generated chart. These instructions effectively transform visual\nfeatures into language representations, thereby facilitating the subsequent\ncode translation process. Second, we decompose the overall chart generation\npipeline into two stages: initial code generation and iterative refinement,\nenabling progressive enhancement of the final output. Experimental results show\nthat, compared to other method, our method achieves superior performance on\nboth the open-source model Qwen2-VL and the closed-source model GPT-4o.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14837v1", "AI": {"title_translation": "通过结构化指令改进图表到代码生成的迭代细化", "tldr": "针对多模态大语言模型在图表到代码生成方面表现不佳的问题，本文提出了{ChartIR}，一种基于结构化指令的迭代细化方法，显著提升了生成性能。", "motivation": "多模态大型语言模型（MLLMs）在图表到代码生成任务上的性能仍不理想，该任务需要精确的视觉理解和将视觉元素准确转换为结构化代码。直接提示MLLMs往往效果不佳。", "method": "本文提出了{ChartIR}，一种基于结构化指令的迭代细化方法。该方法首先区分视觉理解和代码翻译两个任务。在视觉理解方面，设计了描述和差异两种结构化指令，将视觉特征转化为语言表示。其次，将整个图表生成流程分解为初始代码生成和迭代细化两个阶段，以逐步增强最终输出。", "result": "实验结果表明，与现有方法相比，本文提出的方法在开源模型Qwen2-VL和闭源模型GPT-4o上均取得了优越的性能。", "conclusion": "本文提出的{ChartIR}方法通过结构化指令和迭代细化有效解决了图表到代码生成任务的挑战，显著提升了多模态大语言模型的性能。", "translation": "最近，多模态大型语言模型（MLLMs）因其强大的视觉理解能力而受到越来越多的研究关注。尽管它们在各种视觉任务上取得了令人印象深刻的成果，但它们在图表到代码生成方面的性能仍然不尽如人意。这项任务要求MLLMs生成可重现给定图表的可执行代码，这不仅需要精确的视觉理解，还需要将视觉元素准确地转换为结构化代码。直接提示MLLMs执行这项复杂的任务通常会产生不令人满意的结果。为了解决这个挑战，我们提出了{ChartIR}，一种基于结构化指令的迭代细化方法。首先，我们区分了两个任务：视觉理解和代码翻译。为了完成视觉理解部分，我们设计了两种类型的结构化指令：描述和差异。描述指令捕捉参考图表的视觉元素，而差异指令则描述参考图表与生成的图表之间的差异。这些指令有效地将视觉特征转换为语言表示，从而促进后续的代码翻译过程。其次，我们将整个图表生成流程分解为两个阶段：初始代码生成和迭代细化，从而实现最终输出的逐步增强。实验结果表明，与其他方法相比，我们的方法在开源模型Qwen2-VL和闭源模型GPT-4o上都取得了优越的性能。", "summary": "本文提出了一种名为{ChartIR}的迭代细化方法，旨在提高多模态大语言模型（MLLMs）在图表到代码生成任务上的性能。该方法通过区分视觉理解和代码翻译任务，并引入描述和差异两种结构化指令来转化视觉特征。此外，它将生成流程分解为初始代码生成和迭代细化两个阶段。实验结果表明，{ChartIR}在开源和闭源MLLMs上均表现出优越的性能。", "keywords": "图表到代码生成, 迭代细化, 结构化指令, 多模态大语言模型, 视觉理解", "comments": "该论文的创新之处在于将复杂的图表到代码生成任务分解为独立的子任务（视觉理解和代码翻译），并引入了结构化指令（描述、差异）来弥合视觉与语言之间的鸿沟，结合迭代细化方法。这种结构化和迭代的方法是对直接提示方法的重大改进。"}}
{"id": "2506.15647", "title": "Exploring and Exploiting the Inherent Efficiency within Large Reasoning Models for Self-Guided Efficiency Enhancement", "authors": ["Weixiang Zhao", "Jiahe Guo", "Yang Deng", "Xingyu Sui", "Yulin Hu", "Yanyan Zhao", "Wanxiang Che", "Bing Qin", "Tat-Seng Chua", "Ting Liu"], "summary": "Recent advancements in large reasoning models (LRMs) have significantly\nenhanced language models' capabilities in complex problem-solving by emulating\nhuman-like deliberative thinking. However, these models often exhibit\noverthinking (i.e., the generation of unnecessarily verbose and redundant\ncontent), which hinders efficiency and inflates inference cost. In this work,\nwe explore the representational and behavioral origins of this inefficiency,\nrevealing that LRMs inherently possess the capacity for more concise reasoning.\nEmpirical analyses show that correct reasoning paths vary significantly in\nlength, and the shortest correct responses often suffice, indicating untapped\nefficiency potential. Exploiting these findings, we propose two lightweight\nmethods to enhance LRM efficiency. First, we introduce Efficiency Steering, a\ntraining-free activation steering technique that modulates reasoning behavior\nvia a single direction in the model's representation space. Second, we develop\nSelf-Rewarded Efficiency RL, a reinforcement learning framework that\ndynamically balances task accuracy and brevity by rewarding concise correct\nsolutions. Extensive experiments on seven LRM backbones across multiple\nmathematical reasoning benchmarks demonstrate that our methods significantly\nreduce reasoning length while preserving or improving task performance. Our\nresults highlight that reasoning efficiency can be improved by leveraging and\nguiding the intrinsic capabilities of existing models in a self-guided manner.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15647v1", "AI": {"title_translation": "探索和利用大型推理模型内在效率以实现自我引导的效率提升", "tldr": "本文探讨并利用大型推理模型（LRM）中固有的效率，通过两种轻量级方法（效率引导和自奖励效率强化学习）减少推理冗余，同时保持或提升性能。", "motivation": "大型推理模型（LRM）在复杂问题解决方面取得了显著进展，但它们常出现“过度思考”，即生成不必要的冗长和冗余内容，这会降低效率并增加推理成本。", "method": "本文提出了两种轻量级方法来提高LRM的效率：1. 效率引导（Efficiency Steering）：一种无需训练的激活引导技术，通过模型表示空间中的单一方向来调节推理行为。2. 自奖励效率强化学习（Self-Rewarded Efficiency RL）：一个动态平衡任务准确性和简洁性的强化学习框架，通过奖励简洁正确的解决方案来工作。", "result": "在七个LRM骨干模型和多个数学推理基准测试上的广泛实验表明，所提出的方法显著缩短了推理长度，同时保持或提高了任务性能。", "conclusion": "推理效率可以通过利用和引导现有模型固有的能力以自我引导的方式得到改善。", "translation": "大型推理模型（LRM）的最新进展通过模仿人类的审慎思维，显著增强了语言模型解决复杂问题的能力。然而，这些模型常常表现出过度思考（即生成不必要的冗长和冗余内容），这阻碍了效率并增加了推理成本。在这项工作中，我们探索了这种低效率的表征和行为根源，揭示了LRM本身具有更简洁推理的能力。实证分析表明，正确的推理路径长度差异很大，最短的正确响应通常就足够了，这表明存在未开发的效率潜力。利用这些发现，我们提出了两种轻量级方法来提高LRM的效率。首先，我们引入了效率引导（Efficiency Steering），这是一种无需训练的激活引导技术，通过模型表示空间中的单一方向来调节推理行为。其次，我们开发了自奖励效率强化学习（Self-Rewarded Efficiency RL），这是一种强化学习框架，通过奖励简洁正确的解决方案来动态平衡任务准确性和简洁性。在七个LRM骨干模型和多个数学推理基准测试上的广泛实验表明，我们的方法显著缩短了推理长度，同时保持或提高了任务性能。我们的结果强调，可以通过利用和引导现有模型内在的能力以自我引导的方式来提高推理效率。", "summary": "本文针对大型推理模型（LRM）中存在的“过度思考”导致的效率低下和成本增加问题，深入探讨了其内在原因，并揭示了LRM本身具备更简洁推理的潜力。基于此，研究提出了两种轻量级效率提升方法：一是“效率引导”，通过激活引导技术调节推理行为；二是“自奖励效率强化学习”，通过奖励机制平衡准确性和简洁性。实验证明，这些方法在保持或提升性能的同时，显著减少了推理长度，表明LRM的内在效率可以被有效利用和引导。", "keywords": "大型推理模型, 效率提升, 过度思考, 效率引导, 强化学习", "comments": "这篇论文的创新点在于它识别并利用了大型推理模型固有的“过度思考”问题，并提出了两种无需大规模重新训练的轻量级方法来解决这一问题。通过直接干预模型内部表示或采用强化学习奖励机制，它有效地在推理效率和准确性之间找到了平衡。这种自我引导的效率提升策略对于降低大型模型推理成本和提高实用性具有重要意义，尤其是在资源受限或需要快速响应的场景下。"}}
{"id": "2506.15537", "title": "Automatic Metadata Capture and Processing for High-Performance Workflows", "authors": ["Polina Shpilker", "Line Pouchard"], "summary": "Modern workflows run on increasingly heterogeneous computing architectures\nand with this heterogeneity comes additional complexity. We aim to apply the\nFAIR principles for research reproducibility by developing software to collect\nmetadata annotations for workflows run on HPC systems. We experiment with two\npossible formats to uniformly store these metadata, and reorganize the\ncollected metadata to be as easy to use as possible for researchers studying\ntheir workflow performance.", "comment": null, "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15537v1", "AI": {"title_translation": "高性能工作流的自动元数据捕获与处理", "tldr": "开发软件以在异构HPC系统上自动捕获和处理工作流元数据，旨在提高研究可重复性和元数据易用性。", "motivation": "现代工作流在日益异构的计算架构上运行，导致复杂性增加。为了应用FAIR原则并提高研究可重复性，需要开发工具来捕获和管理HPC系统上的工作流元数据。", "method": "通过开发软件来收集在HPC系统上运行的工作流的元数据注释。实验了两种可能的格式来统一存储这些元数据，并重新组织收集到的元数据，使其尽可能方便研究人员研究其工作流性能。", "result": "开发了用于HPC系统工作流的元数据收集软件，并探索了两种统一存储和优化元数据组织的方式，以提高其可用性。", "conclusion": "本研究通过开发自动元数据捕获和处理工具，旨在解决异构HPC系统上工作流的复杂性，并提升研究可重复性及元数据的使用便利性。", "translation": "现代工作流在日益异构的计算架构上运行，这种异构性带来了额外的复杂性。我们旨在通过开发软件来收集在HPC系统上运行的工作流的元数据注释，从而应用FAIR原则以实现研究可重复性。我们试验了两种可能的格式来统一存储这些元数据，并重新组织收集到的元数据，使其尽可能方便研究人员研究其工作流性能。", "summary": "本论文旨在解决现代异构计算架构下工作流的复杂性问题，通过开发软件自动捕获和处理HPC系统上的工作流元数据。研究探索了两种元数据存储格式，并对数据进行重新组织，以提高研究可重复性，并方便研究人员分析工作流性能。", "keywords": "元数据捕获, 高性能计算, 工作流, FAIR原则, 可重复性", "comments": "该论文的创新点在于将FAIR原则应用于高性能计算工作流的元数据捕获和处理，旨在解决异构系统带来的复杂性，并提升研究的可重复性。通过自动化元数据管理，可以显著降低研究人员的负担，并促进对工作流性能的深入分析。"}}
{"id": "2506.15623", "title": "Minding the Politeness Gap in Cross-cultural Communication", "authors": ["Yuka Machino", "Matthias Hofer", "Max Siegel", "Joshua B. Tenenbaum", "Robert D. Hawkins"], "summary": "Misunderstandings in cross-cultural communication often arise from subtle\ndifferences in interpretation, but it is unclear whether these differences\narise from the literal meanings assigned to words or from more general\npragmatic factors such as norms around politeness and brevity. In this paper,\nwe report three experiments examining how speakers of British and American\nEnglish interpret intensifiers like \"quite\" and \"very.\" To better understand\nthese cross-cultural differences, we developed a computational cognitive model\nwhere listeners recursively reason about speakers who balance informativity,\npoliteness, and utterance cost. Our model comparisons suggested that\ncross-cultural differences in intensifier interpretation stem from a\ncombination of (1) different literal meanings, (2) different weights on\nutterance cost. These findings challenge accounts based purely on semantic\nvariation or politeness norms, demonstrating that cross-cultural differences in\ninterpretation emerge from an intricate interplay between the two.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15623v1", "AI": {"title_translation": "弥合跨文化交流中的礼貌鸿沟", "tldr": "本研究通过实验和计算模型，发现英美英语中强度词的跨文化解读差异源于字面意义和表达成本权重的结合，挑战了单一语义或礼貌规范的解释。", "motivation": "跨文化交流中的误解常源于解释上的细微差异，但尚不清楚这些差异是源于词语的字面意义，还是源于礼貌和简洁等更普遍的语用因素。", "method": "报告了三项实验，考察英式英语和美式英语使用者如何解释“quite”和“very”等强度词。开发了一个计算认知模型，该模型中听者递归地推断说话者在信息量、礼貌和表达成本之间进行平衡。", "result": "模型比较表明，强度词的跨文化解释差异源于(1)不同的字面意义，以及(2)不同的表达成本权重。这些发现挑战了纯粹基于语义变异或礼貌规范的解释。", "conclusion": "跨文化解释差异是语义和语用因素（如礼貌规范）之间复杂相互作用的结果。", "translation": "跨文化交流中的误解往往源于解释上的细微差异，但目前尚不清楚这些差异是源于词语的字面意义，还是源于更普遍的语用因素，例如关于礼貌和简洁的规范。在本文中，我们报告了三项实验，考察了英式英语和美式英语使用者如何解释“quite”和“very”等强度词。为了更好地理解这些跨文化差异，我们开发了一个计算认知模型，其中听者递归地推断说话者在信息量、礼貌和表达成本之间进行平衡。我们的模型比较表明，强度词的跨文化解释差异源于(1)不同的字面意义，(2)不同的表达成本权重。这些发现挑战了纯粹基于语义变异或礼貌规范的解释，表明跨文化解释差异是两者之间复杂相互作用的结果。", "summary": "本研究旨在探究跨文化交流中误解的来源，特别是英美英语使用者对强度词“quite”和“very”的解释差异。通过三项实验和一个计算认知模型，该模型模拟了听者在信息量、礼貌和表达成本之间的权衡，研究发现这些跨文化差异不仅源于词语的字面意义不同，也源于对表达成本的不同权重。这表明跨文化解释差异是语义和语用因素复杂相互作用的产物，而非单一因素导致。", "keywords": "跨文化交流, 礼貌, 强度词, 计算认知模型, 语义语用", "comments": "该研究创新性地结合了实验方法和计算认知模型，深入探讨了跨文化交流中礼貌和语义的复杂交互作用。其发现挑战了传统上将跨文化差异归因于单一因素的观点，为理解语言和文化如何共同塑造交流提供了新的视角。其重要性在于为跨文化交流的教学和实践提供了更精细的理论基础。"}}
{"id": "2506.15447", "title": "Model Predictive Path-Following Control for a Quadrotor", "authors": ["David Leprich", "Mario Rosenfelder", "Mario Hermle", "Jingshan Chen", "Peter Eberhard"], "summary": "Automating drone-assisted processes is a complex task. Many solutions rely on\ntrajectory generation and tracking, whereas in contrast, path-following control\nis a particularly promising approach, offering an intuitive and natural\napproach to automate tasks for drones and other vehicles. While different\nsolutions to the path-following problem have been proposed, most of them lack\nthe capability to explicitly handle state and input constraints, are formulated\nin a conservative two-stage approach, or are only applicable to linear systems.\nTo address these challenges, the paper is built upon a Model Predictive\nControl-based path-following framework and extends its application to the\nCrazyflie quadrotor, which is investigated in hardware experiments. A cascaded\ncontrol structure including an underlying attitude controller is included in\nthe Model Predictive Path-Following Control formulation to meet the challenging\nreal-time demands of quadrotor control. The effectiveness of the proposed\nmethod is demonstrated through real-world experiments, representing, to the\nbest of the authors' knowledge, a novel application of this MPC-based\npath-following approach to the quadrotor. Additionally, as an extension to the\noriginal method, to allow for deviations of the path in cases where the precise\nfollowing of the path might be overly restrictive, a corridor path-following\napproach is presented.", "comment": "15 pages, 11 figures, submitted to PAMM 2025", "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15447v1", "AI": {"title_translation": "四旋翼飞行器的模型预测路径跟踪控制", "tldr": "本文提出了一种基于模型预测控制（MPC）的路径跟踪框架，并将其应用于Crazyflie四旋翼飞行器，通过硬件实验验证了其有效性。该方法解决了现有路径跟踪方法在处理约束、分阶段方法或仅适用于线性系统方面的不足，并引入了走廊路径跟踪以允许路径偏差。", "motivation": "自动化无人机辅助过程是一项复杂的任务。许多解决方案依赖于轨迹生成和跟踪，而路径跟踪控制是一种有前景的方法，但现有的大多数路径跟踪解决方案缺乏明确处理状态和输入约束的能力，采用保守的两阶段方法，或仅适用于线性系统。为了解决这些挑战，本文提出了新的方法。", "method": "本文基于模型预测控制（MPC）的路径跟踪框架，并将其应用于Crazyflie四旋翼飞行器。控制方案中包含一个级联控制结构，其中包括一个底层的姿态控制器，以满足四旋翼控制的实时需求。此外，作为对原始方法的扩展，为了允许在精确跟踪路径可能过于严格的情况下进行路径偏差，本文提出了一种走廊路径跟踪方法。", "result": "通过真实世界的实验证明了所提出方法的有效性。据作者所知，这是MPC-based路径跟踪方法在四旋翼飞行器上的首次应用。此外，还成功引入了走廊路径跟踪方法，允许路径偏差。", "conclusion": "本文成功开发并验证了一种基于模型预测控制的路径跟踪框架，该框架能够有效处理状态和输入约束，并适用于四旋翼飞行器。通过真实世界的实验证明了其有效性，并引入了创新的走廊路径跟踪方法，使其在实际应用中更具鲁棒性和灵活性。", "translation": "自动化无人机辅助过程是一项复杂的任务。许多解决方案依赖于轨迹生成和跟踪，而相比之下，路径跟踪控制是一种特别有前景的方法，为无人机和其他车辆的自动化任务提供了一种直观而自然的方法。尽管已经提出了许多路径跟踪问题的解决方案，但其中大多数缺乏明确处理状态和输入约束的能力，采用保守的两阶段方法，或者仅适用于线性系统。为了解决这些挑战，本文建立在基于模型预测控制的路径跟踪框架之上，并将其应用扩展到Crazyflie四旋翼飞行器，并通过硬件实验进行了研究。模型预测路径跟踪控制公式中包含一个级联控制结构，其中包括一个底层的姿态控制器，以满足四旋翼控制的严苛实时要求。通过真实世界的实验证明了所提出方法的有效性，据作者所知，这是这种基于MPC的路径跟踪方法在四旋翼飞行器上的首次新颖应用。此外，作为对原始方法的扩展，为了允许在精确跟踪路径可能过于严格的情况下进行路径偏差，本文提出了一种走廊路径跟踪方法。", "summary": "本文提出了一种用于四旋翼飞行器的模型预测路径跟踪控制方法，旨在解决现有路径跟踪方案在约束处理、保守两阶段方法及线性系统适用性方面的不足。该方法基于模型预测控制框架，并结合级联控制结构以满足实时性要求。研究在Crazyflie四旋翼飞行器上通过硬件实验进行了验证，展示了其有效性，并被认为是该MPC-based路径跟踪方法在四旋翼上的首次应用。此外，为增加灵活性，还引入了允许路径偏差的走廊路径跟踪方法。", "keywords": "模型预测控制, 路径跟踪, 四旋翼飞行器, 实时控制, 走廊路径跟踪", "comments": "本文的创新点在于将MPC-based路径跟踪框架应用于四旋翼飞行器，并首次在硬件实验中验证了其有效性，这对于无人机自动化任务具有重要意义。引入的级联控制结构和走廊路径跟踪方法增强了系统的实时性和鲁棒性，使其在实际应用中更具普适性。该研究解决了现有方法在处理约束方面的局限性，为未来无人机路径跟踪控制提供了新的思路。"}}
{"id": "2506.15470", "title": "Analyzing URA Geometry for Enhanced Spatial Multiplexing and Extended Near-Field Coverage", "authors": ["Ahmed Hussain", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil"], "summary": "With the deployment of large antenna arrays at high frequency bands, future\nwireless communication systems are likely to operate in the radiative\nnear-field. Unlike far-field beam steering, near-field beams can be focused\nwithin a spatial region of finite depth, enabling spatial multiplexing in both\nthe angular and range dimensions. This paper derives the beamdepth for a\ngeneralized uniform rectangular array (URA) and investigates how array geometry\ninfluences the near-field beamdepth and the limits where near-field\nbeamfocusing is achievable. To characterize the near-field boundary in terms of\nbeamfocusing and spatial multiplexing gains, we define the effective\nbeamfocusing Rayleigh distance (EBRD) for a generalized URA. Our analysis\nreveals that while a square URA achieves the narrowest beamdepth, the EBRD is\nmaximized for a wide or tall URA. However, despite its narrow beamdepth, a\nsquare URA may experience a reduction in multiuser sum rate due to its severely\nconstrained EBRD. Simulation results confirm that a wide or tall URA achieves a\nsum rate of 3.5 X more than that of a square URA, benefiting from the extended\nEBRD and improved spatial multiplexing capabilities.", "comment": null, "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15470v1", "AI": {"title_translation": "分析URA几何结构以增强空间复用和扩展近场覆盖", "tldr": "方形URA虽然波束深度最窄，但近场覆盖受限；宽或高的URA能扩展近场覆盖并显著提高和速率，更利于空间复用。", "motivation": "未来的无线通信系统将部署大型高频天线阵列，并在辐射近场中运行。近场波束能够实现角度和距离维度上的空间复用。因此，研究阵列几何结构如何影响近场波束深度和近场聚焦的极限，对于优化空间复用性能至关重要。", "method": "本文推导了广义均匀矩形阵列（URA）的波束深度，并研究了阵列几何结构对近场波束深度和近场波束聚焦极限的影响。为了量化波束聚焦和空间复用增益方面的近场边界，本文定义了广义URA的有效波束聚焦瑞利距离（EBRD）。", "result": "分析表明，方形URA实现了最窄的波束深度，但其有效波束聚焦瑞利距离（EBRD）受到严重限制。相比之下，宽或高的URA能够最大化EBRD。仿真结果证实，宽或高的URA比方形URA实现了3.5倍的和速率增益，这得益于其扩展的EBRD和改进的空间复用能力。", "conclusion": "尽管方形URA具有最窄的波束深度，但在近场通信中，宽或高的URA通过扩展有效波束聚焦瑞利距离，能够显著增强空间复用能力并实现更高的和速率。", "translation": "随着高频段大型天线阵列的部署，未来的无线通信系统很可能在辐射近场中运行。与远场波束控制不同，近场波束可以聚焦在有限深度的空间区域内，从而在角度和距离维度上实现空间复用。本文推导了广义均匀矩形阵列（URA）的波束深度，并研究了阵列几何结构如何影响近场波束深度以及近场波束聚焦可实现的极限。为了表征波束聚焦和空间复用增益方面的近场边界，我们为广义URA定义了有效波束聚焦瑞利距离（EBRD）。我们的分析表明，虽然方形URA实现了最窄的波束深度，但宽或高的URA使EBRD最大化。然而，尽管方形URA具有窄波束深度，但由于其EBRD受到严格限制，多用户和速率可能会降低。仿真结果证实，宽或高的URA实现了比方形URA高3.5倍的和速率，这得益于扩展的EBRD和改进的空间复用能力。", "summary": "本文探讨了均匀矩形阵列（URA）几何结构对未来无线通信系统中近场空间复用和覆盖的影响。论文推导了URA的波束深度，并引入了有效波束聚焦瑞利距离（EBRD）来表征近场边界。研究发现，虽然方形URA能实现最窄的波束深度，但宽或高的URA能最大化EBRD，从而在多用户和速率方面比方形URA高出3.5倍，这得益于其增强的空间复用能力。", "keywords": "URA, 近场, 空间复用, 波束成形, EBRD", "comments": "这篇论文为近场通信中的天线阵列设计提供了宝贵的见解，揭示了波束深度与有效近场覆盖之间的权衡。EBRD的引入是表征近场性能的关键贡献。研究发现宽/高URA在和速率方面优于方形URA，这挑战了波束聚焦的直观认知，并为未来无线系统中的阵列设计提供了实用的指导。"}}
{"id": "2506.14794", "title": "Assembly of Experts: Linear-time construction of the Chimera LLM variants with emergent and adaptable behaviors", "authors": ["Henrik Klagges", "Robert Dahlke", "Fabian Klemm", "Benjamin Merkel", "Daniel Klingmann", "David A. Reiss", "Dan Zecha"], "summary": "Requiring $10^{13}$-$10^{15}$ FLOPs to calculate one 8 bit weight in an LLM\nduring pretraining is extremely expensive and seems inefficient. To better\nleverage the huge investments made into pretrained models, we develop the new\n\"Assembly-of-Experts\" (AoE) construction method to create capable child\nvariants of existing Mixture-of-Experts parent models in linear time. Model\nweight tensors get interpolated individually, allowing to enhance or suppress\nsemantic features of the parents.\n  Varying the proportion of weights taken from the parent models, we observe\nsome properties of the AoE child model changing gradually, while other\nbehavioral traits emerge with a sharp transition. Surprisingly, nearly every\ngenerated model is functional and capable, which makes searching the model\nspace straightforward.\n  We construct the DeepSeek R1T \"Chimera\", a 671B open-weights hybrid model\ncombining DeepSeek's V3-0324 and R1 model variants. The child inherits only the\nrouted expert tensors of R1, but still achieves about R1-level intelligence. At\nthe same time, it uses about 40\\% fewer output tokens, close to V3 speed.\nConstructed without any fine-tuning or distillation, the Chimera exhibits\nsurprisingly compact, orderly reasoning compared to its parent models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14794v1", "AI": {"title_translation": "专家集合：线性时间构建具有涌现和适应性行为的Chimera大型语言模型变体", "tldr": "开发了一种名为“专家集合”（AoE）的新方法，可以在线性时间内从现有专家混合（MoE）父模型构建LLM子变体，通过插值模型权重实现语义特征的增强或抑制，并构建了名为“Chimera”的混合模型，该模型在保持智能水平的同时显著提高了效率和推理能力。", "motivation": "预训练LLM的成本极高且效率低下，需要大量FLOPs来计算单个权重。研究旨在更好地利用已投入的预训练模型，以更经济高效的方式创建新的LLM变体。", "method": "提出了“专家集合”（AoE）构建方法，通过单独插值模型权重张量，在线性时间内从现有的专家混合（MoE）父模型创建子变体。这种方法允许增强或抑制父模型的语义特征。他们通过改变父模型权重的比例来观察子模型的属性变化。", "result": "几乎所有生成的模型都功能强大且有能力，使得模型空间搜索变得简单。构建了671B的DeepSeek R1T “Chimera”混合模型，该模型结合了DeepSeek的V3-0324和R1模型变体。Chimera仅继承了R1的路由专家张量，但仍达到了R1级别的智能，同时输出token减少约40%，速度接近V3。在没有进行任何微调或蒸馏的情况下，Chimera表现出比其父模型更紧凑、更有序的推理能力。", "conclusion": "AoE方法能够在线性时间内高效构建功能强大的LLM子变体，这些变体在保持高智能水平的同时，能显著提升效率和推理能力，并展现出令人惊讶的紧凑有序的推理行为。", "translation": "在预训练期间，计算LLM中一个8位权重需要$10^{13}$-$10^{15}$ FLOPs，这极其昂贵且似乎效率低下。为了更好地利用在预训练模型上的巨大投资，我们开发了一种新的“专家集合”（AoE）构建方法，以线性时间从现有专家混合（Mixture-of-Experts）父模型创建出有能力的子变体。模型权重张量被单独插值，从而可以增强或抑制父模型的语义特征。\n通过改变从父模型中获取权重的比例，我们观察到AoE子模型的某些属性逐渐变化，而其他行为特征则通过急剧转变涌现。令人惊讶的是，几乎每个生成的模型都功能强大且有能力，这使得模型空间搜索变得简单明了。\n我们构建了DeepSeek R1T “Chimera”，一个671B的开源混合模型，结合了DeepSeek的V3-0324和R1模型变体。该子模型仅继承了R1的路由专家张量，但仍达到了R1级别的智能水平。同时，它使用的输出token减少了约40%，接近V3的速度。Chimera在未经任何微调或蒸馏的情况下构建，与它的父模型相比，展现出令人惊讶的紧凑、有序的推理能力。", "summary": "本文提出了一种名为“专家集合”（AoE）的新方法，旨在解决大型语言模型（LLM）预训练成本高昂的问题。AoE方法能够在线性时间内，通过对现有专家混合（MoE）父模型的权重进行单独插值，高效地构建出具有可塑性行为的LLM子变体。研究表明，该方法生成的模型普遍具有功能性，且可以通过调整父模型权重比例来观察行为的渐变或突变。作者成功构建了671B的DeepSeek R1T “Chimera”混合模型，该模型在继承父模型智能水平的同时，显著提升了效率并展现出更优秀的推理能力，且无需额外微调。", "keywords": "专家集合, LLM, 线性时间, Chimera, 模型构建", "comments": "这篇论文的创新点在于提出了“专家集合”（AoE）这一新颖的LLM构建范式，它通过线性时间内的权重插值，有效地利用现有预训练MoE模型来生成新的、具有可控行为的LLM变体。这种方法极大地降低了新模型开发的成本和复杂性，并且在实际应用中，构建出的Chimera模型在保持高智能水平的同时，显著提升了效率和推理的紧凑性，展示了其巨大的实用价值和对未来LLM生态的潜在影响。"}}
{"id": "2506.15556", "title": "PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction", "authors": ["Shufan Li", "Aditya Grover"], "summary": "Large Language Models (LLMs) are widely used in real-time voice chat\napplications, typically in combination with text-to-speech (TTS) systems to\ngenerate audio responses. However, their large size often leads to noticeable\nlatency between the end of user input and the start of audio output, resulting\nin suboptimal user experiences. This latency is particularly evident when LLMs\nare deployed as single-user voice assistants on consumer-grade hardware with\nlimited computing capacity. We discovered that this latency is primarily\ndominated by the time it takes for the LLMs to generate the first sentence,\nwhich is required as input by the TTS systems that synthesize audio responses\non a sentence-by-sentence basis. To address this bottleneck, we propose\nPredictive Generation (PredGen), a novel framework that mitigates-or even\neliminates-this delay through speculative decoding at input time. PredGen\ngenerates candidate responses while the user is still speaking, enabling the\nsystem to begin TTS processing with minimal delay. Simulated experiments on the\nLmsys and MT-Bench datasets show that the proposed method can effectively\nreduce the latency by around 2x across a wide range of use cases, while\nincurring only minimal additional computation cost at input time-computation\nthat would otherwise go unused.", "comment": "16 pages,4 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15556v1", "AI": {"title_translation": "PredGen：通过输入时推测加速大型语言模型推理以实现实时语音交互", "tldr": "PredGen通过在用户说话时推测性生成响应，显著减少了大型语言模型在实时语音交互中生成首句的延迟。", "motivation": "大型语言模型（LLMs）在实时语音聊天应用中与文本转语音（TTS）系统结合使用时，用户输入结束到音频输出开始之间存在明显延迟，尤其是在计算能力有限的消费级硬件上作为单用户语音助手部署时。这种延迟主要由LLM生成第一句话所需的时间决定，而TTS系统需要第一句话作为输入。", "method": "本文提出了预测生成（PredGen）框架，通过在输入时进行推测性解码来缓解或消除延迟。PredGen在用户仍在说话时生成候选响应，从而使系统能够以最小延迟开始TTS处理。", "result": "在Lmsys和MT-Bench数据集上的模拟实验表明，PredGen方法可以将延迟减少约2倍，同时只在输入时产生最小的额外计算成本（这些计算原本是未使用的）。", "conclusion": "PredGen通过在用户输入时进行推测性解码，有效解决了大型语言模型在实时语音交互中首句生成延迟的瓶颈，显著提升了用户体验。", "translation": "大型语言模型（LLMs）广泛应用于实时语音聊天应用程序中，通常与文本转语音（TTS）系统结合使用以生成音频响应。然而，其庞大的体量常常导致用户输入结束到音频输出开始之间出现明显的延迟，从而导致次优的用户体验。当LLMs作为单用户语音助手部署在计算能力有限的消费级硬件上时，这种延迟尤为明显。我们发现，这种延迟主要由LLM生成第一句话所需的时间决定，而TTS系统需要这句话作为输入，并逐句合成音频响应。为了解决这一瓶颈，我们提出了预测生成（PredGen），一个新颖的框架，通过在输入时进行推测性解码来缓解——甚至消除——这种延迟。PredGen在用户仍在说话时生成候选响应，使系统能够以最小的延迟开始TTS处理。在Lmsys和MT-Bench数据集上的模拟实验表明，所提出的方法可以在广泛的使用场景中有效地将延迟减少约2倍，同时只在输入时产生最小的额外计算成本——这些计算原本是未使用的。", "summary": "本文提出了PredGen框架，旨在解决大型语言模型在实时语音交互中由于生成首句而导致的显著延迟问题。该框架通过在用户说话时进行推测性解码来预先生成响应，从而显著缩短了文本转语音系统开始处理的时间。实验结果表明，PredGen能够将延迟减少约2倍，且仅带来微小的额外计算开销，从而提升了用户在消费级硬件上使用LLM语音助手的体验。", "keywords": "大型语言模型, 实时语音交互, 推测性解码, 延迟, PredGen", "comments": "PredGen提出了一种创新的方法来解决LLM在实时语音交互中的首句生成延迟问题，通过利用用户输入时的“空闲”计算资源进行推测性解码，有效提升了用户体验。其创新点在于将推测性生成提前到用户输入阶段，而不是等待用户输入完成，这对于优化实时交互系统至关重要。"}}
{"id": "2506.15365", "title": "FedWSIDD: Federated Whole Slide Image Classification via Dataset Distillation", "authors": ["Haolong Jin", "Shenglin Liu", "Cong Cong", "Qingmin Feng", "Yongzhi Liu", "Lina Huang", "Yingzi Hu"], "summary": "Federated learning (FL) has emerged as a promising approach for collaborative\nmedical image analysis, enabling multiple institutions to build robust\npredictive models while preserving sensitive patient data. In the context of\nWhole Slide Image (WSI) classification, FL faces significant challenges,\nincluding heterogeneous computational resources across participating medical\ninstitutes and privacy concerns. To address these challenges, we propose\nFedWSIDD, a novel FL paradigm that leverages dataset distillation (DD) to learn\nand transmit synthetic slides. On the server side, FedWSIDD aggregates\nsynthetic slides from participating centres and distributes them across all\ncentres. On the client side, we introduce a novel DD algorithm tailored to\nhistopathology datasets which incorporates stain normalisation into the\ndistillation process to generate a compact set of highly informative synthetic\nslides. These synthetic slides, rather than model parameters, are transmitted\nto the server. After communication, the received synthetic slides are combined\nwith original slides for local tasks. Extensive experiments on multiple WSI\nclassification tasks, including CAMELYON16 and CAMELYON17, demonstrate that\nFedWSIDD offers flexibility for heterogeneous local models, enhances local WSI\nclassification performance, and preserves patient privacy. This makes it a\nhighly effective solution for complex WSI classification tasks. The code is\navailable at FedWSIDD.", "comment": "MICCAI 2025", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15365v1", "AI": {"title_translation": "FedWSIDD：基于数据集蒸馏的联邦全玻片图像分类", "tldr": "FedWSIDD通过数据集蒸馏生成和传输合成玻片，以解决联邦学习在全玻片图像分类中面临的异构资源和隐私挑战，从而提高性能并保护患者隐私。", "motivation": "联邦学习（FL）在全玻片图像（WSI）分类中面临挑战，包括参与医疗机构之间计算资源的异构性以及患者数据隐私问题。", "method": "本文提出了FedWSIDD，一种利用数据集蒸馏（DD）学习和传输合成玻片的FL范式。在服务器端，FedWSIDD聚合来自各中心的合成玻片并分发给所有中心。在客户端，引入了一种专门针对组织病理学数据集的DD算法，该算法将染色归一化纳入蒸馏过程，以生成一组紧凑且信息丰富的合成玻片。这些合成玻片（而非模型参数）被传输到服务器。通信后，接收到的合成玻片与原始玻片结合用于本地任务。", "result": "在CAMELYON16和CAMELYON17等多个WSI分类任务上的广泛实验表明，FedWSIDD为异构本地模型提供了灵活性，增强了本地WSI分类性能，并保护了患者隐私。", "conclusion": "FedWSIDD是一种针对复杂WSI分类任务的高效解决方案。", "translation": "联邦学习（FL）已成为一种有前景的协同医疗图像分析方法，它使多个机构能够构建鲁棒的预测模型，同时保护敏感的患者数据。在全玻片图像（WSI）分类的背景下，FL面临着重大挑战，包括参与医疗机构之间计算资源的异构性以及隐私问题。为了解决这些挑战，我们提出了FedWSIDD，这是一种新颖的FL范式，它利用数据集蒸馏（DD）来学习和传输合成玻片。在服务器端，FedWSIDD聚合来自参与中心的合成玻片并将它们分发给所有中心。在客户端，我们引入了一种针对组织病理学数据集量身定制的新型DD算法，该算法将染色归一化纳入蒸馏过程，以生成一组紧凑且信息量大的合成玻片。这些合成玻片，而不是模型参数，被传输到服务器。通信后，接收到的合成玻片与原始玻片结合用于本地任务。在包括CAMELYON16和CAMELYON17在内的多个WSI分类任务上的广泛实验表明，FedWSIDD为异构本地模型提供了灵活性，增强了本地WSI分类性能，并保护了患者隐私。这使其成为复杂WSI分类任务的高度有效解决方案。代码可在FedWSIDD获取。", "summary": "本文提出了FedWSIDD，一种基于数据集蒸馏（DD）的联邦学习（FL）新范式，用于解决全玻片图像（WSI）分类中联邦学习面临的异构计算资源和隐私挑战。FedWSIDD通过生成和传输合成玻片而非模型参数来保护隐私。客户端的DD算法结合了染色归一化，以生成高质量的合成数据。实验证明，FedWSIDD在保持患者隐私的同时，提高了WSI分类性能并支持异构本地模型。", "keywords": "联邦学习, 全玻片图像, 数据集蒸馏, 隐私保护, 异构性", "comments": "FedWSIDD的创新点在于将数据集蒸馏引入联邦学习框架，用于全玻片图像分类，并通过传输合成数据而非模型参数来有效解决隐私和异构性问题。客户端的染色归一化处理提高了合成数据的质量和实用性。这为医疗图像分析中的联邦学习提供了一个有前景的新方向。"}}
{"id": "2506.15118", "title": "CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records", "authors": ["Junke Wang", "Hongshun Ling", "Li Zhang", "Longqian Zhang", "Fang Wang", "Yuan Gao", "Zhi Li"], "summary": "Electronic Health Records (EHR)-based disease prediction models have\ndemonstrated significant clinical value in promoting precision medicine and\nenabling early intervention. However, existing large language models face two\nmajor challenges: insufficient representation of medical knowledge and low\nefficiency in clinical deployment. To address these challenges, this study\nproposes the CKD-EHR (Clinical Knowledge Distillation for EHR) framework, which\nachieves efficient and accurate disease risk prediction through knowledge\ndistillation techniques. Specifically, the large language model Qwen2.5-7B is\nfirst fine-tuned on medical knowledge-enhanced data to serve as the teacher\nmodel.It then generates interpretable soft labels through a multi-granularity\nattention distillation mechanism. Finally, the distilled knowledge is\ntransferred to a lightweight BERT student model. Experimental results show that\non the MIMIC-III dataset, CKD-EHR significantly outperforms the baseline\nmodel:diagnostic accuracy is increased by 9%, F1-score is improved by 27%, and\na 22.2 times inference speedup is achieved. This innovative solution not only\ngreatly improves resource utilization efficiency but also significantly\nenhances the accuracy and timeliness of diagnosis, providing a practical\ntechnical approach for resource optimization in clinical settings. The code and\ndata for this research are available athttps://github.com/209506702/CKD_EHR.", "comment": "20 pages,5 figures", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15118v1", "AI": {"title_translation": "CKD-EHR：面向电子健康记录的临床知识蒸馏", "tldr": "CKD-EHR框架通过知识蒸馏技术，显著提升了电子健康记录（EHR）疾病预测模型的效率和准确性。", "motivation": "现有基于电子健康记录（EHR）的大语言模型在医学知识表示不足和临床部署效率低下方面面临挑战。", "method": "本研究提出了CKD-EHR（Clinical Knowledge Distillation for EHR）框架。该框架首先在医学知识增强数据上微调大型语言模型Qwen2.5-7B作为教师模型，然后通过多粒度注意力蒸馏机制生成可解释的软标签，最后将蒸馏后的知识转移到轻量级BERT学生模型。", "result": "在MIMIC-III数据集上，CKD-EHR的诊断准确率提高了9%，F1分数提高了27%，并实现了22.2倍的推理速度提升，显著优于基线模型。", "conclusion": "CKD-EHR框架不仅大大提高了资源利用效率，而且显著增强了诊断的准确性和及时性，为临床环境中的资源优化提供了一种实用的技术方法。", "translation": "电子健康记录（EHR）疾病预测模型在促进精准医疗和实现早期干预方面展现出显著的临床价值。然而，现有的大语言模型面临两大挑战：医学知识表示不足和临床部署效率低下。为解决这些挑战，本研究提出了CKD-EHR（面向EHR的临床知识蒸馏）框架，该框架通过知识蒸馏技术实现了高效准确的疾病风险预测。具体而言，首先在医学知识增强数据上对大型语言模型Qwen2.5-7B进行微调，作为教师模型。然后，通过多粒度注意力蒸馏机制生成可解释的软标签。最后，将蒸馏后的知识转移到轻量级BERT学生模型。实验结果表明，在MIMIC-III数据集上，CKD-EHR显著优于基线模型：诊断准确率提高了9%，F1分数提高了27%，并实现了22.2倍的推理加速。这一创新解决方案不仅大大提高了资源利用效率，而且显著增强了诊断的准确性和及时性，为临床环境中的资源优化提供了一种实用的技术方法。本研究的代码和数据可在https://github.com/209506702/CKD_EHR获取。", "summary": "本文提出了CKD-EHR框架，旨在通过知识蒸馏技术解决基于电子健康记录（EHR）的大语言模型在医学知识表示和临床部署效率方面的不足。该框架将微调后的Qwen2.5-7B作为教师模型，通过多粒度注意力蒸馏将知识传递给轻量级BERT学生模型。实验证明，CKD-EHR在MIMIC-III数据集上显著提升了疾病预测的准确性（诊断准确率提高9%，F1分数提高27%）和推理速度（22.2倍），为临床诊断提供了高效实用的解决方案。", "keywords": "知识蒸馏, 电子健康记录, 疾病预测, 大语言模型, 临床应用", "comments": "CKD-EHR框架的创新点在于将大型语言模型（LLM）的医学知识通过知识蒸馏技术有效地迁移到轻量级模型中，解决了LLM在临床部署中的效率问题。这种方法不仅提升了预测准确性，还大幅提高了推理速度，对于资源受限的临床环境具有重要实际意义。"}}
{"id": "2506.15077", "title": "A Nonconforming Finite Element Method for Elliptic Interface Problems on Locally Anisotropic Meshes", "authors": ["Hua Wang", "Qichen Zhang"], "summary": "We propose a new nonconforming \\(P_1\\) finite element method for elliptic\ninterface problems. The method is constructed on a locally anisotropic mixed\nmesh, which is generated by fitting the interface through a simple connection\nof intersection points on an interface-unfitted background mesh, as introduced\nin \\cite{Hu2021optimal}. We first establish interpolation error estimates on\nquadrilateral elements satisfying the regular decomposition property (RDP).\nBuilding on this, the main contribution of this work is a novel consistency\nerror analysis for nonconforming elements, which removes the quasi-regularity\nassumption commonly required in existing approaches. Numerical results confirm\nthe theoretical convergence rates and demonstrate the robustness and accuracy\nof the proposed method.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15077v1", "AI": {"title_translation": "局部各向异性网格上椭圆界面问题的非协调有限元方法", "tldr": "提出了一种新的非协调P1有限元方法，用于解决局部各向异性网格上的椭圆界面问题，并通过新颖的误差分析消除了准正则性假设。", "motivation": "旨在开发一种针对椭圆界面问题的鲁棒且准确的非协调有限元方法，并克服现有方法中普遍存在的准正则性假设限制。", "method": "提出了一种新的非协调P1有限元方法，用于椭圆界面问题。该方法构建在局部各向异性混合网格上，通过在与界面不拟合的背景网格上简单连接交点来拟合界面。研究建立了满足正则分解性质的四边形元素的插值误差估计，并提出了一种新颖的非协调元素一致性误差分析，该分析消除了现有方法中通常需要的准正则性假设。", "result": "数值结果证实了理论收敛率，并证明了所提方法的鲁棒性和准确性。", "conclusion": "该非协调有限元方法在局部各向异性网格上对椭圆界面问题有效，且通过新颖的误差分析成功去除了准正则性假设，表现出良好的鲁棒性和准确性。", "translation": "我们提出了一种新的非协调\\(P_1\\)有限元方法，用于解决椭圆界面问题。该方法构建在局部各向异性混合网格上，该网格通过在与界面不拟合的背景网格上简单连接交点来拟合界面，如\\cite{Hu2021optimal}中所述。我们首先建立了满足正则分解性质的四边形元素的插值误差估计。在此基础上，这项工作的主要贡献是针对非协调元素的新颖一致性误差分析，该分析消除了现有方法中通常需要的准正则性假设。数值结果证实了理论收敛率，并证明了所提方法的鲁棒性和准确性。", "summary": "本文提出了一种针对椭圆界面问题的新型非协调P1有限元方法。该方法在局部各向异性混合网格上构建，并通过在不拟合界面的背景网格上连接交点来生成网格。研究建立了四边形元素的插值误差估计，并提供了一种新颖的非协调元素一致性误差分析，成功移除了现有方法中常见的准正则性假设。数值实验验证了理论收敛率，并展示了该方法的鲁棒性和准确性。", "keywords": "非协调有限元方法, 椭圆界面问题, 局部各向异性网格, 误差分析, 准正则性假设", "comments": "该论文的创新之处在于提出了一种新的非协调有限元方法，并特别在误差分析中移除了现有方法普遍要求的准正则性假设，这可能简化了理论分析或拓宽了方法的适用范围。方法的鲁棒性和准确性也通过数值结果得到验证。"}}
{"id": "2506.15220", "title": "video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models", "authors": ["Changli Tang", "Yixuan Li", "Yudong Yang", "Jimin Zhuang", "Guangzhi Sun", "Wei Li", "Zejun Ma", "Chao Zhang"], "summary": "Videos contain a wealth of information, and generating detailed and accurate\ndescriptions in natural language is a key aspect of video understanding. In\nthis paper, we present video-SALMONN 2, an advanced audio-visual large language\nmodel (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with\npaired audio) captioning through directed preference optimisation (DPO). We\npropose new metrics to evaluate the completeness and accuracy of video\ndescriptions, which are optimised using DPO. To further improve training, we\npropose a novel multi-round DPO (MrDPO) approach, which involves periodically\nupdating the DPO reference model, merging and re-initialising the LoRA module\nas a proxy for parameter updates after each training round (1,000 steps), and\nincorporating guidance from ground-truth video captions to stabilise the\nprocess. Experimental results show that MrDPO significantly enhances\nvideo-SALMONN 2's captioning accuracy, reducing the captioning error rates by\n28\\%. The final video-SALMONN 2 model, with just 7 billion parameters,\nsurpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning\ntasks, while maintaining highly competitive performance to the state-of-the-art\non widely used video question-answering benchmarks among models of similar\nsize. Codes are available at\n\\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15220v1", "AI": {"title_translation": "video-SALMONN 2：字幕增强的音视频大型语言模型", "tldr": "video-SALMONN 2是一个利用多轮DPO和LoRA技术，在视频字幕生成方面超越GPT-4o和Gemini-1.5-Pro的音视频大语言模型，并显著降低了错误率。", "motivation": "视频包含大量信息，生成详细准确的自然语言描述是视频理解的关键。", "method": "提出video-SALMONN 2，一个结合低秩适应（LoRA）和定向偏好优化（DPO）的先进音视频大型语言模型，用于增强视频（带配对音频）字幕生成。提出新的指标来评估视频描述的完整性和准确性，并通过DPO进行优化。引入一种新颖的多轮DPO（MrDPO）方法，该方法涉及周期性更新DPO参考模型，合并并重新初始化LoRA模块作为参数更新的代理，并结合真实视频字幕的指导来稳定训练过程。", "result": "MrDPO显著提高了video-SALMONN 2的字幕生成准确性，将字幕错误率降低了28%。最终的video-SALMONN 2模型（仅70亿参数）在视频字幕任务中超越了GPT-4o和Gemini-1.5-Pro等领先模型，并在广泛使用的视频问答基准上保持了与同等规模SOTA模型的高度竞争力。", "conclusion": "Not mentioned in abstract", "translation": "视频包含丰富的信息，生成详细准确的自然语言描述是视频理解的一个关键方面。在本文中，我们提出了video-SALMONN 2，一个先进的音视频大型语言模型（LLM），它采用低秩适应（LoRA）并通过定向偏好优化（DPO）旨在增强视频（带有配对音频）的字幕生成。我们提出了新的指标来评估视频描述的完整性和准确性，并使用DPO进行优化。为了进一步改进训练，我们提出了一种新颖的多轮DPO（MrDPO）方法，该方法包括周期性更新DPO参考模型，在每个训练轮次（1,000步）后合并并重新初始化LoRA模块作为参数更新的代理，并结合真实视频字幕的指导以稳定过程。实验结果表明，MrDPO显著提高了video-SALMONN 2的字幕生成准确性，将字幕错误率降低了28%。最终的video-SALMONN 2模型，仅有70亿参数，在视频字幕任务中超越了GPT-4o和Gemini-1.5-Pro等领先模型，同时在广泛使用的视频问答基准上与同等规模的最新模型保持高度竞争力。代码可在https://github.com/bytedance/video-SALMONN-2 获取。", "summary": "本文提出了video-SALMONN 2，一个基于低秩适应（LoRA）和定向偏好优化（DPO）的音视频大型语言模型，旨在提升视频字幕生成能力。研究引入了新的评估指标和多轮DPO（MrDPO）训练方法，该方法通过周期性更新参考模型和LoRA模块以及结合真实字幕指导来稳定训练。实验证明，MrDPO将字幕错误率降低了28%，且70亿参数的video-SALMONN 2在视频字幕任务上优于GPT-4o和Gemini-1.5-Pro，并在视频问答任务上保持了竞争力。", "keywords": "视频字幕生成, 音视频大型语言模型, 低秩适应, 定向偏好优化, 多轮DPO", "comments": "该论文的创新点在于提出了新颖的多轮DPO (MrDPO) 训练方法，并将其应用于音视频大语言模型，显著提升了视频字幕生成的准确性。其重要性体现在以相对较小的模型规模（70亿参数）超越了现有顶级模型，如GPT-4o和Gemini-1.5-Pro，这对于资源受限的应用场景具有重要意义。"}}
{"id": "2506.15170", "title": "From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in Jailbreak Attacks and Defenses within LLM Ecosystem", "authors": ["Yanxu Mao", "Tiehan Cui", "Peipei Liu", "Datao You", "Hongsong Zhu"], "summary": "Large language models (LLMs) are rapidly evolving from single-modal systems\nto multimodal LLMs and intelligent agents, significantly expanding their\ncapabilities while introducing increasingly severe security risks. This paper\npresents a systematic survey of the growing complexity of jailbreak attacks and\ncorresponding defense mechanisms within the expanding LLM ecosystem. We first\ntrace the developmental trajectory from LLMs to MLLMs and Agents, highlighting\nthe core security challenges emerging at each stage. Next, we categorize\nmainstream jailbreak techniques from both the attack impact and visibility\nperspectives, and provide a comprehensive analysis of representative attack\nmethods, related datasets, and evaluation metrics. On the defense side, we\norganize existing strategies based on response timing and technical approach,\noffering a structured understanding of their applicability and implementation.\nFurthermore, we identify key limitations in existing surveys, such as\ninsufficient attention to agent-specific security issues, the absence of a\nclear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of\nexperimental setups, and outdated coverage of recent advancements. To address\nthese limitations, we provide an updated synthesis of recent work and outline\nfuture research directions in areas such as dataset construction, evaluation\nframework optimization, and strategy generalization. Our study seeks to enhance\nthe understanding of jailbreak mechanisms and facilitate the advancement of\nmore resilient and adaptive defense strategies in the context of ever more\ncapable LLMs.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15170v1", "AI": {"title_translation": "从LLMs到MLLMs再到Agents：LLM生态系统中越狱攻击与防御新兴范式综述", "tldr": "本综述系统性地分析了LLMs、MLLMs和Agents中日益复杂的越狱攻击及其防御机制，指出了现有综述的局限性并提出了未来的研究方向。", "motivation": "大型语言模型（LLMs）正从单模态系统演变为多模态LLMs和智能Agents，这在显著扩展其能力的同时，也带来了日益严重的安全风险。现有综述在Agent特有的安全问题、混合越狱方法的分类、实验设置的详细分析以及最新进展的覆盖方面存在不足，因此本研究旨在弥补这些空白。", "method": "本文首先追溯了从LLMs到MLLMs再到Agents的发展轨迹，并强调了每个阶段出现的核心安全挑战。接着，从攻击影响和可见性角度对主流越狱技术进行分类，并全面分析了代表性攻击方法、相关数据集和评估指标。在防御方面，根据响应时间和技术方法组织现有策略。此外，本文还提供了最新工作的综合更新，并概述了未来研究方向。", "result": "本研究识别了现有综述中的主要局限性，例如对Agent特定安全问题的关注不足、缺乏清晰的混合越狱方法分类、缺少对实验设置的详细分析以及对最新进展的覆盖不足。为此，本文提供了近期工作的更新综合，并提出了数据集构建、评估框架优化和策略泛化等领域的未来研究方向。", "conclusion": "本研究旨在增强对越狱机制的理解，并促进在功能日益强大的LLMs背景下，开发更具弹性和适应性的防御策略。", "translation": "大型语言模型（LLMs）正在从单模态系统迅速发展为多模态LLMs和智能Agents，这显著扩展了它们的能力，同时也带来了日益严重的安全风险。本文系统地综述了在不断扩展的LLM生态系统中，日益复杂的越狱攻击和相应的防御机制。我们首先追溯了从LLMs到MLLMs再到Agents的发展轨迹，强调了每个阶段出现的核心安全挑战。接下来，我们从攻击影响和可见性角度对主流越狱技术进行分类，并全面分析了代表性攻击方法、相关数据集和评估指标。在防御方面，我们根据响应时间和技术方法组织现有策略，提供了对其适用性和实施的结构化理解。此外，我们指出了现有综述中的主要局限性，例如对Agent特定安全问题的关注不足、缺乏清晰的混合越狱方法分类、缺少对实验设置的详细分析以及对最新进展的覆盖不足。为了解决这些局限性，我们提供了最新工作的更新综合，并概述了数据集构建、评估框架优化和策略泛化等领域的未来研究方向。我们的研究旨在增强对越狱机制的理解，并促进在功能日益强大的LLMs背景下，开发更具弹性和适应性的防御策略。", "summary": "本论文对大型语言模型（LLMs）生态系统中不断演进的越狱攻击与防御范式进行了系统性综述。文章首先梳理了LLMs、多模态LLMs（MLLMs）和智能Agents的发展路径及各阶段的安全挑战。随后，从攻击影响和可见性角度对主流越狱技术进行了分类，并详细分析了攻击方法、数据集和评估指标。在防御方面，论文依据响应时间和技术方法组织了现有策略。此外，该综述还识别了现有研究的不足之处，如对Agent特定安全问题的忽视、混合越狱分类的缺失以及实验分析的不足，并提出了未来的研究方向，旨在加深对越狱机制的理解，推动更具韧性的防御策略发展。", "keywords": "LLM安全, 越狱攻击, 多模态LLM, 智能Agent, 防御机制", "comments": "该综述的创新之处在于其对LLM生态系统演进过程中越狱攻击和防御的全面覆盖，特别强调了从LLMs到MLLMs再到Agents的演变所带来的新安全挑战。它不仅系统地分类和分析了现有技术，更重要的是，明确指出了现有综述的局限性并提供了最新的研究综合和未来方向，这对于推动该领域的理论理解和实践发展具有重要意义。"}}
{"id": "2506.15293", "title": "Designing Intent: A Multimodal Framework for Human-Robot Cooperation in Industrial Workspaces", "authors": ["Francesco Chiossi", "Julian Rasch", "Robin Welsch", "Albrecht Schmidt", "Florian Michahelles"], "summary": "As robots enter collaborative workspaces, ensuring mutual understanding\nbetween human workers and robotic systems becomes a prerequisite for trust,\nsafety, and efficiency. In this position paper, we draw on the cooperation\nscenario of the AIMotive project in which a human and a cobot jointly perform\nassembly tasks to argue for a structured approach to intent communication.\nBuilding on the Situation Awareness-based Agent Transparency (SAT) framework\nand the notion of task abstraction levels, we propose a multidimensional design\nspace that maps intent content (SAT1, SAT3), planning horizon (operational to\nstrategic), and modality (visual, auditory, haptic). We illustrate how this\nspace can guide the design of multimodal communication strategies tailored to\ndynamic collaborative work contexts. With this paper, we lay the conceptual\nfoundation for a future design toolkit aimed at supporting transparent\nhuman-robot interaction in the workplace. We highlight key open questions and\ndesign challenges, and propose a shared agenda for multimodal, adaptive, and\ntrustworthy robotic collaboration in hybrid work environments.", "comment": "9 pages", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15293v1", "AI": {"title_translation": "意图设计：工业工作空间中人机协作的多模态框架", "tldr": "本文提出了一个多维设计空间，用于指导工业环境中人机协作中机器人意图的多模态通信策略，以增强信任、安全和效率。", "motivation": "随着机器人进入协作工作空间，确保人机系统之间的相互理解是建立信任、保障安全和提高效率的先决条件。", "method": "本文以AIMotive项目中的人机协作装配任务场景为例，提出了一种结构化的意图通信方法。该方法基于情境感知代理透明度（SAT）框架和任务抽象级别，提出了一个多维设计空间，映射了意图内容（SAT1、SAT3）、规划范围（操作到战略）和模态（视觉、听觉、触觉）。", "result": "本文展示了所提出的设计空间如何指导为动态协作工作环境量身定制的多模态通信策略的设计。它为未来旨在支持工作场所透明人机交互的设计工具包奠定了概念基础。", "conclusion": "本文为未来旨在支持工作场所透明人机交互的设计工具包奠定了概念基础。它强调了关键的开放问题和设计挑战，并提出了在混合工作环境中实现多模态、自适应和值得信赖的机器人协作的共同议程。", "translation": "随着机器人进入协作工作空间，确保人机系统之间的相互理解是建立信任、保障安全和提高效率的先决条件。在这篇立场论文中，我们借鉴了AIMotive项目的协作场景，其中人类和协作机器人共同执行装配任务，旨在提出一种结构化的意图通信方法。基于情境感知代理透明度（SAT）框架和任务抽象级别的概念，我们提出了一个多维设计空间，映射了意图内容（SAT1、SAT3）、规划范围（操作到战略）和模态（视觉、听觉、触觉）。我们阐述了该空间如何指导为动态协作工作环境量身定制的多模态通信策略的设计。通过本文，我们为未来旨在支持工作场所透明人机交互的设计工具包奠定了概念基础。我们强调了关键的开放问题和设计挑战，并提出了在混合工作环境中实现多模态、自适应和值得信赖的机器人协作的共同议程。", "summary": "本文提出了一种多模态框架用于工业工作空间中的人机协作，旨在通过结构化的意图通信来增强信任、安全和效率。该框架基于SAT和任务抽象级别，构建了一个多维设计空间，用于指导视觉、听觉和触觉等多种模态的通信策略，并为未来透明人机交互的设计工具包奠定了概念基础，同时指出了未来的研究方向和挑战。", "keywords": "人机协作, 意图通信, 多模态, 设计空间, 机器人透明度", "comments": "本文作为一篇立场论文，提出了一个新颖的多维设计空间来规范人机协作中的意图通信，这对于提高透明度和信任至关重要。其创新点在于结合了SAT框架和任务抽象级别来系统化地考虑意图内容、规划范围和模态。虽然是概念性工作，但为未来人机交互工具包的开发提供了坚实的理论基础和明确的研究议程。"}}
{"id": "2506.14842", "title": "PictSure: Pretraining Embeddings Matters for In-Context Learning Image Classifiers", "authors": ["Lukas Schiesser", "Cornelius Wolff", "Sophie Haas", "Simon Pukrop"], "summary": "Building image classification models remains cumbersome in data-scarce\ndomains, where collecting large labeled datasets is impractical. In-context\nlearning (ICL) has emerged as a promising paradigm for few-shot image\nclassification (FSIC), enabling models to generalize across domains without\ngradient-based adaptation. However, prior work has largely overlooked a\ncritical component of ICL-based FSIC pipelines: the role of image embeddings.\nIn this work, we present PictSure, an ICL framework that places the embedding\nmodel -- its architecture, pretraining, and training dynamics -- at the center\nof analysis. We systematically examine the effects of different visual encoder\ntypes, pretraining objectives, and fine-tuning strategies on downstream FSIC\nperformance. Our experiments show that the training success and the\nout-of-domain performance are highly dependent on how the embedding models are\npretrained. Consequently, PictSure manages to outperform existing ICL-based\nFSIC models on out-of-domain benchmarks that differ significantly from the\ntraining distribution, while maintaining comparable results on in-domain tasks.\nCode can be found at https://github.com/PictSure/pictsure-library.", "comment": "15 pages, 10 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14842v1", "AI": {"title_translation": "PictSure：预训练嵌入对上下文学习图像分类器的重要性", "tldr": "在上下文学习图像分类中，图像嵌入的预训练至关重要，本文提出的PictSure框架通过优化嵌入模型显著提升了域外性能。", "motivation": "在数据稀缺领域构建图像分类模型十分困难。上下文学习（ICL）是少样本图像分类（FSIC）的一个有前景的范式，但现有工作忽略了图像嵌入在ICL-based FSIC管道中的关键作用。", "method": "本文提出了PictSure，一个将嵌入模型（包括其架构、预训练和训练动态）置于分析核心的ICL框架。系统地研究了不同视觉编码器类型、预训练目标和微调策略对FSIC下游性能的影响。", "result": "实验表明，训练成功和域外性能高度依赖于嵌入模型的预训练方式。PictSure在与训练分布显著不同的域外基准测试中优于现有ICL-based FSIC模型，同时在域内任务上保持了可比结果。", "conclusion": "预训练嵌入模型对于上下文学习图像分类器至关重要，尤其是在提升模型对训练分布之外的数据的泛化能力方面。PictSure框架通过关注和优化嵌入模型的预训练，成功地在域外任务上取得了卓越性能。", "translation": "在数据稀缺领域，构建图像分类模型仍然很麻烦，因为收集大量带标签的数据集不切实际。上下文学习（ICL）已成为少样本图像分类（FSIC）的一个有前景的范式，使模型无需基于梯度的适应即可跨域泛化。然而，先前的工作在很大程度上忽视了基于ICL的FSIC管道中的一个关键组成部分：图像嵌入的作用。在这项工作中，我们提出了PictSure，一个ICL框架，它将嵌入模型——其架构、预训练和训练动态——置于分析中心。我们系统地检查了不同视觉编码器类型、预训练目标和微调策略对下游FSIC性能的影响。我们的实验表明，训练成功和域外性能高度依赖于嵌入模型的预训练方式。因此，PictSure在与训练分布显著不同的域外基准测试中表现优于现有基于ICL的FSIC模型，同时在域内任务上保持了可比结果。代码可在https://github.com/PictSure/pictsure-library找到。", "summary": "本文提出了PictSure，一个专注于图像嵌入模型（包括其架构、预训练和训练动态）的上下文学习（ICL）框架，旨在解决数据稀缺领域图像分类的挑战。研究发现，嵌入模型的预训练方式对模型训练成功和域外泛化性能至关重要。实验结果表明，PictSure在与训练数据分布差异较大的域外基准测试中显著优于现有ICL-based少样本图像分类（FSIC）模型，同时在域内任务上保持了竞争力。", "keywords": "图像分类, 上下文学习, 少样本学习, 嵌入预训练, 域外泛化", "comments": "该论文的创新点在于强调了图像嵌入模型在上下文学习（ICL）少样本图像分类（FSIC）中的关键作用，并系统地分析了预训练策略的影响。它揭示了预训练方式对模型泛化能力，尤其是在域外任务上的重要性，为未来ICL框架的设计提供了宝贵的见解，具有重要的实践指导意义。"}}
{"id": "2506.15595", "title": "LiteGD: Lightweight and dynamic GPU Dispatching for Large-scale Heterogeneous Clusters", "authors": ["Kunming Zhang", "Hanlong Liao", "Guoming Tang"], "summary": "Parallel computing with multiple GPUs has become the dominant paradigm for\nmachine learning tasks, especially those of large language models (LLMs). To\nreduce the latency incurred by inter-GPU communication, a common practice for\nparallel tasks has been to allocate GPUs based on their physical proximity.\nHowever, this long-standing assumption has notable limitations, particularly in\nlarge-scale, heterogeneous GPU clusters where bandwidth distribution among GPUs\nis irregular. In this paper, we introduce LiteGD, a lightweight and dynamic GPU\ndispatching system based on global perspectives. To tackle the difficulty of\nstoring massive GPU topology information, LiteGD adopts a computation-aware\ndesign that leverages a lightweight Transformer network trained on sampled\ndata. Our customized design for network structure ensures both transferability\nand scalability. LiteGD also employs a bidirectional tree search approach to\nfind the optimal GPU dispatching in the data generated in the previous step,\nwhich can identify near-optimal solutions while reducing search overhead. We\nimplement and evaluate LiteGD in both real and simulated GPU clusters with\nhomogeneous and heterogeneous interconnects, respectively. Experimental results\ndemonstrate that LiteGD consistently achieves high GPU bandwidth efficacy\n(approximately 90\\%) across various cluster configurations and 80\\% in\nreal-world H100 cluster, significantly outperforming conventional default and\ninterconnect topology-aware dispatching methods, particularly in large-scale\nheterogeneous environments.", "comment": "12 pages, 19 figures,7 tables", "cate": "cs.DC", "url": "http://arxiv.org/abs/2506.15595v1", "AI": {"title_translation": "LiteGD：大规模异构集群的轻量级动态GPU调度", "tldr": "LiteGD是一种轻量级动态GPU调度系统，通过计算感知设计和双向树搜索，解决了大规模异构集群中GPU通信带宽不规则的问题，显著提高了GPU带宽效率。", "motivation": "现有的多GPU并行计算任务的GPU分配方法通常基于物理邻近性，但这在大规模、异构GPU集群中存在显著局限性，因为GPU之间的带宽分布不规则，导致通信延迟增加。", "method": "LiteGD是一个基于全局视角的轻量级动态GPU调度系统。它采用计算感知设计，利用在采样数据上训练的轻量级Transformer网络来处理和存储大规模GPU拓扑信息，确保可迁移性和可扩展性。此外，它还采用双向树搜索方法，在生成的数据中找到最优的GPU调度方案，从而在降低搜索开销的同时识别接近最优的解决方案。", "result": "LiteGD在各种集群配置中始终实现高GPU带宽效率（约90%），在真实H100集群中达到80%。实验结果表明，LiteGD显著优于传统的默认和互连拓扑感知调度方法，特别是在大规模异构环境中。", "conclusion": "LiteGD通过其创新的调度方法，有效解决了大规模异构GPU集群中的通信效率问题，显著提升了GPU带宽利用率，为并行计算任务提供了更优的解决方案。", "translation": "多GPU并行计算已成为机器学习任务（特别是大型语言模型（LLMs））的主导范式。为了减少GPU间通信引起的延迟，并行任务的常见做法是根据GPU的物理邻近性进行分配。然而，这种长期存在的假设存在显著局限性，特别是在GPU之间带宽分布不规则的大规模异构GPU集群中。在本文中，我们引入了LiteGD，一个基于全局视角的轻量级动态GPU调度系统。为了解决存储海量GPU拓扑信息的难题，LiteGD采用了一种计算感知设计，利用在采样数据上训练的轻量级Transformer网络。我们定制的网络结构设计确保了可迁移性和可扩展性。LiteGD还采用双向树搜索方法，在前面步骤生成的数据中找到最优的GPU调度，这可以在减少搜索开销的同时识别接近最优的解决方案。我们在真实和模拟的GPU集群中分别对LiteGD进行了实现和评估，这些集群具有同构和异构互连。实验结果表明，LiteGD在各种集群配置中始终实现高GPU带宽效率（约90%），并在真实H100集群中达到80%，显著优于传统的默认和互连拓扑感知调度方法，特别是在大规模异构环境中。", "summary": "LiteGD是一个针对大规模异构GPU集群设计的轻量级动态GPU调度系统，旨在解决传统基于物理邻近性分配方法在带宽分布不规则环境下的局限性。它创新性地结合了计算感知设计（利用轻量级Transformer网络处理拓扑信息）和双向树搜索（寻找最优调度），从而在减少搜索开销的同时实现高GPU带宽效率，并显著优于现有调度方法。", "keywords": "GPU调度, 异构集群, Transformer, 带宽效率, 并行计算", "comments": "LiteGD的创新之处在于其结合了机器学习（轻量级Transformer）来处理复杂的GPU拓扑信息，并通过双向树搜索优化调度，这在大规模异构环境中具有重要意义。它解决了传统方法在复杂集群中效率低下的问题，对于提高LLM等并行计算任务的性能至关重要。其在真实H100集群中取得的80%带宽效率是一个令人印象深刻的成果。"}}
{"id": "2506.15683", "title": "PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via Family-Aware Learning", "authors": ["Yuhui Shi", "Yehan Yang", "Qiang Sheng", "Hao Mi", "Beizhe Hu", "Chaoxi Xu", "Juan Cao"], "summary": "With the popularity of large language models (LLMs), undesirable societal\nproblems like misinformation production and academic misconduct have been more\nsevere, making LLM-generated text detection now of unprecedented importance.\nAlthough existing methods have made remarkable progress, a new challenge posed\nby text from privately tuned LLMs remains underexplored. Users could easily\npossess private LLMs by fine-tuning an open-source one with private corpora,\nresulting in a significant performance drop of existing detectors in practice.\nTo address this issue, we propose PhantomHunter, an LLM-generated text detector\nspecialized for detecting text from unseen, privately-tuned LLMs. Its\nfamily-aware learning framework captures family-level traits shared across the\nbase models and their derivatives, instead of memorizing individual\ncharacteristics. Experiments on data from LLaMA, Gemma, and Mistral families\nshow its superiority over 7 baselines and 3 industrial services, with F1 scores\nof over 96%.", "comment": "17 pages, 3 figures, 6 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15683v1", "AI": {"title_translation": "PhantomHunter：通过家族感知学习检测未见的私有微调LLM生成文本", "tldr": "PhantomHunter是一种新型检测器，专门用于检测来自未见的私有微调LLM生成的文本，通过捕获模型家族级特征，其F1分数超过96%，优于现有方法。", "motivation": "随着大型语言模型（LLMs）的普及，误信息传播和学术不端等不良社会问题日益严重，使得LLM生成文本检测变得前所未有的重要。现有方法在检测方面已取得显著进展，但对来自私有微调LLM的文本检测仍未得到充分探索。用户可以轻松地通过私有语料库微调开源LLM，导致现有检测器在实践中性能显著下降。", "method": "我们提出了PhantomHunter，这是一种专门用于检测来自未见的私有微调LLM生成文本的检测器。其家族感知学习框架能够捕获基础模型及其衍生模型之间共享的家族级特征，而不是记忆个体特征。", "result": "在LLaMA、Gemma和Mistral家族数据上的实验表明，PhantomHunter优于7个基线方法和3个工业服务，F1分数超过96%。", "conclusion": "PhantomHunter通过家族感知学习，能够有效检测来自未见私有微调LLM生成的文本，显著提高了LLM生成文本检测的实用性和性能。", "translation": "随着大型语言模型（LLMs）的普及，误信息生产和学术不端等不良社会问题日益严重，使得LLM生成文本检测现在变得前所未有的重要。尽管现有方法已取得显著进展，但来自私有微调LLM的文本所带来的新挑战仍未得到充分探索。用户可以通过使用私有语料库微调开源模型，轻松拥有私有LLM，这导致现有检测器在实践中性能显著下降。为了解决这个问题，我们提出了PhantomHunter，这是一种专门用于检测来自未见的私有微调LLM生成文本的检测器。其家族感知学习框架捕获了基础模型及其衍生模型之间共享的家族级特征，而不是记忆个体特征。在LLaMA、Gemma和Mistral家族数据上的实验表明，它优于7个基线方法和3个工业服务，F1分数超过96%。", "summary": "本文提出了PhantomHunter，一个专门用于检测来自未见的私有微调大型语言模型（LLM）生成文本的检测器。针对现有检测器在私有微调LLM文本上性能下降的问题，PhantomHunter采用家族感知学习框架，通过识别基础模型及其衍生模型共享的家族级特征来提高检测能力。实验结果显示，在LLaMA、Gemma和Mistral家族数据上，PhantomHunter的F1分数超过96%，显著优于现有基线和工业服务，证明了其在实际应用中的优越性。", "keywords": "LLM生成文本检测, 私有微调LLM, 家族感知学习, PhantomHunter, 文本检测", "comments": "PhantomHunter的创新点在于其“家族感知学习”框架，它解决了现有LLM文本检测器无法有效处理私有微调模型生成文本的痛点。通过关注模型家族的共性而非个体特性，该方法提高了泛化能力，对于应对日益增长的私有LLM滥用问题具有重要意义。其高性能（F1>96%）也显示了其实用价值。"}}
{"id": "2506.15465", "title": "DATA-DRIVEN PRONTO: a Model-free Solution for Numerical Optimal Control", "authors": ["Marco Borghesi", "Lorenzo Sforni", "Giuseppe Notarstefano"], "summary": "This article addresses the problem of data-driven numerical optimal control\nfor unknown nonlinear systems. In our scenario, we suppose to have the\npossibility of performing multiple experiments (or simulations) on the system.\nExperiments are performed by relying on a data-driven tracking controller able\nto steer the system towards a desired reference. Our proposed DATA-DRIVEN\nPRONTO algorithm iteratively refines a tentative solution of the optimal\ncontrol problem by computing an approximate descent direction via a local\ntrajectory perturbation. At each iteration, multiple trajectories are gathered\nby perturbing the current trajectory with a suitable dither signal, and then\nused to obtain a data-driven, time-varying linearization. The exploration is\nguided by the tracking controller, so that perturbed trajectories are obtained\nin closed loop. We show local convergence of DATA-DRIVEN PRONTO to a ball about\nan isolated optimal solution, whose radius depends on the amplitude of the\ndither signal. We corroborate the theoretical results by applying it to an\nunderactuated robot.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15465v1", "AI": {"title_translation": "数据驱动的PRONTO：一种用于数值最优控制的无模型解决方案", "tldr": "提出了一种名为DATA-DRIVEN PRONTO的无模型算法，通过迭代地扰动轨迹并进行数据驱动的线性化，解决未知非线性系统的数值最优控制问题，并证明了其局部收敛性。", "motivation": "解决未知非线性系统的数据驱动数值最优控制问题，尤其是在可以进行多次实验或仿真的场景下。", "method": "提出DATA-DRIVEN PRONTO算法。该算法通过局部轨迹扰动计算近似下降方向，迭代地优化最优控制问题的解。在每次迭代中，通过对当前轨迹施加合适的抖动信号来收集多条轨迹，这些轨迹用于获得数据驱动的时变线性化模型。扰动轨迹在闭环中通过跟踪控制器引导。", "result": "DATA-DRIVEN PRONTO算法显示出对孤立最优解附近一个球的局部收敛性。收敛球的半径取决于抖动信号的幅度。理论结果通过应用于欠驱动机器人得到证实。", "conclusion": "DATA-DRIVEN PRONTO提供了一种有效的数据驱动、无模型的数值最优控制方法，适用于未知非线性系统，并具有可证明的局部收敛性。", "translation": "本文探讨了未知非线性系统的数据驱动数值最优控制问题。在我们的设想中，我们假设能够对系统进行多次实验（或仿真）。实验通过依赖于数据驱动的跟踪控制器来执行，该控制器能够引导系统朝向期望的参考。我们提出的DATA-DRIVEN PRONTO算法通过局部轨迹扰动计算近似下降方向，迭代地改进最优控制问题的试探性解决方案。在每次迭代中，通过使用合适的抖动信号扰动当前轨迹来收集多条轨迹，然后用于获得数据驱动的时变线性化。探索由跟踪控制器引导，因此扰动轨迹是在闭环中获得的。我们展示了DATA-DRIVEN PRONTO对孤立最优解附近一个球的局部收敛性，该球的半径取决于抖动信号的幅度。我们通过将其应用于欠驱动机器人来证实了理论结果。", "summary": "本文提出了一种名为DATA-DRIVEN PRONTO的无模型算法，用于解决未知非线性系统的数据驱动数值最优控制问题。该算法通过迭代地扰动当前轨迹并利用所收集的数据进行时变线性化，从而计算近似下降方向以优化最优控制解。实验在闭环跟踪控制器的引导下进行。研究证明了该算法对最优解的局部收敛性，并通过欠驱动机器人应用验证了其有效性。", "keywords": "数据驱动, 无模型控制, 数值最优控制, 非线性系统, PRONTO算法", "comments": "该论文的创新点在于提出了DATA-DRIVEN PRONTO算法，它是一种无模型的数据驱动方法，能够在没有系统精确动力学模型的情况下解决复杂的数值最优控制问题。通过迭代扰动和数据驱动的线性化，该方法提供了一种实用的解决方案。其重要性在于为实际工程应用中系统模型难以获取或不精确的情况提供了一条新途径。局限性可能在于其局部收敛性以及收敛半径对抖动信号幅度的依赖，这可能需要仔细的参数调优。"}}
{"id": "2506.15670", "title": "Near-Field SWIPT with gMIMO in the Upper Mid-Band: Opportunities, Challenges, and the Way Forward", "authors": ["Özlem Tugfe Demir", "Mustafa Ozger", "Ferdi Kara", "Woong-Hee Lee", "Emil Björnson"], "summary": "This paper explores the integration of simultaneous wireless information and\npower transfer (SWIPT) with gigantic multiple-input multiple-output (gMIMO)\ntechnology operating in the upper mid-band frequency range (7-24 GHz). The\nnear-field propagation achieved by gMIMO introduces unique opportunities for\nenergy-efficient, high-capacity communication systems that cater to the demands\nof 6G wireless networks. Exploiting spherical wave propagation, near-field\nSWIPT with gMIMO enables precise energy and data delivery, enhancing spectral\nefficiency through beamfocusing and massive spatial multiplexing. This paper\ndiscusses theoretical principles, design challenges, and enabling solutions,\nincluding advanced channel estimation techniques, precoding strategies, and\ndynamic array configurations such as sparse and modular arrays. Through\nanalytical insights and a case study, this paper demonstrates the feasibility\nof achieving optimized energy harvesting and data throughput in dense and\ndynamic environments. These findings contribute to advancing energy-autonomous\nInternet-of-Everything (IoE) deployments, smart factory networks, and other\nenergy-autonomous applications aligned with the goals of next-generation\nwireless technologies.", "comment": "7 pages, 5 figures", "cate": "eess.SP", "url": "http://arxiv.org/abs/2506.15670v1", "AI": {"title_translation": "上中频段下近场SWIPT与gMIMO的结合：机遇、挑战与未来方向", "tldr": "本文探讨了上中频段近场SWIPT与gMIMO技术的结合，为6G网络提供了节能、高容量的通信系统机会，并通过分析和案例研究证明了其在复杂环境中实现优化能量收集和数据吞吐量的可行性。", "motivation": "为了满足6G无线网络对节能、高容量通信系统的需求，本文旨在探索将同步无线信息与功率传输（SWIPT）与巨型多输入多输出（gMIMO）技术在上中频段集成。", "method": "本文通过探讨理论原理、设计挑战和实现解决方案来达到目的，包括先进的信道估计技术、预编码策略以及稀疏和模块化阵列等动态阵列配置。此外，还通过分析洞察和案例研究来证明其可行性。", "result": "本文通过分析洞察和案例研究，证明了在密集和动态环境中实现优化能量收集和数据吞吐量的可行性。", "conclusion": "本文的研究结果有助于推动能量自主的物联网（IoE）部署、智能工厂网络以及其他符合下一代无线技术目标的能量自主应用。", "translation": "本文探讨了同步无线信息与功率传输（SWIPT）与巨型多输入多输出（gMIMO）技术在上中频段（7-24 GHz）的集成。gMIMO实现的近场传播为满足6G无线网络需求的节能、高容量通信系统带来了独特机遇。利用球形波传播，近场SWIPT与gMIMO通过波束聚焦和大规模空间复用提高了频谱效率，实现了精确的能量和数据传输。本文讨论了理论原理、设计挑战和实现解决方案，包括先进的信道估计技术、预编码策略以及稀疏和模块化阵列等动态阵列配置。通过分析洞察和案例研究，本文证明了在密集和动态环境中实现优化能量收集和数据吞吐量的可行性。这些发现有助于推动能量自主的物联网（IoE）部署、智能工厂网络以及其他符合下一代无线技术目标的能量自主应用。", "summary": "本文探讨了将近场同步无线信息与功率传输（SWIPT）与巨型多输入多输出（gMIMO）技术在上中频段（7-24 GHz）集成的潜力。文章强调了gMIMO的近场传播如何为6G网络提供节能、高容量的通信系统。文中详细讨论了理论原理、设计挑战以及先进信道估计、预编码和动态阵列配置等实现解决方案。通过分析和案例研究，论文证明了在复杂环境中优化能量收集和数据吞吐量的可行性，为能量自主的物联网和智能工厂等应用做出了贡献。", "keywords": "SWIPT, gMIMO, 近场通信, 上中频段, 6G", "comments": "本文创新性地探索了近场SWIPT与gMIMO的结合，解决了未来6G网络的关键挑战。文章对球形波传播的利用以及动态阵列配置等具体实现方案的关注，对于实际部署具有重要意义。"}}
{"id": "2506.14797", "title": "Bound by semanticity: universal laws governing the generalization-identification tradeoff", "authors": ["Marco Nurisso", "Jesseba Fernando", "Raj Deshpande", "Alan Perotti", "Raja Marjieh", "Steven M. Frankland", "Richard L. Lewis", "Taylor W. Webb", "Declan Campbell", "Francesco Vaccarino", "Jonathan D. Cohen", "Giovanni Petri"], "summary": "Intelligent systems must deploy internal representations that are\nsimultaneously structured -- to support broad generalization -- and selective\n-- to preserve input identity. We expose a fundamental limit on this tradeoff.\nFor any model whose representational similarity between inputs decays with\nfinite semantic resolution $\\varepsilon$, we derive closed-form expressions\nthat pin its probability of correct generalization $p_S$ and identification\n$p_I$ to a universal Pareto front independent of input space geometry.\nExtending the analysis to noisy, heterogeneous spaces and to $n>2$ inputs\npredicts a sharp $1/n$ collapse of multi-input processing capacity and a\nnon-monotonic optimum for $p_S$. A minimal ReLU network trained end-to-end\nreproduces these laws: during learning a resolution boundary self-organizes and\nempirical $(p_S,p_I)$ trajectories closely follow theoretical curves for\nlinearly decaying similarity. Finally, we demonstrate that the same limits\npersist in two markedly more complex settings -- a convolutional neural network\nand state-of-the-art vision-language models -- confirming that\nfinite-resolution similarity is a fundamental emergent informational\nconstraint, not merely a toy-model artifact. Together, these results provide an\nexact theory of the generalization-identification trade-off and clarify how\nsemantic resolution shapes the representational capacity of deep networks and\nbrains alike.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14797v1", "AI": {"title_translation": "受语义性束缚：泛化-识别权衡的普适定律", "tldr": "智能系统在泛化和识别之间存在根本权衡，这是由有限语义分辨率引起的，并受深度网络重现的普适定律支配。", "motivation": "智能系统必须部署同时具备结构性（支持广泛泛化）和选择性（保留输入同一性）的内部表征。本文揭示了这种权衡的根本限制。", "method": "本文为输入间表征相似性随有限语义分辨率$\\varepsilon$衰减的任何模型，推导了将正确泛化概率$p_S$和识别概率$p_I$固定在独立于输入空间几何的普适帕累托前沿上的闭合形式表达式。将分析扩展到噪声、异构空间和$n>2$个输入，预测了多输入处理能力急剧的$1/n$崩溃和$p_S$的非单调最优值。一个最小的端到端训练的ReLU网络重现了这些定律。最后，还在卷积神经网络和最先进的视觉-语言模型中证明了相同的限制。", "result": "对于表征相似性随有限语义分辨率衰减的模型，推导出了将正确泛化概率$p_S$和识别概率$p_I$固定在普适帕累托前沿上的闭合形式表达式。扩展分析预测了多输入处理能力急剧的$1/n$崩溃和$p_S$的非单调最优值。一个最小的ReLU网络重现了这些定律，学习过程中分辨率边界自组织，且经验$(p_S,p_I)$轨迹与线性衰减相似性的理论曲线紧密吻合。在卷积神经网络和视觉-语言模型中也证实了这些限制，表明有限分辨率相似性是基本的信息约束。", "conclusion": "这些结果共同提供了一个关于泛化-识别权衡的精确理论，并阐明了语义分辨率如何塑造深度网络和大脑的表征能力。", "translation": "智能系统必须部署同时具备结构性（以支持广泛泛化）和选择性（以保留输入同一性）的内部表征。我们揭示了这种权衡的根本限制。对于任何输入间表征相似性随有限语义分辨率$\\varepsilon$衰减的模型，我们推导出了将其正确泛化概率$p_S$和识别概率$p_I$固定在独立于输入空间几何的普适帕累托前沿上的闭合形式表达式。将分析扩展到噪声、异构空间以及$n>2$个输入，预测了多输入处理能力急剧的$1/n$崩溃和$p_S$的非单调最优值。一个最小的端到端训练的ReLU网络重现了这些定律：在学习过程中，分辨率边界自组织，并且经验$(p_S,p_I)$轨迹密切遵循线性衰减相似性的理论曲线。最后，我们证明了相同的限制在两个明显更复杂的设置中也持续存在——一个卷积神经网络和最先进的视觉-语言模型——证实了有限分辨率相似性是一种基本的涌现信息约束，而不仅仅是玩具模型的人为产物。总而言之，这些结果提供了一个关于泛化-识别权衡的精确理论，并阐明了语义分辨率如何塑造深度网络和大脑的表征能力。", "summary": "本文研究了智能系统中泛化和识别之间由有限语义分辨率驱动的根本权衡。它推导出了支配这种权衡的普适定律和帕累托前沿，且独立于输入空间几何。分析预测了多输入处理能力的$1/n$崩溃和泛化概率的非单调最优值。这些理论发现通过一个最小的ReLU网络得到了经验验证，并在卷积神经网络和视觉-语言模型等复杂模型中得到进一步证实，确立了有限分辨率相似性作为一种基本信息约束。这项工作提供了一个精确理论，阐明了语义分辨率如何影响深度网络和大脑的表征能力。", "keywords": "泛化-识别权衡, 语义分辨率, 普适定律, 深度网络, 表征能力", "comments": "本文通过推导泛化-识别权衡的普适定律，为智能系统中的核心挑战提供了重要的理论贡献。其优势在于证明了这些定律的普适性，从简单的ReLU网络到最先进的复杂模型，表明这是一种基本的信息约束而非模型特有的伪影。这项工作有助于弥合理论理解与实际深度学习性能之间的差距。"}}
{"id": "2506.15395", "title": "A Real-time Endoscopic Image Denoising System", "authors": ["Yu Xing", "Shishi Huang", "Meng Lv", "Guo Chen", "Huailiang Wang", "Lingzhi Sui"], "summary": "Endoscopes featuring a miniaturized design have significantly enhanced\noperational flexibility, portability, and diagnostic capability while\nsubstantially reducing the invasiveness of medical procedures. Recently,\nsingle-use endoscopes equipped with an ultra-compact analogue image sensor\nmeasuring less than 1mm x 1mm bring revolutionary advancements to medical\ndiagnosis. They reduce the structural redundancy and large capital expenditures\nassociated with reusable devices, eliminate the risk of patient infections\ncaused by inadequate disinfection, and alleviate patient suffering. However,\nthe limited photosensitive area results in reduced photon capture per pixel,\nrequiring higher photon sensitivity settings to maintain adequate brightness.\nIn high-contrast medical imaging scenarios, the small-sized sensor exhibits a\nconstrained dynamic range, making it difficult to simultaneously capture\ndetails in both highlights and shadows, and additional localized digital gain\nis required to compensate. Moreover, the simplified circuit design and analog\nsignal transmission introduce additional noise sources. These factors\ncollectively contribute to significant noise issues in processed endoscopic\nimages. In this work, we developed a comprehensive noise model for analog image\nsensors in medical endoscopes, addressing three primary noise types:\nfixed-pattern noise, periodic banding noise, and mixed Poisson-Gaussian noise.\nBuilding on this analysis, we propose a hybrid denoising system that\nsynergistically combines traditional image processing algorithms with advanced\nlearning-based techniques for captured raw frames from sensors. Experiments\ndemonstrate that our approach effectively reduces image noise without fine\ndetail loss or color distortion, while achieving real-time performance on FPGA\nplatforms and an average PSNR improvement from 21.16 to 33.05 on our test\ndataset.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15395v1", "AI": {"title_translation": "实时内窥镜图像去噪系统", "tldr": "开发了一种混合去噪系统，用于解决小型内窥镜图像中的噪声问题，实现了实时去噪并显著提高了图像质量。", "motivation": "小型化一次性内窥镜在医疗诊断中具有革命性进展，但其小尺寸传感器导致光子捕获减少、动态范围受限以及电路设计和模拟信号传输引入额外噪声，从而导致图像噪声问题严重。", "method": "开发了医用内窥镜模拟图像传感器的综合噪声模型，识别出三种主要噪声类型：固定模式噪声、周期性带状噪声和混合泊松-高斯噪声。在此基础上，提出了一种混合去噪系统，结合了传统图像处理算法和先进的基于学习的技术，用于处理传感器捕获的原始帧。", "result": "该方法有效减少了图像噪声，同时没有丢失精细细节或颜色失真，并在FPGA平台上实现了实时性能。在测试数据集上，平均PSNR从21.16提高到33.05。", "conclusion": "该混合去噪系统能够有效解决小型内窥镜图像的噪声问题，同时保持图像质量并实现实时处理，为医疗诊断提供了更清晰的图像。", "translation": "内窥镜的小型化设计显著增强了操作灵活性、便携性和诊断能力，同时大幅降低了医疗程序的侵入性。最近，配备小于1毫米x1毫米超紧凑型模拟图像传感器的一次性内窥镜为医疗诊断带来了革命性进展。它们减少了与可重复使用设备相关的结构冗余和高资本支出，消除了因消毒不足引起的患者感染风险，并减轻了患者痛苦。然而，有限的光敏区域导致每个像素捕获的光子减少，需要更高的光子灵敏度设置以保持足够的亮度。在高对比度医疗成像场景中，小尺寸传感器表现出受限的动态范围，难以同时捕捉高光和阴影中的细节，并且需要额外的局部数字增益来补偿。此外，简化的电路设计和模拟信号传输引入了额外的噪声源。这些因素共同导致处理后的内窥镜图像出现显著的噪声问题。在这项工作中，我们为医用内窥镜中的模拟图像传感器开发了一个全面的噪声模型，解决了三种主要的噪声类型：固定模式噪声、周期性带状噪声和混合泊松-高斯噪声。在此分析的基础上，我们提出了一种混合去噪系统，它协同结合了传统图像处理算法和先进的基于学习的技术，用于从传感器捕获的原始帧。实验表明，我们的方法在不损失精细细节或颜色失真的情况下有效减少了图像噪声，同时在FPGA平台上实现了实时性能，并且在我们的测试数据集上，平均PSNR从21.16提高到33.05。", "summary": "本文针对小型化一次性内窥镜图像中存在的严重噪声问题，开发了一个全面的模拟图像传感器噪声模型，并基于此提出了一种结合传统图像处理和学习算法的混合去噪系统。实验证明，该系统能有效去除噪声，保持图像细节和色彩，并在FPGA上实现实时处理，显著提升了图像质量（PSNR从21.16提高到33.05）。", "keywords": "内窥镜图像去噪, 实时系统, 噪声模型, 混合去噪, FPGA", "comments": "该论文的创新点在于针对小型化内窥镜特有的噪声源，建立了详细的噪声模型，并在此基础上提出了一种结合传统和学习方法的混合去噪系统。其重要性在于解决了困扰小型内窥镜图像质量的关键问题，使得这些革命性的诊断工具能够提供更清晰、更可靠的图像，提高了医疗诊断的准确性。实时性能和FPGA部署也表明了其在实际应用中的潜力。"}}
{"id": "2506.15131", "title": "Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs", "authors": ["Jing Yang Lee", "Kong-Aik Lee", "Woon-Seng Gan"], "summary": "Open-domain Dialogue (OD) exhibits a one-to-many (o2m) property, whereby\nmultiple appropriate responses exist for a single dialogue context. Despite\nprior research showing that modeling this property boosts response diversity,\nmost modern LLM-based dialogue agents do not explicitly do so. In this work, we\nmodel the o2m property of OD in LLMs by decomposing OD generation into two key\ntasks: Multi-Response Generation (MRG) and Preference-based Selection (PS),\nwhich entail generating a set of n semantically and lexically diverse\nhigh-quality responses for a given dialogue context, followed by selecting a\nsingle response based on human preference, respectively. To facilitate MRG and\nPS, we introduce o2mDial, a dialogue corpus explicitly designed to capture the\no2m property by featuring multiple plausible responses for each context.\nLeveraging o2mDial, we propose new in-context learning and instruction-tuning\nstrategies, as well as novel evaluation metrics for MRG, alongside a\nmodel-based approach for PS. Empirical results demonstrate that applying the\nproposed two-stage framework to smaller LLMs for OD generation enhances overall\nresponse diversity while maintaining contextual coherence, improving response\nquality by up to 90%, bringing them closer to the performance of larger models.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15131v1", "AI": {"title_translation": "使用大型语言模型建模开放域对话中的一对多特性", "tldr": "通过分解为多响应生成和基于偏好的选择，并引入新数据集和策略，该研究使小型LLM在开放域对话中实现了更高的响应多样性和质量，接近大型模型。", "motivation": "开放域对话（OD）具有一对多（o2m）特性，即单个对话上下文存在多个合适响应。尽管先前的研究表明建模此特性可提高响应多样性，但大多数现代基于LLM的对话代理未明确建模。", "method": "将开放域对话生成分解为两个关键任务：多响应生成（MRG）和基于偏好的选择（PS）。为此，引入了o2mDial对话语料库来明确捕捉o2m特性。利用o2mDial，提出了新的上下文学习和指令微调策略，以及用于MRG的新评估指标和用于PS的基于模型的方法。", "result": "实证结果表明，将提出的两阶段框架应用于小型LLM进行开放域对话生成，增强了整体响应多样性，同时保持了上下文连贯性，将响应质量提高了高达90%，使其性能接近大型模型。", "conclusion": "通过有效建模开放域对话中的一对多特性，本研究提出的两阶段框架和相关策略显著提升了小型LLM的对话响应质量和多样性，使其性能可媲美大型模型。", "translation": "开放域对话（OD）展现出一种一对多（o2m）特性，即对于单个对话上下文存在多个合适的响应。尽管先前的研究表明建模此特性可以提高响应多样性，但大多数现代基于大型语言模型（LLM）的对话代理并未明确这样做。在这项工作中，我们通过将OD生成分解为两个关键任务来建模LLM中OD的o2m特性：多响应生成（MRG）和基于偏好的选择（PS）。这两个任务分别包括为给定对话上下文生成一组n个语义和词汇上多样化的高质量响应，然后根据人类偏好选择单个响应。为了促进MRG和PS，我们引入了o2mDial，这是一个专门设计用于捕捉o2m特性的对话语料库，其特点是每个上下文都有多个合理的响应。利用o2mDial，我们提出了新的上下文学习和指令微调策略，以及用于MRG的新颖评估指标，同时还有一种用于PS的基于模型的方法。实证结果表明，将提出的两阶段框架应用于小型LLM进行OD生成，增强了整体响应多样性，同时保持了上下文连贯性，将响应质量提高了高达90%，使其性能接近大型模型。", "summary": "本文提出了一种新的方法来解决开放域对话中“一对多”（o2m）特性未被大型语言模型（LLMs）充分建模的问题。研究将对话生成分解为多响应生成（MRG）和基于偏好的选择（PS）两个阶段，并引入了o2mDial数据集以捕获o2m特性。通过新的上下文学习、指令微调策略和评估指标，实验证明该框架能显著提高小型LLM的响应多样性和质量，使其表现接近大型模型。", "keywords": "开放域对话, 一对多特性, 大型语言模型, 多响应生成, 对话多样性", "comments": "本文的创新点在于明确地将开放域对话中的“一对多”特性建模，并将其分解为两个可管理的子任务。引入专门的o2mDial数据集和相应的训练及评估策略，有效地弥补了现有LLM在响应多样性方面的不足。通过使小型LLM达到接近大型模型的性能，这项工作具有重要的实际意义，有助于推动更高效和多样化的对话系统发展。"}}
{"id": "2506.15142", "title": "Fourth- and Higher-Order Semi-Lagrangian Finite Volume Methods for the Two-dimensional Advection Equation on Arbitrarily Complex Domains", "authors": ["Yunxia Sun", "Kaiyi Liang", "Yuke Zhu", "Zhi Lin", "Qinghai Zhang"], "summary": "To numerically solve the two-dimensional advection equation, we propose a\nfamily of fourth- and higher-order semi-Lagrangian finite volume (SLFV) methods\nthat feature (1) fourth-, sixth-, and eighth-order convergence rates, (2)\napplicability to both regular and irregular domains with arbitrarily complex\ntopology and geometry, (3) ease of handling both zero and nonzero source terms,\nand (4) the same algorithmic steps for both periodic and incoming penetration\nconditions. Test results confirm the analysis and demonstrate the accuracy,\nflexibility, robustness, and excellent conditioning of the proposed SLFV\nmethod.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15142v1", "AI": {"title_translation": "二维平流方程在任意复杂域上的四阶及更高阶半拉格朗日有限体积方法", "tldr": "本文提出了一种四阶及更高阶半拉格朗日有限体积（SLFV）方法，用于数值求解二维平流方程。该方法在精度、灵活性、鲁棒性和条件性方面表现出色，并适用于任意复杂的计算域。", "motivation": "旨在数值求解二维平流方程。", "method": "提出了一系列四阶及更高阶的半拉格朗日有限体积（SLFV）方法。这些方法具有以下特点：(1) 四阶、六阶和八阶收敛速度；(2) 适用于拓扑和几何形状任意复杂的规则和不规则域；(3) 易于处理零和非零源项；(4) 对于周期性和入射穿透条件，算法步骤相同。", "result": "测试结果证实了分析，并展示了所提出的SLFV方法的准确性、灵活性、鲁棒性和优良的条件性。", "conclusion": "所提出的四阶及更高阶半拉格朗日有限体积（SLFV）方法能够准确、灵活、鲁棒地数值求解二维平流方程，并适用于任意复杂的计算域。", "translation": "为了数值求解二维平流方程，我们提出了一系列四阶及更高阶的半拉格朗日有限体积（SLFV）方法，其特点是：（1）四阶、六阶和八阶收敛速度；（2）适用于拓扑和几何形状任意复杂的规则和不规则域；（3）易于处理零和非零源项；（4）对于周期性和入射穿透条件，算法步骤相同。测试结果证实了分析，并展示了所提出的SLFV方法的准确性、灵活性、鲁棒性和优良的条件性。", "summary": "本文提出了一种创新的四阶及更高阶半拉格朗日有限体积（SLFV）方法，专为数值求解二维平流方程而设计。该方法实现了高阶收敛率（四阶、六阶、八阶），并具有广泛的适用性，能够处理任意复杂拓扑和几何形状的规则与不规则域。此外，它还能灵活应对零和非零源项，并对不同边界条件采用统一的算法步骤。实验结果充分验证了该方法的分析预测，并突显了其在准确性、灵活性、鲁棒性和条件性方面的卓越性能。", "keywords": "半拉格朗日有限体积方法, 平流方程, 高阶方法, 复杂域, 数值求解", "comments": "该论文提出了一种创新的高阶半拉格朗日有限体积方法，其亮点在于实现了四阶及以上的高精度，并且能够处理任意复杂的计算域。这对于解决实际工程和科学问题中遇到的复杂几何形状和平流现象具有重要意义。该方法在处理源项和边界条件方面的统一性也提高了其通用性。"}}
{"id": "2506.15212", "title": "LLM vs. SAST: A Technical Analysis on Detecting Coding Bugs of GPT4-Advanced Data Analysis", "authors": ["Madjid G. Tehrani", "Eldar Sultanow", "William J. Buchanan", "Mahkame Houmani", "Christel H. Djaha Fodja"], "summary": "With the rapid advancements in Natural Language Processing (NLP), large\nlanguage models (LLMs) like GPT-4 have gained significant traction in diverse\napplications, including security vulnerability scanning. This paper\ninvestigates the efficacy of GPT-4 in identifying software vulnerabilities\ncompared to traditional Static Application Security Testing (SAST) tools.\nDrawing from an array of security mistakes, our analysis underscores the potent\ncapabilities of GPT-4 in LLM-enhanced vulnerability scanning. We unveiled that\nGPT-4 (Advanced Data Analysis) outperforms SAST by an accuracy of 94% in\ndetecting 32 types of exploitable vulnerabilities. This study also addresses\nthe potential security concerns surrounding LLMs, emphasising the imperative of\nsecurity by design/default and other security best practices for AI.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15212v1", "AI": {"title_translation": "大型语言模型（LLM）与静态应用安全测试（SAST）：GPT4高级数据分析在检测编码错误方面的技术分析", "tldr": "本文对比了GPT-4与传统SAST工具在检测软件漏洞方面的效率，发现GPT-4（高级数据分析）在检测32种可利用漏洞时，准确率达94%，优于SAST，并强调了LLM的安全设计重要性。", "motivation": "大型语言模型（LLM）在安全漏洞扫描方面的应用日益增多，但其在识别软件漏洞方面的有效性与传统静态应用安全测试（SAST）工具相比仍需深入研究。", "method": "本研究通过分析一系列安全错误，比较了GPT-4（高级数据分析）与传统SAST工具在识别32种可利用漏洞方面的能力。", "result": "GPT-4（高级数据分析）在检测32种可利用漏洞时的准确率达到94%，表现优于传统SAST工具。", "conclusion": "GPT-4在软件漏洞检测方面展现出强大潜力，但同时需要重视大型语言模型自身面临的潜在安全问题，并强调在AI应用中实施安全设计和最佳实践的重要性。", "translation": "随着自然语言处理（NLP）的快速发展，大型语言模型（LLM）如GPT-4在包括安全漏洞扫描在内的各种应用中获得了显著关注。本文研究了GPT-4在识别软件漏洞方面与传统静态应用安全测试（SAST）工具相比的有效性。通过分析一系列安全错误，我们的分析强调了GPT-4在LLM增强型漏洞扫描中的强大能力。我们发现GPT-4（高级数据分析）在检测32种可利用漏洞时，以94%的准确率优于SAST。本研究还探讨了围绕LLM的潜在安全问题，强调了AI安全设计/默认安全以及其他安全最佳实践的必要性。", "summary": "本文评估了GPT-4（高级数据分析）在检测软件漏洞方面的效能，并将其与传统静态应用安全测试（SAST）工具进行了对比。研究结果显示，GPT-4在检测32种可利用漏洞时达到了94%的准确率，显著优于SAST。此外，论文还讨论了大型语言模型自身可能存在的安全风险，并强调了在AI应用中实施安全设计和最佳实践的重要性。", "keywords": "GPT-4, 大型语言模型, 静态应用安全测试, 漏洞检测, 软件安全", "comments": "这篇论文展示了大型语言模型在软件漏洞检测领域的巨大潜力，尤其是在准确率方面超越了传统的SAST工具，这对于自动化安全分析是一个重要的进步。同时，论文也关注了LLM自身可能带来的安全隐患，提醒了AI应用开发中“安全由设计/默认”原则的重要性，具有前瞻性。"}}
{"id": "2506.15126", "title": "VIMS: A Visual-Inertial-Magnetic-Sonar SLAM System in Underwater Environments", "authors": ["Bingbing Zhang", "Huan Yin", "Shuo Liu", "Fumin Zhang", "Wen Xu"], "summary": "In this study, we present a novel simultaneous localization and mapping\n(SLAM) system, VIMS, designed for underwater navigation. Conventional\nvisual-inertial state estimators encounter significant practical challenges in\nperceptually degraded underwater environments, particularly in scale estimation\nand loop closing. To address these issues, we first propose leveraging a\nlow-cost single-beam sonar to improve scale estimation. Then, VIMS integrates a\nhigh-sampling-rate magnetometer for place recognition by utilizing magnetic\nsignatures generated by an economical magnetic field coil. Building on this, a\nhierarchical scheme is developed for visual-magnetic place recognition,\nenabling robust loop closure. Furthermore, VIMS achieves a balance between\nlocal feature tracking and descriptor-based loop closing, avoiding additional\ncomputational burden on the front end. Experimental results highlight the\nefficacy of the proposed VIMS, demonstrating significant improvements in both\nthe robustness and accuracy of state estimation within underwater environments.", "comment": "This work has been accepted for publication at the IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15126v1", "AI": {"title_translation": "VIMS：水下环境中的视觉-惯性-磁-声纳SLAM系统", "tldr": "VIMS是一种新型水下SLAM系统，通过整合低成本声纳和磁力计，解决了传统视觉-惯性系统在水下环境中尺度估计和回环检测的挑战，显著提升了定位的鲁棒性和精度。", "motivation": "传统视觉-惯性状态估计算法在感知退化的水下环境中面临挑战，尤其是在尺度估计和回环检测方面。", "method": "1. 利用低成本单波束声纳改善尺度估计。2. 集成高采样率磁力计，通过经济型磁场线圈产生的磁特征进行地点识别。3. 开发分层视觉-磁力地点识别方案，实现鲁棒的回环检测。4. 平衡局部特征跟踪和基于描述符的回环检测，避免前端额外计算负担。", "result": "实验结果表明，所提出的VIMS系统有效，显著提高了水下环境中状态估计的鲁棒性和准确性。", "conclusion": "VIMS系统通过结合声纳和磁力计，成功克服了传统视觉-惯性系统在水下环境中的局限性，提供了更鲁棒和精确的定位与建图解决方案。", "translation": "在本研究中，我们提出了一种新颖的同步定位与建图（SLAM）系统VIMS，专为水下导航设计。传统的视觉-惯性状态估计器在感知退化的水下环境中遇到显著的实际挑战，特别是在尺度估计和回环检测方面。为了解决这些问题，我们首先提出利用低成本单波束声纳来改善尺度估计。然后，VIMS通过利用经济型磁场线圈产生的磁特征，整合了高采样率磁力计进行地点识别。在此基础上，开发了一种用于视觉-磁力地点识别的分层方案，实现了鲁棒的回环检测。此外，VIMS在局部特征跟踪和基于描述符的回环检测之间取得了平衡，避免了前端的额外计算负担。实验结果突出了所提出的VIMS的有效性，证明了在水下环境中状态估计的鲁棒性和准确性方面都有显著改进。", "summary": "VIMS是一种为水下环境设计的新型SLAM系统，旨在解决传统视觉-惯性方法在水下尺度估计和回环检测上的难题。该系统通过引入低成本单波束声纳来提高尺度估计精度，并利用高采样率磁力计和磁场线圈进行地点识别，进而开发了分层视觉-磁力回环检测方案。VIMS还在计算效率上进行了优化。实验证明，VIMS显著提升了水下状态估计的鲁棒性和准确性。", "keywords": "水下SLAM, 视觉-惯性, 声纳, 磁力计, 回环检测", "comments": "该论文提出了一种创新的多传感器融合SLAM系统，通过整合声纳和磁力计有效弥补了传统视觉-惯性系统在水下环境中的固有缺陷，尤其是在尺度恢复和回环检测方面。其分层视觉-磁力识别方案以及对计算效率的考量，是系统实用性的重要体现。"}}
{"id": "2506.15294", "title": "UXR Point of View on Product Feature Prioritization Prior To Multi-Million Engineering Commitments", "authors": ["Jonas Lau", "Annie Tran"], "summary": "This paper discusses a popular UX research activity, feature prioritization,\nusing the User Experience Research Point of View (UXR PoV) Playbook framework.\nWe describe an application of multinomial logistic regression, frequently\nmarketed as MaxDiff, for prioritizing product features in consumer product\ndevelopment. It addresses challenges of traditional surveying techniques. We\npropose a solution using MaxDiff to generate a reliable preference list with a\nreasonable sample size. We also adapt the MaxDiff method to reduce the number\nof survey responses in half, making it less tedious from the survey takers'\nperspective. We present a case study using the adapted MaxDiff method for\ntablet feature prioritization research involving users with disabilities.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15294v1", "AI": {"title_translation": "用户体验研究视角下的产品功能优先级排序，在数百万工程投入之前", "tldr": "本文提出并应用了经过调整的MaxDiff方法，用于在产品开发早期阶段进行产品功能优先级排序，以克服传统调查方法的挑战并减少用户负担，并通过案例研究验证了其有效性。", "motivation": "解决传统调查技术在产品功能优先级排序中面临的挑战。", "method": "描述了一种应用多项逻辑回归（常被称为MaxDiff）的方法，用于消费品开发中的产品功能优先级排序。该方法经过调整，可将调查响应数量减少一半，以减轻受访者的负担。", "result": "成功生成了可靠的产品功能偏好列表，且样本量合理。同时，将调查响应数量减少了一半，减轻了受访者的负担。通过案例研究展示了该方法在涉及残障用户的平板电脑功能优先级研究中的应用。", "conclusion": "本文提出的并经过调整的MaxDiff方法，能够有效解决传统调查技术在产品功能优先级排序中的挑战，以合理的样本量生成可靠的偏好列表，并显著减轻受访者的调查负担，尤其适用于在重大工程投入前进行用户研究。", "translation": "本文讨论了一项流行的用户体验研究活动——功能优先级排序，并使用了用户体验研究视角（UXR PoV）手册框架。我们描述了多项逻辑回归（常被称为MaxDiff）在消费品开发中用于产品功能优先级排序的应用。它解决了传统调查技术的挑战。我们提出了一种使用MaxDiff的解决方案，以合理的样本量生成可靠的偏好列表。我们还调整了MaxDiff方法，将调查响应数量减少一半，从而减轻了受访者的负担。我们展示了一个案例研究，该案例使用了调整后的MaxDiff方法进行平板功能优先级研究，涉及残障用户。", "summary": "本文利用用户体验研究视角（UXR PoV）框架，探讨了产品功能优先级排序问题。为解决传统调查方法的挑战，论文提出并应用了MaxDiff（多项逻辑回归）方法来生成可靠的产品功能偏好列表。该研究还对MaxDiff方法进行了改进，成功将调查响应量减少一半，提升了用户体验。通过一个涉及残障用户的平板功能优先级研究案例，验证了该方法的有效性。", "keywords": "产品功能优先级, MaxDiff, 用户体验研究, 逻辑回归, 案例研究", "comments": "该论文的创新点在于将MaxDiff方法应用于产品功能优先级排序，并对其进行了优化，显著减少了调查负担。这对于在产品开发早期阶段，尤其是在涉及数百万美元工程投入前，获取可靠的用户偏好数据具有重要意义。通过案例研究验证了其在特殊用户群体中的应用潜力。"}}
{"id": "2506.15636", "title": "Quantum Error Correction Exploiting Degeneracy to Approach the Hashing Bound", "authors": ["Kenta Kasai"], "summary": "Quantum error correction is essential for realizing scalable quantum\ncomputation. Among various approaches, low-density parity-check codes over\nhigher-order Galois fields have shown promising performance due to their\nstructured sparsity and compatibility with iterative decoding algorithms whose\ncomputational complexity scales linearly with the number of physical qubits. In\nthis work, we demonstrate that explicitly exploiting the degeneracy of quantum\nerrors can significantly enhance the decoding performance. Simulation results\nover the depolarizing channel indicate that the proposed method, at a coding\nrate of 1/3, achieves a frame error rate as low as $10^{-4}$ at a physical\nerror rate of 9.45% for a code with 104,000 logical qubits and 312,000 physical\nqubits, approaching the quantum hashing bound. These findings highlight the\ncritical role of degeneracy in closing the gap to the fundamental limits of\nquantum error correction.", "comment": "This work has been submitted to a journal for possible publication", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.15636v1", "AI": {"title_translation": "利用简并性逼近哈希界限的量子纠错", "tldr": "本文展示了通过显式利用量子误差的简并性，可以显著提高量子纠错的解码性能，在退极化信道上以1/3的编码率实现了低至$10^{-4}$的帧错误率，接近量子哈希界限。", "motivation": "量子纠错对于实现可扩展的量子计算至关重要。", "method": "本文研究了高阶伽罗瓦域上的低密度奇偶校验码，并通过显式利用量子误差的简并性来增强解码性能。", "result": "在退极化信道上，所提出的方法在1/3的编码率下，对于一个包含104,000个逻辑量子比特和312,000个物理量子比特的编码，在9.45%的物理错误率下实现了低至$10^{-4}$的帧错误率，接近量子哈希界限。", "conclusion": "这些发现强调了简并性在缩小与量子纠错基本极限之间差距的关键作用。", "translation": "量子纠错对于实现可扩展的量子计算至关重要。在各种方法中，高阶伽罗瓦域上的低密度奇偶校验码由于其结构化的稀疏性以及与计算复杂度随物理量子比特数量线性扩展的迭代解码算法的兼容性，显示出有前景的性能。在这项工作中，我们证明了显式利用量子误差的简并性可以显著增强解码性能。在退极化信道上的仿真结果表明，所提出的方法在1/3的编码率下，对于一个包含104,000个逻辑量子比特和312,000个物理量子比特的编码，在9.45%的物理错误率下实现了低至$10^{-4}$的帧错误率，接近量子哈希界限。这些发现强调了简并性在缩小与量子纠错基本极限之间差距的关键作用。", "summary": "本研究提出了一种通过显式利用量子误差简并性来增强量子纠错性能的方法。该方法基于高阶伽罗瓦域上的低密度奇偶校验码，并在退极化信道上进行了仿真。结果显示，在1/3的编码率下，对于大规模量子比特编码，该方法能够在高物理错误率下实现极低的帧错误率，性能接近量子哈希界限，突出了简并性在逼近量子纠错理论极限中的重要性。", "keywords": "量子纠错, 简并性, 哈希界限, 低密度奇偶校验码, 退极化信道", "comments": "本文的创新点在于明确利用量子误差的简并性来优化量子纠错码的解码性能。其重要性在于通过这种方法，使得量子纠错码的性能显著提升，甚至能接近量子哈希界限，这对于实现大规模容错量子计算具有重大意义。论文展示了在实际应用中实现高性能量子纠错码的潜力。"}}
{"id": "2506.14846", "title": "Finding Optimal Kernel Size and Dimension in Convolutional Neural Networks An Architecture Optimization Approach", "authors": ["Shreyas Rajeev", "B Sathish Babu"], "summary": "Kernel size selection in Convolutional Neural Networks (CNNs) is a critical\nbut often overlooked design decision that affects receptive field, feature\nextraction, computational cost, and model accuracy. This paper proposes the\nBest Kernel Size Estimation Function (BKSEF), a mathematically grounded and\nempirically validated framework for optimal, layer-wise kernel size\ndetermination. BKSEF balances information gain, computational efficiency, and\naccuracy improvements by integrating principles from information theory, signal\nprocessing, and learning theory. Extensive experiments on CIFAR-10, CIFAR-100,\nImageNet-lite, ChestX-ray14, and GTSRB datasets demonstrate that BKSEF-guided\narchitectures achieve up to 3.1 percent accuracy improvement and 42.8 percent\nreduction in FLOPs compared to traditional models using uniform 3x3 kernels.\nTwo real-world case studies further validate the approach: one for medical\nimage classification in a cloud-based setup, and another for traffic sign\nrecognition on edge devices. The former achieved enhanced interpretability and\naccuracy, while the latter reduced latency and model size significantly, with\nminimal accuracy trade-off. These results show that kernel size can be an\nactive, optimizable parameter rather than a fixed heuristic. BKSEF provides\npractical heuristics and theoretical support for researchers and developers\nseeking efficient and application-aware CNN designs. It is suitable for\nintegration into neural architecture search pipelines and real-time systems,\noffering a new perspective on CNN optimization.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14846v1", "AI": {"title_translation": "在卷积神经网络中寻找最优核大小和维度：一种架构优化方法", "tldr": "提出BKSEF框架，优化CNN核大小，显著提升精度并降低计算成本。", "motivation": "卷积神经网络（CNN）中的核大小选择是一个关键但常被忽视的设计决策，它影响感受野、特征提取、计算成本和模型精度。", "method": "本文提出了最佳核大小估计函数（BKSEF），这是一个基于数学原理并经过经验验证的框架，用于最优的逐层核大小确定。BKSEF通过整合信息论、信号处理和学习理论的原理，平衡信息增益、计算效率和精度提升。", "result": "在CIFAR-10、CIFAR-100、ImageNet-lite、ChestX-ray14和GTSRB数据集上，BKSEF指导的架构相比使用统一3x3核的传统模型，精度最高提升3.1%，FLOPs减少42.8%。在医疗图像分类和交通标志识别的真实案例中，分别实现了增强的可解释性、精度提升、延迟降低和模型大小减小。", "conclusion": "核大小可以作为可优化的参数而非固定启发式，BKSEF为寻求高效和应用感知CNN设计的研究人员和开发人员提供了实用的启发式方法和理论支持，并适用于集成到神经架构搜索流程和实时系统中。", "translation": "卷积神经网络（CNN）中的核大小选择是一个关键但常被忽视的设计决策，它影响感受野、特征提取、计算成本和模型精度。本文提出了最佳核大小估计函数（BKSEF），这是一个基于数学原理并经过经验验证的框架，用于最优的逐层核大小确定。BKSEF通过整合信息论、信号处理和学习理论的原理，平衡信息增益、计算效率和精度提升。在CIFAR-10、CIFAR-100、ImageNet-lite、ChestX-ray14和GTSRB数据集上进行的大量实验表明，BKSEF指导的架构相比使用统一3x3核的传统模型，精度最高提升3.1%，FLOPs减少42.8%。两个真实世界的案例研究进一步验证了该方法：一个用于基于云的医疗图像分类，另一个用于边缘设备的交通标志识别。前者实现了增强的可解释性和精度，后者显著降低了延迟和模型大小，且精度损失最小。这些结果表明，核大小可以是一个主动的、可优化的参数，而不是一个固定的启发式方法。BKSEF为寻求高效和应用感知CNN设计的研究人员和开发人员提供了实用的启发式方法和理论支持。它适用于集成到神经架构搜索流程和实时系统中，为CNN优化提供了新的视角。", "summary": "本文提出最佳核大小估计函数（BKSEF），一个用于卷积神经网络中逐层优化核大小的框架。BKSEF整合信息论、信号处理和学习理论，以平衡信息增益、计算效率和模型精度。实验证明，BKSEF指导的架构在多个数据集上显著提升了精度并降低了计算成本，且在实际应用中展现出优越性能，证明核大小是一个可优化的参数。", "keywords": "卷积神经网络, 核大小优化, 架构优化, BKSEF, 神经网络搜索", "comments": "这篇论文的创新点在于将核大小视为一个可优化的参数，而非固定的启发式选择，并提出了一个理论与实践相结合的框架（BKSEF）来系统地解决这一问题。其重要性体现在显著提升模型性能（精度提升、计算成本降低）和实际应用价值（医疗图像、边缘设备）。该方法为CNN架构优化提供了新的视角和工具，有望集成到自动化神经架构搜索流程中，提高模型设计的效率和效果。"}}
{"id": "2506.15677", "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence", "authors": ["Yining Hong", "Rui Sun", "Bingxuan Li", "Xingcheng Yao", "Maxine Wu", "Alexander Chien", "Da Yin", "Ying Nian Wu", "Zhecan James Wang", "Kai-Wei Chang"], "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.", "comment": null, "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.15677v1", "AI": {"title_translation": "具身网络智能体：弥合物理与数字领域以实现集成智能体智能", "tldr": "引入具身网络智能体范式，并开发了一个统一的模拟平台和基准，以评估需要物理和数字智能协作的任务。", "motivation": "当前的AI智能体要么专注于数字信息，要么专注于物理交互，很少两者兼顾，这限制了它们解决需要集成物理和数字智能的任务的能力。", "method": "提出了具身网络智能体（Embodied Web Agents）这一新范式，并开发了具身网络智能体任务环境，这是一个紧密集成3D室内外环境与功能性网络界面的统一模拟平台。在此基础上，构建并发布了具身网络智能体基准，包含烹饪、导航、购物、旅游和地理定位等任务。", "result": "实验结果显示，最先进的AI系统与人类能力之间存在显著的性能差距。", "conclusion": "具身认知和网络规模知识获取的交叉领域既带来了挑战，也提供了机遇。", "translation": "当前的AI智能体大多是孤立的——它们要么检索并推理在线获取的大量数字信息和知识；要么通过具身感知、规划和行动与物理世界交互——但很少两者兼顾。这种分离限制了它们解决需要集成物理和数字智能的任务的能力，例如根据在线食谱烹饪、使用动态地图数据导航，或利用网络知识解释现实世界地标。我们引入了具身网络智能体，这是一种新型的AI智能体范式，它能够流畅地连接具身和网络规模推理。为了实现这一概念，我们首先开发了具身网络智能体任务环境，这是一个统一的模拟平台，紧密集成了真实的3D室内外环境与功能性网络界面。在此平台的基础上，我们构建并发布了具身网络智能体基准，它包含了一系列多样化的任务，包括烹饪、导航、购物、旅游和地理定位——所有这些任务都需要在物理和数字领域之间进行协调推理，以系统地评估跨领域智能。实验结果揭示了最先进的AI系统与人类能力之间存在显著的性能差距，从而在具身认知和网络规模知识获取的交叉领域确立了挑战和机遇。所有数据集、代码和网站均可在我们的项目页面 https://embodied-web-agent.github.io/ 公开获取。", "summary": "本研究引入了“具身网络智能体”这一新范式，旨在弥合物理具身与网络规模数字推理之间的鸿沟。为实现这一概念，作者开发了一个统一的模拟平台——具身网络智能体任务环境，该平台将真实的3D环境与功能性网络界面相结合。在此基础上，构建并发布了具身网络智能体基准，包含多项需要物理与数字领域协作的任务。实验结果表明，现有AI系统在这些集成任务上与人类能力存在显著差距，突显了具身认知与网络知识结合领域的挑战和机遇。", "keywords": "具身智能体, 网络智能体, 物理-数字集成, 跨领域智能, 基准测试", "comments": "这项工作具有重要意义，它解决了当前AI智能体在物理和数字领域之间割裂的问题，提出了一个创新的具身网络智能体范式。其贡献在于提供了一个统一的模拟平台和基准，为评估和开发能够处理跨领域智能任务的AI系统奠定了基础。实验结果揭示了现有技术与人类能力之间的差距，为未来的研究指明了方向。该研究的创新性在于其对集成智能的愿景及其实现该愿景的工具。"}}
{"id": "2506.14911", "title": "Event-Driven Online Vertical Federated Learning", "authors": ["Ganyu Wang", "Boyu Wang", "Bin Gu", "Charles Ling"], "summary": "Online learning is more adaptable to real-world scenarios in Vertical\nFederated Learning (VFL) compared to offline learning. However, integrating\nonline learning into VFL presents challenges due to the unique nature of VFL,\nwhere clients possess non-intersecting feature sets for the same sample. In\nreal-world scenarios, the clients may not receive data streaming for the\ndisjoint features for the same entity synchronously. Instead, the data are\ntypically generated by an \\emph{event} relevant to only a subset of clients. We\nare the first to identify these challenges in online VFL, which have been\noverlooked by previous research. To address these challenges, we proposed an\nevent-driven online VFL framework. In this framework, only a subset of clients\nwere activated during each event, while the remaining clients passively\ncollaborated in the learning process. Furthermore, we incorporated\n\\emph{dynamic local regret (DLR)} into VFL to address the challenges posed by\nonline learning problems with non-convex models within a non-stationary\nenvironment. We conducted a comprehensive regret analysis of our proposed\nframework, specifically examining the DLR under non-convex conditions with\nevent-driven online VFL. Extensive experiments demonstrated that our proposed\nframework was more stable than the existing online VFL framework under\nnon-stationary data conditions while also significantly reducing communication\nand computation costs.", "comment": "Published as a conference paper at ICLR 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14911v1", "AI": {"title_translation": "事件驱动的在线垂直联邦学习", "tldr": "本文首次提出了一种事件驱动的在线垂直联邦学习（VFL）框架，旨在解决VFL中客户端数据异步生成和非平稳环境下的挑战。该框架通过仅激活部分客户端并引入动态局部遗憾（DLR），实现了在非平稳数据下更高的稳定性和更低的通信计算成本。", "motivation": "在线学习在垂直联邦学习（VFL）中比离线学习更具适应性，但VFL的独特之处在于客户端拥有相同样本的非交叉特征集，且实际数据通常是异步的、由特定事件驱动生成的，这给在线VFL带来了挑战，而这些挑战此前被忽视。", "method": "我们提出了一种事件驱动的在线VFL框架。在该框架中，每次事件发生时只有部分相关客户端被激活，而其余客户端则被动地参与学习过程。此外，我们将动态局部遗憾（DLR）引入VFL，以应对非平稳环境下非凸模型在线学习的挑战。对该框架在非凸条件下进行了全面的DLR遗憾分析。", "result": "实验证明，我们提出的框架在非平稳数据条件下比现有在线VFL框架更稳定，并且显著降低了通信和计算成本。", "conclusion": "本文提出的事件驱动在线垂直联邦学习框架有效解决了在线VFL中客户端数据异步生成和非平稳环境的挑战，显著提升了学习的稳定性和效率。", "translation": "在线学习比离线学习更能适应垂直联邦学习（VFL）中的实际场景。然而，将在线学习整合到VFL中带来了挑战，因为VFL具有独特的性质，即客户端拥有相同样本的非交叉特征集。在实际场景中，客户端可能不会同步接收相同实体的不相交特征的数据流。相反，数据通常由仅与一部分客户端相关的“事件”生成。我们首次识别了在线VFL中的这些挑战，这些挑战已被先前的研究忽视。为了解决这些挑战，我们提出了一种事件驱动的在线VFL框架。在该框架中，每次事件期间只有一部分客户端被激活，而其余客户端被动地参与学习过程。此外，我们将“动态局部遗憾（DLR）”纳入VFL，以解决非平稳环境下非凸模型在线学习问题带来的挑战。我们对所提出的框架进行了全面的遗憾分析，特别是检查了事件驱动在线VFL下非凸条件下的DLR。广泛的实验表明，我们提出的框架在非平稳数据条件下比现有的在线VFL框架更稳定，同时显著降低了通信和计算成本。", "summary": "本文针对在线垂直联邦学习（VFL）中数据异步生成和非平稳环境的挑战，首次提出了一种事件驱动的在线VFL框架。该框架通过在每次事件中仅激活相关客户端，并引入动态局部遗憾（DLR）来处理非凸模型，解决了现有方法在实际场景中的局限性。实验结果表明，该框架在非平稳数据条件下表现出更高的稳定性，并显著降低了通信和计算成本。", "keywords": "事件驱动, 在线学习, 垂直联邦学习, 动态局部遗憾, 非平稳环境", "comments": "该论文创新性地提出了事件驱动的在线VFL框架，有效解决了实际场景中VFL客户端数据异步性的问题，这是对现有同步在线VFL模式的重要改进。引入动态局部遗憾（DLR）以处理非凸模型和非平稳环境，进一步增强了其适用性。其对真实世界数据生成模式的关注以及所展示的效率提升，使其成为该领域的一项重要贡献。"}}
{"id": "2506.15665", "title": "A Data-Integrated Framework for Learning Fractional-Order Nonlinear Dynamical Systems", "authors": ["Bahram Yaghooti", "Chengyu Li", "Bruno Sinopoli"], "summary": "This paper presents a data-integrated framework for learning the dynamics of\nfractional-order nonlinear systems in both discrete-time and continuous-time\nsettings. The proposed framework consists of two main steps. In the first step,\ninput-output experiments are designed to generate the necessary datasets for\nlearning the system dynamics, including the fractional order, the drift vector\nfield, and the control vector field. In the second step, these datasets, along\nwith the memory-dependent property of fractional-order systems, are used to\nestimate the system's fractional order. The drift and control vector fields are\nthen reconstructed using orthonormal basis functions. To validate the proposed\napproach, the algorithm is applied to four benchmark fractional-order systems.\nThe results confirm the effectiveness of the proposed framework in learning the\nsystem dynamics accurately. Finally, the same datasets are used to learn\nequivalent integer-order models. The numerical comparisons demonstrate that\nfractional-order models better capture long-range dependencies, highlighting\nthe limitations of integer-order representations.", "comment": null, "cate": "eess.SY", "url": "http://arxiv.org/abs/2506.15665v1", "AI": {"title_translation": "一种学习分数阶非线性动力学系统的数据集成框架", "tldr": "本文提出了一个数据集成框架，用于学习离散和连续时间分数阶非线性系统的动力学，并通过实验验证了其有效性，同时指出分数阶模型在捕获长程依赖性方面的优势。", "motivation": "为了学习分数阶非线性系统的动力学，包括分数阶、漂移向量场和控制向量场，并解决整数阶模型在捕获长程依赖性方面的局限性。", "method": "所提出的框架包括两个主要步骤：首先，设计输入输出实验以生成学习系统动力学所需的数据集；其次，利用这些数据集和分数阶系统的记忆依赖特性来估计系统的分数阶，并使用正交基函数重构漂移和控制向量场。", "result": "将该算法应用于四个基准分数阶系统，结果证实了所提出框架在准确学习系统动力学方面的有效性。数值比较表明分数阶模型能更好地捕获长程依赖性，突出了整数阶表示的局限性。", "conclusion": "本文提出的数据集成框架能够有效学习分数阶非线性系统的动力学，并且分数阶模型在捕获长程依赖性方面表现出优于整数阶模型的显著优势。", "translation": "本文提出了一种数据集成框架，用于学习离散时间和连续时间设置下的分数阶非线性系统动力学。所提出的框架包括两个主要步骤。第一步，设计输入输出实验以生成学习系统动力学所需的必要数据集，包括分数阶、漂移向量场和控制向量场。第二步，利用这些数据集以及分数阶系统的记忆依赖特性来估计系统的分数阶。然后使用正交基函数重构漂移和控制向量场。为了验证所提出的方法，将该算法应用于四个基准分数阶系统。结果证实了所提出框架在准确学习系统动力学方面的有效性。最后，使用相同的数据集来学习等效的整数阶模型。数值比较表明，分数阶模型能更好地捕获长程依赖性，突出了整数阶表示的局限性。", "summary": "本文提出了一种数据集成框架，用于学习离散和连续时间分数阶非线性系统的动力学。该框架分为两步：首先通过输入输出实验生成所需数据集，然后利用这些数据和分数阶系统的记忆依赖性来估计分数阶，并重构漂移和控制向量场。在四个基准系统上的验证表明该框架能准确学习系统动力学。研究还发现，与整数阶模型相比，分数阶模型在捕获长程依赖性方面表现更优。", "keywords": "分数阶系统, 非线性动力学, 数据集成, 系统辨识, 长程依赖性", "comments": "该研究提供了一个系统化的方法来识别复杂的分数阶非线性系统，其创新点在于结合数据集成和分数阶系统特有的记忆依赖性进行参数估计与场重构。通过与整数阶模型的比较，明确强调了分数阶模型在处理长程依赖性方面的优越性，这对于理解和建模具有复杂时间行为的系统具有重要意义。"}}
{"id": "2506.14795", "title": "Comparative Analysis of QNN Architectures for Wind Power Prediction: Feature Maps and Ansatz Configurations", "authors": ["Batuhan Hangun", "Emine Akpinar", "Oguz Altun", "Onder Eyecioglu"], "summary": "Quantum Machine Learning (QML) is an emerging field at the intersection of\nquantum computing and machine learning, aiming to enhance classical machine\nlearning methods by leveraging quantum mechanics principles such as\nentanglement and superposition. However, skepticism persists regarding the\npractical advantages of QML, mainly due to the current limitations of noisy\nintermediate-scale quantum (NISQ) devices. This study addresses these concerns\nby extensively assessing Quantum Neural Networks (QNNs)-quantum-inspired\ncounterparts of Artificial Neural Networks (ANNs), demonstrating their\neffectiveness compared to classical methods. We systematically construct and\nevaluate twelve distinct QNN configurations, utilizing two unique quantum\nfeature maps combined with six different entanglement strategies for ansatz\ndesign. Experiments conducted on a wind energy dataset reveal that QNNs\nemploying the Z feature map achieve up to 93% prediction accuracy when\nforecasting wind power output using only four input parameters. Our findings\nshow that QNNs outperform classical methods in predictive tasks, underscoring\nthe potential of QML in real-world applications.", "comment": "6 pages, 2 figures", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.14795v1", "AI": {"title_translation": "用于风电预测的QNN架构比较分析：特征图和Ansatz配置", "tldr": "本研究通过比较12种QNN配置在风能数据集上的表现，发现量子神经网络（QNNs）在风电预测方面优于经典方法，特别是使用Z特征图时能达到93%的准确率，证明了QML在实际应用中的潜力。", "motivation": "尽管量子机器学习（QML）在量子计算和机器学习的交叉领域新兴，但由于当前噪声中等规模量子（NISQ）设备的局限性，人们对其实际优势仍持怀疑态度。本研究旨在解决这些担忧，并通过评估量子神经网络（QNNs）来证明其与经典方法相比的有效性。", "method": "本研究系统地构建并评估了十二种不同的QNN配置，结合了两种独特的量子特征图和六种不同的纠缠策略用于ansatz设计。实验在风能数据集上进行。", "result": "实验结果表明，使用Z特征图的QNN在仅使用四个输入参数预测风力输出时，实现了高达93%的预测准确率。研究发现QNN在预测任务中优于经典方法。", "conclusion": "本研究的结论是量子神经网络（QNNs）在预测任务中表现优于经典方法，突显了量子机器学习（QML）在实际应用中的巨大潜力。", "translation": "量子机器学习（QML）是量子计算和机器学习交叉领域的一个新兴领域，旨在通过利用纠缠和叠加等量子力学原理来增强经典机器学习方法。然而，对于QML的实际优势仍然存在疑虑，这主要是由于目前噪声中等规模量子（NISQ）设备的局限性。本研究通过广泛评估量子神经网络（QNNs）——人工神经网络（ANNs）的量子启发对应物——来解决这些担忧，证明了它们与经典方法相比的有效性。我们系统地构建并评估了十二种不同的QNN配置，利用两种独特的量子特征图结合六种不同的纠缠策略进行ansatz设计。在风能数据集上进行的实验表明，采用Z特征图的QNN在仅使用四个输入参数预测风力输出时，实现了高达93%的预测准确率。我们的研究结果表明，QNN在预测任务中优于经典方法，强调了QML在实际应用中的潜力。", "summary": "本研究旨在解决对量子机器学习（QML）实际优势的疑虑，尤其是在当前噪声中等规模量子（NISQ）设备的限制下。通过系统地构建并评估十二种不同的量子神经网络（QNN）配置，结合两种量子特征图和六种纠缠策略，研究人员在风能数据集上进行了实验。结果显示，使用Z特征图的QNN在风力预测任务中表现出色，仅用四个输入参数即可达到93%的准确率，并优于经典方法。这强调了QML在实际应用中的潜力。", "keywords": "量子机器学习, 量子神经网络, 风电预测, 特征图, Ansatz配置", "comments": "该论文通过对不同QNN架构的系统性比较分析，为QML在实际应用（如风电预测）中的潜力提供了实证支持。其创新之处在于详细探讨了特征图和纠缠策略对QNN性能的影响，并量化了其相对于经典方法的优势。这对于推动QML从理论走向实际应用具有重要意义，尤其是在NISQ设备时代，为后续研究提供了宝贵的基准和方向。"}}
{"id": "2506.14802", "title": "ss-Mamba: Semantic-Spline Selective State-Space Model", "authors": ["Zuochen Ye"], "summary": "We propose ss-Mamba, a novel foundation model that enhances time series\nforecasting by integrating semantic-aware embeddings and adaptive spline-based\ntemporal encoding within a selective state-space modeling framework. Building\nupon the recent success of Transformer architectures, ss-Mamba adopts the Mamba\nselective state space model as an efficient alternative that achieves\ncomparable performance while significantly reducing computational complexity\nfrom quadratic to linear time. Semantic index embeddings, initialized from\npretrained language models, allow effective generalization to previously unseen\nseries through meaningful semantic priors. Additionally, spline-based\nKolmogorov-Arnold Networks (KAN) dynamically and interpretably capture complex\nseasonalities and non-stationary temporal effects, providing a powerful\nenhancement over conventional temporal feature encodings. Extensive\nexperimental evaluations confirm that ss-Mamba delivers superior accuracy,\nrobustness, and interpretability, demonstrating its capability as a versatile\nand computationally efficient alternative to traditional Transformer-based\nmodels in time-series forecasting.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14802v1", "AI": {"title_translation": "ss-Mamba：语义样条选择性状态空间模型", "tldr": "ss-Mamba是一个新的时序预测模型，结合了语义嵌入和样条基时间编码，利用Mamba选择性状态空间模型，实现了与Transformer相当的性能，但计算复杂度更低，且具有更好的准确性、鲁棒性和可解释性。", "motivation": "旨在通过引入语义感知嵌入和自适应样条基时间编码，增强时间序列预测能力，并克服传统Transformer架构计算复杂度高（二次方）的问题，提供一个计算效率更高（线性时间）的替代方案。", "method": "提出ss-Mamba模型，它是一个基于Mamba选择性状态空间模型的时序预测基础模型。该模型整合了：1) 从预训练语言模型初始化的语义索引嵌入，以实现对未见序列的有效泛化；2) 基于样条的Kolmogorov-Arnold Networks (KAN)，用于动态且可解释地捕捉复杂季节性和非平稳时间效应。", "result": "广泛的实验评估证实，ss-Mamba在时间序列预测中提供了卓越的准确性、鲁棒性和可解释性。", "conclusion": "ss-Mamba被证明是一种多功能且计算高效的替代方案，可替代传统基于Transformer的模型进行时间序列预测。", "translation": "我们提出了ss-Mamba，一个新颖的基础模型，通过在选择性状态空间建模框架内集成语义感知嵌入和自适应样条基时间编码，增强了时间序列预测。ss-Mamba基于Transformer架构最近的成功，采用Mamba选择性状态空间模型作为一种高效的替代方案，在实现可比性能的同时，将计算复杂度从二次方显著降低到线性时间。从预训练语言模型初始化的语义索引嵌入，通过有意义的语义先验，能够有效地泛化到以前未见的序列。此外，基于样条的Kolmogorov-Arnold网络（KAN）动态且可解释地捕捉复杂季节性和非平稳时间效应，为传统时间特征编码提供了强大的增强。广泛的实验评估证实，ss-Mamba提供了卓越的准确性、鲁棒性和可解释性，展示了其作为时间序列预测中传统基于Transformer模型的通用且计算高效的替代方案的能力。", "summary": "ss-Mamba是一种新颖的基础模型，专为时间序列预测设计。它结合了语义感知嵌入（源自预训练语言模型）和基于样条的Kolmogorov-Arnold网络（KAN）进行时间编码，并构建于高效的Mamba选择性状态空间模型之上。与Transformer相比，ss-Mamba在保持可比性能的同时，将计算复杂度从二次方降低到线性时间。实验证明，该模型在准确性、鲁棒性和可解释性方面表现优越，是传统Transformer模型在时间序列预测领域的有效且计算高效的替代品。", "keywords": "时间序列预测, ss-Mamba, 选择性状态空间模型, 语义嵌入, KAN", "comments": "ss-Mamba的创新之处在于将语义感知嵌入和自适应样条基时间编码与高效的Mamba选择性状态空间模型相结合，解决了传统Transformer模型在时间序列预测中计算复杂度高的问题。其重要性在于提供了一个性能优越且计算效率更高的替代方案，特别是在处理长序列和需要更好可解释性的场景。语义嵌入的引入也增强了模型对未见序列的泛化能力。"}}
{"id": "2506.15489", "title": "Advanced cervical cancer classification: enhancing pap smear images with hybrid PMD Filter-CLAHE", "authors": ["Ach Khozaimi", "Isnani Darti", "Syaiful Anam", "Wuryansari Muharini Kusumawinahyu"], "summary": "Cervical cancer remains a significant health problem, especially in\ndeveloping countries. Early detection is critical for effective treatment.\nConvolutional neural networks (CNN) have shown promise in automated cervical\ncancer screening, but their performance depends on Pap smear image quality.\nThis study investigates the impact of various image preprocessing techniques on\nCNN performance for cervical cancer classification using the SIPaKMeD dataset.\nThree preprocessing techniques were evaluated: perona-malik diffusion (PMD)\nfilter for noise reduction, contrast-limited adaptive histogram equalization\n(CLAHE) for image contrast enhancement, and the proposed hybrid PMD\nfilter-CLAHE approach. The enhanced image datasets were evaluated on pretrained\nmodels, such as ResNet-34, ResNet-50, SqueezeNet-1.0, MobileNet-V2,\nEfficientNet-B0, EfficientNet-B1, DenseNet-121, and DenseNet-201. The results\nshow that hybrid preprocessing PMD filter-CLAHE can improve the Pap smear image\nquality and CNN architecture performance compared to the original images. The\nmaximum metric improvements are 13.62% for accuracy, 10.04% for precision,\n13.08% for recall, and 14.34% for F1-score. The proposed hybrid PMD\nfilter-CLAHE technique offers a new perspective in improving cervical cancer\nclassification performance using CNN architectures.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15489v1", "AI": {"title_translation": "晚期宫颈癌分类：使用混合PMD滤波器-CLAHE增强巴氏涂片图像", "tldr": "本研究提出了一种混合PMD滤波器-CLAHE图像预处理技术，以提高巴氏涂片图像质量，从而提升CNN在宫颈癌分类中的性能。", "motivation": "宫颈癌仍然是一个重大的健康问题，尤其是在发展中国家。早期检测对于有效治疗至关重要。卷积神经网络（CNN）在自动化宫颈癌筛查中显示出前景，但其性能取决于巴氏涂片图像的质量。", "method": "本研究评估了三种图像预处理技术：用于降噪的Perona-Malik扩散（PMD）滤波器、用于图像对比度增强的对比度受限自适应直方图均衡化（CLAHE），以及所提出的混合PMD滤波器-CLAHE方法。增强后的图像数据集在ResNet-34、ResNet-50、SqueezeNet-1.0、MobileNet-V2、EfficientNet-B0、EfficientNet-B1、DenseNet-121和DenseNet-201等预训练模型上进行了评估，使用SIPaKMeD数据集进行宫颈癌分类。", "result": "结果表明，与原始图像相比，混合预处理PMD滤波器-CLAHE可以提高巴氏涂片图像质量和CNN架构性能。各项指标的最大改进为：准确率提高13.62%，精确率提高10.04%，召回率提高13.08%，F1分数提高14.34%。", "conclusion": "所提出的混合PMD滤波器-CLAHE技术为提高使用CNN架构的宫颈癌分类性能提供了新的视角。", "translation": "宫颈癌仍然是一个重大的健康问题，尤其是在发展中国家。早期检测对于有效治疗至关重要。卷积神经网络（CNN）在自动化宫颈癌筛查中显示出前景，但其性能取决于巴氏涂片图像的质量。本研究调查了各种图像预处理技术对CNN在SIPaKMeD数据集上进行宫颈癌分类性能的影响。评估了三种预处理技术：用于降噪的Perona-Malik扩散（PMD）滤波器、用于图像对比度增强的对比度受限自适应直方图均衡化（CLAHE），以及所提出的混合PMD滤波器-CLAHE方法。增强后的图像数据集在ResNet-34、ResNet-50、SqueezeNet-1.0、MobileNet-V2、EfficientNet-B0、EfficientNet-B1、DenseNet-121和DenseNet-201等预训练模型上进行了评估。结果表明，与原始图像相比，混合预处理PMD滤波器-CLAHE可以提高巴氏涂片图像质量和CNN架构性能。各项指标的最大改进为：准确率提高13.62%，精确率提高10.04%，召回率提高13.08%，F1分数提高14.34%。所提出的混合PMD滤波器-CLAHE技术为提高使用CNN架构的宫颈癌分类性能提供了新的视角。", "summary": "本研究旨在解决宫颈癌早期检测中巴氏涂片图像质量对CNN分类性能的影响。论文提出了一种混合PMD滤波器-CLAHE图像预处理方法，并将其与单独的PMD和CLAHE技术进行比较。实验在SIPaKMeD数据集上使用多种预训练CNN模型进行，结果显示，混合预处理显著提升了图像质量和CNN的分类性能，各项指标（准确率、精确率、召回率、F1分数）均有显著提高，证明了该方法在改善宫颈癌分类方面的潜力。", "keywords": "宫颈癌分类, 巴氏涂片图像, 图像预处理, PMD滤波器, CLAHE, 卷积神经网络", "comments": "该论文的创新点在于提出了混合PMD滤波器-CLAHE图像预处理技术，有效地提升了巴氏涂片图像的质量，进而显著改善了基于CNN的宫颈癌分类性能。其重要性在于为自动化宫颈癌筛查提供了更可靠的图像增强方案，尤其对于图像质量不佳的情况具有实际应用价值。研究评估了多种预训练模型，增强了结果的普适性。"}}
{"id": "2506.15138", "title": "Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models", "authors": ["Gyeongje Cho", "Yeonkyoun So", "Chanwoo Park", "Sangmin Lee", "Sungmok Jung", "Jaejin Lee"], "summary": "This paper introduces Thunder-Tok, a new Korean tokenizer designed to reduce\ntoken fertility without compromising model performance. Our approach uses a\nrule-based pre-tokenization method that aligns with the linguistic structure of\nthe Korean language. We also create a seed vocabulary containing tokens that\nresemble linguistic units and employ a branching entropy-based selection\nalgorithm. These techniques increase the average token length, thus lowering\nfertility while preserving linguistic information. Experimental results\nindicate that Thunder-Tok reduces fertility by approximately 10% (i.e., reduces\nthe number of tokens by 10%, improving the inference speed by 10%) compared to\nBPE without compromising performance across various downstream tasks. These\nfindings demonstrate that our linguistically informed approach is effective and\npractical for designing efficient tokenizers for language models.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15138v1", "AI": {"title_translation": "Thunder-Tok：最小化生成式语言模型韩语文本分词中的每词Token数", "tldr": "Thunder-Tok是一种新的韩语分词器，通过结合基于规则的预分词、种子词汇和分支熵选择算法，将Token数量减少约10%，同时不影响模型性能，从而提高推理速度。", "motivation": "该研究的动机是为了在不影响模型性能的前提下，减少韩语文本分词中的Token数量（即降低Token生育率），以提高生成式语言模型的效率和推理速度。", "method": "Thunder-Tok采用了一种基于规则的预分词方法，该方法与韩语的语言结构对齐。同时，它创建了一个包含类似语言单元的种子词汇，并采用了一种基于分支熵的选择算法来增加平均Token长度。", "result": "实验结果表明，与BPE相比，Thunder-Tok将Token生育率降低了约10%（即Token数量减少了10%，推理速度提高了10%），并且在各种下游任务中没有影响性能。", "conclusion": "该研究的结论是，Thunder-Tok的语言学知情方法对于设计高效的语言模型分词器是有效且实用的。", "translation": "本文介绍了Thunder-Tok，这是一种新的韩语分词器，旨在在不影响模型性能的前提下降低Token生育率。我们的方法使用了一种与韩语语言结构相符的基于规则的预分词方法。我们还创建了一个包含类似语言单元的种子词汇，并采用了一种基于分支熵的选择算法。这些技术增加了平均Token长度，从而在保留语言信息的同时降低了生育率。实验结果表明，与BPE相比，Thunder-Tok将生育率降低了约10%（即Token数量减少了10%，推理速度提高了10%），同时在各种下游任务中没有影响性能。这些发现表明，我们基于语言学的方法对于设计高效的语言模型分词器是有效且实用的。", "summary": "Thunder-Tok是一种创新的韩语分词器，旨在减少生成式语言模型中的Token数量，从而提高推理速度，同时保持模型性能。它通过结合基于规则的预分词、语言学单元的种子词汇以及基于分支熵的选择算法来实现这一目标。实验证明，与BPE相比，Thunder-Tok能将Token数量减少约10%，从而使推理速度提高10%，且不影响模型在多种下游任务上的表现。这表明其语言学知情的方法在设计高效分词器方面是有效且实用的。", "keywords": "韩语分词, Token化, Thunder-Tok, Token生育率, 生成式语言模型", "comments": "Thunder-Tok的创新之处在于其结合了语言学知识（基于规则的预分词、语言学单元的种子词汇）与信息理论方法（分支熵选择算法）来优化韩语分词。这种方法不仅显著降低了Token数量，提高了推理效率，而且在不牺牲模型性能的前提下实现了这一目标，这对于资源受限或需要高效率的生成式AI应用尤其重要。该研究的实用性在于它为特定语言（韩语）的分词器设计提供了一个成功的范例，可能启发其他复杂语言的分词优化。"}}
{"id": "2506.15165", "title": "A time-frequency method for acoustic scattering with trapping", "authors": ["Heather Wilber", "Wietse Vaes", "Abinand Gopal", "Gunnar Martinsson"], "summary": "A Fourier transform method is introduced for a class of hybrid time-frequency\nmethods that solve the acoustic scattering problem in regimes where the\nsolution exhibits both highly oscillatory behavior and slow decay in time. This\nextends the applicability of hybrid time-frequency schemes to domains with\ntrapping regions. A fast sinc transform technique for managing highly\noscillatory behavior and long time horizons is combined with a contour\nintegration scheme that improves smoothness properties in the integrand.", "comment": "18 pages, 9 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15165v1", "AI": {"title_translation": "具有陷阱声散射的时频方法", "tldr": "本文介绍了一种新的傅里叶变换时频方法，用于解决声散射问题，特别是在解表现出高振荡和时间上慢衰减，以及存在陷阱区域的复杂情况下。该方法结合了快速Sinc变换和等高线积分技术。", "motivation": "解决声散射问题，尤其是在解表现出高度振荡行为和时间上缓慢衰减的场景，并扩展混合时频方案在具有陷阱区域的域中的适用性。", "method": "引入了一种傅里叶变换方法，属于混合时频方法。该方法结合了用于管理高度振荡行为和长时间范围的快速Sinc变换技术，以及改善被积函数平滑性的等高线积分方案。", "result": "将混合时频方案的适用性扩展到了具有陷阱区域的域。", "conclusion": "该研究成功地通过引入一种新的傅里叶变换时频方法，扩展了混合时频方案在处理包含陷阱区域的声散射问题时的适用性，有效应对了高度振荡和慢衰减的解。", "translation": "引入了一种傅里叶变换方法，用于一类混合时频方法，以解决当解表现出高度振荡行为和时间上缓慢衰减时的声散射问题。这扩展了混合时频方案在具有陷阱区域的域中的适用性。一种用于管理高度振荡行为和长时间范围的快速Sinc变换技术与一种改善被积函数平滑性的等高线积分方案相结合。", "summary": "这篇论文介绍了一种新的傅里叶变换时频方法，旨在解决声散射问题，特别是当解具有高度振荡和长时间衰减特性时。该方法通过结合快速Sinc变换和等高线积分技术，成功地将混合时频方案的应用范围扩展到包含陷阱区域的复杂域。", "keywords": "声散射, 时频方法, 傅里叶变换, 陷阱区域, Sinc变换", "comments": "这项工作通过引入结合快速Sinc变换和等高线积分的傅里叶变换时频方法，有效地扩展了现有混合时频方案在处理复杂声散射问题（特别是涉及陷阱区域和高振荡/慢衰减解）方面的适用性。其创新点在于对现有方法的改进和结合，提升了在特定挑战性条件下的求解能力。"}}
{"id": "2506.15224", "title": "Facility Location Problem under Local Differential Privacy without Super-set Assumption", "authors": ["Kevin Pfisterer", "Quentin Hillebrand", "Vorapong Suppakitpaisarn"], "summary": "In this paper, we introduce an adaptation of the facility location problem\nand analyze it within the framework of local differential privacy (LDP). Under\nthis model, we ensure the privacy of client presence at specific locations.\nWhen n is the number of points, Gupta et al. established a lower bound of\n$\\Omega(\\sqrt{n})$ on the approximation ratio for any differentially private\nalgorithm applied to the original facility location problem. As a result,\nsubsequent works have adopted the super-set assumption, which may, however,\ncompromise user privacy. We show that this lower bound does not apply to our\nadaptation by presenting an LDP algorithm that achieves a constant\napproximation ratio with a relatively small additive factor. Additionally, we\nprovide experimental results demonstrating that our algorithm outperforms the\nstraightforward approach on both synthetically generated and real-world\ndatasets.", "comment": "accepted at DBSec 2025", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15224v1", "AI": {"title_translation": "局部差分隐私下无需超集假设的设施选址问题", "tldr": "本文在局部差分隐私框架下，提出了一种设施选址问题的改进算法，克服了现有方法对用户隐私的潜在损害，并在近似比和实际表现上取得了显著提升。", "motivation": "现有的差分隐私设施选址算法在原始问题上存在$\\Omega(\\sqrt{n})$的近似比下限，导致后续工作不得不采用可能损害用户隐私的超集假设。本文旨在解决这一问题，在不依赖超集假设的情况下，实现更好的近似比。", "method": "本文提出了一种设施选址问题的适配版本，并在局部差分隐私（LDP）框架下进行分析。设计了一个LDP算法，该算法在不使用超集假设的情况下，实现了常数近似比和相对较小的加性因子。", "result": "本文提出的LDP算法在无需超集假设的情况下，实现了常数近似比和相对较小的加性因子，这表明原始设施选址问题的$\\Omega(\\sqrt{n})$近似比下限不适用于本文的适配版本。实验结果表明，该算法在合成数据集和真实世界数据集上的表现优于直接方法。", "conclusion": "本文成功地在局部差分隐私框架下，提出了一种无需超集假设的设施选址问题解决方案，有效提升了用户隐私保护，并实现了理论和实践上的优越性能，证明了在特定问题适应下可以规避传统差分隐私的近似比限制。", "translation": "在本文中，我们引入了设施选址问题的一种改编版本，并在局部差分隐私（LDP）框架内对其进行了分析。在此模型下，我们确保了客户在特定位置存在的隐私。当n是点数时，Gupta等人为应用于原始设施选址问题的任何差分隐私算法建立了$\\Omega(\\sqrt{n})$的近似比下限。因此，后续工作采用了超集假设，但这可能会损害用户隐私。我们通过提出一种LDP算法来表明，该下限不适用于我们的改编版本，该算法以相对较小的加性因子实现了常数近似比。此外，我们提供了实验结果，表明我们的算法在合成生成和真实世界数据集上均优于直接方法。", "summary": "本文在局部差分隐私（LDP）框架下，对设施选址问题进行了改编和分析。针对现有差分隐私算法在原始问题上存在近似比下限且需依赖可能损害用户隐私的超集假设的问题，本文提出了一种新型LDP算法。该算法无需超集假设，即可实现常数近似比和较小的加性因子，并经实验证明在合成及真实数据集上均优于传统方法，有效提升了隐私保护下的性能。", "keywords": "设施选址问题, 局部差分隐私, 超集假设, 近似比, 隐私保护", "comments": "本文的创新之处在于，它解决了在局部差分隐私框架下，设施选址问题由于现有近似比下限而导致的对超集假设的依赖。通过对问题进行巧妙的适配，该研究成功地在不牺牲用户隐私（避免超集假设）的前提下，实现了常数近似比，这在理论上是一个显著的突破。同时，实验结果也验证了其在实际应用中的优越性，对于需要平衡隐私和效率的定位服务具有重要意义。"}}
{"id": "2506.15132", "title": "Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion", "authors": ["Yushi Wang", "Penghui Chen", "Xinyu Han", "Feng Wu", "Mingguo Zhao"], "summary": "Recent advancements in reinforcement learning (RL) have led to significant\nprogress in humanoid robot locomotion, simplifying the design and training of\nmotion policies in simulation. However, the numerous implementation details\nmake transferring these policies to real-world robots a challenging task. To\naddress this, we have developed a comprehensive code framework that covers the\nentire process from training to deployment, incorporating common RL training\nmethods, domain randomization, reward function design, and solutions for\nhandling parallel structures. This library is made available as a community\nresource, with detailed descriptions of its design and experimental results. We\nvalidate the framework on the Booster T1 robot, demonstrating that the trained\npolicies seamlessly transfer to the physical platform, enabling capabilities\nsuch as omnidirectional walking, disturbance resistance, and terrain\nadaptability. We hope this work provides a convenient tool for the robotics\ncommunity, accelerating the development of humanoid robots. The code can be\nfound in https://github.com/BoosterRobotics/booster_gym.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15132v1", "AI": {"title_translation": "Booster Gym：一个人形机器人运动的端到端强化学习框架", "tldr": "Booster Gym是一个端到端强化学习框架，旨在简化人形机器人运动策略从训练到实际部署的转移，并已在Booster T1机器人上验证其有效性。", "motivation": "当前强化学习在人形机器人运动方面取得了显著进展，但在将训练好的策略转移到真实机器人时，存在大量实现细节导致挑战。", "method": "开发了一个全面的代码框架，涵盖从训练到部署的整个过程，整合了常见的强化学习训练方法、领域随机化、奖励函数设计以及处理并行结构的解决方案。", "result": "该框架在Booster T1机器人上得到了验证，训练的策略能够无缝转移到物理平台，实现了全向行走、抗干扰和地形适应等能力。", "conclusion": "该工作为机器人社区提供了一个便捷工具，有望加速人形机器人的开发。", "translation": "近期强化学习（RL）的进步在人形机器人运动方面取得了显著进展，简化了仿真中运动策略的设计和训练。然而，大量的实现细节使得将这些策略转移到真实机器人成为一项具有挑战性的任务。为了解决这个问题，我们开发了一个全面的代码框架，涵盖从训练到部署的整个过程，整合了常见的RL训练方法、领域随机化、奖励函数设计以及处理并行结构的解决方案。该库作为一个社区资源提供，并详细描述了其设计和实验结果。我们在Booster T1机器人上验证了该框架，证明训练的策略能够无缝转移到物理平台，实现了全向行走、抗干扰和地形适应等能力。我们希望这项工作能为机器人社区提供一个便捷的工具，加速人形机器人的开发。代码可在https://github.com/BoosterRobotics/booster_gym找到。", "summary": "Booster Gym是一个为人形机器人运动设计的端到端强化学习框架。该框架旨在解决当前强化学习策略从仿真到真实机器人转移的挑战，通过整合训练、部署、领域随机化和奖励函数设计等功能，提供一个全面的解决方案。实验证明，该框架训练的策略可以无缝转移到Booster T1机器人上，实现全向行走、抗干扰和地形适应，为机器人社区提供了加速人形机器人开发的实用工具。", "keywords": "强化学习, 人形机器人, 运动控制, 端到端框架, 领域随机化", "comments": "这项工作的创新之处在于提供了一个全面的、端到端强化学习框架，显著简化了人形机器人运动策略从仿真到真实世界的转移过程。其重要性在于解决了实际部署中的关键障碍，并通过开源代码库的形式，为更广泛的机器人研究社区提供了实用工具，有望加速人形机器人技术的发展和应用。"}}
{"id": "2506.15314", "title": "Case Study for Developing a UXR Point of View for FinOps Product Innovation", "authors": ["Jason Dong", "Anna Wu"], "summary": "In the dynamic landscape of Cloud financial management, we are sharing a case\nstudy exploring the development of a User Experience Research (UXR) Point of\nView (PoV) to drive FinOps product innovation. We demonstrate how qualitative\nand quantitative research methods working together to navigate the challenges\nof understanding customer needs, aligning cross-functional teams, and\nprioritizing limited resources. Through a multi-phased research approach, the\nresearch team identifies opportunities, quantifies pain points, and segments\ndiverse customer cohorts. This culminated in a UXR PoV that informed the\ncreation of a differentiated product strategy, a 'one-stop shop' dashboard\nempowering FinOps practitioners with actionable insights and tools. This case\nstudy highlights the power of mixed-methods research in uncovering actionable\ninsights that drive impactful product innovation.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15314v1", "AI": {"title_translation": "为FinOps产品创新开发用户体验研究（UXR）视角的案例研究", "tldr": "本案例研究展示了如何通过用户体验研究（UXR）视角推动FinOps产品创新，并强调了混合研究方法的重要性。", "motivation": "在云财务管理动态环境中，理解客户需求、协调跨职能团队和优先分配有限资源面临挑战，因此需要开发用户体验研究（UXR）视角来驱动FinOps产品创新。", "method": "采用了定性和定量研究方法相结合的多阶段研究方法，包括识别机会、量化痛点和细分客户群，最终形成UXR视角。", "result": "成功开发了一个UXR视角，该视角指导了差异化产品策略的制定，并创建了一个“一站式”仪表板，为FinOps从业者提供可操作的见解和工具。", "conclusion": "混合方法研究在发现可操作的见解以推动有影响力的产品创新方面具有强大作用。", "translation": "在云计算财务管理的动态环境中，我们分享一个案例研究，探讨了如何开发用户体验研究（UXR）视角以推动FinOps产品创新。我们展示了定性和定量研究方法如何协同工作，以应对理解客户需求、协调跨职能团队和优先分配有限资源的挑战。通过多阶段研究方法，研究团队识别了机会，量化了痛点，并对不同的客户群体进行了细分。这最终形成了一个UXR视角，为创建差异化产品策略提供了信息，并创建了一个“一站式”仪表板，为FinOps从业者提供了可操作的见解和工具。本案例研究强调了混合方法研究在发现可操作的见解以推动有影响力的产品创新方面的强大作用。", "summary": "本案例研究探讨了在云财务管理领域，如何通过开发用户体验研究（UXR）视角来驱动FinOps产品创新。研究通过结合定性和定量方法，解决了理解客户需求、团队协作和资源分配的挑战，并成功识别了机会、量化了痛点、细分了客户。最终，形成的UXR视角指导了差异化产品策略的制定，并催生了一个赋能FinOps从业者的“一站式”仪表板。该研究强调了混合研究方法在推动产品创新中的重要性。", "keywords": "用户体验研究, FinOps, 产品创新, 案例研究, 混合方法研究", "comments": "这篇案例研究展示了UXR在实际产品开发中的强大应用，特别是通过混合方法论来解决复杂问题。其创新之处在于将UXR视角系统地应用于FinOps这一新兴领域，并成功地将研究成果转化为具体的、有影响力的产品（“一站式”仪表板）。这对于在技术和业务交叉领域进行产品创新提供了宝贵的实践经验。"}}
{"id": "2506.14854", "title": "Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis", "authors": ["Varun Mannam", "Zhenyu Shi"], "summary": "Accurate video annotation plays a vital role in modern retail applications,\nincluding customer behavior analysis, product interaction detection, and\nin-store activity recognition. However, conventional annotation methods heavily\nrely on time-consuming manual labeling by human annotators, introducing\nnon-robust frame selection and increasing operational costs. To address these\nchallenges in the retail domain, we propose a deep learning-based approach that\nautomates key-frame identification in retail videos and provides automatic\nannotations of products and customers. Our method leverages deep neural\nnetworks to learn discriminative features by embedding video frames and\nincorporating object detection-based techniques tailored for retail\nenvironments. Experimental results showcase the superiority of our approach\nover traditional methods, achieving accuracy comparable to human annotator\nlabeling while enhancing the overall efficiency of retail video annotation.\nRemarkably, our approach leads to an average of 2 times cost savings in video\nannotation. By allowing human annotators to verify/adjust less than 5% of\ndetected frames in the video dataset, while automating the annotation process\nfor the remaining frames without reducing annotation quality, retailers can\nsignificantly reduce operational costs. The automation of key-frame detection\nenables substantial time and effort savings in retail video labeling tasks,\nproving highly valuable for diverse retail applications such as shopper journey\nanalysis, product interaction detection, and in-store security monitoring.", "comment": "Submitting to ICCV 2025 workshop:\n  https://retailvisionworkshop.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14854v1", "AI": {"title_translation": "高效零售视频标注：一种用于产品和客户交互分析的鲁棒关键帧生成方法", "tldr": "该论文提出了一种基于深度学习的自动化方法，用于零售视频中的关键帧识别和自动标注，旨在显著降低标注成本并提高效率。", "motivation": "传统的零售视频标注方法高度依赖耗时的人工标注，导致帧选择不鲁棒且运营成本高昂，因此需要一种更高效、经济的解决方案。", "method": "本研究提出了一种基于深度学习的方法，利用深度神经网络学习判别性特征，并通过嵌入视频帧和整合针对零售环境定制的基于目标检测的技术，自动化零售视频中的关键帧识别并提供产品和客户的自动标注。", "result": "实验结果表明，该方法优于传统方法，实现了与人工标注员相当的准确性，并显著提高了零售视频标注的整体效率。该方法平均可节省2倍的视频标注成本，且在不降低标注质量的情况下，人工标注员只需验证/调整不到5%的检测帧。", "conclusion": "关键帧检测的自动化大大节省了零售视频标注任务的时间和精力，对于购物者旅程分析、产品交互检测和店内安全监控等多种零售应用具有极高价值。", "translation": "精确的视频标注在现代零售应用中扮演着至关重要的角色，包括客户行为分析、产品交互检测和店内活动识别。然而，传统的标注方法严重依赖耗时的人工标注员手动标记，引入了不鲁棒的帧选择并增加了运营成本。为了解决零售领域中的这些挑战，我们提出了一种基于深度学习的方法，该方法自动化零售视频中的关键帧识别，并提供产品和客户的自动标注。我们的方法利用深度神经网络通过嵌入视频帧和整合针对零售环境定制的基于目标检测的技术来学习判别性特征。实验结果表明，我们的方法优于传统方法，实现了与人工标注员标注相当的准确性，同时提高了零售视频标注的整体效率。值得注意的是，我们的方法平均可节省2倍的视频标注成本。通过允许人工标注员验证/调整视频数据集中不到5%的检测帧，同时在不降低标注质量的情况下自动化剩余帧的标注过程，零售商可以显著降低运营成本。关键帧检测的自动化大大节省了零售视频标注任务的时间和精力，对于购物者旅程分析、产品交互检测和店内安全监控等多种零售应用具有极高价值。", "summary": "本论文提出了一种创新的深度学习方法，旨在解决零售视频标注中人工操作耗时、成本高昂且不鲁棒的问题。该方法利用深度神经网络和目标检测技术，自动化识别零售视频中的关键帧，并对产品和客户进行自动标注。实验证明，该方法在保持与人工标注相当准确性的同时，显著提升了标注效率，实现了平均2倍的成本节约，且人工干预率低于5%，从而为零售商提供了高效且经济的视频分析解决方案。", "keywords": "零售视频标注, 关键帧生成, 深度学习, 目标检测, 成本节约", "comments": "该论文提出了一种创新性的方法，通过深度学习和自动化关键帧识别，显著提升了零售视频标注的效率并降低了成本。其核心创新在于将深度学习应用于零售场景下的视频标注自动化，尤其是在保证标注质量的同时实现2倍的成本节约，这对于零售行业具有重要的实际应用价值。该方法通过减少人工干预，有望在客户行为分析、产品交互检测和安全监控等领域发挥重要作用。"}}
{"id": "2207.01732", "title": "DEFORMER: Coupling Deformed Localized Patterns with Global Context for Robust End-to-end Speech Recognition", "authors": ["Jiamin Xie", "John H. L. Hansen"], "summary": "Convolutional neural networks (CNN) have improved speech recognition\nperformance greatly by exploiting localized time-frequency patterns. But these\npatterns are assumed to appear in symmetric and rigid kernels by the\nconventional CNN operation. It motivates the question: What about asymmetric\nkernels? In this study, we illustrate adaptive views can discover local\nfeatures which couple better with attention than fixed views of the input. We\nreplace depthwise CNNs in the Conformer architecture with a deformable\ncounterpart, dubbed this \"Deformer\". By analyzing our best-performing model, we\nvisualize both local receptive fields and global attention maps learned by the\nDeformer and show increased feature associations on the utterance level. The\nstatistical analysis of learned kernel offsets provides an insight into the\nchange of information in features with the network depth. Finally, replacing\nonly half of the layers in the encoder, the Deformer improves +5.6% relative\nWER without a LM and +6.4% relative WER with a LM over the Conformer baseline\non the WSJ eval92 set.", "comment": "Accepted to Interspeech 2022", "cate": "eess.AS", "url": "http://arxiv.org/abs/2207.01732v2", "AI": {"title_translation": "DEFORMER: 将变形的局部模式与全局上下文耦合用于鲁棒的端到端语音识别", "tldr": "DEFORMER通过将Conformer中的深度卷积网络替换为可变形对应物，解决了传统CNN在语音识别中对局部模式的对称和刚性假设问题，从而显著提高了端到端语音识别的性能。", "motivation": "传统的卷积神经网络（CNN）在语音识别中利用局部时频模式时，假设这些模式出现在对称和刚性核中。这引发了对非对称核如何处理的疑问，从而促使研究者探索自适应视图来发现更好地与注意力机制结合的局部特征。", "method": "本研究将Conformer架构中的深度卷积网络替换为可变形的对应物，命名为“Deformer”。通过分析性能最佳的模型，研究者可视化了Deformer学习到的局部感受野和全局注意力图，并对学习到的核偏移进行了统计分析。", "result": "在WSJ eval92数据集上，Deformer在不使用语言模型的情况下，相对Conformer基线将词错误率（WER）提高了+5.6%；在使用语言模型的情况下，相对WER提高了+6.4%。研究还表明，Deformer增加了语音层面的特征关联。", "conclusion": "Deformer通过引入可变形卷积，有效地捕获了非对称的局部模式，并与全局上下文更好地耦合，从而显著提升了端到端语音识别的性能。", "translation": "卷积神经网络（CNN）通过利用局部时频模式极大地提高了语音识别性能。但这些模式在传统CNN操作中被假定出现在对称和刚性核中。这引出了一个问题：非对称核如何处理？在本研究中，我们展示了自适应视图可以发现与输入固定视图相比，能更好地与注意力机制耦合的局部特征。我们将Conformer架构中的深度卷积网络替换为可变形的对应物，称之为“Deformer”。通过分析我们性能最佳的模型，我们可视化了Deformer学习到的局部感受野和全局注意力图，并展示了语音层面特征关联的增加。对学习到的核偏移的统计分析提供了对特征信息随网络深度变化的洞察。最后，仅替换编码器中一半的层，Deformer在WSJ eval92数据集上，在不使用语言模型的情况下，相对Conformer基线将词错误率（WER）提高了+5.6%；在使用语言模型的情况下，相对WER提高了+6.4%。", "summary": "本文提出了一种名为“Deformer”的新型架构，旨在解决传统CNN在语音识别中对局部时频模式的刚性假设。Deformer通过将Conformer中的深度卷积网络替换为可变形卷积，使其能够捕获非对称的局部特征，并更好地与全局上下文和注意力机制结合。实验结果表明，Deformer在WSJ eval92数据集上显著降低了词错误率，证明了其在鲁棒端到端语音识别中的有效性，并通过可视化和统计分析揭示了其捕获特征的能力。", "keywords": "语音识别, 可变形卷积, Conformer, 端到端, 词错误率", "comments": "Deformer的创新之处在于将可变形卷积引入到语音识别的Conformer架构中，解决了传统CNN在处理复杂和非对称时频模式时的局限性。这种方法使得模型能够自适应地捕获更丰富的局部特征，并与全局上下文更好地结合，从而显著提升了语音识别的性能。其通过替换部分层就获得显著提升的效率和效果，显示了其潜在的广泛应用价值。"}}
{"id": "2506.15150", "title": "Human Locomotion Implicit Modeling Based Real-Time Gait Phase Estimation", "authors": ["Yuanlong Ji", "Xingbang Yang", "Ruoqi Zhao", "Qihan Ye", "Quan Zheng", "Yubo Fan"], "summary": "Gait phase estimation based on inertial measurement unit (IMU) signals\nfacilitates precise adaptation of exoskeletons to individual gait variations.\nHowever, challenges remain in achieving high accuracy and robustness,\nparticularly during periods of terrain changes. To address this, we develop a\ngait phase estimation neural network based on implicit modeling of human\nlocomotion, which combines temporal convolution for feature extraction with\ntransformer layers for multi-channel information fusion. A channel-wise masked\nreconstruction pre-training strategy is proposed, which first treats gait phase\nstate vectors and IMU signals as joint observations of human locomotion, thus\nenhancing model generalization. Experimental results demonstrate that the\nproposed method outperforms existing baseline approaches, achieving a gait\nphase RMSE of $2.729 \\pm 1.071%$ and phase rate MAE of $0.037 \\pm 0.016%$ under\nstable terrain conditions with a look-back window of 2 seconds, and a phase\nRMSE of $3.215 \\pm 1.303%$ and rate MAE of $0.050 \\pm 0.023%$ under terrain\ntransitions. Hardware validation on a hip exoskeleton further confirms that the\nalgorithm can reliably identify gait cycles and key events, adapting to various\ncontinuous motion scenarios. This research paves the way for more intelligent\nand adaptive exoskeleton systems, enabling safer and more efficient human-robot\ninteraction across diverse real-world environments.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15150v1", "AI": {"title_translation": "人体运动隐式建模的实时步态相位估计", "tldr": "本文提出了一种基于人体运动隐式建模的神经网络，用于实时步态相位估计，在稳定地形和地形变化下均表现出高精度和鲁棒性，并能适应外骨骼系统。", "motivation": "现有的基于IMU信号的步态相位估计算法在准确性和鲁棒性方面仍面临挑战，尤其是在地形变化期间，这限制了外骨骼系统对个体步态变化的精确适应。", "method": "本文提出了一种基于人体运动隐式建模的步态相位估计神经网络，该网络结合了用于特征提取的时间卷积层和用于多通道信息融合的Transformer层。此外，还提出了一种通道级掩蔽重建预训练策略，将步态相位状态向量和IMU信号视为人体运动的联合观测值，以增强模型泛化能力。", "result": "该方法在稳定地形条件下（2秒回溯窗口）的步态相位RMSE为2.729 ± 1.071%，相位速率MAE为0.037 ± 0.016%；在地形过渡期，相位RMSE为3.215 ± 1.303%，速率MAE为0.050 ± 0.023%。实验结果表明，该方法优于现有基线方法。在髋关节外骨骼上的硬件验证进一步证实，该算法能够可靠识别步态周期和关键事件，适应各种连续运动场景。", "conclusion": "本研究为更智能和自适应的外骨骼系统铺平了道路，从而在各种真实世界环境中实现更安全、更高效的人机交互。", "translation": "基于惯性测量单元（IMU）信号的步态相位估计有助于外骨骼精确适应个体步态变化。然而，在实现高精度和鲁棒性方面仍存在挑战，尤其是在地形变化期间。为了解决这个问题，我们开发了一种基于人体运动隐式建模的步态相位估计神经网络，该网络结合了用于特征提取的时间卷积和用于多通道信息融合的Transformer层。提出了一种通道级掩蔽重建预训练策略，该策略首先将步态相位状态向量和IMU信号视为人体运动的联合观测值，从而增强模型泛化能力。实验结果表明，在稳定地形条件下，该方法优于现有基线方法，在2秒回溯窗口下，步态相位RMSE为2.729 ± 1.071%，相位速率MAE为0.037 ± 0.016%；在地形过渡期，相位RMSE为3.215 ± 1.303%，速率MAE为0.050 ± 0.023%。在髋关节外骨骼上的硬件验证进一步证实，该算法能够可靠识别步态周期和关键事件，适应各种连续运动场景。这项研究为更智能和自适应的外骨骼系统铺平了道路，从而在各种真实世界环境中实现更安全、更高效的人机交互。", "summary": "本文提出了一种基于人体运动隐式建模的神经网络，用于实时步态相位估计。该网络结合了时间卷积和Transformer层，并采用通道级掩蔽重建预训练策略以增强泛化能力。实验结果表明，该方法在稳定地形和地形变化下均表现出优于现有方法的精度和鲁棒性，并通过硬件验证确认其在外骨骼系统中的可靠性，为智能自适应外骨骼系统奠定基础。", "keywords": "步态相位估计, 隐式建模, 神经网络, 外骨骼, IMU信号", "comments": "本文的创新之处在于将人体运动的隐式建模与深度学习（时间卷积和Transformer）相结合，并引入了新颖的通道级掩蔽重建预训练策略，有效提升了步态相位估计在复杂地形下的准确性和泛化能力。这项研究对于发展更智能、更适应真实环境的外骨骼系统具有重要意义。"}}
{"id": "2506.14806", "title": "Heavy-Ball Momentum Method in Continuous Time and Discretization Error Analysis", "authors": ["Bochen Lyu", "Xiaojing Zhang", "Fangyi Zheng", "He Wang", "Zheng Wang", "Zhanxing Zhu"], "summary": "This paper establishes a continuous time approximation, a piece-wise\ncontinuous differential equation, for the discrete Heavy-Ball (HB) momentum\nmethod with explicit discretization error. Investigating continuous\ndifferential equations has been a promising approach for studying the discrete\noptimization methods. Despite the crucial role of momentum in gradient-based\noptimization methods, the gap between the original discrete dynamics and the\ncontinuous time approximations due to the discretization error has not been\ncomprehensively bridged yet. In this work, we study the HB momentum method in\ncontinuous time while putting more focus on the discretization error to provide\nadditional theoretical tools to this area. In particular, we design a\nfirst-order piece-wise continuous differential equation, where we add a number\nof counter terms to account for the discretization error explicitly. As a\nresult, we provide a continuous time model for the HB momentum method that\nallows the control of discretization error to arbitrary order of the step size.\nAs an application, we leverage it to find a new implicit regularization of the\ndirectional smoothness and investigate the implicit bias of HB for diagonal\nlinear networks, indicating how our results can be used in deep learning. Our\ntheoretical findings are further supported by numerical experiments.", "comment": "32 pages, 7 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14806v1", "AI": {"title_translation": "重球动量法在连续时间中的应用及离散化误差分析", "tldr": "该研究为离散重球动量法建立了一个考虑离散化误差的连续时间近似模型，并能将误差控制到任意阶。", "motivation": "尽管连续时间方法在研究离散优化算法方面很有前景，但离散动量法与连续时间近似之间因离散化误差造成的鸿沟尚未完全弥合。本文旨在弥合这一差距，为该领域提供额外的理论工具。", "method": "本文为离散重球动量（HB）方法建立了一个分段连续微分方程作为连续时间近似，并显式地考虑了离散化误差。具体来说，设计了一个一阶分段连续微分方程，加入了补偿项以显式地解释离散化误差。理论发现通过数值实验得到支持。", "result": "提供了重球动量法的一个连续时间模型，该模型允许将离散化误差控制到步长的任意阶。作为应用，利用该模型发现了方向平滑度的一种新的隐式正则化，并研究了HB对对角线性网络的隐式偏差，展示了其在深度学习中的应用潜力。", "conclusion": "本文成功地为重球动量法提供了一个能够精确控制离散化误差的连续时间模型，弥补了离散与连续动力学之间的差距，并展示了其在分析优化算法隐式偏置方面的应用价值。", "translation": "本文为离散重球（HB）动量法建立了一个连续时间近似，即一个分段连续微分方程，并显式地考虑了离散化误差。研究连续微分方程一直是研究离散优化方法的一个有前景的方法。尽管动量在基于梯度的优化方法中扮演着关键角色，但由于离散化误差，原始离散动力学与连续时间近似之间的差距尚未得到全面弥合。在这项工作中，我们研究了连续时间中的HB动量法，并更加关注离散化误差，以期为该领域提供额外的理论工具。特别是，我们设计了一个一阶分段连续微分方程，其中我们添加了许多补偿项以显式地解释离散化误差。因此，我们为HB动量法提供了一个连续时间模型，该模型允许将离散化误差控制到步长的任意阶。作为一项应用，我们利用它来发现方向平滑度的一种新的隐式正则化，并研究了HB对对角线性网络的隐式偏差，表明我们的结果如何应用于深度学习。我们的理论发现得到了数值实验的进一步支持。", "summary": "本文针对离散重球（HB）动量法，建立了一个带有显式离散化误差的分段连续微分方程作为连续时间近似。通过设计一个包含补偿项的一阶分段连续微分方程，该研究提供了一个能够将离散化误差控制到步长任意阶的连续时间模型。此外，该模型还被应用于发现方向平滑度的新隐式正则化，并分析了HB在对角线性网络中的隐式偏差，为深度学习提供了理论见解。数值实验进一步验证了理论发现。", "keywords": "重球动量法, 连续时间近似, 离散化误差, 隐式正则化, 深度学习", "comments": "这项工作创新性地通过引入补偿项，显式地解决了离散动量法在连续时间近似中长期存在的离散化误差问题，极大地提升了连续时间模型对离散优化算法的解释和预测能力。其在误差控制上的任意阶能力以及在深度学习隐式偏置分析中的应用潜力，显示了其重要的理论和实践价值。"}}
{"id": "2506.15557", "title": "Construction of an Organ Shape Atlas Using a Hierarchical Mesh Variational Autoencoder", "authors": ["Zijie Wang", "Ryuichi Umehara", "Mitsuhiro Nakamura", "Megumi Nakao"], "summary": "An organ shape atlas, which represents the shape and position of the organs\nand skeleton of a living body using a small number of parameters, is expected\nto have a wide range of clinical applications, including intraoperative\nguidance and radiotherapy. Because the shape and position of soft organs vary\ngreatly among patients, it is difficult for linear models to reconstruct shapes\nthat have large local variations. Because it is difficult for conventional\nnonlinear models to control and interpret the organ shapes obtained, deep\nlearning has been attracting attention in three-dimensional shape\nrepresentation. In this study, we propose an organ shape atlas based on a mesh\nvariational autoencoder (MeshVAE) with hierarchical latent variables. To\nrepresent the complex shapes of biological organs and nonlinear shape\ndifferences between individuals, the proposed method maintains the performance\nof organ shape reconstruction by hierarchizing latent variables and enables\nshape representation using lower-dimensional latent variables. Additionally,\ntemplates that define vertex correspondence between different resolutions\nenable hierarchical representation in mesh data and control the global and\nlocal features of the organ shape. We trained the model using liver and stomach\norgan meshes obtained from 124 cases and confirmed that the model reconstructed\nthe position and shape with an average distance between vertices of 1.5 mm and\nmean distance of 0.7 mm for the liver shape, and an average distance between\nvertices of 1.4 mm and mean distance of 0.8 mm for the stomach shape on test\ndata from 19 of cases. The proposed method continuously represented\ninterpolated shapes, and by changing latent variables at different hierarchical\nlevels, the proposed method hierarchically separated shape features compared\nwith PCA.", "comment": null, "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15557v1", "AI": {"title_translation": "使用分层网格变分自编码器构建器官形状图谱", "tldr": "本研究提出了一种基于分层网格变分自编码器（MeshVAE）的器官形状图谱，能够用少量参数表示复杂的器官形状和位置，并支持分层控制全局和局部特征，在肝脏和胃的形状重建上表现出高精度，具有潜在的临床应用价值。", "motivation": "器官形状图谱在术中指导和放射治疗等临床应用中具有广泛前景。然而，软器官形状和位置的巨大个体差异使得线性模型难以重建具有大局部变化的形状，而传统非线性模型又难以控制和解释所得器官形状。因此，深度学习在三维形状表示方面受到关注。", "method": "本研究提出了一种基于带有分层潜在变量的网格变分自编码器（MeshVAE）的器官形状图谱。为了表示生物器官的复杂形状和个体间的非线性形状差异，该方法通过将潜在变量分层来保持器官形状重建性能，并使用低维潜在变量实现形状表示。此外，定义不同分辨率之间顶点对应关系的模板实现了网格数据的分层表示，并控制了器官形状的全局和局部特征。", "result": "模型使用来自124个病例的肝脏和胃器官网格进行训练。在19个测试病例中，肝脏形状的顶点平均距离为1.5毫米，平均距离为0.7毫米；胃形状的顶点平均距离为1.4毫米，平均距离为0.8毫米，证实了模型能够精确重建位置和形状。所提出的方法能够连续表示插值形状，并且通过改变不同层次的潜在变量，与PCA相比，该方法能够分层分离形状特征。", "conclusion": "本研究成功构建了一个基于分层网格变分自编码器（MeshVAE）的器官形状图谱，该图谱能够有效表示复杂的生物器官形状，实现对全局和局部形状特征的分层控制，并在肝脏和胃的形状重建中展现出高精度，克服了传统方法的局限性。", "translation": "器官形状图谱通过少量参数表示活体器官和骨骼的形状和位置，预计在术中指导和放射治疗等临床应用中具有广泛前景。由于软器官的形状和位置在患者之间差异巨大，线性模型难以重建具有大局部变化的形状。而传统非线性模型难以控制和解释所得的器官形状，因此深度学习在三维形状表示方面受到关注。在本研究中，我们提出了一种基于带有分层潜在变量的网格变分自编码器（MeshVAE）的器官形状图谱。为了表示生物器官的复杂形状和个体间的非线性形状差异，所提出的方法通过将潜在变量分层来保持器官形状重建性能，并使用低维潜在变量实现形状表示。此外，定义不同分辨率之间顶点对应关系的模板实现了网格数据的分层表示，并控制了器官形状的全局和局部特征。我们使用从124个病例中获得的肝脏和胃器官网格训练了该模型，并证实该模型在19个测试病例的测试数据上，肝脏形状的顶点平均距离为1.5毫米，平均距离为0.7毫米；胃形状的顶点平均距离为1.4毫米，平均距离为0.8毫米，重建了位置和形状。所提出的方法连续地表示了插值形状，并且通过改变不同层次的潜在变量，与PCA相比，所提出的方法分层分离了形状特征。", "summary": "本研究提出了一种基于分层网格变分自编码器（MeshVAE）的器官形状图谱，旨在解决传统模型在表示具有大个体差异的复杂器官形状时的局限性。该方法通过引入分层潜在变量，不仅提高了器官形状重建的性能，还能够使用低维潜在变量进行形状表示，并通过不同分辨率的模板控制器官形状的全局和局部特征。模型在肝脏和胃的网格数据上进行了训练和验证，显示出高精度的形状和位置重建能力，并能有效分层分离形状特征，为临床应用提供了新的可能性。", "keywords": "器官形状图谱, 网格变分自编码器, 分层潜在变量, 深度学习, 三维形状表示", "comments": "该论文的创新点在于引入了分层网格变分自编码器（MeshVAE）来构建器官形状图谱，有效解决了传统线性模型难以处理大局部变化以及传统非线性模型难以控制和解释形状的问题。通过分层潜在变量和多分辨率模板，实现了对复杂生物器官形状的高效表示和对全局与局部特征的精细控制，这对于需要精确三维形状重建的临床应用（如术中指导和放射治疗）具有重要意义。其在肝脏和胃数据上的验证结果也证实了方法的有效性和精度。"}}
{"id": "2506.15156", "title": "Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of View", "authors": ["Muhammad Cendekia Airlangga", "Hilal AlQuabeh", "Munachiso S Nwadike", "Kentaro Inui"], "summary": "We study memory in state-space language models using primacy and recency\neffects as behavioral tools to uncover how information is retained and\nforgotten over time. Applying structured recall tasks to the Mamba\narchitecture, we observe a consistent U-shaped accuracy profile, indicating\nstrong performance at the beginning and end of input sequences. We identify\nthree mechanisms that give rise to this pattern. First, long-term memory is\nsupported by a sparse subset of channels within the model's selective state\nspace block, which persistently encode early input tokens and are causally\nlinked to primacy effects. Second, short-term memory is governed by\ndelta-modulated recurrence: recent inputs receive more weight due to\nexponential decay, but this recency advantage collapses when distractor items\nare introduced, revealing a clear limit to memory depth. Third, we find that\nmemory allocation is dynamically modulated by semantic regularity: repeated\nrelations in the input sequence shift the delta gating behavior, increasing the\ntendency to forget intermediate items. We validate these findings via targeted\nablations and input perturbations on two large-scale Mamba-based language\nmodels: one with 1.4B and another with 7B parameters.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15156v1", "AI": {"title_translation": "Mamba中首因效应和近因效应的出现：一个机制的视角", "tldr": "研究Mamba模型中首因和近因效应的出现及其潜在机制，发现长短期记忆由不同机制支持，且记忆分配受语义规则动态调节。", "motivation": "本研究旨在利用首因效应和近因效应作为行为工具，揭示状态空间语言模型（特别是Mamba架构）如何随时间保留和遗忘信息。", "method": "研究人员对Mamba架构应用结构化回忆任务，观察其准确率表现。通过识别导致U形准确率曲线的三种机制，并通过目标消融和输入扰动在1.4B和7B参数的Mamba语言模型上验证了这些发现。", "result": "观察到Mamba模型在输入序列的开始和结束处表现出U形准确率曲线。识别出三种机制：1. 长时记忆由模型选择性状态空间块内稀疏通道支持，编码早期输入标记并与首因效应相关。2. 短时记忆由delta调制循环控制，最近输入获得更多权重，但其优势在引入干扰项时崩溃，揭示了记忆深度限制。3. 记忆分配受语义规则动态调节，重复关系会改变delta门控行为，增加遗忘中间项的倾向。", "conclusion": "Mamba模型中首因和近因效应的出现可归因于特定的长短期记忆机制，即稀疏通道支持长时记忆和delta调制循环控制短时记忆，以及记忆分配受语义规则的动态调节，这些机制共同解释了信息在模型中的保留和遗忘。", "translation": "我们利用首因效应和近因效应作为行为工具，研究状态空间语言模型中的记忆，以揭示信息如何随时间保留和遗忘。将结构化回忆任务应用于Mamba架构，我们观察到一致的U形准确率曲线，表明在输入序列的开始和结束处表现出色。我们确定了导致这种模式出现的三种机制。首先，长期记忆由模型选择性状态空间块内稀疏的通道子集支持，这些通道持续编码早期输入标记，并与首因效应有因果关系。其次，短期记忆由delta调制循环控制：由于指数衰减，最近的输入获得更多权重，但当引入干扰项时，这种近因优势会崩溃，这揭示了记忆深度的明确限制。第三，我们发现记忆分配受语义规则动态调节：输入序列中重复的关系会改变delta门控行为，增加遗忘中间项的倾向。我们通过对两个大型Mamba语言模型（一个1.4B参数，另一个7B参数）进行目标消融和输入扰动来验证这些发现。", "summary": "本研究通过结构化回忆任务，探究了Mamba状态空间语言模型中首因效应和近因效应的出现机制。研究发现Mamba表现出U形准确率曲线，并揭示了三种核心机制：稀疏通道支持长时记忆（首因效应）；delta调制循环控制短时记忆（近因效应），但有记忆深度限制；以及语义规则动态调节记忆分配。这些发现通过对大型Mamba模型的消融实验得到验证。", "keywords": "Mamba, 首因效应, 近因效应, 记忆机制, 状态空间模型", "comments": "这篇论文通过深入分析Mamba架构中的记忆机制，为理解其在处理长序列时的行为提供了宝贵的见解。识别出长短期记忆的不同支持机制以及语义规则对记忆分配的动态影响，对于优化状态空间模型在复杂语言任务中的性能具有重要意义，有助于未来改进模型设计。"}}
{"id": "2506.15185", "title": "Heterogeneous and anisotropic elastic parameter estimation using a novel semi-analytical forward solver", "authors": ["Xiaopeng Zhu", "Zhongyi Huang"], "summary": "An efficient procedure using a novel semi-analytical forward solver for\nidentifying heterogeneous and anisotropic elastic parameters from only one\nfull-field measurement is proposed and explored. We formulate the inverse\nproblem as an special energy functional minimization with total variation(TV)\nregularization. The minimization problem is solved by Adam algorithm, which\nonly requires solving one forward problem and no adjoint problem in each\niteration. In order to deal with the irregularity of the elastic regions, the\nanisotropy and heterogeneity of parameters and potential singularities in\nforward-modeled issues, a novel semi-analytical forward solver named the direct\nmethod of lines is proposed, which discretizes angular variable while\npreserving analytical solutions along remaining coordinates. To validate the\nefficacy of our procedure, a series of numerical experiments are implemented\nsubsequently, achieving reliable performance in both forward modeling and the\nsix elastic arguments reconstruction scenarios.", "comment": "26 pages,9 figures", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15185v1", "AI": {"title_translation": "使用新型半解析正向求解器进行非均匀各向异性弹性参数估计", "tldr": "提出了一种使用新型半解析正向求解器通过单次全场测量估计非均匀各向异性弹性参数的高效方法。", "motivation": "从单次全场测量中识别非均匀和各向异性弹性参数是一个挑战。现有方法可能难以处理弹性区域的不规则性、参数的各向异性和非均匀性以及正向建模中潜在的奇异性。", "method": "该研究提出了一种高效的程序。它将逆问题表述为带有全变分(TV)正则化的特殊能量泛函最小化问题。最小化问题通过Adam算法求解，该算法在每次迭代中仅需解决一个正向问题，无需伴随问题。为了处理弹性区域的不规则性、参数的各向异性和非均匀性以及正向建模中潜在的奇异性，提出了一种名为“直接线法”的新型半解析正向求解器，该求解器离散化角度变量，同时保留沿其余坐标的解析解。", "result": "通过一系列数值实验验证了该程序的有效性，在正向建模和六个弹性参数重建场景中均取得了可靠的性能。", "conclusion": "该研究成功开发了一种高效且可靠的方法，通过单次全场测量估计非均匀和各向异性弹性参数，尤其适用于处理复杂区域和潜在奇异性问题。", "translation": "提出并探索了一种使用新型半解析正向求解器，仅通过一次全场测量识别非均匀和各向异性弹性参数的高效程序。我们将逆问题表述为带有全变分(TV)正则化的特殊能量泛函最小化问题。最小化问题通过Adam算法求解，该算法在每次迭代中仅需解决一个正向问题，无需伴随问题。为了处理弹性区域的不规则性、参数的各向异性和非均匀性以及正向建模中潜在的奇异性，提出了一种名为“直接线法”的新型半解析正向求解器，该求解器离散化角度变量，同时保留沿其余坐标的解析解。随后进行了一系列数值实验来验证该程序的有效性，在正向建模和六个弹性参数重建场景中均取得了可靠的性能。", "summary": "该论文提出了一种高效程序，用于通过单次全场测量估计非均匀和各向异性弹性参数。该方法将逆问题建模为带有TV正则化的能量泛函最小化，并使用Adam算法求解，其特点是每次迭代只需解决一个正向问题。为应对复杂弹性区域、参数异构性和奇异性，引入了名为“直接线法”的新型半解析正向求解器。数值实验验证了该方法在正向建模和弹性参数重建中的可靠性。", "keywords": "弹性参数估计, 半解析求解器, 非均匀各向异性, 逆问题, Adam算法", "comments": "该研究的创新之处在于提出了“直接线法”这一新型半解析正向求解器，有效解决了弹性区域不规则性、参数非均匀性和潜在奇异性等挑战。结合Adam算法，实现了高效的逆问题求解，且仅需单次全场测量，具有较高的实际应用价值。"}}
{"id": "2506.15253", "title": "RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments", "authors": ["Yuchuan Fu", "Xiaohan Yuan", "Dongxia Wang"], "summary": "The rapid deployment of Large language model (LLM) agents in critical domains\nlike healthcare and finance necessitates robust security frameworks. To address\nthe absence of standardized evaluation benchmarks for these agents in dynamic\nenvironments, we introduce RAS-Eval, a comprehensive security benchmark\nsupporting both simulated and real-world tool execution. RAS-Eval comprises 80\ntest cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration\n(CWE) categories, with tools implemented in JSON, LangGraph, and Model Context\nProtocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse\nscenarios, revealing significant vulnerabilities: attacks reduced agent task\ncompletion rates (TCR) by 36.78% on average and achieved an 85.65% success rate\nin academic settings. Notably, scaling laws held for security capabilities,\nwith larger models outperforming smaller counterparts. Our findings expose\ncritical risks in real-world agent deployments and provide a foundational\nframework for future security research. Code and data are available at\nhttps://github.com/lanzer-tree/RAS-Eval.", "comment": "12 pages, 8 figures", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15253v1", "AI": {"title_translation": "RAS-Eval：一个用于真实世界环境中LLM智能体安全评估的综合基准", "tldr": "引入RAS-Eval，一个LLM智能体安全评估基准，支持模拟和真实世界工具执行，揭示了当前LLM智能体在安全方面的显著漏洞。", "motivation": "随着LLM智能体在医疗和金融等关键领域的快速部署，需要强大的安全框架；目前缺乏针对这些智能体在动态环境中标准化评估基准。", "method": "引入RAS-Eval，一个综合安全基准，支持模拟和真实世界工具执行。它包含80个测试用例和3,802个攻击任务，映射到11个CWE类别，工具以JSON、LangGraph和MCP格式实现。评估了6个最先进的LLM。", "result": "攻击平均使智能体任务完成率(TCR)降低36.78%；在学术设置中攻击成功率达到85.65%；安全能力遵循缩放定律，大型模型表现优于小型模型。", "conclusion": "结果揭示了真实世界智能体部署中的关键风险，并为未来的安全研究提供了基础框架。", "translation": "大型语言模型（LLM）智能体在医疗和金融等关键领域的快速部署，需要强大的安全框架。为了解决在动态环境中缺乏针对这些智能体的标准化评估基准的问题，我们引入了RAS-Eval，一个支持模拟和真实世界工具执行的综合安全基准。RAS-Eval包含80个测试用例和3,802个攻击任务，映射到11个通用弱点枚举（CWE）类别，工具以JSON、LangGraph和模型上下文协议（MCP）格式实现。我们评估了6个最先进的LLM在不同场景下的表现，揭示了显著的漏洞：攻击平均使智能体任务完成率（TCR）降低了36.78%，并在学术设置中达到了85.65%的成功率。值得注意的是，安全能力遵循缩放定律，大型模型表现优于小型模型。我们的发现揭示了真实世界智能体部署中的关键风险，并为未来的安全研究提供了基础框架。代码和数据可在https://github.com/lanzer-tree/RAS-Eval获取。", "summary": "本文介绍了RAS-Eval，一个针对大型语言模型（LLM）智能体在真实世界环境中进行安全评估的综合基准。该基准包含80个测试用例和3802个攻击任务，覆盖11个CWE类别，并支持多种工具格式。通过对6个最先进LLM的评估，研究发现攻击可显著降低智能体任务完成率，并在学术场景中取得高成功率，揭示了LLM智能体在实际部署中的严重安全漏洞。", "keywords": "LLM智能体安全, RAS-Eval, 安全评估, 漏洞, CWE", "comments": "RAS-Eval的创新之处在于其提供了首个支持真实世界工具执行的LLM智能体安全评估基准，填补了现有空白。其发现的显著漏洞强调了在关键领域部署LLM智能体时加强安全框架的紧迫性，为未来的LLM安全研究奠定了基础。"}}
{"id": "2506.15146", "title": "TACT: Humanoid Whole-body Contact Manipulation through Deep Imitation Learning with Tactile Modality", "authors": ["Masaki Murooka", "Takahiro Hoshi", "Kensuke Fukumitsu", "Shimpei Masuda", "Marwan Hamze", "Tomoya Sasaki", "Mitsuharu Morisawa", "Eiichi Yoshida"], "summary": "Manipulation with whole-body contact by humanoid robots offers distinct\nadvantages, including enhanced stability and reduced load. On the other hand,\nwe need to address challenges such as the increased computational cost of\nmotion generation and the difficulty of measuring broad-area contact. We\ntherefore have developed a humanoid control system that allows a humanoid robot\nequipped with tactile sensors on its upper body to learn a policy for\nwhole-body manipulation through imitation learning based on human teleoperation\ndata. This policy, named tactile-modality extended ACT (TACT), has a feature to\ntake multiple sensor modalities as input, including joint position, vision, and\ntactile measurements. Furthermore, by integrating this policy with retargeting\nand locomotion control based on a biped model, we demonstrate that the\nlife-size humanoid robot RHP7 Kaleido is capable of achieving whole-body\ncontact manipulation while maintaining balance and walking. Through detailed\nexperimental verification, we show that inputting both vision and tactile\nmodalities into the policy contributes to improving the robustness of\nmanipulation involving broad and delicate contact.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15146v1", "AI": {"title_translation": "TACT：基于触觉模态的深度模仿学习实现人形机器人全身接触操作", "tldr": "本文开发了一种名为TACT的人形机器人控制策略，通过深度模仿学习和多模态（包括触觉）输入，使机器人能够进行全身接触操作，并提高了操作的鲁棒性。", "motivation": "人形机器人全身接触操作具有增强稳定性和降低负载的优势，但面临运动生成计算成本高和广域接触测量困难的挑战。", "method": "开发了一种人形机器人控制系统，使配备触觉传感器的人形机器人通过基于人类遥操作数据的模仿学习来学习全身操作策略。该策略（TACT）可接收关节位置、视觉和触觉等多种传感器模态输入。此外，该策略与基于双足模型的重定向和运动控制相结合。", "result": "成功展示了全尺寸人形机器人RHP7 Kaleido能够实现全身接触操作，同时保持平衡和行走。实验验证表明，将视觉和触觉模态同时输入策略有助于提高涉及广泛和精细接触操作的鲁棒性。", "conclusion": "通过将视觉和触觉模态输入策略，可以显著提高人形机器人全身接触操作的鲁棒性，使其在保持平衡和行走的同时进行复杂的操纵。", "translation": "人形机器人通过全身接触进行操作具有独特的优势，包括增强稳定性和降低负载。另一方面，我们需要解决运动生成计算成本增加以及广域接触测量困难等挑战。因此，我们开发了一种人形机器人控制系统，该系统允许配备上半身触觉传感器的人形机器人通过基于人类遥操作数据的模仿学习来学习全身操作策略。该策略名为触觉模态扩展ACT（TACT），具有接收多种传感器模态作为输入的功能，包括关节位置、视觉和触觉测量。此外，通过将该策略与基于双足模型的重定向和运动控制相结合，我们证明了全尺寸人形机器人RHP7 Kaleido能够实现全身接触操作，同时保持平衡和行走。通过详细的实验验证，我们表明将视觉和触觉模态同时输入策略有助于提高涉及广泛和精细接触操作的鲁棒性。", "summary": "本文提出了一种名为TACT的人形机器人控制策略，旨在实现全身接触操作。该策略通过深度模仿学习，利用人类遥操作数据，并整合了关节位置、视觉和触觉等多种传感器模态。研究人员将TACT策略与重定向和运动控制相结合，成功使人形机器人RHP7 Kaleido在保持平衡和行走的同时进行全身接触操作。实验结果表明，结合视觉和触觉输入显著提升了机器人处理广泛和精细接触操作的鲁棒性。", "keywords": "人形机器人, 全身接触操作, 深度模仿学习, 触觉模态, 鲁棒性", "comments": "这项研究的创新之处在于将触觉模态深度融入到人形机器人全身接触操作的模仿学习框架中，并成功在实际机器人上进行了验证。通过结合视觉和触觉信息，显著提升了机器人在复杂接触任务中的鲁棒性，为未来人形机器人的灵巧操作和人机交互奠定了基础。"}}
{"id": "2506.15325", "title": "Human-Centred AI in FinTech: Developing a User Experience (UX) Research Point of View (PoV) Playbook", "authors": ["Festus Adedoyin", "Huseyin Dogan"], "summary": "Advancements in Artificial Intelligence (AI) have significantly transformed\nthe financial industry, enabling the development of more personalised and\nadaptable financial products and services. This research paper explores various\ninstances where Human-Centred AI (HCAI) has facilitated these advancements,\ndrawing from contemporary studies and industry progress. The paper examines how\nthe application of HCAI-powered data analytics, machine learning, and natural\nlanguage processing enables financial institutions to gain a deeper\nunderstanding of their customers' unique needs, preferences, and behavioural\npatterns. This, in turn, allows for the creation of tailored financial\nsolutions that address individual consumer requirements, ultimately enhancing\noverall user experience and satisfaction. Additionally, the study highlights\nthe integration of AI-powered robo-advisory services, which offer customised\ninvestment recommendations and portfolio management tailored to diverse risk\nprofiles and investment goals. Moreover, the paper underscores the role of AI\nin strengthening fraud detection, risk assessment, and regulatory compliance,\nleading to a more secure and adaptable financial landscape. The findings of\nthis research demonstrate the substantial impact of Human-Centred AI on the\nfinancial industry, offering a strategic framework for financial institutions\nto leverage these technologies. By incorporating a User Experience Research\n(UXR) Point of View (PoV), financial institutions can ensure that AI-driven\nsolutions align with user needs and business objectives.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15325v1", "AI": {"title_translation": "金融科技中的以人为本人工智能：开发用户体验 (UX) 研究视角 (PoV) 手册", "tldr": "本文探讨了以人为本的人工智能 (HCAI) 如何通过数据分析、机器学习和自然语言处理在金融科技中提升用户体验、提供个性化服务、强化风险管理和合规性，并提出了一个用户体验研究视角 (UXR PoV) 框架。", "motivation": "探讨以人为本的人工智能 (HCAI) 如何在金融行业中促进个性化、适应性强的金融产品和服务的发展，并帮助金融机构深入了解客户需求。", "method": "本研究通过探索当代研究和行业进展，审视了HCAI驱动的数据分析、机器学习和自然语言处理的应用，以及AI驱动的机器人顾问服务和AI在欺诈检测、风险评估和监管合规中的作用。", "result": "研究表明，以人为本的人工智能对金融行业产生了重大影响，它使金融机构能够更深入地理解客户需求，创建量身定制的金融解决方案，提升用户体验和满意度，并加强欺诈检测、风险评估和监管合规。", "conclusion": "结论是以人为本的人工智能对金融业有实质性影响，提供了一个战略框架，金融机构可以通过整合用户体验研究视角，确保AI驱动的解决方案符合用户需求和业务目标。", "translation": "人工智能 (AI) 的进步显著改变了金融行业，使得开发更个性化和适应性强的金融产品和服务成为可能。本研究论文借鉴当代研究和行业进展，探讨了以人为本人工智能 (HCAI) 促进这些进步的各种实例。论文审视了HCAI驱动的数据分析、机器学习和自然语言处理的应用如何使金融机构更深入地了解客户的独特需求、偏好和行为模式。这反过来又允许创建满足个体消费者需求的量身定制的金融解决方案，最终提升整体用户体验和满意度。此外，本研究强调了AI驱动的机器人顾问服务的整合，这些服务提供根据不同风险偏好和投资目标量身定制的投资建议和投资组合管理。此外，论文强调了AI在加强欺诈检测、风险评估和监管合规方面的作用，从而形成一个更安全、更适应性强的金融格局。本研究的结果表明，以人为本的人工智能对金融行业产生了实质性影响，为金融机构利用这些技术提供了一个战略框架。通过融入用户体验研究 (UXR) 视角 (PoV)，金融机构可以确保AI驱动的解决方案与用户需求和业务目标保持一致。", "summary": "本研究探讨了以人为本的人工智能 (HCAI) 在金融科技领域的应用及其对行业转型的作用。文章阐述了HCAI如何通过数据分析、机器学习和自然语言处理，帮助金融机构深入理解客户需求，从而提供个性化金融产品、提升用户体验。此外，论文还讨论了AI在机器人顾问服务、欺诈检测、风险评估和监管合规方面的贡献。研究强调HCAI对金融业的重大影响，并提出整合用户体验研究视角对于确保AI解决方案符合用户需求和业务目标的重要性。", "keywords": "以人为本人工智能, 金融科技, 用户体验, 机器人顾问, 风险管理", "comments": "本文的创新之处在于提出了一个用户体验研究视角 (UXR PoV) 框架，以指导金融机构在AI驱动的解决方案中融入以人为本的设计理念。其重要性在于强调了AI在提升金融服务个性化、用户满意度和风险管理方面的潜力，并为行业提供了实用指导。"}}
{"id": "2506.14856", "title": "Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction", "authors": ["Zhengquan Zhang", "Feng Xu", "Mengmi Zhang"], "summary": "Some perspectives naturally provide more information than others. How can an\nAI system determine which viewpoint offers the most valuable insight for\naccurate and efficient 3D object reconstruction? Active view selection (AVS)\nfor 3D reconstruction remains a fundamental challenge in computer vision. The\naim is to identify the minimal set of views that yields the most accurate 3D\nreconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian\nSplatting, from a current observation and computing uncertainty for each\ncandidate viewpoint, we introduce a novel AVS approach guided by neural\nuncertainty maps predicted by a lightweight feedforward deep neural network,\nnamed UPNet. UPNet takes a single input image of a 3D object and outputs a\npredicted uncertainty map, representing uncertainty values across all possible\ncandidate viewpoints. By leveraging heuristics derived from observing many\nnatural objects and their associated uncertainty patterns, we train UPNet to\nlearn a direct mapping from viewpoint appearance to uncertainty in the\nunderlying volumetric representations. Next, our approach aggregates all\npreviously predicted neural uncertainty maps to suppress redundant candidate\nviewpoints and effectively select the most informative one. Using these\nselected viewpoints, we train 3D neural rendering models and evaluate the\nquality of novel view synthesis against other competitive AVS methods.\nRemarkably, despite using half of the viewpoints than the upper bound, our\nmethod achieves comparable reconstruction accuracy. In addition, it\nsignificantly reduces computational overhead during AVS, achieving up to a 400\ntimes speedup along with over 50\\% reductions in CPU, RAM, and GPU usage\ncompared to baseline methods. Notably, our approach generalizes effectively to\nAVS tasks involving novel object categories, without requiring any additional\ntraining.", "comment": "9 pages, 3 figures in the main text. Under review for NeurIPS 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14856v1", "AI": {"title_translation": "窥探未知：基于神经不确定性图的主动视角选择用于三维重建", "tldr": "提出一种基于UPNet神经网络不确定性图的主动视角选择方法，用于高效准确的三维重建，显著提高速度并降低资源消耗，同时保持重建精度。", "motivation": "在三维重建中，如何选择最能提供信息的视角是一个基本挑战，目标是找到最少且信息量最大的视角集合以实现准确高效的三维重建。", "method": "该方法引入了一种新的主动视角选择（AVS）方法，由轻量级前馈深度神经网络UPNet预测的神经不确定性图指导。UPNet接收单个三维物体输入图像，输出表示所有可能候选视角不确定性值的预测不确定性图。通过从观察许多自然物体及其相关不确定性模式中获得的启发式方法训练UPNet，使其学习从视角外观到底层体素表示不确定性的直接映射。然后，该方法聚合所有先前预测的神经不确定性图，以抑制冗余候选视角并有效选择信息量最大的视角。", "result": "尽管使用的视角数量是上限的一半，但该方法实现了可比较的重建精度。此外，它显著降低了AVS期间的计算开销，与基线方法相比，速度提高了400倍，CPU、RAM和GPU使用率降低了50%以上。该方法还能有效泛化到涉及新物体类别的主动视角选择任务，无需额外训练。", "conclusion": "本文提出的基于神经不确定性图的主动视角选择方法，在保持高重建精度的同时，显著提高了三维重建的效率并降低了计算成本，并且具有良好的泛化能力。", "translation": "有些视角自然比其他视角提供更多信息。人工智能系统如何确定哪个视角能为准确高效的三维物体重建提供最有价值的洞察？用于三维重建的主动视角选择（AVS）仍然是计算机视觉领域的一个基本挑战。其目标是识别最少量的视角集合，以产生最准确的三维重建。我们没有像NeRF或3D Gaussian Splatting那样从当前观察中学习辐射场并计算每个候选视角的不确定性，而是引入了一种由轻量级前馈深度神经网络UPNet预测的神经不确定性图引导的新型AVS方法。UPNet接收单个三维物体的输入图像，并输出一个预测的不确定性图，表示所有可能候选视角的不确定性值。通过利用从观察许多自然物体及其相关不确定性模式中获得的启发式方法，我们训练UPNet学习从视角外观到底层体素表示不确定性的直接映射。接下来，我们的方法聚合所有先前预测的神经不确定性图，以抑制冗余候选视角并有效选择信息量最大的视角。使用这些选定的视角，我们训练三维神经渲染模型，并评估新视角合成的质量，与其它有竞争力的主动视角选择方法进行比较。值得注意的是，尽管使用的视角数量是上限的一半，我们的方法仍能达到可比较的重建精度。此外，它显著降低了AVS期间的计算开销，与基线方法相比，速度提高了400倍，CPU、RAM和GPU使用率降低了50%以上。值得一提的是，我们的方法能够有效地泛化到涉及新物体类别的主动视角选择任务，无需任何额外训练。", "summary": "本文提出了一种名为UPNet的新型主动视角选择（AVS）方法，用于高效准确的三维重建。UPNet是一个轻量级神经网络，通过学习从视角外观到体素表示不确定性的直接映射，预测神经不确定性图。该方法通过聚合这些不确定性图来选择信息量最大的视角。实验结果表明，与现有方法相比，该方法在视角数量减半的情况下仍能保持重建精度，同时显著提高计算效率（速度提升高达400倍，资源消耗降低50%以上），并且对新物体类别具有良好的泛化能力。", "keywords": "主动视角选择, 三维重建, 神经不确定性图, UPNet, 计算效率", "comments": "该论文创新性地利用轻量级神经网络UPNet预测神经不确定性图来指导主动视角选择，避免了传统方法中昂贵的辐射场学习和不确定性计算。其主要亮点在于极大地提升了三维重建过程中视角选择的效率，同时保持了重建精度，并展现出对新物体的良好泛化能力，这对于实际应用具有重要意义。该方法为高效三维重建提供了一个有前景的新范式。"}}
{"id": "2309.13018", "title": "Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model", "authors": ["Jiamin Xie", "Ke Li", "Jinxi Guo", "Andros Tjandra", "Yuan Shangguan", "Leda Sari", "Chunyang Wu", "Junteng Jia", "Jay Mahadeokar", "Ozlem Kalinli"], "summary": "Neural network pruning offers an effective method for compressing a\nmultilingual automatic speech recognition (ASR) model with minimal performance\nloss. However, it entails several rounds of pruning and re-training needed to\nbe run for each language. In this work, we propose the use of an adaptive\nmasking approach in two scenarios for pruning a multilingual ASR model\nefficiently, each resulting in sparse monolingual models or a sparse\nmultilingual model (named as Dynamic ASR Pathways). Our approach dynamically\nadapts the sub-network, avoiding premature decisions about a fixed sub-network\nstructure. We show that our approach outperforms existing pruning methods when\ntargeting sparse monolingual models. Further, we illustrate that Dynamic ASR\nPathways jointly discovers and trains better sub-networks (pathways) of a\nsingle multilingual model by adapting from different sub-network\ninitializations, thereby reducing the need for language-specific pruning.", "comment": null, "cate": "eess.AS", "url": "http://arxiv.org/abs/2309.13018v2", "AI": {"title_translation": "动态ASR路径：一种面向多语言ASR模型高效剪枝的自适应掩码方法", "tldr": "本文提出了一种名为“动态ASR路径”的自适应掩码方法，用于高效剪枝多语言ASR模型，从而获得稀疏的单语言模型或稀疏的多语言模型，并优于现有方法。", "motivation": "现有的神经网络剪枝方法在压缩多语言自动语音识别（ASR）模型时，需要为每种语言进行多轮剪枝和再训练，效率低下。", "method": "本文提出了一种自适应掩码方法，用于两种场景下高效剪枝多语言ASR模型：一是生成稀疏的单语言模型，二是生成稀疏的多语言模型（命名为“动态ASR路径”）。该方法能动态调整子网络，避免过早固定子网络结构。", "result": "研究表明，该方法在生成稀疏单语言模型时优于现有剪枝方法。此外，“动态ASR路径”通过从不同的子网络初始化中进行适应，能够共同发现和训练更好的单个多语言模型的子网络（路径），从而减少了对特定语言剪枝的需求。", "conclusion": "本文提出的自适应掩码方法和“动态ASR路径”能够高效地对多语言ASR模型进行剪枝，提高了剪枝性能，并减少了对语言特定剪枝的需求。", "translation": "神经网络剪枝为压缩多语言自动语音识别（ASR）模型提供了一种有效方法，且性能损失极小。然而，它需要为每种语言运行多轮剪枝和再训练。在这项工作中，我们提出了在两种场景下使用自适应掩码方法来高效剪枝多语言ASR模型，每种场景都产生稀疏的单语言模型或稀疏的多语言模型（命名为动态ASR路径）。我们的方法动态地适应子网络，避免了对固定子网络结构的过早决策。我们表明，在目标为稀疏单语言模型时，我们的方法优于现有剪枝方法。此外，我们阐明了动态ASR路径通过从不同的子网络初始化中进行适应，共同发现和训练单个多语言模型中更好的子网络（路径），从而减少了对特定语言剪枝的需求。", "summary": "本文针对多语言ASR模型剪枝中每种语言需多轮剪枝和再训练的问题，提出了一种名为“动态ASR路径”的自适应掩码方法。该方法能动态适应子网络，避免过早固定结构，可用于生成稀疏的单语言模型或稀疏的多语言模型。实验结果表明，该方法在生成稀疏单语言模型时性能优于现有剪枝方法，并且能够共同发现和训练更好的子网络，从而减少了语言特定的剪枝需求。", "keywords": "多语言ASR, 神经网络剪枝, 自适应掩码, 动态ASR路径, 模型压缩", "comments": "本文的创新点在于提出了“动态ASR路径”这一自适应掩码方法，它能够动态调整子网络结构，避免了传统剪枝方法中固定的子网络结构带来的局限性。这种方法不仅提高了多语言ASR模型剪枝的效率，还减少了对特定语言剪枝的依赖，对于大规模多语言模型的部署具有重要意义。"}}
{"id": "2506.15264", "title": "Centroid Approximation for Byzantine-Tolerant Federated Learning", "authors": ["Mélanie Cambus", "Darya Melnyk", "Tijana Milentijević", "Stefan Schmid"], "summary": "Federated learning allows each client to keep its data locally when training\nmachine learning models in a distributed setting. Significant recent research\nestablished the requirements that the input must satisfy in order to guarantee\nconvergence of the training loop. This line of work uses averaging as the\naggregation rule for the training models. In particular, we are interested in\nwhether federated learning is robust to Byzantine behavior, and observe and\ninvestigate a tradeoff between the average/centroid and the validity conditions\nfrom distributed computing. We show that the various validity conditions alone\ndo not guarantee a good approximation of the average. Furthermore, we show that\nreaching good approximation does not give good results in experimental settings\ndue to possible Byzantine outliers. Our main contribution is the first lower\nbound of $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$ on the centroid approximation under\nbox validity that is often considered in the literature, where $n$ is the\nnumber of clients, $t$ the upper bound on the number of Byzantine faults, and\n$d$ is the dimension of the machine learning model. We complement this lower\nbound by an upper bound of $2\\min\\{n,\\sqrt{d}\\}$, by providing a new analysis\nfor the case $n<d$. In addition, we present a new algorithm that achieves a\n$\\sqrt{2d}$-approximation under convex validity, which also proves that the\nexisting lower bound in the literature is tight. We show that all presented\nbounds can also be achieved in the distributed peer-to-peer setting. We\ncomplement our analytical results with empirical evaluations in federated\nstochastic gradient descent and federated averaging settings.", "comment": "19 pages, 10 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15264v1", "AI": {"title_translation": "拜占庭容错联邦学习中的质心近似", "tldr": "本文研究了拜占庭容错联邦学习中的质心近似问题，提出了新的上下界和算法，并通过实验验证了其有效性。", "motivation": "联邦学习在分布式设置下训练模型时面临拜占庭行为的鲁棒性问题。现有的平均聚合规则及其有效性条件不能保证良好的平均近似，且在实验中易受拜占庭离群值影响。", "method": "作者通过理论分析，推导了在盒式有效性下的质心近似的第一个下界（$\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$），并补充了在$n<d$情况下的上界（$2\\min\\{n,\\sqrt{d}\\}$）。此外，提出了一种在凸有效性下实现$\\sqrt{2d}$-近似的新算法。所有提出的界限都可以在分布式点对点设置中实现。", "result": "推导了在盒式有效性下质心近似的第一个下界 $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$；推导了在$n<d$情况下的上界 $2\\min\\{n,\\sqrt{d}\\}$；提出了一种在凸有效性下实现 $\\sqrt{2d}$-近似的新算法；证明了文献中现有下界是紧的；所有提出的界限都可以在分布式点对点设置中实现；通过联邦随机梯度下降和联邦平均设置的经验评估补充了分析结果。", "conclusion": "现有的有效性条件不足以保证良好的平均近似，且拜占庭离群值会影响实验结果。本文通过提供新的理论上下界和算法，显著推进了拜占庭容错联邦学习中质心近似的理解和实现，并证明了现有理论的紧致性。", "translation": "联邦学习允许每个客户端在分布式环境中训练机器学习模型时将数据保存在本地。最近大量研究确定了输入必须满足的条件，以保证训练循环的收敛性。这类工作使用平均作为训练模型的聚合规则。特别是，我们感兴趣的是联邦学习是否对拜占庭行为具有鲁棒性，并观察和研究了平均/质心与分布式计算中有效性条件之间的权衡。我们表明，单独的各种有效性条件并不能保证对平均值的良好近似。此外，我们表明，由于可能的拜占庭离群值，达到良好近似在实验设置中并不能带来好的结果。我们的主要贡献是在文献中经常考虑的盒式有效性下，首次给出了质心近似的下界 $\\min\\{\\frac{n-t}{t},\\sqrt{d}\\}$，其中$n$是客户端数量，$t$是拜占庭故障数量的上限，$d$是机器学习模型的维度。我们通过对$n<d$情况的新分析，用 $2\\min\\{n,\\sqrt{d}\\}$ 的上界补充了这一下界。此外，我们提出了一种在凸有效性下实现 $\\sqrt{2d}$-近似的新算法，这也证明了文献中现有的下界是紧的。我们表明所有提出的界限也可以在分布式点对点设置中实现。我们通过联邦随机梯度下降和联邦平均设置中的经验评估补充了我们的分析结果。", "summary": "本文研究了拜占庭容错联邦学习中的质心近似问题，观察到现有平均聚合和有效性条件在面对拜占庭攻击时的局限性。为解决此问题，作者首次提出了在盒式有效性下质心近似的理论下界，并给出了相应的上界。同时，提出了一种新的算法，在凸有效性下实现了更优的近似，并证明了现有理论的紧致性。这些理论和算法在分布式点对点环境中均可实现，并通过实验验证了其有效性。", "keywords": "联邦学习, 拜占庭容错, 质心近似, 分布式计算, 鲁棒性", "comments": "这篇论文通过深入的理论分析，为拜占庭容错联邦学习提供了一系列重要的质心近似界限和算法。其创新性在于首次给出了特定条件下的下界，并证明了现有理论的紧致性，这对于理解联邦学习在恶意攻击下的鲁棒性具有重要意义。同时，结合经验评估增强了其实用价值。"}}
{"id": "2506.15376", "title": "Comparison of Innovative Strategies for the Coverage Problem: Path Planning, Search Optimization, and Applications in Underwater Robotics", "authors": ["Ahmed Ibrahim", "Francisco F. C. Rego", "Éric Busvelle"], "summary": "In many applications, including underwater robotics, the coverage problem\nrequires an autonomous vehicle to systematically explore a defined area while\nminimizing redundancy and avoiding obstacles. This paper investigates coverage\npath planning strategies to enhance the efficiency of underwater gliders,\nparticularly in maximizing the probability of detecting a radioactive source\nwhile ensuring safe navigation.\n  We evaluate three path-planning approaches: the Traveling Salesman Problem\n(TSP), Minimum Spanning Tree (MST), and Optimal Control Problem (OCP).\nSimulations were conducted in MATLAB, comparing processing time, uncovered\nareas, path length, and traversal time. Results indicate that OCP is preferable\nwhen traversal time is constrained, although it incurs significantly higher\ncomputational costs. Conversely, MST-based approaches provide faster but less\noptimal solutions. These findings offer insights into selecting appropriate\nalgorithms based on mission priorities, balancing efficiency and computational\nfeasibility.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15376v1", "AI": {"title_translation": "覆盖问题创新策略比较：路径规划、搜索优化及在水下机器人中的应用", "tldr": "本文比较了三种路径规划策略（TSP、MST、OCP）在水下机器人覆盖问题中的效率，发现OCP在时间受限时最优但计算成本高，MST更快但非最优。", "motivation": "提高水下机器人在定义区域内的系统探索效率，最小化冗余，避免障碍，特别是在最大化探测放射源概率的同时确保安全导航。", "method": "评估了三种路径规划方法：旅行商问题（TSP）、最小生成树（MST）和最优控制问题（OCP）。在MATLAB中进行仿真，比较了处理时间、未覆盖区域、路径长度和遍历时间。", "result": "OCP在遍历时间受限时更优，但计算成本显著更高。MST方法更快但解决方案非最优。", "conclusion": "研究结果为根据任务优先级选择合适的算法提供了见解，以平衡效率和计算可行性。", "translation": "在许多应用中，包括水下机器人领域，覆盖问题要求自主车辆系统地探索一个限定区域，同时最小化冗余并避开障碍物。本文研究了覆盖路径规划策略，旨在提高水下滑翔机的效率，特别是在确保安全导航的同时最大化探测放射源的概率。\n我们评估了三种路径规划方法：旅行商问题（TSP）、最小生成树（MST）和最优控制问题（OCP）。在MATLAB中进行了仿真，比较了处理时间、未覆盖区域、路径长度和遍历时间。结果表明，当遍历时间受限时，OCP是更优的选择，尽管其计算成本显著更高。相反，基于MST的方法提供更快但非最优的解决方案。这些发现为根据任务优先级选择合适的算法提供了见解，以平衡效率和计算可行性。", "summary": "本文比较了旅行商问题（TSP）、最小生成树（MST）和最优控制问题（OCP）三种路径规划策略在水下机器人覆盖问题中的应用。研究旨在提高水下滑翔机在特定区域内探测放射源的效率和安全性。MATLAB仿真结果显示，OCP在遍历时间受限时表现最佳但计算成本高昂，而MST方法速度更快但效率较低。这些发现有助于根据任务需求权衡算法的效率与计算可行性。", "keywords": "水下机器人, 覆盖问题, 路径规划, 旅行商问题, 最小生成树, 最优控制问题", "comments": "本文通过比较三种经典算法在水下机器人覆盖问题中的表现，为实际应用提供了有价值的参考。其创新点在于将这些算法应用于水下放射源探测这一特定场景，并量化了它们的性能差异。局限性可能在于仿真结果的实际迁移性，以及未深入探讨混合策略或更先进的机器学习方法。"}}
{"id": "2506.14808", "title": "PARC: A Quantitative Framework Uncovering the Symmetries within Vision Language Models", "authors": ["Jenny Schmalfuss", "Nadine Chang", "Vibashan VS", "Maying Shen", "Andres Bruhn", "Jose M. Alvarez"], "summary": "Vision language models (VLMs) respond to user-crafted text prompts and visual\ninputs, and are applied to numerous real-world problems. VLMs integrate visual\nmodalities with large language models (LLMs), which are well known to be\nprompt-sensitive. Hence, it is crucial to determine whether VLMs inherit this\ninstability to varying prompts. We therefore investigate which prompt\nvariations VLMs are most sensitive to and which VLMs are most agnostic to\nprompt variations. To this end, we introduce PARC (Prompt Analysis via\nReliability and Calibration), a VLM prompt sensitivity analysis framework built\non three pillars: (1) plausible prompt variations in both the language and\nvision domain, (2) a novel model reliability score with built-in guarantees,\nand (3) a calibration step that enables dataset- and prompt-spanning prompt\nvariation analysis. Regarding prompt variations, PARC's evaluation shows that\nVLMs mirror LLM language prompt sensitivity in the vision domain, and most\ndestructive variations change the expected answer. Regarding models,\noutstandingly robust VLMs among 22 evaluated models come from the InternVL2\nfamily. We further find indications that prompt sensitivity is linked to\ntraining data. The code will be at https://github.com/NVlabs/PARC.", "comment": "Accepted to CVPR 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14808v1", "AI": {"title_translation": "PARC：一个揭示视觉语言模型内部对称性的定量框架", "tldr": "本文引入PARC框架，量化分析视觉语言模型（VLMs）对提示变化的敏感性，发现VLMs在视觉领域也表现出与LLM类似的提示敏感性，并指出InternVL2系列模型表现出最佳鲁棒性。", "motivation": "视觉语言模型（VLMs）与大型语言模型（LLMs）类似，可能对提示敏感。因此，确定VLMs是否继承了这种对不同提示的不稳定性，以及VLMs对哪些提示变化最敏感、哪些模型对提示变化最不敏感，是至关重要的。", "method": "引入了PARC（Prompt Analysis via Reliability and Calibration）框架，一个基于三个支柱的VLM提示敏感性分析框架：(1) 语言和视觉领域中合理的提示变体；(2) 一个具有内置保证的新颖模型可靠性分数；(3) 一个实现数据集和提示跨度提示变体分析的校准步骤。", "result": "PARC的评估显示VLMs在视觉领域反映了LLM的语言提示敏感性，并且最具破坏性的变体改变了预期答案。在22个评估模型中，InternVL2家族的模型表现出卓越的鲁棒性。研究还发现提示敏感性与训练数据相关。", "conclusion": "VLMs确实继承了LLM的提示敏感性，尤其是在视觉领域。InternVL2系列模型在鲁棒性方面表现突出，提示敏感性可能与训练数据有关。", "translation": "视觉语言模型（VLMs）响应用户制作的文本提示和视觉输入，并应用于众多现实世界问题。VLMs将视觉模态与大型语言模型（LLMs）集成，而LLMs众所周知对提示敏感。因此，确定VLMs是否继承了这种对不同提示的不稳定性至关重要。我们因此调查了VLMs对哪些提示变化最敏感，以及哪些VLMs对提示变化最不敏感。为此，我们引入了PARC（Prompt Analysis via Reliability and Calibration），一个基于三个支柱的VLM提示敏感性分析框架：(1) 语言和视觉领域中合理的提示变体，(2) 一个具有内置保证的新颖模型可靠性分数，以及 (3) 一个实现数据集和提示跨度提示变体分析的校准步骤。关于提示变体，PARC的评估显示VLMs在视觉领域反映了LLM的语言提示敏感性，并且最具破坏性的变体改变了预期答案。关于模型，在22个评估模型中，InternVL2家族的模型表现出卓越的鲁棒性。我们进一步发现提示敏感性与训练数据相关的迹象。代码将在https://github.com/NVlabs/PARC提供。", "summary": "本文提出了PARC框架，一个用于量化分析视觉语言模型（VLMs）对提示变化的敏感性的工具。该框架通过考虑语言和视觉提示变体、引入模型可靠性分数和校准步骤，发现VLMs在视觉领域也存在与大型语言模型（LLMs）类似的提示敏感性。研究结果表明，最具破坏性的提示变化会改变预期答案，并且InternVL2系列模型在提示鲁棒性方面表现最佳，同时提示敏感性可能与训练数据有关。", "keywords": "视觉语言模型, 提示敏感性, PARC框架, 模型鲁棒性, 提示变体", "comments": "本文创新性地提出了一个全面的定量框架PARC，用于分析视觉语言模型（VLMs）的提示敏感性，填补了该领域的一个重要空白。通过引入视觉领域的提示变体和新的可靠性评分，该研究深入揭示了VLMs的行为特性。其发现VLMs在视觉领域也存在提示敏感性，并识别出鲁棒性强的模型家族（InternVL2），对VLMs的开发和应用具有重要指导意义。同时，指出提示敏感性可能与训练数据有关，为未来研究提供了方向。"}}
{"id": "2506.15562", "title": "Automated MRI Tumor Segmentation using hybrid U-Net with Transformer and Efficient Attention", "authors": ["Syed Haider Ali", "Asrar Ahmad", "Muhammad Ali", "Asifullah Khan", "Muhammad Shahban", "Nadeem Shaukat"], "summary": "Cancer is an abnormal growth with potential to invade locally and metastasize\nto distant organs. Accurate auto-segmentation of the tumor and surrounding\nnormal tissues is required for radiotherapy treatment plan optimization. Recent\nAI-based segmentation models are generally trained on large public datasets,\nwhich lack the heterogeneity of local patient populations. While these studies\nadvance AI-based medical image segmentation, research on local datasets is\nnecessary to develop and integrate AI tumor segmentation models directly into\nhospital software for efficient and accurate oncology treatment planning and\nexecution. This study enhances tumor segmentation using computationally\nefficient hybrid UNet-Transformer models on magnetic resonance imaging (MRI)\ndatasets acquired from a local hospital under strict privacy protection. We\ndeveloped a robust data pipeline for seamless DICOM extraction and\npreprocessing, followed by extensive image augmentation to ensure model\ngeneralization across diverse clinical settings, resulting in a total dataset\nof 6080 images for training. Our novel architecture integrates UNet-based\nconvolutional neural networks with a transformer bottleneck and complementary\nattention modules, including efficient attention, Squeeze-and-Excitation (SE)\nblocks, Convolutional Block Attention Module (CBAM), and ResNeXt blocks. To\naccelerate convergence and reduce computational demands, we used a maximum\nbatch size of 8 and initialized the encoder with pretrained ImageNet weights,\ntraining the model on dual NVIDIA T4 GPUs via checkpointing to overcome\nKaggle's runtime limits. Quantitative evaluation on the local MRI dataset\nyielded a Dice similarity coefficient of 0.764 and an Intersection over Union\n(IoU) of 0.736, demonstrating competitive performance despite limited data and\nunderscoring the importance of site-specific model development for clinical\ndeployment.", "comment": "16 pages, 5 figures", "cate": "eess.IV", "url": "http://arxiv.org/abs/2506.15562v1", "AI": {"title_translation": "采用混合U-Net结合Transformer和高效注意力机制的自动化MRI肿瘤分割", "tldr": "本研究提出了一种结合U-Net和Transformer的混合模型，用于在本地MRI数据集上进行自动化肿瘤分割，并在有限数据下取得了有竞争力的性能。", "motivation": "现有AI分割模型多训练于大型公共数据集，缺乏本地患者群体的异质性，导致难以直接整合到医院软件中。因此，需要针对本地数据集开发AI肿瘤分割模型，以实现高效准确的肿瘤治疗计划和执行。", "method": "本研究使用计算高效的混合UNet-Transformer模型增强肿瘤分割。该模型结合了UNet的卷积神经网络、Transformer瓶颈层和互补注意力模块（包括高效注意力、Squeeze-and-Excitation (SE) 块、卷积块注意力模块 (CBAM) 和 ResNeXt 块）。数据处理包括鲁棒的DICOM提取和预处理，以及广泛的图像增强，最终形成6080张图像的训练数据集。训练时使用最大批处理大小为8，编码器用ImageNet预训练权重初始化，并在双NVIDIA T4 GPU上通过检查点进行训练。", "result": "在本地MRI数据集上的定量评估结果显示，Dice相似系数为0.764，交并比（IoU）为0.736。", "conclusion": "尽管数据有限，该模型仍表现出有竞争力的性能，强调了针对特定站点开发模型对于临床部署的重要性。", "translation": "癌症是一种异常生长，有可能局部侵袭并转移到远处的器官。放射治疗计划优化需要准确地自动分割肿瘤和周围正常组织。最近基于人工智能的分割模型通常在大型公共数据集上进行训练，这些数据集缺乏本地患者群体的异质性。虽然这些研究推动了基于人工智能的医学图像分割，但对本地数据集的研究是必要的，以便将人工智能肿瘤分割模型直接开发并整合到医院软件中，以实现高效准确的肿瘤治疗计划和执行。本研究在严格隐私保护下从本地医院获取的磁共振成像（MRI）数据集上，使用计算高效的混合UNet-Transformer模型增强了肿瘤分割。我们开发了一个鲁棒的数据管道，用于无缝的DICOM提取和预处理，随后进行大量图像增强，以确保模型在不同临床环境下的泛化能力，最终用于训练的总数据集达到6080张图像。我们新颖的架构将基于UNet的卷积神经网络与Transformer瓶颈和互补注意力模块（包括高效注意力、Squeeze-and-Excitation (SE) 块、卷积块注意力模块 (CBAM) 和 ResNeXt 块）相结合。为了加速收敛并减少计算需求，我们使用了最大批处理大小为8，并用ImageNet预训练权重初始化编码器，通过检查点在双NVIDIA T4 GPU上训练模型，以克服Kaggle的运行时限制。在本地MRI数据集上的定量评估显示，Dice相似系数为0.764，交并比（IoU）为0.736，尽管数据有限，但仍表现出有竞争力的性能，并强调了针对特定站点开发模型对于临床部署的重要性。", "summary": "本文提出了一种新颖的混合U-Net与Transformer架构，并结合了多种注意力机制（如高效注意力、SE、CBAM和ResNeXt），用于自动化MRI肿瘤分割。模型在从本地医院获取并经过严格预处理和数据增强的有限数据集（6080张图像）上进行训练。实验结果表明，该模型在Dice相似系数和IoU方面均达到有竞争力的性能，突出了为临床应用开发特定站点模型的重要性。", "keywords": "肿瘤分割, MRI, U-Net, Transformer, 注意力机制", "comments": "该研究的创新点在于结合了U-Net和Transformer的优势，并融入了多种注意力机制，以提高肿瘤分割的准确性。其重要性在于强调了在本地数据集上开发模型的必要性，这对于将AI模型直接集成到医院临床工作流程中至关重要。尽管数据量相对有限，但仍取得了不错的性能，这表明了该混合架构的有效性。"}}
{"id": "2506.15208", "title": "A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals", "authors": ["Andrea Cadeddu", "Alessandro Chessa", "Vincenzo De Leo", "Gianni Fenu", "Enrico Motta", "Francesco Osborne", "Diego Reforgiato Recupero", "Angelo Salatino", "Luca Secchi"], "summary": "In 2012, the United Nations introduced 17 Sustainable Development Goals\n(SDGs) aimed at creating a more sustainable and improved future by 2030.\nHowever, tracking progress toward these goals is difficult because of the\nextensive scale and complexity of the data involved. Text classification models\nhave become vital tools in this area, automating the analysis of vast amounts\nof text from a variety of sources. Additionally, large language models (LLMs)\nhave recently proven indispensable for many natural language processing tasks,\nincluding text classification, thanks to their ability to recognize complex\nlinguistic patterns and semantics. This study analyzes various proprietary and\nopen-source LLMs for a single-label, multi-class text classification task\nfocused on the SDGs. Then, it also evaluates the effectiveness of task\nadaptation techniques (i.e., in-context learning approaches), namely Zero-Shot\nand Few-Shot Learning, as well as Fine-Tuning within this domain. The results\nreveal that smaller models, when optimized through prompt engineering, can\nperform on par with larger models like OpenAI's GPT (Generative Pre-trained\nTransformer).", "comment": "Submitted to IEEE Access", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15208v1", "AI": {"title_translation": "大型语言模型任务适应技术在识别可持续发展目标中的比较研究", "tldr": "本研究比较了大型语言模型及其任务适应技术（零样本、少样本学习和微调）在识别可持续发展目标（SDGs）上的表现，发现通过提示工程优化的小型模型可与大型模型媲美。", "motivation": "追踪联合国可持续发展目标（SDGs）的进展因数据规模和复杂性而困难。文本分类模型，特别是大型语言模型（LLMs），是自动化分析文本以识别SDGs的关键工具，因此有必要研究LLMs及其适应技术在此任务中的表现。", "method": "本研究分析了各种专有和开源大型语言模型（LLMs），用于一项侧重于可持续发展目标（SDGs）的单标签、多类别文本分类任务。研究还评估了零样本学习、少样本学习和微调等任务适应技术（即上下文学习方法）的有效性。", "result": "结果表明，通过提示工程优化后的小型模型可以与OpenAI的GPT等大型模型表现相当。", "conclusion": "通过适当的任务适应技术和提示工程，小型大型语言模型在可持续发展目标识别任务中也能达到与大型模型相当的性能，这为资源受限场景提供了可行方案。", "translation": "2012年，联合国提出了17个可持续发展目标（SDGs），旨在到2030年创建一个更可持续、更美好的未来。然而，由于所涉及数据的广泛规模和复杂性，追踪这些目标的进展十分困难。文本分类模型已成为该领域的关键工具，能够自动化分析来自各种来源的大量文本。此外，大型语言模型（LLMs）最近因其识别复杂语言模式和语义的能力，已被证明在许多自然语言处理任务（包括文本分类）中不可或缺。本研究分析了各种专有和开源LLMs，用于一项侧重于SDGs的单标签、多类别文本分类任务。然后，它还评估了在该领域中任务适应技术（即上下文学习方法），即零样本学习和少样本学习，以及微调的有效性。结果表明，通过提示工程优化后的小型模型可以与OpenAI的GPT等大型模型表现相当。", "summary": "本研究对各种专有和开源大型语言模型（LLMs）在识别可持续发展目标（SDGs）的单标签、多类别文本分类任务中的表现进行了比较分析。研究还评估了零样本学习、少样本学习和微调等任务适应技术的效果。核心发现是，经过提示工程优化的小型LLMs在性能上能够与OpenAI GPT等大型模型相媲美，这为SDG追踪提供了高效且可能资源友好的解决方案。", "keywords": "大型语言模型, 可持续发展目标, 任务适应, 文本分类, 提示工程", "comments": "这项研究的创新之处在于其对不同规模LLM及其任务适应技术在特定领域（SDGs识别）的系统比较，特别是揭示了通过提示工程优化小型模型能够达到大型模型性能的潜力。这对于实际应用具有重要意义，尤其是在计算资源有限的情况下，为SDG追踪提供了更经济高效的解决方案。"}}
{"id": "2506.15203", "title": "Reduced Particle in Cell method for the Vlasov-Poisson system using auto-encoder and Hamiltonian neural", "authors": ["Emmanuel Franck", "Laurent Navoret", "Vincent Vigon", "Raphaël Côte", "Guillaume Steimer"], "summary": "Hamiltonian particle-based simulations of plasma dynamics are inherently\ncomputationally intensive, primarily due to the large number of particles\nrequired to obtain accurate solutions. This challenge becomes even more acute\nin many-query contexts, where numerous simulations must be conducted across a\nrange of time and parameter values. Consequently, it is essential to construct\nreduced order models from such discretizations to significantly lower\ncomputational costs while ensuring validity across the specified time and\nparameter domains. Preserving the Hamiltonian structure in these reduced models\nis also crucial, as it helps maintain long-term stability. In this paper, we\nintroduce a nonlinear, non-intrusive, data-driven model order reduction method\nfor the 1D-1V Vlasov--Poisson system, discretized using a Hamiltonian\nParticle-In-Cell scheme. Our approach relies on a two-step projection\nframework: an initial linear projection based on the Proper Symplectic\nDecomposition, followed by a nonlinear projection learned via an autoencoder\nneural network. The reduced dynamics are then modeled using a Hamiltonian\nneural network. The offline phase of the method is split into two stages:\nfirst, constructing the linear projection using full-order model snapshots;\nsecond, jointly training the autoencoder and the Hamiltonian neural network to\nsimultaneously learn the encoder-decoder mappings and the reduced dynamics. We\nvalidate the proposed method on several benchmarks, including Landau damping\nand two-stream instability. The results show that our method has better\nreduction properties than standard linear Hamiltonian reduction methods.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15203v1", "AI": {"title_translation": "使用自编码器和哈密顿神经网络的Vlasov-Poisson系统降阶粒子在单元法", "tldr": "本文提出了一种新的非线性、非侵入式、数据驱动的模型降阶方法，用于减少Vlasov-Poisson系统的粒子模拟计算成本，通过结合线性投影（基于Proper Symplectic Decomposition）、自编码器和哈密顿神经网络实现，并在基准测试中表现出比传统方法更好的降阶性能。", "motivation": "哈密顿粒子基等离子体动力学模拟计算成本高昂，尤其是在多查询场景下，需要大量粒子以获得准确解。因此，需要构建降阶模型以显著降低计算成本，同时确保在指定时间和参数域内的有效性并保持哈密顿结构以维持长期稳定性。", "method": "本文提出了一种针对1D-1V Vlasov-Poisson系统的非线性、非侵入式、数据驱动的模型降阶方法，该系统通过哈密顿粒子在单元（PIC）方案离散化。该方法依赖于两步投影框架：首先是基于Proper Symplectic Decomposition的初始线性投影，然后是通过自编码器神经网络学习的非线性投影。降阶动力学通过哈密顿神经网络建模。离线阶段分为两步：首先使用全阶模型快照构建线性投影；其次联合训练自编码器和哈密顿神经网络，同时学习编码器-解码器映射和降阶动力学。", "result": "该方法在Landau阻尼和双流不稳定性等多个基准测试中得到了验证。结果表明，该方法比标准的线性哈密顿降阶方法具有更好的降阶特性。", "conclusion": "本文提出的结合自编码器和哈密顿神经网络的降阶粒子在单元法能有效降低Vlasov-Poisson系统模拟的计算成本，并保持哈密顿结构，表现出优于传统线性方法的降阶性能。", "translation": "哈密顿粒子基等离子体动力学模拟本质上计算密集，主要原因是需要大量粒子才能获得准确解。在多查询场景中，当需要在不同时间点和参数值下进行大量模拟时，这一挑战变得更加严峻。因此，从这类离散化中构建降阶模型至关重要，以显著降低计算成本，同时确保在指定时间域和参数域内的有效性。在这些降阶模型中保留哈密顿结构也至关重要，因为它有助于维持长期稳定性。在本文中，我们介绍了一种针对1D-1V Vlasov-Poisson系统的非线性、非侵入式、数据驱动的模型降阶方法，该系统使用哈密顿粒子在单元（PIC）方案进行离散化。我们的方法依赖于两步投影框架：首先是基于Proper Symplectic Decomposition的初始线性投影，然后是通过自编码器神经网络学习的非线性投影。降阶动力学随后使用哈密顿神经网络进行建模。该方法的离线阶段分为两个步骤：首先，使用全阶模型快照构建线性投影；其次，联合训练自编码器和哈密顿神经网络，以同时学习编码器-解码器映射和降阶动力学。我们通过多个基准测试验证了所提出的方法，包括Landau阻尼和双流不稳定性。结果表明，我们的方法比标准的线性哈密顿降阶方法具有更好的降阶特性。", "summary": "本文提出了一种新颖的非线性、非侵入式、数据驱动的模型降阶方法，用于解决Vlasov-Poisson系统哈密顿粒子模拟计算成本高昂的问题。该方法结合了基于Proper Symplectic Decomposition的线性投影、通过自编码器学习的非线性投影以及哈密顿神经网络来建模降阶动力学。通过在Landau阻尼和双流不稳定性等基准测试中的验证，结果表明该方法比传统线性哈密顿降阶方法具有更优越的降阶性能，有效降低了计算成本并保持了系统稳定性。", "keywords": "模型降阶, Vlasov-Poisson系统, 自编码器, 哈密顿神经网络, 粒子在单元法", "comments": "该论文提出了一种创新的模型降阶方法，通过结合线性与非线性投影（自编码器）以及哈密顿神经网络，有效地解决了哈密顿粒子模拟计算量大的问题。其创新点在于引入了数据驱动的非线性降阶，同时确保了哈密顿结构的保持，这对于长期模拟的稳定性至关重要。该方法在处理多查询场景和复杂非线性系统方面具有重要意义，超越了传统线性降阶方法的局限性。"}}
{"id": "2506.15388", "title": "Evaluation Pipeline for systematically searching for Anomaly Detection Systems", "authors": ["Florian Rokohl", "Alexander Lehnert", "Marc Reichenbach"], "summary": "Digitalization in the medical world provides major benefits while making it a\ntarget for attackers and thus hard to secure. To deal with network intruders we\npropose an anomaly detection system on hardware to detect malicious clients in\nreal-time. We meet real-time and power restrictions using FPGAs. Overall system\nperformance is achieved via the presented holistic system evaluation.", "comment": "Submitted to 18th HiPEAC Workshop on Reconfigurable Computing\n  (WRC'2024)", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15388v1", "AI": {"title_translation": "系统搜索异常检测系统的评估流程", "tldr": "本文提出了一种基于FPGA的硬件异常检测系统，用于实时检测医疗网络中的恶意客户端，并通过整体系统评估实现了性能。", "motivation": "医疗领域的数字化带来了巨大好处，但也使其成为攻击者的目标，难以保障安全。为了应对网络入侵者，需要一个系统来实时检测恶意客户端。", "method": "提出了一种基于硬件的异常检测系统，利用FPGA来满足实时性和功耗限制，并通过整体系统评估实现系统性能。", "result": "通过所提出的整体系统评估，实现了整体系统性能。", "conclusion": "通过在硬件上部署基于FPGA的异常检测系统，可以实时有效地检测医疗网络中的恶意客户端，并通过全面的评估确保系统性能。", "translation": "医疗领域的数字化带来了巨大的好处，但也使其成为攻击者的目标，因此难以保障安全。为了应对网络入侵者，我们提出了一种基于硬件的异常检测系统，用于实时检测恶意客户端。我们利用FPGA来满足实时性和功耗限制。通过所提出的整体系统评估，实现了整体系统性能。", "summary": "本文针对医疗数字化带来的安全挑战，提出了一种基于FPGA的硬件异常检测系统，旨在实时检测网络中的恶意客户端。该系统利用FPGA满足实时性和功耗限制，并通过全面的系统评估来确保其整体性能。", "keywords": "异常检测系统, 硬件, FPGA, 医疗安全, 实时检测", "comments": "该论文的创新点在于将异常检测系统部署在硬件（FPGA）上，以满足医疗领域对实时性和功耗的严格要求。这种方法对于需要高性能和低延迟安全解决方案的边缘计算或嵌入式系统具有重要意义。然而，抽象中没有详细说明检测的具体算法或评估指标。"}}
{"id": "2506.15332", "title": "Building Blocks of a User Experience Research Point of View", "authors": ["Patricia Diaz"], "summary": "This paper presents three User Experience Research (UXR) perspectives based\non data, evidence and insights - known as Point of View (POV) - showcasing how\nthe strategies and methods of building a POV work in an enterprise setting. The\nPOV are: 1. Smart Visuals: Use AI to extract and translate text from visuals in\nvideos (2019). 2. Assessable Code Editor: Focus on direct AI-feedback to the\nlearner as it is the loop that requires the least effort for the highest\nimpact(2023). 3. Opportunity Landscape: Identify high-impact opportunities at\nthe intersection of emergent technical capabilities that unlock novel\napproaches to critical user needs while addressing business strategic\npriorities (2019). They all seemed far-fetched and went against common\npractice. All were adopted and had long-lasting impact.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15332v1", "AI": {"title_translation": "用户体验研究视角的基石", "tldr": "本文介绍了三个最初看似牵强但最终被采纳并产生长期影响的用户体验研究（UXR）视角（POV），展示了如何在企业环境中构建POV的策略和方法。", "motivation": "本文的动机是展示三个基于数据、证据和洞察的用户体验研究（UXR）视角（POV），尽管它们最初看起来牵强且与常规实践相悖，但最终都被采纳并产生了长期影响，从而证明了在企业环境中构建和应用这些POV的有效性。", "method": "本文通过介绍并详细阐述了三个具体的用户体验研究（UXR）视角（POV）：1. 智能视觉：利用AI从视频中提取和翻译文本。2. 可评估的代码编辑器：专注于直接的AI反馈以实现最高影响和最低努力的学习循环。3. 机会图景：在新兴技术能力与关键用户需求和业务战略优先级交叉点识别高影响力机会。论文展示了这些POV如何在企业环境中构建和应用。", "result": "结果是这三个最初看起来牵强且与常规实践相悖的用户体验研究（UXR）视角（POV）都被成功采纳，并产生了长期的影响。", "conclusion": "本文的结论是，即使是那些最初看起来牵强或与常规实践相悖的用户体验研究视角（POV），如果它们基于数据、证据和洞察，也能够在企业环境中被成功采纳并产生长期积极影响。", "translation": "本文介绍了基于数据、证据和洞察的三种用户体验研究（UXR）视角——称为“观点”（POV），展示了在企业环境中构建POV的策略和方法。这些POV包括：1. 智能视觉：利用人工智能从视频中的视觉内容中提取和翻译文本（2019年）。2. 可评估的代码编辑器：专注于向学习者提供直接的AI反馈，因为这是实现最高影响所需最少努力的循环（2023年）。3. 机会图景：在能够解锁关键用户需求的新颖方法并同时解决业务战略优先事项的新兴技术能力的交叉点识别高影响力机会（2019年）。它们最初都显得牵强，并且与常规做法背道而驰。但它们都被采纳并产生了长期影响。", "summary": "本文探讨了用户体验研究（UXR）中构建“观点”（POV）的基石，通过展示三个在企业环境中被成功实施的特定POV：智能视觉、可评估的代码编辑器和机会图景。尽管这些POV最初被认为是牵强且违反常规实践，但它们最终都被采纳并产生了显著的长期影响，突出了数据、证据和洞察在形成有效UXR策略中的重要性。", "keywords": "用户体验研究, 观点, 智能视觉, 代码编辑器, 机会图景", "comments": "本文的创新之处在于它挑战了用户体验研究（UXR）中的常规实践，并提供了具体的、非传统的POV作为成功案例。它强调了基于数据、证据和洞察力的方法，即使最初不被看好，也能在企业环境中产生长期积极影响。其重要性在于为UXR专业人员提供了信心，鼓励他们探索和推行创新性方法。"}}
{"id": "2506.14903", "title": "DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization", "authors": ["Renjith Prasad", "Abhilekh Borah", "Hasnat Md Abdullah", "Chathurangi Shyalika", "Gurpreet Singh", "Ritvik Garimella", "Rajarshi Roy", "Harshul Surana", "Nasrin Imanpour", "Suranjana Trivedy", "Amit Sheth", "Amitava Das"], "summary": "Alignment is crucial for text-to-image (T2I) models to ensure that generated\nimages faithfully capture user intent while maintaining safety and fairness.\nDirect Preference Optimization (DPO), prominent in large language models\n(LLMs), is extending its influence to T2I systems. This paper introduces\nDPO-Kernels for T2I models, a novel extension enhancing alignment across three\ndimensions: (i) Hybrid Loss, integrating embedding-based objectives with\ntraditional probability-based loss for improved optimization; (ii) Kernelized\nRepresentations, employing Radial Basis Function (RBF), Polynomial, and Wavelet\nkernels for richer feature transformations and better separation between safe\nand unsafe inputs; and (iii) Divergence Selection, expanding beyond DPO's\ndefault Kullback-Leibler (KL) regularizer by incorporating Wasserstein and\nR'enyi divergences for enhanced stability and robustness. We introduce\nDETONATE, the first large-scale benchmark of its kind, comprising approximately\n100K curated image pairs categorized as chosen and rejected. DETONATE\nencapsulates three axes of social bias and discrimination: Race, Gender, and\nDisability. Prompts are sourced from hate speech datasets, with images\ngenerated by leading T2I models including Stable Diffusion 3.5 Large, Stable\nDiffusion XL, and Midjourney. Additionally, we propose the Alignment Quality\nIndex (AQI), a novel geometric measure quantifying latent-space separability of\nsafe/unsafe image activations, revealing hidden vulnerabilities. Empirically,\nwe demonstrate that DPO-Kernels maintain strong generalization bounds via\nHeavy-Tailed Self-Regularization (HT-SR). DETONATE and complete code are\npublicly released.", "comment": "59 pages, 10 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14903v1", "AI": {"title_translation": "DETONATE：文本到图像对齐与核化直接偏好优化的基准", "tldr": "该论文介绍了DPO-Kernels，一种用于文本到图像（T2I）模型的新型对齐增强方法，并推出了DETONATE，一个大规模的T2I对齐基准，用于评估模型在种族、性别和残疾偏见方面的表现。", "motivation": "文本到图像（T2I）模型中的对齐至关重要，以确保生成的图像忠实地捕捉用户意图，同时保持安全性和公平性。直接偏好优化（DPO）正在将其在大型语言模型（LLMs）中的影响力扩展到T2I系统，但需要对其进行增强以更好地适应T2I对齐。", "method": "该论文引入了DPO-Kernels，通过混合损失、核化表示（使用RBF、多项式和小波核）和散度选择（扩展到Wasserstein和R'enyi散度）来增强T2I模型的对齐。同时，论文还构建了DETONATE，一个包含约10万个精选图像对的大规模基准，用于评估T2I模型在种族、性别和残疾社会偏见方面的表现。此外，提出了一种新的几何度量——对齐质量指数（AQI），用于量化安全/不安全图像激活的潜在空间可分离性。", "result": "经验表明，DPO-Kernels通过重尾自正则化（HT-SR）保持了强大的泛化界限。DETONATE基准和完整代码已公开发布。", "conclusion": "该论文通过引入DPO-Kernels和DETONATE基准，为文本到图像模型的对齐问题提供了创新性的解决方案和评估工具，特别关注了社会偏见和模型漏洞的揭示。", "translation": "对齐对于文本到图像（T2I）模型至关重要，以确保生成的图像忠实地捕捉用户意图，同时保持安全性和公平性。直接偏好优化（DPO）在大型语言模型（LLMs）中表现突出，并正在将其影响力扩展到T2I系统。本文介绍了DPO-Kernels，这是T2I模型的一种新颖扩展，它通过三个维度增强了对齐：(i) 混合损失，将基于嵌入的目标与传统的基于概率的损失相结合，以改进优化；(ii) 核化表示，采用径向基函数（RBF）、多项式和小波核，用于更丰富的特征变换以及安全和不安全输入之间更好的分离；(iii) 散度选择，通过引入Wasserstein和R'enyi散度，扩展了DPO默认的Kullback-Leibler（KL）正则化器，以增强稳定性和鲁棒性。我们引入了DETONATE，这是同类中第一个大规模基准，包含大约10万个分类为选择和拒绝的精选图像对。DETONATE包含了社会偏见和歧视的三个轴：种族、性别和残疾。提示语来源于仇恨言论数据集，图像由领先的T2I模型生成，包括Stable Diffusion 3.5 Large、Stable Diffusion XL和Midjourney。此外，我们提出了对齐质量指数（AQI），这是一种新的几何度量，用于量化安全/不安全图像激活的潜在空间可分离性，揭示了隐藏的漏洞。从经验上看，我们证明了DPO-Kernels通过重尾自正则化（HT-SR）保持了强大的泛化界限。DETONATE和完整代码已公开发布。", "summary": "该论文提出了DPO-Kernels，一种新颖的直接偏好优化（DPO）扩展，旨在增强文本到图像（T2I）模型的对齐、安全性和公平性。DPO-Kernels通过结合混合损失、核化表示和多种散度选择来改进优化和特征分离。为评估T2I模型的对齐质量，特别是社会偏见，论文还引入了DETONATE，一个包含约10万个图像对的大规模基准，涵盖种族、性别和残疾偏见。此外，提出了对齐质量指数（AQI）来量化潜在空间中安全/不安全激活的可分离性。实验结果表明DPO-Kernels具有良好的泛化能力，所有资源均已公开。", "keywords": "文本到图像对齐, 直接偏好优化, DPO-Kernels, DETONATE基准, 社会偏见", "comments": "该论文的创新点在于将DPO扩展到T2I领域，并引入了DPO-Kernels，通过多维度优化（混合损失、核化表示、多种散度）来增强模型对齐。DETONATE基准的构建具有重要意义，它是首个大规模专注于T2I社会偏见对齐的基准，为评估和改进模型的公平性提供了宝贵资源。对齐质量指数（AQI）的提出也为量化模型潜在漏洞提供了新视角。这项工作对于提升T2I模型的安全性和鲁棒性具有重要推动作用。"}}
{"id": "2310.18450", "title": "MixRep: Hidden Representation Mixup for Low-Resource Speech Recognition", "authors": ["Jiamin Xie", "John H. L. Hansen"], "summary": "In this paper, we present MixRep, a simple and effective data augmentation\nstrategy based on mixup for low-resource ASR. MixRep interpolates the feature\ndimensions of hidden representations in the neural network that can be applied\nto both the acoustic feature input and the output of each layer, which\ngeneralizes the previous MixSpeech method. Further, we propose to combine the\nmixup with a regularization along the time axis of the input, which is shown as\ncomplementary. We apply MixRep to a Conformer encoder of an E2E LAS\narchitecture trained with a joint CTC loss. We experiment on the WSJ dataset\nand subsets of the SWB dataset, covering reading and telephony conversational\nspeech. Experimental results show that MixRep consistently outperforms other\nregularization methods for low-resource ASR. Compared to a strong SpecAugment\nbaseline, MixRep achieves a +6.5\\% and a +6.7\\% relative WER reduction on the\neval92 set and the Callhome part of the eval'2000 set.", "comment": "Accepted to Interspeech 2023", "cate": "eess.AS", "url": "http://arxiv.org/abs/2310.18450v1", "AI": {"title_translation": "MixRep：低资源语音识别的隐藏表示混叠", "tldr": "MixRep是一种新的数据增强策略，通过混合神经网络的隐藏表示来提高低资源ASR的性能，优于现有方法。", "motivation": "解决低资源语音识别（ASR）中数据稀缺的问题，通过数据增强来提升模型性能。", "method": "提出了MixRep，一种基于mixup的数据增强策略。它通过插值神经网络隐藏表示的特征维度，可应用于声学特征输入和每一层的输出，是MixSpeech的泛化。此外，还提出了将mixup与输入的时间轴正则化相结合的互补方法。将MixRep应用于结合CTC损失的E2E LAS架构的Conformer编码器。", "result": "MixRep在低资源ASR上始终优于其他正则化方法。与强大的SpecAugment基线相比，MixRep在eval92集和eval'2000集的Callhome部分分别实现了+6.5%和+6.7%的相对词错误率（WER）降低。", "conclusion": "MixRep是一种简单有效的数据增强策略，能显著提高低资源语音识别的性能。", "translation": "本文提出了MixRep，一种基于mixup的简单有效的数据增强策略，用于低资源ASR。MixRep对神经网络中隐藏表示的特征维度进行插值，可以应用于声学特征输入和每一层的输出，从而泛化了之前的MixSpeech方法。此外，我们提出将mixup与输入的时间轴正则化相结合，这被证明是互补的。我们将MixRep应用于一个E2E LAS架构的Conformer编码器，该编码器使用联合CTC损失进行训练。我们在WSJ数据集和SWB数据集的子集上进行了实验，涵盖了阅读和电话会话语音。实验结果表明，MixRep在低资源ASR方面始终优于其他正则化方法。与强大的SpecAugment基线相比，MixRep在eval92集和eval'2000集的Callhome部分分别实现了+6.5%和+6.7%的相对WER降低。", "summary": "MixRep是一种针对低资源语音识别（ASR）提出的新型数据增强策略。它通过在神经网络的隐藏表示层进行特征维度插值来实现数据混合，并可与时间轴正则化结合。实验证明，MixRep在WSJ和SWB数据集上显著优于现有正则化方法，相对于SpecAugment基线，在WER上取得了显著的相对降低。", "keywords": "MixRep, 数据增强, 低资源语音识别, 隐藏表示, mixup", "comments": "MixRep的创新之处在于将mixup数据增强方法应用于神经网络的隐藏表示层面，并结合了时间轴正则化，这为低资源ASR提供了一种有效且泛化的数据增强手段。其在多种数据集上的显著性能提升表明了其重要性。"}}
{"id": "2506.15626", "title": "Federated Learning for MRI-based BrainAGE: a multicenter study on post-stroke functional outcome prediction", "authors": ["Vincent Roca", "Marc Tommasi", "Paul Andrey", "Aurélien Bellet", "Markus D. Schirmer", "Hilde Henon", "Laurent Puy", "Julien Ramon", "Grégory Kuchcinski", "Martin Bretzner", "Renaud Lopes"], "summary": "$\\textbf{Objective:}$ Brain-predicted age difference (BrainAGE) is a\nneuroimaging biomarker reflecting brain health. However, training robust\nBrainAGE models requires large datasets, often restricted by privacy concerns.\nThis study evaluates the performance of federated learning (FL) for BrainAGE\nestimation in ischemic stroke patients treated with mechanical thrombectomy,\nand investigates its association with clinical phenotypes and functional\noutcomes.\n  $\\textbf{Methods:}$ We used FLAIR brain images from 1674 stroke patients\nacross 16 hospital centers. We implemented standard machine learning and deep\nlearning models for BrainAGE estimates under three data management strategies:\ncentralized learning (pooled data), FL (local training at each site), and\nsingle-site learning. We reported prediction errors and examined associations\nbetween BrainAGE and vascular risk factors (e.g., diabetes mellitus,\nhypertension, smoking), as well as functional outcomes at three months\npost-stroke. Logistic regression evaluated BrainAGE's predictive value for\nthese outcomes, adjusting for age, sex, vascular risk factors, stroke severity,\ntime between MRI and arterial puncture, prior intravenous thrombolysis, and\nrecanalisation outcome.\n  $\\textbf{Results:}$ While centralized learning yielded the most accurate\npredictions, FL consistently outperformed single-site models. BrainAGE was\nsignificantly higher in patients with diabetes mellitus across all models.\nComparisons between patients with good and poor functional outcomes, and\nmultivariate predictions of these outcomes showed the significance of the\nassociation between BrainAGE and post-stroke recovery.\n  $\\textbf{Conclusion:}$ FL enables accurate age predictions without data\ncentralization. The strong association between BrainAGE, vascular risk factors,\nand post-stroke recovery highlights its potential for prognostic modeling in\nstroke care.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15626v1", "AI": {"title_translation": "联邦学习在基于MRI的脑龄预测中的应用：一项多中心中风后功能预后预测研究", "tldr": "本研究评估了联邦学习在缺血性中风患者脑龄估计中的表现，并发现其在不集中数据的情况下也能提供准确预测，且脑龄与血管危险因素及中风后恢复密切相关。", "motivation": "训练稳健的脑龄（BrainAGE）模型需要大量数据集，但数据隐私问题限制了数据的集中。本研究旨在评估联邦学习（FL）在脑龄估计中的性能，并探讨其与临床表型和功能预后的关联。", "method": "研究使用了来自16个医院中心的1674名中风患者的FLAIR脑图像。采用标准机器学习和深度学习模型，在三种数据管理策略下进行脑龄估计：集中学习、联邦学习（FL）和单中心学习。报告了预测误差，并检查了脑龄与血管危险因素（如糖尿病、高血压、吸烟）以及中风后三个月功能预后的关联。使用逻辑回归评估了脑龄对这些预后的预测价值，并对年龄、性别、血管危险因素、中风严重程度等进行了调整。", "result": "集中学习产生了最准确的预测，而联邦学习（FL）始终优于单中心模型。在所有模型中，糖尿病患者的脑龄显著更高。良好和不良功能预后患者之间的比较，以及这些预后的多变量预测，都显示了脑龄与中风后恢复之间关联的重要性。", "conclusion": "联邦学习（FL）能够在不集中数据的情况下实现准确的年龄预测。脑龄、血管危险因素和中风后恢复之间的强关联突显了其在中风护理中预后建模的潜力。", "translation": "**目的：** 脑预测年龄差（BrainAGE）是一种反映大脑健康的神经影像生物标志物。然而，训练稳健的BrainAGE模型需要大型数据集，而这往往受到隐私问题的限制。本研究评估了联邦学习（FL）在接受机械取栓治疗的缺血性中风患者中进行BrainAGE估计的性能，并探讨其与临床表型和功能预后的关联。\n**方法：** 我们使用了来自16个医院中心的1674名中风患者的FLAIR脑图像。我们实施了标准机器学习和深度学习模型，在三种数据管理策略下进行BrainAGE估计：集中学习（数据池化）、FL（在每个站点进行本地训练）和单中心学习。我们报告了预测误差，并检查了BrainAGE与血管危险因素（例如糖尿病、高血压、吸烟）以及中风后三个月功能预后之间的关联。逻辑回归评估了BrainAGE对这些预后的预测价值，并对年龄、性别、血管危险因素、中风严重程度、MRI与动脉穿刺之间的时间、既往静脉溶栓和再通结果进行了调整。\n**结果：** 尽管集中学习产生了最准确的预测，但FL始终优于单中心模型。在所有模型中，糖尿病患者的BrainAGE显著更高。良好和不良功能预后患者之间的比较，以及这些预后的多变量预测，都显示了BrainAGE与中风后恢复之间关联的重要性。\n**结论：** FL能够在不集中数据的情况下实现准确的年龄预测。BrainAGE、血管危险因素和中风后恢复之间的强关联突显了其在中风护理中预后建模的潜力。", "summary": "本研究评估了联邦学习（FL）在基于MRI的脑龄（BrainAGE）估计及其与中风后功能预后关联中的应用。通过分析来自16个中心1674名中风患者的FLAIR图像，研究比较了集中学习、FL和单中心学习策略。结果显示，FL在不集中数据的情况下能实现准确的年龄预测，且其性能优于单中心模型。研究还发现脑龄与血管危险因素（如糖尿病）以及中风后功能恢复之间存在显著关联，表明BrainAGE在中风预后建模中具有潜力。", "keywords": "联邦学习, 脑龄, MRI, 中风预后, 多中心研究", "comments": "这项研究的创新之处在于将联邦学习应用于医学影像领域，特别是在脑龄预测中，有效解决了医疗数据隐私和数据量不足的挑战。其重要性在于证明了FL在不牺牲预测准确性的前提下，可以实现多中心协作研究，从而加速医学研究和临床应用。这对于未来医疗大数据的利用具有重要指导意义。"}}
{"id": "2506.15527", "title": "On Exact Solutions to the Linear Bellman Equation", "authors": ["David Ohlin", "Richard Pates", "Murat Arcak"], "summary": "This paper presents sufficient conditions for optimal control of systems with\ndynamics given by a linear operator, in order to obtain an explicit solution to\nthe Bellman equation that can be calculated in a distributed fashion. Further,\nthe class of Linearly Solvable MDP is reformulated as a continuous-state\noptimal control problem. It is shown that this class naturally satisfies the\nconditions for explicit solution of the Bellman equation, motivating the\nextension of previous results to semilinear dynamics to account for input\nnonlinearities. The applicability of the given conditions is illustrated in\nscenarios with linear and quadratic cost, corresponding to the Stochastic\nShortest Path and Linear-Quadratic Regulator problems.", "comment": "Preprint to be published in the Control Systems Letters", "cate": "math.OC", "url": "http://arxiv.org/abs/2506.15527v1", "AI": {"title_translation": "关于线性贝尔曼方程精确解的研究", "tldr": "本文提出了线性贝尔曼方程显式解的充分条件，并将其应用于可分布式计算的系统，将线性可解MDP重构为连续状态最优控制问题，并展示了其适用于随机最短路径和线性二次调节器问题。", "motivation": "为了获得贝尔曼方程的显式解，该解可以以分布式方式计算。此外，将线性可解MDP类重新表述为连续状态最优控制问题，并将其先前结果扩展到半线性动力学以考虑输入非线性。", "method": "本文提出了具有线性算子给定动力学系统的最优控制的充分条件，以获得贝尔曼方程的显式解。将线性可解MDP类重新表述为连续状态最优控制问题，并证明该类自然满足贝尔曼方程显式解的条件。", "result": "本文表明，线性可解MDP类自然满足贝尔曼方程显式解的条件，这促使将以前的结果扩展到半线性动力学以考虑输入非线性。所给条件的适用性在具有线性和二次成本的情况下得到了说明，对应于随机最短路径和线性二次调节器问题。", "conclusion": "本文提出的条件能够为具有线性和二次成本的系统提供贝尔曼方程的显式解，并在随机最短路径和线性二次调节器问题中得到应用。", "translation": "本文提出了具有线性算子给定动力学系统的最优控制的充分条件，以获得可以分布式计算的贝尔曼方程的显式解。此外，将线性可解MDP类重新表述为连续状态最优控制问题。研究表明，该类自然满足贝尔曼方程显式解的条件，这促使将以前的结果扩展到半线性动力学以考虑输入非线性。所给条件的适用性在具有线性和二次成本的场景中得到了说明，对应于随机最短路径和线性二次调节器问题。", "summary": "本文研究了线性贝尔曼方程的精确解，提出了在系统动力学由线性算子给出时，获得可分布式计算的贝尔曼方程显式解的充分条件。研究将线性可解MDP类重新表述为连续状态最优控制问题，并证明该类自然满足显式解的条件，从而将现有结果扩展到半线性动力学以处理输入非线性。文章通过随机最短路径和线性二次调节器问题中的线性和二次成本场景，验证了所提条件的适用性。", "keywords": "贝尔曼方程, 精确解, 最优控制, 线性可解MDP, 分布式计算", "comments": "本文的创新点在于提出了获得贝尔曼方程显式解的充分条件，并强调了其分布式计算的潜力。通过将线性可解MDP重新表述为连续状态最优控制问题，并将其扩展到半线性动力学，该研究为处理非线性输入提供了新的视角。其重要性体现在为最优控制问题提供了一种可计算的、更广泛的解决方案。"}}
{"id": "2506.14810", "title": "Intelligent Routing for Sparse Demand Forecasting: A Comparative Evaluation of Selection Strategies", "authors": ["Qiwen Zhang"], "summary": "Sparse and intermittent demand forecasting in supply chains presents a\ncritical challenge, as frequent zero-demand periods hinder traditional model\naccuracy and impact inventory management. We propose and evaluate a\nModel-Router framework that dynamically selects the most suitable forecasting\nmodel-spanning classical, ML, and DL methods for each product based on its\nunique demand pattern. By comparing rule-based, LightGBM, and InceptionTime\nrouters, our approach learns to assign appropriate forecasting strategies,\neffectively differentiating between smooth, lumpy, or intermittent demand\nregimes to optimize predictions. Experiments on the large-scale Favorita\ndataset show our deep learning (Inception Time) router improves forecasting\naccuracy by up to 11.8% (NWRMSLE) over strong, single-model benchmarks with\n4.67x faster inference time. Ultimately, these gains in forecasting precision\nwill drive substantial reductions in both stockouts and wasteful excess\ninventory, underscoring the critical role of intelligent, adaptive Al in\noptimizing contemporary supply chain operations.", "comment": "7 pages, 4 figures, conference", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14810v1", "AI": {"title_translation": "稀疏需求预测的智能路由：选择策略的比较评估", "tldr": "针对供应链中稀疏和间歇性需求预测的挑战，本文提出了一个动态选择最合适预测模型的Model-Router框架，并通过实验证明其能显著提高预测精度并优化库存管理。", "motivation": "供应链中稀疏和间歇性需求预测是一个严峻挑战，频繁的零需求期会阻碍传统模型的准确性并影响库存管理。", "method": "本文提出并评估了一个Model-Router框架，该框架根据每个产品的独特需求模式动态选择最合适的预测模型，涵盖了经典、机器学习和深度学习方法。通过比较基于规则、LightGBM和InceptionTime路由器，该方法学习分配合适的预测策略，有效区分平滑、块状或间歇性需求模式以优化预测。", "result": "在大型Favorita数据集上的实验表明，深度学习（Inception Time）路由器比强大的单一模型基准提高了高达11.8%的预测精度（NWRMSLE），推理时间快4.67倍。", "conclusion": "预测精度的提高将显著减少缺货和浪费的过剩库存，强调了智能、自适应AI在优化当代供应链运营中的关键作用。", "translation": "供应链中稀疏和间歇性需求预测是一个严峻挑战，因为频繁的零需求期阻碍了传统模型的准确性并影响库存管理。我们提出并评估了一个模型路由器（Model-Router）框架，该框架根据每个产品的独特需求模式动态选择最合适的预测模型，涵盖了经典、机器学习和深度学习方法。通过比较基于规则、LightGBM和InceptionTime路由器，我们的方法学会了分配适当的预测策略，有效地区分平滑、块状或间歇性需求模式以优化预测。在大型Favorita数据集上的实验表明，我们的深度学习（Inception Time）路由器比强大的单一模型基准提高了高达11.8%的预测精度（NWRMSLE），推理时间快4.67倍。最终，这些预测精度的提升将显著减少缺货和浪费的过剩库存，强调了智能、自适应AI在优化当代供应链运营中的关键作用。", "summary": "本文针对供应链中稀疏和间歇性需求预测的挑战，提出了一个Model-Router框架。该框架能够根据产品的需求模式动态选择最佳预测模型，涵盖了传统、机器学习和深度学习方法。通过比较不同类型的路由器，研究发现基于深度学习的InceptionTime路由器在提高预测精度和推理速度方面表现出色，最终有助于优化库存管理并减少缺货和过剩库存。", "keywords": "稀疏需求预测, 智能路由, 模型选择, 供应链, 深度学习", "comments": "这项研究的创新之处在于提出了一个动态模型选择的“路由器”框架，以应对稀疏需求预测的复杂性。它通过学习为不同需求模式匹配最合适的预测模型，超越了单一模型的局限性。其重要性在于显著提高了预测精度和效率，对优化供应链库存管理具有直接的实际价值。"}}
{"id": "2506.15211", "title": "ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs", "authors": ["Feng He", "Zijun Chen", "Xinnian Liang", "Tingting Ma", "Yunqi Qiu", "Shuangzhi Wu", "Junchi Yan"], "summary": "Recent advances in Large Reasoning Models (LRMs) trained with Long\nChain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain\ngeneralization capabilities. However, the underlying mechanisms supporting such\ntransfer remain poorly understood. We hypothesize that cross-domain\ngeneralization arises from shared abstract reasoning prototypes -- fundamental\nreasoning patterns that capture the essence of problems across domains. These\nprototypes minimize the nuances of the representation, revealing that seemingly\ndiverse tasks are grounded in shared reasoning structures.Based on this\nhypothesis, we propose ProtoReasoning, a framework that enhances the reasoning\nability of LLMs by leveraging scalable and verifiable prototypical\nrepresentations (Prolog for logical reasoning, PDDL for\nplanning).ProtoReasoning features: (1) an automated prototype construction\npipeline that transforms problems into corresponding prototype representations;\n(2) a comprehensive verification system providing reliable feedback through\nProlog/PDDL interpreters; (3) the scalability to synthesize problems\narbitrarily within prototype space while ensuring correctness. Extensive\nexperiments show that ProtoReasoning achieves 4.7% improvement over baseline\nmodels on logical reasoning (Enigmata-Eval), 6.3% improvement on planning\ntasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics\n(AIME24). Significantly, our ablation studies confirm that learning in\nprototype space also demonstrates enhanced generalization to structurally\nsimilar problems compared to training solely on natural language\nrepresentations, validating our hypothesis that reasoning prototypes serve as\nthe foundation for generalizable reasoning in large language models.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15211v1", "AI": {"title_translation": "ProtoReasoning：原型作为大型语言模型中可泛化推理的基础", "tldr": "ProtoReasoning是一个通过利用可扩展和可验证的原型表示（如Prolog/PDDL）来增强LLM推理和泛化能力的框架，并在多项任务上取得了显著提升。", "motivation": "尽管大型推理模型（LRMs）在长链式思维训练后展现出卓越的跨领域泛化能力，但其底层机制尚不明确。本文假设这种泛化源于共享的抽象推理原型。", "method": "本文提出了ProtoReasoning框架，旨在通过利用可扩展和可验证的原型表示（Prolog用于逻辑推理，PDDL用于规划）来增强LLM的推理能力。该框架包含：1) 一个自动化原型构建管道，将问题转换为相应的原型表示；2) 一个全面的验证系统，通过Prolog/PDDL解释器提供可靠的反馈；3) 在原型空间内任意合成问题并确保正确性的可扩展性。", "result": "ProtoReasoning在逻辑推理（Enigmata-Eval）上比基线模型提高了4.7%，在规划任务上提高了6.3%，在通用推理（MMLU）上提高了4.0%，在数学（AIME24）上提高了1.0%。消融研究证实，与仅在自然语言表示上训练相比，在原型空间中学习更能增强对结构相似问题的泛化能力。", "conclusion": "研究验证了推理原型是大型语言模型中可泛化推理的基础，并且ProtoReasoning框架能够有效提升LLM的推理和泛化能力。", "translation": "大型推理模型（LRMs）最近在长链式思维（Long CoT）推理方面取得的进展，展示了卓越的跨领域泛化能力。然而，支持这种迁移的底层机制仍知之甚少。我们假设跨领域泛化源于共享的抽象推理原型——捕捉跨领域问题本质的基本推理模式。这些原型最大限度地减少了表示的细微差别，揭示了看似不同的任务实际上根植于共享的推理结构。基于这一假设，我们提出了ProtoReasoning，一个通过利用可扩展和可验证的原型表示（Prolog用于逻辑推理，PDDL用于规划）来增强LLM推理能力的框架。ProtoReasoning的特点包括：(1) 一个自动化原型构建管道，将问题转换为相应的原型表示；(2) 一个全面的验证系统，通过Prolog/PDDL解释器提供可靠的反馈；(3) 在原型空间内任意合成问题并确保正确性的可扩展性。广泛的实验表明，ProtoReasoning在逻辑推理（Enigmata-Eval）上比基线模型提高了4.7%，在规划任务上提高了6.3%，在通用推理（MMLU）上提高了4.0%，在数学（AIME24）上提高了1.0%。重要的是，我们的消融研究证实，与仅在自然语言表示上训练相比，在原型空间中学习也表现出对结构相似问题的增强泛化能力，这验证了我们的假设，即推理原型是大型语言模型中可泛化推理的基础。", "summary": "本文提出了ProtoReasoning框架，旨在通过引入抽象推理原型（如Prolog和PDDL）来增强大型语言模型（LLMs）的推理和跨领域泛化能力。该框架包括自动化原型构建、验证系统和可扩展的问题合成。实验结果表明，ProtoReasoning在多项推理任务上取得了显著性能提升，并且通过消融研究验证了原型学习能有效提高LLMs的泛化能力。", "keywords": "原型推理, 大型语言模型, 泛化能力, 逻辑推理, 规划", "comments": "该研究创新性地提出了“抽象推理原型”的概念，并构建了ProtoReasoning框架，通过结构化的原型表示（如Prolog/PDDL）来弥补LLM在复杂推理泛化能力上的不足。其关键在于将自然语言问题转化为形式化原型，并通过验证系统确保正确性，这为提升LLM的可靠性和可解释性提供了新思路。"}}
{"id": "2506.15259", "title": "Splitting-based randomised dynamical low-rank approximations for stiff matrix differential equations", "authors": ["Zi Wu", "Yong-Liang Zhao"], "summary": "In the fields of control theory and machine learning, the dynamic low-rank\napproximation for large-scale matrices has received substantial attention.\nConsidering the large-scale semilinear stiff matrix differential equations, we\npropose a dynamic numerical integrator for obtaining low-rank approximations of\nsolutions. We first decompose the differential equation into a stiff linear\ncomponent and a nonstiff nonlinear term, then employ an exponential integrator\nalong with a dynamic low-rank approach to resolve these subsystems,\nrespectively. Furthermore, the proposed framework naturally extends to\nrank-adaptation scenarios. Through rigorous validation on canonical stiff\nmatrix differential problems, including spatially discretized Allen-Cahn\nequations and differential Riccati equations, we demonstrate that the method\nachieves the theoretically predicted convergence orders. Numerical evidence\nconfirms the robustness and accuracy of the proposed methods.", "comment": "6 figures, 12 tables", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15259v1", "AI": {"title_translation": "基于分裂的随机动态低秩近似用于刚性矩阵微分方程", "tldr": "提出了一种新的基于分裂的随机动态低秩近似方法，用于求解刚性矩阵微分方程，并证明了其鲁棒性和准确性。", "motivation": "在控制理论和机器学习领域，大规模矩阵的动态低秩近似受到广泛关注，尤其针对大规模半线性刚性矩阵微分方程，需要有效的低秩近似方法。", "method": "将微分方程分解为刚性线性分量和非刚性非线性项，然后分别采用指数积分器和动态低秩方法来解决这些子系统。该框架还自然地扩展到秩自适应场景。", "result": "该方法达到了理论预测的收敛阶数。通过对Allen-Cahn方程和微分Riccati方程等典型刚性矩阵微分问题的验证，证实了所提方法的鲁棒性和准确性。", "conclusion": "所提出的基于分裂的随机动态低秩近似方法对于刚性矩阵微分方程是鲁棒、准确的，并能达到理论收敛阶数。", "translation": "在控制理论和机器学习领域，大规模矩阵的动态低秩近似受到了广泛关注。考虑到大规模半线性刚性矩阵微分方程，我们提出了一种用于获取解的低秩近似的动态数值积分器。我们首先将微分方程分解为刚性线性分量和非刚性非线性项，然后分别采用指数积分器和动态低秩方法来解决这些子系统。此外，所提出的框架自然地扩展到秩自适应场景。通过对典型的刚性矩阵微分问题（包括空间离散化的Allen-Cahn方程和微分Riccati方程）进行严格验证，我们证明了该方法达到了理论预测的收敛阶数。数值证据证实了所提方法的鲁棒性和准确性。", "summary": "本文提出了一种新颖的基于分裂的动态数值积分器，用于求解大规模半线性刚性矩阵微分方程的低秩近似解。该方法将方程分解为刚性线性部分和非刚性非线性部分，并分别应用指数积分器和动态低秩方法进行处理，同时支持秩自适应。通过对Allen-Cahn和Riccati方程的验证，证明了该方法达到了理论收敛阶数，并具有良好的鲁棒性和准确性。", "keywords": "动态低秩近似, 刚性矩阵微分方程, 分裂方法, 指数积分器, 秩自适应", "comments": "该研究的创新之处在于结合了分裂方法、指数积分器和动态低秩近似来处理刚性大规模矩阵微分方程。其对秩自适应场景的扩展增加了方法的实用性。通过对典型问题的严格验证，增强了研究结果的可信度。"}}
{"id": "2506.15417", "title": "Detecting Hardware Trojans in Microprocessors via Hardware Error Correction Code-based Modules", "authors": ["Alessandro Palumbo", "Ruben Salvador"], "summary": "Software-exploitable Hardware Trojans (HTs) enable attackers to execute\nunauthorized software or gain illicit access to privileged operations. This\nmanuscript introduces a hardware-based methodology for detecting runtime HT\nactivations using Error Correction Codes (ECCs) on a RISC-V microprocessor.\nSpecifically, it focuses on HTs that inject malicious instructions, disrupting\nthe normal execution flow by triggering unauthorized programs. To counter this\nthreat, the manuscript introduces a Hardware Security Checker (HSC) leveraging\nHamming Single Error Correction (HSEC) architectures for effective HT\ndetection. Experimental results demonstrate that the proposed solution achieves\na 100% detection rate for potential HT activations, with no false positives or\nundetected attacks. The implementation incurs minimal overhead, requiring only\n72 #LUTs, 24 #FFs, and 0.5 #BRAM while maintaining the microprocessor's\noriginal operating frequency and introducing no additional time delay.", "comment": "To appear at the 31st IEEE International Symposium on On-Line Testing\n  and Robust System Design (IOLTS) 2025, 7 pages, 5 figures,", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15417v1", "AI": {"title_translation": "基于硬件纠错码模块的微处理器硬件木马检测", "tldr": "本文提出了一种利用纠错码在RISC-V微处理器上检测运行时硬件木马（HTs）的硬件方法，实现了100%的检测率和极低的开销。", "motivation": "软件可利用的硬件木马（HTs）使攻击者能够执行未经授权的软件或非法获取特权操作。为了对抗这种威胁，本文旨在开发一种有效的检测方法。", "method": "本文引入了一种硬件安全检查器（HSC），利用汉明单错误纠正（HSEC）架构，在RISC-V微处理器上基于硬件纠错码（ECCs）检测运行时硬件木马的激活，特别是那些注入恶意指令以扰乱正常执行流的木马。", "result": "该解决方案对潜在的硬件木马激活实现了100%的检测率，没有误报或未检测到的攻击。实施开销极小，仅需72个LUT、24个FF和0.5个BRAM，同时保持微处理器的原始工作频率且不引入额外的时间延迟。", "conclusion": "所提出的基于硬件纠错码的硬件木马检测方法能够高效且准确地检测微处理器中的恶意指令注入型硬件木马，具有极低的资源开销和高性能。", "translation": "软件可利用的硬件木马（HTs）使攻击者能够执行未经授权的软件或非法获取特权操作。本文介绍了一种基于硬件的方法，用于在RISC-V微处理器上使用纠错码（ECCs）检测运行时HT的激活。具体来说，它侧重于注入恶意指令、通过触发未经授权的程序来扰乱正常执行流的HTs。为了应对这一威胁，本文引入了一种硬件安全检查器（HSC），利用汉明单错误纠正（HSEC）架构进行有效的HT检测。实验结果表明，所提出的解决方案对潜在的HT激活实现了100%的检测率，没有误报或未检测到的攻击。该实现开销极小，仅需72个LUT、24个FF和0.5个BRAM，同时保持微处理器的原始工作频率且不引入额外的时间延迟。", "summary": "本文提出了一种基于硬件纠错码（ECCs）的硬件安全检查器（HSC），用于在RISC-V微处理器运行时检测软件可利用的硬件木马（HTs），特别是注入恶意指令的HTs。该方法利用汉明单错误纠正（HSEC）架构，实验结果显示其对HTs的检测率达到100%，无误报，且硬件开销极低，不影响处理器性能。", "keywords": "硬件木马, 纠错码, RISC-V, 硬件安全, 检测率", "comments": "该论文的创新点在于将纠错码技术应用于硬件木马检测，提供了一种高效且低开销的硬件级解决方案。其在检测率和资源利用方面的优异表现，使其在微处理器安全领域具有重要意义。"}}
{"id": "2506.15157", "title": "Robust Instant Policy: Leveraging Student's t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation", "authors": ["Hanbit Oh", "Andrea M. Salcedo-Vázquez", "Ixchel G. Ramirez-Alpizar", "Yukiyasu Domae"], "summary": "Imitation learning (IL) aims to enable robots to perform tasks autonomously\nby observing a few human demonstrations. Recently, a variant of IL, called\nIn-Context IL, utilized off-the-shelf large language models (LLMs) as instant\npolicies that understand the context from a few given demonstrations to perform\na new task, rather than explicitly updating network models with large-scale\ndemonstrations. However, its reliability in the robotics domain is undermined\nby hallucination issues such as LLM-based instant policy, which occasionally\ngenerates poor trajectories that deviate from the given demonstrations. To\nalleviate this problem, we propose a new robust in-context imitation learning\nalgorithm called the robust instant policy (RIP), which utilizes a Student's\nt-regression model to be robust against the hallucinated trajectories of\ninstant policies to allow reliable trajectory generation. Specifically, RIP\ngenerates several candidate robot trajectories to complete a given task from an\nLLM and aggregates them using the Student's t-distribution, which is beneficial\nfor ignoring outliers (i.e., hallucinations); thereby, a robust trajectory\nagainst hallucinations is generated. Our experiments, conducted in both\nsimulated and real-world environments, show that RIP significantly outperforms\nstate-of-the-art IL methods, with at least $26\\%$ improvement in task success\nrates, particularly in low-data scenarios for everyday tasks. Video results\navailable at https://sites.google.com/view/robustinstantpolicy.", "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2025 accepted", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15157v1", "AI": {"title_translation": "鲁棒即时策略：利用学生t回归模型实现机器人操作的鲁棒上下文模仿学习", "tldr": "本文提出了一种名为鲁棒即时策略（RIP）的新型上下文模仿学习算法，它利用学生t回归模型来聚合LLM生成的候选轨迹，从而有效抵御幻觉问题，显著提高了机器人在低数据场景下的任务成功率。", "motivation": "传统的上下文模仿学习（In-Context IL）利用大型语言模型（LLMs）作为即时策略，但在机器人领域，LLM的幻觉问题（即偶尔生成偏离演示的糟糕轨迹）严重影响了其可靠性。", "method": "本文提出了一种名为鲁棒即时策略（RIP）的新型鲁棒上下文模仿学习算法。RIP通过LLM生成多个候选机器人轨迹来完成给定任务，然后使用学生t分布聚合这些轨迹。学生t分布的特性有助于忽略异常值（即幻觉），从而生成对幻觉具有鲁棒性的轨迹。", "result": "在模拟和真实世界环境中的实验表明，RIP显著优于最先进的模仿学习方法，任务成功率至少提高了26%，特别是在日常任务的低数据场景下表现出色。", "conclusion": "本文提出的鲁棒即时策略（RIP）通过利用学生t回归模型聚合LLM生成的轨迹，有效解决了上下文模仿学习中的幻觉问题，显著提高了机器人操作的可靠性和任务成功率。", "translation": "模仿学习（IL）旨在通过观察少量人类演示，使机器人能够自主执行任务。最近，一种模仿学习的变体，称为上下文模仿学习（In-Context IL），利用现成的大型语言模型（LLMs）作为即时策略，通过理解少量给定演示的上下文来执行新任务，而不是通过大规模演示显式更新网络模型。然而，其在机器人领域的可靠性受到幻觉问题的损害，例如基于LLM的即时策略偶尔会生成偏离给定演示的糟糕轨迹。为了缓解这个问题，我们提出了一种新的鲁棒上下文模仿学习算法，称为鲁棒即时策略（RIP），它利用学生t回归模型来抵御即时策略的幻觉轨迹，从而实现可靠的轨迹生成。具体来说，RIP从LLM生成多个完成给定任务的候选机器人轨迹，并使用学生t分布聚合它们，这有利于忽略异常值（即幻觉）；因此，生成了对幻觉具有鲁棒性的轨迹。我们在模拟和真实世界环境中进行的实验表明，RIP显著优于最先进的模仿学习方法，任务成功率至少提高了26%，特别是在日常任务的低数据场景下。视频结果可在 https://sites.google.com/view/robustinstantpolicy 获取。", "summary": "本文针对机器人上下文模仿学习中大型语言模型（LLM）产生的幻觉轨迹导致可靠性降低的问题，提出了一种名为鲁棒即时策略（RIP）的新算法。RIP利用LLM生成多个候选轨迹，并通过学生t回归模型聚合这些轨迹，以有效识别并抑制异常值（幻觉），从而生成鲁棒的机器人操作轨迹。实验结果表明，RIP在模拟和真实环境中，尤其是在数据量较少的情况下，显著提高了任务成功率，超越了现有先进方法。", "keywords": "模仿学习, 上下文学习, 大型语言模型, 机器人操作, 学生t回归", "comments": "本文的创新点在于将学生t回归模型引入到上下文模仿学习中，以解决LLM在生成机器人轨迹时存在的幻觉问题。这种方法利用了学生t分布对异常值不敏感的特性，有效提升了即时策略的鲁棒性和可靠性。其在低数据场景下的显著性能提升，对于实际机器人部署具有重要意义。"}}
{"id": "2506.15468", "title": "Co-Creative Learning via Metropolis-Hastings Interaction between Humans and AI", "authors": ["Ryota Okumura", "Tadahiro Taniguchi", "Akira Taniguchi", "Yoshinobu Hagiwara"], "summary": "We propose co-creative learning as a novel paradigm where humans and AI,\ni.e., biological and artificial agents, mutually integrate their partial\nperceptual information and knowledge to construct shared external\nrepresentations, a process we interpret as symbol emergence. Unlike traditional\nAI teaching based on unilateral knowledge transfer, this addresses the\nchallenge of integrating information from inherently different modalities. We\nempirically test this framework using a human-AI interaction model based on the\nMetropolis-Hastings naming game (MHNG), a decentralized Bayesian inference\nmechanism. In an online experiment, 69 participants played a joint attention\nnaming game (JA-NG) with one of three computer agent types (MH-based,\nalways-accept, or always-reject) under partial observability. Results show that\nhuman-AI pairs with an MH-based agent significantly improved categorization\naccuracy through interaction and achieved stronger convergence toward a shared\nsign system. Furthermore, human acceptance behavior aligned closely with the\nMH-derived acceptance probability. These findings provide the first empirical\nevidence for co-creative learning emerging in human-AI dyads via MHNG-based\ninteraction. This suggests a promising path toward symbiotic AI systems that\nlearn with humans, rather than from them, by dynamically aligning perceptual\nexperiences, opening a new venue for symbiotic AI alignment.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15468v1", "AI": {"title_translation": "人类与人工智能之间基于Metropolis-Hastings交互的协同创造学习", "tldr": "本文提出了一种新颖的协同创造学习范式，即人类与AI通过Metropolis-Hastings命名博弈进行交互，共同整合信息形成共享表征。实验结果表明，与基于MH的AI交互的人类显著提高了分类准确性并趋向于共享的符号系统，为共生AI系统提供了新的方向。", "motivation": "传统的AI教学基于单向知识传递，无法有效整合来自不同模态的信息。本文旨在解决人类与AI在信息整合方面的挑战，提出一种让生物和人工智能体相互整合部分感知信息和知识以构建共享外部表征的新范式。", "method": "本文提出了协同创造学习范式，并将其解释为符号涌现过程。研究人员通过基于Metropolis-Hastings命名博弈（MHNG）的人机交互模型对该框架进行了实证测试。在一项在线实验中，69名参与者在部分可观察性条件下，与三种计算机代理类型（基于MH、始终接受或始终拒绝）之一玩了一个联合注意力命名博弈（JA-NG）。", "result": "实验结果显示，与基于MH的代理交互的人机配对通过交互显著提高了分类准确性，并实现了向共享符号系统更强的收敛。此外，人类的接受行为与MH导出的接受概率密切吻合。", "conclusion": "这些发现首次为人类与AI通过基于MHNG的交互实现协同创造学习提供了实证证据。这表明通过动态调整感知经验，实现AI与人类共同学习而非仅仅从人类学习，为共生AI对齐开辟了一条有前景的道路。", "translation": "我们提出协同创造学习作为一种新颖的范式，其中人类和人工智能，即生物和人工主体，相互整合其部分感知信息和知识以构建共享的外部表征，我们将这一过程解释为符号涌现。与基于单向知识传递的传统人工智能教学不同，这解决了整合来自本质上不同模态的信息的挑战。我们使用基于Metropolis-Hastings命名博弈（MHNG）的人机交互模型对该框架进行了实证测试，MHNG是一种去中心化的贝叶斯推理机制。在一项在线实验中，69名参与者在部分可观察性条件下，与三种计算机代理类型（基于MH、始终接受或始终拒绝）之一玩了一个联合注意力命名博弈（JA-NG）。结果显示，与基于MH的代理交互的人机配对通过交互显著提高了分类准确性，并实现了向共享符号系统更强的收敛。此外，人类的接受行为与MH导出的接受概率密切吻合。这些发现首次为人类与AI通过基于MHNG的交互实现协同创造学习提供了实证证据。这表明通过动态调整感知经验，实现AI与人类共同学习而非仅仅从人类学习，为共生AI对齐开辟了一条有前景的道路。", "summary": "本文提出了一种名为“协同创造学习”的新范式，旨在解决人类与AI之间信息整合的挑战。该范式主张人类与AI相互整合感知信息和知识以构建共享表征。研究通过基于Metropolis-Hastings命名博弈的在线实验进行验证，结果表明，与基于MH的AI交互的人机配对显著提高了分类准确性并趋向于共享的符号系统。这为实现与人类共同学习而非仅仅从人类学习的共生AI系统提供了经验证据和新的研究方向。", "keywords": "协同创造学习, 人机交互, Metropolis-Hastings命名博弈, 共生AI, 符号涌现", "comments": "该论文提出了一种创新的“协同创造学习”范式，将人机交互从单向知识传递转变为双向信息整合，这在人机协作领域具有重要意义。通过Metropolis-Hastings命名博弈提供实证证据，验证了人类与AI共同构建共享符号系统的可行性，为未来开发更具适应性和共生性的AI系统提供了坚实的基础和新的视角。"}}
{"id": "2506.14907", "title": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning", "authors": ["Yizhen Zhang", "Yang Ding", "Shuoshuo Zhang", "Xinchen Zhang", "Haoling Li", "Zhong-zhi Li", "Peijie Wang", "Jie Wu", "Lei Ji", "Yelong Shen", "Yujiu Yang", "Yeyun Gong"], "summary": "Inspired by the impressive reasoning capabilities demonstrated by\nreinforcement learning approaches like DeepSeek-R1, recent emerging research\nhas begun exploring the use of reinforcement learning (RL) to enhance\nvision-language models (VLMs) for multimodal reasoning tasks. However, most\nexisting multimodal reinforcement learning approaches remain limited to spatial\nreasoning within single-image contexts, yet still struggle to generalize to\nmore complex and real-world scenarios involving multi-image positional\nreasoning, where understanding the relationships across images is crucial. To\naddress this challenge, we propose a general reinforcement learning approach\nPeRL tailored for interleaved multimodal tasks, and a multi-stage strategy\ndesigned to enhance the exploration-exploitation trade-off, thereby improving\nlearning efficiency and task performance. Specifically, we introduce\npermutation of image sequences to simulate varied positional relationships to\nexplore more spatial and positional diversity. Furthermore, we design a rollout\nfiltering mechanism for resampling to focus on trajectories that contribute\nmost to learning optimal behaviors to exploit learned policies effectively. We\nevaluate our model on 5 widely-used multi-image benchmarks and 3 single-image\nbenchmarks. Our experiments confirm that PeRL trained model consistently\nsurpasses R1-related and interleaved VLM baselines by a large margin, achieving\nstate-of-the-art performance on multi-image benchmarks, while preserving\ncomparable performance on single-image tasks.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14907v1", "AI": {"title_translation": "PeRL：用于交错式视觉-语言推理的置换增强强化学习", "tldr": "PeRL提出了一种置换增强强化学习方法，以解决多图像情境下视觉-语言模型在空间推理方面的不足，并在多图像基准测试中取得了最先进的性能。", "motivation": "现有的多模态强化学习方法主要局限于单图像上下文中的空间推理，难以泛化到涉及多图像位置推理的更复杂和真实世界的场景，而理解图像之间的关系至关重要。", "method": "提出了一种通用的强化学习方法PeRL，专为交错式多模态任务设计，并采用多阶段策略以增强探索-利用权衡。具体地，引入图像序列置换来模拟多样的位置关系，以探索更多的空间和位置多样性。此外，设计了一个推出过滤机制进行重采样，以关注最有助于学习最优行为的轨迹。", "result": "PeRL训练的模型在5个广泛使用的多图像基准测试中持续超越R1相关和交错式VLM基线，取得了最先进的性能，同时在单图像任务上保持了可比的性能。", "conclusion": "PeRL通过引入置换增强和推出过滤机制，有效提升了强化学习在多图像视觉-语言推理任务中的表现，解决了现有方法在复杂多图像场景中泛化能力不足的问题，并取得了显著的性能提升。", "translation": "受到DeepSeek-R1等强化学习方法所展示的令人印象深刻的推理能力的启发，最近新兴的研究已经开始探索使用强化学习（RL）来增强视觉-语言模型（VLM），以进行多模态推理任务。然而，大多数现有的多模态强化学习方法仍然局限于单图像上下文中的空间推理，但仍然难以泛化到涉及多图像位置推理的更复杂和真实世界场景，在这些场景中理解图像之间的关系至关重要。为了解决这一挑战，我们提出了一种通用的强化学习方法PeRL，专为交错式多模态任务量身定制，并设计了一种多阶段策略，旨在增强探索-利用权衡，从而提高学习效率和任务性能。具体来说，我们引入图像序列的置换来模拟多样的位置关系，以探索更多的空间和位置多样性。此外，我们设计了一个推出过滤机制进行重采样，以关注最有助于学习最优行为的轨迹，从而有效地利用学习到的策略。我们在5个广泛使用的多图像基准测试和3个单图像基准测试上评估了我们的模型。我们的实验证实，PeRL训练的模型持续大幅超越R1相关和交错式VLM基线，在多图像基准测试中取得了最先进的性能，同时在单图像任务上保持了可比的性能。", "summary": "该论文提出了PeRL，一种用于交错式视觉-语言推理的置换增强强化学习方法。针对现有RL增强VLM在多图像空间推理方面的局限性，PeRL引入了图像序列置换来模拟多样位置关系，并设计了推出过滤机制以优化策略学习。实验结果表明，PeRL在多图像基准测试中显著超越了现有基线，达到了最先进的性能，并在单图像任务上保持了竞争力。", "keywords": "强化学习, 视觉-语言推理, 多模态, 置换, 空间推理", "comments": "PeRL的创新之处在于其针对多图像情境下的视觉-语言推理问题，引入了“图像序列置换”来模拟和探索更丰富的空间关系，以及“推出过滤机制”来优化学习效率和策略利用。这有效地解决了现有方法在处理多图像复杂场景时的泛化性不足问题，对于提升RL在多模态推理中的应用具有重要意义。"}}
{"id": "2506.14811", "title": "Self-Composing Policies for Scalable Continual Reinforcement Learning", "authors": ["Mikel Malagón", "Josu Ceberio", "Jose A. Lozano"], "summary": "This work introduces a growable and modular neural network architecture that\nnaturally avoids catastrophic forgetting and interference in continual\nreinforcement learning. The structure of each module allows the selective\ncombination of previous policies along with its internal policy, accelerating\nthe learning process on the current task. Unlike previous growing neural\nnetwork approaches, we show that the number of parameters of the proposed\napproach grows linearly with respect to the number of tasks, and does not\nsacrifice plasticity to scale. Experiments conducted in benchmark continuous\ncontrol and visual problems reveal that the proposed approach achieves greater\nknowledge transfer and performance than alternative methods.", "comment": "ICML 2024 (oral)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14811v1", "AI": {"title_translation": "可扩展持续强化学习的自组合策略", "tldr": "本文提出了一种可增长、模块化的神经网络架构，用于持续强化学习，能有效避免灾难性遗忘和干扰，并实现参数的线性增长和卓越的知识迁移及性能。", "motivation": "在持续强化学习中，现有方法难以自然地避免灾难性遗忘和干扰，并且在扩展时参数增长可能过快或牺牲可塑性。本文旨在解决这些问题。", "method": "本工作引入了一种可增长、模块化的神经网络架构。该架构的每个模块都允许选择性地结合先前的策略及其内部策略，以加速当前任务的学习过程。与以往的增长型神经网络方法不同，所提出的方法参数数量随任务数量线性增长，且不牺牲可塑性。", "result": "实验表明，所提出的方法参数数量随任务数量线性增长，且不牺牲可塑性。在基准连续控制和视觉问题上的实验揭示，该方法比替代方法实现了更大的知识迁移和更好的性能。", "conclusion": "所提出的自组合策略的模块化神经网络架构在持续强化学习中成功避免了灾难性遗忘和干扰，实现了参数的线性增长，并在知识迁移和性能方面优于现有方法。", "translation": "这项工作引入了一种可增长、模块化的神经网络架构，它在持续强化学习中自然地避免了灾难性遗忘和干扰。每个模块的结构允许选择性地结合先前的策略及其内部策略，从而加速当前任务的学习过程。与以往的增长型神经网络方法不同，我们表明所提出方法的参数数量随任务数量线性增长，并且不牺牲可塑性以实现扩展。在基准连续控制和视觉问题上进行的实验表明，所提出的方法比替代方法实现了更大的知识迁移和更高的性能。", "summary": "本文提出了一种新颖的可增长、模块化的神经网络架构，旨在解决持续强化学习中的灾难性遗忘和干扰问题。该架构的独特之处在于其模块能够选择性地组合旧策略与新策略，从而加速新任务的学习。研究表明，该方法的参数数量随任务数量线性增长，且不牺牲模型的可塑性。实验结果验证了其在知识迁移和性能上优于现有替代方案。", "keywords": "持续强化学习, 灾难性遗忘, 模块化神经网络, 知识迁移, 可扩展性", "comments": "本文提出了一种创新的模块化神经网络架构，有效解决了持续强化学习中长期存在的灾难性遗忘和干扰问题。其核心创新在于模块化的自组合策略，以及参数数量随任务数量线性增长的特性，这显著提升了模型的可扩展性。该方法在保持可塑性的同时实现了卓越的知识迁移和性能，为未来的持续学习研究提供了有价值的方向。"}}
{"id": "2506.15276", "title": "MSNeRV: Neural Video Representation with Multi-Scale Feature Fusion", "authors": ["Jun Zhu", "Xinfeng Zhang", "Lv Tang", "JunHao Jiang"], "summary": "Implicit Neural representations (INRs) have emerged as a promising approach\nfor video compression, and have achieved comparable performance to the\nstate-of-the-art codecs such as H.266/VVC. However, existing INR-based methods\nstruggle to effectively represent detail-intensive and fast-changing video\ncontent. This limitation mainly stems from the underutilization of internal\nnetwork features and the absence of video-specific considerations in network\ndesign. To address these challenges, we propose a multi-scale feature fusion\nframework, MSNeRV, for neural video representation. In the encoding stage, we\nenhance temporal consistency by employing temporal windows, and divide the\nvideo into multiple Groups of Pictures (GoPs), where a GoP-level grid is used\nfor background representation. Additionally, we design a multi-scale spatial\ndecoder with a scale-adaptive loss function to integrate multi-resolution and\nmulti-frequency information. To further improve feature extraction, we\nintroduce a multi-scale feature block that fully leverages hidden features. We\nevaluate MSNeRV on HEVC ClassB and UVG datasets for video representation and\ncompression. Experimental results demonstrate that our model exhibits superior\nrepresentation capability among INR-based approaches and surpasses VTM-23.7\n(Random Access) in dynamic scenarios in terms of compression efficiency.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15276v1", "AI": {"title_translation": "MSNeRV：多尺度特征融合的神经视频表示", "tldr": "MSNeRV提出了一种多尺度特征融合框架，用于神经视频表示，解决了现有INR方法在处理细节丰富和快速变化的视频内容时的不足，并在表示能力和压缩效率上超越了现有技术。", "motivation": "现有基于隐式神经表示（INR）的视频压缩方法难以有效表示细节丰富和快速变化的视频内容。这主要是因为内部网络特征利用不足以及网络设计中缺乏视频特异性考虑。", "method": "本文提出了MSNeRV，一个用于神经视频表示的多尺度特征融合框架。在编码阶段，通过使用时间窗口增强时间一致性，并将视频划分为多个图片组（GoPs），其中GoP级网格用于背景表示。此外，设计了一个带有尺度自适应损失函数的多尺度空间解码器，以整合多分辨率和多频率信息。为了进一步改进特征提取，引入了一个多尺度特征块，充分利用隐藏特征。", "result": "MSNeRV在HEVC ClassB和UVG数据集上进行了视频表示和压缩评估。实验结果表明，该模型在基于INR的方法中表现出卓越的表示能力，并在动态场景中，在压缩效率方面超越了VTM-23.7（随机访问）。", "conclusion": "MSNeRV通过多尺度特征融合和视频特定设计，显著提升了隐式神经表示在视频压缩和表示方面的性能，尤其在处理动态和细节丰富的视频内容时表现出色。", "translation": "隐式神经表示（INRs）已成为一种很有前途的视频压缩方法，并取得了与H.266/VVC等最先进编解码器相当的性能。然而，现有的基于INR的方法在有效表示细节丰富和快速变化的视频内容方面存在困难。这种局限性主要源于内部网络特征的利用不足以及网络设计中缺乏视频特异性考虑。为了解决这些挑战，我们提出了一种用于神经视频表示的多尺度特征融合框架MSNeRV。在编码阶段，我们通过采用时间窗口增强时间一致性，并将视频划分为多个图片组（GoPs），其中GoP级网格用于背景表示。此外，我们设计了一个带有尺度自适应损失函数的多尺度空间解码器，以整合多分辨率和多频率信息。为了进一步改进特征提取，我们引入了一个多尺度特征块，充分利用隐藏特征。我们在HEVC ClassB和UVG数据集上对MSNeRV进行了视频表示和压缩评估。实验结果表明，我们的模型在基于INR的方法中表现出卓越的表示能力，并在动态场景中，在压缩效率方面超越了VTM-23.7（随机访问）。", "summary": "本文提出了一种名为MSNeRV的神经视频表示框架，旨在解决现有隐式神经表示（INR）方法在处理细节丰富和快速变化的视频内容时的不足。MSNeRV通过引入多尺度特征融合、时间窗口、GoP级网格用于背景表示、多尺度空间解码器以及多尺度特征块来增强特征利用和时间一致性。实验证明，MSNeRV在视频表示能力上优于其他基于INR的方法，并在动态场景的压缩效率上超越了VTM-23.7。", "keywords": "神经视频表示, 多尺度特征融合, 视频压缩, 隐式神经表示, 时间一致性", "comments": "本文的创新点在于提出了一个多尺度特征融合框架MSNeRV，通过结合时间窗口、GoP级网格、多尺度空间解码器和多尺度特征块，有效解决了现有INR方法在视频细节和动态内容表示上的局限性。其重要性在于推动了神经视频表示在压缩效率和表示能力上的进步，尤其是在处理高动态视频方面展现出超越传统编解码器的潜力。"}}
{"id": "2506.15215", "title": "MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs", "authors": ["Yongqi Fan", "Yating Wang", "Guandong Wang", "Jie Zhai", "Jingping Liu", "Qi Ye", "Tong Ruan"], "summary": "Open-ended question answering (QA) is a key task for evaluating the\ncapabilities of large language models (LLMs). Compared to closed-ended QA, it\ndemands longer answer statements, more nuanced reasoning processes, and diverse\nexpressions, making refined and interpretable automatic evaluation both crucial\nand challenging. Traditional metrics like ROUGE and BERTScore struggle to\ncapture semantic similarities due to different patterns between model responses\nand reference answers. Current LLM-based evaluation approaches, such as\npairwise or listwise comparisons of candidate answers, lack intuitive\ninterpretability. While pointwise scoring of each response provides some\ndescriptions, it fails to adapt across different question contents. Most\nnotably, existing methods overlook the distinction between factoid and\nnon-factoid questions. To address these challenges, we propose\n\\textbf{MinosEval}, a novel evaluation method that first distinguishes\nopen-ended questions and then ranks candidate answers using different\nevaluation strategies. For factoid questions, it applies an adaptive key-point\nscoring strategy, while for non-factoid questions, it uses an instance-aware\nlistwise ranking strategy. Experiments on multiple open-ended QA datasets,\nincluding self-built ones with more candidate responses to complement community\nresources, show that MinosEval better aligns with human annotations and offers\nmore interpretable results.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15215v1", "AI": {"title_translation": "MinosEval：区分事实型和非事实型问题，用于定制化开放式问答LLM评估", "tldr": "MinosEval是一种新的开放式问答评估方法，它首先区分事实型和非事实型问题，然后采用不同的策略进行评估，并与人工标注更一致。", "motivation": "开放式问答（QA）评估大型语言模型（LLM）能力至关重要且具挑战性。传统指标（如ROUGE、BERTScore）难以捕捉语义相似性。现有基于LLM的评估方法（如成对或列表式比较）缺乏直观可解释性，点对点评分无法适应不同问题内容，并且现有方法忽略了事实型和非事实型问题的区别。", "method": "提出MinosEval，一种新颖的评估方法。它首先区分开放式问题，然后采用不同评估策略对候选答案进行排序。对于事实型问题，采用自适应关键点评分策略；对于非事实型问题，采用实例感知列表式排名策略。", "result": "在多个开放式QA数据集（包括自建数据集）上的实验表明，MinosEval与人工标注的对齐度更高，并提供更可解释的结果。", "conclusion": "MinosEval通过区分事实型和非事实型问题并应用定制化的评估策略，显著提高了开放式问答LLM评估的准确性和可解释性。", "translation": "开放式问答（QA）是评估大型语言模型（LLM）能力的关键任务。与封闭式问答相比，它需要更长的答案陈述、更细致的推理过程和多样化的表达，使得精细和可解释的自动评估既关键又具有挑战性。传统的指标如ROUGE和BERTScore由于模型响应和参考答案之间的不同模式，难以捕捉语义相似性。当前基于LLM的评估方法，例如候选答案的成对或列表式比较，缺乏直观的可解释性。虽然对每个响应进行点对点评分提供了一些描述，但它未能适应不同的问题内容。最值得注意的是，现有方法忽略了事实型和非事实型问题之间的区别。为了解决这些挑战，我们提出了MinosEval，一种新颖的评估方法，它首先区分开放式问题，然后使用不同的评估策略对候选答案进行排序。对于事实型问题，它应用自适应关键点评分策略；而对于非事实型问题，它使用实例感知列表式排名策略。在多个开放式QA数据集（包括为补充社区资源而自建的、具有更多候选响应的数据集）上的实验表明，MinosEval与人工标注的对齐度更高，并提供更可解释的结果。", "summary": "本文提出了MinosEval，一种用于开放式问答LLM评估的新方法。针对传统评估指标和现有LLM评估方法的不足，尤其是忽略事实型和非事实型问题区别的问题，MinosEval首先对问题类型进行区分，然后对不同类型的问题采用定制化的评估策略：事实型问题使用自适应关键点评分，非事实型问题使用实例感知列表式排名。实验证明MinosEval与人工标注更一致，并提供更具解释性的评估结果。", "keywords": "开放式问答, LLM评估, 事实型问题, 非事实型问题, MinosEval", "comments": "MinosEval的创新之处在于其对开放式问题细致的分类处理，即区分事实型和非事实型问题，并为之设计了不同的评估策略，这极大地提升了评估的准确性和可解释性。它弥补了现有评估方法在这方面的不足，对于推动LLM在开放式问答领域的评估标准具有重要意义。"}}
{"id": "2506.15360", "title": "Stochastic Diagonal Estimation Based on Matrix Quadratic Form Oracles", "authors": ["Haishan Ye", "Xiangyu Chang"], "summary": "We study the problem of estimating the diagonal of an implicitly given matrix\n$\\Ab$. For such a matrix we have access to an oracle that allows us to evaluate\nthe matrix quadratic form $ \\ub^\\top \\Ab \\ub$. Based on this query oracle, we\npropose a stochastic diagonal estimation method with random variable $\\ub$\ndrawn from the standard Gaussian distribution. We provide the element-wise and\nnorm-wise sample complexities of the proposed method. Our numerical experiments\non different types and dimensions matrices demonstrate the effectiveness of our\nmethod and validate the tightness of theoretical results.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15360v1", "AI": {"title_translation": "基于矩阵二次型预言机的随机对角线估计", "tldr": "通过查询矩阵二次型预言机，提出了一种基于高斯随机变量的随机对角线估计方法，并分析了其样本复杂度，实验验证了其有效性。", "motivation": "该研究旨在解决在只能通过矩阵二次型预言机访问隐式给定矩阵的情况下，如何估计该矩阵对角线的问题。", "method": "提出了一种随机对角线估计方法。该方法通过从标准高斯分布中抽取随机变量 $\\ub$ 来查询矩阵二次型 $\\ub^\\top \\Ab \\ub$ 预言机以估计矩阵 $\\Ab$ 的对角线。", "result": "提供了所提出方法的逐元素和范数样本复杂度。数值实验表明该方法对不同类型和维度的矩阵都有效，并验证了理论结果的紧密性。", "conclusion": "本研究提出了一种有效的基于矩阵二次型预言机的随机对角线估计方法，并从理论和实践两方面验证了其性能。", "translation": "我们研究估计一个隐式给定矩阵 $\\Ab$ 的对角线的问题。对于这样的矩阵，我们可以访问一个预言机，该预言机允许我们评估矩阵二次型 $\\ub^\\top \\Ab \\ub$。基于这个查询预言机，我们提出了一种随机对角线估计方法，其中随机变量 $\\ub$ 从标准高斯分布中抽取。我们提供了所提出方法的逐元素和范数样本复杂度。我们对不同类型和维度的矩阵进行的数值实验证明了我们方法的有效性，并验证了理论结果的紧密性。", "summary": "本文提出了一种利用矩阵二次型预言机（通过高斯随机变量查询）来估计隐式给定矩阵对角线的随机方法。研究给出了该方法的逐元素和范数样本复杂度，并通过数值实验验证了其有效性和理论结果的准确性。", "keywords": "随机对角线估计, 矩阵二次型, 样本复杂度, 高斯分布, 隐式矩阵", "comments": "该方法创新性地利用二次型预言机解决矩阵对角线估计问题，尤其适用于矩阵本身不易直接访问的场景。其理论分析和实验验证相结合，提升了研究的严谨性。"}}
{"id": "2506.15432", "title": "Side-Channel Extraction of Dataflow AI Accelerator Hardware Parameters", "authors": ["Guillaume Lomet", "Ruben Salvador", "Brice Colombier", "Vincent Grosso", "Olivier Sentieys", "Cedric Killian"], "summary": "Dataflow neural network accelerators efficiently process AI tasks on FPGAs,\nwith deployment simplified by ready-to-use frameworks and pre-trained models.\nHowever, this convenience makes them vulnerable to malicious actors seeking to\nreverse engineer valuable Intellectual Property (IP) through Side-Channel\nAttacks (SCA). This paper proposes a methodology to recover the hardware\nconfiguration of dataflow accelerators generated with the FINN framework.\nThrough unsupervised dimensionality reduction, we reduce the computational\noverhead compared to the state-of-the-art, enabling lightweight classifiers to\nrecover both folding and quantization parameters. We demonstrate an attack\nphase requiring only 337 ms to recover the hardware parameters with an accuracy\nof more than 95% and 421 ms to fully recover these parameters with an averaging\nof 4 traces for a FINN-based accelerator running a CNN, both using a random\nforest classifier on side-channel traces, even with the accelerator dataflow\nfully loaded. This approach offers a more realistic attack scenario than\nexisting methods, and compared to SoA attacks based on tsfresh, our method\nrequires 940x and 110x less time for preparation and attack phases,\nrespectively, and gives better results even without averaging traces.", "comment": "To appear at the 31st IEEE International Symposium on On-Line Testing\n  and Robust System Design (IOLTS) 2025, 7 pages, 4 figures, 1 algorithm", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15432v1", "AI": {"title_translation": "数据流AI加速器硬件参数的侧信道提取", "tldr": "本文提出了一种针对FINN框架生成的数据流AI加速器的侧信道攻击方法，能快速高效地恢复硬件配置参数，且比现有方法更快更准确。", "motivation": "数据流AI加速器在FPGA上高效处理AI任务，但其便捷性也使其容易受到恶意攻击者通过侧信道攻击逆向工程获取有价值的知识产权（IP）。", "method": "提出了一种恢复FINN框架生成的数据流加速器硬件配置的方法。该方法通过无监督降维减少计算开销，并使用轻量级分类器（如随机森林）从侧信道轨迹中恢复折叠和量化参数。", "result": "在337毫秒内以超过95%的准确率恢复硬件参数，在421毫秒内通过平均4条轨迹完全恢复参数。与基于tsfresh的现有攻击相比，该方法在准备和攻击阶段分别快了940倍和110倍，并且在不平均轨迹的情况下也能提供更好的结果。", "conclusion": "该方法提供了一种比现有方法更真实的攻击场景，并且在恢复数据流AI加速器硬件参数方面表现出显著的时间效率和更高的准确性。", "translation": "数据流神经网络加速器在FPGA上高效处理AI任务，通过即用型框架和预训练模型简化了部署。然而，这种便利性使其容易受到恶意攻击者的攻击，他们试图通过侧信道攻击（SCA）逆向工程获取有价值的知识产权（IP）。本文提出了一种恢复使用FINN框架生成的数据流加速器硬件配置的方法。通过无监督降维，我们降低了与现有技术相比的计算开销，使得轻量级分类器能够恢复折叠和量化参数。我们展示了一个攻击阶段，仅需337毫秒即可恢复硬件参数，准确率超过95%；对于运行CNN的基于FINN的加速器，平均4条轨迹仅需421毫秒即可完全恢复这些参数，两者均使用随机森林分类器对侧信道轨迹进行操作，即使在加速器数据流完全加载的情况下也是如此。这种方法提供了比现有方法更真实的攻击场景，与基于tsfresh的现有攻击相比，我们的方法在准备和攻击阶段分别所需时间减少了940倍和110倍，并且即使在不平均轨迹的情况下也能提供更好的结果。", "summary": "本文提出了一种高效的侧信道攻击方法，旨在从基于FINN框架构建的数据流AI加速器中恢复其底层硬件配置参数。通过采用无监督降维技术，该方法显著降低了计算复杂性，并利用轻量级分类器（如随机森林）成功提取了折叠和量化参数。实验证明，该方法能在极短时间内（数百毫秒）以高精度（超过95%）恢复参数，并且在时间效率和攻击效果上均远超现有技术，为数据流AI加速器的IP保护提供了新的警示。", "keywords": "侧信道攻击, 数据流AI加速器, 硬件参数, FINN, 逆向工程", "comments": "这项研究在AI硬件安全领域具有重要意义。它揭示了数据流AI加速器在部署便利性背后的潜在安全漏洞，即通过侧信道攻击逆向工程IP的风险。该方法通过引入无监督降维显著提升了攻击效率和可行性，使得攻击更加轻量化和快速，这对于加速器设计者和部署者来说是一个重要的警示，需要考虑更强的侧信道防护措施。"}}
{"id": "2506.15175", "title": "SHeRLoc: Synchronized Heterogeneous Radar Place Recognition for Cross-Modal Localization", "authors": ["Hanjun Kim", "Minwoo Jung", "Wooseong Yang", "Ayoung Kim"], "summary": "Despite the growing adoption of radar in robotics, the majority of research\nhas been confined to homogeneous sensor types, overlooking the integration and\ncross-modality challenges inherent in heterogeneous radar technologies. This\nleads to significant difficulties in generalizing across diverse radar data\ntypes, with modality-aware approaches that could leverage the complementary\nstrengths of heterogeneous radar remaining unexplored. To bridge these gaps, we\npropose SHeRLoc, the first deep network tailored for heterogeneous radar, which\nutilizes RCS polar matching to align multimodal radar data. Our hierarchical\noptimal transport-based feature aggregation method generates rotationally\nrobust multi-scale descriptors. By employing FFT-similarity-based data mining\nand adaptive margin-based triplet loss, SHeRLoc enables FOV-aware metric\nlearning. SHeRLoc achieves an order of magnitude improvement in heterogeneous\nradar place recognition, increasing recall@1 from below 0.1 to 0.9 on a public\ndataset and outperforming state of-the-art methods. Also applicable to LiDAR,\nSHeRLoc paves the way for cross-modal place recognition and heterogeneous\nsensor SLAM. The source code will be available upon acceptance.", "comment": "This work has been submitted to the IEEE for possible publication", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15175v1", "AI": {"title_translation": "SHeRLoc：异构雷达同步地点识别用于跨模态定位", "tldr": "提出SHeRLoc，首个为异构雷达设计的深度网络，通过RCS极坐标匹配和分层最优传输特征聚合实现跨模态地点识别，显著提升异构雷达地点识别性能。", "motivation": "尽管雷达在机器人领域应用日益广泛，但现有研究主要局限于同质传感器，忽视了异构雷达技术固有的集成和跨模态挑战，导致数据泛化困难，且未充分利用异构雷达的互补优势。", "method": "本文提出了SHeRLoc，这是第一个专为异构雷达设计的深度网络。它利用RCS极坐标匹配来对齐多模态雷达数据，并通过基于分层最优传输的特征聚合方法生成旋转鲁棒的多尺度描述符。此外，SHeRLoc还采用基于FFT相似度的数据挖掘和自适应裕度三元组损失，以实现FOV感知的度量学习。", "result": "SHeRLoc在异构雷达地点识别方面取得了数量级的改进，在公共数据集上将recall@1从低于0.1提高到0.9，并超越了现有最先进的方法。该方法也适用于LiDAR。", "conclusion": "SHeRLoc为跨模态地点识别和异构传感器SLAM铺平了道路。", "translation": "尽管雷达在机器人领域得到越来越多的应用，但大多数研究仅限于同质传感器类型，忽视了异构雷达技术中固有的集成和跨模态挑战。这导致在不同雷达数据类型之间进行泛化存在显著困难，并且尚未探索能够利用异构雷达互补优势的模态感知方法。为了弥补这些差距，我们提出了SHeRLoc，这是第一个专为异构雷达设计的深度网络，它利用RCS极坐标匹配来对齐多模态雷达数据。我们基于分层最优传输的特征聚合方法生成了旋转鲁棒的多尺度描述符。通过采用基于FFT相似度的数据挖掘和自适应裕度三元组损失，SHeRLoc实现了FOV感知的度量学习。SHeRLoc在异构雷达地点识别方面实现了数量级的改进，在公共数据集上将recall@1从0.1以下提高到0.9，并超越了最先进的方法。SHeRLoc也适用于LiDAR，为跨模态地点识别和异构传感器SLAM铺平了道路。源代码将在论文接收后提供。", "summary": "本文提出了SHeRLoc，一个为解决异构雷达跨模态集成和泛化挑战而设计的深度网络。SHeRLoc通过RCS极坐标匹配对齐多模态雷达数据，并利用分层最优传输聚合特征以生成旋转鲁棒的描述符。结合FFT相似度数据挖掘和自适应裕度三元组损失，实现了FOV感知的度量学习。实验结果表明，SHeRLoc在异构雷达地点识别方面显著提升了性能，并可推广至LiDAR，为跨模态定位和SLAM奠定基础。", "keywords": "异构雷达, 地点识别, 跨模态定位, 深度学习, SLAM", "comments": "SHeRLoc的创新性在于它是首个专门为异构雷达设计的深度网络，解决了跨模态数据对齐和泛化难题。其提出的RCS极坐标匹配和分层最优传输特征聚合方法是核心创新点，显著提升了异构雷达地点识别的准确性。该研究对于推动机器人领域中多传感器融合和跨模态SLAM的发展具有重要意义。"}}
{"id": "2506.15497", "title": "Foundation of Affective Computing and Interaction", "authors": ["Changzeng Fu"], "summary": "This book provides a comprehensive exploration of affective computing and\nhuman-computer interaction technologies. It begins with the historical\ndevelopment and basic concepts of human-computer interaction, delving into the\ntechnical frameworks and practical applications of emotional computing, visual\ninteraction, voice interaction, brain-computer interfaces, physiological\nelectrical signal analysis, and social robotics. The book covers a wide range\nof topics, including the psychological and neuroscience foundations of emotion,\nmultimodal emotion recognition, emotional expression mechanisms, and the\nprinciples of brain-computer interfaces.\n  Key technologies such as affective computing based on discrete emotion theory\nand dimensional models, visual perception principles, speech recognition and\nsynthesis, EEG signal acquisition and processing, and multimodal emotion\nrecognition are explained in detail. This book also addresses the technical\nchallenges in the field, including multimodal data fusion, privacy and\nsecurity, and ethical considerations in human-machine relationships. It\ndiscusses the applications of these technologies across various domains such as\neducation, healthcare, entertainment, and intelligent assistance.\n  Looking to the future, the book anticipates trends such as the deep\nintegration of artificial intelligence with emotion recognition, the\nadvancement of multimodal interaction technologies, and the development of more\npersonalized and adaptive emotion recognition systems. It emphasizes the\nimportance of balancing technological innovation with ethical considerations to\nensure the responsible development and application of affective computing\ntechnologies.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15497v1", "AI": {"title_translation": "情感计算与交互的基础", "tldr": "本书全面探讨了情感计算和人机交互技术，涵盖其历史、基本概念、核心技术、应用、挑战及未来趋势，并强调伦理考量。", "motivation": "提供情感计算和人机交互技术的全面探索，涵盖从基础理论到实际应用及未来发展，旨在为读者提供该领域的深入理解。", "method": "本书通过系统性地介绍情感计算和人机交互的历史发展、基本概念、技术框架、关键技术（如多模态情感识别、脑机接口、语音识别等）、技术挑战、应用领域以及未来趋势，进行全面的探索和详细的解释。", "result": "本书详细阐述了情感计算和人机交互的理论基础、多模态情感识别、情感表达机制、脑机接口原理等核心内容，并介绍了基于离散情感理论和维度模型的情感计算、视觉感知、语音识别与合成、脑电信号处理等关键技术。此外，还讨论了数据融合、隐私安全和伦理问题，并展望了AI与情感识别深度融合的未来趋势。", "conclusion": "本书展望了人工智能与情感识别的深度融合、多模态交互技术的进步以及更个性化和自适应情感识别系统的发展，并强调在技术创新与伦理考量之间取得平衡，以确保情感计算技术的负责任开发和应用。", "translation": "本书全面探讨了情感计算和人机交互技术。它从人机交互的历史发展和基本概念开始，深入探讨了情感计算、视觉交互、语音交互、脑机接口、生理电信号分析和社交机器人等技术框架和实际应用。本书涵盖了广泛的主题，包括情感的心理学和神经科学基础、多模态情感识别、情感表达机制以及脑机接口的原理。\n详细解释了基于离散情感理论和维度模型的情感计算、视觉感知原理、语音识别与合成、脑电图信号采集与处理以及多模态情感识别等关键技术。本书还探讨了该领域的技术挑战，包括多模态数据融合、隐私和安全以及人机关系中的伦理考量。它讨论了这些技术在教育、医疗、娱乐和智能辅助等各个领域的应用。\n展望未来，本书预测了人工智能与情感识别的深度融合、多模态交互技术的进步以及更个性化和自适应情感识别系统的发展等趋势。它强调了平衡技术创新与伦理考量的重要性，以确保情感计算技术的负责任开发和应用。", "summary": "本书全面深入探讨了情感计算与人机交互技术，涵盖其历史发展、核心概念、技术框架、关键技术（如多模态情感识别、脑机接口等）、面临的挑战（如数据融合、隐私伦理）以及在教育、医疗等领域的应用。同时，本书展望了未来人工智能与情感识别的深度融合趋势，并强调技术发展中伦理与创新的平衡。", "keywords": "情感计算, 人机交互, 多模态情感识别, 脑机接口, 伦理考量", "comments": "这本书的重要性在于其为情感计算和人机交互领域提供了一个全面的基础性概述，不仅涵盖了技术细节和应用，还深入探讨了伦理挑战和未来发展方向。它对于希望全面了解该领域的学生、研究人员和从业者来说，是一份非常有价值的资源。其创新之处在于将广泛的技术和理论整合在一个框架内，并强调了伦理维度，这在快速发展的人工智能领域尤为重要。"}}
{"id": "2506.14919", "title": "Frequency-Calibrated Membership Inference Attacks on Medical Image Diffusion Models", "authors": ["Xinkai Zhao", "Yuta Tokuoka", "Junichiro Iwasawa", "Keita Oda"], "summary": "The increasing use of diffusion models for image generation, especially in\nsensitive areas like medical imaging, has raised significant privacy concerns.\nMembership Inference Attack (MIA) has emerged as a potential approach to\ndetermine if a specific image was used to train a diffusion model, thus\nquantifying privacy risks. Existing MIA methods often rely on diffusion\nreconstruction errors, where member images are expected to have lower\nreconstruction errors than non-member images. However, applying these methods\ndirectly to medical images faces challenges. Reconstruction error is influenced\nby inherent image difficulty, and diffusion models struggle with high-frequency\ndetail reconstruction. To address these issues, we propose a\nFrequency-Calibrated Reconstruction Error (FCRE) method for MIAs on medical\nimage diffusion models. By focusing on reconstruction errors within a specific\nmid-frequency range and excluding both high-frequency (difficult to\nreconstruct) and low-frequency (less informative) regions, our\nfrequency-selective approach mitigates the confounding factor of inherent image\ndifficulty. Specifically, we analyze the reverse diffusion process, obtain the\nmid-frequency reconstruction error, and compute the structural similarity index\nscore between the reconstructed and original images. Membership is determined\nby comparing this score to a threshold. Experiments on several medical image\ndatasets demonstrate that our FCRE method outperforms existing MIA methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14919v1", "AI": {"title_translation": "医学图像扩散模型上的频率校准成员推断攻击", "tldr": "提出了一种名为FCRE的频率校准重建误差方法，用于对医疗图像扩散模型进行成员推断攻击，通过关注中频重建误差来提高攻击性能。", "motivation": "扩散模型在医疗图像生成中的应用日益增多，引发了严重的隐私担忧。现有的成员推断攻击（MIA）方法依赖重建误差，但直接应用于医疗图像时，面临图像固有难度和高频细节重建困难的挑战。", "method": "提出了一种频率校准重建误差（FCRE）方法，用于对医疗图像扩散模型进行成员推断攻击。该方法通过关注特定中频范围内的重建误差，排除难以重建的高频区域和信息量较少的低频区域，以减轻图像固有难度带来的混淆因素。具体而言，分析逆向扩散过程，获取中频重建误差，并计算重建图像与原始图像之间的结构相似性指数分数，通过与阈值比较来确定成员身份。", "result": "在多个医疗图像数据集上的实验表明，FCRE方法优于现有的MIA方法。", "conclusion": "FCRE方法通过频率选择性地关注中频重建误差，有效解决了现有成员推断攻击在医疗图像扩散模型上遇到的挑战，提高了攻击性能。", "translation": "扩散模型在图像生成中，特别是在医疗成像等敏感领域的应用日益增多，引发了严重的隐私担忧。成员推断攻击（MIA）已成为一种潜在的方法，用于确定特定图像是否曾用于训练扩散模型，从而量化隐私风险。现有的MIA方法通常依赖于扩散重建误差，其中预期成员图像的重建误差低于非成员图像。然而，将这些方法直接应用于医疗图像面临挑战。重建误差受图像固有难度影响，并且扩散模型难以重建高频细节。为了解决这些问题，我们提出了一种频率校准重建误差（FCRE）方法，用于对医疗图像扩散模型进行MIA。通过关注特定中频范围内的重建误差，并排除高频（难以重建）和低频（信息量较少）区域，我们的频率选择性方法减轻了图像固有难度这一混淆因素。具体而言，我们分析逆向扩散过程，获取中频重建误差，并计算重建图像与原始图像之间的结构相似性指数分数。通过将此分数与阈值进行比较来确定成员身份。在多个医疗图像数据集上的实验表明，我们的FCRE方法优于现有的MIA方法。", "summary": "本研究提出了一种名为频率校准重建误差（FCRE）的新方法，用于对医疗图像扩散模型进行成员推断攻击（MIA）。鉴于现有MIA方法在处理医疗图像时面临的挑战，如图像固有难度和高频细节重建困难，FCRE通过选择性地关注中频重建误差来解决这些问题。该方法分析逆向扩散过程，计算重建图像与原始图像在中频范围内的结构相似性指数分数，并以此确定成员身份。实验结果表明，FCRE在多个医疗图像数据集上优于现有MIA方法，有效提升了在敏感医疗数据上量化隐私风险的能力。", "keywords": "成员推断攻击, 扩散模型, 医疗图像, 隐私, 频率校准", "comments": "该论文的创新点在于提出了频率校准重建误差（FCRE）方法，解决了现有成员推断攻击在医疗图像扩散模型上遇到的特定挑战，即图像固有难度和高频细节重建问题。通过关注中频信息，该方法能够更准确地判断图像是否为训练集成员，对于评估和量化医疗领域扩散模型的隐私风险具有重要意义。这为未来设计更安全的医疗图像生成模型提供了新的视角。"}}
{"id": "2506.14813", "title": "Training with Confidence: Catching Silent Errors in Deep Learning Training with Automated Proactive Checks", "authors": ["Yuxuan Jiang", "Ziming Zhou", "Boyu Xu", "Beijie Liu", "Runhui Xu", "Peng Huang"], "summary": "Training deep learning (DL) models is a complex process, making it prone to\nsilent errors that are challenging to detect and diagnose. This paper presents\nTRAINCHECK, a framework that takes a proactive checking approach to address\nsilent training errors. TRAINCHECK automatically infers invariants tailored for\nDL training. It uses these invariants to proactively detect silent errors\nduring the training process while providing debugging help. To evaluate\nTRAINCHECK, we reproduce 20 real-world silent training errors with diverse root\ncauses. TRAINCHECK successfully detects 18 errors within a single training\niteration. It also uncovers 6 unknown bugs in popular training libraries that\nlead to silent errors.", "comment": "19 pages, to appear in 19th USENIX Symposium on Operating Systems\n  Design and Implementation (OSDI '25)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14813v1", "AI": {"title_translation": "自信训练：通过自动化主动检查捕获深度学习训练中的无声错误", "tldr": "TRAINCHECK是一个框架，通过推断不变量并主动检查，有效检测并帮助调试深度学习训练中的无声错误。", "motivation": "深度学习模型训练是一个复杂的过程，容易产生难以检测和诊断的无声错误。", "method": "本文提出了TRAINCHECK框架，它采用主动检查方法来解决无声训练错误。TRAINCHECK自动推断针对深度学习训练定制的不变量，并利用这些不变量在训练过程中主动检测无声错误，同时提供调试帮助。", "result": "TRAINCHECK成功检测了20个真实世界无声训练错误中的18个，且仅在一个训练迭代内完成。它还发现了流行训练库中导致无声错误的6个未知错误。", "conclusion": "TRAINCHECK框架能够有效且及时地检测深度学习训练中的无声错误，并有助于发现底层库中的未知缺陷，显著提升了训练的可靠性。", "translation": "训练深度学习（DL）模型是一个复杂的过程，这使得它容易出现难以检测和诊断的无声错误。本文提出了TRAINCHECK，一个采用主动检查方法来解决无声训练错误的框架。TRAINCHECK自动推断为DL训练量身定制的不变量。它利用这些不变量在训练过程中主动检测无声错误，同时提供调试帮助。为了评估TRAINCHECK，我们重现了20个具有不同根本原因的真实世界无声训练错误。TRAINCHECK在一个训练迭代内成功检测了18个错误。它还发现了流行训练库中导致无声错误的6个未知错误。", "summary": "TRAINCHECK是一个旨在解决深度学习训练中无声错误的框架。它通过自动推断并利用定制的不变量，在训练过程中主动检测这些难以发现的错误，并提供调试支持。实验证明，TRAINCHECK能在一个迭代内检测出绝大多数已知的无声错误，并能揭示流行训练库中的未知缺陷，显著提高了训练的可靠性。", "keywords": "深度学习, 无声错误, 训练检查, 不变量, 错误检测", "comments": "TRAINCHECK的创新点在于其主动检查方法和自动推断不变量的能力，这对于捕捉深度学习训练中常见的隐蔽错误至关重要。它不仅提高了错误检测的效率，还揭示了现有库的潜在缺陷，对提升深度学习系统的健壮性具有重要意义。"}}
{"id": "2506.15239", "title": "Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants", "authors": ["Jaione Bengoetxea", "Itziar Gonzalez-Dios", "Rodrigo Agerri"], "summary": "In this paper, we evaluate the capacity of current language technologies to\nunderstand Basque and Spanish language varieties. We use Natural Language\nInference (NLI) as a pivot task and introduce a novel, manually-curated\nparallel dataset in Basque and Spanish, along with their respective variants.\nOur empirical analysis of crosslingual and in-context learning experiments\nusing encoder-only and decoder-based Large Language Models (LLMs) shows a\nperformance drop when handling linguistic variation, especially in Basque.\nError analysis suggests that this decline is not due to lexical overlap, but\nrather to the linguistic variation itself. Further ablation experiments\nindicate that encoder-only models particularly struggle with Western Basque,\nwhich aligns with linguistic theory that identifies peripheral dialects (e.g.,\nWestern) as more distant from the standard. All data and code are publicly\navailable.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15239v1", "AI": {"title_translation": "迷失在变体中？评估NLI在巴斯克语和西班牙语地理变体中的表现", "tldr": "本文评估了当前语言技术理解巴斯克语和西班牙语变体的能力，发现NLI在处理语言变体时性能下降，尤其是在巴斯克语中，这与词汇重叠无关，而与语言变体本身有关，并且编码器模型在西方巴斯克语上表现尤为挣扎。", "motivation": "评估当前语言技术理解巴斯克语和西班牙语语言变体的能力。", "method": "使用自然语言推理（NLI）作为核心任务，引入了一个新的、手动整理的巴斯克语和西班牙语及其各自变体的并行数据集。通过使用仅编码器和基于解码器的大型语言模型（LLMs）进行跨语言和上下文学习实验。", "result": "实验表明，在处理语言变体时性能下降，尤其是在巴斯克语中。错误分析表明，这种下降并非由于词汇重叠，而是由于语言变体本身。进一步的消融实验表明，仅编码器模型在西方巴斯克语上表现尤为挣扎，这与语言理论认为边缘方言（如西方方言）与标准语更远的观点一致。", "conclusion": "当前的语言技术在处理语言变体时，特别是在巴斯克语中，表现出性能下降，这主要是由语言变体本身而非词汇重叠引起的。仅编码器模型在与标准语差异较大的方言上表现不佳。", "translation": "在本文中，我们评估了当前语言技术理解巴斯克语和西班牙语语言变体的能力。我们使用自然语言推理（NLI）作为核心任务，并引入了一个新的、手动整理的巴斯克语和西班牙语及其各自变体的并行数据集。我们使用仅编码器和基于解码器的大型语言模型（LLMs）进行的跨语言和上下文学习实验的实证分析表明，在处理语言变体时性能下降，尤其是在巴斯克语中。错误分析表明，这种下降并非由于词汇重叠，而是由于语言变体本身。进一步的消融实验表明，仅编码器模型在西方巴斯克语上表现尤为挣扎，这与语言理论认为边缘方言（如西方方言）与标准语更远的观点一致。所有数据和代码均已公开。", "summary": "本文评估了现有语言模型在处理巴斯克语和西班牙语地理变体时的自然语言推理（NLI）性能。研究引入了一个新的手动标注的并行数据集，并利用编码器和解码器LLM进行了跨语言和上下文学习实验。结果显示，模型在处理语言变体时性能显著下降，尤其是在巴斯克语中，这主要归因于语言变体本身而非词汇差异。特别是，编码器模型在与标准语差异较大的西方巴斯克语上表现更差。", "keywords": "语言变体, 自然语言推理, 巴斯克语, 西班牙语, 大型语言模型", "comments": "这项研究通过关注巴斯克语和西班牙语的地理变体，揭示了当前LLMs在处理语言多样性方面的局限性，尤其是在NLI任务中。其创新之处在于构建了一个新的多变体并行数据集，并深入分析了性能下降的原因，指出问题在于语言变体本身而非简单的词汇重叠。这对于提升LLMs在低资源语言和方言理解方面的能力具有重要意义。数据和代码的公开性也促进了后续研究。"}}
{"id": "2506.15481", "title": "A deep shotgun method for solving high-dimensional parabolic partial differential equations", "authors": ["Wenjun Xu", "Wenzhong Zhang"], "summary": "Recent advances in deep learning makes solving parabolic partial differential\nequations (PDEs) in high dimensional spaces possible via forward-backward\nstochastic differential equation (FBSDE) formulations. The implementation of\nmost existing methods requires simulating multiple trajectories of stochastic\nprocesses with a small step size of time discretization to ensure accuracy,\nhence having limited performance, especially when solving on a large time\ninterval. To address such issue, we propose a deep \"shotgun method\" that does\nnot exploit full trajectories, but only utilizes the data distribution of them.\nNumerical results including examples with dimensionality up to 10000\ndemonstrate the competitiveness of the proposed shotgun method in both\nperformance and accuracy.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15481v1", "AI": {"title_translation": "一种求解高维抛物型偏微分方程的深度霰弹方法", "tldr": "现有求解高维抛物型偏微分方程（PDEs）的方法因需要模拟完整轨迹而效率低下。本文提出了一种深度“霰弹方法”，通过利用数据分布而非完整轨迹来提高性能和准确性。", "motivation": "现有基于前向-后向随机微分方程（FBSDE）的深度学习方法在求解高维抛物型偏微分方程（PDEs）时，需要模拟多个随机过程轨迹并采用小时间步长以确保准确性，导致性能受限，尤其是在大时间间隔上。", "method": "本文提出了一种深度“霰弹方法”，该方法不利用完整的轨迹，而只利用它们的数据分布来求解高维抛物型偏微分方程。", "result": "数值结果表明，所提出的霰弹方法在性能和准确性方面都具有竞争力，其中包括维度高达10000的示例。", "conclusion": "深度霰弹方法通过避免完整轨迹模拟，有效解决了高维抛物型偏微分方程，相比现有方法在性能和准确性上均有提升。", "translation": "深度学习的最新进展使得通过前向-后向随机微分方程（FBSDE）公式求解高维空间中的抛物型偏微分方程（PDEs）成为可能。大多数现有方法的实现需要模拟多个随机过程轨迹，且时间离散步长较小以确保准确性，因此性能有限，尤其是在大时间间隔上求解时。为了解决这个问题，我们提出了一种深度“霰弹方法”，该方法不利用完整的轨迹，而只利用它们的数据分布。包括维度高达10000的示例在内的数值结果表明，所提出的霰弹方法在性能和准确性方面都具有竞争力。", "summary": "本论文提出了一种深度“霰弹方法”，旨在高效求解高维抛物型偏微分方程（PDEs），这类方程通常通过前向-后向随机微分方程（FBSDE）公式处理。与依赖耗时完整轨迹模拟的传统方法不同，所提出的方法仅利用轨迹的数据分布。数值实验，包括维度高达10000的案例，表明这种新方法显著提高了性能和准确性。", "keywords": "深度学习, 抛物型偏微分方程, 高维, 霰弹方法, FBSDE", "comments": "这项工作的创新之处在于避免了对完整轨迹的模拟，转而利用数据分布来解决高维抛物型偏微分方程的计算瓶颈。这种方法对于需要高效解决复杂高维问题的应用具有重要意义。"}}
{"id": "2506.15547", "title": "An efficient construction of Raz's two-source randomness extractor with improved parameters", "authors": ["Cameron Foreman", "Lewis Wooltorton", "Kevin Milner", "Florian J. Curchod"], "summary": "Randomness extractors are algorithms that distill weak random sources into\nnear-perfect random numbers. Two-source extractors enable this distillation\nprocess by combining two independent weak random sources. Raz's extractor (STOC\n'05) was the first to achieve this in a setting where one source has linear\nmin-entropy (i.e., proportional to its length), while the other has only\nlogarithmic min-entropy in its length. However, Raz's original construction is\nimpractical due to a polynomial computation time of at least degree 4. Our work\nsolves this problem by presenting an improved version of Raz's extractor with\nquasi-linear computation time, as well as a new analytic theorem with reduced\nentropy requirements. We provide comprehensive analytical and numerical\ncomparisons of our construction with others in the literature, and we derive\nstrong and quantum-proof versions of our efficient Raz extractor. Additionally,\nwe offer an easy-to-use, open-source code implementation of the extractor and a\nnumerical parameter calculation module.", "comment": "12 + 11 pages. Comments welcome!", "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15547v1", "AI": {"title_translation": "一种改进参数的Raz双源随机性提取器的有效构造", "tldr": "本文提出了一种改进的Raz双源随机性提取器，显著降低了计算时间并优化了参数，使其更具实用性。", "motivation": "Raz的双源随机性提取器（STOC '05）首次实现了在其中一个源具有线性最小熵而另一个源仅具有对数最小熵的情况下进行随机性提取。然而，Raz的原始构造由于至少四次方的多项式计算时间而变得不切实际。", "method": "本文通过提出一个计算时间准线性的改进版Raz提取器，以及一个具有降低熵要求的新分析定理来解决计算效率问题。此外，还提供了与现有文献中其他构造的全面分析和数值比较，并推导出了高效Raz提取器的强和量子证明版本。", "result": "所提出的改进版Raz提取器实现了准线性的计算时间，降低了熵要求，并提供了强大的和量子证明的版本。此外，还提供了易于使用的开源代码实现和数值参数计算模块。", "conclusion": "本文成功地将Raz的随机性提取器从理论上不切实际的构造改进为具有准线性计算时间的高效且实用的版本，并通过降低熵要求和提供开源实现进一步增强了其实用性。", "translation": "随机性提取器是将弱随机源提炼成近乎完美随机数的算法。双源提取器通过结合两个独立的弱随机源来实现这一提炼过程。Raz的提取器（STOC '05）首次在其中一个源具有线性最小熵（即与其长度成比例），而另一个源仅具有对数最小熵的情况下实现了这一点。然而，由于至少四次方的多项式计算时间，Raz的原始构造是不切实际的。我们的工作通过提出一个计算时间准线性的改进版Raz提取器，以及一个具有降低熵要求的新分析定理来解决这个问题。我们提供了我们的构造与文献中其他构造的全面分析和数值比较，并且我们推导出了我们高效Raz提取器的强和量子证明版本。此外，我们还提供了该提取器易于使用的开源代码实现和数值参数计算模块。", "summary": "本文针对Raz的双源随机性提取器计算效率低下的问题，提出了一种改进的构造方法。该方法将Raz提取器的计算时间从多项式级别降低到准线性级别，并通过新的分析定理减少了熵需求。研究还提供了与现有方法的对比分析、强大的量子证明版本以及开源实现，极大地提高了Raz提取器的实用性。", "keywords": "随机性提取器, Raz提取器, 双源提取器, 准线性时间, 熵", "comments": "本文的创新之处在于将理论上重要的Raz双源随机性提取器从计算上不切实际的方案转化为高效且实用的工具，通过引入准线性计算时间和降低熵要求，显著提升了其实用性。提供开源代码实现也大大降低了其应用门槛。"}}
{"id": "2506.15249", "title": "Context-Aware Deep Lagrangian Networks for Model Predictive Control", "authors": ["Lucas Schulze", "Jan Peters", "Oleg Arenz"], "summary": "Controlling a robot based on physics-informed dynamic models, such as deep\nLagrangian networks (DeLaN), can improve the generalizability and\ninterpretability of the resulting behavior. However, in complex environments,\nthe number of objects to potentially interact with is vast, and their physical\nproperties are often uncertain. This complexity makes it infeasible to employ a\nsingle global model. Therefore, we need to resort to online system\nidentification of context-aware models that capture only the currently relevant\naspects of the environment. While physical principles such as the conservation\nof energy may not hold across varying contexts, ensuring physical plausibility\nfor any individual context-aware model can still be highly desirable,\nparticularly when using it for receding horizon control methods such as Model\nPredictive Control (MPC). Hence, in this work, we extend DeLaN to make it\ncontext-aware, combine it with a recurrent network for online system\nidentification, and integrate it with a MPC for adaptive, physics-informed\ncontrol. We also combine DeLaN with a residual dynamics model to leverage the\nfact that a nominal model of the robot is typically available. We evaluate our\nmethod on a 7-DOF robot arm for trajectory tracking under varying loads. Our\nmethod reduces the end-effector tracking error by 39%, compared to a 21%\nimprovement achieved by a baseline that uses an extended Kalman filter.", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15249v1", "AI": {"title_translation": "面向模型预测控制的上下文感知深度拉格朗日网络", "tldr": "本文扩展了深度拉格朗日网络（DeLaN），使其具备上下文感知能力，并将其与在线系统识别和模型预测控制（MPC）相结合，实现了自适应、物理信息控制。在7自由度机械臂上，该方法将末端执行器跟踪误差降低了39%。", "motivation": "基于物理信息的动态模型（如深度拉格朗日网络DeLaN）可以提高机器人控制的泛化性和可解释性。然而，在复杂环境中，潜在交互对象众多且物理属性不确定，导致单一全局模型不可行。因此，需要在线识别仅捕获当前相关环境方面的上下文感知模型，并确保其物理合理性，尤其是在模型预测控制（MPC）等方法中使用时。", "method": "本文将深度拉格朗日网络（DeLaN）扩展为上下文感知模型，并将其与循环网络结合进行在线系统识别，然后集成到模型预测控制（MPC）中以实现自适应、物理信息控制。此外，还将DeLaN与残差动力学模型结合，以利用通常可用的机器人标称模型。该方法在7自由度机械臂上进行了轨迹跟踪评估，负载条件各异。", "result": "与使用扩展卡尔曼滤波器的基线方法相比（提高了21%），本文方法将末端执行器跟踪误差降低了39%。", "conclusion": "本文提出的上下文感知深度拉格朗日网络与在线系统识别和模型预测控制相结合的方法，显著提升了机器人在复杂环境下的轨迹跟踪精度，验证了其在自适应、物理信息控制方面的有效性。", "translation": "基于物理信息的动态模型（如深度拉格朗日网络DeLaN）控制机器人可以提高所得行为的泛化性和可解释性。然而，在复杂环境中，可能交互的物体数量巨大，且其物理属性通常不确定。这种复杂性使得采用单一全局模型不可行。因此，我们需要诉诸于在线系统识别上下文感知模型，这些模型仅捕获环境中当前相关的方面。虽然能量守恒等物理原理可能不适用于不同上下文，但确保任何单个上下文感知模型的物理合理性仍然是非常可取的，尤其是在将其用于模型预测控制（MPC）等后退视界控制方法时。因此，在这项工作中，我们扩展了DeLaN使其具有上下文感知能力，将其与循环网络结合进行在线系统识别，并将其与MPC集成以实现自适应、物理信息控制。我们还将DeLaN与残差动力学模型结合，以利用机器人标称模型通常可用的事实。我们在7自由度机械臂上评估了我们的方法在不同负载下的轨迹跟踪性能。与使用扩展卡尔曼滤波器的基线方法相比，我们的方法将末端执行器跟踪误差降低了39%，而基线方法实现了21%的改进。", "summary": "本文提出了一种面向模型预测控制（MPC）的上下文感知深度拉格朗日网络（DeLaN）。针对复杂环境中单一全局模型不可行的问题，该方法通过将DeLaN扩展为上下文感知模型，并结合循环网络进行在线系统识别，实现了自适应、物理信息控制。同时，利用残差动力学模型整合了机器人标称模型。在7自由度机械臂的轨迹跟踪实验中，该方法将末端执行器跟踪误差降低了39%，优于基线方法的21%改进。", "keywords": "深度拉格朗日网络, 模型预测控制, 上下文感知, 在线系统识别, 机器人控制", "comments": "该论文的创新点在于将深度拉格朗日网络（DeLaN）扩展为上下文感知模型，并将其与在线系统识别和模型预测控制（MPC）相结合，有效地解决了复杂环境中机器人控制的挑战。通过整合物理信息和自适应能力，显著提升了控制精度和泛化性。该方法对于需要高精度和强适应性的机器人应用具有重要意义。"}}
{"id": "2506.15512", "title": "Optimizing Web-Based AI Query Retrieval with GPT Integration in LangChain A CoT-Enhanced Prompt Engineering Approach", "authors": ["Wenqi Guan", "Yang Fang"], "summary": "Large Language Models have brought a radical change in the process of remote\nlearning students, among other aspects of educative activities. Current\nretrieval of remote learning resources lacks depth in contextual meaning that\nprovides comprehensive information on complex student queries. This work\nproposes a novel approach to enhancing remote learning retrieval by integrating\nGPT-based models within the LangChain framework. We achieve this system in a\nmore intuitive and productive manner using CoT reasoning and prompt\nengineering. The framework we propose puts much emphasis on increasing the\nprecision and relevance of the retrieval results to return comprehensive and\ncontextually enriched explanations and resources that best suit each student's\nneeds. We also assess the effectiveness of our approach against paradigmatic\nLLMs and report improvements in user satisfaction and learning outcomes.", "comment": null, "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15512v1", "AI": {"title_translation": "结合LangChain中GPT集成优化基于网络的AI查询检索：一种CoT增强的提示工程方法", "tldr": "本文提出一种结合GPT模型和LangChain框架，利用CoT和提示工程，优化远程学习资源检索，提高学生查询的精确性和相关性。", "motivation": "当前远程学习资源检索缺乏深度上下文理解，无法为复杂的学生查询提供全面的信息。", "method": "通过在LangChain框架中集成基于GPT的模型，并利用CoT推理和提示工程来增强远程学习检索系统。", "result": "提高了检索结果的精确性和相关性，返回了更全面和上下文丰富的解释和资源；与现有LLM相比，用户满意度和学习成果有所改善。", "conclusion": "结合GPT模型、LangChain框架、CoT推理和提示工程的方法能有效优化远程学习资源的检索，显著提高用户满意度和学习成果。", "translation": "大型语言模型在远程学习学生及其他教育活动方面带来了根本性变革。当前远程学习资源的检索缺乏上下文意义的深度，无法为复杂的学生查询提供全面的信息。本工作提出了一种新颖的方法，通过在LangChain框架中集成基于GPT的模型来增强远程学习检索。我们利用CoT推理和提示工程，以更直观和高效的方式实现了该系统。我们提出的框架非常注重提高检索结果的精确性和相关性，以返回最适合每个学生需求的全面且上下文丰富的解释和资源。我们还评估了我们方法与典型大型语言模型相比的有效性，并报告了用户满意度和学习成果的改善。", "summary": "本文提出了一种新颖的远程学习资源检索优化方法。通过将GPT模型集成到LangChain框架中，并结合CoT推理和提示工程技术，该方法旨在提高查询结果的精确性、相关性和上下文丰富性，从而更好地满足学生的复杂查询需求。研究评估显示，与现有大型语言模型相比，该方法显著提升了用户满意度和学习成果。", "keywords": "远程学习, GPT模型, LangChain, 提示工程, CoT推理", "comments": "这篇论文的创新点在于将GPT模型与LangChain框架结合，并特别强调了CoT推理和提示工程在优化远程学习资源检索中的应用。这种方法有望显著提升AI在教育领域的应用效果，尤其是在个性化和上下文感知的学习支持方面。其局限性可能在于对特定领域知识的适应性以及计算资源的需求。"}}
{"id": "2506.14934", "title": "Vision Transformers for End-to-End Quark-Gluon Jet Classification from Calorimeter Images", "authors": ["Md Abrar Jahin", "Shahriar Soudeep", "Arian Rahman Aditta", "M. F. Mridha", "Nafiz Fahad", "Md. Jakir Hossen"], "summary": "Distinguishing between quark- and gluon-initiated jets is a critical and\nchallenging task in high-energy physics, pivotal for improving new physics\nsearches and precision measurements at the Large Hadron Collider. While deep\nlearning, particularly Convolutional Neural Networks (CNNs), has advanced jet\ntagging using image-based representations, the potential of Vision Transformer\n(ViT) architectures, renowned for modeling global contextual information,\nremains largely underexplored for direct calorimeter image analysis, especially\nunder realistic detector and pileup conditions. This paper presents a\nsystematic evaluation of ViTs and ViT-CNN hybrid models for quark-gluon jet\nclassification using simulated 2012 CMS Open Data. We construct multi-channel\njet-view images from detector-level energy deposits (ECAL, HCAL) and\nreconstructed tracks, enabling an end-to-end learning approach. Our\ncomprehensive benchmarking demonstrates that ViT-based models, notably\nViT+MaxViT and ViT+ConvNeXt hybrids, consistently outperform established CNN\nbaselines in F1-score, ROC-AUC, and accuracy, highlighting the advantage of\ncapturing long-range spatial correlations within jet substructure. This work\nestablishes the first systematic framework and robust performance baselines for\napplying ViT architectures to calorimeter image-based jet classification using\npublic collider data, alongside a structured dataset suitable for further deep\nlearning research in this domain.", "comment": "Accepted in Third International Workshop on Generalizing from Limited\n  Resources in the Open World Workshop at International Joint Conference on\n  Artificial Intelligence (IJCAI) 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14934v1", "AI": {"title_translation": "视觉Transformer用于量能器图像的端到端夸克-胶子射流分类", "tldr": "本研究系统评估了Vision Transformers (ViTs)及其混合模型在夸克-胶子射流分类中的应用，结果显示ViT模型在各项指标上优于CNN基线，并首次建立了基于ViT的量能器图像射流分类框架。", "motivation": "区分夸克和胶子引起的射流在高能物理中至关重要，能改进新物理搜索和精确测量。尽管CNN在喷注标记方面有进展，但Vision Transformer (ViT)在直接量能器图像分析中的潜力，尤其是在真实探测器和堆积条件下，尚未得到充分探索。", "method": "本文利用模拟的2012 CMS开放数据，构建了来自探测器级能量沉积（ECAL, HCAL）和重建径迹的多通道射流图像，并采用端到端学习方法。系统评估了ViT和ViT-CNN混合模型在夸克-胶子射流分类中的性能。", "result": "基于ViT的模型，特别是ViT+MaxViT和ViT+ConvNeXt混合模型，在F1分数、ROC-AUC和准确性方面始终优于已建立的CNN基线，这突出了捕获射流亚结构中长程空间相关性的优势。", "conclusion": "本工作首次建立了将ViT架构应用于使用公共对撞机数据进行量能器图像射流分类的系统框架和稳健性能基线，同时提供了一个适用于该领域进一步深度学习研究的结构化数据集。", "translation": "区分夸克和胶子引起的射流是高能物理中一项关键且具有挑战性的任务，对于改进大型强子对撞机的新物理搜索和精确测量至关重要。尽管深度学习，特别是卷积神经网络（CNNs），通过基于图像的表示方法推进了射流标记，但Vision Transformer (ViT) 架构在建模全局上下文信息方面的潜力，对于直接量能器图像分析，尤其是在真实的探测器和堆积条件下，仍未得到充分探索。本文系统评估了ViT和ViT-CNN混合模型在夸克-胶子射流分类中的应用，使用了模拟的2012 CMS开放数据。我们从探测器级的能量沉积（ECAL、HCAL）和重建径迹构建了多通道射流视图图像，实现了端到端学习方法。我们全面的基准测试表明，基于ViT的模型，特别是ViT+MaxViT和ViT+ConvNeXt混合模型，在F1分数、ROC-AUC和准确性方面始终优于已建立的CNN基线，这突出了捕获射流亚结构中长程空间相关性的优势。这项工作首次建立了将ViT架构应用于使用公共对撞机数据进行量能器图像射流分类的系统框架和稳健性能基线，同时提供了一个适用于该领域进一步深度学习研究的结构化数据集。", "summary": "本研究探讨了Vision Transformers (ViTs) 在高能物理中夸克-胶子射流分类任务中的应用。通过使用模拟的CMS开放数据，将探测器能量沉积和径迹构建成多通道图像，实现了端到端学习。结果表明，ViT及其混合模型（如ViT+MaxViT和ViT+ConvNeXt）在F1分数、ROC-AUC和准确性方面均优于传统的CNN模型，证明了ViT在捕获射流亚结构长程空间相关性方面的优势。该工作为基于量能器图像的射流分类提供了首个系统框架和性能基线，并发布了相关数据集。", "keywords": "夸克-胶子射流分类, Vision Transformer, 量能器图像, 高能物理, 深度学习", "comments": "本文的创新点在于首次将Vision Transformer架构系统地应用于高能物理中的夸克-胶子射流分类任务，并证明了其相对于传统CNN的优越性，尤其是在捕捉长程空间相关性方面。这为未来高能物理数据分析提供了新的深度学习工具和研究方向，并为该领域提供了有价值的基准和数据集。"}}
{"id": "2506.14815", "title": "Predicting Anthropometric Body Composition Variables Using 3D Optical Imaging and Machine Learning", "authors": ["Gyaneshwar Agrahari", "Kiran Bist", "Monika Pandey", "Jacob Kapita", "Zachary James", "Jackson Knox", "Steven Heymsfield", "Sophia Ramirez", "Peter Wolenski", "Nadejda Drenska"], "summary": "Accurate prediction of anthropometric body composition variables, such as\nAppendicular Lean Mass (ALM), Body Fat Percentage (BFP), and Bone Mineral\nDensity (BMD), is essential for early diagnosis of several chronic diseases.\nCurrently, researchers rely on Dual-Energy X-ray Absorptiometry (DXA) scans to\nmeasure these metrics; however, DXA scans are costly and time-consuming. This\nwork proposes an alternative to DXA scans by applying statistical and machine\nlearning models on biomarkers (height, volume, left calf circumference, etc)\nobtained from 3D optical images. The dataset consists of 847 patients and was\nsourced from Pennington Biomedical Research Center. Extracting patients' data\nin healthcare faces many technical challenges and legal restrictions. However,\nmost supervised machine learning algorithms are inherently data-intensive,\nrequiring a large amount of training data. To overcome these limitations, we\nimplemented a semi-supervised model, the $p$-Laplacian regression model. This\npaper is the first to demonstrate the application of a $p$-Laplacian model for\nregression. Our $p$-Laplacian model yielded errors of $\\sim13\\%$ for ALM,\n$\\sim10\\%$ for BMD, and $\\sim20\\%$ for BFP when the training data accounted for\n10 percent of all data. Among the supervised algorithms we implemented, Support\nVector Regression (SVR) performed the best for ALM and BMD, yielding errors of\n$\\sim 8\\%$ for both, while Least Squares SVR performed the best for BFP with\n$\\sim 11\\%$ error when trained on 80 percent of the data. Our findings position\nthe $p$-Laplacian model as a promising tool for healthcare applications,\nparticularly in a data-constrained environment.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14815v1", "AI": {"title_translation": "使用3D光学成像和机器学习预测人体测量身体成分变量", "tldr": "本文提出使用3D光学图像和机器学习模型（包括首次应用于回归的半监督p-Laplacian模型）预测人体成分变量，作为昂贵且耗时的DXA扫描的替代方案，特别适用于数据受限环境。", "motivation": "准确预测人体成分变量对早期诊断慢性病至关重要，但现有DXA扫描成本高昂且耗时。此外，医疗数据获取面临挑战，导致监督学习数据不足。", "method": "该研究提出一种替代DXA扫描的方法，即对从3D光学图像中获得的生物标志物（如身高、体积、左小腿围等）应用统计和机器学习模型。为了应对数据限制，研究实施了半监督p-Laplacian回归模型，并首次将其应用于回归任务。此外，还评估了支持向量回归（SVR）和最小二乘SVR等监督算法。数据集包含847名患者的数据。", "result": "当训练数据占总数据的10%时，p-Laplacian模型在ALM、BMD和BFP上的误差分别为约13%、约10%和约20%。当训练数据占80%时，监督算法中，SVR在ALM和BMD上表现最佳，误差均为约8%。最小二乘SVR在BFP上表现最佳，误差为约11%。", "conclusion": "p-Laplacian模型被证明是医疗保健应用中一个有前景的工具，特别是在数据受限的环境中。", "translation": "准确预测人体成分变量，如四肢瘦体重（ALM）、体脂百分比（BFP）和骨矿物质密度（BMD），对于多种慢性病的早期诊断至关重要。目前，研究人员依赖双能X射线吸收法（DXA）扫描来测量这些指标；然而，DXA扫描成本高昂且耗时。本研究提出一种替代DXA扫描的方法，即对从3D光学图像中获得的生物标志物（身高、体积、左小腿围等）应用统计和机器学习模型。数据集包含847名患者，来源于Pennington生物医学研究中心。在医疗保健领域提取患者数据面临许多技术挑战和法律限制。然而，大多数监督机器学习算法本质上是数据密集型的，需要大量的训练数据。为了克服这些限制，我们实现了一个半监督模型，即p-Laplacian回归模型。本文首次展示了p-Laplacian模型在回归中的应用。当训练数据占总数据的10%时，我们的p-Laplacian模型在ALM上的误差约为13%，在BMD上约为10%，在BFP上约为20%。在我们实现的监督算法中，支持向量回归（SVR）在ALM和BMD上表现最佳，误差均为约8%，而最小二乘SVR在BFP上表现最佳，误差约为11%（当在80%的数据上进行训练时）。我们的发现表明，p-Laplacian模型是医疗保健应用中一个有前景的工具，特别是在数据受限的环境中。", "summary": "本文提出了一种利用3D光学成像和机器学习模型（包括首次应用于回归的半监督p-Laplacian模型）预测人体成分变量（如ALM、BFP、BMD）的新方法，旨在替代昂贵且耗时的DXA扫描。研究在包含847名患者的数据集上进行了验证，结果显示p-Laplacian模型在数据受限情况下表现良好，监督模型在数据充足时表现更优。该研究强调了p-Laplacian模型在医疗保健领域特别是数据受限环境中的应用潜力。", "keywords": "3D光学成像, 机器学习, 人体成分, p-Laplacian回归, 半监督学习", "comments": "本文的创新点在于首次将p-Laplacian模型应用于回归任务，并将其作为一种在数据受限医疗环境中预测人体成分变量的有效半监督方法。这对于解决医疗数据难以获取的问题具有重要意义。研究同时比较了监督和半监督模型的性能，提供了全面的评估。"}}
{"id": "2506.15241", "title": "Research on Graph-Retrieval Augmented Generation Based on Historical Text Knowledge Graphs", "authors": ["Yang Fan", "Zhang Qi", "Xing Wenqian", "Liu Chang", "Liu Liu"], "summary": "This article addresses domain knowledge gaps in general large language models\nfor historical text analysis in the context of computational humanities and\nAIGC technology. We propose the Graph RAG framework, combining chain-of-thought\nprompting, self-instruction generation, and process supervision to create a The\nFirst Four Histories character relationship dataset with minimal manual\nannotation. This dataset supports automated historical knowledge extraction,\nreducing labor costs. In the graph-augmented generation phase, we introduce a\ncollaborative mechanism between knowledge graphs and retrieval-augmented\ngeneration, improving the alignment of general models with historical\nknowledge. Experiments show that the domain-specific model Xunzi-Qwen1.5-14B,\nwith Simplified Chinese input and chain-of-thought prompting, achieves optimal\nperformance in relation extraction (F1 = 0.68). The DeepSeek model integrated\nwith GraphRAG improves F1 by 11% (0.08-0.19) on the open-domain C-CLUE relation\nextraction dataset, surpassing the F1 value of Xunzi-Qwen1.5-14B (0.12),\neffectively alleviating hallucinations phenomenon, and improving\ninterpretability. This framework offers a low-resource solution for classical\ntext knowledge extraction, advancing historical knowledge services and\nhumanities research.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15241v1", "AI": {"title_translation": "基于历史文本知识图谱的图检索增强生成研究", "tldr": "本文提出了Graph RAG框架，旨在解决通用大型语言模型在历史文本分析中的领域知识空白。通过结合思维链提示、自指令生成和过程监督，创建了低标注的历史人物关系数据集。该框架通过知识图谱与检索增强生成的协同机制，显著提升了关系抽取性能，减少了幻觉现象，并提高了可解释性，为古典文本知识提取提供了低资源解决方案。", "motivation": "为了解决通用大型语言模型在计算人文学科和AIGC技术背景下，进行历史文本分析时存在的领域知识空白问题，并降低历史知识提取中人工标注的成本。", "method": "本文提出了Graph RAG框架，该框架结合了思维链提示（chain-of-thought prompting）、自指令生成（self-instruction generation）和过程监督（process supervision），以最少的人工标注创建了《四史》人物关系数据集。在图增强生成阶段，引入了知识图谱与检索增强生成（RAG）之间的协同机制，以提高通用模型与历史知识的对齐度。", "result": "领域特定模型Xunzi-Qwen1.5-14B在简体中文输入和思维链提示下，在关系抽取方面取得了最佳性能（F1 = 0.68）。集成了GraphRAG的DeepSeek模型在开放域C-CLUE关系抽取数据集上，F1值提升了11%（0.08-0.19），超过了Xunzi-Qwen1.5-14B的F1值（0.12），有效缓解了幻觉现象，并提高了可解释性。", "conclusion": "该Graph RAG框架为古典文本知识提取提供了一种低资源解决方案，有效推动了历史知识服务和人文学科研究的进展。", "translation": "本文针对计算人文学科和AIGC技术背景下，通用大型语言模型在历史文本分析中存在的领域知识空白问题。我们提出了Graph RAG框架，结合思维链提示、自指令生成和过程监督，以最少的人工标注创建了《四史》人物关系数据集。该数据集支持自动化历史知识提取，降低了人力成本。在图增强生成阶段，我们引入了知识图谱与检索增强生成之间的协同机制，提高了通用模型与历史知识的对齐度。实验表明，领域特定模型Xunzi-Qwen1.5-14B在简体中文输入和思维链提示下，在关系抽取方面取得了最佳性能（F1 = 0.68）。集成了GraphRAG的DeepSeek模型在开放域C-CLUE关系抽取数据集上，F1值提升了11%（0.08-0.19），超过了Xunzi-Qwen1.5-14B的F1值（0.12），有效缓解了幻觉现象，并提高了可解释性。该框架为古典文本知识提取提供了一种低资源解决方案，推动了历史知识服务和人文学科研究。", "summary": "本文提出了一种名为Graph RAG的框架，旨在解决通用大型语言模型在历史文本分析中面临的领域知识缺失问题。该框架通过结合思维链提示、自指令生成和过程监督，以极少的人工标注成本构建了历史人物关系数据集。Graph RAG在图增强生成阶段引入了知识图谱与检索增强生成的协同机制，显著提升了模型与历史知识的对齐度。实验结果表明，该框架在关系抽取任务上表现出色，有效减少了幻觉现象并增强了模型的可解释性，为古典文本知识提取提供了一种高效的低资源解决方案，对历史知识服务和人文学科研究具有重要意义。", "keywords": "知识图谱, 检索增强生成, 历史文本, 关系抽取, 大型语言模型", "comments": "该论文提出了一种创新方法，通过Graph RAG框架弥补通用大型语言模型在历史文本分析中的领域知识鸿沟。将知识图谱与检索增强生成技术相结合，并辅以特定的提示工程技巧，以有限的资源提高了在专业领域的准确性并减少了幻觉。特别是，能够以低资源方式创建特定领域数据集的价值尤为突出，这对于资源稀缺的古典文本研究领域具有重要意义。"}}
{"id": "2506.15541", "title": "Intrinsic and Extrinsic Organized Attention: Softmax Invariance and Network Sparsity", "authors": ["Oluwadamilola Fasina", "Ruben V. C. Pohle", "Pei-Chun Su", "Ronald R. Coifman"], "summary": "We examine the intrinsic (within the attention head) and extrinsic (amongst\nthe attention heads) structure of the self-attention mechanism in transformers.\nTheoretical evidence for invariance of the self-attention mechanism to softmax\nactivation is obtained by appealing to paradifferential calculus, (and is\nsupported by computational examples), which relies on the intrinsic\norganization of the attention heads. Furthermore, we use an existing\nmethodology for hierarchical organization of tensors to examine network\nstructure by constructing hierarchal partition trees with respect to the query,\nkey, and head axes of network 3-tensors. Such an organization is consequential\nsince it allows one to profitably execute common signal processing tasks on a\ngeometry where the organized network 3-tensors exhibit regularity. We exemplify\nthis qualitatively, by visualizing the hierarchical organization of the tree\ncomprised of attention heads and the diffusion map embeddings, and\nquantitatively by investigating network sparsity with the expansion\ncoefficients of individual attention heads and the entire network with respect\nto the bi and tri-haar bases (respectively) on the space of queries, keys, and\nheads of the network. To showcase the utility of our theoretical and\nmethodological findings, we provide computational examples using vision and\nlanguage transformers. The ramifications of these findings are two-fold: (1) a\nsubsequent step in interpretability analysis is theoretically admitted, and can\nbe exploited empirically for downstream interpretability tasks (2) one can use\nthe network 3-tensor organization for empirical network applications such as\nmodel pruning (by virtue of network sparsity) and network architecture\ncomparison.", "comment": "16 pages, 6 figures, 2 tables", "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15541v1", "AI": {"title_translation": "内在与外在的组织化注意力：Softmax不变性与网络稀疏性", "tldr": "本文深入研究了Transformer自注意力机制的内在与外在结构，理论证明了Softmax不变性，并利用张量分层组织方法分析网络稀疏性，为模型可解释性和剪枝提供了新视角。", "motivation": "本研究旨在深入理解Transformer中自注意力机制的内在（注意力头内部）和外在（注意力头之间）结构，特别是探究其对Softmax激活的不变性以及网络稀疏性，以期为Transformer的可解释性分析和实际网络应用（如模型剪枝）提供理论基础和方法论支持。", "method": "本文在内在结构方面，运用超微分演算（paradifferential calculus）理论证明了自注意力机制对Softmax激活的不变性，并辅以计算示例。在外在结构方面，利用现有张量分层组织方法，通过构建关于网络3-张量查询、键和头轴的分层分区树来检查网络结构。为了验证这些发现，研究定性地通过可视化注意力头和扩散图嵌入的分层组织进行展示，定量地通过调查查询、键和网络头空间上相对于双haar和三haar基的单个注意力头和整个网络的扩展系数来研究网络稀疏性。最后，使用视觉和语言Transformer提供了计算示例以展示理论和方法学发现的实用性。", "result": "本研究获得了自注意力机制对Softmax激活不变性的理论证据，并通过计算示例得到了支持。研究表明，通过对网络3-张量进行分层组织，可以实现在特定几何上高效执行常见的信号处理任务。此外，通过定性可视化和定量分析（利用haar基的扩展系数），成功展示了注意力头的层次组织和网络稀疏性。", "conclusion": "本研究的发现具有重要意义：首先，理论上为可解释性分析提供了后续步骤，并可凭经验用于下游可解释性任务；其次，网络3-张量组织可用于实际网络应用，例如通过利用网络稀疏性实现模型剪枝以及进行网络架构比较。", "translation": "我们研究了Transformer中自注意力机制的内在（注意力头内部）和外在（注意力头之间）结构。通过诉诸超微分演算（并辅以计算示例），获得了自注意力机制对Softmax激活不变性的理论证据，这依赖于注意力头的内在组织。此外，我们利用现有张量分层组织方法，通过构建关于网络3-张量查询、键和头轴的分层分区树来检查网络结构。这种组织是重要的，因为它允许在组织化的网络3-张量表现出规律性的几何结构上有效地执行常见的信号处理任务。我们通过可视化注意力头和扩散图嵌入组成的树的分层组织来定性地展示了这一点，并通过调查网络查询、键和头空间上相对于双haar和三haar基的单个注意力头和整个网络的扩展系数来定量地研究了网络稀疏性。为了展示我们理论和方法学发现的实用性，我们提供了使用视觉和语言Transformer的计算示例。这些发现的影响是双重的：（1）理论上允许在可解释性分析中迈出后续一步，并可凭经验用于下游可解释性任务；（2）可以利用网络3-张量组织进行经验性的网络应用，例如模型剪枝（凭借网络稀疏性）和网络架构比较。", "summary": "本文深入探讨了Transformer自注意力机制的内在和外在结构。在内在方面，通过超微分演算理论证明了自注意力机制对Softmax激活的不变性。在外在方面，论文引入了一种利用张量分层组织方法构建分层分区树来分析网络结构的新视角。研究表明这种组织化有助于在特定几何上高效执行信号处理任务，并定性和定量地展示了注意力头的层次组织和网络稀疏性。这些发现不仅为Transformer的可解释性分析提供了理论基础，也为模型剪枝和网络架构比较等实际应用提供了新的方法。", "keywords": "自注意力机制, Softmax不变性, 网络稀疏性, 张量分层组织, Transformer", "comments": "这篇论文通过深入分析Transformer自注意力机制的内在和外在结构，为理解其工作原理提供了新的理论视角。特别是Softmax不变性的理论证明，为注意力机制的鲁棒性提供了洞察。此外，将张量分层组织引入网络结构分析，并将其与网络稀疏性及实际应用（如模型剪枝）相结合，展现了其潜在的创新性和实用价值。这不仅有助于提升模型的可解释性，也为优化和压缩大型Transformer模型提供了新的思路。"}}
{"id": "2506.15343", "title": "Offensive Robot Cybersecurity", "authors": ["Víctor Mayoral-Vilches"], "summary": "Offensive Robot Cybersecurity introduces a groundbreaking approach by\nadvocating for offensive security methods empowered by means of automation. It\nemphasizes the necessity of understanding attackers' tactics and identifying\nvulnerabilities in advance to develop effective defenses, thereby improving\nrobots' security posture. This thesis leverages a decade of robotics\nexperience, employing Machine Learning and Game Theory to streamline the\nvulnerability identification and exploitation process. Intrinsically, the\nthesis uncovers a profound connection between robotic architecture and\ncybersecurity, highlighting that the design and creation aspect of robotics\ndeeply intertwines with its protection against attacks. This duality -- whereby\nthe architecture that shapes robot behavior and capabilities also necessitates\na defense mechanism through offensive and defensive cybersecurity strategies --\ncreates a unique equilibrium. Approaching cybersecurity with a dual perspective\nof defense and attack, rooted in an understanding of systems architecture, has\nbeen pivotal. Through comprehensive analysis, including ethical considerations,\nthe development of security tools, and executing cyber attacks on robot\nsoftware, hardware, and industry deployments, this thesis proposes a novel\narchitecture for cybersecurity cognitive engines. These engines, powered by\nadvanced game theory and machine learning, pave the way for autonomous\noffensive cybersecurity strategies for robots, marking a significant shift\ntowards self-defending robotic systems. This research not only underscores the\nimportance of offensive measures in enhancing robot cybersecurity but also sets\nthe stage for future advancements where robots are not just resilient to cyber\nthreats but are equipped to autonomously safeguard themselves.", "comment": "Doctoral thesis", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15343v1", "AI": {"title_translation": "进攻性机器人网络安全", "tldr": "该论文提出了一种通过自动化（机器学习和博弈论）实现进攻性安全的方法，以提高机器人网络安全。它强调了机器人架构与网络安全之间的深层联系，并引入了用于自主进攻策略的网络安全认知引擎。", "motivation": "该研究的动机在于理解攻击者的策略并提前识别漏洞，从而开发有效的防御措施，提高机器人的安全态势。论文还揭示了机器人架构与网络安全之间存在深刻的联系。", "method": "该论文利用了十年的机器人学经验，采用机器学习和博弈论来简化漏洞识别和利用过程。通过全面的分析，包括伦理考量、安全工具的开发以及对机器人软件、硬件和工业部署执行网络攻击，提出了一个用于网络安全认知引擎的新颖架构，这些引擎由先进的博弈论和机器学习驱动。", "result": "该研究揭示了机器人架构与网络安全之间存在的深刻联系。提出了一种新颖的网络安全认知引擎架构，为机器人自主进攻性网络安全策略铺平了道路。研究表明，机器人不仅能够抵御网络威胁，而且能够自主保护自己。", "conclusion": "进攻性措施对于增强机器人网络安全至关重要，并为未来的发展奠定了基础，使机器人不仅能够抵御网络威胁，而且能够自主保护自己。", "translation": "《进攻性机器人网络安全》提出了一种突破性的方法，倡导通过自动化实现进攻性安全方法。它强调了理解攻击者策略并提前识别漏洞以开发有效防御措施的必要性，从而提高机器人的安全态势。本论文利用了十年的机器人学经验，采用机器学习和博弈论来简化漏洞识别和利用过程。本质上，本论文揭示了机器人架构与网络安全之间存在的深刻联系，强调了机器人设计和创建方面与其抵御攻击的保护措施深度交织。这种二元性——即塑造机器人行为和能力的架构也需要通过进攻性和防御性网络安全策略来提供防御机制——创造了一种独特的平衡。以防御和攻击的双重视角来处理网络安全，并根植于对系统架构的理解，这一点至关重要。通过全面的分析，包括伦理考量、安全工具的开发以及对机器人软件、硬件和工业部署执行网络攻击，本论文提出了一个用于网络安全认知引擎的新颖架构。这些由先进博弈论和机器学习驱动的引擎，为机器人的自主进攻性网络安全策略铺平了道路，标志着向自卫型机器人系统迈出了重要一步。这项研究不仅强调了进攻性措施在增强机器人网络安全方面的重要性，也为未来的发展奠定了基础，使机器人不仅能够抵御网络威胁，而且能够自主保护自己。", "summary": "本论文介绍了“进攻性机器人网络安全”的概念，倡导通过自动化实现进攻性安全方法来增强机器人安全。它强调理解攻击者策略和主动识别漏洞的重要性，并利用机器学习和博弈论。该研究揭示了机器人架构与网络安全之间的深层联系，并提出了一种由先进博弈论和机器学习驱动的新型认知引擎架构，旨在实现自主进攻性策略，从而迈向自卫型机器人系统。", "keywords": "进攻性安全, 机器人网络安全, 机器学习, 博弈论, 漏洞识别, 自主防御", "comments": "该论文的创新之处在于倡导将进攻性安全方法应用于机器人网络安全，这超越了传统的防御措施。利用机器学习和博弈论实现自主漏洞识别和利用，以及提出的认知引擎架构，都具有重要意义。它强调了机器人设计与安全之间的关键联系，推动了自卫型机器人的发展。"}}
{"id": "2506.15525", "title": "\"How can we learn and use AI at the same time?:: Participatory Design of GenAI with High School Students", "authors": ["Isabella Pu", "Prerna Ravi", "Linh Dieu Dinh", "Chelsea Joe", "Caitlin Ogoe", "Zixuan Li", "Cynthia Breazeal", "Anastasia K. Ostrowski"], "summary": "As generative AI (GenAI) emerges as a transformative force, clear\nunderstanding of high school students' perspectives is essential for GenAI's\nmeaningful integration in high school environments. In this work, we draw\ninsights from a participatory design workshop where we engaged 17 high school\nstudents -- a group rarely involved in prior research in this area -- through\nthe design of novel GenAI tools and school policies addressing their key\nconcerns. Students identified challenges and developed solutions outlining\ntheir ideal features in GenAI tools, appropriate school use, and regulations.\nThese centered around the problem spaces of combating bias & misinformation,\ntackling crime & plagiarism, preventing over-reliance on AI, and handling false\naccusations of academic dishonesty. Building on our participants'\nunderrepresented perspectives, we propose new guidelines targeted at\neducational technology designers for development of GenAI technologies in high\nschools. We also argue for further incorporation of student voices in\ndevelopment of AI policies in their schools.", "comment": "Copyright protected by ACM, 17 pages, 5 figures, 2 tables, in\n  proceedings of 24th annual ACM Interaction Design and Children Conference\n  (IDC 2025)", "cate": "cs.HC", "url": "http://arxiv.org/abs/2506.15525v1", "AI": {"title_translation": "我们如何同时学习和使用AI？：与高中生共同参与生成式AI设计", "tldr": "本研究通过与高中生进行参与式设计，深入了解他们对生成式AI的看法，并据此提出了在高中环境中有效整合AI的指南。", "motivation": "鉴于生成式AI（GenAI）日益成为一股变革力量，清晰了解高中生对此的视角对于GenAI在高中环境中实现有意义的整合至关重要。", "method": "研究通过组织一个参与式设计工作坊，与17名高中生（该群体在相关研究中此前鲜有涉及）进行互动。在工作坊中，学生们共同设计了新颖的GenAI工具以及旨在解决他们主要担忧的学校政策。", "result": "学生们识别了GenAI使用中的挑战，并提出了解决方案，概述了GenAI工具的理想功能、在学校的适当使用方式以及相关规定。这些解决方案主要围绕以下问题领域：打击偏见和错误信息、处理犯罪和抄袭、防止对AI的过度依赖，以及处理学术不端行为的虚假指控。", "conclusion": "基于参与者未被充分代表的视角，本研究为教育技术设计师提出了在高中开发GenAI技术的新指南。同时，研究也倡导在学校AI政策的制定过程中进一步纳入学生的声音。", "translation": "随着生成式AI（GenAI）成为一股变革力量，清晰了解高中生的视角对于GenAI在高中环境中的有意义整合至关重要。在这项工作中，我们从一个参与式设计工作坊中汲取见解，该工作坊让17名高中生——一个在该领域先前研究中很少涉及的群体——参与其中，通过设计新颖的GenAI工具和解决他们主要担忧的学校政策。学生们识别了挑战并开发了解决方案，概述了GenAI工具的理想功能、适当的学校使用和法规。这些解决方案主要围绕打击偏见和错误信息、处理犯罪和抄袭、防止过度依赖AI以及处理学术不端行为的虚假指控等问题领域。基于我们参与者未被充分代表的视角，我们为教育技术设计师提出了在高中开发GenAI技术的新指南。我们还主张在学校AI政策的制定中进一步纳入学生的声音。", "summary": "本研究通过与17名高中生进行参与式设计工作坊，探讨了生成式AI（GenAI）在高中环境中的整合问题。学生们识别了使用GenAI的挑战（如偏见、抄袭、过度依赖和虚假指控），并提出了解决方案和理想功能。基于这些未被充分代表的学生视角，研究者为教育技术设计师提出了开发GenAI技术的新指南，并强调在AI政策制定中纳入学生声音的重要性。", "keywords": "生成式AI, 参与式设计, 高中生, 教育技术, AI政策", "comments": "这项研究的创新之处在于其采用了参与式设计的方法，直接与高中生合作，而非仅仅从外部观察或假设他们的需求。这为生成式AI在教育领域的负责任和有意义的整合提供了宝贵且独特的学生视角，有助于开发更符合用户需求的工具和政策。其重要性在于强调了在技术发展和政策制定中纳入最终用户（尤其是青少年）声音的必要性。"}}
{"id": "2506.14980", "title": "Advances in Compliance Detection: Novel Models Using Vision-Based Tactile Sensors", "authors": ["Ziteng Li", "Malte Kuhlmann", "Ilana Nisky", "Nicolás Navarro-Guerrero"], "summary": "Compliance is a critical parameter for describing objects in engineering,\nagriculture, and biomedical applications. Traditional compliance detection\nmethods are limited by their lack of portability and scalability, rely on\nspecialized, often expensive equipment, and are unsuitable for robotic\napplications. Moreover, existing neural network-based approaches using\nvision-based tactile sensors still suffer from insufficient prediction\naccuracy. In this paper, we propose two models based on Long-term Recurrent\nConvolutional Networks (LRCNs) and Transformer architectures that leverage RGB\ntactile images and other information captured by the vision-based sensor\nGelSight to predict compliance metrics accurately. We validate the performance\nof these models using multiple metrics and demonstrate their effectiveness in\naccurately estimating compliance. The proposed models exhibit significant\nperformance improvement over the baseline. Additionally, we investigated the\ncorrelation between sensor compliance and object compliance estimation, which\nrevealed that objects that are harder than the sensor are more challenging to\nestimate.", "comment": "Accepted in the IEEE International Conference on Development and\n  Learning (ICDL). The paper contains 8 pages and 7 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.14980v1", "AI": {"title_translation": "合规性检测的进展：使用基于视觉的触觉传感器的新型模型", "tldr": "针对传统方法和现有神经网络在合规性检测上的局限性，本文提出了基于LRCN和Transformer架构的新型模型，利用视觉触觉传感器GelSight的数据实现高精度合规性预测。", "motivation": "传统的合规性检测方法缺乏便携性和可扩展性，依赖昂贵的专业设备，且不适用于机器人应用。现有基于视觉触觉传感器的神经网络方法预测精度不足。", "method": "本文提出了两种基于长期循环卷积网络（LRCNs）和Transformer架构的模型。这些模型利用视觉触觉传感器GelSight捕获的RGB触觉图像及其他信息，以准确预测合规性指标。", "result": "所提出的模型能够准确估计合规性，并相对于基线表现出显著的性能提升。研究还发现，比传感器更硬的物体更难以估计其合规性。", "conclusion": "本文提出的LRCN和Transformer模型能有效提高基于视觉触觉传感器的合规性估计精度，但对于比传感器更硬的物体，其估计仍然面临挑战。", "translation": "合规性是描述工程、农业和生物医学应用中物体的一个关键参数。传统的合规性检测方法受限于其缺乏便携性和可扩展性，依赖于专业的、通常昂贵的设备，并且不适用于机器人应用。此外，现有使用基于视觉的触觉传感器的神经网络方法仍然存在预测精度不足的问题。在本文中，我们提出了两种基于长期循环卷积网络（LRCNs）和Transformer架构的模型，它们利用RGB触觉图像和由基于视觉的传感器GelSight捕获的其他信息来准确预测合规性指标。我们使用多项指标验证了这些模型的性能，并证明了它们在准确估计合规性方面的有效性。所提出的模型相对于基线表现出显著的性能提升。此外，我们研究了传感器合规性与物体合规性估计之间的相关性，结果表明比传感器更硬的物体更难以估计。", "summary": "本文针对传统合规性检测方法和现有神经网络方法在精度与适用性上的不足，提出了两种基于LRCN和Transformer架构的新型模型。这些模型利用视觉触觉传感器GelSight获取的RGB图像及其他信息，实现了对物体合规性的准确预测。实验结果表明，所提模型相比基线有显著性能提升，且揭示了估计比传感器更硬的物体时面临的挑战。", "keywords": "合规性检测, 视觉触觉传感器, LRCN, Transformer, GelSight", "comments": "该论文解决了机器人及其他领域对更准确、更便携的合规性检测的迫切需求。结合GelSight传感器与先进的神经网络架构（LRCNs和Transformers）代表了重要进展，显著提高了现有方法的精度。对更硬物体估计挑战的发现为未来的研究提供了宝贵的见解。"}}
{"id": "2506.14821", "title": "Reinforcing VLMs to Use Tools for Detailed Visual Reasoning Under Resource Constraints", "authors": ["Sunil Kumar", "Bowen Zhao", "Leo Dirac", "Paulina Varshavskaya"], "summary": "Despite tremendous recent advances in large model reasoning ability,\nvision-language models (VLMs) still struggle with detailed visual reasoning,\nespecially when compute resources are limited. To address this challenge, we\ndraw inspiration from methods like Deepseek-r1 for VLMs and train smaller-scale\nmodels with Group Relative Policy Optimization (GRPO) to use external tools\nsuch as zoom. The greatest benefit is obtained with a combination of GRPO\nlearning, a simple reward structure, a simplified tool-calling interface,\nallocating additional tokens to the result of the tool call, and a training\ndata mix that over-represents visually difficult examples. Compared to\nsimilarly-sized baseline models, our method achieves better performance on some\nvisual question-answering (VQA) tasks, thanks to the detailed visual\ninformation gathered from the external tool.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14821v1", "AI": {"title_translation": "在资源限制下强化VLM使用工具进行详细视觉推理", "tldr": "VLMs在资源有限时难以进行详细视觉推理，本文通过GRPO训练小型模型使用外部工具（如缩放）来提升性能。", "motivation": "尽管大型模型推理能力有巨大进步，但视觉语言模型（VLMs）在详细视觉推理方面仍有困难，尤其是在计算资源有限的情况下。", "method": "本文借鉴Deepseek-r1等方法，使用组相对策略优化（GRPO）训练小型VLM模型使用外部工具（如缩放）。该方法结合了GRPO学习、简单的奖励结构、简化的工具调用接口、为工具调用结果分配额外token以及包含过度代表视觉困难示例的训练数据混合。", "result": "与同等规模的基线模型相比，本文方法在某些视觉问答（VQA）任务上取得了更好的性能，这得益于从外部工具收集到的详细视觉信息。", "conclusion": "通过结合GRPO、奖励结构、简化接口、额外token和特定数据混合，可以有效强化小型VLMs使用外部工具进行详细视觉推理，从而提升VQA任务表现。", "translation": "尽管大型模型推理能力近期取得了巨大进展，但视觉语言模型（VLMs）在详细视觉推理方面仍然面临困难，尤其是在计算资源有限的情况下。为了解决这一挑战，我们借鉴了Deepseek-r1等VLM方法，并使用组相对策略优化（GRPO）训练小型模型以使用外部工具，例如缩放。最大的益处来自于GRPO学习、简单的奖励结构、简化的工具调用接口、为工具调用结果分配额外token以及包含过度代表视觉困难示例的训练数据混合的结合。与同等规模的基线模型相比，我们的方法在某些视觉问答（VQA）任务上取得了更好的性能，这得益于从外部工具收集到的详细视觉信息。", "summary": "本文提出一种在资源受限下提升视觉语言模型（VLMs）详细视觉推理能力的方法。通过借鉴Deepseek-r1，并利用组相对策略优化（GRPO）训练小型VLM使用外部工具（如缩放）。该方法结合了GRPO学习、简单奖励结构、简化工具调用接口、额外token分配及特殊数据混合，在视觉问答（VQA）任务上显著优于同规模基线模型。", "keywords": "视觉语言模型, 工具使用, 详细视觉推理, 资源限制, 组相对策略优化", "comments": "该论文的创新点在于将强化学习（GRPO）应用于小型VLM，使其能够在资源有限的情况下有效利用外部工具进行详细视觉推理。其重要性在于为计算资源受限的场景提供了一种提升VLM性能的有效途径，可能推动VLM在边缘设备或低功耗环境下的应用。"}}
{"id": "2506.15246", "title": "TopClustRAG at SIGIR 2025 LiveRAG Challenge", "authors": ["Juli Bakagianni", "John Pavlopoulos", "Aristidis Likas"], "summary": "We present TopClustRAG, a retrieval-augmented generation (RAG) system\ndeveloped for the LiveRAG Challenge, which evaluates end-to-end question\nanswering over large-scale web corpora. Our system employs a hybrid retrieval\nstrategy combining sparse and dense indices, followed by K-Means clustering to\ngroup semantically similar passages. Representative passages from each cluster\nare used to construct cluster-specific prompts for a large language model\n(LLM), generating intermediate answers that are filtered, reranked, and finally\nsynthesized into a single, comprehensive response. This multi-stage pipeline\nenhances answer diversity, relevance, and faithfulness to retrieved evidence.\nEvaluated on the FineWeb Sample-10BT dataset, TopClustRAG ranked 2nd in\nfaithfulness and 7th in correctness on the official leaderboard, demonstrating\nthe effectiveness of clustering-based context filtering and prompt aggregation\nin large-scale RAG systems.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15246v1", "AI": {"title_translation": "TopClustRAG 在 SIGIR 2025 LiveRAG 挑战赛中的表现", "tldr": "TopClustRAG 是一个用于大规模网络语料库问答的 RAG 系统，它结合了混合检索和 K-Means 聚类来增强答案多样性、相关性和忠实度，在 LiveRAG 挑战赛中表现出色。", "motivation": "该论文开发 TopClustRAG 系统是为了参加 LiveRAG 挑战赛，该挑战赛旨在评估在大规模网络语料库上进行端到端问答的能力。", "method": "TopClustRAG 采用混合检索策略，结合稀疏和密集索引。随后，使用 K-Means 聚类对语义相似的段落进行分组。每个簇的代表性段落用于为大型语言模型 (LLM) 构建簇特定提示，生成中间答案。这些中间答案经过过滤、重新排序，最后合成为一个单一的综合响应。", "result": "在 FineWeb Sample-10BT 数据集上评估，TopClustRAG 在官方排行榜上忠实度排名第2，正确性排名第7。", "conclusion": "该论文得出结论，聚类驱动的上下文过滤和提示聚合在大规模 RAG 系统中是有效的。", "translation": "我们介绍了 TopClustRAG，这是一个为 LiveRAG 挑战赛开发的检索增强生成 (RAG) 系统，该挑战赛评估在大规模网络语料库上的端到端问答。我们的系统采用混合检索策略，结合稀疏和密集索引，随后通过 K-Means 聚类将语义相似的段落分组。来自每个簇的代表性段落用于为大型语言模型 (LLM) 构建簇特定提示，生成中间答案，这些答案经过过滤、重新排序，最终合成为一个单一的、全面的响应。这种多阶段管道增强了答案的多样性、相关性和对检索证据的忠实度。在 FineWeb Sample-10BT 数据集上评估，TopClustRAG 在官方排行榜上忠实度排名第2，正确性排名第7，证明了聚类驱动的上下文过滤和提示聚合在大规模 RAG 系统中的有效性。", "summary": "TopClustRAG 是一个为 LiveRAG 挑战赛设计的检索增强生成 (RAG) 系统，旨在解决大规模网络语料库上的问答任务。该系统结合了混合检索（稀疏和密集索引）与 K-Means 聚类，以分组语义相似的段落。它利用每个簇的代表性段落为大型语言模型构建特定的提示，生成中间答案，并通过过滤、重新排序和合成，最终提供全面响应。这种方法显著提升了答案的多样性、相关性和对检索证据的忠实度。在 FineWeb Sample-10BT 数据集上的评估显示，TopClustRAG 在忠实度方面排名第2，在正确性方面排名第7，验证了其在大型 RAG 系统中基于聚类的上下文过滤和提示聚合的有效性。", "keywords": "RAG, 检索增强生成, K-Means 聚类, 大型语言模型, 问答", "comments": "TopClustRAG 的创新之处在于其独特的多阶段管道，特别是将 K-Means 聚类引入到 RAG 流程中，用于上下文过滤和提示聚合。这种方法有效地解决了大规模语料库中信息冗余和上下文过载的问题，通过生成簇特定的提示，提高了 LLM 的响应质量和忠实度。其在 LiveRAG 挑战赛中的出色表现证明了该方法的实际效用和潜力。"}}
{"id": "2506.15627", "title": "Pathwise convergence of a novel numerical scheme based on semi-implicit method for stochastic differential-algebraic equations with non-global Lipschitz coefficients", "authors": ["Guy Tsafack", "Antoine Tambue"], "summary": "This paper delves into the well-posedness and the numerical approximation of\nnon-autonomous stochastic differential algebraic equations (SDAEs) with\nnonlinear local Lipschitz coefficients that satisfy the more general\nmonotonicity condition called Khasminskii condition. The key challenge is the\npresence of a singular matrix which makes the numerical integration hard and\nheavy. To address this challenge, we propose a novel numerical scheme based on\nsemi-implicit method for the drift component of the SDAEs. More precisely we\nsplit the drift term as the sum of a linear term and a nonlinear term. The\nlinear part is approximated implicitly, while the nonlinear part is\napproximated explicitly. The linear component's role is to handle the\nsingularity issues during the numerical integration without the resolution of\nnonlinear algebraic equations in the constraint equations. This novel scheme is\ntherefore very efficient for SDAEs in high dimension that come after the\nspatial discretisation of stochastic partial differential algebraic equations\n(SPDAEs). To prove the pathwise convergence of our novel scheme, we first\nderive a equivalent scheme called dual scheme, suitable for mathematical\nanalysis and linked to the inherent stochastic differential equation resulting\nfrom the elimination of constraints in the initial SDAEs. We prove that our\nnovel scheme converges to the exact solution with rate $\\frac{1}{2}-\\epsilon$,\nfor arbitrary $\\epsilon>0$ in the pathwise sense. Numerical simulations are\nperformed to demonstrate the efficiency of the scheme in high dimension and to\nshow that our theoretical results are in agreement with numerical experiments.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15627v1", "AI": {"title_translation": "随机微分代数方程非全局Lipschitz系数半隐式新数值格式的路径收敛性", "tldr": "本文提出了一种基于半隐式方法的新型数值格式，用于解决具有奇异矩阵的随机微分代数方程（SDAEs）的数值积分难题，并从理论上证明了其路径收敛性。", "motivation": "解决具有非全局Lipschitz系数且存在奇异矩阵的非自治随机微分代数方程（SDAEs）的数值积分困难。", "method": "提出了一种基于半隐式方法的SDAEs漂移分量新数值格式。该方法将漂移项分解为线性和非线性两部分，线性部分隐式近似，非线性部分显式近似。为了证明路径收敛性，首先推导了一个等价的对偶格式。", "result": "该新格式以$\\frac{1}{2}-\\epsilon$（任意$\\epsilon>0$）的速率在路径意义上收敛到精确解。数值模拟表明该格式在高维情况下高效，且理论结果与数值实验一致。", "conclusion": "提出的半隐式新数值格式能够有效且高效地解决具有奇异矩阵的高维随机微分代数方程的数值积分问题，并具有理论上的路径收敛性保证。", "translation": "本文深入探讨了具有满足更一般单调性条件（称为Khasminskii条件）的非线性局部Lipschitz系数的非自治随机微分代数方程（SDAEs）的适定性和数值近似问题。关键挑战在于存在一个奇异矩阵，这使得数值积分变得困难且繁重。为了应对这一挑战，我们提出了一种基于半隐式方法的SDAEs漂移分量的新型数值格式。更精确地说，我们将漂移项分解为线性项和非线性项之和。线性部分采用隐式近似，而非线性部分采用显式近似。线性分量的作用是在数值积分过程中处理奇异性问题，而无需在约束方程中求解非线性代数方程。因此，这种新型格式对于随机偏微分代数方程（SPDAEs）空间离散化后产生的高维SDAEs非常有效。为了证明我们新型格式的路径收敛性，我们首先推导出一个等价的格式，称为对偶格式，该格式适合于数学分析，并与初始SDAEs中消除约束后产生的固有随机微分方程相关联。我们证明了我们的新型格式以$\\frac{1}{2}-\\epsilon$（对于任意$\\epsilon>0$）的速率在路径意义上收敛到精确解。进行了数值模拟以证明该格式在高维情况下的效率，并表明我们的理论结果与数值实验一致。", "summary": "本文研究了具有非全局Lipschitz系数的随机微分代数方程（SDAEs）的数值近似问题，特别是其数值积分中奇异矩阵带来的挑战。作者提出了一种新颖的半隐式数值格式，通过将漂移项分解为隐式处理的线性部分和显式处理的非线性部分，有效解决了奇异性问题。该格式在高维SDAEs中表现出高效性，并从理论上证明了其以$\\frac{1}{2}-\\epsilon$的速率路径收敛到精确解。数值实验也验证了理论结果。", "keywords": "随机微分代数方程, 半隐式方法, 路径收敛性, 奇异矩阵, Khasminskii条件", "comments": "本文的创新之处在于提出了一种针对随机微分代数方程的半隐式数值方案，该方案巧妙地通过分解漂移项并分别处理线性与非线性部分，有效地解决了数值积分中由奇异矩阵引起的难题，避免了高维非线性代数方程的求解。这对于随机偏微分代数方程的空间离散化具有重要意义。文章不仅提出了新方案，还从理论上证明了其路径收敛性，并辅以数值模拟验证，使得研究结果更具说服力。"}}
{"id": "2506.15656", "title": "PhishDebate: An LLM-Based Multi-Agent Framework for Phishing Website Detection", "authors": ["Wenhao Li", "Selvakumar Manickam", "Yung-wey Chong", "Shankar Karuppayah"], "summary": "Phishing websites continue to pose a significant cybersecurity threat, often\nleveraging deceptive structures, brand impersonation, and social engineering\ntactics to evade detection. While recent advances in large language models\n(LLMs) have enabled improved phishing detection through contextual\nunderstanding, most existing approaches rely on single-agent classification\nfacing the risks of hallucination and lack interpretability or robustness. To\naddress these limitations, we propose PhishDebate, a modular multi-agent\nLLM-based debate framework for phishing website detection. PhishDebate employs\nfour specialized agents to independently analyze different textual aspects of a\nwebpage--URL structure, HTML composition, semantic content, and brand\nimpersonation--under the coordination of a Moderator and a final Judge. Through\nstructured debate and divergent thinking, the framework delivers more accurate\nand interpretable decisions. Extensive evaluations on commercial LLMs\ndemonstrate that PhishDebate achieves 98.2% recall and 98.2% True Positive Rate\n(TPR) on a real-world phishing dataset, and outperforms single-agent and Chain\nof Thought (CoT) baselines. Additionally, its modular design allows agent-level\nconfigurability, enabling adaptation to varying resource and application\nrequirements.", "comment": null, "cate": "cs.CR", "url": "http://arxiv.org/abs/2506.15656v1", "AI": {"title_translation": "PhishDebate：一个基于LLM的多智能体网络钓鱼网站检测框架", "tldr": "提出PhishDebate，一个基于LLM的多智能体辩论框架，用于高精度、可解释地检测网络钓鱼网站。", "motivation": "现有的基于LLM的网络钓鱼检测方法多为单智能体分类，存在幻觉、缺乏可解释性和鲁棒性的问题，而网络钓鱼网站仍是重大网络安全威胁。", "method": "提出PhishDebate，一个模块化的多智能体LLM辩论框架。它采用四个专门的智能体（URL结构、HTML组成、语义内容、品牌冒充）在主持人协调下独立分析网页不同文本方面，并由最终法官做出决策，通过结构化辩论和发散思维实现。", "result": "在真实世界网络钓鱼数据集上，PhishDebate实现了98.2%的召回率和98.2%的真阳性率，优于单智能体和CoT基线。其模块化设计允许智能体级别配置。", "conclusion": "PhishDebate通过多智能体辩论机制，有效解决了现有LLM检测方法的局限性，提供了更准确、可解释且鲁棒的网络钓鱼网站检测能力。", "translation": "网络钓鱼网站仍然构成严重的网络安全威胁，它们经常利用欺骗性结构、品牌冒充和社会工程策略来逃避检测。尽管大型语言模型（LLM）的最新进展通过上下文理解提高了网络钓鱼检测能力，但大多数现有方法依赖于单智能体分类，面临幻觉、缺乏可解释性或鲁棒性的风险。为了解决这些局限性，我们提出了PhishDebate，一个模块化的基于LLM的多智能体辩论框架，用于网络钓鱼网站检测。PhishDebate采用四个专门的智能体，在主持人（Moderator）和最终法官（Judge）的协调下，独立分析网页的不同文本方面——URL结构、HTML组成、语义内容和品牌冒充。通过结构化辩论和发散思维，该框架提供了更准确和可解释的决策。对商业LLM的广泛评估表明，PhishDebate在真实世界网络钓鱼数据集上实现了98.2%的召回率和98.2%的真阳性率（TPR），并优于单智能体和思维链（CoT）基线。此外，其模块化设计允许智能体级别的可配置性，从而能够适应不同的资源和应用需求。", "summary": "本文提出了PhishDebate，一个基于LLM的多智能体辩论框架，旨在解决现有单智能体LLM在网络钓鱼网站检测中存在的幻觉、缺乏可解释性和鲁棒性问题。该框架由四个专业智能体协作，通过结构化辩论分析网页的不同文本特征，并在主持人与法官的协调下做出决策。实验证明，PhishDebate在检测准确性、召回率和可解释性方面均优于现有基线方法，并具有良好的模块化可配置性。", "keywords": "网络钓鱼检测, LLM, 多智能体系统, 辩论框架, 网络安全", "comments": "PhishDebate的创新之处在于其多智能体辩论框架，通过模拟人类辩论过程，有效利用LLM的上下文理解能力，并解决单智能体LLM固有的幻觉和可解释性问题。这种协同决策机制显著提高了网络钓鱼检测的准确性和鲁棒性，为LLM在网络安全领域的应用提供了新的范式。"}}
{"id": "2506.13776", "title": "Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations", "authors": ["Kevin L. Wei", "Patricia Paskov", "Sunishchal Dev", "Michael J. Byun", "Anka Reuel", "Xavier Roberts-Gaal", "Rachel Calcott", "Evie Coxon", "Chinmay Deshpande"], "summary": "In this position paper, we argue that human baselines in foundation model\nevaluations must be more rigorous and more transparent to enable meaningful\ncomparisons of human vs. AI performance, and we provide recommendations and a\nreporting checklist towards this end. Human performance baselines are vital for\nthe machine learning community, downstream users, and policymakers to interpret\nAI evaluations. Models are often claimed to achieve \"super-human\" performance,\nbut existing baselining methods are neither sufficiently rigorous nor\nsufficiently well-documented to robustly measure and assess performance\ndifferences. Based on a meta-review of the measurement theory and AI evaluation\nliteratures, we derive a framework with recommendations for designing,\nexecuting, and reporting human baselines. We synthesize our recommendations\ninto a checklist that we use to systematically review 115 human baselines\n(studies) in foundation model evaluations and thus identify shortcomings in\nexisting baselining methods; our checklist can also assist researchers in\nconducting human baselines and reporting results. We hope our work can advance\nmore rigorous AI evaluation practices that can better serve both the research\ncommunity and policymakers. Data is available at:\nhttps://github.com/kevinlwei/human-baselines", "comment": "A version of this paper has been accepted to ICML 2025 as a position\n  paper (spotlight), with the title: \"Position: Human Baselines in Model\n  Evaluations Need Rigor and Transparency (With Recommendations & Reporting\n  Checklist).\"", "cate": "cs.AI", "url": "http://arxiv.org/abs/2506.13776v1", "AI": {"title_translation": "模型评估中严谨透明的人类基线推荐与报告清单", "tldr": "本立场文件提出并提供了一份推荐和报告清单，旨在使基础模型评估中的人类基线更加严谨和透明，以实现有意义的人类与AI性能比较。", "motivation": "当前，基础模型评估中的人类基线既不够严谨也不够透明，导致难以有意义地比较人类与AI的性能。现有方法不足以可靠地衡量和评估性能差异，因此需要更严谨、更透明的人类基线来正确解释AI评估结果，并避免对“超人类”性能的误报。", "method": "作者基于测量理论和AI评估文献的元审查，推导出了一个设计、执行和报告人类基线的框架。他们将这些建议综合成一份清单，并用其系统地审查了115项基础模型评估中的人类基线研究，以识别现有方法的不足。", "result": "研究识别了现有基线方法中的缺点。他们开发的清单不仅可以用于审查现有研究，还可以帮助研究人员进行人类基线研究并报告结果。", "conclusion": "作者希望他们的工作能够推进更严谨的AI评估实践，从而更好地服务于研究界和政策制定者。", "translation": "在这篇立场文件中，我们认为基础模型评估中的人类基线必须更加严谨和透明，以便对人类与AI的性能进行有意义的比较，我们为此提供了建议和一份报告清单。人类性能基线对于机器学习社区、下游用户和政策制定者解释AI评估至关重要。模型经常被声称达到“超人类”性能，但现有的基线方法既不够严谨，也缺乏足够的文档记录，无法稳健地测量和评估性能差异。基于对测量理论和AI评估文献的元审查，我们推导出了一个设计、执行和报告人类基线的框架和建议。我们将建议综合成一份清单，并用其系统地审查了115项基础模型评估中的人类基线（研究），从而识别了现有基线方法的不足；我们的清单也可以帮助研究人员进行人类基线研究和报告结果。我们希望我们的工作能够推进更严谨的AI评估实践，从而更好地服务于研究社区和政策制定者。数据可在以下网址获取：https://github.com/kevinlwei/human-baselines", "summary": "本立场文件强调了在基础模型评估中建立严谨且透明的人类基线的重要性。针对当前基线方法存在的不足，作者基于对测量理论和AI评估文献的元审查，提出了一套设计、执行和报告人类基线的框架和建议，并将其整合为一份可操作的报告清单。通过对115项现有研究的系统审查，该清单揭示了当前人类基线方法的缺陷，并可作为未来研究实践的指导工具，旨在提升AI评估的严谨性。", "keywords": "人类基线, AI评估, 严谨性, 透明度, 报告清单", "comments": "本文作为一篇立场文件，其创新性在于系统地提出了提升AI模型评估中人类基线严谨性和透明度的具体方法和工具（报告清单）。在AI性能被频繁宣称达到“超人类”水平的背景下，这项工作的重要性不言而喻，它直接触及了AI评估的公平性、可靠性和可解释性核心问题，对研究界和政策制定者都具有重要指导意义。其局限性可能在于，作为一个立场文件，它主要提供了建议和框架，具体的实施细节和在不同AI任务中的普适性仍需在未来的实践中进一步验证。"}}
{"id": "2506.15010", "title": "Hyper-Local Deformable Transformers for Text Spotting on Historical Maps", "authors": ["Yijun Lin", "Yao-Yi Chiang"], "summary": "Text on historical maps contains valuable information providing georeferenced\nhistorical, political, and cultural contexts. However, text extraction from\nhistorical maps is challenging due to the lack of (1) effective methods and (2)\ntraining data. Previous approaches use ad-hoc steps tailored to only specific\nmap styles. Recent machine learning-based text spotters (e.g., for scene\nimages) have the potential to solve these challenges because of their\nflexibility in supporting various types of text instances. However, these\nmethods remain challenges in extracting precise image features for predicting\nevery sub-component (boundary points and characters) in a text instance. This\nis critical because map text can be lengthy and highly rotated with complex\nbackgrounds, posing difficulties in detecting relevant image features from a\nrough text region. This paper proposes PALETTE, an end-to-end text spotter for\nscanned historical maps of a wide variety. PALETTE introduces a novel\nhyper-local sampling module to explicitly learn localized image features around\nthe target boundary points and characters of a text instance for detection and\nrecognition. PALETTE also enables hyper-local positional embeddings to learn\nspatial interactions between boundary points and characters within and across\ntext instances. In addition, this paper presents a novel approach to\nautomatically generate synthetic map images, SynthMap+, for training text\nspotters for historical maps. The experiment shows that PALETTE with SynthMap+\noutperforms SOTA text spotters on two new benchmark datasets of historical\nmaps, particularly for long and angled text. We have deployed PALETTE with\nSynthMap+ to process over 60,000 maps in the David Rumsey Historical Map\ncollection and generated over 100 million text labels to support map searching.\nThe project is released at\nhttps://github.com/kartta-foundation/mapkurator-palette-doc.", "comment": "Published in KDD2024", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15010v1", "AI": {"title_translation": "历史地图文本识别的超局部可变形Transformer", "tldr": "PALETTE是一个用于历史地图文本识别的端到端系统，它引入了超局部采样和位置嵌入，并通过自动生成的合成数据SynthMap+进行训练，在处理长文本和倾斜文本方面表现出色，并已应用于大规模地图集。", "motivation": "历史地图中的文本包含宝贵信息，但由于缺乏有效方法和训练数据，文本提取极具挑战性。现有方法针对特定地图风格，而最新的机器学习文本识别器在提取精确图像特征以预测文本实例的子组件（边界点和字符）时仍面临困难，尤其对于冗长、高度旋转且背景复杂的地图文本。", "method": "本文提出了PALETTE，一个用于扫描历史地图的端到端文本识别器。PALETTE引入了一个新颖的超局部采样模块，用于明确学习目标边界点和字符周围的局部图像特征，以进行检测和识别。PALETTE还启用了超局部位置嵌入，以学习文本实例内部和之间边界点与字符的空间交互。此外，本文提出了一种自动生成合成地图图像（SynthMap+）的新方法，用于训练历史地图文本识别器。", "result": "实验表明，结合SynthMap+的PALETTE在两个新的历史地图基准数据集上优于SOTA文本识别器，尤其在处理长文本和倾斜文本方面表现突出。PALETTE与SynthMap+已被部署，处理了David Rumsey历史地图集中超过60,000张地图，并生成了超过1亿个文本标签以支持地图搜索。", "conclusion": "PALETTE及其合成数据生成方法SynthMap+为历史地图的文本识别提供了一个高效且鲁棒的解决方案，尤其擅长处理复杂文本，并已在大规模实际应用中得到验证。", "translation": "历史地图上的文本包含有价值的信息，提供了地理参考的历史、政治和文化背景。然而，由于缺乏（1）有效方法和（2）训练数据，从历史地图中提取文本极具挑战性。以前的方法使用专门针对特定地图风格的临时步骤。最近基于机器学习的文本识别器（例如，用于场景图像的）有潜力解决这些挑战，因为它们在支持各种类型文本实例方面具有灵活性。然而，这些方法在提取精确图像特征以预测文本实例中的每个子组件（边界点和字符）时仍然面临挑战。这至关重要，因为地图文本可能冗长且高度旋转，背景复杂，导致难以从粗略的文本区域中检测相关图像特征。本文提出了PALETTE，一个用于各种扫描历史地图的端到端文本识别器。PALETTE引入了一个新颖的超局部采样模块，以明确学习文本实例目标边界点和字符周围的局部图像特征，用于检测和识别。PALETTE还启用了超局部位置嵌入，以学习文本实例内部和之间边界点和字符的空间交互。此外，本文提出了一种自动生成合成地图图像（SynthMap+）的新方法，用于训练历史地图文本识别器。实验表明，结合SynthMap+的PALETTE在两个新的历史地图基准数据集上优于SOTA文本识别器，特别是在处理长文本和倾斜文本方面。我们已将PALETTE与SynthMap+部署，处理了David Rumsey历史地图集中超过60,000张地图，并生成了超过1亿个文本标签以支持地图搜索。该项目已在https://github.com/kartta-foundation/mapkurator-palette-doc发布。", "summary": "本文提出了一种名为PALETTE的端到端文本识别系统，专为历史地图设计。该系统通过引入超局部采样模块来精确提取文本边界点和字符的局部特征，并利用超局部位置嵌入学习它们之间的空间关系。为了解决训练数据不足的问题，研究人员还开发了SynthMap+来自动生成合成地图图像。实验结果表明，PALETTE结合SynthMap+在新的历史地图数据集上表现优异，尤其擅长处理长文本和倾斜文本。该系统已成功应用于David Rumsey历史地图集，处理了大量地图并生成了过亿的文本标签，极大地支持了地图搜索功能。", "keywords": "历史地图, 文本识别, 可变形Transformer, 超局部采样, 合成数据", "comments": "该论文的创新点在于提出了超局部采样模块和超局部位置嵌入，这对于处理历史地图中复杂、多变且常常高度旋转的文本至关重要。此外，SynthMap+的引入解决了历史地图领域训练数据稀缺的关键问题，为该领域的进一步研究提供了宝贵的资源。大规模的实际部署和生成超过1亿文本标签的成果，充分证明了该方法的实用性和高效性，对历史地图数字化和信息提取具有重要意义。"}}
{"id": "2506.14824", "title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal Large Language Models", "authors": ["Yao Zhang", "Hewei Gao", "Haokun Chen", "Weiguo Li", "Yunpu Ma", "Volker Tresp"], "summary": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal\nreasoning and cross-modal retrieval but face deployment challenges in\nreal-world scenarios due to distributed multimodal data and strict privacy\nrequirements. Federated Learning (FL) offers a solution by enabling\ncollaborative model training without centralizing data. However, realizing FL\nfor MLLMs presents significant challenges, including high computational\ndemands, limited client capacity, substantial communication costs, and\nheterogeneous client data. Existing FL methods assume client-side deployment of\nfull models, an assumption that breaks down for large-scale MLLMs due to their\nmassive size and communication demands. To address these limitations, we\npropose FedNano, the first FL framework that centralizes the LLM on the server\nwhile introducing NanoEdge, a lightweight module for client-specific\nadaptation. NanoEdge employs modality-specific encoders, connectors, and\ntrainable NanoAdapters with low-rank adaptation. This design eliminates the\nneed to deploy LLM on clients, reducing client-side storage by 95%, and\nlimiting communication overhead to only 0.01% of the model parameters. By\ntransmitting only compact NanoAdapter updates, FedNano handles heterogeneous\nclient data and resource constraints while preserving privacy. Experiments\ndemonstrate that FedNano outperforms prior FL baselines, bridging the gap\nbetween MLLM scale and FL feasibility, and enabling scalable, decentralized\nmultimodal AI systems.", "comment": "12 pages, 3 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14824v1", "AI": {"title_translation": "FedNano: 面向预训练多模态大型语言模型的轻量级联邦微调", "tldr": "FedNano是首个将LLM集中在服务器端，并通过轻量级NanoEdge模块实现客户端自适应的联邦学习框架，显著降低了多模态大语言模型（MLLMs）在联邦学习中的部署和通信成本，并超越了现有基线。", "motivation": "多模态大型语言模型（MLLMs）在多模态推理和跨模态检索等任务中表现出色，但由于分布式多模态数据和严格的隐私要求，在实际部署中面临挑战。联邦学习（FL）提供了一种解决方案，但将FL应用于MLLMs面临计算需求高、客户端容量有限、通信成本大以及客户端数据异构性等重大挑战。现有FL方法假设客户端部署完整模型，但这对于大规模MLLMs来说因其庞大的模型尺寸和通信需求而不再适用。", "method": "我们提出了FedNano，这是第一个将大型语言模型（LLM）集中在服务器上，同时引入NanoEdge（一个用于客户端特定适应的轻量级模块）的联邦学习框架。NanoEdge采用模态特定的编码器、连接器和可训练的基于低秩适应（low-rank adaptation）的NanoAdapters。这种设计消除了在客户端部署LLM的需要，仅传输紧凑的NanoAdapter更新，从而处理异构客户端数据和资源限制，同时保护隐私。", "result": "FedNano将客户端存储需求减少了95%，并将通信开销限制在模型参数的0.01%。实验表明，FedNano优于先前的联邦学习基线。", "conclusion": "FedNano弥合了多模态大型语言模型规模与联邦学习可行性之间的差距，并实现了可扩展的去中心化多模态AI系统。", "translation": "多模态大型语言模型（MLLMs）在多模态推理和跨模态检索等任务中表现出色，但在实际场景中，由于分布式多模态数据和严格的隐私要求，面临部署挑战。联邦学习（FL）通过在不集中数据的情况下实现协作模型训练，提供了一种解决方案。然而，为MLLMs实现FL带来了重大挑战，包括高计算需求、有限的客户端容量、巨大的通信成本以及异构的客户端数据。现有的FL方法假设客户端部署完整模型，但这一假设对于大规模MLLMs来说由于其庞大的尺寸和通信需求而失效。为了解决这些限制，我们提出了FedNano，这是第一个将大型语言模型（LLM）集中在服务器端，同时引入NanoEdge（一个用于客户端特定适应的轻量级模块）的联邦学习框架。NanoEdge采用模态特定的编码器、连接器和可训练的基于低秩适应的NanoAdapters。这种设计消除了在客户端部署LLM的需要，将客户端存储减少了95%，并将通信开销限制在模型参数的0.01%。通过仅传输紧凑的NanoAdapter更新，FedNano处理异构客户端数据和资源限制，同时保护隐私。实验表明，FedNano优于先前的FL基线，弥合了MLLM规模与FL可行性之间的差距，并实现了可扩展的去中心化多模态AI系统。", "summary": "FedNano是一个创新的联邦学习框架，旨在解决多模态大型语言模型（MLLMs）在联邦学习部署中面临的巨大挑战，如高计算和通信成本以及客户端资源限制。该框架通过将大型语言模型（LLM）集中在服务器端，并在客户端引入轻量级的NanoEdge模块（包含模态特定编码器、连接器和基于低秩适应的NanoAdapters）来实现客户端适应。这种设计显著减少了客户端存储需求（95%）和通信开销（0.01%模型参数），使得大规模MLLMs的联邦学习成为可能。实验结果表明FedNano优于现有基线，为可扩展的去中心化多模态AI系统铺平了道路。", "keywords": "联邦学习, 多模态大型语言模型, 轻量级微调, NanoEdge, 低秩适应", "comments": "FedNano的创新之处在于其独特的架构设计，通过将大部分模型（LLM）放置在服务器端，并仅在客户端处理轻量级、可训练的自适应模块（NanoEdge），从而有效解决了大规模多模态大语言模型在联邦学习中面临的资源限制和通信瓶颈。这种方法显著降低了客户端的负担，使其更适用于资源受限的边缘设备。其重要性在于，它为在保护隐私的前提下，实现多模态AI模型的分布式训练和部署提供了可行途径，特别是在数据分散且敏感的实际应用场景中。"}}
{"id": "2506.15266", "title": "Thunder-DeID: Accurate and Efficient De-identification Framework for Korean Court Judgments", "authors": ["Sungen Hahm", "Heejin Kim", "Gyuseong Lee", "Hyunji Park", "Jaejin Lee"], "summary": "To ensure a balance between open access to justice and personal data\nprotection, the South Korean judiciary mandates the de-identification of court\njudgments before they can be publicly disclosed. However, the current\nde-identification process is inadequate for handling court judgments at scale\nwhile adhering to strict legal requirements. Additionally, the legal\ndefinitions and categorizations of personal identifiers are vague and not\nwell-suited for technical solutions. To tackle these challenges, we propose a\nde-identification framework called Thunder-DeID, which aligns with relevant\nlaws and practices. Specifically, we (i) construct and release the first Korean\nlegal dataset containing annotated judgments along with corresponding lists of\nentity mentions, (ii) introduce a systematic categorization of Personally\nIdentifiable Information (PII), and (iii) develop an end-to-end deep neural\nnetwork (DNN)-based de-identification pipeline. Our experimental results\ndemonstrate that our model achieves state-of-the-art performance in the\nde-identification of court judgments.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15266v1", "AI": {"title_translation": "Thunder-DeID：韩国法院判决书的准确高效去标识化框架", "tldr": "提出Thunder-DeID框架，通过构建数据集、系统化PII分类和开发DNN管道，解决了韩国法院判决书大规模去标识化的问题，并实现了最先进的性能。", "motivation": "为了在司法公开和个人数据保护之间取得平衡，韩国司法机构要求在公开披露法院判决书之前对其进行去标识化。然而，当前的去标识化流程无法大规模处理判决书并满足严格的法律要求，且个人标识符的法律定义和分类模糊，不适用于技术解决方案。", "method": "提出了一个名为Thunder-DeID的去标识化框架，该框架符合相关法律和实践。具体包括：(i) 构建并发布了第一个包含已标注判决书及其相应实体提及列表的韩国法律数据集；(ii) 引入了个人可识别信息（PII）的系统分类；以及 (iii) 开发了一个端到端的基于深度神经网络（DNN）的去标识化管道。", "result": "实验结果表明，该模型在法院判决书的去标识化方面取得了最先进的性能。", "conclusion": "Thunder-DeID框架有效解决了韩国法院判决书大规模去标识化所面临的挑战，并在准确性上达到了最先进的水平，有助于平衡司法公开与个人数据保护。", "translation": "为了在司法公开和个人数据保护之间取得平衡，韩国司法机构要求在公开披露法院判决书之前对其进行去标识化处理。然而，目前的去标识化流程不足以大规模处理法院判决书，同时遵守严格的法律要求。此外，个人标识符的法律定义和分类模糊不清，不适合技术解决方案。为了应对这些挑战，我们提出了一个名为 Thunder-DeID 的去标识化框架，该框架符合相关法律和实践。具体来说，我们 (i) 构建并发布了第一个包含已标注判决书及其相应实体提及列表的韩国法律数据集，(ii) 引入了个人可识别信息 (PII) 的系统分类，以及 (iii) 开发了一个端到端的基于深度神经网络 (DNN) 的去标识化管道。我们的实验结果表明，我们的模型在法院判决书的去标识化方面取得了最先进的性能。", "summary": "本文提出了Thunder-DeID框架，旨在解决韩国法院判决书大规模去标识化面临的挑战。针对现有流程不足和法律定义模糊的问题，该框架通过构建首个韩国法律标注数据集、系统化PII分类以及开发基于深度神经网络的端到端去标识化管道，实现了对判决书的准确高效去标识化，并在实验中达到了最先进的性能。", "keywords": "去标识化, 韩国法院判决书, 深度神经网络, 个人可识别信息, 法律数据集", "comments": "这篇论文通过提出一个全面的框架，有效解决了韩国法院判决书大规模去标识化的实际且紧迫的问题。其创新之处在于构建了首个韩国法律领域标注数据集，对PII进行了系统分类，并采用了先进的DNN技术，这些都为该领域的未来研究和应用奠定了基础。这项工作对于平衡司法透明度和个人隐私保护具有重要意义。"}}
{"id": "2506.15630", "title": "Non-uniform finite-element meshes defined by ray dynamics for Helmholtz problems", "authors": ["Martin Averseng", "Jeffrey Galkowski", "Euan A. Spence"], "summary": "The $h$-version of the finite-element method ($h$-FEM) applied to the\nhigh-frequency Helmholtz equation has been a classic topic in numerical\nanalysis since the 1990s. It is now rigorously understood that (using piecewise\npolynomials of degree $p$ on a mesh of a maximal width $h$) the conditions\n\"$(hk)^p \\rho$ sufficiently small\" and \"$(hk)^{2p} \\rho$ sufficiently small\"\nguarantee, respectively, $k$-uniform quasioptimality (QO) and bounded relative\nerror (BRE), where $\\rho$ is the norm of the solution operator with $\\rho\\sim\nk$ for non-trapping problems. Empirically, these conditions are observed to be\noptimal in the context of $h$-FEM with a uniform mesh. This paper demonstrates\nthat QO and BRE can be achieved using certain non-uniform meshes that violate\nthe conditions above on $h$ and involve coarser meshes away from trapping and\nin the perfectly matched layer (PML). The main theorem details how varying the\nmeshwidth in one region affects errors both in that region and elsewhere. One\nnotable consequence is that, for any scattering problem (trapping or\nnontrapping), in the PML one only needs $hk$ to be sufficiently small; i.e.\nthere is no pollution in the PML.\n  The motivating idea for the analysis is that the Helmholtz data-to-solution\nmap behaves differently depending on the locations of both the measurement and\ndata, in particular, on the properties of billiards trajectories (i.e. rays)\nthrough these sets. Because of this, it is natural that the approximation\nrequirements for finite-element spaces in a subset should depend on the\nproperties of billiard rays through that set. Inserting this behaviour into the\nlatest duality arguments for the FEM applied to the high-frequency Helmholtz\nequation allows us to retain detailed information about the influence of\n$\\textit{both}$ the mesh structure $\\textit{and}$ the behaviour of the true\nsolution on local errors in FEM.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15630v1", "AI": {"title_translation": "由射线动力学定义的非均匀有限元网格用于亥姆霍兹问题", "tldr": "本文提出使用由射线动力学引导的非均匀有限元网格来解决高频亥姆霍兹问题，证明即使在某些区域使用更粗的网格也能保证方法的准确性，并且完美匹配层(PML)中没有污染。", "motivation": "针对高频亥姆霍兹方程的有限元方法(h-FEM)在均匀网格下，其k-均匀拟最优性(QO)和有界相对误差(BRE)的传统条件被认为是经验最优的。然而，分析的动机在于亥姆霍兹数据到解的映射行为会根据测量和数据的不同位置而变化，特别是取决于穿过这些集合的弹道轨迹（即射线）的性质。因此，子集中有限元空间的近似要求应自然地取决于穿过该集合的弹道射线的性质，这启发了对非均匀网格的研究。", "method": "本文通过使用违反传统h条件的非均匀网格来证明高频亥姆霍兹方程中h-FEM的拟最优性(QO)和有界相对误差(BRE)可以实现，这些网格在远离陷阱区域和完美匹配层(PML)中包含更粗的网格。研究方法是将射线动力学的行为融入到应用于高频亥姆霍兹方程的FEM的最新对偶性论证中，以详细分析网格结构和真实解行为对局部误差的影响。", "result": "研究结果表明，QO和BRE可以通过某些非均匀网格来实现，这些网格可以违反传统上对h的条件，并在远离陷阱区域和完美匹配层(PML)中使用更粗的网格。主要定理详细说明了在一个区域中改变网格宽度如何影响该区域和其它区域的误差。一个显著的推论是，对于任何散射问题，在PML中只需要hk足够小，这意味着PML中没有污染。", "conclusion": "本文得出结论，通过将射线动力学行为融入对偶性论证来构建非均匀有限元网格，可以有效地解决高频亥姆霍兹问题，并在保持拟最优性和有界相对误差的同时，允许在某些区域（如PML和非陷阱区域）使用更粗的网格，从而可能提高计算效率。", "translation": "自20世纪90年代以来，应用于高频亥姆霍兹方程的有限元方法($h$-FEM)的$h$版本一直是数值分析中的经典课题。现在已严格理解，(在最大宽度为$h$的网格上使用$p$次分段多项式)，条件“$(hk)^p \\rho$足够小”和“$(hk)^{2p} \\rho$足够小”分别保证了$k$-均匀拟最优性(QO)和有界相对误差(BRE)，其中$\\rho$是解算子范数，对于无陷阱问题$\\rho\\sim k$。根据经验，这些条件在均匀网格的$h$-FEM背景下被认为是最佳的。本文证明了QO和BRE可以通过某些非均匀网格来实现，这些网格违反了上述关于$h$的条件，并且在远离陷阱区域和完美匹配层(PML)中使用更粗的网格。主要定理详细说明了在一个区域中改变网格宽度如何影响该区域和其它区域的误差。一个显著的推论是，对于任何散射问题(有陷阱或无陷阱)，在PML中只需要$hk$足够小；即PML中没有污染。\n分析的动机是，亥姆霍兹数据到解的映射行为因测量和数据的位置而异，特别是取决于穿过这些集合的弹道轨迹(即射线)的性质。因此，子集中有限元空间的近似要求自然应该取决于穿过该集合的弹道射线的性质。将这种行为插入到应用于高频亥姆霍兹方程的FEM的最新对偶性论证中，使我们能够保留关于网格结构和真实解的行为对FEM局部误差影响的详细信息。", "summary": "本文提出了一种基于射线动力学定义的非均匀有限元网格，用于解决高频亥姆霍兹问题。研究表明，即使这些网格违反了传统的均匀网格条件，并在远离陷阱区域和完美匹配层(PML)使用更粗的网格，也能实现k-均匀拟最优性(QO)和有界相对误差(BRE)。论文阐述了网格宽度变化如何影响局部及其他区域的误差，并指出PML中不存在污染问题。该方法将射线行为融入有限元方法的对偶性论证中，从而能更精细地分析网格结构和真实解行为对局部误差的影响。", "keywords": "非均匀网格, 亥姆霍兹方程, 有限元方法, 射线动力学, 完美匹配层", "comments": "本文的创新之处在于提出并验证了基于射线动力学构建的非均匀有限元网格，能够在高频亥姆霍兹问题中实现传统上仅在均匀网格下才能保证的精度条件。这挑战了均匀网格的经验最优性，并为自适应网格细化提供了理论基础。其重要性在于，通过允许在某些区域（如PML）使用更粗的网格，有可能显著提高计算效率，同时保持解的准确性。PML中无污染的发现也具有实际意义。抽象中未明确提及计算成本的实际节省或具体实施细节，主要侧重于理论证明。"}}
{"id": "2506.15380", "title": "Efficient Navigation Among Movable Obstacles using a Mobile Manipulator via Hierarchical Policy Learning", "authors": ["Taegeun Yang", "Jiwoo Hwang", "Jeil Jeong", "Minsung Yoon", "Sung-Eui Yoon"], "summary": "We propose a hierarchical reinforcement learning (HRL) framework for\nefficient Navigation Among Movable Obstacles (NAMO) using a mobile manipulator.\nOur approach combines interaction-based obstacle property estimation with\nstructured pushing strategies, facilitating the dynamic manipulation of\nunforeseen obstacles while adhering to a pre-planned global path. The\nhigh-level policy generates pushing commands that consider environmental\nconstraints and path-tracking objectives, while the low-level policy precisely\nand stably executes these commands through coordinated whole-body movements.\nComprehensive simulation-based experiments demonstrate improvements in\nperforming NAMO tasks, including higher success rates, shortened traversed path\nlength, and reduced goal-reaching times, compared to baselines. Additionally,\nablation studies assess the efficacy of each component, while a qualitative\nanalysis further validates the accuracy and reliability of the real-time\nobstacle property estimation.", "comment": "8 pages, 6 figures, Accepted to IROS 2025. Supplementary Video:\n  https://youtu.be/sZ8_z7sYVP0", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15380v1", "AI": {"title_translation": "移动机械臂通过分层策略学习实现可移动障碍物高效导航", "tldr": "该论文提出了一种分层强化学习框架，用于移动机械臂在可移动障碍物中进行高效导航，结合了障碍物属性估计和结构化推动策略。", "motivation": "为移动机械臂在存在可移动障碍物的复杂环境中提供高效且鲁棒的导航解决方案。", "method": "提出了一种分层强化学习（HRL）框架，用于移动机械臂的可移动障碍物导航（NAMO）。该方法结合了基于交互的障碍物属性估计和结构化推动策略。高层策略生成考虑环境约束和路径跟踪目标的推动指令，低层策略通过协调全身运动精确稳定地执行这些指令。", "result": "与基线相比，在执行NAMO任务时，成功率更高，遍历路径长度更短，达到目标时间更少。消融研究评估了每个组件的功效，定性分析验证了实时障碍物属性估计的准确性和可靠性。", "conclusion": "该研究提出的分层强化学习框架能够显著提升移动机械臂在可移动障碍物环境中的导航效率和鲁棒性，通过结合障碍物属性估计和分层推动策略实现了动态障碍物操作。", "translation": "我们提出了一种分层强化学习（HRL）框架，用于移动机械臂在可移动障碍物（NAMO）中进行高效导航。我们的方法将基于交互的障碍物属性估计与结构化推动策略相结合，促进了对不可预见障碍物的动态操作，同时遵循预先规划的全局路径。高层策略生成考虑环境约束和路径跟踪目标的推动指令，而低层策略通过协调全身运动精确稳定地执行这些指令。全面的基于模拟的实验表明，与基线相比，执行NAMO任务的性能有所改进，包括更高的成功率、更短的遍历路径长度和更少的到达目标时间。此外，消融研究评估了每个组件的功效，而定性分析进一步验证了实时障碍物属性估计的准确性和可靠性。", "summary": "本文提出了一种用于移动机械臂在可移动障碍物中进行高效导航的分层强化学习（HRL）框架。该框架结合了基于交互的障碍物属性估计和结构化推动策略，使得机械臂能够动态操作不可预见的障碍物并遵循预设路径。高层策略负责生成推动指令，低层策略则精确执行这些指令。模拟实验证明，与基线相比，该方法在成功率、路径长度和目标到达时间方面均有显著提升，并且各组件的有效性和障碍物估计的准确性也得到了验证。", "keywords": "分层强化学习, 移动机械臂, 可移动障碍物导航, 障碍物属性估计, 推动策略", "comments": "该论文的创新点在于提出了一个结合障碍物属性估计和分层策略学习的HRL框架，以解决移动机械臂在动态可移动障碍物环境中的导航问题。这种方法提高了导航的效率和鲁棒性，特别是在处理不可预见障碍物方面，为机器人导航领域提供了一种新颖且实用的解决方案。"}}
{"id": "2506.15033", "title": "Break Stylistic Sophon: Are We Really Meant to Confine the Imagination in Style Transfer?", "authors": ["Gary Song Yan", "Yusen Zhang", "Jinyu Zhao", "Hao Zhang", "Zhangping Yang", "Guanye Xiong", "Yanfei Liu", "Tao Zhang", "Yujie He", "Siyuan Tian", "Yao Gou", "Min Li"], "summary": "In this pioneering study, we introduce StyleWallfacer, a groundbreaking\nunified training and inference framework, which not only addresses various\nissues encountered in the style transfer process of traditional methods but\nalso unifies the framework for different tasks. This framework is designed to\nrevolutionize the field by enabling artist level style transfer and text driven\nstylization. First, we propose a semantic-based style injection method that\nuses BLIP to generate text descriptions strictly aligned with the semantics of\nthe style image in CLIP space. By leveraging a large language model to remove\nstyle-related descriptions from these descriptions, we create a semantic gap.\nThis gap is then used to fine-tune the model, enabling efficient and drift-free\ninjection of style knowledge. Second, we propose a data augmentation strategy\nbased on human feedback, incorporating high-quality samples generated early in\nthe fine-tuning process into the training set to facilitate progressive\nlearning and significantly reduce its overfitting. Finally, we design a\ntraining-free triple diffusion process using the fine-tuned model, which\nmanipulates the features of self-attention layers in a manner similar to the\ncross-attention mechanism. Specifically, in the generation process, the key and\nvalue of the content-related process are replaced with those of the\nstyle-related process to inject style while maintaining text control over the\nmodel. We also introduce query preservation to mitigate disruptions to the\noriginal content. Under such a design, we have achieved high-quality\nimage-driven style transfer and text-driven stylization, delivering\nartist-level style transfer results while preserving the original image\ncontent. Moreover, we achieve image color editing during the style transfer\nprocess for the first time.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15033v1", "AI": {"title_translation": "突破风格化智子：我们真的要限制风格迁移中的想象力吗？", "tldr": "本文介绍了StyleWallfacer，一个统一的训练和推理框架，旨在解决传统风格迁移问题并实现艺术家级别和文本驱动的风格化。该框架通过基于语义的风格注入、基于人类反馈的数据增强以及免训练的三重扩散过程，实现了高质量的风格迁移和首次图像色彩编辑。", "motivation": "传统的风格迁移方法存在各种问题，并且缺乏一个统一的框架来处理不同的任务。本文旨在通过实现艺术家级别的风格迁移和文本驱动的风格化来革新该领域，并克服诸如高效、无漂移的风格注入和过拟合等限制，同时保持内容控制。", "method": "本文引入了StyleWallfacer，一个统一的训练和推理框架。它主要包含三个部分：1. 基于语义的风格注入方法：利用BLIP生成风格图像的文本描述，然后通过大型语言模型移除风格相关描述以创建语义鸿沟，并利用此鸿沟微调模型以实现高效、无漂移的风格知识注入。2. 基于人类反馈的数据增强策略：将微调过程早期生成的高质量样本纳入训练集，以促进渐进式学习并显著减少过拟合。3. 免训练的三重扩散过程：使用微调后的模型，通过操纵自注意力层的特征（类似于交叉注意力机制），将内容相关过程的键和值替换为风格相关过程的键和值，从而注入风格同时保持文本控制。此外，还引入了查询保留机制以减轻对原始内容的干扰。", "result": "实现了高质量的图像驱动风格迁移和文本驱动风格化，提供了艺术家级别的风格迁移结果，同时保留了原始图像内容。首次在风格迁移过程中实现了图像色彩编辑。", "conclusion": "所提出的StyleWallfacer框架，凭借其新颖的基于语义的注入、基于人类反馈的数据增强和三重扩散过程，成功统一了不同的风格迁移任务，产生了艺术家级别的结果，保留了内容，并引入了新颖的色彩编辑能力，解决了传统方法的局限性。", "translation": "在这项开创性的研究中，我们引入了StyleWallfacer，一个突破性的统一训练和推理框架，它不仅解决了传统方法在风格迁移过程中遇到的各种问题，而且统一了不同任务的框架。该框架旨在通过实现艺术家级别的风格迁移和文本驱动的风格化来彻底改变该领域。首先，我们提出了一种基于语义的风格注入方法，该方法使用BLIP在CLIP空间中生成与风格图像语义严格对齐的文本描述。通过利用大型语言模型从这些描述中移除与风格相关的描述，我们创建了一个语义鸿沟。然后利用这个鸿沟来微调模型，从而实现高效、无漂移的风格知识注入。其次，我们提出了一种基于人类反馈的数据增强策略，将微调过程早期生成的高质量样本纳入训练集，以促进渐进式学习并显著减少过拟合。最后，我们设计了一个使用微调模型的免训练三重扩散过程，该过程以类似于交叉注意力机制的方式操纵自注意力层的特征。具体而言，在生成过程中，将与内容相关的过程的键和值替换为与风格相关的过程的键和值，以注入风格同时保持对模型的文本控制。我们还引入了查询保留以减轻对原始内容的干扰。在这种设计下，我们实现了高质量的图像驱动风格迁移和文本驱动风格化，提供了艺术家级别的风格迁移结果，同时保留了原始图像内容。此外，我们首次在风格迁移过程中实现了图像色彩编辑。", "summary": "本文引入了StyleWallfacer，一个新颖的统一框架，用于实现艺术家级别和文本驱动的风格迁移。它通过使用BLIP和大型语言模型创建的语义鸿沟进行基于语义的风格注入、基于人类反馈的数据增强策略以防止过拟合，以及一个操纵自注意力特征的免训练三重扩散过程，解决了传统风格迁移的诸多问题。该框架实现了高质量的图像驱动和文本驱动风格化，同时保留了内容，并首次实现了风格迁移过程中的图像色彩编辑。", "keywords": "风格迁移, 扩散模型, 文本驱动风格化, 语义注入, 数据增强", "comments": "这篇论文提出了一种高度创新的风格迁移方法，通过统一不同的任务并解决常见的局限性。利用大型语言模型创建语义鸿沟进行风格注入，以及基于人类反馈的数据增强是特别新颖的。免训练的三重扩散过程也是一个显著的进步，它允许精细控制和内容保留。首次在风格迁移过程中实现图像色彩编辑的能力增加了重要的价值。统一的框架和艺术家级别的结果推动了当前风格迁移能力的边界。"}}
{"id": "2506.14828", "title": "Accurate and Uncertainty-Aware Multi-Task Prediction of HEA Properties Using Prior-Guided Deep Gaussian Processes", "authors": ["Sk Md Ahnaf Akif Alvi", "Mrinalini Mulukutla", "Nicolas Flores", "Danial Khatamsaz", "Jan Janssen", "Danny Perez", "Douglas Allaire", "Vahid Attari", "Raymundo Arroyave"], "summary": "Surrogate modeling techniques have become indispensable in accelerating the\ndiscovery and optimization of high-entropy alloys(HEAs), especially when\nintegrating computational predictions with sparse experimental observations.\nThis study systematically evaluates the fitting performance of four prominent\nsurrogate models conventional Gaussian Processes(cGP), Deep Gaussian\nProcesses(DGP), encoder-decoder neural networks for multi-output regression and\nXGBoost applied to a hybrid dataset of experimental and computational\nproperties in the AlCoCrCuFeMnNiV HEA system. We specifically assess their\ncapabilities in predicting correlated material properties, including yield\nstrength, hardness, modulus, ultimate tensile strength, elongation, and average\nhardness under dynamic and quasi-static conditions, alongside auxiliary\ncomputational properties. The comparison highlights the strengths of\nhierarchical and deep modeling approaches in handling heteroscedastic,\nheterotopic, and incomplete data commonly encountered in materials informatics.\nOur findings illustrate that DGP infused with machine learning-based prior\noutperform other surrogates by effectively capturing inter-property\ncorrelations and input-dependent uncertainty. This enhanced predictive accuracy\npositions advanced surrogate models as powerful tools for robust and\ndata-efficient materials design.", "comment": "Deep Gaussian Processes Multi-task Gaussian Processes High Entropy\n  Alloys", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14828v1", "AI": {"title_translation": "使用先验引导深度高斯过程对高熵合金性能进行准确且不确定性感知的多任务预测", "tldr": "本研究评估了不同代理模型在预测高熵合金（HEA）性能方面的表现，发现结合机器学习先验的深度高斯过程（DGP）在处理复杂材料数据和捕捉不确定性方面表现最佳。", "motivation": "代理建模技术在加速高熵合金（HEAs）的发现和优化中不可或缺，尤其是在整合计算预测与稀疏实验观测时。本研究旨在系统评估不同代理模型在预测相关材料性能方面的能力，以应对材料信息学中常见的异方差、异位和不完整数据。", "method": "本研究系统评估了四种主要的代理模型：传统高斯过程（cGP）、深度高斯过程（DGP）、用于多输出回归的编解码器神经网络和XGBoost。这些模型应用于AlCoCrCuFeMnNiV HEA系统中实验和计算属性的混合数据集，预测屈服强度、硬度、模量、极限拉伸强度、伸长率以及动态和准静态条件下的平均硬度等相关材料属性和辅助计算属性。", "result": "比较结果突出显示了分层和深度建模方法在处理材料信息学中常见的异方差、异位和不完整数据方面的优势。研究发现，注入机器学习先验的DGP优于其他代理模型，因为它能有效捕捉属性间的相关性和输入相关的不确定性。", "conclusion": "增强的预测精度使得先进的代理模型成为强大且数据高效的材料设计的工具。", "translation": "代理建模技术在加速高熵合金（HEAs）的发现和优化中不可或缺，尤其是在整合计算预测与稀疏实验观测时。本研究系统评估了四种主要代理模型——传统高斯过程（cGP）、深度高斯过程（DGP）、用于多输出回归的编解码器神经网络和XGBoost——在AlCoCrCuFeMnNiV HEA系统中实验和计算属性混合数据集上的拟合性能。我们专门评估了它们在预测相关材料属性方面的能力，包括屈服强度、硬度、模量、极限拉伸强度、伸长率以及动态和准静态条件下的平均硬度，以及辅助计算属性。比较结果突出了分层和深度建模方法在处理材料信息学中常见的异方差、异位和不完整数据方面的优势。我们的研究结果表明，注入机器学习先验的DGP通过有效捕捉属性间的相关性和输入相关的不确定性，优于其他代理模型。这种增强的预测精度使得先进的代理模型成为强大且数据高效的材料设计的工具。", "summary": "本研究评估了四种代理模型（cGP、DGP、编解码器神经网络和XGBoost）在预测高熵合金（HEA）性能方面的表现，尤其是在处理混合实验和计算数据时。研究发现，结合机器学习先验的深度高斯过程（DGP）在处理异方差、异位和不完整数据方面表现最佳，能够有效捕捉属性间相关性和输入相关的不确定性，从而提高了预测精度，为材料设计提供了强大的工具。", "keywords": "高熵合金, 代理模型, 深度高斯过程, 多任务预测, 不确定性感知", "comments": "本研究的创新点在于系统比较了多种先进的代理模型，并强调了深度高斯过程（DGP）在处理复杂材料数据中的优越性，尤其是在结合先验知识以提高不确定性感知和预测精度方面。这对于需要处理稀疏、不完整和异构材料数据的材料信息学领域具有重要意义。"}}
{"id": "2506.15301", "title": "Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment", "authors": ["Shrestha Ghosh", "Moritz Schneider", "Carina Reinicke", "Carsten Eickhoff"], "summary": "Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet,\ntheir adoption in critical domains, such as clinical trial recruitment, remains\nlimited. As trials are designed in natural language and patient data is\nrepresented as both structured and unstructured text, the task of matching\ntrials and patients benefits from knowledge aggregation and reasoning abilities\nof LLMs. Classical approaches are trial-specific and LLMs with their ability to\nconsolidate distributed knowledge hold the potential to build a more general\nsolution. Yet recent applications of LLM-assisted methods rely on proprietary\nmodels and weak evaluation benchmarks. In this survey, we are the first to\nanalyze the task of trial-patient matching and contextualize emerging LLM-based\napproaches in clinical trial recruitment. We critically examine existing\nbenchmarks, approaches and evaluation frameworks, the challenges to adopting\nLLM technologies in clinical research and exciting future directions.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15301v1", "AI": {"title_translation": "队列发现：LLM辅助临床试验招募综述", "tldr": "本综述分析了LLM在临床试验招募中匹配试验和患者的应用，并批判性地审视了现有方法、基准和挑战。", "motivation": "尽管LLM在通用NLP任务中取得了巨大进步，但在临床试验招募等关键领域的应用仍有限。由于试验设计和患者数据包含大量自然语言文本，LLM的知识聚合和推理能力对于匹配试验和患者非常有益，且有潜力构建比传统方法更通用的解决方案。", "method": "本文是一项综述，首次分析了试验-患者匹配任务，并将新兴的基于LLM的方法置于临床试验招募的背景下。它批判性地审视了现有基准、方法和评估框架，以及在临床研究中采用LLM技术面临的挑战和未来的发展方向。", "result": "本综述分析了试验-患者匹配任务，并对LLM辅助的临床试验招募方法进行了背景化处理。它批判性地审视了现有基准、方法和评估框架，并指出了在临床研究中采用LLM技术所面临的挑战。", "conclusion": "本综述分析了LLM在临床试验招募中的应用现状，批判性地审视了现有方法和挑战，并展望了该领域激动人心的未来发展方向。", "translation": "LLM的最新进展极大地改善了通用领域的自然语言处理任务。然而，它们在临床试验招募等关键领域的应用仍然有限。由于试验以自然语言设计，患者数据以结构化和非结构化文本形式表示，因此匹配试验和患者的任务受益于LLM的知识聚合和推理能力。经典方法是针对特定试验的，而LLM凭借其整合分布式知识的能力，有潜力构建一个更通用的解决方案。然而，LLM辅助方法的最新应用依赖于专有模型和薄弱的评估基准。在本综述中，我们首次分析了试验-患者匹配任务，并将新兴的基于LLM的方法置于临床试验招募的背景下。我们批判性地审视了现有基准、方法和评估框架、在临床研究中采用LLM技术所面临的挑战以及激动人心的未来发展方向。", "summary": "本综述探讨了大型语言模型（LLM）在临床试验招募中匹配试验与患者的应用潜力。文章指出，尽管LLM在通用NLP任务中表现出色，但在临床试验招募等关键领域应用有限。本研究首次系统性分析了试验-患者匹配任务，并对新兴的LLM辅助方法进行了背景化处理，批判性地审视了现有基准、方法、评估框架、采用挑战以及未来发展方向，旨在推动LLM在该领域的通用解决方案发展。", "keywords": "LLM, 临床试验招募, 队列发现, 试验-患者匹配, 综述", "comments": "本文作为首篇分析LLM辅助临床试验招募的综述，其创新性在于系统性地梳理了该领域的现状、挑战和未来方向，为研究人员提供了宝贵的参考。其重要性体现在突出了LLM在解决传统方法局限性方面的潜力，并指出了当前依赖专有模型和弱评估基准的局限性，为后续研究指明了方向。"}}
{"id": "2506.15660", "title": "On the Upper Bounds for the Matrix Spectral Norm", "authors": ["Alexey Naumov", "Maxim Rakhuba", "Denis Ryapolov", "Sergey Samsonov"], "summary": "We consider the problem of estimating the spectral norm of a matrix using\nonly matrix-vector products. We propose a new Counterbalance estimator that\nprovides upper bounds on the norm and derive probabilistic guarantees on its\nunderestimation. Compared to standard approaches such as the power method, the\nproposed estimator produces significantly tighter upper bounds in both\nsynthetic and real-world settings. Our method is especially effective for\nmatrices with fast-decaying spectra, such as those arising in deep learning and\ninverse problems.", "comment": null, "cate": "math.NA", "url": "http://arxiv.org/abs/2506.15660v1", "AI": {"title_translation": "矩阵谱范数上界的研究", "tldr": "本文提出了一种新的Counterbalance估计器，用于通过矩阵-向量乘积估计矩阵谱范数的上界，并在合成和实际数据上表现出比传统方法更紧密的上界，特别适用于谱衰减快的矩阵。", "motivation": "本文旨在解决在仅使用矩阵-向量乘积的情况下，估计矩阵谱范数上界的问题，以提供比现有标准方法更紧密和准确的估计。", "method": "提出了一种新的“Counterbalance”估计器，该估计器通过矩阵-向量乘积来提供谱范数的上界，并推导了其低估的概率保证。", "result": "与传统的幂法等方法相比，所提出的估计器在合成数据和实际应用中都能产生明显更紧密的上界。该方法特别适用于谱衰减快的矩阵，例如深度学习和逆问题中出现的矩阵。", "conclusion": "Counterbalance估计器是一种有效且高效的工具，可以为矩阵谱范数提供更紧密、更可靠的上界，尤其在处理大型或特定类型的矩阵时表现出色，具有广泛的应用潜力。", "translation": "我们考虑仅使用矩阵-向量乘积来估计矩阵谱范数的问题。我们提出了一种新的Counterbalance估计器，它提供了范数的上界，并推导了其低估的概率保证。与幂法等标准方法相比，所提出的估计器在合成和实际设置中都能产生明显更紧密的上界。我们的方法对谱衰减快的矩阵特别有效，例如深度学习和逆问题中出现的矩阵。", "summary": "本文提出了一种名为Counterbalance的新型估计器，旨在仅通过矩阵-向量乘积来估计矩阵谱范数的上界。该方法提供了范数的上界，并具有概率保证，在合成和实际数据测试中，其上界比传统方法（如幂法）更紧密，尤其适用于深度学习和逆问题等领域中谱衰减快的矩阵。", "keywords": "矩阵谱范数, 上界估计, Counterbalance估计器, 矩阵-向量乘积, 谱衰减", "comments": "这篇论文的创新点在于提出了Counterbalance估计器，它克服了传统方法在仅使用矩阵-向量乘积时估计谱范数上界的局限性。其重要性体现在为处理大型矩阵和特定应用（如深度学习）提供了更高效、更精确的工具，使得在计算资源有限的情况下也能获得高质量的估计。"}}
{"id": "2506.15402", "title": "MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System", "authors": ["Miaoxin Pan", "Jinnan Li", "Yaowen Zhang", "Yi Yang", "Yufeng Yue"], "summary": "Object-level SLAM offers structured and semantically meaningful environment\nrepresentations, making it more interpretable and suitable for high-level\nrobotic tasks. However, most existing approaches rely on RGB-D sensors or\nmonocular views, which suffer from narrow fields of view, occlusion\nsensitivity, and limited depth perception-especially in large-scale or outdoor\nenvironments. These limitations often restrict the system to observing only\npartial views of objects from limited perspectives, leading to inaccurate\nobject modeling and unreliable data association. In this work, we propose\nMCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully\nleverages surround-view camera configurations to achieve robust, consistent,\nand semantically enriched mapping in complex outdoor scenarios. Our approach\nintegrates point features and object-level landmarks enhanced with\nopen-vocabulary semantics. A semantic-geometric-temporal fusion strategy is\nintroduced for robust object association across multiple views, leading to\nimproved consistency and accurate object modeling, and an omnidirectional loop\nclosure module is designed to enable viewpoint-invariant place recognition\nusing scene-level descriptors. Furthermore, the constructed map is abstracted\ninto a hierarchical 3D scene graph to support downstream reasoning tasks.\nExtensive experiments in real-world demonstrate that MCOO-SLAM achieves\naccurate localization and scalable object-level mapping with improved\nrobustness to occlusion, pose variation, and environmental complexity.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15402v1", "AI": {"title_translation": "MCOO-SLAM：一个多相机全向物体SLAM系统", "tldr": "MCOO-SLAM利用多相机全向配置实现鲁棒、一致、语义丰富的物体级SLAM，解决了现有方法在视野、遮挡和深度感知方面的局限性。", "motivation": "现有的物体级SLAM方法多依赖RGB-D传感器或单目视图，受限于视野窄、对遮挡敏感、深度感知有限，尤其在大规模或室外环境中表现不佳，导致物体建模不准确和数据关联不可靠。", "method": "MCOO-SLAM是一个多相机全向物体SLAM系统。它利用环绕视图相机配置，整合点特征和增强了开放词汇语义的物体级地标。引入语义-几何-时间融合策略以实现跨多视图的鲁棒物体关联。设计了全向闭环模块，使用场景级描述符实现视点不变的地点识别。构建的地图被抽象为分层3D场景图以支持下游推理任务。", "result": "真实世界实验表明，MCOO-SLAM实现了准确的定位和可扩展的物体级建图，并提高了对遮挡、姿态变化和环境复杂性的鲁棒性。", "conclusion": "MCOO-SLAM通过其多相机全向设计和创新的数据融合策略，显著提升了物体级SLAM在复杂室外环境中的性能，为高级机器人任务提供了更可靠和语义丰富的环境表示。", "translation": "物体级SLAM提供结构化和语义上有意义的环境表示，使其更具可解释性，并适用于高级机器人任务。然而，大多数现有方法依赖于RGB-D传感器或单目视图，这些方法存在视野狭窄、对遮挡敏感和深度感知有限的问题——尤其是在大规模或室外环境中。这些局限性通常限制系统只能从有限的视角观察物体部分视图，导致物体建模不准确和数据关联不可靠。在这项工作中，我们提出了MCOO-SLAM，一个新颖的多相机全向物体SLAM系统，它充分利用环绕视图相机配置，在复杂的室外场景中实现鲁棒、一致且语义丰富的建图。我们的方法整合了点特征和通过开放词汇语义增强的物体级地标。引入了一种语义-几何-时间融合策略，用于跨多个视图进行鲁棒的物体关联，从而提高了一致性和准确的物体建模，并且设计了一个全向闭环模块，以使用场景级描述符实现视点不变的地点识别。此外，构建的地图被抽象为分层3D场景图，以支持下游推理任务。在真实世界的广泛实验表明，MCOO-SLAM实现了准确的定位和可扩展的物体级建图，并提高了对遮挡、姿态变化和环境复杂性的鲁棒性。", "summary": "MCOO-SLAM是一个创新的多相机全向物体SLAM系统，旨在克服现有物体级SLAM在视野、遮挡和深度感知上的局限性。它通过整合多相机环绕视图、开放词汇语义增强的物体地标、语义-几何-时间融合策略以及全向闭环模块，实现了在复杂室外环境中鲁棒、一致且语义丰富的物体级建图，并能构建分层3D场景图以支持高级推理任务。实验证明其在定位准确性和物体级建图的可扩展性方面表现出色，且对遮挡、姿态变化和环境复杂性具有更强的鲁棒性。", "keywords": "物体级SLAM, 多相机, 全向, 语义建图, 场景图", "comments": "该论文提出了一种创新的多相机全向物体SLAM系统，解决了现有方法在视野受限和遮挡敏感性方面的关键痛点。其核心创新点在于充分利用多相机环绕视图配置，结合开放词汇语义、多模态融合和全向闭环，显著提升了复杂室外环境下物体级SLAM的鲁棒性和精度。构建分层3D场景图也为后续高级推理任务提供了良好的基础，具有重要的应用潜力。"}}
{"id": "2506.15078", "title": "Enhancing Vector Quantization with Distributional Matching: A Theoretical and Empirical Study", "authors": ["Xianghong Fang", "Litao Guo", "Hengchao Chen", "Yuxuan Zhang", "XiaofanXia", "Dingjie Song", "Yexin Liu", "Hao Wang", "Harry Yang", "Yuan Yuan", "Qiang Sun"], "summary": "The success of autoregressive models largely depends on the effectiveness of\nvector quantization, a technique that discretizes continuous features by\nmapping them to the nearest code vectors within a learnable codebook. Two\ncritical issues in existing vector quantization methods are training\ninstability and codebook collapse. Training instability arises from the\ngradient discrepancy introduced by the straight-through estimator, especially\nin the presence of significant quantization errors, while codebook collapse\noccurs when only a small subset of code vectors are utilized during training. A\ncloser examination of these issues reveals that they are primarily driven by a\nmismatch between the distributions of the features and code vectors, leading to\nunrepresentative code vectors and significant data information loss during\ncompression. To address this, we employ the Wasserstein distance to align these\ntwo distributions, achieving near 100\\% codebook utilization and significantly\nreducing the quantization error. Both empirical and theoretical analyses\nvalidate the effectiveness of the proposed approach.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15078v1", "AI": {"title_translation": "通过分布匹配增强矢量量化：一项理论与实证研究", "tldr": "矢量量化中存在的训练不稳定性和码本崩溃问题源于特征与码向量的分布不匹配。本研究提出使用Wasserstein距离对齐这些分布，从而显著提高码本利用率并降低量化误差。", "motivation": "现有矢量量化方法存在训练不稳定性和码本崩溃问题。这些问题是由于特征和码向量之间的分布不匹配造成的，导致码向量缺乏代表性并造成数据信息丢失。", "method": "本研究采用Wasserstein距离来对齐特征和码向量的分布。", "result": "所提出的方法实现了接近100%的码本利用率，并显著降低了量化误差。实证和理论分析均验证了其有效性。", "conclusion": "通过使用Wasserstein距离进行分布匹配，本方法有效解决了矢量量化中的训练不稳定性和码本崩溃问题，提升了矢量量化的性能。", "translation": "自回归模型的成功在很大程度上取决于矢量量化的有效性，这是一种通过将连续特征映射到可学习码本中最近的码向量来离散化连续特征的技术。现有矢量量化方法的两个关键问题是训练不稳定性和码本崩溃。训练不稳定性源于直通估计器引入的梯度差异，尤其是在存在显著量化误差的情况下；而码本崩溃则发生在训练期间只使用一小部分码向量时。对这些问题的深入研究表明，它们主要是由特征和码向量之间分布不匹配引起的，导致码向量缺乏代表性并在压缩过程中造成显著数据信息丢失。为了解决这个问题，我们采用Wasserstein距离来对齐这两个分布，实现了接近100%的码本利用率，并显著降低了量化误差。实证和理论分析都验证了所提出方法的有效性。", "summary": "本论文旨在解决自回归模型中矢量量化（VQ）的常见问题，即训练不稳定性和码本崩溃。研究指出这些问题源于特征与码向量分布之间的不匹配。为解决此问题，作者提出使用Wasserstein距离来对齐这些分布。实验和理论分析均表明，该方法能够实现接近100%的码本利用率并显著减少量化误差，从而有效提升了矢量量化的性能。", "keywords": "矢量量化, 分布匹配, Wasserstein距离, 码本崩溃, 训练不稳定性", "comments": "本论文通过识别分布不匹配作为矢量量化问题的根本原因，提出了创新的解决方案。采用Wasserstein距离进行分布对齐，提供了一种理论上严谨的方法来提高VQ的稳定性和效率。实现“接近100%的码本利用率”是一个重要的实证成果，表明了该方法的实际应用潜力。"}}
{"id": "2506.15304", "title": "ConLID: Supervised Contrastive Learning for Low-Resource Language Identification", "authors": ["Negar Foroutan", "Jakhongir Saydaliev", "Ye Eun Kim", "Antoine Bosselut"], "summary": "Language identification (LID) is a critical step in curating multilingual LLM\npretraining corpora from web crawls. While many studies on LID model training\nfocus on collecting diverse training data to improve performance, low-resource\nlanguages -- often limited to single-domain data, such as the Bible -- continue\nto perform poorly. To resolve these class imbalance and bias issues, we propose\na novel supervised contrastive learning (SCL) approach to learn\ndomain-invariant representations for low-resource languages. Through an\nextensive analysis, we show that our approach improves LID performance on\nout-of-domain data for low-resource languages by 3.2%, demonstrating its\neffectiveness in enhancing LID models.", "comment": "Submitted to EMNLP", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15304v1", "AI": {"title_translation": "ConLID：低资源语言识别的监督对比学习", "tldr": "ConLID提出了一种监督对比学习方法，用于解决低资源语言识别中存在的类别不平衡和偏差问题，显著提高了域外数据的识别性能。", "motivation": "语言识别（LID）是整理多语言大型语言模型预训练语料库的关键步骤。然而，现有研究主要关注收集多样化的训练数据以提高性能，而低资源语言（通常仅限于单一领域数据，如圣经）的性能仍然不佳。为了解决这些类别不平衡和偏差问题，本研究提出了新的方法。", "method": "我们提出了一种新颖的监督对比学习（SCL）方法来学习低资源语言的领域不变表示。", "result": "通过广泛的分析，我们表明我们的方法将低资源语言在域外数据上的LID性能提高了3.2%。", "conclusion": "我们的方法有效增强了LID模型，特别是在提高低资源语言的域外数据识别性能方面。", "translation": "语言识别（LID）是整理来自网络抓取的多语言大型语言模型预训练语料库的关键步骤。虽然许多关于LID模型训练的研究侧重于收集多样化的训练数据以提高性能，但低资源语言——通常仅限于单一领域数据，例如《圣经》——的性能持续不佳。为了解决这些类别不平衡和偏差问题，我们提出了一种新颖的监督对比学习（SCL）方法，以学习低资源语言的领域不变表示。通过广泛的分析，我们表明我们的方法将低资源语言在域外数据上的LID性能提高了3.2%，证明了其在增强LID模型方面的有效性。", "summary": "ConLID提出了一种监督对比学习（SCL）方法，旨在解决低资源语言识别（LID）中存在的类别不平衡和领域偏差问题。该方法通过学习领域不变表示，显著提高了低资源语言在域外数据上的LID性能，提升了3.2%，证明了其在增强LID模型方面的有效性。", "keywords": "语言识别, 监督对比学习, 低资源语言, 领域不变表示, ConLID", "comments": "本文提出了一种新颖的监督对比学习方法，有效解决了低资源语言识别中常见的域外性能下降问题，其创新点在于通过学习领域不变表示来缓解数据稀缺和偏差带来的影响。这一方法对于构建高质量的多语言LLM语料库具有重要意义。"}}
{"id": "2506.14822", "title": "Analysis and conditional optimization of projection estimates for the distribution of random variable using Legendre polynomials", "authors": ["Tatyana A. Averina", "Konstantin A. Rybakov"], "summary": "Algorithms for jointly obtaining projection estimates of the density and\ndistribution function of a random variable using the Legendre polynomials are\nproposed. For these algorithms, a problem of the conditional optimization is\nsolved. Such an optimization allows one increasing the approximation accuracy\nwith a minimum computational costs. The proposed algorithms are tested on\nexamples with different degree of smoothness of the density.", "comment": null, "cate": "stat.CO", "url": "http://arxiv.org/abs/2506.14822v1", "AI": {"title_translation": "使用勒让德多项式对随机变量分布投影估计的分析与条件优化", "tldr": "本文提出了使用勒让德多项式联合估计随机变量密度和分布函数的投影算法，并通过条件优化提高了精度并降低了计算成本。", "motivation": "为了在最小计算成本下提高随机变量密度和分布函数投影估计的近似精度。", "method": "提出了利用勒让德多项式联合获取随机变量密度和分布函数投影估计的算法，并解决了条件优化问题。", "result": "所提出的算法在不同密度平滑度的示例上进行了测试。", "conclusion": "通过条件优化，利用勒让德多项式实现的随机变量密度和分布函数的投影估计算法能够有效提高近似精度，并保持较低的计算成本。", "translation": "本文提出了使用勒让德多项式联合获取随机变量密度和分布函数的投影估计算法。针对这些算法，解决了条件优化问题。这种优化能够在最小计算成本下提高近似精度。所提出的算法在不同密度平滑度的示例上进行了测试。", "summary": "本文提出了一种利用勒让德多项式对随机变量的密度和分布函数进行联合投影估计的算法。该算法通过解决一个条件优化问题，实现了在最小计算成本下提高近似精度的目标。所提出的算法已在不同平滑度密度的示例上进行了有效性测试。", "keywords": "勒让德多项式, 投影估计, 条件优化, 随机变量分布, 密度估计", "comments": "该论文的创新点在于提出了联合估计随机变量密度和分布函数的方法，并通过条件优化显著提高了估计精度，同时有效控制了计算成本。这种方法对于需要高精度和高效率的统计估计任务具有重要意义。"}}
{"id": "2506.15295", "title": "A theory of Lending Protocols in DeFi", "authors": ["Massimo Bartoletti", "Enrico Lipparini"], "summary": "Lending protocols are one of the main applications of Decentralized Finance\n(DeFi), enabling crypto-assets loan markets with a total value estimated in the\ntens of billions of dollars. Unlike traditional lending systems, these\nprotocols operate without relying on trusted authorities or off-chain\nenforcement mechanisms. To achieve key economic goals such as stability of the\nloan market, they devise instead trustless on-chain mechanisms, such as\nrewarding liquidators who repay the loans of under-collateralized borrowers by\nawarding them part of the borrower's collateral. The complexity of these\nincentive mechanisms, combined with their entanglement in low-level\nimplementation details, makes it challenging to precisely assess the structural\nand economic properties of lending protocols, as well as to analyze user\nstrategies and attacks. Crucially, since participation is open to anyone, any\nweaknesses in the incentive mechanism may give rise to unintended emergent\nbehaviours, or even enable adversarial strategies aimed at making profits to\nthe detriment of legit users, or at undermining the stability of the protocol.\nIn this work, we propose a formal model of lending protocols that captures the\nessential features of mainstream platforms, enabling us to identify and prove\nkey properties related to their economic and strategic dynamics.", "comment": null, "cate": "cs.GT", "url": "http://arxiv.org/abs/2506.15295v1", "AI": {"title_translation": "DeFi中借贷协议的理论", "tldr": "本文提出了一个借贷协议的形式化模型，以分析DeFi中借贷协议的经济和策略动态，解决现有机制复杂性带来的评估挑战和潜在攻击风险。", "motivation": "去中心化金融（DeFi）中的借贷协议因其复杂激励机制和低级实现细节的纠缠，难以精确评估其结构和经济特性，也难以分析用户策略和攻击。此外，开放参与可能导致意外行为或恶意策略，损害合法用户或破坏协议稳定性。", "method": "本文提出了一个借贷协议的形式化模型，该模型捕获了主流平台的关键特征。", "result": "该模型能够识别并证明与借贷协议的经济和策略动态相关的关键属性。", "conclusion": "Not mentioned in abstract", "translation": "借贷协议是去中心化金融（DeFi）的主要应用之一，它促成了加密资产贷款市场，总价值估计达数百亿美元。与传统借贷系统不同，这些协议的运作不依赖于受信任的权威机构或链下执行机制。为了实现贷款市场稳定性等关键经济目标，它们转而设计了无需信任的链上机制，例如通过奖励清算人（他们通过获得借款人部分抵押品的方式偿还抵押不足借款人的贷款）来激励其行为。这些激励机制的复杂性，加上它们与底层实现细节的纠缠，使得精确评估借贷协议的结构和经济属性，以及分析用户策略和攻击变得具有挑战性。至关重要的是，由于任何人都可以参与，激励机制中的任何弱点都可能导致意想不到的涌现行为，甚至可能促成旨在损害合法用户利益或破坏协议稳定性的对抗性策略。在这项工作中，我们提出了一个借贷协议的形式化模型，该模型捕获了主流平台的本质特征，使我们能够识别并证明与其经济和策略动态相关的关键属性。", "summary": "本文针对去中心化金融（DeFi）借贷协议的复杂性和潜在风险，提出了一个形式化模型。该模型旨在克服现有协议因激励机制复杂和实现细节纠缠而导致的评估困难，并分析用户策略及潜在攻击。通过捕获主流平台的核心特征，该模型能够识别并验证这些协议的经济和策略动态中的关键属性，从而有助于理解并可能增强其稳定性和安全性。", "keywords": "DeFi, 借贷协议, 形式化模型, 经济动态, 策略分析", "comments": "该论文通过提出一个形式化模型，为DeFi借贷协议的复杂性提供了理论分析框架。其创新点在于将激励机制和潜在攻击行为纳入模型考量，对于理解DeFi生态系统的经济和策略动态具有重要意义。这有助于提高协议的透明度和安全性，并为未来的协议设计提供理论基础。"}}
{"id": "2506.15450", "title": "SurfAAV: Design and Implementation of a Novel Multimodal Surfing Aquatic-Aerial Vehicle", "authors": ["Kun Liu", "Junhao Xiao", "Hao Lin", "Yue Cao", "Hui Peng", "Kaihong Huang", "Huimin Lu"], "summary": "Despite significant advancements in the research of aquatic-aerial robots,\nexisting configurations struggle to efficiently perform underwater, surface,\nand aerial movement simultaneously. In this paper, we propose a novel\nmultimodal surfing aquatic-aerial vehicle, SurfAAV, which efficiently\nintegrates underwater navigation, surface gliding, and aerial flying\ncapabilities. Thanks to the design of the novel differential thrust vectoring\nhydrofoil, SurfAAV can achieve efficient surface gliding and underwater\nnavigation without the need for a buoyancy adjustment system. This design\nprovides flexible operational capabilities for both surface and underwater\ntasks, enabling the robot to quickly carry out underwater monitoring\nactivities. Additionally, when it is necessary to reach another water body,\nSurfAAV can switch to aerial mode through a gliding takeoff, flying to the\ntarget water area to perform corresponding tasks. The main contribution of this\nletter lies in proposing a new solution for underwater, surface, and aerial\nmovement, designing a novel hybrid prototype concept, developing the required\ncontrol laws, and validating the robot's ability to successfully perform\nsurface gliding and gliding takeoff. SurfAAV achieves a maximum surface gliding\nspeed of 7.96 m/s and a maximum underwater speed of 3.1 m/s. The prototype's\nsurface gliding maneuverability and underwater cruising maneuverability both\nexceed those of existing aquatic-aerial vehicles.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15450v1", "AI": {"title_translation": "SurfAAV：一种新型多模态水空两栖冲浪飞行器的设计与实现", "tldr": "SurfAAV是一种新型多模态水空两栖机器人，通过差分推力矢量水翼设计，实现了高效的水下导航、水面滑翔和空中飞行，解决了现有水空机器人难以同时高效执行多种运动模式的问题，并展现出优越的性能。", "motivation": "尽管水空机器人研究取得了显著进展，但现有配置难以同时高效地执行水下、水面和空中运动。", "method": "本文提出了一种新型多模态水空两栖飞行器SurfAAV，通过创新的差分推力矢量水翼设计，实现了无需浮力调节系统的水面滑翔和水下导航。此外，SurfAAV还能通过滑翔起飞切换到空中模式。研究还开发了所需的控制律，并验证了机器人成功执行水面滑翔和滑翔起飞的能力。", "result": "SurfAAV实现了最大水面滑翔速度7.96米/秒和最大水下速度3.1米/秒。该原型机的水面滑翔机动性和水下巡航机动性均超越了现有水空两栖飞行器。", "conclusion": "本文为水下、水面和空中运动提出了一种新颖的解决方案，设计了一个新型混合原型概念，开发了所需的控制律，并验证了机器人成功执行水面滑翔和滑翔起飞的能力，其性能优于现有水空两栖飞行器。", "translation": "尽管水空机器人研究取得了显著进展，但现有配置难以同时高效地执行水下、水面和空中运动。在本文中，我们提出了一种新型多模态水空两栖飞行器SurfAAV，它高效地整合了水下导航、水面滑翔和空中飞行能力。得益于新型差分推力矢量水翼的设计，SurfAAV无需浮力调节系统即可实现高效的水面滑翔和水下导航。这种设计为水面和水下任务提供了灵活的操作能力，使机器人能够快速开展水下监测活动。此外，当需要到达另一个水域时，SurfAAV可以通过滑翔起飞切换到空中模式，飞往目标水域执行相应任务。本文的主要贡献在于为水下、水面和空中运动提出了一种新的解决方案，设计了一个新型混合原型概念，开发了所需的控制律，并验证了机器人成功执行水面滑翔和滑翔起飞的能力。SurfAAV实现了最大水面滑翔速度7.96米/秒和最大水下速度3.1米/秒。该原型机的水面滑翔机动性和水下巡航机动性均超越了现有水空两栖飞行器。", "summary": "本文介绍了一种名为SurfAAV的新型多模态水空两栖飞行器，旨在解决现有水空机器人难以同时高效执行水下、水面和空中运动的问题。SurfAAV通过独特的差分推力矢量水翼设计，无需浮力调节即可实现高效的水面滑翔和水下导航，并能通过滑翔起飞切换至空中模式。实验验证了其水面滑翔和滑翔起飞能力，并展示了其在水面和水下速度及机动性方面优于现有水空两栖飞行器的性能。", "keywords": "水空两栖机器人, 多模态, 水面滑翔, 水下导航, 差分推力矢量水翼", "comments": "SurfAAV的创新之处在于其差分推力矢量水翼设计，它巧妙地解决了水空两栖机器人长期存在的浮力调节复杂性问题，并实现了多模态的高效切换。这种设计为水下监测和跨水域任务提供了新的可能性，具有重要的实际应用潜力。其在速度和机动性上的提升也表明了该原型概念的优越性。"}}
{"id": "2506.15153", "title": "SynPo: Boosting Training-Free Few-Shot Medical Segmentation via High-Quality Negative Prompts", "authors": ["Yufei Liu", "Haoke Xiao", "Jiaxing Chai", "Yongcun Zhang", "Rong Wang", "Zijie Meng", "Zhiming Luo"], "summary": "The advent of Large Vision Models (LVMs) offers new opportunities for\nfew-shot medical image segmentation. However, existing training-free methods\nbased on LVMs fail to effectively utilize negative prompts, leading to poor\nperformance on low-contrast medical images. To address this issue, we propose\nSynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core\ninsight: improving the quality of negative prompts. To select point prompts in\na more reliable confidence map, we design a novel Confidence Map Synergy Module\nby combining the strengths of DINOv2 and SAM. Based on the confidence map, we\nselect the top-k pixels as the positive points set and choose the negative\npoints set using a Gaussian distribution, followed by independent K-means\nclustering for both sets. Then, these selected points are leveraged as\nhigh-quality prompts for SAM to get the segmentation results. Extensive\nexperiments demonstrate that SynPo achieves performance comparable to\nstate-of-the-art training-based few-shot methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15153v1", "AI": {"title_translation": "SynPo：通过高质量负提示提升免训练少样本医学图像分割", "tldr": "SynPo是一种免训练的少样本医学图像分割方法，通过生成高质量的负提示来解决现有方法在低对比度图像上的性能问题，其性能可与最先进的基于训练的方法相媲美。", "motivation": "现有的基于大型视觉模型（LVMs）的免训练少样本医学图像分割方法未能有效利用负提示，导致在低对比度医学图像上性能不佳。", "method": "本文提出了SynPo，一种基于LVMs（如SAM）的免训练少样本方法。其核心思想是提升负提示的质量。通过设计一个结合DINOv2和SAM优势的置信度图协同模块，以获得更可靠的置信度图。基于该置信度图，选择前k个像素作为正点集，并使用高斯分布选择负点集，随后对两组点进行独立的K均值聚类。最后，这些高质量点被用作SAM的提示以获得分割结果。", "result": "SynPo的性能与最先进的基于训练的少样本方法相当。", "conclusion": "SynPo通过提高负提示的质量，成功解决了现有免训练少样本医学图像分割方法在低对比度图像上性能不佳的问题，并实现了与最先进的基于训练的方法相当的性能。", "translation": "大型视觉模型（LVMs）的出现为少样本医学图像分割提供了新机遇。然而，现有的基于LVMs的免训练方法未能有效利用负提示，导致在低对比度医学图像上性能不佳。为了解决这个问题，我们提出了SynPo，一种基于LVMs（例如SAM）的免训练少样本方法，其核心思想是：提高负提示的质量。为了在更可靠的置信度图中选择点提示，我们设计了一个新颖的置信度图协同模块，结合了DINOv2和SAM的优势。基于置信度图，我们选择前k个像素作为正点集，并使用高斯分布选择负点集，随后对这两个集合进行独立的K均值聚类。然后，这些选定的点被用作SAM的高质量提示，以获得分割结果。大量的实验表明，SynPo实现了与最先进的基于训练的少样本方法相当的性能。", "summary": "SynPo是一种新颖的免训练少样本医学图像分割方法，旨在通过生成高质量的负提示来解决现有基于大型视觉模型的免训练方法在低对比度图像上性能不佳的问题。该方法引入了一个结合DINOv2和SAM的置信度图协同模块，以可靠地选择正负点提示，并通过聚类优化，最终将这些高质量提示输入SAM进行分割。实验证明，SynPo的性能与最先进的基于训练的方法相当。", "keywords": "少样本分割, 医学图像, 负提示, 免训练, 大型视觉模型", "comments": "SynPo的创新点在于其免训练的特性以及对负提示质量的提升，这对于处理低对比度医学图像尤其重要。通过结合DINOv2和SAM的优势来构建置信度图，并采用精细的点选择和聚类策略，该方法有效地弥补了现有免训练方法的不足，并达到了与训练方法相当的性能，这在实际应用中具有显著的优势。"}}
{"id": "2506.14796", "title": "PFMBench: Protein Foundation Model Benchmark", "authors": ["Zhangyang Gao", "Hao Wang", "Cheng Tan", "Chenrui Xu", "Mengdi Liu", "Bozhen Hu", "Linlin Chao", "Xiaoming Zhang", "Stan Z. Li"], "summary": "This study investigates the current landscape and future directions of\nprotein foundation model research. While recent advancements have transformed\nprotein science and engineering, the field lacks a comprehensive benchmark for\nfair evaluation and in-depth understanding. Since ESM-1B, numerous protein\nfoundation models have emerged, each with unique datasets and methodologies.\nHowever, evaluations often focus on limited tasks tailored to specific models,\nhindering insights into broader generalization and limitations. Specifically,\nresearchers struggle to understand the relationships between tasks, assess how\nwell current models perform across them, and determine the criteria in\ndeveloping new foundation models. To fill this gap, we present PFMBench, a\ncomprehensive benchmark evaluating protein foundation models across 38 tasks\nspanning 8 key areas of protein science. Through hundreds of experiments on 17\nstate-of-the-art models across 38 tasks, PFMBench reveals the inherent\ncorrelations between tasks, identifies top-performing models, and provides a\nstreamlined evaluation protocol. Code is available at\n\\href{https://github.com/biomap-research/PFMBench}{\\textcolor{blue}{GitHub}}.", "comment": null, "cate": "q-bio.BM", "url": "http://arxiv.org/abs/2506.14796v1", "AI": {"title_translation": "PFMBench：蛋白质基础模型基准测试", "tldr": "PFMBench是一个全面的蛋白质基础模型基准测试，包含38个任务和17个模型，旨在解决当前评估不足的问题并揭示任务相关性。", "motivation": "当前蛋白质基础模型缺乏一个全面的基准测试平台，导致难以进行公平评估、深入理解模型、揭示泛化能力和局限性，以及确定开发新基础模型的标准。", "method": "本研究提出了PFMBench，这是一个在蛋白质科学8个关键领域的38个任务上评估蛋白质基础模型的综合基准。通过在17个最先进的模型上进行数百次实验，PFMBench旨在提供一个简化的评估协议。", "result": "PFMBench揭示了任务之间固有的相关性，识别出表现最佳的模型，并提供了一个简化的评估协议。", "conclusion": "PFMBench通过提供一个全面的基准测试框架，显著提升了蛋白质基础模型的评估和理解，有助于未来模型的发展。", "translation": "本研究调查了蛋白质基础模型研究的当前格局和未来方向。尽管最近的进展已经改变了蛋白质科学和工程，但该领域缺乏一个全面的基准测试平台，以进行公平评估和深入理解。自ESM-1B以来，许多蛋白质基础模型相继出现，每个模型都有独特的数据集和方法。然而，评估通常侧重于为特定模型量身定制的有限任务，阻碍了对更广泛的泛化能力和局限性的深入了解。具体而言，研究人员难以理解任务之间的关系，评估当前模型在这些任务上的表现如何，并确定开发新基础模型的标准。为了弥补这一空白，我们提出了PFMBench，这是一个在蛋白质科学8个关键领域的38个任务上评估蛋白质基础模型的综合基准。通过在17个最先进的模型上进行数百次实验，PFMBench揭示了任务之间固有的相关性，识别出表现最佳的模型，并提供了一个简化的评估协议。代码可在GitHub上获取。", "summary": "本研究介绍了PFMBench，一个旨在解决蛋白质基础模型缺乏全面评估基准问题的综合平台。PFMBench涵盖蛋白质科学8个关键领域的38个任务，并对17个主流模型进行了数百次实验。它成功揭示了任务间的相关性，识别出高性能模型，并提供了一套简化的评估协议，为蛋白质基础模型的公平评估和未来发展提供了重要工具。", "keywords": "蛋白质基础模型, 基准测试, PFMBench, 模型评估, 蛋白质科学", "comments": "PFMBench的创新之处在于其前所未有的广度和深度，它不仅整合了大量评估任务和模型，还揭示了任务间的内在关联性，这对于推动蛋白质基础模型的公平评估和指导未来研究方向具有重要意义。它填补了领域内长期存在的空白，有望成为蛋白质基础模型研究的标准工具。"}}
{"id": "2506.14843", "title": "CACTUS as a Reliable Tool for Early Classification of Age-related Macular Degeneration", "authors": ["Luca Gherardini", "Imre Lengyel", "Tunde Peto", "Caroline C. W. Klaverd", "Magda A. Meester-Smoord", "Johanna Maria Colijnd", "EYE-RISK Consortium", "E3 Consortium", "Jose Sousa"], "summary": "Machine Learning (ML) is used to tackle various tasks, such as disease\nclassification and prediction. The effectiveness of ML models relies heavily on\nhaving large amounts of complete data. However, healthcare data is often\nlimited or incomplete, which can hinder model performance. Additionally, issues\nlike the trustworthiness of solutions vary with the datasets used. The lack of\ntransparency in some ML models further complicates their understanding and use.\nIn healthcare, particularly in the case of Age-related Macular Degeneration\n(AMD), which affects millions of older adults, early diagnosis is crucial due\nto the absence of effective treatments for reversing progression. Diagnosing\nAMD involves assessing retinal images along with patients' symptom reports.\nThere is a need for classification approaches that consider genetic, dietary,\nclinical, and demographic factors. Recently, we introduced the -Comprehensive\nAbstraction and Classification Tool for Uncovering Structures-(CACTUS), aimed\nat improving AMD stage classification. CACTUS offers explainability and\nflexibility, outperforming standard ML models. It enhances decision-making by\nidentifying key factors and providing confidence in its results. The important\nfeatures identified by CACTUS allow us to compare with existing medical\nknowledge. By eliminating less relevant or biased data, we created a clinical\nscenario for clinicians to offer feedback and address biases.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14843v1", "AI": {"title_translation": "CACTUS作为一种可靠的工具用于年龄相关性黄斑变性的早期分类", "tldr": "CACTUS是一种可解释、灵活的机器学习工具，旨在改善年龄相关性黄斑变性（AMD）的早期分类，它通过识别关键特征并消除不相关数据，在有限或不完整医疗数据下优于标准ML模型。", "motivation": "机器学习模型在医疗领域（如疾病分类和预测）中面临数据量有限、不完整以及模型缺乏透明度的问题。特别是对于年龄相关性黄斑变性（AMD），早期诊断至关重要，但现有诊断方法需要综合多种因素。因此，需要一种能够处理不完整数据、提供可解释性并有效分类AMD的工具。", "method": "本文介绍了一种名为CACTUS（-Comprehensive Abstraction and Classification Tool for Uncovering Structures-）的工具，旨在改善AMD阶段分类。CACTUS通过提供可解释性和灵活性，识别关键特征，并消除不相关或有偏差的数据来增强决策制定，从而优于标准机器学习模型。", "result": "CACTUS在AMD阶段分类方面优于标准机器学习模型。它能识别出重要的特征，这些特征可以与现有医学知识进行比较，并帮助创建临床场景以获取反馈和解决偏差。", "conclusion": "CACTUS被证明是一种可靠的工具，能够有效地进行年龄相关性黄斑变性的早期分类。它不仅提供了优于传统ML模型的性能，还具备可解释性和灵活性，能够识别关键因素并提供结果置信度。", "translation": "机器学习（ML）用于解决各种任务，例如疾病分类和预测。ML模型的有效性在很大程度上依赖于拥有大量的完整数据。然而，医疗保健数据通常是有限或不完整的，这会阻碍模型性能。此外，解决方案的可靠性等问题因所使用的数据集而异。一些ML模型缺乏透明度进一步使其理解和使用复杂化。在医疗保健领域，特别是在影响数百万老年人的年龄相关性黄斑变性（AMD）案例中，由于缺乏逆转疾病进展的有效治疗方法，早期诊断至关重要。诊断AMD涉及评估视网膜图像以及患者的症状报告。需要考虑遗传、饮食、临床和人口统计学因素的分类方法。最近，我们引入了-综合抽象和分类工具以揭示结构-（CACTUS），旨在改善AMD阶段分类。CACTUS提供了可解释性和灵活性，优于标准ML模型。它通过识别关键因素并提供结果置信度来增强决策制定。CACTUS识别出的重要特征使我们能够与现有医学知识进行比较。通过消除不相关或有偏差的数据，我们为临床医生创建了一个临床场景，以便他们提供反馈并解决偏差。", "summary": "本文介绍了一种名为CACTUS（综合抽象和分类工具以揭示结构）的新型机器学习工具，旨在解决医疗数据有限、不完整及模型透明度不足的问题，特别是针对年龄相关性黄斑变性（AMD）的早期分类。CACTUS通过提供可解释性和灵活性，识别关键特征，并有效处理不相关数据，性能优于标准ML模型，从而提升了AMD诊断的决策质量。", "keywords": "年龄相关性黄斑变性, 机器学习, 早期分类, CACTUS, 可解释性", "comments": "CACTUS的创新之处在于其对医疗领域特有数据挑战的应对，特别是有限和不完整数据。其可解释性和识别关键特征的能力对于临床应用至关重要，因为这能增加医生对模型决策的信任。该工具在AMD早期分类上的应用具有重要意义，因为它能帮助克服现有治疗手段的局限性。未来可以进一步探讨其在其他医疗领域的普适性和鲁棒性。"}}
{"id": "2506.15339", "title": "DeVisE: Behavioral Testing of Medical Large Language Models", "authors": ["Camila Zurdo Tagliabue", "Heloisa Oss Boll", "Aykut Erdem", "Erkut Erdem", "Iacer Calixto"], "summary": "Large language models (LLMs) are increasingly used in clinical decision\nsupport, yet current evaluation methods often fail to distinguish genuine\nmedical reasoning from superficial patterns. We introduce DeVisE (Demographics\nand Vital signs Evaluation), a behavioral testing framework for probing\nfine-grained clinical understanding. We construct a dataset of ICU discharge\nnotes from MIMIC-IV, generating both raw (real-world) and template-based\n(synthetic) versions with controlled single-variable counterfactuals targeting\ndemographic (age, gender, ethnicity) and vital sign attributes. We evaluate\nfive LLMs spanning general-purpose and medically fine-tuned variants, under\nboth zero-shot and fine-tuned settings. We assess model behavior via (1)\ninput-level sensitivity - how counterfactuals alter the likelihood of a note;\nand (2) downstream reasoning - how they affect predicted hospital\nlength-of-stay. Our results show that zero-shot models exhibit more coherent\ncounterfactual reasoning patterns, while fine-tuned models tend to be more\nstable yet less responsive to clinically meaningful changes. Notably,\ndemographic factors subtly but consistently influence outputs, emphasizing the\nimportance of fairness-aware evaluation. This work highlights the utility of\nbehavioral testing in exposing the reasoning strategies of clinical LLMs and\ninforming the design of safer, more transparent medical AI systems.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15339v1", "AI": {"title_translation": "DeVisE：医用大型语言模型的行为测试", "tldr": "DeVisE框架通过行为测试评估医学LLM的临床理解，发现零样本模型推理更连贯，而微调模型更稳定但对临床变化不敏感，并强调了公平性评估的重要性。", "motivation": "当前的评估方法常常无法区分医用大型语言模型（LLMs）真正的医学推理与肤浅的模式，因此需要一种方法来探测其精细的临床理解能力。", "method": "引入了DeVisE（人口统计学和生命体征评估）行为测试框架。构建了一个包含来自MIMIC-IV的ICU出院记录的数据集，生成了带有受控单变量反事实（针对人口统计学和生命体征属性）的原始和模板化版本。评估了五种通用型和医学微调型LLM在零样本和微调设置下的表现。通过输入级敏感性（反事实如何改变笔记的可能性）和下游推理（它们如何影响预测的住院时长）来评估模型行为。", "result": "零样本模型表现出更连贯的反事实推理模式。微调模型倾向于更稳定但对临床有意义的变化响应较差。人口统计学因素微妙但持续地影响着输出。", "conclusion": "行为测试有助于揭示临床LLM的推理策略，并为设计更安全、更透明的医学AI系统提供信息。公平性评估因人口统计学因素的影响而显得尤为重要。", "translation": "大型语言模型（LLMs）在临床决策支持中的应用日益增多，然而当前的评估方法常常无法区分真正的医学推理与肤浅的模式。我们引入了DeVisE（人口统计学和生命体征评估），一个用于探测精细临床理解的行为测试框架。我们构建了一个来自MIMIC-IV的ICU出院记录数据集，生成了原始（真实世界）和基于模板（合成）的版本，并带有受控的单变量反事实，目标是人口统计学（年龄、性别、种族）和生命体征属性。我们评估了五种LLM，涵盖通用型和医学微调型，在零样本和微调设置下进行。我们通过以下方式评估模型行为：（1）输入级敏感性——反事实如何改变笔记的可能性；以及（2）下游推理——它们如何影响预测的住院时长。我们的结果显示，零样本模型表现出更连贯的反事实推理模式，而微调模型倾向于更稳定但对临床有意义的变化响应较差。值得注意的是，人口统计学因素微妙但持续地影响着输出，这强调了公平性评估的重要性。这项工作突出了行为测试在揭示临床LLM推理策略和指导更安全、更透明的医学AI系统设计方面的实用性。", "summary": "本文介绍了DeVisE，一个用于评估医学大型语言模型（LLMs）精细临床理解的行为测试框架。该研究利用来自MIMIC-IV的ICU出院记录数据集，通过对人口统计学和生命体征属性进行受控反事实生成，评估了LLM的敏感性和下游推理能力。研究结果表明，零样本模型展现出更连贯的推理模式，而微调模型虽然更稳定但对临床变化响应不足。此外，人口统计学因素持续且微妙地影响模型输出，强调了在医学AI系统设计中进行公平性评估的重要性。", "keywords": "行为测试, 医学LLM, 反事实, 临床推理, 公平性", "comments": "该研究的创新之处在于引入了DeVisE行为测试框架，通过受控反事实方法深入探究LLM的精细临床理解能力，超越了传统的表面模式匹配评估。其发现人口统计学因素对模型输出的微妙影响至关重要，凸显了医学AI中公平性和偏见检测的重要性，对构建更安全、更透明的系统具有显著贡献。同时，对零样本和微调模型行为差异的分析也为模型开发提供了宝贵见解。"}}
{"id": "2506.15518", "title": "Real-Time Initialization of Unknown Anchors for UWB-aided Navigation", "authors": ["Giulio Delama", "Igor Borowski", "Roland Jung", "Stephan Weiss"], "summary": "This paper presents a framework for the real-time initialization of unknown\nUltra-Wideband (UWB) anchors in UWB-aided navigation systems. The method is\ndesigned for localization solutions where UWB modules act as supplementary\nsensors. Our approach enables the automatic detection and calibration of\npreviously unknown anchors during operation, removing the need for manual\nsetup. By combining an online Positional Dilution of Precision (PDOP)\nestimation, a lightweight outlier detection method, and an adaptive robust\nkernel for non-linear optimization, our approach significantly improves\nrobustness and suitability for real-world applications compared to\nstate-of-the-art. In particular, we show that our metric which triggers an\ninitialization decision is more conservative than current ones commonly based\non initial linear or non-linear initialization guesses. This allows for better\ninitialization geometry and subsequently lower initialization errors. We\ndemonstrate the proposed approach on two different mobile robots: an autonomous\nforklift and a quadcopter equipped with a UWB-aided Visual-Inertial Odometry\n(VIO) framework. The results highlight the effectiveness of the proposed method\nwith robust initialization and low positioning error. We open-source our code\nin a C++ library including a ROS wrapper.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15518v1", "AI": {"title_translation": "实时初始化UWB辅助导航中的未知锚点", "tldr": "该论文提出了一种实时初始化UWB辅助导航系统中未知UWB锚点的方法，通过结合PDOP估计、异常值检测和自适应鲁棒核优化，提高了鲁棒性和定位精度，并在移动机器人上进行了验证。", "motivation": "解决UWB辅助导航系统中未知UWB锚点的实时初始化问题，避免手动设置，提高系统在实际应用中的鲁棒性和适用性。", "method": "结合在线位置精度稀释度（PDOP）估计、轻量级异常值检测方法和用于非线性优化的自适应鲁棒核。提出了一种更保守的初始化决策触发指标，以获得更好的初始化几何形状和更低的初始化误差。", "result": "显著提高了鲁棒性和对实际应用的适用性，与现有技术相比，实现了更好的初始化几何形状和更低的初始化误差，并在自主叉车和四旋翼飞行器上验证了该方法的有效性，实现了鲁棒的初始化和低定位误差。", "conclusion": "所提出的方法能够实时初始化未知UWB锚点，显著提高UWB辅助导航系统的鲁棒性和定位精度，从而使其更适用于现实世界的应用。", "translation": "本文提出了一种用于UWB辅助导航系统中未知超宽带（UWB）锚点实时初始化的框架。该方法专为UWB模块作为辅助传感器的定位解决方案而设计。我们的方法能够在操作过程中自动检测和校准以前未知的锚点，从而无需手动设置。通过结合在线位置精度稀释度（PDOP）估计、轻量级异常值检测方法和用于非线性优化的自适应鲁棒核，我们的方法与现有技术相比，显著提高了鲁棒性和对实际应用的适用性。特别是，我们表明我们触发初始化决策的指标比目前通常基于初始线性或非线性初始化猜测的指标更为保守。这使得更好的初始化几何形状和随后的更低的初始化误差成为可能。我们在两种不同的移动机器人上演示了所提出的方法：一辆自主叉车和一架配备UWB辅助视觉惯性里程计（VIO）框架的四旋翼飞行器。结果突出了所提出方法的有效性，具有鲁棒的初始化和低定位误差。我们开源了我们的代码，包括一个ROS封装的C++库。", "summary": "本文提出了一种在UWB辅助导航系统中实时初始化未知UWB锚点的新框架。该方法通过整合在线PDOP估计、轻量级异常值检测和自适应鲁棒核非线性优化，实现了未知锚点的自动检测和校准，无需手动干预。研究表明，该方法采用的初始化决策触发机制更为保守，从而带来了更优的初始化几何形状和更低的定位误差。实验在自主叉车和四旋翼飞行器上进行，验证了该方法在实际应用中实现鲁棒初始化和高精度定位的有效性。", "keywords": "UWB导航, 锚点初始化, 实时定位, 鲁棒性, 移动机器人", "comments": "该论文的创新点在于提出了一个无需手动干预的实时未知UWB锚点初始化框架，通过结合多种先进技术（PDOP估计、异常值检测、自适应鲁棒核）显著提升了系统的鲁棒性和精度。其更保守的初始化决策触发机制是提升性能的关键。开源代码的举措也利于社区的进一步研究和应用。"}}
{"id": "2506.15160", "title": "Enhancing point cloud analysis via neighbor aggregation correction based on cross-stage structure correlation", "authors": ["Jiaqi Shi", "Jin Xiao", "Xiaoguang Hu", "Boyang Song", "Hao Jiang", "Tianyou Chen", "Baochang Zhang"], "summary": "Point cloud analysis is the cornerstone of many downstream tasks, among which\naggregating local structures is the basis for understanding point cloud data.\nWhile numerous works aggregate neighbor using three-dimensional relative\ncoordinates, there are irrelevant point interference and feature hierarchy gap\nproblems due to the limitation of local coordinates. Although some works\naddress this limitation by refining spatial description though explicit\nmodeling of cross-stage structure, these enhancement methods based on direct\ngeometric structure encoding have problems of high computational overhead and\nnoise sensitivity. To overcome these problems, we propose the Point\nDistribution Set Abstraction module (PDSA) that utilizes the correlation in the\nhigh-dimensional space to correct the feature distribution during aggregation,\nwhich improves the computational efficiency and robustness. PDSA distinguishes\nthe point correlation based on a lightweight cross-stage structural descriptor,\nand enhances structural homogeneity by reducing the variance of the neighbor\nfeature matrix and increasing classes separability though long-distance\nmodeling. Additionally, we introducing a key point mechanism to optimize the\ncomputational overhead. The experimental result on semantic segmentation and\nclassification tasks based on different baselines verify the generalization of\nthe method we proposed, and achieve significant performance improvement with\nless parameter cost. The corresponding ablation and visualization results\ndemonstrate the effectiveness and rationality of our method. The code and\ntraining weight is available at: https://github.com/AGENT9717/PointDistribution", "comment": "17 papes, 7 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15160v1", "AI": {"title_translation": "基于跨阶段结构相关性的邻域聚合校正增强点云分析", "tldr": "该论文提出了点分布集抽象（PDSA）模块，利用跨阶段结构相关性校正邻域聚合过程中的特征分布，从而提升点云分析的效率和鲁棒性。", "motivation": "现有基于三维相对坐标的邻域聚合方法存在不相关点干扰和特征层次差距问题。而通过显式建模跨阶段结构来细化空间描述的方法则面临计算开销大和噪声敏感的挑战。", "method": "本文提出了点分布集抽象（PDSA）模块，利用高维空间中的相关性来校正聚合过程中的特征分布，以提高计算效率和鲁棒性。PDSA基于轻量级跨阶段结构描述符区分点相关性，并通过减少邻域特征矩阵方差和通过长距离建模增加类别可分离性来增强结构同质性。此外，引入了关键点机制以优化计算开销。", "result": "在语义分割和分类任务上，基于不同基线的实验结果验证了所提方法的泛化性，并在参数成本更低的情况下实现了显著的性能提升。消融实验和可视化结果也证明了该方法的有效性和合理性。", "conclusion": "本文提出的PDSA模块通过有效解决邻域聚合中的计算开销和噪声敏感性问题，显著提升了点云分析的性能和效率。", "translation": "点云分析是许多下游任务的基石，其中聚合局部结构是理解点云数据的基础。尽管许多工作使用三维相对坐标聚合邻域，但由于局部坐标的限制，存在不相关点干扰和特征层次差距问题。虽然一些工作通过显式建模跨阶段结构来细化空间描述以解决此限制，但这些基于直接几何结构编码的增强方法存在计算开销大和噪声敏感的问题。为了克服这些问题，我们提出了点分布集抽象模块（PDSA），它利用高维空间中的相关性来校正聚合过程中的特征分布，从而提高计算效率和鲁棒性。PDSA基于轻量级跨阶段结构描述符区分点相关性，并通过减少邻域特征矩阵的方差和通过长距离建模增加类别可分离性来增强结构同质性。此外，我们引入了关键点机制来优化计算开销。基于不同基线在语义分割和分类任务上的实验结果验证了我们所提出方法的泛化性，并在参数成本更低的情况下实现了显著的性能提升。相应的消融和可视化结果证明了我们方法的有效性和合理性。代码和训练权重可在：https://github.com/AGENT9717/PointDistribution 获取。", "summary": "本文提出点分布集抽象（PDSA）模块，旨在增强点云分析。针对现有邻域聚合方法中存在的干扰、特征差距、高计算成本和噪声敏感性等问题，PDSA利用高维空间中的跨阶段结构相关性来校正特征分布。它采用轻量级描述符和关键点机制来提高计算效率、鲁棒性和结构同质性。实验结果表明，该方法在不同任务中实现了显著的性能提升和泛化能力，同时降低了参数成本。", "keywords": "点云分析, 邻域聚合, 跨阶段结构相关性, PDSA, 特征分布", "comments": "该论文解决了点云分析中邻域聚合的关键局限性，特别是计算开销和噪声敏感性问题。其创新点在于PDSA模块利用高维空间相关性以及轻量级跨阶段结构描述符进行特征分布校正，并结合关键点机制提高效率。这种方法为传统的几何编码方法提供了一种鲁棒且高效的替代方案，并在性能和资源利用方面展示了实际的改进。"}}
{"id": "2506.14895", "title": "Generalized Reference Kernel With Negative Samples For Support Vector One-class Classification", "authors": ["Jenni Raitoharju"], "summary": "This paper focuses on small-scale one-class classification with some negative\nsamples available. We propose Generalized Reference Kernel with Negative\nSamples (GRKneg) for One-class Support Vector Machine (OC-SVM). We study\ndifferent ways to select/generate the reference vectors and recommend an\napproach for the problem at hand. It is worth noting that the proposed method\ndoes not use any labels in the model optimization but uses the original OC-SVM\nimplementation. Only the kernel used in the process is improved using the\nnegative data. We compare our method with the standard OC-SVM and with the\nbinary Support Vector Machine (SVM) using different amounts of negative\nsamples. Our approach consistently outperforms the standard OC-SVM using Radial\nBasis Function kernel. When there are plenty of negative samples, the binary\nSVM outperforms the one-class approaches as expected, but we show that for the\nlowest numbers of negative samples the proposed approach clearly outperforms\nthe binary SVM.", "comment": "Accepted to EUSIPCO2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14895v1", "AI": {"title_translation": "带有负样本的广义参考核支持向量一类分类", "tldr": "针对小规模一类分类问题，本文提出了一种利用少量负样本改进OC-SVM核函数的方法（GRKneg），在负样本稀少时表现优于标准OC-SVM和二分类SVM。", "motivation": "解决小规模一类分类问题，尤其是在有少量负样本可用的情况下，提升OC-SVM的性能。", "method": "提出了一种名为“带有负样本的广义参考核（GRKneg）”的方法，用于一类支持向量机（OC-SVM）。该方法改进了OC-SVM中使用的核函数，利用负样本数据，但在模型优化时不使用标签。研究了不同的参考向量选择/生成方式。", "result": "所提出的GRKneg方法在有少量负样本时，性能持续优于使用径向基函数核的标准OC-SVM。当负样本数量极少时，GRKneg明显优于二分类SVM。当负样本充足时，二分类SVM表现最佳。", "conclusion": "在负样本稀缺的小规模一类分类任务中，本文提出的GRKneg方法能够有效提升OC-SVM的性能，并优于传统的二分类SVM。", "translation": "本文关注小规模一类分类问题，尤其是在有少量负样本可用的情况下。我们提出了一种用于一类支持向量机（OC-SVM）的带有负样本的广义参考核（GRKneg）。我们研究了选择/生成参考向量的不同方法，并推荐了一种适用于当前问题的方法。值得注意的是，所提出的方法在模型优化中不使用任何标签，但使用了原始的OC-SVM实现。只有过程中使用的核函数通过负数据得到了改进。我们将我们的方法与标准OC-SVM以及使用不同数量负样本的二分类支持向量机（SVM）进行了比较。我们的方法始终优于使用径向基函数核的标准OC-SVM。当负样本充足时，正如预期，二分类SVM优于一类方法，但我们表明，在负样本数量最少的情况下，所提出的方法明显优于二分类SVM。", "summary": "本文针对小规模一类分类任务中存在少量负样本的情况，提出了一种改进一类支持向量机（OC-SVM）的新方法——带有负样本的广义参考核（GRKneg）。该方法通过利用负样本数据改进核函数，且在模型优化时不依赖标签。实验结果表明，GRKneg在负样本量较少时，性能显著优于标准OC-SVM和二分类SVM。", "keywords": "一类分类, 支持向量机, 负样本, 核函数, 小样本学习", "comments": "这项工作通过巧妙地利用少量负样本来改进OC-SVM的核函数，从而提升了在特定场景下（小规模一类分类且有少量负样本）的分类性能。其创新点在于在不改变OC-SVM优化过程的前提下，通过改进核函数来融入负样本信息，避免了标签的使用。这对于实际应用中难以获取大量负样本或标签的情况具有重要意义。"}}
{"id": "2506.15355", "title": "SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models' Knowledge of Indian Culture", "authors": ["Arijit Maji", "Raghvendra Kumar", "Akash Ghosh", "Anushka", "Sriparna Saha"], "summary": "Language Models (LMs) are indispensable tools shaping modern workflows, but\ntheir global effectiveness depends on understanding local socio-cultural\ncontexts. To address this, we introduce SANSKRITI, a benchmark designed to\nevaluate language models' comprehension of India's rich cultural diversity.\nComprising 21,853 meticulously curated question-answer pairs spanning 28 states\nand 8 union territories, SANSKRITI is the largest dataset for testing Indian\ncultural knowledge. It covers sixteen key attributes of Indian culture: rituals\nand ceremonies, history, tourism, cuisine, dance and music, costume, language,\nart, festivals, religion, medicine, transport, sports, nightlife, and\npersonalities, providing a comprehensive representation of India's cultural\ntapestry. We evaluate SANSKRITI on leading Large Language Models (LLMs), Indic\nLanguage Models (ILMs), and Small Language Models (SLMs), revealing significant\ndisparities in their ability to handle culturally nuanced queries, with many\nmodels struggling in region-specific contexts. By offering an extensive,\nculturally rich, and diverse dataset, SANSKRITI sets a new standard for\nassessing and improving the cultural understanding of LMs.", "comment": "ACL 2025 Findings", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15355v1", "AI": {"title_translation": "SANSKRITI：一个评估语言模型印度文化知识的综合基准", "tldr": "SANSKRITI是一个大型基准数据集，用于评估语言模型对印度文化的理解，发现现有模型在处理文化细微差别方面存在显著不足。", "motivation": "语言模型在全球范围内的有效性取决于它们对当地社会文化背景的理解。为了解决现有模型在印度文化知识方面的不足，本文引入了SANSKRITI。", "method": "本文引入了SANSKRITI，一个包含21,853个精心策划的问题-答案对的数据集，涵盖印度28个邦和8个联邦属地的16个关键文化属性。研究者使用SANSKRITI评估了主流大型语言模型（LLMs）、印度语语言模型（ILMs）和小型语言模型（SLMs）。", "result": "评估揭示了模型在处理文化细微差别查询方面存在显著差异，许多模型在特定区域背景下表现不佳。", "conclusion": "SANSKRITI通过提供一个广泛、文化丰富且多样化的数据集，为评估和提高语言模型的文化理解设定了新标准。", "translation": "语言模型（LMs）是塑造现代工作流程不可或缺的工具，但其全球有效性取决于对当地社会文化背景的理解。为解决此问题，我们引入了SANSKRITI，这是一个旨在评估语言模型对印度丰富文化多样性理解的基准。SANSKRITI包含21,853个精心策划的问题-答案对，涵盖28个邦和8个联邦属地，是测试印度文化知识的最大数据集。它涵盖了印度文化的16个关键属性：仪式典礼、历史、旅游、美食、舞蹈音乐、服装、语言、艺术、节日、宗教、医药、交通、体育、夜生活和人物，全面展现了印度的文化图景。我们使用SANSKRITI评估了主流大型语言模型（LLMs）、印度语语言模型（ILMs）和小型语言模型（SLMs），揭示了它们在处理文化细微差别查询方面的显著差异，许多模型在特定区域背景下表现不佳。通过提供一个广泛、文化丰富且多样化的数据集，SANSKRITI为评估和提高语言模型的文化理解设定了新标准。", "summary": "本文介绍了SANSKRITI，一个旨在评估语言模型印度文化知识的综合基准数据集。该数据集包含21,853个问题-答案对，覆盖印度16个文化属性和所有邦及联邦属地。通过对主流语言模型进行评估，研究发现现有模型在处理文化细致问题方面表现不佳，尤其是在区域特定背景下。SANSKRITI为提升语言模型的文化理解能力提供了重要工具。", "keywords": "语言模型, 印度文化, 基准数据集, 文化理解, SANSKRITI", "comments": "SANSKRITI的创新之处在于其作为目前最大的印度文化知识评估数据集，涵盖了广泛的文化维度和地理区域，填补了现有语言模型在非西方文化知识方面评估的空白。这对于促进语言模型在全球范围内的公平性和有效性至关重要。"}}
{"id": "2506.15419", "title": "Density estimation via periodic scaled Korobov kernel method with exponential decay condition", "authors": ["Ziyang Ye", "Haoyuan Tan", "Xiaoqun Wang", "Zhijian He"], "summary": "We propose the periodic scaled Korobov kernel (PSKK) method for nonparametric\ndensity estimation on $\\mathbb{R}^d$. By first wrapping the target density into\na periodic version through modulo operation and subsequently applying kernel\nridge regression in scaled Korobov spaces, we extend the kernel approach\nproposed by Kazashi and Nobile (SIAM J. Numer. Anal., 2023) and eliminate its\nrequirement for inherent periodicity of the density function. This key\nmodification enables effective estimation of densities defined on unbounded\ndomains. We establish rigorous mean integrated squared error (MISE) bounds,\nproving that for densities with smoothness of order $\\alpha$ and exponential\ndecay, our method achieves the $\\mathcal{O}(M^{-1/(1+1/(2\\alpha)+\\epsilon)})$\nMISE convergence rate with an arbitrarily small $\\epsilon>0$. While matching\nthe convergence rate of the previous kernel approach, our approach applies to a\nbroader class of non-periodic distributions. Numerical experiments confirm the\ntheoretical results and demonstrate significant improvement over traditional\nkernel density estimation in large-sample regimes.", "comment": "26 pages, 6 figures", "cate": "math.ST", "url": "http://arxiv.org/abs/2506.15419v1", "AI": {"title_translation": "基于周期尺度Korobov核方法和指数衰减条件的密度估计", "tldr": "提出了一种周期尺度Korobov核（PSKK）方法，用于在无界域上进行非参数密度估计，通过将目标密度转换为周期版本并应用核岭回归，解决了现有方法对固有周期性的要求，并实现了与现有方法相当的收敛速度。", "motivation": "现有的核方法（如Kazashi和Nobile, 2023）在密度估计时需要密度函数具有固有的周期性，这限制了其在无界域上非周期分布的应用。本研究旨在消除这一限制，使核方法能够有效估计定义在无界域上的密度。", "method": "本研究提出了周期尺度Korobov核（PSKK）方法。该方法首先通过模运算将目标密度包裹成周期版本，然后将核岭回归应用于尺度Korobov空间中。这种修改扩展了Kazashi和Nobile（2023）提出的核方法，并消除了其对密度函数固有周期性的要求。", "result": "对于平滑度为$\\\\alpha$且具有指数衰减的密度，本方法实现了$\\\\mathcal{O}(M^{-1/(1+1/(2\\\\alpha)+\\\\\\\\epsilon)})$的均方积分误差（MISE）收敛速度，其中$\\\\epsilon>0$可以任意小。该收敛速度与之前的核方法相匹配，但本方法适用于更广泛的非周期分布。数值实验证实了理论结果，并表明在大样本量情况下，本方法比传统的核密度估计有显著改进。", "conclusion": "周期尺度Korobov核（PSKK）方法成功地扩展了核方法在非参数密度估计中的应用范围，使其能够有效处理无界域上的非周期密度函数，并提供了强大的理论收敛保证和实际性能提升。", "translation": "我们提出了周期尺度Korobov核（PSKK）方法，用于在$\\mathbb{R}^d$上进行非参数密度估计。通过首先通过模运算将目标密度包裹成周期版本，然后将核岭回归应用于尺度Korobov空间中，我们扩展了Kazashi和Nobile（SIAM J. Numer. Anal., 2023）提出的核方法，并消除了其对密度函数固有周期性的要求。这一关键修改使得能够有效估计定义在无界域上的密度。我们建立了严格的均方积分误差（MISE）界限，证明对于平滑度为$\\alpha$且具有指数衰减的密度，我们的方法实现了$\\mathcal{O}(M^{-1/(1+1/(2\\alpha)+\\epsilon)})$的MISE收敛速度，其中$\\epsilon>0$可以任意小。在匹配之前核方法的收敛速度的同时，我们的方法适用于更广泛的非周期分布。数值实验证实了理论结果，并表明在大样本量情况下，比传统的核密度估计有显著改进。", "summary": "本论文提出了一种新的非参数密度估计方法——周期尺度Korobov核（PSKK）方法。该方法通过将目标密度转换为周期形式并应用核岭回归，成功地消除了现有核方法对密度函数固有周期性的限制，使其能够有效估计定义在无界域上的非周期密度。研究证明，对于具有指数衰减和平滑度的密度，该方法能达到与现有方法相当的MISE收敛速度，并且数值实验验证了其理论性能和相对于传统方法的优势。", "keywords": "密度估计, Korobov核, 周期性, 非参数, 收敛速度", "comments": "该论文的主要创新在于通过引入周期包裹和核岭回归的结合，成功地将现有核密度估计方法的适用范围扩展到非周期和无界域的分布。这解决了传统核方法的一个重要局限性，使得该方法在实际应用中更具普适性。理论MISE收敛界限的建立以及数值实验的验证，进一步增强了该方法的可靠性和实用价值。"}}
{"id": "2506.15539", "title": "Aerial Grasping via Maximizing Delta-Arm Workspace Utilization", "authors": ["Haoran Chen", "Weiliang Deng", "Biyu Ye", "Yifan Xiong", "Ximin Lyu"], "summary": "The workspace limits the operational capabilities and range of motion for the\nsystems with robotic arms. Maximizing workspace utilization has the potential\nto provide more optimal solutions for aerial manipulation tasks, increasing the\nsystem's flexibility and operational efficiency. In this paper, we introduce a\nnovel planning framework for aerial grasping that maximizes workspace\nutilization. We formulate an optimization problem to optimize the aerial\nmanipulator's trajectory, incorporating task constraints to achieve efficient\nmanipulation. To address the challenge of incorporating the delta arm's\nnon-convex workspace into optimization constraints, we leverage a Multilayer\nPerceptron (MLP) to map position points to feasibility\nprobabilities.Furthermore, we employ Reversible Residual Networks (RevNet) to\napproximate the complex forward kinematics of the delta arm, utilizing\nefficient model gradients to eliminate workspace constraints. We validate our\nmethods in simulations and real-world experiments to demonstrate their\neffectiveness.", "comment": "8 pages, 7 figures", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15539v1", "AI": {"title_translation": "采用最大化Delta机械臂工作空间利用率的空中抓取", "tldr": "本文提出了一种新颖的空中抓取规划框架，通过最大化Delta机械臂工作空间利用率来提高空中操作系统的灵活性和效率。", "motivation": "机器人机械臂系统的工作空间限制了其操作能力和运动范围。最大化工作空间利用率有望为空中操作任务提供更优解决方案，提高系统灵活性和操作效率。", "method": "引入了一种新颖的空中抓取规划框架，该框架最大化工作空间利用率。通过公式化一个优化问题来优化空中机械臂的轨迹，并结合任务约束。为了解决Delta机械臂非凸工作空间带来的挑战，利用多层感知器（MLP）将位置点映射到可行性概率。此外，采用可逆残差网络（RevNet）来近似Delta机械臂的复杂正向运动学，利用高效模型梯度消除工作空间约束。", "result": "在仿真和真实世界实验中验证了所提出的方法，证明了其有效性。", "conclusion": "通过最大化Delta机械臂工作空间利用率，所提出的规划框架能够实现高效的空中抓取，提高系统的灵活性和操作效率。", "translation": "工作空间限制了机器人机械臂系统的操作能力和运动范围。最大化工作空间利用率有潜力为空中操作任务提供更优的解决方案，从而提高系统的灵活性和操作效率。在本文中，我们介绍了一种新颖的空中抓取规划框架，该框架最大化工作空间利用率。我们制定了一个优化问题来优化空中机械臂的轨迹，并结合任务约束以实现高效操作。为了解决将Delta机械臂的非凸工作空间纳入优化约束的挑战，我们利用多层感知器（MLP）将位置点映射到可行性概率。此外，我们采用可逆残差网络（RevNet）来近似Delta机械臂的复杂正向运动学，利用高效的模型梯度来消除工作空间约束。我们在仿真和真实世界实验中验证了我们的方法，以证明其有效性。", "summary": "本文提出了一种新颖的空中抓取规划框架，旨在通过最大化Delta机械臂的工作空间利用率来克服其操作能力限制。该框架将空中机械臂轨迹优化表述为一个优化问题，并利用MLP处理非凸工作空间，同时采用RevNet近似复杂正向运动学以消除工作空间约束。仿真和真实实验验证了其有效性。", "keywords": "空中抓取, 工作空间利用率, Delta机械臂, 优化, 机器学习", "comments": "这篇论文的创新点在于结合了机器学习方法（MLP和RevNet）来解决Delta机械臂非凸工作空间和复杂运动学在空中抓取规划中的挑战，有效地提高了工作空间利用率和操作效率。这种方法为复杂的机器人系统规划提供了新的思路。"}}
{"id": "2506.15166", "title": "Echo-DND: A dual noise diffusion model for robust and precise left ventricle segmentation in echocardiography", "authors": ["Abdur Rahman", "Keerthiveena Balraj", "Manojkumar Ramteke", "Anurag Singh Rathore"], "summary": "Recent advancements in diffusion probabilistic models (DPMs) have\nrevolutionized image processing, demonstrating significant potential in medical\napplications. Accurate segmentation of the left ventricle (LV) in\nechocardiograms is crucial for diagnostic procedures and necessary treatments.\nHowever, ultrasound images are notoriously noisy with low contrast and\nambiguous LV boundaries, thereby complicating the segmentation process. To\naddress these challenges, this paper introduces Echo-DND, a novel dual-noise\ndiffusion model specifically designed for this task. Echo-DND leverages a\nunique combination of Gaussian and Bernoulli noises. It also incorporates a\nmulti-scale fusion conditioning module to improve segmentation precision.\nFurthermore, it utilizes spatial coherence calibration to maintain spatial\nintegrity in segmentation masks. The model's performance was rigorously\nvalidated on the CAMUS and EchoNet-Dynamic datasets. Extensive evaluations\ndemonstrate that the proposed framework outperforms existing SOTA models. It\nachieves high Dice scores of 0.962 and 0.939 on these datasets, respectively.\nThe proposed Echo-DND model establishes a new standard in echocardiogram\nsegmentation, and its architecture holds promise for broader applicability in\nother medical imaging tasks, potentially improving diagnostic accuracy across\nvarious medical domains. Project page: https://abdur75648.github.io/Echo-DND", "comment": "Version of record published in Discover Applied Sciences (Springer\n  Nature). The definitive article is available at\n  https://doi.org/10.1007/s42452-025-07055-5", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15166v1", "AI": {"title_translation": "Echo-DND：一种用于超声心动图中心脏左心室分割的鲁棒精确双噪声扩散模型", "tldr": "Echo-DND是一种新的双噪声扩散模型，通过结合高斯和伯努利噪声、多尺度融合和空间一致性校准，显著提高了超声心动图中左心室分割的精度和鲁棒性，并在CAMUS和EchoNet-Dynamic数据集上超越了现有SOTA模型。", "motivation": "超声图像噪声大、对比度低、左心室边界模糊，使得准确分割左心室变得复杂，而准确的左心室分割对诊断和治疗至关重要。", "method": "本文引入了Echo-DND，一种新颖的双噪声扩散模型，专门用于左心室分割。它结合了高斯噪声和伯努利噪声，并融入了多尺度融合条件模块以提高分割精度。此外，它还利用空间一致性校准来保持分割掩膜的空间完整性。", "result": "Echo-DND在CAMUS和EchoNet-Dynamic数据集上进行了严格验证，实现了高Dice分数，分别为0.962和0.939。该框架性能优于现有SOTA模型。", "conclusion": "Echo-DND模型在超声心动图分割领域建立了新标准，其架构有望在其他医学成像任务中具有更广泛的适用性，从而可能提高各种医学领域的诊断准确性。", "translation": "扩散概率模型（DPMs）的最新进展彻底改变了图像处理，在医学应用中展现出巨大的潜力。在超声心动图中准确分割左心室（LV）对于诊断程序和必要的治疗至关重要。然而，超声图像以其噪声大、对比度低和左心室边界模糊而闻名，从而使分割过程复杂化。为了应对这些挑战，本文引入了Echo-DND，一种专门为此任务设计的新颖双噪声扩散模型。Echo-DND利用高斯噪声和伯努利噪声的独特组合。它还结合了多尺度融合条件模块以提高分割精度。此外，它利用空间一致性校准来保持分割掩膜的空间完整性。该模型在CAMUS和EchoNet-Dynamic数据集上进行了严格验证。广泛的评估表明，所提出的框架优于现有SOTA模型。它在这些数据集上分别达到了0.962和0.939的高Dice分数。所提出的Echo-DND模型在超声心动图分割中建立了新标准，其架构有望在其他医学成像任务中具有更广泛的适用性，可能提高各种医学领域的诊断准确性。项目页面：https://abdur75648.github.io/Echo-DND", "summary": "本文提出了Echo-DND，一种新颖的双噪声扩散模型，用于解决超声心动图中左心室分割面临的噪声大、对比度低和边界模糊等挑战。该模型结合了高斯和伯努利噪声，并集成了多尺度融合条件模块和空间一致性校准，以提高分割精度和保持空间完整性。在CAMUS和EchoNet-Dynamic数据集上的广泛评估表明，Echo-DND的表现优于现有最先进模型，并取得了显著的Dice分数，分别为0.962和0.939，为超声心动图分割设立了新标准，并有望应用于其他医学成像领域。", "keywords": "左心室分割, 双噪声扩散模型, 超声心动图, 医学图像处理, 深度学习", "comments": "Echo-DND的创新之处在于其独特的双噪声扩散模型设计，结合了高斯和伯努利噪声，以及多尺度融合和空间一致性校准，有效解决了超声图像固有的噪声和低对比度问题。其在关键医学图像分割任务上的SOTA性能显示了巨大的应用潜力，可能显著提高诊断准确性。"}}
{"id": "2506.14798", "title": "MODS: Multi-source Observations Conditional Diffusion Model for Meteorological State Downscaling", "authors": ["Siwei Tu", "Jingyi Xu", "Weidong Yang", "Lei Bai", "Ben Fei"], "summary": "Accurate acquisition of high-resolution surface meteorological conditions is\ncritical for forecasting and simulating meteorological variables. Directly\napplying spatial interpolation methods to derive meteorological values at\nspecific locations from low-resolution grid fields often yields results that\ndeviate significantly from the actual conditions. Existing downscaling methods\nprimarily rely on the coupling relationship between geostationary satellites\nand ERA5 variables as a condition. However, using brightness temperature data\nfrom geostationary satellites alone fails to comprehensively capture all the\nchanges in meteorological variables in ERA5 maps. To address this limitation,\nwe can use a wider range of satellite data to make more full use of its\ninversion effects on various meteorological variables, thus producing more\nrealistic results across different meteorological variables. To further improve\nthe accuracy of downscaling meteorological variables at any location, we\npropose the Multi-source Observation Down-Scaling Model (MODS). It is a\nconditional diffusion model that fuses data from multiple geostationary\nsatellites GridSat, polar-orbiting satellites (AMSU-A, HIRS, and MHS), and\ntopographic data (GEBCO), as conditions, and is pre-trained on the ERA5\nreanalysis dataset. During training, latent features from diverse conditional\ninputs are extracted separately and fused into ERA5 maps via a multi-source\ncross-attention module. By exploiting the inversion relationships between\nreanalysis data and multi-source atmospheric variables, MODS generates\natmospheric states that align more closely with real-world conditions. During\nsampling, MODS enhances downscaling consistency by incorporating low-resolution\nERA5 maps and station-level meteorological data as guidance. Experimental\nresults demonstrate that MODS achieves higher fidelity when downscaling ERA5\nmaps to a 6.25 km resolution.", "comment": null, "cate": "physics.ao-ph", "url": "http://arxiv.org/abs/2506.14798v1", "AI": {"title_translation": "MODS：多源观测条件扩散模型用于气象状态降尺度", "tldr": "MODS是一个条件扩散模型，通过融合多源卫星和地形数据，实现高分辨率气象变量的降尺度，提高准确性。", "motivation": "高分辨率地表气象条件对气象预报和模拟至关重要。现有空间插值方法和仅依赖地球静止卫星数据的降尺度方法存在局限性，导致结果与实际情况偏差大，未能全面捕捉气象变量的变化。", "method": "本文提出了多源观测降尺度模型（MODS），这是一个条件扩散模型。它融合了来自多颗地球静止卫星（GridSat）、极轨卫星（AMSU-A、HIRS和MHS）以及地形数据（GEBCO）作为条件，并在ERA5再分析数据集上进行预训练。训练过程中，从多样化的条件输入中提取潜在特征，并通过多源交叉注意力模块融合到ERA5图中。采样时，MODS通过结合低分辨率ERA5图和站点级气象数据作为指导来增强降尺度的一致性。", "result": "实验结果表明，MODS在将ERA5图降尺度到6.25公里分辨率时实现了更高的保真度。", "conclusion": "MODS通过利用再分析数据与多源大气变量之间的反演关系，生成了更符合实际条件的大气状态，有效提高了气象变量降尺度的准确性。", "translation": "准确获取高分辨率地表气象条件对气象变量的预报和模拟至关重要。直接将空间插值方法应用于从低分辨率网格场推导特定位置的气象值，通常会导致结果与实际情况显著偏离。现有的降尺度方法主要依赖于地球静止卫星和ERA5变量之间的耦合关系作为条件。然而，单独使用地球静止卫星的亮度温度数据未能全面捕捉ERA5图中气象变量的所有变化。为了解决这一局限性，我们可以利用更广泛的卫星数据，更充分地利用其对各种气象变量的反演效应，从而在不同的气象变量中产生更真实的结果。为了进一步提高任意位置气象变量降尺度的准确性，我们提出了多源观测降尺度模型（MODS）。它是一个条件扩散模型，融合了来自多颗地球静止卫星GridSat、极轨卫星（AMSU-A、HIRS和MHS）以及地形数据（GEBCO）作为条件，并在ERA5再分析数据集上进行预训练。在训练过程中，来自不同条件输入的潜在特征被单独提取，并通过多源交叉注意力模块融合到ERA5图中。通过利用再分析数据和多源大气变量之间的反演关系，MODS生成了更符合真实条件的大气状态。在采样过程中，MODS通过结合低分辨率ERA5图和站点级气象数据作为指导来增强降尺度的一致性。实验结果表明，MODS在将ERA5图降尺度到6.25公里分辨率时实现了更高的保真度。", "summary": "本研究提出了一种名为MODS（多源观测降尺度模型）的条件扩散模型，旨在解决现有气象变量降尺度方法中分辨率不足和数据利用不全面的问题。MODS通过融合多源地球静止卫星、极轨卫星和地形数据作为条件，并在ERA5再分析数据集上预训练。模型利用多源交叉注意力模块融合不同数据特征，并在采样阶段通过低分辨率ERA5图和站点数据进行引导，从而生成更接近真实情况的高分辨率气象状态，实验证明其在将ERA5图降尺度至6.25公里分辨率时表现出更高的准确性。", "keywords": "气象降尺度, 条件扩散模型, 多源观测, ERA5, 卫星数据融合", "comments": "该论文的创新点在于提出了一个融合多源卫星和地形数据的条件扩散模型MODS，用于气象变量的降尺度。它克服了传统方法仅依赖单一卫星数据或空间插值的局限性，通过多源数据融合和交叉注意力机制，更全面地捕捉气象变量的变化，显著提高了降尺度结果的准确性和真实性。其在6.25公里分辨率上的高保真度表现，预示着在气象预报和模拟领域具有重要的应用潜力。"}}
{"id": "2506.15372", "title": "COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization and Headline Generation", "authors": ["Raghvendra Kumar", "S. A. Mohammed Salman", "Aryan Sahu", "Tridib Nandi", "Pragathi Y. P.", "Sriparna Saha", "Jose G. Moreno"], "summary": "Despite progress in comment-aware multimodal and multilingual summarization\nfor English and Chinese, research in Indian languages remains limited. This\nstudy addresses this gap by introducing COSMMIC, a pioneering comment-sensitive\nmultimodal, multilingual dataset featuring nine major Indian languages. COSMMIC\ncomprises 4,959 article-image pairs and 24,484 reader comments, with\nground-truth summaries available in all included languages. Our approach\nenhances summaries by integrating reader insights and feedback. We explore\nsummarization and headline generation across four configurations: (1) using\narticle text alone, (2) incorporating user comments, (3) utilizing images, and\n(4) combining text, comments, and images. To assess the dataset's\neffectiveness, we employ state-of-the-art language models such as LLama3 and\nGPT-4. We conduct a comprehensive study to evaluate different component\ncombinations, including identifying supportive comments, filtering out noise\nusing a dedicated comment classifier using IndicBERT, and extracting valuable\ninsights from images with a multilingual CLIP-based classifier. This helps\ndetermine the most effective configurations for natural language generation\n(NLG) tasks. Unlike many existing datasets that are either text-only or lack\nuser comments in multimodal settings, COSMMIC uniquely integrates text, images,\nand user feedback. This holistic approach bridges gaps in Indian language\nresources, advancing NLP research and fostering inclusivity.", "comment": "ACL 2025 MAINs", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15372v1", "AI": {"title_translation": "COSMMIC：评论敏感的多模态多语言印度语语料库，用于摘要生成和标题生成", "tldr": "COSMMIC是一个开创性的评论敏感多模态多语言印度语语料库，包含九种主要印度语言，旨在通过整合用户反馈来改进摘要和标题生成。", "motivation": "尽管在英语和汉语的评论感知多模态多语言摘要方面取得了进展，但印度语言的研究仍然有限。本研究旨在通过引入COSMMIC数据集来弥补这一空白。", "method": "本研究引入了COSMMIC数据集，其中包含4,959个文章-图像对和24,484条读者评论，并提供九种印度主要语言的真实摘要。研究通过整合读者见解和反馈来增强摘要，并探索了四种配置下的摘要和标题生成：仅文章文本、结合用户评论、利用图像以及结合文本、评论和图像。为评估数据集的有效性，研究使用了LLama3和GPT-4等最先进的语言模型，并进行了全面研究，包括识别支持性评论、使用IndicBERT的评论分类器过滤噪音，以及使用基于CLIP的多语言分类器从图像中提取有价值的见解。", "result": "研究评估了不同组件组合（文本、评论、图像）对自然语言生成（NLG）任务的有效性，并确定了最有效的配置。COSMMIC数据集独特地整合了文本、图像和用户反馈，与许多现有仅文本或缺乏用户评论的多模态数据集不同。", "conclusion": "COSMMIC数据集的引入弥补了印度语言资源方面的空白，推动了自然语言处理（NLP）研究，并促进了语言的包容性。", "translation": "尽管在评论感知多模态多语言摘要领域，英语和汉语的研究取得了进展，但印度语言的研究仍然有限。本研究通过引入COSMMIC，一个开创性的评论敏感多模态多语言数据集，弥补了这一空白，该数据集涵盖了九种主要的印度语言。COSMMIC包含4,959个文章-图像对和24,484条读者评论，并提供所有包含语言的真实摘要。我们的方法通过整合读者见解和反馈来增强摘要。我们探索了四种配置下的摘要生成和标题生成：(1) 仅使用文章文本，(2) 整合用户评论，(3) 利用图像，以及 (4) 结合文本、评论和图像。为了评估数据集的有效性，我们采用了LLama3和GPT-4等最先进的语言模型。我们进行了一项全面的研究，评估了不同的组件组合，包括识别支持性评论、使用专门的IndicBERT评论分类器过滤噪音，以及使用基于CLIP的多语言分类器从图像中提取有价值的见解。这有助于确定自然语言生成（NLG）任务最有效的配置。与许多现有数据集不同，这些数据集要么仅是文本，要么在多模态设置中缺乏用户评论，COSMMIC独特地整合了文本、图像和用户反馈。这种整体方法弥补了印度语言资源方面的空白，推动了NLP研究并促进了包容性。", "summary": "COSMMIC是一个开创性的评论敏感多模态多语言印度语语料库，涵盖九种主要印度语言，旨在通过整合文章文本、图像和读者评论来改进摘要和标题生成。该数据集包含4,959个文章-图像对和24,484条评论，并提供多语言的真实摘要。研究评估了不同信息源组合对自然语言生成任务的影响，并利用先进的语言模型和专门的分类器进行评论过滤和图像特征提取，旨在弥补印度语言在多模态多语言摘要研究中的空白。", "keywords": "COSMMIC, 多模态, 多语言, 印度语言, 摘要生成, 标题生成, 语料库", "comments": "该研究通过创建COSMMIC数据集，首次为印度多种主要语言提供了评论敏感的多模态多语言资源，显著填补了该领域的研究空白。其创新之处在于独特地整合了文本、图像和用户评论，这对于提升印度语言的摘要和标题生成质量具有重要意义，并为自然语言生成（NLG）任务提供了更全面的视角。该数据集的发布有望促进印度语言NLP研究的进步和包容性。"}}
{"id": "2506.15349", "title": "Enhancing One-run Privacy Auditing with Quantile Regression-Based Membership Inference", "authors": ["Terrance Liu", "Matteo Boglioni", "Yiwei Fu", "Shengyuan Hu", "Pratiksha Thaker", "Zhiwei Steven Wu"], "summary": "Differential privacy (DP) auditing aims to provide empirical lower bounds on\nthe privacy guarantees of DP mechanisms like DP-SGD. While some existing\ntechniques require many training runs that are prohibitively costly, recent\nwork introduces one-run auditing approaches that effectively audit DP-SGD in\nwhite-box settings while still being computationally efficient. However, in the\nmore practical black-box setting where gradients cannot be manipulated during\ntraining and only the last model iterate is observed, prior work shows that\nthere is still a large gap between the empirical lower bounds and theoretical\nupper bounds. Consequently, in this work, we study how incorporating approaches\nfor stronger membership inference attacks (MIA) can improve one-run auditing in\nthe black-box setting. Evaluating on image classification models trained on\nCIFAR-10 with DP-SGD, we demonstrate that our proposed approach, which utilizes\nquantile regression for MIA, achieves tighter bounds while crucially\nmaintaining the computational efficiency of one-run methods.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15349v1", "AI": {"title_translation": "使用分位数回归的成员推断增强单次运行隐私审计", "tldr": "本文通过将基于分位数回归的成员推断攻击（MIA）纳入单次运行审计中，显著提高了黑盒设置下差分隐私（DP）审计的准确性，同时保持了计算效率。", "motivation": "现有的差分隐私（DP）审计技术在白盒设置下计算效率高，但在更实际的黑盒设置中，经验下限与理论上限之间仍存在较大差距，导致审计准确性不足。因此，需要探索如何通过更强的成员推断攻击来改善黑盒设置下的单次运行审计。", "method": "本文研究了如何将更强的成员推断攻击（MIA）方法整合到黑盒设置的单次运行审计中。具体地，提出了一种利用分位数回归进行MIA的方法。", "result": "在CIFAR-10上训练的图像分类模型上评估结果表明，所提出的方法在实现更紧密的隐私边界的同时，关键地保持了单次运行方法的计算效率。", "conclusion": "通过将基于分位数回归的成员推断攻击纳入单次运行审计中，可以有效提高黑盒设置下差分隐私审计的准确性，同时保持计算效率。", "translation": "差分隐私（DP）审计旨在为DP机制（如DP-SGD）的隐私保证提供经验下限。虽然一些现有技术需要许多训练运行，成本高昂，但最近的工作引入了单次运行审计方法，这些方法在白盒设置下有效审计DP-SGD，同时计算效率也很高。然而，在更实际的黑盒设置中，即训练期间梯度无法被操纵且只能观察到最后一个模型迭代的情况下，先前的研究表明，经验下限和理论上限之间仍然存在很大差距。因此，在这项工作中，我们研究了如何将更强的成员推断攻击（MIA）方法纳入单次运行审计中，以改善黑盒设置下的审计。在CIFAR-10上训练的图像分类模型上进行评估，我们证明了我们提出的方法（利用分位数回归进行MIA）在实现更紧密的边界的同时，关键地保持了单次运行方法的计算效率。", "summary": "本文旨在解决黑盒设置下单次运行差分隐私（DP）审计中经验下限与理论上限之间存在的较大差距问题。研究提出了一种新颖的方法，通过将基于分位数回归的成员推断攻击（MIA）整合到单次运行审计流程中。实验结果表明，该方法能够实现更紧密的隐私边界，同时有效保持了单次运行方法的计算效率，从而显著提升了黑盒场景下隐私审计的准确性和实用性。", "keywords": "差分隐私审计, 成员推断攻击, 分位数回归, 黑盒设置, 单次运行审计", "comments": "本文的创新之处在于将分位数回归应用于成员推断攻击（MIA），并将其整合到黑盒设置的单次运行隐私审计中。这种方法有效地弥补了现有黑盒审计方法中经验与理论界限之间的差距，同时保持了计算效率，对于实际应用中的隐私保护具有重要意义。"}}
{"id": "2506.15607", "title": "GRIM: Task-Oriented Grasping with Conditioning on Generative Examples", "authors": ["Shailesh", "Alok Raj", "Nayan Kumar", "Priya Shukla", "Andrew Melnik", "Micheal Beetz", "Gora Chand Nandi"], "summary": "Task-Oriented Grasping (TOG) presents a significant challenge, requiring a\nnuanced understanding of task semantics, object affordances, and the functional\nconstraints dictating how an object should be grasped for a specific task. To\naddress these challenges, we introduce GRIM (Grasp Re-alignment via Iterative\nMatching), a novel training-free framework for task-oriented grasping.\nInitially, a coarse alignment strategy is developed using a combination of\ngeometric cues and principal component analysis (PCA)-reduced DINO features for\nsimilarity scoring. Subsequently, the full grasp pose associated with the\nretrieved memory instance is transferred to the aligned scene object and\nfurther refined against a set of task-agnostic, geometrically stable grasps\ngenerated for the scene object, prioritizing task compatibility. In contrast to\nexisting learning-based methods, GRIM demonstrates strong generalization\ncapabilities, achieving robust performance with only a small number of\nconditioning examples.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15607v1", "AI": {"title_translation": "GRIM：基于生成示例条件化的任务导向抓取", "tldr": "GRIM是一个无需训练的任务导向抓取框架，通过迭代匹配和几何稳定抓取精炼，在少量条件示例下实现强大的泛化能力和鲁棒性能。", "motivation": "任务导向抓取（TOG）面临巨大挑战，需要对任务语义、物体功能和抓取功能约束有细致的理解。现有方法可能无法很好地泛化或需要大量训练数据。GRIM旨在解决这些挑战，提供一个无需训练且泛化能力强的解决方案。", "method": "GRIM（Grasp Re-alignment via Iterative Matching）是一个无需训练的任务导向抓取框架。首先，利用几何线索和PCA降维的DINO特征进行相似度评分，开发了一种粗略对齐策略。随后，将检索到的记忆实例的完整抓取姿态转移到对齐的场景物体上，并针对一组为场景物体生成的、任务无关的几何稳定抓取进行进一步精炼，同时优先考虑任务兼容性。", "result": "与现有基于学习的方法相比，GRIM展示了强大的泛化能力，仅通过少量条件示例就实现了鲁棒的性能。", "conclusion": "GRIM通过其无需训练的框架和创新的抓取姿态精炼方法，成功解决了任务导向抓取的挑战，并在少量示例下表现出卓越的泛化能力和鲁棒性。", "translation": "任务导向抓取（TOG）提出了一个重大挑战，需要对任务语义、物体功能以及决定物体如何为特定任务抓取的功能约束有细致的理解。为了应对这些挑战，我们引入了GRIM（Grasp Re-alignment via Iterative Matching），一个新颖的无需训练的任务导向抓取框架。最初，利用几何线索和主成分分析（PCA）降维的DINO特征组合进行相似度评分，开发了一种粗略对齐策略。随后，将与检索到的记忆实例相关的完整抓取姿态转移到对齐的场景物体上，并针对一组为场景物体生成的、任务无关的几何稳定抓取进行进一步精炼，优先考虑任务兼容性。与现有基于学习的方法相比，GRIM展示了强大的泛化能力，仅通过少量条件示例就实现了鲁棒的性能。", "summary": "本文提出了GRIM（Grasp Re-alignment via Iterative Matching），一个无需训练的任务导向抓取（TOG）框架。GRIM通过结合几何线索和PCA降维的DINO特征进行粗略对齐，然后将检索到的抓取姿态转移并针对几何稳定的任务无关抓取进行精炼，以实现任务兼容性。该方法与现有学习方法不同，仅需少量条件示例即可展现出强大的泛化能力和鲁棒的性能，有效解决了TOG的复杂性。", "keywords": "任务导向抓取, 无需训练, 抓取精炼, 泛化能力, 机器人抓取", "comments": "GRIM的创新之处在于其无需训练的特性，这使其在数据有限或需要快速部署的场景中具有显著优势。通过结合几何与特征相似性进行初步对齐，并利用任务无关的几何稳定抓取进行精炼，GRIM展现了强大的泛化能力，仅需少量示例即可达到鲁棒性能，这对于实际应用非常重要。"}}
{"id": "2506.15180", "title": "ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections", "authors": ["Ziling Huang", "Yidan Zhang", "Shin'ichi Satoh"], "summary": "Large-scale visual search engines are expected to solve a dual problem at\nonce: (i) locate every image that truly contains the object described by a\nsentence and (ii) identify the object's bounding box or exact pixels within\neach hit. Existing techniques address only one side of this challenge. Visual\ngrounding yields tight boxes and masks but rests on the unrealistic assumption\nthat the object is present in every test image, producing a flood of false\nalarms when applied to web-scale collections. Text-to-image retrieval excels at\nsifting through massive databases to rank relevant images, yet it stops at\nwhole-image matches and offers no fine-grained localization. We introduce\nReferring Search and Discovery (ReSeDis), the first task that unifies\ncorpus-level retrieval with pixel-level grounding. Given a free-form\ndescription, a ReSeDis model must decide whether the queried object appears in\neach image and, if so, where it is, returning bounding boxes or segmentation\nmasks. To enable rigorous study, we curate a benchmark in which every\ndescription maps uniquely to object instances scattered across a large, diverse\ncorpus, eliminating unintended matches. We further design a task-specific\nmetric that jointly scores retrieval recall and localization precision.\nFinally, we provide a straightforward zero-shot baseline using a frozen\nvision-language model, revealing significant headroom for future study. ReSeDis\noffers a realistic, end-to-end testbed for building the next generation of\nrobust and scalable multimodal search systems.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15180v1", "AI": {"title_translation": "ReSeDis：一个用于大规模图像集合中基于指称的对象搜索数据集", "tldr": "ReSeDis引入了一个新的任务和数据集，它结合了图像检索和像素级定位，以解决现有视觉搜索引擎无法同时处理大规模图像中对象定位和识别的问题。", "motivation": "现有的大规模视觉搜索引擎在处理“定位包含特定描述对象的图像”和“识别图像中对象的精确位置”这两个问题时，只能解决其中一个。视觉定位技术假设对象存在于每张测试图像中，导致在网络规模数据集中产生大量误报；而文本到图像检索虽然能筛选出相关图像，但缺乏细粒度的定位能力。", "method": "本文提出了Referring Search and Discovery (ReSeDis)任务，这是首个将语料库级检索与像素级定位相结合的任务。为此，他们策划了一个基准数据集，其中每个描述都唯一映射到分散在大型多样语料库中的对象实例，以消除意外匹配。此外，他们还设计了一个任务特定的度量标准，可以同时评估检索召回率和定位精度。最后，他们提供了一个使用冻结视觉-语言模型的直接零样本基线。", "result": "ReSeDis数据集和任务的引入，为统一语料库级检索和像素级定位提供了第一个解决方案。通过基准数据集、特定度量标准和一个零样本基线，揭示了未来研究的巨大潜力。", "conclusion": "ReSeDis提供了一个现实、端到端的测试平台，用于构建下一代鲁健且可扩展的多模态搜索系统。", "translation": "大规模视觉搜索引擎有望同时解决两个问题：(i) 定位真正包含句子描述的对象的每张图像，以及 (ii) 在每次命中中识别对象的边界框或精确像素。现有技术只解决了这个挑战的一方面。视觉定位产生紧密的边界框和掩码，但它基于不切实际的假设，即对象存在于每张测试图像中，当应用于网络规模集合时会产生大量误报。文本到图像检索擅长筛选海量数据库以对相关图像进行排名，但它止步于整图匹配，并且不提供细粒度的定位。我们引入了指称搜索与发现 (ReSeDis)，这是第一个统一了语料库级检索与像素级定位的任务。给定一个自由形式的描述，ReSeDis模型必须决定查询的对象是否出现在每张图像中，如果是，则返回边界框或分割掩码。为了实现严谨的研究，我们策划了一个基准，其中每个描述都唯一映射到分散在大型、多样化语料库中的对象实例，从而消除了意外匹配。我们进一步设计了一个任务特定的度量标准，可以同时评估检索召回率和定位精度。最后，我们提供了一个使用冻结视觉-语言模型的直接零样本基线，揭示了未来研究的巨大潜力。ReSeDis为构建下一代鲁棒且可扩展的多模态搜索系统提供了一个现实、端到端的测试平台。", "summary": "本文介绍了ReSeDis（指称搜索与发现），这是一个新颖的任务和数据集，旨在解决现有大规模视觉搜索引擎在同时进行图像检索和细粒度对象定位方面的不足。ReSeDis任务要求模型根据自由形式的描述，判断对象是否存在于图像中并提供精确的边界框或分割掩码。为此，研究人员构建了一个大规模、多样化的基准数据集，并设计了一个联合评估检索召回率和定位精度的任务特定度量标准。通过提供一个零样本基线，ReSeDis为开发下一代鲁棒可扩展的多模态搜索系统提供了一个现实的端到端测试平台，并揭示了该领域的广阔研究前景。", "keywords": "指称搜索, 对象定位, 大规模图像检索, ReSeDis, 数据集", "comments": "ReSeDis的创新之处在于首次将大规模图像检索与像素级对象定位这两个传统上独立的问题结合起来，提出了一个统一的任务。这解决了现有技术在实际应用中（如网络规模搜索）的局限性。该数据集的设计考虑了唯一映射和多样性，有助于进行严谨的评估。它为未来多模态搜索系统的发展提供了一个重要的、更贴近实际的基准。"}}
{"id": "2506.14929", "title": "FedOne: Query-Efficient Federated Learning for Black-box Discrete Prompt Learning", "authors": ["Ganyu Wang", "Jinjie Fang", "Maxwell J. Ying", "Bin Gu", "Xi Chen", "Boyu Wang", "Charles Ling"], "summary": "Black-Box Discrete Prompt Learning is a prompt-tuning method that optimizes\ndiscrete prompts without accessing model parameters or gradients, making the\nprompt tuning on a cloud-based Large Language Model (LLM) feasible. Adapting\nfederated learning to BDPL could further enhance prompt tuning performance by\nleveraging data from diverse sources. However, all previous research on\nfederated black-box prompt tuning had neglected the substantial query cost\nassociated with the cloud-based LLM service. To address this gap, we conducted\na theoretical analysis of query efficiency within the context of federated\nblack-box prompt tuning. Our findings revealed that degrading FedAvg to\nactivate only one client per round, a strategy we called \\textit{FedOne},\nenabled optimal query efficiency in federated black-box prompt learning.\nBuilding on this insight, we proposed the FedOne framework, a federated\nblack-box discrete prompt learning method designed to maximize query efficiency\nwhen interacting with cloud-based LLMs. We conducted numerical experiments on\nvarious aspects of our framework, demonstrating a significant improvement in\nquery efficiency, which aligns with our theoretical results.", "comment": "Published in Proceedings of the 42nd International Conference on\n  Machine Learning", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14929v1", "AI": {"title_translation": "FedOne：面向黑盒离散提示学习的查询高效联邦学习", "tldr": "FedOne通过每轮只激活一个客户端，显著提高了联邦黑盒离散提示学习的查询效率。", "motivation": "所有以前关于联邦黑盒提示调优的研究都忽略了与基于云的大型语言模型（LLM）服务相关的巨大查询成本。", "method": "提出了FedOne框架，该框架基于理论分析，发现将FedAvg降级为每轮只激活一个客户端（即FedOne策略）可以实现联邦黑盒离散提示学习中的最优查询效率。", "result": "数值实验证明FedOne显著提高了查询效率，与理论结果一致。", "conclusion": "FedOne框架能够最大化与云端LLM交互时的查询效率，有效解决了联邦黑盒离散提示学习中的查询成本问题。", "translation": "黑盒离散提示学习是一种提示调优方法，它在不访问模型参数或梯度的情况下优化离散提示，这使得在基于云的大型语言模型（LLM）上进行提示调优成为可能。将联邦学习应用于BDPL可以利用来自不同来源的数据进一步提高提示调优性能。然而，所有以前关于联邦黑盒提示调优的研究都忽略了与基于云的LLM服务相关的巨大查询成本。为了解决这一差距，我们对联邦黑盒提示调优背景下的查询效率进行了理论分析。我们的发现表明，将FedAvg降级为每轮只激活一个客户端（我们称之为FedOne策略）可以在联邦黑盒离散提示学习中实现最优查询效率。基于这一见解，我们提出了FedOne框架，这是一种联邦黑盒离散提示学习方法，旨在最大化与基于云的LLM交互时的查询效率。我们对框架的各个方面进行了数值实验，结果表明查询效率显著提高，这与我们的理论结果一致。", "summary": "本文提出了FedOne框架，旨在解决联邦黑盒离散提示学习中与云端LLM交互时的高查询成本问题。通过理论分析发现，每轮仅激活一个客户端的策略（FedOne）能实现最优查询效率。实验结果证实了FedOne在提高查询效率方面的显著效果。", "keywords": "联邦学习, 黑盒离散提示学习, 查询效率, 大型语言模型, FedOne", "comments": "FedOne通过简化联邦学习的客户端激活机制，巧妙地解决了黑盒离散提示学习在云端LLM应用中的查询效率瓶颈，具有重要的实际应用价值。其创新点在于将理论分析与实际框架设计相结合，优化了资源消耗。"}}
{"id": "2506.15415", "title": "Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning", "authors": ["Stanley Ngugi"], "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet\ntheir performance in low-resource languages (LRLs), such as Swahili, often lags\ndue to data scarcity and underrepresentation in pre-training. A key challenge\nis achieving robust cross-lingual lexical alignment, crucial for tasks like\ntranslation and cross-lingual information retrieval. This paper introduces\nTargeted Lexical Injection (TLI), a novel and efficient fine-tuning approach.\nWe first demonstrate that Lugha-Llama-8B-wura, a Swahili-centric LLM, exhibits\nstrong, near-perfect lexical alignment for Swahili-English word pairs in its\nearly internal layers (specifically Layer 2, with ~0.99998 average cosine\nsimilarity based on a pilot study), a capability not fully reflected in its\nfinal output representations (baseline ~0.32 similarity on our evaluation set).\nTLI leverages this insight by using Low-Rank Adaptation (LoRA) and a\ncontrastive learning objective to fine-tune the model, specifically targeting\nembeddings from this empirically identified optimal early layer. Our\nexperiments show that TLI significantly improves the output-level lexical\nalignment for 623 trained Swahili-English word pairs, increasing average cosine\nsimilarity from 0.3211 to 0.4113 (+28.08%, p < 1.33 x 10^-240). More\nimportantly, these improvements generalize remarkably well to 63 unseen control\nword pairs, with similarity increasing from 0.3143 to 0.4033 (+28.32%, p < 7.17\nx 10^-27). These findings suggest TLI enhances the model's ability to preserve\nand propagate its inherent early-layer cross-lingual knowledge, offering a\nparameter-efficient and effective strategy for improving lexical alignment in\nLRL-focused LLMs.", "comment": "11 pages, 3 figures, 2 tables. Research on parameter-efficient\n  fine-tuning (PEFT) for low-resource languages (Swahili). Investigates\n  cross-lingual lexical alignment in Lugha-Llama using LoRA and contrastive\n  learning", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15415v1", "AI": {"title_translation": "靶向词汇注入：通过早期层LoRA微调解锁Lugha-Llama中潜在的跨语言对齐", "tldr": "本文提出了一种名为靶向词汇注入（TLI）的新型微调方法，通过利用Lugha-Llama模型早期层中已有的强大跨语言词汇对齐能力，显著提高了低资源语言（如斯瓦希里语）LLM的输出层词汇对齐，并且泛化能力强。", "motivation": "大型语言模型（LLMs）在低资源语言（LRLs）中的表现因数据稀缺和预训练不足而滞后。一个关键挑战是实现稳健的跨语言词汇对齐，这对于翻译和跨语言信息检索等任务至关重要。", "method": "本文提出了靶向词汇注入（TLI）方法。首先发现斯瓦希里语中心LLM Lugha-Llama-8B-wura在其早期内部层（特别是第2层）对斯瓦希里语-英语词对表现出接近完美的词汇对齐（平均余弦相似度约0.99998）。TLI利用低秩适应（LoRA）和对比学习目标对模型进行微调，专门针对从这个经验识别出的最佳早期层提取的嵌入。", "result": "TLI显著改善了623个训练过的斯瓦希里语-英语词对的输出级别词汇对齐，平均余弦相似度从0.3211提高到0.4113（+28.08%）。更重要的是，这些改进对63个未见的对照词对表现出卓越的泛化能力，相似度从0.3143提高到0.4033（+28.32%）。", "conclusion": "这些发现表明，TLI增强了模型保留和传播其固有的早期层跨语言知识的能力，为改善以LRL为中心的LLM中的词汇对齐提供了一种参数高效且有效的策略。", "translation": "大型语言模型（LLMs）展现出卓越的能力，然而，它们在低资源语言（LRLs），如斯瓦希里语中的表现，往往因数据稀缺和预训练不足而滞后。一个关键挑战是实现稳健的跨语言词汇对齐，这对于翻译和跨语言信息检索等任务至关重要。本文介绍了靶向词汇注入（TLI），一种新颖且高效的微调方法。我们首先证明，以斯瓦希里语为中心的LLM Lugha-Llama-8B-wura在其早期内部层（特别是第2层，根据一项初步研究，平均余弦相似度约为0.99998）中表现出强大、接近完美的斯瓦希里语-英语词对词汇对齐能力，但这种能力并未完全反映在其最终输出表示中（基线在我们的评估集上相似度约为0.32）。TLI利用这一洞察，通过使用低秩适应（LoRA）和对比学习目标对模型进行微调，专门针对从这个经验识别出的最佳早期层提取的嵌入。我们的实验表明，TLI显著改善了623个训练过的斯瓦希里语-英语词对的输出级别词汇对齐，平均余弦相似度从0.3211提高到0.4113（+28.08%，p < 1.33 x 10^-240）。更重要的是，这些改进对63个未见的对照词对表现出卓越的泛化能力，相似度从0.3143提高到0.4033（+28.32%，p < 7.17 x 10^-27）。这些发现表明，TLI增强了模型保留和传播其固有的早期层跨语言知识的能力，为改善以LRL为中心的LLM中的词汇对齐提供了一种参数高效且有效的策略。", "summary": "本文提出了一种名为靶向词汇注入（TLI）的新型高效微调方法，旨在解决大型语言模型在低资源语言中跨语言词汇对齐能力不足的问题。研究发现，尽管模型最终输出表现不佳，但Lugha-Llama模型在早期内部层已存在强大的斯瓦希里语-英语词汇对齐能力。TLI利用低秩适应（LoRA）和对比学习，专门针对这些早期层进行微调，显著提高了模型在训练和未见词对上的输出层词汇对齐效果，证明了其在增强低资源语言LLM固有跨语言知识方面的有效性和参数效率。", "keywords": "低资源语言, 跨语言对齐, LoRA, 微调, 大型语言模型", "comments": "该论文的创新点在于发现了LLM在早期层中潜在的、未被充分利用的跨语言对齐能力，并提出了一种参数高效的TLI方法来激活和利用这种能力。这对于提升低资源语言LLM的性能具有重要意义，因为它提供了一种无需大量额外数据即可改善跨语言任务表现的有效途径。"}}
{"id": "2506.15674", "title": "Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers", "authors": ["Tommaso Green", "Martin Gubri", "Haritz Puerto", "Sangdoo Yun", "Seong Joon Oh"], "summary": "We study privacy leakage in the reasoning traces of large reasoning models\nused as personal agents. Unlike final outputs, reasoning traces are often\nassumed to be internal and safe. We challenge this assumption by showing that\nreasoning traces frequently contain sensitive user data, which can be extracted\nvia prompt injections or accidentally leak into outputs. Through probing and\nagentic evaluations, we demonstrate that test-time compute approaches,\nparticularly increased reasoning steps, amplify such leakage. While increasing\nthe budget of those test-time compute approaches makes models more cautious in\ntheir final answers, it also leads them to reason more verbosely and leak more\nin their own thinking. This reveals a core tension: reasoning improves utility\nbut enlarges the privacy attack surface. We argue that safety efforts must\nextend to the model's internal thinking, not just its outputs.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15674v1", "AI": {"title_translation": "泄露的思想：大型推理模型并非私密思考者", "tldr": "大型推理模型在推理过程中会泄露敏感用户数据，增加推理步骤会加剧这种泄露，揭示了实用性与隐私之间的核心矛盾。", "motivation": "研究挑战了大型推理模型在作为个人代理时，其推理轨迹被认为是内部且安全的假设，指出这些轨迹可能包含敏感用户数据并被泄露。", "method": "通过探测（probing）和代理评估（agentic evaluations）来研究隐私泄露，并特别关注测试时计算方法，尤其是增加推理步骤的影响。", "result": "推理轨迹频繁包含敏感用户数据，可通过提示注入或意外泄露到输出中。测试时计算方法，特别是增加推理步骤，会放大这种泄露。虽然增加这些方法的预算能使模型在最终答案上更谨慎，但也会导致它们推理更冗长，并在思考过程中泄露更多信息。这揭示了推理提高实用性但扩大隐私攻击面的核心矛盾。", "conclusion": "安全工作必须扩展到模型的内部思考，而不仅仅是其最终输出。", "translation": "我们研究了大型推理模型作为个人代理时，其推理轨迹中的隐私泄露。与最终输出不同，推理轨迹通常被认为是内部和安全的。我们通过展示推理轨迹频繁包含敏感用户数据，可以通过提示注入或意外泄露到输出中来挑战这一假设。通过探测和代理评估，我们证明了测试时计算方法，特别是增加推理步骤，会放大这种泄露。虽然增加这些测试时计算方法的预算使模型在最终答案上更谨慎，但也会导致它们推理更冗长，并在自己的思考中泄露更多。这揭示了一个核心矛盾：推理提高了实用性，但扩大了隐私攻击面。我们认为安全工作必须扩展到模型的内部思考，而不仅仅是其输出。", "summary": "本研究探讨了大型推理模型在使用作为个人代理时，其内部推理轨迹的隐私泄露问题。研究挑战了推理轨迹是安全私密的假设，发现这些轨迹常包含敏感用户数据，且可通过提示注入或意外泄露。通过探测和代理评估，结果表明增加推理步骤等测试时计算方法会加剧数据泄露。尽管增加计算预算能使模型在最终输出上更谨慎，却也导致内部思考过程更冗长并泄露更多信息。这揭示了模型实用性提升与隐私攻击面扩大之间的矛盾，强调安全措施需涵盖模型的内部思考过程。", "keywords": "大型推理模型, 隐私泄露, 推理轨迹, 数据安全, 提示注入", "comments": "这篇论文创新性地挑战了大型推理模型内部推理轨迹的隐私安全性，揭示了一个先前未被充分关注的隐私泄露途径。其重要性在于指出了当前AI安全研究的盲点，即不应仅关注模型的最终输出，而应将安全努力延伸到模型的“思考”过程。研究揭示的实用性与隐私之间的核心矛盾，对未来大型模型的设计和部署提出了新的挑战，即如何在提升模型推理能力的同时，有效保护用户隐私，这对于AI伦理和负责任的AI发展具有深远影响。"}}
{"id": "2506.15666", "title": "Vision in Action: Learning Active Perception from Human Demonstrations", "authors": ["Haoyu Xiong", "Xiaomeng Xu", "Jimmy Wu", "Yifan Hou", "Jeannette Bohg", "Shuran Song"], "summary": "We present Vision in Action (ViA), an active perception system for bimanual\nrobot manipulation. ViA learns task-relevant active perceptual strategies\n(e.g., searching, tracking, and focusing) directly from human demonstrations.\nOn the hardware side, ViA employs a simple yet effective 6-DoF robotic neck to\nenable flexible, human-like head movements. To capture human active perception\nstrategies, we design a VR-based teleoperation interface that creates a shared\nobservation space between the robot and the human operator. To mitigate VR\nmotion sickness caused by latency in the robot's physical movements, the\ninterface uses an intermediate 3D scene representation, enabling real-time view\nrendering on the operator side while asynchronously updating the scene with the\nrobot's latest observations. Together, these design elements enable the\nlearning of robust visuomotor policies for three complex, multi-stage bimanual\nmanipulation tasks involving visual occlusions, significantly outperforming\nbaseline systems.", "comment": null, "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15666v1", "AI": {"title_translation": "行动中的视觉：从人类演示中学习主动感知", "tldr": "ViA是一个用于双臂机器人操作的主动感知系统，它通过人类演示学习感知策略，并利用一个6自由度机械颈和VR遥操作界面，在复杂双臂操作任务中表现出色。", "motivation": "为了使双臂机器人操作具备人类般的主动感知能力（如搜索、跟踪和聚焦），并解决现有系统在视觉遮挡等复杂多阶段任务中的不足。", "method": "本文提出了Vision in Action (ViA)系统。硬件方面，ViA采用一个简单有效的6自由度机械颈实现灵活的头部运动。为了捕捉人类主动感知策略，设计了一个基于VR的遥操作界面，在机器人和操作员之间创建共享观察空间。为缓解VR运动眩晕，该界面使用中间3D场景表示，实现操作员侧的实时视图渲染，同时异步更新机器人最新观察到的场景。", "result": "ViA系统使机器人能够学习鲁健的视觉运动策略，并在涉及视觉遮挡的三个复杂、多阶段双臂操作任务中，显著优于基线系统。", "conclusion": "通过从人类演示中学习主动感知策略并结合创新的硬件和VR遥操作界面设计，ViA系统能够显著提升双臂机器人在复杂操作任务中的表现。", "translation": "我们提出了“行动中的视觉”（ViA），一个用于双臂机器人操作的主动感知系统。ViA直接从人类演示中学习与任务相关的主动感知策略（例如，搜索、跟踪和聚焦）。在硬件方面，ViA采用一个简单而有效的6自由度机器人颈部，以实现灵活的、类似人类的头部运动。为了捕捉人类主动感知策略，我们设计了一个基于VR的遥操作界面，在机器人和人类操作员之间创建了一个共享的观察空间。为了缓解由机器人物理运动延迟引起的VR运动眩晕，该界面使用一个中间3D场景表示，从而在操作员侧实现实时视图渲染，同时异步更新机器人最新的观察结果。总而言之，这些设计元素使得机器人能够学习针对三个复杂、多阶段双臂操作任务的鲁棒视觉运动策略，这些任务涉及视觉遮挡，并且显著优于基线系统。", "summary": "本文介绍了Vision in Action (ViA)系统，一个用于双臂机器人操作的主动感知框架。ViA通过从人类演示中学习搜索、跟踪和聚焦等感知策略，使机器人能够执行复杂的双臂操作任务。该系统结合了创新的6自由度机器人颈部和基于VR的遥操作界面，该界面通过中间3D场景表示解决了VR延迟问题。实验结果表明，ViA在处理视觉遮挡的多阶段任务中，其视觉运动策略的鲁棒性显著优于现有基线系统。", "keywords": "主动感知, 机器人操作, 人类演示, 双臂机器人, VR遥操作", "comments": "ViA的创新之处在于其直接从人类演示中学习主动感知策略的能力，并结合了新颖的硬件设计（6-DoF机械颈）和VR遥操作界面，有效解决了传统机器人感知和操作中的视觉遮挡问题。特别是VR界面中3D场景表示的应用，巧妙地缓解了运动眩晕，提升了遥操作的可用性，对未来机器人学习和人机协作领域具有重要意义。"}}
{"id": "2506.15200", "title": "Conquering the Retina: Bringing Visual in-Context Learning to OCT", "authors": ["Alessio Negrini", "Simon Reiß"], "summary": "Recent advancements in medical image analysis have led to the development of\nhighly specialized models tailored to specific clinical tasks. These models\nhave demonstrated exceptional performance and remain a crucial research\ndirection. Yet, their applicability is limited to predefined tasks, requiring\nexpertise and extensive resources for development and adaptation. In contrast,\ngeneralist models offer a different form of utility: allowing medical\npractitioners to define tasks on the fly without the need for task-specific\nmodel development. In this work, we explore how to train generalist models for\nthe domain of retinal optical coherence tomography using visual in-context\nlearning (VICL), i.e., training models to generalize across tasks based on a\nfew examples provided at inference time. To facilitate rigorous assessment, we\npropose a broad evaluation protocol tailored to VICL in OCT. We extensively\nevaluate a state-of-the-art medical VICL approach on multiple retinal OCT\ndatasets, establishing a first baseline to highlight the potential and current\nlimitations of in-context learning for OCT. To foster further research and\npractical adoption, we openly release our code.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15200v1", "AI": {"title_translation": "征服视网膜：将视觉上下文学习引入OCT", "tldr": "本文探索了如何使用视觉上下文学习（VICL）为视网膜光学相干断层扫描（OCT）领域训练通用模型，并建立了一个初步基线来评估其潜力与局限性。", "motivation": "现有的医学图像分析模型高度专业化，但其适用性受限于预定义任务，开发和适应需要大量资源。相比之下，通用模型允许医生即时定义任务，无需开发特定任务模型。本文旨在解决这一局限性，探索通用模型在OCT领域的应用。", "method": "作者通过视觉上下文学习（VICL）训练通用模型，使其能够根据推理时提供的少量示例来泛化不同任务。他们提出了一个针对OCT中VICL的广泛评估协议，并在一系列视网膜OCT数据集上评估了一种最先进的医学VICL方法。他们还开源了代码。", "result": "研究在多个视网膜OCT数据集上建立了一个初步基线，突出了上下文学习在OCT领域的潜力和当前局限性。", "conclusion": "视觉上下文学习在视网膜OCT领域具有巨大潜力，但仍存在局限性，需要进一步研究和实际应用。", "translation": "近期医学图像分析的进展催生了针对特定临床任务的高度专业化模型。这些模型表现出卓越的性能，并仍然是一个重要的研究方向。然而，它们的适用性仅限于预定义任务，其开发和适应需要专业知识和大量资源。相比之下，通用模型提供了另一种形式的实用性：允许医疗从业者即时定义任务，而无需开发特定任务的模型。在这项工作中，我们探索了如何使用视觉上下文学习（VICL），即训练模型根据推理时提供的少量示例来泛化不同任务，为视网膜光学相干断层扫描（OCT）领域训练通用模型。为了促进严格评估，我们提出了一种针对OCT中VICL的广泛评估协议。我们广泛评估了一种最先进的医学VICL方法在多个视网膜OCT数据集上的性能，建立了一个初步基线，以突出上下文学习在OCT领域的潜力和当前局限性。为了促进进一步的研究和实际应用，我们公开了我们的代码。", "summary": "本文旨在克服传统医学图像分析模型在任务特定性上的局限，提出将视觉上下文学习（VICL）应用于视网膜光学相干断层扫描（OCT）领域，以开发能够即时适应新任务的通用模型。研究构建了一套针对OCT中VICL的评估协议，并在多个数据集上对现有VICL方法进行了广泛评估，首次建立了该领域的基线，揭示了VICL在OCT应用的潜力和当前挑战。为推动后续研究，作者已公开代码。", "keywords": "视觉上下文学习, 光学相干断层扫描, 通用模型, 医学图像分析, 视网膜", "comments": "本文的创新之处在于将视觉上下文学习（VICL）这一新兴范式引入到医学图像分析的特定领域——视网膜OCT，以应对传统专业化模型在任务适应性上的局限。通过提出VICL在OCT领域的评估协议并建立初步基线，为该方向的未来研究奠定了基础。公开代码有助于促进社区的进一步探索和实际应用。"}}
{"id": "2506.15425", "title": "Understanding GUI Agent Localization Biases through Logit Sharpness", "authors": ["Xingjian Tao", "Yiwei Wang", "Yujun Cai", "Zhicheng Yang", "Jing Tang"], "summary": "Multimodal large language models (MLLMs) have enabled GUI agents to interact\nwith operating systems by grounding language into spatial actions. Despite\ntheir promising performance, these models frequently exhibit\nhallucinations-systematic localization errors that compromise reliability. We\npropose a fine-grained evaluation framework that categorizes model predictions\ninto four distinct types, revealing nuanced failure modes beyond traditional\naccuracy metrics. To better quantify model uncertainty, we introduce the Peak\nSharpness Score (PSS), a metric that evaluates the alignment between semantic\ncontinuity and logits distribution in coordinate prediction. Building on this\ninsight, we further propose Context-Aware Cropping, a training-free technique\nthat improves model performance by adaptively refining input context. Extensive\nexperiments demonstrate that our framework and methods provide actionable\ninsights and enhance the interpretability and robustness of GUI agent behavior.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15425v1", "AI": {"title_translation": "通过Logit Sharpness理解GUI代理定位偏差", "tldr": "本文提出了一个精细的评估框架、一种不确定性量化指标（PSS）和一种训练无关的上下文裁剪技术，以解决GUI代理的定位错误并提高其鲁棒性。", "motivation": "尽管多模态大型语言模型（MLLMs）驱动的GUI代理表现出潜力，但它们经常出现系统性的定位错误（幻觉），这损害了可靠性。", "method": "提出一个精细的评估框架，将模型预测分为四种类型，以揭示传统准确度指标之外的细微故障模式。引入峰值尖锐度分数（PSS）来量化模型不确定性，该分数评估语义连续性与坐标预测中logits分布的一致性。进一步提出上下文感知裁剪（Context-Aware Cropping），一种无需训练的技术，通过自适应地优化输入上下文来提高模型性能。", "result": "广泛的实验表明，所提出的框架和方法提供了可操作的见解，并增强了GUI代理行为的可解释性和鲁棒性。", "conclusion": "所提出的评估框架、不确定性量化指标（PSS）和上下文感知裁剪技术能有效解决GUI代理的定位偏差问题，提高其可靠性、可解释性和鲁棒性。", "translation": "多模态大型语言模型（MLLMs）使GUI代理能够通过将语言映射到空间动作来与操作系统进行交互。尽管它们表现出良好的性能，但这些模型经常出现幻觉——系统性的定位错误，从而损害了可靠性。我们提出了一个精细的评估框架，将模型预测分为四种不同的类型，揭示了超越传统准确度指标的细微故障模式。为了更好地量化模型不确定性，我们引入了峰值尖锐度分数（PSS），这是一个评估语义连续性与坐标预测中logits分布之间对齐程度的指标。在此基础上，我们进一步提出了上下文感知裁剪（Context-Aware Cropping），这是一种无需训练的技术，通过自适应地优化输入上下文来提高模型性能。广泛的实验表明，我们的框架和方法提供了可操作的见解，并增强了GUI代理行为的可解释性和鲁棒性。", "summary": "本文针对多模态大型语言模型（MLLMs）驱动的GUI代理在定位中存在的系统性错误问题，提出了一个精细的评估框架来识别细微故障模式。为量化模型不确定性，引入了峰值尖锐度分数（PSS）。此外，还提出了一种无需训练的上下文感知裁剪技术来改进模型性能。实验证明，这些方法提高了GUI代理行为的可解释性和鲁棒性。", "keywords": "GUI代理, 定位偏差, Logit Sharpness, MLLMs, 评估框架", "comments": "本文的创新之处在于提出了一个多维度的评估框架来分析GUI代理的定位偏差，并引入了PSS来量化不确定性，这比传统精度指标更具洞察力。此外，训练无关的上下文感知裁剪技术为提升模型性能提供了一种实用且高效的途径。这些贡献对于提高GUI代理的可靠性和可解释性具有重要意义。"}}
{"id": "2506.15680", "title": "Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos", "authors": ["Kaifeng Zhang", "Baoyu Li", "Kris Hauser", "Yunzhu Li"], "summary": "Modeling the dynamics of deformable objects is challenging due to their\ndiverse physical properties and the difficulty of estimating states from\nlimited visual information. We address these challenges with a neural dynamics\nframework that combines object particles and spatial grids in a hybrid\nrepresentation. Our particle-grid model captures global shape and motion\ninformation while predicting dense particle movements, enabling the modeling of\nobjects with varied shapes and materials. Particles represent object shapes,\nwhile the spatial grid discretizes the 3D space to ensure spatial continuity\nand enhance learning efficiency. Coupled with Gaussian Splattings for visual\nrendering, our framework achieves a fully learning-based digital twin of\ndeformable objects and generates 3D action-conditioned videos. Through\nexperiments, we demonstrate that our model learns the dynamics of diverse\nobjects -- such as ropes, cloths, stuffed animals, and paper bags -- from\nsparse-view RGB-D recordings of robot-object interactions, while also\ngeneralizing at the category level to unseen instances. Our approach\noutperforms state-of-the-art learning-based and physics-based simulators,\nparticularly in scenarios with limited camera views. Furthermore, we showcase\nthe utility of our learned models in model-based planning, enabling\ngoal-conditioned object manipulation across a range of tasks. The project page\nis available at https://kywind.github.io/pgnd .", "comment": "Project page: https://kywind.github.io/pgnd", "cate": "cs.RO", "url": "http://arxiv.org/abs/2506.15680v1", "AI": {"title_translation": "粒子-网格神经动力学用于从RGB-D视频中学习可变形物体模型", "tldr": "提出一种结合粒子和网格的神经动力学框架，能高效学习可变形物体的动态模型，并支持机器人操作。", "motivation": "由于可变形物体物理特性多样且难以从有限视觉信息中估计状态，因此对其动态建模具有挑战性。", "method": "本文提出了一种粒子-网格神经动力学框架，采用混合表示结合物体粒子和空间网格。粒子用于表示物体形状，空间网格用于离散化3D空间以确保空间连续性并提高学习效率。该框架结合高斯溅射进行视觉渲染，从而实现可变形物体的全学习数字孪生。", "result": "该模型能够从稀疏视图RGB-D记录中学习绳索、布料、毛绒玩具和纸袋等多种物体的动态，并在类别级别泛化到未见实例。实验证明，该方法优于最先进的基于学习和基于物理的模拟器，尤其是在有限相机视图场景下。此外，学习到的模型可用于模型基规划，实现目标导向的物体操作。", "conclusion": "该粒子-网格神经动力学框架有效解决了可变形物体动态建模的挑战，并为机器人操作提供了强大的工具。", "translation": "由于可变形物体多样化的物理特性以及从有限视觉信息中估计状态的困难，对其动态建模具有挑战性。我们通过一个结合物体粒子和空间网格的混合表示的神经动力学框架来解决这些挑战。我们的粒子-网格模型捕获全局形状和运动信息，同时预测密集的粒子运动，从而能够对具有不同形状和材料的物体进行建模。粒子代表物体形状，而空间网格将3D空间离散化以确保空间连续性并提高学习效率。结合高斯溅射进行视觉渲染，我们的框架实现了可变形物体的完全基于学习的数字孪生，并生成3D动作条件视频。通过实验，我们证明了我们的模型可以从机器人-物体交互的稀疏视图RGB-D记录中学习各种物体——如绳索、布料、毛绒玩具和纸袋——的动态，同时在类别级别泛化到未见实例。我们的方法优于最先进的基于学习和基于物理的模拟器，特别是在有限相机视图的场景中。此外，我们展示了我们学习到的模型在基于模型的规划中的效用，从而在各种任务中实现目标条件下的物体操作。项目页面可在https://kywind.github.io/pgnd 获取。", "summary": "本文提出了一种名为粒子-网格神经动力学（PGND）的新框架，用于从RGB-D视频中学习可变形物体的动态模型。该框架结合了粒子表示物体形状和空间网格确保连续性与效率，并利用高斯溅射进行渲染，实现了可变形物体的数字孪生。实验证明PGND能有效学习多种物体的动态，在有限视图下优于现有方法，并可应用于机器人规划和操作。", "keywords": "可变形物体建模, 神经动力学, 粒子-网格表示, RGB-D视频, 机器人操作", "comments": "该研究的创新之处在于其混合表示（粒子-网格）和端到端学习框架，有效解决了可变形物体建模的复杂性。其在稀疏视图下的优越性能和在机器人操作中的应用潜力，显示了其重要性。"}}
{"id": "2506.15201", "title": "Privacy-Shielded Image Compression: Defending Against Exploitation from Vision-Language Pretrained Models", "authors": ["Xuelin Shen", "Jiayin Xu", "Kangsheng Yin", "Wenhan Yang"], "summary": "The improved semantic understanding of vision-language pretrained (VLP)\nmodels has made it increasingly difficult to protect publicly posted images\nfrom being exploited by search engines and other similar tools. In this\ncontext, this paper seeks to protect users' privacy by implementing defenses at\nthe image compression stage to prevent exploitation. Specifically, we propose a\nflexible coding method, termed Privacy-Shielded Image Compression (PSIC), that\ncan produce bitstreams with multiple decoding options. By default, the\nbitstream is decoded to preserve satisfactory perceptual quality while\npreventing interpretation by VLP models. Our method also retains the original\nimage compression functionality. With a customizable input condition, the\nproposed scheme can reconstruct the image that preserves its full semantic\ninformation. A Conditional Latent Trigger Generation (CLTG) module is proposed\nto produce bias information based on customizable conditions to guide the\ndecoding process into different reconstructed versions, and an\nUncertainty-Aware Encryption-Oriented (UAEO) optimization function is designed\nto leverage the soft labels inferred from the target VLP model's uncertainty on\nthe training data. This paper further incorporates an adaptive multi-objective\noptimization strategy to obtain improved encrypting performance and perceptual\nquality simultaneously within a unified training process. The proposed scheme\nis plug-and-play and can be seamlessly integrated into most existing Learned\nImage Compression (LIC) models. Extensive experiments across multiple\ndownstream tasks have demonstrated the effectiveness of our design.", "comment": "11 pages, 6 figures, publised to ICML 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15201v1", "AI": {"title_translation": "隐私保护图像压缩：防御视觉-语言预训练模型的利用", "tldr": "提出一种名为PSIC的图像压缩方法，能在保持图像质量的同时，阻止VLP模型理解图像语义，并支持按需恢复完整语义信息。", "motivation": "视觉-语言预训练（VLP）模型日益增强的语义理解能力使得公开图像容易被搜索引擎等工具利用，侵犯用户隐私。", "method": "本文提出隐私保护图像压缩（PSIC）方法，这是一种灵活的编码方法，能生成具有多重解码选项的比特流。默认解码保持感知质量并阻止VLP模型解释。通过可定制的输入条件，方案可重建完整语义信息的图像。引入条件潜在触发生成（CLTG）模块生成偏差信息以指导解码，并设计不确定性感知加密导向（UAEO）优化函数利用目标VLP模型的不确定性。还整合了自适应多目标优化策略。该方案即插即用，可集成到现有LIC模型中。", "result": "广泛的实验证明了该设计的有效性。", "conclusion": "该方法能够有效地在图像压缩阶段保护用户隐私，同时保持图像质量和原始压缩功能，并通过灵活的机制支持语义信息的按需恢复。", "translation": "视觉-语言预训练（VLP）模型语义理解能力的提升，使得保护公开图片不被搜索引擎及其他类似工具利用变得越来越困难。在此背景下，本文旨在通过在图像压缩阶段实施防御来防止图片被利用，从而保护用户隐私。具体来说，我们提出了一种灵活的编码方法，称为隐私保护图像压缩（PSIC），它可以生成具有多种解码选项的比特流。默认情况下，比特流被解码以保持令人满意的感知质量，同时阻止VLP模型进行解释。我们的方法还保留了原始的图像压缩功能。通过可定制的输入条件，所提出的方案可以重建保留其完整语义信息的图像。提出了一种条件潜在触发生成（CLTG）模块，根据可定制的条件生成偏差信息，以引导解码过程生成不同的重建版本；并设计了一种不确定性感知加密导向（UAEO）优化函数，利用从目标VLP模型在训练数据上的不确定性中推断出的软标签。本文进一步结合了自适应多目标优化策略，在统一的训练过程中同时获得改进的加密性能和感知质量。所提出的方案是即插即用的，可以无缝集成到大多数现有的学习图像压缩（LIC）模型中。对多个下游任务的广泛实验证明了我们设计的有效性。", "summary": "本文提出一种名为隐私保护图像压缩（PSIC）的新型图像压缩方法，旨在防御视觉-语言预训练（VLP）模型对公开图像的语义利用。PSIC能够生成多解码选项的比特流：默认解码在保持良好感知质量的同时阻止VLP模型理解图像语义；通过特定条件，可重建出包含完整语义信息的图像。该方法引入了条件潜在触发生成（CLTG）模块和不确定性感知加密导向（UAEO）优化函数，并采用自适应多目标优化策略。PSIC易于集成到现有学习图像压缩模型中，实验证明其有效性。", "keywords": "隐私保护, 图像压缩, 视觉-语言模型, 语义理解, 学习图像压缩", "comments": "该论文提出了一种创新的方法，在图像压缩层面解决了VLP模型带来的隐私泄露风险。其即插即用的特性和多解码选项的设计，使其在实际应用中具有较高的灵活性和实用性。特别是通过引导解码过程和利用VLP模型的不确定性来达到隐私保护目的，展现了巧妙的设计。"}}
{"id": "2506.15451", "title": "AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent System Need", "authors": ["Zhouhong Gu", "Xiaoxuan Zhu", "Yin Cai", "Hao Shen", "Xingzhou Chen", "Qingyi Wang", "Jialin Li", "Xiaoran Shi", "Haoran Guo", "Wenxuan Huang", "Hongwei Feng", "Yanghua Xiao", "Zheyu Ye", "Yao Hu", "Shaosheng Cao"], "summary": "Large language model based multi-agent systems have demonstrated significant\npotential in social simulation and complex task resolution domains. However,\ncurrent frameworks face critical challenges in system architecture design,\ncross-domain generalizability, and performance guarantees, particularly as task\ncomplexity and number of agents increases. We introduces AgentGroupChat-V2, a\nnovel framework addressing these challenges through three core innovations: (1)\na divide-and-conquer fully parallel architecture that decomposes user queries\ninto hierarchical task forest structures enabling dependency management and\ndistributed concurrent processing. (2) an adaptive collaboration engine that\ndynamically selects heterogeneous LLM combinations and interaction modes based\non task characteristics. (3) agent organization optimization strategies\ncombining divide-and-conquer approaches for efficient problem decomposition.\nExtensive experiments demonstrate AgentGroupChat-V2's superior performance\nacross diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best\nbaseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME\n(nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance\nadvantages become increasingly pronounced with higher task difficulty,\nparticularly on Level 5 MATH problems where improvements exceed 11 percentage\npoints compared to state-of-the-art baselines. These results confirm that\nAgentGroupChat-V2 provides a comprehensive solution for building efficient,\ngeneral-purpose LLM multi-agent systems with significant advantages in complex\nreasoning scenarios. Code is available at\nhttps://github.com/MikeGu721/AgentGroupChat-V2.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15451v1", "AI": {"title_translation": "AgentGroupChat-V2：分而治之是基于LLM的多智能体系统所需", "tldr": "AgentGroupChat-V2是一个新的LLM多智能体系统框架，通过分而治之的并行架构、自适应协作引擎和优化策略，显著提升了复杂任务处理和跨领域泛化能力。", "motivation": "当前基于LLM的多智能体系统在系统架构设计、跨领域泛化能力和性能保障方面面临挑战，尤其是在任务复杂性和智能体数量增加时。", "method": "提出AgentGroupChat-V2框架，包含三项核心创新：1. 分而治之的完全并行架构，将用户查询分解为分层任务森林结构，实现依赖管理和分布式并发处理。2. 自适应协作引擎，根据任务特性动态选择异构LLM组合和交互模式。3. 结合分而治之方法的智能体组织优化策略，实现高效问题分解。", "result": "在GSM8K上达到91.50%的准确率（超过最佳基线5.6个百分点）；在竞赛级AIME上达到30.4%的准确率（几乎是其他方法的两倍）；在HumanEval上达到79.20%的pass@1。性能优势随任务难度增加而愈发显著，尤其在Level 5 MATH问题上比SOTA基线提升超过11个百分点。", "conclusion": "AgentGroupChat-V2为构建高效、通用LLM多智能体系统提供了全面解决方案，在复杂推理场景中具有显著优势。", "translation": "基于大型语言模型的多智能体系统在社会模拟和复杂任务解决领域展现出巨大潜力。然而，当前框架在系统架构设计、跨领域泛化能力和性能保障方面面临严峻挑战，尤其是在任务复杂性和智能体数量增加时。我们引入了AgentGroupChat-V2，这是一个新颖的框架，通过三项核心创新来解决这些挑战：（1）一种分而治之的完全并行架构，将用户查询分解为分层任务森林结构，从而实现依赖管理和分布式并发处理。（2）一个自适应协作引擎，根据任务特性动态选择异构LLM组合和交互模式。（3）结合分而治之方法的智能体组织优化策略，以实现高效的问题分解。广泛的实验表明，AgentGroupChat-V2在不同领域表现出卓越的性能，在GSM8K上达到91.50%的准确率（超过最佳基线5.6个百分点），在竞赛级AIME上达到30.4%的准确率（几乎是其他方法的两倍），在HumanEval上达到79.20%的pass@1。性能优势随任务难度增加而愈发显著，尤其在Level 5 MATH问题上，与最先进的基线相比，提升超过11个百分点。这些结果证实AgentGroupChat-V2为构建高效、通用LLM多智能体系统提供了一个全面的解决方案，在复杂推理场景中具有显著优势。代码可在https://github.com/MikeGu721/AgentGroupChat-V2获取。", "summary": "AgentGroupChat-V2是一个创新的基于LLM的多智能体系统框架，旨在解决现有系统在架构、泛化性和性能方面的挑战。它通过引入分而治之的并行架构、自适应协作引擎和智能体组织优化策略，实现了对复杂任务的高效分解、并发处理和动态协作。实验结果表明，AgentGroupChat-V2在数学问题解决和代码生成等多个基准测试中显著超越了现有方法，尤其在难度较高的任务上表现出更强的优势，证明了其在复杂推理场景中的卓越能力和通用性。", "keywords": "LLM多智能体系统, 分而治之, 并行架构, 任务分解, 自适应协作", "comments": "该论文提出AgentGroupChat-V2框架，通过其独特的分而治之并行架构和自适应协作引擎，有效解决了LLM多智能体系统在处理复杂任务时的可扩展性和性能瓶颈。其创新点在于将任务分解为可并发处理的子任务，并能动态选择最佳的LLM组合，这对于提升多智能体系统的效率和泛化能力具有重要意义。实验结果在多个高难度基准测试上表现出色，尤其在数学推理和代码生成方面取得了显著进步，表明了该方法在实际复杂推理场景中的巨大潜力。"}}
{"id": "2506.15218", "title": "DM-FNet: Unified multimodal medical image fusion via diffusion process-trained encoder-decoder", "authors": ["Dan He", "Weisheng Li", "Guofen Wang", "Yuping Huang", "Shiqiang Liu"], "summary": "Multimodal medical image fusion (MMIF) extracts the most meaningful\ninformation from multiple source images, enabling a more comprehensive and\naccurate diagnosis. Achieving high-quality fusion results requires a careful\nbalance of brightness, color, contrast, and detail; this ensures that the fused\nimages effectively display relevant anatomical structures and reflect the\nfunctional status of the tissues. However, existing MMIF methods have limited\ncapacity to capture detailed features during conventional training and suffer\nfrom insufficient cross-modal feature interaction, leading to suboptimal fused\nimage quality. To address these issues, this study proposes a two-stage\ndiffusion model-based fusion network (DM-FNet) to achieve unified MMIF. In\nStage I, a diffusion process trains UNet for image reconstruction. UNet\ncaptures detailed information through progressive denoising and represents\nmultilevel data, providing a rich set of feature representations for the\nsubsequent fusion network. In Stage II, noisy images at various steps are input\ninto the fusion network to enhance the model's feature recognition capability.\nThree key fusion modules are also integrated to process medical images from\ndifferent modalities adaptively. Ultimately, the robust network structure and a\nhybrid loss function are integrated to harmonize the fused image's brightness,\ncolor, contrast, and detail, enhancing its quality and information density. The\nexperimental results across various medical image types demonstrate that the\nproposed method performs exceptionally well regarding objective evaluation\nmetrics. The fused image preserves appropriate brightness, a comprehensive\ndistribution of radioactive tracers, rich textures, and clear edges. The code\nis available at https://github.com/HeDan-11/DM-FNet.", "comment": "This paper has been accepted by IEEE Transactions on Multimedia (TMM)\n  in March 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15218v1", "AI": {"title_translation": "DM-FNet：基于扩散过程训练的编码器-解码器统一多模态医学图像融合", "tldr": "本文提出DM-FNet，一个基于扩散模型的两阶段网络，用于统一多模态医学图像融合，解决了现有方法在特征捕获和跨模态交互方面的不足，并显著提升了融合图像质量。", "motivation": "现有多模态医学图像融合（MMIF）方法在常规训练中捕获细节特征的能力有限，并且跨模态特征交互不足，导致融合图像质量不佳，无法有效平衡亮度、颜色、对比度和细节。", "method": "提出DM-FNet，一个两阶段的基于扩散模型的融合网络。第一阶段，使用扩散过程训练UNet进行图像重建，以捕获详细信息并提供丰富的特征表示。第二阶段，将不同步骤的噪声图像输入融合网络，以增强特征识别能力，并集成三个关键融合模块以自适应处理不同模态的医学图像。最后，结合鲁棒的网络结构和混合损失函数来协调融合图像的亮度、颜色、对比度和细节。", "result": "该方法在各种医学图像类型上的实验结果表明，在客观评估指标方面表现出色。融合图像保留了适当的亮度、放射性示踪剂的全面分布、丰富的纹理和清晰的边缘。", "conclusion": "提出的DM-FNet通过其两阶段扩散模型训练和融合策略，有效解决了现有MMIF方法的局限性，显著提升了融合图像的质量和信息密度，实现了统一的多模态医学图像融合。", "translation": "多模态医学图像融合（MMIF）从多个源图像中提取最有意义的信息，从而实现更全面、更准确的诊断。实现高质量的融合结果需要仔细平衡亮度、颜色、对比度和细节；这确保了融合图像能够有效地显示相关的解剖结构并反映组织的功​​能状态。然而，现有的MMIF方法在常规训练中捕获细节特征的能力有限，并且跨模态特征交互不足，导致融合图像质量不佳。为了解决这些问题，本研究提出了一种两阶段的基于扩散模型的融合网络（DM-FNet），以实现统一的MMIF。在第一阶段，扩散过程训练UNet进行图像重建。UNet通过渐进式去噪捕获详细信息并表示多级数据，为后续的融合网络提供丰富的特征表示。在第二阶段，将不同步骤的噪声图像输入融合网络，以增强模型的特征识别能力。同时，还集成了三个关键融合模块，以自适应地处理来自不同模态的医学图像。最终，鲁棒的网络结构和混合损失函数被整合，以协调融合图像的亮度、颜色、对比度和细节，从而提高其质量和信息密度。在各种医学图像类型上的实验结果表明，所提出的方法在客观评估指标方面表现出色。融合图像保留了适当的亮度、放射性示踪剂的全面分布、丰富的纹理和清晰的边缘。代码可在https://github.com/HeDan-11/DM-FNet获取。", "summary": "本文提出DM-FNet，一个基于扩散模型的两阶段网络，用于统一多模态医学图像融合（MMIF）。该方法旨在克服现有MMIF在特征捕获和跨模态交互方面的不足。第一阶段利用扩散过程训练UNet进行图像重建，以获取丰富的多级特征。第二阶段通过输入不同噪声步长的图像和集成关键融合模块来增强特征识别和自适应处理能力。结合鲁棒的网络结构和混合损失函数，DM-FNet能有效平衡融合图像的亮度、颜色、对比度和细节。实验证明，该方法在客观指标上表现优异，能生成高质量、信息丰富的融合图像。", "keywords": "多模态医学图像融合, 扩散模型, 深度学习, 图像融合, DM-FNet", "comments": "该研究的创新点在于将扩散模型引入多模态医学图像融合领域，特别是其两阶段训练策略，通过扩散过程训练UNet来增强特征表示能力，有效解决了现有方法在细节捕获和跨模态特征交互上的不足。这种方法有望显著提升医学诊断的准确性和全面性，具有重要的临床应用潜力。"}}
{"id": "2506.14965", "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective", "authors": ["Zhoujun Cheng", "Shibo Hao", "Tianyang Liu", "Fan Zhou", "Yutao Xie", "Feng Yao", "Yuexin Bian", "Yonghao Zhuang", "Nilabjo Dey", "Yuheng Zha", "Yi Gu", "Kun Zhou", "Yuqi Wang", "Yuan Li", "Richard Fan", "Jianshu She", "Chengqian Gao", "Abulhair Saparov", "Haonan Li", "Taylor W. Killian", "Mikhail Yurochkin", "Zhengzhong Liu", "Eric P. Xing", "Zhiting Hu"], "summary": "Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360", "comment": "38 pages, 9 figures. Under review", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14965v1", "AI": {"title_translation": "从跨领域视角重新审视用于大型语言模型推理的强化学习", "tldr": "本文引入了跨领域强化学习推理语料库Guru，发现RL对LLM推理的提升效果因领域而异，并提出RL可能促进真正的技能习得，同时Guru模型在多领域推理任务上达到了SOTA性能。", "motivation": "当前强化学习（RL）在提升大型语言模型（LLM）推理能力方面的研究主要集中在数学和代码领域，这限制了对其在通用推理中更广泛适用性的理解。此外，跨不同推理领域缺乏可靠、可扩展的RL奖励信号是一个关键挑战。", "method": "1. 构建了一个名为Guru的精选RL推理语料库，包含92K个可验证的示例，涵盖数学、代码、科学、逻辑、模拟和表格六个推理领域，通过领域特定奖励设计、去重和过滤确保其可靠性和有效性。 2. 基于Guru语料库，系统地重新审视了LLM推理中RL的既定发现。 3. 训练并提出了Guru-7B和Guru-32B两个模型。", "result": "1. 观察到RL对LLM推理的提升效果在不同领域之间存在显著差异。 2. 发现预训练中常见领域（数学、代码、科学）容易受益于跨领域RL训练，而预训练中暴露较少领域（逻辑、模拟、表格）需要领域内训练才能获得显著性能提升。 3. Guru-7B和Guru-32B模型在公开模型中，使用公开数据RL训练后达到了最先进的性能，在17个任务评估套件上分别比最佳基线高出7.9%和6.7%。 4. 模型有效提高了其基础模型的Pass@k性能，尤其是在预训练数据中不太可能出现的复杂任务上。", "conclusion": "强化学习可能不仅是激发预训练模型中现有知识，更可能促进大型语言模型真正技能的习得，尤其是在预训练中不常见的领域。通过构建高质量的跨领域RL语料库和训练模型，可以显著提升LLM的通用推理能力。", "translation": "强化学习（RL）已成为改善大型语言模型（LLM）推理的一种有前景的方法，但大多数开放性工作仅狭隘地关注数学和代码，限制了我们对其在通用推理中更广泛适用性的理解。一个关键挑战在于缺乏跨不同推理领域的可靠、可扩展的RL奖励信号。我们引入了Guru，这是一个精选的RL推理语料库，包含92K个可验证的示例，涵盖数学、代码、科学、逻辑、模拟和表格六个推理领域——每个领域都通过领域特定的奖励设计、去重和过滤来构建，以确保RL训练的可靠性和有效性。基于Guru，我们系统地重新审视了LLM推理中RL的既定发现，并观察到跨领域的显著差异。例如，虽然先前的工作表明RL主要从预训练模型中激发现有知识，但我们的结果揭示了一种更细致的模式：预训练期间经常出现的领域（数学、代码、科学）容易受益于跨领域RL训练，而预训练暴露有限的领域（逻辑、模拟和表格）需要领域内训练才能获得有意义的性能提升，这表明RL可能促进真正的技能习得。最后，我们提出了Guru-7B和Guru-32B，这两个模型在开放模型中使用公开可用数据进行RL训练后，实现了最先进的性能，在我们的17任务评估套件上，跨六个推理领域分别超越最佳基线7.9%和6.7%。我们还表明，我们的模型有效提高了其基础模型的Pass@k性能，特别是在预训练数据中不太可能出现的复杂任务上。我们发布了数据、模型、训练和评估代码，以促进通用推理，网址为：https://github.com/LLM360/Reasoning360。", "summary": "本文旨在解决强化学习（RL）在大型语言模型（LLM）通用推理应用中领域局限性和奖励信号缺乏的问题。为此，作者构建了名为Guru的92K跨领域RL推理语料库，涵盖数学、代码、科学、逻辑、模拟和表格六大领域。基于此语料库，研究发现RL对LLM推理的提升效果因领域而异：预训练中常见的领域（如数学、代码）易受益于跨领域RL训练，而预训练中不常见的领域（如逻辑、模拟）则需领域内训练，这暗示RL可能促进LLM真正的技能习得。最终，训练出的Guru-7B和Guru-32B模型在多领域推理任务上取得了SOTA性能，并提高了复杂任务的Pass@k表现。", "keywords": "强化学习, 大型语言模型, 推理, 跨领域, 语料库", "comments": "这篇论文通过构建一个大规模、高质量的跨领域RL推理语料库Guru，填补了当前RL在LLM通用推理应用中数据稀缺和领域局限的空白。其创新之处在于系统地分析了RL对LLM推理能力影响的领域差异，并提出了RL可能促进“真正技能习得”而非仅仅“激发现有知识”的见解，这对于理解RL在LLM训练中的深层作用具有重要意义。此外，开源数据和模型也为未来的研究提供了宝贵资源。"}}
{"id": "2506.15455", "title": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation", "authors": ["Xinnuo Xu", "Rachel Lawrence", "Kshitij Dubey", "Atharva Pandey", "Risa Ueno", "Fabian Falck", "Aditya V. Nori", "Rahul Sharma", "Amit Sharma", "Javier Gonzalez"], "summary": "Recent Large Language Models (LLMs) have reported high accuracy on reasoning\nbenchmarks. However, it is still unclear whether the observed results arise\nfrom true reasoning or from statistical recall of the training set. Inspired by\nthe ladder of causation (Pearl, 2009) and its three levels (associations,\ninterventions and counterfactuals), this paper introduces RE-IMAGINE, a\nframework to characterize a hierarchy of reasoning ability in LLMs, alongside\nan automated pipeline to generate problem variations at different levels of the\nhierarchy. By altering problems in an intermediate symbolic representation,\nRE-IMAGINE generates arbitrarily many problems that are not solvable using\nmemorization alone. Moreover, the framework is general and can work across\nreasoning domains, including math, code, and logic. We demonstrate our\nframework on four widely-used benchmarks to evaluate several families of LLMs,\nand observe reductions in performance when the models are queried with problem\nvariations. These assessments indicate a degree of reliance on statistical\nrecall for past performance, and open the door to further research targeting\nskills across the reasoning hierarchy.", "comment": "ICML 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15455v1", "AI": {"title_translation": "RE-IMAGINE：用于推理评估的符号基准合成", "tldr": "RE-IMAGINE是一个框架，通过生成新的、不可记忆的问题变体，评估大型语言模型是真正推理还是依赖记忆。", "motivation": "当前大型语言模型在推理基准上表现出高准确性，但尚不清楚这是否源于真正的推理能力，还是仅仅是对训练数据的统计性回忆。", "method": "本文引入了RE-IMAGINE框架，该框架受因果关系阶梯（Pearl, 2009）启发，用于表征LLM推理能力的分层结构，并提供了一个自动化流程来生成不同层级的变体问题。通过改变中间符号表示中的问题，RE-IMAGINE能够生成大量无法通过单纯记忆解决的问题。该框架通用，可应用于数学、代码和逻辑等推理领域。", "result": "通过在四个广泛使用的基准上评估多个LLM家族，发现当模型被查询变体问题时，其性能出现下降。", "conclusion": "这些评估表明LLM过去的表现一定程度上依赖于统计性记忆，并为进一步研究推理层级中的各项技能开辟了道路。", "translation": "最近大型语言模型（LLMs）在推理基准上报告了高准确性。然而，目前尚不清楚观察到的结果是源于真正的推理还是训练集的统计性回忆。受因果关系阶梯（Pearl, 2009）及其三个层级（关联、干预和反事实）的启发，本文引入了RE-IMAGINE，一个用于表征LLMs推理能力层次结构的框架，以及一个自动化管道，用于生成层次结构中不同层级的问题变体。通过改变中间符号表示中的问题，RE-IMAGINE生成了任意数量无法仅通过记忆解决的问题。此外，该框架具有通用性，可以跨推理领域工作，包括数学、代码和逻辑。我们通过在四个广泛使用的基准上演示了我们的框架，以评估几个LLM家族，并观察到当模型被查询问题变体时性能下降。这些评估表明过去性能一定程度上依赖于统计性记忆，并为进一步研究推理层次结构中的技能打开了大门。", "summary": "本文提出了RE-IMAGINE框架，旨在区分大型语言模型是真正推理还是依赖记忆。该框架受因果关系阶梯启发，能自动化生成不同层级的推理问题变体，这些问题无法通过记忆解决。通过在数学、代码和逻辑等领域应用该框架评估LLMs，研究发现模型在面对变体问题时性能下降，表明其表现有赖于统计性回忆，并强调了未来研究需关注推理能力的分层评估。", "keywords": "大型语言模型, 推理评估, 基准合成, 符号表示, 因果关系阶梯", "comments": "RE-IMAGINE的创新之处在于其通过符号表示生成不可记忆的推理问题变体，从而更准确地评估LLM的真实推理能力，而非其记忆能力。这对于理解LLM的内在机制和推动其在复杂推理任务上的发展具有重要意义。"}}
{"id": "2506.14978", "title": "ODD: Overlap-aware Estimation of Model Performance under Distribution Shift", "authors": ["Aayush Mishra", "Anqi Liu"], "summary": "Reliable and accurate estimation of the error of an ML model in unseen test\ndomains is an important problem for safe intelligent systems. Prior work uses\ndisagreement discrepancy (DIS^2) to derive practical error bounds under\ndistribution shifts. It optimizes for a maximally disagreeing classifier on the\ntarget domain to bound the error of a given source classifier. Although this\napproach offers a reliable and competitively accurate estimate of the target\nerror, we identify a problem in this approach which causes the disagreement\ndiscrepancy objective to compete in the overlapping region between source and\ntarget domains. With an intuitive assumption that the target disagreement\nshould be no more than the source disagreement in the overlapping region due to\nhigh enough support, we devise Overlap-aware Disagreement Discrepancy (ODD).\nMaximizing ODD only requires disagreement in the non-overlapping target domain,\nremoving the competition. Our ODD-based bound uses domain-classifiers to\nestimate domain-overlap and better predicts target performance than DIS^2. We\nconduct experiments on a wide array of benchmarks to show that our method\nimproves the overall performance-estimation error while remaining valid and\nreliable. Our code and results are available on GitHub.", "comment": "Accepted to the 41st Conference on Uncertainty in Artificial\n  Intelligence, 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14978v1", "AI": {"title_translation": "ODD：分布偏移下模型性能的重叠感知估计", "tldr": "现有方法DIS^2在估计模型在分布偏移下的性能时，在源域和目标域的重叠区域存在竞争问题。本文提出ODD，通过仅在非重叠目标域计算分歧来解决此问题，实验证明ODD能更准确地估计模型性能。", "motivation": "准确估计机器学习模型在未见测试域中的误差对于安全的智能系统至关重要。先前的DIS^2方法在源域和目标域的重叠区域存在分歧竞争问题，导致其目标函数在该区域优化不当。", "method": "论文提出了重叠感知分歧差异（ODD）。ODD基于一个直观假设：在重叠区域，目标分歧不应超过源分歧。ODD通过仅在非重叠目标域计算分歧来最大化目标函数，从而消除了竞争问题。ODD还利用域分类器来估计域重叠。", "result": "实验表明，ODD方法在广泛的基准测试中改善了整体性能估计误差，同时保持了有效性和可靠性，并且比DIS^2能更好地预测目标性能。", "conclusion": "ODD通过解决DIS^2在重叠区域的竞争问题，提供了一种更准确、更可靠的分布偏移下模型性能估计方法。", "translation": "可靠准确地估计机器学习模型在未见测试域中的误差是安全智能系统的重要问题。先前的研究使用分歧差异（DIS^2）来推导分布偏移下的实际误差界限。它通过在目标域上优化一个最大分歧分类器来限制给定源分类器的误差。尽管这种方法提供了可靠且具有竞争力的目标误差估计，但我们发现这种方法存在一个问题，即分歧差异目标在源域和目标域之间的重叠区域中存在竞争。凭借一个直观的假设，即由于足够高的支持度，目标分歧在重叠区域不应超过源分歧，我们设计了重叠感知分歧差异（ODD）。最大化ODD仅需要非重叠目标域中的分歧，从而消除了竞争。我们基于ODD的界限使用域分类器来估计域重叠，并且比DIS^2更好地预测目标性能。我们在广泛的基准测试上进行了实验，结果表明我们的方法在保持有效性和可靠性的同时，改善了整体性能估计误差。我们的代码和结果可在GitHub上获取。", "summary": "本文关注在分布偏移下准确估计机器学习模型性能的问题。针对现有DIS^2方法在源域和目标域重叠区域存在的优化竞争问题，作者提出了重叠感知分歧差异（ODD）。ODD通过假设重叠区域的目标分歧不应超过源分歧，并仅在非重叠目标域中最大化分歧，从而消除了竞争。实验结果表明，ODD相比DIS^2能更准确、更可靠地估计模型在分布偏移下的性能。", "keywords": "分布偏移, 模型性能估计, 分歧差异, ODD, 域适应", "comments": "这篇论文的创新点在于识别并解决了现有DIS^2方法在处理分布偏移时，源域与目标域重叠区域内分歧计算的竞争问题。通过引入“重叠感知”的概念和ODD框架，该方法提供了一种更精细、更准确的性能估计机制，对于提高机器学习模型在实际复杂环境中的可靠性和安全性具有重要意义。"}}
{"id": "2506.15480", "title": "Context-Informed Grounding Supervision", "authors": ["Hyunji Lee", "Seunghyun Yoon", "Yunjae Won", "Hanseok Oh", "Geewook Kim", "Trung Bui", "Franck Dernoncourt", "Elias Stengel-Eskin", "Mohit Bansal", "Minjoon Seo"], "summary": "Large language models (LLMs) are often supplemented with external knowledge\nto provide information not encoded in their parameters or to reduce\nhallucination. In such cases, we expect the model to generate responses by\ngrounding its response in the provided external context. However, prior work\nhas shown that simply appending context at inference time does not ensure\ngrounded generation. To address this, we propose Context-INformed Grounding\nSupervision (CINGS), a post-training supervision in which the model is trained\nwith relevant context prepended to the response, while computing the loss only\nover the response tokens and masking out the context. Our experiments\ndemonstrate that models trained with CINGS exhibit stronger grounding in both\ntextual and visual domains compared to standard instruction-tuned models. In\nthe text domain, CINGS outperforms other training methods across 11\ninformation-seeking datasets and is complementary to inference-time grounding\ntechniques. In the vision-language domain, replacing a vision-language model's\nLLM backbone with a CINGS-trained model reduces hallucinations across four\nbenchmarks and maintains factual consistency throughout the generated response.\nThis improved grounding comes without degradation in general downstream\nperformance. Finally, we analyze the mechanism underlying the enhanced\ngrounding in CINGS and find that it induces a shift in the model's prior\nknowledge and behavior, implicitly encouraging greater reliance on the external\ncontext.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15480v1", "AI": {"title_translation": "上下文感知接地监督", "tldr": "大型语言模型在利用外部知识时常出现幻觉或未能有效接地。本文提出CINGS，一种后训练监督方法，通过在训练时将上下文预置并仅对响应计算损失，显著增强模型在文本和视觉领域的接地能力，同时不影响通用性能。", "motivation": "大型语言模型（LLMs）在补充外部知识时，往往无法将其生成响应有效地基于所提供的外部上下文，导致信息未被充分利用或产生幻觉。现有方法仅在推理时附加上下文并不能保证接地生成。", "method": "本文提出了上下文感知接地监督（Context-INformed Grounding Supervision, CINGS），这是一种后训练监督方法。在训练过程中，模型会预置相关上下文到响应中，但损失计算仅针对响应部分的标记，而上下文部分则被屏蔽。", "result": "与标准指令微调模型相比，CINGS训练的模型在文本和视觉领域均展现出更强的接地能力。在文本领域，CINGS在11个信息检索数据集中表现优于其他训练方法，并与推理时接地技术互补。在视觉-语言领域，用CINGS训练的模型替换视觉-语言模型的LLM骨干，能减少四个基准测试中的幻觉并保持事实一致性。此外，这种改进的接地能力并未导致通用下游性能的下降。分析表明，CINGS通过改变模型的先验知识和行为，隐式地促使其更多地依赖外部上下文。", "conclusion": "CINGS通过改变模型的内部机制，使其更倾向于依赖外部上下文，从而有效增强了大型语言模型在跨模态生成中的接地能力，显著减少了幻觉并提高了信息忠实度。", "translation": "大型语言模型（LLMs）通常会补充外部知识，以提供其参数中未编码的信息或减少幻觉。在这种情况下，我们期望模型通过将其响应基于所提供的外部上下文来生成响应。然而，先前的研究表明，仅仅在推理时附加上下文并不能确保接地生成。为了解决这个问题，我们提出了上下文感知接地监督（Context-INformed Grounding Supervision, CINGS），这是一种后训练监督方法，其中模型在训练时将相关上下文预置到响应中，同时仅对响应标记计算损失并屏蔽上下文。我们的实验表明，与标准指令微调模型相比，使用CINGS训练的模型在文本和视觉领域都表现出更强的接地能力。在文本领域，CINGS在11个信息检索数据集中优于其他训练方法，并且与推理时接地技术互补。在视觉-语言领域，用CINGS训练的模型替换视觉-语言模型的LLM骨干，可以减少四个基准测试中的幻觉，并在生成的响应中保持事实一致性。这种改进的接地能力并未导致通用下游性能的下降。最后，我们分析了CINGS中增强接地的底层机制，发现它导致了模型先验知识和行为的转变，隐式地鼓励更多地依赖外部上下文。", "summary": "针对大型语言模型在利用外部知识时难以有效接地并易产生幻觉的问题，本文提出了一种名为上下文感知接地监督（CINGS）的后训练方法。CINGS在训练时将相关上下文预置到模型输入中，但仅对响应部分的标记计算损失。实验证明，CINGS显著提升了模型在文本和视觉模态的接地能力，性能超越了现有训练方法，并有效减少了幻觉，同时未损害模型的通用性能。研究还揭示，CINGS通过促使模型更多地依赖外部上下文来达到这一效果。", "keywords": "接地, 大型语言模型, 幻觉, 上下文, 后训练", "comments": "CINGS提出了一种创新的后训练监督范式，有效解决了大型语言模型在利用外部知识时接地不足和产生幻觉的关键问题。其方法巧妙地通过损失函数的设计，引导模型在训练阶段就学会更好地依赖外部上下文，而非仅仅在推理时被动接收。该方法在文本和视觉领域都展现出优越性，且不影响通用性能，这对于LLM的实际应用具有重要意义。对底层机制的分析也为理解模型行为提供了深入见解。"}}
{"id": "2506.15065", "title": "HEAL: An Empirical Study on Hallucinations in Embodied Agents Driven by Large Language Models", "authors": ["Trishna Chakraborty", "Udita Ghosh", "Xiaopan Zhang", "Fahim Faisal Niloy", "Yue Dong", "Jiachen Li", "Amit K. Roy-Chowdhury", "Chengyu Song"], "summary": "Large language models (LLMs) are increasingly being adopted as the cognitive\ncore of embodied agents. However, inherited hallucinations, which stem from\nfailures to ground user instructions in the observed physical environment, can\nlead to navigation errors, such as searching for a refrigerator that does not\nexist. In this paper, we present the first systematic study of hallucinations\nin LLM-based embodied agents performing long-horizon tasks under scene-task\ninconsistencies. Our goal is to understand to what extent hallucinations occur,\nwhat types of inconsistencies trigger them, and how current models respond. To\nachieve these goals, we construct a hallucination probing set by building on an\nexisting benchmark, capable of inducing hallucination rates up to 40x higher\nthan base prompts. Evaluating 12 models across two simulation environments, we\nfind that while models exhibit reasoning, they fail to resolve scene-task\ninconsistencies-highlighting fundamental limitations in handling infeasible\ntasks. We also provide actionable insights on ideal model behavior for each\nscenario, offering guidance for developing more robust and reliable planning\nstrategies.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15065v1", "AI": {"title_translation": "HEAL：大型语言模型驱动具身智能体幻觉的实证研究", "tldr": "大型语言模型（LLMs）在具身智能体中作为认知核心时，由于未能将用户指令与物理环境结合，会导致幻觉和导航错误。本研究首次系统探讨了在场景-任务不一致下，LLM驱动的具身智能体中的幻觉问题，发现模型在处理不可行任务时存在根本性局限，并提供了改进策略的见解。", "motivation": "大型语言模型（LLMs）被广泛应用于具身智能体的认知核心，但其固有的幻觉问题（未能将用户指令与物理环境结合）会导致导航错误，例如寻找不存在的物体。本研究旨在首次系统性地探讨在场景-任务不一致下，LLM驱动的具身智能体中幻觉的发生程度、触发因素以及当前模型的响应方式。", "method": "研究构建了一个幻觉探测集，该探测集基于现有基准，能够将幻觉率提高高达40倍。随后，在两个模拟环境中对12个模型进行了评估。", "result": "模型展现出推理能力，但未能解决场景与任务之间的不一致问题，这突显了它们在处理不可行任务方面的根本性局限。构建的探测集能够将幻觉率提高高达40倍。", "conclusion": "本研究揭示了LLM驱动的具身智能体在处理场景-任务不一致时存在的显著局限性，导致幻觉。研究提供了关于理想模型行为的可行见解，为开发更稳健可靠的规划策略提供了指导。", "translation": "大型语言模型（LLMs）正越来越多地被采纳为具身智能体的认知核心。然而，源于未能将用户指令与观察到的物理环境相结合的固有幻觉，可能导致导航错误，例如寻找不存在的冰箱。在本文中，我们首次系统研究了在场景-任务不一致下执行长时任务的基于LLM的具身智能体中的幻觉。我们的目标是了解幻觉发生的程度、哪些类型的不一致会触发它们，以及当前模型如何响应。为了实现这些目标，我们通过在一个现有基准上构建一个幻觉探测集，该探测集能够比基础提示诱导高达40倍的幻觉率。在两个模拟环境中评估了12个模型后，我们发现尽管模型表现出推理能力，但它们未能解决场景-任务不一致——这突出了在处理不可行任务方面的根本性局限。我们还为每种场景提供了关于理想模型行为的可行见解，为开发更稳健可靠的规划策略提供了指导。", "summary": "本论文题为“HEAL”，首次对大型语言模型（LLM）驱动的具身智能体在执行长时任务和面临场景-任务不一致时出现的幻觉进行了系统实证研究。研究旨在探究幻觉的发生程度、触发因素以及当前模型的应对方式。通过构建一个能够显著提高幻觉率的探测集，研究在两个模拟环境中评估了12个模型。结果表明，尽管模型具有推理能力，但它们在解决任务与环境之间的不一致性方面存在根本性缺陷，突显了处理不可行任务时的主要局限。论文最后提出了关于改进规划策略的实用建议，以开发更稳健可靠的具身智能体。", "keywords": "大型语言模型, 具身智能体, 幻觉, 场景-任务不一致, 经验研究", "comments": "这篇论文解决了LLM驱动的具身智能体中“幻觉”这一关键问题，这对实际部署构成了重大挑战。通过系统性研究、构建特定的探测集以及对多个模型的实证评估，该研究为当前基于LLM的方法在处理场景-任务不一致方面的局限性提供了宝贵的见解。所提供的可行见解对于未来开发更稳健可靠的具身AI至关重要。"}}
{"id": "2506.15231", "title": "Convolutional Feature Enhancement and Attention Fusion BiFPN for Ship Detection in SAR Images", "authors": ["Liangjie Meng", "Danxia Li", "Jinrong He", "Lili Ma", "Zhixin Li"], "summary": "Synthetic Aperture Radar (SAR) enables submeter-resolution imaging and\nall-weather monitoring via active microwave and advanced signal processing.\nCurrently, SAR has found extensive applications in critical maritime domains\nsuch as ship detection. However, SAR ship detection faces several challenges,\nincluding significant scale variations among ships, the presence of small\noffshore vessels mixed with noise, and complex backgrounds for large nearshore\nships. To address these issues, this paper proposes a novel feature enhancement\nand fusion framework named C-AFBiFPN. C-AFBiFPN constructs a Convolutional\nFeature Enhancement (CFE) module following the backbone network, aiming to\nenrich feature representation and enhance the ability to capture and represent\nlocal details and contextual information. Furthermore, C-AFBiFPN innovatively\nintegrates BiFormer attention within the fusion strategy of BiFPN, creating the\nAFBiFPN network. AFBiFPN improves the global modeling capability of cross-scale\nfeature fusion and can adaptively focus on critical feature regions. The\nexperimental results on SAR Ship Detection Dataset (SSDD) indicate that the\nproposed approach substantially enhances detection accuracy for small targets,\nrobustness against occlusions, and adaptability to multi-scale features.", "comment": "5 pages, 4 figures, 2 tables. Code available at\n  https://github.com/mlj666219/C-AFBiFPN/tree/master", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15231v1", "AI": {"title_translation": "卷积特征增强与注意力融合BiFPN用于SAR图像舰船检测", "tldr": "针对SAR图像舰船检测中尺度变化、小目标与复杂背景等挑战，本文提出了C-AFBiFPN框架，通过卷积特征增强和BiFormer注意力融合改进BiFPN，显著提升了小目标检测精度、鲁棒性和多尺度适应性。", "motivation": "SAR图像舰船检测面临船只尺度变化大、小目标易与噪声混淆、大近岸船只背景复杂等挑战。", "method": "提出了C-AFBiFPN框架。该框架在骨干网络后构建卷积特征增强（CFE）模块，以丰富特征表示并增强局部细节和上下文信息捕获能力。此外，C-AFBiFPN将BiFormer注意力创新性地集成到BiFPN的融合策略中，形成AFBiFPN网络，以提高跨尺度特征融合的全局建模能力，并自适应地关注关键特征区域。", "result": "在SAR舰船检测数据集（SSDD）上的实验结果表明，所提出的方法显著提高了小目标的检测精度、对遮挡的鲁棒性以及对多尺度特征的适应性。", "conclusion": "本文提出的C-AFBiFPN框架有效解决了SAR图像舰船检测中的多尺度、小目标和复杂背景等挑战，通过特征增强和注意力融合显著提升了检测性能。", "translation": "合成孔径雷达（SAR）通过主动微波和先进的信号处理技术实现亚米级分辨率成像和全天候监测。目前，SAR已在舰船检测等关键海洋领域得到广泛应用。然而，SAR舰船检测面临多项挑战，包括舰船之间显著的尺度变化、混杂噪声的小型离岸船只以及大型近岸船只的复杂背景。为解决这些问题，本文提出了一种新颖的特征增强与融合框架，命名为C-AFBiFPN。C-AFBiFPN在骨干网络之后构建了一个卷积特征增强（CFE）模块，旨在丰富特征表示并增强捕获和表示局部细节和上下文信息的能力。此外，C-AFBiFPN创新性地将BiFormer注意力整合到BiFPN的融合策略中，创建了AFBiFPN网络。AFBiFPN提高了跨尺度特征融合的全局建模能力，并且可以自适应地关注关键特征区域。SAR舰船检测数据集（SSDD）上的实验结果表明，所提出的方法显著提高了小目标的检测精度、对遮挡的鲁棒性以及对多尺度特征的适应性。", "summary": "本文针对SAR图像舰船检测中存在的尺度变化大、小目标与噪声混淆、复杂背景等挑战，提出了一种名为C-AFBiFPN的新型特征增强与融合框架。该框架通过引入卷积特征增强（CFE）模块丰富特征表示，并创新性地将BiFormer注意力融入BiFPN的融合策略，形成AFBiFPN网络，以增强跨尺度特征融合的全局建模能力和关键区域关注能力。实验证明，该方法显著提升了小目标检测精度、抗遮挡鲁棒性和多尺度适应性。", "keywords": "SAR图像, 舰船检测, 特征增强, 注意力机制, BiFPN", "comments": "本文的创新点在于结合了卷积特征增强和BiFormer注意力机制来改进BiFPN，以应对SAR图像舰船检测中的特有挑战。通过CFE模块提升局部细节和上下文信息，以及通过AFBiFPN增强全局建模和关键区域关注，该方法在处理多尺度和小目标方面表现出显著优势，对SAR图像目标检测领域具有重要意义。"}}
{"id": "2506.14986", "title": "Early Prediction of Multiple Sclerosis Disability Progression via Multimodal Foundation Model Benchmarks", "authors": ["Maxime Usdin", "Lito Kriara", "Licinio Craveiro"], "summary": "Early multiple sclerosis (MS) disability progression prediction is\nchallenging due to disease heterogeneity. This work predicts 48- and 72-week\ndisability using sparse baseline clinical data and 12 weeks of daily digital\nFloodlight data from the CONSONANCE clinical trial. We employed\nstate-of-the-art tabular and time-series foundation models (FMs), a custom\nmultimodal attention-based transformer, and machine learning methods. Despite\nthe difficulty of early prediction (AUROC 0.63), integrating digital data via\nadvanced models improved performance over clinical data alone. A transformer\nmodel using unimodal embeddings from the Moment FM yielded the best result, but\nour multimodal transformer consistently outperformed its unimodal counterpart,\nconfirming the advantages of combining clinical with digital data. Our findings\ndemonstrate the promise of FMs and multimodal approaches to extract predictive\nsignals from complex and diverse clinical and digital life sciences data (e.g.,\nimaging, omics), enabling more accurate prognostics for MS and potentially\nother complex diseases.", "comment": "Accepted to IJCAI 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14986v1", "AI": {"title_translation": "基于多模态基础模型基准的早期多发性硬化症残疾进展预测", "tldr": "本研究利用多模态基础模型和Transformer，结合临床与数字数据，早期预测多发性硬化症残疾进展，结果显示多模态方法优于单一模态，证明了其在复杂疾病预测中的潜力。", "motivation": "由于疾病异质性，早期预测多发性硬化症（MS）残疾进展具有挑战性。本研究旨在通过整合稀疏基线临床数据和数字健康数据，提高MS残疾进展的早期预测能力。", "method": "研究利用CONSONANCE临床试验中的稀疏基线临床数据和12周的每日数字Floodlight数据。采用了最先进的表格和时间序列基础模型（FMs）、定制的多模态注意力Transformer以及传统的机器学习方法。", "result": "早期预测的难度较高（AUROC 0.63）。然而，通过先进模型整合数字数据显著提高了预测性能，优于仅使用临床数据。使用Moment FM单模态嵌入的Transformer模型表现最佳，但多模态Transformer模型始终优于其单模态对应物。", "conclusion": "研究结果证明了基础模型和多模态方法在从复杂多样的临床和数字生命科学数据（如影像、组学）中提取预测信号的潜力，从而为多发性硬化症以及其他复杂疾病提供更准确的预后。", "translation": "多发性硬化症（MS）的早期残疾进展预测由于疾病异质性而充满挑战。这项工作利用CONSONANCE临床试验中稀疏的基线临床数据和12周的每日数字Floodlight数据，预测48周和72周的残疾进展。我们采用了最先进的表格和时间序列基础模型（FMs）、定制的多模态注意力Transformer以及机器学习方法。尽管早期预测存在难度（AUROC 0.63），但通过先进模型整合数字数据比单独使用临床数据提高了性能。使用Moment FM单模态嵌入的Transformer模型获得了最佳结果，但我们的多模态Transformer始终优于其单模态对应物，证实了结合临床数据与数字数据的优势。我们的发现表明，基础模型和多模态方法在从复杂多样的临床和数字生命科学数据（例如，影像、组学）中提取预测信号方面具有前景，从而能够为MS以及潜在的其他复杂疾病提供更准确的预后。", "summary": "本研究旨在解决多发性硬化症（MS）早期残疾进展预测的挑战，通过整合临床试验中的基线临床数据和每日数字Floodlight数据，使用先进的表格和时间序列基础模型以及定制的多模态Transformer进行预测。尽管预测难度大，但结果表明，结合数字数据的多模态方法显著优于仅使用临床数据，其中多模态Transformer表现出优越性。这验证了基础模型和多模态方法在从复杂多源数据中提取预测信号、改善MS及其他复杂疾病预后方面的巨大潜力。", "keywords": "多发性硬化症, 残疾进展预测, 多模态, 基础模型, 数字健康数据", "comments": "这项工作创新性地将多模态基础模型应用于多发性硬化症的早期残疾进展预测，特别强调了结合数字健康数据的优势。它展示了Transformer和基础模型在处理复杂异构医疗数据方面的潜力，为未来更精准的疾病预后提供了新的思路。其重要性在于为MS患者提供更早的干预机会，并为其他复杂疾病的预测模型开发提供了借鉴。"}}
{"id": "2506.15498", "title": "SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling", "authors": ["Md Imbesat Hassan Rizvi", "Xiaodan Zhu", "Iryna Gurevych"], "summary": "Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables single-pass, per-step annotation by aligning each solution step to one\nor multiple steps in a reference solution, accompanied by explicit reasoning\nfor evaluation. We show that reference-guided step-level evaluation effectively\nfacilitates process supervision on four datasets spanning three domains:\nmathematical reasoning, multi-hop compositional question answering, and spatial\nreasoning. We demonstrate that SPARE, when compared to baselines, improves\nreasoning performance when used for: (1) fine-tuning models in an offline RL\nsetup for inference-time greedy-decoding, and (2) training reward models for\nranking/aggregating multiple LLM-generated outputs. Additionally, SPARE\nachieves competitive performance on challenging mathematical datasets while\noffering 2.6 times greater efficiency, requiring only 38% of the runtime,\ncompared to tree search-based automatic annotation. The codebase, along with a\ntrained SPARE-PRM model, is publicly released to facilitate further research\nand reproducibility.", "comment": "8 pages main content, 4 figures, 4 tables", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15498v1", "AI": {"title_translation": "SPARE：基于参考引导评估的单次标注，用于自动化过程监督和奖励建模", "tldr": "SPARE是一种新的单次标注框架，通过参考引导评估，显著提高了大型语言模型在复杂推理任务中的过程监督效率和性能，尤其在数学推理等领域。", "motivation": "现有的大型语言模型（LLM）在复杂多步推理中，过程或逐步监督至关重要，但高效、高质量的自动化过程标注仍然是一个重大挑战。", "method": "本文引入了SPARE（Single-Pass Annotation with Reference-Guided Evaluation），这是一个新颖的结构化框架。它通过将每个解决方案步骤与参考解决方案中的一个或多个步骤对齐，并附带明确的评估理由，实现单次、逐步标注。", "result": "参考引导的步骤级评估有效促进了数学推理、多跳组合问答和空间推理三个领域四个数据集上的过程监督。SPARE在离线强化学习设置中用于微调模型以进行推理时贪婪解码，以及用于训练奖励模型以对多个LLM生成输出进行排名/聚合时，与基线相比提高了推理性能。在具有挑战性的数学数据集上，SPARE取得了具有竞争力的性能，同时效率提高了2.6倍，仅需要树搜索自动标注38%的运行时间。代码库和训练好的SPARE-PRM模型已公开发布。", "conclusion": "SPARE提供了一种高效、高质量的自动化过程标注方法，显著提升了LLM在复杂推理任务中的性能，并具有更高的效率，对于促进未来研究具有重要意义。", "translation": "过程或逐步监督在推动大型语言模型（LLMs）的复杂多步推理能力方面发挥了关键作用。然而，高效、高质量的自动化过程标注仍然是一个重大挑战。为了解决这个问题，我们引入了单次标注与参考引导评估（SPARE），这是一个新颖的结构化框架，通过将每个解决方案步骤与参考解决方案中的一个或多个步骤对齐，并附带明确的评估理由，从而实现单次、逐步标注。我们表明，参考引导的步骤级评估有效促进了数学推理、多跳组合问答和空间推理三个领域四个数据集上的过程监督。我们证明，与基线相比，SPARE在以下方面提高了推理性能：(1) 在离线强化学习设置中用于推理时贪婪解码的微调模型，以及 (2) 用于对多个LLM生成输出进行排名/聚合的奖励模型训练。此外，与基于树搜索的自动标注相比，SPARE在具有挑战性的数学数据集上取得了具有竞争力的性能，同时效率提高了2.6倍，仅需要38%的运行时间。代码库以及训练好的SPARE-PRM模型已公开发布，以促进进一步的研究和可复现性。", "summary": "本文提出了SPARE（Single-Pass Annotation with Reference-Guided Evaluation），一个用于自动化过程监督和奖励建模的新颖框架。SPARE通过单次、参考引导的逐步标注，有效提高了大型语言模型在数学推理、多跳问答和空间推理等复杂任务中的推理性能。实验表明，SPARE在提高性能的同时，比传统方法效率更高，尤其在数学推理任务上表现突出。", "keywords": "过程监督, 奖励建模, 大型语言模型, 单次标注, 参考引导评估", "comments": "SPARE的创新之处在于其单次标注和参考引导评估机制，显著提高了过程监督的效率和质量，解决了LLM复杂推理能力提升中的关键瓶颈。其在效率上的显著提升（2.6倍）和代码的开源，对于推动LLM在多步推理领域的进一步研究具有重要意义。"}}
{"id": "2506.15242", "title": "RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories", "authors": ["Qingsong Yan", "Qiang Wang", "Kaiyong Zhao", "Jie Chen", "Bo Li", "Xiaowen Chu", "Fei Deng"], "summary": "Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have emerged\nas powerful tools for 3D reconstruction and SLAM tasks. However, their\nperformance depends heavily on accurate camera pose priors. Existing approaches\nattempt to address this issue by introducing external constraints but fall\nshort of achieving satisfactory accuracy, particularly when camera trajectories\nare complex. In this paper, we propose a novel method, RA-NeRF, capable of\npredicting highly accurate camera poses even with complex camera trajectories.\nFollowing the incremental pipeline, RA-NeRF reconstructs the scene using NeRF\nwith photometric consistency and incorporates flow-driven pose regulation to\nenhance robustness during initialization and localization. Additionally,\nRA-NeRF employs an implicit pose filter to capture the camera movement pattern\nand eliminate the noise for pose estimation. To validate our method, we conduct\nextensive experiments on the Tanks\\&Temple dataset for standard evaluation, as\nwell as the NeRFBuster dataset, which presents challenging camera pose\ntrajectories. On both datasets, RA-NeRF achieves state-of-the-art results in\nboth camera pose estimation and visual quality, demonstrating its effectiveness\nand robustness in scene reconstruction under complex pose trajectories.", "comment": "IROS 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15242v1", "AI": {"title_translation": "RA-NeRF: 复杂轨迹下具有精确相机位姿估计的鲁棒神经辐射场重建", "tldr": "RA-NeRF 提出了一种在复杂相机轨迹下也能准确估计相机位姿并进行场景重建的方法，结合了光度一致性、流驱动位姿调节和隐式位姿滤波器，在多个数据集上实现了最先进的性能。", "motivation": "现有的神经辐射场 (NeRF) 和 3D Gaussian Splatting (3DGS) 等 3D 重建和 SLAM 工具的性能严重依赖于准确的相机位姿先验，但在复杂相机轨迹下，现有方法通过引入外部约束仍无法达到令人满意的精度。", "method": "RA-NeRF 采用增量式流程，利用 NeRF 进行场景重建，并结合光度一致性。它引入了流驱动的位姿调节来增强初始化和定位的鲁棒性，并采用隐式位姿滤波器来捕获相机运动模式并消除位姿估计中的噪声。", "result": "RA-NeRF 在 Tanks&Temple 数据集和 NeRFBuster 数据集（包含挑战性相机位姿轨迹）上进行了广泛实验，在相机位姿估计和视觉质量方面均取得了最先进的结果。", "conclusion": "RA-NeRF 在复杂位姿轨迹下的场景重建中表现出有效性和鲁棒性，能够预测高度准确的相机位姿并实现卓越的重建质量。", "translation": "神经辐射场 (NeRF) 和 3D Gaussian Splatting (3DGS) 已成为 3D 重建和 SLAM 任务的强大工具。然而，它们的性能严重依赖于准确的相机位姿先验。现有方法试图通过引入外部约束来解决这个问题，但未能达到令人满意的精度，尤其是在相机轨迹复杂时。在本文中，我们提出了一种新颖的方法 RA-NeRF，即使在复杂相机轨迹下也能预测高度准确的相机位姿。RA-NeRF 遵循增量式流程，使用 NeRF 和光度一致性重建场景，并结合流驱动的位姿调节以增强初始化和定位期间的鲁棒性。此外，RA-NeRF 采用隐式位姿滤波器来捕获相机运动模式并消除位姿估计中的噪声。为了验证我们的方法，我们在 Tanks&Temple 数据集上进行了标准评估，并在 NeRFBuster 数据集上进行了广泛实验，该数据集呈现了具有挑战性的相机位姿轨迹。在这两个数据集上，RA-NeRF 在相机位姿估计和视觉质量方面均取得了最先进的结果，证明了其在复杂位姿轨迹下场景重建的有效性和鲁棒性。", "summary": "RA-NeRF 是一种新颖的神经辐射场重建方法，旨在解决 NeRF 和 3DGS 在复杂相机轨迹下相机位姿估计精度不足的问题。该方法通过结合光度一致性、流驱动位姿调节和隐式位姿滤波器，能够准确预测相机位姿并鲁棒地重建场景。实验结果表明，RA-NeRF 在标准数据集和挑战性数据集上均达到了相机位姿估计和视觉质量的最先进水平。", "keywords": "神经辐射场,相机位姿估计,3D重建,复杂轨迹,鲁棒性", "comments": "RA-NeRF 的创新之处在于其针对复杂相机轨迹下的相机位姿估计问题提出了多方面的解决方案，包括流驱动的位姿调节和隐式位姿滤波器，这显著提升了 NeRF 在实际应用中的鲁棒性和准确性。该工作对于提高 NeRF 在非结构化和动态环境中的表现具有重要意义。"}}
{"id": "2506.14988", "title": "Fair Algorithms with Probing for Multi-Agent Multi-Armed Bandits", "authors": ["Tianyi Xu", "Jiaxin Liu", "Zizhan Zheng"], "summary": "We propose a multi-agent multi-armed bandit (MA-MAB) framework aimed at\nensuring fair outcomes across agents while maximizing overall system\nperformance. A key challenge in this setting is decision-making under limited\ninformation about arm rewards. To address this, we introduce a novel probing\nframework that strategically gathers information about selected arms before\nallocation. In the offline setting, where reward distributions are known, we\nleverage submodular properties to design a greedy probing algorithm with a\nprovable performance bound. For the more complex online setting, we develop an\nalgorithm that achieves sublinear regret while maintaining fairness. Extensive\nexperiments on synthetic and real-world datasets show that our approach\noutperforms baseline methods, achieving better fairness and efficiency.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.14988v1", "AI": {"title_translation": "具有探测功能的多智能体多臂老虎机公平算法", "tldr": "本文提出了一个多智能体多臂老虎机（MA-MAB）框架，通过引入探测机制，在有限信息下实现公平结果并最大化系统性能，并在离线和在线设置中设计了算法，实验证明其在公平性和效率方面优于基线方法。", "motivation": "在多智能体多臂老虎机（MA-MAB）框架中，在最大化整体系统性能的同时确保智能体之间的公平结果是一个关键挑战，特别是在对臂奖励信息有限的情况下进行决策。", "method": "本文提出了一个新颖的探测框架，在分配之前策略性地收集选定臂的信息。在奖励分布已知的离线设置中，利用次模性质设计了一个具有可证明性能边界的贪婪探测算法。对于更复杂的在线设置，开发了一种实现次线性遗憾并保持公平性的算法。", "result": "在合成数据集和真实世界数据集上进行的广泛实验表明，所提出的方法优于基线方法，实现了更好的公平性和效率。", "conclusion": "本文提出的具有探测机制的多智能体多臂老虎机公平算法在保证公平性的同时，有效提升了系统性能，并在离线和在线设置中均表现出优越性。", "translation": "我们提出了一个多智能体多臂老虎机（MA-MAB）框架，旨在确保智能体之间的公平结果，同时最大化整体系统性能。在这种设置中，一个关键挑战是在关于臂奖励信息有限的情况下进行决策。为了解决这个问题，我们引入了一个新颖的探测框架，在分配之前策略性地收集选定臂的信息。在奖励分布已知的离线设置中，我们利用次模性质设计了一个具有可证明性能边界的贪婪探测算法。对于更复杂的在线设置，我们开发了一种实现次线性遗憾并保持公平性的算法。在合成数据集和真实世界数据集上进行的广泛实验表明，我们的方法优于基线方法，实现了更好的公平性和效率。", "summary": "本文提出了一个用于多智能体多臂老虎机（MA-MAB）的公平算法框架，旨在平衡智能体间的公平性与整体系统性能。为解决信息有限下的决策问题，引入了一种新颖的探测机制，在分配前策略性地收集臂信息。在离线设置中，基于次模性质设计了贪婪探测算法；在线设置中，开发了实现次线性遗憾并保持公平性的算法。实验证明，该方法在公平性和效率上均优于现有基线方法。", "keywords": "多智能体多臂老虎机, 公平算法, 探测机制, 次模优化, 在线学习", "comments": "该论文的创新点在于引入了“探测”机制来解决多智能体多臂老虎机问题中信息有限下的公平性与效率权衡。通过将探测与次模优化相结合，为离线设置提供了理论保证，并在在线设置中实现了次线性遗憾，这对于实际应用具有重要意义。该方法在解决公平性这一复杂问题上展现了潜力。"}}
{"id": "2506.15504", "title": "Enhancing Hyperbole and Metaphor Detection with Their Bidirectional Dynamic Interaction and Emotion Knowledge", "authors": ["Li Zheng", "Sihang Wang", "Hao Fei", "Zuquan Peng", "Fei Li", "Jianming Fu", "Chong Teng", "Donghong Ji"], "summary": "Text-based hyperbole and metaphor detection are of great significance for\nnatural language processing (NLP) tasks. However, due to their semantic\nobscurity and expressive diversity, it is rather challenging to identify them.\nExisting methods mostly focus on superficial text features, ignoring the\nassociations of hyperbole and metaphor as well as the effect of implicit\nemotion on perceiving these rhetorical devices. To implement these hypotheses,\nwe propose an emotion-guided hyperbole and metaphor detection framework based\non bidirectional dynamic interaction (EmoBi). Firstly, the emotion analysis\nmodule deeply mines the emotion connotations behind hyperbole and metaphor.\nNext, the emotion-based domain mapping module identifies the target and source\ndomains to gain a deeper understanding of the implicit meanings of hyperbole\nand metaphor. Finally, the bidirectional dynamic interaction module enables the\nmutual promotion between hyperbole and metaphor. Meanwhile, a verification\nmechanism is designed to ensure detection accuracy and reliability. Experiments\nshow that EmoBi outperforms all baseline methods on four datasets.\nSpecifically, compared to the current SoTA, the F1 score increased by 28.1% for\nhyperbole detection on the TroFi dataset and 23.1% for metaphor detection on\nthe HYPO-L dataset. These results, underpinned by in-depth analyses, underscore\nthe effectiveness and potential of our approach for advancing hyperbole and\nmetaphor detection.", "comment": "Accepted by ACL 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15504v1", "AI": {"title_translation": "基于双向动态交互和情感知识的夸张与隐喻检测增强", "tldr": "提出EmoBi框架，通过情感分析和双向交互显著提升夸张和隐喻检测性能。", "motivation": "夸张和隐喻检测对自然语言处理（NLP）任务意义重大，但由于其语义模糊性和表达多样性，识别它们极具挑战性。现有方法大多关注表层文本特征，忽略了夸张和隐喻之间的关联以及隐式情感对感知这些修辞手法的影响。", "method": "本文提出了一个情感引导的夸张和隐喻双向动态交互检测框架（EmoBi）。该框架包含情感分析模块（用于深入挖掘情感内涵）、情感域映射模块（用于识别目标和源域以理解隐含意义），以及双向动态交互模块（用于促进夸张和隐喻之间的相互提升）。同时，设计了一个验证机制以确保检测的准确性和可靠性。", "result": "实验结果表明，EmoBi在四个数据集上均优于所有基线方法。具体而言，与当前最先进方法（SoTA）相比，在TroFi数据集上夸张检测的F1分数提高了28.1%，在HYPO-L数据集上隐喻检测的F1分数提高了23.1%。", "conclusion": "这些结果通过深入分析得到支持，强调了该方法在推进夸张和隐喻检测方面的有效性和潜力。", "translation": "基于文本的夸张和隐喻检测对自然语言处理（NLP）任务具有重要意义。然而，由于其语义模糊性和表达多样性，识别它们相当具有挑战性。现有方法大多关注表层文本特征，忽略了夸张和隐喻之间的关联以及隐式情感对感知这些修辞手法的影响。为了实现这些假设，我们提出了一个基于双向动态交互的情感引导夸张和隐喻检测框架（EmoBi）。首先，情感分析模块深入挖掘夸张和隐喻背后的情感内涵。其次，基于情感的领域映射模块识别目标域和源域，以更深入地理解夸张和隐喻的隐含意义。最后，双向动态交互模块实现了夸张和隐喻之间的相互促进。同时，设计了一种验证机制以确保检测的准确性和可靠性。实验表明，EmoBi在四个数据集上均优于所有基线方法。具体而言，与当前的最新技术（SoTA）相比，在TroFi数据集上夸张检测的F1分数提高了28.1%，在HYPO-L数据集上隐喻检测的F1分数提高了23.1%。这些结果在深入分析的支持下，强调了我们方法在推进夸张和隐喻检测方面的有效性和潜力。", "summary": "本文提出了一种名为EmoBi的情感引导双向动态交互框架，用于增强文本夸张和隐喻检测。针对现有方法忽略语义关联和情感影响的问题，EmoBi通过情感分析、情感域映射和双向交互模块，深入挖掘修辞手法的情感内涵并促进夸张与隐喻间的相互提升。实验结果表明，EmoBi在多个数据集上显著优于基线方法，尤其在夸张和隐喻检测方面F1分数有大幅提升，证明了其有效性和潜力。", "keywords": "夸张检测, 隐喻检测, 情感分析, 双向动态交互, 自然语言处理", "comments": "这篇论文的创新点在于引入了情感知识和夸张与隐喻之间的双向动态交互来提升检测性能，这超越了传统方法仅关注表面特征的局限。特别是其在多个数据集上取得的显著性能提升，如夸张检测F1分数提高28.1%，隐喻检测F1分数提高23.1%，表明了该方法在处理语义模糊性方面的强大能力和实际应用价值，对自然语言处理领域具有重要意义。"}}
{"id": "2506.15244", "title": "Retrospective Memory for Camouflaged Object Detection", "authors": ["Chenxi Zhang", "Jiayun Wu", "Qing Zhang", "Yazhe Zhai", "Youwei Pang"], "summary": "Camouflaged object detection (COD) primarily focuses on learning subtle yet\ndiscriminative representations from complex scenes. Existing methods\npredominantly follow the parametric feedforward architecture based on static\nvisual representation modeling. However, they lack explicit mechanisms for\nacquiring historical context, limiting their adaptation and effectiveness in\nhandling challenging camouflage scenes. In this paper, we propose a\nrecall-augmented COD architecture, namely RetroMem, which dynamically modulates\ncamouflage pattern perception and inference by integrating relevant historical\nknowledge into the process. Specifically, RetroMem employs a two-stage training\nparadigm consisting of a learning stage and a recall stage to construct,\nupdate, and utilize memory representations effectively. During the learning\nstage, we design a dense multi-scale adapter (DMA) to improve the pretrained\nencoder's capability to capture rich multi-scale visual information with very\nfew trainable parameters, thereby providing foundational inferences. In the\nrecall stage, we propose a dynamic memory mechanism (DMM) and an inference\npattern reconstruction (IPR). These components fully leverage the latent\nrelationships between learned knowledge and current sample context to\nreconstruct the inference of camouflage patterns, thereby significantly\nimproving the model's understanding of camouflage scenes. Extensive experiments\non several widely used datasets demonstrate that our RetroMem significantly\noutperforms existing state-of-the-art methods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15244v1", "AI": {"title_translation": "伪装目标检测的追溯记忆", "tldr": "提出RetroMem，一个基于回忆增强的伪装目标检测架构，通过集成历史知识和两阶段训练（学习与回忆），显著提升了模型在复杂伪装场景下的检测性能。", "motivation": "现有伪装目标检测方法主要采用参数前馈架构和静态视觉表示建模，缺乏获取历史上下文的显式机制，这限制了它们在处理具有挑战性的伪装场景时的适应性和有效性。", "method": "本文提出了一种回忆增强的伪装目标检测（COD）架构，名为RetroMem。该架构通过整合相关的历史知识来动态调节伪装模式的感知和推理。RetroMem采用两阶段训练范式：学习阶段和回忆阶段。在学习阶段，设计了密集多尺度适配器（DMA）以增强预训练编码器捕获丰富多尺度视觉信息的能力，并提供基础推理。在回忆阶段，提出了动态记忆机制（DMM）和推理模式重建（IPR），这些组件充分利用学习到的知识和当前样本上下文之间的潜在关系来重建伪装模式的推理，从而显著提高模型对伪装场景的理解。", "result": "在多个广泛使用的数据集上进行的大量实验表明，所提出的RetroMem显著优于现有最先进的伪装目标检测方法。", "conclusion": "本文提出的RetroMem通过引入回忆增强机制和两阶段训练范式，有效解决了现有伪装目标检测方法在处理复杂伪装场景时缺乏历史上下文和适应性的问题，并通过动态记忆和推理模式重建显著提升了模型性能。", "translation": "伪装目标检测 (COD) 主要关注从复杂场景中学习细微但具有辨别力的表示。现有方法主要遵循基于静态视觉表示建模的参数前馈架构。然而，它们缺乏获取历史上下文的显式机制，限制了它们在处理具有挑战性的伪装场景时的适应性和有效性。在本文中，我们提出了一种回忆增强的 COD 架构，即 RetroMem，它通过将相关的历史知识整合到过程中，动态地调节伪装模式的感知和推理。具体来说，RetroMem 采用两阶段训练范式，包括学习阶段和回忆阶段，以有效地构建、更新和利用记忆表示。在学习阶段，我们设计了一个密集多尺度适配器 (DMA) 来提高预训练编码器捕获丰富多尺度视觉信息的能力，且只需极少的可训练参数，从而提供基础推理。在回忆阶段，我们提出了动态记忆机制 (DMM) 和推理模式重建 (IPR)。这些组件充分利用学习到的知识和当前样本上下文之间的潜在关系来重建伪装模式的推理，从而显著提高了模型对伪装场景的理解。在几个广泛使用的数据集上进行的大量实验表明，我们的 RetroMem 显著优于现有最先进的方法。", "summary": "本文针对现有伪装目标检测（COD）方法缺乏历史上下文利用机制的不足，提出了一种名为RetroMem的回忆增强型COD架构。RetroMem通过两阶段训练（学习阶段与回忆阶段）来有效管理和利用记忆表示。在学习阶段，通过密集多尺度适配器（DMA）增强编码器捕获多尺度信息；在回忆阶段，利用动态记忆机制（DMM）和推理模式重建（IPR）整合历史知识与当前上下文以优化伪装模式推理。实验证明RetroMem显著超越了现有SOTA方法。", "keywords": "伪装目标检测, 追溯记忆, 动态记忆机制, 多尺度适配器, 深度学习", "comments": "这篇论文通过引入“追溯记忆”的概念，解决了传统前馈网络在伪装目标检测中缺乏历史上下文感知能力的问题。其创新点在于提出的两阶段训练范式和记忆机制，这使得模型能够动态地利用历史知识来改善对复杂伪装模式的理解。这种方法为提升模型在挑战性场景下的鲁棒性和适应性提供了新的思路。"}}
{"id": "2506.15019", "title": "Stable CDE Autoencoders with Acuity Regularization for Offline Reinforcement Learning in Sepsis Treatment", "authors": ["Yue Gao"], "summary": "Effective reinforcement learning (RL) for sepsis treatment depends on\nlearning stable, clinically meaningful state representations from irregular ICU\ntime series. While previous works have explored representation learning for\nthis task, the critical challenge of training instability in sequential\nrepresentations and its detrimental impact on policy performance has been\noverlooked. This work demonstrates that Controlled Differential Equations (CDE)\nstate representation can achieve strong RL policies when two key factors are\nmet: (1) ensuring training stability through early stopping or stabilization\nmethods, and (2) enforcing acuity-aware representations by correlation\nregularization with clinical scores (SOFA, SAPS-II, OASIS). Experiments on the\nMIMIC-III sepsis cohort reveal that stable CDE autoencoder produces\nrepresentations strongly correlated with acuity scores and enables RL policies\nwith superior performance (WIS return $> 0.9$). In contrast, unstable CDE\nrepresentation leads to degraded representations and policy failure (WIS return\n$\\sim$ 0). Visualizations of the latent space show that stable CDEs not only\nseparate survivor and non-survivor trajectories but also reveal clear acuity\nscore gradients, whereas unstable training fails to capture either pattern.\nThese findings highlight practical guidelines for using CDEs to encode\nirregular medical time series in clinical RL, emphasizing the need for training\nstability in sequential representation learning.", "comment": "Accepted to IJCAI2025 AI4TS", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15019v1", "AI": {"title_translation": "用于败血症治疗离线强化学习的具有敏锐度正则化的稳定CDE自编码器", "tldr": "本研究提出并验证了在败血症治疗离线强化学习中，稳定且经过敏锐度正则化的受控微分方程（CDE）状态表示能够实现卓越的策略性能，并强调了训练稳定性的关键作用。", "motivation": "现有的败血症治疗强化学习方法在学习稳定的、有临床意义的状态表示方面存在挑战，尤其是在序列表示的训练不稳定性及其对策略性能的负面影响方面被忽视了。", "method": "本研究使用受控微分方程（CDE）进行状态表示学习，并通过以下两个关键因素确保其有效性：1) 通过早期停止或稳定化方法确保训练稳定性；2) 通过与临床评分（SOFA、SAPS-II、OASIS）进行相关性正则化来强制执行敏锐度感知的表示。", "result": "在MIMIC-III败血症队列上的实验表明，稳定的CDE自编码器产生的表示与敏锐度评分高度相关，并使强化学习策略表现出卓越的性能（WIS回报 > 0.9）。相比之下，不稳定的CDE表示会导致表示质量下降和策略失败（WIS回报 ~ 0）。潜在空间的视觉化显示，稳定的CDE不仅能够区分幸存者和非幸存者的轨迹，而且还揭示了清晰的敏锐度评分梯度，而不稳定的训练则未能捕捉到这些模式。", "conclusion": "本研究的发现为在临床强化学习中使用CDE编码不规则医疗时间序列提供了实用指导，强调了序列表示学习中训练稳定性的必要性。", "translation": "败血症治疗的有效强化学习（RL）取决于从不规则的ICU时间序列中学习稳定、具有临床意义的状态表示。虽然以前的工作已经探索了针对此任务的表示学习，但序列表示训练的不稳定性及其对策略性能的有害影响这一关键挑战却被忽视了。这项工作表明，当满足两个关键因素时，受控微分方程（CDE）状态表示可以实现强大的RL策略：(1) 通过早期停止或稳定化方法确保训练稳定性，以及 (2) 通过与临床评分（SOFA、SAPS-II、OASIS）进行相关性正则化来强制执行敏锐度感知的表示。在MIMIC-III败血症队列上的实验表明，稳定的CDE自编码器产生的表示与敏锐度评分高度相关，并使RL策略表现出卓越的性能（WIS回报 > 0.9）。相比之下，不稳定的CDE表示会导致表示质量下降和策略失败（WIS回报 ~ 0）。潜在空间的视觉化显示，稳定的CDE不仅能够区分幸存者和非幸存者的轨迹，而且还揭示了清晰的敏锐度评分梯度，而不稳定的训练则未能捕捉到这些模式。这些发现为在临床RL中使用CDE编码不规则医疗时间序列提供了实用指导，强调了序列表示学习中训练稳定性的必要性。", "summary": "本研究提出了一种用于败血症治疗离线强化学习的稳定CDE自编码器，以解决从不规则ICU时间序列中学习稳定、临床有意义的状态表示的挑战。研究指出，通过确保训练稳定性（如早期停止）和使用临床评分（SOFA, SAPS-II, OASIS）进行相关性正则化以强制执行敏锐度感知表示，CDE能够实现强大的RL策略。实验结果表明，稳定的CDE自编码器产生的表示与患者敏锐度高度相关，并显著提升了RL策略的性能（WIS回报 > 0.9），同时其潜在空间能清晰区分幸存者与非幸存者轨迹并显示敏锐度梯度。研究强调了序列表示学习中训练稳定性的重要性，并为临床RL中CDE的应用提供了实用指导。", "keywords": "败血症治疗, 强化学习, CDE, 状态表示学习, 训练稳定性", "comments": "该论文的创新点在于识别并解决了强化学习中序列表示学习的训练不稳定性问题，并引入了敏锐度正则化来增强表示的临床意义。通过结合CDE、稳定性保障和临床评分正则化，研究为处理不规则医疗时间序列提供了一个有效且鲁棒的框架，对于提高败血症等复杂疾病的治疗策略具有重要实践意义。其强调训练稳定性的发现对于未来在该领域的研究具有指导价值。"}}
{"id": "2506.15522", "title": "Lessons from Training Grounded LLMs with Verifiable Rewards", "authors": ["Shang Hong Sim", "Tej Deep Pala", "Vernon Toh", "Hai Leong Chieu", "Amir Zadeh", "Chuan Li", "Navonil Majumder", "Soujanya Poria"], "summary": "Generating grounded and trustworthy responses remains a key challenge for\nlarge language models (LLMs). While retrieval-augmented generation (RAG) with\ncitation-based grounding holds promise, instruction-tuned models frequently\nfail even in straightforward scenarios: missing explicitly stated answers,\nciting incorrectly, or refusing when evidence is available. In this work, we\nexplore how reinforcement learning (RL) and internal reasoning can enhance\ngrounding in LLMs. We use the GRPO (Group Relative Policy Optimization) method\nto train models using verifiable outcome-based rewards targeting answer\ncorrectness, citation sufficiency, and refusal quality, without requiring gold\nreasoning traces or expensive annotations. Through comprehensive experiments\nacross ASQA, QAMPARI, ELI5, and ExpertQA we show that reasoning-augmented\nmodels significantly outperform instruction-only variants, especially in\nhandling unanswerable queries and generating well-cited responses. A two-stage\ntraining setup, first optimizing answer and citation behavior and then refusal,\nfurther improves grounding by stabilizing the learning signal. Additionally, we\nrevisit instruction tuning via GPT-4 distillation and find that combining it\nwith GRPO enhances performance on long-form, generative QA tasks. Overall, our\nfindings highlight the value of reasoning, stage-wise optimization, and\noutcome-driven RL for building more verifiable and reliable LLMs.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15522v1", "AI": {"title_translation": "从训练基于可验证奖励的接地LLM中吸取的教训", "tldr": "大型语言模型（LLM）在生成接地和可信响应方面仍面临挑战。本文通过使用强化学习（RL）和内部推理，结合GRPO方法和两阶段训练，利用可验证的结果奖励（答案正确性、引用充分性、拒绝质量）训练模型。实验证明，增强推理的模型显著优于仅指令微调的模型，尤其是在处理无法回答的查询和生成良好引用的响应方面，为构建更可验证和可靠的LLM提供了有价值的见解。", "motivation": "大型语言模型（LLM）在生成接地和可信的响应方面面临关键挑战。尽管基于引用的检索增强生成（RAG）具有潜力，但指令微调模型在简单场景中也常失败，例如遗漏明确答案、错误引用或在有证据时拒绝回答。", "method": "本文探索了强化学习（RL）和内部推理如何增强LLM的接地能力。研究使用GRPO（Group Relative Policy Optimization）方法，通过可验证的基于结果的奖励（针对答案正确性、引用充分性和拒绝质量）来训练模型，无需黄金推理轨迹或昂贵的标注。实验涵盖ASQA、QAMPARI、ELI5和ExpertQA数据集。此外，研究采用两阶段训练设置，首先优化答案和引用行为，然后优化拒绝行为，以稳定学习信号。研究还结合了通过GPT-4蒸馏进行的指令微调与GRPO。", "result": "增强推理的模型显著优于仅指令微调的模型，特别是在处理无法回答的查询和生成良好引用的响应方面。两阶段训练设置通过稳定学习信号进一步提高了接地能力。将GPT-4蒸馏与GRPO结合可以提高长篇生成式问答任务的性能。", "conclusion": "本研究结果强调了推理、阶段性优化和结果驱动的强化学习对于构建更可验证和可靠的LLM的价值。", "translation": "生成接地和可信的响应仍然是大型语言模型（LLM）面临的关键挑战。尽管基于引用的检索增强生成（RAG）具有前景，但指令微调模型即使在简单的场景中也经常失败：遗漏明确陈述的答案、错误引用或在有证据时拒绝回答。在这项工作中，我们探索了强化学习（RL）和内部推理如何增强LLM的接地能力。我们使用GRPO（Group Relative Policy Optimization）方法，通过可验证的基于结果的奖励（针对答案正确性、引用充分性和拒绝质量）来训练模型，而无需黄金推理轨迹或昂贵的标注。通过在ASQA、QAMPARI、ELI5和ExpertQA上的全面实验，我们表明增强推理的模型显著优于仅指令微调的模型，特别是在处理无法回答的查询和生成良好引用的响应方面。两阶段训练设置，首先优化答案和引用行为，然后优化拒绝行为，通过稳定学习信号进一步提高了接地能力。此外，我们重新审视了通过GPT-4蒸馏进行的指令微调，并发现将其与GRPO结合可以提高长篇生成式问答任务的性能。总的来说，我们的发现突出了推理、阶段性优化和结果驱动的强化学习对于构建更可验证和可靠的LLM的价值。", "summary": "本研究旨在解决大型语言模型（LLM）在生成接地和可信响应方面的挑战，特别是指令微调模型在引用和拒绝方面的不足。论文提出了一种结合强化学习（RL）和内部推理的方法，通过使用GRPO算法和基于可验证结果的奖励（包括答案正确性、引用充分性和拒绝质量）来训练模型。研究采用两阶段训练策略，并结合了GPT-4蒸馏的指令微调。实验结果表明，与仅指令微调的模型相比，增强推理的模型在处理无法回答的查询和生成良好引用的响应方面表现出显著优势。这些发现强调了推理、阶段性优化和结果驱动的RL在构建更可验证和可靠的LLM中的重要作用。", "keywords": "接地LLM, 强化学习, 可验证奖励, GRPO, 可信度", "comments": "这篇论文的创新点在于引入了可验证的基于结果的奖励机制，并通过GRPO方法在没有昂贵标注的情况下训练LLM，有效解决了LLM在生成接地和可信响应方面的固有问题。其提出的两阶段训练设置和结合GPT-4蒸馏的方法，为提升LLM在复杂问答任务中的表现提供了新的思路，尤其是在处理无法回答的查询和生成可靠引用方面具有重要意义。"}}
{"id": "2506.15260", "title": "Domain Adaptation for Image Classification of Defects in Semiconductor Manufacturing", "authors": ["Adrian Poniatowski", "Natalie Gentner", "Manuel Barusco", "Davide Dalle Pezze", "Samuele Salti", "Gian Antonio Susto"], "summary": "In the semiconductor sector, due to high demand but also strong and\nincreasing competition, time to market and quality are key factors in securing\nsignificant market share in various application areas. Thanks to the success of\ndeep learning methods in recent years in the computer vision domain, Industry\n4.0 and 5.0 applications, such as defect classification, have achieved\nremarkable success. In particular, Domain Adaptation (DA) has proven highly\neffective since it focuses on using the knowledge learned on a (source) domain\nto adapt and perform effectively on a different but related (target) domain. By\nimproving robustness and scalability, DA minimizes the need for extensive\nmanual re-labeling or re-training of models. This not only reduces\ncomputational and resource costs but also allows human experts to focus on\nhigh-value tasks. Therefore, we tested the efficacy of DA techniques in\nsemi-supervised and unsupervised settings within the context of the\nsemiconductor field. Moreover, we propose the DBACS approach, a\nCycleGAN-inspired model enhanced with additional loss terms to improve\nperformance. All the approaches are studied and validated on real-world\nElectron Microscope images considering the unsupervised and semi-supervised\nsettings, proving the usefulness of our method in advancing DA techniques for\nthe semiconductor field.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15260v1", "AI": {"title_translation": "半导体制造中缺陷图像分类的域适应", "tldr": "本文研究了半导体缺陷图像分类中的域适应（DA）技术，并提出了一种名为DBACS的CycleGAN启发模型，以减少手动标注和提高性能，并在真实电子显微镜图像上验证了其有效性。", "motivation": "半导体行业对上市时间和质量要求高，深度学习在缺陷分类中取得成功，但需要减少大规模手动重新标注和模型重新训练，降低计算和资源成本，并使专家专注于高价值任务。因此，探索域适应技术以提高鲁棒性和可扩展性是必要的。", "method": "作者在半监督和无监督设置下测试了域适应（DA）技术在半导体领域的有效性。此外，他们提出了一种名为DBACS的方法，该方法是一个受CycleGAN启发并增强了额外损失项的模型，旨在提高性能。所有方法都在真实世界的电子显微镜图像上进行了研究和验证。", "result": "论文在无监督和半监督设置下，通过在真实世界电子显微镜图像上的研究和验证，证明了所提出方法在推进半导体领域域适应技术方面的有效性。", "conclusion": "本文证明了域适应技术，特别是所提出的DBACS方法，在半导体制造缺陷图像分类中是有效的，能够提高鲁棒性和可扩展性，并减少对大量手动标注和再训练的需求。", "translation": "在半导体行业，由于高需求以及日益激烈的竞争，上市时间和质量是确保在各个应用领域获得显著市场份额的关键因素。近年来，得益于深度学习方法在计算机视觉领域的成功，工业4.0和5.0应用，例如缺陷分类，取得了显著成就。特别是，域适应（DA）已被证明高效，因为它专注于利用在（源）域上学习到的知识，以适应并在不同但相关的（目标）域上有效执行。通过提高鲁棒性和可扩展性，DA最大程度地减少了对大量手动重新标注或模型重新训练的需求。这不仅降低了计算和资源成本，还使人类专家能够专注于高价值任务。因此，我们在半导体领域背景下，测试了DA技术在半监督和无监督设置中的功效。此外，我们提出了一种名为DBACS的方法，这是一个受CycleGAN启发并增强了额外损失项的模型，以提高性能。所有方法都在考虑无监督和半监督设置的真实世界电子显微镜图像上进行了研究和验证，证明了我们方法在推进半导体领域DA技术方面的有用性。", "summary": "这项研究探讨了域适应（DA）技术在半导体制造缺陷图像分类中的应用，以应对行业对高效、低成本解决方案的需求。论文测试了DA在半监督和无监督环境下的有效性，并提出了一种名为DBACS的新方法，该方法受CycleGAN启发并加入了额外的损失项以提升性能。研究结果在真实世界的电子显微镜图像上得到了验证，证明了所提方法在推进半导体领域DA技术方面的实用性。", "keywords": "域适应, 图像分类, 半导体制造, 缺陷检测, DBACS", "comments": "这篇论文通过引入域适应技术，特别是其提出的DBACS方法，为半导体制造中的缺陷图像分类提供了一个有前景的解决方案。其创新点在于结合CycleGAN并增加损失项以优化性能，并在半监督和无监督设置下进行验证，这对于数据标注成本高昂的工业领域具有重要意义。该工作有望显著减少人工干预和计算资源消耗，提升缺陷检测的效率和鲁棒性。"}}
{"id": "2506.15021", "title": "SFT-GO: Supervised Fine-Tuning with Group Optimization for Large Language Models", "authors": ["Gyuhak Kim", "Sumiran Singh Thakur", "Su Min Park", "Wei Wei", "Yujia Bao"], "summary": "Supervised fine-tuning (SFT) has become an essential step in tailoring large\nlanguage models (LLMs) to align with human expectations and specific downstream\ntasks. However, existing SFT methods typically treat each training instance as\na uniform sequence, giving equal importance to all tokens regardless of their\nrelevance. This overlooks the fact that only a subset of tokens often contains\ncritical, task-specific information. To address this limitation, we introduce\nSupervised Fine-Tuning with Group Optimization (SFT-GO), a novel approach that\ntreats groups of tokens differently based on their importance.SFT-GO groups\ntokens in each sample based on their importance values and optimizes the LLM\nusing a weighted combination of the worst-group loss and the standard\ncross-entropy loss. This mechanism adaptively emphasizes the most challenging\ntoken groups and guides the model to better handle different group\ndistributions, thereby improving overall learning dynamics. We provide a\ntheoretical analysis of SFT-GO's convergence rate, demonstrating its\nefficiency. Empirically, we apply SFT-GO with three different token grouping\nstrategies and show that models trained with SFT-GO consistently outperform\nbaseline approaches across popular LLM benchmarks. These improvements hold\nacross various datasets and base models, demonstrating the robustness and the\neffectiveness of our method.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15021v1", "AI": {"title_translation": "SFT-GO：基于组优化的指令微调大语言模型", "tldr": "SFT-GO通过对令牌进行分组并差异化优化，解决了现有SFT方法中令牌重要性被忽视的问题，从而提升了LLM的微调效果。", "motivation": "现有的监督微调（SFT）方法将每个训练实例视为统一序列，对所有令牌同等对待，忽视了只有部分令牌包含关键任务特定信息的事实。", "method": "SFT-GO通过令牌的重要性对其进行分组，并使用最差组损失和标准交叉熵损失的加权组合来优化大语言模型。这种机制自适应地强调最具挑战性的令牌组。", "result": "经验证明，SFT-GO在三种不同的令牌分组策略下，在流行的大语言模型基准测试中始终优于基线方法，并且在各种数据集和基础模型上都表现出改进，证明了其鲁棒性和有效性。", "conclusion": "SFT-GO通过引入基于令牌重要性的分组优化，显著提升了大型语言模型在监督微调中的性能和鲁棒性。", "translation": "监督微调（SFT）已成为调整大型语言模型（LLMs）以符合人类期望和特定下游任务的关键步骤。然而，现有的SFT方法通常将每个训练实例视为统一序列，对所有令牌给予同等重要性，而不管其相关性如何。这忽略了通常只有部分令牌包含关键的任务特定信息这一事实。为了解决这一局限性，我们引入了SFT-GO（Supervised Fine-Tuning with Group Optimization），这是一种新颖的方法，它根据令牌的重要性对令牌组进行差异化处理。SFT-GO根据重要性值对每个样本中的令牌进行分组，并使用最差组损失和标准交叉熵损失的加权组合来优化LLM。这种机制自适应地强调最具挑战性的令牌组，并引导模型更好地处理不同的组分布，从而改善整体学习动态。我们提供了SFT-GO收敛速度的理论分析，证明了其效率。在经验上，我们将SFT-GO应用于三种不同的令牌分组策略，结果表明，使用SFT-GO训练的模型在流行的大语言模型基准测试中始终优于基线方法。这些改进在各种数据集和基础模型上都成立，证明了我们方法的鲁棒性和有效性。", "summary": "本文提出了SFT-GO，一种新的监督微调方法，旨在解决现有SFT对所有令牌一视同仁的问题。SFT-GO根据令牌的重要性对其进行分组，并通过结合最差组损失和标准交叉熵损失进行优化，从而更有效地指导模型学习。理论分析证明了其收敛效率，实验结果表明SFT-GO在多个LLM基准测试中表现优于现有方法，且具有良好的鲁棒性。", "keywords": "监督微调, 组优化, 大语言模型, 令牌重要性, SFT-GO", "comments": "SFT-GO的创新点在于它认识到并解决了SFT中令牌重要性不均的问题，通过引入“组优化”机制，使得模型能够更聚焦于关键和困难的令牌组，这对于提升LLM的微调效果具有重要意义。其结合理论分析和广泛实证验证，增强了方法的可信度。"}}
{"id": "2506.15545", "title": "RATTENTION: Towards the Minimal Sliding Window Size in Local-Global Attention Models", "authors": ["Bailin Wang", "Chang Lan", "Chong Wang", "Ruoming Pang"], "summary": "Local-global attention models have recently emerged as compelling\nalternatives to standard Transformers, promising improvements in both training\nand inference efficiency. However, the crucial choice of window size presents a\nPareto tradeoff: larger windows maintain performance akin to full attention but\noffer minimal efficiency gains in short-context scenarios, while smaller\nwindows can lead to performance degradation. Current models, such as Gemma2 and\nMistral, adopt conservative window sizes (e.g., 4096 out of an 8192 pretraining\nlength) to preserve performance. This work investigates strategies to shift\nthis Pareto frontier, enabling local-global models to achieve efficiency gains\neven in short-context regimes. Our core motivation is to address the intrinsic\nlimitation of local attention -- its complete disregard for tokens outside the\ndefined window. We explore RATTENTION, a variant of local attention integrated\nwith a specialized linear attention mechanism designed to capture information\nfrom these out-of-window tokens. Pretraining experiments at the 3B and 12B\nscales demonstrate that RATTENTION achieves a superior Pareto tradeoff between\nperformance and efficiency. As a sweet spot, RATTENTION with a window size of\njust 512 consistently matches the performance of full-attention models across\ndiverse settings. Furthermore, the recurrent nature inherent in the linear\nattention component of RATTENTION contributes to enhanced long-context\nperformance, as validated on the RULER benchmark. Crucially, these improvements\ndo not compromise training efficiency; thanks to a specialized kernel\nimplementation and the reduced window size, RATTENTION maintains training\nspeeds comparable to existing state-of-the-art approaches.", "comment": "9 pages", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15545v1", "AI": {"title_translation": "RATTENTION：迈向局部-全局注意力模型中的最小滑动窗口大小", "tldr": "RATTENTION是一种新型的局部-全局注意力模型，通过结合线性注意力机制捕获窗口外信息，实现在保持性能的同时显著减小滑动窗口大小（例如512），从而提高短上下文和长上下文场景的效率，且不影响训练速度。", "motivation": "当前局部-全局注意力模型在窗口大小选择上存在帕累托权衡：大窗口性能好但效率提升有限，小窗口则性能下降。现有模型采用保守的大窗口。核心动机是解决局部注意力固有的局限性——完全忽略窗口外的token，旨在即使在短上下文场景下也能实现效率提升。", "method": "本文探索了RATTENTION，这是一种局部注意力变体，其集成了专门的线性注意力机制，旨在捕获来自窗口外token的信息。通过专门的内核实现和减小的窗口大小，RATTENTION保持了训练效率。", "result": "在3B和12B规模的预训练实验表明，RATTENTION在性能和效率之间实现了卓越的帕累托权衡。RATTENTION在窗口大小仅为512时，能持续匹配全注意力模型在各种设置下的性能。此外，RATTENTION中线性注意力组件固有的循环特性增强了长上下文性能，并在RULER基准测试中得到验证。这些改进并未牺牲训练效率，训练速度与现有最先进方法相当。", "conclusion": "RATTENTION通过创新的局部-线性注意力结合，成功地解决了局部-全局注意力模型中滑动窗口大小的帕累托权衡问题。它能够在显著减小窗口大小（如512）的同时保持甚至提升性能，特别是在长上下文处理方面，并且不影响训练效率，为构建更高效的Transformer模型提供了新的方向。", "translation": "局部-全局注意力模型最近已成为标准Transformer的引人注目的替代方案，有望在训练和推理效率方面实现改进。然而，窗口大小的关键选择呈现出帕累托权衡：较大的窗口保持与全注意力相似的性能，但在短上下文场景中提供的效率增益微乎其微，而较小的窗口可能导致性能下降。当前模型，如Gemma2和Mistral，采用保守的窗口大小（例如，8192预训练长度中的4096）以保持性能。这项工作研究了如何改变这种帕累托边界的策略，使局部-全局模型即使在短上下文情况下也能实现效率增益。我们的核心动机是解决局部注意力固有的局限性——它完全忽略了定义窗口之外的token。我们探索了RATTENTION，这是一种局部注意力变体，其集成了专门的线性注意力机制，旨在捕获来自这些窗口外token的信息。在3B和12B规模的预训练实验表明，RATTENTION在性能和效率之间实现了卓越的帕累托权衡。作为一个最佳选择，窗口大小仅为512的RATTENTION在各种设置下持续匹配全注意力模型的性能。此外，RATTENTION中线性注意力组件固有的循环特性有助于增强长上下文性能，这在RULER基准测试中得到了验证。至关重要的是，这些改进并未牺牲训练效率；得益于专门的内核实现和减小的窗口大小，RATTENTION保持了与现有最先进方法相当的训练速度。", "summary": "本文提出RATTENTION，一种新型局部-全局注意力模型，旨在解决现有模型在滑动窗口大小选择上的性能与效率权衡问题。RATTENTION通过将局部注意力与专门的线性注意力机制相结合，有效捕获窗口外信息。实验证明，在保持与全注意力模型相当的性能下，RATTENTION可将窗口大小显著减小至512，并提升长上下文处理能力，同时不影响训练效率，从而实现了性能与效率的更优平衡。", "keywords": "RATTENTION, 局部-全局注意力, 滑动窗口, 效率, 长上下文", "comments": "RATTENTION的创新之处在于其将局部注意力与线性注意力机制巧妙结合，克服了传统局部注意力无法处理窗口外信息的局限性。这不仅使得模型能够在更小的滑动窗口下保持甚至超越全注意力的性能，还通过线性注意力的循环特性提升了长上下文能力。其对训练效率的保持也显示了实际应用潜力，为构建更高效、更长上下文感知的Transformer模型提供了重要思路。"}}
{"id": "2506.15025", "title": "Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size", "authors": ["Soufiane Hayou", "Liyuan Liu"], "summary": "Pretraining large language models is a costly process. To make this process\nmore efficient, several methods have been proposed to optimize model\narchitecture/parametrization and hardware use. On the parametrization side,\n$\\mu P$ (Maximal Update Parametrization) parametrizes model weights and\nlearning rate (LR) in a way that makes hyperparameters (HPs) transferable with\nwidth (embedding dimension): HPs can be tuned for a small model and used for\nlarger models without additional tuning. While $\\mu$P showed impressive results\nin practice, recent empirical studies have reported conflicting observations\nwhen applied to LLMs. One limitation of the theory behind $\\mu$P is the fact\nthat input dimension (vocabulary size in LLMs) is considered fixed when taking\nthe width to infinity. This is unrealistic since vocabulary size is generally\nmuch larger than width in practice. In this work, we provide a theoretical\nanalysis of the effect of vocabulary size on training dynamics, and\nsubsequently show that as vocabulary size increases, the training dynamics\n\\emph{interpolate between the $\\mu$P regime and another regime that we call\nLarge Vocab (LV) Regime}, where optimal scaling rules are different from those\npredicted by $\\mu$P. Our analysis reveals that in the LV regime, the optimal\nembedding LR to hidden LR ratio should roughly scale as $\\Theta(\\sqrt{width})$,\nsurprisingly close to the empirical findings previously reported in the\nliterature, and different from the $\\Theta(width)$ ratio predicted by $\\mu$P.\nWe conduct several experiments to validate our theory, and pretrain a 1B model\nfrom scratch to show the benefit of our suggested scaling rule for the\nembedding LR.", "comment": "TD,LR: How to set the learning rate for emebdding layer in LLMs?", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15025v1", "AI": {"title_translation": "大型语言模型中最佳嵌入学习率：词汇量大小的影响", "tldr": "研究发现LLM中词汇量大小会影响最佳嵌入学习率的缩放规则，提出了一个新的“大词汇量（LV）”机制，其中最优嵌入学习率与隐藏层学习率之比与$\\\\sqrt{width}$成比例，而非$\\\\mu P$预测的$width$。", "motivation": "预训练大型语言模型成本高昂，而现有的$\\\\mu P$理论在应用于LLM时存在局限性，特别是在处理巨大的词汇量时，其假设输入维度固定与实际不符，导致经验观察结果存在冲突。", "method": "本文通过对词汇量大小对训练动态影响的理论分析，并辅以多项实验验证，包括从头预训练一个1B模型，来验证提出的缩放规则。", "result": "随着词汇量增加，训练动态在$\\\\mu P$机制和新的“大词汇量（LV）”机制之间插值。在LV机制下，最佳嵌入学习率与隐藏层学习率之比应大致按$\\\\Theta(\\\\sqrt{width})$缩放，这与现有经验发现吻合，但与$\\\\mu P$预测的$\\\\Theta(width)$不同。", "conclusion": "词汇量大小是影响LLM训练动态和最优学习率缩放规则的关键因素。在词汇量较大时，应采用新的LV机制下的缩放规则，即嵌入学习率与隐藏层学习率之比与$\\\\sqrt{width}$成比例，这能带来实际效益。", "translation": "预训练大型语言模型是一个成本高昂的过程。为了提高这一过程的效率，人们提出了几种方法来优化模型架构/参数化和硬件使用。在参数化方面，$\\\\mu P$（最大更新参数化）以一种使超参数（HPs）可随宽度（嵌入维度）转移的方式参数化模型权重和学习率（LR）：超参数可以针对小型模型进行调整，然后用于大型模型而无需额外调整。虽然$\\\\mu P$在实践中取得了令人印象深刻的结果，但最近的实证研究报告称，在应用于大型语言模型时存在相互冲突的观察结果。$\\\\mu P$理论的一个局限性在于，在将宽度取到无穷大时，输入维度（大型语言模型中的词汇量大小）被认为是固定的。这不符合实际，因为在实践中词汇量大小通常远大于宽度。在这项工作中，我们对词汇量大小对训练动态的影响进行了理论分析，并随后表明，随着词汇量的增加，训练动态在$\\\\mu P$机制和我们称之为大词汇量（LV）机制之间插值，其中最优缩放规则与$\\\\mu P$预测的不同。我们的分析揭示，在LV机制中，最佳嵌入学习率与隐藏层学习率之比应大致按$\\\\Theta(\\\\sqrt{width})$缩放，这与文献中先前报道的经验发现惊人地接近，并且与$\\\\mu P$预测的$\\\\Theta(width)$比率不同。我们进行了多项实验来验证我们的理论，并从头开始预训练了一个1B模型，以展示我们建议的嵌入学习率缩放规则的益处。", "summary": "本文探讨了大型语言模型（LLM）中词汇量大小对训练动态和最优学习率缩放规则的影响。针对$\\\\mu P$理论在LLM中因未考虑词汇量变化而导致的局限性，作者进行了理论分析，并提出了“大词汇量（LV）”机制。研究发现，当词汇量增大时，训练动态在$\\\\mu P$和LV机制之间插值，且在LV机制下，最佳嵌入学习率与隐藏层学习率之比应与$\\\\sqrt{width}$成比例，而非$\\\\mu P$预测的$width$。这一发现与现有经验观察相符，并通过实验验证了其有效性，包括预训练一个1B模型。", "keywords": "大型语言模型, 学习率, 嵌入层, 词汇量大小, $\\\\mu P$理论, 缩放规则", "comments": "本文对$\\\\mu P$理论在LLM应用中的局限性进行了深入分析，特别是引入了词汇量大小这一关键变量，并提出了新的“大词汇量（LV）”机制。其理论分析解释了现有经验观察中的冲突，并提出了更符合实际的嵌入学习率缩放规则。这对于优化LLM的预训练效率和超参数调优具有重要指导意义，有助于克服现有理论的不足。"}}
{"id": "2506.15553", "title": "Approximating Language Model Training Data from Weights", "authors": ["John X. Morris", "Junjie Oscar Yin", "Woojeong Kim", "Vitaly Shmatikov", "Alexander M. Rush"], "summary": "Modern language models often have open weights but closed training data. We\nformalize the problem of data approximation from model weights and propose\nseveral baselines and metrics. We develop a gradient-based approach that\nselects the highest-matching data from a large public text corpus and show its\neffectiveness at recovering useful data given only weights of the original and\nfinetuned models. Even when none of the true training data is known, our method\nis able to locate a small subset of public Web documents can be used to train a\nmodel to close to the original model performance given models trained for both\nclassification and supervised-finetuning. On the AG News classification task,\nour method improves performance from 65% (using randomly selected data) to 80%,\napproaching the expert benchmark of 88%. When applied to a model trained with\nSFT on MSMARCO web documents, our method reduces perplexity from 3.3 to 2.3,\ncompared to an expert LLAMA model's perplexity of 2.0.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15553v1", "AI": {"title_translation": "从模型权重近似语言模型训练数据", "tldr": "该研究提出了一种基于梯度的方法，用于从语言模型权重中近似训练数据。该方法能有效恢复有用数据，并使训练出的模型性能接近原始模型，即使在不了解真实训练数据的情况下。", "motivation": "现代语言模型通常权重开放但训练数据封闭，因此需要解决从模型权重近似训练数据的问题。", "method": "开发了一种基于梯度的方法，该方法从大型公共文本语料库中选择匹配度最高的数据，以恢复给定原始和微调模型权重下的有用数据。", "result": "在AG News分类任务上，性能从65%提升至80%，接近88%的专家基准。在MSMARCO网络文档上训练的模型，困惑度从3.3降低到2.3，接近专家LLAMA模型的2.0。该方法能够在不知道真实训练数据的情况下，找到一小部分公共网络文档来训练模型，使其性能接近原始模型。", "conclusion": "该方法能够有效地从模型权重中恢复有用的训练数据，并且即使在不知道真实训练数据的情况下，也能用于训练性能接近原始模型的模型。", "translation": "现代语言模型通常权重开放但训练数据封闭。我们形式化了从模型权重近似数据的问题，并提出了几种基线和度量。我们开发了一种基于梯度的方法，从大型公共文本语料库中选择匹配度最高的数据，并展示了在仅给定原始和微调模型权重的情况下恢复有用数据的有效性。即使在不知道任何真实训练数据的情况下，我们的方法也能够找到一小部分公共网络文档，这些文档可以用于训练模型，使其性能接近原始模型，适用于分类和监督微调模型。在AG News分类任务上，我们的方法将性能从65%（使用随机选择的数据）提高到80%，接近88%的专家基准。当应用于使用SFT在MSMARCO网络文档上训练的模型时，我们的方法将困惑度从3.3降低到2.3，而专家LLAMA模型的困惑度为2.0。", "summary": "本论文解决了语言模型权重开放但训练数据封闭的背景下，从模型权重近似训练数据的难题。文章形式化了这个问题，并提出了一种新颖的基于梯度的方法。该方法能有效地从公共语料库中识别出高度匹配的数据，并证明了其仅利用原始和微调模型的权重就能恢复有用训练数据的能力。研究表明，即使无法获取真实的训练数据，通过该方法识别出的一小部分公共网络文档也能训练出性能接近原始模型的模型，例如在AG News分类任务上准确率从65%提升到80%，在MSMARCO数据集上困惑度从3.3降低到2.3。", "keywords": "语言模型, 训练数据近似, 模型权重, 基于梯度, 数据恢复", "comments": "这项工作的创新之处在于形式化并提供了一种实用的、基于梯度的方法，用于从模型权重推断训练数据，这对于当前大型语言模型训练数据集不公开的普遍实践具有高度相关性。该研究对于理解模型偏差、重现结果以及潜在地缓解知识产权问题具有重要意义。在不直接访问原始数据集的情况下近似训练数据的能力是一个显著的进步。"}}
{"id": "2506.15279", "title": "BCRNet: Enhancing Landmark Detection in Laparoscopic Liver Surgery via Bezier Curve Refinement", "authors": ["Qian Li", "Feng Liu", "Shuojue Yang", "Daiyun Shen", "Yueming Jin"], "summary": "Laparoscopic liver surgery, while minimally invasive, poses significant\nchallenges in accurately identifying critical anatomical structures. Augmented\nreality (AR) systems, integrating MRI/CT with laparoscopic images based on\n2D-3D registration, offer a promising solution for enhancing surgical\nnavigation. A vital aspect of the registration progress is the precise\ndetection of curvilinear anatomical landmarks in laparoscopic images. In this\npaper, we propose BCRNet (Bezier Curve Refinement Net), a novel framework that\nsignificantly enhances landmark detection in laparoscopic liver surgery\nprimarily via the Bezier curve refinement strategy. The framework starts with a\nMulti-modal Feature Extraction (MFE) module designed to robustly capture\nsemantic features. Then we propose Adaptive Curve Proposal Initialization\n(ACPI) to generate pixel-aligned Bezier curves and confidence scores for\nreliable initial proposals. Additionally, we design the Hierarchical Curve\nRefinement (HCR) mechanism to enhance these proposals iteratively through a\nmulti-stage process, capturing fine-grained contextual details from multi-scale\npixel-level features for precise Bezier curve adjustment. Extensive evaluations\non the L3D and P2ILF datasets demonstrate that BCRNet outperforms\nstate-of-the-art methods, achieving significant performance improvements. Code\nwill be available.", "comment": "Accepted at MICCAI 2025, 11 pages, 2 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15279v1", "AI": {"title_translation": "BCRNet：通过贝塞尔曲线细化增强腹腔镜肝脏手术中的地标检测", "tldr": "BCRNet通过贝塞尔曲线细化策略，显著增强了腹腔镜肝脏手术中的地标检测。", "motivation": "腹腔镜肝脏手术中准确识别关键解剖结构面临挑战。增强现实（AR）系统整合MRI/CT与腹腔镜图像以增强手术导航，其中一个关键方面是精确检测腹腔镜图像中的曲线解剖地标。", "method": "本文提出了BCRNet（Bezier Curve Refinement Net）框架，主要通过贝塞尔曲线细化策略增强地标检测。该框架首先包含一个多模态特征提取（MFE）模块，用于鲁棒地捕获语义特征。接着，提出了自适应曲线提议初始化（ACPI）模块，用于生成像素对齐的贝塞尔曲线和置信度分数，以提供可靠的初始提议。此外，设计了分层曲线细化（HCR）机制，通过多阶段过程迭代增强这些提议，从多尺度像素级特征中捕获细粒度上下文细节，以实现精确的贝塞尔曲线调整。", "result": "在L3D和P2ILF数据集上的广泛评估表明，BCRNet优于现有最先进的方法，取得了显著的性能提升。", "conclusion": "BCRNet框架通过贝塞尔曲线细化策略，有效提升了腹腔镜肝脏手术中曲线解剖地标的检测精度，优于现有方法。", "translation": "腹腔镜肝脏手术虽然是微创的，但在准确识别关键解剖结构方面带来了巨大的挑战。增强现实（AR）系统，通过基于2D-3D配准将MRI/CT与腹腔镜图像整合，为增强手术导航提供了一个有前景的解决方案。配准过程中一个至关重要的方面是精确检测腹腔镜图像中的曲线解剖地标。本文中，我们提出了BCRNet（Bezier Curve Refinement Net），一个主要通过贝塞尔曲线细化策略显著增强腹腔镜肝脏手术中地标检测的新型框架。该框架始于一个多模态特征提取（MFE）模块，旨在鲁棒地捕获语义特征。然后，我们提出了自适应曲线提议初始化（ACPI），以生成像素对齐的贝塞尔曲线和置信分数，用于可靠的初始提议。此外，我们设计了分层曲线细化（HCR）机制，通过多阶段过程迭代增强这些提议，从多尺度像素级特征中捕获细粒度上下文细节，以实现精确的贝塞尔曲线调整。在L3D和P2ILF数据集上的广泛评估表明，BCRNet优于现有最先进的方法，取得了显著的性能提升。代码将可用。", "summary": "BCRNet是一个新颖的框架，旨在通过贝塞尔曲线细化策略显著增强腹腔镜肝脏手术中的曲线解剖地标检测。该框架包括多模态特征提取（MFE）模块、自适应曲线提议初始化（ACPI）模块用于生成初始贝塞尔曲线提议，以及分层曲线细化（HCR）机制用于迭代精确调整。在L3D和P2ILF数据集上的评估证明，BCRNet的性能优于现有最先进的方法。", "keywords": "腹腔镜手术, 地标检测, 贝塞尔曲线, 增强现实", "comments": "该论文的创新点在于其独特的贝塞尔曲线细化策略，以及为实现精确曲线地标检测而设计的结构化方法（MFE、ACPI、HCR）。这对于增强现实辅助的手术导航至关重要，有望提升手术的安全性和效率。"}}
{"id": "2506.15051", "title": "Sequential Policy Gradient for Adaptive Hyperparameter Optimization", "authors": ["Zheng Li", "Jerry Cheng", "Huanying Helen Gu"], "summary": "Reinforcement learning is essential for neural architecture search and\nhyperparameter optimization, but the conventional approaches impede widespread\nuse due to prohibitive time and computational costs. Inspired by DeepSeek-V3\nmulti-token prediction architecture, we propose Sequential Policy Gradient\nmodeling (SPG), a novel trajectory generation paradigm for lightweight online\nhyperparameter optimization. In contrast to conventional policy gradient\nmethods, SPG extends the base model with temporary modules, enabling it to\ngenerate state-action (padded) trajectories in a single forward pass. Our\nexperiments demonstrate that models gain performance when retrained with SPG on\ntheir original datasets and also outperform standard transfer fine-tuning. We\nevaluate on five datasets spanning computer vision (ImageNet, COCO), natural\nlanguage processing (GLUE, SQuAD), and audio (SUPERB) to assess the industrial\napplicability of SPG. The proposed method demonstrates consistent improvements\nacross widely adopted models, achieving performance gains of $+0.2\\sim7\\%$,\nwith significantly low computational costs. Fully reproducible code and\npre-trained models: https://huggingface.co/UniversalAlgorithmic/SPG.", "comment": "10 pages, 2 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15051v1", "AI": {"title_translation": "自适应超参数优化的序列策略梯度", "tldr": "SPG是一种新型轻量级在线超参数优化方法，通过单次前向传播生成轨迹，显著降低计算成本并提升模型性能。", "motivation": "传统强化学习方法在神经网络架构搜索和超参数优化中面临过高的计算成本，阻碍了其广泛应用。", "method": "受DeepSeek-V3多令牌预测架构启发，提出序列策略梯度建模（SPG）。SPG通过用临时模块扩展基础模型，使其能够在单次前向传播中生成状态-动作（填充）轨迹，从而实现轻量级在线超参数优化。", "result": "实验表明，使用SPG重新训练模型在原始数据集上性能有所提升，并且优于标准迁移微调。在计算机视觉（ImageNet, COCO）、自然语言处理（GLUE, SQuAD）和音频（SUPERB）的五个数据集上评估，SPG在广泛采用的模型上持续实现+0.2%~7%的性能提升，且计算成本显著降低。", "conclusion": "SPG是一种高效且计算成本低的超参数优化方法，能够在多种任务和模型上持续提升性能，并具有良好的工业适用性。", "translation": "强化学习对于神经网络架构搜索和超参数优化至关重要，但传统方法因高昂的时间和计算成本而阻碍了其广泛应用。受DeepSeek-V3多令牌预测架构的启发，我们提出了序列策略梯度建模（SPG），这是一种用于轻量级在线超参数优化的新型轨迹生成范式。与传统策略梯度方法不同，SPG通过临时模块扩展了基础模型，使其能够在单次前向传播中生成状态-动作（填充）轨迹。我们的实验表明，使用SPG在其原始数据集上重新训练的模型性能有所提升，并且也优于标准迁移微调。我们在涵盖计算机视觉（ImageNet，COCO）、自然语言处理（GLUE，SQuAD）和音频（SUPERB）的五个数据集上进行评估，以评估SPG的工业适用性。所提出的方法在广泛采用的模型上表现出持续的改进，实现了+0.2%~7%的性能提升，同时计算成本显著降低。完全可复现的代码和预训练模型：https://huggingface.co/UniversalAlgorithmic/SPG。", "summary": "SPG是一种新颖的轻量级在线超参数优化方法。它通过在单次前向传播中生成轨迹，显著降低了传统强化学习方法的计算成本。实验表明，SPG在多种计算机视觉、自然语言处理和音频任务上，能为模型带来显著的性能提升（0.2%~7%），同时保持极低的计算开销，展现出强大的工业应用潜力。", "keywords": "序列策略梯度, 超参数优化, 强化学习, 轻量级, 在线优化", "comments": "SPG的创新在于其单次前向传播生成轨迹的设计，极大地降低了超参数优化的计算成本，解决了传统强化学习方法效率低下的痛点。其在多领域数据集上的普适性和显著的性能提升，预示着其在实际应用中的巨大潜力。"}}
{"id": "2506.15560", "title": "RaCalNet: Radar Calibration Network for Sparse-Supervised Metric Depth Estimation", "authors": ["Xingrui Qin", "Wentao Zhao", "Chuan Cao", "Yihe Niu", "Houcheng Jiang", "Jingchuan Wang"], "summary": "Dense metric depth estimation using millimeter-wave radar typically requires\ndense LiDAR supervision, generated via multi-frame projection and\ninterpolation, to guide the learning of accurate depth from sparse radar\nmeasurements and RGB images. However, this paradigm is both costly and\ndata-intensive. To address this, we propose RaCalNet, a novel framework that\neliminates the need for dense supervision by using sparse LiDAR to supervise\nthe learning of refined radar measurements, resulting in a supervision density\nof merely around 1% compared to dense-supervised methods. Unlike previous\napproaches that associate radar points with broad image regions and rely\nheavily on dense labels, RaCalNet first recalibrates and refines sparse radar\npoints to construct accurate depth priors. These priors then serve as reliable\nanchors to guide monocular depth prediction, enabling metric-scale estimation\nwithout resorting to dense supervision. This design improves structural\nconsistency and preserves fine details. Despite relying solely on sparse\nsupervision, RaCalNet surpasses state-of-the-art dense-supervised methods,\nproducing depth maps with clear object contours and fine-grained textures.\nExtensive experiments on the ZJU-4DRadarCam dataset and real-world deployment\nscenarios demonstrate its effectiveness, reducing RMSE by 35.30% and 34.89%,\nrespectively.", "comment": "9 pages, 7 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15560v1", "AI": {"title_translation": "RaCalNet: 用于稀疏监督度量深度估计的雷达校准网络", "tldr": "RaCalNet提出了一种新的框架，通过使用稀疏的LiDAR监督来校准和优化雷达测量，从而在仅1%的监督密度下实现高精度的度量深度估计，超越了现有的密集监督方法。", "motivation": "毫米波雷达的密集度量深度估计通常需要密集的LiDAR监督，但这既昂贵又数据密集，限制了其应用。", "method": "RaCalNet通过使用稀疏LiDAR监督来学习精炼的雷达测量，从而消除了对密集监督的需求。它首先重新校准和优化稀疏雷达点以构建准确的深度先验，然后这些先验作为可靠的锚点来指导单目深度预测，实现度量尺度的估计。", "result": "RaCalNet在仅依赖稀疏监督的情况下，超越了最先进的密集监督方法，生成的深度图具有清晰的物体轮廓和精细的纹理。在ZJU-4DRadarCam数据集和实际部署场景中，RMSE分别降低了35.30%和34.89%。", "conclusion": "RaCalNet在没有密集监督的情况下，通过创新的雷达点校准和精炼方法，实现了卓越的度量深度估计性能，并在实验中展现了其有效性。", "translation": "毫米波雷达的密集度量深度估计通常需要密集的LiDAR监督，这通过多帧投影和插值生成，以指导从稀疏雷达测量和RGB图像中学习准确的深度。然而，这种范式既昂贵又数据密集。为了解决这个问题，我们提出了RaCalNet，一个新颖的框架，通过使用稀疏LiDAR监督来学习精炼的雷达测量，从而消除了对密集监督的需求，与密集监督方法相比，其监督密度仅为1%左右。与以往将雷达点与宽泛图像区域关联并严重依赖密集标签的方法不同，RaCalNet首先重新校准和精炼稀疏雷达点以构建准确的深度先验。这些先验随后作为可靠的锚点来指导单目深度预测，从而无需依赖密集监督即可实现度量尺度的估计。这种设计提高了结构一致性并保留了精细细节。尽管仅依赖稀疏监督，RaCalNet超越了最先进的密集监督方法，生成的深度图具有清晰的物体轮廓和精细的纹理。在ZJU-4DRadarCam数据集和实际部署场景中的大量实验证明了其有效性，RMSE分别降低了35.30%和34.89%。", "summary": "RaCalNet是一个新颖的框架，旨在解决毫米波雷达密集度量深度估计中对昂贵且数据密集型密集LiDAR监督的依赖。它通过使用稀疏LiDAR监督来校准和精炼雷达测量，生成精确的深度先验，这些先验随后指导单目深度预测。该方法仅需1%的监督密度，却能生成具有清晰细节的高质量深度图，并在实验中显著优于现有的密集监督方法，显著降低了RMSE。", "keywords": "雷达校准, 深度估计, 稀疏监督, 度量深度, RaCalNet", "comments": "RaCalNet的创新点在于其提出了一种高效的稀疏监督策略，通过对雷达数据的精细校准和先验构建，解决了传统密集监督的成本和数据瓶颈。其在极低监督密度下超越SOTA密集监督方法的表现，显示出巨大的应用潜力，尤其是在资源受限或难以获取密集标注数据的场景。"}}
{"id": "2506.15285", "title": "AI-driven visual monitoring of industrial assembly tasks", "authors": ["Mattia Nardon", "Stefano Messelodi", "Antonio Granata", "Fabio Poiesi", "Alberto Danese", "Davide Boscaini"], "summary": "Visual monitoring of industrial assembly tasks is critical for preventing\nequipment damage due to procedural errors and ensuring worker safety. Although\ncommercial solutions exist, they typically require rigid workspace setups or\nthe application of visual markers to simplify the problem. We introduce ViMAT,\na novel AI-driven system for real-time visual monitoring of assembly tasks that\noperates without these constraints. ViMAT combines a perception module that\nextracts visual observations from multi-view video streams with a reasoning\nmodule that infers the most likely action being performed based on the observed\nassembly state and prior task knowledge. We validate ViMAT on two assembly\ntasks, involving the replacement of LEGO components and the reconfiguration of\nhydraulic press molds, demonstrating its effectiveness through quantitative and\nqualitative analysis in challenging real-world scenarios characterized by\npartial and uncertain visual observations. Project page:\nhttps://tev-fbk.github.io/ViMAT", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15285v1", "AI": {"title_translation": "人工智能驱动的工业装配任务视觉监控", "tldr": "ViMAT是一个新型AI驱动系统，用于实时视觉监控工业装配任务，无需刚性设置或视觉标记，并在实际场景中表现出有效性。", "motivation": "工业装配任务的视觉监控对于防止程序错误造成的设备损坏和确保工人安全至关重要。现有商业解决方案通常需要僵硬的工作空间设置或应用视觉标记来简化问题，这限制了其应用。", "method": "ViMAT系统结合了一个感知模块，该模块从多视角视频流中提取视觉观察结果；以及一个推理模块，该模块根据观察到的装配状态和先验任务知识推断最可能的正在执行的动作。", "result": "ViMAT在两项装配任务中进行了验证，包括乐高组件的更换和液压机模具的重新配置，通过定量和定性分析，在具有部分和不确定视觉观察的挑战性现实场景中，证明了其有效性。", "conclusion": "ViMAT系统在无需额外约束的情况下，能够有效实现工业装配任务的实时视觉监控，并在复杂现实场景中展现出强大的鲁棒性和实用性。", "translation": "工业装配任务的视觉监控对于防止程序错误造成的设备损坏和确保工人安全至关重要。尽管存在商业解决方案，但它们通常需要僵硬的工作空间设置或应用视觉标记来简化问题。我们引入了ViMAT，一个新型的人工智能驱动系统，用于实时视觉监控装配任务，且无需这些约束。ViMAT结合了一个感知模块，该模块从多视角视频流中提取视觉观察结果，以及一个推理模块，该模块根据观察到的装配状态和先验任务知识推断最可能的正在执行的动作。我们在两项装配任务中验证了ViMAT，包括乐高组件的更换和液压机模具的重新配置，通过定量和定性分析，在具有部分和不确定视觉观察的挑战性现实场景中，证明了其有效性。项目页面：https://tev-fbk.github.io/ViMAT", "summary": "本论文介绍了ViMAT，一个创新的人工智能驱动系统，旨在实现工业装配任务的实时视觉监控，克服了现有商业解决方案对刚性设置或视觉标记的依赖。ViMAT通过结合感知模块（从多视角视频流提取视觉信息）和推理模块（根据观察状态和先验知识推断动作）来运作。该系统已在乐高组件更换和液压机模具重新配置等实际工业场景中得到验证，展示了其在部分和不确定视觉观察条件下的有效性和鲁棒性。", "keywords": "工业装配, 视觉监控, 人工智能, 实时系统, 机器视觉", "comments": "ViMAT的创新之处在于其无需额外物理约束（如刚性设置或视觉标记）即可进行视觉监控的能力，这显著提升了其在复杂工业环境中的适用性。其模块化设计（感知与推理分离）也使其具有良好的可扩展性。该系统对于提高工业生产效率、减少错误和保障工人安全具有重要意义。"}}
{"id": "2506.15054", "title": "Muon Optimizes Under Spectral Norm Constraints", "authors": ["Lizhang Chen", "Jonathan Li", "Qiang Liu"], "summary": "The pursuit of faster optimization algorithms remains an active and important\nresearch direction in deep learning. Recently, the Muon optimizer [JJB+24] has\ndemonstrated promising empirical performance, but its theoretical foundation\nremains less understood. In this paper, we bridge this gap and provide a\ntheoretical analysis of Muon by placing it within the Lion-$\\mathcal{K}$ family\nof optimizers [CLLL24]. Specifically, we show that Muon corresponds to\nLion-$\\mathcal{K}$ when equipped with the nuclear norm, and we leverage the\ntheoretical results of Lion-$\\mathcal{K}$ to establish that Muon (with\ndecoupled weight decay) implicitly solves an optimization problem that enforces\na constraint on the spectral norm of weight matrices. This perspective not only\ndemystifies the implicit regularization effects of Muon but also leads to\nnatural generalizations through varying the choice of convex map $\\mathcal{K}$,\nallowing for the exploration of a broader class of implicitly regularized and\nconstrained optimization algorithms.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15054v1", "AI": {"title_translation": "Muon 在谱范数约束下优化", "tldr": "本文通过将Muon优化器置于Lion-$\\mathcal{K}$优化器家族中，对其进行了理论分析，揭示了其隐式正则化效应和谱范数约束。", "motivation": "深度学习中对更快速优化算法的追求是一个活跃且重要的研究方向。Muon优化器虽然经验性能良好，但其理论基础尚不明确，因此需要填补这一空白。", "method": "将Muon优化器置于Lion-$\\mathcal{K}$优化器家族中进行分析。具体地，证明Muon在配备核范数时对应于Lion-$\\mathcal{K}$，并利用Lion-$\\mathcal{K}$的理论结果来证明Muon（带解耦权重衰减）隐式地解决了对权重矩阵的谱范数施加约束的优化问题。", "result": "证明了Muon（带解耦权重衰减）隐式地解决了对权重矩阵的谱范数施加约束的优化问题。这一视角揭示了Muon的隐式正则化效应。", "conclusion": "通过将Muon置于Lion-$\\mathcal{K}$家族中，本文不仅阐明了Muon的隐式正则化效应，还为通过改变凸映射$\\mathcal{K}$的选择来探索更广泛的隐式正则化和约束优化算法提供了途径。", "translation": "在深度学习中，追求更快的优化算法仍然是一个活跃而重要的研究方向。最近，Muon优化器 [JJB+24] 展现出有前景的经验性能，但其理论基础尚不明确。在本文中，我们弥补了这一空白，通过将Muon置于Lion-$\\mathcal{K}$优化器家族 [CLLL24] 中，对其进行了理论分析。具体来说，我们表明当Muon配备核范数时，它对应于Lion-$\\mathcal{K}$，并且我们利用Lion-$\\mathcal{K}$的理论结果来确立Muon（带解耦权重衰减）隐式地解决了一个对权重矩阵的谱范数施加约束的优化问题。这一视角不仅揭示了Muon的隐式正则化效应，而且通过改变凸映射$\\mathcal{K}$的选择，导致了自然的推广，从而允许探索更广泛的隐式正则化和约束优化算法。", "summary": "本文通过将Muon优化器置于Lion-$\\mathcal{K}$家族中，对其理论基础进行了深入分析。研究表明，Muon在配备核范数时等同于Lion-$\\mathcal{K}$，并揭示了Muon（带解耦权重衰减）隐式地通过对权重矩阵施加谱范数约束来进行优化，从而解释了其隐式正则化效应，并为开发新型隐式正则化优化算法提供了新思路。", "keywords": "Muon优化器, 谱范数, 隐式正则化, Lion-K, 优化算法", "comments": "本文通过将Muon与Lion-$\\mathcal{K}$家族联系起来，为其经验上的成功提供了理论解释，特别是揭示了其隐式的谱范数约束。这种理论框架的建立不仅增强了我们对现有优化器工作原理的理解，也为未来设计具有特定正则化效果的新型优化算法提供了指导，具有重要的理论和实践意义。"}}
{"id": "2506.15568", "title": "Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models", "authors": ["Zhengyang Shan", "Emily Ruth Diana", "Jiawei Zhou"], "summary": "We present a comprehensive evaluation of gender fairness in large language\nmodels (LLMs), focusing on their ability to handle both binary and non-binary\ngenders. While previous studies primarily focus on binary gender distinctions,\nwe introduce the Gender Inclusivity Fairness Index (GIFI), a novel and\ncomprehensive metric that quantifies the diverse gender inclusivity of LLMs.\nGIFI consists of a wide range of evaluations at different levels, from simply\nprobing the model with respect to provided gender pronouns to testing various\naspects of model generation and cognitive behaviors under different gender\nassumptions, revealing biases associated with varying gender identifiers. We\nconduct extensive evaluations with GIFI on 22 prominent open-source and\nproprietary LLMs of varying sizes and capabilities, discovering significant\nvariations in LLMs' gender inclusivity. Our study highlights the importance of\nimproving LLMs' inclusivity, providing a critical benchmark for future\nadvancements in gender fairness in generative models.", "comment": "Accepted by ACL 2025 Main", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15568v1", "AI": {"title_translation": "性别包容性公平指数（GIFI）：评估大型语言模型中性别多样性的多层次框架", "tldr": "本文提出了性别包容性公平指数（GIFI），这是一个新的综合指标，用于评估大型语言模型（LLMs）在处理二元和非二元性别方面的性别公平性。研究发现不同LLMs的性别包容性存在显著差异，并强调了提高LLMs包容性的重要性。", "motivation": "以往关于大型语言模型性别公平性的研究主要集中在二元性别区分，但缺乏对非二元性别的考虑。因此，本文旨在引入一个更全面、更细致的评估框架，以量化和揭示LLMs在处理多种性别方面的包容性。", "method": "本文引入了性别包容性公平指数（GIFI），这是一个新颖且全面的指标，用于量化大型语言模型的多元性别包容性。GIFI包含从探测模型对提供性别代词的反应到测试模型在不同性别假设下的生成和认知行为的各个层面的广泛评估。研究对22个主流的开源和专有大型语言模型进行了广泛评估。", "result": "研究发现，在不同大小和能力的大型语言模型中，其性别包容性存在显著差异。", "conclusion": "本研究强调了提高大型语言模型包容性的重要性，并为未来生成模型在性别公平性方面的进展提供了一个关键的基准。", "translation": "我们对大型语言模型（LLMs）的性别公平性进行了全面评估，重点关注它们处理二元和非二元性别的能力。虽然之前的研究主要关注二元性别区分，但我们引入了性别包容性公平指数（GIFI），这是一个新颖且全面的指标，用于量化LLMs的多元性别包容性。GIFI包含不同层面的广泛评估，从简单地用提供的性别代词探测模型，到测试模型在不同性别假设下的生成和认知行为的各个方面，揭示了与不同性别标识符相关的偏见。我们使用GIFI对22个主流的开源和专有LLMs进行了广泛评估，这些LLMs的大小和能力各不相同，结果发现了LLMs性别包容性的显著差异。我们的研究强调了提高LLMs包容性的重要性，为生成模型在性别公平性方面的未来进展提供了一个关键的基准。", "summary": "本文提出了性别包容性公平指数（GIFI），这是一个用于全面评估大型语言模型（LLMs）性别公平性的多层次框架，特别关注了二元和非二元性别。GIFI通过多层面评估量化LLMs的多元性别包容性，并应用于22个主流LLMs。研究发现不同LLMs的性别包容性存在显著差异，强调了提升LLMs包容性的重要性，并为该领域的未来研究提供了基准。", "keywords": "大型语言模型, 性别公平性, 性别包容性, GIFI, 偏见评估", "comments": "本文的创新之处在于提出了一个名为GIFI的全面多层次评估框架，首次将对大型语言模型性别公平性的评估扩展到非二元性别，弥补了以往研究的不足。GIFI的引入为量化和揭示LLMs的多元性别偏见提供了一个重要的工具。该研究通过对大量主流LLMs的评估，揭示了当前LLMs在性别包容性方面的显著差异，为未来模型开发提供了关键的改进方向和基准。"}}
{"id": "2506.15635", "title": "FindingDory: A Benchmark to Evaluate Memory in Embodied Agents", "authors": ["Karmesh Yadav", "Yusuf Ali", "Gunshi Gupta", "Yarin Gal", "Zsolt Kira"], "summary": "Large vision-language models have recently demonstrated impressive\nperformance in planning and control tasks, driving interest in their\napplication to real-world robotics. However, deploying these models for\nreasoning in embodied contexts is limited by their ability to incorporate\nlong-term experience collected across multiple days and represented by vast\ncollections of images. Current VLMs typically struggle to process more than a\nfew hundred images concurrently, highlighting the need for more efficient\nmechanisms to handle long-term memory in embodied settings. To effectively\nevaluate these models for long-horizon control, a benchmark must specifically\ntarget scenarios where memory is crucial for success. Existing long-video QA\nbenchmarks overlook embodied challenges like object manipulation and\nnavigation, which demand low-level skills and fine-grained reasoning over past\ninteractions. Moreover, effective memory integration in embodied agents\ninvolves both recalling relevant historical information and executing actions\nbased on that information, making it essential to study these aspects together\nrather than in isolation. In this work, we introduce a new benchmark for\nlong-range embodied tasks in the Habitat simulator. This benchmark evaluates\nmemory-based capabilities across 60 tasks requiring sustained engagement and\ncontextual awareness in an environment. The tasks can also be procedurally\nextended to longer and more challenging versions, enabling scalable evaluation\nof memory and reasoning. We also present baselines that integrate\nstate-of-the-art VLMs with low level navigation policies, assessing their\nperformance on these memory-intensive tasks and highlight areas for\nimprovement.", "comment": "Our dataset and code will be made available at:\n  https://findingdory-benchmark.github.io/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15635v1", "AI": {"title_translation": "FindingDory：一个评估具身智能体记忆能力的基准", "tldr": "本文提出了一个名为 FindingDory 的新基准，用于评估具身智能体中大型视觉语言模型的长期记忆能力，解决了现有模型和基准在处理长期经验和具身任务方面的局限性。", "motivation": "大型视觉语言模型在机器人应用中面临挑战，因为它们难以整合跨多天收集的大量长期图像经验，且现有模型无法同时处理大量图像，导致在具身环境中处理长期记忆效率低下。此外，现有长视频问答基准未能涵盖具身任务（如物体操纵和导航）所需的低级技能和细粒度推理，且具身智能体中的记忆整合需要同时研究信息回忆和动作执行。", "method": "引入了一个新的基准 FindingDory，用于 Habitat 模拟器中的长距离具身任务。该基准包含60个任务，用于评估基于记忆的能力，这些任务需要持续参与和环境上下文感知。任务可以程序化扩展，以实现记忆和推理的可扩展评估。同时，提出了将最先进的视觉语言模型与低级导航策略相结合的基线。", "result": "基线评估了最先进的视觉语言模型在这些记忆密集型任务上的性能，并突出了需要改进的领域。", "conclusion": "FindingDory 基准的引入，为评估具身智能体中视觉语言模型的长期记忆能力提供了一个关键工具，并有助于识别和改进现有模型在处理复杂具身任务时的不足。", "translation": "大型视觉语言模型最近在规划和控制任务中表现出色，这推动了它们在现实世界机器人技术中应用的热情。然而，在具身环境中部署这些模型进行推理受到它们整合跨多天收集并由大量图像表示的长期经验的能力的限制。当前的视觉语言模型通常难以同时处理数百张以上的图像，这凸显了在具身环境中处理长期记忆需要更高效的机制。为了有效评估这些模型进行长时程控制，基准必须专门针对记忆对成功至关重要的场景。现有的长视频问答基准忽略了具身挑战，如物体操纵和导航，这些挑战需要低级技能和对过去交互的细粒度推理。此外，具身智能体中有效的记忆整合涉及回忆相关的历史信息并根据该信息执行动作，因此将这些方面结合而非孤立地研究至关重要。在这项工作中，我们引入了一个新的基准，用于Habitat模拟器中的长距离具身任务。该基准评估了在需要持续参与和环境上下文感知的60个任务中的基于记忆的能力。这些任务还可以通过程序化扩展到更长和更具挑战性的版本，从而实现记忆和推理的可扩展评估。我们还提出了将最先进的视觉语言模型与低级导航策略相结合的基线，评估它们在这些记忆密集型任务上的性能，并突出需要改进的领域。", "summary": "本文介绍了 FindingDory，一个在 Habitat 模拟器中用于评估具身智能体长期记忆能力的新基准。该基准旨在解决当前大型视觉语言模型在处理大量长期经验方面的局限性，以及现有长视频问答基准未能涵盖具身操作和导航挑战的问题。FindingDory 包含60个需要持续参与和上下文感知的任务，这些任务可程序化扩展以实现可伸缩评估。研究还提出了将先进视觉语言模型与低级导航策略相结合的基线，以评估其在记忆密集型任务上的表现并指出改进方向。", "keywords": "具身智能体, 记忆, 基准, 视觉语言模型, Habitat", "comments": "FindingDory 基准的创新之处在于其专注于具身智能体的长期记忆能力，这对于实现现实世界机器人应用至关重要。它通过在 Habitat 模拟器中设计长距离、记忆密集型任务，弥补了现有基准在具身挑战方面的不足。任务的可程序化扩展性使其能够支持未来更复杂、更具挑战性的研究。该工作为评估和改进具身 AI 模型在长期记忆和推理方面的表现提供了宝贵的工具。"}}
{"id": "2506.15298", "title": "MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering", "authors": ["Xinqi Fan", "Jingting Li", "John See", "Moi Hoon Yap", "Wen-Huang Cheng", "Xiaobai Li", "Xiaopeng Hong", "Su-Jing Wang", "Adrian K. Davision"], "summary": "Facial micro-expressions (MEs) are involuntary movements of the face that\noccur spontaneously when a person experiences an emotion but attempts to\nsuppress or repress the facial expression, typically found in a high-stakes\nenvironment. In recent years, substantial advancements have been made in the\nareas of ME recognition, spotting, and generation. However, conventional\napproaches that treat spotting and recognition as separate tasks are\nsuboptimal, particularly for analyzing long-duration videos in realistic\nsettings. Concurrently, the emergence of multimodal large language models\n(MLLMs) and large vision-language models (LVLMs) offers promising new avenues\nfor enhancing ME analysis through their powerful multimodal reasoning\ncapabilities. The ME grand challenge (MEGC) 2025 introduces two tasks that\nreflect these evolving research directions: (1) ME spot-then-recognize\n(ME-STR), which integrates ME spotting and subsequent recognition in a unified\nsequential pipeline; and (2) ME visual question answering (ME-VQA), which\nexplores ME understanding through visual question answering, leveraging MLLMs\nor LVLMs to address diverse question types related to MEs. All participating\nalgorithms are required to run on this test set and submit their results on a\nleaderboard. More details are available at https://megc2025.github.io.", "comment": "Micro-Expression Grand Challenge (MEGC) at ACM MM 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15298v1", "AI": {"title_translation": "MEGC2025：微表情发现与识别及视觉问答挑战赛", "tldr": "MEGC2025引入了两项微表情分析挑战：ME-STR统一发现与识别，以及ME-VQA利用多模态大模型进行理解，以推动微表情研究发展。", "motivation": "传统微表情分析方法将发现和识别视为独立任务，在处理长视频时表现不佳。同时，多模态大语言模型和大型视觉语言模型的兴起为微表情分析提供了新的机遇，需要新的挑战来探索这些方向。", "method": "MEGC2025挑战赛引入了两项任务：1) 微表情发现-然后-识别 (ME-STR)，将微表情发现和随后的识别整合到统一的顺序流程中。2) 微表情视觉问答 (ME-VQA)，通过视觉问答探索微表情理解，利用多模态大语言模型或大型视觉语言模型解决与微表情相关的各种问题。所有算法需在测试集上运行并提交结果到排行榜。", "result": "Not mentioned in abstract", "conclusion": "Not mentioned in abstract", "translation": "面部微表情（MEs）是面部非自主的运动，当一个人经历某种情绪但试图压抑或抑制面部表情时自发产生，通常出现在高风险环境中。近年来，在微表情识别、发现和生成领域取得了实质性进展。然而，将发现和识别视为独立任务的传统方法并非最优，特别是在分析真实环境中的长时间视频时。同时，多模态大语言模型（MLLMs）和大型视觉语言模型（LVLMs）的出现，通过其强大的多模态推理能力，为增强微表情分析提供了有前景的新途径。MEGC 2025微表情大挑战引入了两项反映这些不断发展研究方向的任务：(1) 微表情发现-然后-识别（ME-STR），将微表情发现和随后的识别整合到一个统一的顺序流程中；(2) 微表情视觉问答（ME-VQA），通过视觉问答探索微表情理解，利用MLLMs或LVLMs解决与微表情相关的各种问题。所有参与算法都要求在此测试集上运行，并将结果提交到排行榜。更多详细信息可在https://megc2025.github.io查阅。", "summary": "MEGC2025是一项微表情分析挑战赛，旨在推动该领域的发展。针对传统方法在长视频分析中的不足以及多模态大模型的兴起，该挑战赛引入了两项新任务：ME-STR（微表情发现与识别的统一流程）和ME-VQA（利用多模态大模型进行微表情视觉问答）。参赛者需提交算法结果到排行榜。", "keywords": "微表情, 挑战赛, 发现与识别, 视觉问答, 多模态大模型", "comments": "该论文（挑战赛介绍）的创新点在于提出了将微表情发现和识别整合的统一任务（ME-STR），并首次探索了利用多模态大模型进行微表情理解的视觉问答任务（ME-VQA）。这反映了当前AI领域多模态融合的趋势，为微表情分析提供了新的研究范式和方向，对于推动该领域在真实世界应用中的发展具有重要意义。"}}
{"id": "2506.15569", "title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification", "authors": ["Chengye Wang", "Yifei Shen", "Zexi Kuang", "Arman Cohan", "Yilun Zhao"], "summary": "We introduce SciVer, the first benchmark specifically designed to evaluate\nthe ability of foundation models to verify claims within a multimodal\nscientific context. SciVer consists of 3,000 expert-annotated examples over\n1,113 scientific papers, covering four subsets, each representing a common\nreasoning type in multimodal scientific claim verification. To enable\nfine-grained evaluation, each example includes expert-annotated supporting\nevidence. We assess the performance of 21 state-of-the-art multimodal\nfoundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and\nQwen2.5-VL. Our experiment reveals a substantial performance gap between these\nmodels and human experts on SciVer. Through an in-depth analysis of\nretrieval-augmented generation (RAG), and human-conducted error evaluations, we\nidentify critical limitations in current open-source models, offering key\ninsights to advance models' comprehension and reasoning in multimodal\nscientific literature tasks.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15569v1", "AI": {"title_translation": "SciVer：评估多模态科学主张验证的基础模型", "tldr": "引入了SciVer，一个专门用于评估基础模型在多模态科学背景下验证主张能力的基准，揭示了当前模型与人类专家之间存在显著性能差距。", "motivation": "目前缺乏专门用于评估基础模型在多模态科学背景下验证主张能力的基准，因此需要SciVer来填补这一空白并推动模型在此领域的发展。", "method": "研究引入了SciVer基准，包含3000个专家标注的示例，覆盖1113篇科学论文和四种推理类型，并包含专家标注的支持证据。评估了21个最先进的多模态基础模型，并进行了深入的检索增强生成（RAG）分析和人工错误评估。", "result": "实验结果显示，当前最先进的多模态基础模型在SciVer上的表现与人类专家之间存在显著的性能差距。研究还通过深入分析识别了当前开源模型的关键局限性。", "conclusion": "当前的多模态基础模型在多模态科学文献任务中的理解和推理能力存在显著局限性，需要进一步的研究和改进以缩小与人类专家之间的性能差距。", "translation": "我们引入了SciVer，这是第一个专门用于评估基础模型在多模态科学背景下验证主张能力的基准。SciVer包含3000个专家标注的示例，涵盖1113篇科学论文，覆盖四种子集，每种代表多模态科学主张验证中常见的推理类型。为了实现细粒度评估，每个示例都包含专家标注的支持证据。我们评估了21个最先进的多模态基础模型，包括o4-mini、Gemini-2.5-Flash、Llama-3.2-Vision和Qwen2.5-VL。我们的实验揭示了这些模型在SciVer上与人类专家之间存在显著的性能差距。通过对检索增强生成（RAG）的深入分析和人工进行的错误评估，我们识别了当前开源模型的关键局限性，为推进模型在多模态科学文献任务中的理解和推理能力提供了关键见解。", "summary": "本研究引入了SciVer，一个专门用于评估基础模型在多模态科学主张验证能力的基准。SciVer包含3000个专家标注的示例和支持证据，涵盖多种推理类型。研究评估了21个最先进的多模态基础模型，发现它们与人类专家之间存在显著的性能差距。通过分析，研究揭示了当前开源模型的局限性，并为提升模型在科学文献理解和推理方面的能力提供了见解。", "keywords": "SciVer, 基础模型, 多模态, 科学主张验证, 基准", "comments": "SciVer作为首个专门针对多模态科学主张验证的基准，填补了该领域的空白，具有重要的创新性。它提供了高质量的专家标注数据和细粒度评估能力，对于推动多模态基础模型在科学领域的应用和发展至关重要。研究结果明确指出了当前模型与人类专家之间的巨大差距，为未来的模型改进指明了方向。"}}
{"id": "2506.15313", "title": "MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual Learning", "authors": ["Leonid Ivanov", "Vasily Yuryev", "Dmitry Yudin"], "summary": "In autonomous driving, high-definition (HD) maps and semantic maps in\nbird's-eye view (BEV) are essential for accurate localization, planning, and\ndecision-making. This paper introduces an enhanced End-to-End model named MapFM\nfor online vectorized HD map generation. We show significantly boost feature\nrepresentation quality by incorporating powerful foundation model for encoding\ncamera images. To further enrich the model's understanding of the environment\nand improve prediction quality, we integrate auxiliary prediction heads for\nsemantic segmentation in the BEV representation. This multi-task learning\napproach provides richer contextual supervision, leading to a more\ncomprehensive scene representation and ultimately resulting in higher accuracy\nand improved quality of the predicted vectorized HD maps. The source code is\navailable at https://github.com/LIvanoff/MapFM.", "comment": "Preprint. Submitted. 12 pages, 4 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15313v1", "AI": {"title_translation": "MapFM：基于基础模型和多任务上下文学习的高清地图构建", "tldr": "MapFM是一个端到端模型，利用基础模型和多任务学习提升在线矢量化高清地图的生成质量和准确性。", "motivation": "在自动驾驶中，高清（HD）地图和鸟瞰图（BEV）中的语义地图对于精确定位、规划和决策至关重要。本文旨在解决在线矢量化高清地图生成的需求。", "method": "MapFM是一个增强的端到端模型，用于在线矢量化高清地图生成。它通过整合强大的基础模型来编码相机图像，显著提升特征表示质量。为了进一步丰富模型对环境的理解并提高预测质量，该模型集成了辅助预测头，用于BEV表示中的语义分割。这种多任务学习方法提供了更丰富的上下文监督。", "result": "通过结合基础模型和多任务学习，MapFM显著提升了特征表示质量，并实现了更全面的场景表示，最终提高了预测矢量化高清地图的准确性和质量。", "conclusion": "MapFM通过结合强大的基础模型和多任务上下文学习，为自动驾驶中的在线矢量化高清地图生成提供了一个高效且高质量的解决方案。", "translation": "在自动驾驶中，高清（HD）地图和鸟瞰图（BEV）中的语义地图对于精确定位、规划和决策至关重要。本文介绍了一个名为MapFM的增强型端到端模型，用于在线矢量化高清地图生成。我们展示了通过整合强大的基础模型来编码相机图像，显著提升了特征表示质量。为了进一步丰富模型对环境的理解并提高预测质量，我们集成了辅助预测头，用于BEV表示中的语义分割。这种多任务学习方法提供了更丰富的上下文监督，从而实现了更全面的场景表示，并最终提高了预测矢量化高清地图的准确性和质量。源代码可在https://github.com/LIvanoff/MapFM获取。", "summary": "MapFM是一个新颖的端到端模型，专注于在线矢量化高清地图的生成，这对于自动驾驶至关重要。该模型通过利用强大的基础模型编码相机图像来增强特征表示，并通过集成用于鸟瞰图语义分割的辅助预测头，采用多任务学习策略，从而获得更丰富的上下文信息。这种方法显著提高了所生成高清地图的准确性和质量。", "keywords": "高清地图, 自动驾驶, 基础模型, 多任务学习, 矢量化地图", "comments": "MapFM的创新之处在于将强大的基础模型引入高清地图生成，并结合多任务学习以提供丰富的上下文监督。这种结合有望显著提升自动驾驶中地图构建的效率和精度。"}}
{"id": "2506.15583", "title": "DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through Iterative Graph Refinement", "authors": ["Shaoqing Lin", "Chong Teng", "Fei Li", "Donghong Ji", "Lizhen Qu", "Zhuang Li"], "summary": "Vision-Language Models (VLMs) now generate discourse-level, multi-sentence\nvisual descriptions, challenging text scene graph parsers originally designed\nfor single-sentence caption-to-graph mapping. Current approaches typically\nmerge sentence-level parsing outputs for discourse input, often missing\nphenomena like cross-sentence coreference, resulting in fragmented graphs and\ndegraded downstream VLM task performance. To address this, we introduce a new\ntask, Discourse-level text Scene Graph parsing (DiscoSG), supported by our\ndataset DiscoSG-DS, which comprises 400 expert-annotated and 8,430 synthesised\nmulti-sentence caption-graph pairs for images. Each caption averages 9\nsentences, and each graph contains at least 3 times more triples than those in\nexisting datasets. While fine-tuning large PLMs (i.e., GPT-4) on DiscoSG-DS\nimproves SPICE by approximately 48% over the best sentence-merging baseline,\nhigh inference cost and restrictive licensing hinder its open-source use, and\nsmaller fine-tuned PLMs struggle with complex graphs. We propose\nDiscoSG-Refiner, which drafts a base graph using one small PLM, then employs a\nsecond PLM to iteratively propose graph edits, reducing full-graph generation\noverhead. Using two Flan-T5-Base models, DiscoSG-Refiner still improves SPICE\nby approximately 30% over the best baseline while achieving 86 times faster\ninference than GPT-4. It also consistently improves downstream VLM tasks like\ndiscourse-level caption evaluation and hallucination detection. Code and data\nare available at: https://github.com/ShaoqLin/DiscoSG", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15583v1", "AI": {"title_translation": "DiscoSG：通过迭代图细化实现篇章级文本场景图解析", "tldr": "该论文引入了DiscoSG，一项用于篇章级文本场景图解析的新任务，并提出了一个新数据集DiscoSG-DS。为解决大型预训练语言模型（PLM）的成本和小型PLM的性能问题，作者提出了DiscoSG-Refiner，它通过迭代图细化显著提高了性能，同时实现了比GPT-4快86倍的推理速度，并改进了下游VLM任务。", "motivation": "当前的文本场景图解析器主要为单句字幕设计，难以处理视觉-语言模型（VLM）生成的篇章级多句视觉描述，导致图碎片化并降低下游VLM任务的性能，因为它们未能处理跨句共指等现象。", "method": "1. 引入了新任务：篇章级文本场景图解析（DiscoSG）。2. 构建了新数据集：DiscoSG-DS，包含400对专家标注和8,430对合成的多句字幕-图对。3. 提出了DiscoSG-Refiner：该方法使用一个小型预训练语言模型（PLM）草拟基础图，然后利用第二个PLM迭代地提出图编辑，从而减少了完整图生成的开销。具体使用了两个Flan-T5-Base模型。", "result": "1. 在DiscoSG-DS上微调大型PLM（如GPT-4）比最佳句子合并基线提高了约48%的SPICE分数。2. DiscoSG-Refiner（使用两个Flan-T5-Base模型）比最佳基线提高了约30%的SPICE分数。3. DiscoSG-Refiner的推理速度比GPT-4快86倍。4. DiscoSG-Refiner持续改进了下游VLM任务，如篇章级字幕评估和幻觉检测。", "conclusion": "DiscoSG-Refiner有效解决了篇章级文本场景图解析的挑战，在性能上可与GPT-4等大型模型相媲美，同时效率显著提高，并适用于下游VLM任务。", "translation": "视觉-语言模型（VLM）现在可以生成篇章级、多句视觉描述，这给最初为单句字幕到图映射设计的文本场景图解析器带来了挑战。当前方法通常合并句子级解析输出以处理篇章输入，但经常忽略跨句共指等现象，导致图碎片化并降低下游VLM任务性能。为了解决这个问题，我们引入了一项新任务——篇章级文本场景图解析（DiscoSG），并由我们的数据集DiscoSG-DS支持，该数据集包含400对专家标注的和8,430对合成的多句图像字幕-图对。每个字幕平均包含9个句子，每个图包含的谓词三元组数量至少是现有数据集的3倍。尽管在DiscoSG-DS上微调大型预训练语言模型（即GPT-4）比最佳句子合并基线提高了约48%的SPICE分数，但高推理成本和限制性许可阻碍了其开源使用，并且较小的微调预训练语言模型难以处理复杂图。我们提出了DiscoSG-Refiner，它使用一个小型预训练语言模型草拟基础图，然后使用第二个预训练语言模型迭代地提出图编辑，从而减少了完整图生成的开销。使用两个Flan-T5-Base模型，DiscoSG-Refiner仍然比最佳基线提高了约30%的SPICE分数，同时实现了比GPT-4快86倍的推理速度。它还持续改进了下游VLM任务，如篇章级字幕评估和幻觉检测。代码和数据可在：https://github.com/ShaoqLin/DiscoSG 获取。", "summary": "这篇论文解决了将篇章级多句视觉描述解析为文本场景图的挑战，现有单句解析器对此任务力不从心。论文引入了一项新任务DiscoSG和一个大规模数据集DiscoSG-DS，其中包含复杂的跨句字幕-图对。为克服大型预训练语言模型（PLM）的局限性（高成本、推理慢）和小型PLM的不足（难以处理复杂性），作者提出了DiscoSG-Refiner。这种新颖的方法采用迭代细化过程，使用两个小型PLM，显著提高了性能（例如，Flan-T5-Base模型的SPICE分数提高了30%），同时实现了更快的推理速度（比GPT-4快86倍），并对下游VLM任务有所助益。", "keywords": "篇章级解析, 文本场景图, 视觉-语言模型, 迭代细化, DiscoSG", "comments": "该论文引入了一项关键的新任务和数据集，用于视觉-语言模型中的篇章级理解，鉴于VLM生成能力的进步，这具有高度相关性。DiscoSG-Refiner的迭代细化方法具有创新性，它提供了一种实用解决方案，以较小、更高效的模型实现高性能，解决了大型PLM的实际限制。其在下游任务中表现出的改进突显了其实用价值。"}}
{"id": "2506.15318", "title": "OpenPath: Open-Set Active Learning for Pathology Image Classification via Pre-trained Vision-Language Models", "authors": ["Lanfeng Zhong", "Xin Liao", "Shichuan Zhang", "Shaoting Zhang", "Guotai Wang"], "summary": "Pathology image classification plays a crucial role in accurate medical\ndiagnosis and treatment planning. Training high-performance models for this\ntask typically requires large-scale annotated datasets, which are both\nexpensive and time-consuming to acquire. Active Learning (AL) offers a solution\nby iteratively selecting the most informative samples for annotation, thereby\nreducing the labeling effort. However, most AL methods are designed under the\nassumption of a closed-set scenario, where all the unannotated images belong to\ntarget classes. In real-world clinical environments, the unlabeled pool often\ncontains a substantial amount of Out-Of-Distribution (OOD) data, leading to low\nefficiency of annotation in traditional AL methods. Furthermore, most existing\nAL methods start with random selection in the first query round, leading to a\nsignificant waste of labeling costs in open-set scenarios. To address these\nchallenges, we propose OpenPath, a novel open-set active learning approach for\npathological image classification leveraging a pre-trained Vision-Language\nModel (VLM). In the first query, we propose task-specific prompts that combine\ntarget and relevant non-target class prompts to effectively select\nIn-Distribution (ID) and informative samples from the unlabeled pool. In\nsubsequent queries, Diverse Informative ID Sampling (DIS) that includes\nPrototype-based ID candidate Selection (PIS) and Entropy-Guided Stochastic\nSampling (EGSS) is proposed to ensure both purity and informativeness in a\nquery, avoiding the selection of OOD samples. Experiments on two public\npathology image datasets show that OpenPath significantly enhances the model's\nperformance due to its high purity of selected samples, and outperforms several\nstate-of-the-art open-set AL methods. The code is available at\n\\href{https://github.com/HiLab-git/OpenPath}{https://github.com/HiLab-git/OpenPath}..", "comment": "MICCAI 2025 early accept", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15318v1", "AI": {"title_translation": "OpenPath：基于预训练视觉-语言模型的病理图像分类开放集主动学习", "tldr": "OpenPath是一种新颖的开放集主动学习方法，利用预训练VLM解决病理图像分类中OOD数据和初始查询效率低的问题，显著提高性能。", "motivation": "病理图像分类对医疗诊断至关重要，但高成本和耗时的数据标注是主要挑战。传统主动学习（AL）方法多基于闭集假设，无法有效处理真实临床环境中包含大量分布外（OOD）数据的开放集场景。此外，现有AL方法首轮随机选择样本，导致开放集场景下标注效率低下。", "method": "本文提出了OpenPath，一种利用预训练视觉-语言模型（VLM）的开放集主动学习方法。在首次查询中，使用结合目标和非目标类别的任务特定提示来选择分布内（ID）和信息量大的样本。在后续查询中，提出多样化信息量ID采样（DIS），其中包含基于原型的ID候选选择（PIS）和熵引导随机采样（EGSS），以确保所选样本的纯度和信息量，避免选择OOD样本。", "result": "在两个公共病理图像数据集上的实验表明，OpenPath显著提升了模型的性能。其性能提升得益于所选样本的高纯度，并且优于几种最先进的开放集主动学习方法。", "conclusion": "OpenPath通过其创新的ID样本选择策略，有效解决了开放集病理图像分类中OOD数据和初始查询效率低下的挑战，显著提升了主动学习的效率和模型的性能。", "translation": "病理图像分类在准确的医疗诊断和治疗规划中起着至关重要的作用。训练用于此任务的高性能模型通常需要大规模的标注数据集，而这既昂贵又耗时。主动学习（AL）通过迭代选择最具信息量的样本进行标注，从而减少标注工作量，提供了一种解决方案。然而，大多数AL方法是在闭集场景的假设下设计的，即所有未标注的图像都属于目标类别。在真实世界的临床环境中，未标注池通常包含大量分布外（OOD）数据，导致传统方法的标注效率低下。此外，大多数现有AL方法在首次查询轮次中从随机选择开始，导致在开放集场景中浪费了大量的标注成本。为了解决这些挑战，我们提出了OpenPath，一种新颖的开放集主动学习方法，用于病理图像分类，它利用了预训练的视觉-语言模型（VLM）。在首次查询中，我们提出了结合目标和相关非目标类别提示的任务特定提示，以有效地从未标注池中选择分布内（ID）和信息量大的样本。在后续查询中，提出了多样化信息量ID采样（DIS），包括基于原型的ID候选选择（PIS）和熵引导随机采样（EGSS），以确保查询的纯度和信息量，避免选择OOD样本。在两个公共病理图像数据集上的实验表明，OpenPath由于其选择样本的高纯度而显著增强了模型的性能，并且优于几种最先进的开放集AL方法。代码可在\nhttps://github.com/HiLab-git/OpenPath\n获取。", "summary": "本文提出了OpenPath，一种针对病理图像分类的开放集主动学习新方法，旨在解决传统方法在真实临床环境中面临的OOD数据和初始查询效率低下的问题。OpenPath利用预训练的视觉-语言模型，通过在首次查询中使用任务特定提示高效选择ID和信息量样本，并在后续查询中采用多样化信息量ID采样（DIS），包括基于原型的ID候选选择（PIS）和熵引导随机采样（EGSS），以确保样本的纯度和信息量。实验证明，OpenPath显著提升了模型性能并超越了现有最先进的开放集主动学习方法。", "keywords": "开放集主动学习, 病理图像分类, 视觉-语言模型, OOD检测, 样本选择", "comments": "OpenPath的创新之处在于其针对开放集场景设计了全面的主动学习策略，特别是利用VLM和分阶段的样本选择机制（初始查询的任务特定提示和后续查询的DIS），有效解决了OOD数据干扰和初始查询效率问题。这对于减少医疗图像标注成本，加速AI在病理诊断中的应用具有重要意义。"}}
{"id": "2506.15079", "title": "Neural Canonical Polyadic Factorization for Traffic Analysis", "authors": ["Yikai Hou", "Peng Tang"], "summary": "Modern intelligent transportation systems rely on accurate spatiotemporal\ntraffic analysis to optimize urban mobility and infrastructure resilience.\nHowever, pervasive missing data caused by sensor failures and heterogeneous\nsensing gaps fundamentally hinders reliable traffic modeling. This paper\nproposes a Neural Canonical Polyadic Factorization (NCPF) model that synergizes\nlow-rank tensor algebra with deep representation learning for robust traffic\ndata imputation. The model innovatively embeds CP decomposition into neural\narchitecture through learnable embedding projections, where sparse traffic\ntensors are encoded into dense latent factors across road segments, time\nintervals, and mobility metrics. A hierarchical feature fusion mechanism\nemploys Hadamard products to explicitly model multilinear interactions, while\nstacked multilayer perceptron layers nonlinearly refine these representations\nto capture complex spatiotemporal couplings. Extensive evaluations on six urban\ntraffic datasets demonstrate NCPF's superiority over six state-of-the-art\nbaselines. By unifying CP decomposition's interpretable factor analysis with\nneural network's nonlinear expressive power, NCPF provides a principled yet\nflexible approaches for high-dimensional traffic data imputation, offering\ncritical support for next-generation transportation digital twins and adaptive\ntraffic control systems.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15079v1", "AI": {"title_translation": "用于交通分析的神经规范多重分解", "tldr": "本文提出了一种神经规范多重分解（NCPF）模型，结合低秩张量代数和深度表示学习，用于鲁棒的交通数据插补，并在多个交通数据集上表现优于现有基线。", "motivation": "现代智能交通系统依赖准确的时空交通分析来优化城市交通和基础设施弹性。然而，传感器故障和异构传感间隙导致的普遍缺失数据严重阻碍了可靠的交通建模。", "method": "本文提出了一种神经规范多重分解（NCPF）模型。该模型将CP分解创新性地嵌入到神经网络架构中，通过可学习的嵌入投影，将稀疏交通张量编码成跨路段、时间间隔和移动性指标的密集潜在因子。一个分层特征融合机制采用Hadamard积来明确建模多线性交互，而堆叠的多层感知器层非线性地细化这些表示，以捕获复杂的时空耦合。", "result": "在六个城市交通数据集上的广泛评估表明，NCPF优于六个最先进的基线模型。", "conclusion": "通过将CP分解的可解释因子分析与神经网络的非线性表达能力相结合，NCPF为高维交通数据插补提供了一种既有原则又灵活的方法，为下一代交通数字孪生和自适应交通控制系统提供了关键支持。", "translation": "现代智能交通系统依赖准确的时空交通分析来优化城市交通和基础设施弹性。然而，传感器故障和异构传感间隙导致的普遍缺失数据严重阻碍了可靠的交通建模。本文提出了一种神经规范多重分解（NCPF）模型，该模型将低秩张量代数与深度表示学习相结合，用于鲁棒的交通数据插补。该模型创新性地将CP分解嵌入到神经网络架构中，通过可学习的嵌入投影，将稀疏交通张量编码成跨路段、时间间隔和移动性指标的密集潜在因子。一个分层特征融合机制采用Hadamard积来明确建模多线性交互，而堆叠的多层感知器层非线性地细化这些表示，以捕获复杂的时空耦合。在六个城市交通数据集上的广泛评估表明，NCPF优于六个最先进的基线模型。通过将CP分解的可解释因子分析与神经网络的非线性表达能力相结合，NCPF为高维交通数据插补提供了一种既有原则又灵活的方法，为下一代交通数字孪生和自适应交通控制系统提供了关键支持。", "summary": "本文提出了一种名为神经规范多重分解（NCPF）的新模型，旨在解决智能交通系统中因缺失数据导致的交通建模挑战。NCPF结合了低秩张量代数和深度表示学习，通过将CP分解嵌入到神经网络架构中，利用可学习的嵌入投影将稀疏交通张量编码为密集潜在因子。模型采用分层特征融合机制和多层感知器层来捕捉复杂时空耦合。实验结果表明，NCPF在多个城市交通数据集上的数据插补性能优于现有基线，为交通数字孪生和自适应交通控制系统提供了有效支持。", "keywords": "交通分析, 数据插补, 规范多重分解, 神经网络, 时空数据", "comments": "NCPF的创新之处在于将CP分解的可解释性与神经网络的非线性表达能力结合起来，形成了一个原理性且灵活的数据插补方法。这种结合使得模型既能处理高维稀疏数据，又能捕获复杂的时空依赖关系，对于提升智能交通系统的鲁棒性具有重要意义。"}}
{"id": "2506.15594", "title": "WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts", "authors": ["Negar Foroutan", "Angelika Romanou", "Matin Ansaripour", "Julian Martin Eisenschlos", "Karl Aberer", "Rémi Lebret"], "summary": "Documents are fundamental to preserving and disseminating information, often\nincorporating complex layouts, tables, and charts that pose significant\nchallenges for automatic document understanding (DU). While vision-language\nlarge models (VLLMs) have demonstrated improvements across various tasks, their\neffectiveness in processing long-context vision inputs remains unclear. This\npaper introduces WikiMixQA, a benchmark comprising 1,000 multiple-choice\nquestions (MCQs) designed to evaluate cross-modal reasoning over tables and\ncharts extracted from 4,000 Wikipedia pages spanning seven distinct topics.\nUnlike existing benchmarks, WikiMixQA emphasizes complex reasoning by requiring\nmodels to synthesize information from multiple modalities. We evaluate 12\nstate-of-the-art vision-language models, revealing that while proprietary\nmodels achieve ~70% accuracy when provided with direct context, their\nperformance deteriorates significantly when retrieval from long documents is\nrequired. Among these, GPT-4-o is the only model exceeding 50% accuracy in this\nsetting, whereas open-source models perform considerably worse, with a maximum\naccuracy of 27%. These findings underscore the challenges of long-context,\nmulti-modal reasoning and establish WikiMixQA as a crucial benchmark for\nadvancing document understanding research.", "comment": "ACL 2025 (Findings)", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15594v1", "AI": {"title_translation": "WikiMixQA：一个用于表格和图表问答的多模态基准", "tldr": "WikiMixQA是一个新的多模态问答基准，旨在评估视觉-语言大模型在处理来自长文档的表格和图表时的复杂推理能力。研究发现，虽然专有模型在直接上下文下表现良好，但在需要从长文档中检索信息时性能显著下降，凸显了长上下文多模态推理的挑战。", "motivation": "文档理解，特别是包含复杂布局、表格和图表的文档，对自动化系统提出了重大挑战。尽管视觉-语言大模型（VLLMs）在各种任务中有所改进，但它们在处理长上下文视觉输入方面的有效性尚不明确。因此，需要一个专门的基准来评估VLLMs在这些复杂场景下的跨模态推理能力。", "method": "本文引入了WikiMixQA，这是一个包含1000个多项选择题（MCQs）的基准，旨在评估模型对从4000个维基百科页面中提取的表格和图表的跨模态推理能力。该基准特别强调复杂推理，要求模型综合来自多种模态的信息。研究评估了12个最先进的视觉-语言模型。", "result": "研究发现，当提供直接上下文时，专有模型达到了约70%的准确率；然而，当需要从长文档中检索信息时，它们的性能显著下降。在这种设置下，GPT-4o是唯一一个准确率超过50%的模型，而开源模型的表现则差得多，最高准确率仅为27%。", "conclusion": "这些发现突显了长上下文、多模态推理的挑战，并确立了WikiMixQA作为推动文档理解研究的关键基准。", "translation": "文档是保存和传播信息的基础，通常包含复杂的布局、表格和图表，这对自动文档理解（DU）构成了重大挑战。虽然视觉-语言大模型（VLLMs）在各种任务中都表现出改进，但它们在处理长上下文视觉输入方面的有效性仍不清楚。本文介绍了WikiMixQA，这是一个包含1000个多项选择题（MCQs）的基准，旨在评估对从涵盖七个不同主题的4000个维基百科页面中提取的表格和图表的跨模态推理。与现有基准不同，WikiMixQA通过要求模型综合来自多种模态的信息来强调复杂推理。我们评估了12个最先进的视觉-语言模型，结果显示，虽然专有模型在提供直接上下文时能达到约70%的准确率，但当需要从长文档中检索时，它们的性能显著下降。其中，GPT-4o是唯一一个在此设置下准确率超过50%的模型，而开源模型的表现则差得多，最高准确率为27%。这些发现强调了长上下文、多模态推理的挑战，并确立了WikiMixQA作为推动文档理解研究的关键基准。", "summary": "本文介绍了WikiMixQA，一个新型的多模态问答基准，用于评估视觉-语言大模型在处理包含表格和图表的长文档时的复杂推理能力。该基准包含1000个多项选择题，数据来源于维基百科。通过对12个主流模型的评估，研究发现现有模型在需要从长文档中检索信息进行问答时性能显著下降，尤其开源模型的表现远逊于专有模型。这突出了长上下文、多模态推理的挑战，并强调了WikiMixQA在推动文档理解研究中的重要性。", "keywords": "多模态基准, 问答, 文档理解, 表格和图表, 视觉-语言模型", "comments": "WikiMixQA的创新之处在于其专注于长上下文、多模态（表格和图表）的复杂推理，这对于当前的文档理解模型是一个巨大的挑战。它填补了现有基准的空白，特别是强调了从长文档中检索信息的能力。该基准的建立对于评估和推动未来视觉-语言模型在实际文档理解场景中的发展具有重要意义。"}}
{"id": "2506.15368", "title": "Open-World Object Counting in Videos", "authors": ["Niki Amini-Naieni", "Andrew Zisserman"], "summary": "We introduce a new task of open-world object counting in videos: given a text\ndescription, or an image example, that specifies the target object, the\nobjective is to enumerate all the unique instances of the target objects in the\nvideo. This task is especially challenging in crowded scenes with occlusions\nand similar objects, where avoiding double counting and identifying\nreappearances is crucial. To this end, we make the following contributions: we\nintroduce a model, CountVid, for this task. It leverages an image-based\ncounting model, and a promptable video segmentation and tracking model to\nenable automated, open-world object counting across video frames. To evaluate\nits performance, we introduce VideoCount, a new dataset for our novel task\nbuilt from the TAO and MOT20 tracking datasets, as well as from videos of\npenguins and metal alloy crystallization captured by x-rays. Using this\ndataset, we demonstrate that CountVid provides accurate object counts, and\nsignificantly outperforms strong baselines. The VideoCount dataset, the\nCountVid model, and all the code are available at\nhttps://github.com/niki-amini-naieni/CountVid/.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15368v1", "AI": {"title_translation": "视频开放世界目标计数", "tldr": "引入了一个新的视频开放世界目标计数任务，并提出了CountVid模型和VideoCount数据集，CountVid在挑战性场景下表现出色。", "motivation": "在拥挤、有遮挡和相似物体的场景中，避免重复计数和识别再出现是视频目标计数任务的巨大挑战。", "method": "提出了CountVid模型，它利用基于图像的计数模型以及可提示的视频分割和跟踪模型，实现视频帧间的自动化开放世界目标计数。同时，构建了VideoCount数据集用于评估。", "result": "CountVid提供了准确的目标计数，并且显著优于强大的基线模型。", "conclusion": "CountVid模型能够有效应对视频开放世界目标计数任务的挑战，并在新数据集VideoCount上表现出优越的性能。", "translation": "我们引入了一项新的视频开放世界目标计数任务：给定文本描述或图像示例来指定目标对象，目标是枚举视频中所有目标对象的唯一实例。这项任务在拥挤、有遮挡和相似物体的场景中尤其具有挑战性，其中避免重复计数和识别再出现至关重要。为此，我们做出了以下贡献：我们为这项任务引入了一个模型，CountVid。它利用基于图像的计数模型和可提示的视频分割和跟踪模型，以实现跨视频帧的自动化开放世界目标计数。为了评估其性能，我们引入了VideoCount，这是一个为我们新颖任务构建的新数据集，它来自TAO和MOT20跟踪数据集，以及通过X射线捕获的企鹅和金属合金结晶视频。使用这个数据集，我们证明了CountVid提供了准确的目标计数，并且显著优于强大的基线。VideoCount数据集、CountVid模型和所有代码均可在https://github.com/niki-amini-naieni/CountVid/获取。", "summary": "本文提出了一项新的视频开放世界目标计数任务，旨在根据文本或图像示例在视频中识别并计数所有唯一的目标实例。针对拥挤、遮挡和相似对象等挑战，研究人员开发了CountVid模型，该模型结合了图像计数与视频分割跟踪技术。为评估CountVid，创建了VideoCount数据集。实验结果表明，CountVid在准确性上显著优于现有基线。", "keywords": "开放世界计数, 视频目标计数, CountVid, VideoCount, 目标跟踪", "comments": "本文创新性地提出了“视频开放世界目标计数”这一新任务，并针对该任务的挑战性（如重复计数和再出现问题）提出了有效的解决方案CountVid模型。同时，创建了专门的数据集VideoCount，为未来的研究提供了基准。其结合图像计数与视频跟踪的思路具有借鉴意义。"}}
{"id": "2506.15115", "title": "Towards Reliable Forgetting: A Survey on Machine Unlearning Verification, Challenges, and Future Directions", "authors": ["Lulu Xue", "Shengshan Hu", "Wei Lu", "Yan Shen", "Dongxu Li", "Peijin Guo", "Ziqi Zhou", "Minghui Li", "Yanjun Zhang", "Leo Yu Zhang"], "summary": "With growing demands for privacy protection, security, and legal compliance\n(e.g., GDPR), machine unlearning has emerged as a critical technique for\nensuring the controllability and regulatory alignment of machine learning\nmodels. However, a fundamental challenge in this field lies in effectively\nverifying whether unlearning operations have been successfully and thoroughly\nexecuted. Despite a growing body of work on unlearning techniques, verification\nmethodologies remain comparatively underexplored and often fragmented. Existing\napproaches lack a unified taxonomy and a systematic framework for evaluation.\nTo bridge this gap, this paper presents the first structured survey of machine\nunlearning verification methods. We propose a taxonomy that organizes current\ntechniques into two principal categories -- behavioral verification and\nparametric verification -- based on the type of evidence used to assess\nunlearning fidelity. We examine representative methods within each category,\nanalyze their underlying assumptions, strengths, and limitations, and identify\npotential vulnerabilities in practical deployment. In closing, we articulate a\nset of open problems in current verification research, aiming to provide a\nfoundation for developing more robust, efficient, and theoretically grounded\nunlearning verification mechanisms.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15115v1", "AI": {"title_translation": "迈向可靠遗忘：机器学习非学习验证、挑战与未来方向综述", "tldr": "该论文首次系统综述了机器学习非学习验证方法，提出了一个统一的分类法，并分析了现有技术的优缺点和未来研究方向。", "motivation": "随着隐私保护、安全和法律合规（如GDPR）需求的增长，机器学习非学习已成为确保机器学习模型可控性和符合监管要求的关键技术。然而，该领域的一个根本挑战在于有效验证非学习操作是否成功且彻底执行。尽管非学习技术的研究不断增加，但验证方法相对未被充分探索且零散，缺乏统一的分类法和系统评估框架。", "method": "本文首次对机器学习非学习验证方法进行了结构化综述。作者提出了一个分类法，将现有技术分为行为验证和参数验证两大类，并检查了每类中的代表性方法，分析了其基本假设、优点和局限性，并指出了实际部署中潜在的漏洞。", "result": "本综述提出了一个将现有机器学习非学习验证技术组织成行为验证和参数验证两大类别的分类法。它分析了每类中代表性方法的假设、优缺点及潜在漏洞，并指出了当前验证研究中存在的一系列开放问题。", "conclusion": "该综述旨在为开发更强大、高效和理论基础更扎实的非学习验证机制提供基础，通过阐明当前验证研究中的开放问题。", "translation": "随着对隐私保护、安全和法律合规（例如GDPR）需求的增长，机器学习非学习已成为确保机器学习模型可控性和监管一致性的关键技术。然而，该领域的一个根本挑战在于有效验证非学习操作是否已成功且彻底执行。尽管关于非学习技术的工作日益增多，但验证方法仍相对未被充分探索且常常支离破碎。现有方法缺乏统一的分类法和系统评估框架。为了弥补这一空白，本文首次对机器学习非学习验证方法进行了结构化综述。我们提出了一个分类法，根据用于评估非学习保真度的证据类型，将当前技术分为两个主要类别——行为验证和参数验证。我们检查了每个类别中的代表性方法，分析了它们的基本假设、优点和局限性，并识别了实际部署中潜在的漏洞。最后，我们阐明了当前验证研究中的一系列开放问题，旨在为开发更强大、高效且具有理论基础的非学习验证机制提供基础。", "summary": "本论文首次对机器学习非学习验证方法进行了结构化综述，以解决该领域验证方法不足且分散的挑战。论文提出了一个统一的分类法，将现有验证技术分为行为验证和参数验证两大类，并详细分析了每类方法的假设、优缺点和潜在漏洞。此外，论文还指出了当前验证研究中的开放问题，旨在为未来开发更可靠的非学习验证机制奠定基础。", "keywords": "机器学习非学习, 验证, 综述, 隐私保护, 分类法", "comments": "这篇综述论文填补了机器学习非学习领域的一个重要空白，即非学习操作的验证。通过提出一个统一的分类法并系统分析现有方法，它为未来的研究提供了一个清晰的框架和方向。其价值在于对现有零散知识的整合和对未来挑战的明确界定，对于推动机器学习在隐私保护和合规性方面的实际应用具有重要意义。"}}
{"id": "2506.15598", "title": "From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative and Difficulty Concerns", "authors": ["Bernardo Leite", "Henrique Lopes Cardoso", "Pedro Pinto", "Abel Ferreira", "Luís Abreu", "Isabel Rangel", "Sandra Monteiro"], "summary": "While MCQs are valuable for learning and evaluation, manually creating them\nwith varying difficulty levels and targeted reading skills remains a\ntime-consuming and costly task. Recent advances in generative AI provide an\nopportunity to automate MCQ generation efficiently. However, assessing the\nactual quality and reliability of generated MCQs has received limited attention\n-- particularly regarding cases where generation fails. This aspect becomes\nparticularly important when the generated MCQs are meant to be applied in\nreal-world settings. Additionally, most MCQ generation studies focus on\nEnglish, leaving other languages underexplored. This paper investigates the\ncapabilities of current generative models in producing MCQs for reading\ncomprehension in Portuguese, a morphologically rich language. Our study focuses\non generating MCQs that align with curriculum-relevant narrative elements and\nspan different difficulty levels. We evaluate these MCQs through expert review\nand by analyzing the psychometric properties extracted from student responses\nto assess their suitability for elementary school students. Our results show\nthat current models can generate MCQs of comparable quality to human-authored\nones. However, we identify issues related to semantic clarity and\nanswerability. Also, challenges remain in generating distractors that engage\nstudents and meet established criteria for high-quality MCQ option design.", "comment": "This is a preprint version of the manuscript currently under review\n  at an international journal", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15598v1", "AI": {"title_translation": "从模型到课堂：评估葡萄牙语生成式多项选择题的叙事性和难度", "tldr": "本研究评估了当前生成模型在为葡萄牙语阅读理解生成多项选择题（MCQs）方面的能力，重点关注叙事元素和不同难度级别。结果显示生成的多项选择题质量与人工创作的相当，但在语义清晰度、可回答性和干扰项设计上仍存在挑战。", "motivation": "手动创建具有不同难度和针对性阅读技能的多项选择题（MCQs）耗时且成本高昂。尽管生成式AI提供了自动化机会，但对生成MCQs的实际质量和可靠性（特别是在生成失败的情况下）关注有限，尤其是在将其应用于实际场景时。此外，大多数MCQ生成研究集中于英语，其他语言（如形态丰富的葡萄牙语）未得到充分探索。", "method": "本研究调查了当前生成模型在为葡萄牙语阅读理解生成MCQs方面的能力。研究重点是生成符合课程相关叙事元素并涵盖不同难度级别的MCQs。通过专家评审和分析学生回答中提取的心理测量特性来评估这些MCQs，以评估其对小学生适用性。", "result": "结果显示，当前模型生成的MCQs质量可与人工创作的媲美。然而，研究发现存在语义清晰度和可回答性方面的问题。此外，在生成能吸引学生并符合高质量MCQ选项设计既定标准的干扰项方面仍面临挑战。", "conclusion": "当前生成模型在生成葡萄牙语阅读理解多项选择题方面表现出巨大潜力，其质量可与人类作者创作的相媲美。然而，为了在实际教育环境中实现可靠应用，仍需解决语义清晰度、可回答性以及高质量干扰项生成等关键挑战。", "translation": "尽管多项选择题（MCQs）对学习和评估很有价值，但手动创建具有不同难度级别和目标阅读技能的多项选择题仍然是一项耗时且成本高昂的任务。生成式AI的最新进展为高效自动化MCQ生成提供了机会。然而，对生成MCQs的实际质量和可靠性的评估受到的关注有限——特别是在生成失败的情况下。当生成的MCQs旨在应用于实际场景时，这一方面变得尤为重要。此外，大多数MCQ生成研究都集中于英语，而其他语言则未得到充分探索。本文研究了当前生成模型在为葡萄牙语（一种形态丰富的语言）阅读理解生成MCQs方面的能力。我们的研究重点是生成符合课程相关叙事元素并涵盖不同难度级别的MCQs。我们通过专家评审和分析从学生回答中提取的心理测量特性来评估这些MCQs，以评估它们对小学生的适用性。我们的结果表明，当前模型可以生成与人工创作的MCQs质量相当的MCQs。然而，我们发现存在与语义清晰度和可回答性相关的问题。此外，在生成能吸引学生并符合高质量MCQ选项设计既定标准的干扰项方面仍然存在挑战。", "summary": "本研究评估了生成模型在为葡萄牙语阅读理解生成多项选择题（MCQs）方面的能力，旨在解决手动创建MCQs的耗时问题和现有研究对非英语语言的忽视。论文关注生成符合课程叙事并具有不同难度级别的MCQs。通过专家评审和学生反馈的心理测量分析，研究发现生成MCQs的质量与人工创作的相当，但仍存在语义清晰度、可回答性以及高质量干扰项生成方面的挑战，这表明在实际教学应用前仍需改进。", "keywords": "多项选择题生成, 葡萄牙语, 生成式AI, 阅读理解, 质量评估", "comments": "这篇论文的创新点在于它关注了非英语（葡萄牙语）的MCQ生成，并深入评估了生成MCQs在实际应用中的质量问题，包括对叙事元素和难度级别的考量。其重要性在于指出了生成式AI在教育应用中的潜力与局限性，特别是在语言和内容质量方面的挑战。局限性可能在于其评估范围（仅限小学水平）和对模型内部机制的深入分析不足。"}}
{"id": "2506.15369", "title": "Unsupervised Pelage Pattern Unwrapping for Animal Re-identification", "authors": ["Aleksandr Algasov", "Ekaterina Nepovinnykh", "Fedor Zolotarev", "Tuomas Eerola", "Heikki Kälviäinen", "Pavel Zemčík", "Charles V. Stewart"], "summary": "Existing individual re-identification methods often struggle with the\ndeformable nature of animal fur or skin patterns which undergo geometric\ndistortions due to body movement and posture changes. In this paper, we propose\na geometry-aware texture mapping approach that unwarps pelage patterns, the\nunique markings found on an animal's skin or fur, into a canonical UV space,\nenabling more robust feature matching. Our method uses surface normal\nestimation to guide the unwrapping process while preserving the geometric\nconsistency between the 3D surface and the 2D texture space. We focus on two\nchallenging species: Saimaa ringed seals (Pusa hispida saimensis) and leopards\n(Panthera pardus). Both species have distinctive yet highly deformable fur\npatterns. By integrating our pattern-preserving UV mapping with existing\nre-identification techniques, we demonstrate improved accuracy across diverse\nposes and viewing angles. Our framework does not require ground truth UV\nannotations and can be trained in a self-supervised manner. Experiments on seal\nand leopard datasets show up to a 5.4% improvement in re-identification\naccuracy.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15369v1", "AI": {"title_translation": "动物再识别中无监督皮毛图案展开", "tldr": "针对动物毛皮图案因身体运动而变形导致再识别困难的问题，本文提出一种无监督的几何感知纹理映射方法，将皮毛图案展开到规范的UV空间，以实现更鲁棒的特征匹配，并在海豹和豹子数据集上将再识别准确率提高了高达5.4%。", "motivation": "现有动物个体再识别方法难以处理动物毛皮或皮肤图案的可变形性，这些图案会因身体运动和姿态变化而发生几何畸变，导致特征匹配不鲁棒。", "method": "本文提出一种几何感知纹理映射方法，将皮毛图案展开到规范的UV空间。该方法利用表面法线估计来指导展开过程，同时保持3D表面和2D纹理空间之间的几何一致性。它将图案保留的UV映射与现有再识别技术相结合，且无需地面真实UV标注，可自监督训练。", "result": "在塞马环斑海豹和豹子数据集上的实验表明，该方法使再识别准确率提高了高达5.4%，并在不同姿态和视角下展示了准确性的提高。", "conclusion": "本文提出的无监督皮毛图案展开方法通过处理动物图案变形问题，显著提高了动物再识别的准确性和特征匹配的鲁棒性。", "translation": "现有动物个体再识别方法常难以处理动物毛皮或皮肤图案的可变形性，这些图案会因身体运动和姿态变化而发生几何畸变。本文提出一种几何感知纹理映射方法，将动物皮肤或毛发上独特的皮毛图案展开到规范的UV空间，从而实现更鲁棒的特征匹配。我们的方法利用表面法线估计来指导展开过程，同时保持3D表面和2D纹理空间之间的几何一致性。我们专注于两种具有挑战性的物种：塞马环斑海豹（Pusa hispida saimensis）和豹子（Panthera pardus）。这两种物种都具有独特但高度可变形的毛皮图案。通过将我们保留图案的UV映射与现有再识别技术相结合，我们展示了在不同姿态和视角下准确性的提高。我们的框架不需要地面真实UV标注，并且可以以自监督方式进行训练。在海豹和豹子数据集上的实验表明，再识别准确率提高了高达5.4%。", "summary": "本文提出一种无监督的几何感知纹理映射方法，旨在解决动物再识别中因毛皮/皮肤图案变形导致的挑战。该方法将独特的皮毛图案展开到规范的UV空间，通过表面法线估计指导展开过程，从而实现更鲁棒的特征匹配。该方法与现有再识别技术相结合，并在塞马环斑海豹和豹子等物种上进行验证，结果显示再识别准确率提高了高达5.4%，且无需地面真实UV标注。", "keywords": "动物再识别, 皮毛图案, 无监督学习, UV展开, 几何畸变", "comments": "该论文的创新之处在于通过无监督的几何感知展开方法，解决了动物再识别中关键的图案变形问题。其自监督特性和不依赖地面真实UV标注的特点，在实际野生动物监测和保护工作中具有重要的实用价值。尽管准确率提升5.4%看似不大，但在对个体识别可靠性要求极高的现实应用中，这一提升可能带来显著的影响。"}}
{"id": "2506.15181", "title": "ImprovDML: Improved Trade-off in Private Byzantine-Resilient Distributed Machine Learning", "authors": ["Bing Liu", "Chengcheng Zhao", "Li Chai", "Peng Cheng", "Yaonan Wang"], "summary": "Jointly addressing Byzantine attacks and privacy leakage in distributed\nmachine learning (DML) has become an important issue. A common strategy\ninvolves integrating Byzantine-resilient aggregation rules with differential\nprivacy mechanisms. However, the incorporation of these techniques often\nresults in a significant degradation in model accuracy. To address this issue,\nwe propose a decentralized DML framework, named ImprovDML, that achieves high\nmodel accuracy while simultaneously ensuring privacy preservation and\nresilience to Byzantine attacks. The framework leverages a kind of resilient\nvector consensus algorithms that can compute a point within the normal\n(non-Byzantine) agents' convex hull for resilient aggregation at each\niteration. Then, multivariate Gaussian noises are introduced to the gradients\nfor privacy preservation. We provide convergence guarantees and derive\nasymptotic learning error bounds under non-convex settings, which are tighter\nthan those reported in existing works. For the privacy analysis, we adopt the\nnotion of concentrated geo-privacy, which quantifies privacy preservation based\non the Euclidean distance between inputs. We demonstrate that it enables an\nimproved trade-off between privacy preservation and model accuracy compared to\ndifferential privacy. Finally, numerical simulations validate our theoretical\nresults.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15181v1", "AI": {"title_translation": "ImprovDML：改进的私有拜占庭容错分布式机器学习中的权衡", "tldr": "ImprovDML是一个去中心化的分布式机器学习框架，在拜占庭攻击和隐私保护下，显著提高了模型准确性，并通过集中式地理隐私实现了更好的隐私-准确性权衡。", "motivation": "在分布式机器学习中，同时解决拜占庭攻击和隐私泄露是一个重要问题。现有策略（结合拜占庭容错聚合规则与差分隐私机制）通常会导致模型准确性显著下降，因此需要一种能同时确保高模型准确性、隐私保护和拜占庭攻击弹性的新方法。", "method": "本文提出了一个名为ImprovDML的去中心化分布式机器学习框架。该框架利用一种弹性向量共识算法，在每次迭代中计算正常（非拜占庭）代理凸包内的一个点进行弹性聚合。然后，引入多元高斯噪声到梯度中以实现隐私保护。在隐私分析方面，采用了集中式地理隐私的概念，该概念基于输入间的欧几里得距离量化隐私保护。", "result": "ImprovDML在确保隐私保护和拜占庭攻击弹性的同时，实现了高模型准确性。在非凸设置下，提供了收敛性保证并推导了渐近学习误差界，这些界限比现有工作中报告的更紧。与差分隐私相比，它在隐私保护和模型准确性之间实现了更好的权衡。数值模拟验证了理论结果。", "conclusion": "ImprovDML框架通过创新的弹性聚合和隐私保护机制，成功地在分布式机器学习中解决了拜占庭攻击和隐私泄露问题，并在模型准确性、隐私保护和拜占庭弹性之间实现了显著改进的权衡。", "translation": "在分布式机器学习（DML）中，同时解决拜占庭攻击和隐私泄露已成为一个重要问题。一种常见的策略是将拜占庭容错聚合规则与差分隐私机制相结合。然而，这些技术的结合往往会导致模型准确性显著下降。为了解决这个问题，我们提出了一个名为ImprovDML的去中心化DML框架，该框架在确保隐私保护和拜占庭攻击弹性的同时，实现了高模型准确性。该框架利用一种弹性向量共识算法，可以在每次迭代中计算正常（非拜占庭）代理凸包内的一个点进行弹性聚合。然后，将多元高斯噪声引入梯度中以实现隐私保护。我们在非凸设置下提供了收敛性保证，并推导了渐近学习误差界，这些界限比现有工作中报告的更紧。对于隐私分析，我们采用了集中式地理隐私的概念，该概念基于输入之间的欧几里得距离量化隐私保护。我们证明，与差分隐私相比，它能够在隐私保护和模型准确性之间实现更好的权衡。最后，数值模拟验证了我们的理论结果。", "summary": "ImprovDML是一个去中心化的分布式机器学习框架，旨在解决现有方法在处理拜占庭攻击和隐私泄露时导致模型准确性下降的问题。该框架通过利用弹性向量共识算法进行聚合，并引入多元高斯噪声实现隐私保护。它采用集中式地理隐私进行隐私分析，并在理论上证明了其在非凸设置下的收敛性和更紧的误差界。实验结果表明，ImprovDML在隐私保护、拜占庭容错和模型准确性之间实现了显著改进的权衡。", "keywords": "分布式机器学习, 拜占庭容错, 隐私保护, 模型准确性, 集中式地理隐私", "comments": "本文的创新点在于提出了ImprovDML框架，通过结合弹性向量共识算法和多元高斯噪声，在分布式机器学习中实现了隐私保护和拜占庭容错的平衡。其重要性在于解决了现有技术在隐私和准确性之间权衡不佳的问题，并提供了更紧凑的理论误差界。采用集中式地理隐私的概念也提供了一个新的视角来量化隐私保护，并展现了优于传统差分隐私的潜力。"}}
{"id": "2506.15617", "title": "The Compositional Architecture of Regret in Large Language Models", "authors": ["Xiangxiang Cui", "Shu Yang", "Tianjin Huang", "Wanyu Lin", "Lijie Hu", "Di Wang"], "summary": "Regret in Large Language Models refers to their explicit regret expression\nwhen presented with evidence contradicting their previously generated\nmisinformation. Studying the regret mechanism is crucial for enhancing model\nreliability and helps in revealing how cognition is coded in neural networks.\nTo understand this mechanism, we need to first identify regret expressions in\nmodel outputs, then analyze their internal representation. This analysis\nrequires examining the model's hidden states, where information processing\noccurs at the neuron level. However, this faces three key challenges: (1) the\nabsence of specialized datasets capturing regret expressions, (2) the lack of\nmetrics to find the optimal regret representation layer, and (3) the lack of\nmetrics for identifying and analyzing regret neurons. Addressing these\nlimitations, we propose: (1) a workflow for constructing a comprehensive regret\ndataset through strategically designed prompting scenarios, (2) the Supervised\nCompression-Decoupling Index (S-CDI) metric to identify optimal regret\nrepresentation layers, and (3) the Regret Dominance Score (RDS) metric to\nidentify regret neurons and the Group Impact Coefficient (GIC) to analyze\nactivation patterns. Our experimental results successfully identified the\noptimal regret representation layer using the S-CDI metric, which significantly\nenhanced performance in probe classification experiments. Additionally, we\ndiscovered an M-shaped decoupling pattern across model layers, revealing how\ninformation processing alternates between coupling and decoupling phases.\nThrough the RDS metric, we categorized neurons into three distinct functional\ngroups: regret neurons, non-regret neurons, and dual neurons.", "comment": "23 pages", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15617v1", "AI": {"title_translation": "大型语言模型中后悔的组合架构", "tldr": "本文研究大型语言模型中的后悔表达机制，提出了一系列方法和指标来识别和分析后悔相关的神经活动。", "motivation": "研究后悔机制对于提高模型可靠性至关重要，并有助于揭示认知是如何在神经网络中编码的。", "method": "提出一个工作流程来构建后悔数据集；引入Supervised Compression-Decoupling Index (S-CDI) 度量来识别最佳后悔表示层；引入Regret Dominance Score (RDS) 度量来识别后悔神经元，并引入Group Impact Coefficient (GIC) 来分析激活模式。", "result": "成功使用S-CDI度量识别了最佳后悔表示层，显著提高了探针分类实验的性能。发现模型层之间存在M形解耦模式，揭示了信息处理如何在耦合和解耦阶段之间交替。通过RDS度量将神经元分为后悔神经元、非后悔神经元和双重神经元三类。", "conclusion": "研究成功识别了大型语言模型中后悔的最佳表示层和相关神经元，揭示了其内部信息处理机制，为提高模型可靠性提供了新的视角。", "translation": "大型语言模型中的后悔是指当模型接收到与其先前生成的错误信息相矛盾的证据时，其明确表达的后悔。研究后悔机制对于提高模型可靠性至关重要，并有助于揭示认知是如何在神经网络中编码的。为了理解这种机制，我们首先需要识别模型输出中的后悔表达，然后分析其内部表示。这种分析需要检查模型在神经元层面进行信息处理的隐藏状态。然而，这面临三个关键挑战：(1) 缺乏捕获后悔表达的专用数据集，(2) 缺乏寻找最佳后悔表示层的度量，以及 (3) 缺乏识别和分析后悔神经元的度量。为了解决这些限制，我们提出：(1) 通过策略性设计的提示场景构建全面后悔数据集的工作流程，(2) 用于识别最佳后悔表示层的监督压缩-解耦指数（S-CDI）度量，以及 (3) 用于识别后悔神经元的后悔主导得分（RDS）度量和用于分析激活模式的群组影响系数（GIC）。我们的实验结果成功地使用S-CDI度量识别了最佳后悔表示层，这显著提高了探针分类实验的性能。此外，我们发现模型层之间存在M形解耦模式，揭示了信息处理如何在耦合和解耦阶段之间交替。通过RDS度量，我们将神经元分为三个不同的功能组：后悔神经元、非后悔神经元和双重神经元。", "summary": "本文研究大型语言模型（LLMs）如何通过明确的后悔表达来处理其先前生成的错误信息。为理解LLMs中的后悔机制，作者提出了解决数据、度量和神经元识别三大挑战的方法：构建后悔数据集的工作流程，以及S-CDI、RDS和GIC三种新度量。实验成功识别了最佳后悔表示层，揭示了M形解耦模式，并将神经元分为后悔、非后悔和双重三类，为理解LLMs的认知机制和提高可靠性提供了新工具和见解。", "keywords": "大型语言模型, 后悔机制, 神经元分析, S-CDI, RDS", "comments": "该研究通过提出一套新颖的度量和方法，系统地解决了大型语言模型中后悔机制的识别和分析难题。其创新点在于引入了S-CDI、RDS和GIC等量化指标，使得对模型内部认知过程的探究成为可能。这对于提升LLM的透明度、可信度和纠错能力具有重要意义。特别是对后悔神经元的分类和M形解耦模式的发现，为未来LLM的架构优化和行为控制提供了深入的洞察。这项工作为理解LLM的“认知”层面迈出了重要一步。"}}
{"id": "2506.15381", "title": "When Model Knowledge meets Diffusion Model: Diffusion-assisted Data-free Image Synthesis with Alignment of Domain and Class", "authors": ["Yujin Kim", "Hyunsoo Kim", "Hyunwoo J. Kim", "Suhyun Kim"], "summary": "Open-source pre-trained models hold great potential for diverse applications,\nbut their utility declines when their training data is unavailable. Data-Free\nImage Synthesis (DFIS) aims to generate images that approximate the learned\ndata distribution of a pre-trained model without accessing the original data.\nHowever, existing DFIS meth ods produce samples that deviate from the training\ndata distribution due to the lack of prior knowl edge about natural images. To\novercome this limitation, we propose DDIS, the first Diffusion-assisted\nData-free Image Synthesis method that leverages a text-to-image diffusion model\nas a powerful image prior, improving synthetic image quality. DDIS extracts\nknowledge about the learned distribution from the given model and uses it to\nguide the diffusion model, enabling the generation of images that accurately\nalign with the training data distribution. To achieve this, we introduce Domain\nAlignment Guidance (DAG) that aligns the synthetic data domain with the\ntraining data domain during the diffusion sampling process. Furthermore, we\noptimize a single Class Alignment Token (CAT) embedding to effectively capture\nclass-specific attributes in the training dataset. Experiments on PACS and Ima\ngeNet demonstrate that DDIS outperforms prior DFIS methods by generating\nsamples that better reflect the training data distribution, achieving SOTA\nperformance in data-free applications.", "comment": "Published at ICML 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15381v1", "AI": {"title_translation": "当模型知识遇上扩散模型：扩散辅助的无数据图像合成，结合域与类别对齐", "tldr": "DDIS是一种新的无数据图像合成方法，它利用文本到图像扩散模型作为强大的图像先验，并通过域对齐和类别对齐来生成高质量图像，优于现有方法。", "motivation": "现有的无数据图像合成（DFIS）方法生成的样本偏离训练数据分布，因为它们缺乏关于自然图像的先验知识。", "method": "提出DDIS，首次将文本到图像扩散模型作为强大的图像先验。它从给定模型中提取知识来指导扩散模型。具体包括：1) 域对齐指导（DAG），在扩散采样过程中对齐合成数据域与训练数据域。2) 优化单个类别对齐Token（CAT）嵌入，以有效捕获训练数据集中的类别特定属性。", "result": "在PACS和ImageNet上的实验表明，DDIS生成能更好反映训练数据分布的样本，性能优于先前的DFIS方法，并在无数据应用中取得了最先进的（SOTA）性能。", "conclusion": "DDIS通过引入扩散模型作为图像先验，并结合域对齐和类别对齐机制，显著提升了无数据图像合成的质量和与原始训练数据分布的对齐程度，为数据受限的应用提供了有效解决方案。", "translation": "开源预训练模型在各种应用中具有巨大潜力，但当其训练数据不可用时，其效用会下降。无数据图像合成（DFIS）旨在生成近似预训练模型学习到的数据分布的图像，而无需访问原始数据。然而，由于缺乏关于自然图像的先验知识，现有的DFIS方法生成的样本偏离了训练数据分布。为了克服这一限制，我们提出了DDIS，这是第一个扩散辅助的无数据图像合成方法，它利用文本到图像扩散模型作为强大的图像先验，从而提高了合成图像的质量。DDIS从给定模型中提取关于学习分布的知识，并用它来指导扩散模型，从而能够生成与训练数据分布精确对齐的图像。为了实现这一点，我们引入了域对齐指导（DAG），它在扩散采样过程中将合成数据域与训练数据域对齐。此外，我们优化了一个单一的类别对齐Token（CAT）嵌入，以有效地捕获训练数据集中的类别特定属性。在PACS和ImageNet上的实验表明，DDIS通过生成更能反映训练数据分布的样本，优于先前的DFIS方法，并在无数据应用中取得了最先进的（SOTA）性能。", "summary": "本文提出了DDIS，一种新颖的扩散辅助无数据图像合成方法，旨在解决现有DFIS方法生成图像偏离原始训练数据分布的问题。DDIS利用文本到图像扩散模型作为强大的图像先验，并通过引入域对齐指导（DAG）和优化类别对齐Token（CAT）嵌入，确保合成图像与训练数据分布高度对齐。实验证明，DDIS在无数据图像合成任务上超越了现有方法，取得了最先进的性能。", "keywords": "无数据图像合成, 扩散模型, 域对齐, 类别对齐, 图像生成", "comments": "本文的创新点在于首次将强大的文本到图像扩散模型引入无数据图像合成领域，并设计了域对齐和类别对齐机制，有效解决了现有方法生成的图像与原始数据分布不一致的问题。这对于数据隐私受限或数据不可用的场景具有重要意义，极大地提升了无数据图像合成的实用价值和图像质量。"}}
{"id": "2506.15190", "title": "Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal Behaviors", "authors": ["Jiyi Wang", "Jingyang Ke", "Bo Dai", "Anqi Wu"], "summary": "Animals flexibly recombine a finite set of core motor primitives to meet\ndiverse task demands, but existing behavior-segmentation methods oversimplify\nthis process by imposing discrete syllables under restrictive generative\nassumptions. To reflect the animal behavior generation procedure, we introduce\nskill-based imitation learning (SKIL) for behavior understanding, a\nreinforcement learning-based imitation framework that (1) infers interpretable\nskill sets, i.e., latent basis functions of behavior, by leveraging\nrepresentation learning on transition probabilities, and (2) parameterizes\npolicies as dynamic mixtures of these skills. We validate our approach on a\nsimple grid world, a discrete labyrinth, and unconstrained videos of freely\nmoving animals. Across tasks, it identifies reusable skill components, learns\ncontinuously evolving compositional policies, and generates realistic\ntrajectories beyond the capabilities of traditional discrete models. By\nexploiting generative behavior modeling with compositional representations, our\nmethod offers a concise, principled account of how complex animal behaviors\nemerge from dynamic combinations of fundamental motor primitives.", "comment": "9 pages and 4 figures for the main text", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15190v1", "AI": {"title_translation": "学习任务无关的技能基础以揭示动物行为中的运动原语", "tldr": "该论文引入了SKIL，一个基于强化学习的框架，通过推断可解释、可重用的技能集来理解动物行为，超越了离散模型以表示连续、组合的策略。", "motivation": "现有行为分割方法通过在限制性生成假设下强加离散音节来过度简化动物行为生成过程，未能反映动物如何灵活地重新组合有限的核心运动原语集。", "method": "本文引入了基于技能的模仿学习（SKIL），一个基于强化学习的模仿框架。SKIL（1）通过利用转移概率上的表示学习来推断可解释的技能集（即行为的潜在基函数），以及（2）将策略参数化为这些技能的动态混合。", "result": "SKIL在一个简单的网格世界、一个离散迷宫和自由移动动物的无约束视频上进行了验证。它识别出可重用的技能组件，学习持续演变的组合策略，并生成了超出传统离散模型能力范围的真实轨迹。", "conclusion": "通过利用具有组合表示的生成行为建模，SKIL提供了一种简洁、有原则的解释，说明复杂的动物行为是如何从基本运动原语的动态组合中产生的。", "translation": "动物灵活地重新组合有限的核心运动原语集以满足多样化的任务需求，但现有的行为分割方法通过在限制性生成假设下强加离散音节来过度简化此过程。为了反映动物行为生成过程，我们引入了基于技能的模仿学习（SKIL）用于行为理解，这是一个基于强化学习的模仿框架，它（1）通过利用转移概率上的表示学习来推断可解释的技能集，即行为的潜在基函数，并且（2）将策略参数化为这些技能的动态混合。我们在一个简单的网格世界、一个离散迷宫和自由移动动物的无约束视频上验证了我们的方法。在各项任务中，它识别出可重用的技能组件，学习持续演变的组合策略，并生成超出传统离散模型能力范围的真实轨迹。通过利用具有组合表示的生成行为建模，我们的方法提供了一种简洁、有原则的解释，说明复杂的动物行为是如何从基本运动原语的动态组合中产生的。", "summary": "本文介绍了基于技能的模仿学习（SKIL），一个旨在通过推断可解释的、任务无关的技能基础来理解动物行为的强化学习框架。与依赖离散音节的传统方法不同，SKIL利用转移概率上的表示学习来识别行为的潜在基函数，并将策略参数化为这些技能的动态混合。SKIL在各种任务中得到验证，成功地揭示了可重用的技能组件，学习了连续的组合策略，并生成了真实的轨迹，为复杂的动物行为如何从基本运动原语中产生提供了更具原则性的解释。", "keywords": "运动原语, 技能学习, 强化学习, 行为理解, 组合表示", "comments": "其创新之处在于超越了离散、限制性模型，转向了动物运动原语的更连续和组合的表示，这更好地反映了生物现实。利用强化学习和转移概率上的表示学习来推断可解释的技能集是其关键优势。这种方法可以显著推进我们对运动控制的理解，并实现更真实的合成行为。"}}
{"id": "2506.15404", "title": "NERO: Explainable Out-of-Distribution Detection with Neuron-level Relevance", "authors": ["Anju Chhetri", "Jari Korhonen", "Prashnna Gyawali", "Binod Bhattarai"], "summary": "Ensuring reliability is paramount in deep learning, particularly within the\ndomain of medical imaging, where diagnostic decisions often hinge on model\noutputs. The capacity to separate out-of-distribution (OOD) samples has proven\nto be a valuable indicator of a model's reliability in research. In medical\nimaging, this is especially critical, as identifying OOD inputs can help flag\npotential anomalies that might otherwise go undetected. While many OOD\ndetection methods rely on feature or logit space representations, recent works\nsuggest these approaches may not fully capture OOD diversity. To address this,\nwe propose a novel OOD scoring mechanism, called NERO, that leverages\nneuron-level relevance at the feature layer. Specifically, we cluster\nneuron-level relevance for each in-distribution (ID) class to form\nrepresentative centroids and introduce a relevance distance metric to quantify\na new sample's deviation from these centroids, enhancing OOD separability.\nAdditionally, we refine performance by incorporating scaled relevance in the\nbias term and combining feature norms. Our framework also enables explainable\nOOD detection. We validate its effectiveness across multiple deep learning\narchitectures on the gastrointestinal imaging benchmarks Kvasir and\nGastroVision, achieving improvements over state-of-the-art OOD detection\nmethods.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15404v1", "AI": {"title_translation": "NERO：基于神经元层级相关性的可解释性分布外检测", "tldr": "NERO是一种新的OOD检测方法，通过利用神经元层级相关性提高医学图像中OOD样本的可分离性，并实现可解释性。", "motivation": "深度学习在医疗成像中的可靠性至关重要，识别分布外（OOD）样本是模型可靠性的重要指标，尤其在医疗领域可用于标记潜在异常。现有OOD检测方法可能无法完全捕捉OOD多样性。", "method": "提出了一种名为NERO的新型OOD评分机制，利用特征层的神经元层级相关性。具体地，它聚类每个分布内（ID）类别的神经元层级相关性以形成代表性中心，并引入相关性距离度量来量化新样本与这些中心的偏差。此外，通过在偏置项中结合缩放相关性并组合特征范数来优化性能。该框架还支持可解释的OOD检测。", "result": "在胃肠道成像基准Kvasir和GastroVision上，NERO在多种深度学习架构上验证了其有效性，并优于最先进的OOD检测方法。", "conclusion": "NERO通过利用神经元层级相关性，提供了一种有效且可解释的OOD检测方法，特别适用于医疗成像领域，提高了模型在识别异常样本时的可靠性。", "translation": "确保可靠性在深度学习中至关重要，特别是在医疗成像领域，诊断决策常常依赖于模型输出。分离分布外（OOD）样本的能力已被证明是模型可靠性的一个有价值的指标。在医疗成像中，这一点尤其关键，因为识别OOD输入有助于标记可能未被发现的潜在异常。虽然许多OOD检测方法依赖于特征或逻辑空间表示，但最近的研究表明这些方法可能无法完全捕捉OOD多样性。为了解决这个问题，我们提出了一种新颖的OOD评分机制，称为NERO，它利用特征层的神经元层级相关性。具体来说，我们聚类每个分布内（ID）类别的神经元层级相关性以形成代表性中心，并引入相关性距离度量来量化新样本与这些中心的偏差，从而增强OOD的可分离性。此外，我们通过在偏置项中结合缩放相关性并组合特征范数来优化性能。我们的框架还支持可解释的OOD检测。我们在胃肠道成像基准Kvasir和GastroVision上，在多种深度学习架构上验证了其有效性，并实现了对最先进OOD检测方法的改进。", "summary": "NERO是一种新颖的分布外（OOD）检测方法，通过利用深度学习模型中特征层的神经元层级相关性来提高OOD样本的可分离性。它通过聚类分布内类别的神经元相关性形成代表性中心，并使用相关性距离来衡量新样本的偏差。该方法在医疗成像领域尤为重要，因为它能识别潜在异常并提供可解释的OOD检测。在Kvasir和GastroVision数据集上的实验表明，NERO优于现有最先进的OOD检测方法。", "keywords": "分布外检测, 神经元层级相关性, 可解释性AI, 医疗成像, 深度学习可靠性", "comments": "这篇论文的创新点在于提出了NERO，一种利用神经元层级相关性进行OOD检测的方法，这与传统的基于特征或逻辑空间的方法不同，可能更好地捕捉OOD多样性。其在医疗成像领域的应用显示出重要性，因为OOD检测有助于提高模型在关键诊断决策中的可靠性。此外，该方法的可解释性也增加了其在实际应用中的价值。"}}
{"id": "2506.15199", "title": "Interpretability and Generalization Bounds for Learning Spatial Physics", "authors": ["Alejandro Francisco Queiruga", "Theo Gutman-Solo", "Shuai Jiang"], "summary": "While there are many applications of ML to scientific problems that look\npromising, visuals can be deceiving. For scientific applications, actual\nquantitative accuracy is crucial. This work applies the rigor of numerical\nanalysis for differential equations to machine learning by specifically\nquantifying the accuracy of applying different ML techniques to the elementary\n1D Poisson differential equation. Beyond the quantity and discretization of\ndata, we identify that the function space of the data is critical to the\ngeneralization of the model. We prove generalization bounds and convergence\nrates under finite data discretizations and restricted training data subspaces\nby analyzing the training dynamics and deriving optimal parameters for both a\nwhite-box differential equation discovery method and a black-box linear model.\nThe analytically derived generalization bounds are replicated empirically.\nSimilar lack of generalization is empirically demonstrated for deep linear\nmodels, shallow neural networks, and physics-specific DeepONets and Neural\nOperators. We theoretically and empirically demonstrate that generalization to\nthe true physical equation is not guaranteed in each explored case.\nSurprisingly, we find that different classes of models can exhibit opposing\ngeneralization behaviors. Based on our theoretical analysis, we also\ndemonstrate a new mechanistic interpretability lens on scientific models\nwhereby Green's function representations can be extracted from the weights of\nblack-box models. Our results inform a new cross-validation technique for\nmeasuring generalization in physical systems. We propose applying it to the\nPoisson equation as an evaluation benchmark of future methods.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15199v1", "AI": {"title_translation": "学习空间物理的可解释性与泛化界限", "tldr": "本文通过数值分析方法，严格量化了机器学习模型在解决泊松方程时的准确性和泛化能力，证明了在有限数据下，泛化到真实物理方程并非总能得到保证，并提出了新的可解释性方法和泛化测量技术。", "motivation": "ML在科学问题中的应用虽然前景广阔，但视觉效果可能具有欺骗性，实际的定量准确性至关重要。", "method": "将微分方程的数值分析严谨性应用于机器学习，量化了不同ML技术在基本一维泊松微分方程上的准确性。通过分析训练动态，并为白盒微分方程发现方法和黑盒线性模型推导最优参数，证明了有限数据离散化和受限训练数据子空间下的泛化界限和收敛率。", "result": "数据的功能空间对模型泛化至关重要；证明了有限数据离散化和受限训练数据子空间下的泛化界限和收敛率；经验性地复制了分析推导的泛化界限；经验性地证明了深度线性模型、浅层神经网络以及物理专用DeepONets和神经算子都存在类似的泛化不足；理论和经验上证明了在每种探索的案例中，无法保证泛化到真实的物理方程；不同类别的模型可以表现出相反的泛化行为；提出了一种新的机制可解释性视角，可以通过黑盒模型的权重提取格林函数表示。", "conclusion": "机器学习模型在学习空间物理时，泛化到真实的物理方程并非总能得到保证，且不同模型可能表现出相反的泛化行为。本研究结果启发了一种测量物理系统中泛化的新交叉验证技术，并建议将其应用于泊松方程作为未来方法的评估基准。此外，还提供了一种新的可解释性方法，通过权重提取格林函数。", "translation": "尽管机器学习在科学问题中的许多应用前景广阔，但视觉效果可能具有欺骗性。对于科学应用而言，实际的定量准确性至关重要。本研究将微分方程数值分析的严谨性应用于机器学习，通过量化不同机器学习技术在基本一维泊松微分方程上的应用准确性。除了数据的数量和离散化，我们发现数据的功能空间对模型的泛化至关重要。通过分析训练动态并为白盒微分方程发现方法和黑盒线性模型推导最优参数，我们证明了在有限数据离散化和受限训练数据子空间下的泛化界限和收敛率。分析推导的泛化界限得到了经验复制。深度线性模型、浅层神经网络以及物理专用DeepONets和神经算子也经验性地表现出类似的泛化不足。我们从理论和经验上证明，在每种探索的案例中，无法保证泛化到真实的物理方程。令人惊讶的是，我们发现不同类别的模型可以表现出相反的泛化行为。基于我们的理论分析，我们还展示了一种新的科学模型机制可解释性视角，通过该视角可以从黑盒模型的权重中提取格林函数表示。我们的结果启发了一种测量物理系统中泛化的新交叉验证技术。我们建议将其应用于泊松方程作为未来方法的评估基准。", "summary": "本文将数值分析的严谨性应用于机器学习，量化了不同ML技术在解决基本一维泊松微分方程时的准确性。研究发现数据的功能空间对模型泛化至关重要，并证明了有限数据下模型的泛化界限和收敛率。通过理论和实验验证，论文指出当前机器学习模型（包括深度线性模型、神经网络和物理专用模型）在泛化到真实物理方程时存在局限性，且泛化并非总能保证，甚至不同模型可能表现出相反的泛化行为。此外，本文还提出了一种新的机制可解释性方法，可以通过黑盒模型权重提取格林函数表示，并基于研究结果提出了一种测量物理系统泛化的新交叉验证技术和评估基准。", "keywords": "可解释性, 泛化界限, 空间物理, 泊松方程, 数值分析", "comments": "这篇论文的创新点在于将数值分析的严谨性引入到机器学习在科学应用中的泛化性研究。它不仅提供了理论上的泛化界限证明，还通过广泛的经验验证揭示了当前物理机器学习模型在泛化到真实物理方程方面的局限性。特别是发现不同模型可能表现出相反的泛化行为，以及提出通过格林函数进行可解释性的新视角，都具有重要的启发意义。此外，提出的新交叉验证技术和泊松方程基准对于未来研究具有指导价值。"}}
{"id": "2506.15629", "title": "Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability", "authors": ["Yusuke Sakai", "Hidetaka Kamigaito", "Taro Watanabe"], "summary": "In generative commonsense reasoning tasks such as CommonGen, generative large\nlanguage models (LLMs) compose sentences that include all given concepts.\nHowever, when focusing on instruction-following capabilities, if a prompt\nspecifies a concept order, LLMs must generate sentences that adhere to the\nspecified order. To address this, we propose Ordered CommonGen, a benchmark\ndesigned to evaluate the compositional generalization and instruction-following\nabilities of LLMs. This benchmark measures ordered coverage to assess whether\nconcepts are generated in the specified order, enabling a simultaneous\nevaluation of both abilities. We conducted a comprehensive analysis using 36\nLLMs and found that, while LLMs generally understand the intent of\ninstructions, biases toward specific concept order patterns often lead to\nlow-diversity outputs or identical results even when the concept order is\naltered. Moreover, even the most instruction-compliant LLM achieved only about\n75% ordered coverage, highlighting the need for improvements in both\ninstruction-following and compositional generalization capabilities.", "comment": "ACL 2025 Main", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15629v1", "AI": {"title_translation": "重新审视大型语言模型在考虑指令遵循能力下的组合泛化能力", "tldr": "本文提出了Ordered CommonGen基准来评估大型语言模型在生成式常识推理任务中的组合泛化能力和指令遵循能力。研究发现，尽管LLM能理解指令意图，但在遵循特定概念顺序方面表现不佳，即使是表现最好的模型也仅达到约75%的有序覆盖率，凸显了在这些方面改进的必要性。", "motivation": "在生成式常识推理任务中，大型语言模型（LLMs）需要根据给定概念组合句子。然而，当提示指定了概念顺序时，LLMs很难严格遵循。这表明需要一个能同时评估LLM组合泛化能力和指令遵循能力的基准。", "method": "本文提出了Ordered CommonGen，这是一个旨在评估大型语言模型组合泛化能力和指令遵循能力的基准。该基准通过测量“有序覆盖率”来评估概念是否按指定顺序生成。研究对36个LLM进行了全面分析。", "result": "研究发现，尽管大型语言模型普遍理解指令意图，但它们对特定概念顺序模式存在偏见，这常导致低多样性输出或即使概念顺序改变也产生相同结果。此外，即使是最符合指令的LLM也仅实现了约75%的有序覆盖率。", "conclusion": "大型语言模型在指令遵循和组合泛化能力方面仍需改进，尤其是在按指定顺序生成概念方面。", "translation": "在CommonGen等生成式常识推理任务中，生成式大型语言模型（LLMs）会组合包含所有给定概念的句子。然而，当关注指令遵循能力时，如果提示指定了概念顺序，LLMs必须生成遵循指定顺序的句子。为了解决这个问题，我们提出了Ordered CommonGen，这是一个旨在评估LLM组合泛化和指令遵循能力的基准。该基准测量有序覆盖率，以评估概念是否按指定顺序生成，从而能够同时评估这两种能力。我们使用36个LLM进行了全面分析，发现尽管LLM普遍理解指令的意图，但对特定概念顺序模式的偏见常常导致低多样性输出，或者即使概念顺序改变也产生相同的结果。此外，即使是最符合指令的LLM也仅实现了约75%的有序覆盖率，这凸显了在指令遵循和组合泛化能力两方面都需要改进。", "summary": "本文针对大型语言模型在生成式常识推理任务中遵循指定概念顺序的不足，提出了Ordered CommonGen基准。该基准通过测量“有序覆盖率”来同时评估LLM的组合泛化和指令遵循能力。通过对36个LLM的分析，研究发现LLM虽能理解指令，但存在概念顺序偏见，导致输出多样性低，且即使表现最好的模型也仅达到75%的有序覆盖率，表明LLM在这两方面仍需显著提升。", "keywords": "大型语言模型, 组合泛化, 指令遵循, Ordered CommonGen, 有序覆盖率", "comments": "本文提出了一个新颖的基准Ordered CommonGen，有效地结合了对大型语言模型组合泛化能力和指令遵循能力的评估，填补了现有评估方法中的空白。通过引入“有序覆盖率”这一指标，能够更细致地揭示LLM在处理结构化指令时的局限性。研究结果揭示了LLM在指令遵循方面存在的固有偏见和不足，即使是先进模型也未完全解决，这对于未来LLM的设计和改进具有重要指导意义。"}}
{"id": "2506.15442", "title": "Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material", "authors": ["Team Hunyuan3D", "Shuhui Yang", "Mingxin Yang", "Yifei Feng", "Xin Huang", "Sheng Zhang", "Zebin He", "Di Luo", "Haolin Liu", "Yunfei Zhao", "Qingxiang Lin", "Zeqiang Lai", "Xianghui Yang", "Huiwen Shi", "Zibo Zhao", "Bowen Zhang", "Hongyu Yan", "Lifu Wang", "Sicong Liu", "Jihong Zhang", "Meng Chen", "Liang Dong", "Yiwen Jia", "Yulin Cai", "Jiaao Yu", "Yixuan Tang", "Dongyuan Guo", "Junlin Yu", "Hao Zhang", "Zheng Ye", "Peng He", "Runzhou Wu", "Shida Wei", "Chao Zhang", "Yonghao Tan", "Yifu Sun", "Lin Niu", "Shirui Huang", "Bojian Zheng", "Shu Liu", "Shilin Chen", "Xiang Yuan", "Xiaofeng Yang", "Kai Liu", "Jianchen Zhu", "Peng Chen", "Tian Liu", "Di Wang", "Yuhong Liu", "Linus", "Jie Jiang", "Jingwei Huang", "Chunchao Guo"], "summary": "3D AI-generated content (AIGC) is a passionate field that has significantly\naccelerated the creation of 3D models in gaming, film, and design. Despite the\ndevelopment of several groundbreaking models that have revolutionized 3D\ngeneration, the field remains largely accessible only to researchers,\ndevelopers, and designers due to the complexities involved in collecting,\nprocessing, and training 3D models. To address these challenges, we introduce\nHunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a\ncomprehensive, step-by-step guide on processing 3D data, training a 3D\ngenerative model, and evaluating its performance using Hunyuan3D 2.1, an\nadvanced system for producing high-resolution, textured 3D assets. The system\ncomprises two core components: the Hunyuan3D-DiT for shape generation and the\nHunyuan3D-Paint for texture synthesis. We will explore the entire workflow,\nincluding data preparation, model architecture, training strategies, evaluation\nmetrics, and deployment. By the conclusion of this tutorial, you will have the\nknowledge to finetune or develop a robust 3D generative model suitable for\napplications in gaming, virtual reality, and industrial design.", "comment": "Github link: https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15442v1", "AI": {"title_translation": "混元3D 2.1：从图像到高保真3D资产与生产级PBR材质", "tldr": "Hunyuan3D 2.1是一个系统和教程，旨在简化从图像生成高保真、生产级3D资产的过程，提高3D AI生成内容（AIGC）的可访问性。", "motivation": "尽管3D AI生成内容（AIGC）领域发展迅速，但由于3D模型收集、处理和训练的复杂性，该领域主要局限于研究人员、开发人员和设计师。本研究旨在解决这些挑战，提高3D AIGC的普及性。", "method": "本论文通过教程的形式介绍了Hunyuan3D 2.1系统，这是一个用于生成高分辨率、带纹理3D资产的先进系统。该系统包含两个核心组件：用于形状生成的Hunyuan3D-DiT和用于纹理合成的Hunyuan3D-Paint。教程提供了处理3D数据、训练3D生成模型以及评估其性能的全面、分步指南，涵盖了数据准备、模型架构、训练策略、评估指标和部署的整个工作流程。", "result": "Hunyuan3D 2.1系统能够从图像生成高分辨率、带纹理的3D资产，并具备生产就绪的PBR材质。本教程旨在使学习者掌握微调或开发鲁棒3D生成模型的知识。", "conclusion": "本教程旨在赋能用户，使其能够微调或开发适用于游戏、虚拟现实和工业设计等应用的鲁棒3D生成模型，从而提高3D AI生成内容（AIGC）的可访问性和实用性。", "translation": "3D AI生成内容（AIGC）是一个充满激情的领域，它显著加速了游戏、电影和设计中3D模型的创建。尽管已经开发出一些革命性的突破性模型，彻底改变了3D生成，但由于收集、处理和训练3D模型所涉及的复杂性，该领域在很大程度上仍仅限于研究人员、开发人员和设计师。为了解决这些挑战，我们在此教程中以Hunyuan3D 2.1作为案例研究。本教程提供了关于处理3D数据、训练3D生成模型以及使用Hunyuan3D 2.1（一个用于生成高分辨率、带纹理3D资产的先进系统）评估其性能的全面、分步指南。该系统包含两个核心组件：用于形状生成的Hunyuan3D-DiT和用于纹理合成的Hunyuan3D-Paint。我们将探讨整个工作流程，包括数据准备、模型架构、训练策略、评估指标和部署。通过本教程的学习，您将掌握微调或开发适用于游戏、虚拟现实和工业设计应用的鲁棒3D生成模型的知识。", "summary": "本论文以Hunyuan3D 2.1系统为案例，提供了一个全面的教程，旨在解决3D AI生成内容（AIGC）领域中3D模型创建的复杂性和可访问性问题。Hunyuan3D 2.1是一个先进的系统，包含用于形状生成的Hunyuan3D-DiT和用于纹理合成的Hunyuan3D-Paint，能够从图像生成高保真、生产就绪的PBR材质3D资产。教程详细指导了数据处理、模型训练和性能评估的整个工作流程，旨在使更多用户能够开发和微调鲁棒的3D生成模型，以应用于游戏、虚拟现实和工业设计。", "keywords": "3D AIGC, Hunyuan3D, 生成模型, PBR材质, 3D资产", "comments": "这篇论文（以教程形式呈现）具有重要意义，因为它通过提供一个全面的指南和专门的系统（Hunyuan3D 2.1），直接解决了3D AI生成内容（AIGC）领域的可访问性障碍。其创新之处在于提供了一种实用、分步的方法来生成生产就绪的PBR材质3D资产，这对于游戏和电影等行业的实际应用至关重要。该系统为形状和纹理生成分别设计了模块化组件，是一个值得注意的架构选择。"}}
{"id": "2506.15251", "title": "Singular Value Decomposition on Kronecker Adaptation for Large Language Model", "authors": ["Yee Hin Chong", "Peng Qu"], "summary": "Large pre-trained Transformer models achieve state-of-the-art results across\ndiverse language and reasoning tasks, but full fine-tuning incurs substantial\nstorage, memory, and computational overhead. Parameter-efficient fine-tuning\n(PEFT) methods mitigate these costs by learning only a small subset of\ntask-specific parameters, yet existing approaches either introduce\ninference-time latency (adapter modules), suffer from suboptimal convergence\n(randomly initialized low-rank updates), or rely on fixed rank choices that may\nnot match task complexity (Kronecker-based decompositions).\n  We propose SoKA (SVD on Kronecker Adaptation), a novel PEFT strategy that\ncombines Kronecker-product tensor factorization with SVD-driven initialization\nand spectrum-aware dynamic rank selection. Our Kronecker-Product SVD (KPSVD)\nprocedure extracts principal components of the full weight update into compact\nKronecker factors, while an adaptive rank selection algorithm uses\nenergy-threshold and elbow-point criteria to prune negligible components.\n  Empirical evaluation on LLaMA2-7B across arithmetic reasoning (GSM8K), formal\nmathematics (MATH), and code generation (MBPP) demonstrates that SoKA requires\nonly 0.99M trainable parameters, 25% fewer than LoRA/PiSSA, while matching or\nexceeding baseline performance. Moreover, SoKA exhibits faster convergence and\nmore stable gradients, highlighting its robustness and efficiency for\nlarge-scale model adaptation.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15251v1", "AI": {"title_translation": "大型语言模型中基于克罗内克适应的奇异值分解", "tldr": "SoKA是一种新的PEFT方法，结合克罗内克积分解和SVD初始化，实现参数高效且性能优越的大模型微调。", "motivation": "大型预训练Transformer模型全量微调成本高昂，而现有参数高效微调（PEFT）方法存在推理延迟、收敛不佳或固定秩选择不匹配任务复杂度的不足。", "method": "本文提出SoKA（SVD on Kronecker Adaptation），一种结合克罗内克积张量分解与SVD驱动初始化和频谱感知动态秩选择的PEFT策略。通过克罗内克积SVD（KPSVD）过程将全量权重更新的主成分提取为紧凑的克罗内克因子，并利用能量阈值和拐点标准进行自适应秩选择以修剪可忽略的成分。", "result": "在LLaMA2-7B上，SoKA仅需0.99M可训练参数，比LoRA/PiSSA少25%，同时达到或超越基线性能。此外，SoKA表现出更快的收敛速度和更稳定的梯度。", "conclusion": "SoKA是一种有效且高效的PEFT策略，能够显著减少大型语言模型微调的参数量，同时保持或提升性能，并具有更快的收敛和更稳定的梯度，展现了其在大规模模型适应方面的鲁棒性和效率。", "translation": "大型预训练Transformer模型在各种语言和推理任务中取得了最先进的结果，但完全微调会产生大量的存储、内存和计算开销。参数高效微调（PEFT）方法通过仅学习一小部分特定任务的参数来减轻这些成本，但现有方法要么引入推理时延迟（适配器模块），要么收敛不佳（随机初始化的低秩更新），要么依赖于可能与任务复杂性不匹配的固定秩选择（基于克罗内克的分解）。\n我们提出了SoKA（SVD on Kronecker Adaptation），一种新颖的PEFT策略，它将克罗内克积张量分解与SVD驱动初始化和频谱感知动态秩选择相结合。我们的克罗内克积SVD（KPSVD）过程将全量权重更新的主成分提取为紧凑的克罗内克因子，同时自适应秩选择算法使用能量阈值和拐点标准来修剪可忽略的成分。\n在LLaMA2-7B上对算术推理（GSM8K）、形式数学（MATH）和代码生成（MBPP）的实证评估表明，SoKA仅需要0.99M可训练参数，比LoRA/PiSSA少25%，同时达到或超过基线性能。此外，SoKA表现出更快的收敛速度和更稳定的梯度，突出了其在大规模模型适应方面的鲁棒性和效率。", "summary": "本文提出SoKA，一种新型参数高效微调策略，通过结合克罗内克积张量分解与SVD驱动初始化和动态秩选择，旨在解决现有PEFT方法在推理延迟、收敛性和固定秩选择上的不足。实验证明，SoKA在LLaMA2-7B上以更少的参数实现了与LoRA/PiSSA相当或更优的性能，并展现出更快的收敛和更稳定的梯度，验证了其在大规模语言模型适应中的高效性和鲁棒性。", "keywords": "参数高效微调, 克罗内克积, 奇异值分解, 大型语言模型, 动态秩选择", "comments": "SoKA的创新点在于将克罗内克积分解与SVD驱动初始化及动态秩选择相结合，有效解决了现有PEFT方法的一些痛点。其优势在于显著减少了可训练参数量，同时保持甚至超越了基线性能，并改善了收敛特性。这对于降低大型语言模型微调的资源消耗具有重要意义。"}}
{"id": "2506.15650", "title": "Oldies but Goldies: The Potential of Character N-grams for Romanian Texts", "authors": ["Dana Lupsa", "Sanda-Maria Avram"], "summary": "This study addresses the problem of authorship attribution for Romanian texts\nusing the ROST corpus, a standard benchmark in the field. We systematically\nevaluate six machine learning techniques: Support Vector Machine (SVM),\nLogistic Regression (LR), k-Nearest Neighbors (k-NN), Decision Trees (DT),\nRandom Forests (RF), and Artificial Neural Networks (ANN), employing character\nn-gram features for classification. Among these, the ANN model achieved the\nhighest performance, including perfect classification in four out of fifteen\nruns when using 5-gram features. These results demonstrate that lightweight,\ninterpretable character n-gram approaches can deliver state-of-the-art accuracy\nfor Romanian authorship attribution, rivaling more complex methods. Our\nfindings highlight the potential of simple stylometric features in resource,\nconstrained or under-studied language settings.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15650v1", "AI": {"title_translation": "老歌新唱：字符N-gram在罗马尼亚语文本中的潜力", "tldr": "本研究评估了字符N-gram特征在罗马尼亚语文本作者归属中的潜力，发现ANN模型在使用5-gram时表现最佳，证明了轻量级N-gram方法能达到SOTA性能。", "motivation": "该研究旨在解决罗马尼亚语文本的作者归属问题，并探索字符N-gram特征在这种场景下的有效性。", "method": "研究使用了ROST语料库，并系统评估了六种机器学习技术（SVM, LR, k-NN, DT, RF, ANN），采用字符N-gram特征进行分类。", "result": "ANN模型取得了最高性能，在使用5-gram特征时，在15次运行中有4次达到了完美分类。结果表明轻量级、可解释的字符N-gram方法在罗马尼亚语作者归属中能提供与更复杂方法媲美的SOTA准确率。", "conclusion": "字符N-gram方法在资源受限或研究不足的语言环境中，对于作者归属等任务具有显著潜力。", "translation": "本研究利用ROST语料库（该领域的标准基准）解决了罗马尼亚语文本的作者归属问题。我们系统地评估了六种机器学习技术：支持向量机（SVM）、逻辑回归（LR）、k近邻（k-NN）、决策树（DT）、随机森林（RF）和人工神经网络（ANN），并采用字符N-gram特征进行分类。其中，ANN模型取得了最高性能，在使用5-gram特征时，在十五次运行中有四次达到了完美分类。这些结果表明，轻量级、可解释的字符N-gram方法可以为罗马尼亚语作者归属提供最先进的准确性，与更复杂的方法相媲美。我们的发现强调了简单文体特征在资源受限或研究不足的语言环境中的潜力。", "summary": "本研究探讨了字符N-gram特征在罗马尼亚语文本作者归属任务中的应用。通过在ROST语料库上对SVM、LR、k-NN、DT、RF和ANN六种机器学习模型进行系统评估，发现ANN模型在结合5-gram特征时表现最佳，取得了接近完美的分类性能。这表明简单的字符N-gram方法在处理罗马尼亚语作者归属问题上，能达到与复杂方法相当甚至更优的先进水平，突显了其在资源受限语言环境中的应用潜力。", "keywords": "作者归属, 字符N-gram, 罗马尼亚语, 机器学习, ANN", "comments": "这篇论文通过证明简单的字符N-gram特征在罗马尼亚语作者归属任务中可以达到与复杂方法媲美的先进性能，提供了重要的见解。其创新之处在于强调了“老技术”在特定场景下的“黄金”潜力，尤其是在资源稀缺或研究不足的语言中。这对于实际应用具有重要意义，因为它表明不需要复杂的模型也能获得良好的结果。"}}
{"id": "2506.15477", "title": "Multimodal Large Language Models for Medical Report Generation via Customized Prompt Tuning", "authors": ["Chunlei Li", "Jingyang Hou", "Yilei Shi", "Jingliang Hu", "Xiao Xiang Zhu", "Lichao Mou"], "summary": "Medical report generation from imaging data remains a challenging task in\nclinical practice. While large language models (LLMs) show great promise in\naddressing this challenge, their effective integration with medical imaging\ndata still deserves in-depth exploration. In this paper, we present MRG-LLM, a\nnovel multimodal large language model (MLLM) that combines a frozen LLM with a\nlearnable visual encoder and introduces a dynamic prompt customization\nmechanism. Our key innovation lies in generating instance-specific prompts\ntailored to individual medical images through conditional affine\ntransformations derived from visual features. We propose two implementations:\nprompt-wise and promptbook-wise customization, enabling precise and targeted\nreport generation. Extensive experiments on IU X-ray and MIMIC-CXR datasets\ndemonstrate that MRG-LLM achieves state-of-the-art performance in medical\nreport generation. Our code will be made publicly available.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15477v1", "AI": {"title_translation": "用于医学报告生成的定制化提示调优多模态大语言模型", "tldr": "MRG-LLM是一种新型的多模态大语言模型，通过动态提示定制机制，实现了医学报告生成的SOTA性能。", "motivation": "从影像数据生成医学报告在临床实践中仍然是一项具有挑战性的任务。尽管大语言模型（LLMs）在解决这一挑战方面显示出巨大潜力，但它们与医学影像数据的有效整合仍值得深入探索。", "method": "本文提出了MRG-LLM，一种新颖的多模态大语言模型（MLLM），它结合了一个冻结的大语言模型和一个可学习的视觉编码器，并引入了动态提示定制机制。其关键创新在于通过源自视觉特征的条件仿射变换生成针对单个医学图像的实例特定提示。提出了两种实现方式：逐提示和逐提示手册定制，以实现精确和有针对性的报告生成。", "result": "在IU X-ray和MIMIC-CXR数据集上的大量实验表明，MRG-LLM在医学报告生成方面取得了最先进的性能。", "conclusion": "MRG-LLM通过其新颖的多模态架构和动态提示定制机制，有效解决了医学报告生成的挑战，并在相关数据集上达到了最先进的水平。", "translation": "医学报告生成从影像数据在临床实践中仍然是一项具有挑战性的任务。尽管大语言模型（LLMs）在解决这一挑战方面显示出巨大潜力，但它们与医学影像数据的有效整合仍值得深入探索。在本文中，我们提出了MRG-LLM，一个新颖的多模态大语言模型（MLLM），它结合了一个冻结的大语言模型和一个可学习的视觉编码器，并引入了一个动态提示定制机制。我们的关键创新在于通过源自视觉特征的条件仿射变换生成针对单个医学图像的实例特定提示。我们提出了两种实现方式：逐提示和逐提示手册定制，以实现精确和有针对性的报告生成。在IU X-ray和MIMIC-CXR数据集上的大量实验表明，MRG-LLM在医学报告生成方面取得了最先进的性能。我们的代码将公开发布。", "summary": "本文提出了MRG-LLM，一个用于医学报告生成的新型多模态大语言模型。它将冻结的大语言模型与可学习的视觉编码器相结合，并通过基于视觉特征的条件仿射变换实现动态、实例特定的提示定制。该模型旨在解决医学影像数据与LLM整合的挑战，并在IU X-ray和MIMIC-CXR数据集上取得了最先进的性能。", "keywords": "医学报告生成, 多模态大语言模型, 提示调优, 视觉编码器, 动态提示定制", "comments": "该论文的创新点在于其动态提示定制机制，特别是通过条件仿射变换生成实例特定提示，这使得模型能够更精确地理解和描述医学图像内容。这种方法有效地解决了现有LLM在医学影像数据集成方面的局限性，并为多模态医疗AI应用提供了新的思路。其SOTA的性能也证明了该方法的有效性。"}}
{"id": "2506.15662", "title": "CC-LEARN: Cohort-based Consistency Learning", "authors": ["Xiao Ye", "Shaswat Shrivastava", "Zhaonan Li", "Jacob Dineen", "Shijie Lu", "Avneet Ahuja", "Ming Shen", "Zhikun Xu", "Ben Zhou"], "summary": "Large language models excel at many tasks but still struggle with consistent,\nrobust reasoning. We introduce Cohort-based Consistency Learning (CC-Learn), a\nreinforcement learning framework that improves the reliability of LLM reasoning\nby training on cohorts of similar questions derived from shared programmatic\nabstractions. To enforce cohort-level consistency, we define a composite\nobjective combining cohort accuracy, a retrieval bonus for effective problem\ndecomposition, and a rejection penalty for trivial or invalid lookups that\nreinforcement learning can directly optimize, unlike supervised fine-tuning.\nOptimizing this reward guides the model to adopt uniform reasoning patterns\nacross all cohort members. Experiments on challenging reasoning benchmarks\n(including ARC-Challenge and StrategyQA) show that CC-Learn boosts both\naccuracy and reasoning stability over pretrained and SFT baselines. These\nresults demonstrate that cohort-level RL effectively enhances reasoning\nconsistency in LLMs.", "comment": null, "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15662v1", "AI": {"title_translation": "CC-LEARN：基于群组的一致性学习", "tldr": "CC-Learn是一个新的强化学习框架，通过在相似问题群组上训练，提高了大型语言模型推理的一致性和鲁棒性。", "motivation": "大型语言模型在许多任务中表现出色，但在一致、鲁棒的推理方面仍然存在困难。", "method": "本文引入了基于群组的一致性学习（CC-Learn），这是一个强化学习框架，通过在源自共享程序化抽象的相似问题群组上进行训练，从而提高大型语言模型的推理可靠性。为了强制执行群组级别的一致性，定义了一个复合目标，结合了群组准确性、有效问题分解的检索奖励，以及对琐碎或无效查找的拒绝惩罚，这些都可以通过强化学习直接优化，而监督微调则不能。优化此奖励可引导模型在所有群组成员中采用统一的推理模式。", "result": "在具有挑战性的推理基准（包括ARC-Challenge和StrategyQA）上的实验表明，CC-Learn比预训练和SFT基线提高了准确性和推理稳定性。", "conclusion": "这些结果表明，群组级强化学习有效地增强了大型语言模型中的推理一致性。", "translation": "大型语言模型在许多任务中表现出色，但在一致、鲁棒的推理方面仍然存在困难。我们引入了基于群组的一致性学习（CC-Learn），这是一个强化学习框架，通过在源自共享程序化抽象的相似问题群组上进行训练，从而提高大型语言模型的推理可靠性。为了强制执行群组级别的一致性，我们定义了一个复合目标，结合了群组准确性、有效问题分解的检索奖励，以及对琐碎或无效查找的拒绝惩罚，这些都可以通过强化学习直接优化，而监督微调则不能。优化此奖励可引导模型在所有群组成员中采用统一的推理模式。在具有挑战性的推理基准（包括ARC-Challenge和StrategyQA）上的实验表明，CC-Learn比预训练和SFT基线提高了准确性和推理稳定性。这些结果表明，群组级强化学习有效地增强了大型语言模型中的推理一致性。", "summary": "CC-Learn是一个新的强化学习框架，旨在通过在从共享抽象派生的相似问题群组上进行训练，提高大型语言模型（LLMs）的推理一致性。它通过一个结合了群组准确性、检索奖励和拒绝惩罚的复合目标来优化模型，从而引导模型采用统一的推理模式。实验结果显示，CC-Learn在多个推理基准上显著提升了LLMs的准确性和推理稳定性，证明了群组级强化学习在增强LLMs推理一致性方面的有效性。", "keywords": "大型语言模型, 强化学习, 推理一致性, 群组学习, 鲁棒性", "comments": "该论文创新性地将强化学习应用于提高大型语言模型的推理一致性，特别是引入了“群组一致性”的概念和相应的复合奖励函数，这与传统的监督微调方法不同。其通过优化统一推理模式的思路，对于提升LLM在复杂推理任务中的鲁棒性具有重要意义。"}}
{"id": "2506.15483", "title": "GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects", "authors": ["Shujia Li", "Haiyu Zhang", "Xinyuan Chen", "Yaohui Wang", "Yutong Ban"], "summary": "While diffusion models and large-scale motion datasets have advanced\ntext-driven human motion synthesis, extending these advances to 4D human-object\ninteraction (HOI) remains challenging, mainly due to the limited availability\nof large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel\ntwo-stage framework aimed at achieving two key objectives: 1) generalization to\nunseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the\ninitial stage of our framework, we employ an Object-AnchorNet to reconstruct\nsparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI\ndatasets, thereby mitigating the dependence on large-scale 4D HOI datasets.\nSubsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the\nsecond stage to seamlessly interpolate sparse 3D HOI keyframes into densely\ntemporally coherent 4D HOI sequences. To enhance the quality of generated 4D\nHOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to\nextract human-object contact patterns and a novel Contact-Aware HOI Attention\nto effectively integrate the contact signals into diffusion models.\nExperimental results show that we achieve state-of-the-art results on the\npublicly available OMOMO and 3D-FUTURE datasets, demonstrating strong\ngeneralization abilities to unseen objects, while enabling high-fidelity 4D HOI\ngeneration.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15483v1", "AI": {"title_translation": "GenHOI：泛化文本驱动的未见物体4D人机交互合成", "tldr": "GenHOI是一个两阶段框架，通过Object-AnchorNet和Contact-Aware Diffusion Model，实现了对未见物体的高保真4D人机交互序列的文本驱动合成，解决了大规模4D HOI数据集稀缺的问题。", "motivation": "尽管扩散模型和大规模运动数据集推动了文本驱动的人体运动合成，但将这些进展扩展到4D人机交互（HOI）仍然具有挑战性，主要原因是缺乏大规模的4D HOI数据集。", "method": "本文提出了GenHOI，一个新颖的两阶段框架。第一阶段，采用Object-AnchorNet，仅从3D HOI数据集中学习，重建未见物体的稀疏3D HOI关键帧，从而减少对大规模4D HOI数据集的依赖。第二阶段，引入Contact-Aware Diffusion Model (ContactDM)，将稀疏的3D HOI关键帧无缝插值成密集且时间连贯的4D HOI序列。为了提高质量，ContactDM中提出了Contact-Aware Encoder来提取人机接触模式，并提出了Contact-Aware HOI Attention来有效整合接触信号。", "result": "实验结果表明，该方法在公开的OMOMO和3D-FUTURE数据集上取得了最先进的结果，展示了对未见物体的强大泛化能力，同时实现了高保真4D HOI生成。", "conclusion": "GenHOI成功地解决了4D人机交互合成中对未见物体的泛化和高保真度问题，通过其创新的两阶段框架和接触感知机制，有效克服了大规模数据集的限制。", "translation": "尽管扩散模型和大规模运动数据集推动了文本驱动的人体运动合成，但将这些进展扩展到4D人机交互（HOI）仍然具有挑战性，主要原因是缺乏大规模的4D HOI数据集。在本研究中，我们介绍了GenHOI，一个新颖的两阶段框架，旨在实现两个关键目标：1）泛化到未见物体，以及2）合成高保真4D HOI序列。在我们框架的初始阶段，我们采用Object-AnchorNet来重建未见物体的稀疏3D HOI关键帧，仅从3D HOI数据集中学习，从而减轻了对大规模4D HOI数据集的依赖。随后，我们在第二阶段引入了一个接触感知扩散模型（ContactDM），以将稀疏的3D HOI关键帧无缝插值成密集且时间连贯的4D HOI序列。为了提高生成的4D HOI序列的质量，我们在ContactDM中提出了一个新颖的接触感知编码器来提取人机接触模式，并提出了一个新颖的接触感知HOI注意力机制来有效整合接触信号到扩散模型中。实验结果表明，我们在公开的OMOMO和3D-FUTURE数据集上取得了最先进的结果，展示了对未见物体的强大泛化能力，同时实现了高保真4D HOI生成。", "summary": "本文提出了GenHOI，一个用于文本驱动4D人机交互合成的两阶段框架，旨在解决大规模4D HOI数据集稀缺导致难以泛化到未见物体的问题。第一阶段，Object-AnchorNet从3D HOI数据学习，重建未见物体的稀疏3D HOI关键帧。第二阶段，Contact-Aware Diffusion Model (ContactDM) 将关键帧插值成高保真4D HOI序列，并通过接触感知编码器和注意力机制增强质量。实验证明GenHOI在现有数据集上实现了SOTA性能，并展示了对未见物体的强大泛化能力。", "keywords": "4D人机交互, 扩散模型, 未见物体泛化, 文本驱动合成, 接触感知", "comments": "GenHOI通过其新颖的两阶段方法，巧妙地解决了4D人机交互合成中数据稀疏性和对未见物体泛化两大挑战。特别是通过分离稀疏关键帧重建和密集序列插值，并引入接触感知机制，显著提升了合成的保真度和泛化能力，为该领域提供了重要的进展。"}}
{"id": "2506.15271", "title": "Unlocking Post-hoc Dataset Inference with Synthetic Data", "authors": ["Bihe Zhao", "Pratyush Maini", "Franziska Boenisch", "Adam Dziedzic"], "summary": "The remarkable capabilities of Large Language Models (LLMs) can be mainly\nattributed to their massive training datasets, which are often scraped from the\ninternet without respecting data owners' intellectual property rights. Dataset\nInference (DI) offers a potential remedy by identifying whether a suspect\ndataset was used in training, thereby enabling data owners to verify\nunauthorized use. However, existing DI methods require a private set-known to\nbe absent from training-that closely matches the compromised dataset's\ndistribution. Such in-distribution, held-out data is rarely available in\npractice, severely limiting the applicability of DI. In this work, we address\nthis challenge by synthetically generating the required held-out set. Our\napproach tackles two key obstacles: (1) creating high-quality, diverse\nsynthetic data that accurately reflects the original distribution, which we\nachieve via a data generator trained on a carefully designed suffix-based\ncompletion task, and (2) bridging likelihood gaps between real and synthetic\ndata, which is realized through post-hoc calibration. Extensive experiments on\ndiverse text datasets show that using our generated data as a held-out set\nenables DI to detect the original training sets with high confidence, while\nmaintaining a low false positive rate. This result empowers copyright owners to\nmake legitimate claims on data usage and demonstrates our method's reliability\nfor real-world litigations. Our code is available at\nhttps://github.com/sprintml/PostHocDatasetInference.", "comment": "Accepted at ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15271v1", "AI": {"title_translation": "利用合成数据解锁事后数据集推断", "tldr": "本文提出了一种利用合成数据生成私有留出集的方法，以解决现有数据集推断（DI）方法中缺乏匹配分布的私有集的问题，从而在LLM数据版权侵权检测中实现高置信度的事后DI。", "motivation": "大型语言模型（LLMs）的训练数据集常未经数据所有者授权从互联网抓取，侵犯了知识产权。数据集推断（DI）旨在识别训练中是否使用了某个数据集，以帮助数据所有者验证未经授权的使用。然而，现有DI方法要求一个与受损数据集分布紧密匹配的私有留出集（已知未用于训练），这种数据在实践中很少可用，严重限制了DI的适用性。", "method": "本文通过合成生成所需的留出集来解决现有DI方法的局限性。方法包含两个主要步骤：1) 通过在精心设计的基于后缀的完成任务上训练数据生成器，创建高质量、多样化且准确反映原始分布的合成数据；2) 通过事后校准弥合真实数据和合成数据之间的似然差距。", "result": "在各种文本数据集上的广泛实验表明，使用本文生成的合成数据作为留出集，能够使数据集推断（DI）以高置信度检测出原始训练集，同时保持较低的误报率。", "conclusion": "该研究结果使版权所有者能够就数据使用提出合法主张，并证明了该方法在现实世界诉讼中的可靠性。通过解决现有DI方法的实际限制，该工作显著增强了LLM数据版权保护的能力。", "translation": "大型语言模型（LLMs）的卓越能力主要归因于其庞大的训练数据集，这些数据集通常未经数据所有者的知识产权许可从互联网上抓取。数据集推断（DI）提供了一种潜在的补救措施，通过识别某个可疑数据集是否用于训练，从而使数据所有者能够验证未经授权的使用。然而，现有的DI方法需要一个私有集——已知未参与训练——且该私有集需与受损数据集的分布紧密匹配。这种分布内、未被保留的数据在实践中很少可用，严重限制了DI的适用性。在这项工作中，我们通过合成生成所需的保留集来解决这一挑战。我们的方法解决了两个关键障碍：(1) 创建高质量、多样化且准确反映原始分布的合成数据，我们通过在精心设计的基于后缀的完成任务上训练数据生成器来实现这一点；(2) 弥合真实数据和合成数据之间的似然差距，这通过事后校准实现。在各种文本数据集上进行的大量实验表明，使用我们生成的数据作为保留集，使DI能够以高置信度检测到原始训练集，同时保持较低的误报率。这一结果使版权所有者能够对数据使用提出合法主张，并证明了我们方法在现实世界诉讼中的可靠性。我们的代码可在 https://github.com/sprintml/PostHocDatasetInference 获取。", "summary": "本研究提出了一种创新方法，通过合成生成高质量、多样化的数据作为留出集，解决了现有数据集推断（DI）方法在识别大型语言模型（LLMs）训练数据时面临的实际挑战。该方法利用基于后缀的完成任务训练数据生成器，并通过事后校准弥合真实与合成数据间的差异。实验证明，该方法能以高置信度和低误报率检测原始训练集，从而赋能数据所有者进行版权维权，并为实际诉讼提供可靠工具。", "keywords": "数据集推断, 合成数据, 版权保护, 大型语言模型, 事后推断", "comments": "本文的创新点在于通过合成数据生成解决了数据集推断（DI）在实际应用中的一个核心难题，即缺乏匹配分布的私有留出集。这极大地拓展了DI方法的适用性，使其能够真正用于LLM数据版权侵权的检测。其重要性体现在为数据所有者提供了有效的工具来保护其知识产权，尤其是在LLM训练数据来源日益复杂且常涉嫌侵权的大背景下。该方法结合了数据生成和校准技术，增强了检测的可靠性和准确性，对于未来数据隐私和版权保护领域具有重要意义。"}}
{"id": "2506.15524", "title": "NTIRE 2025 Image Shadow Removal Challenge Report", "authors": ["Florin-Alexandru Vasluianu", "Tim Seizinger", "Zhuyun Zhou", "Cailian Chen", "Zongwei Wu", "Radu Timofte", "Mingjia Li", "Jin Hu", "Hainuo Wang", "Hengxing Liu", "Jiarui Wang", "Qiming Hu", "Xiaojie Guo", "Xin Lu", "Jiarong Yang", "Yuanfei Bao", "Anya Hu", "Zihao Fan", "Kunyu Wang", "Jie Xiao", "Xi Wang", "Xueyang Fu", "Zheng-Jun Zha", "Yu-Fan Lin", "Chia-Ming Lee", "Chih-Chung Hsu", "Xingbo Wang", "Dong Li", "Yuxu Chen", "Bin Chen", "Yuanbo Zhou", "Yuanbin Chen", "Hongwei Wang", "Jiannan Lin", "Qinquan Gao", "Tong Tong", "Zhao Zhang", "Yanyan Wei", "Wei Dong", "Han Zhou", "Seyed Amirreza Mousavi", "Jun Chen", "Haobo Liang", "Jiajie Jing", "Junyu Li", "Yan Yang", "Seoyeon Lee", "Chaewon Kim", "Ziyu Feng", "Shidi Chen", "Bowen Luan", "Zewen Chen", "Vijayalaxmi Ashok Aralikatti", "G Gyaneshwar Rao", "Nikhil Akalwadi", "Chaitra Desai", "Ramesh Ashok Tabib", "Uma Mudenagudi", "Anas M. Ali", "Bilel Benjdira", "Wadii Boulila", "Alexandru Brateanu", "Cosmin Ancuti", "Tanmay Chaturvedi", "Manish Kumar", "Anmol Srivastav", "Daksh Trivedi", "Shashwat Thakur", "Kishor Upla", "Zeyu Xiao", "Zhuoyuan Li", "Boda Zhou", "Shashank Shekhar", "Kele Xu", "Qisheng Xu", "Zijian Gao", "Tianjiao Wan", "Suiyi Zhao", "Bo Wang", "Yan Luo", "Mingshen Wang", "Yilin Zhang"], "summary": "This work examines the findings of the NTIRE 2025 Shadow Removal Challenge. A\ntotal of 306 participants have registered, with 17 teams successfully\nsubmitting their solutions during the final evaluation phase. Following the\nlast two editions, this challenge had two evaluation tracks: one focusing on\nreconstruction fidelity and the other on visual perception through a user\nstudy. Both tracks were evaluated with images from the WSRD+ dataset,\nsimulating interactions between self- and cast-shadows with a large number of\ndiverse objects, textures, and materials.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15524v1", "AI": {"title_translation": "NTIRE 2025 图像去影挑战报告", "tldr": "NTIRE 2025图像去影挑战报告，共有306名参与者，17支队伍提交了解决方案。挑战设有重建保真度和视觉感知两个评估赛道，均使用WSRD+数据集进行评估。", "motivation": "本文旨在总结并报告NTIRE 2025图像去影挑战赛的结果和发现。", "method": "NTIRE 2025图像去影挑战赛设有重建保真度赛道和视觉感知（通过用户研究）赛道。这两个赛道均使用WSRD+数据集进行评估，该数据集模拟了自影和投射影与大量不同物体、纹理和材料之间的相互作用。", "result": "共有306名参与者注册，其中17支队伍在最终评估阶段成功提交了解决方案。", "conclusion": "Not mentioned in abstract", "translation": "本文审视了NTIRE 2025去影挑战赛的结果。共有306名参与者注册，其中17支队伍在最终评估阶段成功提交了解决方案。继前两届之后，本次挑战赛设有两个评估赛道：一个侧重于重建保真度，另一个通过用户研究侧重于视觉感知。两个赛道均使用WSRD+数据集进行评估，该数据集模拟了自影和投射影与大量不同物体、纹理和材料之间的相互作用。", "summary": "本文是对NTIRE 2025图像去影挑战赛的报告，该挑战吸引了306名注册者和17支最终提交解决方案的队伍。挑战包含重建保真度和视觉感知两个评估赛道，均基于WSRD+数据集进行，该数据集模拟了复杂的光影交互。", "keywords": "图像去影, NTIRE 2025, 挑战赛, WSRD+数据集, 视觉感知", "comments": "这篇论文报告了一个重要的图像处理挑战赛，即NTIRE 2025图像去影挑战。其重要性在于推动了图像去影领域的研究进展，并通过设立重建保真度和视觉感知两个赛道，全面评估了去影算法的性能。使用WSRD+数据集进一步确保了评估的全面性和真实性。"}}
{"id": "2506.15289", "title": "DOVA-PATBM: An Intelligent, Adaptive, and Scalable Framework for Optimizing Large-Scale EV Charging Infrastructure", "authors": ["Chuan Li", "Shunyu Zhao", "Vincent Gauthier", "Hassine Moungla"], "summary": "The accelerating uptake of battery-electric vehicles demands infrastructure\nplanning tools that are both data-rich and geographically scalable. Whereas\nmost prior studies optimise charging locations for single cities, state-wide\nand national networks must reconcile the conflicting requirements of dense\nmetropolitan cores, car-dependent exurbs, and power-constrained rural\ncorridors.\n  We present DOVA-PATBM (Deployment Optimisation with Voronoi-oriented,\nAdaptive, POI-Aware Temporal Behaviour Model), a geo-computational framework\nthat unifies these contexts in a single pipeline. The method rasterises\nheterogeneous data (roads, population, night lights, POIs, and feeder lines)\nonto a hierarchical H3 grid, infers intersection importance with a\nzone-normalised graph neural network centrality model, and overlays a Voronoi\ntessellation that guarantees at least one five-port DC fast charger within\nevery 30 km radius. Hourly arrival profiles, learned from loop-detector and\nfloating-car traces, feed a finite M/M/c queue to size ports under\nfeeder-capacity and outage-risk constraints. A greedy maximal-coverage\nheuristic with income-weighted penalties then selects the minimum number of\nsites that satisfy coverage and equity targets.\n  Applied to the State of Georgia, USA, DOVA-PATBM (i) increases 30 km tile\ncoverage by 12 percentage points, (ii) halves the mean distance that low-income\nresidents travel to the nearest charger, and (iii) meets sub-transmission\nheadroom everywhere -- all while remaining computationally tractable for\nnational-scale roll-outs. These results demonstrate that a tightly integrated,\nGNN-driven, multi-resolution approach can bridge the gap between academic\noptimisation and deployable infrastructure policy.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15289v1", "AI": {"title_translation": "DOVA-PATBM：一种用于优化大规模电动汽车充电基础设施的智能、自适应和可扩展框架", "tldr": "DOVA-PATBM是一个地理计算框架，用于优化大规模电动汽车充电基础设施的部署，通过整合异构数据和先进模型，显著提高了覆盖率、公平性并满足了电力容量限制。", "motivation": "随着电动汽车的加速普及，需要能够处理大规模地理范围（包括城市、郊区和农村）且数据丰富的充电基础设施规划工具，以协调不同区域的冲突需求。", "method": "DOVA-PATBM（Deployment Optimisation with Voronoi-oriented, Adaptive, POI-Aware Temporal Behaviour Model）是一个地理计算框架。它将异构数据（道路、人口、夜间灯光、兴趣点和馈线）栅格化到分层H3网格上，使用区域归一化的图神经网络中心性模型推断交叉点重要性，并叠加Voronoi细分以确保每30公里半径内至少有一个五端口直流快速充电器。根据环路检测器和浮动汽车轨迹学习的小时到达配置文件，输入有限的M/M/c排队模型，在馈线容量和停电风险约束下确定端口大小。最后，使用带有收入加权惩罚的贪婪最大覆盖启发式算法选择满足覆盖和公平目标所需的最小站点数量。", "result": "应用于美国佐治亚州时，DOVA-PATBM (i) 将30公里瓦片覆盖率提高了12个百分点，(ii) 将低收入居民到最近充电器的平均距离减半，以及 (iii) 在任何地方都满足了次输电净空，同时在计算上对于国家规模的推广仍然可行。", "conclusion": "结果表明，紧密集成、GNN驱动、多分辨率的方法可以弥合学术优化与可部署基础设施政策之间的差距。", "translation": "电动汽车的加速普及需要既数据丰富又地理可扩展的基础设施规划工具。虽然大多数先前的研究优化了单个城市的充电位置，但州级和国家级网络必须协调密集大都市核心区、依赖汽车的郊区和电力受限的农村走廊的冲突需求。\n我们提出了DOVA-PATBM（基于Voronoi导向、自适应、兴趣点感知时间行为模型的部署优化），这是一个将这些情境统一到一个单一流程中的地理计算框架。该方法将异构数据（道路、人口、夜间灯光、兴趣点和馈线）栅格化到分层H3网格上，使用区域归一化的图神经网络中心性模型推断交叉点的重要性，并叠加Voronoi细分，确保每30公里半径内至少有一个五端口直流快速充电器。从环路检测器和浮动汽车轨迹学习到的小时到达剖面，输入一个有限的M/M/c排队模型，以在馈线容量和停电风险约束下确定端口大小。然后，带有收入加权惩罚的贪婪最大覆盖启发式算法选择满足覆盖和公平目标的最小站点数量。\n应用于美国佐治亚州时，DOVA-PATBM (i) 将30公里瓦片覆盖率提高了12个百分点，(ii) 将低收入居民到最近充电器的平均距离减半，以及 (iii) 在任何地方都满足了次输电净空——所有这些都保持了国家级推广的计算可行性。这些结果表明，紧密集成、GNN驱动、多分辨率的方法可以弥合学术优化与可部署基础设施政策之间的差距。", "summary": "DOVA-PATBM是一个智能、自适应且可扩展的地理计算框架，旨在优化大规模电动汽车充电基础设施的部署。该框架通过将道路、人口、POI等多种异构数据整合到分层H3网格中，并结合图神经网络、Voronoi细分、M/M/c排队模型和贪婪最大覆盖启发式算法，实现了对充电站点位置、端口数量和公平性的综合优化。在佐治亚州的应用表明，DOVA-PATBM显著提升了充电覆盖率，减少了低收入居民的充电距离，并确保了电力容量，同时保持了国家级部署的计算可行性，证明了其在弥合学术研究与实际基础设施政策之间的有效性。", "keywords": "电动汽车充电, 基础设施优化, 地理计算, 图神经网络, Voronoi", "comments": "本文提出DOVA-PATBM框架，其创新性在于将多源异构地理数据与先进的地理计算方法（H3网格、GNN、Voronoi、排队论、贪婪启发式）紧密集成，以解决大规模EV充电基础设施规划的复杂性。该方法不仅考虑了覆盖率和电力容量，还特别关注了社会公平性（通过收入加权惩罚）。其在国家级部署上的计算可行性，使其具有重要的实际应用价值，能够有效指导未来电动汽车充电网络的建设。"}}
{"id": "2506.15676", "title": "Gender-Neutral Machine Translation Strategies in Practice", "authors": ["Hillary Dawkins", "Isar Nejadgholi", "Chi-kiu Lo"], "summary": "Gender-inclusive machine translation (MT) should preserve gender ambiguity in\nthe source to avoid misgendering and representational harms. While gender\nambiguity often occurs naturally in notional gender languages such as English,\nmaintaining that gender neutrality in grammatical gender languages is a\nchallenge. Here we assess the sensitivity of 21 MT systems to the need for\ngender neutrality in response to gender ambiguity in three translation\ndirections of varying difficulty. The specific gender-neutral strategies that\nare observed in practice are categorized and discussed. Additionally, we\nexamine the effect of binary gender stereotypes on the use of gender-neutral\ntranslation. In general, we report a disappointing absence of gender-neutral\ntranslations in response to gender ambiguity. However, we observe a small\nhandful of MT systems that switch to gender neutral translation using specific\nstrategies, depending on the target language.", "comment": "to appear at GITT 2025", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15676v1", "AI": {"title_translation": "实践中的中性机器翻译策略", "tldr": "机器翻译系统在处理性别模糊性时常未能保持性别中立，可能导致误判性别。本文评估了21个机器翻译系统在不同语言对中的表现，并识别了现有的策略，发现普遍缺乏性别中立的输出，但少数系统采用了特定策略。", "motivation": "性别包容的机器翻译应保留源语言中的性别模糊性，以避免误判性别和代表性损害。在语法性别语言中保持性别中立性尤其具有挑战性。本文的动机是评估当前机器翻译系统如何处理这一需求。", "method": "研究评估了21个机器翻译系统在三种不同难度翻译方向上对性别模糊性中性别中立需求的敏感性。文中对实践中观察到的特定性别中立策略进行了分类和讨论，并考察了二元性别刻板印象对性别中立翻译使用的影响。", "result": "总体而言，在应对性别模糊性时，性别中立翻译的缺失令人失望。然而，观察到少数机器翻译系统根据目标语言，使用特定策略转向性别中立翻译。", "conclusion": "大多数机器翻译系统在源语言存在性别模糊性时未能生成性别中立的翻译，但少数系统通过采用特定策略显示出潜力。", "translation": "性别包容的机器翻译（MT）应保留源语言中的性别模糊性，以避免误判性别和代表性损害。虽然性别模糊性在英语等概念性别语言中自然存在，但在语法性别语言中保持性别中立性是一个挑战。本文评估了21个机器翻译系统在三种不同难度翻译方向上对性别模糊性中性别中立需求的敏感性。对实践中观察到的特定性别中立策略进行了分类和讨论。此外，我们还考察了二元性别刻板印象对性别中立翻译使用的影响。总的来说，我们报告称，在应对性别模糊性时，性别中立翻译的缺失令人失望。然而，我们观察到少数机器翻译系统根据目标语言，使用特定策略转向性别中立翻译。", "summary": "本文探讨了21个机器翻译（MT）系统在从性别模糊的源文本进行翻译时（特别是在概念性别语言和语法性别语言之间）如何处理性别中立性。文章对观察到的性别中立策略进行了分类，并考察了二元性别刻板印象的影响。研究发现，大多数系统普遍缺乏性别中立的输出，但指出少数系统成功采用了特定的性别中立翻译策略。", "keywords": "中性机器翻译, 性别模糊性, 机器翻译系统, 翻译策略, 性别刻板印象", "comments": "这篇论文揭示了机器翻译中一个重要的伦理和实际问题：未能保持性别中立性可能导致误判性别。它对21个系统的系统性评估以及对策略的分类为未来旨在提高包容性的机器翻译发展提供了宝贵的见解。研究发现只有少数系统显示出潜力，这表明该领域有很大的改进空间。"}}
{"id": "2506.15549", "title": "CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse Myocardial Scar Synthesis and Segmentation", "authors": ["Farheen Ramzan", "Yusuf Kiberu", "Nikesh Jathanna", "Shahnaz Jamil-Copley", "Richard H. Clayton", "Chen", "Chen"], "summary": "Deep learning-based myocardial scar segmentation from late gadolinium\nenhancement (LGE) cardiac MRI has shown great potential for accurate and timely\ndiagnosis and treatment planning for structural cardiac diseases. However, the\nlimited availability and variability of LGE images with high-quality scar\nlabels restrict the development of robust segmentation models. To address this,\nwe introduce CLAIM: \\textbf{C}linically-Guided \\textbf{L}GE\n\\textbf{A}ugmentation for Real\\textbf{i}stic and Diverse \\textbf{M}yocardial\nScar Synthesis and Segmentation framework, a framework for anatomically\ngrounded scar generation and segmentation. At its core is the SMILE module\n(Scar Mask generation guided by cLinical knowledgE), which conditions a\ndiffusion-based generator on the clinically adopted AHA 17-segment model to\nsynthesize images with anatomically consistent and spatially diverse scar\npatterns. In addition, CLAIM employs a joint training strategy in which the\nscar segmentation network is optimized alongside the generator, aiming to\nenhance both the realism of synthesized scars and the accuracy of the scar\nsegmentation performance. Experimental results show that CLAIM produces\nanatomically coherent scar patterns and achieves higher Dice similarity with\nreal scar distributions compared to baseline models. Our approach enables\ncontrollable and realistic myocardial scar synthesis and has demonstrated\nutility for downstream medical imaging task.", "comment": "14 Pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15549v1", "AI": {"title_translation": "CLAIM：临床指导的LGE增强，用于真实多样化的心肌瘢痕合成与分割", "tldr": "本研究提出了一个名为CLAIM的框架，通过临床知识指导的LGE图像增强来合成真实多样化的心肌瘢痕，以解决数据稀缺问题并提高分割模型性能。", "motivation": "高质量心肌瘢痕LGE图像数据的有限可用性和变异性限制了鲁棒分割模型的发展。", "method": "提出了CLAIM框架，核心是SMILE模块，一个基于扩散的生成器，通过临床采纳的AHA 17段模型指导生成解剖学上一致且空间多样化的瘢痕图像。此外，CLAIM采用联合训练策略，同时优化瘢痕分割网络和生成器。", "result": "CLAIM生成了符合解剖学的心肌瘢痕模式，并且与基线模型相比，其Dice相似性与真实瘢痕分布更高。", "conclusion": "该方法实现了可控且真实的心肌瘢痕合成，并已证明对下游医学影像任务有用。", "translation": "深度学习驱动的心肌瘢痕分割技术，基于晚期钆增强（LGE）心脏磁共振成像，在结构性心脏疾病的准确及时诊断和治疗规划方面展现出巨大潜力。然而，高质量瘢痕标签LGE图像的有限可用性和变异性限制了鲁棒分割模型的发展。为解决此问题，我们引入了CLAIM：一个临床指导的LGE增强框架，用于真实多样化的心肌瘢痕合成与分割，这是一个用于解剖学基础瘢痕生成和分割的框架。其核心是SMILE模块（临床知识指导的瘢痕掩膜生成），它通过临床采纳的AHA 17段模型来调节基于扩散的生成器，以合成具有解剖学一致性和空间多样化瘢痕模式的图像。此外，CLAIM采用联合训练策略，其中瘢痕分割网络与生成器一同优化，旨在增强合成瘢痕的真实性以及瘢痕分割性能的准确性。实验结果表明，与基线模型相比，CLAIM生成了符合解剖学的心肌瘢痕模式，并实现了与真实瘢痕分布更高的Dice相似性。我们的方法实现了可控且真实的心肌瘢痕合成，并已证明对下游医学影像任务有用。", "summary": "本研究提出了CLAIM框架，通过临床知识指导的LGE图像增强，解决心肌瘢痕分割中高质量数据稀缺问题。CLAIM包含SMILE模块，利用AHA 17段模型指导扩散生成器合成真实且多样化的瘢痕图像，并通过联合训练策略同时优化生成器和分割网络。实验证明CLAIM能生成解剖学一致的瘢痕，并提高分割准确性，对下游医学影像任务具有实用价值。", "keywords": "心肌瘢痕分割, LGE MRI, 数据增强, 扩散模型, 临床指导", "comments": "该论文的创新点在于提出了一个临床指导的LGE图像增强框架CLAIM，通过引入SMILE模块和联合训练策略，有效地解决了高质量心肌瘢痕LGE图像数据稀缺的问题。其方法结合了临床知识（AHA 17段模型）与先进的扩散模型，能够生成解剖学上真实且多样化的瘢痕模式，显著提升了心肌瘢痕分割模型的鲁棒性和准确性，为医学影像领域的深度学习应用提供了有价值的数据增强解决方案。"}}
{"id": "2506.15305", "title": "Conditional Generative Modeling for Enhanced Credit Risk Management in Supply Chain Finance", "authors": ["Qingkai Zhang", "L. Jeff Hong", "Houmin Yan"], "summary": "The rapid expansion of cross-border e-commerce (CBEC) has created significant\nopportunities for small and medium-sized enterprises (SMEs), yet financing\nremains a critical challenge due to SMEs' limited credit histories. Third-party\nlogistics (3PL)-led supply chain finance (SCF) has emerged as a promising\nsolution, leveraging in-transit inventory as collateral. We propose an advanced\ncredit risk management framework tailored for 3PL-led SCF, addressing the dual\nchallenges of credit risk assessment and loan size determination. Specifically,\nwe leverage conditional generative modeling of sales distributions through\nQuantile-Regression-based Generative Metamodeling (QRGMM) as the foundation for\nrisk estimation. We propose a unified framework that enables flexible\nestimation of multiple risk measures while introducing a functional risk\nmeasure formulation that systematically captures the relationship between these\nrisk measures and varying loan levels, supported by theoretical guarantees. To\ncapture complex covariate interactions in e-commerce sales data, we integrate\nQRGMM with Deep Factorization Machines (DeepFM). Extensive experiments on\nsynthetic and real-world data validate the efficacy of our model for credit\nrisk assessment and loan size determination. This study represents a pioneering\napplication of generative AI in CBEC SCF risk management, offering a solid\nfoundation for enhanced credit practices and improved SME access to capital.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15305v1", "AI": {"title_translation": "基于条件生成建模的供应链金融信用风险管理增强", "tldr": "本研究提出了一种利用条件生成建模（QRGMM与DeepFM结合）的先进框架，用于跨境电商供应链金融中的信用风险评估和贷款规模确定，旨在解决中小企业融资难题。", "motivation": "跨境电商的快速发展为中小企业带来了机遇，但由于信用历史有限，融资仍是其面临的关键挑战。第三方物流主导的供应链金融（SCF）利用在途库存作为抵押，是一种有前景的解决方案，但需要更先进的信用风险管理框架。", "method": "提出了一种针对第三方物流主导的供应链金融的先进信用风险管理框架，旨在解决信用风险评估和贷款规模确定的双重挑战。具体来说，该框架利用基于分位数回归的生成元建模（QRGMM）进行销售分布的条件生成建模，作为风险估计的基础。提出了一个统一的框架，能够灵活估计多种风险度量，并引入了一种函数风险度量公式，系统地捕捉这些风险度量与不同贷款水平之间的关系，并提供理论保证。为捕捉电商销售数据中复杂的协变量交互，将QRGMM与深度因子分解机（DeepFM）相结合。", "result": "在合成数据和真实世界数据上进行了大量实验，验证了所提出模型在信用风险评估和贷款规模确定方面的有效性。", "conclusion": "本研究代表了生成式AI在跨境电商供应链金融风险管理中的开创性应用，为增强信用实践和改善中小企业融资渠道提供了坚实基础。", "translation": "跨境电商（CBEC）的迅速扩张为中小企业（SMEs）创造了巨大的机遇，然而，由于中小企业信用历史有限，融资仍然是一个严峻的挑战。第三方物流（3PL）主导的供应链金融（SCF）已成为一种有前景的解决方案，它利用在途库存作为抵押。我们提出了一种针对第三方物流主导的SCF的先进信用风险管理框架，解决了信用风险评估和贷款规模确定的双重挑战。具体而言，我们利用基于分位数回归的生成元建模（QRGMM）对销售分布进行条件生成建模，作为风险估计的基础。我们提出了一个统一的框架，该框架能够灵活地估计多种风险度量，同时引入了一种函数风险度量公式，系统地捕捉这些风险度量与不同贷款水平之间的关系，并得到了理论保证的支持。为了捕捉电商销售数据中复杂的协变量交互，我们将QRGMM与深度因子分解机（DeepFM）相结合。在合成数据和真实世界数据上进行的广泛实验验证了我们模型在信用风险评估和贷款规模确定方面的有效性。本研究代表了生成式AI在CBEC SCF风险管理中的开创性应用，为增强信用实践和改善中小企业融资渠道提供了坚实的基础。", "summary": "本研究针对跨境电商（CBEC）中小企业（SMEs）融资难题，提出了一种基于条件生成建模的先进供应链金融（SCF）信用风险管理框架。该框架利用基于分位数回归的生成元建模（QRGMM）对销售分布进行条件生成建模以估计风险，并与深度因子分解机（DeepFM）结合以处理复杂数据交互。它能有效进行信用风险评估和贷款规模确定，并引入了捕获风险度量与贷款水平关系的函数公式。实验验证了其有效性，是生成式AI在CBEC SCF风险管理领域的开创性应用，有助于改善中小企业融资。", "keywords": "信用风险管理, 供应链金融, 生成建模, QRGMM, DeepFM", "comments": "该研究的创新之处在于将生成式AI（尤其是QRGMM与DeepFM的结合）应用于跨境电商供应链金融的信用风险管理，为中小企业融资提供了一种新颖且有效的解决方案。其提出的统一框架和函数风险度量公式具有理论支撑和实际应用价值，有望显著提升信用风险评估的准确性和贷款决策的灵活性。这是生成式AI在金融风险管理领域的一个重要拓展和先驱性应用。"}}
{"id": "2506.15681", "title": "GenRecal: Generation after Recalibration from Large to Small Vision-Language Models", "authors": ["Byung-Kwan Lee", "Ryo Hachiuma", "Yong Man Ro", "Yu-Chiang Frank Wang", "Yueh-Hua Wu"], "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.", "comment": "Project page: https://byungkwanlee.github.io/GenRecal-page/", "cate": "cs.CL", "url": "http://arxiv.org/abs/2506.15681v1", "AI": {"title_translation": "GenRecal：从大型到小型视觉-语言模型的校准后生成", "tldr": "提出GenRecal框架，通过校准器解决异构VLM知识蒸馏难题，使小型VLM性能超越大型模型。", "motivation": "大型视觉-语言模型（VLMs）计算成本高昂，难以部署到资源受限设备。现有知识蒸馏方法难以处理VLM架构多样性（基于不同LLM，不同token类型）带来的异构性挑战。", "method": "提出GenRecal（校准后生成）框架，一个通用的VLM蒸馏框架。GenRecal包含一个Recalibrator（校准器），用于对齐和调整异构VLM之间的特征表示，从而实现跨不同类型VLM的有效知识迁移。", "result": "GenRecal显著提升了基线性能，并在多个挑战性基准测试中最终超越了大型开源和闭源VLM。", "conclusion": "GenRecal提供了一种有效且通用的方法，能够将大型VLM的知识蒸馏到小型模型中，克服了VLM架构多样性带来的挑战，并实现了卓越的性能。", "translation": "近期视觉-语言模型（VLMs）的进展利用大型语言模型（LLMs）实现了与GPT-4V等闭源系统相当的性能。然而，由于其巨大的计算需求，在实际场景中部署这些模型，特别是在资源受限设备上，仍然具有挑战性。这激发了人们将知识从大型VLM蒸馏到更小、更高效模型中的兴趣。一个关键挑战源于VLM架构的多样性，这些架构建立在不同的LLMs之上，并采用不同的token类型——在词汇量、token分割和token索引顺序上有所不同。为了解决这种对特定VLM类型限制的挑战，我们提出了“校准后生成”（GenRecal），一种新颖的、通用的VLM蒸馏框架。GenRecal包含一个校准器（Recalibrator），用于对齐和调整异构VLM之间的特征表示，从而实现跨不同类型VLM的有效知识迁移。通过在多个挑战性基准测试中进行大量实验，我们证明GenRecal显著提高了基线性能，最终超越了大型开源和闭源VLM。", "summary": "本文提出了GenRecal，一个通用的视觉-语言模型（VLM）知识蒸馏框架，旨在解决大型VLM在资源受限设备上的部署难题以及异构VLM架构间的知识迁移挑战。GenRecal引入了一个校准器，能够对齐并调整不同VLM间的特征表示，从而实现有效的知识转移。实验结果表明，GenRecal显著提升了基线性能，并超越了大型开源和闭源VLM。", "keywords": "视觉-语言模型, 知识蒸馏, 模型压缩, 异构VLM, 校准", "comments": "这项工作创新性地提出了一个通用的蒸馏框架GenRecal，通过引入Recalibrator解决了VLM架构异构性带来的知识迁移难题，这对于推动VLM在边缘设备上的实际应用具有重要意义。其超越大型模型的性能表现也凸显了该方法的有效性。"}}
{"id": "2506.14861", "title": "BMFM-RNA: An Open Framework for Building and Evaluating Transcriptomic Foundation Models", "authors": ["Bharath Dandala", "Michael M. Danziger", "Ella Barkan", "Tanwi Biswas", "Viatcheslav Gurev", "Jianying Hu", "Matthew Madgwick", "Akira Koseki", "Tal Kozlovski", "Michal Rosen-Zvi", "Yishai Shimoni", "Ching-Huei Tsou"], "summary": "Transcriptomic foundation models (TFMs) have recently emerged as powerful\ntools for analyzing gene expression in cells and tissues, supporting key tasks\nsuch as cell-type annotation, batch correction, and perturbation prediction.\nHowever, the diversity of model implementations and training strategies across\nrecent TFMs, though promising, makes it challenging to isolate the contribution\nof individual design choices or evaluate their potential synergies. This\nhinders the field's ability to converge on best practices and limits the\nreproducibility of insights across studies. We present BMFM-RNA, an\nopen-source, modular software package that unifies diverse TFM pretraining and\nfine-tuning objectives within a single framework. Leveraging this capability,\nwe introduce a novel training objective, whole cell expression decoder (WCED),\nwhich captures global expression patterns using an autoencoder-like CLS\nbottleneck representation. In this paper, we describe the framework, supported\ninput representations, and training objectives. We evaluated four model\ncheckpoints pretrained on CELLxGENE using combinations of masked language\nmodeling (MLM), WCED and multitask learning. Using the benchmarking\ncapabilities of BMFM-RNA, we show that WCED-based models achieve performance\nthat matches or exceeds state-of-the-art approaches like scGPT across more than\na dozen datasets in both zero-shot and fine-tuning tasks. BMFM-RNA, available\nas part of the biomed-multi-omics project (\nhttps://github.com/BiomedSciAI/biomed-multi-omic ), offers a reproducible\nfoundation for systematic benchmarking and community-driven exploration of\noptimal TFM training strategies, enabling the development of more effective\ntools to leverage the latest advances in AI for understanding cell biology.", "comment": null, "cate": "q-bio.GN", "url": "http://arxiv.org/abs/2506.14861v1", "AI": {"title_translation": "BMFM-RNA：一个用于构建和评估转录组基础模型的开放框架", "tldr": "BMFM-RNA是一个开源框架，统一了转录组基础模型的训练目标，并引入了新的WCED训练目标，其性能与现有最佳方法相当或更优，为系统性基准测试和探索最佳训练策略提供了可复现的基础。", "motivation": "当前转录组基础模型（TFMs）在实现和训练策略上的多样性，使得难以分离单个设计选择的贡献或评估它们的潜在协同作用，这阻碍了领域内最佳实践的形成，并限制了研究结果的可复现性。", "method": "本研究提出了BMFM-RNA，一个开源、模块化的软件包，它在一个框架内统一了多样化的TFM预训练和微调目标。在此基础上，引入了一种新的训练目标——全细胞表达解码器（WCED），它利用类似自编码器的CLS瓶颈表示来捕获全局表达模式。作者评估了在CELLxGENE上预训练的四个模型检查点，结合使用了掩码语言建模（MLM）、WCED和多任务学习。", "result": "BMFM-RNA的基准测试能力显示，基于WCED的模型在十多个数据集上的零样本和微调任务中，其性能与scGPT等最先进的方法相当或超越。", "conclusion": "BMFM-RNA提供了一个可复现的基础，用于系统性基准测试和社区驱动的优化TFM训练策略的探索，从而能够开发出更有效的工具，以利用AI的最新进展来理解细胞生物学。", "translation": "转录组基础模型（TFMs）最近已成为分析细胞和组织中基因表达的强大工具，支持细胞类型注释、批次校正和扰动预测等关键任务。然而，近期TFMs在模型实现和训练策略上的多样性，尽管前景广阔，但使得难以分离单个设计选择的贡献或评估它们的潜在协同作用。这阻碍了该领域形成最佳实践的能力，并限制了研究结果在不同研究中的可复现性。我们提出了BMFM-RNA，一个开源、模块化的软件包，它在一个单一框架内统一了多样化的TFM预训练和微调目标。利用这一能力，我们引入了一种新颖的训练目标，即全细胞表达解码器（WCED），它使用类似自编码器的CLS瓶颈表示捕获全局表达模式。在本文中，我们描述了该框架、支持的输入表示和训练目标。我们评估了在CELLxGENE上预训练的四个模型检查点，这些检查点结合使用了掩码语言建模（MLM）、WCED和多任务学习。利用BMFM-RNA的基准测试能力，我们展示了基于WCED的模型在十多个数据集上的零样本和微调任务中，其性能与scGPT等最先进的方法相当或超越。BMFM-RNA作为biomed-multi-omics项目的一部分提供（https://github.com/BiomedSciAI/biomed-multi-omic），为系统性基准测试和社区驱动的优化TFM训练策略探索提供了可复现的基础，从而能够开发出更有效的工具，以利用AI的最新进展来理解细胞生物学。", "summary": "BMFM-RNA是一个开源、模块化的框架，旨在解决当前转录组基础模型（TFMs）训练策略多样性带来的挑战，这些挑战阻碍了最佳实践的形成和结果的可复现性。该框架统一了TFMs的预训练和微调目标，并引入了新的全细胞表达解码器（WCED）训练目标。实验结果表明，基于WCED的模型在多项任务中达到或超越了现有最先进的性能。BMFM-RNA为TFMs的系统性基准测试和社区驱动的策略探索提供了可复现的基础，有助于推动细胞生物学领域的AI应用。", "keywords": "转录组基础模型, BMFM-RNA, WCED, 基因表达, 开源框架", "comments": "BMFM-RNA的创新之处在于其作为一个开放框架，统一了多样化的转录组基础模型训练目标，并引入了新颖的WCED训练目标。这对于解决现有TFM领域中模型实现和训练策略多样性导致的可复现性差和最佳实践难以确立的问题至关重要。其开源性质和提供基准测试能力，有望促进社区协作和加速TFM的开发。该工作的重要性在于为转录组学研究提供了一个标准化的、可重复的平台，使得研究人员能够更有效地评估和比较不同的TFM设计选择，从而推动AI在细胞生物学理解方面的应用。"}}
{"id": "2506.15307", "title": "SecFwT: Efficient Privacy-Preserving Fine-Tuning of Large Language Models Using Forward-Only Passes", "authors": ["Jinglong Luo", "Zhuo Zhang", "Yehong Zhang", "Shiyu Liu", "Ye Dong", "Xun Zhou", "Hui Wang", "Yue Yu", "Zenglin Xu"], "summary": "Large language models (LLMs) have transformed numerous fields, yet their\nadaptation to specialized tasks in privacy-sensitive domains, such as\nhealthcare and finance, is constrained by the scarcity of accessible training\ndata due to stringent privacy requirements. Secure multi-party computation\n(MPC)-based privacy-preserving machine learning offers a powerful approach to\nprotect both model parameters and user data, but its application to LLMs has\nbeen largely limited to inference, as fine-tuning introduces significant\ncomputational challenges, particularly in privacy-preserving backward\npropagation and optimizer operations. This paper identifies two primary\nobstacles to MPC-based privacy-preserving fine-tuning of LLMs: (1) the\nsubstantial computational overhead of backward and optimizer processes, and (2)\nthe inefficiency of softmax-based attention mechanisms in MPC settings. To\naddress these challenges, we propose SecFwT, the first MPC-based framework\ndesigned for efficient, privacy-preserving LLM fine-tuning. SecFwT introduces a\nforward-only tuning paradigm to eliminate backward and optimizer computations\nand employs MPC-friendly Random Feature Attention to approximate softmax\nattention, significantly reducing costly non-linear operations and\ncomputational complexity. Experimental results demonstrate that SecFwT delivers\nsubstantial improvements in efficiency and privacy preservation, enabling\nscalable and secure fine-tuning of LLMs for privacy-critical applications.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15307v1", "AI": {"title_translation": "SecFwT：使用仅前向传播实现大型语言模型高效隐私保护微调", "tldr": "SecFwT是一种基于安全多方计算（MPC）的框架，通过采用仅前向传播和随机特征注意力，解决了大型语言模型（LLM）隐私保护微调中的计算难题，实现了高效且可扩展的微调。", "motivation": "大型语言模型（LLMs）在许多领域取得了变革性进展，但在医疗和金融等隐私敏感领域，由于严格的隐私要求导致可访问训练数据稀缺，其适应性受到限制。基于安全多方计算（MPC）的隐私保护机器学习能够保护模型参数和用户数据，但其在LLMs上的应用主要限于推理，因为微调会引入显著的计算挑战，特别是在隐私保护的反向传播和优化器操作中。本文指出了基于MPC的LLM隐私保护微调的两个主要障碍：(1) 反向和优化器过程的巨大计算开销，以及 (2) MPC设置中基于softmax的注意力机制效率低下。", "method": "本文提出了SecFwT，这是第一个基于MPC的框架，旨在实现高效、隐私保护的LLM微调。SecFwT引入了仅前向调优范式以消除反向和优化器计算，并采用MPC友好的随机特征注意力来近似softmax注意力，显著减少了昂贵的非线性操作和计算复杂性。", "result": "实验结果表明，SecFwT在效率和隐私保护方面取得了显著的改进，从而为隐私关键型应用实现了LLMs的可扩展和安全微调。", "conclusion": "SecFwT是首个基于MPC的框架，实现了LLM的高效隐私保护微调，使其能够应用于隐私敏感型场景。", "translation": "大型语言模型（LLMs）已经改变了众多领域，但它们在医疗保健和金融等隐私敏感领域适应专业任务的能力受到严格隐私要求导致可访问训练数据稀缺的限制。基于安全多方计算（MPC）的隐私保护机器学习提供了一种强大的方法来保护模型参数和用户数据，但其在LLMs上的应用主要限于推理，因为微调引入了显著的计算挑战，特别是在隐私保护的反向传播和优化器操作中。本文指出了基于MPC的LLM隐私保护微调的两个主要障碍：(1) 反向和优化器过程的巨大计算开销，以及 (2) MPC设置中基于softmax的注意力机制效率低下。为了解决这些挑战，我们提出了SecFwT，这是第一个基于MPC的框架，旨在实现高效、隐私保护的LLM微调。SecFwT引入了仅前向调优范式以消除反向和优化器计算，并采用MPC友好的随机特征注意力来近似softmax注意力，显著减少了昂贵的非线性操作和计算复杂性。实验结果表明，SecFwT在效率和隐私保护方面取得了显著的改进，从而为隐私关键型应用实现了LLMs的可扩展和安全微调。", "summary": "本文提出了SecFwT，一个创新的基于安全多方计算（MPC）的框架，用于高效的隐私保护大型语言模型（LLM）微调。针对传统MPC微调中反向传播和softmax注意力的高计算开销问题，SecFwT引入了“仅前向调优”范式以避免反向计算，并采用MPC友好的随机特征注意力。实验证明，SecFwT显著提升了效率和隐私保护能力，使得LLMs能在隐私敏感应用中进行可扩展且安全的微调。", "keywords": "大型语言模型, 隐私保护, 微调, 安全多方计算, 仅前向传播, 注意力机制", "comments": "SecFwT的创新点在于提出了“仅前向调优”范式和MPC友好的随机特征注意力，有效地解决了MPC环境下LLM微调的计算瓶颈。这对于在医疗、金融等高度隐私敏感领域部署LLM具有重要意义，因为它首次提供了一个可行的、高效且安全的微调解决方案。该研究的重要性在于推动了隐私保护机器学习在大型模型应用上的边界。"}}
{"id": "2506.15563", "title": "Control and Realism: Best of Both Worlds in Layout-to-Image without Training", "authors": ["Bonan Li", "Yinhan Hu", "Songhua Liu", "Xinchao Wang"], "summary": "Layout-to-Image generation aims to create complex scenes with precise control\nover the placement and arrangement of subjects. Existing works have\ndemonstrated that pre-trained Text-to-Image diffusion models can achieve this\ngoal without training on any specific data; however, they often face challenges\nwith imprecise localization and unrealistic artifacts. Focusing on these\ndrawbacks, we propose a novel training-free method, WinWinLay. At its core,\nWinWinLay presents two key strategies, Non-local Attention Energy Function and\nAdaptive Update, that collaboratively enhance control precision and realism. On\none hand, we theoretically demonstrate that the commonly used attention energy\nfunction introduces inherent spatial distribution biases, hindering objects\nfrom being uniformly aligned with layout instructions. To overcome this issue,\nnon-local attention prior is explored to redistribute attention scores,\nfacilitating objects to better conform to the specified spatial conditions. On\nthe other hand, we identify that the vanilla backpropagation update rule can\ncause deviations from the pre-trained domain, leading to out-of-distribution\nartifacts. We accordingly introduce a Langevin dynamics-based adaptive update\nscheme as a remedy that promotes in-domain updating while respecting layout\nconstraints. Extensive experiments demonstrate that WinWinLay excels in\ncontrolling element placement and achieving photorealistic visual fidelity,\noutperforming the current state-of-the-art methods.", "comment": "Accepted by ICML2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15563v1", "AI": {"title_translation": "控制与真实感：无需训练的布局到图像生成中的两全其美", "tldr": "WinWinLay是一种无需训练的布局到图像生成方法，通过解决现有方法的定位不精确和不真实伪影问题，显著提高了控制精度和图像真实感。", "motivation": "现有布局到图像生成方法虽然无需训练即可利用预训练的文本到图像扩散模型，但常面临定位不精确和产生不真实伪影的挑战。", "method": "提出了一种名为WinWinLay的无需训练方法，其核心是两个关键策略：1. 非局部注意力能量函数：纠正常用注意力能量函数固有的空间分布偏差，通过探索非局部注意力先验来重新分配注意力分数，使对象更好地符合指定空间条件。2. 自适应更新：针对香草反向传播更新规则导致的预训练域偏差和域外伪影问题，引入基于朗之万动力学的自适应更新方案，促进域内更新同时尊重布局约束。", "result": "广泛的实验表明，WinWinLay在控制元素放置和实现逼真视觉保真度方面表现出色，优于当前最先进的方法。", "conclusion": "WinWinLay成功地在无需训练的情况下，解决了布局到图像生成中控制精度和图像真实感之间的权衡问题，实现了两全其美。", "translation": "布局到图像生成旨在创建对主体位置和排列具有精确控制的复杂场景。现有工作表明，预训练的文本到图像扩散模型无需在任何特定数据上进行训练即可实现此目标；然而，它们经常面临定位不精确和不真实伪影的挑战。针对这些缺点，我们提出了一种新颖的无需训练方法WinWinLay。WinWinLay的核心是提出了两种关键策略，即非局部注意力能量函数和自适应更新，它们协同增强了控制精度和真实感。一方面，我们从理论上证明了常用的注意力能量函数引入了固有的空间分布偏差，阻碍了对象与布局指令的均匀对齐。为了克服这个问题，我们探索了非局部注意力先验来重新分配注意力分数，促进对象更好地符合指定的空间条件。另一方面，我们发现香草反向传播更新规则可能导致偏离预训练域，从而产生域外伪影。因此，我们引入了一种基于朗之万动力学的自适应更新方案作为补救措施，该方案在尊重布局约束的同时促进域内更新。广泛的实验表明，WinWinLay在控制元素放置和实现逼真视觉保真度方面表现出色，超越了当前最先进的方法。", "summary": "本文提出了一种名为WinWinLay的无需训练的布局到图像生成方法，旨在解决现有方法在控制精度和图像真实感方面的不足。WinWinLay通过引入非局部注意力能量函数来校正空间分布偏差，并采用基于朗之万动力学的自适应更新方案来避免域外伪影。实验证明，WinWinLay在元素放置控制和视觉真实感方面均优于现有SOTA方法。", "keywords": "布局到图像生成, 无需训练, 扩散模型, 控制精度, 图像真实感", "comments": "WinWinLay的创新之处在于其无需训练的特性，通过理论分析并提出两种新颖的策略（非局部注意力能量函数和自适应更新）来解决布局到图像生成中的核心问题：控制精度和图像真实感。它直接针对现有预训练模型在布局生成时遇到的局限性，提供了一种高效且效果显著的改进方案，避免了昂贵的数据集训练，具有重要的实际应用价值。"}}
{"id": "2506.14862", "title": "Identifiability by common backdoor in summary causal graphs of time series", "authors": ["Clément Yvernes", "Charles K. Assaad", "Emilie Devijver", "Eric Gaussier"], "summary": "The identifiability problem for interventions aims at assessing whether the\ntotal effect of some given interventions can be written with a do-free formula,\nand thus be computed from observational data only. We study this problem,\nconsidering multiple interventions and multiple effects, in the context of time\nseries when only abstractions of the true causal graph in the form of summary\ncausal graphs are available. We focus in this study on identifiability by a\ncommon backdoor set, and establish, for time series with and without\nconsistency throughout time, conditions under which such a set exists. We also\nprovide algorithms of limited complexity to decide whether the problem is\nidentifiable or not.", "comment": null, "cate": "math.ST", "url": "http://arxiv.org/abs/2506.14862v1", "AI": {"title_translation": "时序数据摘要因果图中的公共后门可识别性", "tldr": "研究了在时间序列摘要因果图中，通过公共后门集识别干预总效应的问题，并提供了判断算法。", "motivation": "评估干预的总效应是否可以通过无do-算子的公式表示，从而仅从观测数据中计算出来（即可识别性问题）。特别是在时间序列中，当只有真实因果图的抽象形式（摘要因果图）可用时，以及存在多重干预和多重效应的情况。", "method": "专注于通过公共后门集进行识别，并为具有或不具有时间一致性的时间序列建立了此类集合存在的条件。还提供了复杂度有限的算法来判断问题是否可识别。", "result": "建立了在时间序列（有或无时间一致性）中存在公共后门集的条件，并提供了判断可识别性的算法。", "conclusion": "论文解决了时间序列摘要因果图中通过公共后门集识别干预总效应的问题，并提供了实用的判断方法。", "translation": "干预的可识别性问题旨在评估某些给定干预的总效应是否可以用无do-算子的公式表示，从而仅从观测数据中计算出来。我们研究了这个问题，考虑了在时间序列背景下，当只有真实因果图的抽象形式（即摘要因果图）可用时，以及存在多重干预和多重效应的情况。本研究重点关注通过公共后门集进行识别，并为具有和不具有时间一致性的时间序列，建立了此类集合存在的条件。我们还提供了复杂度有限的算法来判断问题是否可识别。", "summary": "本文研究了在时间序列的摘要因果图中，评估多重干预总效应可识别性的问题。作者聚焦于通过公共后门集实现可识别性，并为不同时间一致性的时间序列建立了后门集存在的条件。此外，论文还提供了用于判断可识别性的低复杂度算法。", "keywords": "可识别性, 公共后门, 摘要因果图, 时间序列, 因果推断", "comments": "该研究在时间序列和摘要因果图的复杂背景下，解决了因果推断中的关键可识别性问题，特别是引入了公共后门集的概念，并提供了实用的算法，具有理论和实际意义。"}}
{"id": "2506.15309", "title": "Active Learning-Guided Seq2Seq Variational Autoencoder for Multi-target Inhibitor Generation", "authors": ["Júlia Vilalta-Mor", "Alexis Molina", "Laura Ortega Varga", "Isaac Filella-Merce", "Victor Guallar"], "summary": "Simultaneously optimizing molecules against multiple therapeutic targets\nremains a profound challenge in drug discovery, particularly due to sparse\nrewards and conflicting design constraints. We propose a structured active\nlearning (AL) paradigm integrating a sequence-to-sequence (Seq2Seq) variational\nautoencoder (VAE) into iterative loops designed to balance chemical diversity,\nmolecular quality, and multi-target affinity. Our method alternates between\nexpanding chemically feasible regions of latent space and progressively\nconstraining molecules based on increasingly stringent multi-target docking\nthresholds. In a proof-of-concept study targeting three related coronavirus\nmain proteases (SARS-CoV-2, SARS-CoV, MERS-CoV), our approach efficiently\ngenerated a structurally diverse set of pan-inhibitor candidates. We\ndemonstrate that careful timing and strategic placement of chemical filters\nwithin this active learning pipeline markedly enhance exploration of beneficial\nchemical space, transforming the sparse-reward, multi-objective drug design\nproblem into an accessible computational task. Our framework thus provides a\ngeneralizable roadmap for efficiently navigating complex polypharmacological\nlandscapes.", "comment": "16 pages, 7 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15309v1", "AI": {"title_translation": "活性学习引导的Seq2Seq变分自编码器用于多靶点抑制剂生成", "tldr": "该论文提出了一种结合Seq2Seq VAE的主动学习范式，用于生成多靶点抑制剂，有效探索化学空间，并将药物设计转化为可行的计算任务。", "motivation": "在药物发现中，同时优化针对多个治疗靶点的分子仍然是一个重大挑战，特别是由于稀疏奖励和相互冲突的设计约束。", "method": "该论文提出了一种结构化的主动学习（AL）范式，将序列到序列（Seq2Seq）变分自编码器（VAE）集成到迭代循环中。该方法在扩展潜在空间的化学可行区域和根据日益严格的多靶点对接阈值逐步约束分子之间交替进行，并涉及化学过滤器的仔细时机选择和战略性放置。", "result": "在一项针对三种相关冠状病毒主蛋白酶（SARS-CoV-2、SARS-CoV、MERS-CoV）的概念验证研究中，该方法有效地生成了一组结构多样的泛抑制剂候选物。此外，化学过滤器的仔细时机选择和战略性放置显著增强了对有益化学空间的探索。", "conclusion": "该框架将稀疏奖励、多目标药物设计问题转化为可访问的计算任务，并为有效导航复杂的多药理学景观提供了通用路线图。", "translation": "同时优化针对多个治疗靶点的分子仍然是药物发现中的一个重大挑战，特别是由于稀疏奖励和相互冲突的设计约束。我们提出了一种结构化的主动学习（AL）范式，将序列到序列（Seq2Seq）变分自编码器（VAE）集成到迭代循环中，旨在平衡化学多样性、分子质量和多靶点亲和力。我们的方法在扩展潜在空间的化学可行区域和根据日益严格的多靶点对接阈值逐步约束分子之间交替进行。在一项针对三种相关冠状病毒主蛋白酶（SARS-CoV-2、SARS-CoV、MERS-CoV）的概念验证研究中，我们的方法有效地生成了一组结构多样的泛抑制剂候选物。我们证明，在此主动学习管道中，化学过滤器的仔细时机选择和战略性放置显著增强了对有益化学空间的探索，将稀疏奖励、多目标药物设计问题转化为可访问的计算任务。因此，我们的框架为有效导航复杂的多药理学景观提供了通用路线图。", "summary": "本文介绍了一种结构化的主动学习框架，该框架集成了Seq2Seq变分自编码器，以解决多靶点药物发现的挑战。该方法通过探索潜在空间和应用对接约束，迭代平衡化学多样性、质量和多靶点亲和力。在冠状病毒蛋白酶上的概念验证研究表明，它能有效地生成多样化的多靶点抑制剂候选物，证明战略性过滤可以增强化学空间探索并简化复杂的药物设计问题。", "keywords": "主动学习, Seq2Seq VAE, 多靶点抑制剂, 药物发现, 多药理学", "comments": "该论文的创新之处在于将主动学习与Seq2Seq VAE相结合，解决了具有挑战性的多靶点药物发现问题，尤其是在处理稀疏奖励和冲突约束方面的能力。在AL管道中战略性地使用化学过滤器来增强探索是特别有见地的。这种方法为多药理学设计提供了通用解决方案。"}}
{"id": "2506.15564", "title": "Show-o2: Improved Native Unified Multimodal Models", "authors": ["Jinheng Xie", "Zhenheng Yang", "Mike Zheng Shou"], "summary": "This paper presents improved native unified multimodal models, \\emph{i.e.,}\nShow-o2, that leverage autoregressive modeling and flow matching. Built upon a\n3D causal variational autoencoder space, unified visual representations are\nconstructed through a dual-path of spatial (-temporal) fusion, enabling\nscalability across image and video modalities while ensuring effective\nmultimodal understanding and generation. Based on a language model,\nautoregressive modeling and flow matching are natively applied to the language\nhead and flow head, respectively, to facilitate text token prediction and\nimage/video generation. A two-stage training recipe is designed to effectively\nlearn and scale to larger models. The resulting Show-o2 models demonstrate\nversatility in handling a wide range of multimodal understanding and generation\ntasks across diverse modalities, including text, images, and videos. Code and\nmodels are released at https://github.com/showlab/Show-o.", "comment": "Technical report", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15564v1", "AI": {"title_translation": "Show-o2: 改进的本地统一多模态模型", "tldr": "本文提出了Show-o2，一种利用自回归建模和流匹配的改进本地统一多模态模型，能够处理文本、图像和视频的理解与生成任务。", "motivation": "本文旨在提出改进的本地统一多模态模型，以实现跨模态（图像、视频、文本）的有效理解和生成。", "method": "Show-o2模型利用自回归建模和流匹配技术。它构建于一个3D因果变分自编码器空间之上，通过空间（-时间）融合的双路径构建统一视觉表示，以实现图像和视频模态的扩展性。自回归建模和流匹配分别应用于语言头和流头，以促进文本标记预测和图像/视频生成。模型采用两阶段训练方案进行有效学习和扩展。", "result": "Show-o2模型在处理广泛的多模态理解和生成任务（包括文本、图像和视频）方面表现出多功能性。", "conclusion": "Show-o2模型成功地利用自回归建模和流匹配，在统一的框架下实现了改进的多模态理解和生成能力，并在多种模态上展现了强大的性能。", "translation": "本文提出了改进的本地统一多模态模型，即 Show-o2，它利用自回归建模和流匹配。Show-o2 构建于一个 3D 因果变分自编码器空间之上，通过空间（-时间）融合的双路径构建统一的视觉表示，从而实现图像和视频模态的扩展性，同时确保有效的多模态理解和生成。基于语言模型，自回归建模和流匹配分别自然地应用于语言头和流头，以促进文本标记预测和图像/视频生成。设计了一个两阶段的训练方案，以有效地学习并扩展到更大的模型。由此产生的 Show-o2 模型在处理文本、图像和视频等不同模态的广泛多模态理解和生成任务方面表现出多功能性。代码和模型已在 https://github.com/showlab/Show-o 发布。", "summary": "本文介绍了Show-o2，一种改进的本地统一多模态模型，它结合了自回归建模和流匹配技术。该模型建立在3D因果变分自编码器空间之上，通过双路径空间融合构建统一视觉表示，以实现跨图像和视频模态的理解和生成。Show-o2通过将自回归建模和流匹配应用于其语言头和流头，支持文本预测和图像/视频生成。采用两阶段训练策略，使其能够有效学习并扩展到大型模型，最终在多种模态的多模态理解和生成任务中展现出强大的通用性。", "keywords": "多模态模型, 自回归建模, 流匹配, 统一表示, 图像生成", "comments": "这篇论文的创新点在于结合了自回归建模和流匹配技术，并将其应用于统一的3D因果变分自编码器空间，实现了跨图像、视频和文本的统一多模态理解和生成。其双路径空间融合方法提高了模型在不同视觉模态上的可扩展性。代码和模型的发布也提升了其研究价值和可复现性。"}}
{"id": "2506.15329", "title": "When and How Unlabeled Data Provably Improve In-Context Learning", "authors": ["Yingcong Li", "Xiangyu Chang", "Muti Kara", "Xiaofeng Liu", "Amit Roy-Chowdhury", "Samet Oymak"], "summary": "Recent research shows that in-context learning (ICL) can be effective even\nwhen demonstrations have missing or incorrect labels. To shed light on this\ncapability, we examine a canonical setting where the demonstrations are drawn\naccording to a binary Gaussian mixture model (GMM) and a certain fraction of\nthe demonstrations have missing labels. We provide a comprehensive theoretical\nstudy to show that: (1) The loss landscape of one-layer linear attention models\nrecover the optimal fully-supervised estimator but completely fail to exploit\nunlabeled data; (2) In contrast, multilayer or looped transformers can\neffectively leverage unlabeled data by implicitly constructing estimators of\nthe form $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ with $X$ and $y$ denoting\nfeatures and partially-observed labels (with missing entries set to zero). We\ncharacterize the class of polynomials that can be expressed as a function of\ndepth and draw connections to Expectation Maximization, an iterative\npseudo-labeling algorithm commonly used in semi-supervised learning.\nImportantly, the leading polynomial power is exponential in depth, so mild\namount of depth/looping suffices. As an application of theory, we propose\nlooping off-the-shelf tabular foundation models to enhance their\nsemi-supervision capabilities. Extensive evaluations on real-world datasets\nshow that our method significantly improves the semisupervised tabular learning\nperformance over the standard single pass inference.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15329v1", "AI": {"title_translation": "上下文学习中无标签数据如何以及何时能够被证明有效提升性能", "tldr": "本文理论证明了多层或循环Transformer模型在上下文学习中能够有效利用无标签数据，而单层模型不能，并通过实验验证了其在半监督学习中的应用效果。", "motivation": "现有研究表明上下文学习（ICL）在示范数据标签缺失或不正确的情况下仍有效，本文旨在深入理解ICL利用无标签数据的能力。", "method": "作者在一个二元高斯混合模型（GMM）的典型设置下，对具有部分缺失标签的示范数据进行了全面的理论研究。他们还提出通过循环现有的表格基础模型来增强其半监督能力，并在真实世界数据集上进行了评估。", "result": "1. 单层线性注意力模型的损失平面能恢复最优全监督估计器，但无法利用无标签数据。2. 多层或循环Transformer模型能通过隐式构建特定形式的估计器有效利用无标签数据。3. 这些模型能表达的函数类别与深度有关，与期望最大化（EM）算法有联系，且领先的多项式幂随深度呈指数增长。4. 将理论应用于实际，通过循环表格基础模型显著提升了半监督表格学习的性能。", "conclusion": "深度和循环在Transformer模型中对于上下文学习利用无标签数据至关重要，这为半监督学习提供了新的视角和有效的改进方法。", "translation": "最近的研究表明，即使演示数据标签缺失或不正确，上下文学习（ICL）也能有效。为了阐明这种能力，我们研究了一个典型设置：演示数据根据二元高斯混合模型（GMM）抽取，并且其中一部分演示数据标签缺失。我们提供了一项全面的理论研究，表明：(1) 单层线性注意力模型的损失平面可以恢复最优的全监督估计器，但完全无法利用无标签数据；(2) 相比之下，多层或循环Transformer模型可以通过隐式构建形式为 $\\sum_{i\\ge 0} a_i (X^\\top X)^iX^\\top y$ 的估计器来有效利用无标签数据，其中 $X$ 和 $y$ 表示特征和部分观测标签（缺失条目设为零）。我们刻画了可以表示为深度函数的这类多项式，并将其与半监督学习中常用的迭代伪标签算法——期望最大化（Expectation Maximization）联系起来。重要的是，领先的多项式幂随深度呈指数增长，因此适度的深度/循环就足够了。作为理论的应用，我们提出循环现成的表格基础模型以增强其半监督能力。在真实世界数据集上的广泛评估表明，我们的方法显著提高了半监督表格学习的性能，优于标准的单次推理。", "summary": "本文深入探讨了上下文学习（ICL）如何利用无标签数据，通过理论分析发现单层线性注意力模型无法利用无标签数据，而多层或循环Transformer模型能通过构建特定估计器有效利用。研究表明，Transformer的深度或循环次数与利用无标签数据的能力呈指数关系，并与期望最大化算法相关联。实验证明，将此理论应用于表格基础模型能显著提升半监督学习性能。", "keywords": "上下文学习, 无标签数据, Transformer, 半监督学习, 理论分析", "comments": "这篇论文的创新点在于它为上下文学习中无标签数据的利用提供了坚实的理论基础，特别是区分了单层和多层模型的能力差异，并将其与深度和期望最大化算法联系起来。将理论应用于实际的表格基础模型，并取得显著性能提升，显示了其重要性和实用价值。"}}
{"id": "2506.15565", "title": "Baltimore Atlas: FreqWeaver Adapter for Semi-supervised Ultra-high Spatial Resolution Land Cover Classification", "authors": ["Junhao Wu", "Aboagye-Ntow Stephen", "Chuyuan Wang", "Gang Chen", "Xin Huang"], "summary": "Ultra-high Spatial Resolution Land Cover Classification is essential for\nfine-grained land cover analysis, yet it remains challenging due to the high\ncost of pixel-level annotations, significant scale variation, and the limited\nadaptability of large-scale vision models. Existing methods typically focus on\n1-meter spatial resolution imagery and rely heavily on annotated data, whereas\npractical applications often require processing higher-resolution imagery under\nweak supervision. To address this, we propose a parameter-efficient\nsemi-supervised segmentation framework for 0.3 m spatial resolution imagery,\nwhich leverages the knowledge of SAM2 and introduces a remote sensing-specific\nFreqWeaver Adapter to enhance fine-grained detail modeling while maintaining a\nlightweight design at only 5.96% of the total model parameters. By effectively\nleveraging unlabeled data and maintaining minimal parameter overhead, the\nproposed method delivers robust segmentation results with superior structural\nconsistency, achieving a 1.78% improvement over existing parameter-efficient\ntuning strategies and a 3.44% gain compared to state-of-the-art high-resolution\nremote sensing segmentation approaches.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15565v1", "AI": {"title_translation": "巴尔的摩地图集：用于半监督超高空间分辨率土地覆盖分类的 FreqWeaver 适配器", "tldr": "本文提出了一种名为 Baltimore Atlas 的参数高效半监督分割框架，用于 0.3 米空间分辨率影像的土地覆盖分类，通过引入 FreqWeaver 适配器和利用 SAM2 的知识，在保持轻量级的同时显著提高了分割性能。", "motivation": "超高空间分辨率土地覆盖分类对于精细粒度的土地覆盖分析至关重要，但由于像素级标注成本高、尺度变化显著以及大型视觉模型适应性有限，仍然具有挑战性。现有方法主要关注 1 米空间分辨率影像并严重依赖标注数据，而实际应用通常需要在弱监督下处理更高分辨率的影像。", "method": "提出了一种参数高效的半监督分割框架，用于 0.3 米空间分辨率影像。该框架利用 SAM2 的知识，并引入了一种遥感专用的 FreqWeaver 适配器，以增强细粒度细节建模，同时保持轻量级设计（仅占总模型参数的 5.96%）。", "result": "通过有效利用未标注数据并保持最小参数开销，所提出的方法提供了鲁棒的分割结果，具有卓越的结构一致性。与现有参数高效微调策略相比，性能提升了 1.78%，与最先进的高分辨率遥感分割方法相比，性能提升了 3.44%。", "conclusion": "本文提出的 Baltimore Atlas 框架通过引入 FreqWeaver 适配器，有效解决了超高空间分辨率土地覆盖分类中弱监督下标注数据不足和模型适应性有限的挑战，实现了卓越的分割性能和结构一致性。", "translation": "超高空间分辨率土地覆盖分类对于精细粒度的土地覆盖分析至关重要，但由于像素级标注成本高、尺度变化显著以及大型视觉模型适应性有限，仍然具有挑战性。现有方法通常侧重于 1 米空间分辨率影像并严重依赖标注数据，而实际应用往往需要在弱监督下处理更高分辨率的影像。为了解决这个问题，我们提出了一种用于 0.3 米空间分辨率影像的参数高效半监督分割框架，该框架利用 SAM2 的知识并引入了一个遥感专用的 FreqWeaver 适配器，以增强细粒度细节建模，同时保持轻量级设计，仅占总模型参数的 5.96%。通过有效利用未标注数据并保持最小参数开销，所提出的方法提供了鲁棒的分割结果，具有卓越的结构一致性，与现有参数高效微调策略相比提高了 1.78%，与最先进的高分辨率遥感分割方法相比提高了 3.44%。", "summary": "本文提出了 Baltimore Atlas，一个针对 0.3 米超高空间分辨率影像的参数高效半监督土地覆盖分类框架。该方法通过结合 SAM2 的知识和引入一个轻量级的遥感专用 FreqWeaver 适配器，有效解决了高分辨率影像标注成本高和模型适应性差的问题。实验结果表明，该方法在利用未标注数据和保持低参数开销的同时，实现了优于现有方法的分割性能和结构一致性。", "keywords": "土地覆盖分类, 半监督学习, 超高空间分辨率, FreqWeaver Adapter, 遥感影像", "comments": "该论文的创新点在于提出了一个轻量级的半监督框架，通过引入遥感专用的 FreqWeaver 适配器，有效地将大型视觉模型（如 SAM2）的知识应用于超高分辨率遥感影像的土地覆盖分类，解决了高精度标注数据稀缺的问题。其参数效率高，并且在弱监督下取得了显著的性能提升，对于实际应用具有重要意义。"}}
{"id": "2506.15330", "title": "Universal Laboratory Model: prognosis of abnormal clinical outcomes based on routine tests", "authors": ["Pavel Karpov", "Ilya Petrenkov", "Ruslan Raiman"], "summary": "Clinical laboratory results are ubiquitous in any diagnosis making.\nPredicting abnormal values of not prescribed tests based on the results of\nperformed tests looks intriguing, as it would be possible to make early\ndiagnosis available to everyone. The special place is taken by the Common Blood\nCount (CBC) test, as it is the most widely used clinical procedure. Combining\nroutine biochemical panels with CBC presents a set of test-value pairs that\nvaries from patient to patient, or, in common settings, a table with missing\nvalues. Here we formulate a tabular modeling problem as a set translation\nproblem where the source set comprises pairs of GPT-like label column embedding\nand its corresponding value while the target set consists of the same type\nembeddings only. The proposed approach can effectively deal with missing values\nwithout implicitly estimating them and bridges the world of LLM with the\ntabular domain. Applying this method to clinical laboratory data, we achieve an\nimprovement up to 8% AUC for joint predictions of high uric acid, glucose,\ncholesterol, and low ferritin levels.", "comment": "7 pages, 2 figues", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15330v1", "AI": {"title_translation": "普适实验室模型：基于常规检测对异常临床结果的预测", "tldr": "该研究提出一种将表格建模视为集合翻译问题的方法，利用GPT类嵌入处理临床实验室数据中的缺失值，并实现对异常临床结果（如高尿酸、葡萄糖、胆固醇和低铁蛋白）的联合预测，AUC提高了8%。", "motivation": "预测未开具检测的异常值，基于已进行的检测结果，以实现早期诊断的普及。特别关注常规血常规（CBC）测试。", "method": "将表格建模问题公式化为集合翻译问题。源集合包含GPT类标签列嵌入及其对应值，目标集合仅包含相同类型的嵌入。该方法能有效处理缺失值，无需隐式估计，并连接了大型语言模型（LLM）与表格领域。", "result": "将该方法应用于临床实验室数据，在联合预测高尿酸、葡萄糖、胆固醇和低铁蛋白水平方面，AUC提高了8%。", "conclusion": "该方法能有效处理临床实验室数据中的缺失值，并显著提高对多种异常临床结果的联合预测准确性，表明了其在早期诊断中的潜力。", "translation": "临床实验室结果在任何诊断中都无处不在。根据已进行的检测结果预测未开具检测的异常值，这看起来很有趣，因为这将使早期诊断人人可及。血常规（CBC）检测占据特殊地位，因为它是最广泛使用的临床程序。将常规生化指标与血常规结合起来，呈现出一组因患者而异的检测值对，或者在常见设置中，一个带有缺失值的表格。在这里，我们将表格建模问题公式化为集合翻译问题，其中源集合包含GPT类标签列嵌入及其对应值，而目标集合仅包含相同类型的嵌入。所提出的方法可以有效地处理缺失值而无需隐式估计它们，并连接了大型语言模型（LLM）与表格领域。将此方法应用于临床实验室数据，我们在联合预测高尿酸、葡萄糖、胆固醇和低铁蛋白水平方面，AUC提高了高达8%。", "summary": "本文提出了一种创新的表格建模方法，将临床实验室数据中的预测问题视为集合翻译问题。该方法利用GPT类标签列嵌入来处理常规生化面板和血常规数据中常见的缺失值，无需进行隐式估计。通过将大型语言模型（LLM）的概念引入表格领域，该模型能够基于已进行的检测结果预测未开具检测的异常值。实验结果表明，该方法在联合预测高尿酸、葡萄糖、胆固醇和低铁蛋白等异常临床结果方面，其AUC值最高可提高8%，为普及早期诊断提供了潜力。", "keywords": "临床预测, 缺失值处理, 表格建模, 集合翻译, GPT嵌入", "comments": "这篇论文的创新之处在于将表格数据建模问题转化为集合翻译问题，并引入了GPT风格的嵌入来处理临床数据中普遍存在的缺失值，从而弥合了LLM与传统表格数据分析之间的鸿沟。这种方法有望提高早期诊断的效率和可及性，尤其是在资源受限或数据不完整的临床环境中。其在AUC上8%的提升显示了实际应用潜力。"}}
{"id": "2506.15577", "title": "A Unified Graph-based Framework for Scalable 3D Tree Reconstruction and Non-Destructive Biomass Estimation from Point Clouds", "authors": ["Di Wang", "Shi Li"], "summary": "Estimating forest above-ground biomass (AGB) is crucial for assessing carbon\nstorage and supporting sustainable forest management. Quantitative Structural\nModel (QSM) offers a non-destructive approach to AGB estimation through 3D tree\nstructural reconstruction. However, current QSM methods face significant\nlimitations, as they are primarily designed for individual trees,depend on\nhigh-quality point cloud data from terrestrial laser scanning (TLS), and also\nrequire multiple pre-processing steps that hinder scalability and practical\ndeployment. This study presents a novel unified framework that enables\nend-to-end processing of large-scale point clouds using an innovative\ngraph-based pipeline. The proposed approach seamlessly integrates tree\nsegmentation,leaf-wood separation and 3D skeletal reconstruction through\ndedicated graph operations including pathing and abstracting for tree topology\nreasoning. Comprehensive validation was conducted on datasets with varying leaf\nconditions (leaf-on and leaf-off), spatial scales (tree- and plot-level), and\ndata sources (TLS and UAV-based laser scanning, ULS). Experimental results\ndemonstrate strong performance under challenging conditions, particularly in\nleaf-on scenarios (~20% relative error) and low-density ULS datasets with\npartial coverage (~30% relative error). These findings indicate that the\nproposed framework provides a robust and scalable solution for large-scale,\nnon-destructive AGB estimation. It significantly reduces dependency on\nspecialized pre-processing tools and establishes ULS as a viable alternative to\nTLS. To our knowledge, this is the first method capable of enabling seamless,\nend-to-end 3D tree reconstruction at operational scales. This advancement\nsubstantially improves the feasibility of QSM-based AGB estimation, paving the\nway for broader applications in forest inventory and climate change research.", "comment": "17 pages,19 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15577v1", "AI": {"title_translation": "一种统一的基于图的框架，用于可扩展的3D树木重建和基于点云的无损生物量估算", "tldr": "本文提出了一种统一的基于图的框架，用于从点云中进行大规模、端到端的3D树木重建和无损生物量估算，解决了现有QSM方法的可伸缩性和预处理依赖问题。", "motivation": "现有定量结构模型（QSM）方法在进行地上生物量（AGB）估算时，主要针对单棵树，依赖高质量的地面激光扫描（TLS）点云数据，并且需要多个预处理步骤，这些限制阻碍了其可伸缩性和实际部署。", "method": "本研究提出了一种新颖的统一框架，通过创新的基于图的管道实现大规模点云的端到端处理。该方法通过专门的图操作（包括路径查找和抽象化）无缝集成了树木分割、叶木分离和3D骨架重建，以进行树木拓扑推理。", "result": "在不同叶片条件（有叶和无叶）、空间尺度（树和样地级别）和数据源（TLS和无人机激光扫描，ULS）的数据集上进行了全面验证。实验结果表明，在挑战性条件下（特别是有叶场景和低密度ULS数据集）表现强劲，相对误差分别约为20%和30%。", "conclusion": "所提出的框架为大规模、无损AGB估算提供了一个鲁棒且可扩展的解决方案。它显著减少了对专业预处理工具的依赖，并确立了ULS作为TLS的可行替代方案。这是第一个在操作尺度上实现无缝、端到端3D树木重建的方法，显著提高了基于QSM的AGB估算的S可行性，为森林清查和气候变化研究中的更广泛应用铺平了道路。", "translation": "估算森林地上生物量（AGB）对于评估碳储量和支持可持续森林管理至关重要。定量结构模型（QSM）通过3D树木结构重建提供了一种无损的AGB估算方法。然而，当前的QSM方法面临显著限制，因为它们主要针对单棵树设计，依赖于来自地面激光扫描（TLS）的高质量点云数据，并且还需要多个预处理步骤，这些都阻碍了可伸缩性和实际部署。本研究提出了一种新颖的统一框架，通过创新的基于图的管道实现大规模点云的端到端处理。所提出的方法通过专门的图操作（包括路径查找和抽象化）无缝集成了树木分割、叶木分离和3D骨架重建，以进行树木拓扑推理。在不同叶片条件（有叶和无叶）、空间尺度（树和样地级别）和数据源（TLS和基于无人机的激光扫描，ULS）的数据集上进行了全面验证。实验结果表明，在挑战性条件下表现强劲，特别是在有叶场景中（约20%的相对误差）和低密度、部分覆盖的ULS数据集中（约30%的相对误差）。这些发现表明，所提出的框架为大规模、无损AGB估算提供了一个鲁棒且可扩展的解决方案。它显著减少了对专业预处理工具的依赖，并确立了ULS作为TLS的可行替代方案。据我们所知，这是第一个能够在操作尺度上实现无缝、端到端3D树木重建的方法。这一进展大大提高了基于QSM的AGB估算的可行性，为森林清查和气候变化研究中的更广泛应用铺平了道路。", "summary": "本文提出了一种统一的基于图的框架，旨在解决现有QSM方法在3D树木重建和地上生物量（AGB）估算中面临的可伸缩性及预处理依赖问题。该框架通过创新的图基管道，实现了从大规模点云的端到端处理，无缝整合了树木分割、叶木分离和3D骨架重建。实验结果表明，该方法在有叶和低密度ULS数据集等挑战性条件下表现出色，提供了鲁棒且可扩展的解决方案，并确立了ULS作为TLS的有效替代方案，显著提升了QSM-based AGB估算在实际应用中的可行性。", "keywords": "3D树木重建, 生物量估算, 点云, 图算法, 无损测量", "comments": "本文的创新之处在于提出了一个统一的基于图的框架，首次实现了大规模点云的端到端3D树木重建和生物量估算，显著降低了对繁琐预处理的依赖。其重要性体现在提高了QSM方法的可伸缩性和实用性，特别是通过支持无人机激光扫描（ULS）数据，拓宽了AGB估算的应用范围。该方法在有叶和低密度数据下的表现也令人印象深刻，为森林管理和气候变化研究提供了更高效、更经济的工具。"}}
{"id": "2506.15337", "title": "Knowledge Distillation Framework for Accelerating High-Accuracy Neural Network-Based Molecular Dynamics Simulations", "authors": ["Naoki Matsumura", "Yuta Yoshimoto", "Yuto Iwasaki", "Meguru Yamazaki", "Yasufumi Sakai"], "summary": "Neural network potentials (NNPs) offer a powerful alternative to traditional\nforce fields for molecular dynamics (MD) simulations. Accurate and stable MD\nsimulations, crucial for evaluating material properties, require training data\nencompassing both low-energy stable structures and high-energy structures.\nConventional knowledge distillation (KD) methods fine-tune a pre-trained NNP as\na teacher model to generate training data for a student model. However, in\nmaterial-specific models, this fine-tuning process increases energy barriers,\nmaking it difficult to create training data containing high-energy structures.\nTo address this, we propose a novel KD framework that leverages a\nnon-fine-tuned, off-the-shelf pre-trained NNP as a teacher. Its gentler energy\nlandscape facilitates the exploration of a wider range of structures, including\nthe high-energy structures crucial for stable MD simulations. Our framework\nemploys a two-stage training process: first, the student NNP is trained with a\ndataset generated by the off-the-shelf teacher; then, it is fine-tuned with a\nsmaller, high-accuracy density functional theory (DFT) dataset. We demonstrate\nthe effectiveness of our framework by applying it to both organic (polyethylene\nglycol) and inorganic (L$_{10}$GeP$_{2}$S$_{12}$) materials, achieving\ncomparable or superior accuracy in reproducing physical properties compared to\nexisting methods. Importantly, our method reduces the number of expensive DFT\ncalculations by 10x compared to existing NNP generation methods, without\nsacrificing accuracy.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15337v1", "AI": {"title_translation": "用于加速高精度神经网络分子动力学模拟的知识蒸馏框架", "tldr": "本文提出了一种用于分子动力学（MD）模拟中神经网络势能（NNPs）的新型知识蒸馏（KD）框架。该框架利用未经微调的教师模型生成高能量结构训练数据，显著减少了昂贵的密度泛函理论（DFT）计算量，同时保持了高精度。", "motivation": "传统的知识蒸馏方法在为特定材料模型微调预训练的神经网络势能（NNP）作为教师模型时，会增加能垒，导致难以生成对稳定分子动力学（MD）模拟至关重要的高能量结构训练数据。", "method": "我们提出了一种新颖的知识蒸馏（KD）框架，该框架利用未经微调的现成预训练NNP作为教师模型。这个教师模型具有更平缓的能量景观，有助于探索更广泛的结构，包括高能量结构。该框架采用两阶段训练过程：首先，学生NNP使用由现成教师生成的数据集进行训练；然后，使用更小、高精度的密度泛函理论（DFT）数据集进行微调。", "result": "该框架在有机（聚乙二醇）和无机（L$_{10}$GeP$_{2}$S$_{12}$）材料上均表现出有效性，与现有方法相比，在重现物理性质方面实现了相当或更高的准确性。重要的是，与现有NNP生成方法相比，我们的方法将昂贵的DFT计算数量减少了10倍，同时没有牺牲准确性。", "conclusion": "所提出的知识蒸馏框架通过生成关键的高能量训练数据并显著降低计算成本，有效地加速了基于高精度神经网络的分子动力学模拟，证明了其对各种材料的实用性。", "translation": "神经网络势能（NNPs）为分子动力学（MD）模拟提供了传统力场的强大替代方案。准确稳定的MD模拟对于评估材料性质至关重要，需要包含低能量稳定结构和高能量结构的训练数据。传统的知识蒸馏（KD）方法将预训练的NNP作为教师模型进行微调，以生成学生模型的训练数据。然而，在特定材料模型中，这种微调过程会增加能垒，使得难以创建包含高能量结构的训练数据。为了解决这个问题，我们提出了一种新颖的KD框架，该框架利用未经微调的现成预训练NNP作为教师。其更平缓的能量景观有助于探索更广泛的结构，包括对稳定MD模拟至关重要的高能量结构。我们的框架采用两阶段训练过程：首先，学生NNP使用由现成教师生成的数据集进行训练；然后，使用更小的高精度密度泛函理论（DFT）数据集进行微调。我们通过将其应用于有机材料（聚乙二醇）和无机材料（L$_{10}$GeP$_{2}$S$_{12}$）来证明我们框架的有效性，与现有方法相比，在重现物理性质方面实现了相当或更高的准确性。重要的是，与现有NNP生成方法相比，我们的方法将昂贵的DFT计算数量减少了10倍，同时没有牺牲准确性。", "summary": "本文提出了一种用于加速高精度神经网络分子动力学模拟的新型知识蒸馏框架。该框架通过利用未经微调的现成预训练神经网络势能（NNP）作为教师模型，解决了生成高能量训练数据的挑战，因为该教师模型具有更平缓的能量景观。该框架采用两阶段训练过程：首先使用现成教师生成的数据集训练学生NNP，然后使用少量高精度密度泛函理论（DFT）数据集进行微调。这种方法在保持或超越现有方法准确性的同时，将昂贵的DFT计算量减少了10倍，证明了其对有机和无机材料的有效性。", "keywords": "知识蒸馏, 神经网络势能, 分子动力学, DFT, 高能量结构", "comments": "本文为分子动力学模拟中的神经网络势能（NNPs）知识蒸馏引入了一种创新方法，特别解决了生成高能量训练数据的关键问题。通过利用未经微调的教师模型，它巧妙地规避了传统知识蒸馏中遇到的能垒问题。最重要的贡献是在不牺牲准确性的前提下，将昂贵的密度泛函理论（DFT）计算量大幅减少了10倍，这是使高精度分子动力学模拟在计算上更可行、更易于实现的重要一步。"}}
{"id": "2506.15591", "title": "One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution", "authors": ["Yujing Sun", "Lingchen Sun", "Shuaizheng Liu", "Rongyuan Wu", "Zhengqiang Zhang", "Lei Zhang"], "summary": "It is a challenging problem to reproduce rich spatial details while\nmaintaining temporal consistency in real-world video super-resolution\n(Real-VSR), especially when we leverage pre-trained generative models such as\nstable diffusion (SD) for realistic details synthesis. Existing SD-based\nReal-VSR methods often compromise spatial details for temporal coherence,\nresulting in suboptimal visual quality. We argue that the key lies in how to\neffectively extract the degradation-robust temporal consistency priors from the\nlow-quality (LQ) input video and enhance the video details while maintaining\nthe extracted consistency priors. To achieve this, we propose a Dual LoRA\nLearning (DLoRAL) paradigm to train an effective SD-based one-step diffusion\nmodel, achieving realistic frame details and temporal consistency\nsimultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module\nto aggregate complementary information across frames, and train a\nConsistency-LoRA (C-LoRA) to learn robust temporal representations from\ndegraded inputs. After consistency learning, we fix the CFR and C-LoRA modules\nand train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with\nthe temporal space defined by C-LoRA to keep temporal coherence. The two phases\nalternate iteratively for optimization, collaboratively delivering consistent\nand detail-rich outputs. During inference, the two LoRA branches are merged\ninto the SD model, allowing efficient and high-quality video restoration in a\nsingle diffusion step. Experiments show that DLoRAL achieves strong performance\nin both accuracy and speed. Code and models are available at\nhttps://github.com/yjsunnn/DLoRAL.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15591v1", "AI": {"title_translation": "一步扩散实现细节丰富且时间一致的视频超分辨率", "tldr": "提出了一种名为DLoRAL的双LoRA学习范式，用于训练基于SD的一步扩散模型，以同时实现细节丰富和时间一致的视频超分辨率。", "motivation": "在真实世界视频超分辨率（Real-VSR）中，尤其是在利用预训练生成模型如稳定扩散（SD）合成逼真细节时，重现丰富的空间细节并保持时间一致性是一个挑战。现有基于SD的Real-VSR方法通常为了时间连贯性而牺牲空间细节，导致视觉质量不佳。", "method": "提出了一种名为“双LoRA学习（DLoRAL）”的范式来训练一个有效的一步扩散模型。具体来说，引入了一个“跨帧检索（CFR）”模块来聚合跨帧的互补信息，并训练一个“一致性LoRA（C-LoRA）”从降级输入中学习鲁棒的时间表示。在一致性学习之后，固定CFR和C-LoRA模块，并训练一个“细节LoRA（D-LoRA）”来增强空间细节，同时与C-LoRA定义的时间空间对齐以保持时间连贯性。这两个阶段迭代交替优化。在推理过程中，两个LoRA分支被合并到SD模型中，实现单次扩散步骤中高效高质量的视频恢复。", "result": "实验表明，DLoRAL在准确性和速度方面都取得了强大的性能。", "conclusion": "DLoRAL通过其双LoRA学习范式，成功地解决了真实世界视频超分辨率中细节再现和时间一致性之间的权衡问题，实现了单一步骤的高效高质量视频恢复。", "translation": "在真实世界视频超分辨率（Real-VSR）中，重现丰富的空间细节同时保持时间一致性是一个具有挑战性的问题，特别是当我们利用预训练的生成模型（如稳定扩散（SD））来合成逼真细节时。现有基于SD的Real-VSR方法通常为了时间连贯性而牺牲空间细节，导致视觉质量不佳。我们认为，关键在于如何有效地从低质量（LQ）输入视频中提取对退化鲁棒的时间一致性先验，并在保持提取的一致性先验的同时增强视频细节。为了实现这一点，我们提出了一种双LoRA学习（DLoRAL）范式来训练一个有效的基于SD的一步扩散模型，同时实现逼真的帧细节和时间一致性。具体来说，我们引入了一个跨帧检索（CFR）模块来聚合跨帧的互补信息，并训练一个一致性LoRA（C-LoRA）来从降级输入中学习鲁棒的时间表示。在一致性学习之后，我们固定CFR和C-LoRA模块，并训练一个细节LoRA（D-LoRA）来增强空间细节，同时与C-LoRA定义的时间空间对齐以保持时间连贯性。这两个阶段交替迭代优化，协同提供一致且细节丰富的输出。在推理过程中，两个LoRA分支被合并到SD模型中，允许在单个扩散步骤中进行高效和高质量的视频恢复。实验表明，DLoRAL在准确性和速度方面都取得了强大的性能。代码和模型可在https://github.com/yjsunnn/DLoRAL 获取。", "summary": "该论文提出了一种名为DLoRAL的双LoRA学习范式，旨在解决真实世界视频超分辨率中细节丰富性和时间一致性难以兼顾的问题。通过引入跨帧检索模块和分别训练一致性LoRA（C-LoRA）与细节LoRA（D-LoRA），DLoRAL能够从低质量视频中学习鲁棒的时间一致性，并在保持一致性的同时增强空间细节。该方法采用两阶段交替优化，最终将两个LoRA分支合并到稳定扩散模型中，实现单一步骤的高效高质量视频恢复。实验证明，DLoRAL在准确性和速度上均表现出色。", "keywords": "视频超分辨率, 扩散模型, LoRA, 时间一致性, 细节增强", "comments": "该论文的创新点在于提出了双LoRA学习范式（DLoRAL），通过解耦一致性学习和细节增强，有效地解决了视频超分辨率中细节与时间一致性之间的矛盾。其一步扩散的推理方式显著提高了效率，同时保证了高质量的输出，对于实际应用具有重要意义。结合LoRA与预训练SD模型，也体现了对现有强大模型的高效利用。"}}
{"id": "2506.14923", "title": "Forecasting the spatiotemporal evolution of fluid-induced microearthquakes with deep learning", "authors": ["Jaehong Chung", "Michael Manga", "Timothy Kneafsey", "Tapan Mukerji", "Mengsu Hu"], "summary": "Microearthquakes (MEQs) generated by subsurface fluid injection record the\nevolving stress state and permeability of reservoirs. Forecasting their full\nspatiotemporal evolution is therefore critical for applications such as\nenhanced geothermal systems (EGS), CO$_2$ sequestration and other\ngeo-engineering applications. We present a transformer-based deep learning\nmodel that ingests hydraulic stimulation history and prior MEQ observations to\nforecast four key quantities: cumulative MEQ count, cumulative logarithmic\nseismic moment, and the 50th- and 95th-percentile extents ($P_{50}, P_{95}$) of\nthe MEQ cloud. Applied to the EGS Collab Experiment 1 dataset, the model\nachieves $R^2 >0.98$ for the 1-second forecast horizon and $R^2 >0.88$ for the\n15-second forecast horizon across all targets, and supplies uncertainty\nestimates through a learned standard deviation term. These accurate,\nuncertainty-quantified forecasts enable real-time inference of fracture\npropagation and permeability evolution, demonstrating the strong potential of\ndeep-learning approaches to improve seismic-risk assessment and guide\nmitigation strategies in future fluid-injection operations.", "comment": null, "cate": "physics.geo-ph", "url": "http://arxiv.org/abs/2506.14923v1", "AI": {"title_translation": "利用深度学习预测流体诱发微地震的时空演化", "tldr": "本文提出了一种基于Transformer的深度学习模型，用于高精度预测流体诱发微地震的时空演化，并提供不确定性估计，对地质工程中的地震风险评估具有重要意义。", "motivation": "地下流体注入产生的微地震（MEQs）记录了储层应力状态和渗透率的演变。因此，预测其完整的时空演化对于增强型地热系统（EGS）、二氧化碳封存及其他地质工程应用至关重要。", "method": "本文提出了一种基于Transformer的深度学习模型，该模型接收水力压裂历史和先前的微地震观测数据作为输入，以预测四个关键量：累积微地震计数、累积对数地震矩以及微地震云的第50和第95百分位范围（P50, P95）。", "result": "该模型应用于EGS Collab实验1数据集，在所有目标上，1秒预测时程的R²值大于0.98，15秒预测时程的R²值大于0.88。模型还通过学习到的标准差项提供不确定性估计。", "conclusion": "这些准确且量化不确定性的预测能够实现裂缝扩展和渗透率演化的实时推断，展示了深度学习方法在改善地震风险评估和指导未来流体注入操作中缓解策略的巨大潜力。", "translation": "地下流体注入产生的微地震（MEQs）记录了储层应力状态和渗透率的演变。因此，预测其完整的时空演化对于增强型地热系统（EGS）、二氧化碳封存及其他地质工程应用至关重要。本文提出了一种基于Transformer的深度学习模型，该模型接收水力压裂历史和先前的微地震观测数据作为输入，以预测四个关键量：累积微地震计数、累积对数地震矩以及微地震云的第50和第95百分位范围（P50, P95）。该模型应用于EGS Collab实验1数据集，在所有目标上，1秒预测时程的R²值大于0.98，15秒预测时程的R²值大于0.88，并能通过学习到的标准差项提供不确定性估计。这些准确且量化不确定性的预测能够实现裂缝扩展和渗透率演化的实时推断，展示了深度学习方法在改善地震风险评估和指导未来流体注入操作中缓解策略的巨大潜力。", "summary": "本文提出了一种基于Transformer的深度学习模型，旨在预测流体诱发微地震（MEQs）的时空演化。该模型结合水力压裂历史和过往MEQ观测数据，能够预测MEQ的累积数量、累积对数地震矩以及MEQ云的50和95百分位范围。在EGS Collab实验1数据集上的应用结果显示，该模型在1秒预测时程的R²值超过0.98，在15秒预测时程的R²值超过0.88，并能提供不确定性估计。这项研究展示了深度学习在实时推断裂缝扩展和渗透率演化、改善地震风险评估以及指导未来流体注入操作中的巨大潜力。", "keywords": "微地震, 深度学习, 预测, 时空演化, 流体注入", "comments": "该论文的创新之处在于将Transformer模型应用于流体诱发微地震的时空演化预测，这是一个复杂的地球物理问题。模型在预测精度上取得了显著的R²值，并且能够提供不确定性估计，这对于地质工程中实时风险评估和缓解策略的实际应用具有重要意义。这代表了地质工程安全领域的一个有前景的进步。"}}
{"id": "2506.15346", "title": "Acoustic Waveform Inversion with Image-to-Image Schrödinger Bridges", "authors": ["A. S. Stankevich", "I. B. Petrov"], "summary": "Recent developments in application of deep learning models to acoustic Full\nWaveform Inversion (FWI) are marked by the use of diffusion models as prior\ndistributions for Bayesian-like inference procedures. The advantage of these\nmethods is the ability to generate high-resolution samples, which are otherwise\nunattainable with classical inversion methods or other deep learning-based\nsolutions. However, the iterative and stochastic nature of sampling from\ndiffusion models along with heuristic nature of output control remain limiting\nfactors for their applicability. For instance, an optimal way to include the\napproximate velocity model into diffusion-based inversion scheme remains\nunclear, even though it is considered an essential part of FWI pipeline. We\naddress the issue by employing a Schr\\\"odinger Bridge that interpolates between\nthe distributions of ground truth and smoothed velocity models. To facilitate\nthe learning of nonlinear drifts that transfer samples between distributions we\nextend the concept of Image-to-Image Schr\\\"odinger Bridge\n($\\text{I}^2\\text{SB}$) to conditional sampling, resulting in a conditional\nImage-to-Image Schr\\\"odinger Bridge (c$\\text{I}^2\\text{SB}$) framework. To\nvalidate our method, we assess its effectiveness in reconstructing the\nreference velocity model from its smoothed approximation, coupled with the\nobserved seismic signal of fixed shape. Our experiments demonstrate that the\nproposed solution outperforms our reimplementation of conditional diffusion\nmodel suggested in earlier works, while requiring only a few neural function\nevaluations (NFEs) to achieve sample fidelity superior to that attained with\nsupervised learning-based approach. The supplementary code implementing the\nalgorithms described in this paper can be found in the repository\nhttps://github.com/stankevich-mipt/seismic_inversion_via_I2SB.", "comment": "Submitted to \"Computational Mathematics And Mathematical Physics\",\n  ISSN 1555-6662, issue 8, August 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15346v1", "AI": {"title_translation": "基于图像到图像薛定谔桥的声波波形反演", "tldr": "本文提出了一种条件图像到图像薛定谔桥（cI2SB）框架，用于声波全波形反演（FWI），解决了现有扩散模型方法的局限性，并在性能和计算效率上超越了扩散模型和监督学习方法。", "motivation": "现有的基于深度学习的声波全波形反演（FWI）方法，特别是扩散模型，虽然能生成高分辨率样本，但其迭代和随机采样特性以及输出控制的启发式性质限制了其适用性。此外，如何有效地将近似速度模型纳入基于扩散的反演方案仍不明确，而这却是FWI流程中必不可少的部分。", "method": "本文通过采用薛定谔桥来解决上述问题，该桥在真实（ground truth）速度模型和平滑速度模型的分布之间进行插值。为了促进学习在分布之间转移样本的非线性漂移，作者将图像到图像薛定谔桥（I²SB）的概念扩展到条件采样，从而形成了条件图像到图像薛定谔桥（cI2SB）框架。", "result": "实验结果表明，所提出的cI2SB解决方案优于早期工作中提出的条件扩散模型的重新实现，并且仅需要少量神经函数评估（NFEs）即可实现优于监督学习方法的样本保真度。", "conclusion": "本文提出的条件图像到图像薛定谔桥（cI2SB）框架为声波全波形反演提供了一种有效且高效的解决方案，成功克服了现有扩散模型在迭代采样和结合近似速度模型方面的局限性，并在性能和计算效率上均展现出优越性。", "translation": "声波波形反演与图像到图像薛定谔桥\n\n深度学习模型应用于声波全波形反演（FWI）的最新进展以使用扩散模型作为贝叶斯类推理过程的先验分布为标志。这些方法的优点是能够生成高分辨率样本，这是经典反演方法或其他基于深度学习的解决方案无法实现的。然而，从扩散模型采样的迭代和随机性质以及输出控制的启发式性质仍然是其适用性的限制因素。例如，将近似速度模型纳入基于扩散的反演方案的最佳方式仍不清楚，尽管它被认为是FWI流程的重要组成部分。我们通过采用薛定谔桥来解决这个问题，该桥在真实（ground truth）和平滑速度模型的分布之间进行插值。为了促进学习在分布之间转移样本的非线性漂移，我们将图像到图像薛定谔桥（I²SB）的概念扩展到条件采样，从而形成了条件图像到图像薛定谔桥（cI2SB）框架。为了验证我们的方法，我们评估了其在结合固定形状的观测地震信号的情况下，从平滑近似中重建参考速度模型的有效性。我们的实验表明，所提出的解决方案优于我们重新实现的早期工作中提出的条件扩散模型，同时仅需要少量神经函数评估（NFEs）即可实现优于监督学习方法的样本保真度。实现本文所述算法的补充代码可在https://github.com/stankevich-mipt/seismic_inversion_via_I2SB仓库中找到。", "summary": "本文提出了一种用于声波全波形反演（FWI）的条件图像到图像薛定谔桥（cI2SB）框架，旨在解决现有深度学习方法（特别是扩散模型）在迭代采样和整合近似速度模型方面的局限性。cI2SB通过在真实和平滑速度模型分布之间插值来学习非线性漂移。实验证明，该方法在重建参考速度模型方面优于条件扩散模型和监督学习方法，并且所需计算量更少，样本保真度更高。", "keywords": "声波波形反演, 薛定谔桥, 深度学习, 扩散模型, 速度模型", "comments": "该论文的创新之处在于将薛定谔桥的概念引入到声波全波形反演（FWI）中，并提出了条件图像到图像薛定谔桥（cI2SB）框架。这有效地解决了传统扩散模型在FWI中面临的迭代效率低和难以有效整合先验信息（如近似速度模型）的问题。通过实现更高效且高保真度的样本生成，该方法为地球物理反演领域提供了新的思路和强大的工具，尤其是在处理高分辨率反演问题时。"}}
{"id": "2506.15596", "title": "Mono-Modalizing Extremely Heterogeneous Multi-Modal Medical Image Registration", "authors": ["Kyobin Choo", "Hyunkyung Han", "Jinyeong Kim", "Chanyong Yoon", "Seong Jae Hwang"], "summary": "In clinical practice, imaging modalities with functional characteristics,\nsuch as positron emission tomography (PET) and fractional anisotropy (FA), are\noften aligned with a structural reference (e.g., MRI, CT) for accurate\ninterpretation or group analysis, necessitating multi-modal deformable image\nregistration (DIR). However, due to the extreme heterogeneity of these\nmodalities compared to standard structural scans, conventional unsupervised DIR\nmethods struggle to learn reliable spatial mappings and often distort images.\nWe find that the similarity metrics guiding these models fail to capture\nalignment between highly disparate modalities. To address this, we propose\nM2M-Reg (Multi-to-Mono Registration), a novel framework that trains multi-modal\nDIR models using only mono-modal similarity while preserving the established\narchitectural paradigm for seamless integration into existing models. We also\nintroduce GradCyCon, a regularizer that leverages M2M-Reg's cyclic training\nscheme to promote diffeomorphism. Furthermore, our framework naturally extends\nto a semi-supervised setting, integrating pre-aligned and unaligned pairs only,\nwithout requiring ground-truth transformations or segmentation masks.\nExperiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset\ndemonstrate that M2M-Reg achieves up to 2x higher DSC than prior methods for\nPET-MRI and FA-MRI registration, highlighting its effectiveness in handling\nhighly heterogeneous multi-modal DIR. Our code is available at\nhttps://github.com/MICV-yonsei/M2M-Reg.", "comment": "11 pages, 3 figures, 2 tables, Accepted at Medical Image Computing\n  and Computer Assisted Intervention (MICCAI) 2025", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15596v1", "AI": {"title_translation": "极度异构多模态医学图像配准的单模态化", "tldr": "针对极度异构的多模态医学图像配准，本文提出了M2M-Reg框架，通过仅使用单模态相似性训练模型，并引入GradCyCon正则化器，显著提高了配准精度，解决了传统方法难以学习可靠空间映射的问题。", "motivation": "在临床实践中，功能性成像模态（如PET、FA）常需与结构性参考图像（如MRI、CT）进行对齐，以进行准确解释或群组分析，这需要多模态可变形图像配准（DIR）。然而，由于这些模态与标准结构扫描之间存在极度异构性，传统的无监督DIR方法难以学习可靠的空间映射，并且其相似性度量无法捕捉高度不同模态之间的对齐关系，导致图像失真。", "method": "本文提出了M2M-Reg（Multi-to-Mono Registration）框架。该框架通过仅使用单模态相似性来训练多模态DIR模型，同时保留了现有模型的架构范式以实现无缝集成。此外，引入了GradCyCon正则化器，该正则化器利用M2M-Reg的循环训练方案来促进微分同胚性。该框架还自然地扩展到半监督设置，仅整合预对齐和未对齐的图像对，无需真实变换或分割掩模。", "result": "在阿尔茨海默病神经影像学倡议（ADNI）数据集上的实验表明，M2M-Reg在PET-MRI和FA-MRI配准方面比现有方法取得了高达2倍的DSC（Dice相似系数）提升，突显了其在处理高度异构多模态DIR方面的有效性。", "conclusion": "M2M-Reg通过利用单模态相似性训练多模态配准模型并结合GradCyCon正则化器，有效解决了极度异构多模态医学图像配准的挑战，显著提高了配准精度，使其适用于临床应用。", "translation": "在临床实践中，具有功能特性的成像模态，例如正电子发射断层扫描（PET）和分数各向异性（FA），通常与结构参考（例如MRI、CT）对齐以进行准确解释或群组分析，这需要多模态可变形图像配准（DIR）。然而，由于这些模态与标准结构扫描相比存在极度异构性，传统的无监督DIR方法难以学习可靠的空间映射并经常扭曲图像。我们发现指导这些模型的相似性度量未能捕捉高度不同模态之间的对齐。为了解决这个问题，我们提出了M2M-Reg（多到单模态配准），一个新颖的框架，它仅使用单模态相似性来训练多模态DIR模型，同时保留已建立的架构范式以便无缝集成到现有模型中。我们还引入了GradCyCon，一个利用M2M-Reg循环训练方案来促进微分同胚性的正则化器。此外，我们的框架自然地扩展到半监督设置，仅集成预对齐和未对齐的图像对，而无需真实变换或分割掩模。在阿尔茨海默病神经影像学倡议（ADNI）数据集上的实验表明，M2M-Reg在PET-MRI和FA-MRI配准方面比现有方法取得了高达2倍的DSC，突出了其在处理高度异构多模态DIR方面的有效性。我们的代码可在https://github.com/MICV-yonsei/M2M-Reg获取。", "summary": "本文提出了一种名为M2M-Reg的新型框架，旨在解决极度异构多模态医学图像配准中传统方法面临的挑战。M2M-Reg通过仅利用单模态相似性来训练多模态可变形图像配准模型，并引入了GradCyCon正则化器以确保变换的平滑性。该方法无需真实变换或分割掩模，并可扩展至半监督学习。实验结果显示，M2M-Reg在PET-MRI和FA-MRI配准任务上显著优于现有方法，证明了其在处理高度异构数据方面的有效性。", "keywords": "多模态图像配准, 可变形图像配准, 医学图像处理, 单模态相似性, 深度学习", "comments": "本文的创新之处在于提出了一种独特的“多到单模态”训练范式，即在多模态配准任务中仅利用单模态相似性，这有效克服了传统相似性度量在处理极度异构模态时的局限性。此外，引入的GradCyCon正则化器通过循环训练促进了变换的微分同胚性，进一步提高了配准的可靠性。该方法能够无需地面真值变换或分割掩模，并支持半监督学习，使其在实际临床应用中更具普适性和实用价值。"}}
{"id": "2506.15610", "title": "BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion", "authors": ["Yuqing Lan", "Chenyang Zhu", "Zhirui Gao", "Jiazhao Zhang", "Yihan Cao", "Renjiao Yi", "Yijie Wang", "Kai Xu"], "summary": "Open-vocabulary 3D object detection has gained significant interest due to\nits critical applications in autonomous driving and embodied AI. Existing\ndetection methods, whether offline or online, typically rely on dense point\ncloud reconstruction, which imposes substantial computational overhead and\nmemory constraints, hindering real-time deployment in downstream tasks. To\naddress this, we propose a novel reconstruction-free online framework tailored\nfor memory-efficient and real-time 3D detection. Specifically, given streaming\nposed RGB-D video input, we leverage Cubify Anything as a pre-trained visual\nfoundation model (VFM) for single-view 3D object detection by bounding boxes,\ncoupled with CLIP to capture open-vocabulary semantics of detected objects. To\nfuse all detected bounding boxes across different views into a unified one, we\nemploy an association module for correspondences of multi-views and an\noptimization module to fuse the 3D bounding boxes of the same instance\npredicted in multi-views. The association module utilizes 3D Non-Maximum\nSuppression (NMS) and a box correspondence matching module, while the\noptimization module uses an IoU-guided efficient random optimization technique\nbased on particle filtering to enforce multi-view consistency of the 3D\nbounding boxes while minimizing computational complexity. Extensive experiments\non ScanNetV2 and CA-1M datasets demonstrate that our method achieves\nstate-of-the-art performance among online methods. Benefiting from this novel\nreconstruction-free paradigm for 3D object detection, our method exhibits great\ngeneralization abilities in various scenarios, enabling real-time perception\neven in environments exceeding 1000 square meters.", "comment": "11 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15610v1", "AI": {"title_translation": "BoxFusion：通过实时多视图边界框融合实现免重建开放词汇3D目标检测", "tldr": "BoxFusion提出了一种免重建的在线框架，利用多视图边界框融合实现实时的开放词汇3D目标检测，克服了现有方法的计算和内存限制。", "motivation": "现有的开放词汇3D目标检测方法通常依赖于密集的点云重建，这导致巨大的计算开销和内存限制，阻碍了在下游任务中的实时部署。", "method": "我们提出了一个新颖的免重建在线框架。该框架利用预训练的视觉基础模型Cubify Anything进行单视图3D目标检测，并结合CLIP捕获开放词汇语义。为了融合不同视图中检测到的边界框，我们采用一个关联模块（使用3D NMS和边界框对应匹配）和一个优化模块（使用基于粒子滤波的IoU引导高效随机优化技术）来融合同一实例在多视图中预测的3D边界框。", "result": "我们的方法在ScanNetV2和CA-1M数据集上实现了在线方法中的最先进性能。受益于新颖的免重建范式，我们的方法在各种场景中表现出强大的泛化能力，即使在超过1000平方米的环境中也能实现实时感知。", "conclusion": "该论文提出了一种创新的免重建在线3D目标检测范式，有效解决了现有方法的计算和内存瓶颈，实现了实时、高泛化能力的开放词汇3D目标检测。", "translation": "开放词汇3D目标检测因其在自动驾驶和具身AI中的关键应用而受到广泛关注。现有的检测方法，无论是离线还是在线，通常依赖于密集的点云重建，这带来了巨大的计算开销和内存限制，阻碍了在下游任务中的实时部署。为了解决这个问题，我们提出了一种新颖的免重建在线框架，专为内存高效和实时3D检测而设计。具体而言，给定流式姿态RGB-D视频输入，我们利用Cubify Anything作为预训练的视觉基础模型（VFM），通过边界框进行单视图3D目标检测，并结合CLIP捕获检测对象的开放词汇语义。为了将所有检测到的边界框跨不同视图融合成一个统一的边界框，我们采用了一个关联模块来处理多视图的对应关系，以及一个优化模块来融合在多视图中预测的同一实例的3D边界框。关联模块利用3D非最大抑制（NMS）和边界框对应匹配模块，而优化模块则使用基于粒子滤波的IoU引导高效随机优化技术，以强制执行3D边界框的多视图一致性，同时最小化计算复杂度。在ScanNetV2和CA-1M数据集上的大量实验表明，我们的方法在在线方法中实现了最先进的性能。受益于这种新颖的免重建3D目标检测范式，我们的方法在各种场景中表现出强大的泛化能力，即使在超过1000平方米的环境中也能实现实时感知。", "summary": "BoxFusion提出了一种新颖的免重建在线框架，用于开放词汇3D目标检测，旨在解决传统方法中密集点云重建带来的计算和内存限制。该框架利用Cubify Anything和CLIP进行单视图检测和语义理解，并通过一个关联模块和优化模块将多视图边界框高效融合。实验证明，该方法在ScanNetV2和CA-1M数据集上达到了最先进的性能，并展现出优异的泛化能力和实时感知能力，即使在大型环境中也能有效运作。", "keywords": "开放词汇3D目标检测, 免重建, 多视图融合, 实时感知, 边界框检测", "comments": "BoxFusion的创新点在于其提出的免重建范式，这显著降低了3D目标检测的计算和内存需求，使其更适合实时部署。结合预训练视觉基础模型和高效的多视图边界框融合策略，为开放词汇3D检测提供了一个有前景的解决方案，尤其是在自主驾驶和具身AI等对实时性要求高的应用中。"}}
{"id": "2506.15378", "title": "Sampling 3D Molecular Conformers with Diffusion Transformers", "authors": ["J. Thorben Frank", "Winfried Ripken", "Gregor Lied", "Klaus-Robert Müller", "Oliver T. Unke", "Stefan Chmiela"], "summary": "Diffusion Transformers (DiTs) have demonstrated strong performance in\ngenerative modeling, particularly in image synthesis, making them a compelling\nchoice for molecular conformer generation. However, applying DiTs to molecules\nintroduces novel challenges, such as integrating discrete molecular graph\ninformation with continuous 3D geometry, handling Euclidean symmetries, and\ndesigning conditioning mechanisms that generalize across molecules of varying\nsizes and structures. We propose DiTMC, a framework that adapts DiTs to address\nthese challenges through a modular architecture that separates the processing\nof 3D coordinates from conditioning on atomic connectivity. To this end, we\nintroduce two complementary graph-based conditioning strategies that integrate\nseamlessly with the DiT architecture. These are combined with different\nattention mechanisms, including both standard non-equivariant and\nSO(3)-equivariant formulations, enabling flexible control over the trade-off\nbetween between accuracy and computational efficiency. Experiments on standard\nconformer generation benchmarks (GEOM-QM9, -DRUGS, -XL) demonstrate that DiTMC\nachieves state-of-the-art precision and physical validity. Our results\nhighlight how architectural choices and symmetry priors affect sample quality\nand efficiency, suggesting promising directions for large-scale generative\nmodeling of molecular structures. Code available at\nhttps://github.com/ML4MolSim/dit_mc.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15378v1", "AI": {"title_translation": "使用扩散Transformer采样三维分子构象", "tldr": "DiTMC框架通过模块化架构和图基条件策略，将扩散Transformer应用于分子构象生成，在标准基准测试中实现了最先进的精度和物理有效性。", "motivation": "扩散Transformer（DiTs）在生成建模中表现出色，特别是在图像合成方面，使其成为分子构象生成的有力选择。然而，将DiTs应用于分子会带来新的挑战，例如整合离散分子图信息与连续三维几何、处理欧几里得对称性，以及设计能够推广到不同大小和结构的分子上的条件机制。", "method": "我们提出了DiTMC框架，通过模块化架构分离三维坐标处理与原子连接条件，以解决上述挑战。为此，我们引入了两种与DiT架构无缝集成的互补图基条件策略。这些策略与不同的注意力机制（包括标准非等变和SO(3)等变公式）相结合，从而灵活控制精度和计算效率之间的权衡。", "result": "在标准构象生成基准测试（GEOM-QM9、-DRUGS、-XL）上的实验表明，DiTMC实现了最先进的精度和物理有效性。", "conclusion": "我们的结果突出显示了架构选择和对称先验如何影响样本质量和效率，为分子结构的大规模生成建模提供了有前景的方向。", "translation": "扩散Transformer（DiTs）在生成建模中表现出强大的性能，特别是在图像合成方面，使其成为分子构象生成的有力选择。然而，将DiTs应用于分子会引入新的挑战，例如将离散分子图信息与连续三维几何相结合，处理欧几里得对称性，以及设计能够推广到不同大小和结构的分子上的条件机制。我们提出了DiTMC，一个通过模块化架构适应DiTs以解决这些挑战的框架，该架构将三维坐标的处理与原子连接的条件处理分离。为此，我们引入了两种与DiT架构无缝集成的互补图基条件策略。这些策略与不同的注意力机制相结合，包括标准非等变和SO(3)等变公式，从而灵活控制精度和计算效率之间的权衡。在标准构象生成基准测试（GEOM-QM9、-DRUGS、-XL）上的实验表明，DiTMC实现了最先进的精度和物理有效性。我们的结果突出显示了架构选择和对称先验如何影响样本质量和效率，为分子结构的大规模生成建模提供了有前景的方向。代码可在https://github.com/ML4MolSim/dit_mc获取。", "summary": "该论文提出了DiTMC框架，旨在将扩散Transformer（DiTs）应用于三维分子构象生成，以解决整合离散图信息、处理欧几里得对称性和泛化能力等挑战。DiTMC采用模块化架构，分离三维坐标处理与原子连接条件，并引入两种图基条件策略和灵活的注意力机制（包括SO(3)等变）。实验证明，DiTMC在分子构象生成基准测试中达到了最先进的精度和物理有效性，并揭示了架构选择和对称性对样本质量和效率的影响，为大规模分子结构生成建模指明了方向。", "keywords": "扩散Transformer, 分子构象生成, DiTMC, 三维几何, SO(3)等变性", "comments": "该论文的创新点在于成功地将扩散Transformer应用于分子构象生成这一复杂任务，并通过DiTMC框架解决了分子特有的挑战。其模块化设计和图基条件策略是关键创新，尤其是在处理离散图信息和连续几何方面。引入SO(3)等变注意力机制，实现了精度和计算效率的权衡，这对于实际应用非常重要。研究结果表明了DiTMC的有效性，并为未来大规模分子生成建模提供了有价值的见解。"}}
{"id": "2506.15625", "title": "HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization", "authors": ["Roey Ron", "Guy Tevet", "Haim Sawdayee", "Amit H. Bermano"], "summary": "We present HOIDiNi, a text-driven diffusion framework for synthesizing\nrealistic and plausible human-object interaction (HOI). HOI generation is\nextremely challenging since it induces strict contact accuracies alongside a\ndiverse motion manifold. While current literature trades off between realism\nand physical correctness, HOIDiNi optimizes directly in the noise space of a\npretrained diffusion model using Diffusion Noise Optimization (DNO), achieving\nboth. This is made feasible thanks to our observation that the problem can be\nseparated into two phases: an object-centric phase, primarily making discrete\nchoices of hand-object contact locations, and a human-centric phase that\nrefines the full-body motion to realize this blueprint. This structured\napproach allows for precise hand-object contact without compromising motion\nnaturalness. Quantitative, qualitative, and subjective evaluations on the GRAB\ndataset alone clearly indicate HOIDiNi outperforms prior works and baselines in\ncontact accuracy, physical validity, and overall quality. Our results\ndemonstrate the ability to generate complex, controllable interactions,\nincluding grasping, placing, and full-body coordination, driven solely by\ntextual prompts. https://hoidini.github.io.", "comment": "Project page: https://hoidini.github.io", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15625v1", "AI": {"title_translation": "HOIDiNi：通过扩散噪声优化实现人-物交互", "tldr": "HOIDiNi是一个文本驱动的扩散框架，通过直接在预训练扩散模型的噪声空间中优化，实现了真实且合理的人-物交互生成，解决了现有方法在真实性和物理正确性之间的权衡问题。", "motivation": "人-物交互（HOI）生成极具挑战性，因为它需要严格的接触精度以及多样化的运动流形。现有的文献在真实性和物理正确性之间进行权衡。", "method": "HOIDiNi通过使用扩散噪声优化（DNO）直接在预训练扩散模型的噪声空间中进行优化。其方法分为两个阶段：以物体为中心的阶段（主要选择手-物体接触位置）和以人为中心的阶段（细化全身运动以实现蓝图）。这种结构化方法可以在不影响运动自然性的情况下实现精确的手-物体接触。", "result": "在GRAB数据集上的定量、定性和主观评估表明，HOIDiNi在接触精度、物理有效性和整体质量方面均优于现有工作和基线。结果证明了其能够仅由文本提示驱动生成复杂、可控的交互，包括抓取、放置和全身协调。", "conclusion": "HOIDiNi通过其独特的扩散噪声优化和两阶段结构化方法，成功解决了人-物交互生成的挑战，实现了高真实性和物理正确性的人-物交互合成。", "translation": "我们提出了HOIDiNi，一个文本驱动的扩散框架，用于合成真实且合理的人-物交互（HOI）。HOI生成极具挑战性，因为它需要严格的接触精度以及多样化的运动流形。虽然当前的文献在真实性和物理正确性之间进行权衡，但HOIDiNi通过使用扩散噪声优化（DNO）直接在预训练扩散模型的噪声空间中进行优化，从而同时实现了这两者。这得益于我们的观察，即该问题可以分为两个阶段：以物体为中心的阶段，主要进行手-物体接触位置的离散选择；以及以人为中心的阶段，细化全身运动以实现此蓝图。这种结构化方法可以在不影响运动自然性的情况下实现精确的手-物体接触。仅在GRAB数据集上进行的定量、定性和主观评估清楚地表明，HOIDiNi在接触精度、物理有效性和整体质量方面均优于现有工作和基线。我们的结果证明了其能够仅由文本提示驱动生成复杂、可控的交互，包括抓取、放置和全身协调。https://hoidini.github.io。", "summary": "HOIDiNi是一个新颖的文本驱动扩散框架，专门用于生成真实且物理正确的人-物交互。它通过在预训练扩散模型的噪声空间中进行优化，并采用独特的两阶段方法——先确定物体接触点，再细化全身运动——克服了现有方法在真实性和物理正确性上的权衡。实验结果表明，HOIDiNi在接触精度、物理有效性和整体质量方面显著优于现有技术，能够仅通过文本提示生成复杂的、可控的交互。", "keywords": "人-物交互, 扩散模型, 噪声优化, 文本驱动, 运动生成", "comments": "HOIDiNi的创新之处在于其将人-物交互生成问题分解为离散的物体接触选择和连续的全身运动细化两个阶段，并结合了扩散噪声优化，有效地解决了现有方法在真实性和物理正确性之间的矛盾。这种方法对于需要高精度物理交互的虚拟环境、机器人学和动画生成领域具有重要意义。"}}
{"id": "2506.15383", "title": "Global Ground Metric Learning with Applications to scRNA data", "authors": ["Damin Kühn", "Michael T. Schaub"], "summary": "Optimal transport provides a robust framework for comparing probability\ndistributions. Its effectiveness is significantly influenced by the choice of\nthe underlying ground metric. Traditionally, the ground metric has either been\n(i) predefined, e.g., as the Euclidean distance, or (ii) learned in a\nsupervised way, by utilizing labeled data to learn a suitable ground metric for\nenhanced task-specific performance. Yet, predefined metrics typically cannot\naccount for the inherent structure and varying importance of different features\nin the data, and existing supervised approaches to ground metric learning often\ndo not generalize across multiple classes or are restricted to distributions\nwith shared supports. To address these limitations, we propose a novel approach\nfor learning metrics for arbitrary distributions over a shared metric space.\nOur method provides a distance between individual points like a global metric,\nbut requires only class labels on a distribution-level for training. The\nlearned global ground metric enables more accurate optimal transport distances,\nleading to improved performance in embedding, clustering and classification\ntasks. We demonstrate the effectiveness and interpretability of our approach\nusing patient-level scRNA-seq data spanning multiple diseases.", "comment": "This method is provided as a Python package on PyPI, see\n  https://github.com/DaminK/ggml-ot", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15383v1", "AI": {"title_translation": "全局基础度量学习及其在单细胞RNA数据中的应用", "tldr": "本文提出一种新的全局基础度量学习方法，旨在解决最优传输中基础度量选择的局限性，通过仅使用分布级标签来学习全局度量，显著提升了嵌入、聚类和分类任务的性能，并在scRNA数据上得到验证。", "motivation": "最优传输的有效性受基础度量选择影响。传统预定义度量无法捕获数据结构和特征重要性，而现有监督学习方法泛化性差或受限于共享支持集。", "method": "提出一种新颖的方法，用于在共享度量空间中学习任意分布的度量。该方法能够提供个体点之间的距离（如全局度量），但训练时仅需分布级别的类别标签。", "result": "学习到的全局基础度量能够实现更准确的最优传输距离，从而显著提升嵌入、聚类和分类任务的性能。该方法在跨多种疾病的患者级scRNA-seq数据上展示了其有效性和可解释性。", "conclusion": "本文提出了一种创新的全局基础度量学习方法，有效克服了传统方法的局限性，显著提高了最优传输在嵌入、聚类和分类任务中的表现，并特别适用于复杂的生物医学数据如scRNA。", "translation": "最优传输提供了一个比较概率分布的强大框架。其有效性受到底层基础度量选择的显著影响。传统上，基础度量要么是预定义的，例如欧氏距离，要么是以监督方式学习的，通过利用标记数据学习一个合适的基础度量以增强特定任务的性能。然而，预定义度量通常无法解释数据中固有的结构和不同特征的不同重要性，现有的监督式基础度量学习方法往往不能泛化到多个类别，或者局限于具有共享支持的分布。为了解决这些局限性，我们提出了一种新的方法，用于在共享度量空间中学习任意分布的度量。我们的方法提供了像全局度量一样的个体点之间的距离，但训练时仅需要分布级别的类别标签。学习到的全局基础度量能够实现更准确的最优传输距离，从而提高嵌入、聚类和分类任务的性能。我们使用跨多种疾病的患者级scRNA-seq数据证明了我们方法的有效性和可解释性。", "summary": "本文针对最优传输中基础度量选择的现有局限性，提出了一种新颖的全局基础度量学习方法。该方法允许在共享度量空间中学习任意分布的度量，其独特之处在于仅需分布级别的类别标签进行训练，即可获得点之间的全局距离。研究表明，通过学习到的全局基础度量，能显著提升最优传输距离的准确性，进而提高数据嵌入、聚类和分类任务的性能。该方法在患者级scRNA-seq数据上的应用，进一步验证了其有效性和可解释性。", "keywords": "最优传输, 基础度量学习, 单细胞RNA数据, 距离度量, 机器学习", "comments": "该研究创新性地解决了最优传输中基础度量选择的难题，特别是在缺乏个体点标签时仍能通过分布级别标签进行全局度量学习，这对于处理如scRNA-seq等复杂生物医学数据具有重要意义。其提出的方法不仅提升了最优传输的准确性，还扩展了其应用场景，具有较高的理论和实践价值。"}}
{"id": "2506.15385", "title": "Provable Maximum Entropy Manifold Exploration via Diffusion Models", "authors": ["Riccardo De Santi", "Marin Vlastelica", "Ya-Ping Hsieh", "Zebang Shen", "Niao He", "Andreas Krause"], "summary": "Exploration is critical for solving real-world decision-making problems such\nas scientific discovery, where the objective is to generate truly novel designs\nrather than mimic existing data distributions. In this work, we address the\nchallenge of leveraging the representational power of generative models for\nexploration without relying on explicit uncertainty quantification. We\nintroduce a novel framework that casts exploration as entropy maximization over\nthe approximate data manifold implicitly defined by a pre-trained diffusion\nmodel. Then, we present a novel principle for exploration based on density\nestimation, a problem well-known to be challenging in practice. To overcome\nthis issue and render this method truly scalable, we leverage a fundamental\nconnection between the entropy of the density induced by a diffusion model and\nits score function. Building on this, we develop an algorithm based on mirror\ndescent that solves the exploration problem as sequential fine-tuning of a\npre-trained diffusion model. We prove its convergence to the optimal\nexploratory diffusion model under realistic assumptions by leveraging recent\nunderstanding of mirror flows. Finally, we empirically evaluate our approach on\nboth synthetic and high-dimensional text-to-image diffusion, demonstrating\npromising results.", "comment": "ICML 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15385v1", "AI": {"title_translation": "可证明的最大熵流形探索通过扩散模型", "tldr": "本文提出一种利用扩散模型进行可证明的最大熵探索框架，通过将探索视为预训练扩散模型隐式定义的数据流形上的熵最大化问题，并基于镜像下降算法进行序贯微调，实现生成新颖设计的目标。", "motivation": "探索对于解决现实世界的决策问题（如科学发现）至关重要，其目标是生成真正新颖的设计而非模仿现有数据分布。目前面临的挑战是如何利用生成模型的表示能力进行探索，同时避免依赖显式的不确定性量化。", "method": "作者提出一个新颖的框架，将探索转化为预训练扩散模型隐式定义的近似数据流形上的熵最大化问题。为了克服密度估计的挑战并实现可扩展性，该方法利用扩散模型诱导的密度熵与其得分函数之间的基本联系。在此基础上，开发了一种基于镜像下降的算法，通过对预训练扩散模型进行序贯微调来解决探索问题。", "result": "该方法在合成数据和高维文本到图像扩散任务上进行了实证评估，展示了有前景的结果。研究还通过利用镜像流的最新理解，证明了其在现实假设下收敛到最优探索性扩散模型。", "conclusion": "本文提出了一种可证明的最大熵流形探索方法，利用扩散模型进行新颖设计生成。该方法通过将探索问题转化为熵最大化，并利用扩散模型的特性和镜像下降算法实现，理论上保证了收敛性，并在实践中表现出良好效果。", "translation": "探索对于解决现实世界的决策问题（例如科学发现）至关重要，其目标是生成真正新颖的设计，而不是模仿现有数据分布。在这项工作中，我们解决了利用生成模型的表示能力进行探索而无需依赖显式不确定性量化的挑战。我们引入了一个新颖的框架，将探索铸造成在预训练扩散模型隐式定义的近似数据流形上的熵最大化问题。然后，我们提出了一种基于密度估计的新探索原则，这是一个在实践中众所周知的具有挑战性的问题。为了克服这个问题并使该方法真正可扩展，我们利用了扩散模型诱导的密度熵与其得分函数之间的基本联系。在此基础上，我们开发了一种基于镜像下降的算法，将探索问题作为预训练扩散模型的序贯微调来解决。通过利用对镜像流的最新理解，我们证明了其在现实假设下收敛到最优探索性扩散模型。最后，我们在合成数据和高维文本到图像扩散任务上对我们的方法进行了实证评估，展示了有前景的结果。", "summary": "本文提出了一种名为“可证明的最大熵流形探索”的新型框架，旨在通过扩散模型生成新颖数据而非简单复制现有数据。该方法将探索定义为预训练扩散模型所定义的近似数据流形上的熵最大化问题。为解决密度估计的难题并确保可扩展性，研究利用了扩散模型密度熵与得分函数间的关联。在此基础上，开发了一种基于镜像下降的算法，通过对扩散模型进行序贯微调来实现探索。该算法在理论上被证明在现实假设下能收敛到最优探索模型，并在合成数据和文本到图像生成任务中取得了显著效果。", "keywords": "扩散模型, 熵最大化, 流形探索, 镜像下降, 新颖设计生成", "comments": "本文的创新之处在于将探索问题巧妙地转化为扩散模型隐式流形上的熵最大化，并提出了一个理论上可证明收敛的算法。其利用扩散模型得分函数与熵的联系，有效规避了显式不确定性量化的难题，提升了方法的可扩展性。对于科学发现和新颖设计生成等领域具有重要意义。"}}
{"id": "2506.15645", "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language Models", "authors": ["Shuo Xing", "Lanqing Guo", "Hongyuan Hua", "Seoyoung Lee", "Peiran Li", "Yufei Wang", "Zhangyang Wang", "Zhengzhong Tu"], "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.", "comment": "18 pages", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15645v1", "AI": {"title_translation": "多模态大语言模型中视觉质量悖论的揭秘", "tldr": "研究发现多模态大语言模型在视觉质量下降时性能反而可能提升，提出VQ-TTT方法以自适应调整输入图像，显著提升模型准确性，重新定义了MLLM的“更好”视觉输入。", "motivation": "尽管多模态大语言模型（MLLMs）在视觉-语言基准任务上表现出色，但目前尚不清楚输入视觉质量如何影响它们的响应。具体来说，研究旨在探究图像更高的感知质量是否能转化为更好的MLLM理解，并发现现有研究对此知之甚少。", "method": "研究首先对主流MLLMs和一系列视觉-语言基准任务进行了首次系统性研究，对每张图像应用了受控的降级和风格转变。为了解决发现的视觉质量悖论，研究引入了视觉质量测试时调优（VQ-TTT）模块。VQ-TTT是一种轻量级适应模块，它在冻结的视觉编码器之前插入一个可学习的低秩核来调节频率内容，并通过LoRA仅微调浅层视觉编码器层。VQ-TTT能够在单次前向传播中动态调整每个输入图像，使其与特定任务的模型偏好对齐。", "result": "研究发现了一个视觉质量悖论：当图像偏离人类感知的保真度时，模型、任务甚至单个实例的性能都可能提升。现成的修复管道无法调和这些特有的偏好。引入VQ-TTT后，在所有评估的MLLMs和所有数据集上，VQ-TTT在没有外部模型、缓存特征或额外训练数据的情况下，显著提升了平均准确性。", "conclusion": "这些发现重新定义了多模态大语言模型（MLLMs）的“更好”视觉输入，并强调在新时代，AI作为主要数据使用者，需要自适应而非普遍“干净”的图像。", "translation": "最近的多模态大语言模型（MLLMs）在基准视觉-语言任务上表现出色，但关于输入视觉质量如何影响其响应知之甚少。图像更高的感知质量是否能转化为更好的MLLM理解？我们进行了首次系统性研究，涵盖了主流MLLMs和一系列视觉-语言基准，对每张图像应用了受控的降级和风格转变。令人惊讶的是，我们发现了一个视觉质量悖论：当图像偏离人类感知的保真度时，模型、任务甚至单个实例的性能都可能提升。现成的修复管道无法调和这些特有的偏好。为了弥补这一差距，我们引入了视觉质量测试时调优（VQ-TTT）——一个轻量级适应模块，它：（1）在冻结的视觉编码器之前插入一个可学习的低秩核来调节频率内容；（2）仅通过LoRA微调浅层视觉编码器层。VQ-TTT在单次前向传播中动态调整每个输入图像，使其与特定任务的模型偏好对齐。在所有评估的MLLMs和所有数据集上，VQ-TTT在没有外部模型、缓存特征或额外训练数据的情况下，显著提升了平均准确性。这些发现重新定义了MLLMs的“更好”视觉输入，并强调在新时代，AI作为主要数据使用者，需要自适应而非普遍“干净”的图像。", "summary": "本研究揭示了多模态大语言模型（MLLMs）中一个反直觉的视觉质量悖论：图像的低感知质量有时反而能提升模型性能。通过对主流MLLMs进行系统性研究和受控图像降级，发现模型、任务甚至个体实例的表现可能在图像偏离人类感知保真度时得到改善。针对这一现象，论文提出了一种名为视觉质量测试时调优（VQ-TTT）的轻量级适应模块。VQ-TTT通过在视觉编码器前插入可学习的低秩核以及微调浅层视觉编码器层，实现对输入图像的动态调整，以适应特定任务的模型偏好。实验结果表明，VQ-TTT显著提升了评估的MLLMs在所有数据集上的平均准确性，且无需额外模型或数据。这些发现重新定义了MLLMs的“更好”视觉输入，并强调了对自适应而非普遍“干净”图像的需求。", "keywords": "多模态大语言模型, 视觉质量悖论, 测试时调优, 图像自适应, 视觉理解", "comments": "这项研究具有创新性，它挑战了传统观念中“更高视觉质量等同于更好模型理解”的假设，揭示了多模态大语言模型中存在的“视觉质量悖论”。这一发现对于理解和优化MLLMs的输入处理至关重要。提出的VQ-TTT方法是一种高效且轻量级的解决方案，通过测试时自适应调整图像，有效弥合了模型对图像质量的“特有偏好”与人类感知之间的差距。这为未来设计更鲁棒、更高效的MLLMs提供了新的视角和实用工具，尤其是在AI日益成为数据主要消费者的背景下，强调了自适应图像处理的重要性。"}}
{"id": "2506.15649", "title": "Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning", "authors": ["Ankan Deria", "Adinath Madhavrao Dukre", "Feilong Tang", "Sara Atito", "Sudipta Roy", "Muhammad Awais", "Muhammad Haris Khan", "Imran Razzak"], "summary": "Despite significant advances in inference-time search for vision-language\nmodels (VLMs), existing approaches remain both computationally expensive and\nprone to unpenalized, low-confidence generations which often lead to persistent\nhallucinations. We introduce \\textbf{Value-guided Inference with Margin-based\nReward (ViMaR)}, a two-stage inference framework that improves both efficiency\nand output fidelity by combining a temporal-difference value model with a\nmargin-aware reward adjustment. In the first stage, we perform a single pass to\nidentify the highest-value caption among diverse candidates. In the second\nstage, we selectively refine only those segments that were overlooked or\nexhibit weak visual grounding, thereby eliminating frequently rewarded\nevaluations. A calibrated margin-based penalty discourages low-confidence\ncontinuations while preserving descriptive richness. Extensive experiments\nacross multiple VLM architectures demonstrate that ViMaR generates captions\nthat are significantly more reliable, factually accurate, detailed, and\nexplanatory, while achieving over 4$\\times$ speedup compared to existing\nvalue-guided methods. Specifically, we show that ViMaR trained solely on LLaVA\nMistral-7B, \\textit{generalizes effectively to guide decoding in a stronger\nunseen model}. To further validate this, we adapt the ViMaR to steer generation\nin LLaVA-OneVision-Qwen2-7B, leading to consistent improvements in caption\nquality and demonstrating robust cross-model guidance. This cross-model\ngeneralization highlights ViMaR's flexibility and modularity, positioning it as\na scalable and transferable inference-time decoding strategy. Furthermore, when\nViMaR-generated captions are used for self-training, the underlying models\nachieve substantial gains across a broad suite of visual comprehension\nbenchmarks, underscoring the potential of fast, accurate, and self-improving\nVLM pipelines.", "comment": null, "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15649v1", "AI": {"title_translation": "双阶段价值引导推理与基于边距的奖励调整，用于快速忠实的VLM字幕生成", "tldr": "提出ViMaR，一个双阶段推理框架，通过价值模型和边距奖励调整，显著提高VLM字幕生成的效率和准确性，同时实现跨模型泛化和自训练提升。", "motivation": "现有VLM推理搜索方法计算成本高，且易产生未受惩罚的低置信度生成，导致持续幻觉。", "method": "引入ViMaR（Value-guided Inference with Margin-based Reward），一个双阶段推理框架。第一阶段单次遍历识别最高价值的候选字幕；第二阶段选择性地细化被忽略或视觉基础薄弱的部分，并使用校准的基于边距的惩罚来抑制低置信度延续。结合了时序差分价值模型和边距感知奖励调整。", "result": "ViMaR生成的字幕更可靠、事实更准确、更详细、更具解释性，速度比现有价值引导方法快4倍以上。在LLaVA Mistral-7B上训练的ViMaR能有效指导更强的未见模型（LLaVA-OneVision-Qwen2-7B）的解码，实现跨模型泛化。当ViMaR生成的字幕用于自训练时，底层模型在视觉理解基准上获得显著提升。", "conclusion": "ViMaR是一个可扩展、可迁移的推理时解码策略，通过快速、准确和自改进的VLM管道，在字幕质量和模型性能上都带来了显著提升。", "translation": "尽管视觉语言模型（VLM）的推理时搜索取得了显著进展，但现有方法仍然计算成本高昂，并且容易产生未受惩罚的低置信度生成，这常常导致持续的幻觉。我们引入了**基于边距奖励的价值引导推理（ViMaR）**，这是一个双阶段推理框架，通过结合时序差分价值模型和边距感知奖励调整，提高了效率和输出保真度。在第一阶段，我们进行单次遍历，从多样化的候选字幕中识别出最高价值的字幕。在第二阶段，我们只选择性地细化那些被忽略或表现出弱视觉基础的部分，从而消除了频繁奖励的评估。校准的基于边距的惩罚会阻止低置信度的延续，同时保留描述的丰富性。对多种VLM架构的广泛实验表明，ViMaR生成的字幕显著更可靠、事实更准确、更详细、更具解释性，同时比现有价值引导方法实现了超过4倍的速度提升。具体而言，我们展示了仅在LLaVA Mistral-7B上训练的ViMaR能够**有效泛化以指导更强的未见模型的解码**。为了进一步验证这一点，我们调整了ViMaR以引导LLaVA-OneVision-Qwen2-7B中的生成，从而持续提高了字幕质量，并展示了强大的跨模型指导能力。这种跨模型泛化突出了ViMaR的灵活性和模块性，使其成为一种可扩展和可迁移的推理时解码策略。此外，当ViMaR生成的字幕用于自训练时，底层模型在一系列广泛的视觉理解基准上取得了实质性进展，这强调了快速、准确和自改进的VLM管道的潜力。", "summary": "本文提出了ViMaR，一个双阶段推理框架，旨在解决现有VLM字幕生成中效率低和幻觉问题。ViMaR结合了时序差分价值模型和边距感知奖励调整，通过两阶段过程（识别高价值字幕和选择性细化）及基于边距的惩罚来提高输出质量和效率。实验证明，ViMaR在字幕可靠性、准确性和细节方面表现优异，速度提升超4倍，并展现出强大的跨模型泛化能力和通过自训练提升基础模型性能的潜力。", "keywords": "价值引导推理, 视觉语言模型, 字幕生成, 边距奖励, 跨模型泛化", "comments": "这篇论文的创新点在于提出了一个双阶段的推理框架ViMaR，结合了价值引导和边距奖励调整，有效地解决了VLM字幕生成中的效率和幻觉问题。其关键创新包括：1) 双阶段的精炼过程，避免了不必要的计算；2) 基于边距的奖励调整，有效抑制低置信度生成；3) 强调了其跨模型泛化能力，这一点对于实际应用非常重要，意味着其作为一种通用解码策略的潜力；4) 证明了其生成的字幕可用于自训练，进一步提升基础模型性能，这暗示了构建更高效、自进化的VLM管道的可能性。"}}
{"id": "2506.15408", "title": "Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI", "authors": ["David Dembinsky", "Adriano Lucieri", "Stanislav Frolov", "Hiba Najjar", "Ko Watanabe", "Andreas Dengel"], "summary": "Modern AI systems frequently rely on opaque black-box models, most notably\nDeep Neural Networks, whose performance stems from complex architectures with\nmillions of learned parameters. While powerful, their complexity poses a major\nchallenge to trustworthiness, particularly due to a lack of transparency.\nExplainable AI (XAI) addresses this issue by providing human-understandable\nexplanations of model behavior. However, to ensure their usefulness and\ntrustworthiness, such explanations must be rigorously evaluated. Despite the\ngrowing number of XAI methods, the field lacks standardized evaluation\nprotocols and consensus on appropriate metrics. To address this gap, we conduct\na systematic literature review following the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a\nunified framework for the eValuation of XAI (VXAI). We identify 362 relevant\npublications and aggregate their contributions into 41 functionally similar\nmetric groups. In addition, we propose a three-dimensional categorization\nscheme spanning explanation type, evaluation contextuality, and explanation\nquality desiderata. Our framework provides the most comprehensive and\nstructured overview of VXAI to date. It supports systematic metric selection,\npromotes comparability across methods, and offers a flexible foundation for\nfuture extensions.", "comment": "Submitted to TMLR, under review", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15408v1", "AI": {"title_translation": "统一VXAI：可解释人工智能评估的系统综述与框架", "tldr": "本文通过系统综述，提出了一个统一的VXAI框架，以解决当前可解释人工智能（XAI）评估中缺乏标准化协议和共识的问题，并对现有评估指标进行了聚合和分类。", "motivation": "现代AI系统（尤其是深度神经网络）的复杂性导致其缺乏透明度，影响了可信度。尽管可解释人工智能（XAI）旨在提供人类可理解的解释，但该领域缺乏标准化的评估协议和对适当指标的共识，这阻碍了XAI解释的有用性和可信度。", "method": "本文遵循PRISMA指南进行了一项系统文献综述，并在此基础上提出了一个统一的可解释人工智能评估框架（VXAI）。研究识别了362篇相关出版物，并将其贡献整合为41个功能相似的指标组。此外，还提出了一个涵盖解释类型、评估情境性和解释质量要求的三个维度分类方案。", "result": "研究识别了362篇相关出版物，并将其贡献聚合为41个功能相似的指标组。此外，提出了一种涵盖解释类型、评估情境性和解释质量要求的三个维度分类方案。", "conclusion": "所提出的VXAI框架是迄今为止对可解释人工智能评估最全面、结构化的概述。它支持系统化的指标选择，促进了不同方法之间的可比性，并为未来的扩展提供了灵活的基础。", "translation": "现代AI系统经常依赖不透明的黑盒模型，最著名的是深度神经网络，其性能源于具有数百万个学习参数的复杂架构。虽然功能强大，但它们的复杂性对可信度构成了重大挑战，特别是由于缺乏透明度。可解释人工智能（XAI）通过提供人类可理解的模型行为解释来解决这个问题。然而，为了确保其有用性和可信度，此类解释必须经过严格评估。尽管XAI方法数量不断增长，但该领域缺乏标准化的评估协议和对适当指标的共识。为了弥补这一空白，我们遵循系统综述和荟萃分析优先报告项目（PRISMA）指南进行了一项系统文献综述，并引入了一个统一的可解释人工智能评估框架（VXAI）。我们识别了362篇相关出版物，并将其贡献聚合为41个功能相似的指标组。此外，我们提出了一个涵盖解释类型、评估情境性和解释质量要求的三个维度分类方案。我们的框架提供了迄今为止对VXAI最全面和结构化的概述。它支持系统化的指标选择，促进了不同方法之间的可比性，并为未来的扩展提供了灵活的基础。", "summary": "本文通过对可解释人工智能（XAI）评估领域进行系统综述，旨在解决现有评估协议和指标缺乏标准化的挑战。研究识别了362篇相关文献，并将其评估指标归纳为41个类别，同时提出了一个基于解释类型、评估情境和解释质量的三维分类方案。最终，本文提出了一个统一的VXAI框架，该框架被认为是目前最全面、结构化的评估概述，能够支持指标选择、增强方法间可比性，并为未来研究奠定基础。", "keywords": "可解释人工智能, XAI, 评估框架, 系统综述, VXAI", "comments": "本文通过系统综述构建了一个统一的VXAI评估框架，创新性地解决了XAI领域评估标准缺失的问题。其对现有文献的全面梳理和指标的聚合分类，为XAI的未来研究和应用提供了重要的结构化基础。该框架有望提高XAI方法评估的严谨性和可比性，对于推动XAI的可信度和实用性具有重要意义。"}}
{"id": "2506.15041", "title": "Identifying economic narratives in large text corpora -- An integrated approach using Large Language Models", "authors": ["Tobias Schmidt", "Kai-Robin Lange", "Matthias Reccius", "Henrik Müller", "Michael Roos", "Carsten Jentsch"], "summary": "As interest in economic narratives has grown in recent years, so has the\nnumber of pipelines dedicated to extracting such narratives from texts.\nPipelines often employ a mix of state-of-the-art natural language processing\ntechniques, such as BERT, to tackle this task. While effective on foundational\nlinguistic operations essential for narrative extraction, such models lack the\ndeeper semantic understanding required to distinguish extracting economic\nnarratives from merely conducting classic tasks like Semantic Role Labeling.\nInstead of relying on complex model pipelines, we evaluate the benefits of\nLarge Language Models (LLMs) by analyzing a corpus of Wall Street Journal and\nNew York Times newspaper articles about inflation. We apply a rigorous\nnarrative definition and compare GPT-4o outputs to gold-standard narratives\nproduced by expert annotators. Our results suggests that GPT-4o is capable of\nextracting valid economic narratives in a structured format, but still falls\nshort of expert-level performance when handling complex documents and\nnarratives. Given the novelty of LLMs in economic research, we also provide\nguidance for future work in economics and the social sciences that employs LLMs\nto pursue similar objectives.", "comment": "53 pages, 5 figures", "cate": "econ.GN", "url": "http://arxiv.org/abs/2506.15041v1", "AI": {"title_translation": "在大规模文本语料库中识别经济叙事——一种使用大型语言模型的集成方法", "tldr": "本研究评估了大型语言模型（LLM）在从新闻文章中提取经济叙事方面的能力，发现GPT-4o能够提取有效叙事但仍未达到专家水平。", "motivation": "随着对经济叙事兴趣的增长，现有用于从文本中提取经济叙事的NLP管道（如BERT）虽然在基础语言操作上有效，但缺乏更深层次的语义理解，无法区分经济叙事提取与经典任务（如语义角色标注）。因此，研究旨在评估大型语言模型（LLM）在此任务中的益处。", "method": "研究评估了大型语言模型（LLM）的益处，通过分析华尔街日报和纽约时报关于通货膨胀的文章语料库。研究应用了严格的叙事定义，并将GPT-4o的输出与专家标注者生成的黄金标准叙事进行比较。", "result": "结果表明，GPT-4o能够以结构化格式提取有效的经济叙事，但在处理复杂文档和叙事时，仍未能达到专家级别的表现。", "conclusion": "GPT-4o在经济叙事提取方面展现出潜力，但与专家级表现仍有差距。鉴于LLM在经济研究中的新颖性，本研究也为未来在经济学和社会科学中利用LLM追求类似目标的工作提供了指导。", "translation": "近年来，随着对经济叙事兴趣的增长，从文本中提取此类叙事的管道数量也随之增加。这些管道通常采用最先进的自然语言处理技术组合，例如BERT，来解决这项任务。虽然这些模型在叙事提取所需的基础语言操作上是有效的，但它们缺乏更深层次的语义理解，无法将经济叙事提取与仅仅进行语义角色标注等经典任务区分开来。本研究不依赖复杂的模型管道，而是通过分析《华尔街日报》和《纽约时报》关于通货膨胀的文章语料库来评估大型语言模型（LLM）的益处。我们应用了严格的叙事定义，并将GPT-4o的输出与专家标注者生成的黄金标准叙事进行比较。我们的结果表明，GPT-4o能够以结构化格式提取有效的经济叙事，但在处理复杂文档和叙事时，仍未能达到专家级别的表现。鉴于LLM在经济研究中的新颖性，我们还为未来在经济学和社会科学中利用LLM追求类似目标的工作提供了指导。", "summary": "本研究旨在评估大型语言模型（LLM）在识别和提取经济叙事方面的能力，以应对现有NLP模型在深层语义理解方面的不足。研究以通货膨胀为主题，选取《华尔街日报》和《纽约时报》的文章作为语料库，并使用严格的叙事定义，将GPT-4o的输出与专家标注的黄金标准进行对比。结果显示，GPT-4o能够有效提取结构化的经济叙事，但在处理复杂文本和叙事时，其性能仍未达到专家水平。鉴于LLM在经济研究中的应用尚属新颖，本研究还为未来在经济学和社会科学领域利用LLM进行类似研究提供了指导。", "keywords": "经济叙事, 大型语言模型, GPT-4o, 自然语言处理, 文本分析", "comments": "本文创新性地将大型语言模型应用于经济叙事提取这一复杂任务，揭示了LLM在深层语义理解方面的潜力。尽管GPT-4o未能完全达到专家水平，但其能够提取有效叙事的结果证明了LLM在此领域的应用前景，并为后续研究提供了宝贵的经验和方向。其局限性在于处理复杂文档时的性能不足，这指明了未来LLM在特定领域应用中需要改进的方向。"}}
{"id": "2506.15673", "title": "UniRelight: Learning Joint Decomposition and Synthesis for Video Relighting", "authors": ["Kai He", "Ruofan Liang", "Jacob Munkberg", "Jon Hasselgren", "Nandita Vijaykumar", "Alexander Keller", "Sanja Fidler", "Igor Gilitschenski", "Zan Gojcic", "Zian Wang"], "summary": "We address the challenge of relighting a single image or video, a task that\ndemands precise scene intrinsic understanding and high-quality light transport\nsynthesis. Existing end-to-end relighting models are often limited by the\nscarcity of paired multi-illumination data, restricting their ability to\ngeneralize across diverse scenes. Conversely, two-stage pipelines that combine\ninverse and forward rendering can mitigate data requirements but are\nsusceptible to error accumulation and often fail to produce realistic outputs\nunder complex lighting conditions or with sophisticated materials. In this\nwork, we introduce a general-purpose approach that jointly estimates albedo and\nsynthesizes relit outputs in a single pass, harnessing the generative\ncapabilities of video diffusion models. This joint formulation enhances\nimplicit scene comprehension and facilitates the creation of realistic lighting\neffects and intricate material interactions, such as shadows, reflections, and\ntransparency. Trained on synthetic multi-illumination data and extensive\nautomatically labeled real-world videos, our model demonstrates strong\ngeneralization across diverse domains and surpasses previous methods in both\nvisual fidelity and temporal consistency.", "comment": "Project page: https://research.nvidia.com/labs/toronto-ai/UniRelight/", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15673v1", "AI": {"title_translation": "UniRelight：学习视频重新照明的联合分解与合成", "tldr": "UniRelight是一种新的视频重新照明模型，它利用视频扩散模型，在一个过程中联合估计场景内在属性并合成重新照明的视频，从而实现更好的泛化能力和真实感。", "motivation": "现有端到端重新照明模型受限于配对多照明数据稀缺，泛化能力差。两阶段管道易受误差累积影响，在复杂条件下难以产生真实感输出。", "method": "提出UniRelight，一种通用方法，利用视频扩散模型的生成能力，在单次通过中联合估计反照率并合成重新照明的输出。该模型在合成多照明数据和大量自动标注的真实世界视频上进行训练。", "result": "模型在不同领域展现出强大的泛化能力，并在视觉保真度和时间一致性方面超越了以往的方法。", "conclusion": "利用视频扩散模型进行联合分解和合成的方法显著改进了视频重新照明，提供了更好的真实感和泛化能力。", "translation": "我们解决了单张图像或视频的重新照明挑战，这项任务需要精确的场景内在理解和高质量的光传输合成。现有的端到端重新照明模型常常受限于配对的多照明数据稀缺，这限制了它们在不同场景中的泛化能力。相反，结合逆向和正向渲染的两阶段管道可以减轻数据需求，但容易出现误差累积，并且在复杂照明条件或复杂材质下通常无法产生真实感输出。在这项工作中，我们引入了一种通用方法，通过利用视频扩散模型的生成能力，在一次通过中联合估计反照率并合成重新照明的输出。这种联合公式增强了隐式场景理解，并促进了真实感照明效果和复杂材质相互作用（如阴影、反射和透明度）的创建。我们的模型在合成的多照明数据和大量自动标注的真实世界视频上进行训练，在不同领域展现出强大的泛化能力，并在视觉保真度和时间一致性方面超越了以往的方法。", "summary": "UniRelight提出了一种新颖、通用的视频重新照明方法，它利用视频扩散模型，在一个过程中联合估计场景反照率并合成重新照明的输出。该方法解决了现有端到端模型（数据稀缺、泛化能力差）和两阶段管道（误差累积、真实感不足）的局限性。UniRelight在多样化数据上进行训练，实现了卓越的视觉保真度和时间一致性，并在各种场景和复杂照明条件下展现出强大的泛化能力。", "keywords": "视频重新照明, 联合分解, 扩散模型, 反照率估计, 光传输合成", "comments": "该研究的创新之处在于利用视频扩散模型进行联合分解和合成以实现重新照明，有效解决了以往方法的数据稀缺和误差累积问题。其处理复杂材质交互和良好泛化的能力，使其在真实感视频重新照明领域迈出了重要一步。"}}
{"id": "2506.15421", "title": "Reward Models in Deep Reinforcement Learning: A Survey", "authors": ["Rui Yu", "Shenghua Wan", "Yucen Wang", "Chen-Xiao Gao", "Le Gan", "Zongzhang Zhang", "De-Chuan Zhan"], "summary": "In reinforcement learning (RL), agents continually interact with the\nenvironment and use the feedback to refine their behavior. To guide policy\noptimization, reward models are introduced as proxies of the desired\nobjectives, such that when the agent maximizes the accumulated reward, it also\nfulfills the task designer's intentions. Recently, significant attention from\nboth academic and industrial researchers has focused on developing reward\nmodels that not only align closely with the true objectives but also facilitate\npolicy optimization. In this survey, we provide a comprehensive review of\nreward modeling techniques within the deep RL literature. We begin by outlining\nthe background and preliminaries in reward modeling. Next, we present an\noverview of recent reward modeling approaches, categorizing them based on the\nsource, the mechanism, and the learning paradigm. Building on this\nunderstanding, we discuss various applications of these reward modeling\ntechniques and review methods for evaluating reward models. Finally, we\nconclude by highlighting promising research directions in reward modeling.\nAltogether, this survey includes both established and emerging methods, filling\nthe vacancy of a systematic review of reward models in current literature.", "comment": "IJCAI 2025 Survey Track (To Appear)", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15421v1", "AI": {"title_translation": "深度强化学习中的奖励模型：一项综述", "tldr": "该综述全面回顾了深度强化学习中的奖励建模技术，涵盖背景、分类方法、应用、评估以及未来的研究方向。", "motivation": "在强化学习中，奖励模型是指导策略优化的关键。随着学术界和工业界对开发有效奖励模型的关注日益增加，本综述旨在对深度强化学习中的奖励建模技术提供全面且系统的回顾，以填补当前文献中的空白。", "method": "本综述首先概述了奖励建模的背景和基础知识，然后根据来源、机制和学习范式对最近的奖励建模方法进行分类和概述。在此基础上，讨论了各种应用，并回顾了评估奖励模型的方法。", "result": "本综述涵盖了奖励建模领域中已建立和新兴的方法，提供了一个对奖励模型进行系统性回顾的框架。", "conclusion": "本综述通过强调奖励建模中前景广阔的研究方向来结束。", "translation": "在强化学习（RL）中，智能体持续与环境交互并利用反馈来优化其行为。为了指导策略优化，引入了奖励模型作为期望目标的代理，这样当智能体最大化累积奖励时，也能实现任务设计者的意图。最近，学术界和工业界的研究人员都将大量注意力集中在开发不仅与真实目标紧密对齐而且有助于策略优化的奖励模型上。在这项综述中，我们对深度强化学习文献中的奖励建模技术进行了全面回顾。我们首先概述了奖励建模的背景和基础知识。接下来，我们概述了最近的奖励建模方法，并根据来源、机制和学习范式对其进行分类。在此理解的基础上，我们讨论了这些奖励建模技术的各种应用，并回顾了评估奖励模型的方法。最后，我们通过强调奖励建模中前景广阔的研究方向来结束。总而言之，这项综述包含了成熟的和新兴的方法，填补了当前文献中缺乏对奖励模型进行系统回顾的空白。", "summary": "本综述全面回顾了深度强化学习中的奖励建模技术。它首先概述了奖励建模的背景和基础知识，然后根据来源、机制和学习范式对近期方法进行了分类和概述。文章还讨论了这些技术的各种应用和评估方法，旨在填补当前文献中系统性奖励模型综述的空白，并指出了未来的研究方向。", "keywords": "奖励模型, 深度强化学习, 综述, 策略优化, 奖励建模技术", "comments": "这是一篇重要的综述性论文，系统地梳理了深度强化学习中奖励模型的发展现状。它通过对方法进行分类、讨论应用和评估，为研究人员提供了全面的视角，并指明了该领域未来的研究方向，具有较高的参考价值。"}}
{"id": "2506.15675", "title": "Sekai: A Video Dataset towards World Exploration", "authors": ["Zhen Li", "Chuanhao Li", "Xiaofeng Mao", "Shaoheng Lin", "Ming Li", "Shitian Zhao", "Zhaopan Xu", "Xinyue Li", "Yukang Feng", "Jianwen Sun", "Zizhen Li", "Fanrui Zhang", "Jiaxin Ai", "Zhixiang Wang", "Yuwei Wu", "Tong He", "Jiangmiao Pang", "Yu Qiao", "Yunde Jia", "Kaipeng Zhang"], "summary": "Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.", "comment": "12 pages, 6 figures", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15675v1", "AI": {"title_translation": "Sekai：一个面向世界探索的视频数据集", "tldr": "Sekai是一个大规模的第一人称视角全球视频数据集，旨在解决现有视频生成数据集在世界探索训练方面的局限性。", "motivation": "现有视频生成数据集存在地点有限、时长短、场景静态以及缺乏探索和世界相关标注的局限性，不适合用于世界探索训练。", "method": "本文介绍了Sekai数据集，这是一个高质量的第一人称视角全球视频数据集，包含超过5,000小时的步行或无人机视角视频，来自100多个国家和地区的750个城市。开发了一个高效的工具箱来收集、预处理和标注视频，包括地点、场景、天气、人群密度、字幕和摄像机轨迹。", "result": "实验证明了数据集的质量。使用数据集的一个子集训练了一个名为YUME的交互式视频世界探索模型。", "conclusion": "Sekai数据集将有益于视频生成和世界探索领域，并激发有价值的应用。", "translation": "视频生成技术取得了显著进展，有望成为交互式世界探索的基础。然而，现有视频生成数据集不适合世界探索训练，因为它们存在一些局限性：地点有限、时长短、场景静态以及缺乏探索和世界相关的标注。在本文中，我们介绍了Sekai（日语中意为“世界”），一个高质量的第一人称视角全球视频数据集，具有丰富的世界探索标注。它包含来自100多个国家和地区750个城市的超过5,000小时的步行或无人机视角（FPV和UVA）视频。我们开发了一个高效且有效的工具箱，用于收集、预处理和标注视频，包括地点、场景、天气、人群密度、字幕和摄像机轨迹。实验证明了数据集的质量。此外，我们使用一个子集训练了一个交互式视频世界探索模型，名为YUME（日语中意为“梦想”）。我们相信Sekai将有益于视频生成和世界探索领域，并激发有价值的应用。", "summary": "本文提出了Sekai数据集，一个大规模、高质量的第一人称视角全球视频数据集，旨在克服现有视频生成数据集在世界探索训练方面的不足。Sekai包含超过5000小时的视频，覆盖全球100多个国家和地区，并提供了丰富的地点、场景、天气等标注。研究人员还开发了高效的工具箱用于数据处理，并利用该数据集训练了一个交互式视频世界探索模型YUME，证明了数据集的实用性和潜力。", "keywords": "视频数据集, 世界探索, 第一人称视角, 视频生成, Sekai", "comments": "Sekai数据集的创新之处在于其大规模的全球覆盖（超过100个国家和地区）、长时长（5000+小时）以及丰富的第一人称视角和无人机视角视频，并提供了详细的探索相关标注，这显著弥补了现有数据集的不足。它为视频生成和世界探索领域提供了宝贵资源，有望推动相关技术发展和实际应用。"}}
{"id": "2506.15446", "title": "Zero-Shot Reinforcement Learning Under Partial Observability", "authors": ["Scott Jeen", "Tom Bewley", "Jonathan M. Cullen"], "summary": "Recent work has shown that, under certain assumptions, zero-shot\nreinforcement learning (RL) methods can generalise to any unseen task in an\nenvironment after reward-free pre-training. Access to Markov states is one such\nassumption, yet, in many real-world applications, the Markov state is only\npartially observable. Here, we explore how the performance of standard\nzero-shot RL methods degrades when subjected to partially observability, and\nshow that, as in single-task RL, memory-based architectures are an effective\nremedy. We evaluate our memory-based zero-shot RL methods in domains where the\nstates, rewards and a change in dynamics are partially observed, and show\nimproved performance over memory-free baselines. Our code is open-sourced via:\nhttps://enjeeneer.io/projects/bfms-with-memory/.", "comment": "Reinforcement Learning Conference 2025", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15446v1", "AI": {"title_translation": "零样本强化学习在部分可观测性下的表现", "tldr": "研究零样本强化学习在部分可观测环境下的性能下降问题，并提出基于记忆的架构作为有效解决方案。", "motivation": "现有零样本强化学习方法假设可访问马尔可夫状态，但在许多实际应用中，马尔可夫状态是部分可观测的，这会导致性能下降，因此需要探索如何在这种情况下保持或提高性能。", "method": "本文探讨了标准零样本强化学习方法在部分可观测性下的性能下降情况，并提出并评估了基于记忆的架构作为有效补救措施。实验在状态、奖励和动态变化部分可观测的领域进行。", "result": "记忆型零样本强化学习方法在状态、奖励和动态变化部分可观测的领域中表现出比无记忆基线方法更好的性能。", "conclusion": "记忆型架构是解决部分可观测环境下零样本强化学习性能下降的有效方法。", "translation": "最近的研究表明，在某些假设下，零样本强化学习（RL）方法在无奖励预训练后可以泛化到环境中任何未见过的任务。访问马尔可夫状态是其中一个假设，然而，在许多实际应用中，马尔可夫状态仅是部分可观测的。本文探讨了标准零样本RL方法在部分可观测性下性能如何下降，并表明，与单任务RL一样，基于记忆的架构是有效的补救措施。我们在状态、奖励和动态变化部分可观测的领域中评估了我们的基于记忆的零样本RL方法，并显示出比无记忆基线方法更高的性能。我们的代码已通过 https://enjeeneer.io/projects/bfms-with-memory/ 开源。", "summary": "本文探讨了在部分可观测环境下零样本强化学习（RL）的性能问题。研究发现，在部分可观测性下，标准零样本RL方法的性能会下降。为解决此问题，作者提出并评估了基于记忆的架构，并证明其在状态、奖励和动态变化部分可观测的领域中优于无记忆的基线方法，表明记忆机制是应对部分可观测性的有效策略。", "keywords": "零样本强化学习, 部分可观测性, 记忆型架构, 泛化, 强化学习", "comments": "这篇论文的创新点在于将零样本强化学习与部分可观测性相结合，填补了现有研究的空白。它强调了在更实际的场景中应用零样本RL的挑战，并提出了基于记忆的解决方案，这对于推动RL在复杂现实世界中的应用具有重要意义。同时，代码的开源也促进了研究的复现和进一步发展。"}}
{"id": "2506.15682", "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model", "authors": ["Anirud Aggarwal", "Abhinav Shrivastava", "Matthew Gwilliam"], "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.", "comment": "29 pages, 22 figures, 9 tables", "cate": "cs.CV", "url": "http://arxiv.org/abs/2506.15682v1", "AI": {"title_translation": "进化缓存加速现成扩散模型", "tldr": "ECAD是一种基于遗传算法的进化缓存方法，可以为扩散模型学习高效的缓存策略，显著提高推理速度并保持图像质量，优于现有方法。", "motivation": "扩散模型图像生成质量高但推理速度慢且计算成本高。现有缓存方法依赖僵硬的启发式规则，导致加速有限或泛化性差。", "method": "本文提出进化缓存加速扩散模型（ECAD），这是一种遗传算法，通过少量校准提示学习高效的、针对特定模型的缓存调度，形成帕累托前沿。ECAD无需修改网络参数或参考图像。", "result": "ECAD提供了显著的推理加速，实现了对质量-延迟权衡的细粒度控制，并能无缝适应不同的扩散模型。ECAD学习到的调度能够有效地泛化到校准时未见的S分辨率和模型变体。在PixArt-alpha、PixArt-Sigma和FLUX-1.dev上，使用多种指标（FID、CLIP、Image Reward）和基准（COCO、MJHQ-30k、PartiPrompts）评估，ECAD始终优于现有方法。在PixArt-alpha上，ECAD找到的调度在COCO FID上比现有最先进方法提高了4.47，同时推理加速从2.35倍增加到2.58倍。", "conclusion": "ECAD被确立为一种可扩展且可泛化的加速扩散推理的方法。", "translation": "基于扩散的图像生成模型在生成高质量合成内容方面表现出色，但推理速度慢且计算成本高。先前的工作试图通过在扩散Transformer的推理步骤中缓存和重用特征来缓解这一问题。然而，这些方法通常依赖于僵硬的启发式规则，导致加速有限或在不同架构上的泛化性差。我们提出进化缓存加速扩散模型（ECAD），这是一种遗传算法，仅使用一小部分校准提示，学习高效的、针对特定模型的缓存调度，形成帕累托前沿。ECAD无需修改网络参数或参考图像。它提供了显著的推理加速，实现了对质量-延迟权衡的细粒度控制，并能无缝适应不同的扩散模型。值得注意的是，ECAD学习到的调度可以有效地泛化到校准时未见的分辨率和模型变体。我们在PixArt-alpha、PixArt-Sigma和FLUX-1.dev上，使用多种指标（FID、CLIP、Image Reward）在不同基准（COCO、MJHQ-30k、PartiPrompts）上评估ECAD，证明了其相对于先前方法的持续改进。在PixArt-alpha上，ECAD识别出一个调度，其COCO FID比现有最先进方法高出4.47，同时推理加速从2.35倍增加到2.58倍。我们的结果确立了ECAD作为一种可扩展且可泛化的加速扩散推理的方法。我们的项目网站可在https://aniaggarwal.github.io/ecad 访问，代码可在https://github.com/aniaggarwal/ecad 访问。", "summary": "该论文提出了一种名为ECAD（进化缓存加速扩散模型）的新方法，旨在解决扩散模型推理速度慢的问题。ECAD利用遗传算法学习高效的、针对特定模型的缓存调度，从而在不修改网络参数或参考图像的情况下显著提高推理速度并保持图像质量。它能够自适应不同的扩散模型，并泛化到未见的分辨率和模型变体。实验结果表明，ECAD在多个扩散模型和基准测试上均优于现有最先进的加速方法，证明了其可扩展性和泛化能力。", "keywords": "扩散模型, 缓存, 遗传算法, 推理加速, 进化缓存", "comments": "该论文的创新点在于引入遗传算法来动态学习扩散模型的缓存策略，而非依赖固定的启发式规则。这使得ECAD能够为特定模型找到最优的帕累托前沿，在速度和质量之间实现更好的权衡。其无需修改网络参数的特性也增加了实用性。该方法对于加速计算密集型扩散模型的应用具有重要意义。"}}
{"id": "2506.14995", "title": "Improved Image Reconstruction and Diffusion Parameter Estimation Using a Temporal Convolutional Network Model of Gradient Trajectory Errors", "authors": ["Jonathan B. Martin", "Hannah E. Alderson", "John C. Gore", "Mark D. Does", "Kevin D. Harkins"], "summary": "Summary: Errors in gradient trajectories introduce significant artifacts and\ndistortions in magnetic resonance images, particularly in non-Cartesian imaging\nsequences, where imperfect gradient waveforms can greatly reduce image quality.\nPurpose: Our objective is to develop a general, nonlinear gradient system model\nthat can accurately predict gradient distortions using convolutional networks.\nMethods: A set of training gradient waveforms were measured on a small animal\nimaging system, and used to train a temporal convolutional network to predict\nthe gradient waveforms produced by the imaging system. Results: The trained\nnetwork was able to accurately predict nonlinear distortions produced by the\ngradient system. Network prediction of gradient waveforms was incorporated into\nthe image reconstruction pipeline and provided improvements in image quality\nand diffusion parameter mapping compared to both the nominal gradient waveform\nand the gradient impulse response function. Conclusion: Temporal convolutional\nnetworks can more accurately model gradient system behavior than existing\nlinear methods and may be used to retrospectively correct gradient errors.", "comment": null, "cate": "physics.med-ph", "url": "http://arxiv.org/abs/2506.14995v1", "AI": {"title_translation": "采用时间卷积网络模型校正梯度轨迹误差，改进图像重建和扩散参数估计", "tldr": "本文利用时间卷积网络（TCN）模型预测梯度轨迹误差，显著改善了磁共振图像重建质量和扩散参数估计的准确性。", "motivation": "梯度轨迹误差会在磁共振图像中引入显著的伪影和畸变，尤其是在非笛卡尔成像序列中，这会大大降低图像质量。因此，需要开发一个通用的、非线性的梯度系统模型来准确预测这些畸变。", "method": "研究人员在小型动物成像系统上测量了一组训练梯度波形，并用这些数据训练了一个时间卷积网络（TCN）来预测成像系统实际产生的梯度波形。随后，将TCN预测的梯度波形整合到图像重建流程中。", "result": "训练后的网络能够准确预测梯度系统产生的非线性畸变。与使用名义梯度波形和梯度脉冲响应函数相比，将网络预测的梯度波形整合到图像重建流程中显著改善了图像质量和扩散参数映射。", "conclusion": "时间卷积网络比现有线性方法能更准确地模拟梯度系统行为，并且可以用于回顾性校正梯度误差。", "translation": "摘要：梯度轨迹误差会在磁共振图像中引入显著的伪影和畸变，尤其是在非笛卡尔成像序列中，不完善的梯度波形会大大降低图像质量。\n目的：我们的目标是开发一个通用的、非线性的梯度系统模型，该模型可以使用卷积网络准确预测梯度畸变。\n方法：在小型动物成像系统上测量了一组训练梯度波形，并用于训练一个时间卷积网络，以预测成像系统产生的梯度波形。结果：训练后的网络能够准确预测梯度系统产生的非线性畸变。将网络预测的梯度波形整合到图像重建流程中，与名义梯度波形和梯度脉冲响应函数相比，图像质量和扩散参数映射均得到了改善。结论：时间卷积网络比现有线性方法能更准确地模拟梯度系统行为，并可用于回顾性校正梯度误差。", "summary": "本文提出使用时间卷积网络（TCN）来建模和预测磁共振成像中由梯度轨迹误差引起的非线性畸变。通过在小型动物成像系统上训练TCN，该网络能够准确预测实际梯度波形。将TCN预测的波形整合到图像重建流程中，显著提高了图像质量和扩散参数映射的准确性，证明TCN在校正梯度误差方面的优越性。", "keywords": "梯度轨迹误差, 时间卷积网络, 图像重建, 扩散参数估计, 磁共振成像", "comments": "本文创新性地将时间卷积网络应用于磁共振成像中的梯度轨迹误差校正问题。通过利用TCN的非线性建模能力，该研究超越了传统的线性校正方法，显著提升了图像重建质量和扩散参数估计的准确性。其重要性在于为高精度非笛卡尔MRI序列提供了一种有效的误差校正方案，具有重要的实际应用价值。"}}
{"id": "2506.15448", "title": "Semi-supervised Graph Anomaly Detection via Robust Homophily Learning", "authors": ["Guoguo Ai", "Hezhe Qiao", "Hui Yan", "Guansong Pang"], "summary": "Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled\nnormal nodes to identify abnormal nodes from a large set of unlabeled nodes in\na graph. Current methods in this line posit that 1) normal nodes share a\nsimilar level of homophily and 2) the labeled normal nodes can well represent\nthe homophily patterns in the normal class. However, this assumption often does\nnot hold well since normal nodes in a graph can exhibit diverse homophily in\nreal-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily\nLearning, to adaptively learn such homophily patterns. RHO consists of two\nnovel modules, adaptive frequency response filters (AdaFreq) and graph\nnormality alignment (GNA). AdaFreq learns a set of adaptive spectral filters\nthat capture different frequency components of the labeled normal nodes with\nvarying homophily in the channel-wise and cross-channel views of node\nattributes. GNA is introduced to enforce consistency between the channel-wise\nand cross-channel homophily representations to robustify the normality learned\nby the filters in the two views. Experiments on eight real-world GAD datasets\nshow that RHO can effectively learn varying, often under-represented, homophily\nin the small normal node set and substantially outperforms state-of-the-art\ncompeting methods. Code is available at https://github.com/mala-lab/RHO.", "comment": "18 pages, 11 figures, 3 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15448v1", "AI": {"title_translation": "半监督图异常检测通过鲁棒同质性学习", "tldr": "本文提出RHO框架，通过自适应频率响应滤波器和图常态对齐来解决半监督图异常检测中正常节点同质性多样性的问题，显著优于现有方法。", "motivation": "现有半监督图异常检测方法假设正常节点同质性水平相似且标记正常节点能很好地代表正常类同质性模式，但这一假设在真实世界数据集中往往不成立，因为正常节点可以表现出多样化的同质性。", "method": "提出RHO（鲁棒同质性学习）框架。RHO包含两个模块：1) 自适应频率响应滤波器（AdaFreq），学习一组自适应谱滤波器，从节点属性的通道级和跨通道视图中捕获具有不同同质性的标记正常节点的不同频率分量；2) 图常态对齐（GNA），用于强制通道级和跨通道同质性表示之间的一致性，以增强滤波器在两种视图中学到的常态的鲁棒性。", "result": "在八个真实世界GAD数据集上的实验表明，RHO能有效学习小部分正常节点集合中多变、常被低估的同质性，并显著优于最先进的竞争方法。", "conclusion": "RHO通过自适应地学习正常节点多样化的同质性模式，能够有效提升半监督图异常检测的性能。", "translation": "半监督图异常检测（GAD）利用一小部分标记的正常节点从图中大量未标记节点中识别异常节点。当前该领域的方法假设：1）正常节点共享相似的同质性水平；2）标记的正常节点能够很好地代表正常类中的同质性模式。然而，由于图中的正常节点在真实世界的GAD数据集中可以表现出多样化的同质性，这一假设通常不成立。在本文中，我们提出了RHO，即鲁棒同质性学习，以自适应地学习这种同质性模式。RHO由两个新颖的模块组成：自适应频率响应滤波器（AdaFreq）和图常态对齐（GNA）。AdaFreq学习一组自适应谱滤波器，这些滤波器从节点属性的通道级和跨通道视图中捕获具有不同同质性的标记正常节点的不同频率分量。引入GNA是为了强制通道级和跨通道同质性表示之间的一致性，以增强滤波器在两种视图中学到的常态的鲁棒性。在八个真实世界GAD数据集上的实验表明，RHO能够有效学习小部分正常节点集合中多变、常被低估的同质性，并显著优于最先进的竞争方法。代码可在https://github.com/mala-lab/RHO获取。", "summary": "本文提出RHO（鲁棒同质性学习）框架，旨在解决半监督图异常检测中正常节点同质性多样性的挑战。RHO包含自适应频率响应滤波器（AdaFreq）和图常态对齐（GNA）两个模块，分别用于自适应捕获不同同质性的频率分量并增强常态表示的鲁棒性。实验证明RHO在多个真实数据集上优于现有方法，能有效处理正常节点同质性变异问题。", "keywords": "图异常检测, 半监督学习, 同质性学习, 鲁棒性, 频率响应滤波器", "comments": "这篇论文通过质疑现有半监督图异常检测方法中关于正常节点同质性的假设，提出了RHO框架。其创新点在于引入了AdaFreq和GNA模块，能够自适应地学习并鲁棒化多样化的正常节点同质性模式，这对于提升真实世界场景下的检测性能具有重要意义，因为它更好地反映了实际数据的复杂性。"}}
{"id": "2506.15452", "title": "Warping and Matching Subsequences Between Time Series", "authors": ["Simiao Lin", "Wannes Meert", "Pieter Robberechts", "Hendrik Blockeel"], "summary": "Comparing time series is essential in various tasks such as clustering and\nclassification. While elastic distance measures that allow warping provide a\nrobust quantitative comparison, a qualitative comparison on top of them is\nmissing. Traditional visualizations focus on point-to-point alignment and do\nnot convey the broader structural relationships at the level of subsequences.\nThis limitation makes it difficult to understand how and where one time series\nshifts, speeds up or slows down with respect to another. To address this, we\npropose a novel technique that simplifies the warping path to highlight,\nquantify and visualize key transformations (shift, compression, difference in\namplitude). By offering a clearer representation of how subsequences match\nbetween time series, our method enhances interpretability in time series\ncomparison.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15452v1", "AI": {"title_translation": "时间序列间的子序列扭曲与匹配", "tldr": "提出一种新方法，通过简化扭曲路径来可视化和量化时间序列子序列间的关键变换，以增强时间序列比较的可解释性。", "motivation": "现有的时间序列比较方法（如弹性距离度量）虽然提供了鲁棒的定量比较，但缺乏定性比较，且传统可视化侧重于点对点对齐，难以理解时间序列如何相对移动、加速或减速。", "method": "提出一种新颖的技术，通过简化扭曲路径来突出、量化和可视化关键变换（位移、压缩、振幅差异）。", "result": "提供了更清晰的子序列匹配表示，增强了时间序列比较的可解释性。", "conclusion": "该方法通过简化扭曲路径并量化可视化关键变换，显著提高了时间序列比较的解释性。", "translation": "比较时间序列在聚类和分类等各种任务中至关重要。虽然允许扭曲的弹性距离度量提供了鲁棒的定量比较，但在此之上却缺少定性比较。传统的可视化方法侧重于点对点对齐，未能传达子序列层面上更广泛的结构关系。这种局限性使得难以理解一个时间序列相对于另一个时间序列如何以及在何处发生位移、加速或减速。为了解决这个问题，我们提出了一种新颖的技术，通过简化扭曲路径来突出、量化和可视化关键变换（位移、压缩、振幅差异）。通过提供时间序列间子序列如何匹配的更清晰表示，我们的方法增强了时间序列比较的可解释性。", "summary": "本文提出一种新颖的时间序列比较技术，旨在解决现有方法在定性比较和可视化方面的不足。通过简化扭曲路径，该方法能够突出、量化并可视化时间序列子序列之间的关键变换，如位移、压缩和振幅差异，从而显著增强了时间序列比较的可解释性。", "keywords": "时间序列比较, 子序列匹配, 扭曲路径, 可解释性, 数据可视化", "comments": "该论文的创新点在于将定性分析引入时间序列的弹性距离比较中，通过简化扭曲路径并可视化关键变换，弥补了传统方法仅关注点对点对齐的不足，显著提升了时间序列比较的解释性，对于理解时间序列的动态行为具有重要意义。"}}
{"id": "2506.15479", "title": "Creating User-steerable Projections with Interactive Semantic Mapping", "authors": ["Artur André Oliveira", "Mateus Espadoto", "Roberto Hirata Jr.", "Roberto M. Cesar Jr.", "Alex C. Telea"], "summary": "Dimensionality reduction (DR) techniques map high-dimensional data into\nlower-dimensional spaces. Yet, current DR techniques are not designed to\nexplore semantic structure that is not directly available in the form of\nvariables or class labels. We introduce a novel user-guided projection\nframework for image and text data that enables customizable, interpretable,\ndata visualizations via zero-shot classification with Multimodal Large Language\nModels (MLLMs). We enable users to steer projections dynamically via\nnatural-language guiding prompts, to specify high-level semantic relationships\nof interest to the users which are not explicitly present in the data\ndimensions. We evaluate our method across several datasets and show that it not\nonly enhances cluster separation, but also transforms DR into an interactive,\nuser-driven process. Our approach bridges the gap between fully automated DR\ntechniques and human-centered data exploration, offering a flexible and\nadaptive way to tailor projections to specific analytical needs.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15479v1", "AI": {"title_translation": "创建用户可控的交互式语义映射投影", "tldr": "提出一种新的用户引导投影框架，利用多模态大语言模型，让用户通过自然语言提示动态控制降维可视化，以探索数据中未明确存在的语义结构，从而增强聚类分离并实现交互式降维。", "motivation": "当前的降维（DR）技术无法探索数据中非变量或类别标签形式的语义结构。", "method": "引入了一种新颖的用户引导投影框架，适用于图像和文本数据。该框架通过多模态大语言模型（MLLMs）进行零样本分类，使用户能够通过自然语言提示动态引导投影，以指定数据维度中未明确存在的高级语义关系。", "result": "该方法不仅增强了聚类分离，还将降维过程转变为一个交互式、用户驱动的过程。", "conclusion": "该方法弥合了全自动化降维技术与以人为中心的探索之间的鸿沟，提供了一种灵活自适应的方式来根据特定分析需求定制投影。", "translation": "降维（DR）技术将高维数据映射到低维空间。然而，当前的降维技术并非旨在探索不以变量或类标签形式直接可用的语义结构。我们引入了一种新颖的图像和文本数据用户引导投影框架，该框架通过多模态大语言模型（MLLMs）的零样本分类，实现可定制、可解释的数据可视化。我们使用户能够通过自然语言引导提示动态地引导投影，以指定用户感兴趣的、数据维度中未明确存在的高级语义关系。我们在多个数据集上评估了我们的方法，结果表明它不仅增强了聚类分离，而且将降维转化为一个交互式、用户驱动的过程。我们的方法弥合了全自动化降维技术与以人为中心的数据探索之间的鸿沟，提供了一种灵活自适应的方式来根据特定分析需求定制投影。", "summary": "本文提出了一种创新的用户引导投影框架，旨在解决现有降维技术无法有效探索数据中隐式语义结构的问题。该框架利用多模态大语言模型（MLLMs）的零样本分类能力，允许用户通过自然语言提示动态控制图像和文本数据的降维可视化，从而揭示数据维度中未明确定义的高级语义关系。实验结果表明，该方法显著提升了聚类分离效果，并将降维过程转变为一个交互式、以用户为中心的操作，有效连接了自动化降维和人工数据探索。", "keywords": "降维, 用户引导, 语义映射, 多模态大语言模型, 交互式可视化", "comments": "这篇论文的创新点在于将多模态大语言模型（MLLMs）与降维技术相结合，创造了一个用户可控的交互式数据探索环境。通过允许用户使用自然语言提示来引导投影，它极大地增强了数据可视化的可解释性和灵活性，弥补了传统降维方法在捕获隐式语义方面的不足。这种人机协作的方式对于需要深度语义理解的数据分析领域具有重要意义。"}}
{"id": "2506.15538", "title": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework", "authors": ["Laura Kopf", "Nils Feldhus", "Kirill Bykov", "Philine Lou Bommer", "Anna Hedström", "Marina M. -C. Höhne", "Oliver Eberle"], "summary": "Automated interpretability research aims to identify concepts encoded in\nneural network features to enhance human understanding of model behavior.\nCurrent feature description methods face two critical challenges: limited\nrobustness and the flawed assumption that each neuron encodes only a single\nconcept (monosemanticity), despite growing evidence that neurons are often\npolysemantic. This assumption restricts the expressiveness of feature\ndescriptions and limits their ability to capture the full range of behaviors\nencoded in model internals. To address this, we introduce Polysemantic FeatuRe\nIdentification and Scoring Method (PRISM), a novel framework that captures the\ninherent complexity of neural network features. Unlike prior approaches that\nassign a single description per feature, PRISM provides more nuanced\ndescriptions for both polysemantic and monosemantic features. We apply PRISM to\nlanguage models and, through extensive benchmarking against existing methods,\ndemonstrate that our approach produces more accurate and faithful feature\ndescriptions, improving both overall description quality (via a description\nscore) and the ability to capture distinct concepts when polysemanticity is\npresent (via a polysemanticity score).", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15538v1", "AI": {"title_translation": "使用PRISM捕获多义性：一个多概念特征描述框架", "tldr": "PRISM是一个新的框架，旨在解决现有神经元特征描述方法在鲁棒性和处理多义性方面的局限性，它能更准确地描述神经网络特征，包括多义和单义特征。", "motivation": "现有的特征描述方法在鲁棒性方面存在局限性，并且错误地假设每个神经元只编码一个单一概念（单义性），尽管有越来越多的证据表明神经元通常是多义的。这种假设限制了特征描述的表达能力，并限制了它们捕捉模型内部编码的全部行为的能力。", "method": "我们引入了多义特征识别和评分方法（PRISM），这是一个新颖的框架，旨在捕捉神经网络特征的内在复杂性。与以往为每个特征分配单一描述的方法不同，PRISM为多义和单义特征提供了更细致的描述。", "result": "通过将PRISM应用于语言模型，并通过与现有方法的广泛基准测试，我们证明了我们的方法能产生更准确、更忠实的特征描述，提高了整体描述质量（通过描述分数）以及在存在多义性时捕捉不同概念的能力（通过多义性分数）。", "conclusion": "PRISM框架有效解决了现有特征描述方法在处理神经元多义性方面的不足，并能提供更准确和忠实的特征描述，从而增强对模型行为的理解。", "translation": "自动化可解释性研究旨在识别神经网络特征中编码的概念，以增强人类对模型行为的理解。当前的特征描述方法面临两个关键挑战：有限的鲁棒性和每个神经元只编码一个单一概念（单义性）的错误假设，尽管越来越多的证据表明神经元通常是多义的。这种假设限制了特征描述的表达能力，并限制了它们捕捉模型内部编码的全部行为的能力。为了解决这个问题，我们引入了多义特征识别和评分方法（PRISM），这是一个新颖的框架，旨在捕捉神经网络特征的内在复杂性。与以往为每个特征分配单一描述的方法不同，PRISM为多义和单义特征提供了更细致的描述。我们将PRISM应用于语言模型，并通过与现有方法的广泛基准测试，证明了我们的方法能产生更准确、更忠实的特征描述，提高了整体描述质量（通过描述分数）以及在存在多义性时捕捉不同概念的能力（通过多义性分数）。", "summary": "该论文提出了PRISM（Polysemantic FeatuRe Identification and Scoring Method）框架，旨在解决现有神经网络特征描述方法在鲁棒性和多义性处理上的不足。传统方法假设神经元是单义的，而PRISM能够为多义和单义特征提供更细致、更准确的描述。通过在语言模型上的应用和基准测试，PRISM被证明能提供更高质量的特征描述，并有效捕捉神经元的多概念编码，从而增强对模型行为的理解。", "keywords": "神经网络可解释性, 多义性, 特征描述, PRISM, 概念识别", "comments": "PRISM的创新之处在于其明确地解决了神经元的多义性问题，这是当前可解释性研究中一个被忽视但至关重要的问题。通过提供更细致的特征描述，PRISM有望显著提高我们对复杂神经网络内部工作机制的理解，对于提升模型透明度和可靠性具有重要意义。"}}
{"id": "2506.14803", "title": "Omnidirectional Video Super-Resolution using Deep Learning", "authors": ["Arbind Agrahari Baniya", "Tsz-Kwan Lee", "Peter W. Eklund", "Sunil Aryal"], "summary": "Omnidirectional Videos (or 360{\\deg} videos) are widely used in Virtual\nReality (VR) to facilitate immersive and interactive viewing experiences.\nHowever, the limited spatial resolution in 360{\\deg} videos does not allow for\neach degree of view to be represented with adequate pixels, limiting the visual\nquality offered in the immersive experience. Deep learning Video\nSuper-Resolution (VSR) techniques used for conventional videos could provide a\npromising software-based solution; however, these techniques do not tackle the\ndistortion present in equirectangular projections of 360{\\deg} video signals.\nAn additional obstacle is the limited availability of 360{\\deg} video datasets\nfor study. To address these issues, this paper creates a novel 360{\\deg} Video\nDataset (360VDS) with a study of the extensibility of conventional VSR models\nto 360{\\deg} videos. This paper further proposes a novel deep learning model\nfor 360{\\deg} Video Super-Resolution (360{\\deg} VSR), called Spherical Signal\nSuper-resolution with a Proportioned Optimisation (S3PO). S3PO adopts recurrent\nmodelling with an attention mechanism, unbound from conventional VSR techniques\nlike alignment. With a purpose-built feature extractor and a novel loss\nfunction addressing spherical distortion, S3PO outperforms most\nstate-of-the-art conventional VSR models and 360{\\deg}~specific\nsuper-resolution models on 360{\\deg} video datasets. A step-wise ablation study\nis presented to understand and demonstrate the impact of the chosen\narchitectural sub-components, targeted training and optimisation.", "comment": null, "cate": "cs.MM", "url": "http://arxiv.org/abs/2506.14803v1", "AI": {"title_translation": "基于深度学习的全向视频超分辨率", "tldr": "该论文提出了一种名为S3PO的深度学习模型，用于全向（360度）视频的超分辨率，并创建了一个新的360度视频数据集，以解决360度视频分辨率低、传统超分辨率方法不适用以及数据集稀缺的问题。S3PO在360度视频上表现优于现有模型。", "motivation": "全向（360度）视频在虚拟现实中广泛应用，但其有限的空间分辨率导致视觉质量不佳。传统的视频超分辨率技术无法处理360度视频等距柱状投影中的畸变，并且缺乏可用的360度视频数据集。", "method": "本文创建了一个新的360度视频数据集（360VDS），并研究了传统VSR模型对360度视频的扩展性。此外，本文提出了一种名为S3PO的深度学习模型，用于360度视频超分辨率。S3PO采用带有注意力机制的循环建模，具有专门的特征提取器和解决球形畸变的新型损失函数。论文还进行了分步消融研究以理解其架构组件的影响。", "result": "S3PO在360度视频数据集上优于大多数最先进的传统VSR模型和360度特定超分辨率模型。", "conclusion": "本文通过创建新的360度视频数据集并提出专门的深度学习模型S3PO，成功解决了全向视频分辨率低和传统超分辨率方法不适用等问题，显著提升了360度视频的视觉质量。", "translation": "全向视频（或360度视频）在虚拟现实（VR）中被广泛使用，以提供沉浸式和交互式的观看体验。然而，360度视频有限的空间分辨率无法使每个视角都得到足够的像素表示，从而限制了沉浸式体验所提供的视觉质量。用于传统视频的深度学习视频超分辨率（VSR）技术可能提供一个有前景的基于软件的解决方案；然而，这些技术无法解决360度视频信号等距柱状投影中存在的畸变。另一个障碍是用于研究的360度视频数据集的可用性有限。为了解决这些问题，本文创建了一个新颖的360度视频数据集（360VDS），并研究了传统VSR模型对360度视频的扩展性。本文还提出了一种用于360度视频超分辨率（360度VSR）的新型深度学习模型，名为带有比例优化的球面信号超分辨率（S3PO）。S3PO采用带有注意力机制的循环建模，不受传统VSR技术（如对齐）的约束。S3PO通过专门的特征提取器和解决球形畸变的新型损失函数，在360度视频数据集上优于大多数最先进的传统VSR模型和360度特定超分辨率模型。本文还进行了分步消融研究，以理解和证明所选架构子组件、目标训练和优化的影响。", "summary": "本论文致力于解决全向（360度）视频在虚拟现实中存在的低分辨率和等距柱状投影畸变问题。为克服现有挑战，作者首先构建了一个新颖的360度视频数据集（360VDS）。在此基础上，提出了一种名为S3PO的深度学习模型，专门用于360度视频超分辨率。S3PO模型整合了循环建模、注意力机制、定制的特征提取器以及针对球形畸变设计的损失函数。实验结果表明，S3PO在360度视频数据集上显著优于多数现有先进的传统VSR模型和360度专用超分辨率模型。", "keywords": "全向视频, 超分辨率, 深度学习, 360度视频, S3PO", "comments": "本文的创新点在于针对360度视频的特性，解决了传统超分辨率方法无法处理的畸变问题，并弥补了360度视频数据集的不足。S3PO模型结合了循环建模和注意力机制，并设计了专门的损失函数，体现了对球面信号处理的深入考量。其性能优于现有模型，显示出在提升VR沉浸式体验方面的巨大潜力。"}}
{"id": "2506.15492", "title": "LIT-LVM: Structured Regularization for Interaction Terms in Linear Predictors using Latent Variable Models", "authors": ["Mohammadreza Nemati", "Zhipeng Huang", "Kevin S. Xu"], "summary": "Some of the simplest, yet most frequently used predictors in statistics and\nmachine learning use weighted linear combinations of features. Such linear\npredictors can model non-linear relationships between features by adding\ninteraction terms corresponding to the products of all pairs of features. We\nconsider the problem of accurately estimating coefficients for interaction\nterms in linear predictors. We hypothesize that the coefficients for different\ninteraction terms have an approximate low-dimensional structure and represent\neach feature by a latent vector in a low-dimensional space. This\nlow-dimensional representation can be viewed as a structured regularization\napproach that further mitigates overfitting in high-dimensional settings beyond\nstandard regularizers such as the lasso and elastic net. We demonstrate that\nour approach, called LIT-LVM, achieves superior prediction accuracy compared to\nelastic net and factorization machines on a wide variety of simulated and real\ndata, particularly when the number of interaction terms is high compared to the\nnumber of samples. LIT-LVM also provides low-dimensional latent representations\nfor features that are useful for visualizing and analyzing their relationships.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15492v1", "AI": {"title_translation": "LIT-LVM：使用潜变量模型的线性预测器中交互项的结构化正则化", "tldr": "LIT-LVM 是一种使用潜变量模型对线性预测器中的交互项进行结构化正则化的方法，旨在提高预测精度，尤其是在高维设置中。", "motivation": "统计学和机器学习中常用的线性预测器可以通过添加交互项来模拟特征之间的非线性关系。然而，准确估计交互项的系数是一个挑战，特别是在高维设置中，标准正则化方法（如 lasso 和 elastic net）可能不足以缓解过拟合。本文假设不同交互项的系数具有近似的低维结构。", "method": "本文提出了一种名为 LIT-LVM 的方法。该方法通过在低维空间中用潜向量表示每个特征，从而对交互项的系数施加低维结构。这种低维表示被视为一种结构化正则化方法，旨在在高维设置中进一步缓解过拟合。", "result": "LIT-LVM 在各种模拟和真实数据集上，与 elastic net 和因子分解机相比，实现了卓越的预测精度，尤其是在交互项数量相对于样本数量较高的情况下。LIT-LVM 还为特征提供了低维潜表示，这对于可视化和分析它们之间的关系非常有用。", "conclusion": "LIT-LVM 是一种有效的结构化正则化方法，用于线性预测器中的交互项。它显著提高了预测精度，特别是在高维场景下，并提供了有用的特征低维表示。", "translation": "统计学和机器学习中一些最简单但最常用的预测器使用特征的加权线性组合。此类线性预测器可以通过添加对应于所有特征对乘积的交互项来模拟特征之间的非线性关系。我们考虑了在线性预测器中准确估计交互项系数的问题。我们假设不同交互项的系数具有近似的低维结构，并通过低维空间中的潜向量表示每个特征。这种低维表示可以被视为一种结构化正则化方法，在高维设置中，它比 lasso 和 elastic net 等标准正则化器更能缓解过拟合。我们证明，我们的方法 LIT-LVM 在各种模拟和真实数据上，与 elastic net 和因子分解机相比，实现了卓越的预测精度，特别是在交互项数量相对于样本数量较高的情况下。LIT-LVM 还为特征提供了低维潜表示，这对于可视化和分析它们之间的关系非常有用。", "summary": "本文介绍了 LIT-LVM，一种利用潜变量模型对线性预测器中交互项进行结构化正则化的新方法。该方法通过在低维空间中将每个特征表示为一个潜向量，从而对交互项的系数施加近似的低维结构。这种方法被视为一种结构化正则化，在高维设置中比标准正则化器更能有效缓解过拟合。实验结果表明，与 elastic net 和因子分解机相比，LIT-LVM 在预测精度方面表现更优，尤其是在交互项数量远超样本数量的情况下。此外，LIT-LVM 还提供了有用的低维特征潜表示，有助于可视化和分析特征之间的关系。", "keywords": "结构化正则化, 交互项, 潜变量模型, 线性预测器, 高维", "comments": "LIT-LVM 的创新之处在于它提出了一种利用潜变量模型对线性预测器中的交互项进行结构化正则化的新颖方法。通过假设交互项系数的低维结构，该方法在高维设置中有效缓解了过拟合，并提供了可解释的特征表示，这在实践中非常重要。抽象中未提及局限性。"}}
{"id": "2506.15606", "title": "LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning", "authors": ["Gabrel J. Perin", "Runjin Chen", "Xuxi Chen", "Nina S. T. Hirata", "Zhangyang Wang", "Junyuan Hong"], "summary": "Large Language Models (LLMs) have become indispensable in real-world\napplications. However, their widespread adoption raises significant safety\nconcerns, particularly in responding to socially harmful questions. Despite\nsubstantial efforts to improve model safety through alignment, aligned models\ncan still have their safety protections undermined by subsequent fine-tuning -\neven when the additional training data appears benign. In this paper, we\nempirically demonstrate that this vulnerability stems from the sensitivity of\nsafety-critical low-rank subspaces in LLM parameters to fine-tuning. Building\non this insight, we propose a novel training-free method, termed Low-Rank\nExtrapolation (LoX), to enhance safety robustness by extrapolating the safety\nsubspace of an aligned LLM. Our experimental results confirm the effectiveness\nof LoX, demonstrating significant improvements in robustness against both\nbenign and malicious fine-tuning attacks while preserving the model's\nadaptability to new tasks. For instance, LoX leads to 11% to 54% absolute\nreductions in attack success rates (ASR) facing benign or malicious fine-tuning\nattacks. By investigating the ASR landscape of parameters, we attribute the\nsuccess of LoX to that the extrapolation moves LLM parameters to a flatter\nzone, thereby less sensitive to perturbations. The code is available at\ngithub.com/VITA-Group/LoX.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15606v1", "AI": {"title_translation": "LoX: 低秩外推增强LLM安全对抗微调", "tldr": "LoX是一种无需训练的方法，通过低秩外推提高大型语言模型（LLM）的安全性，使其在微调后仍能抵抗恶意攻击，同时保持任务适应性。", "motivation": "大型语言模型（LLMs）在实际应用中广泛采用，但其安全性面临重大挑战，尤其是在回应有害问题时。尽管已进行对齐，但即使是良性微调数据，也能削弱LLM的安全保护，这源于安全关键的低秩子空间对微调的敏感性。", "method": "提出了一种名为低秩外推（LoX）的无需训练的方法。该方法通过外推对齐LLM的安全子空间来增强模型安全鲁棒性。", "result": "LoX显著提高了模型对良性和恶意微调攻击的鲁棒性，同时保留了模型对新任务的适应性。例如，LoX使攻击成功率（ASR）在面对良性或恶意微调攻击时绝对降低了11%到54%。研究发现，外推将LLM参数移动到更平坦的区域，从而降低了对扰动的敏感性。", "conclusion": "LoX通过将LLM参数移动到更平坦的区域，有效增强了LLM的安全性，使其在微调后仍能抵抗攻击，同时保持了任务适应性。", "translation": "大型语言模型（LLMs）已成为现实世界应用中不可或缺的一部分。然而，它们的广泛采用引发了重大的安全担忧，特别是在回应社会有害问题方面。尽管为通过对齐提高模型安全性付出了巨大努力，但对齐的模型其安全保护仍可能被后续的微调所削弱——即使额外的训练数据看起来是良性的。在本文中，我们通过经验证明，这种脆弱性源于LLM参数中安全关键的低秩子空间对微调的敏感性。基于这一洞察，我们提出了一种新颖的无需训练的方法，称为低秩外推（LoX），通过外推对齐LLM的安全子空间来增强安全鲁棒性。我们的实验结果证实了LoX的有效性，表明在对抗良性和恶意微调攻击方面，模型鲁棒性显著提高，同时保留了模型对新任务的适应性。例如，LoX使面对良性或恶意微调攻击时的攻击成功率（ASR）绝对降低了11%到54%。通过研究参数的ASR景观，我们将LoX的成功归因于外推将LLM参数移动到更平坦的区域，从而降低了对扰动的敏感性。代码可在github.com/VITA-Group/LoX获取。", "summary": "本文提出了一种名为LoX的无需训练方法，旨在解决大型语言模型（LLMs）在微调后安全性被削弱的问题。研究发现，LLM参数中安全关键的低秩子空间对微调敏感，导致模型易受攻击。LoX通过外推LLM的安全子空间来增强鲁棒性，实验证明其能显著降低良性或恶意微调攻击的攻击成功率（ASR），降幅达11%至54%，同时不影响模型对新任务的适应性。LoX的成功在于它将LLM参数推向一个更平坦的区域，降低了对扰动的敏感度。", "keywords": "大型语言模型安全, 微调鲁棒性, 低秩外推, 模型对齐, 攻击成功率", "comments": "这篇论文的创新点在于提出了一个无需训练（training-free）的方法LoX来提升LLM的安全性，有效解决了即使是良性微调也能损害模型安全的问题。其核心洞察是识别出安全关键的低秩子空间对微调的敏感性，并通过低秩外推来稳定这些子空间，将参数移动到更平坦的区域，从而增强了模型的鲁棒性。这种方法在实际应用中具有重要意义，因为它提供了一种高效且易于部署的安全加固方案。"}}
{"id": "2506.15499", "title": "Pixel-level Certified Explanations via Randomized Smoothing", "authors": ["Alaa Anani", "Tobias Lorenz", "Mario Fritz", "Bernt Schiele"], "summary": "Post-hoc attribution methods aim to explain deep learning predictions by\nhighlighting influential input pixels. However, these explanations are highly\nnon-robust: small, imperceptible input perturbations can drastically alter the\nattribution map while maintaining the same prediction. This vulnerability\nundermines their trustworthiness and calls for rigorous robustness guarantees\nof pixel-level attribution scores. We introduce the first certification\nframework that guarantees pixel-level robustness for any black-box attribution\nmethod using randomized smoothing. By sparsifying and smoothing attribution\nmaps, we reformulate the task as a segmentation problem and certify each\npixel's importance against $\\ell_2$-bounded perturbations. We further propose\nthree evaluation metrics to assess certified robustness, localization, and\nfaithfulness. An extensive evaluation of 12 attribution methods across 5\nImageNet models shows that our certified attributions are robust,\ninterpretable, and faithful, enabling reliable use in downstream tasks. Our\ncode is at https://github.com/AlaaAnani/certified-attributions.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15499v1", "AI": {"title_translation": "像素级可认证解释通过随机平滑", "tldr": "提出首个使用随机平滑为任何黑盒归因方法提供像素级鲁棒性保证的认证框架，解决现有解释方法不鲁棒的问题。", "motivation": "现有的后验归因方法在解释深度学习预测时，其生成的归因图对微小输入扰动高度不鲁棒，这损害了其可信度，因此需要对像素级归因分数提供严格的鲁棒性保证。", "method": "引入了一个使用随机平滑的认证框架，能为任何黑盒归因方法提供像素级鲁棒性保证。该方法通过稀疏化和平滑归因图，将任务重新表述为分割问题，并针对 $\\ell_2$ 范数有界扰动认证每个像素的重要性。此外，还提出了三个评估指标来评估认证鲁棒性、定位性和忠实性。", "result": "对12种归因方法在5个ImageNet模型上的广泛评估表明，所提出的认证归因具有鲁棒性、可解释性和忠实性，从而可以在下游任务中可靠使用。", "conclusion": "通过引入首个像素级认证解释框架，解决了现有归因方法缺乏鲁棒性的问题，并证明了其在实际应用中的可靠性。", "translation": "后验归因方法旨在通过突出有影响力的输入像素来解释深度学习预测。然而，这些解释高度不鲁棒：微小、难以察觉的输入扰动可以极大地改变归因图，同时保持相同的预测。这种脆弱性削弱了它们的可信度，并要求对像素级归因分数提供严格的鲁棒性保证。我们引入了第一个认证框架，该框架使用随机平滑为任何黑盒归因方法提供像素级鲁棒性。通过稀疏化和平滑归因图，我们将任务重新表述为分割问题，并针对 $\\ell_2$ 范数有界扰动认证每个像素的重要性。我们进一步提出了三个评估指标来评估认证鲁棒性、定位性和忠实性。对5个ImageNet模型上的12种归因方法的广泛评估表明，我们认证的归因是鲁棒的、可解释的和忠实的，从而可以在下游任务中可靠使用。我们的代码位于 https://github.com/AlaaAnani/certified-attributions。", "summary": "本文针对深度学习后验归因方法中归因图对微小扰动不鲁棒的问题，提出了首个基于随机平滑的像素级认证框架。该框架将归因图处理为分割问题，并能为任何黑盒归因方法提供像素级鲁棒性保证。实验结果表明，所认证的归因方法具有鲁棒性、可解释性和忠实性，提升了归因解释在实际应用中的可靠性。", "keywords": "像素级解释, 随机平滑, 归因方法, 鲁棒性, 可认证", "comments": "这项工作通过引入首个像素级认证框架，为深度学习模型的解释性（特别是归因方法）带来了严格的鲁棒性保证，解决了现有方法普遍存在的脆弱性问题，极大地提升了归因解释的可信度和实用性。其创新点在于将归因任务转化为分割问题并利用随机平滑进行认证。"}}
{"id": "2506.15651", "title": "AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning", "authors": ["Tevin Wang", "Chenyan Xiong"], "summary": "Rule-based rewards offer a promising strategy for improving reinforcement\nlearning from human feedback (RLHF), but current approaches often rely on\nmanual rule engineering. We present AutoRule, a fully automated method for\nextracting rules from preference feedback and formulating them into rule-based\nrewards. AutoRule extraction operates in three stages: it leverages a reasoning\nmodel to interpret user preferences, identifies candidate rules from the\nreasoning chain of these interpretations, and synthesizes them into a unified\nrule set. Leveraging the finalized rule set, we employ language-model verifiers\nto compute the fraction of rules satisfied by each output, using this metric as\nan auxiliary reward alongside the learned reward model during policy\noptimization. Training a Llama-3-8B model with AutoRule results in a 28.6\\%\nrelative improvement in length-controlled win rate on AlpacaEval2.0, and a\n6.1\\% relative gain in second-turn performance on a held-out MT-Bench subset,\ncompared to a GRPO baseline trained with the same learned reward model but\nwithout the rule-based auxiliary reward. Our analysis confirms that the\nextracted rules exhibit good agreement with dataset preference. We find that\nAutoRule demonstrates reduced reward hacking compared to a learned reward model\nwhen run over two episodes. Finally, our case study suggests that the extracted\nrules capture unique qualities valued in different datasets. The extracted\nrules are provided in the appendix, and the code is open-sourced at\nhttps://github.com/cxcscmu/AutoRule.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15651v1", "AI": {"title_translation": "AutoRule: 思考链提取的基于规则的奖励改进偏好学习", "tldr": "AutoRule是一个自动化方法，用于从偏好反馈中提取规则并将其转化为奖励，从而显著提升RLHF的性能，减少奖励作弊。", "motivation": "当前的基于规则的奖励方法在强化学习中依赖于手动规则工程，这限制了其应用和效率。本文旨在自动化这一过程。", "method": "AutoRule包含三个阶段：利用推理模型解释用户偏好，从这些解释的推理链中识别候选规则，并将它们合成为统一的规则集。最终的规则集通过语言模型验证器计算每个输出满足规则的比例，此指标作为辅助奖励与学习到的奖励模型一起用于策略优化。", "result": "使用AutoRule训练的Llama-3-8B模型在AlpacaEval2.0上，与GRPO基线相比，长度控制的胜率相对提高了28.6%；在MT-Bench的保留子集上，第二轮性能相对提高了6.1%。分析证实提取的规则与数据集偏好高度一致，并且AutoRule在两个回合中运行显示出比学习奖励模型更低的奖励作弊。", "conclusion": "AutoRule提供了一种有效且自动化的方法来提取和利用基于规则的奖励，显著提升了RLHF的性能，并减少了奖励作弊问题，证明了其在捕捉数据集独特价值方面的潜力。", "translation": "基于规则的奖励为从人类反馈中进行强化学习（RLHF）提供了一种有前景的策略，但当前的方法通常依赖于手动规则工程。我们提出了AutoRule，这是一种从偏好反馈中提取规则并将其表述为基于规则的奖励的完全自动化方法。AutoRule的提取过程分三个阶段操作：它利用推理模型来解释用户偏好，从这些解释的推理链中识别候选规则，并将它们合成为一个统一的规则集。利用最终确定的规则集，我们采用语言模型验证器来计算每个输出满足规则的比例，并将此度量作为辅助奖励与学习到的奖励模型一起在策略优化期间使用。使用AutoRule训练Llama-3-8B模型，与使用相同学习奖励模型但没有基于规则辅助奖励的GRPO基线相比，在AlpacaEval2.0上，长度控制的胜率相对提高了28.6%，在MT-Bench的保留子集上，第二轮性能相对提高了6.1%。我们的分析证实，提取的规则与数据集偏好表现出良好的一致性。我们发现，与学习奖励模型相比，AutoRule在运行两个回合时表现出更少的奖励作弊。最后，我们的案例研究表明，提取的规则捕捉了不同数据集中所重视的独特品质。提取的规则在附录中提供，代码已在https://github.com/cxcscmu/AutoRule开源。", "summary": "AutoRule是一个自动化的框架，旨在解决RLHF中手动规则工程的局限性。它通过三个阶段从用户偏好中提取规则：推理模型解释偏好、从推理链中识别候选规则、以及合成统一的规则集。这些规则随后被用作辅助奖励来指导策略优化。实验结果表明，AutoRule显著提高了Llama-3-8B模型在AlpacaEval2.0和MT-Bench上的性能，并有效减少了奖励作弊问题，证明了其在自动化规则提取和提升偏好学习方面的有效性。", "keywords": "强化学习, 人类反馈, 规则提取, 奖励建模, 自动化", "comments": "AutoRule的创新之处在于其完全自动化的规则提取流程，这极大地降低了人工干预的需求，克服了现有方法的瓶颈。通过将提取的规则作为辅助奖励，它有效地补充了学习到的奖励模型，提升了模型性能并缓解了奖励作弊问题。该方法对于推动RLHF的实际应用具有重要意义。"}}
{"id": "2506.15506", "title": "Insights on Adversarial Attacks for Tabular Machine Learning via a Systematic Literature Review", "authors": ["Salijona Dyrmishi", "Mohamed Djilani", "Thibault Simonetto", "Salah Ghamizi", "Maxime Cordy"], "summary": "Adversarial attacks in machine learning have been extensively reviewed in\nareas like computer vision and NLP, but research on tabular data remains\nscattered. This paper provides the first systematic literature review focused\non adversarial attacks targeting tabular machine learning models. We highlight\nkey trends, categorize attack strategies and analyze how they address practical\nconsiderations for real-world applicability. Additionally, we outline current\nchallenges and open research questions. By offering a clear and structured\noverview, this review aims to guide future efforts in understanding and\naddressing adversarial vulnerabilities in tabular machine learning.", "comment": "This paper is currently under review at ACM Computing Surveys", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15506v1", "AI": {"title_translation": "通过系统文献综述深入了解表格机器学习的对抗性攻击", "tldr": "本文首次对针对表格机器学习模型的对抗性攻击进行了系统文献综述，旨在为未来研究提供指导。", "motivation": "机器学习中的对抗性攻击在计算机视觉和自然语言处理等领域已被广泛回顾，但针对表格数据的研究仍然分散。本文旨在填补这一空白，提供首个专注于表格机器学习模型对抗性攻击的系统文献综献。", "method": "本文采用系统文献综述的方法，聚焦于针对表格机器学习模型的对抗性攻击。", "result": "综述结果突出了关键趋势，对攻击策略进行了分类，并分析了它们如何解决实际应用中的考虑因素。此外，还概述了当前的挑战和开放性研究问题。", "conclusion": "通过提供清晰和结构化的概述，本综述旨在指导未来在理解和解决表格机器学习中对抗性漏洞方面的努力。", "translation": "机器学习中的对抗性攻击已在计算机视觉和自然语言处理等领域得到广泛回顾，但针对表格数据的研究仍显分散。本文提供了首个专注于针对表格机器学习模型对抗性攻击的系统文献综述。我们强调了关键趋势，对攻击策略进行了分类，并分析了它们如何解决实际适用性的实际考虑因素。此外，我们还概述了当前的挑战和开放性研究问题。通过提供清晰和结构化的概述，本综述旨在指导未来在理解和解决表格机器学习中对抗性漏洞方面的努力。", "summary": "本文是首个针对表格机器学习模型对抗性攻击的系统文献综述。它分析了该领域分散的研究现状，识别了关键趋势，对攻击策略进行了分类，并探讨了实际应用考量。该综述还指出了当前面临的挑战和未解决的研究问题，旨在为未来理解和应对表格机器学习对抗性漏洞的研究提供方向。", "keywords": "对抗性攻击, 表格机器学习, 系统文献综述, 机器学习安全, 漏洞", "comments": "本文的创新之处在于它是首次专门针对表格机器学习领域对抗性攻击的系统文献综述，填补了该领域研究分散的空白。其重要性在于为研究人员提供了一个结构化的概述，有助于识别关键趋势、现有挑战和未来的研究方向，从而推动表格数据安全性的发展。"}}
{"id": "2506.15507", "title": "Over-squashing in Spatiotemporal Graph Neural Networks", "authors": ["Ivan Marisca", "Jacob Bamberger", "Cesare Alippi", "Michael M. Bronstein"], "summary": "Graph Neural Networks (GNNs) have achieved remarkable success across various\ndomains. However, recent theoretical advances have identified fundamental\nlimitations in their information propagation capabilities, such as\nover-squashing, where distant nodes fail to effectively exchange information.\nWhile extensively studied in static contexts, this issue remains unexplored in\nSpatiotemporal GNNs (STGNNs), which process sequences associated with graph\nnodes. Nonetheless, the temporal dimension amplifies this challenge by\nincreasing the information that must be propagated. In this work, we formalize\nthe spatiotemporal over-squashing problem and demonstrate its distinct\ncharacteristics compared to the static case. Our analysis reveals that\ncounterintuitively, convolutional STGNNs favor information propagation from\npoints temporally distant rather than close in time. Moreover, we prove that\narchitectures that follow either time-and-space or time-then-space processing\nparadigms are equally affected by this phenomenon, providing theoretical\njustification for computationally efficient implementations. We validate our\nfindings on synthetic and real-world datasets, providing deeper insights into\ntheir operational dynamics and principled guidance for more effective designs.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15507v1", "AI": {"title_translation": "时空图神经网络中的过度挤压问题", "tldr": "本文形式化并分析了时空图神经网络（STGNNs）中的过度挤压问题，发现卷积STGNNs更倾向于传播时间上遥远而非接近的信息，并证明不同处理范式下的架构同样受此问题影响。", "motivation": "图神经网络（GNNs）在信息传播能力上存在局限性，如“过度挤压”问题，即远距离节点无法有效交换信息。尽管该问题在静态GNNs中已被广泛研究，但在时空图神经网络（STGNNs）中仍未被探索，且时间维度会放大这一挑战。", "method": "本文形式化了时空过度挤压问题，并分析了其与静态情况的不同特性。通过理论分析和证明，揭示了卷积STGNNs的信息传播偏好，并探讨了不同处理范式下架构受影响的程度。研究结果在合成和真实世界数据集上进行了验证。", "result": "分析表明，卷积STGNNs反常地偏向于传播时间上遥远而非接近的信息。此外，本文证明了遵循“时间-空间”或“时间然后空间”处理范式的架构同样受到过度挤压现象的影响。", "conclusion": "本文首次形式化了时空图神经网络中的过度挤压问题，揭示了其独特的传播特性，并为更有效的设计提供了理论指导和深入见解。", "translation": "图神经网络（GNNs）在各个领域取得了显著成功。然而，最近的理论进展发现其信息传播能力存在根本性局限，例如过度挤压，即远距离节点未能有效交换信息。尽管这个问题在静态环境中得到了广泛研究，但在处理与图节点相关联序列的时空图神经网络（STGNNs）中仍未被探索。尽管如此，时间维度通过增加必须传播的信息量而加剧了这一挑战。在这项工作中，我们形式化了时空过度挤压问题，并展示了其与静态情况相比的独特特征。我们的分析表明，反直觉的是，卷积STGNNs更倾向于传播来自时间上遥远而非接近的点的信息。此外，我们证明了遵循时间-空间或时间然后空间处理范式的架构同样受到这种现象的影响，这为计算效率高的实现提供了理论依据。我们在合成和真实世界数据集上验证了我们的发现，为它们的运行动态提供了更深入的见解，并为更有效的设计提供了原则性指导。", "summary": "本文首次形式化并深入分析了时空图神经网络（STGNNs）中尚未被探索的过度挤压问题。研究发现，与静态GNNs不同，STGNNs中的过度挤压问题因时间维度而加剧。理论分析表明，卷积STGNNs反常地倾向于传播时间上较远的信息，并且无论是“时间-空间”还是“时间然后空间”的处理范式，其架构都同样受到过度挤压的影响。这些发现通过合成和真实世界数据集得到验证，为STGNNs的设计提供了新的见解和指导。", "keywords": "时空图神经网络, 过度挤压, 信息传播, 卷积GNN, 理论分析", "comments": "本文首次将“过度挤压”这一在静态图神经网络中已广泛研究的问题引入并形式化到时空图神经网络领域，具有重要的理论创新性。其发现卷积STGNNs的“反直觉”信息传播偏好，以及不同处理范式下过度挤压的普遍性，为未来STGNNs的架构设计提供了关键的理论指导，有助于提升其信息传播效率和性能。"}}
{"id": "2506.15679", "title": "Dense SAE Latents Are Features, Not Bugs", "authors": ["Xiaoqing Sun", "Alessandro Stolfo", "Joshua Engels", "Ben Wu", "Senthooran Rajamanoharan", "Mrinmaya Sachan", "Max Tegmark"], "summary": "Sparse autoencoders (SAEs) are designed to extract interpretable features\nfrom language models by enforcing a sparsity constraint. Ideally, training an\nSAE would yield latents that are both sparse and semantically meaningful.\nHowever, many SAE latents activate frequently (i.e., are \\emph{dense}), raising\nconcerns that they may be undesirable artifacts of the training procedure. In\nthis work, we systematically investigate the geometry, function, and origin of\ndense latents and show that they are not only persistent but often reflect\nmeaningful model representations. We first demonstrate that dense latents tend\nto form antipodal pairs that reconstruct specific directions in the residual\nstream, and that ablating their subspace suppresses the emergence of new dense\nfeatures in retrained SAEs -- suggesting that high density features are an\nintrinsic property of the residual space. We then introduce a taxonomy of dense\nlatents, identifying classes tied to position tracking, context binding,\nentropy regulation, letter-specific output signals, part-of-speech, and\nprincipal component reconstruction. Finally, we analyze how these features\nevolve across layers, revealing a shift from structural features in early\nlayers, to semantic features in mid layers, and finally to output-oriented\nsignals in the last layers of the model. Our findings indicate that dense\nlatents serve functional roles in language model computation and should not be\ndismissed as training noise.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15679v1", "AI": {"title_translation": "密集型SAE潜变量是特征，而非缺陷", "tldr": "稀疏自编码器（SAE）中常被视为“缺陷”的密集型潜变量，实际上是具有功能性作用的有意义特征，不应被视为训练噪声。", "motivation": "稀疏自编码器（SAE）旨在通过强制稀疏性约束来提取语言模型中的可解释特征。然而，许多SAE潜变量激活频繁（即是“密集”的），这引发了它们可能是训练过程产生的 undesirable artifacts（不良伪影）的担忧。本研究旨在系统地调查这些密集型潜变量的几何结构、功能和起源，以证明它们并非缺陷。", "method": "本研究系统地调查了密集型潜变量的几何结构、功能和起源。研究者首先证明了密集型潜变量倾向于形成对跖对，并能重构残差流中的特定方向；通过消融其子空间，可以抑制重新训练的SAE中新密集特征的出现。接着，研究者引入了密集型潜变量的分类法，识别出与位置追踪、上下文绑定、熵调节、特定字母输出信号、词性以及主成分重构相关的类别。最后，研究者分析了这些特征在不同层级上的演变。", "result": "研究发现，密集型潜变量不仅持续存在，而且常常反映了有意义的模型表示。它们倾向于形成对跖对，重构残差流中的特定方向。消融其子空间可以抑制新的密集特征的出现，表明高密度特征是残差空间的内在属性。研究引入的分类法识别出与位置追踪、上下文绑定、熵调节、特定字母输出信号、词性以及主成分重构相关的密集潜变量类别。此外，分析显示这些特征在层级上发生演变，从早期层的结构特征，到中间层的语义特征，再到最后层的面向输出的信号。", "conclusion": "本研究的发现表明，密集型潜变量在语言模型计算中发挥着功能性作用，不应被视为训练噪声。", "translation": "稀疏自编码器（SAE）旨在通过强制稀疏性约束来从语言模型中提取可解释的特征。理想情况下，训练SAE会产生既稀疏又语义有意义的潜变量。然而，许多SAE潜变量激活频繁（即是“密集”的），这引发了它们可能是训练过程产生的 undesirable artifacts（不良伪影）的担忧。在这项工作中，我们系统地调查了密集型潜变量的几何结构、功能和起源，并表明它们不仅持续存在，而且常常反映了有意义的模型表示。我们首先证明了密集型潜变量倾向于形成对跖对，重构残差流中的特定方向，并且消融其子空间会抑制重新训练的SAE中新密集特征的出现——这表明高密度特征是残差空间的内在属性。然后，我们引入了密集型潜变量的分类法，识别出与位置追踪、上下文绑定、熵调节、特定字母输出信号、词性以及主成分重构相关的类别。最后，我们分析了这些特征在层级上的演变，揭示了从早期层的结构特征，到中间层的语义特征，再到模型最后层的面向输出信号的转变。我们的发现表明，密集型潜变量在语言模型计算中发挥着功能性作用，不应被视为训练噪声。", "summary": "本论文研究了稀疏自编码器（SAE）中常被误认为是训练伪影的“密集型”潜变量。通过系统性调查，研究发现这些密集型潜变量并非缺陷，而是语言模型中有意义且功能性的特征。它们表现出特定的几何结构（如形成对跖对），并参与多种功能，例如位置追踪、上下文绑定和输出信号生成。研究还揭示了这些特征在模型不同层级上的演变模式。论文最终指出，密集型潜变量在语言模型计算中扮演重要角色，不应被忽视。", "keywords": "稀疏自编码器, 密集潜变量, 语言模型, 可解释性, 特征分析", "comments": "这篇论文挑战了稀疏自编码器（SAE）领域中一个普遍的假设，即高频激活的“密集”潜变量是无用的训练噪声。通过详尽的分析和实验，它证明了这些密集型潜变量实际上是语言模型中具有特定功能和语义意义的特征。这项工作对于深入理解SAE如何编码信息以及提升大型语言模型的可解释性具有重要意义，可能为未来更有效、更具解释性的SAE设计提供新的方向。"}}
{"id": "2506.15513", "title": "RePCS: Diagnosing Data Memorization in LLM-Powered Retrieval-Augmented Generation", "authors": ["Le Vu Anh", "Nguyen Viet Anh", "Mehmet Dik", "Luong Van Nghia"], "summary": "Retrieval-augmented generation (RAG) has become a common strategy for\nupdating large language model (LLM) responses with current, external\ninformation. However, models may still rely on memorized training data, bypass\nthe retrieved evidence, and produce contaminated outputs. We introduce\nRetrieval-Path Contamination Scoring (RePCS), a diagnostic method that detects\nsuch behavior without requiring model access or retraining. RePCS compares two\ninference paths: (i) a parametric path using only the query, and (ii) a\nretrieval-augmented path using both the query and retrieved context by\ncomputing the Kullback-Leibler (KL) divergence between their output\ndistributions. A low divergence suggests that the retrieved context had minimal\nimpact, indicating potential memorization. This procedure is model-agnostic,\nrequires no gradient or internal state access, and adds only a single\nadditional forward pass. We further derive PAC-style guarantees that link the\nKL threshold to user-defined false positive and false negative rates. On the\nPrompt-WNQA benchmark, RePCS achieves a ROC-AUC of 0.918. This result\noutperforms the strongest prior method by 6.5 percentage points while keeping\nlatency overhead below 4.7% on an NVIDIA T4 GPU. RePCS offers a lightweight,\nblack-box safeguard to verify whether a RAG system meaningfully leverages\nretrieval, making it especially valuable in safety-critical applications.", "comment": "11 pages, 7 figures, 5 tables", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15513v1", "AI": {"title_translation": "RePCS：诊断LLM驱动的检索增强生成中的数据记忆问题", "tldr": "RePCS是一种轻量级、黑盒诊断方法，用于检测LLM驱动的检索增强生成（RAG）系统中模型是否依赖记忆数据而非检索到的证据。", "motivation": "检索增强生成（RAG）是更新大型语言模型（LLM）响应的常用策略，但模型仍可能依赖记忆的训练数据，绕过检索到的证据，并产生受污染的输出。因此需要一种方法来诊断这种行为。", "method": "论文介绍了检索路径污染评分（RePCS），这是一种诊断方法，通过计算参数路径（仅使用查询）和检索增强路径（使用查询和检索上下文）之间输出分布的Kullback-Leibler（KL）散度来检测模型是否依赖记忆数据。低散度表明检索上下文影响最小，可能存在记忆化。该方法与模型无关，无需访问梯度或内部状态，只需额外一次前向传播。", "result": "在Prompt-WNQA基准测试中，RePCS的ROC-AUC达到0.918，比最强现有方法高出6.5个百分点，同时在NVIDIA T4 GPU上的延迟开销低于4.7%。", "conclusion": "RePCS提供了一种轻量级、黑盒的保障措施，用于验证RAG系统是否有效利用了检索，这在安全关键型应用中尤其有价值。", "translation": "检索增强生成（RAG）已成为使用当前外部信息更新大型语言模型（LLM）响应的常用策略。然而，模型可能仍依赖于记忆的训练数据，绕过检索到的证据，并产生受污染的输出。我们引入了检索路径污染评分（RePCS），这是一种诊断方法，无需模型访问或重新训练即可检测此类行为。RePCS通过计算其输出分布之间的Kullback-Leibler（KL）散度来比较两条推理路径：(i) 仅使用查询的参数路径，以及 (ii) 同时使用查询和检索上下文的检索增强路径。低散度表明检索上下文影响最小，指示可能存在记忆化。此过程与模型无关，无需梯度或内部状态访问，并且仅增加一次额外的前向传播。我们进一步推导了PAC风格的保证，将KL阈值与用户定义的假阳性率和假阴性率相关联。在Prompt-WNQA基准测试中，RePCS的ROC-AUC达到0.918。此结果比最强的现有方法高出6.5个百分点，同时在NVIDIA T4 GPU上的延迟开销低于4.7%。RePCS提供了一种轻量级、黑盒的保障措施，用于验证RAG系统是否有效利用了检索，这在安全关键型应用中尤其有价值。", "summary": "RePCS是一种用于诊断LLM驱动的检索增强生成（RAG）系统中数据记忆问题的轻量级、黑盒方法。它通过比较模型的参数路径和检索增强路径的输出分布之间的KL散度来检测模型是否有效利用了检索到的信息。实验结果表明，RePCS在Prompt-WNQA基准测试上表现优异，ROC-AUC达到0.918，且具有低延迟开销，使其成为安全关键应用中验证RAG系统有效性的重要工具。", "keywords": "检索增强生成, 数据记忆, LLM, RePCS, KL散度", "comments": "RePCS的创新之处在于它提供了一种无需模型访问或重新训练的黑盒诊断方法，且与模型无关，这大大降低了其应用门槛。其对KL散度的巧妙运用，以及PAC风格的保证，增加了方法的理论严谨性。在安全关键应用中，确保RAG系统有效利用检索而非依赖记忆数据至关重要，RePCS为此提供了一个实用的解决方案。其高性能和低开销使其具有很高的实用价值。"}}
{"id": "2506.15535", "title": "A Simplified Analysis of SGD for Linear Regression with Weight Averaging", "authors": ["Alexandru Meterez", "Depen Morwani", "Costin-Andrei Oncescu", "Jingfeng Wu", "Cengiz Pehlevan", "Sham Kakade"], "summary": "Theoretically understanding stochastic gradient descent (SGD) in\noverparameterized models has led to the development of several optimization\nalgorithms that are widely used in practice today. Recent work\nby~\\citet{zou2021benign} provides sharp rates for SGD optimization in linear\nregression using constant learning rate, both with and without tail iterate\naveraging, based on a bias-variance decomposition of the risk. In our work, we\nprovide a simplified analysis recovering the same bias and variance bounds\nprovided in~\\citep{zou2021benign} based on simple linear algebra tools,\nbypassing the requirement to manipulate operators on positive semi-definite\n(PSD) matrices. We believe our work makes the analysis of SGD on linear\nregression very accessible and will be helpful in further analyzing\nmini-batching and learning rate scheduling, leading to improvements in the\ntraining of realistic models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15535v1", "AI": {"title_translation": "带有权重平均的线性回归SGD简化分析", "tldr": "本文提供了一种基于简单线性代数工具的SGD线性回归简化分析方法，该方法能恢复现有研究的偏差和方差界限，并使分析更易于理解。", "motivation": "现有对过参数化模型中随机梯度下降（SGD）的理论分析，特别是Zou等人（2021）的工作，虽然提供了精确的速率界限，但其分析涉及对半正定（PSD）矩阵的操作，可能较为复杂。本文的动机是提供一个基于简单线性代数工具的简化分析，使其更易于理解和应用。", "method": "本文采用基于简单线性代数工具的简化分析方法，绕过了对半正定（PSD）矩阵进行操作的要求，从而恢复了Zou等人（2021）工作中提供的相同偏差和方差界限。", "result": "本文的简化分析成功恢复了Zou等人（2021）工作中提供的相同偏差和方差界限。", "conclusion": "本文的工作使得SGD在线性回归上的分析变得非常容易理解，并将有助于进一步分析小批量处理（mini-batching）和学习率调度（learning rate scheduling），从而改进实际模型的训练。", "translation": "从理论上理解过参数化模型中的随机梯度下降（SGD）促使了当今实践中广泛使用的几种优化算法的发展。Zou等人（2021）最近的工作基于风险的偏差-方差分解，为使用恒定学习率的线性回归中的SGD优化提供了精确的速率，无论是否包含尾部迭代平均。在我们的工作中，我们提供了一种简化的分析方法，该方法基于简单的线性代数工具，绕过了对半正定（PSD）矩阵操作的要求，从而恢复了Zou等人（2021）提供的相同偏差和方差界限。我们相信我们的工作使得线性回归上SGD的分析非常容易理解，并将有助于进一步分析小批量处理和学习率调度，从而改进实际模型的训练。", "summary": "本文针对过参数化模型中随机梯度下降（SGD）在线性回归上的分析，提出了一种简化方法。该方法利用简单的线性代数工具，成功地恢复了现有研究中关于SGD偏差和方差的精确界限，避免了复杂的操作符操作。这项工作旨在提高SGD分析的可理解性，并为未来在小批量处理和学习率调度方面的进一步研究提供便利，最终有助于改进实际模型的训练。", "keywords": "随机梯度下降, 线性回归, 权重平均, 简化分析, 偏差-方差分解", "comments": "本文的创新之处在于提供了一种更易于理解和操作的SGD理论分析方法，特别是绕过了复杂的PSD矩阵操作。这对于推动SGD在过参数化模型中的理论研究具有重要意义，因为它降低了分析的门槛，并有望加速在实际应用中优化算法的改进。其重要性在于，简化后的分析可能促进对SGD在更复杂场景（如mini-batching和学习率调度）下行为的深入理解。"}}
{"id": "2506.15544", "title": "Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning", "authors": ["Roger Creus Castanyer", "Johan Obando-Ceron", "Lu Li", "Pierre-Luc Bacon", "Glen Berseth", "Aaron Courville", "Pablo Samuel Castro"], "summary": "Scaling deep reinforcement learning networks is challenging and often results\nin degraded performance, yet the root causes of this failure mode remain poorly\nunderstood. Several recent works have proposed mechanisms to address this, but\nthey are often complex and fail to highlight the causes underlying this\ndifficulty. In this work, we conduct a series of empirical analyses which\nsuggest that the combination of non-stationarity with gradient pathologies, due\nto suboptimal architectural choices, underlie the challenges of scale. We\npropose a series of direct interventions that stabilize gradient flow, enabling\nrobust performance across a range of network depths and widths. Our\ninterventions are simple to implement and compatible with well-established\nalgorithms, and result in an effective mechanism that enables strong\nperformance even at large scales. We validate our findings on a variety of\nagents and suites of environments.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15544v1", "AI": {"title_translation": "深度强化学习中大规模稳定学习的稳定梯度", "tldr": "本文通过实证分析指出深度强化学习网络扩展性差的原因是由于非平稳性与梯度病态的结合，并提出了一系列简单的干预措施来稳定梯度流，从而在大规模网络中实现稳定的性能。", "motivation": "扩展深度强化学习网络具有挑战性，并且经常导致性能下降，但这种失败模式的根本原因尚不清楚。尽管最近的一些工作提出了解决方案，但它们通常很复杂，未能突出导致困难的原因。", "method": "本文进行了一系列实证分析，揭示了非平稳性与梯度病态（由于次优架构选择）相结合是导致大规模扩展挑战的根本原因。在此基础上，提出了一系列直接干预措施来稳定梯度流。", "result": "所提出的干预措施能够稳定梯度流，从而在不同网络深度和宽度下实现稳健的性能。这些干预措施易于实现，与现有算法兼容，并能在大规模情况下实现强大的性能。研究结果在各种智能体和环境套件上得到了验证。", "conclusion": "通过稳定梯度流的简单干预措施，可以有效解决深度强化学习网络在大规模扩展时遇到的性能下降问题，从而实现强大的性能。", "translation": "扩展深度强化学习网络具有挑战性，并且经常导致性能下降，但这种失败模式的根本原因尚不清楚。最近的一些工作提出了解决机制，但它们通常很复杂，未能突出导致这种困难的原因。在这项工作中，我们进行了一系列实证分析，表明非平稳性与梯度病态（由于次优架构选择）的结合是导致大规模挑战的根本原因。我们提出了一系列直接干预措施来稳定梯度流，从而在不同网络深度和宽度下实现稳健的性能。我们的干预措施易于实现，与成熟的算法兼容，并形成一种有效的机制，即使在大规模情况下也能实现强大的性能。我们在各种智能体和环境套件上验证了我们的发现。", "summary": "本文通过实证分析探讨了深度强化学习网络在扩展时性能下降的根本原因，指出是由于非平稳性与梯度病态的结合。为解决此问题，作者提出了一系列简单易行的干预措施来稳定梯度流。这些方法被证明能有效提升大规模深度强化学习网络的性能，并在多种智能体和环境中得到验证。", "keywords": "深度强化学习, 梯度稳定, 大规模学习, 非平稳性, 梯度病态", "comments": "该论文的创新点在于通过实证分析揭示了深度强化学习大规模扩展失败的深层原因，即非平稳性与梯度病态的结合。其重要性在于提出了简单且兼容的干预措施来稳定梯度流，有效解决了大规模深度强化学习中的性能下降问题，为未来的研究提供了实用的解决方案。"}}
{"id": "2506.15554", "title": "DAILOC: Domain-Incremental Learning for Indoor Localization using Smartphones", "authors": ["Akhil Singampalli", "Danish Gufran", "Sudeep Pasricha"], "summary": "Wi-Fi fingerprinting-based indoor localization faces significant challenges\nin real-world deployments due to domain shifts arising from device\nheterogeneity and temporal variations within indoor environments. Existing\napproaches often address these issues independently, resulting in poor\ngeneralization and susceptibility to catastrophic forgetting over time. In this\nwork, we propose DAILOC, a novel domain-incremental learning framework that\njointly addresses both temporal and device-induced domain shifts. DAILOC\nintroduces a novel disentanglement strategy that separates domain shifts from\nlocation-relevant features using a multi-level variational autoencoder.\nAdditionally, we introduce a novel memory-guided class latent alignment\nmechanism to address the effects of catastrophic forgetting over time.\nExperiments across multiple smartphones, buildings, and time instances\ndemonstrate that DAILOC significantly outperforms state-of-the-art methods,\nachieving up to 2.74x lower average error and 4.6x lower worst-case error.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15554v1", "AI": {"title_translation": "DAILOC：基于智能手机的室内定位领域增量学习", "tldr": "DAILOC是一个新的领域增量学习框架，它通过解耦策略和记忆引导对齐机制，共同解决了Wi-Fi指纹室内定位中由设备异构性和时间变化引起的领域漂移问题，显著优于现有方法。", "motivation": "Wi-Fi指纹室内定位在实际部署中面临重大挑战，因为设备异构性和室内环境中的时间变化会导致领域漂移。现有方法通常独立解决这些问题，导致泛化能力差，并且容易随时间发生灾难性遗忘。", "method": "本文提出了DAILOC，一个新颖的领域增量学习框架，它共同解决了时间和设备引起的领域漂移。DAILOC引入了一种新颖的解耦策略，使用多级变分自编码器将领域漂移与位置相关特征分离。此外，还引入了一种新颖的记忆引导类潜在对齐机制，以解决随时间发生的灾难性遗忘效应。", "result": "实验表明，DAILOC显著优于现有最先进的方法，平均误差降低了2.74倍，最差情况误差降低了4.6倍。", "conclusion": "DAILOC通过其创新的解耦策略和记忆引导对齐机制，成功地共同解决了Wi-Fi指纹室内定位中的领域漂移和灾难性遗忘问题，显著提升了定位性能。", "translation": "基于Wi-Fi指纹的室内定位在实际部署中面临重大挑战，因为设备异构性和室内环境中的时间变化会导致领域漂移。现有方法通常独立解决这些问题，导致泛化能力差，并且容易随时间发生灾难性遗忘。在这项工作中，我们提出了DAILOC，一个新颖的领域增量学习框架，它共同解决了时间和设备引起的领域漂移。DAILOC引入了一种新颖的解耦策略，使用多级变分自编码器将领域漂移与位置相关特征分离。此外，我们还引入了一种新颖的记忆引导类潜在对齐机制，以解决随时间发生的灾难性遗忘效应。在多个智能手机、建筑物和时间实例上的实验表明，DAILOC显著优于现有最先进的方法，平均误差降低了2.74倍，最差情况误差降低了4.6倍。", "summary": "DAILOC是一个新颖的领域增量学习框架，旨在解决基于Wi-Fi指纹的室内定位中因设备异构性和时间变化导致的领域漂移问题。该框架通过引入多级变分自编码器进行特征解耦，并结合记忆引导的类潜在对齐机制来缓解灾难性遗忘。实验证明，DAILOC在多个智能手机、建筑物和时间实例上表现出色，显著优于现有最先进的方法，大幅降低了平均和最差情况的定位误差。", "keywords": "室内定位, 领域增量学习, Wi-Fi指纹, 领域漂移, 灾难性遗忘", "comments": "DAILOC的创新之处在于它首次共同解决了Wi-Fi指纹室内定位中的时间和设备引起的领域漂移问题，并引入了新颖的解耦策略和记忆引导对齐机制来对抗灾难性遗忘。其在实际部署中的性能提升对于提高室内定位的鲁棒性和准确性具有重要意义。"}}
{"id": "2506.15559", "title": "Towards Explainable Indoor Localization: Interpreting Neural Network Learning on Wi-Fi Fingerprints Using Logic Gates", "authors": ["Danish Gufran", "Sudeep Pasricha"], "summary": "Indoor localization using deep learning (DL) has demonstrated strong accuracy\nin mapping Wi-Fi RSS fingerprints to physical locations; however, most existing\nDL frameworks function as black-box models, offering limited insight into how\npredictions are made or how models respond to real-world noise over time. This\nlack of interpretability hampers our ability to understand the impact of\ntemporal variations - caused by environmental dynamics - and to adapt models\nfor long-term reliability. To address this, we introduce LogNet, a novel logic\ngate-based framework designed to interpret and enhance DL-based indoor\nlocalization. LogNet enables transparent reasoning by identifying which access\npoints (APs) are most influential for each reference point (RP) and reveals how\nenvironmental noise disrupts DL-driven localization decisions. This\ninterpretability allows us to trace and diagnose model failures and adapt DL\nsystems for more stable long-term deployments. Evaluations across multiple\nreal-world building floorplans and over two years of temporal variation show\nthat LogNet not only interprets the internal behavior of DL models but also\nimproves performance-achieving up to 1.1x to 2.8x lower localization error,\n3.4x to 43.3x smaller model size, and 1.5x to 3.6x lower latency compared to\nprior DL-based models.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15559v1", "AI": {"title_translation": "走向可解释的室内定位：使用逻辑门解释Wi-Fi指纹上的神经网络学习", "tldr": "提出LogNet框架，利用逻辑门解释和增强基于深度学习的室内Wi-Fi定位，提高了准确性、缩小了模型大小并降低了延迟，同时提供了可解释性以应对环境变化。", "motivation": "现有的基于深度学习的室内定位模型是黑盒模型，缺乏可解释性，难以理解预测过程、模型对噪声的响应以及如何应对时间变化，从而影响长期可靠性。", "method": "引入LogNet，一个基于逻辑门的新型框架，用于解释和增强基于深度学习的室内定位。LogNet通过识别哪些接入点（AP）对每个参考点最具影响力，并揭示环境噪声如何扰乱定位决策，从而实现透明推理。", "result": "LogNet不仅能够解释深度学习模型的内部行为，还提高了性能，与先前的DL模型相比，定位误差降低了1.1到2.8倍，模型大小缩小了3.4到43.3倍，延迟降低了1.5到3.6倍。", "conclusion": "LogNet通过提供可解释性，使DL系统能够更好地适应长期部署，并显著提升了基于Wi-Fi指纹的室内定位的性能和可靠性。", "translation": "使用深度学习（DL）进行室内定位在将Wi-Fi RSS指纹映射到物理位置方面表现出强大的准确性；然而，大多数现有的DL框架作为黑盒模型运行，对如何进行预测或模型如何随时间响应真实世界噪声提供有限的洞察。这种缺乏可解释性阻碍了我们理解由环境动态引起的时间变化的影响以及调整模型以实现长期可靠性的能力。为了解决这个问题，我们引入了LogNet，一个新颖的基于逻辑门的框架，旨在解释和增强基于DL的室内定位。LogNet通过识别哪些接入点（AP）对每个参考点（RP）最具影响力，并揭示环境噪声如何扰乱DL驱动的定位决策，从而实现透明推理。这种可解释性使我们能够追踪和诊断模型故障，并调整DL系统以实现更稳定的长期部署。在多个真实世界建筑平面图和两年多的时间变化中进行的评估表明，LogNet不仅解释了DL模型的内部行为，而且还提高了性能——与先前的基于DL的模型相比，定位误差降低了1.1到2.8倍，模型大小缩小了3.4到43.3倍，延迟降低了1.5到3.6倍。", "summary": "本文提出了LogNet，一个基于逻辑门的新型框架，旨在解决深度学习在室内Wi-Fi指纹定位中缺乏可解释性的问题。LogNet通过透明推理揭示了关键接入点和环境噪声对定位决策的影响，从而提高了模型对时间变化的适应性和长期可靠性。实验结果表明，LogNet不仅能解释DL模型的内部行为，还能显著提升定位精度、减小模型大小并降低延迟。", "keywords": "室内定位, 深度学习, 可解释性, Wi-Fi指纹, 逻辑门", "comments": "这篇论文通过引入逻辑门来增强深度学习模型的可解释性，为“黑盒”DL模型在室内定位领域的应用带来了创新。其核心贡献在于不仅提供了预测的洞察力，还通过可解释性实现了性能提升和模型优化，尤其是在应对环境动态和实现长期部署方面的价值突出。"}}
{"id": "2506.15566", "title": "Task-Agnostic Experts Composition for Continual Learning", "authors": ["Luigi Quarantiello", "Andrea Cossu", "Vincenzo Lomonaco"], "summary": "Compositionality is one of the fundamental abilities of the human reasoning\nprocess, that allows to decompose a complex problem into simpler elements. Such\nproperty is crucial also for neural networks, especially when aiming for a more\nefficient and sustainable AI framework. We propose a compositional approach by\nensembling zero-shot a set of expert models, assessing our methodology using a\nchallenging benchmark, designed to test compositionality capabilities. We show\nthat our Expert Composition method is able to achieve a much higher accuracy\nthan baseline algorithms while requiring less computational resources, hence\nbeing more efficient.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15566v1", "AI": {"title_translation": "任务无关专家组合的持续学习", "tldr": "提出了一种任务无关的专家组合方法，通过零样本集成专家模型，在持续学习中实现了更高的准确性和更低的计算资源消耗。", "motivation": "组合性是人类推理的基本能力，对于神经网络实现高效和可持续的AI框架也至关重要。", "method": "通过零样本集成一组专家模型，提出了一种组合方法，并在一个旨在测试组合能力的挑战性基准上进行评估。", "result": "专家组合方法比基线算法实现了更高的准确性，同时需要更少的计算资源，因此效率更高。", "conclusion": "提出的专家组合方法在持续学习中表现出优越的性能和效率。", "translation": "组合性是人类推理过程的基本能力之一，它允许将复杂问题分解为更简单的元素。这种特性对于神经网络也至关重要，尤其是在旨在实现更高效和可持续的AI框架时。我们提出了一种通过零样本集成一组专家模型的组合方法，并使用一个旨在测试组合能力挑战性基准来评估我们的方法。我们展示了我们的专家组合方法能够比基线算法实现更高的准确性，同时需要更少的计算资源，因此效率更高。", "summary": "该论文提出了一种名为“专家组合”的零样本集成方法，旨在提高持续学习中神经网络的组合性。通过在一个挑战性基准上进行评估，该方法在保持更高准确性的同时，显著降低了计算资源消耗，从而实现了更高效的AI框架。", "keywords": "持续学习, 专家组合, 零样本, 组合性, 神经网络", "comments": "这篇论文的创新点在于提出了一个任务无关的专家组合方法，它利用零样本集成来模仿人类的组合推理能力。其重要性在于，它为持续学习提供了一个更高效、更可持续的AI框架，解决了传统方法在准确性和资源消耗上的权衡问题。"}}
{"id": "2506.15588", "title": "Memory-Efficient Differentially Private Training with Gradient Random Projection", "authors": ["Alex Mulrooney", "Devansh Gupta", "James Flemings", "Huanyu Zhang", "Murali Annavaram", "Meisam Razaviyayn", "Xinwei Zhang"], "summary": "Differential privacy (DP) protects sensitive data during neural network\ntraining, but standard methods like DP-Adam suffer from high memory overhead\ndue to per-sample gradient clipping, limiting scalability. We introduce\nDP-GRAPE (Gradient RAndom ProjEction), a DP training method that significantly\nreduces memory usage while maintaining utility on par with first-order DP\napproaches. Rather than directly applying DP to GaLore, DP-GRAPE introduces\nthree key modifications: (1) gradients are privatized after projection, (2)\nrandom Gaussian matrices replace SVD-based subspaces, and (3) projection is\napplied during backpropagation. These contributions eliminate the need for\ncostly SVD computations, enable substantial memory savings, and lead to\nimproved utility. Despite operating in lower-dimensional subspaces, our\ntheoretical analysis shows that DP-GRAPE achieves a privacy-utility trade-off\ncomparable to DP-SGD. Our extensive empirical experiments show that DP-GRAPE\ncan reduce the memory footprint of DP training without sacrificing accuracy or\ntraining time. In particular, DP-GRAPE reduces memory usage by over 63% when\npre-training Vision Transformers and over 70% when fine-tuning RoBERTa-Large as\ncompared to DP-Adam, while achieving similar performance. We further\ndemonstrate that DP-GRAPE scales to fine-tuning large models such as OPT with\nup to 6.7 billion parameters.", "comment": null, "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15588v1", "AI": {"title_translation": "梯度随机投影的内存高效差分隐私训练", "tldr": "提出DP-GRAPE，一种新的差分隐私训练方法，通过梯度随机投影显著减少内存使用，同时保持与现有方法相当的性能和隐私保护。", "motivation": "标准的差分隐私训练方法（如DP-Adam）由于逐样本梯度裁剪导致内存开销高，限制了可扩展性。", "method": "提出DP-GRAPE（Gradient RAndom ProjEction），通过三项关键修改实现：1) 梯度在投影后进行隐私化；2) 使用随机高斯矩阵代替基于SVD的子空间；3) 在反向传播期间应用投影。这些修改避免了昂贵的SVD计算，实现了显著的内存节省，并提升了效用。", "result": "经验实验表明，DP-GRAPE在不牺牲准确性或训练时间的情况下，减少了DP训练的内存占用。与DP-Adam相比，在预训练Vision Transformers时内存使用减少超过63%，在微调RoBERTa-Large时减少超过70%，同时实现了相似的性能。DP-GRAPE还能扩展到微调像OPT这样高达67亿参数的大模型。理论分析表明，DP-GRAPE在较低维子空间中也能实现与DP-SGD相当的隐私-效用权衡。", "conclusion": "DP-GRAPE是一种高效且可扩展的差分隐私训练方法，它通过创新的梯度随机投影技术显著降低了内存消耗，同时保持了与现有先进方法相当的性能和隐私保护，并能应用于大规模模型。", "translation": "差分隐私 (DP) 在神经网络训练期间保护敏感数据，但像DP-Adam这样的标准方法由于逐样本梯度裁剪而导致内存开销高，限制了可扩展性。我们引入了DP-GRAPE（梯度随机投影），这是一种DP训练方法，它显著减少了内存使用，同时保持了与一阶DP方法相当的效用。DP-GRAPE没有直接将DP应用于GaLore，而是引入了三个关键修改：(1) 梯度在投影后进行隐私化，(2) 随机高斯矩阵取代了基于SVD的子空间，以及 (3) 在反向传播期间应用投影。这些贡献消除了对昂贵SVD计算的需求，实现了显著的内存节省，并带来了改进的效用。尽管在较低维子空间中操作，我们的理论分析表明DP-GRAPE实现了与DP-SGD相当的隐私-效用权衡。我们广泛的实证实验表明，DP-GRAPE可以在不牺牲准确性或训练时间的情况下减少DP训练的内存占用。特别是，与DP-Adam相比，DP-GRAPE在预训练Vision Transformers时将内存使用减少了63%以上，在微调RoBERTa-Large时减少了70%以上，同时实现了相似的性能。我们进一步证明DP-GRAPE可以扩展到微调像OPT这样高达67亿参数的大模型。", "summary": "本文提出DP-GRAPE，一种内存高效的差分隐私训练方法，旨在解决现有DP方法（如DP-Adam）因逐样本梯度裁剪导致的高内存开销问题。DP-GRAPE通过在梯度投影后进行隐私化、使用随机高斯矩阵以及在反向传播中应用投影，显著减少了内存占用，同时保持了与一阶DP方法相当的性能。实验证明，DP-GRAPE在多种模型和任务上大幅降低了内存使用，并能扩展到大规模模型，同时保持了良好的隐私-效用权衡。", "keywords": "差分隐私, 梯度投影, 内存效率, 神经网络训练, 大规模模型", "comments": "这篇论文通过引入梯度随机投影，巧妙地解决了差分隐私训练中长期存在的内存效率问题。其创新点在于将隐私化操作与梯度投影结合，并用更高效的随机矩阵替代SVD，这不仅降低了计算复杂度，还显著减少了内存占用，使得DP训练能够扩展到更大的模型和数据集。这项工作对于推动差分隐私在实际大规模应用中的落地具有重要意义。"}}
{"id": "2506.15620", "title": "GFLC: Graph-based Fairness-aware Label Correction for Fair Classification", "authors": ["Modar Sulaiman", "Kallol Roy"], "summary": "Fairness in machine learning (ML) has a critical importance for building\ntrustworthy machine learning system as artificial intelligence (AI) systems\nincreasingly impact various aspects of society, including healthcare decisions\nand legal judgments. Moreover, numerous studies demonstrate evidence of unfair\noutcomes in ML and the need for more robust fairness-aware methods. However,\nthe data we use to train and develop debiasing techniques often contains biased\nand noisy labels. As a result, the label bias in the training data affects\nmodel performance and misrepresents the fairness of classifiers during testing.\nTo tackle this problem, our paper presents Graph-based Fairness-aware Label\nCorrection (GFLC), an efficient method for correcting label noise while\npreserving demographic parity in datasets. In particular, our approach combines\nthree key components: prediction confidence measure, graph-based regularization\nthrough Ricci-flow-optimized graph Laplacians, and explicit demographic parity\nincentives. Our experimental findings show the effectiveness of our proposed\napproach and show significant improvements in the trade-off between performance\nand fairness metrics compared to the baseline.", "comment": "25 pages, 6 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15620v1", "AI": {"title_translation": "GFLC：基于图的公平性感知标签校正用于公平分类", "tldr": "GFLC提出了一种基于图的公平性感知标签校正方法，通过结合预测置信度、基于Ricci流优化的图拉普拉斯正则化和显式人口统计学平等激励，有效校正训练数据中的标签噪声，同时保持人口统计学平等，显著改善了性能与公平性指标之间的权衡。", "motivation": "机器学习系统日益影响社会各方面，但训练数据常包含有偏和噪声标签，导致模型性能受损，并在测试时错误地表示分类器的公平性。因此，需要一种方法来校正标签噪声同时保持公平性。", "method": "本文提出了GFLC（Graph-based Fairness-aware Label Correction），该方法结合了三个关键组件：预测置信度度量、通过Ricci流优化图拉普拉斯实现的基于图的正则化，以及显式的人口统计学平等激励。", "result": "实验结果表明，所提出的方法是有效的，并且与基线相比，在性能和公平性指标之间的权衡方面有显著改进。", "conclusion": "GFLC是一种有效的方法，可以校正训练数据中的标签噪声，同时保持人口统计学平等，从而提高公平分类器的性能和可靠性。", "translation": "机器学习（ML）中的公平性对于构建值得信赖的机器学习系统至关重要，因为人工智能（AI）系统日益影响着社会的各个方面，包括医疗决策和法律判决。此外，大量研究表明ML中存在不公平结果的证据，并且需要更强大的公平性感知方法。然而，我们用于训练和开发去偏技术的S数据通常包含有偏和噪声标签。因此，训练数据中的标签偏差会影响模型性能，并在测试期间错误地表示分类器的公平性。为了解决这个问题，本文提出了基于图的公平性感知标签校正（GFLC），这是一种在保持数据集中人口统计学平等的同时校正标签噪声的有效方法。特别是，我们的方法结合了三个关键组件：预测置信度度量、通过Ricci流优化图拉普拉斯实现的基于图的正则化，以及显式的人口统计学平等激励。我们的实验结果表明了我们提出方法的有效性，并且与基线相比，在性能和公平性指标之间的权衡方面显示出显著改进。", "summary": "本文提出了GFLC（Graph-based Fairness-aware Label Correction），旨在解决机器学习中因训练数据标签偏差和噪声导致的不公平问题。GFLC通过结合预测置信度、基于Ricci流优化的图拉普拉斯正则化以及显式的人口统计学平等激励，有效校正标签噪声，同时保持数据集中的人口统计学平等。实验证明，GFLC显著改善了性能与公平性指标之间的权衡。", "keywords": "公平性, 标签校正, 图学习, 人口统计学平等, 机器学习", "comments": "GFLC的创新之处在于其结合了图正则化（特别是通过Ricci流优化的图拉普拉斯）和显式公平性激励来同时处理标签噪声和公平性问题。这提供了一个新颖的视角来构建更可靠和公平的机器学习系统，尤其是在数据质量不佳的情况下。"}}
{"id": "2506.15654", "title": "CAWR: Corruption-Averse Advantage-Weighted Regression for Robust Policy Optimization", "authors": ["Ranting Hu"], "summary": "Offline reinforcement learning (offline RL) algorithms often require\nadditional constraints or penalty terms to address distribution shift issues,\nsuch as adding implicit or explicit policy constraints during policy\noptimization to reduce the estimation bias of functions. This paper focuses on\na limitation of the Advantage-Weighted Regression family (AWRs), i.e., the\npotential for learning over-conservative policies due to data corruption,\nspecifically the poor explorations in suboptimal offline data. We study it from\ntwo perspectives: (1) how poor explorations impact the theoretically optimal\npolicy based on KL divergence, and (2) how such poor explorations affect the\napproximation of the theoretically optimal policy. We prove that such\nover-conservatism is mainly caused by the sensitivity of the loss function for\npolicy optimization to poor explorations, and the proportion of poor\nexplorations in offline datasets. To address this concern, we propose\nCorruption-Averse Advantage-Weighted Regression (CAWR), which incorporates a\nset of robust loss functions during policy optimization and an advantage-based\nprioritized experience replay method to filter out poor explorations. Numerical\nexperiments on the D4RL benchmark show that our method can learn superior\npolicies from suboptimal offline data, significantly enhancing the performance\nof policy optimization.", "comment": "23 pages, 14 figures", "cate": "cs.LG", "url": "http://arxiv.org/abs/2506.15654v1", "AI": {"title_translation": "CAWR: 面向鲁棒策略优化的抗损坏优势加权回归", "tldr": "AWR在次优数据下可能导致策略过于保守，本文提出CAWR，通过鲁棒损失函数和优先经验回放来解决此问题，并在D4RL基准测试中表现优异。", "motivation": "离线强化学习中的优势加权回归（AWR）系列算法，由于数据损坏（特别是次优离线数据中的不良探索），可能学习到过于保守的策略。本文旨在解决这一限制，并从理论上分析了不良探索对最优策略的影响。", "method": "本文提出了一种名为“抗损坏优势加权回归”（CAWR）的方法。该方法在策略优化过程中结合了一组鲁棒损失函数，并采用了一种基于优势的优先经验回放方法来过滤掉不良探索。", "result": "在D4RL基准测试上的数值实验表明，CAWR方法可以从次优离线数据中学习到更优的策略，显著提高了策略优化的性能。", "conclusion": "CAWR通过解决AWR在次优数据下过度保守的问题，能够学习到更优的策略，从而提升离线强化学习的性能。", "translation": "离线强化学习（offline RL）算法通常需要额外的约束或惩罚项来解决分布偏移问题，例如在策略优化期间添加隐式或显式策略约束以减少函数估计偏差。本文关注优势加权回归（AWRs）系列算法的一个局限性，即由于数据损坏（特别是次优离线数据中的不良探索）可能导致学习到过于保守的策略。我们从两个角度研究了这个问题：(1) 不良探索如何影响基于KL散度的理论最优策略，以及 (2) 不良探索如何影响理论最优策略的近似。我们证明了这种过度保守主要是由策略优化损失函数对不良探索的敏感性，以及离线数据集中不良探索的比例引起的。为了解决这个问题，我们提出了抗损坏优势加权回归（CAWR），它在策略优化期间结合了一组鲁棒损失函数，并采用了一种基于优势的优先经验回放方法来过滤掉不良探索。在D4RL基准测试上的数值实验表明，我们的方法可以从次优离线数据中学习到更优的策略，显著提高了策略优化的性能。", "summary": "针对离线强化学习中优势加权回归（AWR）算法在次优数据下可能学习到过于保守策略的问题，本文深入分析了不良探索对理论最优策略的影响。研究发现，这种过度保守源于损失函数对不良探索的敏感性及其在数据集中的比例。为解决此问题，论文提出了CAWR（抗损坏优势加权回归），该方法引入了鲁棒损失函数并采用基于优势的优先经验回放来过滤不良探索。实验结果显示，CAWR在D4RL基准测试上能从次优数据中学习到更优的策略，显著提升了策略优化性能。", "keywords": "离线强化学习, 优势加权回归, 数据损坏, 鲁棒策略优化, 优先经验回放", "comments": "本文创新性地指出了AWR系列算法在面对数据损坏（特别是次优数据中的不良探索）时可能导致策略过于保守的问题，并通过理论分析揭示了其原因。提出的CAWR方法通过结合鲁棒损失函数和优势优先经验回放，有效地解决了这一痛点，为离线RL在次优数据环境下的应用提供了更鲁棒的解决方案。其在D4RL基准上的显著性能提升证明了该方法的有效性和重要性。"}}
{"id": "2506.14772", "title": "SimBank: from Simulation to Solution in Prescriptive Process Monitoring", "authors": ["Jakob De Moor", "Hans Weytjens", "Johannes De Smedt", "Jochen De Weerdt"], "summary": "Prescriptive Process Monitoring (PresPM) is an emerging area within Process\nMining, focused on optimizing processes through real-time interventions for\neffective decision-making. PresPM holds significant promise for organizations\nseeking enhanced operational performance. However, the current literature faces\ntwo key limitations: a lack of extensive comparisons between techniques and\ninsufficient evaluation approaches. To address these gaps, we introduce\nSimBank: a simulator designed for accurate benchmarking of PresPM methods.\nModeled after a bank's loan application process, SimBank enables extensive\ncomparisons of both online and offline PresPM methods. It incorporates a\nvariety of intervention optimization problems with differing levels of\ncomplexity and supports experiments on key causal machine learning challenges,\nsuch as assessing a method's robustness to confounding in data. SimBank\nadditionally offers a comprehensive evaluation capability: for each test case,\nit can generate the true outcome under each intervention action, which is not\npossible using recorded datasets. The simulator incorporates parallel\nactivities and loops, drawing from common logs to generate cases that closely\nresemble real-life process instances. Our proof of concept demonstrates\nSimBank's benchmarking capabilities through experiments with various PresPM\nmethods across different interventions, highlighting its value as a publicly\navailable simulator for advancing research and practice in PresPM.", "comment": null, "cate": "cs.DB", "url": "http://arxiv.org/abs/2506.14772v1", "AI": {"title_translation": "SimBank：从模拟到规范性过程监控的解决方案", "tldr": "SimBank是一个模拟器，用于对规范性过程监控（PresPM）方法进行基准测试，旨在解决当前文献中缺乏广泛比较和充分评估方法的问题。", "motivation": "当前规范性过程监控（PresPM）文献面临两个主要限制：技术之间缺乏广泛比较以及评估方法不足。", "method": "SimBank是一个旨在精确基准测试PresPM方法的模拟器。它以银行贷款申请流程为模型，能够对在线和离线PresPM方法进行广泛比较。它结合了不同复杂程度的干预优化问题，并支持因果机器学习挑战的实验，例如评估方法对数据中混杂因素的鲁棒性。SimBank还能为每个测试案例生成每种干预行动下的真实结果，这是使用记录数据集无法实现的。该模拟器包含并行活动和循环，从常见日志中提取数据以生成与现实生活过程实例高度相似的案例。", "result": "概念验证通过对各种PresPM方法在不同干预措施下的实验，展示了SimBank的基准测试能力。", "conclusion": "SimBank是一个有价值的、可公开获取的模拟器，用于推进PresPM领域的研究和实践。", "translation": "规范性过程监控（PresPM）是过程挖掘领域的一个新兴方向，专注于通过实时干预来优化流程，以实现有效的决策。PresPM对于寻求提升运营绩效的组织具有重要前景。然而，当前文献面临两个关键限制：缺乏对技术的大量比较以及评估方法不足。为了弥补这些空白，我们引入了SimBank：一个旨在精确基准测试PresPM方法的模拟器。SimBank以银行贷款申请流程为模型，能够对在线和离线PresPM方法进行广泛比较。它结合了不同复杂程度的干预优化问题，并支持关键因果机器学习挑战的实验，例如评估方法对数据中混杂因素的鲁棒性。SimBank还提供全面的评估能力：对于每个测试案例，它都能生成每种干预行动下的真实结果，这是使用记录数据集无法实现的。该模拟器包含并行活动和循环，从常见日志中提取数据以生成与现实生活过程实例高度相似的案例。我们的概念验证通过对各种PresPM方法在不同干预措施下的实验，展示了SimBank的基准测试能力，突显了其作为可公开获取的模拟器在推进PresPM研究和实践中的价值。", "summary": "规范性过程监控（PresPM）旨在通过实时干预优化流程，但目前面临技术比较不足和评估方法缺乏的挑战。本文介绍了SimBank，一个基于银行贷款申请流程的模拟器，旨在为PresPM方法提供精确的基准测试。SimBank支持对在线和离线PresPM方法的广泛比较，包含不同复杂度的干预优化问题，并能进行因果机器学习挑战实验，如评估方法对混杂因素的鲁棒性。其独特之处在于能够为每个测试案例生成每种干预行动下的真实结果，这是传统数据集无法实现的。SimBank还整合了并行活动和循环，以生成贴近现实的流程实例。概念验证实验表明，SimBank作为一个可公开获取的模拟器，对推进PresPM研究和实践具有重要价值。", "keywords": "规范性过程监控, 模拟, 基准测试, 过程挖掘, 因果机器学习", "comments": "SimBank解决了PresPM领域的一个关键需求，即提供一个标准化、受控的环境进行基准测试，这在真实世界数据集中很难实现。其能够为干预措施生成真实结果的能力尤其具有创新性，这比传统方法能提供更稳健的评估。这个模拟器有望显著加速新PresPM技术的开发和验证。"}}
{"id": "2506.14850", "title": "Beyond Force Metrics: Pre-Training MLFFs for Stable MD Simulations", "authors": ["Shagun Maheshwari", "Janghoon Ock", "Adeesh Kolluru", "Amir Barati Farimani", "John R. Kitchin"], "summary": "Machine-learning force fields (MLFFs) have emerged as a promising solution\nfor speeding up ab initio molecular dynamics (MD) simulations, where accurate\nforce predictions are critical but often computationally expensive. In this\nwork, we employ GemNet-T, a graph neural network model, as an MLFF and\ninvestigate two training strategies: (1) direct training on MD17 (10K samples)\nwithout pre-training, and (2) pre-training on the large-scale OC20 dataset\nfollowed by fine-tuning on MD17 (10K). While both approaches achieve low force\nmean absolute errors (MAEs), reaching 5 meV/A per atom, we find that lower\nforce errors do not necessarily guarantee stable MD simulations. Notably, the\npre-trained GemNet-T model yields significantly improved simulation stability,\nsustaining trajectories up to three times longer than the model trained from\nscratch. These findings underscore the value of pre-training on large, diverse\ndatasets to capture complex molecular interactions and highlight that force MAE\nalone is not always a sufficient metric of MD simulation stability.", "comment": null, "cate": "physics.chem-ph", "url": "http://arxiv.org/abs/2506.14850v1", "AI": {"title_translation": "超越力学指标：预训练MLFFs以实现稳定的MD模拟", "tldr": "研究发现，预训练的机器学习力场（MLFFs）在分子动力学（MD）模拟中能提供比仅基于力误差训练的模型更长的稳定轨迹，表明力误差并非衡量MD稳定性的唯一指标。", "motivation": "加速从头算分子动力学（MD）模拟，因为准确的力预测至关重要但计算成本高昂。同时，作者发现低力误差不一定保证稳定的MD模拟。", "method": "使用GemNet-T作为MLFF，并比较两种训练策略：1) 直接在MD17数据集上训练；2) 在大型OC20数据集上预训练，然后在MD17上微调。", "result": "两种方法都达到了低的力平均绝对误差（MAE），但预训练的GemNet-T模型显著提高了模拟稳定性，轨迹持续时间是未预训练模型的三倍。这表明仅凭力MAE不足以衡量MD模拟的稳定性。", "conclusion": "在大型多样化数据集上进行预训练对于捕获复杂的分子相互作用并提高MLFFs在MD模拟中的稳定性非常重要，且力MAE并非衡量MD模拟稳定性的充分指标。", "translation": "机器学习力场（MLFFs）已成为加速从头算分子动力学（MD）模拟的一种有前景的解决方案，其中准确的力预测至关重要但通常计算成本高昂。在这项工作中，我们使用图神经网络模型GemNet-T作为MLFF，并研究了两种训练策略：(1) 在MD17（10K样本）上直接训练，不进行预训练；(2) 在大规模OC20数据集上进行预训练，然后在MD17（10K）上进行微调。虽然这两种方法都达到了较低的力平均绝对误差（MAE），每原子达到5 meV/A，但我们发现较低的力误差不一定能保证稳定的MD模拟。值得注意的是，预训练的GemNet-T模型显著提高了模拟稳定性，其轨迹持续时间比从头训练的模型长达三倍。这些发现强调了在大型、多样化数据集上进行预训练以捕获复杂分子相互作用的价值，并突出表明力MAE并非总是MD模拟稳定性的充分指标。", "summary": "本研究旨在通过预训练机器学习力场（MLFFs）来提高分子动力学（MD）模拟的稳定性。研究人员使用GemNet-T模型，比较了在MD17数据集上直接训练和在OC20数据集上预训练后在MD17上微调两种策略。尽管两种方法都实现了较低的力平均绝对误差（MAE），但预训练模型显著提高了模拟稳定性，轨迹持续时间延长达三倍。这表明预训练在捕获复杂分子相互作用方面具有重要价值，且力MAE并非衡量MD模拟稳定性的唯一充分指标。", "keywords": "机器学习力场, 分子动力学, 预训练, GemNet-T, 模拟稳定性", "comments": "这项工作创新性地指出，对于机器学习力场在分子动力学模拟中的应用，传统的力误差指标并非衡量模拟稳定性的充分条件。通过引入大规模数据集预训练的方法，显著提升了模拟的稳定性，为未来MLFFs的开发和应用提供了新的方向和评估标准。其局限性可能在于预训练数据集的选择和模型的泛化能力。"}}
{"id": "2506.14853", "title": "DisProtEdit: Exploring Disentangled Representations for Multi-Attribute Protein Editing", "authors": ["Max Ku", "Sun Sun", "Hongyu Guo", "Wenhu Chen"], "summary": "We introduce DisProtEdit, a controllable protein editing framework that\nleverages dual-channel natural language supervision to learn disentangled\nrepresentations of structural and functional properties. Unlike prior\napproaches that rely on joint holistic embeddings, DisProtEdit explicitly\nseparates semantic factors, enabling modular and interpretable control. To\nsupport this, we construct SwissProtDis, a large-scale multimodal dataset where\neach protein sequence is paired with two textual descriptions, one for\nstructure and one for function, automatically decomposed using a large language\nmodel. DisProtEdit aligns protein and text embeddings using alignment and\nuniformity objectives, while a disentanglement loss promotes independence\nbetween structural and functional semantics. At inference time, protein editing\nis performed by modifying one or both text inputs and decoding from the updated\nlatent representation. Experiments on protein editing and representation\nlearning benchmarks demonstrate that DisProtEdit performs competitively with\nexisting methods while providing improved interpretability and controllability.\nOn a newly constructed multi-attribute editing benchmark, the model achieves a\nboth-hit success rate of up to 61.7%, highlighting its effectiveness in\ncoordinating simultaneous structural and functional edits.", "comment": "Accepted to ICMLW (GenBio) 2025 and ICMLW (FM4LS) 2025", "cate": "q-bio.QM", "url": "http://arxiv.org/abs/2506.14853v1", "AI": {"title_translation": "DisProtEdit: 探索多属性蛋白质编辑的解耦表示", "tldr": "DisProtEdit是一个利用双通道自然语言监督学习结构和功能解耦表示的蛋白质编辑框架，实现了模块化和可解释的控制，并构建了新的数据集和实现了多属性编辑。", "motivation": "现有蛋白质编辑方法依赖于联合整体嵌入，缺乏模块化和可解释的控制。本文旨在通过解耦表示解决这个问题，实现可控的蛋白质编辑。", "method": "本文引入DisProtEdit框架，利用双通道自然语言监督学习蛋白质结构和功能属性的解耦表示。为支持此框架，构建了大规模多模态数据集SwissProtDis，其中蛋白质序列与结构和功能两种文本描述配对。DisProtEdit通过对齐和统一目标对齐蛋白质和文本嵌入，同时利用解耦损失促进结构和功能语义之间的独立性。在推理时，通过修改一个或两个文本输入并从更新的潜在表示解码来执行蛋白质编辑。", "result": "DisProtEdit在蛋白质编辑和表示学习基准上与现有方法表现相当。在一个新建的多属性编辑基准上，模型实现了高达61.7%的双命中成功率，突出了其在协调同时进行结构和功能编辑方面的有效性。", "conclusion": "DisProtEdit通过学习解耦表示，实现了可控且可解释的蛋白质编辑，并在多属性编辑任务上表现出色，证明了其在同时进行结构和功能编辑方面的潜力。", "translation": "我们引入了DisProtEdit，一个可控的蛋白质编辑框架，它利用双通道自然语言监督来学习结构和功能属性的解耦表示。与依赖于联合整体嵌入的现有方法不同，DisProtEdit明确分离了语义因子，实现了模块化和可解释的控制。为了支持这一点，我们构建了SwissProtDis，一个大规模多模态数据集，其中每个蛋白质序列都与两个文本描述配对，一个用于结构，一个用于功能，这些描述是使用大型语言模型自动分解的。DisProtEdit使用对齐和统一目标对齐蛋白质和文本嵌入，同时解耦损失促进了结构和功能语义之间的独立性。在推理时，通过修改一个或两个文本输入并从更新的潜在表示解码来执行蛋白质编辑。在蛋白质编辑和表示学习基准上的实验表明，DisProtEdit与现有方法相比具有竞争力，同时提供了更高的可解释性和可控性。在一个新建的多属性编辑基准上，该模型实现了高达61.7%的双命中成功率，突出了其在协调同时进行结构和功能编辑方面的有效性。", "summary": "DisProtEdit是一个创新的蛋白质编辑框架，它通过利用双通道自然语言监督学习蛋白质结构和功能的解耦表示，实现了模块化和可解释的控制。该框架构建了大规模多模态数据集SwissProtDis，并通过对齐、统一和解耦损失进行训练。实验证明，DisProtEdit在蛋白质编辑和表示学习任务上表现出色，尤其在同时进行结构和功能编辑时，展现出显著的有效性。", "keywords": "蛋白质编辑, 解耦表示, 自然语言监督, 多属性控制, 蛋白质工程", "comments": "DisProtEdit的创新之处在于其引入了双通道自然语言监督和解耦表示学习，从而实现了比现有方法更具可解释性和可控性的蛋白质编辑。构建SwissProtDis数据集是其重要贡献，为多模态蛋白质研究提供了资源。其在多属性编辑上的高成功率表明了其在复杂蛋白质工程应用中的潜力。"}}
{"id": "2506.14858", "title": "CutReg: A loss regularizer for enhancing the scalability of QML via adaptive circuit cutting", "authors": ["Maniraman Periyasamy", "Christian Ufrecht", "Daniel D. Scherer", "Wolfgang Mauerer"], "summary": "Whether QML can offer a transformative advantage remains an open question.\nThe severe constraints of NISQ hardware, particularly in circuit depth and\nconnectivity, hinder both the validation of quantum advantage and the empirical\ninvestigation of major obstacles like barren plateaus. Circuit cutting\ntechniques have emerged as a strategy to execute larger quantum circuits on\nsmaller, less connected hardware by dividing them into subcircuits. However,\nthis partitioning increases the number of samples needed to estimate the\nexpectation value accurately through classical post-processing compared to\nestimating it directly from the full circuit. This work introduces a novel\nregularization term into the QML optimization process, directly penalizing the\noverhead associated with sampling. We demonstrate that this approach enables\nthe optimizer to balance the advantages of gate cutting against the\noptimization of the typical ML cost function. Specifically, it navigates the\ntrade-off between minimizing the cutting overhead and maintaining the overall\naccuracy of the QML model, paving the way to study larger complex problems in\npursuit of quantum advantage.", "comment": "This work has been submitted to the QML@QCE workshop for possible\n  publication", "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.14858v1", "AI": {"title_translation": "CutReg: 一种通过自适应电路切割增强 QML 可扩展性的损失正则化器", "tldr": "提出了一种名为 CutReg 的新正则化项，用于量子机器学习 (QML) 优化，以平衡电路切割的优势和采样开销，从而在 NISQ 硬件上实现更大的量子电路执行。", "motivation": "NISQ 硬件的严格限制（电路深度和连接性）阻碍了量子优势的验证和对诸如贫瘠高原等主要障碍的实证研究。尽管电路切割技术可以使大型量子电路在小型硬件上执行，但它增加了样本需求，从而带来了开销。", "method": "本文引入了一种新颖的正则化项，直接惩罚与采样相关的开销，并将其纳入 QML 优化过程中。这种方法使优化器能够平衡门切割的优势与典型机器学习成本函数的优化。", "result": "该方法使优化器能够平衡门切割的优势与典型机器学习成本函数的优化。具体而言，它能在最小化切割开销和保持 QML 模型整体精度之间进行权衡。", "conclusion": "这种方法为研究更大的复杂问题以追求量子优势铺平了道路。", "translation": "量子机器学习（QML）是否能提供变革性优势仍是一个悬而未决的问题。NISQ 硬件的严格限制，特别是在电路深度和连接性方面，阻碍了量子优势的验证以及对贫瘠高原等主要障碍的实证研究。电路切割技术已成为一种在更小、连接性更低的硬件上执行更大量子电路的策略，通过将它们划分为子电路。然而，与直接从完整电路估计相比，这种划分增加了通过经典后处理准确估计期望值所需的样本数量。这项工作在 QML 优化过程中引入了一个新颖的正则化项，直接惩罚与采样相关的开销。我们证明，这种方法使优化器能够平衡门切割的优势与典型机器学习成本函数的优化。具体而言，它能在最小化切割开销和保持 QML 模型整体精度之间进行权衡，从而为研究更大的复杂问题以追求量子优势铺平了道路。", "summary": "该论文提出了一种名为 CutReg 的新型损失正则化器，用于量子机器学习 (QML) 优化。该正则化器旨在解决 NISQ 硬件上电路切割技术带来的采样开销问题。通过直接惩罚采样开销，CutReg 允许优化器在利用电路切割执行大型量子电路的优势与保持 QML 模型整体精度之间取得平衡，从而有助于在受限的量子硬件上探索更复杂的量子问题并追求量子优势。", "keywords": "量子机器学习, 电路切割, 正则化, NISQ, 可扩展性", "comments": "该论文的创新点在于引入了一个新的正则化项来解决量子电路切割中固有的采样开销问题。这对于在当前受限的 NISQ 硬件上扩展量子机器学习应用具有重要意义，因为它提供了一种在切割效率和模型精度之间进行智能权衡的机制。这有助于推动 QML 的实际应用和量子优势的探索。"}}
{"id": "2506.14899", "title": "Optimal Convergence Rates of Deep Neural Network Classifiers", "authors": ["Zihan Zhang", "Lei Shi", "Ding-Xuan Zhou"], "summary": "In this paper, we study the binary classification problem on $[0,1]^d$ under\nthe Tsybakov noise condition (with exponent $s \\in [0,\\infty]$) and the\ncompositional assumption. This assumption requires the conditional class\nprobability function of the data distribution to be the composition of $q+1$\nvector-valued multivariate functions, where each component function is either a\nmaximum value function or a H\\\"{o}lder-$\\beta$ smooth function that depends\nonly on $d_*$ of its input variables. Notably, $d_*$ can be significantly\nsmaller than the input dimension $d$. We prove that, under these conditions,\nthe optimal convergence rate for the excess 0-1 risk of classifiers is $$\n\\left( \\frac{1}{n}\n\\right)^{\\frac{\\beta\\cdot(1\\wedge\\beta)^q}{{\\frac{d_*}{s+1}+(1+\\frac{1}{s+1})\\cdot\\beta\\cdot(1\\wedge\\beta)^q}}}\\;\\;\\;,\n$$ which is independent of the input dimension $d$. Additionally, we\ndemonstrate that ReLU deep neural networks (DNNs) trained with hinge loss can\nachieve this optimal convergence rate up to a logarithmic factor. This result\nprovides theoretical justification for the excellent performance of ReLU DNNs\nin practical classification tasks, particularly in high-dimensional settings.\nThe technique used to establish these results extends the oracle inequality\npresented in our previous work. The generalized approach is of independent\ninterest.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.14899v1", "AI": {"title_translation": "深度神经网络分类器的最优收敛速度", "tldr": "本文研究了在Tsybakov噪声条件和组合假设下，深度神经网络分类器在二元分类问题上的最优收敛速度，并证明ReLU深度神经网络可以达到此最优速度。", "motivation": "研究在Tsybakov噪声条件和组合假设下二元分类问题的最优收敛速度，并为ReLU深度神经网络在实际高维分类任务中的优异表现提供理论依据。", "method": "在Tsybakov噪声条件和组合假设下，通过扩展先前的oracle不等式技术，证明了分类器0-1风险的最优收敛率，并展示ReLU深度神经网络可以达到此最优收敛率。", "result": "在给定条件下，分类器0-1风险的最优收敛率为 $$\\left( \\frac{1}{n} \\right)^{\\frac{\\beta\\cdot(1\\wedge\\beta)^q}{{\\frac{d_*}{s+1}+(1+\\frac{1}{s+1})\\cdot\\beta\\cdot(1\\wedge\\beta)^q}}}$$，该速率与输入维度$d$无关。ReLU深度神经网络（DNNs）使用hinge损失训练时，可以达到此最优收敛率（至多相差一个对数因子）。", "conclusion": "在特定的噪声和组合函数假设下，ReLU深度神经网络分类器能够达到与输入维度无关的最优收敛速度，这为它们在高维分类任务中的卓越性能提供了理论支持。", "translation": "本文研究了在Tsybakov噪声条件（指数$s \\in [0,\\infty]$）和组合假设下，在$[0,1]^d$上的二元分类问题。该假设要求数据分布的条件类别概率函数是$q+1$个向量值多元函数的组合，其中每个分量函数要么是最大值函数，要么是H\"{o}lder-$\beta$光滑函数，且仅依赖于其$d_*$个输入变量。值得注意的是，$d_*$可以显著小于输入维度$d$。我们证明，在这些条件下，分类器0-1风险的最优收敛速度为 $$\\left( \\frac{1}{n} \\right)^{\\frac{\\beta\\cdot(1\\wedge\\beta)^q}{{\\frac{d_*}{s+1}+(1+\\frac{1}{s+1})\\cdot\\beta\\cdot(1\\wedge\\beta)^q}}}$$，该速度与输入维度$d$无关。此外，我们证明了使用hinge损失训练的ReLU深度神经网络（DNNs）可以达到此最优收敛速度（至多相差一个对数因子）。这一结果为ReLU深度神经网络在实际分类任务中，特别是在高维设置下的出色表现提供了理论依据。用于建立这些结果的技术扩展了我们先前工作中提出的oracle不等式。这种广义方法具有独立的兴趣。", "summary": "本文研究了在Tsybakov噪声条件和组合函数假设下，二元分类问题的最优收敛速度。研究表明，在这些条件下，分类器0-1风险的最优收敛率与输入维度无关。此外，论文证明了ReLU深度神经网络在使用hinge损失训练时，可以达到这种最优收敛率（至多相差一个对数因子），从而为ReLU DNNs在高维分类任务中的优异性能提供了理论支持。", "keywords": "深度神经网络, 收敛速度, 二元分类, Tsybakov噪声, ReLU", "comments": "这项研究通过理论分析，为ReLU深度神经网络在高维分类问题中的卓越性能提供了强有力的数学依据。其创新之处在于考虑了Tsybakov噪声条件和独特的组合函数假设，并推导出了与输入维度无关的最优收敛率。这对于理解深度学习模型的泛化能力及其在高维数据上的有效性具有重要意义。该研究扩展了现有的oracle不等式技术，其广义方法本身也具有研究价值。"}}
{"id": "2506.14920", "title": "Q2SAR: A Quantum Multiple Kernel Learning Approach for Drug Discovery", "authors": ["Alejandro Giraldo", "Daniel Ruiz", "Mariano Caruso", "Javier Mancilla", "Guido Bellomo"], "summary": "Quantitative Structure-Activity Relationship (QSAR) modeling is a cornerstone\nof computational drug discovery. This research demonstrates the successful\napplication of a Quantum Multiple Kernel Learning (QMKL) framework to enhance\nQSAR classification, showing a notable performance improvement over classical\nmethods. We apply this methodology to a dataset for identifying DYRK1A kinase\ninhibitors. The workflow involves converting SMILES representations into\nnumerical molecular descriptors, reducing dimensionality via Principal\nComponent Analysis (PCA), and employing a Support Vector Machine (SVM) trained\non an optimized combination of multiple quantum and classical kernels. By\nbenchmarking the QMKL-SVM against a classical Gradient Boosting model, we show\nthat the quantum-enhanced approach achieves a superior AUC score, highlighting\nits potential to provide a quantum advantage in challenging cheminformatics\nclassification tasks.", "comment": null, "cate": "quant-ph", "url": "http://arxiv.org/abs/2506.14920v1", "AI": {"title_translation": "Q2SAR：一种用于药物发现的量子多核学习方法", "tldr": "本研究展示了量子多核学习（QMKL）框架在增强QSAR分类方面的成功应用，并在识别DYRK1A激酶抑制剂的数据集上实现了优于传统方法的性能。", "motivation": "药物发现中的定量构效关系（QSAR）建模是计算药物发现的基石。本研究旨在通过应用量子多核学习（QMKL）框架来增强QSAR分类，并寻求超越经典方法的性能提升。", "method": "研究将SMILES表示转换为数值分子描述符，通过主成分分析（PCA）降维，并使用在优化组合的多个量子和经典核上训练的支持向量机（SVM）。通过将QMKL-SVM与经典梯度提升模型进行基准测试，评估其性能。", "result": "量子增强方法（QMKL-SVM）在识别DYRK1A激酶抑制剂的数据集上实现了更高的AUC分数，显示出相对于经典梯度提升模型的显著性能提升。", "conclusion": "量子增强的QMKL-SVM方法在挑战性的化学信息学分类任务中具有提供量子优势的潜力。", "translation": "定量构效关系（QSAR）建模是计算药物发现的基石。本研究展示了量子多核学习（QMKL）框架在增强QSAR分类方面的成功应用，显示出相对于经典方法的显著性能改进。我们将此方法应用于识别DYRK1A激酶抑制剂的数据集。工作流程包括将SMILES表示转换为数值分子描述符，通过主成分分析（PCA）降维，并采用在优化组合的多个量子和经典核上训练的支持向量机（SVM）。通过将QMKL-SVM与经典梯度提升模型进行基准测试，我们表明量子增强方法实现了更高的AUC分数，突显了其在具有挑战性的化学信息学分类任务中提供量子优势的潜力。", "summary": "本文提出了一种名为Q2SAR的量子多核学习（QMKL）框架，用于增强定量构效关系（QSAR）分类。该方法通过将SMILES转换为分子描述符，利用PCA降维，并结合量子与经典核优化训练的支持向量机（SVM）。在识别DYRK1A激酶抑制剂的数据集上，QMKL-SVM在AUC分数上优于传统的梯度提升模型，表明了其在化学信息学分类任务中实现量子优势的潜力。", "keywords": "QSAR, 量子多核学习, 药物发现, 化学信息学, SVM", "comments": "这项研究通过将量子计算原理引入多核学习框架，为药物发现中的QSAR建模提供了新的视角。其创新之处在于结合了量子和经典核来优化SVM，并在实际药物发现任务中展示了超越传统方法的潜力，这对于探索量子计算在化学信息学领域的应用具有重要意义。"}}
{"id": "2506.14925", "title": "Digital twin for virtual sensing of ferry quays via a Gaussian Process Latent Force Model", "authors": ["Luigi Sibille", "Torodd Skjerve Nord", "Alice Cicirello"], "summary": "Ferry quays experience rapid deterioration due to their exposure to harsh\nmaritime environments and ferry impacts. Vibration-based structural health\nmonitoring offers a valuable approach to assessing structural integrity and\nunderstanding the structural implications of these impacts. However, practical\nlimitations often restrict sensor placement at critical locations.\nConsequently, virtual sensing techniques become essential for establishing a\nDigital Twin and estimating the structural response. This study investigates\nthe application of the Gaussian Process Latent Force Model (GPLFM) for virtual\nsensing on the Magerholm ferry quay, combining in-operation experimental data\ncollected during a ferry impact with a detailed physics-based model. The\nproposed Physics-Encoded Machine Learning model integrates a reduced-order\nstructural model with a data-driven GPLFM representing the unknown impact\nforces via their modal contributions. Significant challenges are addressed for\nthe development of the Digital Twin of the ferry quay, including unknown impact\ncharacteristics (location, direction, intensity), time-varying boundary\nconditions, and sparse sensor configurations. Results show that the GPLFM\nprovides accurate acceleration response estimates at most locations, even under\nsimplifying modeling assumptions such as linear time-invariant behavior during\nthe impact phase. Lower accuracy was observed at locations in the impact zone.\nA numerical study was conducted to explore an optimal real-world sensor\nplacement strategy using a Backward Sequential Sensor Placement approach.\nSensitivity analyses were conducted to examine the influence of sensor types,\nsampling frequencies, and incorrectly assumed damping ratios. The results\nsuggest that the GP latent forces can help accommodate modeling and measurement\nuncertainties, maintaining acceptable estimation accuracy across scenarios.", "comment": "14 Figures, 1 Table", "cate": "stat.AP", "url": "http://arxiv.org/abs/2506.14925v1", "AI": {"title_translation": "数字孪生通过高斯过程潜在力模型实现渡轮码头虚拟传感", "tldr": "渡轮码头快速劣化，传感器放置受限。本文利用数字孪生技术，结合高斯过程潜在力模型（GPLFM），通过整合真实数据和物理模型，实现渡轮码头的虚拟传感，以估计结构响应，即使在传感器稀疏和撞击力未知的情况下也能保持良好精度。", "motivation": "渡轮码头由于恶劣的海洋环境和渡轮撞击而快速劣化。基于振动的结构健康监测方法很有价值，但实际限制了关键位置的传感器放置。因此，虚拟传感技术对于建立数字孪生和估计结构响应至关重要。", "method": "本研究将高斯过程潜在力模型（GPLFM）应用于渡轮码头的虚拟传感，结合了渡轮撞击期间收集的运行中实验数据和一个详细的基于物理的模型。所提出的物理编码机器学习模型将降阶结构模型与数据驱动的GPLFM相结合，后者通过其模态贡献来表示未知的撞击力。研究还通过反向序列传感器放置方法进行了数值研究，以探索最佳的实际传感器放置策略，并进行了灵敏度分析，以检查传感器类型、采样频率和错误假设的阻尼比的影响。", "result": "结果表明，即使在简化的建模假设下（例如撞击阶段的线性时不变行为），GPLFM也能在大多数位置提供准确的加速度响应估计。在撞击区域的位置观察到较低的准确性。GP潜在力可以帮助适应建模和测量不确定性，在各种场景中保持可接受的估计精度。", "conclusion": "高斯过程潜在力模型（GPLFM）对于渡轮码头数字孪生中的虚拟传感是有效的，即使面对稀疏传感器配置和未知撞击特性等挑战，也能提供准确的结构响应估计并适应不确定性。", "translation": "渡轮码头由于暴露在恶劣的海洋环境和渡轮撞击下而迅速劣化。基于振动的结构健康监测为评估结构完整性和理解这些撞击对结构的影响提供了一种有价值的方法。然而，实际限制常常限制了在关键位置的传感器放置。因此，虚拟传感技术对于建立数字孪生和估计结构响应变得至关重要。本研究调查了高斯过程潜在力模型（GPLFM）在Magerholm渡轮码头虚拟传感中的应用，结合了渡轮撞击期间收集的运行中实验数据和详细的基于物理的模型。所提出的物理编码机器学习模型将降阶结构模型与数据驱动的GPLFM相结合，后者通过其模态贡献来表示未知的撞击力。在渡轮码头数字孪生的开发过程中，解决了重大挑战，包括未知的撞击特性（位置、方向、强度）、时变边界条件和稀疏传感器配置。结果表明，即使在简化的建模假设下（例如撞击阶段的线性时不变行为），GPLFM也能在大多数位置提供准确的加速度响应估计。在撞击区域的位置观察到较低的准确性。进行了一项数值研究，以探索使用反向序列传感器放置方法的最佳真实世界传感器放置策略。进行了灵敏度分析，以检查传感器类型、采样频率和错误假设的阻尼比的影响。结果表明，GP潜在力可以帮助适应建模和测量不确定性，在各种场景中保持可接受的估计精度。", "summary": "本论文提出了一种用于渡轮码头虚拟传感的数字孪生方法，以应对其快速劣化问题。该方法利用物理编码机器学习模型，将降阶结构模型与数据驱动的高斯过程潜在力模型（GPLFM）相结合。通过结合实验数据和基于物理的模型，该模型能够有效估计结构响应，即使在传感器数据有限和撞击力未知的情况下。研究表明，GPLFM在估计加速度响应方面表现出高精度，并能处理建模和测量不确定性，解决了稀疏传感器配置和未知撞击特性等挑战。此外，还探讨了最佳传感器放置策略。", "keywords": "数字孪生, 虚拟传感, 高斯过程潜在力模型, 结构健康监测, 渡轮码头", "comments": "该论文展示了高斯过程潜在力模型（GPLFM）在渡轮码头结构健康监测这一挑战性领域中虚拟传感的创新应用。其优势在于将基于物理的模型与数据驱动的机器学习相结合，以克服实际传感器限制和未知的外部力，这对于实际的结构健康监测至关重要。GPLFM适应建模和测量不确定性的能力是一个显著的优点。"}}
{"id": "2506.14950", "title": "Double Machine Learning for Conditional Moment Restrictions: IV regression, Proximal Causal Learning and Beyond", "authors": ["Daqian Shao", "Ashkan Soleymani", "Francesco Quinzan", "Marta Kwiatkowska"], "summary": "Solving conditional moment restrictions (CMRs) is a key problem considered in\nstatistics, causal inference, and econometrics, where the aim is to solve for a\nfunction of interest that satisfies some conditional moment equalities.\nSpecifically, many techniques for causal inference, such as instrumental\nvariable (IV) regression and proximal causal learning (PCL), are CMR problems.\nMost CMR estimators use a two-stage approach, where the first-stage estimation\nis directly plugged into the second stage to estimate the function of interest.\nHowever, naively plugging in the first-stage estimator can cause heavy bias in\nthe second stage. This is particularly the case for recently proposed CMR\nestimators that use deep neural network (DNN) estimators for both stages, where\nregularisation and overfitting bias is present. We propose DML-CMR, a two-stage\nCMR estimator that provides an unbiased estimate with fast convergence rate\nguarantees. We derive a novel learning objective to reduce bias and develop the\nDML-CMR algorithm following the double/debiased machine learning (DML)\nframework. We show that our DML-CMR estimator can achieve the minimax optimal\nconvergence rate of $O(N^{-1/2})$ under parameterisation and mild regularity\nconditions, where $N$ is the sample size. We apply DML-CMR to a range of\nproblems using DNN estimators, including IV regression and proximal causal\nlearning on real-world datasets, demonstrating state-of-the-art performance\nagainst existing CMR estimators and algorithms tailored to those problems.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.14950v1", "AI": {"title_translation": "条件矩约束的双重机器学习：工具变量回归、近端因果学习及其他", "tldr": "本文提出DML-CMR，一种基于双重机器学习的条件矩约束（CMR）估计器，旨在解决传统两阶段方法中的偏差问题，并实现无偏估计和快速收敛。", "motivation": "解决统计学、因果推断和计量经济学中条件矩约束（CMRs）的关键问题，特别是现有两阶段CMR估计器（包括使用深度神经网络的）中第一阶段估计器直接插入第二阶段导致的严重偏差问题。", "method": "提出DML-CMR，一个两阶段CMR估计器，通过推导一个新颖的学习目标来减少偏差，并遵循双重/去偏机器学习（DML）框架开发算法。", "result": "DML-CMR能提供无偏估计和快速收敛率保证，在参数化和温和正则条件下达到$O(N^{-1/2})$的最小最大最优收敛率。在工具变量回归和近端因果学习等问题上，使用深度神经网络估计器在真实世界数据集上表现出优于现有CMR估计器和针对这些问题定制的算法的最新性能。", "conclusion": "DML-CMR是一种有效且稳健的解决条件矩约束问题的方法，它通过双重机器学习框架克服了传统两阶段估计器的偏差问题，并实现了优越的性能和理论保证。", "translation": "解决条件矩约束（CMRs）是统计学、因果推断和计量经济学中考虑的一个关键问题，其目标是求解满足某些条件矩等式的感兴趣函数。具体而言，许多因果推断技术，如工具变量（IV）回归和近端因果学习（PCL），都是CMR问题。大多数CMR估计器采用两阶段方法，其中第一阶段的估计结果直接代入第二阶段以估计感兴趣的函数。然而，简单地代入第一阶段估计器可能导致第二阶段出现严重偏差。对于最近提出的使用深度神经网络（DNN）估计器进行两阶段估计的CMR估计器尤其如此，其中存在正则化和过拟合偏差。我们提出了DML-CMR，一个两阶段CMR估计器，它提供无偏估计和快速收敛率保证。我们推导了一个新颖的学习目标以减少偏差，并遵循双重/去偏机器学习（DML）框架开发了DML-CMR算法。我们证明了我们的DML-CMR估计器在参数化和温和正则条件下可以达到$O(N^{-1/2})$的最小最大最优收敛率，其中N是样本量。我们将DML-CMR应用于使用DNN估计器的一系列问题，包括在真实世界数据集上的工具变量回归和近端因果学习，展示了相对于现有CMR估计器和针对这些问题定制的算法的最新性能。", "summary": "本文提出DML-CMR，一种基于双重机器学习框架的两阶段条件矩约束（CMR）估计器，旨在解决传统方法中因第一阶段估计器直接代入第二阶段所导致的偏差问题。DML-CMR通过引入新颖的学习目标，实现了无偏估计和快速收敛，并在理论上达到最小最大最优收敛率。实验证明，DML-CMR在工具变量回归和近端因果学习等实际应用中表现出优于现有方法的性能。", "keywords": "条件矩约束, 双重机器学习, 工具变量回归, 近端因果学习", "comments": "本文创新性地将双重机器学习（DML）框架应用于条件矩约束（CMR）问题，有效解决了传统两阶段估计器（特别是结合深度神经网络时）的偏差问题。其理论上证明的无偏性、快速收敛率和最优收敛率保证，以及在实际应用中的优越表现，凸显了该方法在因果推断和计量经济学领域的潜在重要性。"}}
{"id": "2506.14952", "title": "An Observation on Lloyd's k-Means Algorithm in High Dimensions", "authors": ["David Silva-Sánchez", "Roy R. Lederman"], "summary": "Clustering and estimating cluster means are core problems in statistics and\nmachine learning, with k-means and Expectation Maximization (EM) being two\nwidely used algorithms. In this work, we provide a theoretical explanation for\nthe failure of k-means in high-dimensional settings with high noise and limited\nsample sizes, using a simple Gaussian Mixture Model (GMM). We identify regimes\nwhere, with high probability, almost every partition of the data becomes a\nfixed point of the k-means algorithm. This study is motivated by challenges in\nthe analysis of more complex cases, such as masked GMMs, and those arising from\napplications in Cryo-Electron Microscopy.", "comment": "27 pages, 3 figures, 4 supplemental figures", "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.14952v1", "AI": {"title_translation": "关于高维Lloyd's k-Means算法的观察", "tldr": "在高维、高噪声和有限样本量下，k-means算法在高概率下失败，因为几乎所有数据分区都成为其不动点。", "motivation": "本研究的动机是为了应对更复杂情况（如掩膜高斯混合模型）以及低温电子显微镜应用中出现的分析挑战。", "method": "本研究使用简单高斯混合模型（GMM）为k-means算法在高维、高噪声和有限样本量设置下失效提供了理论解释。", "result": "研究发现，在特定条件下，k-means算法在高概率下，数据几乎所有分区都成为其固定点。", "conclusion": "在高维、高噪声和有限样本量设置下，k-means算法会失效，因为许多数据分区会成为其固定点，这为算法的局限性提供了理论解释。", "translation": "聚类和估计聚类均值是统计学和机器学习中的核心问题，k-均值和期望最大化（EM）是两种广泛使用的算法。在这项工作中，我们使用简单的高斯混合模型（GMM），为k-均值算法在高维、高噪声和有限样本量设置下的失效提供了理论解释。我们确定了在这些条件下，数据几乎所有分区都以高概率成为k-均值算法不动点的区域。这项研究的动机是为了应对更复杂情况的分析挑战，例如掩膜GMM，以及低温电子显微镜应用中出现的问题。", "summary": "本研究在高维、高噪声和有限样本量背景下，利用简单高斯混合模型（GMM）对k-means算法的失效进行了理论解释。研究发现，在这种设置下，数据几乎所有分区都以高概率成为k-means算法的固定点，从而揭示了其在高维复杂数据场景中表现不佳的原因。此项工作旨在解决更复杂模型和实际应用（如低温电子显微镜）中的分析难题。", "keywords": "k-means, 高维, 高斯混合模型, 聚类, 固定点", "comments": "这项研究通过提供k-means算法在高维、高噪声和有限样本量下失效的理论解释，填补了现有理解的空白。其创新之处在于识别出导致算法失效的“固定点”现象，为理解算法局限性提供了深刻见解。这对于处理大规模、复杂数据集（如低温电子显微镜数据）的应用具有重要意义。"}}
{"id": "2506.14957", "title": "POCO: Scalable Neural Forecasting through Population Conditioning", "authors": ["Yu Duan", "Hamza Tahir Chaudhry", "Misha B. Ahrens", "Christopher D Harvey", "Matthew G Perich", "Karl Deisseroth", "Kanaka Rajan"], "summary": "Predicting future neural activity is a core challenge in modeling brain\ndynamics, with applications ranging from scientific investigation to\nclosed-loop neurotechnology. While recent models of population activity\nemphasize interpretability and behavioral decoding, neural\nforecasting-particularly across multi-session, spontaneous recordings-remains\nunderexplored. We introduce POCO, a unified forecasting model that combines a\nlightweight univariate forecaster with a population-level encoder to capture\nboth neuron-specific and brain-wide dynamics. Trained across five calcium\nimaging datasets spanning zebrafish, mice, and C. elegans, POCO achieves\nstate-of-the-art accuracy at cellular resolution in spontaneous behaviors.\nAfter pre-training, POCO rapidly adapts to new recordings with minimal\nfine-tuning. Notably, POCO's learned unit embeddings recover biologically\nmeaningful structure-such as brain region clustering-without any anatomical\nlabels. Our comprehensive analysis reveals several key factors influencing\nperformance, including context length, session diversity, and preprocessing.\nTogether, these results position POCO as a scalable and adaptable approach for\ncross-session neural forecasting and offer actionable insights for future model\ndesign. By enabling accurate, generalizable forecasting models of neural\ndynamics across individuals and species, POCO lays the groundwork for adaptive\nneurotechnologies and large-scale efforts for neural foundation models.", "comment": null, "cate": "q-bio.NC", "url": "http://arxiv.org/abs/2506.14957v1", "AI": {"title_translation": "POCO：通过群体条件实现可扩展的神经预测", "tldr": "POCO是一种新的神经预测模型，它结合了单变量预测器和群体编码器，实现了跨会话神经活动的准确预测，并能适应新数据。", "motivation": "预测未来的神经活动是脑动力学建模的核心挑战，尤其是在多会话、自发记录中，神经预测仍未得到充分探索，而现有模型更侧重于可解释性和行为解码。", "method": "本文引入了POCO，一个统一的预测模型，它结合了轻量级的单变量预测器和群体级编码器，以捕获神经元特异性和全脑动力学。", "result": "POCO在斑马鱼、小鼠和秀丽隐杆线虫的五个钙成像数据集上进行训练，在自发行为中实现了细胞分辨率的最新预测精度。预训练后，POCO能以最少的微调快速适应新记录。值得注意的是，POCO学习到的单元嵌入在没有任何解剖标签的情况下，恢复了具有生物学意义的结构（如脑区聚类）。全面的分析揭示了影响性能的几个关键因素，包括上下文长度、会话多样性和预处理。", "conclusion": "这些结果表明POCO是一种可扩展且适应性强的跨会话神经预测方法，并为未来的模型设计提供了可操作的见解。通过实现跨个体和物种的神经动力学准确、可泛化预测模型，POCO为自适应神经技术和神经基础模型的大规模工作奠定了基础。", "translation": "预测未来的神经活动是建模大脑动力学的核心挑战，其应用范围从科学研究到闭环神经技术。尽管最近的群体活动模型强调可解释性和行为解码，但神经预测——尤其是在多会话、自发记录中——仍未得到充分探索。我们引入了POCO，一个统一的预测模型，它结合了轻量级的单变量预测器和群体级编码器，以捕获神经元特异性和全脑动力学。POCO在涵盖斑马鱼、小鼠和秀丽隐杆线虫的五个钙成像数据集上进行训练，在自发行为中实现了细胞分辨率的最新精度。预训练后，POCO能以最少的微调快速适应新的记录。值得注意的是，POCO学习到的单元嵌入在没有任何解剖标签的情况下，恢复了具有生物学意义的结构——例如脑区聚类。我们的全面分析揭示了影响性能的几个关键因素，包括上下文长度、会话多样性和预处理。总而言之，这些结果将POCO定位为一种可扩展且适应性强的跨会话神经预测方法，并为未来的模型设计提供了可操作的见解。通过实现跨个体和物种的神经动力学准确、可泛化预测模型，POCO为自适应神经技术和大规模神经基础模型工作奠定了基础。", "summary": "POCO是一种新颖的神经预测模型，旨在解决跨多会话、自发记录中神经活动预测的挑战。该模型结合了轻量级单变量预测器和群体级编码器，以捕获神经元特异性和全脑动力学。POCO在多种物种的钙成像数据集上表现出最先进的预测精度，并能快速适应新数据，其学习到的嵌入还能揭示生物学结构。这项研究为可扩展的神经预测提供了新方法，并为未来的神经技术奠定了基础。", "keywords": "神经预测, 群体条件, 脑动力学, 钙成像, 神经技术", "comments": "POCO的创新之处在于其结合了单变量和群体级编码器来捕捉多尺度神经动力学，以及在没有解剖标签的情况下恢复生物学结构的能力。其可扩展性和对新数据的快速适应性使其在神经科学研究和神经技术应用中具有重要意义。该工作为未来构建神经基础模型提供了坚实的基础。"}}
{"id": "2506.15020", "title": "Data analysis using discrete cubical homology", "authors": ["Chris Kapulkin", "Nathan Kershaw"], "summary": "We present a new tool for data analysis: persistence discrete homology, which\nis well-suited to analyze filtrations of graphs. In particular, we provide a\nnovel way of representing high-dimensional data as a filtration of graphs using\npairwise correlations. We discuss several applications of these tools, e.g., in\nweather and financial data, comparing them to the standard methods used in the\nrespective fields.", "comment": "17 pages; comments welcome", "cate": "math.AT", "url": "http://arxiv.org/abs/2506.15020v1", "AI": {"title_translation": "使用离散立方同调进行数据分析", "tldr": "提出了一种新的数据分析工具：持久性离散同调，用于分析图的过滤，并通过成对相关性将高维数据表示为图的过滤。", "motivation": "旨在提供一种新的数据分析工具，以有效分析图的过滤和高维数据。", "method": "提出了持久性离散同调作为数据分析工具，并利用成对相关性将高维数据表示为图的过滤。", "result": "该工具成功应用于天气和金融数据分析，并与现有标准方法进行了比较。", "conclusion": "持久性离散同调是一种有效的数据分析新工具，尤其适用于图的过滤和高维数据的表示。", "translation": "我们提出了一种新的数据分析工具：持久性离散同调，它非常适合分析图的过滤。特别是，我们提供了一种新颖的方法，利用成对相关性将高维数据表示为图的过滤。我们讨论了这些工具的几个应用，例如在天气和金融数据中，并将其与各自领域中使用的标准方法进行了比较。", "summary": "本文介绍了一种名为持久性离散同调的新型数据分析工具，该工具特别适用于图的过滤分析。作者提出了一种利用成对相关性将高维数据转化为图过滤的新颖表示方法。文章还探讨了该工具在天气和金融数据等领域的应用，并将其性能与现有标准方法进行了对比。", "keywords": "持久性同调, 离散同调, 图过滤, 高维数据, 数据分析", "comments": "这篇论文的创新点在于提出了持久性离萨同调作为一种新的数据分析范式，并提供了一种将高维数据映射到图过滤的独特方法。其重要性在于为复杂数据集的拓扑分析提供了新的视角和工具，尤其在金融和气象等领域可能具有广泛应用潜力。"}}
{"id": "2506.15315", "title": "Proximal Operators of Sorted Nonconvex Penalties", "authors": ["Anne Gagneux", "Mathurin Massias", "Emmanuel Soubies"], "summary": "This work studies the problem of sparse signal recovery with automatic\ngrouping of variables. To this end, we investigate sorted nonsmooth penalties\nas a regularization approach for generalized linear models. We focus on a\nfamily of sorted nonconvex penalties which generalizes the Sorted L1 Norm\n(SLOPE). These penalties are designed to promote clustering of variables due to\ntheir sorted nature, while the nonconvexity reduces the shrinkage of\ncoefficients. Our goal is to provide efficient ways to compute their proximal\noperator, enabling the use of popular proximal algorithms to solve composite\noptimization problems with this choice of sorted penalties. We distinguish\nbetween two classes of problems: the weakly convex case where computing the\nproximal operator remains a convex problem, and the nonconvex case where\ncomputing the proximal operator becomes a challenging nonconvex combinatorial\nproblem. For the weakly convex case (e.g. sorted MCP and SCAD), we explain how\nthe Pool Adjacent Violators (PAV) algorithm can exactly compute the proximal\noperator. For the nonconvex case (e.g. sorted Lq with q in ]0,1[), we show that\na slight modification of this algorithm turns out to be remarkably efficient to\ntackle the computation of the proximal operator. We also present new\ntheoretical insights on the minimizers of the nonconvex proximal problem. We\ndemonstrate the practical interest of using such penalties on several\nexperiments.", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.15315v1", "AI": {"title_translation": "有序非凸惩罚的近端算子", "tldr": "本文研究了用于稀疏信号恢复的有序非凸惩罚的近端算子，区分了弱凸和非凸情况，并提供了高效的计算方法和理论见解。", "motivation": "本文旨在通过研究作为广义线性模型正则化方法的有序非光滑惩罚（特别是推广了有序L1范数（SLOPE）的有序非凸惩罚），来解决具有变量自动分组的稀疏信号恢复问题。目标是提供计算其近端算子的有效方法，以便在复合优化问题中使用流行的近端算法。", "method": "本文研究了有序非光滑惩罚，重点关注一类推广了SLOPE的有序非凸惩罚。作者区分了计算近端算子的两类问题：弱凸情况和非凸情况。对于弱凸情况（例如有序MCP和SCAD），解释了邻接违规池（PAV）算法如何精确计算近端算子。对于非凸情况（例如q在]0,1[范围内的有序Lq），展示了对PAV算法进行轻微修改后在处理近端算子计算时非常高效。此外，还提出了关于非凸近端问题极小值点的新理论见解。", "result": "对于弱凸情况（例如有序MCP和SCAD），PAV算法可以精确计算近端算子。对于非凸情况（例如q在]0,1[范围内的有序Lq），PAV算法的轻微修改被证明在处理近端算子计算时非常高效。提出了关于非凸近端问题极小值点的新理论见解。通过多项实验证明了使用此类惩罚的实际意义。", "conclusion": "本文为有序非凸惩罚的近端算子提供了高效的计算方法，使其能够应用于稀疏信号恢复中的复合优化问题，并提供了新的理论见解。", "translation": "这项工作研究了具有变量自动分组的稀疏信号恢复问题。为此，我们研究了有序非光滑惩罚作为广义线性模型的正则化方法。我们专注于一类有序非凸惩罚，它推广了有序L1范数（SLOPE）。这些惩罚因其有序性而旨在促进变量的聚类，同时非凸性减少了系数的收缩。我们的目标是提供计算其近端算子的有效方法，从而能够使用流行的近端算法来解决选择这类有序惩罚的复合优化问题。我们区分了两类问题：弱凸情况（其中计算近端算子仍然是凸问题）和非凸情况（其中计算近端算子成为一个具有挑战性的非凸组合问题）。对于弱凸情况（例如有序MCP和SCAD），我们解释了邻接违规池（PAV）算法如何精确计算近端算子。对于非凸情况（例如q在]0,1[范围内的有序Lq），我们表明该算法的轻微修改在处理近端算子的计算时非常高效。我们还提出了关于非凸近端问题极小值点的新理论见解。我们通过多项实验证明了使用此类惩罚的实际意义。", "summary": "本文研究了如何使用有序非凸惩罚来解决具有变量自动分组的稀疏信号恢复问题。这类惩罚推广了SLOPE，旨在促进变量聚类并减少系数收缩。核心贡献在于为这类惩罚的近端算子提供了高效的计算方法，这对于使用近端算法解决复合优化问题至关重要。作者区分了弱凸情况（PAV算法可精确计算）和更具挑战性的非凸情况（修改后的PAV算法表现出显著效率）。此外，本文还提供了关于非凸近端问题极小值点的新理论见解，并通过实验验证了其实用性。", "keywords": "稀疏信号恢复, 有序惩罚, 非凸优化, 近端算子, PAV算法", "comments": "本文在稀疏信号恢复领域，将有序惩罚扩展到非凸范畴，并专注于解决其近端算子的计算难题，具有创新性。对PAV算法在弱凸和非凸场景下的适应性改进是一个重要贡献，使得这些惩罚在实际的复合优化问题中得以应用。同时，对非凸近端问题极小值点的理论见解也进一步提升了论文的重要性。"}}
{"id": "2506.15387", "title": "Multi-Timescale Gradient Sliding for Distributed Optimization", "authors": ["Junhui Zhang", "Patrick Jaillet"], "summary": "We propose two first-order methods for convex, non-smooth, distributed\noptimization problems, hereafter called Multi-Timescale Gradient Sliding\n(MT-GS) and its accelerated variant (AMT-GS). Our MT-GS and AMT-GS can take\nadvantage of similarities between (local) objectives to reduce the\ncommunication rounds, are flexible so that different subsets (of agents) can\ncommunicate at different, user-picked rates, and are fully deterministic. These\nthree desirable features are achieved through a block-decomposable primal-dual\nformulation, and a multi-timescale variant of the sliding method introduced in\nLan et al. (2020), Lan (2016), where different dual blocks are updated at\npotentially different rates.\n  To find an $\\epsilon$-suboptimal solution, the complexities of our algorithms\nachieve optimal dependency on $\\epsilon$: MT-GS needs\n$O(\\overline{r}A/\\epsilon)$ communication rounds and\n$O(\\overline{r}/\\epsilon^2)$ subgradient steps for Lipchitz objectives, and\nAMT-GS needs $O(\\overline{r}A/\\sqrt{\\epsilon\\mu})$ communication rounds and\n$O(\\overline{r}/(\\epsilon\\mu))$ subgradient steps if the objectives are also\n$\\mu$-strongly convex. Here, $\\overline{r}$ measures the ``average rate of\nupdates'' for dual blocks, and $A$ measures similarities between (subgradients\nof) local functions. In addition, the linear dependency of communication rounds\non $A$ is optimal (Arjevani and Shamir 2015), thereby providing a positive\nanswer to the open question whether such dependency is achievable for\nnon-smooth objectives (Arjevani and Shamir 2015).", "comment": null, "cate": "math.OC", "url": "http://arxiv.org/abs/2506.15387v1", "AI": {"title_translation": "多尺度梯度滑动用于分布式优化", "tldr": "本文提出了两种新的分布式优化一阶方法MT-GS和AMT-GS，它们能有效减少通信轮次，并达到最优复杂度。", "motivation": "本文旨在解决凸、非光滑、分布式优化问题，通过利用局部目标之间的相似性来减少通信轮次，并提供灵活的用户自定义通信速率。", "method": "本文提出了两种一阶方法：多尺度梯度滑动（MT-GS）及其加速变体（AMT-GS）。这些方法通过块可分解的原始-对偶公式和Lan等人（2020）、Lan（2016）引入的滑动方法的多尺度变体实现，其中不同的对偶块可以以不同的速率更新。", "result": "对于$\\epsilon$-次优解，MT-GS在Lipschitz目标下需要$O(\\overline{r}A/\\epsilon)$次通信轮和$O(\\overline{r}/\\epsilon^2)$次次梯度步；AMT-GS在目标为$\\mu$-强凸时需要$O(\\overline{r}A/\\sqrt{\\epsilon\\mu})$次通信轮和$O(\\overline{r}/(\\epsilon\\mu))$次次梯度步。算法的复杂度对$\\epsilon$具有最优依赖性，且通信轮次对$A$的线性依赖性也是最优的。", "conclusion": "本文提出的MT-GS和AMT-GS算法在分布式优化中达到了对$\\epsilon$的最优复杂度，并且解决了非光滑目标下通信轮次对相似性参数$A$的线性依赖是否可实现这一开放问题。", "translation": "我们提出了两种用于凸、非光滑、分布式优化问题的一阶方法，分别称为多尺度梯度滑动（MT-GS）及其加速变体（AMT-GS）。我们的MT-GS和AMT-GS可以利用（局部）目标之间的相似性来减少通信轮次，具有灵活性，允许（代理）的不同子集以不同的、用户选择的速率进行通信，并且是完全确定性的。这三个理想的特性是通过块可分解的原始-对偶公式以及Lan等人（2020）、Lan（2016）中引入的滑动方法的多尺度变体实现的，其中不同的对偶块可以以潜在的不同速率进行更新。\n为了找到一个$\\epsilon$-次优解，我们算法的复杂度对$\\epsilon$实现了最优依赖性：对于Lipschitz目标，MT-GS需要$O(\\overline{r}A/\\epsilon)$次通信轮和$O(\\overline{r}/\\epsilon^2)$次次梯度步；如果目标也是$\\mu$-强凸的，AMT-GS需要$O(\\overline{r}A/\\sqrt{\\epsilon\\mu})$次通信轮和$O(\\overline{r}/(\\epsilon\\mu))$次次梯度步。这里，$\\overline{r}$衡量对偶块的“平均更新速率”，$A$衡量局部函数（的次梯度）之间的相似性。此外，通信轮次对$A$的线性依赖是最佳的（Arjevani和Shamir 2015），从而为非光滑目标是否可以实现这种依赖的开放问题提供了肯定的答案（Arjevani和Shamir 2015）。", "summary": "本文提出了两种用于凸、非光滑分布式优化的新型一阶方法：多尺度梯度滑动（MT-GS）及其加速版本（AMT-GS）。这些方法利用目标相似性来减少通信，并允许灵活的通信速率。通过块可分解的原始-对偶公式和多尺度滑动方法，它们在找到$\\epsilon$-次优解时，其通信轮次和次梯度步数复杂度对$\\epsilon$具有最优依赖性，并解决了非光滑目标下通信轮次对相似性参数$A$的线性依赖的开放问题。", "keywords": "分布式优化, 梯度滑动, 多尺度, 一阶方法, 复杂度分析", "comments": "本文的创新之处在于提出了多尺度梯度滑动方法，并将其应用于分布式优化，实现了通信轮次的显著减少。其关键优势在于算法的灵活性、确定性以及对问题参数的最优复杂度依赖性，特别是解决了非光滑目标下通信复杂度与相似性参数的线性依赖这一开放问题，具有重要的理论和实践意义。"}}
{"id": "2506.15505", "title": "Time-dependent density estimation using binary classifiers", "authors": ["Agnimitra Dasgupta", "Javier Murgoitio-Esandi", "Ali Fardisi", "Assad A Oberai"], "summary": "We propose a data-driven method to learn the time-dependent probability\ndensity of a multivariate stochastic process from sample paths, assuming that\nthe initial probability density is known and can be evaluated. Our method uses\na novel time-dependent binary classifier trained using a contrastive\nestimation-based objective that trains the classifier to discriminate between\nrealizations of the stochastic process at two nearby time instants.\nSignificantly, the proposed method explicitly models the time-dependent\nprobability distribution, which means that it is possible to obtain the value\nof the probability density within the time horizon of interest. Additionally,\nthe input before the final activation in the time-dependent classifier is a\nsecond-order approximation to the partial derivative, with respect to time, of\nthe logarithm of the density. We apply the proposed approach to approximate the\ntime-dependent probability density functions for systems driven by stochastic\nexcitations. We also use the proposed approach to synthesize new samples of a\nrandom vector from a given set of its realizations. In such applications, we\ngenerate sample paths necessary for training using stochastic interpolants.\nSubsequently, new samples are generated using gradient-based Markov chain Monte\nCarlo methods because automatic differentiation can efficiently provide the\nnecessary gradient. Further, we demonstrate the utility of an explicit\napproximation to the time-dependent probability density function through\napplications in unsupervised outlier detection. Through several numerical\nexperiments, we show that the proposed method accurately reconstructs complex\ntime-dependent, multi-modal, and near-degenerate densities, scales effectively\nto moderately high-dimensional problems, and reliably detects rare events among\nreal-world data.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.15505v1", "AI": {"title_translation": "使用二元分类器进行时间依赖密度估计", "tldr": "提出了一种使用二元分类器估计多元随机过程时间依赖概率密度的新数据驱动方法，能准确重建复杂密度并应用于异常检测和样本合成。", "motivation": "学习多元随机过程的时间依赖概率密度。", "method": "提出一种数据驱动方法，使用新颖的时间依赖二元分类器，通过基于对比估计的目标函数训练，以区分两个相邻时刻的随机过程实现。分类器的最终激活前的输入是对数密度关于时间偏导数的二阶近似。训练样本路径通过随机插值生成，新样本通过基于梯度的马尔可夫链蒙特卡洛（MCMC）方法生成。", "result": "该方法能够显式建模时间依赖概率分布，并在感兴趣的时间范围内获得概率密度值。它能准确重建复杂、时间依赖、多模态和近简并密度，有效扩展到中等高维问题，并可靠地检测真实数据中的罕见事件。该方法已应用于随机激励系统的时间依赖概率密度函数近似、随机向量新样本合成以及无监督异常检测。", "conclusion": "该方法提供了一种有效且实用的时间依赖概率密度函数近似，并在多个应用中展示了其准确性、可扩展性和实用性，尤其在复杂密度重建和异常检测方面表现出色。", "translation": "我们提出了一种数据驱动方法，用于从样本路径中学习多元随机过程的时间依赖概率密度，假设初始概率密度已知且可评估。我们的方法使用一种新颖的时间依赖二元分类器，该分类器通过基于对比估计的目标函数进行训练，旨在区分两个相邻时间点的随机过程实现。值得注意的是，所提出的方法明确地建模了时间依赖概率分布，这意味着可以在感兴趣的时间范围内获得概率密度值。此外，时间依赖分类器中最终激活前的输入是对数密度关于时间偏导数的二阶近似。我们将所提出的方法应用于近似由随机激励驱动的系统的时间依赖概率密度函数。我们还使用所提出的方法从给定的一组实现中合成随机向量的新样本。在此类应用中，我们使用随机插值器生成训练所需的样本路径。随后，由于自动微分可以有效地提供必要的梯度，因此使用基于梯度的马尔可夫链蒙特卡洛方法生成新样本。此外，我们通过在无监督异常检测中的应用，展示了时间依赖概率密度函数显式近似的效用。通过多项数值实验，我们表明所提出的方法能够准确重建复杂的时间依赖、多模态和近简并密度，有效地扩展到中等高维问题，并可靠地检测真实世界数据中的罕见事件。", "summary": "这篇论文提出了一种新的数据驱动方法，利用时间依赖二元分类器来估计多元随机过程的时间依赖概率密度。该分类器通过对比估计进行训练，以区分相邻时间点的过程实现。该方法能显式建模时间依赖分布，并能近似对数密度的时间偏导数。实验表明，该方法能准确重建复杂、多模态和近简并的密度，有效处理中高维问题，并成功应用于随机激励系统、新样本合成和无监督异常检测，尤其在检测真实世界数据中的罕见事件方面表现可靠。", "keywords": "时间依赖密度估计, 二元分类器, 对比估计, 随机过程, 异常检测", "comments": "这项工作创新性地将二元分类器与对比估计结合，用于时间依赖密度估计，摆脱了传统参数或非参数方法的限制。其能够显式建模时间依赖分布并处理复杂密度（多模态、近简并）的能力是其重要亮点。此外，结合自动微分和MCMC进行样本合成，以及在异常检测中的应用，都展示了该方法的广泛实用性。"}}
{"id": "2506.15643", "title": "Revisiting Randomization in Greedy Model Search", "authors": ["Xin Chen", "Jason M. Klusowski", "Yan Shuo Tan", "Chang Yu"], "summary": "Combining randomized estimators in an ensemble, such as via random forests,\nhas become a fundamental technique in modern data science, but can be\ncomputationally expensive. Furthermore, the mechanism by which this improves\npredictive performance is poorly understood. We address these issues in the\ncontext of sparse linear regression by proposing and analyzing an ensemble of\ngreedy forward selection estimators that are randomized by feature subsampling\n-- at each iteration, the best feature is selected from within a random subset.\nWe design a novel implementation based on dynamic programming that greatly\nimproves its computational efficiency. Furthermore, we show via careful\nnumerical experiments that our method can outperform popular methods such as\nlasso and elastic net across a wide range of settings. Next, contrary to\nprevailing belief that randomized ensembling is analogous to shrinkage, we show\nvia numerical experiments that it can simultaneously reduce training error and\ndegrees of freedom, thereby shifting the entire bias-variance trade-off curve\nof the base estimator. We prove this fact rigorously in the setting of\northogonal features, in which case, the ensemble estimator rescales the\nordinary least squares coefficients with a two-parameter family of logistic\nweights, thereby enlarging the model search space. These results enhance our\nunderstanding of random forests and suggest that implicit regularization in\ngeneral may have more complicated effects than explicit regularization.", "comment": null, "cate": "stat.ML", "url": "http://arxiv.org/abs/2506.15643v1", "AI": {"title_translation": "重新审视贪婪模型搜索中的随机化", "tldr": "本文提出一种基于特征子采样的随机化贪婪前向选择集成方法，通过动态规划提高效率，并在稀疏线性回归中表现优于Lasso，同时揭示随机化集成能同时降低训练误差和自由度，从而改变偏差-方差权衡曲线。", "motivation": "随机化集成（如随机森林）计算成本高昂，且其提高预测性能的机制尚不明确。", "method": "针对稀疏线性回归，提出一种通过特征子采样随机化的贪婪前向选择估计器集成方法，在每次迭代中从随机子集中选择最佳特征。同时，设计了一种基于动态规划的新颖实现以显著提高计算效率。", "result": "1. 该方法在多种设置下优于Lasso和Elastic Net等流行方法。 2. 数值实验表明，与普遍认为随机化集成类似于收缩不同，它能同时降低训练误差和自由度，从而改变基估计器的偏差-方差权衡曲线。 3. 在正交特征设置下，严格证明了上述事实，此时集成估计器通过两参数逻辑权重重新缩放普通最小二乘系数，从而扩大了模型搜索空间。", "conclusion": "这些结果增进了对随机森林的理解，并表明隐式正则化通常可能比显式正则化具有更复杂的影响。", "translation": "将随机估计器组合成集成模型（例如通过随机森林）已成为现代数据科学中的一项基本技术，但其计算成本可能很高。此外，其提高预测性能的机制尚不明确。本文通过在稀疏线性回归的背景下提出并分析一种贪婪前向选择估计器的集成方法来解决这些问题，该方法通过特征子采样进行随机化——在每次迭代中，从一个随机子集中选择最佳特征。我们设计了一种基于动态规划的新颖实现，极大地提高了其计算效率。此外，我们通过仔细的数值实验表明，我们的方法在广泛的设置下可以胜过Lasso和Elastic Net等流行方法。接下来，与普遍认为随机化集成类似于收缩的观点相反，我们通过数值实验表明，它可以在同时减少训练误差和自由度的同时，从而改变基估计器的整个偏差-方差权衡曲线。我们在正交特征的设置中严格证明了这一事实，在这种情况下，集成估计器通过两参数逻辑权重族重新缩放普通最小二乘系数，从而扩大了模型搜索空间。这些结果增强了我们对随机森林的理解，并表明一般的隐式正则化可能比显式正则化具有更复杂的影响。", "summary": "本文针对随机化集成方法计算成本高昂且机制不明的问题，在稀疏线性回归背景下提出一种基于特征子采样的随机化贪婪前向选择集成方法，并通过动态规划提高效率。研究表明，该方法在性能上优于Lasso和Elastic Net，并且揭示随机化集成能同时降低训练误差和自由度，从而改变偏差-方差权衡曲线，这与传统观念不同。理论上，该方法通过重新缩放系数扩大了模型搜索空间。这些发现加深了对随机森林和隐式正则化的理解。", "keywords": "随机化集成, 贪婪模型搜索, 特征子采样, 稀疏线性回归, 偏差-方差权衡", "comments": "本文的创新之处在于提出了一种高效的随机化贪婪模型搜索方法，并通过严谨的数值实验和理论分析，揭示了随机化集成并非简单地等同于收缩，而是能够通过同时降低训练误差和自由度来改变偏差-方差权衡曲线，这对于理解随机森林等集成方法的内在机制具有重要意义。它强调了隐式正则化可能比显式正则化具有更复杂且深远的影响，为未来的模型设计和理论研究提供了新的视角。"}}
