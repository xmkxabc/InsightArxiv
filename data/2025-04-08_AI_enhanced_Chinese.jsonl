{"id": "2504.03739", "pdf": "https://arxiv.org/pdf/2504.03739", "abs": "https://arxiv.org/abs/2504.03739", "authors": ["Mingyan Liu"], "title": "A Unified Virtual Mixture-of-Experts Framework:Enhanced Inference and Hallucination Mitigation in Single-Model System", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative models, such as GPT and BERT, have significantly improved\nperformance in tasks like text generation and summarization. However,\nhallucinations \"where models generate non-factual or misleading content\" are\nespecially problematic in smaller-scale architectures, limiting their\nreal-world applicability.In this paper, we propose a unified Virtual\nMixture-of-Experts (MoE) fusion strategy that enhances inference performance\nand mitigates hallucinations in a single Qwen 1.5 0.5B model without increasing\nthe parameter count. Our method leverages multiple domain-specific expert\nprompts (with the number of experts being adjustable) to guide the model from\ndifferent perspectives. We apply a statistical outlier truncation strategy\nbased on the mean and standard deviation to filter out abnormally high\nprobability predictions, and we inject noise into the embedding space to\npromote output diversity. To clearly assess the contribution of each module, we\nadopt a fixed voting mechanism rather than a dynamic gating network, thereby\navoiding additional confounding factors. We provide detailed theoretical\nderivations from both statistical and ensemble learning perspectives to\ndemonstrate how our method reduces output variance and suppresses\nhallucinations. Extensive ablation experiments on dialogue generation tasks\nshow that our approach significantly improves inference accuracy and robustness\nin small models. Additionally, we discuss methods for evaluating the\northogonality of virtual experts and outline the potential for future work\ninvolving dynamic expert weight allocation using gating networks.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u865a\u62df\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u878d\u5408\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u5c0f\u578b\u6a21\u578b\uff08Qwen 1.5 0.5B\uff09\u7684\u63a8\u7406\u6027\u80fd\u5e76\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u6a21\u578b\uff08\u5982GPT\u548cBERT\uff09\u5728\u6587\u672c\u751f\u6210\u548c\u6458\u8981\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5c0f\u578b\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff08\u751f\u6210\u975e\u4e8b\u5b9e\u6216\u8bef\u5bfc\u6027\u5185\u5bb9\uff09\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u591a\u9886\u57df\u4e13\u5bb6\u63d0\u793a\u5f15\u5bfc\u6a21\u578b\uff0c\u7ed3\u5408\u7edf\u8ba1\u5f02\u5e38\u622a\u65ad\u7b56\u7565\u548c\u5d4c\u5165\u7a7a\u95f4\u566a\u58f0\u6ce8\u5165\uff0c\u4f7f\u7528\u56fa\u5b9a\u6295\u7968\u673a\u5236\u800c\u975e\u52a8\u6001\u95e8\u63a7\u7f51\u7edc\u3002", "result": "\u5728\u5bf9\u8bdd\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5c0f\u578b\u6a21\u578b\u7684\u63a8\u7406\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u51cf\u5c11\u8f93\u51fa\u65b9\u5dee\u548c\u6291\u5236\u5e7b\u89c9\uff0c\u4e3a\u5c0f\u578b\u6a21\u578b\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u5e76\u63a2\u8ba8\u4e86\u52a8\u6001\u4e13\u5bb6\u6743\u91cd\u5206\u914d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.03786", "pdf": "https://arxiv.org/pdf/2504.03786", "abs": "https://arxiv.org/abs/2504.03786", "authors": ["Sifan Li", "Yujun Cai", "Bryan Hooi", "Nanyun Peng", "Yiwei Wang"], "title": "Do \"New Snow Tablets\" Contain Snow? Large Language Models Over-Rely on Names to Identify Ingredients of Chinese Drugs", "categories": ["cs.CL"], "comment": null, "summary": "Traditional Chinese Medicine (TCM) has seen increasing adoption in\nhealthcare, with specialized Large Language Models (LLMs) emerging to support\nclinical applications. A fundamental requirement for these models is accurate\nidentification of TCM drug ingredients. In this paper, we evaluate how general\nand TCM-specialized LLMs perform when identifying ingredients of Chinese drugs.\nOur systematic analysis reveals consistent failure patterns: models often\ninterpret drug names literally, overuse common herbs regardless of relevance,\nand exhibit erratic behaviors when faced with unfamiliar formulations. LLMs\nalso fail to understand the verification task. These findings demonstrate that\ncurrent LLMs rely primarily on drug names rather than possessing systematic\npharmacological knowledge. To address these limitations, we propose a Retrieval\nAugmented Generation (RAG) approach focused on ingredient names. Experiments\nacross 220 TCM formulations show our method significantly improves accuracy\nfrom approximately 50% to 82% in ingredient verification tasks. Our work\nhighlights critical weaknesses in current TCM-specific LLMs and offers a\npractical solution for enhancing their clinical reliability.", "AI": {"task": "\u8bc4\u4f30\u901a\u7528\u548c\u4e2d\u533b\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u4e2d\u836f\u6210\u5206\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4e2d\u533b\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u836f\u6210\u5206\u8bc6\u522b\u4e2d\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u5f71\u54cd\u4e34\u5e8a\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u6210\u5206\u540d\u79f0\u7684\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u6210\u5206\u9a8c\u8bc1\u4efb\u52a1\u7684\u51c6\u786e\u7387\u4ece\u7ea650%\u63d0\u5347\u81f382%\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u4e2d\u533b\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5173\u952e\u5f31\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u5347\u5176\u4e34\u5e8a\u53ef\u9760\u6027\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.03790", "pdf": "https://arxiv.org/pdf/2504.03790", "abs": "https://arxiv.org/abs/2504.03790", "authors": ["Gon\u00e7alo Faria", "Noah A. Smith"], "title": "Sample, Don't Search: Rethinking Test-Time Alignment for Language Models", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Increasing test-time computation has emerged as a promising direction for\nimproving language model performance, particularly in scenarios where model\nfinetuning is impractical or impossible due to computational constraints or\nprivate model weights. However, existing test-time search methods using a\nreward model (RM) often degrade in quality as compute scales, due to the\nover-optimization of what are inherently imperfect reward proxies. We introduce\nQAlign, a new test-time alignment approach. As we scale test-time compute,\nQAlign converges to sampling from the optimal aligned distribution for each\nindividual prompt. By adopting recent advances in Markov chain Monte Carlo for\ntext generation, our method enables better-aligned outputs without modifying\nthe underlying model or even requiring logit access. We demonstrate the\neffectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and\nGSM-Symbolic) using a task-specific RM, showing consistent improvements over\nexisting test-time compute methods like best-of-n and majority voting.\nFurthermore, when applied with more realistic RMs trained on the Tulu 3\npreference dataset, QAlign outperforms direct preference optimization (DPO),\nbest-of-n, majority voting, and weighted majority voting on a diverse range of\ndatasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical\nsolution to aligning language models at test time using additional computation\nwithout degradation, our approach expands the limits of the capability that can\nbe obtained from off-the-shelf language models without further training.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u65b9\u6cd5QAlign\uff0c\u4ee5\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u641c\u7d22\u65b9\u6cd5\u5728\u8ba1\u7b97\u89c4\u6a21\u6269\u5927\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u4e3a\u8fc7\u5ea6\u4f18\u5316\u4e86\u4e0d\u5b8c\u7f8e\u7684\u5956\u52b1\u4ee3\u7406\u3002", "method": "\u91c7\u7528\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u8fdb\u884c\u6587\u672c\u751f\u6210\uff0c\u65e0\u9700\u4fee\u6539\u5e95\u5c42\u6a21\u578b\u6216\u8bbf\u95eelogit\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\uff0cQAlign\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "QAlign\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u8fdb\u4e00\u6b65\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u73b0\u6210\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u3002"}}
{"id": "2504.03794", "pdf": "https://arxiv.org/pdf/2504.03794", "abs": "https://arxiv.org/abs/2504.03794", "authors": ["Liangwei Yang", "Yuhui Xu", "Juntao Tan", "Doyen Sahoo", "Silvio Savarese", "Caiming Xiong", "Huan Wang", "Shelby Heinecke"], "title": "Entropy-Based Block Pruning for Efficient Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures", "summary": "As large language models continue to scale, their growing computational and\nstorage demands pose significant challenges for real-world deployment. In this\nwork, we investigate redundancy within Transformer-based models and propose an\nentropy-based pruning strategy to enhance efficiency while maintaining\nperformance. Empirical analysis reveals that the entropy of hidden\nrepresentations decreases in the early blocks but progressively increases\nacross most subsequent blocks. This trend suggests that entropy serves as a\nmore effective measure of information richness within computation blocks.\nUnlike cosine similarity, which primarily captures geometric relationships,\nentropy directly quantifies uncertainty and information content, making it a\nmore reliable criterion for pruning. Extensive experiments demonstrate that our\nentropy-based pruning approach surpasses cosine similarity-based methods in\nreducing model size while preserving accuracy, offering a promising direction\nfor efficient model deployment.", "AI": {"task": "\u7814\u7a76\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u4e2d\u7684\u5197\u4f59\u5e76\u63d0\u51fa\u57fa\u4e8e\u71b5\u7684\u526a\u679d\u7b56\u7565\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89c4\u6a21\u548c\u5b58\u50a8\u9700\u6c42\u589e\u957f\u7ed9\u5b9e\u9645\u90e8\u7f72\u5e26\u6765\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u71b5\u7684\u526a\u679d\u7b56\u7565\uff0c\u5229\u7528\u9690\u85cf\u8868\u793a\u71b5\u7684\u53d8\u5316\u8d8b\u52bf\u4f5c\u4e3a\u4fe1\u606f\u4e30\u5bcc\u5ea6\u7684\u8861\u91cf\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u71b5\u7684\u526a\u679d\u65b9\u6cd5\u5728\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u7684\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u6027\u7684\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u71b5\u7684\u526a\u679d\u7b56\u7565\u4e3a\u9ad8\u6548\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2504.03705", "pdf": "https://arxiv.org/pdf/2504.03705", "abs": "https://arxiv.org/abs/2504.03705", "authors": ["Luca Marini"], "title": "Semi-supervised learning for marine anomaly detection on board satellites", "categories": ["cs.CV"], "comment": "Master's project", "summary": "Aquatic bodies face numerous environmental threats caused by several marine\nanomalies. Marine debris can devastate habitats and endanger marine life\nthrough entanglement, while harmful algal blooms can produce toxins that\nnegatively affect marine ecosystems. Additionally, ships may discharge oil or\nengage in illegal and overfishing activities, causing further harm. These\nmarine anomalies can be identified by applying trained deep learning (DL)\nmodels on multispectral satellite imagery. Furthermore, the detection of other\nanomalies, such as clouds, could be beneficial in filtering out irrelevant\nimages. However, DL models often require a large volume of labeled data for\ntraining, which can be both costly and time-consuming, particularly for marine\nanomaly detection where expert annotation is needed. A potential solution is\nthe use of semi-supervised learning methods, which can also utilize unlabeled\ndata. In this project, we implement and study the performance of FixMatch for\nSemantic Segmentation, a semi-supervised algorithm for semantic segmentation.\nFirstly, we found that semi-supervised models perform best with a high\nconfidence threshold of 0.9 when there is a limited amount of labeled data.\nSecondly, we compare the performance of semi-supervised models with\nfully-supervised models under varying amounts of labeled data. Our findings\nsuggest that semi-supervised models outperform fully-supervised models with\nlimited labeled data, while fully-supervised models have a slightly better\nperformance with larger volumes of labeled data. We propose two hypotheses to\nexplain why fully-supervised models surpass semi-supervised ones when a high\nvolume of labeled data is used. All of our experiments were conducted using a\nU-Net model architecture with a limited number of parameters to ensure\ncompatibility with space-rated hardware.", "AI": {"task": "\u5229\u7528\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff08FixMatch\uff09\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff0c\u4ee5\u68c0\u6d4b\u6d77\u6d0b\u5f02\u5e38\u3002", "motivation": "\u6d77\u6d0b\u5f02\u5e38\uff08\u5982\u6d77\u6d0b\u5783\u573e\u3001\u6709\u5bb3\u85fb\u534e\u3001\u975e\u6cd5\u6355\u635e\u7b49\uff09\u5bf9\u751f\u6001\u7cfb\u7edf\u9020\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u4f46\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u3002\u534a\u76d1\u7763\u5b66\u4e60\u53ef\u4ee5\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\uff0c\u964d\u4f4e\u6210\u672c\u3002", "method": "\u91c7\u7528FixMatch\u7b97\u6cd5\u8fdb\u884c\u8bed\u4e49\u5206\u5272\uff0c\u4f7f\u7528U-Net\u6a21\u578b\u67b6\u6784\uff0c\u6bd4\u8f83\u534a\u76d1\u7763\u4e0e\u5168\u76d1\u7763\u6a21\u578b\u5728\u4e0d\u540c\u6807\u6ce8\u6570\u636e\u91cf\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u534a\u76d1\u7763\u6a21\u578b\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u65f6\u8868\u73b0\u4f18\u4e8e\u5168\u76d1\u7763\u6a21\u578b\uff0c\u800c\u5168\u76d1\u7763\u6a21\u578b\u5728\u6807\u6ce8\u6570\u636e\u5145\u8db3\u65f6\u7565\u4f18\u3002", "conclusion": "\u534a\u76d1\u7763\u5b66\u4e60\u5728\u6d77\u6d0b\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u65f6\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2504.03803", "pdf": "https://arxiv.org/pdf/2504.03803", "abs": "https://arxiv.org/abs/2504.03803", "authors": ["Sander Noels", "Guillaume Bied", "Maarten Buyl", "Alexander Rogiers", "Yousra Fettach", "Jefrey Lijffijt", "Tijl De Bie"], "title": "What Large Language Models Do Not Talk About: An Empirical Study of Moderation and Censorship Practices", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": "17 pages, 38 pages in total including appendix; 5 figures, 22 figures\n  in appendix", "summary": "Large Language Models (LLMs) are increasingly deployed as gateways to\ninformation, yet their content moderation practices remain underexplored. This\nwork investigates the extent to which LLMs refuse to answer or omit information\nwhen prompted on political topics. To do so, we distinguish between hard\ncensorship (i.e., generated refusals, error messages, or canned denial\nresponses) and soft censorship (i.e., selective omission or downplaying of key\nelements), which we identify in LLMs' responses when asked to provide\ninformation on a broad range of political figures. Our analysis covers 14\nstate-of-the-art models from Western countries, China, and Russia, prompted in\nall six official United Nations (UN) languages. Our analysis suggests that\nalthough censorship is observed across the board, it is predominantly tailored\nto an LLM provider's domestic audience and typically manifests as either hard\ncensorship or soft censorship (though rarely both concurrently). These findings\nunderscore the need for ideological and geographic diversity among publicly\navailable LLMs, and greater transparency in LLM moderation strategies to\nfacilitate informed user choices. All data are made freely available.", "AI": {"task": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u653f\u6cbb\u8bdd\u9898\u4e0a\u7684\u5185\u5bb9\u5ba1\u67e5\u5b9e\u8df5\uff0c\u533a\u5206\u786c\u5ba1\u67e5\u548c\u8f6f\u5ba1\u67e5\u3002", "motivation": "LLMs\u4f5c\u4e3a\u4fe1\u606f\u95e8\u6237\u7684\u90e8\u7f72\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5176\u5185\u5bb9\u5ba1\u67e5\u5b9e\u8df5\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5206\u679014\u4e2a\u6765\u81ea\u897f\u65b9\u56fd\u5bb6\u3001\u4e2d\u56fd\u548c\u4fc4\u7f57\u65af\u7684\u6700\u5148\u8fdb\u6a21\u578b\u5728\u8054\u5408\u56fd\u516d\u79cd\u5b98\u65b9\u8bed\u8a00\u4e2d\u7684\u56de\u7b54\uff0c\u533a\u5206\u786c\u5ba1\u67e5\u548c\u8f6f\u5ba1\u67e5\u3002", "result": "\u5ba1\u67e5\u666e\u904d\u5b58\u5728\uff0c\u4f46\u4e3b\u8981\u9488\u5bf9LLM\u63d0\u4f9b\u8005\u7684\u56fd\u5185\u53d7\u4f17\uff0c\u8868\u73b0\u4e3a\u786c\u5ba1\u67e5\u6216\u8f6f\u5ba1\u67e5\uff08\u5f88\u5c11\u540c\u65f6\u51fa\u73b0\uff09\u3002", "conclusion": "\u9700\u8981\u516c\u5f00\u53ef\u7528\u7684LLMs\u5728\u610f\u8bc6\u5f62\u6001\u548c\u5730\u7406\u4e0a\u7684\u591a\u6837\u6027\uff0c\u4ee5\u53ca\u66f4\u900f\u660e\u7684\u5ba1\u67e5\u7b56\u7565\uff0c\u4ee5\u652f\u6301\u7528\u6237\u77e5\u60c5\u9009\u62e9\u3002"}}
{"id": "2504.03712", "pdf": "https://arxiv.org/pdf/2504.03712", "abs": "https://arxiv.org/abs/2504.03712", "authors": ["Jan Lewen", "Max Pargmann", "Jenia Jitsev", "Mehdi Cherti", "Robert Pitz-Paal", "Daniel Maldonado Quinto"], "title": "Scalable heliostat surface predictions from focal spots: Sim-to-Real transfer of inverse Deep Learning Raytracing", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "Concentrating Solar Power (CSP) plants are a key technology in the transition\ntoward sustainable energy. A critical factor for their safe and efficient\noperation is the distribution of concentrated solar flux on the receiver.\nHowever, flux distributions from individual heliostats are sensitive to surface\nimperfections. Measuring these surfaces across many heliostats remains\nimpractical in real-world deployments. As a result, control systems often\nassume idealized heliostat surfaces, leading to suboptimal performance and\npotential safety risks. To address this, inverse Deep Learning Raytracing\n(iDLR) has been introduced as a novel method for inferring heliostat surface\nprofiles from target images recorded during standard calibration procedures. In\nthis work, we present the first successful Sim-to-Real transfer of iDLR,\nenabling accurate surface predictions directly from real-world target images.\nWe evaluate our method on 63 heliostats under real operational conditions. iDLR\nsurface predictions achieve a median mean absolute error (MAE) of 0.17 mm and\nshow good agreement with deflectometry ground truth in 84% of cases. When used\nin raytracing simulations, it enables flux density predictions with a mean\naccuracy of 90% compared to deflectometry over our dataset, and outperforms the\ncommonly used ideal heliostat surface assumption by 26%. We tested this\napproach in a challenging double-extrapolation scenario-involving unseen sun\npositions and receiver projection-and found that iDLR maintains high predictive\naccuracy, highlighting its generalization capabilities. Our results demonstrate\nthat iDLR is a scalable, automated, and cost-effective solution for integrating\nrealistic heliostat surface models into digital twins. This opens the door to\nimproved flux control, more precise performance modeling, and ultimately,\nenhanced efficiency and safety in future CSP plants.", "AI": {"task": "\u5229\u7528\u9006\u6df1\u5ea6\u5b66\u4e60\u5149\u7ebf\u8ffd\u8e2a\uff08iDLR\uff09\u65b9\u6cd5\u4ece\u5b9e\u9645\u76ee\u6807\u56fe\u50cf\u4e2d\u63a8\u65ad\u5b9a\u65e5\u955c\u8868\u9762\u8f6e\u5ed3\uff0c\u4ee5\u63d0\u9ad8\u805a\u5149\u592a\u9633\u80fd\u53d1\u7535\uff08CSP\uff09\u7cfb\u7edf\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5b9a\u65e5\u955c\u8868\u9762\u7f3a\u9677\u4f1a\u5f71\u54cd\u805a\u5149\u592a\u9633\u80fd\u53d1\u7535\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u4e2d\u96be\u4ee5\u6d4b\u91cf\u8fd9\u4e9b\u7f3a\u9677\uff0c\u5bfc\u81f4\u63a7\u5236\u7cfb\u7edf\u5047\u8bbe\u7406\u60f3\u8868\u9762\uff0c\u4ece\u800c\u5f71\u54cd\u6548\u679c\u3002", "method": "\u63d0\u51fa\u9006\u6df1\u5ea6\u5b66\u4e60\u5149\u7ebf\u8ffd\u8e2a\uff08iDLR\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6807\u51c6\u6821\u51c6\u8fc7\u7a0b\u4e2d\u8bb0\u5f55\u7684\u76ee\u6807\u56fe\u50cf\u63a8\u65ad\u5b9a\u65e5\u955c\u8868\u9762\u8f6e\u5ed3\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4ece\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002", "result": "\u572863\u4e2a\u5b9a\u65e5\u955c\u4e0a\u9a8c\u8bc1\uff0ciDLR\u7684\u4e2d\u4f4d\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a0.17\u6beb\u7c73\uff0c84%\u60c5\u51b5\u4e0b\u4e0e\u5730\u9762\u771f\u5b9e\u6570\u636e\u4e00\u81f4\uff0c\u6a21\u62df\u4e2d\u901a\u91cf\u5bc6\u5ea6\u9884\u6d4b\u51c6\u786e\u7387\u8fbe90%\uff0c\u6bd4\u7406\u60f3\u5047\u8bbe\u63d0\u9ad826%\u3002", "conclusion": "iDLR\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u81ea\u52a8\u5316\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u7528\u4e8e\u6570\u5b57\u5b6a\u751f\u4e2d\u96c6\u6210\u771f\u5b9e\u5b9a\u65e5\u955c\u8868\u9762\u6a21\u578b\uff0c\u63d0\u5347\u672a\u6765CSP\u7cfb\u7edf\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2504.03846", "pdf": "https://arxiv.org/pdf/2504.03846", "abs": "https://arxiv.org/abs/2504.03846", "authors": ["Wei-Lin Chen", "Zhepei Wei", "Xinyu Zhu", "Shi Feng", "Yu Meng"], "title": "Do LLM Evaluators Prefer Themselves for a Reason?", "categories": ["cs.CL"], "comment": "Preprint. 31 pages", "summary": "Large language models (LLMs) are increasingly used as automatic evaluators in\napplications such as benchmarking, reward modeling, and self-refinement. Prior\nwork highlights a potential self-preference bias where LLMs favor their own\ngenerated responses, a tendency often intensifying with model size and\ncapability. This raises a critical question: Is self-preference detrimental, or\ndoes it simply reflect objectively superior outputs from more capable models?\nDisentangling these has been challenging due to the usage of subjective tasks\nin previous studies. To address this, we investigate self-preference using\nverifiable benchmarks (mathematical reasoning, factual knowledge, code\ngeneration) that allow objective ground-truth assessment. This enables us to\ndistinguish harmful self-preference (favoring objectively worse responses) from\nlegitimate self-preference (favoring genuinely superior ones). We conduct\nlarge-scale experiments under controlled evaluation conditions across diverse\nmodel families (e.g., Llama, Qwen, Gemma, Mistral, Phi, GPT, DeepSeek). Our\nfindings reveal three key insights: (1) Better generators are better judges --\nLLM evaluators' accuracy strongly correlates with their task performance, and\nmuch of the self-preference in capable models is legitimate. (2) Harmful\nself-preference persists, particularly when evaluator models perform poorly as\ngenerators on specific task instances. Stronger models exhibit more pronounced\nharmful bias when they err, though such incorrect generations are less\nfrequent. (3) Inference-time scaling strategies, such as generating a long\nChain-of-Thought before evaluation, effectively reduce the harmful\nself-preference. These results provide a more nuanced understanding of\nLLM-based evaluation and practical insights for improving its reliability.", "AI": {"task": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u81ea\u52a8\u8bc4\u4f30\u4e2d\u7684\u81ea\u6211\u504f\u597d\u504f\u5dee\u53ca\u5176\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u81ea\u6211\u504f\u597d\u662f\u5426\u5bf9\u8bc4\u4f30\u7ed3\u679c\u6709\u5bb3\uff0c\u6216\u4ec5\u53cd\u6620\u6a21\u578b\u80fd\u529b\u7684\u5ba2\u89c2\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u53ef\u9a8c\u8bc1\u7684\u57fa\u51c6\uff08\u6570\u5b66\u63a8\u7406\u3001\u4e8b\u5b9e\u77e5\u8bc6\u3001\u4ee3\u7801\u751f\u6210\uff09\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u533a\u5206\u6709\u5bb3\u4e0e\u5408\u6cd5\u7684\u81ea\u6211\u504f\u597d\u3002", "result": "\u53d1\u73b0\u66f4\u597d\u7684\u751f\u6210\u6a21\u578b\u4e5f\u662f\u66f4\u597d\u7684\u8bc4\u4f30\u8005\uff0c\u6709\u5bb3\u81ea\u6211\u504f\u597d\u5728\u6a21\u578b\u8868\u73b0\u5dee\u65f6\u66f4\u660e\u663e\uff0c\u63a8\u7406\u65f6\u7b56\u7565\u53ef\u51cf\u5c11\u6709\u5bb3\u504f\u5dee\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u5bf9LLM\u8bc4\u4f30\u7684\u66f4\u7ec6\u81f4\u7406\u89e3\uff0c\u5e76\u4e3a\u63d0\u9ad8\u5176\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2504.03724", "pdf": "https://arxiv.org/pdf/2504.03724", "abs": "https://arxiv.org/abs/2504.03724", "authors": ["Zhiqiang Wang", "Pengbin Feng", "Yanbin Lin", "Shuzhang Cai", "Zongao Bian", "Jinghua Yan", "Xingquan Zhu"], "title": "CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 6 figures and 4 tables", "summary": "We propose Fuzzy Group Relative Policy Reward (FGRPR), a novel framework that\nintegrates Group Relative Policy Optimization (GRPO) with a fuzzy reward\nfunction to enhance learning efficiency. Unlike the conventional binary 0/1\naccuracy reward, our fuzzy reward model provides nuanced incentives,\nencouraging more precise outputs. Experimental results demonstrate that GRPO\nwith a standard 0/1 accuracy reward underperforms compared to supervised\nfine-tuning (SFT). In contrast, FGRPR, applied to Qwen2.5-VL(3B and 7B),\nsurpasses all baseline models, including GPT4o, LLaMA2(90B), and SFT, across\nfive in-domain datasets. On an out-of-domain dataset, FGRPR achieves\nperformance comparable to SFT but excels when target values are larger, as its\nfuzzy reward function assigns higher rewards to closer approximations. This\napproach is broadly applicable to tasks where the precision of the answer is\ncritical. Code and data: https://github.com/yeyimilk/CrowdVLM-R1", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aFuzzy Group Relative Policy Reward (FGRPR)\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408Group Relative Policy Optimization (GRPO)\u548c\u6a21\u7cca\u5956\u52b1\u51fd\u6570\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u4e8c\u51430/1\u51c6\u786e\u5ea6\u5956\u52b1\u4e0d\u591f\u7cbe\u7ec6\uff0c\u65e0\u6cd5\u9f13\u52b1\u66f4\u7cbe\u786e\u7684\u8f93\u51fa\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u5956\u52b1\u673a\u5236\u3002", "method": "\u5c06GRPO\u4e0e\u6a21\u7cca\u5956\u52b1\u51fd\u6570\u7ed3\u5408\uff0c\u901a\u8fc7\u63d0\u4f9b\u66f4\u7ec6\u81f4\u7684\u6fc0\u52b1\u6765\u4f18\u5316\u6a21\u578b\u8f93\u51fa\u3002", "result": "FGRPR\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff08\u5305\u62ecGPT4o\u3001LLaMA2\u548cSFT\uff09\uff0c\u5c24\u5176\u5728\u76ee\u6807\u503c\u8f83\u5927\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "FGRPR\u9002\u7528\u4e8e\u9700\u8981\u7b54\u6848\u7cbe\u786e\u6027\u7684\u4efb\u52a1\uff0c\u5176\u6a21\u7cca\u5956\u52b1\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.03906", "pdf": "https://arxiv.org/pdf/2504.03906", "abs": "https://arxiv.org/abs/2504.03906", "authors": ["Abhilekh Borah", "Hasnat Md Abdullah", "Kangda Wei", "Ruihong Huang"], "title": "CliME: Evaluating Multimodal Climate Discourse on Social Media and the Climate Alignment Quotient (CAQ)", "categories": ["cs.CL"], "comment": "16 pages, 9 figures", "summary": "The rise of Large Language Models (LLMs) has raised questions about their\nability to understand climate-related contexts. Though climate change dominates\nsocial media, analyzing its multimodal expressions is understudied, and current\ntools have failed to determine whether LLMs amplify credible solutions or\nspread unsubstantiated claims. To address this, we introduce CliME (Climate\nChange Multimodal Evaluation), a first-of-its-kind multimodal dataset,\ncomprising 2579 Twitter and Reddit posts. The benchmark features a diverse\ncollection of humorous memes and skeptical posts, capturing how these formats\ndistill complex issues into viral narratives that shape public opinion and\npolicy discussions. To systematically evaluate LLM performance, we present the\nClimate Alignment Quotient (CAQ), a novel metric comprising five distinct\ndimensions: Articulation, Evidence, Resonance, Transition, and Specificity.\nAdditionally, we propose three analytical lenses: Actionability, Criticality,\nand Justice, to guide the assessment of LLM-generated climate discourse using\nCAQ. Our findings, based on the CAQ metric, indicate that while most evaluated\nLLMs perform relatively well in Criticality and Justice, they consistently\nunderperform on the Actionability axis. Among the models evaluated, Claude 3.7\nSonnet achieves the highest overall performance. We publicly release our CliME\ndataset and code to foster further research in this domain.", "AI": {"task": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7406\u89e3\u548c\u5904\u7406\u6c14\u5019\u76f8\u5173\u5185\u5bb9\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u6c14\u5019\u95ee\u9898\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\u5e7f\u6cdb\u4f20\u64ad\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u591a\u6a21\u6001\u8868\u8fbe\u7684\u5206\u6790\u5de5\u5177\uff0c\u65e0\u6cd5\u5224\u65adLLMs\u662f\u4f20\u64ad\u53ef\u4fe1\u89e3\u51b3\u65b9\u6848\u8fd8\u662f\u672a\u7ecf\u8bc1\u5b9e\u7684\u89c2\u70b9\u3002", "method": "\u5f15\u5165CliME\u6570\u636e\u96c6\u548cClimate Alignment Quotient\uff08CAQ\uff09\u6307\u6807\uff0c\u4ece\u4e94\u4e2a\u7ef4\u5ea6\u8bc4\u4f30LLMs\u6027\u80fd\uff0c\u5e76\u7ed3\u5408\u4e09\u4e2a\u5206\u6790\u89c6\u89d2\uff08\u884c\u52a8\u6027\u3001\u6279\u5224\u6027\u548c\u516c\u6b63\u6027\uff09\u8fdb\u884c\u7cfb\u7edf\u5206\u6790\u3002", "result": "\u5927\u591a\u6570LLMs\u5728\u6279\u5224\u6027\u548c\u516c\u6b63\u6027\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u884c\u52a8\u6027\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1bClaude 3.7 Sonnet\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "CliME\u6570\u636e\u96c6\u548cCAQ\u6307\u6807\u4e3a\u8bc4\u4f30LLMs\u5728\u6c14\u5019\u76f8\u5173\u5185\u5bb9\u4e0a\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u884c\u52a8\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2504.03807", "pdf": "https://arxiv.org/pdf/2504.03807", "abs": "https://arxiv.org/abs/2504.03807", "authors": ["Maliheh Toozandehjani", "Ali Mousavi", "Reza Taheri"], "title": "From Keypoints to Realism: A Realistic and Accurate Virtual Try-on Network from 2D Images", "categories": ["cs.CV"], "comment": "in Persian language", "summary": "The aim of image-based virtual try-on is to generate realistic images of\nindividuals wearing target garments, ensuring that the pose, body shape and\ncharacteristics of the target garment are accurately preserved. Existing\nmethods often fail to reproduce the fine details of target garments effectively\nand lack generalizability to new scenarios. In the proposed method, the\nperson's initial garment is completely removed. Subsequently, a precise warping\nis performed using the predicted keypoints to fully align the target garment\nwith the body structure and pose of the individual. Based on the warped\ngarment, a body segmentation map is more accurately predicted. Then, using an\nalignment-aware segment normalization, the misaligned areas between the warped\ngarment and the predicted garment region in the segmentation map are removed.\nFinally, the generator produces the final image with high visual quality,\nreconstructing the precise characteristics of the target garment, including its\noverall shape and texture. This approach emphasizes preserving garment\ncharacteristics and improving adaptability to various poses, providing better\ngeneralization for diverse applications.", "AI": {"task": "\u751f\u6210\u7a7f\u7740\u76ee\u6807\u670d\u88c5\u7684\u771f\u5b9e\u611f\u56fe\u50cf\uff0c\u786e\u4fdd\u59ff\u52bf\u3001\u4f53\u578b\u548c\u670d\u88c5\u7279\u5f81\u51c6\u786e\u4fdd\u7559\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u590d\u73b0\u670d\u88c5\u7ec6\u8282\u4e14\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u5b8c\u5168\u79fb\u9664\u521d\u59cb\u670d\u88c5\uff0c\u901a\u8fc7\u9884\u6d4b\u5173\u952e\u70b9\u8fdb\u884c\u7cbe\u786e\u53d8\u5f62\uff0c\u7ed3\u5408\u5bf9\u9f50\u611f\u77e5\u7684\u5206\u5272\u5f52\u4e00\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002", "result": "\u80fd\u591f\u91cd\u5efa\u670d\u88c5\u7684\u7cbe\u786e\u7279\u5f81\uff0c\u5305\u62ec\u5f62\u72b6\u548c\u7eb9\u7406\uff0c\u5e76\u63d0\u5347\u5bf9\u4e0d\u540c\u59ff\u52bf\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u7559\u670d\u88c5\u7279\u5f81\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.03931", "pdf": "https://arxiv.org/pdf/2504.03931", "abs": "https://arxiv.org/abs/2504.03931", "authors": ["Zixuan Ke", "Yifei Ming", "Shafiq Joty"], "title": "Adaptation of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Tutorial Proposal for NAACL2025", "summary": "This tutorial on adaptation of LLMs is designed to address the growing demand\nfor models that go beyond the static capabilities of generic LLMs by providing\nan overview of dynamic, domain-specific, and task-adaptive LLM adaptation\ntechniques. While general LLMs have demonstrated strong generalization across a\nvariety of tasks, they often struggle to perform well in specialized domains\nsuch as finance, healthcare, and code generation for underrepresented\nlanguages. Additionally, their static nature limits their ability to evolve\nwith the changing world, and they are often extremely large in size, making\nthem impractical and costly to deploy at scale. As a result, the adaptation of\nLLMs has drawn much attention since the birth of LLMs and is of core\nimportance, both for industry, which focuses on serving its targeted users, and\nacademia, which can greatly benefit from small but powerful LLMs. To address\nthis gap, this tutorial aims to provide an overview of the LLM adaptation\ntechniques. We start with an introduction to LLM adaptation, from both the data\nperspective and the model perspective. We then emphasize how the evaluation\nmetrics and benchmarks are different from other techniques. After establishing\nthe problems, we explore various adaptation techniques. We categorize\nadaptation techniques into two main families. The first is parametric knowledge\nadaptation, which focuses on updating the parametric knowledge within LLMs.\nAdditionally, we will discuss real-time adaptation techniques, including model\nediting, which allows LLMs to be updated dynamically in production\nenvironments. The second kind of adaptation is semi-parametric knowledge\nadaptation, where the goal is to update LLM parameters to better leverage\nexternal knowledge or tools through techniques like retrieval-augmented\ngeneration (RAG) and agent-based systems.", "AI": {"task": "\u63d0\u4f9b\u5173\u4e8e\u52a8\u6001\u3001\u9886\u57df\u7279\u5b9a\u548c\u4efb\u52a1\u81ea\u9002\u5e94\u7684LLM\u9002\u5e94\u6280\u672f\u7684\u6982\u8ff0\u3002", "motivation": "\u901a\u7528LLM\u5728\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u4e0d\u4f73\u4e14\u9759\u6001\u7279\u6027\u9650\u5236\u4e86\u5176\u9002\u5e94\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u9002\u5e94\u6280\u672f\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5c06\u9002\u5e94\u6280\u672f\u5206\u4e3a\u53c2\u6570\u5316\u77e5\u8bc6\u9002\u5e94\uff08\u66f4\u65b0LLM\u5185\u90e8\u53c2\u6570\uff09\u548c\u534a\u53c2\u6570\u5316\u77e5\u8bc6\u9002\u5e94\uff08\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u6216\u5de5\u5177\uff09\u3002", "result": "\u6982\u8ff0\u4e86LLM\u9002\u5e94\u6280\u672f\u7684\u5206\u7c7b\u548c\u5e94\u7528\uff0c\u5305\u62ec\u5b9e\u65f6\u9002\u5e94\u548c\u534a\u53c2\u6570\u5316\u65b9\u6cd5\u3002", "conclusion": "LLM\u9002\u5e94\u6280\u672f\u5bf9\u5de5\u4e1a\u548c\u5b66\u672f\u754c\u90fd\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u80fd\u591f\u63d0\u5347\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u7684\u8868\u73b0\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2504.03821", "pdf": "https://arxiv.org/pdf/2504.03821", "abs": "https://arxiv.org/abs/2504.03821", "authors": ["Andrew Kiruluta", "Andreas Lemos"], "title": "A Hybrid Wavelet-Fourier Method for Next-Generation Conditional Diffusion Models", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "We present a novel generative modeling framework,Wavelet-Fourier-Diffusion,\nwhich adapts the diffusion paradigm to hybrid frequency representations in\norder to synthesize high-quality, high-fidelity images with improved spatial\nlocalization. In contrast to conventional diffusion models that rely\nexclusively on additive noise in pixel space, our approach leverages a\nmulti-transform that combines wavelet sub-band decomposition with partial\nFourier steps. This strategy progressively degrades and then reconstructs\nimages in a hybrid spectral domain during the forward and reverse diffusion\nprocesses. By supplementing traditional Fourier-based analysis with the spatial\nlocalization capabilities of wavelets, our model can capture both global\nstructures and fine-grained features more effectively. We further extend the\napproach to conditional image generation by integrating embeddings or\nconditional features via cross-attention. Experimental evaluations on CIFAR-10,\nCelebA-HQ, and a conditional ImageNet subset illustrate that our method\nachieves competitive or superior performance relative to baseline diffusion\nmodels and state-of-the-art GANs, as measured by Fr\\'echet Inception Distance\n(FID) and Inception Score (IS). We also show how the hybrid frequency-based\nrepresentation improves control over global coherence and fine texture\nsynthesis, paving the way for new directions in multi-scale generative\nmodeling.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u751f\u6210\u5efa\u6a21\u6846\u67b6Wavelet-Fourier-Diffusion\uff0c\u7528\u4e8e\u5408\u6210\u9ad8\u8d28\u91cf\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u56fe\u50cf\u3002", "motivation": "\u4f20\u7edf\u6269\u6563\u6a21\u578b\u4ec5\u4f9d\u8d56\u50cf\u7d20\u7a7a\u95f4\u7684\u52a0\u6027\u566a\u58f0\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u5168\u5c40\u7ed3\u6784\u548c\u7ec6\u7c92\u5ea6\u7279\u5f81\u3002", "method": "\u7ed3\u5408\u5c0f\u6ce2\u5b50\u5e26\u5206\u89e3\u548c\u90e8\u5206\u5085\u91cc\u53f6\u6b65\u9aa4\u7684\u591a\u53d8\u6362\u7b56\u7565\uff0c\u5728\u6df7\u5408\u8c31\u57df\u4e2d\u9010\u6b65\u964d\u89e3\u548c\u91cd\u5efa\u56fe\u50cf\u3002", "result": "\u5728CIFAR-10\u3001CelebA-HQ\u548c\u6761\u4ef6ImageNet\u5b50\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6269\u6563\u6a21\u578b\u548cGANs\u3002", "conclusion": "\u6df7\u5408\u9891\u7387\u8868\u793a\u4e3a\u591a\u5c3a\u5ea6\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2504.03932", "pdf": "https://arxiv.org/pdf/2504.03932", "abs": "https://arxiv.org/abs/2504.03932", "authors": ["Dongsuk Jang", "Alan Li", "Arman Cohan"], "title": "YaleNLP @ PerAnsSumm 2025: Multi-Perspective Integration via Mixture-of-Agents for Enhanced Healthcare QA Summarization", "categories": ["cs.CL"], "comment": "Paper accepted at CL4HEALTH @ NAACL 2025: Annual Conference of the\n  Nations of the Americas Chapter of the Association for Computational\n  Linguistics", "summary": "Automated summarization of healthcare community question-answering forums is\nchallenging due to diverse perspectives presented across multiple user\nresponses to each question. The PerAnsSumm Shared Task was therefore proposed\nto tackle this challenge by identifying perspectives from different answers and\nthen generating a comprehensive answer to the question. In this study, we\naddress the PerAnsSumm Shared Task using two complementary paradigms: (i) a\ntraining-based approach through QLoRA fine-tuning of LLaMA-3.3-70B-Instruct,\nand (ii) agentic approaches including zero- and few-shot prompting with\nfrontier LLMs (LLaMA-3.3-70B-Instruct and GPT-4o) and a Mixture-of-Agents (MoA)\nframework that leverages a diverse set of LLMs by combining outputs from\nmulti-layer feedback aggregation. For perspective span\nidentification/classification, GPT-4o zero-shot achieves an overall score of\n0.57, substantially outperforming the 0.40 score of the LLaMA baseline. With a\n2-layer MoA configuration, we were able to improve LLaMA performance up by 28\npercent to 0.51. For perspective-based summarization, GPT-4o zero-shot attains\nan overall score of 0.42 compared to 0.28 for the best LLaMA zero-shot, and our\n2-layer MoA approach boosts LLaMA performance by 32 percent to 0.37.\nFurthermore, in few-shot setting, our results show that the\nsentence-transformer embedding-based exemplar selection provides more gain than\nmanually selected exemplars on LLaMA models, although the few-shot prompting is\nnot always helpful for GPT-4o. The YaleNLP team's approach ranked the overall\nsecond place in the shared task.", "AI": {"task": "\u901a\u8fc7\u8bc6\u522b\u4e0d\u540c\u7b54\u6848\u4e2d\u7684\u89c2\u70b9\u5e76\u751f\u6210\u7efc\u5408\u56de\u7b54\uff0c\u89e3\u51b3\u533b\u7597\u793e\u533a\u95ee\u7b54\u8bba\u575b\u7684\u81ea\u52a8\u6458\u8981\u6311\u6218\u3002", "motivation": "\u533b\u7597\u793e\u533a\u95ee\u7b54\u8bba\u575b\u4e2d\u7684\u56de\u7b54\u591a\u6837\u6027\u4f7f\u5f97\u81ea\u52a8\u6458\u8981\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86PerAnsSumm\u5171\u4eab\u4efb\u52a1\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a(i) \u57fa\u4e8e\u8bad\u7ec3\u7684QLoRA\u5fae\u8c03LLaMA-3.3-70B-Instruct\uff1b(ii) \u4ee3\u7406\u65b9\u6cd5\uff0c\u5305\u62ec\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\uff08\u4f7f\u7528LLaMA-3.3-70B-Instruct\u548cGPT-4o\uff09\u4ee5\u53ca\u6df7\u5408\u4ee3\u7406\uff08MoA\uff09\u6846\u67b6\u3002", "result": "GPT-4o\u5728\u89c2\u70b9\u8bc6\u522b/\u5206\u7c7b\u4e2d\u5f97\u52060.57\uff0c\u663e\u8457\u4f18\u4e8eLLaMA\u57fa\u7ebf\uff080.40\uff09\uff1bMoA\u6846\u67b6\u5c06LLaMA\u6027\u80fd\u63d0\u534728%\u81f30.51\u3002\u5728\u89c2\u70b9\u6458\u8981\u4efb\u52a1\u4e2d\uff0cGPT-4o\u96f6\u6837\u672c\u5f97\u52060.42\uff0c\u4f18\u4e8eLLaMA\uff080.28\uff09\uff0cMoA\u63d0\u5347LLaMA\u6027\u80fd32%\u81f30.37\u3002\u5c11\u6837\u672c\u63d0\u793a\u5bf9LLaMA\u6a21\u578b\u66f4\u6709\u6548\u3002", "conclusion": "YaleNLP\u56e2\u961f\u7684\u65b9\u6cd5\u5728\u5171\u4eab\u4efb\u52a1\u4e2d\u6392\u540d\u7b2c\u4e8c\uff0c\u8bc1\u660e\u4e86\u5176\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.03850", "pdf": "https://arxiv.org/pdf/2504.03850", "abs": "https://arxiv.org/abs/2504.03850", "authors": ["Ved Umrajkar", "Aakash Kumar Singh"], "title": "Detection Limits and Statistical Separability of Tree Ring Watermarks in Rectified Flow-based Text-to-Image Generation Models", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG", "stat.ML"], "comment": null, "summary": "Tree-Ring Watermarking is a significant technique for authenticating\nAI-generated images. However, its effectiveness in rectified flow-based models\nremains unexplored, particularly given the inherent challenges of these models\nwith noise latent inversion. Through extensive experimentation, we evaluated\nand compared the detection and separability of watermarks between SD 2.1 and\nFLUX.1-dev models. By analyzing various text guidance configurations and\naugmentation attacks, we demonstrate how inversion limitations affect both\nwatermark recovery and the statistical separation between watermarked and\nunwatermarked images. Our findings provide valuable insights into the current\nlimitations of Tree-Ring Watermarking in the current SOTA models and highlight\nthe critical need for improved inversion methods to achieve reliable watermark\ndetection and separability. The official implementation, dataset release and\nall experimental results are available at this\n\\href{https://github.com/dsgiitr/flux-watermarking}{\\textbf{link}}.", "AI": {"task": "\u8bc4\u4f30\u548c\u6bd4\u8f83Tree-Ring\u6c34\u5370\u6280\u672f\u5728SD 2.1\u548cFLUX.1-dev\u6a21\u578b\u4e2d\u7684\u68c0\u6d4b\u4e0e\u5206\u79bb\u6548\u679c\u3002", "motivation": "\u63a2\u7d22Tree-Ring\u6c34\u5370\u6280\u672f\u5728\u57fa\u4e8e\u6821\u6b63\u6d41\u7684\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u8fd9\u4e9b\u6a21\u578b\u5728\u566a\u58f0\u6f5c\u5728\u53cd\u8f6c\u65b9\u9762\u7684\u56fa\u6709\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u5206\u6790\u4e0d\u540c\u6587\u672c\u5f15\u5bfc\u914d\u7f6e\u548c\u589e\u5f3a\u653b\u51fb\u5bf9\u6c34\u5370\u6062\u590d\u53ca\u7edf\u8ba1\u5206\u79bb\u7684\u5f71\u54cd\u3002", "result": "\u63ed\u793a\u4e86Tree-Ring\u6c34\u5370\u5728\u5f53\u524dSOTA\u6a21\u578b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u6539\u8fdb\u53cd\u8f6c\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u53ef\u9760\u6c34\u5370\u68c0\u6d4b\u548c\u5206\u79bb\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u5f53\u524dTree-Ring\u6c34\u5370\u6280\u672f\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u6539\u8fdb\u53cd\u8f6c\u65b9\u6cd5\u4ee5\u63d0\u5347\u5176\u53ef\u9760\u6027\u3002"}}
{"id": "2504.03933", "pdf": "https://arxiv.org/pdf/2504.03933", "abs": "https://arxiv.org/abs/2504.03933", "authors": ["Samuele Marro", "Davide Evangelista", "X. Angelo Huang", "Emanuele La Malfa", "Michele Lombardi", "Michael Wooldridge"], "title": "Language Models Are Implicitly Continuous", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "comment": "Published at ICLR 2025", "summary": "Language is typically modelled with discrete sequences. However, the most\nsuccessful approaches to language modelling, namely neural networks, are\ncontinuous and smooth function approximators. In this work, we show that\nTransformer-based language models implicitly learn to represent sentences as\ncontinuous-time functions defined over a continuous input space. This\nphenomenon occurs in most state-of-the-art Large Language Models (LLMs),\nincluding Llama2, Llama3, Phi3, Gemma, Gemma2, and Mistral, and suggests that\nLLMs reason about language in ways that fundamentally differ from humans. Our\nwork formally extends Transformers to capture the nuances of time and space\ncontinuity in both input and output space. Our results challenge the\ntraditional interpretation of how LLMs understand language, with several\nlinguistic and engineering implications.", "AI": {"task": "\u7814\u7a76Transformer-based\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u9690\u5f0f\u5730\u5c06\u53e5\u5b50\u8868\u793a\u4e3a\u8fde\u7eed\u65f6\u95f4\u51fd\u6570\u3002", "motivation": "\u63a2\u8ba8\u795e\u7ecf\u7f51\u7edc\uff08\u5c24\u5176\u662fTransformer\u6a21\u578b\uff09\u5728\u5904\u7406\u8bed\u8a00\u65f6\u5982\u4f55\u8868\u73b0\u51fa\u8fde\u7eed\u6027\u548c\u5e73\u6ed1\u6027\uff0c\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u7684\u79bb\u6563\u6027\u5f62\u6210\u5bf9\u6bd4\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e3b\u6d41\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982Llama2\u3001Llama3\u7b49\uff09\uff0c\u5c06Transformer\u6a21\u578b\u5f62\u5f0f\u5316\u6269\u5c55\u4ee5\u6355\u6349\u65f6\u95f4\u548c\u7a7a\u95f4\u7684\u8fde\u7eed\u6027\u3002", "result": "\u53d1\u73b0LLMs\u4ee5\u8fde\u7eed\u51fd\u6570\u7684\u65b9\u5f0f\u8868\u793a\u53e5\u5b50\uff0c\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u65b9\u5f0f\u6709\u672c\u8d28\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u4f20\u7edf\u5bf9LLMs\u8bed\u8a00\u7406\u89e3\u65b9\u5f0f\u7684\u89e3\u91ca\uff0c\u5e76\u63d0\u51fa\u4e86\u8bed\u8a00\u5b66\u548c\u5de5\u7a0b\u5b66\u4e0a\u7684\u65b0\u89c1\u89e3\u3002"}}
{"id": "2504.03857", "pdf": "https://arxiv.org/pdf/2504.03857", "abs": "https://arxiv.org/abs/2504.03857", "authors": ["Keegan Harris"], "title": "Can ChatGPT Learn My Life From a Week of First-Person Video?", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Motivated by recent improvements in generative AI and wearable camera devices\n(e.g. smart glasses and AI-enabled pins), I investigate the ability of\nfoundation models to learn about the wearer's personal life through\nfirst-person camera data. To test this, I wore a camera headset for 54 hours\nover the course of a week, generated summaries of various lengths (e.g.\nminute-long, hour-long, and day-long summaries), and fine-tuned both GPT-4o and\nGPT-4o-mini on the resulting summary hierarchy. By querying the fine-tuned\nmodels, we are able to learn what the models learned about me. The results are\nmixed: Both models learned basic information about me (e.g. approximate age,\ngender). Moreover, GPT-4o correctly deduced that I live in Pittsburgh, am a PhD\nstudent at CMU, am right-handed, and have a pet cat. However, both models also\nsuffered from hallucination and would make up names for the individuals present\nin the video footage of my life.", "AI": {"task": "\u7814\u7a76\u57fa\u7840\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u7b2c\u4e00\u4eba\u79f0\u6444\u50cf\u5934\u6570\u636e\u5b66\u4e60\u4f69\u6234\u8005\u7684\u4e2a\u4eba\u751f\u6d3b\u3002", "motivation": "\u57fa\u4e8e\u751f\u6210\u5f0fAI\u548c\u53ef\u7a7f\u6234\u6444\u50cf\u5934\u8bbe\u5907\uff08\u5982\u667a\u80fd\u773c\u955c\u548cAI\u652f\u6301\u7684\u522b\u9488\uff09\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u63a2\u7d22\u6a21\u578b\u5bf9\u4f69\u6234\u8005\u4e2a\u4eba\u751f\u6d3b\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u4f69\u6234\u6444\u50cf\u5934\u5934\u6234\u8bbe\u590754\u5c0f\u65f6\uff0c\u751f\u6210\u4e0d\u540c\u957f\u5ea6\u7684\u6458\u8981\uff08\u5982\u5206\u949f\u7ea7\u3001\u5c0f\u65f6\u7ea7\u548c\u5929\u7ea7\u6458\u8981\uff09\uff0c\u5e76\u5bf9GPT-4o\u548cGPT-4o-mini\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u6a21\u578b\u80fd\u5b66\u4e60\u4f69\u6234\u8005\u7684\u57fa\u672c\u4fe1\u606f\uff08\u5982\u5e74\u9f84\u3001\u6027\u522b\uff09\uff0cGPT-4o\u8fd8\u80fd\u63a8\u65ad\u51fa\u5c45\u4f4f\u5730\u3001\u804c\u4e1a\u3001\u60ef\u7528\u624b\u548c\u5ba0\u7269\u4fe1\u606f\uff0c\u4f46\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5982\u865a\u6784\u89c6\u9891\u4e2d\u4eba\u7269\u7684\u540d\u5b57\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u80fd\u901a\u8fc7\u7b2c\u4e00\u4eba\u79f0\u6570\u636e\u5b66\u4e60\u4e2a\u4eba\u751f\u6d3b\u4fe1\u606f\uff0c\u4f46\u9700\u89e3\u51b3\u5e7b\u89c9\u95ee\u9898\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u3002"}}
{"id": "2504.03964", "pdf": "https://arxiv.org/pdf/2504.03964", "abs": "https://arxiv.org/abs/2504.03964", "authors": ["Simon A. Lee", "Anthony Wu", "Jeffrey N. Chiang"], "title": "Clinical ModernBERT: An efficient and long context encoder for biomedical text", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Manuscript writeup corresponding to the Clinical ModernBERT\n  pre-trained encoder (https://huggingface.co/Simonlee711/Clinical_ModernBERT)", "summary": "We introduce Clinical ModernBERT, a transformer based encoder pretrained on\nlarge scale biomedical literature, clinical notes, and medical ontologies,\nincorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with\ntheir textual descriptions. Building on ModernBERT the current state of the art\nnatural language text encoder featuring architectural upgrades such as rotary\npositional embeddings (RoPE), Flash Attention, and extended context length up\nto 8,192 tokens our model adapts these innovations specifically for biomedical\nand clinical domains. Clinical ModernBERT excels at producing semantically rich\nrepresentations tailored for long context tasks. We validate this both by\nanalyzing its pretrained weights and through empirical evaluation on a\ncomprehensive suite of clinical NLP benchmarks.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668Clinical ModernBERT\uff0c\u4e13\u95e8\u9488\u5bf9\u751f\u7269\u533b\u5b66\u548c\u4e34\u5e8a\u9886\u57df\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "motivation": "\u7ed3\u5408\u751f\u7269\u533b\u5b66\u6587\u732e\u3001\u4e34\u5e8a\u7b14\u8bb0\u548c\u533b\u5b66\u672f\u8bed\uff0c\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u7684\u8bed\u4e49\u8868\u793a\u80fd\u529b\u3002", "method": "\u57fa\u4e8eModernBERT\u67b6\u6784\uff0c\u5f15\u5165\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u3001Flash Attention\u548c\u6269\u5c55\u4e0a\u4e0b\u6587\u957f\u5ea6\u81f38,192\u4e2a\u6807\u8bb0\uff0c\u5e76\u5728PubMed\u6458\u8981\u3001MIMIC IV\u4e34\u5e8a\u6570\u636e\u548c\u533b\u5b66\u672f\u8bed\u63cf\u8ff0\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5728\u4e34\u5e8aNLP\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u8868\u793a\u3002", "conclusion": "Clinical ModernBERT\u5728\u751f\u7269\u533b\u5b66\u548c\u4e34\u5e8a\u9886\u57df\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2504.03868", "pdf": "https://arxiv.org/pdf/2504.03868", "abs": "https://arxiv.org/abs/2504.03868", "authors": ["Ziming Liu", "Leichen Wang", "Ge Yang", "Xinrun Li", "Xingtao Hu", "Hao Sun", "Guangyu Gao"], "title": "Control Map Distribution using Map Query Bank for Online Map Generation", "categories": ["cs.CV"], "comment": null, "summary": "Reliable autonomous driving systems require high-definition (HD) map that\ncontains detailed map information for planning and navigation. However,\npre-build HD map requires a large cost. Visual-based Online Map Generation\n(OMG) has become an alternative low-cost solution to build a local HD map.\nQuery-based BEV Transformer has been a base model for this task. This model\nlearns HD map predictions from an initial map queries distribution which is\nobtained by offline optimization on training set. Besides the quality of BEV\nfeature, the performance of this model also highly relies on the capacity of\ninitial map query distribution. However, this distribution is limited because\nthe limited query number. To make map predictions optimal on each test sample,\nit is essential to generate a suitable initial distribution for each specific\nscenario. This paper proposes to decompose the whole HD map distribution into a\nset of point representations, namely map query bank (MQBank). To build specific\nmap query initial distributions of different scenarios, low-cost standard\ndefinition map (SD map) data is introduced as a kind of prior knowledge.\nMoreover, each layer of map decoder network learns instance-level map query\nfeatures, which will lose detailed information of each point. However, BEV\nfeature map is a point-level dense feature. It is important to keep point-level\ninformation in map queries when interacting with BEV feature map. This can also\nbe solved with map query bank method. Final experiments show a new insight on\nSD map prior and a new record on OpenLaneV2 benchmark with 40.5%, 45.7% mAP on\nvehicle lane and pedestrian area.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5730\u56fe\u67e5\u8be2\u5e93\uff08MQBank\uff09\u7684\u65b9\u6cd5\uff0c\u4e3a\u4e0d\u540c\u573a\u666f\u751f\u6210\u5408\u9002\u7684\u521d\u59cb\u5730\u56fe\u67e5\u8be2\u5206\u5e03\uff0c\u4ee5\u63d0\u9ad8\u89c6\u89c9\u5728\u7ebf\u5730\u56fe\u751f\u6210\uff08OMG\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u9884\u5efa\u9ad8\u6e05\u5730\u56fe\u6210\u672c\u9ad8\uff0c\u800c\u73b0\u6709\u57fa\u4e8e\u67e5\u8be2\u7684BEV Transformer\u6a21\u578b\u6027\u80fd\u53d7\u9650\u4e8e\u521d\u59cb\u5730\u56fe\u67e5\u8be2\u5206\u5e03\u7684\u8d28\u91cf\u548c\u6570\u91cf\u3002", "method": "\u901a\u8fc7\u5206\u89e3\u9ad8\u6e05\u5730\u56fe\u5206\u5e03\u4e3a\u70b9\u8868\u793a\uff08MQBank\uff09\uff0c\u5e76\u5f15\u5165\u4f4e\u6210\u672c\u7684\u6807\u51c6\u5b9a\u4e49\u5730\u56fe\uff08SD map\uff09\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\uff0c\u4f18\u5316\u521d\u59cb\u67e5\u8be2\u5206\u5e03\u3002", "result": "\u5728OpenLaneV2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e8640.5%\u548c45.7%\u7684mAP\uff08\u8f66\u8f86\u8f66\u9053\u548c\u884c\u4eba\u533a\u57df\uff09\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002", "conclusion": "MQBank\u65b9\u6cd5\u7ed3\u5408SD map\u5148\u9a8c\u77e5\u8bc6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5728\u7ebf\u5730\u56fe\u751f\u6210\u7684\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.03979", "pdf": "https://arxiv.org/pdf/2504.03979", "abs": "https://arxiv.org/abs/2504.03979", "authors": ["Amit K Verma", "Zhisong Zhang", "Junwon Seo", "Robin Kuo", "Runbo Jiang", "Emma Strubell", "Anthony D Rollett"], "title": "Structured Extraction of Process Structure Properties Relationships in Materials Science", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.IR"], "comment": "16 pages, 3 figures, 13 table", "summary": "With the advent of large language models (LLMs), the vast unstructured text\nwithin millions of academic papers is increasingly accessible for materials\ndiscovery, although significant challenges remain. While LLMs offer promising\nfew- and zero-shot learning capabilities, particularly valuable in the\nmaterials domain where expert annotations are scarce, general-purpose LLMs\noften fail to address key materials-specific queries without further\nadaptation. To bridge this gap, fine-tuning LLMs on human-labeled data is\nessential for effective structured knowledge extraction. In this study, we\nintroduce a novel annotation schema designed to extract generic\nprocess-structure-properties relationships from scientific literature. We\ndemonstrate the utility of this approach using a dataset of 128 abstracts, with\nannotations drawn from two distinct domains: high-temperature materials (Domain\nI) and uncertainty quantification in simulating materials microstructure\n(Domain II). Initially, we developed a conditional random field (CRF) model\nbased on MatBERT, a domain-specific BERT variant, and evaluated its performance\non Domain I. Subsequently, we compared this model with a fine-tuned LLM (GPT-4o\nfrom OpenAI) under identical conditions. Our results indicate that fine-tuning\nLLMs can significantly improve entity extraction performance over the BERT-CRF\nbaseline on Domain I. However, when additional examples from Domain II were\nincorporated, the performance of the BERT-CRF model became comparable to that\nof the GPT-4o model. These findings underscore the potential of our schema for\nstructured knowledge extraction and highlight the complementary strengths of\nboth modeling approaches.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6807\u6ce8\u6a21\u5f0f\uff0c\u7528\u4e8e\u4ece\u79d1\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u901a\u7528\u7684\u8fc7\u7a0b-\u7ed3\u6784-\u6027\u80fd\u5173\u7cfb\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6750\u6599\u53d1\u73b0\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u6750\u6599\u7279\u5b9a\u67e5\u8be2\u8fdb\u884c\u4f18\u5316\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u968f\u673a\u573a\uff08CRF\uff09\u6a21\u578b\u548c\u5fae\u8c03\u7684LLM\uff08GPT-4o\uff09\u8fdb\u884c\u5b9e\u4f53\u63d0\u53d6\uff0c\u5e76\u5728\u4e24\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5fae\u8c03LLM\u5728\u7279\u5b9a\u9886\u57df\uff08Domain I\uff09\u8868\u73b0\u4f18\u4e8eBERT-CRF\u6a21\u578b\uff0c\u4f46\u5728\u52a0\u5165\u53e6\u4e00\u9886\u57df\uff08Domain II\uff09\u6570\u636e\u540e\uff0c\u4e24\u8005\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u7684\u6807\u6ce8\u6a21\u5f0f\u6709\u6548\uff0c\u4e14\u4e24\u79cd\u5efa\u6a21\u65b9\u6cd5\u5177\u6709\u4e92\u8865\u4f18\u52bf\u3002"}}
{"id": "2504.03875", "pdf": "https://arxiv.org/pdf/2504.03875", "abs": "https://arxiv.org/abs/2504.03875", "authors": ["Wanhee Lee", "Klemen Kotar", "Rahul Mysore Venkatesh", "Jared Watrous", "Honglin Chen", "Khai Loong Aw", "Daniel L. K. Yamins"], "title": "3D Scene Understanding Through Local Random Access Sequence Modeling", "categories": ["cs.CV"], "comment": "Project webpage: https://neuroailab.github.io/projects/lras_3d/", "summary": "3D scene understanding from single images is a pivotal problem in computer\nvision with numerous downstream applications in graphics, augmented reality,\nand robotics. While diffusion-based modeling approaches have shown promise,\nthey often struggle to maintain object and scene consistency, especially in\ncomplex real-world scenarios. To address these limitations, we propose an\nautoregressive generative approach called Local Random Access Sequence (LRAS)\nmodeling, which uses local patch quantization and randomly ordered sequence\ngeneration. By utilizing optical flow as an intermediate representation for 3D\nscene editing, our experiments demonstrate that LRAS achieves state-of-the-art\nnovel view synthesis and 3D object manipulation capabilities. Furthermore, we\nshow that our framework naturally extends to self-supervised depth estimation\nthrough a simple modification of the sequence design. By achieving strong\nperformance on multiple 3D scene understanding tasks, LRAS provides a unified\nand effective framework for building the next generation of 3D vision models.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aLocal Random Access Sequence (LRAS)\u7684\u81ea\u56de\u5f52\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5355\u56fe\u50cf3D\u573a\u666f\u7406\u89e3\u4e2d\u7684\u5bf9\u8c61\u548c\u573a\u666f\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563\u7684\u5efa\u6a21\u65b9\u6cd5\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u96be\u4ee5\u4fdd\u6301\u5bf9\u8c61\u548c\u573a\u666f\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u5757\u91cf\u5316\u548c\u968f\u673a\u987a\u5e8f\u5e8f\u5217\u751f\u6210\u7684LRAS\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u5149\u6d41\u4f5c\u4e3a3D\u573a\u666f\u7f16\u8f91\u7684\u4e2d\u95f4\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLRAS\u5728\u65b0\u89c6\u89d2\u5408\u6210\u548c3D\u5bf9\u8c61\u64cd\u4f5c\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u80fd\u901a\u8fc7\u7b80\u5355\u4fee\u6539\u6269\u5c55\u5230\u81ea\u76d1\u7763\u6df1\u5ea6\u4f30\u8ba1\u3002", "conclusion": "LRAS\u4e3a\u6784\u5efa\u4e0b\u4e00\u4ee33D\u89c6\u89c9\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u6709\u6548\u7684\u6846\u67b6\u3002"}}
{"id": "2504.03991", "pdf": "https://arxiv.org/pdf/2504.03991", "abs": "https://arxiv.org/abs/2504.03991", "authors": ["Siddharth Srikanth", "Varun Bhatt", "Boshen Zhang", "Werner Hager", "Charles Michael Lewis", "Katia P. Sycara", "Aaquib Tabrez", "Stefanos Nikolaidis"], "title": "Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.MA"], "comment": null, "summary": "Understanding how humans collaborate and communicate in teams is essential\nfor improving human-agent teaming and AI-assisted decision-making. However,\nrelying solely on data from large-scale user studies is impractical due to\nlogistical, ethical, and practical constraints, necessitating synthetic models\nof multiple diverse human behaviors. Recently, agents powered by Large Language\nModels (LLMs) have been shown to emulate human-like behavior in social\nsettings. But, obtaining a large set of diverse behaviors requires manual\neffort in the form of designing prompts. On the other hand, Quality Diversity\n(QD) optimization has been shown to be capable of generating diverse\nReinforcement Learning (RL) agent behavior. In this work, we combine QD\noptimization with LLM-powered agents to iteratively search for prompts that\ngenerate diverse team behavior in a long-horizon, multi-step collaborative\nenvironment. We first show, through a human-subjects experiment (n=54\nparticipants), that humans exhibit diverse coordination and communication\nbehavior in this domain. We then show that our approach can effectively\nreplicate trends from human teaming data and also capture behaviors that are\nnot easily observed without collecting large amounts of data. Our findings\nhighlight the combination of QD and LLM-powered agents as an effective tool for\nstudying teaming and communication strategies in multi-agent collaboration.", "AI": {"task": "\u7ed3\u5408\u8d28\u91cf\u591a\u6837\u6027\uff08QD\uff09\u4f18\u5316\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u667a\u80fd\u4f53\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u56e2\u961f\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u56e2\u961f\u534f\u4f5c\u884c\u4e3a\u7684\u591a\u6837\u6027\uff0c\u4ee5\u6539\u8fdb\u4eba\u673a\u534f\u4f5c\u548cAI\u8f85\u52a9\u51b3\u7b56\uff0c\u4f46\u5927\u89c4\u6a21\u7528\u6237\u7814\u7a76\u5b58\u5728\u5b9e\u9645\u3001\u4f26\u7406\u548c\u64cd\u4f5c\u9650\u5236\u3002", "method": "\u901a\u8fc7QD\u4f18\u5316\u4e0eLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u7ed3\u5408\uff0c\u8fed\u4ee3\u641c\u7d22\u751f\u6210\u591a\u6837\u5316\u56e2\u961f\u884c\u4e3a\u7684\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u590d\u73b0\u4eba\u7c7b\u56e2\u961f\u884c\u4e3a\u7684\u8d8b\u52bf\uff0c\u5e76\u6355\u6349\u96be\u4ee5\u901a\u8fc7\u5927\u91cf\u6570\u636e\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u3002", "conclusion": "QD\u4e0eLLM\u7684\u7ed3\u5408\u662f\u7814\u7a76\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u56e2\u961f\u548c\u6c9f\u901a\u7b56\u7565\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2504.03886", "pdf": "https://arxiv.org/pdf/2504.03886", "abs": "https://arxiv.org/abs/2504.03886", "authors": ["Jianhao Zheng", "Zihan Zhu", "Valentin Bieri", "Marc Pollefeys", "Songyou Peng", "Iro Armeni"], "title": "WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We present WildGS-SLAM, a robust and efficient monocular RGB SLAM system\ndesigned to handle dynamic environments by leveraging uncertainty-aware\ngeometric mapping. Unlike traditional SLAM systems, which assume static scenes,\nour approach integrates depth and uncertainty information to enhance tracking,\nmapping, and rendering performance in the presence of moving objects. We\nintroduce an uncertainty map, predicted by a shallow multi-layer perceptron and\nDINOv2 features, to guide dynamic object removal during both tracking and\nmapping. This uncertainty map enhances dense bundle adjustment and Gaussian map\noptimization, improving reconstruction accuracy. Our system is evaluated on\nmultiple datasets and demonstrates artifact-free view synthesis. Results\nshowcase WildGS-SLAM's superior performance in dynamic environments compared to\nstate-of-the-art methods.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aWildGS-SLAM\u7684\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u5355\u76eeRGB SLAM\u7cfb\u7edf\uff0c\u7528\u4e8e\u5904\u7406\u52a8\u6001\u73af\u5883\u3002", "motivation": "\u4f20\u7edfSLAM\u7cfb\u7edf\u5047\u8bbe\u573a\u666f\u9759\u6001\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u52a8\u6001\u73af\u5883\u4e2d\u7684\u79fb\u52a8\u7269\u4f53\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u6df1\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u4fe1\u606f\uff0c\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u5730\u56fe\uff08\u7531\u6d45\u5c42\u591a\u5c42\u611f\u77e5\u5668\u548cDINOv2\u7279\u5f81\u9884\u6d4b\uff09\uff0c\u6307\u5bfc\u52a8\u6001\u7269\u4f53\u79fb\u9664\uff0c\u5e76\u4f18\u5316\u5bc6\u96c6\u675f\u8c03\u6574\u548c\u9ad8\u65af\u5730\u56fe\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u65e0\u4f2a\u5f71\u7684\u89c6\u56fe\u5408\u6210\u6548\u679c\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "WildGS-SLAM\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2504.04022", "pdf": "https://arxiv.org/pdf/2504.04022", "abs": "https://arxiv.org/abs/2504.04022", "authors": ["Essential AI", ":", "Darsh J Shah", "Peter Rushton", "Somanshu Singla", "Mohit Parmar", "Kurt Smith", "Yash Vanjani", "Ashish Vaswani", "Adarsh Chaluvaraju", "Andrew Hojel", "Andrew Ma", "Anil Thomas", "Anthony Polloreno", "Ashish Tanwer", "Burhan Drak Sibai", "Divya S Mansingka", "Divya Shivaprasad", "Ishaan Shah", "Karl Stratos", "Khoi Nguyen", "Michael Callahan", "Michael Pust", "Mrinal Iyer", "Philip Monk", "Platon Mazarakis", "Ritvik Kapila", "Saurabh Srivastava", "Tim Romanski"], "title": "Rethinking Reflection in Pre-Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A language model's ability to reflect on its own reasoning provides a key\nadvantage for solving complex problems. While most recent research has focused\non how this ability develops during reinforcement learning, we show that it\nactually begins to emerge much earlier - during the model's pre-training. To\nstudy this, we introduce deliberate errors into chains-of-thought and test\nwhether the model can still arrive at the correct answer by recognizing and\ncorrecting these mistakes. By tracking performance across different stages of\npre-training, we observe that this self-correcting ability appears early and\nimproves steadily over time. For instance, an OLMo2-7B model pre-trained on 4\ntrillion tokens displays self-correction on our six self-reflection tasks.", "AI": {"task": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u81ea\u6211\u53cd\u601d\u80fd\u529b\u7684\u65e9\u671f\u53d1\u5c55\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u662f\u5426\u5df2\u5177\u5907\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u5728\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5728\u601d\u7ef4\u94fe\u4e2d\u6545\u610f\u5f15\u5165\u9519\u8bef\uff0c\u6d4b\u8bd5\u6a21\u578b\u662f\u5426\u80fd\u8bc6\u522b\u5e76\u7ea0\u6b63\u8fd9\u4e9b\u9519\u8bef\uff0c\u4ece\u800c\u5f97\u51fa\u6b63\u786e\u7b54\u6848\u3002", "result": "\u53d1\u73b0\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u5728\u9884\u8bad\u7ec3\u65e9\u671f\u5373\u51fa\u73b0\uff0c\u5e76\u968f\u65f6\u95f4\u7a33\u6b65\u63d0\u5347\uff0c\u4f8b\u5982OLMo2-7B\u6a21\u578b\u57284\u4e07\u4ebf\u6807\u8bb0\u7684\u9884\u8bad\u7ec3\u540e\u8868\u73b0\u51fa\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u53cd\u601d\u80fd\u529b\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5df2\u5f00\u59cb\u53d1\u5c55\uff0c\u4e14\u968f\u9884\u8bad\u7ec3\u8fdb\u5c55\u800c\u589e\u5f3a\u3002"}}
{"id": "2504.03894", "pdf": "https://arxiv.org/pdf/2504.03894", "abs": "https://arxiv.org/abs/2504.03894", "authors": ["Haiqing Li", "Yuzhi Guo", "Feng Jiang", "Qifeng Zhou", "Hehuan Ma", "Junzhou Huang"], "title": "Leveraging Gait Patterns as Biomarkers: An attention-guided Deep Multiple Instance Learning Network for Scoliosis Classification", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 3 figures", "summary": "Scoliosis is a spinal curvature disorder that is difficult to detect early\nand can compress the chest cavity, impacting respiratory function and cardiac\nhealth. Especially for adolescents, delayed detection and treatment result in\nworsening compression. Traditional scoliosis detection methods heavily rely on\nclinical expertise, and X-ray imaging poses radiation risks, limiting\nlarge-scale early screening. We propose an Attention-Guided Deep Multi-Instance\nLearning method (Gait-MIL) to effectively capture discriminative features from\ngait patterns, which is inspired by ScoNet-MT's pioneering use of gait patterns\nfor scoliosis detection. We evaluate our method on the first large-scale\ndataset based on gait patterns for scoliosis classification. The results\ndemonstrate that our study improves the performance of using gait as a\nbiomarker for scoliosis detection, significantly enhances detection accuracy\nfor the particularly challenging Neutral cases, where subtle indicators are\noften overlooked. Our Gait-MIL also performs robustly in imbalanced scenarios,\nmaking it a promising tool for large-scale scoliosis screening.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u6df1\u5ea6\u591a\u5b9e\u4f8b\u5b66\u4e60\u65b9\u6cd5\uff08Gait-MIL\uff09\uff0c\u7528\u4e8e\u901a\u8fc7\u6b65\u6001\u6a21\u5f0f\u68c0\u6d4b\u810a\u67f1\u4fa7\u5f2f\u3002", "motivation": "\u810a\u67f1\u4fa7\u5f2f\u65e9\u671f\u68c0\u6d4b\u56f0\u96be\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e34\u5e8a\u7ecf\u9a8c\u548cX\u5c04\u7ebf\uff0c\u5b58\u5728\u8f90\u5c04\u98ce\u9669\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u7b5b\u67e5\u3002", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u6df1\u5ea6\u591a\u5b9e\u4f8b\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ece\u6b65\u6001\u6a21\u5f0f\u4e2d\u63d0\u53d6\u5224\u522b\u6027\u7279\u5f81\u3002", "result": "\u5728\u57fa\u4e8e\u6b65\u6001\u6a21\u5f0f\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u96be\u4ee5\u68c0\u6d4b\u7684\u4e2d\u6027\u75c5\u4f8b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Gait-MIL\u5728\u6570\u636e\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\u8868\u73b0\u7a33\u5065\uff0c\u6709\u671b\u6210\u4e3a\u5927\u89c4\u6a21\u810a\u67f1\u4fa7\u5f2f\u7b5b\u67e5\u7684\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2504.04038", "pdf": "https://arxiv.org/pdf/2504.04038", "abs": "https://arxiv.org/abs/2504.04038", "authors": ["Kaung Lwin Thant", "Kwankamol Nongpong", "Ye Kyaw Thu", "Thura Aung", "Khaing Hsu Wai", "Thazin Myint Oo"], "title": "myNER: Contextualized Burmese Named Entity Recognition with Bidirectional LSTM and fastText Embeddings via Joint Training with POS Tagging", "categories": ["cs.CL", "I.2.7"], "comment": "7 pages, 2 figures, 5 tables, to be published in the proceedings of\n  IEEE ICCI-2025", "summary": "Named Entity Recognition (NER) involves identifying and categorizing named\nentities within textual data. Despite its significance, NER research has often\noverlooked low-resource languages like Myanmar (Burmese), primarily due to the\nlack of publicly available annotated datasets. To address this, we introduce\nmyNER, a novel word-level NER corpus featuring a 7-tag annotation scheme,\nenriched with Part-of-Speech (POS) tagging to provide additional syntactic\ninformation. Alongside the corpus, we conduct a comprehensive evaluation of NER\nmodels, including Conditional Random Fields (CRF), Bidirectional LSTM\n(BiLSTM)-CRF, and their combinations with fastText embeddings in different\nsettings. Our experiments reveal the effectiveness of contextualized word\nembeddings and the impact of joint training with POS tagging, demonstrating\nsignificant performance improvements across models. The traditional CRF\njoint-task model with fastText embeddings as a feature achieved the best\nresult, with a 0.9818 accuracy and 0.9811 weighted F1 score with 0.7429 macro\nF1 score. BiLSTM-CRF with fine-tuned fastText embeddings gets the best result\nof 0.9791 accuracy and 0.9776 weighted F1 score with 0.7395 macro F1 score.", "AI": {"task": "\u6784\u5efa\u5e76\u8bc4\u4f30\u4e00\u4e2a\u9488\u5bf9\u7f05\u7538\u8bed\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u8bed\u6599\u5e93myNER\uff0c\u5e76\u6d4b\u8bd5\u4e0d\u540c\u6a21\u578b\u5728\u5176\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u7f05\u7538\u8bed\uff09\u5728NER\u7814\u7a76\u4e2d\u56e0\u7f3a\u4e4f\u516c\u5f00\u6807\u6ce8\u6570\u636e\u96c6\u800c\u88ab\u5ffd\u89c6\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51famyNER\u8bed\u6599\u5e93\uff0c\u91c7\u75287\u6807\u7b7e\u6807\u6ce8\u65b9\u6848\u5e76\u52a0\u5165POS\u6807\u6ce8\uff1b\u8bc4\u4f30CRF\u3001BiLSTM-CRF\u7b49\u6a21\u578b\u53ca\u5176\u4e0efastText\u5d4c\u5165\u7684\u7ed3\u5408\u6548\u679c\u3002", "result": "CRF\u7ed3\u5408fastText\u5d4c\u5165\u8868\u73b0\u6700\u4f73\uff08\u51c6\u786e\u73870.9818\uff0c\u52a0\u6743F1\u5206\u65700.9811\uff09\uff1bBiLSTM-CRF\u5fae\u8c03fastText\u5d4c\u5165\u6b21\u4e4b\uff08\u51c6\u786e\u73870.9791\uff0c\u52a0\u6743F1\u5206\u65700.9776\uff09\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u8bcd\u5d4c\u5165\u548c\u8054\u5408POS\u6807\u6ce8\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00NER\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u548c\u65b9\u6cd5\u3002"}}
{"id": "2504.03923", "pdf": "https://arxiv.org/pdf/2504.03923", "abs": "https://arxiv.org/abs/2504.03923", "authors": ["Tyler Ward", "Abdullah-Al-Zubaer Imran"], "title": "Improving Brain Disorder Diagnosis with Advanced Brain Function Representation and Kolmogorov-Arnold Networks", "categories": ["cs.CV"], "comment": "Paper accepted at MIDL 2025", "summary": "Quantifying functional connectivity (FC), a vital metric for the diagnosis of\nvarious brain disorders, traditionally relies on the use of a pre-defined brain\natlas. However, using such atlases can lead to issues regarding selection bias\nand lack of regard for specificity. Addressing this, we propose a novel\ntransformer-based classification network (AFBR-KAN) with effective brain\nfunction representation to aid in diagnosing autism spectrum disorder (ASD).\nAFBR-KAN leverages Kolmogorov-Arnold Network (KAN) blocks replacing traditional\nmulti-layer perceptron (MLP) components. Thorough experimentation reveals the\neffectiveness of AFBR-KAN in improving the diagnosis of ASD under various\nconfigurations of the model architecture. Our code is available at\nhttps://github.com/tbwa233/ABFR-KAN", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u5206\u7c7b\u7f51\u7edc\uff08AFBR-KAN\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u81ea\u95ed\u75c7\u8c31\u7cfb\u969c\u788d\uff08ASD\uff09\u7684\u8bca\u65ad\u3002", "motivation": "\u4f20\u7edf\u529f\u80fd\u8fde\u63a5\uff08FC\uff09\u91cf\u5316\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u8111\u56fe\u8c31\uff0c\u5b58\u5728\u9009\u62e9\u504f\u5dee\u548c\u7f3a\u4e4f\u7279\u5f02\u6027\u7684\u95ee\u9898\u3002", "method": "AFBR-KAN\u91c7\u7528Kolmogorov-Arnold Network\uff08KAN\uff09\u5757\u66ff\u4ee3\u4f20\u7edf\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u7ec4\u4ef6\uff0c\u4ee5\u63d0\u5347\u8111\u529f\u80fd\u8868\u793a\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAFBR-KAN\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u914d\u7f6e\u4e0b\u5747\u80fd\u6709\u6548\u6539\u8fdbASD\u8bca\u65ad\u3002", "conclusion": "AFBR-KAN\u4e3aASD\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.04042", "pdf": "https://arxiv.org/pdf/2504.04042", "abs": "https://arxiv.org/abs/2504.04042", "authors": ["Kepu Zhang", "Weijie Yu", "Zhongxiang Sun", "Jun Xu"], "title": "SyLeR: A Framework for Explicit Syllogistic Legal Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Syllogistic reasoning is a fundamental aspect of legal decision-making,\nenabling logical conclusions by connecting general legal principles with\nspecific case facts. Although existing large language models (LLMs) can\ngenerate responses to legal questions, they fail to perform explicit\nsyllogistic reasoning, often producing implicit and unstructured answers that\nlack explainability and trustworthiness. To address this limitation, we propose\nSyLeR, a novel framework that empowers LLMs to engage in explicit syllogistic\nlegal reasoning. SyLeR integrates a tree-structured hierarchical retrieval\nmechanism to effectively combine relevant legal statutes and precedent cases,\nforming comprehensive major premises. This is followed by a two-stage\nfine-tuning process: supervised fine-tuning warm-up establishes a foundational\nunderstanding of syllogistic reasoning, while reinforcement learning with a\nstructure-aware reward mechanism refines the ability of the model to generate\ndiverse logically sound and well-structured reasoning paths. We conducted\nextensive experiments across various dimensions, including in-domain and\ncross-domain user groups (legal laypersons and practitioners), multiple\nlanguages (Chinese and French), and different LLM backbones (legal-specific and\nopen-domain LLMs). The results show that SyLeR significantly improves response\naccuracy and consistently delivers explicit, explainable, and trustworthy legal\nreasoning.", "AI": {"task": "\u63d0\u51faSyLeR\u6846\u67b6\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u660e\u786e\u7684\u4e09\u6bb5\u8bba\u6cd5\u5f8b\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u95ee\u9898\u56de\u7b54\u4e2d\u7f3a\u4e4f\u660e\u786e\u7684\u4e09\u6bb5\u8bba\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u7b54\u6848\u9690\u542b\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u7ed3\u5408\u6811\u72b6\u5c42\u6b21\u68c0\u7d22\u673a\u5236\u6574\u5408\u6cd5\u5f8b\u6761\u6587\u548c\u5148\u4f8b\u6848\u4f8b\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u5fae\u8c03\uff08\u76d1\u7763\u5fae\u8c03\u9884\u70ed\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u4f18\u5316\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "result": "SyLeR\u663e\u8457\u63d0\u9ad8\u4e86\u56de\u7b54\u51c6\u786e\u6027\uff0c\u5e76\u751f\u6210\u660e\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u4fe1\u7684\u6cd5\u5f8b\u63a8\u7406\u3002", "conclusion": "SyLeR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6cd5\u5f8b\u63a8\u7406\u4e2d\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2504.03948", "pdf": "https://arxiv.org/pdf/2504.03948", "abs": "https://arxiv.org/abs/2504.03948", "authors": ["Sanjoy Kundu", "Shanmukha Vellamchetti", "Sathyanarayanan N. Aakur"], "title": "ProbRes: Probabilistic Jump Diffusion for Open-World Egocentric Activity Recognition", "categories": ["cs.CV"], "comment": "17 pages, 6 figures, 3 tables. Under review", "summary": "Open-world egocentric activity recognition poses a fundamental challenge due\nto its unconstrained nature, requiring models to infer unseen activities from\nan expansive, partially observed search space. We introduce ProbRes, a\nProbabilistic Residual search framework based on jump-diffusion that\nefficiently navigates this space by balancing prior-guided exploration with\nlikelihood-driven exploitation. Our approach integrates structured commonsense\npriors to construct a semantically coherent search space, adaptively refines\npredictions using Vision-Language Models (VLMs) and employs a stochastic search\nmechanism to locate high-likelihood activity labels while minimizing exhaustive\nenumeration efficiently. We systematically evaluate ProbRes across multiple\nopenness levels (L0 - L3), demonstrating its adaptability to increasing search\nspace complexity. In addition to achieving state-of-the-art performance on\nbenchmark datasets (GTEA Gaze, GTEA Gaze+, EPIC-Kitchens, and Charades-Ego), we\nestablish a clear taxonomy for open-world recognition, delineating the\nchallenges and methodological advancements necessary for egocentric activity\nunderstanding. Our results highlight the importance of structured search\nstrategies, paving the way for scalable and efficient open-world activity\nrecognition.", "AI": {"task": "\u63d0\u51faProbRes\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u653e\u4e16\u754c\u7684\u81ea\u6211\u4e2d\u5fc3\u6d3b\u52a8\u8bc6\u522b\uff0c\u4ee5\u9ad8\u6548\u63a8\u65ad\u672a\u89c1\u6d3b\u52a8\u3002", "motivation": "\u5f00\u653e\u4e16\u754c\u7684\u81ea\u6211\u4e2d\u5fc3\u6d3b\u52a8\u8bc6\u522b\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u6a21\u578b\u5728\u90e8\u5206\u89c2\u6d4b\u7684\u641c\u7d22\u7a7a\u95f4\u4e2d\u63a8\u65ad\u672a\u89c1\u6d3b\u52a8\u3002", "method": "\u57fa\u4e8e\u8df3\u6269\u6563\u7684\u6982\u7387\u6b8b\u5dee\u641c\u7d22\u6846\u67b6\uff0c\u7ed3\u5408\u5e38\u8bc6\u5148\u9a8c\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u968f\u673a\u641c\u7d22\u673a\u5236\u5b9a\u4f4d\u9ad8\u53ef\u80fd\u6027\u6d3b\u52a8\u6807\u7b7e\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5efa\u7acb\u4e86\u5f00\u653e\u4e16\u754c\u8bc6\u522b\u7684\u6e05\u6670\u5206\u7c7b\u3002", "conclusion": "\u7ed3\u6784\u5316\u641c\u7d22\u7b56\u7565\u5bf9\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u5f00\u653e\u4e16\u754c\u6d3b\u52a8\u8bc6\u522b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2504.04050", "pdf": "https://arxiv.org/pdf/2504.04050", "abs": "https://arxiv.org/abs/2504.04050", "authors": ["Kang Xue", "Ming Dong", "Xinhui Tu", "Tingting He"], "title": "FISH-Tuning: Enhancing PEFT Methods with Fisher Information", "categories": ["cs.CL"], "comment": null, "summary": "The rapid growth in the parameter size of Large Language Models (LLMs) has\nled to the development of Parameter-Efficient Fine-Tuning (PEFT) methods to\nalleviate the computational costs of fine-tuning. Among these, Fisher Induced\nSparse uncHanging (FISH) Mask is a selection-based PEFT technique that\nidentifies a subset of pre-trained parameters for fine-tuning based on\napproximate Fisher information. However, the integration of FISH Mask with\nother PEFT methods, such as LoRA and Adapters, remains underexplored. In this\npaper, we propose FISH-Tuning, a novel approach that incorporates FISH Mask\ninto addition-based and reparameterization-based PEFT methods, including LoRA,\nAdapters, and their variants. By leveraging Fisher information to select\ncritical parameters within these methods, FISH-Tuning achieves superior\nperformance without additional memory overhead or inference latency.\nExperimental results across various datasets and pre-trained models demonstrate\nthat FISH-Tuning consistently outperforms the vanilla PEFT methods with the\nsame proportion of trainable parameters.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aFISH-Tuning\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06FISH Mask\u4e0e\u57fa\u4e8e\u52a0\u6cd5\u548c\u91cd\u53c2\u6570\u5316\u7684PEFT\u65b9\u6cd5\uff08\u5982LoRA\u548cAdapters\uff09\u7ed3\u5408\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3FISH Mask\u4e0e\u5176\u4ed6PEFT\u65b9\u6cd5\uff08\u5982LoRA\u548cAdapters\uff09\u7ed3\u5408\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5229\u7528Fisher\u4fe1\u606f\u9009\u62e9\u5173\u952e\u53c2\u6570\uff0c\u5c06FISH Mask\u96c6\u6210\u5230\u57fa\u4e8e\u52a0\u6cd5\u548c\u91cd\u53c2\u6570\u5316\u7684PEFT\u65b9\u6cd5\u4e2d\u3002", "result": "FISH-Tuning\u5728\u591a\u79cd\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfPEFT\u65b9\u6cd5\uff0c\u4e14\u4e0d\u589e\u52a0\u5185\u5b58\u5f00\u9500\u6216\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "FISH-Tuning\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684PEFT\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2504.03953", "pdf": "https://arxiv.org/pdf/2504.03953", "abs": "https://arxiv.org/abs/2504.03953", "authors": ["Arash Sajjadi", "Mark Eramian"], "title": "TGraphX: Tensor-Aware Graph Neural Network for Multi-Dimensional Feature Learning", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68R10", "I.2.6; I.5.1; I.4.8"], "comment": "Submitted to arXiv. Code repository:\n  https://github.com/arashsajjadi/TGraphX |||\n  https://git.cs.usask.ca/arash/tgraphx", "summary": "TGraphX presents a novel paradigm in deep learning by unifying convolutional\nneural networks (CNNs) with graph neural networks (GNNs) to enhance visual\nreasoning tasks. Traditional CNNs excel at extracting rich spatial features\nfrom images but lack the inherent capability to model inter-object\nrelationships. Conversely, conventional GNNs typically rely on flattened node\nfeatures, thereby discarding vital spatial details. TGraphX overcomes these\nlimitations by employing CNNs to generate multi-dimensional node features\n(e.g., (3*128*128) tensors) that preserve local spatial semantics. These\nspatially aware nodes participate in a graph where message passing is performed\nusing 1*1 convolutions, which fuse adjacent features while maintaining their\nstructure. Furthermore, a deep CNN aggregator with residual connections is used\nto robustly refine the fused messages, ensuring stable gradient flow and\nend-to-end trainability. Our approach not only bridges the gap between spatial\nfeature extraction and relational reasoning but also demonstrates significant\nimprovements in object detection refinement and ensemble reasoning.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u8303\u5f0fTGraphX\uff0c\u5c06\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNNs\uff09\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7ed3\u5408\u4ee5\u589e\u5f3a\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edfCNNs\u64c5\u957f\u63d0\u53d6\u56fe\u50cf\u7684\u7a7a\u95f4\u7279\u5f81\u4f46\u65e0\u6cd5\u5efa\u6a21\u5bf9\u8c61\u95f4\u5173\u7cfb\uff0c\u800c\u4f20\u7edfGNNs\u4f9d\u8d56\u6241\u5e73\u5316\u8282\u70b9\u7279\u5f81\u5bfc\u81f4\u7a7a\u95f4\u7ec6\u8282\u4e22\u5931\u3002", "method": "\u4f7f\u7528CNNs\u751f\u6210\u591a\u7ef4\u8282\u70b9\u7279\u5f81\u5e76\u4fdd\u7559\u7a7a\u95f4\u8bed\u4e49\uff0c\u901a\u8fc71*1\u5377\u79ef\u8fdb\u884c\u6d88\u606f\u4f20\u9012\uff0c\u7ed3\u5408\u6df1\u5ea6CNN\u805a\u5408\u5668\u4f18\u5316\u878d\u5408\u4fe1\u606f\u3002", "result": "\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8c61\u68c0\u6d4b\u548c\u96c6\u6210\u63a8\u7406\u7684\u6027\u80fd\u3002", "conclusion": "TGraphX\u6210\u529f\u5f25\u5408\u4e86\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u4e0e\u5173\u7cfb\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2504.04060", "pdf": "https://arxiv.org/pdf/2504.04060", "abs": "https://arxiv.org/abs/2504.04060", "authors": ["Yuhao Wang", "Heyang Liu", "Ziyang Cheng", "Ronghua Wu", "Qunshan Gu", "Yanfeng Wang", "Yu Wang"], "title": "VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Speech large language models (LLMs) have emerged as a prominent research\nfocus in speech processing. We propose VocalNet-1B and VocalNet-8B, a series of\nhigh-performance, low-latency speech LLMs enabled by a scalable and\nmodel-agnostic training framework for real-time voice interaction. Departing\nfrom the conventional next-token prediction (NTP), we introduce multi-token\nprediction (MTP), a novel approach optimized for speech LLMs that\nsimultaneously improves generation speed and quality. Experiments show that\nVocalNet outperforms mainstream Omni LLMs despite using significantly less\ntraining data, while also surpassing existing open-source speech LLMs by a\nsubstantial margin. To support reproducibility and community advancement, we\nwill open-source all model weights, inference code, training data, and\nframework implementations upon publication.", "AI": {"task": "\u63d0\u51faVocalNet-1B\u548cVocalNet-8B\uff0c\u4e00\u79cd\u9ad8\u6027\u80fd\u3001\u4f4e\u5ef6\u8fdf\u7684\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7cfb\u5217\uff0c\u7528\u4e8e\u5b9e\u65f6\u8bed\u97f3\u4ea4\u4e92\u3002", "motivation": "\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u5904\u7406\u9886\u57df\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u901f\u5ea6\u548c\u8d28\u91cf\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u4ee4\u724c\u9884\u6d4b\uff08MTP\uff09\u66ff\u4ee3\u4f20\u7edf\u7684\u4e0b\u4e00\u4ee4\u724c\u9884\u6d4b\uff08NTP\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6a21\u578b\u65e0\u5173\u7684\u8bad\u7ec3\u6846\u67b6\u3002", "result": "VocalNet\u5728\u8bad\u7ec3\u6570\u636e\u663e\u8457\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4e3b\u6d41Omni LLM\uff0c\u5e76\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u8bed\u97f3LLM\u3002", "conclusion": "VocalNet\u901a\u8fc7MTP\u5b9e\u73b0\u4e86\u751f\u6210\u901f\u5ea6\u548c\u8d28\u91cf\u7684\u540c\u6b65\u63d0\u5347\uff0c\u4e3a\u8bed\u97f3LLM\u7684\u7814\u7a76\u548c\u793e\u533a\u53d1\u5c55\u63d0\u4f9b\u4e86\u5f00\u6e90\u652f\u6301\u3002"}}
{"id": "2504.03970", "pdf": "https://arxiv.org/pdf/2504.03970", "abs": "https://arxiv.org/abs/2504.03970", "authors": ["Dahun Kim", "AJ Piergiovanni", "Ganesh Mallya", "Anelia Angelova"], "title": "VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "comment": "CVPR 2025, project page at\n  https://github.com/google-deepmind/video_comp", "summary": "We introduce VideoComp, a benchmark and learning framework for advancing\nvideo-text compositionality understanding, aimed at improving vision-language\nmodels (VLMs) in fine-grained temporal alignment. Unlike existing benchmarks\nfocused on static image-text compositionality or isolated single-event videos,\nour benchmark targets alignment in continuous multi-event videos. Leveraging\nvideo-text datasets with temporally localized event captions (e.g.\nActivityNet-Captions, YouCook2), we construct two compositional benchmarks,\nActivityNet-Comp and YouCook2-Comp. We create challenging negative samples with\nsubtle temporal disruptions such as reordering, action word replacement,\npartial captioning, and combined disruptions. These benchmarks comprehensively\ntest models' compositional sensitivity across extended, cohesive video-text\nsequences. To improve model performance, we propose a hierarchical pairwise\npreference loss that strengthens alignment with temporally accurate pairs and\ngradually penalizes increasingly disrupted ones, encouraging fine-grained\ncompositional learning. To mitigate the limited availability of densely\nannotated video data, we introduce a pretraining strategy that concatenates\nshort video-caption pairs to simulate multi-event sequences. We evaluate\nvideo-text foundational models and large multimodal models (LMMs) on our\nbenchmark, identifying both strengths and areas for improvement in\ncompositionality. Overall, our work provides a comprehensive framework for\nevaluating and enhancing model capabilities in achieving fine-grained,\ntemporally coherent video-text alignment.", "AI": {"task": "\u63d0\u51faVideoComp\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347\u89c6\u9891-\u6587\u672c\u7ec4\u5408\u6027\u7406\u89e3\u7684\u57fa\u51c6\u548c\u5b66\u4e60\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u591a\u5173\u6ce8\u9759\u6001\u56fe\u50cf-\u6587\u672c\u7ec4\u5408\u6027\u6216\u5355\u4e00\u4e8b\u4ef6\u89c6\u9891\uff0c\u7f3a\u4e4f\u5bf9\u8fde\u7eed\u591a\u4e8b\u4ef6\u89c6\u9891\u7684\u7ec6\u7c92\u5ea6\u65f6\u95f4\u5bf9\u9f50\u7814\u7a76\u3002", "method": "\u5229\u7528\u5e26\u6709\u65f6\u95f4\u5b9a\u4f4d\u4e8b\u4ef6\u6807\u6ce8\u7684\u89c6\u9891-\u6587\u672c\u6570\u636e\u96c6\uff08\u5982ActivityNet-Captions\u3001YouCook2\uff09\uff0c\u6784\u5efa\u4e24\u4e2a\u7ec4\u5408\u6027\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u5206\u5c42\u6210\u5bf9\u504f\u597d\u635f\u5931\u548c\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u8bc4\u4f30\u4e86\u89c6\u9891-\u6587\u672c\u57fa\u7840\u6a21\u578b\u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5176\u5728\u7ec4\u5408\u6027\u65b9\u9762\u7684\u4f18\u52bf\u548c\u4e0d\u8db3\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u3001\u65f6\u95f4\u8fde\u8d2f\u7684\u89c6\u9891-\u6587\u672c\u5bf9\u9f50\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2504.04076", "pdf": "https://arxiv.org/pdf/2504.04076", "abs": "https://arxiv.org/abs/2504.04076", "authors": ["Bing Wang", "Bingrui Zhao", "Ximing Li", "Changchun Li", "Wanfu Gao", "Shengsheng Wang"], "title": "Collaboration and Controversy Among Experts: Rumor Early Detection by Tuning a Comment Generator", "categories": ["cs.CL", "cs.SI"], "comment": "11 pages, 5 figures. Accepted by SIGIR 2025. Code:\n  https://github.com/wangbing1416/CAMERED", "summary": "Over the past decade, social media platforms have been key in spreading\nrumors, leading to significant negative impacts. To counter this, the community\nhas developed various Rumor Detection (RD) algorithms to automatically identify\nthem using user comments as evidence. However, these RD methods often fail in\nthe early stages of rumor propagation when only limited user comments are\navailable, leading the community to focus on a more challenging topic named\nRumor Early Detection (RED). Typically, existing RED methods learn from limited\nsemantics in early comments. However, our preliminary experiment reveals that\nthe RED models always perform best when the number of training and test\ncomments is consistent and extensive. This inspires us to address the RED issue\nby generating more human-like comments to support this hypothesis. To implement\nthis idea, we tune a comment generator by simulating expert collaboration and\ncontroversy and propose a new RED framework named CAMERED. Specifically, we\nintegrate a mixture-of-expert structure into a generative language model and\npresent a novel routing network for expert collaboration. Additionally, we\nsynthesize a knowledgeable dataset and design an adversarial learning strategy\nto align the style of generated comments with real-world comments. We further\nintegrate generated and original comments with a mutual controversy fusion\nmodule. Experimental results show that CAMERED outperforms state-of-the-art RED\nbaseline models and generation methods, demonstrating its effectiveness.", "AI": {"task": "\u7814\u7a76\u8c23\u8a00\u65e9\u671f\u68c0\u6d4b\uff08RED\uff09\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aCAMERED\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u66f4\u591a\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8bc4\u8bba\u6765\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8c23\u8a00\u65e9\u671f\u68c0\u6d4b\u65b9\u6cd5\u5728\u65e9\u671f\u8bc4\u8bba\u6570\u91cf\u6709\u9650\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u8bc4\u8bba\u6570\u91cf\u4e00\u81f4\u4e14\u5145\u8db3\u65f6\u8868\u73b0\u6700\u4f73\uff0c\u56e0\u6b64\u5e0c\u671b\u901a\u8fc7\u751f\u6210\u66f4\u591a\u8bc4\u8bba\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u4e13\u5bb6\u534f\u4f5c\u4e0e\u4e89\u8bae\u8c03\u6574\u8bc4\u8bba\u751f\u6210\u5668\uff0c\u63d0\u51faCAMERED\u6846\u67b6\uff0c\u5305\u62ec\u6df7\u5408\u4e13\u5bb6\u7ed3\u6784\u7684\u751f\u6210\u8bed\u8a00\u6a21\u578b\u3001\u65b0\u578b\u8def\u7531\u7f51\u7edc\u3001\u77e5\u8bc6\u6570\u636e\u96c6\u5408\u6210\u53ca\u5bf9\u6297\u5b66\u4e60\u7b56\u7565\u3002", "result": "CAMERED\u5728\u5b9e\u9a8c\u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684RED\u57fa\u7ebf\u6a21\u578b\u548c\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "CAMERED\u901a\u8fc7\u751f\u6210\u66f4\u591a\u4eba\u7c7b\u8bc4\u8bba\u6709\u6548\u63d0\u5347\u4e86\u8c23\u8a00\u65e9\u671f\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2504.04001", "pdf": "https://arxiv.org/pdf/2504.04001", "abs": "https://arxiv.org/abs/2504.04001", "authors": ["Chuang Yang", "Xu Han", "Tao Han", "Han Han", "Bingxuan Zhao", "Qi Wang"], "title": "Edge Approximation Text Detector", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Pursuing efficient text shape representations helps scene text detection\nmodels focus on compact foreground regions and optimize the contour\nreconstruction steps to simplify the whole detection pipeline. Current\napproaches either represent irregular shapes via box-to-polygon strategy or\ndecomposing a contour into pieces for fitting gradually, the deficiency of\ncoarse contours or complex pipelines always exists in these models. Considering\nthe above issues, we introduce EdgeText to fit text contours compactly while\nalleviating excessive contour rebuilding processes. Concretely, it is observed\nthat the two long edges of texts can be regarded as smooth curves. It allows us\nto build contours via continuous and smooth edges that cover text regions\ntightly instead of fitting piecewise, which helps avoid the two limitations in\ncurrent models. Inspired by this observation, EdgeText formulates the text\nrepresentation as the edge approximation problem via parameterized curve\nfitting functions. In the inference stage, our model starts with locating text\ncenters, and then creating curve functions for approximating text edges relying\non the points. Meanwhile, truncation points are determined based on the\nlocation features. In the end, extracting curve segments from curve functions\nby using the pixel coordinate information brought by truncation points to\nreconstruct text contours. Furthermore, considering the deep dependency of\nEdgeText on text edges, a bilateral enhanced perception (BEP) module is\ndesigned. It encourages our model to pay attention to the recognition of edge\nfeatures. Additionally, to accelerate the learning of the curve function\nparameters, we introduce a proportional integral loss (PI-loss) to force the\nproposed model to focus on the curve distribution and avoid being disturbed by\ntext scales.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aEdgeText\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7d27\u51d1\u5730\u62df\u5408\u6587\u672c\u8f6e\u5ed3\u5e76\u7b80\u5316\u68c0\u6d4b\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8868\u793a\u4e0d\u89c4\u5219\u6587\u672c\u5f62\u72b6\u65f6\u5b58\u5728\u8f6e\u5ed3\u7c97\u7cd9\u6216\u6d41\u7a0b\u590d\u6742\u7684\u95ee\u9898\uff0cEdgeText\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u53c2\u6570\u5316\u66f2\u7ebf\u62df\u5408\u51fd\u6570\u8fd1\u4f3c\u6587\u672c\u8fb9\u7f18\uff0c\u7ed3\u5408\u53cc\u8fb9\u589e\u5f3a\u611f\u77e5\u6a21\u5757\uff08BEP\uff09\u548c\u6bd4\u4f8b\u79ef\u5206\u635f\u5931\uff08PI-loss\uff09\u4f18\u5316\u6a21\u578b\u3002", "result": "EdgeText\u80fd\u591f\u7d27\u51d1\u5730\u62df\u5408\u6587\u672c\u8f6e\u5ed3\uff0c\u540c\u65f6\u51cf\u5c11\u8f6e\u5ed3\u91cd\u5efa\u8fc7\u7a0b\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u6548\u7387\u3002", "conclusion": "EdgeText\u901a\u8fc7\u66f2\u7ebf\u62df\u5408\u548c\u4f18\u5316\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u7b80\u5316\u4e86\u6587\u672c\u68c0\u6d4b\u6d41\u7a0b\u3002"}}
{"id": "2504.04083", "pdf": "https://arxiv.org/pdf/2504.04083", "abs": "https://arxiv.org/abs/2504.04083", "authors": ["Aviv Brokman", "Xuguang Ai", "Yuhang Jiang", "Shashank Gupta", "Ramakanth Kavuluru"], "title": "A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs: Experiments with OpenAI Models", "categories": ["cs.CL"], "comment": null, "summary": "Objective: Zero-shot methodology promises to cut down on costs of dataset\nannotation and domain expertise needed to make use of NLP. Generative large\nlanguage models trained to align with human goals have achieved high zero-shot\nperformance across a wide variety of tasks. As of yet, it is unclear how well\nthese models perform on biomedical relation extraction (RE). To address this\nknowledge gap, we explore patterns in the performance of OpenAI LLMs across a\ndiverse sampling of RE tasks.\n  Methods: We use OpenAI GPT-4-turbo and their reasoning model o1 to conduct\nend-to-end RE experiments on seven datasets. We use the JSON generation\ncapabilities of GPT models to generate structured output in two ways: (1) by\ndefining an explicit schema describing the structure of relations, and (2)\nusing a setting that infers the structure from the prompt language.\n  Results: Our work is the first to study and compare the performance of the\nGPT-4 and o1 for the end-to-end zero-shot biomedical RE task across a broad\narray of datasets. We found the zero-shot performances to be proximal to that\nof fine-tuned methods. The limitations of this approach are that it performs\npoorly on instances containing many relations and errs on the boundaries of\ntextual mentions.\n  Conclusion: Recent large language models exhibit promising zero-shot\ncapabilities in complex biomedical RE tasks, offering competitive performance\nwith reduced dataset curation and NLP modeling needs at the cost of increased\ncomputing, potentially increasing medical community accessibility. Addressing\nthe limitations we identify could further boost reliability. The code, data,\nand prompts for all our experiments are publicly available:\nhttps://github.com/bionlproc/ZeroShotRE", "AI": {"task": "\u63a2\u7d22OpenAI\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u96f6\u6837\u672c\u65b9\u6cd5\u53ef\u4ee5\u964d\u4f4e\u6570\u636e\u96c6\u6807\u6ce8\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u9700\u6c42\uff0c\u4f46\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9b\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u5173\u7cfb\u62bd\u53d6\u4e2d\u7684\u8868\u73b0\u5982\u4f55\u3002", "method": "\u4f7f\u7528OpenAI GPT-4-turbo\u548c\u63a8\u7406\u6a21\u578bo1\uff0c\u5728\u4e03\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7aef\u5230\u7aef\u5173\u7cfb\u62bd\u53d6\u5b9e\u9a8c\uff0c\u901a\u8fc7\u4e24\u79cd\u65b9\u5f0f\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\uff1a\u5b9a\u4e49\u660e\u786e\u7684\u5173\u7cfb\u7ed3\u6784\u6a21\u5f0f\u6216\u4ece\u63d0\u793a\u8bed\u8a00\u4e2d\u63a8\u65ad\u7ed3\u6784\u3002", "result": "\u96f6\u6837\u672c\u6027\u80fd\u63a5\u8fd1\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f46\u5728\u5904\u7406\u591a\u5173\u7cfb\u5b9e\u4f8b\u548c\u6587\u672c\u63d0\u53ca\u8fb9\u754c\u65f6\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u5173\u7cfb\u62bd\u53d6\u4e2d\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u6570\u636e\u6807\u6ce8\u548c\u5efa\u6a21\u9700\u6c42\uff0c\u4f46\u9700\u89e3\u51b3\u5176\u5c40\u9650\u6027\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\u3002"}}
{"id": "2504.04010", "pdf": "https://arxiv.org/pdf/2504.04010", "abs": "https://arxiv.org/abs/2504.04010", "authors": ["Maksim Siniukov", "Di Chang", "Minh Tran", "Hongkun Gong", "Ashutosh Chaubey", "Mohammad Soleymani"], "title": "DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion", "categories": ["cs.CV", "cs.LG", "I.4.9"], "comment": "Project page: https://havent-invented.github.io/DiTaiListener", "summary": "Generating naturalistic and nuanced listener motions for extended\ninteractions remains an open problem. Existing methods often rely on\nlow-dimensional motion codes for facial behavior generation followed by\nphotorealistic rendering, limiting both visual fidelity and expressive\nrichness. To address these challenges, we introduce DiTaiListener, powered by a\nvideo diffusion model with multimodal conditions. Our approach first generates\nshort segments of listener responses conditioned on the speaker's speech and\nfacial motions with DiTaiListener-Gen. It then refines the transitional frames\nvia DiTaiListener-Edit for a seamless transition. Specifically,\nDiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener\nhead portrait generation by introducing a Causal Temporal Multimodal Adapter\n(CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter\nintegrates speakers' input in a causal manner into the video generation process\nto ensure temporally coherent listener responses. For long-form video\ngeneration, we introduce DiTaiListener-Edit, a transition refinement\nvideo-to-video diffusion model. The model fuses video segments into smooth and\ncontinuous videos, ensuring temporal consistency in facial expressions and\nimage quality when merging short video segments produced by DiTaiListener-Gen.\nQuantitatively, DiTaiListener achieves the state-of-the-art performance on\nbenchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion\nrepresentation (+6.1% in FD metric on VICO) spaces. User studies confirm the\nsuperior performance of DiTaiListener, with the model being the clear\npreference in terms of feedback, diversity, and smoothness, outperforming\ncompetitors by a significant margin.", "AI": {"task": "\u751f\u6210\u81ea\u7136\u4e14\u7ec6\u817b\u7684\u542c\u8005\u52a8\u4f5c\u4ee5\u652f\u6301\u957f\u65f6\u95f4\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4f4e\u7ef4\u8fd0\u52a8\u7f16\u7801\u548c\u7167\u7247\u7ea7\u6e32\u67d3\uff0c\u9650\u5236\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u8868\u73b0\u529b\u3002", "method": "\u63d0\u51faDiTaiListener\uff0c\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6761\u4ef6\uff0c\u5206\u4e24\u9636\u6bb5\u751f\u6210\u548c\u7f16\u8f91\u542c\u8005\u52a8\u4f5c\u3002", "result": "\u5728\u7167\u7247\u771f\u5b9e\u6027\u548c\u52a8\u4f5c\u8868\u793a\u4e0a\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u5176\u663e\u8457\u4f18\u4e8e\u7ade\u4e89\u5bf9\u624b\u3002", "conclusion": "DiTaiListener\u5728\u751f\u6210\u81ea\u7136\u542c\u8005\u52a8\u4f5c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2504.04131", "pdf": "https://arxiv.org/pdf/2504.04131", "abs": "https://arxiv.org/abs/2504.04131", "authors": ["Michael J Bommarito", "Daniel Martin Katz", "Jillian Bommarito"], "title": "Precise Legal Sentence Boundary Detection for Retrieval at Scale: NUPunkt and CharBoundary", "categories": ["cs.CL"], "comment": "12 pages, 5 figures, 6 tables", "summary": "We present NUPunkt and CharBoundary, two sentence boundary detection\nlibraries optimized for high-precision, high-throughput processing of legal\ntext in large-scale applications such as due diligence, e-discovery, and legal\nresearch. These libraries address the critical challenges posed by legal\ndocuments containing specialized citations, abbreviations, and complex sentence\nstructures that confound general-purpose sentence boundary detectors.\n  Our experimental evaluation on five diverse legal datasets comprising over\n25,000 documents and 197,000 annotated sentence boundaries demonstrates that\nNUPunkt achieves 91.1% precision while processing 10 million characters per\nsecond with modest memory requirements (432 MB). CharBoundary models offer\nbalanced and adjustable precision-recall tradeoffs, with the large model\nachieving the highest F1 score (0.782) among all tested methods.\n  Notably, NUPunkt provides a 29-32% precision improvement over general-purpose\ntools while maintaining exceptional throughput, processing multi-million\ndocument collections in minutes rather than hours. Both libraries run\nefficiently on standard CPU hardware without requiring specialized\naccelerators. NUPunkt is implemented in pure Python with zero external\ndependencies, while CharBoundary relies only on scikit-learn and optional ONNX\nruntime integration for optimized performance. Both libraries are available\nunder the MIT license, can be installed via PyPI, and can be interactively\ntested at https://sentences.aleainstitute.ai/.\n  These libraries address critical precision issues in retrieval-augmented\ngeneration systems by preserving coherent legal concepts across sentences,\nwhere each percentage improvement in precision yields exponentially greater\nreductions in context fragmentation, creating cascading benefits throughout\nretrieval pipelines and significantly enhancing downstream reasoning quality.", "AI": {"task": "\u5f00\u53d1\u4e24\u4e2a\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u541e\u5410\u91cf\u7684\u53e5\u5b50\u8fb9\u754c\u68c0\u6d4b\u5e93\uff08NUPunkt\u548cCharBoundary\uff09\uff0c\u7528\u4e8e\u5904\u7406\u6cd5\u5f8b\u6587\u672c\u3002", "motivation": "\u89e3\u51b3\u6cd5\u5f8b\u6587\u6863\u4e2d\u7279\u6b8a\u5f15\u7528\u3001\u7f29\u5199\u548c\u590d\u6742\u53e5\u5b50\u7ed3\u6784\u5bf9\u901a\u7528\u53e5\u5b50\u8fb9\u754c\u68c0\u6d4b\u5de5\u5177\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u5728\u4e94\u4e2a\u6cd5\u5f8b\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6027\u80fd\uff0cNUPunkt\u91c7\u7528\u7eafPython\u5b9e\u73b0\uff0cCharBoundary\u57fa\u4e8escikit-learn\u548cONNX\u8fd0\u884c\u65f6\u3002", "result": "NUPunkt\u8fbe\u523091.1%\u7684\u7cbe\u786e\u5ea6\uff0c\u5904\u7406\u901f\u5ea6\u4e3a\u6bcf\u79d21000\u4e07\u5b57\u7b26\uff1bCharBoundary\u5728F1\u5206\u6570\u4e0a\u8868\u73b0\u6700\u4f73\uff080.782\uff09\u3002", "conclusion": "\u8fd9\u4e24\u4e2a\u5e93\u663e\u8457\u63d0\u5347\u4e86\u6cd5\u5f8b\u6587\u672c\u5904\u7406\u7684\u7cbe\u786e\u5ea6\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5e94\u7528\uff0c\u4e14\u65e0\u9700\u4e13\u7528\u786c\u4ef6\u3002"}}
{"id": "2504.04012", "pdf": "https://arxiv.org/pdf/2504.04012", "abs": "https://arxiv.org/abs/2504.04012", "authors": ["Houzhang Fang", "Xiaolin Wang", "Zengyang Li", "Lu Wang", "Qingshan Li", "Yi Chang", "Luxin Yan"], "title": "Detection-Friendly Nonuniformity Correction: A Union Framework for Infrared UAVTarget Detection", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted by CVPR2025", "summary": "Infrared unmanned aerial vehicle (UAV) images captured using thermal\ndetectors are often affected by temperature dependent low-frequency\nnonuniformity, which significantly reduces the contrast of the images.\nDetecting UAV targets under nonuniform conditions is crucial in UAV\nsurveillance applications. Existing methods typically treat infrared\nnonuniformity correction (NUC) as a preprocessing step for detection, which\nleads to suboptimal performance. Balancing the two tasks while enhancing\ndetection beneficial information remains challenging. In this paper, we present\na detection-friendly union framework, termed UniCD, that simultaneously\naddresses both infrared NUC and UAV target detection tasks in an end-to-end\nmanner. We first model NUC as a small number of parameter estimation problem\njointly driven by priors and data to generate detection-conducive images. Then,\nwe incorporate a new auxiliary loss with target mask supervision into the\nbackbone of the infrared UAV target detection network to strengthen target\nfeatures while suppressing the background. To better balance correction and\ndetection, we introduce a detection-guided self-supervised loss to reduce\nfeature discrepancies between the two tasks, thereby enhancing detection\nrobustness to varying nonuniformity levels. Additionally, we construct a new\nbenchmark composed of 50,000 infrared images in various nonuniformity types,\nmulti-scale UAV targets and rich backgrounds with target annotations, called\nIRBFD. Extensive experiments on IRBFD demonstrate that our UniCD is a robust\nunion framework for NUC and UAV target detection while achieving real-time\nprocessing capabilities. Dataset can be available at\nhttps://github.com/IVPLaboratory/UniCD.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aUniCD\u7684\u8054\u5408\u6846\u67b6\uff0c\u540c\u65f6\u89e3\u51b3\u7ea2\u5916\u975e\u5747\u5300\u6027\u6821\u6b63\uff08NUC\uff09\u548c\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u7ea2\u5916\u975e\u5747\u5300\u6027\u6821\u6b63\u4f5c\u4e3a\u68c0\u6d4b\u7684\u9884\u5904\u7406\u6b65\u9aa4\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u5e73\u8861\u6821\u6b63\u4e0e\u68c0\u6d4b\u7684\u8054\u5408\u6846\u67b6\u3002", "method": "\u5c06NUC\u5efa\u6a21\u4e3a\u5c11\u91cf\u53c2\u6570\u4f30\u8ba1\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u8f85\u52a9\u635f\u5931\u548c\u76ee\u6807\u63a9\u7801\u76d1\u7763\uff0c\u540c\u65f6\u63d0\u51fa\u68c0\u6d4b\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u635f\u5931\u4ee5\u51cf\u5c11\u4efb\u52a1\u95f4\u7684\u7279\u5f81\u5dee\u5f02\u3002", "result": "\u5728\u6784\u5efa\u7684IRBFD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86UniCD\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002", "conclusion": "UniCD\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8054\u5408\u6846\u67b6\uff0c\u80fd\u540c\u65f6\u4f18\u5316\u7ea2\u5916\u975e\u5747\u5300\u6027\u6821\u6b63\u548c\u65e0\u4eba\u673a\u76ee\u6807\u68c0\u6d4b\u3002"}}
{"id": "2504.04141", "pdf": "https://arxiv.org/pdf/2504.04141", "abs": "https://arxiv.org/abs/2504.04141", "authors": ["Yougang Lyu", "Shijie Ren", "Yue Feng", "Zihan Wang", "Zhumin Chen", "Zhaochun Ren", "Maarten de Rijke"], "title": "Cognitive Debiasing Large Language Models for Decision-Making", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown potential in supporting\ndecision-making applications, particularly as personal conversational\nassistants in the financial, healthcare, and legal domains. While prompt\nengineering strategies have enhanced the capabilities of LLMs in\ndecision-making, cognitive biases inherent to LLMs present significant\nchallenges. Cognitive biases are systematic patterns of deviation from norms or\nrationality in decision-making that can lead to the production of inaccurate\noutputs. Existing cognitive bias mitigation strategies assume that input\nprompts contain (exactly) one type of cognitive bias and therefore fail to\nperform well in realistic settings where there maybe any number of biases.\n  To fill this gap, we propose a cognitive debiasing approach, called\nself-debiasing, that enhances the reliability of LLMs by iteratively refining\nprompts. Our method follows three sequential steps -- bias determination, bias\nanalysis, and cognitive debiasing -- to iteratively mitigate potential\ncognitive biases in prompts. Experimental results on finance, healthcare, and\nlegal decision-making tasks, using both closed-source and open-source LLMs,\ndemonstrate that the proposed self-debiasing method outperforms both advanced\nprompt engineering methods and existing cognitive debiasing techniques in\naverage accuracy under no-bias, single-bias, and multi-bias settings.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u81ea\u53bb\u504f\uff08self-debiasing\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u6765\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u51b3\u7b56\u5e94\u7528\u4e2d\u7684\u8ba4\u77e5\u504f\u5dee\u3002", "motivation": "\u73b0\u6709\u8ba4\u77e5\u504f\u5dee\u7f13\u89e3\u7b56\u7565\u5047\u8bbe\u8f93\u5165\u63d0\u793a\u4ec5\u5305\u542b\u4e00\u79cd\u8ba4\u77e5\u504f\u5dee\uff0c\u65e0\u6cd5\u5e94\u5bf9\u73b0\u5b9e\u573a\u666f\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u591a\u79cd\u504f\u5dee\u3002", "method": "\u91c7\u7528\u4e09\u6b65\u8fed\u4ee3\u6d41\u7a0b\uff1a\u504f\u5dee\u786e\u5b9a\u3001\u504f\u5dee\u5206\u6790\u548c\u8ba4\u77e5\u53bb\u504f\uff0c\u4ee5\u9010\u6b65\u6d88\u9664\u63d0\u793a\u4e2d\u7684\u6f5c\u5728\u8ba4\u77e5\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u81ea\u53bb\u504f\u65b9\u6cd5\u5728\u65e0\u504f\u5dee\u3001\u5355\u504f\u5dee\u548c\u591a\u504f\u5dee\u573a\u666f\u4e0b\u7684\u5e73\u5747\u51c6\u786e\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u53bb\u504f\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347LLMs\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2504.04024", "pdf": "https://arxiv.org/pdf/2504.04024", "abs": "https://arxiv.org/abs/2504.04024", "authors": ["Yifan Li", "Wentao Bao", "Botao Ye", "Zhen Tan", "Tianlong Chen", "Huan Liu", "Yu Kong"], "title": "Window Token Concatenation for Efficient Visual Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "To effectively reduce the visual tokens in Visual Large Language Models\n(VLLMs), we propose a novel approach called Window Token Concatenation (WiCo).\nSpecifically, we employ a sliding window to concatenate spatially adjacent\nvisual tokens. However, directly concatenating these tokens may group diverse\ntokens into one, and thus obscure some fine details. To address this challenge,\nwe propose fine-tuning the last few layers of the vision encoder to adaptively\nadjust the visual tokens, encouraging that those within the same window exhibit\nsimilar features. To further enhance the performance on fine-grained visual\nunderstanding tasks, we introduce WiCo+, which decomposes the visual tokens in\nlater layers of the LLM. Such a design enjoys the merits of the large\nperception field of the LLM for fine-grained visual understanding while keeping\na small number of visual tokens for efficient inference. We perform extensive\nexperiments on both coarse- and fine-grained visual understanding tasks based\non LLaVA-1.5 and Shikra, showing better performance compared with existing\ntoken reduction projectors. The code is available:\nhttps://github.com/JackYFL/WiCo.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aWindow Token Concatenation (WiCo)\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b(VLLMs)\u4e2d\u7684\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\u3002", "motivation": "\u76f4\u63a5\u62fc\u63a5\u76f8\u90bb\u89c6\u89c9\u6807\u8bb0\u53ef\u80fd\u4f1a\u6a21\u7cca\u7ec6\u8282\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u9002\u5e94\u8c03\u6574\u6807\u8bb0\u4ee5\u4fdd\u7559\u7ec6\u7c92\u5ea6\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u62fc\u63a5\u76f8\u90bb\u89c6\u89c9\u6807\u8bb0\uff0c\u5e76\u5fae\u8c03\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6700\u540e\u51e0\u5c42\u4ee5\u81ea\u9002\u5e94\u8c03\u6574\u6807\u8bb0\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faWiCo+\uff0c\u5728LLM\u7684\u540e\u7eed\u5c42\u5206\u89e3\u89c6\u89c9\u6807\u8bb0\u3002", "result": "\u5728\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6807\u8bb0\u51cf\u5c11\u65b9\u6cd5\u3002", "conclusion": "WiCo\u548cWiCo+\u5728\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u7684\u540c\u65f6\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2504.04142", "pdf": "https://arxiv.org/pdf/2504.04142", "abs": "https://arxiv.org/abs/2504.04142", "authors": ["Kees van Deemter"], "title": "My Life in Artificial Intelligence: People, anecdotes, and some lessons learnt", "categories": ["cs.CL", "cs.AI"], "comment": "34 pages", "summary": "In this very personal workography, I relate my 40-year experiences as a\nresearcher and educator in and around Artificial Intelligence (AI), more\nspecifically Natural Language Processing. I describe how curiosity, and the\ncircumstances of the day, led me to work in both industry and academia, and in\nvarious countries, including The Netherlands (Amsterdam, Eindhoven, and\nUtrecht), the USA (Stanford), England (Brighton), Scotland (Aberdeen), and\nChina (Beijing and Harbin). People and anecdotes play a large role in my story;\nthe history of AI forms its backdrop. I focus on things that might be of\ninterest to (even) younger colleagues, given the choices they face in their own\nwork and life at a time when AI is finally emerging from the shadows.", "AI": {"task": "\u63cf\u8ff0\u4f5c\u800540\u5e74\u6765\u5728\u4eba\u5de5\u667a\u80fd\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u7814\u7a76\u548c\u6559\u80b2\u7ecf\u5386\u3002", "motivation": "\u5206\u4eab\u4e2a\u4eba\u7ecf\u5386\u548c\u89c1\u89e3\uff0c\u4e3a\u5e74\u8f7b\u540c\u4e8b\u63d0\u4f9b\u53c2\u8003\uff0c\u5c24\u5176\u662f\u5728\u4eba\u5de5\u667a\u80fd\u9010\u6e10\u6210\u4e3a\u4e3b\u6d41\u9886\u57df\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u901a\u8fc7\u4e2a\u4eba\u56de\u5fc6\u548c\u8f76\u4e8b\uff0c\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u7684\u5386\u53f2\u80cc\u666f\uff0c\u5c55\u793a\u4f5c\u8005\u5728\u4e0d\u540c\u56fd\u5bb6\u548c\u673a\u6784\u7684\u5de5\u4f5c\u7ecf\u5386\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4efd\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u5386\u7a0b\u548c\u4e2a\u4eba\u804c\u4e1a\u9009\u62e9\u7684\u72ec\u7279\u89c6\u89d2\u3002", "conclusion": "\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u81ea\u5df1\u7684\u7ecf\u5386\u6fc0\u52b1\u5e74\u8f7b\u540c\u4e8b\uff0c\u5e76\u4e3a\u4ed6\u4eec\u63d0\u4f9b\u804c\u4e1a\u53d1\u5c55\u7684\u542f\u793a\u3002"}}
{"id": "2504.04025", "pdf": "https://arxiv.org/pdf/2504.04025", "abs": "https://arxiv.org/abs/2504.04025", "authors": ["Daniel Rivera", "Jacob Huddin", "Alexander Banerjee", "Rongzhen Zhang", "Brenda Mai", "Hanadi El Achi", "Jacob Armstrong", "Amer Wahed", "Andy Nguyen"], "title": "Artificial intelligence application in lymphoma diagnosis: from Convolutional Neural Network to Vision Transformer", "categories": ["cs.CV", "cs.LG"], "comment": "14 pages, 6 figures, 1 table", "summary": "Recently, vision transformers were shown to be capable of outperforming\nconvolutional neural networks when pretrained on sufficiently large datasets.\nVision transformer models show good accuracy on large scale datasets, with\nfeatures of multi-modal training. Due to their promising feature detection, we\naim to explore vision transformer models for diagnosis of anaplastic large cell\nlymphoma versus classical Hodgkin lymphoma using pathology whole slide images\nof HE slides. We compared the classification performance of the vision\ntransformer to our previously designed convolutional neural network on the same\ndataset. The dataset includes whole slide images of HE slides for 20 cases,\nincluding 10 cases in each diagnostic category. From each whole slide image, 60\nimage patches having size of 100 by 100 pixels and at magnification of 20 were\nobtained to yield 1200 image patches, from which 90 percent were used for\ntraining, 9 percent for validation, and 10 percent for testing. The test\nresults from the convolutional neural network model had previously shown an\nexcellent diagnostic accuracy of 100 percent. The test results from the vision\ntransformer model also showed a comparable accuracy at 100 percent. To the best\nof the authors' knowledge, this is the first direct comparison of predictive\nperformance between a vision transformer model and a convolutional neural\nnetwork model using the same dataset of lymphoma. Overall, convolutional neural\nnetwork has a more mature architecture than vision transformer and is usually\nthe best choice when large scale pretraining is not an available option.\nNevertheless, our current study shows comparable and excellent accuracy of\nvision transformer compared to that of convolutional neural network even with a\nrelatively small dataset of anaplastic large cell lymphoma and classical\nHodgkin lymphoma.", "AI": {"task": "\u6bd4\u8f83\u89c6\u89c9Transformer\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u8bca\u65ad\u95f4\u53d8\u6027\u5927\u7ec6\u80de\u6dcb\u5df4\u7624\u4e0e\u7ecf\u5178\u970d\u5947\u91d1\u6dcb\u5df4\u7624\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9Transformer\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728\u75c5\u7406\u56fe\u50cf\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u76f8\u540c\u7684\u6570\u636e\u96c6\uff0820\u4f8b\u5168\u5207\u7247\u56fe\u50cf\uff0c\u6bcf\u4f8b\u63d0\u53d660\u4e2a\u56fe\u50cf\u5757\uff09\uff0c\u5206\u522b\u8bad\u7ec3\u89c6\u89c9Transformer\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5e76\u6bd4\u8f83\u5176\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u5747\u8fbe\u5230100%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u89c6\u89c9Transformer\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u76f8\u5f53\uff0c\u4f46\u540e\u8005\u67b6\u6784\u66f4\u6210\u719f\uff0c\u9002\u5408\u65e0\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u573a\u666f\u3002"}}
{"id": "2504.04150", "pdf": "https://arxiv.org/pdf/2504.04150", "abs": "https://arxiv.org/abs/2504.04150", "authors": ["Yidong Wang"], "title": "Reasoning on Multiple Needles In A Haystack", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Needle In A Haystack (NIAH) task has been widely used to evaluate the\nlong-context question-answering capabilities of Large Language Models (LLMs).\nHowever, its reliance on simple retrieval limits its effectiveness. To address\nthis limitation, recent studies have introduced the Multiple Needles In A\nHaystack Reasoning (MNIAH-R) task, which incorporates supporting documents\n(Multiple needles) of multi-hop reasoning tasks into a distracting context\n(Haystack}). Despite this advancement, existing approaches still fail to\naddress the issue of models providing direct answers from internal knowledge,\nand they do not explain or mitigate the decline in accuracy as context length\nincreases. In this paper, we tackle the memory-based answering problem by\nfiltering out direct-answer questions, and we reveal that performance\ndegradation is primarily driven by the reduction in the length of the thinking\nprocess as the input length increases. Building on this insight, we decompose\nthe thinking process into retrieval and reasoning stages and introduce a\nreflection mechanism for multi-round extension. We also train a model using the\ngenerated iterative thinking process, which helps mitigate the performance\ndegradation. Furthermore, we demonstrate the application of this\nretrieval-reflection capability in mathematical reasoning scenarios, improving\nGPT-4o's performance on AIME2024.", "AI": {"task": "\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u4efb\u52a1\u4e2d\u6a21\u578b\u76f4\u63a5\u4f9d\u8d56\u5185\u90e8\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u5e76\u63ed\u793a\u6027\u80fd\u4e0b\u964d\u7684\u539f\u56e0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u89e3\u51b3\u6a21\u578b\u76f4\u63a5\u56de\u7b54\u95ee\u9898\u6216\u89e3\u91ca\u6027\u80fd\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u800c\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8fc7\u6ee4\u76f4\u63a5\u56de\u7b54\u95ee\u9898\uff0c\u5206\u89e3\u601d\u8003\u8fc7\u7a0b\u4e3a\u68c0\u7d22\u548c\u63a8\u7406\u9636\u6bb5\uff0c\u5e76\u5f15\u5165\u591a\u8f6e\u6269\u5c55\u7684\u53cd\u601d\u673a\u5236\u3002", "result": "\u63ed\u793a\u4e86\u6027\u80fd\u4e0b\u964d\u4e3b\u8981\u7531\u601d\u8003\u8fc7\u7a0b\u957f\u5ea6\u51cf\u5c11\u5f15\u8d77\uff0c\u5e76\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u7f13\u89e3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u68c0\u7d22-\u53cd\u601d\u673a\u5236\u5728\u6570\u5b66\u63a8\u7406\u573a\u666f\u4e2d\u63d0\u5347\u4e86GPT-4o\u5728AIME2024\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2504.04029", "pdf": "https://arxiv.org/pdf/2504.04029", "abs": "https://arxiv.org/abs/2504.04029", "authors": ["Shintaro Shiba", "Yoshimitsu Aoki", "Guillermo Gallego"], "title": "Simultaneous Motion And Noise Estimation with Event Cameras", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": "13 pages, 13 figures, 6 tables", "summary": "Event cameras are emerging vision sensors, whose noise is challenging to\ncharacterize. Existing denoising methods for event cameras consider other tasks\nsuch as motion estimation separately (i.e., sequentially after denoising).\nHowever, motion is an intrinsic part of event data, since scene edges cannot be\nsensed without motion. This work proposes, to the best of our knowledge, the\nfirst method that simultaneously estimates motion in its various forms (e.g.,\nego-motion, optical flow) and noise. The method is flexible, as it allows\nreplacing the 1-step motion estimation of the widely-used Contrast Maximization\nframework with any other motion estimator, such as deep neural networks. The\nexperiments show that the proposed method achieves state-of-the-art results on\nthe E-MLB denoising benchmark and competitive results on the DND21 benchmark,\nwhile showing its efficacy on motion estimation and intensity reconstruction\ntasks. We believe that the proposed approach contributes to strengthening the\ntheory of event-data denoising, as well as impacting practical denoising\nuse-cases, as we release the code upon acceptance. Project page:\nhttps://github.com/tub-rip/ESMD", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540c\u65f6\u4f30\u8ba1\u8fd0\u52a8\u548c\u566a\u58f0\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u53bb\u566a\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u4ef6\u76f8\u673a\u53bb\u566a\u65b9\u6cd5\u5c06\u8fd0\u52a8\u4f30\u8ba1\u7b49\u4efb\u52a1\u4e0e\u53bb\u566a\u5206\u5f00\u5904\u7406\uff0c\u800c\u8fd0\u52a8\u662f\u4e8b\u4ef6\u6570\u636e\u7684\u5185\u5728\u90e8\u5206\uff0c\u56e0\u6b64\u9700\u8981\u540c\u65f6\u5904\u7406\u3002", "method": "\u901a\u8fc7\u5c06\u5e7f\u6cdb\u4f7f\u7528\u7684\u5bf9\u6bd4\u5ea6\u6700\u5927\u5316\u6846\u67b6\u4e2d\u7684\u5355\u6b65\u8fd0\u52a8\u4f30\u8ba1\u66ff\u6362\u4e3a\u5176\u4ed6\u8fd0\u52a8\u4f30\u8ba1\u5668\uff08\u5982\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff09\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u8fd0\u52a8\u548c\u566a\u58f0\u8054\u5408\u4f30\u8ba1\u3002", "result": "\u5728E-MLB\u53bb\u566a\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728DND21\u57fa\u51c6\u4e0a\u8868\u73b0\u7ade\u4e89\u6027\uff0c\u540c\u65f6\u5728\u8fd0\u52a8\u4f30\u8ba1\u548c\u5f3a\u5ea6\u91cd\u5efa\u4efb\u52a1\u4e2d\u6709\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63a8\u52a8\u4e86\u4e8b\u4ef6\u6570\u636e\u53bb\u566a\u7406\u8bba\u7684\u53d1\u5c55\uff0c\u8fd8\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.04151", "pdf": "https://arxiv.org/pdf/2504.04151", "abs": "https://arxiv.org/abs/2504.04151", "authors": ["Kazuki Yano", "Takumi Ito", "Jun Suzuki"], "title": "STEP: Staged Parameter-Efficient Pre-training for Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025 Main", "summary": "Pre-training large language models (LLMs) faces significant memory challenges\ndue to the large size of model parameters. We introduce STaged\nparameter-Efficient Pre-training (STEP), which integrates parameter-efficient\ntuning techniques with model growth. We conduct experiments on pre-training\nLLMs of various sizes and demonstrate that STEP achieves up to a 53.9%\nreduction in maximum memory requirements compared to vanilla pre-training while\nmaintaining equivalent performance. Furthermore, we show that the model by STEP\nperforms comparably to vanilla pre-trained models on downstream tasks after\ninstruction tuning.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aSTEP\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u65f6\u7684\u5185\u5b58\u9700\u6c42\u3002", "motivation": "\u9884\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u53c2\u6570\u89c4\u6a21\u5bfc\u81f4\u5185\u5b58\u9700\u6c42\u5de8\u5927\uff0c\u4e9f\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u8c03\u4f18\u6280\u672f\u548c\u6a21\u578b\u589e\u957f\u7b56\u7565\uff0c\u63d0\u51faSTaged parameter-Efficient Pre-training (STEP)\u3002", "result": "STEP\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\uff0c\u6700\u5927\u5185\u5b58\u9700\u6c42\u51cf\u5c1153.9%\uff0c\u4e14\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0e\u5e38\u89c4\u9884\u8bad\u7ec3\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "STEP\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u76f8\u5f53\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u9700\u6c42\u3002"}}
{"id": "2504.04034", "pdf": "https://arxiv.org/pdf/2504.04034", "abs": "https://arxiv.org/abs/2504.04034", "authors": ["Dianshuo Li", "Li Chen", "Yunxiang Cao", "Kai Zhu", "Jun Cheng"], "title": "UCS: A Universal Model for Curvilinear Structure Segmentation", "categories": ["cs.CV"], "comment": "11 pages, 9 figures", "summary": "Curvilinear structure segmentation (CSS) is vital in various domains,\nincluding medical imaging, landscape analysis, industrial surface inspection,\nand plant analysis. While existing methods achieve high performance within\nspecific domains, their generalizability is limited. On the other hand,\nlarge-scale models such as Segment Anything Model (SAM) exhibit strong\ngeneralization but are not optimized for curvilinear structures. Existing\nadaptations of SAM primarily focus on general object segmentation and lack\nspecialized design for CSS tasks. To bridge this gap, we propose the Universal\nCurvilinear structure Segmentation (\\textit{UCS}) model, which adapts SAM to\nCSS tasks while enhancing its generalization. \\textit{UCS} features a novel\nencoder architecture integrating a pretrained SAM encoder with two innovations:\na Sparse Adapter, strategically inserted to inherit the pre-trained SAM\nencoder's generalization capability while minimizing the number of fine-tuning\nparameters, and a Prompt Generation module, which leverages Fast Fourier\nTransform with a high-pass filter to generate curve-specific prompts.\nFurthermore, the \\textit{UCS} incorporates a mask decoder that eliminates\nreliance on manual interaction through a dual-compression module: a\nHierarchical Feature Compression module, which aggregates the outputs of the\nsampled encoder to enhance detail preservation, and a Guidance Feature\nCompression module, which extracts and compresses image-driven guidance\nfeatures. Evaluated on a comprehensive multi-domain dataset, including an\nin-house dataset covering eight natural curvilinear structures, \\textit{UCS}\ndemonstrates state-of-the-art generalization and open-set segmentation\nperformance across medical, engineering, natural, and plant imagery,\nestablishing a new benchmark for universal CSS.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u901a\u7528\u66f2\u7ebf\u7ed3\u6784\u5206\u5272\uff08UCS\uff09\u6a21\u578b\uff0c\u4ee5\u589e\u5f3aSegment Anything Model\uff08SAM\uff09\u5728\u66f2\u7ebf\u7ed3\u6784\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5b9a\u9886\u57df\u8868\u73b0\u4f18\u5f02\u4f46\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u800cSAM\u867d\u6cdb\u5316\u80fd\u529b\u5f3a\u4f46\u672a\u9488\u5bf9\u66f2\u7ebf\u7ed3\u6784\u4f18\u5316\u3002", "method": "UCS\u6a21\u578b\u901a\u8fc7\u7a00\u758f\u9002\u914d\u5668\u548c\u63d0\u793a\u751f\u6210\u6a21\u5757\u6539\u8fdbSAM\u7f16\u7801\u5668\uff0c\u5e76\u91c7\u7528\u53cc\u538b\u7f29\u6a21\u5757\u7684\u63a9\u7801\u89e3\u7801\u5668\u3002", "result": "\u5728\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\uff0cUCS\u5c55\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6cdb\u5316\u548c\u5f00\u653e\u96c6\u5206\u5272\u6027\u80fd\u3002", "conclusion": "UCS\u4e3a\u901a\u7528\u66f2\u7ebf\u7ed3\u6784\u5206\u5272\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2504.04152", "pdf": "https://arxiv.org/pdf/2504.04152", "abs": "https://arxiv.org/abs/2504.04152", "authors": ["Zihao Li", "Shaoxiong Ji", "Hengyu Luo", "J\u00f6rg Tiedemann"], "title": "Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit significant disparities in performance\nacross languages, primarily benefiting high-resource languages while\nmarginalizing underrepresented ones. Continual Pretraining (CPT) has emerged as\na promising approach to address this imbalance, although the relative\neffectiveness of monolingual, bilingual, and code-augmented data strategies\nremains unclear. This study systematically evaluates 36 CPT configurations\ninvolving three multilingual base models, across 30+ languages categorized as\naltruistic, selfish, and stagnant, spanning various resource levels. Our\nfindings reveal three major insights: (1) Bilingual CPT improves multilingual\nclassification but often causes language mixing issues during generation. (2)\nIncluding programming code data during CPT consistently enhances multilingual\nclassification accuracy, particularly benefiting low-resource languages, but\nintroduces a trade-off by slightly degrading generation quality. (3) Contrary\nto prior work, we observe substantial deviations from language classifications\naccording to their impact on cross-lingual transfer: Languages classified as\naltruistic often negatively affect related languages, selfish languages show\nconditional and configuration-dependent behavior, and stagnant languages\ndemonstrate surprising adaptability under certain CPT conditions. These nuanced\ninteractions emphasize the complexity of multilingual representation learning,\nunderscoring the importance of systematic studies on generalizable language\nclassification to inform future multilingual CPT strategies.", "AI": {"task": "\u7cfb\u7edf\u8bc4\u4f3036\u79cd\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u914d\u7f6e\u5bf9\u591a\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u6027\u80fd\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e0e\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u4e09\u79cd\u591a\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u5355\u8bed\u3001\u53cc\u8bed\u548c\u4ee3\u7801\u589e\u5f3a\u6570\u636e\u7b56\u7565\uff0c\u5bf930\u591a\u79cd\u8bed\u8a00\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u53cc\u8bedCPT\u63d0\u5347\u591a\u8bed\u8a00\u5206\u7c7b\u4f46\u5bfc\u81f4\u751f\u6210\u95ee\u9898\uff1b\u4ee3\u7801\u6570\u636e\u589e\u5f3a\u63d0\u5347\u5206\u7c7b\u4f46\u5bf9\u751f\u6210\u8d28\u91cf\u6709\u8f7b\u5fae\u8d1f\u9762\u5f71\u54cd\uff1b\u8bed\u8a00\u5206\u7c7b\u5bf9\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u5f71\u54cd\u4e0e\u5148\u524d\u7814\u7a76\u4e0d\u540c\u3002", "conclusion": "\u5f3a\u8c03\u591a\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u7684\u590d\u6742\u6027\uff0c\u9700\u7cfb\u7edf\u6027\u7814\u7a76\u8bed\u8a00\u5206\u7c7b\u4ee5\u6307\u5bfc\u672a\u6765\u591a\u8bed\u8a00CPT\u7b56\u7565\u3002"}}
{"id": "2504.04045", "pdf": "https://arxiv.org/pdf/2504.04045", "abs": "https://arxiv.org/abs/2504.04045", "authors": ["Conghao Xiong", "Hao Chen", "Joseph J. Y. Sung"], "title": "A Survey of Pathology Foundation Model: Progress and Future Directions", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Computational pathology, analyzing whole slide images for automated cancer\ndiagnosis, relies on the multiple instance learning framework where performance\nheavily depends on the feature extractor and aggregator. Recent Pathology\nFoundation Models (PFMs), pretrained on large-scale histopathology data, have\nsignificantly enhanced capabilities of extractors and aggregators but lack\nsystematic analysis frameworks. This survey presents a hierarchical taxonomy\norganizing PFMs through a top-down philosophy that can be utilized to analyze\nFMs in any domain: model scope, model pretraining, and model design.\nAdditionally, we systematically categorize PFM evaluation tasks into\nslide-level, patch-level, multimodal, and biological tasks, providing\ncomprehensive benchmarking criteria. Our analysis identifies critical\nchallenges in both PFM development (pathology-specific methodology, end-to-end\npretraining, data-model scalability) and utilization (effective adaptation,\nmodel maintenance), paving the way for future directions in this promising\nfield. Resources referenced in this survey are available at\nhttps://github.com/BearCleverProud/AwesomeWSI.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u5c42\u6b21\u5316\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u5206\u6790\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\uff08PFMs\uff09\u53ca\u5176\u8bc4\u4f30\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u7cfb\u7edf\u5206\u6790\u6846\u67b6\uff0c\u9650\u5236\u4e86\u5176\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u5e94\u7528\u548c\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6a21\u578b\u8303\u56f4\u3001\u6a21\u578b\u9884\u8bad\u7ec3\u548c\u6a21\u578b\u8bbe\u8ba1\u7684\u5c42\u6b21\u5316\u5206\u7c7b\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790PFMs\uff0c\u5e76\u5c06\u5176\u8bc4\u4f30\u4efb\u52a1\u5206\u4e3a\u5e7b\u706f\u7247\u7ea7\u3001\u8865\u4e01\u7ea7\u3001\u591a\u6a21\u6001\u548c\u751f\u7269\u5b66\u4efb\u52a1\u3002", "result": "\u63d0\u51fa\u4e86\u5168\u9762\u7684PFM\u5206\u6790\u6846\u67b6\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u6307\u51fa\u4e86PFM\u5f00\u53d1\u548c\u5229\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u8d44\u6e90\u652f\u6301\u3002"}}
{"id": "2504.04155", "pdf": "https://arxiv.org/pdf/2504.04155", "abs": "https://arxiv.org/abs/2504.04155", "authors": ["Hengyu Luo", "Zihao Li", "Joseph Attieh", "Sawal Devkota", "Ona de Gibert", "Shaoxiong Ji", "Peiqin Lin", "Bhavani Sai Praneeth Varma Mantina", "Ananda Sreenidhi", "Ra\u00fal V\u00e1zquez", "Mengjie Wang", "Samea Yusofi", "J\u00f6rg Tiedemann"], "title": "GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are advancing at an unprecedented pace globally,\nwith regions increasingly adopting these models for applications in their\nprimary language. Evaluation of these models in diverse linguistic\nenvironments, especially in low-resource languages, has become a major\nchallenge for academia and industry. Existing evaluation frameworks are\ndisproportionately focused on English and a handful of high-resource languages,\nthereby overlooking the realistic performance of LLMs in multilingual and\nlower-resource scenarios. To address this gap, we introduce GlotEval, a\nlightweight framework designed for massively multilingual evaluation.\nSupporting seven key tasks (machine translation, text classification,\nsummarization, open-ended generation, reading comprehension, sequence labeling,\nand intrinsic evaluation), spanning over dozens to hundreds of languages,\nGlotEval highlights consistent multilingual benchmarking, language-specific\nprompt templates, and non-English-centric machine translation. This enables a\nprecise diagnosis of model strengths and weaknesses in diverse linguistic\ncontexts. A multilingual translation case study demonstrates GlotEval's\napplicability for multilingual and language-specific evaluations.", "AI": {"task": "\u4ecb\u7ecdGlotEval\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u8bed\u8a00\u8bc4\u4f30\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u8fc7\u4e8e\u5173\u6ce8\u82f1\u8bed\u548c\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u5ffd\u89c6\u4e86\u591a\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1GlotEval\u6846\u67b6\uff0c\u652f\u6301\u4e03\u9879\u5173\u952e\u4efb\u52a1\uff0c\u8986\u76d6\u6570\u5341\u81f3\u6570\u767e\u79cd\u8bed\u8a00\uff0c\u5f3a\u8c03\u4e00\u81f4\u6027\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bed\u8a00\u7279\u5b9a\u63d0\u793a\u6a21\u677f\u3002", "result": "GlotEval\u80fd\u591f\u7cbe\u786e\u8bca\u65ad\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u901a\u8fc7\u591a\u8bed\u8a00\u7ffb\u8bd1\u6848\u4f8b\u9a8c\u8bc1\u5176\u9002\u7528\u6027\u3002", "conclusion": "GlotEval\u586b\u8865\u4e86\u591a\u8bed\u8a00\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u3002"}}
{"id": "2504.04051", "pdf": "https://arxiv.org/pdf/2504.04051", "abs": "https://arxiv.org/abs/2504.04051", "authors": ["Xuyang Guo", "Zekai Huang", "Jiayan Huo", "Yingyu Liang", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang"], "title": "Can You Count to Nine? A Human Evaluation Benchmark for Counting Limits in Modern Text-to-Video Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative models have driven significant progress in a variety of AI tasks,\nincluding text-to-video generation, where models like Video LDM and Stable\nVideo Diffusion can produce realistic, movie-level videos from textual\ninstructions. Despite these advances, current text-to-video models still face\nfundamental challenges in reliably following human commands, particularly in\nadhering to simple numerical constraints. In this work, we present\nT2VCountBench, a specialized benchmark aiming at evaluating the counting\ncapability of SOTA text-to-video models as of 2025. Our benchmark employs\nrigorous human evaluations to measure the number of generated objects and\ncovers a diverse range of generators, covering both open-source and commercial\nmodels. Extensive experiments reveal that all existing models struggle with\nbasic numerical tasks, almost always failing to generate videos with an object\ncount of 9 or fewer. Furthermore, our comprehensive ablation studies explore\nhow factors like video style, temporal dynamics, and multilingual inputs may\ninfluence counting performance. We also explore prompt refinement techniques\nand demonstrate that decomposing the task into smaller subtasks does not easily\nalleviate these limitations. Our findings highlight important challenges in\ncurrent text-to-video generation and provide insights for future research aimed\nat improving adherence to basic numerical constraints.", "AI": {"task": "\u8bc4\u4f302025\u5e74\u6700\u5148\u8fdb\u7684\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u5728\u8ba1\u6570\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u6a21\u578b\u5728\u6587\u672c\u5230\u89c6\u9891\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u9075\u5faa\u7b80\u5355\u6570\u503c\u7ea6\u675f\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51faT2VCountBench\u57fa\u51c6\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u4eba\u5de5\u8bc4\u4f30\u6d4b\u91cf\u751f\u6210\u5bf9\u8c61\u7684\u6570\u91cf\uff0c\u5e76\u8986\u76d6\u591a\u79cd\u5f00\u6e90\u548c\u5546\u4e1a\u6a21\u578b\u3002", "result": "\u6240\u6709\u73b0\u6709\u6a21\u578b\u5728\u57fa\u672c\u6570\u503c\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u51e0\u4e4e\u65e0\u6cd5\u751f\u6210\u5bf9\u8c61\u6570\u91cf\u4e3a9\u6216\u66f4\u5c11\u7684\u89c6\u9891\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u91cd\u8981\u6311\u6218\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u6570\u503c\u7ea6\u675f\u9075\u5faa\u80fd\u529b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2504.04204", "pdf": "https://arxiv.org/pdf/2504.04204", "abs": "https://arxiv.org/abs/2504.04204", "authors": ["Jimmy Wang", "Thomas Zollo", "Richard Zemel", "Hongseok Namkoong"], "title": "Adaptive Elicitation of Latent Information Using Natural Language", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Eliciting information to reduce uncertainty about a latent entity is a\ncritical task in many application domains, e.g., assessing individual student\nlearning outcomes, diagnosing underlying diseases, or learning user\npreferences. Though natural language is a powerful medium for this purpose,\nlarge language models (LLMs) and existing fine-tuning algorithms lack\nmechanisms for strategically gathering information to refine their own\nunderstanding of the latent entity. To harness the generalization power and\nworld knowledge of LLMs in developing effective information-gathering\nstrategies, we propose an adaptive elicitation framework that actively reduces\nuncertainty on the latent entity. Since probabilistic modeling of an abstract\nlatent entity is difficult, our framework adopts a predictive view of\nuncertainty, using a meta-learned language model to simulate future\nobservations and enable scalable uncertainty quantification over complex\nnatural language. Through autoregressive forward simulation, our model\nquantifies how new questions reduce epistemic uncertainty, enabling the\ndevelopment of sophisticated information-gathering strategies to choose the\nmost informative next queries. In experiments on the 20 questions game, dynamic\nopinion polling, and adaptive student assessment, our method consistently\noutperforms baselines in identifying critical unknowns and improving downstream\npredictions, illustrating the promise of strategic information gathering in\nnatural language settings.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u4fe1\u606f\u83b7\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u51cf\u5c11\u6f5c\u5728\u5b9e\u4f53\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u4f18\u5316\u4fe1\u606f\u6536\u96c6\u7b56\u7565\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u662f\u4fe1\u606f\u83b7\u53d6\u7684\u6709\u529b\u5a92\u4ecb\uff0c\u4f46\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u5fae\u8c03\u7b97\u6cd5\u7f3a\u4e4f\u6218\u7565\u6027\u4fe1\u606f\u6536\u96c6\u673a\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u51cf\u5c11\u5bf9\u6f5c\u5728\u5b9e\u4f53\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u9884\u6d4b\u6027\u4e0d\u786e\u5b9a\u6027\u89c6\u89d2\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u672a\u6765\u89c2\u5bdf\uff0c\u5b9e\u73b0\u590d\u6742\u81ea\u7136\u8bed\u8a00\u7684\u53ef\u6269\u5c55\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u5e76\u5229\u7528\u81ea\u56de\u5f52\u524d\u5411\u6a21\u62df\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u67e5\u8be2\u3002", "result": "\u572820\u4e2a\u95ee\u9898\u6e38\u620f\u3001\u52a8\u6001\u610f\u89c1\u8c03\u67e5\u548c\u81ea\u9002\u5e94\u5b66\u751f\u8bc4\u4f30\u7b49\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u5173\u952e\u672a\u77e5\u4fe1\u606f\u548c\u6539\u8fdb\u4e0b\u6e38\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5728\u81ea\u7136\u8bed\u8a00\u73af\u5883\u4e2d\u6218\u7565\u6027\u4fe1\u606f\u6536\u96c6\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u4e0d\u786e\u5b9a\u6027\u5e76\u4f18\u5316\u4fe1\u606f\u83b7\u53d6\u7b56\u7565\u3002"}}
{"id": "2504.04065", "pdf": "https://arxiv.org/pdf/2504.04065", "abs": "https://arxiv.org/abs/2504.04065", "authors": ["Jiaqi Deng", "Kaize Shi", "Zonghan Wu", "Huan Huo", "Dingxian Wang", "Guandong Xu"], "title": "UniRVQA: A Unified Framework for Retrieval-Augmented Vision Question Answering via Self-Reflective Joint Training", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": "10 pages, 5 figures", "summary": "Knowledge-based Vision Question Answering (KB-VQA) systems address complex\nvisual-grounded questions requiring external knowledge, such as web-sourced\nencyclopedia articles. Existing methods often use sequential and separate\nframeworks for the retriever and the generator with limited parametric\nknowledge sharing. However, since both retrieval and generation tasks require\naccurate understanding of contextual and external information, such separation\ncan potentially lead to suboptimal system performance. Another key challenge is\nthe integration of multimodal information. General-purpose multimodal\npre-trained models, while adept at multimodal representation learning, struggle\nwith fine-grained retrieval required for knowledge-intensive visual questions.\nRecent specialized pre-trained models mitigate the issue, but are\ncomputationally expensive. To bridge the gap, we propose a Unified\nRetrieval-Augmented VQA framework (UniRVQA). UniRVQA adapts general multimodal\npre-trained models for fine-grained knowledge-intensive tasks within a unified\nframework, enabling cross-task parametric knowledge sharing and the extension\nof existing multimodal representation learning capability. We further introduce\na reflective-answering mechanism that allows the model to explicitly evaluate\nand refine its knowledge boundary. Additionally, we integrate late interaction\ninto the retrieval-augmented generation joint training process to enhance\nfine-grained understanding of queries and documents. Our approach achieves\ncompetitive performance against state-of-the-art models, delivering a\nsignificant 4.7% improvement in answering accuracy, and brings an average 7.5%\nboost in base MLLMs' VQA performance.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u68c0\u7d22\u589e\u5f3a\u89c6\u89c9\u95ee\u7b54\u6846\u67b6\uff08UniRVQA\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u68c0\u7d22\u4e0e\u751f\u6210\u4efb\u52a1\u5206\u79bb\u5bfc\u81f4\u7684\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u73b0\u6709KB-VQA\u7cfb\u7edf\u901a\u5e38\u91c7\u7528\u5206\u79bb\u7684\u68c0\u7d22\u5668\u548c\u751f\u6210\u5668\u6846\u67b6\uff0c\u9650\u5236\u4e86\u53c2\u6570\u5316\u77e5\u8bc6\u5171\u4eab\uff0c\u4e14\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u68c0\u7d22\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "UniRVQA\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u6574\u5408\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5f15\u5165\u53cd\u5c04\u56de\u7b54\u673a\u5236\u548c\u540e\u671f\u4ea4\u4e92\uff0c\u589e\u5f3a\u7ec6\u7c92\u5ea6\u7406\u89e3\u548c\u77e5\u8bc6\u8fb9\u754c\u8bc4\u4f30\u3002", "result": "UniRVQA\u5728\u56de\u7b54\u51c6\u786e\u7387\u4e0a\u663e\u8457\u63d0\u53474.7%\uff0c\u5e76\u5728\u57fa\u7840MLLMs\u7684VQA\u6027\u80fd\u4e0a\u5e73\u5747\u63d0\u53477.5%\u3002", "conclusion": "UniRVQA\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u548c\u53cd\u5c04\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u77e5\u8bc6\u5bc6\u96c6\u578b\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2504.04215", "pdf": "https://arxiv.org/pdf/2504.04215", "abs": "https://arxiv.org/abs/2504.04215", "authors": ["Vishnu Kabir Chhabra", "Mohammad Mahdi Khalili"], "title": "Towards Understanding and Improving Refusal in Compressed Models via Mechanistic Interpretability", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid growth of large language models has spurred significant interest in\nmodel compression as a means to enhance their accessibility and practicality.\nWhile extensive research has explored model compression through the lens of\nsafety, findings suggest that safety-aligned models often lose elements of\ntrustworthiness post-compression. Simultaneously, the field of mechanistic\ninterpretability has gained traction, with notable discoveries, such as the\nidentification of a single direction in the residual stream mediating refusal\nbehaviors across diverse model architectures. In this work, we investigate the\nsafety of compressed models by examining the mechanisms of refusal, adopting a\nnovel interpretability-driven perspective to evaluate model safety.\nFurthermore, leveraging insights from our interpretability analysis, we propose\na lightweight, computationally efficient method to enhance the safety of\ncompressed models without compromising their performance or utility.", "AI": {"task": "\u7814\u7a76\u538b\u7f29\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u901a\u8fc7\u62d2\u7edd\u884c\u4e3a\u7684\u673a\u5236\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u4ee5\u589e\u5f3a\u538b\u7f29\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u538b\u7f29\u53ef\u80fd\u5bfc\u81f4\u5b89\u5168\u6027\u4e0b\u964d\uff0c\u800c\u673a\u5236\u53ef\u89e3\u91ca\u6027\u9886\u57df\u7684\u65b0\u53d1\u73b0\u4e3a\u5206\u6790\u6a21\u578b\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "method": "\u91c7\u7528\u53ef\u89e3\u91ca\u6027\u9a71\u52a8\u7684\u89c6\u89d2\u5206\u6790\u62d2\u7edd\u884c\u4e3a\u7684\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4e0d\u5f71\u54cd\u6027\u80fd\u6216\u5b9e\u7528\u6027\u7684\u60c5\u51b5\u4e0b\u589e\u5f3a\u538b\u7f29\u6a21\u578b\u5b89\u5168\u6027\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u538b\u7f29\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u6027\u80fd\u3002"}}
{"id": "2504.04085", "pdf": "https://arxiv.org/pdf/2504.04085", "abs": "https://arxiv.org/abs/2504.04085", "authors": ["Xiao-Hui Li", "Fei Yin", "Cheng-Lin Liu"], "title": "DocSAM: Unified Document Image Segmentation via Query Decomposition and Heterogeneous Mixed Learning", "categories": ["cs.CV", "cs.AI"], "comment": "This paper has been accepted by CVPR 2025", "summary": "Document image segmentation is crucial for document analysis and recognition\nbut remains challenging due to the diversity of document formats and\nsegmentation tasks. Existing methods often address these tasks separately,\nresulting in limited generalization and resource wastage. This paper introduces\nDocSAM, a transformer-based unified framework designed for various document\nimage segmentation tasks, such as document layout analysis, multi-granularity\ntext segmentation, and table structure recognition, by modelling these tasks as\na combination of instance and semantic segmentation. Specifically, DocSAM\nemploys Sentence-BERT to map category names from each dataset into semantic\nqueries that match the dimensionality of instance queries. These two sets of\nqueries interact through an attention mechanism and are cross-attended with\nimage features to predict instance and semantic segmentation masks. Instance\ncategories are predicted by computing the dot product between instance and\nsemantic queries, followed by softmax normalization of scores. Consequently,\nDocSAM can be jointly trained on heterogeneous datasets, enhancing robustness\nand generalization while reducing computational and storage resources.\nComprehensive evaluations show that DocSAM surpasses existing methods in\naccuracy, efficiency, and adaptability, highlighting its potential for\nadvancing document image understanding and segmentation across various\napplications. Codes are available at https://github.com/xhli-git/DocSAM.", "AI": {"task": "\u63d0\u51faDocSAM\uff0c\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u79cd\u6587\u6863\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u56e0\u4efb\u52a1\u5206\u79bb\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u8d44\u6e90\u6d6a\u8d39\u95ee\u9898\u3002", "method": "\u4f7f\u7528Sentence-BERT\u5c06\u7c7b\u522b\u540d\u79f0\u6620\u5c04\u4e3a\u8bed\u4e49\u67e5\u8be2\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u56fe\u50cf\u7279\u5f81\u4ea4\u4e92\uff0c\u9884\u6d4b\u5b9e\u4f8b\u548c\u8bed\u4e49\u5206\u5272\u63a9\u7801\u3002", "result": "DocSAM\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u9002\u5e94\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DocSAM\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u5f02\u6784\u6570\u636e\u96c6\uff0c\u63d0\u5347\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u548c\u5b58\u50a8\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2504.04216", "pdf": "https://arxiv.org/pdf/2504.04216", "abs": "https://arxiv.org/abs/2504.04216", "authors": ["Yuantao Zhang", "Zhankui Yang"], "title": "A Perplexity and Menger Curvature-Based Approach for Similarity Evaluation of Large Language Models", "categories": ["cs.CL"], "comment": "13 pages", "summary": "The rise of Large Language Models (LLMs) has brought about concerns regarding\ncopyright infringement and unethical practices in data and model usage. For\ninstance, slight modifications to existing LLMs may be used to falsely claim\nthe development of new models, leading to issues of model copying and\nviolations of ownership rights. This paper addresses these challenges by\nintroducing a novel metric for quantifying LLM similarity, which leverages\nperplexity curves and differences in Menger curvature. Comprehensive\nexperiments validate the performance of our methodology, demonstrating its\nsuperiority over baseline methods and its ability to generalize across diverse\nmodels and domains. Furthermore, we highlight the capability of our approach in\ndetecting model replication through simulations, emphasizing its potential to\npreserve the originality and integrity of LLMs. Code is available at\nhttps://github.com/zyttt-coder/LLM_similarity.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf\u6807\u51c6\u6765\u91cf\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u76f8\u4f3c\u6027\uff0c\u4ee5\u89e3\u51b3\u6a21\u578b\u590d\u5236\u548c\u6240\u6709\u6743\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5174\u8d77\u5f15\u53d1\u4e86\u5173\u4e8e\u6570\u636e\u4f7f\u7528\u548c\u6a21\u578b\u6240\u6709\u6743\u7684\u4f26\u7406\u95ee\u9898\uff0c\u4f8b\u5982\u901a\u8fc7\u8f7b\u5fae\u4fee\u6539\u73b0\u6709\u6a21\u578b\u6765\u865a\u5047\u5ba3\u79f0\u65b0\u6a21\u578b\u7684\u5f00\u53d1\u3002", "method": "\u5229\u7528\u56f0\u60d1\u5ea6\u66f2\u7ebf\u548cMenger\u66f2\u7387\u5dee\u5f02\u6765\u91cf\u5316\u6a21\u578b\u76f8\u4f3c\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u4e0d\u540c\u6a21\u578b\u548c\u9886\u57df\uff0c\u5e76\u80fd\u6709\u6548\u68c0\u6d4b\u6a21\u578b\u590d\u5236\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u4fdd\u62a4\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u539f\u521b\u6027\u548c\u5b8c\u6574\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.04099", "pdf": "https://arxiv.org/pdf/2504.04099", "abs": "https://arxiv.org/abs/2504.04099", "authors": ["Chunzhao Xie", "Tongxuan Liu", "Lei Jiang", "Yuting Zeng", "jinrong Guo", "Yunheng Shen", "Weizhe Huang", "Jing Li", "Xiaohua Xu"], "title": "TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time Accumulative Connection", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large Vision-Language Models have demonstrated remarkable performance across\nvarious tasks; however, the challenge of hallucinations constrains their\npractical applications. The hallucination problem arises from multiple factors,\nincluding the inherent hallucinations in language models, the limitations of\nvisual encoders in perception, and biases introduced by multimodal data.\nExtensive research has explored ways to mitigate hallucinations. For instance,\nOPERA prevents the model from overly focusing on \"anchor tokens\", thereby\nreducing hallucinations, whereas VCD mitigates hallucinations by employing a\ncontrastive decoding approach. In this paper, we investigate the correlation\nbetween the decay of attention to image tokens and the occurrence of\nhallucinations. Based on this finding, we propose Temporal Attention Real-time\nAccumulative Connection (TARAC), a novel training-free method that dynamically\naccumulates and updates LVLMs' attention on image tokens during generation. By\nenhancing the model's attention to image tokens, TARAC mitigates hallucinations\ncaused by the decay of attention on image tokens. We validate the effectiveness\nof TARAC across multiple models and datasets, demonstrating that our approach\nsubstantially mitigates hallucinations. In particular, TARAC reduces $C_S$ by\n25.2 and $C_I$ by 8.7 compared to VCD on the CHAIR benchmark.", "AI": {"task": "\u7814\u7a76\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u6ce8\u610f\u529b\u8870\u51cf\u4e0e\u5e7b\u89c9\u73b0\u8c61\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5b9e\u65f6\u52a8\u6001\u6ce8\u610f\u529b\u7d2f\u79ef\u65b9\u6cd5\uff08TARAC\uff09\u6765\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u56e0\u5e7b\u89c9\u95ee\u9898\u53d7\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5b8c\u5168\u89e3\u51b3\u6ce8\u610f\u529b\u8870\u51cf\u5bfc\u81f4\u7684\u5e7b\u89c9\u3002", "method": "\u63d0\u51faTemporal Attention Real-time Accumulative Connection (TARAC)\uff0c\u52a8\u6001\u7d2f\u79ef\u548c\u66f4\u65b0\u6a21\u578b\u5bf9\u56fe\u50cf\u6807\u8bb0\u7684\u6ce8\u610f\u529b\u3002", "result": "TARAC\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\uff0c\u5728CHAIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4VCD\u65b9\u6cd5\u964d\u4f4e\u4e86$C_S$ 25.2\u548c$C_I$ 8.7\u3002", "conclusion": "TARAC\u901a\u8fc7\u589e\u5f3a\u5bf9\u56fe\u50cf\u6807\u8bb0\u7684\u6ce8\u610f\u529b\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6ce8\u610f\u529b\u8870\u51cf\u5bfc\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2504.04238", "pdf": "https://arxiv.org/pdf/2504.04238", "abs": "https://arxiv.org/abs/2504.04238", "authors": ["Yuheng Wu", "Wentao Guo", "Zirui Liu", "Heng Ji", "Zhaozhuo Xu", "Denghui Zhang"], "title": "Sensitivity Meets Sparsity: The Impact of Extremely Sparse Parameter Patterns on Theory-of-Mind of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper investigates the emergence of Theory-of-Mind (ToM) capabilities in\nlarge language models (LLMs) from a mechanistic perspective, focusing on the\nrole of extremely sparse parameter patterns. We introduce a novel method to\nidentify ToM-sensitive parameters and reveal that perturbing as little as\n0.001% of these parameters significantly degrades ToM performance while also\nimpairing contextual localization and language understanding. To understand\nthis effect, we analyze their interaction with core architectural components of\nLLMs. Our findings demonstrate that these sensitive parameters are closely\nlinked to the positional encoding module, particularly in models using Rotary\nPosition Embedding (RoPE), where perturbations disrupt dominant-frequency\nactivations critical for contextual processing. Furthermore, we show that\nperturbing ToM-sensitive parameters affects LLM's attention mechanism by\nmodulating the angle between queries and keys under positional encoding. These\ninsights provide a deeper understanding of how LLMs acquire social reasoning\nabilities, bridging AI interpretability with cognitive science. Our results\nhave implications for enhancing model alignment, mitigating biases, and\nimproving AI systems designed for human interaction.", "AI": {"task": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2dTheory-of-Mind\uff08ToM\uff09\u80fd\u529b\u7684\u673a\u5236\u6027\u6d8c\u73b0\u3002", "motivation": "\u63a2\u8ba8\u6781\u7a00\u758f\u53c2\u6570\u6a21\u5f0f\u5728ToM\u80fd\u529b\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u589e\u8fdb\u5bf9LLMs\u793e\u4f1a\u63a8\u7406\u80fd\u529b\u7684\u7406\u89e3\uff0c\u5e76\u8fde\u63a5AI\u53ef\u89e3\u91ca\u6027\u4e0e\u8ba4\u77e5\u79d1\u5b66\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\u8bc6\u522bToM\u654f\u611f\u53c2\u6570\uff0c\u5e76\u5206\u6790\u5176\u4e0eLLMs\u6838\u5fc3\u67b6\u6784\u7ec4\u4ef6\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u6270\u52a8\u6781\u5c11\u6570\uff080.001%\uff09ToM\u654f\u611f\u53c2\u6570\u4f1a\u663e\u8457\u964d\u4f4eToM\u6027\u80fd\uff0c\u5e76\u5f71\u54cd\u4e0a\u4e0b\u6587\u5b9a\u4f4d\u548c\u8bed\u8a00\u7406\u89e3\uff1b\u8fd9\u4e9b\u53c2\u6570\u4e0e\u4f4d\u7f6e\u7f16\u7801\u6a21\u5757\uff08\u5982RoPE\uff09\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5982\u4f55\u83b7\u5f97\u793e\u4f1a\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u6a21\u578b\u5bf9\u9f50\u3001\u504f\u89c1\u7f13\u89e3\u53ca\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u6539\u8fdb\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2504.04124", "pdf": "https://arxiv.org/pdf/2504.04124", "abs": "https://arxiv.org/abs/2504.04124", "authors": ["Muhammad Ahmed Ullah Khan", "Abdul Hannan Khan", "Andreas Dengel"], "title": "EMF: Event Meta Formers for Event-based Real-time Traffic Object Detection", "categories": ["cs.CV"], "comment": "10 pages, 2 figures", "summary": "Event cameras have higher temporal resolution, and require less storage and\nbandwidth compared to traditional RGB cameras. However, due to relatively\nlagging performance of event-based approaches, event cameras have not yet\nreplace traditional cameras in performance-critical applications like\nautonomous driving. Recent approaches in event-based object detection try to\nbridge this gap by employing computationally expensive transformer-based\nsolutions. However, due to their resource-intensive components, these solutions\nfail to exploit the sparsity and higher temporal resolution of event cameras\nefficiently. Moreover, these solutions are adopted from the vision domain,\nlacking specificity to the event cameras. In this work, we explore efficient\nand performant alternatives to recurrent vision transformer models and propose\na novel event-based object detection backbone. The proposed backbone employs a\nnovel Event Progression Extractor module, tailored specifically for event data,\nand uses Metaformer concept with convolution-based efficient components. We\nevaluate the resultant model on well-established traffic object detection\nbenchmarks and conduct cross-dataset evaluation to test its ability to\ngeneralize. The proposed model outperforms the state-of-the-art on Prophesee\nGen1 dataset by 1.6 mAP while reducing inference time by 14%. Our proposed EMF\nbecomes the fastest DNN-based architecture in the domain by outperforming most\nefficient event-based object detectors. Moreover, the proposed model shows\nbetter ability to generalize to unseen data and scales better with the\nabundance of data.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u4e8b\u4ef6\u76f8\u673a\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u4ee5\u66ff\u4ee3\u4f20\u7edf\u8ba1\u7b97\u5bc6\u96c6\u578b\u7684\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u5177\u6709\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u4f4e\u5b58\u50a8\u5e26\u5bbd\u9700\u6c42\uff0c\u4f46\u5176\u6027\u80fd\u5c1a\u672a\u5728\u81ea\u52a8\u9a7e\u9a76\u7b49\u5173\u952e\u5e94\u7528\u4e2d\u8d85\u8d8a\u4f20\u7edf\u76f8\u673a\uff0c\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u7279\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e8b\u4ef6\u76ee\u6807\u68c0\u6d4b\u4e3b\u5e72\u7f51\u7edc\uff0c\u5305\u542b\u4e13\u4e3a\u4e8b\u4ef6\u6570\u636e\u5b9a\u5236\u7684Event Progression Extractor\u6a21\u5757\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u5377\u79ef\u7684\u9ad8\u6548Metaformer\u67b6\u6784\u3002", "result": "\u5728Prophesee Gen1\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u6027\u80fd\u63d0\u53471.6 mAP\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1114%\uff0c\u6210\u4e3a\u8be5\u9886\u57df\u6700\u5feb\u7684DNN\u67b6\u6784\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u6269\u5c55\u6027\u66f4\u4f18\u3002", "conclusion": "\u63d0\u51fa\u7684EMF\u6a21\u578b\u5728\u4e8b\u4ef6\u76f8\u673a\u76ee\u6807\u68c0\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e0e\u9ad8\u6027\u80fd\u7684\u5e73\u8861\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.04264", "pdf": "https://arxiv.org/pdf/2504.04264", "abs": "https://arxiv.org/abs/2504.04264", "authors": ["Mingyang Wang", "Heike Adel", "Lukas Lange", "Yihong Liu", "Ercong Nie", "Jannik Str\u00f6tgen", "Hinrich Sch\u00fctze"], "title": "Lost in Multilinguality: Dissecting Cross-lingual Factual Inconsistency in Transformer Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Multilingual language models (MLMs) store factual knowledge across languages\nbut often struggle to provide consistent responses to semantically equivalent\nprompts in different languages. While previous studies point out this\ncross-lingual inconsistency issue, the underlying causes remain unexplored. In\nthis work, we use mechanistic interpretability methods to investigate\ncross-lingual inconsistencies in MLMs. We find that MLMs encode knowledge in a\nlanguage-independent concept space through most layers, and only transition to\nlanguage-specific spaces in the final layers. Failures during the language\ntransition often result in incorrect predictions in the target language, even\nwhen the answers are correct in other languages. To mitigate this inconsistency\nissue, we propose a linear shortcut method that bypasses computations in the\nfinal layers, enhancing both prediction accuracy and cross-lingual consistency.\nOur findings shed light on the internal mechanisms of MLMs and provide a\nlightweight, effective strategy for producing more consistent factual outputs.", "AI": {"task": "\u7814\u7a76\u591a\u8bed\u8a00\u6a21\u578b\uff08MLMs\uff09\u4e2d\u8de8\u8bed\u8a00\u4e0d\u4e00\u81f4\u6027\u7684\u539f\u56e0\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6cd5\u3002", "motivation": "MLMs\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u5bf9\u8bed\u4e49\u76f8\u540c\u7684\u63d0\u793a\u7ed9\u51fa\u4e0d\u4e00\u81f4\u7684\u56de\u5e94\uff0c\u4f46\u5176\u539f\u56e0\u5c1a\u672a\u660e\u786e\u3002", "method": "\u4f7f\u7528\u673a\u5236\u89e3\u91ca\u6027\u65b9\u6cd5\u5206\u6790MLMs\u7684\u8de8\u8bed\u8a00\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u51fa\u7ebf\u6027\u6377\u5f84\u65b9\u6cd5\u7ed5\u8fc7\u6700\u7ec8\u5c42\u7684\u8ba1\u7b97\u3002", "result": "\u53d1\u73b0MLMs\u5728\u5927\u591a\u6570\u5c42\u4e2d\u901a\u8fc7\u8bed\u8a00\u65e0\u5173\u7684\u6982\u5ff5\u7a7a\u95f4\u7f16\u7801\u77e5\u8bc6\uff0c\u4ec5\u5728\u6700\u540e\u51e0\u5c42\u8f6c\u5411\u8bed\u8a00\u7279\u5b9a\u7a7a\u95f4\uff0c\u8bed\u8a00\u8f6c\u6362\u5931\u8d25\u5bfc\u81f4\u76ee\u6807\u8bed\u8a00\u9884\u6d4b\u9519\u8bef\u3002\u7ebf\u6027\u6377\u5f84\u65b9\u6cd5\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8de8\u8bed\u8a00\u4e00\u81f4\u6027\u3002", "conclusion": "\u63ed\u793a\u4e86MLMs\u7684\u5185\u90e8\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6709\u6548\u7b56\u7565\u4ee5\u63d0\u5347\u4e8b\u5b9e\u8f93\u51fa\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2504.04126", "pdf": "https://arxiv.org/pdf/2504.04126", "abs": "https://arxiv.org/abs/2504.04126", "authors": ["Zhenzhi Wang", "Yixuan Li", "Yanhong Zeng", "Yuwei Guo", "Dahua Lin", "Tianfan Xue", "Bo Dai"], "title": "Multi-identity Human Image Animation with Structural Video Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages", "summary": "Generating human videos from a single image while ensuring high visual\nquality and precise control is a challenging task, especially in complex\nscenarios involving multiple individuals and interactions with objects.\nExisting methods, while effective for single-human cases, often fail to handle\nthe intricacies of multi-identity interactions because they struggle to\nassociate the correct pairs of human appearance and pose condition and model\nthe distribution of 3D-aware dynamics. To address these limitations, we present\nStructural Video Diffusion, a novel framework designed for generating realistic\nmulti-human videos. Our approach introduces two core innovations:\nidentity-specific embeddings to maintain consistent appearances across\nindividuals and a structural learning mechanism that incorporates depth and\nsurface-normal cues to model human-object interactions. Additionally, we expand\nexisting human video dataset with 25K new videos featuring diverse multi-human\nand object interaction scenarios, providing a robust foundation for training.\nExperimental results demonstrate that Structural Video Diffusion achieves\nsuperior performance in generating lifelike, coherent videos for multiple\nsubjects with dynamic and rich interactions, advancing the state of\nhuman-centric video generation.", "AI": {"task": "\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u591a\u4eba\u7269\u4ea4\u4e92\u89c6\u9891\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u4eba\u7269\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u96be\u4ee5\u5904\u7406\u591a\u4eba\u7269\u4ea4\u4e92\u7684\u590d\u6742\u6027\uff0c\u5c24\u5176\u662f\u5916\u89c2\u4e0e\u59ff\u6001\u7684\u5173\u8054\u4ee5\u53ca3D\u52a8\u6001\u5efa\u6a21\u3002", "method": "\u63d0\u51faStructural Video Diffusion\u6846\u67b6\uff0c\u5305\u62ec\u8eab\u4efd\u7279\u5b9a\u5d4c\u5165\u548c\u7ed3\u6784\u5b66\u4e60\u673a\u5236\uff0c\u7ed3\u5408\u6df1\u5ea6\u4e0e\u8868\u9762\u6cd5\u7ebf\u7ebf\u7d22\u5efa\u6a21\u4eba-\u7269\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u903c\u771f\u3001\u8fde\u8d2f\u7684\u591a\u4eba\u7269\u4ea4\u4e92\u89c6\u9891\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "Structural Video Diffusion\u63a8\u52a8\u4e86\u4ee5\u4eba\u7269\u4e3a\u4e2d\u5fc3\u7684\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.04275", "pdf": "https://arxiv.org/pdf/2504.04275", "abs": "https://arxiv.org/abs/2504.04275", "authors": ["T\u00falio Sousa de Gois", "Paloma Batista Cardoso"], "title": "negativas: a prototype for searching and classifying sentential negation in speech data", "categories": ["cs.CL"], "comment": null, "summary": "Negation is a universal feature of natural languages. In Brazilian\nPortuguese, the most commonly used negation particle is n\\~ao, which can scope\nover nouns or verbs. When it scopes over a verb, n\\~ao can occur in three\npositions: pre-verbal (NEG1), double negation (NEG2), or post-verbal (NEG3),\ne.g., n\\~ao gosto, n\\~ao gosto n\\~ao, gosto n\\~ao (\"I do not like it\"). From a\nvariationist perspective, these structures are different forms of expressing\nnegation. Pragmatically, they serve distinct communicative functions, such as\npoliteness and modal evaluation. Despite their grammatical acceptability, these\nforms differ in frequency. NEG1 dominates across Brazilian regions, while NEG2\nand NEG3 appear more rarely, suggesting its use is contextually restricted.\nThis low-frequency challenges research, often resulting in subjective,\nnon-generalizable interpretations of verbal negation with n\\~ao. To address\nthis, we developed negativas, a tool for automatically identifying NEG1, NEG2,\nand NEG3 in transcribed data. The tool's development involved four stages: i)\nanalyzing a dataset of 22 interviews from the Falares Sergipanos database,\nannotated by three linguists, ii) creating a code using natural language\nprocessing (NLP) techniques, iii) running the tool, iv) evaluating accuracy.\nInter-annotator consistency, measured using Fleiss' Kappa, was moderate (0.57).\nThe tool identified 3,338 instances of n\\~ao, classifying 2,085 as NEG1, NEG2,\nor NEG3, achieving a 93% success rate. However, negativas has limitations. NEG1\naccounted for 91.5% of identified structures, while NEG2 and NEG3 represented\n7.2% and 1.2%, respectively. The tool struggled with NEG2, sometimes\nmisclassifying instances as overlapping structures (NEG1/NEG2/NEG3).", "AI": {"task": "\u5f00\u53d1\u4e00\u4e2a\u5de5\u5177\uff08negativas\uff09\u6765\u81ea\u52a8\u8bc6\u522b\u5df4\u897f\u8461\u8404\u7259\u8bed\u4e2d\u4e09\u79cd\u5426\u5b9a\u7ed3\u6784\uff08NEG1\u3001NEG2\u3001NEG3\uff09\u3002", "motivation": "\u5df4\u897f\u8461\u8404\u7259\u8bed\u4e2d\u5426\u5b9a\u7ed3\u6784\u7684\u4f7f\u7528\u9891\u7387\u4e0d\u5747\uff0c\u5c24\u5176\u662fNEG2\u548cNEG3\u7684\u4f4e\u9891\u51fa\u73b0\u5bfc\u81f4\u7814\u7a76\u56f0\u96be\uff0c\u73b0\u6709\u89e3\u91ca\u4e3b\u89c2\u4e14\u96be\u4ee5\u63a8\u5e7f\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6807\u6ce8\u6570\u636e\u96c6\uff0822\u4e2a\u8bbf\u8c08\uff09\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5f00\u53d1\u5de5\u5177\uff0c\u5e76\u8bc4\u4f30\u5176\u51c6\u786e\u6027\u3002", "result": "\u5de5\u5177\u6210\u529f\u8bc6\u522b\u4e863,338\u4e2a\u5426\u5b9a\u5b9e\u4f8b\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe93%\uff0c\u4f46NEG2\u548cNEG3\u7684\u8bc6\u522b\u4ecd\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "negativas\u5de5\u5177\u5728\u8bc6\u522b\u9ad8\u9891NEG1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u4f4e\u9891\u7ed3\u6784\u7684\u8bc6\u522b\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2504.04130", "pdf": "https://arxiv.org/pdf/2504.04130", "abs": "https://arxiv.org/abs/2504.04130", "authors": ["Andrei-Alexandru Preda", "Iulian-Marius T\u0103iatu", "Dumitru-Clementin Cercel"], "title": "Scaling Federated Learning Solutions with Kubernetes for Synthesizing Histopathology Images", "categories": ["cs.CV"], "comment": null, "summary": "In the field of deep learning, large architectures often obtain the best\nperformance for many tasks, but also require massive datasets. In the\nhistological domain, tissue images are expensive to obtain and constitute\nsensitive medical information, raising concerns about data scarcity and\nprivacy. Vision Transformers are state-of-the-art computer vision models that\nhave proven helpful in many tasks, including image classification. In this\nwork, we combine vision Transformers with generative adversarial networks to\ngenerate histopathological images related to colorectal cancer and test their\nquality by augmenting a training dataset, leading to improved classification\naccuracy. Then, we replicate this performance using the federated learning\ntechnique and a realistic Kubernetes setup with multiple nodes, simulating a\nscenario where the training dataset is split among several hospitals unable to\nshare their information directly due to privacy concerns.", "AI": {"task": "\u7ed3\u5408\u89c6\u89c9Transformer\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u751f\u6210\u7ed3\u76f4\u80a0\u764c\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u6280\u672f\u5728\u591a\u8282\u70b9Kubernetes\u73af\u5883\u4e2d\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u89c6\u89c9Transformer\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u751f\u6210\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u5728\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u9ad8\uff0c\u5206\u7c7b\u51c6\u786e\u6027\u5f97\u5230\u63d0\u5347\uff0c\u4e14\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.04279", "pdf": "https://arxiv.org/pdf/2504.04279", "abs": "https://arxiv.org/abs/2504.04279", "authors": ["Hongchao Fang", "Can Qin", "Ran Xu", "Feng Liu", "Yixin Liu", "Lichao Sun", "Dongwon Lee", "Lifu Huang", "Wenpeng Yin"], "title": "Could AI Trace and Explain the Origins of AI-Generated Images and Text?", "categories": ["cs.CL"], "comment": null, "summary": "AI-generated content is becoming increasingly prevalent in the real world,\nleading to serious ethical and societal concerns. For instance, adversaries\nmight exploit large multimodal models (LMMs) to create images that violate\nethical or legal standards, while paper reviewers may misuse large language\nmodels (LLMs) to generate reviews without genuine intellectual effort. While\nprior work has explored detecting AI-generated images and texts, and\noccasionally tracing their source models, there is a lack of a systematic and\nfine-grained comparative study. Important dimensions--such as AI-generated\nimages vs. text, fully vs. partially AI-generated images, and general vs.\nmalicious use cases--remain underexplored. Furthermore, whether AI systems like\nGPT-4o can explain why certain forged content is attributed to specific\ngenerative models is still an open question, with no existing benchmark\naddressing this. To fill this gap, we introduce AI-FAKER, a comprehensive\nmultimodal dataset with over 280,000 samples spanning multiple LLMs and LMMs,\ncovering both general and malicious use cases for AI-generated images and\ntexts. Our experiments reveal two key findings: (i) AI authorship detection\ndepends not only on the generated output but also on the model's original\ntraining intent; and (ii) GPT-4o provides highly consistent but less specific\nexplanations when analyzing content produced by OpenAI's own models, such as\nDALL-E and GPT-4o itself.", "AI": {"task": "\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83\u548c\u68c0\u6d4bAI\u751f\u6210\u5185\u5bb9\uff08\u56fe\u50cf\u548c\u6587\u672c\uff09\u7684\u6765\u6e90\u548c\u5c5e\u6027\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "AI\u751f\u6210\u5185\u5bb9\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u666e\u53ca\u5f15\u53d1\u4e86\u4f26\u7406\u548c\u793e\u4f1a\u95ee\u9898\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u591a\u7ef4\u5ea6\uff08\u5982\u5168AI\u751f\u6210\u4e0e\u90e8\u5206AI\u751f\u6210\u3001\u901a\u7528\u4e0e\u6076\u610f\u7528\u4f8b\uff09\u7684\u7cfb\u7edf\u7814\u7a76\uff0c\u4ee5\u53caAI\u7cfb\u7edf\u662f\u5426\u80fd\u89e3\u91ca\u4f2a\u9020\u5185\u5bb9\u7684\u6765\u6e90\u3002", "method": "\u5f15\u5165AI-FAKER\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc728\u4e07\u4e2a\u6837\u672c\uff0c\u6db5\u76d6\u591a\u79cdLLMs\u548cLMMs\uff0c\u8986\u76d6\u901a\u7528\u548c\u6076\u610f\u7528\u4f8b\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a(i) AI\u4f5c\u8005\u68c0\u6d4b\u4e0d\u4ec5\u4f9d\u8d56\u751f\u6210\u5185\u5bb9\uff0c\u8fd8\u4e0e\u6a21\u578b\u7684\u539f\u59cb\u8bad\u7ec3\u610f\u56fe\u76f8\u5173\uff1b(ii) GPT-4o\u5bf9OpenAI\u81ea\u8eab\u6a21\u578b\u751f\u6210\u7684\u5185\u5bb9\u63d0\u4f9b\u4e00\u81f4\u4f46\u4e0d\u591f\u5177\u4f53\u7684\u89e3\u91ca\u3002", "conclusion": "AI-FAKER\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\uff0c\u63ed\u793a\u4e86AI\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2504.04156", "pdf": "https://arxiv.org/pdf/2504.04156", "abs": "https://arxiv.org/abs/2504.04156", "authors": ["Kai Fang", "Anqi Zhang", "Guangyu Gao", "Jianbo Jiao", "Chi Harold Liu", "Yunchao Wei"], "title": "CoMBO: Conflict Mitigation via Branched Optimization for Class Incremental Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Effective Class Incremental Segmentation (CIS) requires simultaneously\nmitigating catastrophic forgetting and ensuring sufficient plasticity to\nintegrate new classes. The inherent conflict above often leads to a\nback-and-forth, which turns the objective into finding the balance between the\nperformance of previous~(old) and incremental~(new) classes. To address this\nconflict, we introduce a novel approach, Conflict Mitigation via Branched\nOptimization~(CoMBO). Within this approach, we present the Query Conflict\nReduction module, designed to explicitly refine queries for new classes through\nlightweight, class-specific adapters. This module provides an additional branch\nfor the acquisition of new classes while preserving the original queries for\ndistillation. Moreover, we develop two strategies to further mitigate the\nconflict following the branched structure, \\textit{i.e.}, the Half-Learning\nHalf-Distillation~(HDHL) over classification probabilities, and the\nImportance-Based Knowledge Distillation~(IKD) over query features. HDHL\nselectively engages in learning for classification probabilities of queries\nthat match the ground truth of new classes, while aligning unmatched ones to\nthe corresponding old probabilities, thus ensuring retention of old knowledge\nwhile absorbing new classes via learning negative samples. Meanwhile, IKD\nassesses the importance of queries based on their matching degree to old\nclasses, prioritizing the distillation of important features and allowing less\ncritical features to evolve. Extensive experiments in Class Incremental\nPanoptic and Semantic Segmentation settings have demonstrated the superior\nperformance of CoMBO. Project page: https://guangyu-ryan.github.io/CoMBO.", "AI": {"task": "\u89e3\u51b3\u7c7b\u522b\u589e\u91cf\u5206\u5272\uff08CIS\uff09\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u4e0e\u65b0\u7c7b\u522b\u5b66\u4e60\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "motivation": "\u7c7b\u522b\u589e\u91cf\u5206\u5272\u4e2d\uff0c\u707e\u96be\u6027\u9057\u5fd8\u4e0e\u65b0\u7c7b\u522b\u5b66\u4e60\u7684\u51b2\u7a81\u5bfc\u81f4\u6027\u80fd\u5e73\u8861\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoMBO\u7684\u65b0\u65b9\u6cd5\uff0c\u5305\u62ec\u67e5\u8be2\u51b2\u7a81\u51cf\u5c11\u6a21\u5757\u3001\u534a\u5b66\u4e60\u534a\u84b8\u998f\u7b56\u7565\uff08HDHL\uff09\u548c\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u77e5\u8bc6\u84b8\u998f\uff08IKD\uff09\u3002", "result": "\u5728\u7c7b\u522b\u589e\u91cf\u5168\u666f\u548c\u8bed\u4e49\u5206\u5272\u5b9e\u9a8c\u4e2d\uff0cCoMBO\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CoMBO\u901a\u8fc7\u5206\u652f\u4f18\u5316\u548c\u51b2\u7a81\u7f13\u89e3\u7b56\u7565\uff0c\u6709\u6548\u5e73\u8861\u4e86\u65b0\u65e7\u7c7b\u522b\u7684\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u7c7b\u522b\u589e\u91cf\u5206\u5272\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\u3002"}}
{"id": "2504.04292", "pdf": "https://arxiv.org/pdf/2504.04292", "abs": "https://arxiv.org/abs/2504.04292", "authors": ["Jie Yang", "Yiqiu Tang", "Yongjie Li", "Lihua Zhang", "Haoran Zhang"], "title": "Cross-Asset Risk Management: Integrating LLMs for Real-Time Monitoring of Equity, Fixed Income, and Currency Markets", "categories": ["cs.CL", "cs.CE"], "comment": "Accepted by IJCNN 2025", "summary": "Large language models (LLMs) have emerged as powerful tools in the field of\nfinance, particularly for risk management across different asset classes. In\nthis work, we introduce a Cross-Asset Risk Management framework that utilizes\nLLMs to facilitate real-time monitoring of equity, fixed income, and currency\nmarkets. This innovative approach enables dynamic risk assessment by\naggregating diverse data sources, ultimately enhancing decision-making\nprocesses. Our model effectively synthesizes and analyzes market signals to\nidentify potential risks and opportunities while providing a holistic view of\nasset classes. By employing advanced analytics, we leverage LLMs to interpret\nfinancial texts, news articles, and market reports, ensuring that risks are\ncontextualized within broader market narratives. Extensive backtesting and\nreal-time simulations validate the framework, showing increased accuracy in\npredicting market shifts compared to conventional methods. The focus on\nreal-time data integration enhances responsiveness, allowing financial\ninstitutions to manage risks adeptly under varying market conditions and\npromoting financial stability through the advanced application of LLMs in risk\nanalysis.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8de8\u8d44\u4ea7\u98ce\u9669\u7ba1\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u76d1\u63a7\u80a1\u7968\u3001\u56fa\u5b9a\u6536\u76ca\u548c\u8d27\u5e01\u5e02\u573a\u3002", "motivation": "\u5229\u7528LLMs\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u6574\u5408\u591a\u6837\u5316\u6570\u636e\u6e90\uff0c\u63d0\u5347\u98ce\u9669\u7ba1\u7406\u7684\u52a8\u6001\u6027\u548c\u51b3\u7b56\u6548\u7387\u3002", "method": "\u901a\u8fc7LLMs\u5206\u6790\u91d1\u878d\u6587\u672c\u3001\u65b0\u95fb\u548c\u5e02\u573a\u62a5\u544a\uff0c\u7ed3\u5408\u5b9e\u65f6\u6570\u636e\u96c6\u6210\u548c\u9ad8\u7ea7\u5206\u6790\u6280\u672f\uff0c\u52a8\u6001\u8bc4\u4f30\u98ce\u9669\u3002", "result": "\u6846\u67b6\u901a\u8fc7\u56de\u6d4b\u548c\u5b9e\u65f6\u6a21\u62df\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u9884\u6d4b\u5e02\u573a\u53d8\u5316\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5b9e\u65f6\u6570\u636e\u96c6\u6210\u548cLLMs\u7684\u5148\u8fdb\u5e94\u7528\uff0c\u63d0\u5347\u4e86\u91d1\u878d\u673a\u6784\u7684\u98ce\u9669\u7ba1\u7406\u80fd\u529b\uff0c\u4fc3\u8fdb\u4e86\u91d1\u878d\u7a33\u5b9a\u6027\u3002"}}
{"id": "2504.04158", "pdf": "https://arxiv.org/pdf/2504.04158", "abs": "https://arxiv.org/abs/2504.04158", "authors": ["Yunlong Lin", "Zixu Lin", "Haoyu Chen", "Panwang Pan", "Chenxin Li", "Sixiang Chen", "Yeying Jin", "Wenbo Li", "Xinghao Ding"], "title": "JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration", "categories": ["cs.CV"], "comment": "25 pages, 15 figures", "summary": "Vision-centric perception systems struggle with unpredictable and coupled\nweather degradations in the wild. Current solutions are often limited, as they\neither depend on specific degradation priors or suffer from significant domain\ngaps. To enable robust and autonomous operation in real-world conditions, we\npropose JarvisIR, a VLM-powered agent that leverages the VLM as a controller to\nmanage multiple expert restoration models. To further enhance system\nrobustness, reduce hallucinations, and improve generalizability in real-world\nadverse weather, JarvisIR employs a novel two-stage framework consisting of\nsupervised fine-tuning and human feedback alignment. Specifically, to address\nthe lack of paired data in real-world scenarios, the human feedback alignment\nenables the VLM to be fine-tuned effectively on large-scale real-world data in\nan unsupervised manner. To support the training and evaluation of JarvisIR, we\nintroduce CleanBench, a comprehensive dataset consisting of high-quality and\nlarge-scale instruction-responses pairs, including 150K synthetic entries and\n80K real entries. Extensive experiments demonstrate that JarvisIR exhibits\nsuperior decision-making and restoration capabilities. Compared with existing\nmethods, it achieves a 50% improvement in the average of all perception metrics\non CleanBench-Real. Project page: https://cvpr2025-jarvisir.github.io/.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u540d\u4e3aJarvisIR\u7684\u89c6\u89c9\u611f\u77e5\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u611f\u77e5\u7cfb\u7edf\u5728\u590d\u6742\u5929\u6c14\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f9d\u8d56\u7279\u5b9a\u9000\u5316\u5148\u9a8c\u6216\u5b58\u5728\u663e\u8457\u9886\u57df\u5dee\u8ddd\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u81ea\u4e3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faJarvisIR\uff0c\u5229\u7528VLM\u4f5c\u4e3a\u63a7\u5236\u5668\u7ba1\u7406\u591a\u4e2a\u4e13\u5bb6\u4fee\u590d\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u6846\u67b6\uff08\u76d1\u7763\u5fae\u8c03\u548c\u4eba\u7c7b\u53cd\u9988\u5bf9\u9f50\uff09\u589e\u5f3a\u7cfb\u7edf\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "JarvisIR\u5728CleanBench-Real\u6570\u636e\u96c6\u4e0a\u7684\u611f\u77e5\u6307\u6807\u5e73\u5747\u63d0\u534750%\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u51b3\u7b56\u548c\u4fee\u590d\u80fd\u529b\u3002", "conclusion": "JarvisIR\u901a\u8fc7\u7ed3\u5408VLM\u548c\u4e13\u5bb6\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u89c6\u89c9\u611f\u77e5\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.04295", "pdf": "https://arxiv.org/pdf/2504.04295", "abs": "https://arxiv.org/abs/2504.04295", "authors": ["Jie Yang", "Yiqiu Tang", "Yongjie Li", "Lihua Zhang", "Haoran Zhang"], "title": "Dynamic Hedging Strategies in Derivatives Markets with LLM-Driven Sentiment and News Analytics", "categories": ["cs.CL", "cs.CE"], "comment": "Accepted by IJCNN 2025", "summary": "Dynamic hedging strategies are essential for effective risk management in\nderivatives markets, where volatility and market sentiment can greatly impact\nperformance. This paper introduces a novel framework that leverages large\nlanguage models (LLMs) for sentiment analysis and news analytics to inform\nhedging decisions. By analyzing textual data from diverse sources like news\narticles, social media, and financial reports, our approach captures critical\nsentiment indicators that reflect current market conditions. The framework\nallows for real-time adjustments to hedging strategies, adapting positions\nbased on continuous sentiment signals. Backtesting results on historical\nderivatives data reveal that our dynamic hedging strategies achieve superior\nrisk-adjusted returns compared to conventional static approaches. The\nincorporation of LLM-driven sentiment analysis into hedging practices presents\na significant advancement in decision-making processes within derivatives\ntrading. This research showcases how sentiment-informed dynamic hedging can\nenhance portfolio management and effectively mitigate associated risks.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u60c5\u611f\u5206\u6790\u548c\u65b0\u95fb\u5206\u6790\u4ee5\u6307\u5bfc\u5bf9\u51b2\u51b3\u7b56\u7684\u65b0\u6846\u67b6\u3002", "motivation": "\u52a8\u6001\u5bf9\u51b2\u7b56\u7565\u5728\u884d\u751f\u54c1\u5e02\u573a\u4e2d\u5bf9\u98ce\u9669\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u800c\u5e02\u573a\u60c5\u7eea\u548c\u6ce2\u52a8\u6027\u5bf9\u8868\u73b0\u6709\u91cd\u5927\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u65b0\u95fb\u6587\u7ae0\u3001\u793e\u4ea4\u5a92\u4f53\u548c\u8d22\u52a1\u62a5\u544a\u7b49\u6587\u672c\u6570\u636e\uff0c\u6355\u6349\u53cd\u6620\u5e02\u573a\u60c5\u7eea\u7684\u5173\u952e\u6307\u6807\uff0c\u5b9e\u65f6\u8c03\u6574\u5bf9\u51b2\u7b56\u7565\u3002", "result": "\u5386\u53f2\u6570\u636e\u56de\u6d4b\u663e\u793a\uff0c\u52a8\u6001\u5bf9\u51b2\u7b56\u7565\u5728\u98ce\u9669\u8c03\u6574\u540e\u6536\u76ca\u4e0a\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u65b9\u6cd5\u3002", "conclusion": "\u5c06LLM\u9a71\u52a8\u7684\u60c5\u611f\u5206\u6790\u5f15\u5165\u5bf9\u51b2\u5b9e\u8df5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u884d\u751f\u54c1\u4ea4\u6613\u4e2d\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u6709\u6548\u964d\u4f4e\u98ce\u9669\u3002"}}
{"id": "2504.04185", "pdf": "https://arxiv.org/pdf/2504.04185", "abs": "https://arxiv.org/abs/2504.04185", "authors": ["Dong Liu", "Yuanchao Wu", "Bowen Tong", "Jiansong Deng"], "title": "SDEIT: Semantic-Driven Electrical Impedance Tomography", "categories": ["cs.CV"], "comment": null, "summary": "Regularization methods using prior knowledge are essential in solving\nill-posed inverse problems such as Electrical Impedance Tomography (EIT).\nHowever, designing effective regularization and integrating prior information\ninto EIT remains challenging due to the complexity and variability of\nanatomical structures. In this work, we introduce SDEIT, a novel\nsemantic-driven framework that integrates Stable Diffusion 3.5 into EIT,\nmarking the first use of large-scale text-to-image generation models in EIT.\nSDEIT employs natural language prompts as semantic priors to guide the\nreconstruction process. By coupling an implicit neural representation (INR)\nnetwork with a plug-and-play optimization scheme that leverages SD-generated\nimages as generative priors, SDEIT improves structural consistency and recovers\nfine details. Importantly, this method does not rely on paired training\ndatasets, increasing its adaptability to varied EIT scenarios. Extensive\nexperiments on both simulated and experimental data demonstrate that SDEIT\noutperforms state-of-the-art techniques, offering superior accuracy and\nrobustness. This work opens a new pathway for integrating multimodal priors\ninto ill-posed inverse problems like EIT.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aSDEIT\u7684\u65b0\u6846\u67b6\uff0c\u5c06Stable Diffusion 3.5\u4e0eEIT\u7ed3\u5408\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u4f5c\u4e3a\u8bed\u4e49\u5148\u9a8c\u6307\u5bfc\u91cd\u5efa\u8fc7\u7a0b\u3002", "motivation": "\u89e3\u51b3EIT\u4e2d\u8bbe\u8ba1\u6709\u6548\u6b63\u5219\u5316\u548c\u6574\u5408\u5148\u9a8c\u4fe1\u606f\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u89e3\u5256\u7ed3\u6784\u7684\u590d\u6742\u6027\u548c\u53d8\u5f02\u6027\u3002", "method": "\u7ed3\u5408\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7f51\u7edc\u548c\u5373\u63d2\u5373\u7528\u4f18\u5316\u65b9\u6848\uff0c\u5229\u7528SD\u751f\u6210\u7684\u56fe\u50cf\u4f5c\u4e3a\u751f\u6210\u5148\u9a8c\u3002", "result": "\u5728\u6a21\u62df\u548c\u5b9e\u9a8c\u6570\u636e\u4e0a\uff0cSDEIT\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "SDEIT\u4e3a\u591a\u6a21\u6001\u5148\u9a8c\u6574\u5408\u5230EIT\u7b49\u4e0d\u9002\u5b9a\u9006\u95ee\u9898\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2504.04310", "pdf": "https://arxiv.org/pdf/2504.04310", "abs": "https://arxiv.org/abs/2504.04310", "authors": ["Weiwei Sun", "Shengyu Feng", "Shanda Li", "Yiming Yang"], "title": "CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although LLM-based agents have attracted significant attention in domains\nsuch as software engineering and machine learning research, their role in\nadvancing combinatorial optimization (CO) remains relatively underexplored.\nThis gap underscores the need for a deeper understanding of their potential in\ntackling structured, constraint-intensive problems-a pursuit currently limited\nby the absence of comprehensive benchmarks for systematic investigation. To\naddress this, we introduce CO-Bench, a benchmark suite featuring 36 real-world\nCO problems drawn from a broad range of domains and complexity levels. CO-Bench\nincludes structured problem formulations and curated data to support rigorous\ninvestigation of LLM agents. We evaluate multiple agent frameworks against\nestablished human-designed algorithms, revealing key strengths and limitations\nof current approaches and identifying promising directions for future research.\nCO-Bench is publicly available at https://github.com/sunnweiwei/CO-Bench.", "AI": {"task": "\u4ecb\u7ecd\u5e76\u8bc4\u4f30CO-Bench\uff0c\u4e00\u4e2a\u5305\u542b36\u4e2a\u5b9e\u9645\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u7528\u4e8e\u7cfb\u7edf\u7814\u7a76\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u7cfb\u7edf\u7814\u7a76\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u89e3\u51b3\u7ed3\u6784\u5316\u3001\u7ea6\u675f\u5bc6\u96c6\u578b\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\uff0c\u56e0\u6b64\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1CO-Bench\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5305\u542b36\u4e2a\u5b9e\u9645\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u591a\u79cd\u4ee3\u7406\u6846\u67b6\u4e0e\u4eba\u7c7b\u8bbe\u8ba1\u7b97\u6cd5\u7684\u6027\u80fd\u5bf9\u6bd4\u3002", "result": "\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6709\u524d\u666f\u65b9\u5411\u3002", "conclusion": "CO-Bench\u4e3a\u7cfb\u7edf\u7814\u7a76\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u5e76\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2504.04190", "pdf": "https://arxiv.org/pdf/2504.04190", "abs": "https://arxiv.org/abs/2504.04190", "authors": ["Yuyang Zhang", "Baao Xie", "Hu Zhu", "Qi Wang", "Huanting Guo", "Xin Jin", "Wenjun Zeng"], "title": "Interpretable Single-View 3D Gaussian Splatting using Unsupervised Hierarchical Disentangled Representation Learning", "categories": ["cs.CV"], "comment": null, "summary": "Gaussian Splatting (GS) has recently marked a significant advancement in 3D\nreconstruction, delivering both rapid rendering and high-quality results.\nHowever, existing 3DGS methods pose challenges in understanding underlying 3D\nsemantics, which hinders model controllability and interpretability. To address\nit, we propose an interpretable single-view 3DGS framework, termed 3DisGS, to\ndiscover both coarse- and fine-grained 3D semantics via hierarchical\ndisentangled representation learning (DRL). Specifically, the model employs a\ndual-branch architecture, consisting of a point cloud initialization branch and\na triplane-Gaussian generation branch, to achieve coarse-grained\ndisentanglement by separating 3D geometry and visual appearance features.\nSubsequently, fine-grained semantic representations within each modality are\nfurther discovered through DRL-based encoder-adapters. To our knowledge, this\nis the first work to achieve unsupervised interpretable 3DGS. Evaluations\nindicate that our model achieves 3D disentanglement while preserving\nhigh-quality and rapid reconstruction.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u5355\u89c6\u89d23D\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff083DisGS\uff09\uff0c\u901a\u8fc7\u5206\u5c42\u89e3\u8026\u8868\u793a\u5b66\u4e60\uff08DRL\uff09\u53d1\u73b0\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u76843D\u8bed\u4e49\u3002", "motivation": "\u73b0\u67093D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u7406\u89e3\u5e95\u5c423D\u8bed\u4e49\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u67b6\u6784\uff08\u70b9\u4e91\u521d\u59cb\u5316\u5206\u652f\u548c\u4e09\u5e73\u9762-\u9ad8\u65af\u751f\u6210\u5206\u652f\uff09\u5b9e\u73b0\u7c97\u7c92\u5ea6\u89e3\u8026\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eDRL\u7684\u7f16\u7801\u5668\u9002\u914d\u5668\u8fdb\u4e00\u6b65\u53d1\u73b0\u7ec6\u7c92\u5ea6\u8bed\u4e49\u8868\u793a\u3002", "result": "\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u548c\u5feb\u901f\u91cd\u5efa\u7684\u540c\u65f6\u5b9e\u73b0\u4e863D\u89e3\u8026\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5b9e\u73b0\u65e0\u76d1\u7763\u53ef\u89e3\u91ca3D\u9ad8\u65af\u6cfc\u6e85\u7684\u5de5\u4f5c\u3002"}}
{"id": "2504.04314", "pdf": "https://arxiv.org/pdf/2504.04314", "abs": "https://arxiv.org/abs/2504.04314", "authors": ["Justin Miller", "Tristram Alexander"], "title": "Balancing Complexity and Informativeness in LLM-Based Clustering: Finding the Goldilocks Zone", "categories": ["cs.CL", "cs.AI", "math.ST", "stat.TH"], "comment": "12 pages, 4 figures, 2 tables", "summary": "The challenge of clustering short text data lies in balancing informativeness\nwith interpretability. Traditional evaluation metrics often overlook this\ntrade-off. Inspired by linguistic principles of communicative efficiency, this\npaper investigates the optimal number of clusters by quantifying the trade-off\nbetween informativeness and cognitive simplicity. We use large language models\n(LLMs) to generate cluster names and evaluate their effectiveness through\nsemantic density, information theory, and clustering accuracy. Our results show\nthat Gaussian Mixture Model (GMM) clustering on embeddings generated by a LLM,\nincreases semantic density compared to random assignment, effectively grouping\nsimilar bios. However, as clusters increase, interpretability declines, as\nmeasured by a generative LLM's ability to correctly assign bios based on\ncluster names. A logistic regression analysis confirms that classification\naccuracy depends on the semantic similarity between bios and their assigned\ncluster names, as well as their distinction from alternatives.\n  These findings reveal a \"Goldilocks zone\" where clusters remain distinct yet\ninterpretable. We identify an optimal range of 16-22 clusters, paralleling\nlinguistic efficiency in lexical categorization. These insights inform both\ntheoretical models and practical applications, guiding future research toward\noptimising cluster interpretability and usefulness.", "AI": {"task": "\u7814\u7a76\u77ed\u6587\u672c\u6570\u636e\u805a\u7c7b\u4e2d\u4fe1\u606f\u91cf\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u5ffd\u89c6\u4e86\u4fe1\u606f\u91cf\u4e0e\u53ef\u89e3\u91ca\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u53d7\u8bed\u8a00\u4ea4\u9645\u6548\u7387\u539f\u5219\u542f\u53d1\uff0c\u63a2\u7d22\u6700\u4f18\u805a\u7c7b\u6570\u91cf\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u805a\u7c7b\u540d\u79f0\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u5bc6\u5ea6\u3001\u4fe1\u606f\u8bba\u548c\u805a\u7c7b\u51c6\u786e\u6027\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u5728LLM\u751f\u6210\u7684\u5d4c\u5165\u4e0a\u805a\u7c7b\uff0c\u63d0\u9ad8\u4e86\u8bed\u4e49\u5bc6\u5ea6\uff1b\u4f46\u968f\u7740\u805a\u7c7b\u6570\u91cf\u589e\u52a0\uff0c\u53ef\u89e3\u91ca\u6027\u4e0b\u964d\u3002", "conclusion": "\u53d1\u73b016-22\u4e2a\u805a\u7c7b\u7684\u201c\u9ec4\u91d1\u533a\u95f4\u201d\uff0c\u65e2\u4fdd\u6301\u533a\u5206\u5ea6\u53c8\u5177\u5907\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u7406\u8bba\u548c\u5b9e\u8df5\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2504.04191", "pdf": "https://arxiv.org/pdf/2504.04191", "abs": "https://arxiv.org/abs/2504.04191", "authors": ["Jieming Cui", "Tengyu Liu", "Ziyu Meng", "Jiale Yu", "Ran Song", "Wei Zhang", "Yixin Zhu", "Siyuan Huang"], "title": "GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Learning open-vocabulary physical skills for simulated agents presents a\nsignificant challenge in artificial intelligence. Current reinforcement\nlearning approaches face critical limitations: manually designed rewards lack\nscalability across diverse tasks, while demonstration-based methods struggle to\ngeneralize beyond their training distribution. We introduce GROVE, a\ngeneralized reward framework that enables open-vocabulary physical skill\nlearning without manual engineering or task-specific demonstrations. Our key\ninsight is that Large Language Models(LLMs) and Vision Language Models(VLMs)\nprovide complementary guidance -- LLMs generate precise physical constraints\ncapturing task requirements, while VLMs evaluate motion semantics and\nnaturalness. Through an iterative design process, VLM-based feedback\ncontinuously refines LLM-generated constraints, creating a self-improving\nreward system. To bridge the domain gap between simulation and natural images,\nwe develop Pose2CLIP, a lightweight mapper that efficiently projects agent\nposes directly into semantic feature space without computationally expensive\nrendering. Extensive experiments across diverse embodiments and learning\nparadigms demonstrate GROVE's effectiveness, achieving 22.2% higher motion\nnaturalness and 25.7% better task completion scores while training 8.4x faster\nthan previous methods. These results establish a new foundation for scalable\nphysical skill acquisition in simulated environments.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aGROVE\u7684\u901a\u7528\u5956\u52b1\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u7269\u7406\u6280\u80fd\u5b66\u4e60\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u624b\u52a8\u8bbe\u8ba1\u7684\u5956\u52b1\u7f3a\u4e4f\u8de8\u4efb\u52a1\u7684\u53ef\u6269\u5c55\u6027\uff0c\u800c\u57fa\u4e8e\u6f14\u793a\u7684\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u8bad\u7ec3\u5206\u5e03\u4e4b\u5916\u3002", "method": "\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u8bbe\u8ba1\u8fc7\u7a0b\u751f\u6210\u548c\u6539\u8fdb\u5956\u52b1\u7cfb\u7edf\uff0c\u5e76\u5f00\u53d1Pose2CLIP\u4ee5\u89e3\u51b3\u6a21\u62df\u4e0e\u81ea\u7136\u56fe\u50cf\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002", "result": "GROVE\u5728\u8fd0\u52a8\u81ea\u7136\u5ea6\u548c\u4efb\u52a1\u5b8c\u6210\u5ea6\u4e0a\u5206\u522b\u63d0\u9ad8\u4e8622.2%\u548c25.7%\uff0c\u4e14\u8bad\u7ec3\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb8.4\u500d\u3002", "conclusion": "GROVE\u4e3a\u6a21\u62df\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u7269\u7406\u6280\u80fd\u5b66\u4e60\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.04325", "pdf": "https://arxiv.org/pdf/2504.04325", "abs": "https://arxiv.org/abs/2504.04325", "authors": ["Juan Sosa", "Alejandro Urrego", "Cesar Prieto", "Emma J. Camargo-D\u00edaz"], "title": "Constructing the Truth: Text Mining and Linguistic Networks in Public Hearings of Case 03 of the Special Jurisdiction for Peace (JEP)", "categories": ["cs.CL", "stat.AP", "stat.ME"], "comment": "48 pages, in Spanish language, 11 tablas, 24 figures", "summary": "Case 03 of the Special Jurisdiction for Peace (JEP), focused on the so-called\nfalse positives in Colombia, represents one of the most harrowing episodes of\nthe Colombian armed conflict. This article proposes an innovative methodology\nbased on natural language analysis and semantic co-occurrence models to\nexplore, systematize, and visualize narrative patterns present in the public\nhearings of victims and appearing parties. By constructing skipgram networks\nand analyzing their modularity, the study identifies thematic clusters that\nreveal regional and procedural status differences, providing empirical evidence\non dynamics of victimization, responsibility, and acknowledgment in this case.\nThis computational approach contributes to the collective construction of both\njudicial and extrajudicial truth, offering replicable tools for other\ntransitional justice cases. The work is grounded in the pillars of truth,\njustice, reparation, and non-repetition, proposing a critical and in-depth\nreading of contested memories.", "AI": {"task": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5206\u6790\u548c\u8bed\u4e49\u5171\u73b0\u6a21\u578b\u63a2\u7d22\u3001\u7cfb\u7edf\u5316\u548c\u53ef\u89c6\u5316\u54e5\u4f26\u6bd4\u4e9a\u7279\u522b\u548c\u5e73\u6cd5\u5ead\uff08JEP\uff09\u6848\u4f8b03\u4e2d\u53d7\u5bb3\u8005\u548c\u51fa\u5ead\u65b9\u7684\u53d9\u8ff0\u6a21\u5f0f\u3002", "motivation": "\u63ed\u793a\u54e5\u4f26\u6bd4\u4e9a\u6b66\u88c5\u51b2\u7a81\u4e2d\u201c\u5047\u9633\u6027\u201d\u4e8b\u4ef6\u7684\u53d7\u5bb3\u5316\u3001\u8d23\u4efb\u548c\u627f\u8ba4\u52a8\u6001\uff0c\u4e3a\u8fc7\u6e21\u53f8\u6cd5\u6848\u4f8b\u63d0\u4f9b\u53ef\u590d\u5236\u7684\u5de5\u5177\u3002", "method": "\u6784\u5efaskipgram\u7f51\u7edc\u5e76\u5206\u6790\u5176\u6a21\u5757\u6027\uff0c\u8bc6\u522b\u4e3b\u9898\u96c6\u7fa4\u4ee5\u63ed\u793a\u533a\u57df\u548c\u7a0b\u5e8f\u72b6\u6001\u5dee\u5f02\u3002", "result": "\u8bc6\u522b\u51fa\u63ed\u793a\u53d7\u5bb3\u5316\u3001\u8d23\u4efb\u548c\u627f\u8ba4\u52a8\u6001\u7684\u4e3b\u9898\u96c6\u7fa4\uff0c\u4e3a\u53f8\u6cd5\u548c\u6cd5\u5916\u771f\u76f8\u7684\u96c6\u4f53\u6784\u5efa\u63d0\u4f9b\u5de5\u5177\u3002", "conclusion": "\u8be5\u8ba1\u7b97\u65b9\u6cd5\u4e3a\u8fc7\u6e21\u53f8\u6cd5\u6848\u4f8b\u63d0\u4f9b\u4e86\u521b\u65b0\u5de5\u5177\uff0c\u652f\u6301\u771f\u76f8\u3001\u6b63\u4e49\u3001\u8d54\u507f\u548c\u4e0d\u91cd\u590d\u7684\u652f\u67f1\u3002"}}
{"id": "2504.04196", "pdf": "https://arxiv.org/pdf/2504.04196", "abs": "https://arxiv.org/abs/2504.04196", "authors": ["Hamza Riaz", "Alan F. Smeaton"], "title": "The Effects of Grouped Structural Global Pruning of Vision Transformers on Domain Generalisation", "categories": ["cs.CV", "cs.LG"], "comment": "9 pages", "summary": "With the growing sizes of AI models like large language models (LLMs) and\nvision transformers, deploying them on devices with limited computational\nresources is a significant challenge particularly when addressing domain\ngeneralisation (DG) tasks. This paper introduces a novel grouped structural\npruning method for pre-trained vision transformers (ViT, BeiT, and DeiT),\nevaluated on the PACS and Office-Home DG benchmarks. Our method uses dependency\ngraph analysis to identify and remove redundant groups of neurons, weights,\nfilters, or attention heads within transformers, using a range of selection\nmetrics. Grouped structural pruning is applied at pruning ratios of 50\\%, 75\\%\nand 95\\% and the models are then fine-tuned on selected distributions from DG\nbenchmarks to evaluate their overall performance in DG tasks. Results show\nsignificant improvements in inference speed and fine-tuning time with minimal\ntrade-offs in accuracy and DG task performance. For instance, on the PACS\nbenchmark, pruning ViT, BeiT, and DeiT models by 50\\% using the Hessian metric\nresulted in accuracy drops of only -2.94\\%, -1.42\\%, and -1.72\\%, respectively,\nwhile achieving speed boosts of 2.5x, 1.81x, and 2.15x. These findings\ndemonstrate the effectiveness of our approach in balancing model efficiency\nwith domain generalisation performance.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u9884\u8bad\u7ec3\u89c6\u89c9Transformer\u7684\u5206\u7ec4\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u4ee5\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u63d0\u5347\u6a21\u578b\u6548\u7387\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\uff08\u5982\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9Transformer\uff09\u89c4\u6a21\u7684\u589e\u5927\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u8bbe\u5907\u4e0a\u90e8\u7f72\u8fd9\u4e9b\u6a21\u578b\u6210\u4e3a\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u9886\u57df\u6cdb\u5316\u4efb\u52a1\u65f6\u3002", "method": "\u901a\u8fc7\u4f9d\u8d56\u56fe\u5206\u6790\u8bc6\u522b\u5e76\u79fb\u9664Transformer\u4e2d\u5197\u4f59\u7684\u795e\u7ecf\u5143\u3001\u6743\u91cd\u3001\u6ee4\u6ce2\u5668\u6216\u6ce8\u610f\u529b\u5934\u7ec4\uff0c\u91c7\u7528\u591a\u79cd\u9009\u62e9\u6307\u6807\u8fdb\u884c\u526a\u679d\u3002", "result": "\u5728PACS\u548cOffice-Home DG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u526a\u679d\u540e\u7684\u6a21\u578b\u5728\u63a8\u7406\u901f\u5ea6\u548c\u5fae\u8c03\u65f6\u95f4\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u51c6\u786e\u7387\u548c\u9886\u57df\u6cdb\u5316\u6027\u80fd\u635f\u5931\u6781\u5c0f\u3002\u4f8b\u5982\uff0cViT\u3001BeiT\u548cDeiT\u6a21\u578b\u526a\u679d50%\u540e\uff0c\u901f\u5ea6\u63d0\u53472.5\u500d\u30011.81\u500d\u548c2.15\u500d\uff0c\u51c6\u786e\u7387\u4ec5\u4e0b\u964d2.94%\u30011.42%\u548c1.72%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6a21\u578b\u6548\u7387\u548c\u9886\u57df\u6cdb\u5316\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u6709\u6548\u5e73\u8861\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2504.04332", "pdf": "https://arxiv.org/pdf/2504.04332", "abs": "https://arxiv.org/abs/2504.04332", "authors": ["Quan Shi", "Carlos Jimenez", "Stephen Dong", "Brian Seo", "Caden Yao", "Adam Kelch", "Karthik Narasimhan"], "title": "IMPersona: Evaluating Individual Level LM Impersonation", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 9 pages main", "summary": "As language models achieve increasingly human-like capabilities in\nconversational text generation, a critical question emerges: to what extent can\nthese systems simulate the characteristics of specific individuals? To evaluate\nthis, we introduce IMPersona, a framework for evaluating LMs at impersonating\nspecific individuals' writing style and personal knowledge. Using supervised\nfine-tuning and a hierarchical memory-inspired retrieval system, we demonstrate\nthat even modestly sized open-source models, such as Llama-3.1-8B-Instruct, can\nachieve impersonation abilities at concerning levels. In blind conversation\nexperiments, participants (mis)identified our fine-tuned models with memory\nintegration as human in 44.44% of interactions, compared to just 25.00% for the\nbest prompting-based approach. We analyze these results to propose detection\nmethods and defense strategies against such impersonation attempts. Our\nfindings raise important questions about both the potential applications and\nrisks of personalized language models, particularly regarding privacy,\nsecurity, and the ethical deployment of such technologies in real-world\ncontexts.", "AI": {"task": "\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u7279\u5b9a\u4e2a\u4f53\u5199\u4f5c\u98ce\u683c\u548c\u4e2a\u4eba\u77e5\u8bc6\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u6587\u672c\u751f\u6210\u65b9\u9762\u8d8a\u6765\u8d8a\u63a5\u8fd1\u4eba\u7c7b\u80fd\u529b\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u6a21\u62df\u7279\u5b9a\u4e2a\u4f53\u7684\u80fd\u529b\uff0c\u4ee5\u63a2\u8ba8\u6f5c\u5728\u7684\u5e94\u7528\u548c\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\u548c\u5206\u5c42\u8bb0\u5fc6\u68c0\u7d22\u7cfb\u7edf\uff08IMPersona\u6846\u67b6\uff09\u5bf9\u5f00\u6e90\u6a21\u578b\uff08\u5982Llama-3.1-8B-Instruct\uff09\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728\u76f2\u6d4b\u5bf9\u8bdd\u5b9e\u9a8c\u4e2d\uff0c\u5fae\u8c03\u6a21\u578b\u88ab\u8bef\u8ba4\u4e3a\u4eba\u7c7b\u7684\u6982\u7387\u4e3a44.44%\uff0c\u8fdc\u9ad8\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\uff0825.00%\uff09\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4e2a\u6027\u5316\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u5728\u5e94\u7528\u548c\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u68c0\u6d4b\u65b9\u6cd5\u548c\u9632\u5fa1\u7b56\u7565\uff0c\u5e76\u547c\u5401\u5173\u6ce8\u9690\u79c1\u3001\u5b89\u5168\u548c\u4f26\u7406\u95ee\u9898\u3002"}}
{"id": "2504.04221", "pdf": "https://arxiv.org/pdf/2504.04221", "abs": "https://arxiv.org/abs/2504.04221", "authors": ["Rami Huu Nguyen", "Kenichi Maeda", "Mahsa Geshvadi", "Daniel Haehn"], "title": "Evaluating Graphical Perception with Multimodal LLMs", "categories": ["cs.CV"], "comment": "6 pages, 5 figures, 1 teaser, IEEE Pacific Visualization 2025\n  Conference", "summary": "Multimodal Large Language Models (MLLMs) have remarkably progressed in\nanalyzing and understanding images. Despite these advancements, accurately\nregressing values in charts remains an underexplored area for MLLMs. For\nvisualization, how do MLLMs perform when applied to graphical perception tasks?\nOur paper investigates this question by reproducing Cleveland and McGill's\nseminal 1984 experiment and comparing it against human task performance. Our\nstudy primarily evaluates fine-tuned and pretrained models and zero-shot\nprompting to determine if they closely match human graphical perception. Our\nfindings highlight that MLLMs outperform human task performance in some cases\nbut not in others. We highlight the results of all experiments to foster an\nunderstanding of where MLLMs succeed and fail when applied to data\nvisualization.", "AI": {"task": "\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u56fe\u8868\u6570\u503c\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4eba\u7c7b\u56fe\u5f62\u611f\u77e5\u80fd\u529b\u8fdb\u884c\u6bd4\u8f83\u3002", "motivation": "\u5c3d\u7ba1MLLMs\u5728\u56fe\u50cf\u5206\u6790\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u56fe\u8868\u6570\u503c\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u590d\u73b0Cleveland\u548cMcGill\u76841984\u5e74\u7ecf\u5178\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u5fae\u8c03\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u96f6\u6837\u672c\u63d0\u793a\u4e0e\u4eba\u7c7b\u4efb\u52a1\u8868\u73b0\u3002", "result": "MLLMs\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4eba\u7c7b\u8868\u73b0\uff0c\u4f46\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86MLLMs\u5728\u6570\u636e\u53ef\u89c6\u5316\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u4e0e\u5931\u8d25\u4e4b\u5904\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7406\u89e3\u5176\u80fd\u529b\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2504.04335", "pdf": "https://arxiv.org/pdf/2504.04335", "abs": "https://arxiv.org/abs/2504.04335", "authors": ["Yuya Ogasa", "Yuki Arase"], "title": "Hallucination Detection using Multi-View Attention Features", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "This study tackles token-level hallucination detection in outputs of large\nlanguage models. Previous studies revealed that attention exhibits irregular\npatterns when hallucination occurs. Inspired by this, we extract features from\nthe attention matrix that provide complementary views of (a) the average\nattention each token receives, which helps identify whether certain tokens are\noverly influential or ignored, (b) the diversity of attention each token\nreceives, which reveals whether attention is biased toward specific subsets,\nand (c) the diversity of tokens a token attends to during generation, which\nindicates whether the model references a narrow or broad range of information.\nThese features are input to a Transformer-based classifier to conduct\ntoken-level classification to identify hallucinated spans. Experimental results\nindicate that the proposed method outperforms strong baselines on hallucination\ndetection with longer input contexts, i.e., data-to-text and summarization\ntasks.", "AI": {"task": "\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u8bcd\u7ea7\u5e7b\u89c9\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u53d1\u73b0\u5e7b\u89c9\u53d1\u751f\u65f6\u6ce8\u610f\u529b\u6a21\u5f0f\u5f02\u5e38\uff0c\u56e0\u6b64\u63d0\u53d6\u6ce8\u610f\u529b\u77e9\u9635\u7279\u5f81\u4ee5\u8f85\u52a9\u68c0\u6d4b\u3002", "method": "\u4ece\u6ce8\u610f\u529b\u77e9\u9635\u63d0\u53d6\u4e09\u7c7b\u7279\u5f81\uff08\u5e73\u5747\u6ce8\u610f\u529b\u3001\u6ce8\u610f\u529b\u591a\u6837\u6027\u3001\u5173\u6ce8\u8303\u56f4\u591a\u6837\u6027\uff09\uff0c\u8f93\u5165Transformer\u5206\u7c7b\u5668\u8fdb\u884c\u8bcd\u7ea7\u5206\u7c7b\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u8f93\u5165\u4efb\u52a1\uff08\u5982\u6570\u636e\u5230\u6587\u672c\u548c\u6458\u8981\uff09\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u8bcd\u7ea7\u5e7b\u89c9\uff0c\u5c24\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2504.04225", "pdf": "https://arxiv.org/pdf/2504.04225", "abs": "https://arxiv.org/abs/2504.04225", "authors": ["Hamza Riaz", "Alan F. Smeaton"], "title": "Resilience of Vision Transformers for Domain Generalisation in the Presence of Out-of-Distribution Noisy Images", "categories": ["cs.CV"], "comment": "31 pages", "summary": "Modern AI models excel in controlled settings but often fail in real-world\nscenarios where data distributions shift unpredictably - a challenge known as\ndomain generalisation (DG). This paper tackles this limitation by rigorously\nevaluating vision tramsformers, specifically the BEIT architecture which is a\nmodel pre-trained with masked image modelling (MIM), against synthetic\nout-of-distribution (OOD) benchmarks designed to mimic real-world noise and\nocclusions. We introduce a novel framework to generate OOD test cases by\nstrategically masking object regions in images using grid patterns (25\\%, 50\\%,\n75\\% occlusion) and leveraging cutting-edge zero-shot segmentation via Segment\nAnything and Grounding DINO to ensure precise object localisation. Experiments\nacross three benchmarks (PACS, Office-Home, DomainNet) demonstrate BEIT's known\nrobustness while maintaining 94\\% accuracy on PACS and 87\\% on Office-Home,\ndespite significant occlusions, outperforming CNNs and other vision\ntransformers by margins of up to 37\\%. Analysis of self-attention distances\nreveals that the BEIT dependence on global features correlates with its\nresilience. Furthermore, our synthetic benchmarks expose critical failure\nmodes: performance degrades sharply when occlusions disrupt object shapes e.g.\n68\\% drop for external grid masking vs. 22\\% for internal masking. This work\nprovides two key advances (1) a scalable method to generate OOD benchmarks\nusing controllable noise, and (2) empirical evidence that MIM and\nself-attention mechanism in vision transformers enhance DG by learning\ninvariant features. These insights bridge the gap between lab-trained models\nand real-world deployment that offer a blueprint for building AI systems that\ngeneralise reliably under uncertainty.", "AI": {"task": "\u8bc4\u4f30BEIT\u67b6\u6784\u5728\u5408\u6210\u5206\u5e03\u5916\uff08OOD\uff09\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u89e3\u51b3\u89c6\u89c9\u53d8\u6362\u5668\u5728\u57df\u6cdb\u5316\uff08DG\uff09\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u4ee3AI\u6a21\u578b\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6570\u636e\u5206\u5e03\u4e0d\u53ef\u9884\u6d4b\u53d8\u5316\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u57df\u6cdb\u5316\uff08DG\uff09\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7f51\u683c\u6a21\u5f0f\uff0825%\u300150%\u300175%\u906e\u6321\uff09\u7b56\u7565\u6027\u906e\u6321\u56fe\u50cf\u533a\u57df\uff0c\u7ed3\u5408Segment Anything\u548cGrounding DINO\u8fdb\u884c\u7cbe\u786e\u5bf9\u8c61\u5b9a\u4f4d\uff0c\u751f\u6210OOD\u6d4b\u8bd5\u6848\u4f8b\u3002", "result": "BEIT\u5728PACS\u548cOffice-Home\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u4fdd\u630194%\u548c87%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8eCNN\u548c\u5176\u4ed6\u89c6\u89c9\u53d8\u6362\u5668\uff08\u6700\u9ad837%\u4f18\u52bf\uff09\u3002", "conclusion": "\u63d0\u51fa\u4e86\u751f\u6210OOD\u57fa\u51c6\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660eMIM\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u901a\u8fc7\u5b66\u4e60\u4e0d\u53d8\u7279\u5f81\u589e\u5f3a\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u4e3aAI\u7cfb\u7edf\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u53ef\u9760\u6cdb\u5316\u63d0\u4f9b\u84dd\u56fe\u3002"}}
{"id": "2504.04336", "pdf": "https://arxiv.org/pdf/2504.04336", "abs": "https://arxiv.org/abs/2504.04336", "authors": ["Cong Sun", "Kurt Teichman", "Yiliang Zhou", "Brian Critelli", "David Nauheim", "Graham Keir", "Xindi Wang", "Judy Zhong", "Adam E Flanders", "George Shih", "Yifan Peng"], "title": "Generative Large Language Models Trained for Detecting Errors in Radiology Reports", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this retrospective study, a dataset was constructed with two parts. The\nfirst part included 1,656 synthetic chest radiology reports generated by GPT-4\nusing specified prompts, with 828 being error-free synthetic reports and 828\ncontaining errors. The second part included 614 reports: 307 error-free reports\nbetween 2011 and 2016 from the MIMIC-CXR database and 307 corresponding\nsynthetic reports with errors generated by GPT-4 on the basis of these\nMIMIC-CXR reports and specified prompts. All errors were categorized into four\ntypes: negation, left/right, interval change, and transcription errors. Then,\nseveral models, including Llama-3, GPT-4, and BiomedBERT, were refined using\nzero-shot prompting, few-shot prompting, or fine-tuning strategies. Finally,\nthe performance of these models was evaluated using the F1 score, 95\\%\nconfidence interval (CI) and paired-sample t-tests on our constructed dataset,\nwith the prediction results further assessed by radiologists. Using zero-shot\nprompting, the fine-tuned Llama-3-70B-Instruct model achieved the best\nperformance with the following F1 scores: 0.769 for negation errors, 0.772 for\nleft/right errors, 0.750 for interval change errors, 0.828 for transcription\nerrors, and 0.780 overall. In the real-world evaluation phase, two radiologists\nreviewed 200 randomly selected reports output by the model. Of these, 99 were\nconfirmed to contain errors detected by the models by both radiologists, and\n163 were confirmed to contain model-detected errors by at least one\nradiologist. Generative LLMs, fine-tuned on synthetic and MIMIC-CXR radiology\nreports, greatly enhanced error detection in radiology reports.", "AI": {"task": "\u6784\u5efa\u5e76\u8bc4\u4f30\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u68c0\u6d4b\u653e\u5c04\u5b66\u62a5\u544a\u4e2d\u7684\u9519\u8bef\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u5728\u9519\u8bef\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u7684\u7ed3\u5408\uff0c\u63d0\u9ad8\u653e\u5c04\u5b66\u62a5\u544a\u4e2d\u9519\u8bef\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u4ee5\u63d0\u5347\u62a5\u544a\u8d28\u91cf\u3002", "method": "\u6784\u5efa\u5305\u542b\u5408\u6210\u548c\u771f\u5b9e\u653e\u5c04\u5b66\u62a5\u544a\u7684\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u96f6\u6837\u672c\u63d0\u793a\u3001\u5c11\u6837\u672c\u63d0\u793a\u6216\u5fae\u8c03\u7b56\u7565\u4f18\u5316\u6a21\u578b\uff08\u5982Llama-3\u3001GPT-4\u3001BiomedBERT\uff09\uff0c\u5e76\u901a\u8fc7F1\u5206\u6570\u3001\u7f6e\u4fe1\u533a\u95f4\u548c\u653e\u5c04\u79d1\u533b\u751f\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5fae\u8c03\u540e\u7684Llama-3-70B-Instruct\u6a21\u578b\u5728\u96f6\u6837\u672c\u63d0\u793a\u4e0b\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u5206\u522b\u4e3a\uff1a\u5426\u5b9a\u9519\u8bef0.769\uff0c\u5de6\u53f3\u9519\u8bef0.772\uff0c\u95f4\u9694\u53d8\u5316\u9519\u8bef0.750\uff0c\u8f6c\u5f55\u9519\u8bef0.828\uff0c\u603b\u4f530.780\u3002\u653e\u5c04\u79d1\u533b\u751f\u9a8c\u8bc1\u4e86\u6a21\u578b\u68c0\u6d4b\u5230\u7684\u9519\u8bef\u3002", "conclusion": "\u751f\u6210\u5f0fLLM\u5728\u5408\u6210\u548c\u771f\u5b9e\u653e\u5c04\u5b66\u62a5\u544a\u4e0a\u7684\u5fae\u8c03\u663e\u8457\u63d0\u5347\u4e86\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2504.04252", "pdf": "https://arxiv.org/pdf/2504.04252", "abs": "https://arxiv.org/abs/2504.04252", "authors": ["Muhammad Osama Zeeshan", "Marco Pedersoli", "Alessandro Lameiras Koerich", "Eric Grange"], "title": "Progressive Multi-Source Domain Adaptation for Personalized Facial Expression Recognition", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Personalized facial expression recognition (FER) involves adapting a machine\nlearning model using samples from labeled sources and unlabeled target domains.\nGiven the challenges of recognizing subtle expressions with considerable\ninterpersonal variability, state-of-the-art unsupervised domain adaptation\n(UDA) methods focus on the multi-source UDA (MSDA) setting, where each domain\ncorresponds to a specific subject, and improve model accuracy and robustness.\nHowever, when adapting to a specific target, the diverse nature of multiple\nsource domains translates to a large shift between source and target data.\nState-of-the-art MSDA methods for FER address this domain shift by considering\nall the sources to adapt to the target representations. Nevertheless, adapting\nto a target subject presents significant challenges due to large distributional\ndifferences between source and target domains, often resulting in negative\ntransfer. In addition, integrating all sources simultaneously increases\ncomputational costs and causes misalignment with the target. To address these\nissues, we propose a progressive MSDA approach that gradually introduces\ninformation from subjects based on their similarity to the target subject. This\nwill ensure that only the most relevant sources from the target are selected,\nwhich helps avoid the negative transfer caused by dissimilar sources. We first\nexploit the closest sources to reduce the distribution shift with the target\nand then move towards the furthest while only considering the most relevant\nsources based on the predetermined threshold. Furthermore, to mitigate\ncatastrophic forgetting caused by the incremental introduction of source\nsubjects, we implemented a density-based memory mechanism that preserves the\nmost relevant historical source samples for adaptation. Our experiments show\nthe effectiveness of our proposed method on pain datasets: Biovid and\nUNBC-McMaster.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u6e10\u8fdb\u5f0f\u591a\u6e90\u65e0\u76d1\u7763\u57df\u9002\u5e94\uff08MSDA\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\uff0c\u4ee5\u89e3\u51b3\u6e90\u57df\u4e0e\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u5927\u5206\u5e03\u5dee\u5f02\u548c\u8d1f\u8fc1\u79fb\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u6e90\u57df\u4e0e\u76ee\u6807\u57df\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u4f20\u7edf\u7684MSDA\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u8d1f\u8fc1\u79fb\u548c\u8ba1\u7b97\u6210\u672c\u589e\u52a0\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u9009\u62e9\u548c\u6574\u5408\u6700\u76f8\u5173\u7684\u6e90\u57df\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u6e10\u8fdb\u5f0fMSDA\u65b9\u6cd5\uff0c\u9010\u6b65\u5f15\u5165\u4e0e\u76ee\u6807\u57df\u76f8\u4f3c\u7684\u6e90\u57df\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u5bc6\u5ea6\u7684\u8bb0\u5fc6\u673a\u5236\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5728Biovid\u548cUNBC-McMaster\u75bc\u75db\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6e10\u8fdb\u5f0fMSDA\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c11\u8d1f\u8fc1\u79fb\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u76ee\u6807\u57df\u4e0a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2504.04342", "pdf": "https://arxiv.org/pdf/2504.04342", "abs": "https://arxiv.org/abs/2504.04342", "authors": ["Ayan Sengupta", "Siddhant Chaudhary", "Tanmoy Chakraborty"], "title": "Compression Laws for Large Language Models", "categories": ["cs.CL"], "comment": "16 pages, 11 figures, 6 tables", "summary": "We introduce compression laws for language language models (LLMs). While\nrecent scaling laws have sought to understand how LLMs scale with respect to\nmodel size, pre-training data, and computational resources, we focus on\nunderstanding how model compression affects the performance of a pre-trained\nLLM on downstream tasks. We empirically examine the effects of structured model\ncompression on LLMs through over $1000$ experiments across eight models with\nsizes ranging from $0.5B$ to $14B$ parameters. Our findings indicate that the\ntest cross-entropy loss increases quadratically with the compression ratio,\nwhereas performance on downstream tasks declines only linearly. Our study\nemphasizes the importance of recovery fine-tuning in enhancing generation loss,\nshowing that the test loss of compressed LLMs can improve by up to 55% with\nrecovery fine-tuning. At higher compression ratios (up to 90%), compressed LLMs\ndemonstrate a speed increase of 60% during inference compared to their\nuncompressed counterparts, compensating for the performance degradation at this\nlevel. However, for smaller models ($\\le 7B$), the computational gains are\nlimited, peaking at just 35%. We conclude that model compression can be highly\nbeneficial for larger models, especially when a smaller model within the same\ncomputational budget is not available. These insights provide the practical\nguidelines for utilizing model compression techniques for adopting LLMs in\nreal-life applications in resource-constrained settings.", "AI": {"task": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u538b\u7f29\u89c4\u5f8b\u53ca\u5176\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u6a21\u578b\u538b\u7f29\u5982\u4f55\u5f71\u54cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u5bf9\u538b\u7f29\u6548\u5e94\u7684\u7406\u89e3\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8d85\u8fc71000\u6b21\u5b9e\u9a8c\uff0c\u5bf98\u4e2a\u53c2\u6570\u89c4\u6a21\u4ece0.5B\u523014B\u7684\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u538b\u7f29\uff0c\u5e76\u5206\u6790\u538b\u7f29\u6bd4\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\u3002", "result": "\u6d4b\u8bd5\u4ea4\u53c9\u71b5\u635f\u5931\u968f\u538b\u7f29\u6bd4\u5448\u4e8c\u6b21\u589e\u957f\uff0c\u800c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4ec5\u7ebf\u6027\u4e0b\u964d\uff1b\u6062\u590d\u5fae\u8c03\u53ef\u63d0\u5347\u751f\u6210\u635f\u5931\u8fbe55%\uff0c\u9ad8\u538b\u7f29\u6bd4\uff0890%\uff09\u4e0b\u63a8\u7406\u901f\u5ea6\u63d0\u534760%\u3002", "conclusion": "\u6a21\u578b\u538b\u7f29\u5bf9\u5927\u578b\u6a21\u578b\u5c24\u4e3a\u6709\u76ca\uff0c\u5c24\u5176\u662f\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u65e0\u6cd5\u4f7f\u7528\u66f4\u5c0f\u6a21\u578b\u65f6\uff0c\u63d0\u4f9b\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6307\u5bfc\u539f\u5219\u3002"}}
{"id": "2504.04271", "pdf": "https://arxiv.org/pdf/2504.04271", "abs": "https://arxiv.org/abs/2504.04271", "authors": ["Mete Ahishali", "Anis Ur Rahman", "Einari Heinaro", "Samuli Junttila"], "title": "ADA-Net: Attention-Guided Domain Adaptation Network with Contrastive Learning for Standing Dead Tree Segmentation Using Aerial Imagery", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Information on standing dead trees is important for understanding forest\necosystem functioning and resilience but has been lacking over large geographic\nregions. Climate change has caused large-scale tree mortality events that can\nremain undetected due to limited data. In this study, we propose a novel method\nfor segmenting standing dead trees using aerial multispectral orthoimages.\nBecause access to annotated datasets has been a significant problem in forest\nremote sensing due to the need for forest expertise, we introduce a method for\ndomain transfer by leveraging domain adaptation to learn a transformation from\na source domain X to target domain Y. In this Image-to-Image translation task,\nwe aim to utilize available annotations in the target domain by pre-training a\nsegmentation network. When images from a new study site without annotations are\nintroduced (source domain X), these images are transformed into the target\ndomain. Then, transfer learning is applied by inferring the pre-trained network\non domain-adapted images. In addition to investigating the feasibility of\ncurrent domain adaptation approaches for this objective, we propose a novel\napproach called the Attention-guided Domain Adaptation Network (ADA-Net) with\nenhanced contrastive learning. Accordingly, the ADA-Net approach provides new\nstate-of-the-art domain adaptation performance levels outperforming existing\napproaches. We have evaluated the proposed approach using two datasets from\nFinland and the US. The USA images are converted to the Finland domain, and we\nshow that the synthetic USA2Finland dataset exhibits similar characteristics to\nthe Finland domain images. The software implementation is shared at\nhttps://github.com/meteahishali/ADA-Net. The data is publicly available at\nhttps://www.kaggle.com/datasets/meteahishali/aerial-imagery-for-standing-dead-tree-segmentation.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u57df\u9002\u5e94\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u591a\u5149\u8c31\u822a\u62cd\u56fe\u50cf\u4e2d\u5206\u5272\u7ad9\u7acb\u67af\u6811\u3002", "motivation": "\u6c14\u5019\u53d8\u5316\u5bfc\u81f4\u5927\u89c4\u6a21\u6811\u6728\u6b7b\u4ea1\u4e8b\u4ef6\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u6709\u9650\uff0c\u8fd9\u4e9b\u4e8b\u4ef6\u53ef\u80fd\u672a\u88ab\u68c0\u6d4b\u5230\uff0c\u800c\u7ad9\u7acb\u67af\u6811\u7684\u4fe1\u606f\u5bf9\u7406\u89e3\u68ee\u6797\u751f\u6001\u7cfb\u7edf\u529f\u80fd\u548c\u6062\u590d\u529b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u57df\u9002\u5e94\u7f51\u7edc\uff08ADA-Net\uff09\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5c06\u6e90\u57df\u56fe\u50cf\u8f6c\u6362\u4e3a\u76ee\u6807\u57df\u56fe\u50cf\uff0c\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5206\u5272\u7f51\u7edc\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u3002", "result": "ADA-Net\u5728\u57df\u9002\u5e94\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u6570\u636e\u96c6\uff08USA2Finland\uff09\u4e0e\u76ee\u6807\u57df\u56fe\u50cf\u5177\u6709\u76f8\u4f3c\u7279\u5f81\u3002", "conclusion": "ADA-Net\u4e3a\u5927\u89c4\u6a21\u68ee\u6797\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u516c\u5f00\u4e86\u8f6f\u4ef6\u5b9e\u73b0\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2504.04373", "pdf": "https://arxiv.org/pdf/2504.04373", "abs": "https://arxiv.org/abs/2504.04373", "authors": ["Shenyang Liu", "Yang Gao", "Shaoyan Zhai", "Liqiang Wang"], "title": "StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style Transformation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "2024 IEEE International Conference on Big Data (BigData)", "summary": "Prompt Recovery, reconstructing prompts from the outputs of large language\nmodels (LLMs), has grown in importance as LLMs become ubiquitous. Most users\naccess LLMs through APIs without internal model weights, relying only on\noutputs and logits, which complicates recovery. This paper explores a unique\nprompt recovery task focused on reconstructing prompts for style transfer and\nrephrasing, rather than typical question-answering. We introduce a dataset\ncreated with LLM assistance, ensuring quality through multiple techniques, and\ntest methods like zero-shot, few-shot, jailbreak, chain-of-thought,\nfine-tuning, and a novel canonical-prompt fallback for poor-performing cases.\nOur results show that one-shot and fine-tuning yield the best outcomes but\nhighlight flaws in traditional sentence similarity metrics for evaluating\nprompt recovery. Contributions include (1) a benchmark dataset, (2)\ncomprehensive experiments on prompt recovery strategies, and (3) identification\nof limitations in current evaluation metrics, all of which advance general\nprompt recovery research, where the structure of the input prompt is\nunrestricted.", "AI": {"task": "\u63a2\u7d22\u4e00\u79cd\u72ec\u7279\u7684\u63d0\u793a\u6062\u590d\u4efb\u52a1\uff0c\u4e13\u6ce8\u4e8e\u98ce\u683c\u8f6c\u6362\u548c\u91cd\u8ff0\u7684\u63d0\u793a\u91cd\u5efa\uff0c\u800c\u975e\u5178\u578b\u7684\u95ee\u7b54\u4efb\u52a1\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u666e\u53ca\uff0c\u63d0\u793a\u6062\u590d\uff08\u4eceLLM\u8f93\u51fa\u4e2d\u91cd\u5efa\u63d0\u793a\uff09\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u4f46\u5927\u591a\u6570\u7528\u6237\u53ea\u80fd\u901a\u8fc7API\u8bbf\u95ee\u6a21\u578b\uff0c\u7f3a\u4e4f\u5185\u90e8\u6743\u91cd\uff0c\u4ec5\u4f9d\u8d56\u8f93\u51fa\u548clogits\uff0c\u589e\u52a0\u4e86\u6062\u590d\u7684\u590d\u6742\u6027\u3002", "method": "\u901a\u8fc7LLM\u8f85\u52a9\u521b\u5efa\u6570\u636e\u96c6\uff0c\u91c7\u7528\u591a\u79cd\u6280\u672f\u786e\u4fdd\u8d28\u91cf\uff0c\u5e76\u6d4b\u8bd5\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u8d8a\u72f1\u3001\u601d\u7ef4\u94fe\u3001\u5fae\u8c03\u7b49\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4e00\u79cd\u9488\u5bf9\u6027\u80fd\u4e0d\u4f73\u60c5\u51b5\u7684\u65b0\u578b\u89c4\u8303\u63d0\u793a\u56de\u9000\u7b56\u7565\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5355\u6837\u672c\u548c\u5fae\u8c03\u65b9\u6cd5\u6548\u679c\u6700\u4f73\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u4f20\u7edf\u53e5\u5b50\u76f8\u4f3c\u6027\u5ea6\u91cf\u5728\u8bc4\u4f30\u63d0\u793a\u6062\u590d\u65f6\u7684\u7f3a\u9677\u3002", "conclusion": "\u8d21\u732e\u5305\u62ec\uff081\uff09\u57fa\u51c6\u6570\u636e\u96c6\uff0c\uff082\uff09\u5168\u9762\u7684\u63d0\u793a\u6062\u590d\u7b56\u7565\u5b9e\u9a8c\uff0c\uff083\uff09\u5f53\u524d\u8bc4\u4f30\u6307\u6807\u7684\u5c40\u9650\u6027\u8bc6\u522b\uff0c\u63a8\u52a8\u4e86\u65e0\u9650\u5236\u8f93\u5165\u63d0\u793a\u7ed3\u6784\u7684\u901a\u7528\u63d0\u793a\u6062\u590d\u7814\u7a76\u3002"}}
{"id": "2504.04294", "pdf": "https://arxiv.org/pdf/2504.04294", "abs": "https://arxiv.org/abs/2504.04294", "authors": ["Zhisheng Huang", "Peng Wang", "Jingdong Zhang", "Yuan Liu", "Xin Li", "Wenping Wang"], "title": "3R-GS: Best Practice in Optimizing Camera Poses Along with 3DGS", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has revolutionized neural rendering with its\nefficiency and quality, but like many novel view synthesis methods, it heavily\ndepends on accurate camera poses from Structure-from-Motion (SfM) systems.\nAlthough recent SfM pipelines have made impressive progress, questions remain\nabout how to further improve both their robust performance in challenging\nconditions (e.g., textureless scenes) and the precision of camera parameter\nestimation simultaneously. We present 3R-GS, a 3D Gaussian Splatting framework\nthat bridges this gap by jointly optimizing 3D Gaussians and camera parameters\nfrom large reconstruction priors MASt3R-SfM. We note that naively performing\njoint 3D Gaussian and camera optimization faces two challenges: the sensitivity\nto the quality of SfM initialization, and its limited capacity for global\noptimization, leading to suboptimal reconstruction results. Our 3R-GS,\novercomes these issues by incorporating optimized practices, enabling robust\nscene reconstruction even with imperfect camera registration. Extensive\nexperiments demonstrate that 3R-GS delivers high-quality novel view synthesis\nand precise camera pose estimation while remaining computationally efficient.\nProject page: https://zsh523.github.io/3R-GS/", "AI": {"task": "\u63d0\u51fa3R-GS\u6846\u67b6\uff0c\u8054\u5408\u4f18\u53163D\u9ad8\u65af\u548c\u76f8\u673a\u53c2\u6570\uff0c\u4ee5\u63d0\u5347\u795e\u7ecf\u6e32\u67d3\u7684\u8d28\u91cf\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093D\u9ad8\u65af\u6e32\u67d3\u65b9\u6cd5\u5bf9SfM\u76f8\u673a\u59ff\u6001\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6311\u6218\u6027\u573a\u666f\uff08\u5982\u65e0\u7eb9\u7406\u573a\u666f\uff09\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u5927\u5c3a\u5ea6\u91cd\u5efa\u5148\u9a8cMASt3R-SfM\uff0c\u8054\u5408\u4f18\u53163D\u9ad8\u65af\u548c\u76f8\u673a\u53c2\u6570\uff0c\u5e76\u5f15\u5165\u4f18\u5316\u7b56\u7565\u4ee5\u514b\u670d\u521d\u59cb\u5316\u548c\u5168\u5c40\u4f18\u5316\u9650\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c3R-GS\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\u5408\u6210\u548c\u7cbe\u786e\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3002", "conclusion": "3R-GS\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u6e32\u67d3\u7684\u9c81\u68d2\u6027\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u7cbe\u5ea6\u3002"}}
{"id": "2504.04377", "pdf": "https://arxiv.org/pdf/2504.04377", "abs": "https://arxiv.org/abs/2504.04377", "authors": ["Priyanshu Kumar", "Devansh Jain", "Akhila Yerukola", "Liwei Jiang", "Himanshu Beniwal", "Thomas Hartvigsen", "Maarten Sap"], "title": "PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages", "categories": ["cs.CL"], "comment": null, "summary": "Truly multilingual safety moderation efforts for Large Language Models (LLMs)\nhave been hindered by a narrow focus on a small set of languages (e.g.,\nEnglish, Chinese) as well as a limited scope of safety definition, resulting in\nsignificant gaps in moderation capabilities. To bridge these gaps, we release\nPOLYGUARD, a new state-of-the-art multilingual safety model for safeguarding\nLLM generations, and the corresponding training and evaluation datasets.\nPOLYGUARD is trained on POLYGUARDMIX, the largest multilingual safety training\ncorpus to date containing 1.91M samples across 17 languages (e.g., Chinese,\nCzech, English, Hindi). We also introduce POLYGUARDPROMPTS, a high quality\nmultilingual benchmark with 29K samples for the evaluation of safety\nguardrails. Created by combining naturally occurring multilingual human-LLM\ninteractions and human-verified machine translations of an English-only safety\ndataset (WildGuardMix; Han et al., 2024), our datasets contain prompt-output\npairs with labels of prompt harmfulness, response harmfulness, and response\nrefusal. Through extensive evaluations across multiple safety and toxicity\nbenchmarks, we demonstrate that POLYGUARD outperforms existing state-of-the-art\nopen-weight and commercial safety classifiers by 5.5%. Our contributions\nadvance efforts toward safer multilingual LLMs for all global users.", "AI": {"task": "\u5f00\u53d1POLYGUARD\uff0c\u4e00\u79cd\u591a\u8bed\u8a00\u5b89\u5168\u6a21\u578b\uff0c\u7528\u4e8e\u4fdd\u62a4\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u751f\u6210\u5185\u5bb9\u3002", "motivation": "\u5f53\u524d\u591a\u8bed\u8a00\u5b89\u5168\u5ba1\u6838\u4e3b\u8981\u96c6\u4e2d\u5728\u5c11\u6570\u8bed\u8a00\uff08\u5982\u82f1\u8bed\u3001\u4e2d\u6587\uff09\uff0c\u4e14\u5b89\u5168\u5b9a\u4e49\u8303\u56f4\u6709\u9650\uff0c\u5bfc\u81f4\u5ba1\u6838\u80fd\u529b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "method": "\u53d1\u5e03POLYGUARD\u6a21\u578b\u53ca\u914d\u5957\u6570\u636e\u96c6POLYGUARDMIX\uff08\u5305\u542b17\u79cd\u8bed\u8a00\u76841.91M\u6837\u672c\uff09\u548cPOLYGUARDPROMPTS\uff0829K\u6837\u672c\u7684\u8bc4\u4f30\u57fa\u51c6\uff09\u3002", "result": "POLYGUARD\u5728\u591a\u4e2a\u5b89\u5168\u548c\u6bd2\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u73b0\u6709\u6700\u4f73\u5f00\u6e90\u548c\u5546\u4e1a\u5b89\u5168\u5206\u7c7b\u5668\u9ad8\u51fa5.5%\u3002", "conclusion": "POLYGUARD\u7684\u8d21\u732e\u63a8\u52a8\u4e86\u5168\u7403\u7528\u6237\u4f7f\u7528\u66f4\u5b89\u5168\u7684\u591a\u8bed\u8a00LLM\u7684\u52aa\u529b\u3002"}}
{"id": "2504.04323", "pdf": "https://arxiv.org/pdf/2504.04323", "abs": "https://arxiv.org/abs/2504.04323", "authors": ["Yiming Shi", "Shaoshuai Yang", "Xun Zhu", "Haoyu Wang", "Miao Li", "Ji Wu"], "title": "MedM-VL: What Makes a Good Medical LVLM?", "categories": ["cs.CV"], "comment": null, "summary": "Medical image analysis is a fundamental component. As deep learning\nprogresses, the focus has shifted from single-task applications, such as\nclassification and segmentation, to more complex multimodal tasks, including\nmedical visual question answering and report generation. Traditional shallow\nand task-specific models are increasingly limited in addressing the complexity\nand scalability required in clinical practice. The emergence of large language\nmodels (LLMs) has driven the development of medical Large Vision-Language\nModels (LVLMs), offering a unified solution for diverse vision-language tasks.\nIn this study, we investigate various architectural designs for medical LVLMs\nbased on the widely adopted LLaVA framework, which follows an\nencoder-connector-LLM paradigm. We construct two distinct models targeting 2D\nand 3D modalities, respectively. These models are designed to support both\ngeneral-purpose medical tasks and domain-specific fine-tuning, thereby serving\nas effective foundation models. To facilitate reproducibility and further\nresearch, we develop a modular and extensible codebase, MedM-VL, and release\ntwo LVLM variants: MedM-VL-2D for 2D medical image analysis and\nMedM-VL-CT-Chest for 3D CT-based applications. The code and models are\navailable at: https://github.com/MSIIP/MedM-VL", "AI": {"task": "\u7814\u7a76\u57fa\u4e8eLLaVA\u6846\u67b6\u7684\u533b\u5b66\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u4ee5\u652f\u63012D\u548c3D\u533b\u5b66\u56fe\u50cf\u7684\u591a\u6a21\u6001\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u6d45\u5c42\u548c\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u96be\u4ee5\u6ee1\u8db3\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u590d\u6742\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u9700\u6c42\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\u63a8\u52a8\u4e86\u533b\u5b66LVLMs\u7684\u7814\u53d1\u3002", "method": "\u57fa\u4e8eLLaVA\u6846\u67b6\uff0c\u8bbe\u8ba1\u5e76\u6784\u5efa\u4e86\u4e24\u79cd\u9488\u5bf92D\u548c3D\u6a21\u6001\u7684\u533b\u5b66LVLMs\u6a21\u578b\uff0c\u652f\u6301\u901a\u7528\u533b\u5b66\u4efb\u52a1\u548c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u3002", "result": "\u5f00\u53d1\u4e86\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u7684\u4ee3\u7801\u5e93MedM-VL\uff0c\u5e76\u53d1\u5e03\u4e86\u4e24\u79cdLVLM\u53d8\u4f53\uff1aMedM-VL-2D\u548cMedM-VL-CT-Chest\u3002", "conclusion": "\u533b\u5b66LVLMs\u4e3a\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u63d0\u4f9b\u4e86\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2504.04385", "pdf": "https://arxiv.org/pdf/2504.04385", "abs": "https://arxiv.org/abs/2504.04385", "authors": ["Xiaokai Wang", "Guiran Liu", "Binrong Zhu", "Jacky He", "Hongye Zheng", "Hanlu Zhang"], "title": "Pre-trained Language Models and Few-shot Learning for Medical Entity Extraction", "categories": ["cs.CL"], "comment": null, "summary": "This study proposes a medical entity extraction method based on Transformer\nto enhance the information extraction capability of medical literature.\nConsidering the professionalism and complexity of medical texts, we compare the\nperformance of different pre-trained language models (BERT, BioBERT,\nPubMedBERT, ClinicalBERT) in medical entity extraction tasks. Experimental\nresults show that PubMedBERT achieves the best performance (F1-score = 88.8%),\nindicating that a language model pre-trained on biomedical literature is more\neffective in the medical domain. In addition, we analyze the impact of\ndifferent entity extraction methods (CRF, Span-based, Seq2Seq) and find that\nthe Span-based approach performs best in medical entity extraction tasks\n(F1-score = 88.6%). It demonstrates superior accuracy in identifying entity\nboundaries. In low-resource scenarios, we further explore the application of\nFew-shot Learning in medical entity extraction. Experimental results show that\neven with only 10-shot training samples, the model achieves an F1-score of\n79.1%, verifying the effectiveness of Few-shot Learning under limited data\nconditions. This study confirms that the combination of pre-trained language\nmodels and Few-shot Learning can enhance the accuracy of medical entity\nextraction. Future research can integrate knowledge graphs and active learning\nstrategies to improve the model's generalization and stability, providing a\nmore effective solution for medical NLP research. Keywords- Natural Language\nProcessing, medical named entity recognition, pre-trained language model,\nFew-shot Learning, information extraction, deep learning", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u533b\u5b66\u5b9e\u4f53\u62bd\u53d6\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u533b\u5b66\u6587\u732e\u7684\u4fe1\u606f\u62bd\u53d6\u80fd\u529b\u3002", "motivation": "\u8003\u8651\u5230\u533b\u5b66\u6587\u672c\u7684\u4e13\u4e1a\u6027\u548c\u590d\u6742\u6027\uff0c\u7814\u7a76\u65e8\u5728\u6bd4\u8f83\u4e0d\u540c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u5b9e\u4f53\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63a2\u7d22Few-shot Learning\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u5e94\u7528\u3002", "method": "\u6bd4\u8f83\u4e86BERT\u3001BioBERT\u3001PubMedBERT\u548cClinicalBERT\u7b49\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u53caCRF\u3001Span-based\u548cSeq2Seq\u7b49\u5b9e\u4f53\u62bd\u53d6\u65b9\u6cd5\uff0c\u5e76\u7814\u7a76\u4e86Few-shot Learning\u7684\u6548\u679c\u3002", "result": "PubMedBERT\u5728\u533b\u5b66\u5b9e\u4f53\u62bd\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff08F1-score = 88.8%\uff09\uff0cSpan-based\u65b9\u6cd5\u5728\u5b9e\u4f53\u8fb9\u754c\u8bc6\u522b\u4e0a\u8868\u73b0\u6700\u4f18\uff08F1-score = 88.6%\uff09\uff0cFew-shot Learning\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u4ecd\u80fd\u8fbe\u523079.1%\u7684F1-score\u3002", "conclusion": "\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548cFew-shot Learning\u53ef\u63d0\u5347\u533b\u5b66\u5b9e\u4f53\u62bd\u53d6\u7684\u51c6\u786e\u6027\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2504.04339", "pdf": "https://arxiv.org/pdf/2504.04339", "abs": "https://arxiv.org/abs/2504.04339", "authors": ["Peng Gao", "Yujian Lee", "Zailong Chen", "Hui zhang", "Xubo Liu", "Yiyang Hu", "Guquang Jing"], "title": "NCL-CIR: Noise-aware Contrastive Learning for Composed Image Retrieval", "categories": ["cs.CV"], "comment": "Has been accepted by ICASSP2025", "summary": "Composed Image Retrieval (CIR) seeks to find a target image using a\nmulti-modal query, which combines an image with modification text to pinpoint\nthe target. While recent CIR methods have shown promise, they mainly focus on\nexploring relationships between the query pairs (image and text) through data\naugmentation or model design. These methods often assume perfect alignment\nbetween queries and target images, an idealized scenario rarely encountered in\npractice. In reality, pairs are often partially or completely mismatched due to\nissues like inaccurate modification texts, low-quality target images, and\nannotation errors. Ignoring these mismatches leads to numerous False Positive\nPair (FFPs) denoted as noise pairs in the dataset, causing the model to overfit\nand ultimately reducing its performance. To address this problem, we propose\nthe Noise-aware Contrastive Learning for CIR (NCL-CIR), comprising two key\ncomponents: the Weight Compensation Block (WCB) and the Noise-pair Filter Block\n(NFB). The WCB coupled with diverse weight maps can ensure more stable token\nrepresentations of multi-modal queries and target images. Meanwhile, the NFB,\nin conjunction with the Gaussian Mixture Model (GMM) predicts noise pairs by\nevaluating loss distributions, and generates soft labels correspondingly,\nallowing for the design of the soft-label based Noise Contrastive Estimation\n(NCE) loss function. Consequently, the overall architecture helps to mitigate\nthe influence of mismatched and partially matched samples, with experimental\nresults demonstrating that NCL-CIR achieves exceptional performance on the\nbenchmark datasets.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u566a\u58f0\u611f\u77e5\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff08NCL-CIR\uff09\u6765\u89e3\u51b3\u7ec4\u5408\u56fe\u50cf\u68c0\u7d22\uff08CIR\uff09\u4e2d\u67e5\u8be2\u5bf9\u4e0e\u76ee\u6807\u56fe\u50cf\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709CIR\u65b9\u6cd5\u5047\u8bbe\u67e5\u8be2\u5bf9\u4e0e\u76ee\u6807\u56fe\u50cf\u5b8c\u7f8e\u5bf9\u9f50\uff0c\u800c\u73b0\u5b9e\u4e2d\u5e38\u56e0\u6587\u672c\u4e0d\u51c6\u786e\u3001\u56fe\u50cf\u8d28\u91cf\u4f4e\u6216\u6807\u6ce8\u9519\u8bef\u5bfc\u81f4\u4e0d\u5339\u914d\uff0c\u4ea7\u751f\u566a\u58f0\u5bf9\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51faNCL-CIR\uff0c\u5305\u542b\u6743\u91cd\u8865\u507f\u5757\uff08WCB\uff09\u548c\u566a\u58f0\u5bf9\u8fc7\u6ee4\u5757\uff08NFB\uff09\uff0c\u7ed3\u5408\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u9884\u6d4b\u566a\u58f0\u5bf9\u5e76\u8bbe\u8ba1\u8f6f\u6807\u7b7e\u7684\u566a\u58f0\u5bf9\u6bd4\u4f30\u8ba1\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660eNCL-CIR\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u6548\u51cf\u5c11\u4e0d\u5339\u914d\u6837\u672c\u7684\u5f71\u54cd\u3002", "conclusion": "NCL-CIR\u901a\u8fc7\u566a\u58f0\u611f\u77e5\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86CIR\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u566a\u58f0\u5bf9\u95ee\u9898\u3002"}}
{"id": "2504.04444", "pdf": "https://arxiv.org/pdf/2504.04444", "abs": "https://arxiv.org/abs/2504.04444", "authors": ["Daniel Bershatsky", "Ivan Oseledets"], "title": "On the Spatial Structure of Mixture-of-Experts in Transformers", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ICLR 2025 Workshop on Sparsity in LLMs (SLLM)", "summary": "A common assumption is that MoE routers primarily leverage semantic features\nfor expert selection. However, our study challenges this notion by\ndemonstrating that positional token information also plays a crucial role in\nrouting decisions. Through extensive empirical analysis, we provide evidence\nsupporting this hypothesis, develop a phenomenological explanation of the\nobserved behavior, and discuss practical implications for MoE-based\narchitectures.", "AI": {"task": "\u7814\u7a76MoE\u8def\u7531\u5668\u5728\u4e13\u5bb6\u9009\u62e9\u4e2d\u662f\u5426\u4e3b\u8981\u4f9d\u8d56\u8bed\u4e49\u7279\u5f81\u3002", "motivation": "\u6311\u6218MoE\u8def\u7531\u5668\u4ec5\u4f9d\u8d56\u8bed\u4e49\u7279\u5f81\u7684\u5e38\u89c1\u5047\u8bbe\uff0c\u63a2\u7d22\u4f4d\u7f6e\u6807\u8bb0\u4fe1\u606f\u5728\u8def\u7531\u51b3\u7b56\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u5206\u6790\u9a8c\u8bc1\u5047\u8bbe\uff0c\u5e76\u63d0\u51fa\u73b0\u8c61\u5b66\u89e3\u91ca\u3002", "result": "\u53d1\u73b0\u4f4d\u7f6e\u6807\u8bb0\u4fe1\u606f\u5728\u8def\u7531\u51b3\u7b56\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u57fa\u4e8eMoE\u7684\u67b6\u6784\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002"}}
{"id": "2504.04340", "pdf": "https://arxiv.org/pdf/2504.04340", "abs": "https://arxiv.org/abs/2504.04340", "authors": ["Ying Zhao"], "title": "AnomalyHybrid: A Domain-agnostic Generative Framework for General Anomaly Detection", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025 workshop on Harnessing Generative Models for\n  Synthetic Visual Datasets (SyntaGen)", "summary": "Anomaly generation is an effective way to mitigate data scarcity for anomaly\ndetection task. Most existing works shine at industrial anomaly generation with\nmultiple specialists or large generative models, rarely generalizing to\nanomalies in other applications. In this paper, we present AnomalyHybrid, a\ndomain-agnostic framework designed to generate authentic and diverse anomalies\nsimply by combining the reference and target images. AnomalyHybrid is a\nGenerative Adversarial Network(GAN)-based framework having two decoders that\nintegrate the appearance of reference image into the depth and edge structures\nof target image respectively. With the help of depth decoders, AnomalyHybrid\nachieves authentic generation especially for the anomalies with depth values\nchanging, such a s protrusion and dent. More, it relaxes the fine granularity\nstructural control of the edge decoder and brings more diversity. Without using\nannotations, AnomalyHybrid is easily trained with sets of color, depth and edge\nof same images having different augmentations. Extensive experiments carried on\nHeliconiusButterfly, MVTecAD and MVTec3D datasets demonstrate that\nAnomalyHybrid surpasses the GAN-based state-of-the-art on anomaly generation\nand its downstream anomaly classification, detection and segmentation tasks. On\nMVTecAD dataset, AnomalyHybrid achieves 2.06/0.32 IS/LPIPS for anomaly\ngeneration, 52.6 Acc for anomaly classification with ResNet34, 97.3/72.9 AP for\nimage/pixel-level anomaly detection with a simple UNet.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u9886\u57df\u65e0\u5173\u7684\u5f02\u5e38\u751f\u6210\u6846\u67b6AnomalyHybrid\uff0c\u901a\u8fc7\u7ed3\u5408\u53c2\u8003\u56fe\u50cf\u548c\u76ee\u6807\u56fe\u50cf\u751f\u6210\u771f\u5b9e\u4e14\u591a\u6837\u5316\u7684\u5f02\u5e38\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u4e13\u6ce8\u4e8e\u5de5\u4e1a\u5f02\u5e38\u751f\u6210\uff0c\u96be\u4ee5\u63a8\u5e7f\u5230\u5176\u4ed6\u5e94\u7528\u9886\u57df\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u901a\u7528\u7684\u5f02\u5e38\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eGAN\u7684\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u89e3\u7801\u5668\uff0c\u5206\u522b\u5c06\u53c2\u8003\u56fe\u50cf\u7684\u5916\u89c2\u6574\u5408\u5230\u76ee\u6807\u56fe\u50cf\u7684\u6df1\u5ea6\u548c\u8fb9\u7f18\u7ed3\u6784\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u8d28\u91cf\u9ad8\uff0c\u4e14\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u5206\u5272\uff09\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "AnomalyHybrid\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u5f02\u5e38\u751f\u6210\u65b9\u6cd5\uff0c\u65e0\u9700\u6807\u6ce8\u5373\u53ef\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2504.04462", "pdf": "https://arxiv.org/pdf/2504.04462", "abs": "https://arxiv.org/abs/2504.04462", "authors": ["David Herrera-Poyatos", "Carlos Pel\u00e1ez-Gonz\u00e1lez", "Cristina Zuheros", "Andr\u00e9s Herrera-Poyatos", "Virilo Tejedor", "Francisco Herrera", "Rosana Montes"], "title": "An overview of model uncertainty and variability in LLM-based sentiment analysis. Challenges, mitigation strategies and the role of explainability", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages and 3 figures", "summary": "Large Language Models (LLMs) have significantly advanced sentiment analysis,\nyet their inherent uncertainty and variability pose critical challenges to\nachieving reliable and consistent outcomes. This paper systematically explores\nthe Model Variability Problem (MVP) in LLM-based sentiment analysis,\ncharacterized by inconsistent sentiment classification, polarization, and\nuncertainty arising from stochastic inference mechanisms, prompt sensitivity,\nand biases in training data. We analyze the core causes of MVP, presenting\nillustrative examples and a case study to highlight its impact. In addition, we\ninvestigate key challenges and mitigation strategies, paying particular\nattention to the role of temperature as a driver of output randomness and\nemphasizing the crucial role of explainability in improving transparency and\nuser trust. By providing a structured perspective on stability,\nreproducibility, and trustworthiness, this study helps develop more reliable,\nexplainable, and robust sentiment analysis models, facilitating their\ndeployment in high-stakes domains such as finance, healthcare, and\npolicymaking, among others.", "AI": {"task": "\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u60c5\u611f\u5206\u6790\u4e2d\u7684\u6a21\u578b\u53d8\u5f02\u6027\u95ee\u9898\uff08MVP\uff09\u3002", "motivation": "LLMs\u5728\u60c5\u611f\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u53d8\u5f02\u6027\uff0c\u8fd9\u5bf9\u5b9e\u73b0\u53ef\u9760\u548c\u4e00\u81f4\u7684\u7ed3\u679c\u63d0\u51fa\u4e86\u5173\u952e\u6311\u6218\u3002", "method": "\u5206\u6790MVP\u7684\u6838\u5fc3\u539f\u56e0\uff0c\u63d0\u4f9b\u793a\u4f8b\u548c\u6848\u4f8b\u7814\u7a76\uff0c\u5e76\u63a2\u8ba8\u5173\u952e\u6311\u6218\u548c\u7f13\u89e3\u7b56\u7565\uff0c\u7279\u522b\u662f\u6e29\u5ea6\u53c2\u6570\u7684\u4f5c\u7528\u548c\u53ef\u89e3\u91ca\u6027\u7684\u91cd\u8981\u6027\u3002", "result": "\u901a\u8fc7\u7ed3\u6784\u5316\u89c6\u89d2\uff0c\u63d0\u9ad8\u4e86\u60c5\u611f\u5206\u6790\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u7814\u7a76\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u548c\u7a33\u5065\u7684\u60c5\u611f\u5206\u6790\u6a21\u578b\uff0c\u63a8\u52a8\u5176\u5728\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2504.04348", "pdf": "https://arxiv.org/pdf/2504.04348", "abs": "https://arxiv.org/abs/2504.04348", "authors": ["Shihao Wang", "Zhiding Yu", "Xiaohui Jiang", "Shiyi Lan", "Min Shi", "Nadine Chang", "Jan Kautz", "Ying Li", "Jose M. Alvarez"], "title": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The advances in vision-language models (VLMs) have led to a growing interest\nin autonomous driving to leverage their strong reasoning capabilities. However,\nextending these capabilities from 2D to full 3D understanding is crucial for\nreal-world applications. To address this challenge, we propose OmniDrive, a\nholistic vision-language dataset that aligns agent models with 3D driving tasks\nthrough counterfactual reasoning. This approach enhances decision-making by\nevaluating potential scenarios and their outcomes, similar to human drivers\nconsidering alternative actions. Our counterfactual-based synthetic data\nannotation process generates large-scale, high-quality datasets, providing\ndenser supervision signals that bridge planning trajectories and language-based\nreasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely\nOmni-L and Omni-Q, to assess the importance of vision-language alignment versus\n3D perception, revealing critical insights into designing effective LLM-agents.\nSignificant improvements on the DriveLM Q\\&A benchmark and nuScenes open-loop\nplanning demonstrate the effectiveness of our dataset and methods.", "AI": {"task": "\u63d0\u51faOmniDrive\u6570\u636e\u96c6\u548c\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63a8\u7406\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u4ece2D\u6269\u5c55\u52303D\u9a7e\u9a76\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57282D\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u57283D\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4ecd\u9700\u63d0\u5347\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u7406\u89e3\u548c\u51b3\u7b56\u3002", "method": "\u63d0\u51faOmniDrive\u6570\u636e\u96c6\uff0c\u91c7\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\uff0c\u5e76\u8bbe\u8ba1Omni-L\u548cOmni-Q\u4e24\u79cd\u6846\u67b6\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u4e0e3D\u611f\u77e5\u7684\u91cd\u8981\u6027\u3002", "result": "\u5728DriveLM Q&A\u57fa\u51c6\u548cnuScenes\u5f00\u73af\u89c4\u5212\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "OmniDrive\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e863D\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u4e3aLLM-agent\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2504.04473", "pdf": "https://arxiv.org/pdf/2504.04473", "abs": "https://arxiv.org/abs/2504.04473", "authors": ["Archana Sahu", "Plaban Kumar Bhowmick"], "title": "Directed Graph-alignment Approach for Identification of Gaps in Short Answers", "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 11 figures", "summary": "In this paper, we have presented a method for identifying missing items known\nas gaps in the student answers by comparing them against the corresponding\nmodel answer/reference answers, automatically. The gaps can be identified at\nword, phrase or sentence level. The identified gaps are useful in providing\nfeedback to the students for formative assessment. The problem of gap\nidentification has been modelled as an alignment of a pair of directed graphs\nrepresenting a student answer and the corresponding model answer for a given\nquestion. To validate the proposed approach, the gap annotated student answers\nconsidering answers from three widely known datasets in the short answer\ngrading domain, namely, University of North Texas (UNT), SciEntsBank, and\nBeetle have been developed and this gap annotated student answers' dataset is\navailable at: https://github.com/sahuarchana7/gaps-answers-dataset. Evaluation\nmetrics used in the traditional machine learning tasks have been adopted to\nevaluate the task of gap identification. Though performance of the proposed\napproach varies across the datasets and the types of the answers, overall the\nperformance is observed to be promising.", "AI": {"task": "\u81ea\u52a8\u8bc6\u522b\u5b66\u751f\u7b54\u6848\u4e0e\u53c2\u8003\u7b54\u6848\u4e4b\u95f4\u7684\u7f3a\u5931\u9879\uff08\u5373\u201cgap\u201d\uff09\uff0c\u7528\u4e8e\u5f62\u6210\u6027\u8bc4\u4f30\u3002", "motivation": "\u901a\u8fc7\u8bc6\u522b\u5b66\u751f\u7b54\u6848\u4e2d\u7684\u7f3a\u5931\u9879\uff0c\u4e3a\u5b66\u751f\u63d0\u4f9b\u53cd\u9988\uff0c\u5e2e\u52a9\u6539\u8fdb\u5b66\u4e60\u6548\u679c\u3002", "method": "\u5c06\u5b66\u751f\u7b54\u6848\u4e0e\u53c2\u8003\u7b54\u6848\u5efa\u6a21\u4e3a\u4e00\u5bf9\u6709\u5411\u56fe\uff0c\u901a\u8fc7\u56fe\u5bf9\u9f50\u65b9\u6cd5\u8bc6\u522b\u7f3a\u5931\u9879\uff08\u5355\u8bcd\u3001\u77ed\u8bed\u6216\u53e5\u5b50\u7ea7\u522b\uff09\u3002", "result": "\u5728\u4e0d\u540c\u6570\u636e\u96c6\uff08UNT\u3001SciEntsBank\u3001Beetle\uff09\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u6027\u80fd\u56e0\u6570\u636e\u96c6\u548c\u7b54\u6848\u7c7b\u578b\u800c\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8bc6\u522b\u7f3a\u5931\u9879\u65b9\u9762\u8868\u73b0\u6709\u6f5c\u529b\uff0c\u53ef\u7528\u4e8e\u5b66\u751f\u53cd\u9988\u3002"}}
{"id": "2504.04423", "pdf": "https://arxiv.org/pdf/2504.04423", "abs": "https://arxiv.org/abs/2504.04423", "authors": ["Yang Jiao", "Haibo Qiu", "Zequn Jie", "Shaoxiang Chen", "Jingjing Chen", "Lin Ma", "Yu-Gang Jiang"], "title": "UniToken: Harmonizing Multimodal Understanding and Generation through Unified Visual Encoding", "categories": ["cs.CV", "cs.AI"], "comment": "Accpeted to CVPR 2025 workshop", "summary": "We introduce UniToken, an auto-regressive generation model that encodes\nvisual inputs through a combination of discrete and continuous representations,\nenabling seamless integration of unified visual understanding and image\ngeneration tasks. Unlike previous approaches that rely on unilateral visual\nrepresentations, our unified visual encoding framework captures both high-level\nsemantics and low-level details, delivering multidimensional information that\nempowers heterogeneous tasks to selectively assimilate domain-specific\nknowledge based on their inherent characteristics. Through in-depth\nexperiments, we uncover key principles for developing a unified model capable\nof both visual understanding and image generation. Extensive evaluations across\na diverse range of prominent benchmarks demonstrate that UniToken achieves\nstate-of-the-art performance, surpassing existing approaches. These results\nestablish UniToken as a robust foundation for future research in this domain.\nThe code and models are available at https://github.com/SxJyJay/UniToken.", "AI": {"task": "\u63d0\u51faUniToken\uff0c\u4e00\u79cd\u7ed3\u5408\u79bb\u6563\u548c\u8fde\u7eed\u8868\u793a\u7684\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u7edf\u4e00\u89c6\u89c9\u7406\u89e3\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5355\u4e00\u89c6\u89c9\u8868\u793a\uff0c\u65e0\u6cd5\u540c\u65f6\u6355\u6349\u9ad8\u5c42\u8bed\u4e49\u548c\u4f4e\u5c42\u7ec6\u8282\uff0c\u9650\u5236\u4e86\u591a\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7edf\u4e00\u7684\u89c6\u89c9\u7f16\u7801\u6846\u67b6\uff0c\u7ed3\u5408\u79bb\u6563\u548c\u8fde\u7eed\u8868\u793a\uff0c\u652f\u6301\u591a\u7ef4\u4fe1\u606f\u63d0\u53d6\u548c\u9009\u62e9\u6027\u77e5\u8bc6\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u7edf\u4e00\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "UniToken\u4e3a\u89c6\u89c9\u7406\u89e3\u548c\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u5f3a\u5927\u7684\u57fa\u7840\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.04514", "pdf": "https://arxiv.org/pdf/2504.04514", "abs": "https://arxiv.org/abs/2504.04514", "authors": ["Yao Tao", "Yehui Tang", "Yun Wang", "Mingjian Zhu", "Hailin Hu", "Yunhe Wang"], "title": "Saliency-driven Dynamic Token Pruning for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u663e\u8457\u6027\u9a71\u52a8\u7684\u52a8\u6001\u4ee4\u724c\u4fee\u526a\u6846\u67b6\uff08SDTP\uff09\uff0c\u4ee5\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u957f\u5e8f\u5217\u63a8\u7406\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u7531\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\uff0cLLMs\u5728\u957f\u5e8f\u5217\u63a8\u7406\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u4e14\u5e76\u975e\u6240\u6709\u4ee4\u724c\u5bf9\u63a8\u7406\u8d21\u732e\u76f8\u540c\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u663e\u8457\u6027\u9a71\u52a8\u9884\u6d4b\u6a21\u5757\uff0c\u901a\u8fc7\u9690\u85cf\u72b6\u6001\u4f30\u8ba1\u4ee4\u724c\u91cd\u8981\u6027\u5206\u6570\uff0c\u5e76\u5728\u4e0d\u540c\u5c42\u7ea7\u52a8\u6001\u4fee\u526a\u5197\u4f59\u4ee4\u724c\uff0c\u540c\u65f6\u63d0\u51fa\u57fa\u4e8e\u6392\u540d\u7684\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSDTP\u80fd\u4fee\u526a65%\u7684\u8f93\u5165\u4ee4\u724c\uff0c\u51cf\u5c1133%~47%\u7684FLOPs\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53471.75\u500d\uff0c\u6027\u80fd\u63a5\u8fd1\u539f\u6a21\u578b\u3002", "conclusion": "SDTP\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u4ee4\u724c\u4fee\u526a\u65b9\u6cd5\uff0c\u53ef\u4e0e\u5176\u4ed6\u538b\u7f29\u6280\u672f\u7ed3\u5408\u8fdb\u4e00\u6b65\u4f18\u5316\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2504.04427", "pdf": "https://arxiv.org/pdf/2504.04427", "abs": "https://arxiv.org/abs/2504.04427", "authors": ["Shiyan Liu", "Rui Qu", "Yan Jin"], "title": "FluentLip: A Phonemes-Based Two-stage Approach for Audio-Driven Lip Synthesis with Optical Flow Consistency", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Generating consecutive images of lip movements that align with a given speech\nin audio-driven lip synthesis is a challenging task. While previous studies\nhave made strides in synchronization and visual quality, lip intelligibility\nand video fluency remain persistent challenges. This work proposes FluentLip, a\ntwo-stage approach for audio-driven lip synthesis, incorporating three featured\nstrategies. To improve lip synchronization and intelligibility, we integrate a\nphoneme extractor and encoder to generate a fusion of audio and phoneme\ninformation for multimodal learning. Additionally, we employ optical flow\nconsistency loss to ensure natural transitions between image frames.\nFurthermore, we incorporate a diffusion chain during the training of Generative\nAdversarial Networks (GANs) to improve both stability and efficiency. We\nevaluate our proposed FluentLip through extensive experiments, comparing it\nwith five state-of-the-art (SOTA) approaches across five metrics, including a\nproposed metric called Phoneme Error Rate (PER) that evaluates lip pose\nintelligibility and video fluency. The experimental results demonstrate that\nour FluentLip approach is highly competitive, achieving significant\nimprovements in smoothness and naturalness. In particular, it outperforms these\nSOTA approaches by approximately $\\textbf{16.3%}$ in Fr\\'echet Inception\nDistance (FID) and $\\textbf{35.2%}$ in PER.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aFluentLip\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u7528\u4e8e\u97f3\u9891\u9a71\u52a8\u7684\u5507\u90e8\u5408\u6210\uff0c\u4ee5\u89e3\u51b3\u5507\u90e8\u540c\u6b65\u3001\u6e05\u6670\u5ea6\u548c\u89c6\u9891\u6d41\u7545\u6027\u7684\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u5148\u524d\u7814\u7a76\u5728\u540c\u6b65\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5507\u90e8\u6e05\u6670\u5ea6\u548c\u89c6\u9891\u6d41\u7545\u6027\u4ecd\u662f\u6301\u7eed\u5b58\u5728\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u97f3\u7d20\u63d0\u53d6\u5668\u548c\u7f16\u7801\u5668\u8fdb\u884c\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u4f7f\u7528\u5149\u6d41\u4e00\u81f4\u6027\u635f\u5931\u786e\u4fdd\u5e27\u95f4\u81ea\u7136\u8fc7\u6e21\uff0c\u5e76\u5728GAN\u8bad\u7ec3\u4e2d\u5f15\u5165\u6269\u6563\u94fe\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFluentLip\u5728\u5e73\u6ed1\u6027\u548c\u81ea\u7136\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u4e94\u79cdSOTA\u65b9\u6cd5\uff0cFID\u548cPER\u5206\u522b\u63d0\u534716.3%\u548c35.2%\u3002", "conclusion": "FluentLip\u5728\u97f3\u9891\u9a71\u52a8\u7684\u5507\u90e8\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u540c\u6b65\u6027\u3001\u6e05\u6670\u5ea6\u548c\u6d41\u7545\u6027\u3002"}}
{"id": "2504.04534", "pdf": "https://arxiv.org/pdf/2504.04534", "abs": "https://arxiv.org/abs/2504.04534", "authors": ["Anantharaman Janakiraman", "Behnaz Ghoraani"], "title": "An Empirical Comparison of Text Summarization: A Multi-Dimensional Evaluation of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Text summarization is crucial for mitigating information overload across\ndomains like journalism, medicine, and business. This research evaluates\nsummarization performance across 17 large language models (OpenAI, Google,\nAnthropic, open-source) using a novel multi-dimensional framework. We assessed\nmodels on seven diverse datasets (BigPatent, BillSum, CNN/DailyMail, PubMed,\nSAMSum, WikiHow, XSum) at three output lengths (50, 100, 150 tokens) using\nmetrics for factual consistency, semantic similarity, lexical overlap, and\nhuman-like quality, while also considering efficiency factors. Our findings\nreveal significant performance differences, with specific models excelling in\nfactual accuracy (deepseek-v3), human-like quality (claude-3-5-sonnet), and\nprocessing efficiency/cost-effectiveness (gemini-1.5-flash, gemini-2.0-flash).\nPerformance varies dramatically by dataset, with models struggling on technical\ndomains but performing well on conversational content. We identified a critical\ntension between factual consistency (best at 50 tokens) and perceived quality\n(best at 150 tokens). Our analysis provides evidence-based recommendations for\ndifferent use cases, from high-stakes applications requiring factual accuracy\nto resource-constrained environments needing efficient processing. This\ncomprehensive approach enhances evaluation methodology by integrating quality\nmetrics with operational considerations, incorporating trade-offs between\naccuracy, efficiency, and cost-effectiveness to guide model selection for\nspecific applications.", "AI": {"task": "\u8bc4\u4f3017\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u5e76\u4e3a\u4e0d\u540c\u9886\u57df\uff08\u5982\u65b0\u95fb\u3001\u533b\u5b66\u3001\u5546\u4e1a\uff09\u63d0\u4f9b\u9ad8\u6548\u7684\u6458\u8981\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u591a\u7ef4\u5ea6\u6846\u67b6\u8bc4\u4f30\u6a21\u578b\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u6db5\u76d6\u4e0d\u540c\u8f93\u51fa\u957f\u5ea6\u548c\u591a\u79cd\u6307\u6807\uff08\u4e8b\u5b9e\u4e00\u81f4\u6027\u3001\u8bed\u4e49\u76f8\u4f3c\u6027\u3001\u8bcd\u6c47\u91cd\u53e0\u3001\u4eba\u7c7b\u611f\u77e5\u8d28\u91cf\u53ca\u6548\u7387\u56e0\u7d20\uff09\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u67d0\u4e9b\u6a21\u578b\u5728\u7279\u5b9a\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff08\u5982\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u4eba\u7c7b\u611f\u77e5\u8d28\u91cf\u3001\u5904\u7406\u6548\u7387\uff09\u3002\u6027\u80fd\u56e0\u6570\u636e\u96c6\u800c\u5f02\uff0c\u6280\u672f\u9886\u57df\u8868\u73b0\u8f83\u5dee\uff0c\u5bf9\u8bdd\u5185\u5bb9\u8868\u73b0\u8f83\u597d\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u6a21\u578b\u9009\u62e9\u5efa\u8bae\uff0c\u5e76\u6539\u8fdb\u4e86\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7efc\u5408\u8003\u8651\u4e86\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002"}}
{"id": "2504.04435", "pdf": "https://arxiv.org/pdf/2504.04435", "abs": "https://arxiv.org/abs/2504.04435", "authors": ["Tatiana Merkulova", "Bharani Jayakumar"], "title": "Evaluation framework for Image Segmentation Algorithms", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a comprehensive evaluation framework for image\nsegmentation algorithms, encompassing naive methods, machine learning\napproaches, and deep learning techniques. We begin by introducing the\nfundamental concepts and importance of image segmentation, and the role of\ninteractive segmentation in enhancing accuracy. A detailed background theory\nsection explores various segmentation methods, including thresholding, edge\ndetection, region growing, feature extraction, random forests, support vector\nmachines, convolutional neural networks, U-Net, and Mask R-CNN. The\nimplementation and experimental setup are thoroughly described, highlighting\nthree primary approaches: algorithm assisting user, user assisting algorithm,\nand hybrid methods. Evaluation metrics such as Intersection over Union (IoU),\ncomputation time, and user interaction time are employed to measure\nperformance. A comparative analysis presents detailed results, emphasizing the\nstrengths, limitations, and trade-offs of each method. The paper concludes with\ninsights into the practical applicability of these approaches across various\nscenarios and outlines future work, focusing on expanding datasets, developing\nmore representative approaches, integrating real-time feedback, and exploring\nweakly supervised and self-supervised learning paradigms to enhance\nsegmentation accuracy and efficiency. Keywords: Image Segmentation, Interactive\nSegmentation, Machine Learning, Deep Learning, Computer Vision", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u5168\u9762\u7684\u56fe\u50cf\u5206\u5272\u7b97\u6cd5\u8bc4\u4f30\u6846\u67b6\uff0c\u6db5\u76d6\u4f20\u7edf\u65b9\u6cd5\u3001\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u3002", "motivation": "\u63a2\u8ba8\u56fe\u50cf\u5206\u5272\u7684\u57fa\u672c\u6982\u5ff5\u53ca\u5176\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u4ea4\u4e92\u5f0f\u5206\u5272\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u591a\u79cd\u5206\u5272\u65b9\u6cd5\uff0c\u5305\u62ec\u9608\u503c\u6cd5\u3001\u8fb9\u7f18\u68c0\u6d4b\u3001\u533a\u57df\u751f\u957f\u3001\u7279\u5f81\u63d0\u53d6\u3001\u968f\u673a\u68ee\u6797\u3001\u652f\u6301\u5411\u91cf\u673a\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001U-Net\u548cMask R-CNN\uff0c\u5e76\u63cf\u8ff0\u4e86\u5b9e\u9a8c\u8bbe\u7f6e\u548c\u4e09\u79cd\u4e3b\u8981\u65b9\u6cd5\uff1a\u7b97\u6cd5\u8f85\u52a9\u7528\u6237\u3001\u7528\u6237\u8f85\u52a9\u7b97\u6cd5\u548c\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u3001\u8ba1\u7b97\u65f6\u95f4\u548c\u7528\u6237\u4ea4\u4e92\u65f6\u95f4\u7b49\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\uff0c\u5206\u6790\u4e86\u6bcf\u79cd\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\u548c\u6743\u8861\u3002", "conclusion": "\u603b\u7ed3\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u9002\u7528\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u5de5\u4f5c\u7684\u65b9\u5411\uff0c\u5305\u62ec\u6269\u5c55\u6570\u636e\u96c6\u3001\u5f00\u53d1\u66f4\u5177\u4ee3\u8868\u6027\u7684\u65b9\u6cd5\u3001\u96c6\u6210\u5b9e\u65f6\u53cd\u9988\u4ee5\u53ca\u63a2\u7d22\u5f31\u76d1\u7763\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\u3002"}}
{"id": "2504.04569", "pdf": "https://arxiv.org/pdf/2504.04569", "abs": "https://arxiv.org/abs/2504.04569", "authors": ["Chitranshu Harbola", "Anupam Purwar"], "title": "KnowsLM: A framework for evaluation of small language models for knowledge augmentation and humanised conversations", "categories": ["cs.CL"], "comment": null, "summary": "In the evolving landscape of conversational AI, generating concise,\ncontext-aware, and human-like dialogue using small and medium-sized language\nmodels (LLMs) remains a complex challenge. This study investigates the\ninfluence of LoRA rank, dataset scale, and prompt prefix design on both\nknowledge retention and stylistic alignment. While fine-tuning improves fluency\nand enables stylistic customization, its ability to integrate unseen knowledge\nis constrained -- particularly with smaller datasets. Conversely, RAG-augmented\nmodels, equipped to incorporate external documents at inference, demonstrated\nsuperior factual accuracy on out-of-distribution prompts, though they lacked\nthe stylistic consistency achieved by fine-tuning. Evaluations by LLM-based\njudges across knowledge accuracy, conversational quality, and conciseness\nsuggest that fine-tuning is best suited for tone adaptation, whereas RAG excels\nat real-time knowledge augmentation.", "AI": {"task": "\u7814\u7a76LoRA\u79e9\u3001\u6570\u636e\u96c6\u89c4\u6a21\u548c\u63d0\u793a\u524d\u7f00\u8bbe\u8ba1\u5bf9\u77e5\u8bc6\u4fdd\u7559\u548c\u98ce\u683c\u5bf9\u9f50\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u5bf9\u8bddAI\u4e2d\uff0c\u5982\u4f55\u5229\u7528\u4e2d\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b80\u6d01\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u4e14\u7c7b\u4eba\u7684\u5bf9\u8bdd\u662f\u4e00\u4e2a\u590d\u6742\u6311\u6218\u3002", "method": "\u7ed3\u5408\u5fae\u8c03\uff08fine-tuning\uff09\u548cRAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u65b9\u6cd5\uff0c\u8bc4\u4f30\u77e5\u8bc6\u51c6\u786e\u6027\u3001\u5bf9\u8bdd\u8d28\u91cf\u548c\u7b80\u6d01\u6027\u3002", "result": "\u5fae\u8c03\u5728\u98ce\u683c\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u800cRAG\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u4e0a\u66f4\u4f18\u3002", "conclusion": "\u5fae\u8c03\u9002\u7528\u4e8e\u98ce\u683c\u9002\u5e94\uff0cRAG\u66f4\u9002\u5408\u5b9e\u65f6\u77e5\u8bc6\u589e\u5f3a\u3002"}}
{"id": "2504.04448", "pdf": "https://arxiv.org/pdf/2504.04448", "abs": "https://arxiv.org/abs/2504.04448", "authors": ["Etienne Chassaing", "Florent Forest", "Olga Fink", "Malcolm Mielle"], "title": "Thermoxels: a voxel-based method to generate simulation-ready 3D thermal models", "categories": ["cs.CV", "eess.IV"], "comment": "7 pages, 2 figures", "summary": "In the European Union, buildings account for 42% of energy use and 35% of\ngreenhouse gas emissions. Since most existing buildings will still be in use by\n2050, retrofitting is crucial for emissions reduction. However, current\nbuilding assessment methods rely mainly on qualitative thermal imaging, which\nlimits data-driven decisions for energy savings. On the other hand,\nquantitative assessments using finite element analysis (FEA) offer precise\ninsights but require manual CAD design, which is tedious and error-prone.\nRecent advances in 3D reconstruction, such as Neural Radiance Fields (NeRF) and\nGaussian Splatting, enable precise 3D modeling from sparse images but lack\nclearly defined volumes and the interfaces between them needed for FEA. We\npropose Thermoxels, a novel voxel-based method able to generate FEA-compatible\nmodels, including both geometry and temperature, from a sparse set of RGB and\nthermal images. Using pairs of RGB and thermal images as input, Thermoxels\nrepresents a scene's geometry as a set of voxels comprising color and\ntemperature information. After optimization, a simple process is used to\ntransform Thermoxels' models into tetrahedral meshes compatible with FEA. We\ndemonstrate Thermoxels' capability to generate RGB+Thermal meshes of 3D scenes,\nsurpassing other state-of-the-art methods. To showcase the practical\napplications of Thermoxels' models, we conduct a simple heat conduction\nsimulation using FEA, achieving convergence from an initial state defined by\nThermoxels' thermal reconstruction. Additionally, we compare Thermoxels' image\nsynthesis abilities with current state-of-the-art methods, showing competitive\nresults, and discuss the limitations of existing metrics in assessing mesh\nquality.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aThermoxels\u7684\u57fa\u4e8e\u4f53\u7d20\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u7684RGB\u548c\u70ed\u56fe\u50cf\u751f\u6210\u4e0e\u6709\u9650\u5143\u5206\u6790\uff08FEA\uff09\u517c\u5bb9\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5efa\u7b51\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u5b9a\u6027\u70ed\u6210\u50cf\u6216\u624b\u52a8CAD\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u6570\u636e\u9a71\u52a8\u7684\u8282\u80fd\u51b3\u7b56\uff0c\u800cThermoxels\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528RGB\u548c\u70ed\u56fe\u50cf\u5bf9\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u4f53\u7d20\u8868\u793a\u573a\u666f\u7684\u51e0\u4f55\u548c\u6e29\u5ea6\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u4f18\u5316\u4e3aFEA\u517c\u5bb9\u7684\u56db\u9762\u4f53\u7f51\u683c\u3002", "result": "Thermoxels\u80fd\u591f\u751f\u6210RGB+\u70ed\u7f51\u683c\uff0c\u5e76\u5728\u70ed\u4f20\u5bfc\u6a21\u62df\u4e2d\u5b9e\u73b0\u6536\u655b\uff0c\u5176\u56fe\u50cf\u5408\u6210\u80fd\u529b\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "Thermoxels\u4e3a\u5efa\u7b51\u80fd\u6548\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.04616", "pdf": "https://arxiv.org/pdf/2504.04616", "abs": "https://arxiv.org/abs/2504.04616", "authors": ["Qi Zhang", "Huitong Pan", "Zhijia Chen", "Longin Jan Latecki", "Cornelia Caragea", "Eduard Dragut"], "title": "DynClean: Training Dynamics-based Label Cleaning for Distantly-Supervised Named Entity Recognition", "categories": ["cs.CL"], "comment": "Accepted to NAACL2025-Findings", "summary": "Distantly Supervised Named Entity Recognition (DS-NER) has attracted\nattention due to its scalability and ability to automatically generate labeled\ndata. However, distant annotation introduces many mislabeled instances,\nlimiting its performance. Most of the existing work attempt to solve this\nproblem by developing intricate models to learn from the noisy labels. An\nalternative approach is to attempt to clean the labeled data, thus increasing\nthe quality of distant labels. This approach has received little attention for\nNER. In this paper, we propose a training dynamics-based label cleaning\napproach, which leverages the behavior of a model as training progresses to\ncharacterize the distantly annotated samples. We also introduce an automatic\nthreshold estimation strategy to locate the errors in distant labels. Extensive\nexperimental results demonstrate that: (1) models trained on our cleaned DS-NER\ndatasets, which were refined by directly removing identified erroneous\nannotations, achieve significant improvements in F1-score, ranging from 3.18%\nto 8.95%; and (2) our method outperforms numerous advanced DS-NER approaches\nacross four datasets.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bad\u7ec3\u52a8\u6001\u7684\u6807\u7b7e\u6e05\u6d17\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u8fdc\u7a0b\u76d1\u7763\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08DS-NER\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u8fdc\u7a0b\u6807\u6ce8\u4f1a\u5f15\u5165\u5927\u91cf\u9519\u8bef\u6807\u7b7e\uff0c\u9650\u5236\u4e86DS-NER\u7684\u6027\u80fd\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u96c6\u4e2d\u4e8e\u590d\u6742\u6a21\u578b\u8bbe\u8ba1\uff0c\u800c\u6807\u7b7e\u6e05\u6d17\u65b9\u6cd5\u5728NER\u9886\u57df\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u5229\u7528\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u884c\u4e3a\u6765\u8868\u5f81\u8fdc\u7a0b\u6807\u6ce8\u6837\u672c\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u9608\u503c\u4f30\u8ba1\u7b56\u7565\u5b9a\u4f4d\u9519\u8bef\u6807\u7b7e\u3002", "result": "\u5728\u6e05\u6d17\u540e\u7684DS-NER\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u6a21\u578bF1\u5206\u6570\u663e\u8457\u63d0\u5347\uff083.18%\u81f38.95%\uff09\uff0c\u4e14\u65b9\u6cd5\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u591a\u79cd\u5148\u8fdbDS-NER\u65b9\u6cd5\u3002", "conclusion": "\u6807\u7b7e\u6e05\u6d17\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347DS-NER\u6027\u80fd\uff0c\u4e3a\u8fdc\u7a0b\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u601d\u8def\u3002"}}
{"id": "2504.04454", "pdf": "https://arxiv.org/pdf/2504.04454", "abs": "https://arxiv.org/abs/2504.04454", "authors": ["Lei Cheng", "Mahdi Saleh", "Qing Cheng", "Lu Sang", "Hongli Xu", "Daniel Cremers", "Federico Tombari"], "title": "PRISM: Probabilistic Representation for Integrated Shape Modeling and Generation", "categories": ["cs.CV"], "comment": "Project page: https://starry-lei.github.io/prism_3d_shape", "summary": "Despite the advancements in 3D full-shape generation, accurately modeling\ncomplex geometries and semantics of shape parts remains a significant\nchallenge, particularly for shapes with varying numbers of parts. Current\nmethods struggle to effectively integrate the contextual and structural\ninformation of 3D shapes into their generative processes. We address these\nlimitations with PRISM, a novel compositional approach for 3D shape generation\nthat integrates categorical diffusion models with Statistical Shape Models\n(SSM) and Gaussian Mixture Models (GMM). Our method employs compositional SSMs\nto capture part-level geometric variations and uses GMM to represent part\nsemantics in a continuous space. This integration enables both high fidelity\nand diversity in generated shapes while preserving structural coherence.\nThrough extensive experiments on shape generation and manipulation tasks, we\ndemonstrate that our approach significantly outperforms previous methods in\nboth quality and controllability of part-level operations. Our code will be\nmade publicly available.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aPRISM\u7684\u7ec4\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b33D\u5168\u5f62\u72b6\u751f\u6210\u4e2d\u590d\u6742\u51e0\u4f55\u548c\u8bed\u4e49\u5efa\u6a21\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u54083D\u5f62\u72b6\u7684\u4e0a\u4e0b\u6587\u548c\u7ed3\u6784\u4fe1\u606f\uff0c\u5bfc\u81f4\u751f\u6210\u5f62\u72b6\u7684\u8d28\u91cf\u548c\u53ef\u63a7\u6027\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u5206\u7c7b\u6269\u6563\u6a21\u578b\u3001\u7edf\u8ba1\u5f62\u72b6\u6a21\u578b\uff08SSM\uff09\u548c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\uff0c\u901a\u8fc7\u7ec4\u5408SSM\u6355\u6349\u90e8\u4ef6\u7ea7\u51e0\u4f55\u53d8\u5316\uff0c\u4f7f\u7528GMM\u8868\u793a\u90e8\u4ef6\u8bed\u4e49\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPRISM\u5728\u5f62\u72b6\u751f\u6210\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u5f62\u72b6\u5177\u6709\u9ad8\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "conclusion": "PRISM\u901a\u8fc7\u7ec4\u5408\u65b9\u6cd5\u63d0\u5347\u4e863D\u5f62\u72b6\u751f\u6210\u7684\u6027\u80fd\u548c\u53ef\u63a7\u6027\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2504.04635", "pdf": "https://arxiv.org/pdf/2504.04635", "abs": "https://arxiv.org/abs/2504.04635", "authors": ["Patrick Queiroz Da Silva", "Hari Sethuraman", "Dheeraj Rajagopal", "Hannaneh Hajishirzi", "Sachin Kumar"], "title": "Steering off Course: Reliability Challenges in Steering Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Steering methods for language models (LMs) have gained traction as\nlightweight alternatives to fine-tuning, enabling targeted modifications to\nmodel activations. However, prior studies primarily report results on a few\nmodels, leaving critical gaps in understanding the robustness of these methods.\nIn this work, we systematically examine three prominent steering methods --\nDoLa, function vectors, and task vectors. In contrast to the original studies,\nwhich evaluated a handful of models, we test up to 36 models belonging to 14\nfamilies with sizes ranging from 1.5B to 70B parameters. Our experiments reveal\nsubstantial variability in the effectiveness of the steering approaches, with a\nlarge number of models showing no improvement and at times degradation in\nsteering performance. Our analysis demonstrate fundamental flaws in the\nassumptions underlying these methods, challenging their reliability as scalable\nsteering solutions.", "AI": {"task": "\u7cfb\u7edf\u8bc4\u4f30\u4e09\u79cd\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u65b9\u6cd5\uff08DoLa\u3001\u51fd\u6570\u5411\u91cf\u548c\u4efb\u52a1\u5411\u91cf\uff09\u5728\u591a\u79cd\u6a21\u578b\u4e0a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u9488\u5bf9\u5c11\u6570\u6a21\u578b\u8bc4\u4f30\u5f15\u5bfc\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u65b9\u6cd5\u9c81\u68d2\u6027\u7684\u5168\u9762\u7406\u89e3\u3002", "method": "\u572814\u4e2a\u5bb6\u65cf\u768436\u4e2a\u6a21\u578b\uff08\u53c2\u6570\u89c4\u6a21\u4ece1.5B\u523070B\uff09\u4e0a\u6d4b\u8bd5\u4e09\u79cd\u5f15\u5bfc\u65b9\u6cd5\u3002", "result": "\u5f15\u5bfc\u65b9\u6cd5\u7684\u6548\u679c\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8bb8\u591a\u6a21\u578b\u672a\u663e\u793a\u6539\u8fdb\u751a\u81f3\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u8fd9\u4e9b\u65b9\u6cd5\u7684\u57fa\u672c\u5047\u8bbe\u5b58\u5728\u7f3a\u9677\uff0c\u6311\u6218\u4e86\u5176\u4f5c\u4e3a\u53ef\u6269\u5c55\u5f15\u5bfc\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2504.04457", "pdf": "https://arxiv.org/pdf/2504.04457", "abs": "https://arxiv.org/abs/2504.04457", "authors": ["Alejandro Fontan", "Tobias Fischer", "Javier Civera", "Michael Milford"], "title": "VSLAM-LAB: A Comprehensive Framework for Visual SLAM Methods and Datasets", "categories": ["cs.CV"], "comment": null, "summary": "Visual Simultaneous Localization and Mapping (VSLAM) research faces\nsignificant challenges due to fragmented toolchains, complex system\nconfigurations, and inconsistent evaluation methodologies. To address these\nissues, we present VSLAM-LAB, a unified framework designed to streamline the\ndevelopment, evaluation, and deployment of VSLAM systems. VSLAM-LAB simplifies\nthe entire workflow by enabling seamless compilation and configuration of VSLAM\nalgorithms, automated dataset downloading and preprocessing, and standardized\nexperiment design, execution, and evaluation--all accessible through a single\ncommand-line interface. The framework supports a wide range of VSLAM systems\nand datasets, offering broad compatibility and extendability while promoting\nreproducibility through consistent evaluation metrics and analysis tools. By\nreducing implementation complexity and minimizing configuration overhead,\nVSLAM-LAB empowers researchers to focus on advancing VSLAM methodologies and\naccelerates progress toward scalable, real-world solutions. We demonstrate the\nease with which user-relevant benchmarks can be created: here, we introduce\ndifficulty-level-based categories, but one could envision environment-specific\nor condition-specific categories.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6VSLAM-LAB\uff0c\u4ee5\u7b80\u5316VSLAM\u7cfb\u7edf\u7684\u5f00\u53d1\u3001\u8bc4\u4f30\u548c\u90e8\u7f72\u3002", "motivation": "\u89e3\u51b3VSLAM\u7814\u7a76\u4e2d\u5de5\u5177\u94fe\u5206\u6563\u3001\u7cfb\u7edf\u914d\u7f6e\u590d\u6742\u548c\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u652f\u6301VSLAM\u7b97\u6cd5\u7f16\u8bd1\u4e0e\u914d\u7f6e\u3001\u6570\u636e\u96c6\u81ea\u52a8\u4e0b\u8f7d\u4e0e\u9884\u5904\u7406\u3001\u6807\u51c6\u5316\u5b9e\u9a8c\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u7684\u7edf\u4e00\u6846\u67b6\u3002", "result": "VSLAM-LAB\u901a\u8fc7\u7b80\u5316\u5de5\u4f5c\u6d41\u7a0b\u548c\u63d0\u4f9b\u6807\u51c6\u5316\u5de5\u5177\uff0c\u63d0\u9ad8\u4e86\u7814\u7a76\u6548\u7387\u548c\u53ef\u91cd\u590d\u6027\u3002", "conclusion": "VSLAM-LAB\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u517c\u5bb9\u6027\u5f3a\u7684\u5de5\u5177\uff0c\u63a8\u52a8\u4e86VSLAM\u6280\u672f\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.04640", "pdf": "https://arxiv.org/pdf/2504.04640", "abs": "https://arxiv.org/abs/2504.04640", "authors": ["Eylon Caplan", "Tania Chakraborty", "Dan Goldwasser"], "title": "Splits! A Flexible Dataset for Evaluating a Model's Demographic Social Inference", "categories": ["cs.CL", "cs.AI"], "comment": "Under review for COLM 2025", "summary": "Understanding how people of various demographics think, feel, and express\nthemselves (collectively called group expression) is essential for social\nscience and underlies the assessment of bias in Large Language Models (LLMs).\nWhile LLMs can effectively summarize group expression when provided with\nempirical examples, coming up with generalizable theories of how a group's\nexpression manifests in real-world text is challenging. In this paper, we\ndefine a new task called Group Theorization, in which a system must write\ntheories that differentiate expression across demographic groups. We make\navailable a large dataset on this task, Splits!, constructed by splitting\nReddit posts by neutral topics (e.g. sports, cooking, and movies) and by\ndemographics (e.g. occupation, religion, and race). Finally, we suggest a\nsimple evaluation framework for assessing how effectively a method can generate\n'better' theories about group expression, backed by human validation. We\npublicly release the raw corpora and evaluation scripts for Splits! to help\nresearchers assess how methods infer--and potentially misrepresent--group\ndifferences in expression. We make Splits! and our evaluation module available\nat https://github.com/eyloncaplan/splits.", "AI": {"task": "\u5b9a\u4e49\u5e76\u89e3\u51b3\u4e00\u4e2a\u65b0\u4efb\u52a1\u201cGroup Theorization\u201d\uff0c\u5373\u7cfb\u7edf\u9700\u64b0\u5199\u80fd\u591f\u533a\u5206\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u5b66\u7fa4\u4f53\u8868\u8fbe\u7684\u7406\u8bba\u3002", "motivation": "\u7406\u89e3\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u7684\u8868\u8fbe\u65b9\u5f0f\u5bf9\u793e\u4f1a\u79d1\u5b66\u548c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u7684\u504f\u89c1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u53ef\u63a8\u5e7f\u7684\u7406\u8bba\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aSplits!\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e2d\u6027\u4e3b\u9898\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u5206\u5272Reddit\u5e16\u5b50\uff0c\u5e76\u63d0\u51fa\u7b80\u5355\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u516c\u5f00\u53d1\u5e03\u4e86Splits!\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u811a\u672c\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u8bc4\u4f30\u65b9\u6cd5\u5982\u4f55\u63a8\u65ad\u548c\u53ef\u80fd\u8bef\u8bfb\u7fa4\u4f53\u8868\u8fbe\u5dee\u5f02\u3002", "conclusion": "\u901a\u8fc7Group Theorization\u4efb\u52a1\u548cSplits!\u6570\u636e\u96c6\uff0c\u4e3a\u7814\u7a76\u7fa4\u4f53\u8868\u8fbe\u5dee\u5f02\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u548c\u65b9\u6cd5\u3002"}}
{"id": "2504.04463", "pdf": "https://arxiv.org/pdf/2504.04463", "abs": "https://arxiv.org/abs/2504.04463", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Spatial-Geometry Enhanced 3D Dynamic Snake Convolutional Neural Network for Hyperspectral Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks face several challenges in hyperspectral image\nclassification, including complex and sparse ground object distributions, small\nclustered structures, and elongated multi-branch features that often lead to\nmissing detections. To better adapt to ground object distributions and achieve\nadaptive dynamic feature responses while skipping redundant information, this\npaper proposes a Spatial-Geometry Enhanced 3D Dynamic Snake Network (SG-DSCNet)\nbased on an improved 3D-DenseNet model. The network employs Dynamic Snake\nConvolution (DSCConv), which introduces deformable offsets to enhance kernel\nflexibility through constrained self-learning, thereby improving regional\nperception of ground objects. Additionally, we propose a multi-view feature\nfusion strategy that generates multiple morphological kernel templates from\nDSCConv to observe target structures from different perspectives and achieve\nefficient feature fusion through summarizing key characteristics. This dynamic\napproach enables the model to focus more flexibly on critical spatial\nstructures when processing different regions, rather than relying on fixed\nreceptive fields of single static kernels. The DSC module enhances model\nrepresentation capability through dynamic kernel aggregation without increasing\nnetwork depth or width. Experimental results demonstrate superior performance\non the IN, UP, and KSC datasets, outperforming mainstream hyperspectral\nclassification methods.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdb3D-DenseNet\u6a21\u578b\u7684\u7a7a\u95f4\u51e0\u4f55\u589e\u5f3a3D\u52a8\u6001\u86c7\u5f62\u7f51\u7edc\uff08SG-DSCNet\uff09\uff0c\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u9762\u4e34\u7684\u590d\u6742\u7a00\u758f\u5730\u7269\u5206\u5e03\u3001\u5c0f\u7c07\u7ed3\u6784\u548c\u591a\u5206\u652f\u7279\u5f81\u5bfc\u81f4\u7684\u6f0f\u68c0\u95ee\u9898\u3002", "method": "\u91c7\u7528\u52a8\u6001\u86c7\u5f62\u5377\u79ef\uff08DSCConv\uff09\u589e\u5f3a\u6838\u7075\u6d3b\u6027\uff0c\u63d0\u51fa\u591a\u89c6\u89d2\u7279\u5f81\u878d\u5408\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u6838\u805a\u5408\u63d0\u5347\u6a21\u578b\u8868\u793a\u80fd\u529b\u3002", "result": "\u5728IN\u3001UP\u548cKSC\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4e3b\u6d41\u9ad8\u5149\u8c31\u5206\u7c7b\u65b9\u6cd5\u3002", "conclusion": "SG-DSCNet\u901a\u8fc7\u52a8\u6001\u7279\u5f81\u54cd\u5e94\u548c\u591a\u89c6\u89d2\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2504.04698", "pdf": "https://arxiv.org/pdf/2504.04698", "abs": "https://arxiv.org/abs/2504.04698", "authors": ["Yuren Mao", "Yu Mi", "Peigen Liu", "Mengfei Zhang", "Hanqing Liu", "Yunjun Gao"], "title": "scAgent: Universal Single-Cell Annotation via a LLM Agent", "categories": ["cs.CL"], "comment": null, "summary": "Cell type annotation is critical for understanding cellular heterogeneity.\nBased on single-cell RNA-seq data and deep learning models, good progress has\nbeen made in annotating a fixed number of cell types within a specific tissue.\nHowever, universal cell annotation, which can generalize across tissues,\ndiscover novel cell types, and extend to novel cell types, remains less\nexplored. To fill this gap, this paper proposes scAgent, a universal cell\nannotation framework based on Large Language Models (LLMs). scAgent can\nidentify cell types and discover novel cell types in diverse tissues;\nfurthermore, it is data efficient to learn novel cell types. Experimental\nstudies in 160 cell types and 35 tissues demonstrate the superior performance\nof scAgent in general cell-type annotation, novel cell discovery, and\nextensibility to novel cell type.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u7ec6\u80de\u6ce8\u91ca\u6846\u67b6scAgent\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u53d1\u73b0\u591a\u79cd\u7ec4\u7ec7\u4e2d\u7684\u7ec6\u80de\u7c7b\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u7ec4\u7ec7\u4e2d\u7684\u56fa\u5b9a\u7ec6\u80de\u7c7b\u578b\uff0c\u800c\u901a\u7528\u7ec6\u80de\u6ce8\u91ca\u65b9\u6cd5\u5728\u8de8\u7ec4\u7ec7\u3001\u53d1\u73b0\u65b0\u7ec6\u80de\u7c7b\u578b\u548c\u6269\u5c55\u65b0\u7ec6\u80de\u7c7b\u578b\u65b9\u9762\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u57fa\u4e8e\u5355\u7ec6\u80deRNA-seq\u6570\u636e\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6784\u5efascAgent\u6846\u67b6\u3002", "result": "\u5728160\u79cd\u7ec6\u80de\u7c7b\u578b\u548c35\u79cd\u7ec4\u7ec7\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cscAgent\u5728\u901a\u7528\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u3001\u65b0\u7ec6\u80de\u53d1\u73b0\u548c\u6269\u5c55\u65b0\u7ec6\u80de\u7c7b\u578b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "scAgent\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u7ec6\u80de\u6ce8\u91ca\u6846\u67b6\uff0c\u80fd\u591f\u8de8\u7ec4\u7ec7\u8bc6\u522b\u548c\u53d1\u73b0\u7ec6\u80de\u7c7b\u578b\uff0c\u5e76\u5177\u6709\u6570\u636e\u9ad8\u6548\u6027\u3002"}}
{"id": "2504.04470", "pdf": "https://arxiv.org/pdf/2504.04470", "abs": "https://arxiv.org/abs/2504.04470", "authors": ["Jiabao Guo", "Ajian Liu", "Yunfeng Diao", "Jin Zhang", "Hui Ma", "Bo Zhao", "Richang Hong", "Meng Wang"], "title": "Domain Generalization for Face Anti-spoofing via Content-aware Composite Prompt Engineering", "categories": ["cs.CV"], "comment": null, "summary": "The challenge of Domain Generalization (DG) in Face Anti-Spoofing (FAS) is\nthe significant interference of domain-specific signals on subtle spoofing\nclues. Recently, some CLIP-based algorithms have been developed to alleviate\nthis interference by adjusting the weights of visual classifiers. However, our\nanalysis of this class-wise prompt engineering suffers from two shortcomings\nfor DG FAS: (1) The categories of facial categories, such as real or spoof,\nhave no semantics for the CLIP model, making it difficult to learn accurate\ncategory descriptions. (2) A single form of prompt cannot portray the various\ntypes of spoofing. In this work, instead of class-wise prompts, we propose a\nnovel Content-aware Composite Prompt Engineering (CCPE) that generates\ninstance-wise composite prompts, including both fixed template and learnable\nprompts. Specifically, our CCPE constructs content-aware prompts from two\nbranches: (1) Inherent content prompt explicitly benefits from abundant\ntransferred knowledge from the instruction-based Large Language Model (LLM).\n(2) Learnable content prompts implicitly extract the most informative visual\ncontent via Q-Former. Moreover, we design a Cross-Modal Guidance Module (CGM)\nthat dynamically adjusts unimodal features for fusion to achieve better\ngeneralized FAS. Finally, our CCPE has been validated for its effectiveness in\nmultiple cross-domain experiments and achieves state-of-the-art (SOTA) results.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u5185\u5bb9\u611f\u77e5\u590d\u5408\u63d0\u793a\u5de5\u7a0b\uff08CCPE\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u9886\u57df\u6cdb\u5316\uff08DG\uff09\u5728\u4eba\u8138\u53cd\u6b3a\u9a97\uff08FAS\uff09\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eCLIP\u7684\u7b97\u6cd5\u5728DG FAS\u4e2d\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a\u7c7b\u522b\u8bed\u4e49\u7f3a\u5931\u548c\u5355\u4e00\u63d0\u793a\u5f62\u5f0f\u65e0\u6cd5\u63cf\u8ff0\u591a\u79cd\u6b3a\u9a97\u7c7b\u578b\u3002", "method": "\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u522b\u7684\u590d\u5408\u63d0\u793a\uff08\u5305\u62ec\u56fa\u5b9a\u6a21\u677f\u548c\u53ef\u5b66\u4e60\u63d0\u793a\uff09\u548c\u8de8\u6a21\u6001\u5f15\u5bfc\u6a21\u5757\uff08CGM\uff09\u52a8\u6001\u8c03\u6574\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2a\u8de8\u57df\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86CCPE\u7684\u6709\u6548\u6027\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\uff08SOTA\uff09\u7ed3\u679c\u3002", "conclusion": "CCPE\u901a\u8fc7\u5185\u5bb9\u611f\u77e5\u63d0\u793a\u548c\u8de8\u6a21\u6001\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86DG FAS\u7684\u6027\u80fd\u3002"}}
{"id": "2504.04700", "pdf": "https://arxiv.org/pdf/2504.04700", "abs": "https://arxiv.org/abs/2504.04700", "authors": ["Hyunseo Shin", "Wonseok Hwang"], "title": "Causal Retrieval with Semantic Consideration", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nenhanced the performance of conversational AI systems. To extend their\ncapabilities to knowledge-intensive domains such as biomedical and legal\nfields, where the accuracy is critical, LLMs are often combined with\ninformation retrieval (IR) systems to generate responses based on retrieved\ndocuments. However, for IR systems to effectively support such applications,\nthey must go beyond simple semantic matching and accurately capture diverse\nquery intents, including causal relationships. Existing IR models primarily\nfocus on retrieving documents based on surface-level semantic similarity,\noverlooking deeper relational structures such as causality. To address this, we\npropose CAWAI, a retrieval model that is trained with dual objectives: semantic\nand causal relations. Our extensive experiments demonstrate that CAWAI\noutperforms various models on diverse causal retrieval tasks especially under\nlarge-scale retrieval settings. We also show that CAWAI exhibits strong\nzero-shot generalization across scientific domain QA tasks.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u8bed\u4e49\u548c\u56e0\u679c\u5173\u7cfb\u7684\u68c0\u7d22\u6a21\u578bCAWAI\uff0c\u4ee5\u63d0\u5347\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u4e2d\u5bf9\u8bddAI\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u8868\u9762\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u5ffd\u7565\u4e86\u66f4\u6df1\u5c42\u6b21\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u800c\u8fd9\u5bf9\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\uff08\u5982\u751f\u7269\u533b\u5b66\u548c\u6cd5\u5f8b\uff09\u7684\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faCAWAI\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u76ee\u6807\u8bad\u7ec3\uff08\u8bed\u4e49\u548c\u56e0\u679c\u5173\u7cfb\uff09\u6765\u6539\u8fdb\u68c0\u7d22\u6548\u679c\u3002", "result": "CAWAI\u5728\u591a\u79cd\u56e0\u679c\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u68c0\u7d22\u573a\u666f\u4e0b\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CAWAI\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u548c\u56e0\u679c\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u4e2d\u7684\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2504.04471", "pdf": "https://arxiv.org/pdf/2504.04471", "abs": "https://arxiv.org/abs/2504.04471", "authors": ["Zhuo Zhi", "Qiangqiang Wu", "Minghe shen", "Wenbo Li", "Yinchuan Li", "Kun Shao", "Kaiwen Zhou"], "title": "VideoAgent2: Enhancing the LLM-Based Agent System for Long-Form Video Understanding by Uncertainty-Aware CoT", "categories": ["cs.CV"], "comment": null, "summary": "Long video understanding has emerged as an increasingly important yet\nchallenging task in computer vision. Agent-based approaches are gaining\npopularity for processing long videos, as they can handle extended sequences\nand integrate various tools to capture fine-grained information. However,\nexisting methods still face several challenges: (1) they often rely solely on\nthe reasoning ability of large language models (LLMs) without dedicated\nmechanisms to enhance reasoning in long video scenarios; and (2) they remain\nvulnerable to errors or noise from external tools. To address these issues, we\npropose a specialized chain-of-thought (CoT) process tailored for long video\nanalysis. Our proposed CoT with plan-adjust mode enables the LLM to\nincrementally plan and adapt its information-gathering strategy. We further\nincorporate heuristic uncertainty estimation of both the LLM and external tools\nto guide the CoT process. This allows the LLM to assess the reliability of\nnewly collected information, refine its collection strategy, and make more\nrobust decisions when synthesizing final answers. Empirical experiments show\nthat our uncertainty-aware CoT effectively mitigates noise from external tools,\nleading to more reliable outputs. We implement our approach in a system called\nVideoAgent2, which also includes additional modules such as general context\nacquisition and specialized tool design. Evaluation on three dedicated long\nvideo benchmarks (and their subsets) demonstrates that VideoAgent2 outperforms\nthe previous state-of-the-art agent-based method, VideoAgent, by an average of\n13.1% and achieves leading performance among all zero-shot approaches", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u4e13\u4e3a\u957f\u89c6\u9891\u5206\u6790\u8bbe\u8ba1\u7684\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u8fc7\u7a0b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7f3a\u4e4f\u9488\u5bf9\u957f\u89c6\u9891\u573a\u666f\u7684\u4e13\u7528\u673a\u5236\uff0c\u4e14\u6613\u53d7\u5916\u90e8\u5de5\u5177\u9519\u8bef\u6216\u566a\u58f0\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u5e26\u6709\u8ba1\u5212\u8c03\u6574\u6a21\u5f0f\u7684CoT\u8fc7\u7a0b\uff0c\u7ed3\u5408\u542f\u53d1\u5f0f\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u6307\u5bfcLLM\u9010\u6b65\u89c4\u5212\u548c\u8c03\u6574\u4fe1\u606f\u6536\u96c6\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684CoT\u6709\u6548\u51cf\u5c11\u4e86\u5916\u90e8\u5de5\u5177\u7684\u566a\u58f0\uff0cVideoAgent2\u5728\u4e09\u4e2a\u957f\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u4f18\u4e8e\u4e4b\u524d\u6700\u5148\u8fdb\u65b9\u6cd513.1%\u3002", "conclusion": "VideoAgent2\u901a\u8fc7\u4e13\u7528CoT\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u89c6\u9891\u7406\u89e3\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2504.04713", "pdf": "https://arxiv.org/pdf/2504.04713", "abs": "https://arxiv.org/abs/2504.04713", "authors": ["Yifei Yu", "Qian-Wen Zhang", "Lingfeng Qiao", "Di Yin", "Fang Li", "Jie Wang", "Zengxi Chen", "Suncong Zheng", "Xiaolong Liang", "Xing Sun"], "title": "Sequential-NIAH: A Needle-In-A-Haystack Benchmark for Extracting Sequential Needles from Long Contexts", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Evaluating the ability of large language models (LLMs) to handle extended\ncontexts is critical, particularly for retrieving information relevant to\nspecific queries embedded within lengthy inputs. We introduce Sequential-NIAH,\na benchmark specifically designed to evaluate the capability of LLMs to extract\nsequential information items (known as needles) from long contexts. The\nbenchmark comprises three types of needle generation pipelines: synthetic,\nreal, and open-domain QA. It includes contexts ranging from 8K to 128K tokens\nin length, with a dataset of 14,000 samples (2,000 reserved for testing). To\nfacilitate evaluation on this benchmark, we trained a synthetic data-driven\nevaluation model capable of evaluating answer correctness based on\nchronological or logical order, achieving an accuracy of 99.49% on synthetic\ntest data. We conducted experiments on six well-known LLMs, revealing that even\nthe best-performing model achieved a maximum accuracy of only 63.15%. Further\nanalysis highlights the growing challenges posed by increasing context lengths\nand the number of needles, underscoring substantial room for improvement.\nAdditionally, noise robustness experiments validate the reliability of the\nbenchmark, making Sequential-NIAH an important reference for advancing research\non long text extraction capabilities of LLMs.", "AI": {"task": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5904\u7406\u957f\u6587\u672c\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u4ece\u5197\u957f\u8f93\u5165\u4e2d\u63d0\u53d6\u4e0e\u7279\u5b9a\u67e5\u8be2\u76f8\u5173\u7684\u4fe1\u606f\u3002", "motivation": "\u9700\u8981\u8bc4\u4f30LLMs\u5728\u957f\u6587\u672c\u4e2d\u63d0\u53d6\u987a\u5e8f\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u4ee5\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "method": "\u5f15\u5165Sequential-NIAH\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u4e09\u79cd\u9488\u751f\u6210\u7ba1\u9053\uff08\u5408\u6210\u3001\u771f\u5b9e\u3001\u5f00\u653e\u57dfQA\uff09\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5408\u6210\u6570\u636e\u9a71\u52a8\u7684\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u5728\u516d\u79cd\u77e5\u540dLLMs\u4e0a\u5b9e\u9a8c\uff0c\u6700\u4f73\u6a21\u578b\u51c6\u786e\u7387\u4ec5\u4e3a63.15%\uff0c\u8868\u660e\u957f\u4e0a\u4e0b\u6587\u548c\u66f4\u591a\u9488\u7684\u6311\u6218\u3002", "conclusion": "Sequential-NIAH\u662f\u8bc4\u4f30LLMs\u957f\u6587\u672c\u63d0\u53d6\u80fd\u529b\u7684\u91cd\u8981\u57fa\u51c6\uff0c\u9a8c\u8bc1\u4e86\u5176\u53ef\u9760\u6027\u5e76\u6307\u51fa\u4e86\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2504.04482", "pdf": "https://arxiv.org/pdf/2504.04482", "abs": "https://arxiv.org/abs/2504.04482", "authors": ["Mengxia Dai", "Wenqian Luo", "Tianyang Li"], "title": "Statistical Guarantees Of False Discovery Rate In Medical Instance Segmentation Tasks Based on Conformal Risk Control", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Instance segmentation plays a pivotal role in medical image analysis by\nenabling precise localization and delineation of lesions, tumors, and\nanatomical structures. Although deep learning models such as Mask R-CNN and\nBlendMask have achieved remarkable progress, their application in high-risk\nmedical scenarios remains constrained by confidence calibration issues, which\nmay lead to misdiagnosis. To address this challenge, we propose a robust\nquality control framework based on conformal prediction theory. This framework\ninnovatively constructs a risk-aware dynamic threshold mechanism that\nadaptively adjusts segmentation decision boundaries according to clinical\nrequirements.Specifically, we design a \\textbf{calibration-aware loss function}\nthat dynamically tunes the segmentation threshold based on a user-defined risk\nlevel $\\alpha$. Utilizing exchangeable calibration data, this method ensures\nthat the expected FNR or FDR on test data remains below $\\alpha$ with high\nprobability. The framework maintains compatibility with mainstream segmentation\nmodels (e.g., Mask R-CNN, BlendMask+ResNet-50-FPN) and datasets (PASCAL VOC\nformat) without requiring architectural modifications. Empirical results\ndemonstrate that we rigorously bound the FDR metric marginally over the test\nset via our developed calibration framework.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u7406\u8bba\u7684\u9c81\u68d2\u8d28\u91cf\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5b9e\u4f8b\u5206\u5272\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982Mask R-CNN\u548cBlendMask\uff09\u5728\u9ad8\u98ce\u9669\u533b\u5b66\u573a\u666f\u4e2d\u56e0\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\u5bfc\u81f4\u7684\u8bef\u8bca\u98ce\u9669\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u5b9a\u4e49\u98ce\u9669\u6c34\u5e73\u03b1\u7684\u6821\u51c6\u611f\u77e5\u635f\u5931\u51fd\u6570\uff0c\u52a8\u6001\u8c03\u6574\u5206\u5272\u9608\u503c\uff0c\u5e76\u901a\u8fc7\u53ef\u4ea4\u6362\u6821\u51c6\u6570\u636e\u786e\u4fdd\u6d4b\u8bd5\u6570\u636e\u7684FNR\u6216FDR\u4f4e\u4e8e\u03b1\u3002", "result": "\u6846\u67b6\u5728\u6d4b\u8bd5\u96c6\u4e0a\u4e25\u683c\u9650\u5236\u4e86FDR\u6307\u6807\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u4e3b\u6d41\u5206\u5272\u6a21\u578b\u548c\u6570\u636e\u96c6\u7684\u7ed3\u6784\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\uff0c\u4e3a\u9ad8\u98ce\u9669\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8d28\u91cf\u63a7\u5236\u3002"}}
{"id": "2504.04715", "pdf": "https://arxiv.org/pdf/2504.04715", "abs": "https://arxiv.org/abs/2504.04715", "authors": ["Will Cai", "Tianneng Shi", "Xuandong Zhao", "Dawn Song"], "title": "Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) accessed via black-box APIs\nintroduces a significant trust challenge: users pay for services based on\nadvertised model capabilities (e.g., size, performance), but providers may\ncovertly substitute the specified model with a cheaper, lower-quality\nalternative to reduce operational costs. This lack of transparency undermines\nfairness, erodes trust, and complicates reliable benchmarking. Detecting such\nsubstitutions is difficult due to the black-box nature, typically limiting\ninteraction to input-output queries. This paper formalizes the problem of model\nsubstitution detection in LLM APIs. We systematically evaluate existing\nverification techniques, including output-based statistical tests, benchmark\nevaluations, and log probability analysis, under various realistic attack\nscenarios like model quantization, randomized substitution, and benchmark\nevasion. Our findings reveal the limitations of methods relying solely on text\noutputs, especially against subtle or adaptive attacks. While log probability\nanalysis offers stronger guarantees when available, its accessibility is often\nlimited. We conclude by discussing the potential of hardware-based solutions\nlike Trusted Execution Environments (TEEs) as a pathway towards provable model\nintegrity, highlighting the trade-offs between security, performance, and\nprovider adoption. Code is available at\nhttps://github.com/sunblaze-ucb/llm-api-audit", "AI": {"task": "Formalize and address the problem of detecting model substitution in Large Language Model (LLM) APIs.", "motivation": "The lack of transparency in black-box LLM APIs undermines trust and fairness, as providers may substitute models with cheaper alternatives without disclosure.", "method": "Systematically evaluate existing verification techniques (output-based statistical tests, benchmark evaluations, log probability analysis) under realistic attack scenarios.", "result": "Existing methods relying solely on text outputs have limitations, especially against subtle or adaptive attacks; log probability analysis offers stronger guarantees but is less accessible.", "conclusion": "Hardware-based solutions like Trusted Execution Environments (TEEs) may provide provable model integrity, balancing security, performance, and adoption."}}
{"id": "2504.04485", "pdf": "https://arxiv.org/pdf/2504.04485", "abs": "https://arxiv.org/abs/2504.04485", "authors": ["Yapeng Mi", "Zhi Gao", "Xiaojian Ma", "Qing Li"], "title": "Building LLM Agents by Incorporating Insights from Computer Systems", "categories": ["cs.CV"], "comment": null, "summary": "LLM-driven autonomous agents have emerged as a promising direction in recent\nyears. However, many of these LLM agents are designed empirically or based on\nintuition, often lacking systematic design principles, which results in diverse\nagent structures with limited generality and scalability. In this paper, we\nadvocate for building LLM agents by incorporating insights from computer\nsystems. Inspired by the von Neumann architecture, we propose a structured\nframework for LLM agentic systems, emphasizing modular design and universal\nprinciples. Specifically, this paper first provides a comprehensive review of\nLLM agents from the computer system perspective, then identifies key challenges\nand future directions inspired by computer system design, and finally explores\nthe learning mechanisms for LLM agents beyond the computer system. The insights\ngained from this comparative analysis offer a foundation for systematic LLM\nagent design and advancement.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u7cfb\u7edf\u89c6\u89d2\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u8bbe\u8ba1LLM\u9a71\u52a8\u7684\u81ea\u4e3b\u4ee3\u7406\u3002", "motivation": "\u5f53\u524dLLM\u4ee3\u7406\u7684\u8bbe\u8ba1\u7f3a\u4e4f\u7cfb\u7edf\u6027\u539f\u5219\uff0c\u5bfc\u81f4\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u53d7\u9650\u3002", "method": "\u501f\u9274\u51af\u00b7\u8bfa\u4f9d\u66fc\u67b6\u6784\uff0c\u63d0\u51fa\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u901a\u7528\u539f\u5219\u7684\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u8ba1\u7b97\u673a\u7cfb\u7edf\u89c6\u89d2\u7684\u7efc\u8ff0\u548c\u6bd4\u8f83\u5206\u6790\uff0c\u4e3aLLM\u4ee3\u7406\u7684\u7cfb\u7edf\u5316\u8bbe\u8ba1\u63d0\u4f9b\u57fa\u7840\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u4ee3\u7406\u7684\u8bbe\u8ba1\u548c\u53d1\u5c55\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u65b9\u5411\u548c\u672a\u6765\u7814\u7a76\u8def\u5f84\u3002"}}
{"id": "2504.04717", "pdf": "https://arxiv.org/pdf/2504.04717", "abs": "https://arxiv.org/abs/2504.04717", "authors": ["Yubo Li", "Xiaobin Shen", "Xinyu Yao", "Xueying Ding", "Yidi Miao", "Ramayya Krishnan", "Rema Padman"], "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "After 136 days of meticulous preparation, we're thrilled to finally\n  share our comprehensive survey on llm multi-turn interactions with the\n  community!", "summary": "Recent advancements in large language models (LLMs) have revolutionized their\nability to handle single-turn tasks, yet real-world applications demand\nsophisticated multi-turn interactions. This survey provides a comprehensive\nreview of recent advancements in evaluating and enhancing multi-turn\ninteractions in LLMs. Focusing on task-specific scenarios, from instruction\nfollowing in diverse domains such as math and coding to complex conversational\nengagements in roleplay, healthcare, education, and even adversarial jailbreak\nsettings, we systematically examine the challenges of maintaining context,\ncoherence, fairness, and responsiveness over prolonged dialogues. The paper\norganizes current benchmarks and datasets into coherent categories that reflect\nthe evolving landscape of multi-turn dialogue evaluation. In addition, we\nreview a range of enhancement methodologies under multi-turn settings,\nincluding model-centric strategies (contextual learning, supervised\nfine-tuning, reinforcement learning, and new architectures), external\nintegration approaches (memory-augmented, retrieval-based methods, and\nknowledge graph), and agent-based techniques for collaborative interactions.\nFinally, we discuss open challenges and propose future directions for research\nto further advance the robustness and effectiveness of multi-turn interactions\nin LLMs. Related resources and papers are available at\nhttps://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.", "AI": {"task": "\u5168\u9762\u7efc\u8ff0\u548c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u8868\u73b0\u53ca\u5176\u589e\u5f3a\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u9700\u8981\u590d\u6742\u7684\u591a\u8f6e\u4ea4\u4e92\u80fd\u529b\uff0c\u4f46\u76ee\u524dLLMs\u5728\u6b64\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u591a\u8f6e\u4ea4\u4e92\u7684\u6311\u6218\uff0c\u6574\u7406\u73b0\u6709\u57fa\u51c6\u548c\u6570\u636e\u96c6\uff0c\u5e76\u7efc\u8ff0\u591a\u79cd\u589e\u5f3a\u65b9\u6cd5\uff08\u5982\u6a21\u578b\u4e2d\u5fc3\u7b56\u7565\u3001\u5916\u90e8\u96c6\u6210\u65b9\u6cd5\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u6280\u672f\uff09\u3002", "result": "\u603b\u7ed3\u4e86\u591a\u8f6e\u4ea4\u4e92\u7684\u73b0\u72b6\u548c\u589e\u5f3a\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u591a\u8f6e\u4ea4\u4e92\u662fLLMs\u7684\u91cd\u8981\u53d1\u5c55\u65b9\u5411\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u63d0\u9ad8\u5176\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2504.04490", "pdf": "https://arxiv.org/pdf/2504.04490", "abs": "https://arxiv.org/abs/2504.04490", "authors": ["Kayato Nishitsunoi", "Yoshiyuki Ohmura", "Takayuki Komatsu", "Yasuo Kuniyoshi"], "title": "Learning Conditionally Independent Transformations using Normal Subgroups in Group Theory", "categories": ["cs.CV"], "comment": "8 pages, 10 figures, conference paper", "summary": "Humans develop certain cognitive abilities to recognize objects and their\ntransformations without explicit supervision, highlighting the importance of\nunsupervised representation learning. A fundamental challenge in unsupervised\nrepresentation learning is to separate different transformations in learned\nfeature representations. Although algebraic approaches have been explored, a\ncomprehensive theoretical framework remains underdeveloped. Existing methods\ndecompose transformations based on algebraic independence, but these methods\nprimarily focus on commutative transformations and do not extend to cases where\ntransformations are conditionally independent but noncommutative. To extend\ncurrent representation learning frameworks, we draw inspiration from Galois\ntheory, where the decomposition of groups through normal subgroups provides an\napproach for the analysis of structured transformations. Normal subgroups\nnaturally extend commutativity under certain conditions and offer a foundation\nfor the categorization of transformations, even when they do not commute. In\nthis paper, we propose a novel approach that leverages normal subgroups to\nenable the separation of conditionally independent transformations, even in the\nabsence of commutativity. Through experiments on geometric transformations in\nimages, we show that our method successfully categorizes conditionally\nindependent transformations, such as rotation and translation, in an\nunsupervised manner, suggesting a close link between group decomposition via\nnormal subgroups and transformation categorization in representation learning.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u6b63\u89c4\u5b50\u7fa4\u5206\u79bb\u6761\u4ef6\u72ec\u7acb\u53d8\u6362\u7684\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u4e2d\u5206\u79bb\u4e0d\u540c\u53d8\u6362\u7684\u7406\u8bba\u6846\u67b6\u5c1a\u4e0d\u5b8c\u5584\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u53ef\u4ea4\u6362\u53d8\u6362\uff0c\u65e0\u6cd5\u5904\u7406\u6761\u4ef6\u72ec\u7acb\u4f46\u975e\u4ea4\u6362\u7684\u53d8\u6362\u3002", "method": "\u501f\u9274\u4f3d\u7f57\u74e6\u7406\u8bba\u4e2d\u7684\u6b63\u89c4\u5b50\u7fa4\u5206\u89e3\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\u5206\u79bb\u6761\u4ef6\u72ec\u7acb\u53d8\u6362\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6210\u529f\u65e0\u76d1\u7763\u5730\u5206\u7c7b\u6761\u4ef6\u72ec\u7acb\u53d8\u6362\uff08\u5982\u65cb\u8f6c\u548c\u5e73\u79fb\uff09\u3002", "conclusion": "\u6b63\u89c4\u5b50\u7fa4\u5206\u89e3\u4e0e\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u53d8\u6362\u5206\u7c7b\u5b58\u5728\u7d27\u5bc6\u8054\u7cfb\u3002"}}
{"id": "2504.04718", "pdf": "https://arxiv.org/pdf/2504.04718", "abs": "https://arxiv.org/abs/2504.04718", "authors": ["Minki Kang", "Jongwon Jeong", "Jaewoong Cho"], "title": "T1: Tool-integrated Self-verification for Test-time Compute Scaling in Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent studies have demonstrated that test-time compute scaling effectively\nimproves the performance of small language models (sLMs). However, prior\nresearch has mainly examined test-time compute scaling with an additional\nlarger model as a verifier, leaving self-verification by sLMs underexplored. In\nthis work, we investigate whether sLMs can reliably self-verify their outputs\nunder test-time scaling. We find that even with knowledge distillation from\nlarger verifiers, sLMs struggle with verification tasks requiring memorization,\nsuch as numerical calculations and fact-checking. To address this limitation,\nwe propose Tool-integrated self-verification (T1), which delegates\nmemorization-heavy verification steps to external tools, such as a code\ninterpreter. Our theoretical analysis shows that tool integration reduces\nmemorization demands and improves test-time scaling performance. Experiments on\nthe MATH benchmark demonstrate that, with T1, a Llama-3.2 1B model under\ntest-time scaling outperforms the significantly larger Llama-3.1 8B model.\nMoreover, T1 generalizes effectively to both mathematical (MATH500) and\nmulti-domain knowledge-intensive tasks (MMLU-Pro). Our findings highlight the\npotential of tool integration to substantially improve the self-verification\nabilities of sLMs.", "AI": {"task": "\u7814\u7a76\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08sLM\uff09\u5728\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u4e0b\u662f\u5426\u80fd\u53ef\u9760\u5730\u81ea\u6211\u9a8c\u8bc1\u5176\u8f93\u51fa\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u66f4\u5927\u7684\u6a21\u578b\u4f5c\u4e3a\u9a8c\u8bc1\u5668\uff0c\u800c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u9a8c\u8bc1\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u5de5\u5177\u96c6\u6210\u7684\u81ea\u6211\u9a8c\u8bc1\u65b9\u6cd5\uff08T1\uff09\uff0c\u5c06\u8bb0\u5fc6\u5bc6\u96c6\u578b\u9a8c\u8bc1\u6b65\u9aa4\u59d4\u6258\u7ed9\u5916\u90e8\u5de5\u5177\uff08\u5982\u4ee3\u7801\u89e3\u91ca\u5668\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528T1\u7684Llama-3.2 1B\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e0b\u8868\u73b0\u4f18\u4e8e\u66f4\u5927\u7684Llama-3.1 8B\u6a21\u578b\uff0c\u5e76\u5728\u6570\u5b66\u548c\u591a\u9886\u57df\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u5de5\u5177\u96c6\u6210\u80fd\u663e\u8457\u63d0\u5347\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u9a8c\u8bc1\u80fd\u529b\u3002"}}
{"id": "2504.04494", "pdf": "https://arxiv.org/pdf/2504.04494", "abs": "https://arxiv.org/abs/2504.04494", "authors": ["Marin Ben\u010devi\u0107", "Robert \u0160ojo", "Irena Gali\u0107"], "title": "Skin Color Measurement from Dermatoscopic Images: An Evaluation on a Synthetic Dataset", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents a comprehensive evaluation of skin color measurement\nmethods from dermatoscopic images using a synthetic dataset (S-SYNTH) with\ncontrolled ground-truth melanin content, lesion shapes, hair models, and 18\ndistinct lighting conditions. This allows for rigorous assessment of the\nrobustness and invariance to lighting conditions. We assess four classes of\nimage colorimetry approaches: segmentation-based, patch-based, color\nquantization, and neural networks. We use these methods to estimate the\nIndividual Typology Angle (ITA) and Fitzpatrick types from dermatoscopic\nimages. Our results show that segmentation-based and color quantization methods\nyield robust, lighting-invariant estimates, whereas patch-based approaches\nexhibit significant lighting-dependent biases that require calibration.\nFurthermore, neural network models, particularly when combined with heavy\nblurring to reduce overfitting, can provide light-invariant Fitzpatrick\npredictions, although their generalization to real-world images remains\nunverified. We conclude with practical recommendations for designing fair and\nreliable skin color estimation methods.", "AI": {"task": "\u8bc4\u4f30\u76ae\u80a4\u989c\u8272\u6d4b\u91cf\u65b9\u6cd5\u5728\u76ae\u80a4\u955c\u56fe\u50cf\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\uff08S-SYNTH\uff09\u63a7\u5236\u591a\u79cd\u53d8\u91cf\uff0c\u4ee5\u8bc4\u4f30\u65b9\u6cd5\u5bf9\u5149\u7167\u6761\u4ef6\u7684\u9c81\u68d2\u6027\u548c\u4e0d\u53d8\u6027\u3002", "method": "\u8bc4\u4f30\u56db\u7c7b\u56fe\u50cf\u6bd4\u8272\u65b9\u6cd5\uff1a\u57fa\u4e8e\u5206\u5272\u3001\u57fa\u4e8e\u8865\u4e01\u3001\u989c\u8272\u91cf\u5316\u548c\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u57fa\u4e8e\u5206\u5272\u548c\u989c\u8272\u91cf\u5316\u7684\u65b9\u6cd5\u8868\u73b0\u9c81\u68d2\u4e14\u5bf9\u5149\u7167\u4e0d\u53d8\uff0c\u800c\u57fa\u4e8e\u8865\u4e01\u7684\u65b9\u6cd5\u9700\u8981\u6821\u51c6\uff1b\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u6a21\u7cca\u5904\u7406\u53ef\u63d0\u4f9b\u5149\u7167\u4e0d\u53d8\u7684\u9884\u6d4b\uff0c\u4f46\u6cdb\u5316\u80fd\u529b\u672a\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u4e86\u8bbe\u8ba1\u516c\u5e73\u53ef\u9760\u76ae\u80a4\u989c\u8272\u4f30\u8ba1\u65b9\u6cd5\u7684\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2504.04737", "pdf": "https://arxiv.org/pdf/2504.04737", "abs": "https://arxiv.org/abs/2504.04737", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Shivam Mishra", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "title": "TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "In the landscape of Fact-based Judgment Prediction and Explanation (FJPE),\nreliance on factual data is essential for developing robust and realistic\nAI-driven decision-making tools. This paper introduces TathyaNyaya, the largest\nannotated dataset for FJPE tailored to the Indian legal context, encompassing\njudgments from the Supreme Court of India and various High Courts. Derived from\nthe Hindi terms \"Tathya\" (fact) and \"Nyaya\" (justice), the TathyaNyaya dataset\nis uniquely designed to focus on factual statements rather than complete legal\ntexts, reflecting real-world judicial processes where factual data drives\noutcomes. Complementing this dataset, we present FactLegalLlama, an\ninstruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM),\noptimized for generating high-quality explanations in FJPE tasks. Finetuned on\nthe factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy\nwith coherent, contextually relevant explanations, addressing the critical need\nfor transparency and interpretability in AI-assisted legal systems. Our\nmethodology combines transformers for binary judgment prediction with\nFactLegalLlama for explanation generation, creating a robust framework for\nadvancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses\nexisting datasets in scale and diversity but also establishes a benchmark for\nbuilding explainable AI systems in legal analysis. The findings underscore the\nimportance of factual precision and domain-specific tuning in enhancing\npredictive performance and interpretability, positioning TathyaNyaya and\nFactLegalLlama as foundational resources for AI-assisted legal decision-making.", "AI": {"task": "\u6784\u5efaTathyaNyaya\u6570\u636e\u96c6\u5e76\u5f00\u53d1FactLegalLlama\u6a21\u578b\uff0c\u7528\u4e8e\u5370\u5ea6\u6cd5\u5f8b\u80cc\u666f\u4e0b\u7684\u57fa\u4e8e\u4e8b\u5b9e\u7684\u5224\u51b3\u9884\u6d4b\u4e0e\u89e3\u91ca\uff08FJPE\uff09\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u4e8b\u5b9e\u6570\u636e\u7684AI\u9a71\u52a8\u51b3\u7b56\u5de5\u5177\uff0c\u4ee5\u589e\u5f3a\u6cd5\u5f8b\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u7ed3\u5408TathyaNyaya\u6570\u636e\u96c6\uff08\u5370\u5ea6\u6700\u9ad8\u6cd5\u9662\u548c\u9ad8\u7b49\u6cd5\u9662\u7684\u5224\u51b3\uff09\u548cFactLegalLlama\u6a21\u578b\uff08\u57fa\u4e8eLLaMa-3-8B\u5fae\u8c03\u7684\u6a21\u578b\uff09\uff0c\u7528\u4e8e\u5224\u51b3\u9884\u6d4b\u548c\u89e3\u91ca\u751f\u6210\u3002", "result": "TathyaNyaya\u6570\u636e\u96c6\u5728\u89c4\u6a21\u548c\u591a\u6837\u6027\u4e0a\u8d85\u8d8a\u73b0\u6709\u6570\u636e\u96c6\uff0cFactLegalLlama\u6a21\u578b\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TathyaNyaya\u548cFactLegalLlama\u4e3aAI\u8f85\u52a9\u6cd5\u5f8b\u51b3\u7b56\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u5f3a\u8c03\u4e86\u4e8b\u5b9e\u7cbe\u786e\u6027\u548c\u9886\u57df\u7279\u5b9a\u8c03\u4f18\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2504.04495", "pdf": "https://arxiv.org/pdf/2504.04495", "abs": "https://arxiv.org/abs/2504.04495", "authors": ["Peng Wu", "Wanshun Su", "Guansong Pang", "Yujia Sun", "Qingsen Yan", "Peng Wang", "Yanning Zhang"], "title": "AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection", "categories": ["cs.CV", "I.4.9; I.5.4"], "comment": "11 pages, 4 figures, 6 tables", "summary": "With the increasing adoption of video anomaly detection in intelligent\nsurveillance domains, conventional visual-based detection approaches often\nstruggle with information insufficiency and high false-positive rates in\ncomplex environments. To address these limitations, we present a novel weakly\nsupervised framework that leverages audio-visual collaboration for robust video\nanomaly detection. Capitalizing on the exceptional cross-modal representation\nlearning capabilities of Contrastive Language-Image Pretraining (CLIP) across\nvisual, audio, and textual domains, our framework introduces two major\ninnovations: an efficient audio-visual fusion that enables adaptive cross-modal\nintegration through lightweight parametric adaptation while maintaining the\nfrozen CLIP backbone, and a novel audio-visual prompt that dynamically enhances\ntext embeddings with key multimodal information based on the semantic\ncorrelation between audio-visual features and textual labels, significantly\nimproving CLIP's generalization for the video anomaly detection task. Moreover,\nto enhance robustness against modality deficiency during inference, we further\ndevelop an uncertainty-driven feature distillation module that synthesizes\naudio-visual representations from visual-only inputs. This module employs\nuncertainty modeling based on the diversity of audio-visual features to\ndynamically emphasize challenging features during the distillation process. Our\nframework demonstrates superior performance across multiple benchmarks, with\naudio integration significantly boosting anomaly detection accuracy in various\nscenarios. Notably, with unimodal data enhanced by uncertainty-driven\ndistillation, our approach consistently outperforms current unimodal VAD\nmethods.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5f31\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u97f3\u9891-\u89c6\u89c9\u534f\u4f5c\u8fdb\u884c\u9c81\u68d2\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u89c6\u89c9\u68c0\u6d4b\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u4fe1\u606f\u4e0d\u8db3\u548c\u9ad8\u8bef\u62a5\u7387\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u8bed\u8a00-\u56fe\u50cf\u9884\u8bad\u7ec3\uff08CLIP\uff09\u7684\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u80fd\u529b\uff0c\u63d0\u51fa\u9ad8\u6548\u7684\u97f3\u9891-\u89c6\u89c9\u878d\u5408\u548c\u52a8\u6001\u97f3\u9891-\u89c6\u89c9\u63d0\u793a\uff0c\u5e76\u5f00\u53d1\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u7279\u5f81\u84b8\u998f\u6a21\u5757\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u97f3\u9891\u96c6\u6210\u663e\u8457\u63d0\u9ad8\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u97f3\u9891-\u89c6\u89c9\u534f\u4f5c\u548c\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2504.04745", "pdf": "https://arxiv.org/pdf/2504.04745", "abs": "https://arxiv.org/abs/2504.04745", "authors": ["Ankush Raut", "Xiaofeng Zhu", "Maria Leonor Pacheco"], "title": "Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs", "categories": ["cs.CL"], "comment": "13 pages, 23 figures. Submitted to XLLM @ ACL 2025", "summary": "This paper evaluates the ability of Large Language Models (LLMs) to leverage\ncontextual information in the form of structured linguistic representations.\nSpecifically, we examine the impact of encoding both short and long contexts\nusing Abstract Meaning Representation (AMR) structures across a diverse set of\nlanguage tasks. We perform our analysis using 8-bit quantized and\ninstruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our\nresults indicate that, for tasks involving short contexts, augmenting the\nprompt with the AMR of the original language context often degrades the\nperformance of the underlying LLM. However, for tasks that involve long\ncontexts, such as dialogue summarization in the SAMSum dataset, this\nenhancement improves LLM performance, for example, by increasing the zero-shot\ncosine similarity score of Llama 3.1 from 66.2% to 76%. This improvement is\nmore evident in the newer and larger LLMs, but does not extend to the older or\nsmaller ones. In addition, we observe that LLMs can effectively reconstruct the\noriginal text from a linearized AMR, achieving a cosine similarity of 81.3% in\nthe best-case scenario.", "AI": {"task": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5229\u7528\u7ed3\u6784\u5316\u8bed\u8a00\u8868\u793a\uff08AMR\uff09\u5904\u7406\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u80fd\u529b\u3002", "motivation": "\u7814\u7a76AMR\u7ed3\u6784\u5bf9LLMs\u5728\u4e0d\u540c\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u77ed\u4e0a\u4e0b\u6587\u548c\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u3002", "method": "\u4f7f\u75288\u4f4d\u91cf\u5316\u548c\u6307\u4ee4\u8c03\u4f18\u7248\u672c\u7684Llama 3.1\u3001Phi-3\u548cMistral 7B\uff0c\u5206\u6790AMR\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u77ed\u4e0a\u4e0b\u6587\u4e2dAMR\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u800c\u957f\u4e0a\u4e0b\u6587\u4e2d\uff08\u5982\u5bf9\u8bdd\u6458\u8981\uff09\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08\u5982Llama 3.1\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4ece66.2%\u63d0\u9ad8\u523076%\uff09\u3002", "conclusion": "AMR\u5bf9LLMs\u7684\u6027\u80fd\u63d0\u5347\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u66f4\u663e\u8457\uff0c\u4e14\u5bf9\u8f83\u65b0\u3001\u8f83\u5927\u7684\u6a21\u578b\u6548\u679c\u66f4\u597d\u3002"}}
{"id": "2504.04510", "pdf": "https://arxiv.org/pdf/2504.04510", "abs": "https://arxiv.org/abs/2504.04510", "authors": ["Shijian Wang", "Linxin Song", "Ryotaro Shimizu", "Masayuki Goto", "Hanqian Wu"], "title": "Attributed Synthetic Data Generation for Zero-shot Domain-specific Image Classification", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot domain-specific image classification is challenging in classifying\nreal images without ground-truth in-domain training examples. Recent research\ninvolved knowledge from texts with a text-to-image model to generate in-domain\ntraining images in zero-shot scenarios. However, existing methods heavily rely\non simple prompt strategies, limiting the diversity of synthetic training\nimages, thus leading to inferior performance compared to real images. In this\npaper, we propose AttrSyn, which leverages large language models to generate\nattributed prompts. These prompts allow for the generation of more diverse\nattributed synthetic images. Experiments for zero-shot domain-specific image\nclassification on two fine-grained datasets show that training with synthetic\nimages generated by AttrSyn significantly outperforms CLIP's zero-shot\nclassification under most situations and consistently surpasses simple prompt\nstrategies.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aAttrSyn\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u96f6\u6837\u672c\u9886\u57df\u7279\u5b9a\u56fe\u50cf\u5206\u7c7b\u4e2d\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u5408\u6210\u8bad\u7ec3\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u63d0\u793a\u7b56\u7565\uff0c\u5bfc\u81f4\u5408\u6210\u56fe\u50cf\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u6027\u80fd\u4e0d\u5982\u771f\u5b9e\u56fe\u50cf\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5e26\u6709\u5c5e\u6027\u7684\u63d0\u793a\uff0c\u4ece\u800c\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u5408\u6210\u56fe\u50cf\u3002", "result": "\u5728\u4e24\u79cd\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAttrSyn\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u8bad\u7ec3\u6548\u679c\u663e\u8457\u4f18\u4e8eCLIP\u7684\u96f6\u6837\u672c\u5206\u7c7b\uff0c\u5e76\u6301\u7eed\u8d85\u8d8a\u7b80\u5355\u63d0\u793a\u7b56\u7565\u3002", "conclusion": "AttrSyn\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u7684\u5408\u6210\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u9886\u57df\u7279\u5b9a\u56fe\u50cf\u5206\u7c7b\u7684\u6027\u80fd\u3002"}}
{"id": "2504.04771", "pdf": "https://arxiv.org/pdf/2504.04771", "abs": "https://arxiv.org/abs/2504.04771", "authors": ["Leonardo Ranaldi", "Federico Ranaldi", "Fabio Massimo Zanzotto", "Barry Haddow", "Alexandra Birch"], "title": "Improving Multilingual Retrieval-Augmented Language Models through Dialectic Reasoning Argumentations", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) is key to enhancing large language\nmodels (LLMs) to systematically access richer factual knowledge. Yet, using RAG\nbrings intrinsic challenges, as LLMs must deal with potentially conflicting\nknowledge, especially in multilingual retrieval, where the heterogeneity of\nknowledge retrieved may deliver different outlooks. To make RAG more\nanalytical, critical and grounded, we introduce Dialectic-RAG (DRAG), a modular\napproach guided by Argumentative Explanations, i.e., structured reasoning\nprocess that systematically evaluates retrieved\n  information by comparing, contrasting, and resolving conflicting\nperspectives. Given a query and a set of multilingual related documents, DRAG\nselects and exemplifies relevant knowledge for delivering dialectic\nexplanations that, by critically weighing opposing arguments and filtering\nextraneous content, clearly determine the final response. Through a series of\nin-depth experiments, we show the impact of our framework both as an in-context\nlearning strategy and for constructing demonstrations to instruct smaller\nmodels. The final results demonstrate that DRAG significantly improves RAG\napproaches, requiring low-impact computational effort and providing robustness\nto knowledge perturbations.", "AI": {"task": "\u901a\u8fc7\u5f15\u5165Dialectic-RAG (DRAG)\u65b9\u6cd5\uff0c\u63d0\u5347\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u5206\u6790\u548c\u6279\u5224\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3RAG\u5728\u591a\u8bed\u8a00\u68c0\u7d22\u4e2d\u53ef\u80fd\u9047\u5230\u7684\u77e5\u8bc6\u51b2\u7a81\u548c\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u4f7f\u5176\u66f4\u5177\u5206\u6790\u6027\u548c\u6279\u5224\u6027\u3002", "method": "\u63d0\u51faDRAG\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8bba\u8bc1\u6027\u89e3\u91ca\uff08Argumentative Explanations\uff09\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u7cfb\u7edf\u8bc4\u4f30\u68c0\u7d22\u4fe1\u606f\uff0c\u6bd4\u8f83\u3001\u5bf9\u6bd4\u5e76\u89e3\u51b3\u51b2\u7a81\u89c2\u70b9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDRAG\u663e\u8457\u63d0\u5347\u4e86RAG\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u4e14\u5bf9\u77e5\u8bc6\u6270\u52a8\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "DRAG\u4e3aRAG\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5206\u6790\u3001\u6279\u5224\u548c\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u68c0\u7d22\u573a\u666f\u3002"}}
{"id": "2504.04517", "pdf": "https://arxiv.org/pdf/2504.04517", "abs": "https://arxiv.org/abs/2504.04517", "authors": ["Jiancheng Pan", "Yanxing Liu", "Xiao He", "Long Peng", "Jiahao Li", "Yuze Sun", "Xiaomeng Huang"], "title": "Enhance Then Search: An Augmentation-Search Strategy with Foundation Models for Cross-Domain Few-Shot Object Detection", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 6 figures", "summary": "Foundation models pretrained on extensive datasets, such as GroundingDINO and\nLAE-DINO, have performed remarkably in the cross-domain few-shot object\ndetection (CD-FSOD) task. Through rigorous few-shot training, we found that the\nintegration of image-based data augmentation techniques and grid-based\nsub-domain search strategy significantly enhances the performance of these\nfoundation models. Building upon GroundingDINO, we employed several widely used\nimage augmentation methods and established optimization objectives to\neffectively navigate the expansive domain space in search of optimal\nsub-domains. This approach facilitates efficient few-shot object detection and\nintroduces an approach to solving the CD-FSOD problem by efficiently searching\nfor the optimal parameter configuration from the foundation model. Our findings\nsubstantially advance the practical deployment of vision-language models in\ndata-scarce environments, offering critical insights into optimizing their\ncross-domain generalization capabilities without labor-intensive retraining.\nCode is available at https://github.com/jaychempan/ETS.", "AI": {"task": "\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u6570\u636e\u589e\u5f3a\u6280\u672f\u548c\u57fa\u4e8e\u7f51\u683c\u7684\u5b50\u57df\u641c\u7d22\u7b56\u7565\uff0c\u63d0\u5347\u57fa\u7840\u6a21\u578b\u5728\u8de8\u57df\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\uff08CD-FSOD\uff09\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u8de8\u57df\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5982\u4f55\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u6027\u80fd\u5e76\u51cf\u5c11\u5bf9\u5927\u91cf\u6570\u636e\u7684\u4f9d\u8d56\u662f\u5173\u952e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u56fe\u50cf\u6570\u636e\u589e\u5f3a\u6280\u672f\u548c\u57fa\u4e8e\u7f51\u683c\u7684\u5b50\u57df\u641c\u7d22\u7b56\u7565\uff0c\u7ed3\u5408GroundingDINO\u6a21\u578b\uff0c\u4f18\u5316\u53c2\u6570\u914d\u7f6e\u3002", "result": "\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f18\u5316\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\uff0c\u5e76\u51cf\u5c11\u4e86\u91cd\u65b0\u8bad\u7ec3\u7684\u8d1f\u62c5\u3002"}}
{"id": "2504.04782", "pdf": "https://arxiv.org/pdf/2504.04782", "abs": "https://arxiv.org/abs/2504.04782", "authors": ["Mia Jacobsen", "Ross Deans Kristensen-McLachlan"], "title": "I only read it for the plot! Maturity Ratings Affect Fanfiction Style and Community Engagement", "categories": ["cs.CL"], "comment": "Accepted to the 5th International Conference on Natural Language\n  Processing for Digital Humanities (NLP4DH 2025)", "summary": "We consider the textual profiles of different fanfiction maturity ratings,\nhow they vary across fan groups, and how this relates to reader engagement\nmetrics. Previous studies have shown that fanfiction writing is motivated by a\ncombination of admiration for and frustration with the fan object. These\nfindings emerge when looking at fanfiction as a whole, as well as when it is\ndivided into subgroups, also called fandoms. However, maturity ratings are used\nto indicate the intended audience of the fanfiction, as well as whether the\nstory includes mature themes and explicit scenes. Since these ratings can be\nused to filter readers and writers, they can also be seen as a proxy for\ndifferent reader/writer motivations and desires. We find that explicit\nfanfiction in particular has a distinct textual profile when compared to other\nmaturity ratings. These findings thus nuance our understanding of reader/writer\nmotivations in fanfiction communities, and also highlights the influence of the\ncommunity norms and fan behavior more generally on these cultural products.", "AI": {"task": "\u5206\u6790\u4e0d\u540c\u7c89\u4e1d\u5c0f\u8bf4\u6210\u719f\u5ea6\u8bc4\u7ea7\u7684\u6587\u672c\u7279\u5f81\u53ca\u5176\u5bf9\u8bfb\u8005\u53c2\u4e0e\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u7c89\u4e1d\u5c0f\u8bf4\u5199\u4f5c\u52a8\u673a\u4e0e\u6210\u719f\u5ea6\u8bc4\u7ea7\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u793e\u533a\u89c4\u8303\u548c\u7c89\u4e1d\u884c\u4e3a\u5bf9\u6587\u5316\u4ea7\u54c1\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u6210\u719f\u5ea6\u8bc4\u7ea7\u7684\u6587\u672c\u7279\u5f81\u53ca\u5176\u5728\u7c89\u4e1d\u7fa4\u4f53\u4e2d\u7684\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u660e\u786e\u6807\u8bb0\u4e3a\u6210\u4eba\u7684\u7c89\u4e1d\u5c0f\u8bf4\u5177\u6709\u72ec\u7279\u7684\u6587\u672c\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u6df1\u5316\u4e86\u5bf9\u7c89\u4e1d\u793e\u533a\u4e2d\u8bfb\u8005\u548c\u4f5c\u8005\u52a8\u673a\u7684\u7406\u89e3\uff0c\u5e76\u5f3a\u8c03\u4e86\u793e\u533a\u89c4\u8303\u5bf9\u6587\u5316\u4ea7\u54c1\u7684\u5f71\u54cd\u3002"}}
{"id": "2504.04519", "pdf": "https://arxiv.org/pdf/2504.04519", "abs": "https://arxiv.org/abs/2504.04519", "authors": ["Junjie Jiang", "Zelin Wang", "Manqi Zhao", "Yin Li", "DongSheng Jiang"], "title": "SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Segment Anything 2 (SAM2) enables robust single-object tracking using\nsegmentation. To extend this to multi-object tracking (MOT), we propose\nSAM2MOT, introducing a novel Tracking by Segmentation paradigm. Unlike Tracking\nby Detection or Tracking by Query, SAM2MOT directly generates tracking boxes\nfrom segmentation masks, reducing reliance on detection accuracy. SAM2MOT has\ntwo key advantages: zero-shot generalization, allowing it to work across\ndatasets without fine-tuning, and strong object association, inherited from\nSAM2. To further improve performance, we integrate a trajectory manager system\nfor precise object addition and removal, and a cross-object interaction module\nto handle occlusions. Experiments on DanceTrack, UAVDT, and BDD100K show\nstate-of-the-art results. Notably, SAM2MOT outperforms existing methods on\nDanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5206\u5272\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5SAM2MOT\uff0c\u6269\u5c55\u4e86SAM2\u7684\u5355\u76ee\u6807\u8ddf\u8e2a\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u4f9d\u8d56\u68c0\u6d4b\u51c6\u786e\u6027\u7684\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u548c\u5f3a\u5bf9\u8c61\u5173\u8054\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u5206\u5272\u63a9\u7801\u751f\u6210\u8ddf\u8e2a\u6846\uff0c\u5f15\u5165\u8f68\u8ff9\u7ba1\u7406\u7cfb\u7edf\u548c\u8de8\u5bf9\u8c61\u4ea4\u4e92\u6a21\u5757\u3002", "result": "\u5728DanceTrack\u3001UAVDT\u548cBDD100K\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5c24\u5176\u5728DanceTrack\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "SAM2MOT\u5728\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u96f6\u6837\u672c\u6cdb\u5316\u548c\u5f3a\u5bf9\u8c61\u5173\u8054\u7684\u4f18\u52bf\u3002"}}
{"id": "2504.04823", "pdf": "https://arxiv.org/pdf/2504.04823", "abs": "https://arxiv.org/abs/2504.04823", "authors": ["Ruikang Liu", "Yuxuan Sun", "Manyi Zhang", "Haoli Bai", "Xianzhi Yu", "Tiezheng Yu", "Chun Yuan", "Lu Hou"], "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.", "AI": {"task": "\u7814\u7a76\u91cf\u5316\u5bf9\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5c3d\u7ba1\u91cf\u5316\u5df2\u5e7f\u6cdb\u7528\u4e8e\u964d\u4f4e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6210\u672c\uff0c\u4f46\u5176\u5bf9\u63a8\u7406\u6a21\u578b\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u5bf9\u5f00\u6e90\u6a21\u578b\uff08DeepSeek-R1-Distilled Qwen\u3001LLaMA\u5bb6\u65cf\u3001QwQ-32B\uff09\u8fdb\u884c\u6743\u91cd\u3001KV\u7f13\u5b58\u548c\u6fc0\u6d3b\u91cf\u5316\uff0c\u5e76\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0W8A8\u6216W4A16\u91cf\u5316\u53ef\u5b9e\u73b0\u65e0\u635f\uff0c\u4f46\u66f4\u4f4e\u6bd4\u7279\u5bbd\u5ea6\u4f1a\u663e\u8457\u964d\u4f4e\u51c6\u786e\u6027\uff1b\u6a21\u578b\u5927\u5c0f\u3001\u6765\u6e90\u548c\u4efb\u52a1\u96be\u5ea6\u662f\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u91cf\u5316\u6a21\u578b\u6027\u80fd\u53ef\u901a\u8fc7\u8c03\u6574\u6a21\u578b\u89c4\u6a21\u6216\u63a8\u7406\u6b65\u9aa4\u63d0\u5347\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5f00\u6e90\u3002"}}
{"id": "2504.04535", "pdf": "https://arxiv.org/pdf/2504.04535", "abs": "https://arxiv.org/abs/2504.04535", "authors": ["Weikai Lin", "Tianrui Ma", "Adith Boloor", "Yu Feng", "Ruofan Xing", "Xuan Zhang", "Yuhao Zhu"], "title": "SnapPix: Efficient-Coding--Inspired In-Sensor Compression for Edge Vision", "categories": ["cs.CV", "cs.AI", "I.2"], "comment": "7 pages, Accepted to Design Automation Conference (DAC), 2025", "summary": "Energy-efficient image acquisition on the edge is crucial for enabling remote\nsensing applications where the sensor node has weak compute capabilities and\nmust transmit data to a remote server/cloud for processing. To reduce the edge\nenergy consumption, this paper proposes a sensor-algorithm co-designed system\ncalled SnapPix, which compresses raw pixels in the analog domain inside the\nsensor. We use coded exposure (CE) as the in-sensor compression strategy as it\noffers the flexibility to sample, i.e., selectively expose pixels, both\nspatially and temporally. SNAPPIX has three contributions. First, we propose a\ntask-agnostic strategy to learn the sampling/exposure pattern based on the\nclassic theory of efficient coding. Second, we co-design the downstream vision\nmodel with the exposure pattern to address the pixel-level non-uniformity\nunique to CE-compressed images. Finally, we propose lightweight augmentations\nto the image sensor hardware to support our in-sensor CE compression.\nEvaluating on action recognition and video reconstruction, SnapPix outperforms\nstate-of-the-art video-based methods at the same speed while reducing the\nenergy by up to 15.4x. We have open-sourced the code at:\nhttps://github.com/horizon-research/SnapPix.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aSnapPix\u7684\u4f20\u611f\u5668-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u6a21\u62df\u57df\u5185\u538b\u7f29\u539f\u59cb\u50cf\u7d20\u4ee5\u51cf\u5c11\u8fb9\u7f18\u80fd\u8017\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u80fd\u529b\u6709\u9650\u4e14\u9700\u8fdc\u7a0b\u4f20\u8f93\u6570\u636e\uff0c\u964d\u4f4e\u80fd\u8017\u662f\u5173\u952e\u3002", "method": "\u91c7\u7528\u7f16\u7801\u66dd\u5149\uff08CE\uff09\u4f5c\u4e3a\u4f20\u611f\u5668\u5185\u538b\u7f29\u7b56\u7565\uff0c\u63d0\u51fa\u4efb\u52a1\u65e0\u5173\u7684\u91c7\u6837/\u66dd\u5149\u6a21\u5f0f\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u534f\u540c\u8bbe\u8ba1\u4e0b\u6e38\u89c6\u89c9\u6a21\u578b\u4ee5\u89e3\u51b3CE\u538b\u7f29\u56fe\u50cf\u7684\u975e\u5747\u5300\u6027\u95ee\u9898\u3002", "result": "\u5728\u52a8\u4f5c\u8bc6\u522b\u548c\u89c6\u9891\u91cd\u5efa\u4efb\u52a1\u4e2d\uff0cSnapPix\u5728\u76f8\u540c\u901f\u5ea6\u4e0b\u4f18\u4e8e\u73b0\u6709\u89c6\u9891\u65b9\u6cd5\uff0c\u80fd\u8017\u964d\u4f4e\u9ad8\u8fbe15.4\u500d\u3002", "conclusion": "SnapPix\u901a\u8fc7\u4f20\u611f\u5668-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u548c\u786c\u4ef6\u8f7b\u91cf\u7ea7\u6539\u8fdb\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8fb9\u7f18\u56fe\u50cf\u91c7\u96c6\u7684\u80fd\u8017\u3002"}}
{"id": "2504.04849", "pdf": "https://arxiv.org/pdf/2504.04849", "abs": "https://arxiv.org/abs/2504.04849", "authors": ["Sam Kirkham"], "title": "Discovering dynamical laws for speech gestures", "categories": ["cs.CL", "nlin.AO"], "comment": "Accepted for publication in 'Cognitive Science'", "summary": "A fundamental challenge in the cognitive sciences is discovering the dynamics\nthat govern behaviour. Take the example of spoken language, which is\ncharacterised by a highly variable and complex set of physical movements that\nmap onto the small set of cognitive units that comprise language. What are the\nfundamental dynamical principles behind the movements that structure speech\nproduction? In this study, we discover models in the form of symbolic equations\nthat govern articulatory gestures during speech. A sparse symbolic regression\nalgorithm is used to discover models from kinematic data on the tongue and\nlips. We explore these candidate models using analytical techniques and\nnumerical simulations, and find that a second-order linear model achieves high\nlevels of accuracy, but a nonlinear force is required to properly model\narticulatory dynamics in approximately one third of cases. This supports the\nproposal that an autonomous, nonlinear, second-order differential equation is a\nviable dynamical law for articulatory gestures in speech. We conclude by\nidentifying future opportunities and obstacles in data-driven model discovery\nand outline prospects for discovering the dynamical principles that govern\nlanguage, brain and behaviour.", "AI": {"task": "\u53d1\u73b0\u63a7\u5236\u8a00\u8bed\u53d1\u97f3\u52a8\u4f5c\u7684\u52a8\u529b\u5b66\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22\u8a00\u8bed\u53d1\u97f3\u52a8\u4f5c\u80cc\u540e\u7684\u57fa\u672c\u52a8\u529b\u5b66\u539f\u7406\uff0c\u4ee5\u7406\u89e3\u590d\u6742\u7684\u7269\u7406\u8fd0\u52a8\u5982\u4f55\u6620\u5c04\u5230\u8bed\u8a00\u8ba4\u77e5\u5355\u5143\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u7b26\u53f7\u56de\u5f52\u7b97\u6cd5\u4ece\u820c\u5934\u548c\u5634\u5507\u7684\u8fd0\u52a8\u5b66\u6570\u636e\u4e2d\u53d1\u73b0\u7b26\u53f7\u65b9\u7a0b\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u548c\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u5019\u9009\u6a21\u578b\u3002", "result": "\u4e8c\u9636\u7ebf\u6027\u6a21\u578b\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7ea6\u4e09\u5206\u4e4b\u4e00\u7684\u60c5\u51b5\u9700\u8981\u975e\u7ebf\u6027\u529b\u6765\u51c6\u786e\u5efa\u6a21\u53d1\u97f3\u52a8\u4f5c\u3002", "conclusion": "\u81ea\u4e3b\u3001\u975e\u7ebf\u6027\u3001\u4e8c\u9636\u5fae\u5206\u65b9\u7a0b\u662f\u63cf\u8ff0\u8a00\u8bed\u53d1\u97f3\u52a8\u4f5c\u7684\u53ef\u884c\u52a8\u529b\u5b66\u5b9a\u5f8b\uff0c\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u6570\u636e\u9a71\u52a8\u6a21\u578b\u53d1\u73b0\u7684\u6f5c\u529b\u3002"}}
{"id": "2504.04540", "pdf": "https://arxiv.org/pdf/2504.04540", "abs": "https://arxiv.org/abs/2504.04540", "authors": ["Weichen Zhang", "Ruiying Peng", "Chen Gao", "Jianjie Fang", "Xin Zeng", "Kaiyuan Li", "Ziyou Wang", "Jinqiang Cui", "Xin Wang", "Xinlei Chen", "Yong Li"], "title": "The Point, the Vision and the Text: Does Point Cloud Boost Spatial Reasoning of Large Language Models?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "3D Large Language Models (LLMs) leveraging spatial information in point\nclouds for 3D spatial reasoning attract great attention. Despite some promising\nresults, the role of point clouds in 3D spatial reasoning remains\nunder-explored. In this work, we comprehensively evaluate and analyze these\nmodels to answer the research question: \\textit{Does point cloud truly boost\nthe spatial reasoning capacities of 3D LLMs?} We first evaluate the spatial\nreasoning capacity of LLMs with different input modalities by replacing the\npoint cloud with the visual and text counterparts. We then propose a novel 3D\nQA (Question-answering) benchmark, ScanReQA, that comprehensively evaluates\nmodels' understanding of binary spatial relationships. Our findings reveal\nseveral critical insights: 1) LLMs without point input could even achieve\ncompetitive performance even in a zero-shot manner; 2) existing 3D LLMs\nstruggle to comprehend the binary spatial relationships; 3) 3D LLMs exhibit\nlimitations in exploiting the structural coordinates in point clouds for\nfine-grained spatial reasoning. We think these conclusions can help the next\nstep of 3D LLMs and also offer insights for foundation models in other\nmodalities. We release datasets and reproducible codes in the anonymous project\npage: https://3d-llm.xyz.", "AI": {"task": "\u8bc4\u4f30\u70b9\u4e91\u662f\u5426\u771f\u6b63\u63d0\u5347\u4e863D\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u70b9\u4e91\u57283D\u7a7a\u95f4\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5177\u4f53\u4f5c\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u56e0\u6b64\u9700\u8981\u5168\u9762\u8bc4\u4f30\u548c\u5206\u6790\u3002", "method": "\u901a\u8fc7\u66ff\u6362\u70b9\u4e91\u4e3a\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\uff0c\u8bc4\u4f30\u4e0d\u540c\u8f93\u5165\u6a21\u6001\u4e0bLLMs\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u65b0\u76843D\u95ee\u7b54\u57fa\u51c6ScanReQA\u3002", "result": "1\uff09\u65e0\u70b9\u4e91\u8f93\u5165\u7684LLMs\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u8868\u73b0\u7ade\u4e89\u6027\uff1b2\uff09\u73b0\u67093D LLMs\u96be\u4ee5\u7406\u89e3\u4e8c\u5143\u7a7a\u95f4\u5173\u7cfb\uff1b3\uff093D LLMs\u5728\u5229\u7528\u70b9\u4e91\u7ed3\u6784\u5750\u6807\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u63a8\u7406\u65f6\u5b58\u5728\u5c40\u9650\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a3D LLMs\u7684\u4e0b\u4e00\u6b65\u53d1\u5c55\u63d0\u4f9b\u4e86\u5e2e\u52a9\uff0c\u5e76\u4e3a\u5176\u4ed6\u6a21\u6001\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2504.04861", "pdf": "https://arxiv.org/pdf/2504.04861", "abs": "https://arxiv.org/abs/2504.04861", "authors": ["Hongtao Wang", "Renchi Yang", "Hewen Wang", "Haoran Zheng", "Jianliang Xu"], "title": "SAFT: Structure-aware Transformers for Textual Interaction Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Textual interaction networks (TINs) are an omnipresent data structure used to\nmodel the interplay between users and items on e-commerce websites, social\nnetworks, etc., where each interaction is associated with a text description.\nClassifying such textual interactions (TIC) finds extensive use in detecting\nspam reviews in e-commerce, fraudulent transactions in finance, and so on.\nExisting TIC solutions either (i) fail to capture the rich text semantics due\nto the use of context-free text embeddings, and/or (ii) disregard the bipartite\nstructure and node heterogeneity of TINs, leading to compromised TIC\nperformance. In this work, we propose SAFT, a new architecture that integrates\nlanguage- and graph-based modules for the effective fusion of textual and\nstructural semantics in the representation learning of interactions. In\nparticular, line graph attention (LGA)/gated attention units (GAUs) and\npretrained language models (PLMs) are capitalized on to model the\ninteraction-level and token-level signals, which are further coupled via the\nproxy token in an iterative and contextualized fashion. Additionally, an\nefficient and theoretically-grounded approach is developed to encode the local\nand global topology information pertaining to interactions into structural\nembeddings. The resulting embeddings not only inject the structural features\nunderlying TINs into the textual interaction encoding but also facilitate the\ndesign of graph sampling strategies. Extensive empirical evaluations on\nmultiple real TIN datasets demonstrate the superiority of SAFT over the\nstate-of-the-art baselines in TIC accuracy.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aSAFT\u7684\u65b0\u67b6\u6784\uff0c\u7528\u4e8e\u6709\u6548\u878d\u5408\u6587\u672c\u548c\u7ed3\u6784\u8bed\u4e49\u4ee5\u63d0\u5347\u6587\u672c\u4ea4\u4e92\u5206\u7c7b\uff08TIC\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709TIC\u89e3\u51b3\u65b9\u6848\u672a\u80fd\u5145\u5206\u6355\u6349\u6587\u672c\u8bed\u4e49\u6216\u5ffd\u89c6\u6587\u672c\u4ea4\u4e92\u7f51\u7edc\uff08TINs\uff09\u7684\u4e8c\u5206\u7ed3\u6784\u548c\u8282\u70b9\u5f02\u8d28\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u7ed3\u5408\u8bed\u8a00\u548c\u56fe\u6a21\u5757\uff0c\u5229\u7528\u7ebf\u56fe\u6ce8\u610f\u529b\uff08LGA\uff09/\u95e8\u63a7\u6ce8\u610f\u529b\u5355\u5143\uff08GAUs\uff09\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u5efa\u6a21\u4ea4\u4e92\u7ea7\u548c\u8bcd\u7ea7\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u4ee3\u7406\u4ee4\u724c\u8fed\u4ee3\u8026\u5408\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u7406\u8bba\u65b9\u6cd5\u7f16\u7801\u5c40\u90e8\u548c\u5168\u5c40\u62d3\u6251\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9eTIN\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAFT\u5728TIC\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SAFT\u901a\u8fc7\u878d\u5408\u6587\u672c\u548c\u7ed3\u6784\u8bed\u4e49\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u4ea4\u4e92\u5206\u7c7b\u7684\u6027\u80fd\u3002"}}
{"id": "2504.04549", "pdf": "https://arxiv.org/pdf/2504.04549", "abs": "https://arxiv.org/abs/2504.04549", "authors": ["Han Yuan", "Lican Kang", "Yong Li"], "title": "Opening the black box of deep learning: Validating the statistical association between explainable artificial intelligence (XAI) and clinical domain knowledge in fundus image-based glaucoma diagnosis", "categories": ["cs.CV"], "comment": null, "summary": "While deep learning has exhibited remarkable predictive capabilities in\nvarious medical image tasks, its inherent black-box nature has hindered its\nwidespread implementation in real-world healthcare settings. Our objective is\nto unveil the decision-making processes of deep learning models in the context\nof glaucoma classification by employing several Class Activation Map (CAM)\ntechniques to generate model focus regions and comparing them with clinical\ndomain knowledge of the anatomical area (optic cup, optic disk, and blood\nvessels). Four deep neural networks, including VGG-11, ResNet-18, DeiT-Tiny,\nand Swin Transformer-Tiny, were developed using binary diagnostic labels of\nglaucoma and five CAM methods (Grad-CAM, XGrad-CAM, Score-CAM, Eigen-CAM, and\nLayer-CAM) were employed to highlight the model focus area. We applied the\npaired-sample t-test to compare the percentage of anatomies in the model focus\narea to the proportion of anatomies in the entire image. After that, Pearson's\nand Spearman's correlation tests were implemented to examine the relationship\nbetween model predictive ability and the percentage of anatomical structures in\nthe model focus area. On five public glaucoma datasets, all deep learning\nmodels consistently displayed statistically significantly higher percentages of\nanatomical structures in the focus area than the proportions of anatomical\nstructures in the entire image. Also, we validated the positive relationship\nbetween the percentage of anatomical structures in the focus area and model\npredictive performance. Our study provides evidence of the convergence of\ndecision logic between deep neural networks and human clinicians through\nrigorous statistical tests. We anticipate that it can help alleviate\nclinicians' concerns regarding the trustworthiness of deep learning in\nhealthcare. For reproducibility, the code and dataset have been released at\nGitHub.", "AI": {"task": "\u901a\u8fc7\u591a\u79cd\u7c7b\u6fc0\u6d3b\u6620\u5c04\uff08CAM\uff09\u6280\u672f\u63ed\u793a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u9752\u5149\u773c\u5206\u7c7b\u4e2d\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u9ed1\u76d2\u7279\u6027\u963b\u788d\u4e86\u5176\u5728\u73b0\u5b9e\u533b\u7597\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08VGG-11\u3001ResNet-18\u3001DeiT-Tiny\u548cSwin Transformer-Tiny\uff09\u548c\u4e94\u79cdCAM\u65b9\u6cd5\uff08Grad-CAM\u3001XGrad-CAM\u3001Score-CAM\u3001Eigen-CAM\u548cLayer-CAM\uff09\u751f\u6210\u6a21\u578b\u5173\u6ce8\u533a\u57df\uff0c\u5e76\u4e0e\u4e34\u5e8a\u89e3\u5256\u77e5\u8bc6\u5bf9\u6bd4\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728\u5173\u6ce8\u533a\u57df\u4e2d\u89e3\u5256\u7ed3\u6784\u7684\u6bd4\u4f8b\u663e\u8457\u9ad8\u4e8e\u6574\u5f20\u56fe\u50cf\u4e2d\u7684\u6bd4\u4f8b\uff0c\u4e14\u6a21\u578b\u9884\u6d4b\u80fd\u529b\u4e0e\u5173\u6ce8\u533a\u57df\u4e2d\u89e3\u5256\u7ed3\u6784\u7684\u6bd4\u4f8b\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e0e\u4eba\u7c7b\u4e34\u5e8a\u533b\u751f\u7684\u51b3\u7b56\u903b\u8f91\u5b58\u5728\u4e00\u81f4\u6027\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5bf9\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u7597\u4e2d\u53ef\u4fe1\u5ea6\u7684\u4fe1\u5fc3\u3002"}}
{"id": "2504.04891", "pdf": "https://arxiv.org/pdf/2504.04891", "abs": "https://arxiv.org/abs/2504.04891", "authors": ["Longdi Xian", "Jianzhang Ni", "Mingzhu Wang"], "title": "Leveraging Large Language Models for Cost-Effective, Multilingual Depression Detection and Severity Assessment", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Depression is a prevalent mental health disorder that is difficult to detect\nearly due to subjective symptom assessments. Recent advancements in large\nlanguage models have offered efficient and cost-effective approaches for this\nobjective. In this study, we evaluated the performance of four LLMs in\ndepression detection using clinical interview data. We selected the best\nperforming model and further tested it in the severity evaluation scenario and\nknowledge enhanced scenario. The robustness was evaluated in complex diagnostic\nscenarios using a dataset comprising 51074 statements from six different mental\ndisorders. We found that DeepSeek V3 is the most reliable and cost-effective\nmodel for depression detection, performing well in both zero-shot and few-shot\nscenarios, with zero-shot being the most efficient choice. The evaluation of\nseverity showed low agreement with the human evaluator, particularly for mild\ndepression. The model maintains stably high AUCs for detecting depression in\ncomplex diagnostic scenarios. These findings highlight DeepSeek V3s strong\npotential for text-based depression detection in real-world clinical\napplications. However, they also underscore the need for further refinement in\nseverity assessment and the mitigation of potential biases to enhance clinical\nreliability.", "AI": {"task": "\u8bc4\u4f30\u56db\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6291\u90c1\u75c7\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u8fdb\u4e00\u6b65\u6d4b\u8bd5\u6700\u4f73\u6a21\u578b\u5728\u4e25\u91cd\u6027\u8bc4\u4f30\u548c\u77e5\u8bc6\u589e\u5f3a\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u6291\u90c1\u75c7\u662f\u4e00\u79cd\u666e\u904d\u7684\u5fc3\u7406\u5065\u5eb7\u969c\u788d\uff0c\u65e9\u671f\u68c0\u6d4b\u56f0\u96be\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e34\u5e8a\u8bbf\u8c08\u6570\u636e\u8bc4\u4f30\u56db\u79cdLLM\u7684\u6027\u80fd\uff0c\u9009\u62e9\u6700\u4f73\u6a21\u578b\u5e76\u5728\u4e25\u91cd\u6027\u8bc4\u4f30\u548c\u77e5\u8bc6\u589e\u5f3a\u573a\u666f\u4e2d\u6d4b\u8bd5\uff0c\u540c\u65f6\u5728\u590d\u6742\u8bca\u65ad\u573a\u666f\u4e2d\u8bc4\u4f30\u9c81\u68d2\u6027\u3002", "result": "DeepSeek V3\u662f\u6700\u53ef\u9760\u4e14\u7ecf\u6d4e\u7684\u6291\u90c1\u75c7\u68c0\u6d4b\u6a21\u578b\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u4e25\u91cd\u6027\u8bc4\u4f30\u4e2d\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u8005\u4e00\u81f4\u6027\u8f83\u4f4e\u3002", "conclusion": "DeepSeek V3\u5728\u6291\u90c1\u75c7\u68c0\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4e25\u91cd\u6027\u8bc4\u4f30\u548c\u51cf\u5c11\u6f5c\u5728\u504f\u89c1\u4ee5\u63d0\u9ad8\u4e34\u5e8a\u53ef\u9760\u6027\u3002"}}
{"id": "2504.04550", "pdf": "https://arxiv.org/pdf/2504.04550", "abs": "https://arxiv.org/abs/2504.04550", "authors": ["Alkesh Patel", "Vibhav Chitalia", "Yinfei Yang"], "title": "Advancing Egocentric Video Question Answering with Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "8 pages", "summary": "Egocentric Video Question Answering (QA) requires models to handle\nlong-horizon temporal reasoning, first-person perspectives, and specialized\nchallenges like frequent camera movement. This paper systematically evaluates\nboth proprietary and open-source Multimodal Large Language Models (MLLMs) on\nQaEgo4Dv2 - a refined dataset of egocentric videos derived from QaEgo4D. Four\npopular MLLMs (GPT-4o, Gemini-1.5-Pro, Video-LLaVa-7B and Qwen2-VL-7B-Instruct)\nare assessed using zero-shot and fine-tuned approaches for both OpenQA and\nCloseQA settings. We introduce QaEgo4Dv2 to mitigate annotation noise in\nQaEgo4D, enabling more reliable comparison. Our results show that fine-tuned\nVideo-LLaVa-7B and Qwen2-VL-7B-Instruct achieve new state-of-the-art\nperformance, surpassing previous benchmarks by up to +2.6% ROUGE/METEOR (for\nOpenQA) and +13% accuracy (for CloseQA). We also present a thorough error\nanalysis, indicating the model's difficulty in spatial reasoning and\nfine-grained object recognition - key areas for future improvement.", "AI": {"task": "\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728QaEgo4Dv2\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5305\u62ec\u96f6\u6837\u672c\u548c\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u95ee\u7b54\u4e2d\u7684\u957f\u65f6\u7a0b\u65f6\u5e8f\u63a8\u7406\u548c\u76f8\u673a\u79fb\u52a8\u7b49\u6311\u6218\u3002", "method": "\u4f7f\u7528\u56db\u79cdMLLMs\uff08GPT-4o\u3001Gemini-1.5-Pro\u3001Video-LLaVa-7B\u548cQwen2-VL-7B-Instruct\uff09\u5728QaEgo4Dv2\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bc4\u4f30\u3002", "result": "\u5fae\u8c03\u7684Video-LLaVa-7B\u548cQwen2-VL-7B-Instruct\u5728OpenQA\u548cCloseQA\u4e2d\u5206\u522b\u63d0\u5347\u4e862.6%\u548c13%\u7684\u6027\u80fd\u3002", "conclusion": "\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u7ec6\u7c92\u5ea6\u7269\u4f53\u8bc6\u522b\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2504.04915", "pdf": "https://arxiv.org/pdf/2504.04915", "abs": "https://arxiv.org/abs/2504.04915", "authors": ["Ran Xu", "Wenqi Shi", "Yuchen Zhuang", "Yue Yu", "Joyce C. Ho", "Haoyu Wang", "Carl Yang"], "title": "Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Work in progress. Code: https://github.com/ritaranx/Collab-RAG/", "summary": "Retrieval-Augmented Generation (RAG) systems often struggle to handle\nmulti-hop question-answering tasks accurately due to irrelevant context\nretrieval and limited complex reasoning capabilities. We introduce Collab-RAG,\na collaborative training framework that leverages mutual enhancement between a\nwhite-box small language model (SLM) and a blackbox large language model (LLM)\nfor RAG. Specifically, the SLM decomposes complex queries into simpler\nsub-questions, thus enhancing the accuracy of the retrieval and facilitating\nmore effective reasoning by the black-box LLM. Concurrently, the black-box LLM\nprovides feedback signals to improve the SLM's decomposition capability. We\nobserve that Collab-RAG relies solely on supervision from an affordable\nblack-box LLM without additional distillation from frontier LLMs, yet\ndemonstrates strong generalization across multiple black-box LLMs. Experimental\nevaluations across five multi-hop QA datasets demonstrate that Collab-RAG\nsubstantially outperforms existing black-box-only and SLM fine-tuning baselines\nby 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a\nfrozen 32B LLM in question decomposition, highlighting the efficiency of\nCollab-RAG in improving reasoning and retrieval for complex questions. The code\nof Collab-RAG is available on https://github.com/ritaranx/Collab-RAG/.", "AI": {"task": "\u63d0\u51faCollab-RAG\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u4f5c\u8bad\u7ec3\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3RAG\u7cfb\u7edf\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u56e0\u65e0\u5173\u4e0a\u4e0b\u6587\u68c0\u7d22\u548c\u6709\u9650\u590d\u6742\u63a8\u7406\u80fd\u529b\u5bfc\u81f4\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u767d\u76d2\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u548c\u9ed1\u76d2\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u534f\u4f5c\u8bad\u7ec3\uff0cSLM\u5206\u89e3\u590d\u6742\u95ee\u9898\u4e3a\u5b50\u95ee\u9898\uff0cLLM\u63d0\u4f9b\u53cd\u9988\u4fe1\u53f7\u3002", "result": "\u5728\u4e94\u4e2a\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf1.8%-14.2%\uff0c3B SLM\u5728\u95ee\u9898\u5206\u89e3\u4e0a\u8d85\u8d8a32B LLM\u3002", "conclusion": "Collab-RAG\u901a\u8fc7\u534f\u4f5c\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u590d\u6742\u95ee\u9898\u7684\u63a8\u7406\u548c\u68c0\u7d22\u6548\u7387\uff0c\u4e14\u65e0\u9700\u989d\u5916\u84b8\u998f\u524d\u6cbfLLM\u3002"}}
{"id": "2504.04566", "pdf": "https://arxiv.org/pdf/2504.04566", "abs": "https://arxiv.org/abs/2504.04566", "authors": ["Maregu Assefa", "Muzammal Naseer", "Iyyakutti Iyappan Ganapathi", "Syed Sadaf Ali", "Mohamed L Seghier", "Naoufel Werghi"], "title": "DyCON: Dynamic Uncertainty-aware Consistency and Contrastive Learning for Semi-supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Semi-supervised learning in medical image segmentation leverages unlabeled\ndata to reduce annotation burdens through consistency learning. However,\ncurrent methods struggle with class imbalance and high uncertainty from\npathology variations, leading to inaccurate segmentation in 3D medical images.\nTo address these challenges, we present DyCON, a Dynamic Uncertainty-aware\nConsistency and Contrastive Learning framework that enhances the generalization\nof consistency methods with two complementary losses: Uncertainty-aware\nConsistency Loss (UnCL) and Focal Entropy-aware Contrastive Loss (FeCL). UnCL\nenforces global consistency by dynamically weighting the contribution of each\nvoxel to the consistency loss based on its uncertainty, preserving\nhigh-uncertainty regions instead of filtering them out. Initially, UnCL\nprioritizes learning from uncertain voxels with lower penalties, encouraging\nthe model to explore challenging regions. As training progress, the penalty\nshift towards confident voxels to refine predictions and ensure global\nconsistency. Meanwhile, FeCL enhances local feature discrimination in\nimbalanced regions by introducing dual focal mechanisms and adaptive confidence\nadjustments into the contrastive principle. These mechanisms jointly\nprioritizes hard positives and negatives while focusing on uncertain sample\npairs, effectively capturing subtle lesion variations under class imbalance.\nExtensive evaluations on four diverse medical image segmentation datasets\n(ISLES'22, BraTS'19, LA, Pancreas) show DyCON's superior performance against\nSOTA methods.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4e00\u81f4\u6027\u548c\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff08DyCON\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u9ad8\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u56e0\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u75c5\u7406\u53d8\u5316\u5bfc\u81f4\u7684\u9ad8\u4e0d\u786e\u5b9a\u6027\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "DyCON\u7ed3\u5408\u4e86\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4e00\u81f4\u6027\u635f\u5931\uff08UnCL\uff09\u548c\u7126\u70b9\u71b5\u611f\u77e5\u5bf9\u6bd4\u635f\u5931\uff08FeCL\uff09\uff0c\u52a8\u6001\u8c03\u6574\u6743\u91cd\u4ee5\u4f18\u5316\u5168\u5c40\u4e00\u81f4\u6027\u548c\u5c40\u90e8\u7279\u5f81\u533a\u5206\u3002", "result": "\u5728\u56db\u4e2a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\uff08ISLES'22\u3001BraTS'19\u3001LA\u3001Pancreas\uff09\u4e0a\uff0cDyCON\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DyCON\u901a\u8fc7\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.04953", "pdf": "https://arxiv.org/pdf/2504.04953", "abs": "https://arxiv.org/abs/2504.04953", "authors": ["Jos\u00e9 Pombal", "Dongkeun Yoon", "Patrick Fernandes", "Ian Wu", "Seungone Kim", "Ricardo Rei", "Graham Neubig", "Andr\u00e9 F. T. Martins"], "title": "M-Prometheus: A Suite of Open Multilingual LLM Judges", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The use of language models for automatically evaluating long-form text\n(LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are\noptimized exclusively for English, with strategies for enhancing their\nmultilingual evaluation capabilities remaining largely unexplored in the\ncurrent literature. This has created a disparity in the quality of automatic\nevaluation methods for non-English languages, ultimately hindering the\ndevelopment of models with better multilingual capabilities. To bridge this\ngap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from\n3B to 14B parameters that can provide both direct assessment and pairwise\ncomparison feedback on multilingual outputs. M-Prometheus models outperform\nstate-of-the-art open LLM judges on multilingual reward benchmarks spanning\nmore than 20 languages, as well as on literary machine translation (MT)\nevaluation covering 4 language pairs. Furthermore, M-Prometheus models can be\nleveraged at decoding time to significantly improve generated outputs across\nall 3 tested languages, showcasing their utility for the development of better\nmultilingual models. Lastly, through extensive ablations, we identify the key\nfactors for obtaining an effective multilingual judge, including backbone model\nselection and training on natively multilingual feedback data instead of\ntranslated data. We release our models, training dataset, and code.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u591a\u8bed\u8a00\u81ea\u52a8\u8bc4\u4f30\u957f\u6587\u672c\u7684\u8bed\u8a00\u6a21\u578b\uff08M-Prometheus\uff09\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5de5\u5177\u4ec5\u9488\u5bf9\u82f1\u8bed\u4f18\u5316\uff0c\u7f3a\u4e4f\u591a\u8bed\u8a00\u8bc4\u4f30\u80fd\u529b\uff0c\u963b\u788d\u4e86\u591a\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faM-Prometheus\uff0c\u4e00\u5957\u53c2\u6570\u89c4\u6a21\u4ece3B\u523014B\u7684\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u591a\u8bed\u8a00\u8f93\u51fa\u7684\u76f4\u63a5\u8bc4\u4f30\u548c\u6210\u5bf9\u6bd4\u8f83\u53cd\u9988\u3002", "result": "M-Prometheus\u5728\u591a\u8bed\u8a00\u5956\u52b1\u57fa\u51c6\u6d4b\u8bd5\u548c\u6587\u5b66\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u80fd\u663e\u8457\u63d0\u5347\u751f\u6210\u6587\u672c\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u786e\u5b9a\u4e86\u6784\u5efa\u6709\u6548\u591a\u8bed\u8a00\u8bc4\u4f30\u6a21\u578b\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u516c\u5f00\u4e86\u6a21\u578b\u3001\u8bad\u7ec3\u6570\u636e\u548c\u4ee3\u7801\u3002"}}
{"id": "2504.04572", "pdf": "https://arxiv.org/pdf/2504.04572", "abs": "https://arxiv.org/abs/2504.04572", "authors": ["Mohamed Eltahir", "Osamah Sarraj", "Mohammed Bremoo", "Mohammed Khurd", "Abdulrahman Alfrihidi", "Taha Alshatiri", "Mohammad Almatrafi", "Tanveer Hussain"], "title": "Multimodal Lengthy Videos Retrieval Framework and Evaluation Metric", "categories": ["cs.CV"], "comment": null, "summary": "Precise video retrieval requires multi-modal correlations to handle unseen\nvocabulary and scenes, becoming more complex for lengthy videos where models\nmust perform effectively without prior training on a specific dataset. We\nintroduce a unified framework that combines a visual matching stream and an\naural matching stream with a unique subtitles-based video segmentation\napproach. Additionally, the aural stream includes a complementary audio-based\ntwo-stage retrieval mechanism that enhances performance on long-duration\nvideos. Considering the complex nature of retrieval from lengthy videos and its\ncorresponding evaluation, we introduce a new retrieval evaluation method\nspecifically designed for long-video retrieval to support further research. We\nconducted experiments on the YouCook2 benchmark, showing promising retrieval\nperformance.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u5339\u914d\u6d41\u548c\u542c\u89c9\u5339\u914d\u6d41\uff0c\u7528\u4e8e\u7cbe\u786e\u7684\u89c6\u9891\u68c0\u7d22\uff0c\u7279\u522b\u662f\u9488\u5bf9\u957f\u89c6\u9891\u3002", "motivation": "\u89e3\u51b3\u957f\u89c6\u9891\u68c0\u7d22\u4e2d\u591a\u6a21\u6001\u5173\u8054\u7684\u590d\u6742\u6027\uff0c\u4ee5\u53ca\u672a\u89c1\u8bcd\u6c47\u548c\u573a\u666f\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u5339\u914d\u6d41\u548c\u542c\u89c9\u5339\u914d\u6d41\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5b57\u5e55\u7684\u89c6\u9891\u5206\u5272\u65b9\u6cd5\u548c\u4e24\u9636\u6bb5\u97f3\u9891\u68c0\u7d22\u673a\u5236\u3002", "result": "\u5728YouCook2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u6709\u524d\u666f\u7684\u68c0\u7d22\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u65b0\u7684\u957f\u89c6\u9891\u68c0\u7d22\u8bc4\u4f30\u65b9\u6cd5\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2504.04963", "pdf": "https://arxiv.org/pdf/2504.04963", "abs": "https://arxiv.org/abs/2504.04963", "authors": ["Yuzhe Zhang", "Min Cen", "Hong Zhang"], "title": "Constraint Multi-class Positive and Unlabeled Learning for Distantly Supervised Named Entity Recognition", "categories": ["cs.CL"], "comment": "28pages, 3 figures. First submitted in Oct. 2023", "summary": "Distantly supervised named entity recognition (DS-NER) has been proposed to\nexploit the automatically labeled training data by external knowledge bases\ninstead of human annotations. However, it tends to suffer from a high false\nnegative rate due to the inherent incompleteness. To address this issue, we\npresent a novel approach called \\textbf{C}onstraint \\textbf{M}ulti-class\n\\textbf{P}ositive and \\textbf{U}nlabeled Learning (CMPU), which introduces a\nconstraint factor on the risk estimator of multiple positive classes. It\nsuggests that the constraint non-negative risk estimator is more robust against\noverfitting than previous PU learning methods with limited positive data. Solid\ntheoretical analysis on CMPU is provided to prove the validity of our approach.\nExtensive experiments on two benchmark datasets that were labeled using diverse\nexternal knowledge sources serve to demonstrate the superior performance of\nCMPU in comparison to existing DS-NER methods.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aCMPU\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8fdc\u7a0b\u76d1\u7763\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08DS-NER\uff09\u4e2d\u7684\u9ad8\u5047\u9634\u6027\u7387\u95ee\u9898\u3002", "motivation": "\u8fdc\u7a0b\u76d1\u7763\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08DS-NER\uff09\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\u5e93\u81ea\u52a8\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5b58\u5728\u56fa\u6709\u7684\u4e0d\u5b8c\u6574\u6027\u5bfc\u81f4\u9ad8\u5047\u9634\u6027\u7387\u3002", "method": "\u5f15\u5165\u7ea6\u675f\u591a\u7c7b\u6b63\u4f8b\u548c\u65e0\u6807\u8bb0\u5b66\u4e60\uff08CMPU\uff09\uff0c\u5728\u591a\u4e2a\u6b63\u7c7b\u7684\u98ce\u9669\u4f30\u8ba1\u5668\u4e2d\u52a0\u5165\u7ea6\u675f\u56e0\u5b50\uff0c\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660eCMPU\u7684\u6709\u6548\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709DS-NER\u65b9\u6cd5\u3002", "conclusion": "CMPU\u662f\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3DS-NER\u4e2d\u7684\u9ad8\u5047\u9634\u6027\u7387\u95ee\u9898\u3002"}}
{"id": "2504.04582", "pdf": "https://arxiv.org/pdf/2504.04582", "abs": "https://arxiv.org/abs/2504.04582", "authors": ["Nicolo Resmini", "Eugenio Lomurno", "Cristian Sbrolli", "Matteo Matteucci"], "title": "Your Image Generator Is Your New Private Dataset", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Generative diffusion models have emerged as powerful tools to synthetically\nproduce training data, offering potential solutions to data scarcity and\nreducing labelling costs for downstream supervised deep learning applications.\nHowever, effectively leveraging text-conditioned image generation for building\nclassifier training sets requires addressing key issues: constructing\ninformative textual prompts, adapting generative models to specific domains,\nand ensuring robust performance. This paper proposes the Text-Conditioned\nKnowledge Recycling (TCKR) pipeline to tackle these challenges. TCKR combines\ndynamic image captioning, parameter-efficient diffusion model fine-tuning, and\nGenerative Knowledge Distillation techniques to create synthetic datasets\ntailored for image classification. The pipeline is rigorously evaluated on ten\ndiverse image classification benchmarks. The results demonstrate that models\ntrained solely on TCKR-generated data achieve classification accuracies on par\nwith (and in several cases exceeding) models trained on real images.\nFurthermore, the evaluation reveals that these synthetic-data-trained models\nexhibit substantially enhanced privacy characteristics: their vulnerability to\nMembership Inference Attacks is significantly reduced, with the membership\ninference AUC lowered by 5.49 points on average compared to using real training\ndata, demonstrating a substantial improvement in the performance-privacy\ntrade-off. These findings indicate that high-fidelity synthetic data can\neffectively replace real data for training classifiers, yielding strong\nperformance whilst simultaneously providing improved privacy protection as a\nvaluable emergent property. The code and trained models are available in the\naccompanying open-source repository.", "AI": {"task": "\u63d0\u51faText-Conditioned Knowledge Recycling (TCKR)\u6d41\u7a0b\uff0c\u89e3\u51b3\u6587\u672c\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u5728\u6784\u5efa\u5206\u7c7b\u5668\u8bad\u7ec3\u96c6\u65f6\u7684\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u6269\u6563\u6a21\u578b\u80fd\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u6807\u6ce8\u6210\u672c\u95ee\u9898\uff0c\u4f46\u9700\u89e3\u51b3\u6587\u672c\u63d0\u793a\u6784\u5efa\u3001\u9886\u57df\u9002\u5e94\u548c\u6027\u80fd\u9c81\u68d2\u6027\u7b49\u6311\u6218\u3002", "method": "\u7ed3\u5408\u52a8\u6001\u56fe\u50cf\u63cf\u8ff0\u3001\u53c2\u6570\u9ad8\u6548\u6269\u6563\u6a21\u578b\u5fae\u8c03\u548c\u751f\u6210\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u751f\u6210\u5b9a\u5236\u5316\u7684\u5408\u6210\u6570\u636e\u96c6\u3002", "result": "\u5728\u5341\u4e2a\u5206\u7c7b\u57fa\u51c6\u4e0a\uff0cTCKR\u751f\u6210\u7684\u6570\u636e\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u6027\u80fd\u4e0e\u771f\u5b9e\u6570\u636e\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u9690\u79c1\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u9ad8\u4fdd\u771f\u5408\u6210\u6570\u636e\u53ef\u66ff\u4ee3\u771f\u5b9e\u6570\u636e\uff0c\u63d0\u4f9b\u5f3a\u6027\u80fd\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684\u53cc\u91cd\u4f18\u52bf\u3002"}}
{"id": "2504.04966", "pdf": "https://arxiv.org/pdf/2504.04966", "abs": "https://arxiv.org/abs/2504.04966", "authors": ["Shion Fukuhata", "Yoshinobu Kano"], "title": "Few Dimensions are Enough: Fine-tuning BERT with Selected Dimensions Revealed Its Redundant Nature", "categories": ["cs.CL"], "comment": "11 pages, 2 figures", "summary": "When fine-tuning BERT models for specific tasks, it is common to select part\nof the final layer's output and input it into a newly created fully connected\nlayer. However, it remains unclear which part of the final layer should be\nselected and what information each dimension of the layers holds. In this\nstudy, we comprehensively investigated the effectiveness and redundancy of\ntoken vectors, layers, and dimensions through BERT fine-tuning on GLUE tasks.\nThe results showed that outputs other than the CLS vector in the final layer\ncontain equivalent information, most tasks require only 2-3 dimensions, and\nwhile the contribution of lower layers decreases, there is little difference\namong higher layers. We also evaluated the impact of freezing pre-trained\nlayers and conducted cross-fine-tuning, where fine-tuning is applied\nsequentially to different tasks. The findings suggest that hidden layers may\nchange significantly during fine-tuning, BERT has considerable redundancy,\nenabling it to handle multiple tasks simultaneously, and its number of\ndimensions may be excessive.", "AI": {"task": "\u7814\u7a76\u5728BERT\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5982\u4f55\u9009\u62e9\u6700\u7ec8\u5c42\u7684\u8f93\u51fa\u90e8\u5206\u4ee5\u53ca\u5404\u7ef4\u5ea6\u4fe1\u606f\u7684\u4f5c\u7528\u3002", "motivation": "\u5f53\u524dBERT\u5fae\u8c03\u65f6\u901a\u5e38\u9009\u62e9\u6700\u7ec8\u5c42\u7684\u90e8\u5206\u8f93\u51fa\u8f93\u5165\u5230\u65b0\u7684\u5168\u8fde\u63a5\u5c42\uff0c\u4f46\u5177\u4f53\u9009\u62e9\u54ea\u4e00\u90e8\u5206\u4ee5\u53ca\u5404\u7ef4\u5ea6\u7684\u4fe1\u606f\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7BERT\u5728GLUE\u4efb\u52a1\u4e0a\u7684\u5fae\u8c03\uff0c\u5168\u9762\u8bc4\u4f30\u4e86token\u5411\u91cf\u3001\u5c42\u548c\u7ef4\u5ea6\u7684\u6709\u6548\u6027\u4e0e\u5197\u4f59\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u6700\u7ec8\u5c42\u4e2d\u9664CLS\u5411\u91cf\u5916\u7684\u8f93\u51fa\u5305\u542b\u7b49\u4ef7\u4fe1\u606f\uff0c\u5927\u591a\u6570\u4efb\u52a1\u4ec5\u97002-3\u4e2a\u7ef4\u5ea6\uff0c\u9ad8\u5c42\u95f4\u5dee\u5f02\u5c0f\u4f46\u4f4e\u5c42\u8d21\u732e\u9012\u51cf\u3002\u6b64\u5916\uff0c\u9690\u85cf\u5c42\u5728\u5fae\u8c03\u4e2d\u53ef\u80fd\u663e\u8457\u53d8\u5316\uff0cBERT\u5b58\u5728\u5197\u4f59\u6027\uff0c\u80fd\u540c\u65f6\u5904\u7406\u591a\u4efb\u52a1\uff0c\u4f46\u7ef4\u5ea6\u53ef\u80fd\u8fc7\u591a\u3002", "conclusion": "BERT\u5728\u5fae\u8c03\u4e2d\u5177\u6709\u663e\u8457\u5197\u4f59\u6027\uff0c\u9690\u85cf\u5c42\u53d8\u5316\u5927\uff0c\u9ad8\u5c42\u95f4\u5dee\u5f02\u5c0f\uff0c\u4e14\u7ef4\u5ea6\u53ef\u80fd\u8fc7\u591a\uff0c\u9002\u5408\u591a\u4efb\u52a1\u5904\u7406\u3002"}}
{"id": "2504.04597", "pdf": "https://arxiv.org/pdf/2504.04597", "abs": "https://arxiv.org/abs/2504.04597", "authors": ["Haebeom Jung", "Namtae Kim", "Jungwoo Kim", "Jaesik Park"], "title": "Targetless LiDAR-Camera Calibration with Anchored 3D Gaussians", "categories": ["cs.CV"], "comment": "Project page: https://zang09.github.io/tlc-calib-site", "summary": "We present a targetless LiDAR-camera calibration method that jointly\noptimizes sensor poses and scene geometry from arbitrary scenes, without\nrelying on traditional calibration targets such as checkerboards or spherical\nreflectors. Our approach leverages a 3D Gaussian-based scene representation. We\nfirst freeze reliable LiDAR points as anchors, then jointly optimize the poses\nand auxiliary Gaussian parameters in a fully differentiable manner using a\nphotometric loss. This joint optimization significantly reduces sensor\nmisalignment, resulting in higher rendering quality and consistently improved\nPSNR compared to the carefully calibrated poses provided in popular datasets.\nWe validate our method through extensive experiments on two real-world\nautonomous driving datasets, KITTI-360 and Waymo, each featuring distinct\nsensor configurations. Additionally, we demonstrate the robustness of our\napproach using a custom LiDAR-camera setup, confirming strong performance\nacross diverse hardware configurations.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u6807\u5b9a\u76ee\u6807\u7684LiDAR-\u76f8\u673a\u6807\u5b9a\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4f20\u611f\u5668\u4f4d\u59ff\u548c\u573a\u666f\u51e0\u4f55\u3002", "motivation": "\u4f20\u7edf\u6807\u5b9a\u65b9\u6cd5\u4f9d\u8d56\u6807\u5b9a\u76ee\u6807\uff08\u5982\u68cb\u76d8\u683c\u6216\u7403\u5f62\u53cd\u5c04\u5668\uff09\uff0c\u9650\u5236\u4e86\u5e94\u7528\u573a\u666f\u7684\u7075\u6d3b\u6027\u3002", "method": "\u5229\u75283D\u9ad8\u65af\u573a\u666f\u8868\u793a\uff0c\u51bb\u7ed3\u53ef\u9760\u7684LiDAR\u70b9\u4f5c\u4e3a\u951a\u70b9\uff0c\u901a\u8fc7\u5149\u5ea6\u635f\u5931\u8054\u5408\u4f18\u5316\u4f4d\u59ff\u548c\u9ad8\u65af\u53c2\u6570\u3002", "result": "\u663e\u8457\u51cf\u5c11\u4e86\u4f20\u611f\u5668\u9519\u4f4d\uff0c\u63d0\u9ad8\u4e86\u6e32\u67d3\u8d28\u91cf\u548cPSNR\uff0c\u4f18\u4e8e\u6570\u636e\u96c6\u4e2d\u63d0\u4f9b\u7684\u6807\u5b9a\u4f4d\u59ff\u3002", "conclusion": "\u65b9\u6cd5\u5728KITTI-360\u548cWaymo\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u4e14\u5728\u4e0d\u540c\u786c\u4ef6\u914d\u7f6e\u4e0b\u8868\u73b0\u9c81\u68d2\u3002"}}
{"id": "2504.04976", "pdf": "https://arxiv.org/pdf/2504.04976", "abs": "https://arxiv.org/abs/2504.04976", "authors": ["Carlos Pel\u00e1ez-Gonz\u00e1lez", "Andr\u00e9s Herrera-Poyatos", "Cristina Zuheros", "David Herrera-Poyatos", "Virilo Tejedor", "Francisco Herrera"], "title": "A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language Models", "categories": ["cs.CL", "I.2.7"], "comment": "21 pages, 5 figures", "summary": "The study of large language models (LLMs) is a key area in open-world machine\nlearning. Although LLMs demonstrate remarkable natural language processing\ncapabilities, they also face several challenges, including consistency issues,\nhallucinations, and jailbreak vulnerabilities. Jailbreaking refers to the\ncrafting of prompts that bypass alignment safeguards, leading to unsafe outputs\nthat compromise the integrity of LLMs. This work specifically focuses on the\nchallenge of jailbreak vulnerabilities and introduces a novel taxonomy of\njailbreak attacks grounded in the training domains of LLMs. It characterizes\nalignment failures through generalization, objectives, and robustness gaps. Our\nprimary contribution is a perspective on jailbreak, framed through the\ndifferent linguistic domains that emerge during LLM training and alignment.\nThis viewpoint highlights the limitations of existing approaches and enables us\nto classify jailbreak attacks on the basis of the underlying model deficiencies\nthey exploit. Unlike conventional classifications that categorize attacks based\non prompt construction methods (e.g., prompt templating), our approach provides\na deeper understanding of LLM behavior. We introduce a taxonomy with four\ncategories -- mismatched generalization, competing objectives, adversarial\nrobustness, and mixed attacks -- offering insights into the fundamental nature\nof jailbreak vulnerabilities. Finally, we present key lessons derived from this\ntaxonomic study.", "AI": {"task": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8d8a\u72f1\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8bad\u7ec3\u9886\u57df\u7684\u65b0\u578b\u5206\u7c7b\u6cd5\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u9762\u4e34\u4e00\u81f4\u6027\u3001\u5e7b\u89c9\u548c\u8d8a\u72f1\u6f0f\u6d1e\u7b49\u6311\u6218\uff0c\u5c24\u5176\u662f\u8d8a\u72f1\u6f0f\u6d1e\u4f1a\u7ed5\u8fc7\u5bf9\u9f50\u4fdd\u62a4\u673a\u5236\uff0c\u5bfc\u81f4\u4e0d\u5b89\u5168\u8f93\u51fa\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLLMs\u8bad\u7ec3\u9886\u57df\u7684\u8d8a\u72f1\u653b\u51fb\u5206\u7c7b\u6cd5\uff0c\u4ece\u6cdb\u5316\u3001\u76ee\u6807\u548c\u9c81\u68d2\u6027\u89d2\u5ea6\u5206\u6790\u5bf9\u9f50\u5931\u8d25\u3002", "result": "\u63d0\u51fa\u56db\u79cd\u8d8a\u72f1\u653b\u51fb\u7c7b\u522b\uff1a\u4e0d\u5339\u914d\u6cdb\u5316\u3001\u7ade\u4e89\u76ee\u6807\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u6df7\u5408\u653b\u51fb\uff0c\u63ed\u793a\u4e86\u6f0f\u6d1e\u7684\u672c\u8d28\u3002", "conclusion": "\u901a\u8fc7\u5206\u7c7b\u6cd5\u6df1\u5165\u7406\u89e3LLM\u884c\u4e3a\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5173\u952e\u542f\u793a\u3002"}}
{"id": "2504.04631", "pdf": "https://arxiv.org/pdf/2504.04631", "abs": "https://arxiv.org/abs/2504.04631", "authors": ["Lei Wan", "Jianxin Zhao", "Andreas Wiedholz", "Manuel Bied", "Mateus Martinez de Lucena", "Abhishek Dinkar Jagtap", "Andreas Festag", "Ant\u00f4nio Augusto Fr\u00f6hlich", "Hannan Ejaz Keen", "Alexey Vinel"], "title": "Systematic Literature Review on Vehicular Collaborative Perception -- A Computer Vision Perspective", "categories": ["cs.CV"], "comment": "39 pages, 25 figures", "summary": "The effectiveness of autonomous vehicles relies on reliable perception\ncapabilities. Despite significant advancements in artificial intelligence and\nsensor fusion technologies, current single-vehicle perception systems continue\nto encounter limitations, notably visual occlusions and limited long-range\ndetection capabilities. Collaborative Perception (CP), enabled by\nVehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communication, has\nemerged as a promising solution to mitigate these issues and enhance the\nreliability of autonomous systems. Beyond advancements in communication, the\ncomputer vision community is increasingly focusing on improving vehicular\nperception through collaborative approaches. However, a systematic literature\nreview that thoroughly examines existing work and reduces subjective bias is\nstill lacking. Such a systematic approach helps identify research gaps,\nrecognize common trends across studies, and inform future research directions.\nIn response, this study follows the PRISMA 2020 guidelines and includes 106\npeer-reviewed articles. These publications are analyzed based on modalities,\ncollaboration schemes, and key perception tasks. Through a comparative\nanalysis, this review illustrates how different methods address practical\nissues such as pose errors, temporal latency, communication constraints, domain\nshifts, heterogeneity, and adversarial attacks. Furthermore, it critically\nexamines evaluation methodologies, highlighting a misalignment between current\nmetrics and CP's fundamental objectives. By delving into all relevant topics\nin-depth, this review offers valuable insights into challenges, opportunities,\nand risks, serving as a reference for advancing research in vehicular\ncollaborative perception.", "AI": {"task": "\u7cfb\u7edf\u7efc\u8ff0\u534f\u4f5c\u611f\u77e5\uff08CP\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u5355\u8f66\u611f\u77e5\u7cfb\u7edf\u5b58\u5728\u89c6\u89c9\u906e\u6321\u548c\u8fdc\u8ddd\u79bb\u68c0\u6d4b\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u534f\u4f5c\u611f\u77e5\uff08CP\uff09\u901a\u8fc7V2V\u548cV2I\u901a\u4fe1\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u4ee5\u51cf\u5c11\u4e3b\u89c2\u504f\u5dee\u3002", "method": "\u9075\u5faaPRISMA 2020\u6307\u5357\uff0c\u5206\u6790106\u7bc7\u540c\u884c\u8bc4\u5ba1\u6587\u7ae0\uff0c\u57fa\u4e8e\u6a21\u6001\u3001\u534f\u4f5c\u65b9\u6848\u548c\u5173\u952e\u611f\u77e5\u4efb\u52a1\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u5bf9\u59ff\u6001\u8bef\u5dee\u3001\u65f6\u95f4\u5ef6\u8fdf\u3001\u901a\u4fe1\u9650\u5236\u7b49\u95ee\u9898\u7684\u5904\u7406\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u4e0eCP\u76ee\u6807\u7684\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u7efc\u8ff0\u6df1\u5165\u63a2\u8ba8\u4e86\u6311\u6218\u3001\u673a\u9047\u548c\u98ce\u9669\uff0c\u4e3a\u672a\u6765\u534f\u4f5c\u611f\u77e5\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2504.04994", "pdf": "https://arxiv.org/pdf/2504.04994", "abs": "https://arxiv.org/abs/2504.04994", "authors": ["Ling Hu", "Yuemei Xu", "Xiaoyang Gu", "Letao Han"], "title": "Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the impressive performance of large language models (LLMs), they can\npresent unintended biases and harmful behaviors driven by encoded values,\nemphasizing the urgent need to understand the value mechanisms behind them.\nHowever, current research primarily evaluates these values through external\nresponses with a focus on AI safety, lacking interpretability and failing to\nassess social values in real-world contexts. In this paper, we propose a novel\nframework called ValueExploration, which aims to explore the behavior-driven\nmechanisms of National Social Values within LLMs at the neuron level. As a case\nstudy, we focus on Chinese Social Values and first construct C-voice, a\nlarge-scale bilingual benchmark for identifying and evaluating Chinese Social\nValues in LLMs. By leveraging C-voice, we then identify and locate the neurons\nresponsible for encoding these values according to activation difference.\nFinally, by deactivating these neurons, we analyze shifts in model behavior,\nuncovering the internal mechanism by which values influence LLM\ndecision-making. Extensive experiments on four representative LLMs validate the\nefficacy of our framework. The benchmark and code will be available.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aValueExploration\u7684\u65b0\u6846\u67b6\uff0c\u65e8\u5728\u4ece\u795e\u7ecf\u5143\u5c42\u9762\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u56fd\u5bb6\u793e\u4f1a\u4ef7\u503c\u89c2\u7684\u884c\u4e3a\u9a71\u52a8\u673a\u5236\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u53ef\u80fd\u5b58\u5728\u7531\u7f16\u7801\u4ef7\u503c\u89c2\u9a71\u52a8\u7684\u504f\u89c1\u548c\u6709\u5bb3\u884c\u4e3a\uff0c\u4e9f\u9700\u7406\u89e3\u5176\u80cc\u540e\u7684\u4ef7\u503c\u89c2\u673a\u5236\u3002\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u901a\u8fc7\u5916\u90e8\u54cd\u5e94\u8bc4\u4f30\u8fd9\u4e9b\u4ef7\u503c\u89c2\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e14\u672a\u80fd\u8bc4\u4f30\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u793e\u4f1a\u4ef7\u503c\u89c2\u3002", "method": "\u63d0\u51faValueExploration\u6846\u67b6\uff0c\u6784\u5efaC-voice\u53cc\u8bed\u57fa\u51c6\uff0c\u901a\u8fc7\u6fc0\u6d3b\u5dee\u5f02\u8bc6\u522b\u548c\u5b9a\u4f4d\u7f16\u7801\u4ef7\u503c\u89c2\u7684\u795e\u7ecf\u5143\uff0c\u5e76\u901a\u8fc7\u505c\u7528\u8fd9\u4e9b\u795e\u7ecf\u5143\u5206\u6790\u6a21\u578b\u884c\u4e3a\u53d8\u5316\u3002", "result": "\u5728\u56db\u4e2a\u4ee3\u8868\u6027LLMs\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63ed\u793a\u4e86\u4ef7\u503c\u89c2\u5982\u4f55\u5f71\u54cdLLM\u51b3\u7b56\u7684\u5185\u90e8\u673a\u5236\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u7528\u7684\u57fa\u51c6\u548c\u4ee3\u7801\u3002"}}
{"id": "2504.04633", "pdf": "https://arxiv.org/pdf/2504.04633", "abs": "https://arxiv.org/abs/2504.04633", "authors": ["Yanshu Li", "Hongyang He", "Yi Cao", "Qisen Cheng", "Xiang Fu", "Ruixiang Tang"], "title": "M2IV: Towards Efficient and Fine-grained Multimodal In-Context Learning in Large Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "Preprint, 28 pages, 10 figures, 15 tables", "summary": "Multimodal in-context learning (ICL) is a vital capability for Large\nVision-Language Models (LVLMs), allowing task adaptation via contextual prompts\nwithout parameter retraining. However, its application is hindered by the\ntoken-intensive nature of inputs and the high complexity of cross-modal\nfew-shot learning, which limits the expressive power of representation methods.\nTo tackle these challenges, we propose \\textbf{M2IV}, a method that substitutes\nexplicit demonstrations with learnable \\textbf{I}n-context \\textbf{V}ectors\ndirectly integrated into LVLMs. By exploiting the complementary strengths of\nmulti-head attention (\\textbf{M}HA) and multi-layer perceptrons (\\textbf{M}LP),\nM2IV achieves robust cross-modal fidelity and fine-grained semantic\ndistillation through training. This significantly enhances performance across\ndiverse LVLMs and tasks and scales efficiently to many-shot scenarios,\nbypassing the context window limitations. We also introduce \\textbf{VLibrary},\na repository for storing and retrieving M2IV, enabling flexible LVLM steering\nfor tasks like cross-modal alignment, customized generation and safety\nimprovement. Experiments across seven benchmarks and three LVLMs show that M2IV\nsurpasses Vanilla ICL and prior representation engineering approaches, with an\naverage accuracy gain of \\textbf{3.74\\%} over ICL with the same shot count,\nalongside substantial efficiency advantages.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aM2IV\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u4e0a\u4e0b\u6587\u5411\u91cf\u66ff\u4ee3\u663e\u5f0f\u6f14\u793a\uff0c\u4ee5\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5728LVLMs\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u8f93\u5165\u7684\u9ad8\u4ee4\u724c\u5bc6\u96c6\u6027\u548c\u8de8\u6a21\u6001\u5c11\u6837\u672c\u5b66\u4e60\u7684\u9ad8\u590d\u6742\u6027\u7684\u9650\u5236\uff0c\u5f71\u54cd\u4e86\u8868\u793a\u65b9\u6cd5\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "M2IV\u65b9\u6cd5\u7ed3\u5408\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u548c\u591a\u5c42\u611f\u77e5\u5668\uff08MLP\uff09\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u8bad\u7ec3\u5b9e\u73b0\u8de8\u6a21\u6001\u4fdd\u771f\u5ea6\u548c\u7ec6\u7c92\u5ea6\u8bed\u4e49\u84b8\u998f\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e09\u79cdLVLMs\u4e0a\uff0cM2IV\u5e73\u5747\u51c6\u786e\u7387\u6bd4\u4f20\u7edfICL\u9ad8\u51fa3.74%\uff0c\u5e76\u5177\u6709\u663e\u8457\u7684\u6548\u7387\u4f18\u52bf\u3002", "conclusion": "M2IV\u901a\u8fc7\u53ef\u5b66\u4e60\u4e0a\u4e0b\u6587\u5411\u91cf\u548cVLibrary\u7684\u5f15\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86LVLMs\u7684\u6027\u80fd\u548c\u7075\u6d3b\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002"}}
{"id": "2504.05008", "pdf": "https://arxiv.org/pdf/2504.05008", "abs": "https://arxiv.org/abs/2504.05008", "authors": ["Anastasiia Ivanova", "Natalia Fedorova", "Sergey Tilga", "Ekaterina Artemova"], "title": "Surveying Professional Writers on AI: Limitations, Expectations, and Fears", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base.", "AI": {"task": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e13\u4e1a\u5199\u4f5c\u4e2d\u7684\u5e94\u7528\u53ca\u5176\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u591a\u8bed\u8a00\u652f\u6301\u3001\u4f26\u7406\u95ee\u9898\u4ee5\u53ca\u5bf9\u4f5c\u8005\u58f0\u97f3\u548c\u521b\u9020\u529b\u7684\u957f\u671f\u5f71\u54cd\u7b49\u65b9\u9762\u7684\u672a\u5145\u5206\u7814\u7a76\u9886\u57df\u3002", "method": "\u901a\u8fc7\u95ee\u5377\u8c03\u67e5\uff08N=301\uff09\u548c\u4e92\u52a8\u8c03\u67e5\uff08N=36\uff09\u6536\u96c6\u4e13\u4e1a\u5199\u4f5c\u8005\u7684\u6570\u636e\uff0c\u5206\u6790LLMs\u572825\u79cd\u4ee5\u4e0a\u8bed\u8a00\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\u3001\u4f26\u7406\u95ee\u9898\u53ca\u7528\u6237\u671f\u671b\u3002", "result": "\u8c03\u67e5\u63ed\u793a\u4e86LLMs\u5bf9\u975e\u82f1\u8bed\u7528\u6237\u7684\u91cd\u8981\u6027\u3001\u9519\u8bef\u4fe1\u606f\u7a0b\u5ea6\u3001\u9886\u57df\u548c\u98ce\u683c\u9002\u5e94\u6027\u3001\u4ee5\u53ca\u53ef\u7528\u6027\u548c\u5173\u952e\u529f\u80fd\u7b49\u65b9\u9762\u7684\u89c1\u89e3\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u53ef\u4e3aLLMs\u7684\u8fdb\u4e00\u6b65\u5f00\u53d1\u63d0\u4f9b\u6307\u5bfc\uff0c\u4f7f\u5199\u4f5c\u8005\u548c\u66f4\u5e7f\u6cdb\u7684\u7528\u6237\u7fa4\u4f53\u53d7\u76ca\u3002"}}
{"id": "2504.04653", "pdf": "https://arxiv.org/pdf/2504.04653", "abs": "https://arxiv.org/abs/2504.04653", "authors": ["Yimu Wang", "Mozhgan Nasr Azadani", "Sean Sedwards", "Krzysztof Czarnecki"], "title": "LEO-MINI: An Efficient Multimodal Large Language Model using Conditional Token Reduction and Mixture of Multi-Modal Experts", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Redundancy of visual tokens in multi-modal large language models (MLLMs)\nsignificantly reduces their computational efficiency. Recent approaches, such\nas resamplers and summarizers, have sought to reduce the number of visual\ntokens, but at the cost of visual reasoning ability. To address this, we\npropose LEO-MINI, a novel MLLM that significantly reduces the number of visual\ntokens and simultaneously boosts visual reasoning capabilities. For efficiency,\nLEO-MINI incorporates CoTR, a novel token reduction module to consolidate a\nlarge number of visual tokens into a smaller set of tokens, using the\nsimilarity between visual tokens, text tokens, and a compact learnable query.\nFor effectiveness, to scale up the model's ability with minimal computational\noverhead, LEO-MINI employs MMoE, a novel mixture of multi-modal experts module.\nMMOE employs a set of LoRA experts with a novel router to switch between them\nbased on the input text and visual tokens instead of only using the input\nhidden state. MMoE also includes a general LoRA expert that is always activated\nto learn general knowledge for LLM reasoning. For extracting richer visual\nfeatures, MMOE employs a set of vision experts trained on diverse\ndomain-specific data. To demonstrate LEO-MINI's improved efficiency and\nperformance, we evaluate it against existing efficient MLLMs on various\nbenchmark vision-language tasks.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aLEO-MINI\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u51cf\u5c11\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\u5e76\u63d0\u5347\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u51cf\u5c11\u89c6\u89c9\u6807\u8bb0\u6570\u91cf\u65f6\u4f1a\u727a\u7272\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u9ad8\u6548\u51cf\u5c11\u6807\u8bb0\u6570\u91cf\u53c8\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "LEO-MINI\u7ed3\u5408\u4e86CoTR\uff08\u4e00\u79cd\u65b0\u9896\u7684\u6807\u8bb0\u51cf\u5c11\u6a21\u5757\uff09\u548cMMoE\uff08\u4e00\u79cd\u591a\u6a21\u6001\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff09\uff0c\u901a\u8fc7\u89c6\u89c9\u6807\u8bb0\u76f8\u4f3c\u6027\u548c\u52a8\u6001\u8def\u7531\u673a\u5236\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "result": "LEO-MINI\u5728\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "conclusion": "LEO-MINI\u901a\u8fc7\u521b\u65b0\u7684\u6807\u8bb0\u51cf\u5c11\u548c\u4e13\u5bb6\u6df7\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2504.05020", "pdf": "https://arxiv.org/pdf/2504.05020", "abs": "https://arxiv.org/abs/2504.05020", "authors": ["Charco Hui", "Yalu Wen"], "title": "Batch Aggregation: An Approach to Enhance Text Classification with Correlated Augmented Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language processing models often face challenges due to limited\nlabeled data, especially in domain specific areas, e.g., clinical trials. To\novercome this, text augmentation techniques are commonly used to increases\nsample size by transforming the original input data into artificial ones with\nthe label preserved. However, traditional text classification methods ignores\nthe relationship between augmented texts and treats them as independent samples\nwhich may introduce classification error. Therefore, we propose a novel\napproach called 'Batch Aggregation' (BAGG) which explicitly models the\ndependence of text inputs generated through augmentation by incorporating an\nadditional layer that aggregates results from correlated texts. Through\nstudying multiple benchmark data sets across different domains, we found that\nBAGG can improve classification accuracy. We also found that the increase of\nperformance with BAGG is more obvious in domain specific data sets, with\naccuracy improvements of up to 10-29%. Through the analysis of benchmark data,\nthe proposed method addresses limitations of traditional techniques and\nimproves robustness in text classification tasks. Our result demonstrates that\nBAGG offers more robust results and outperforms traditional approaches when\ntraining data is limited.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a'Batch Aggregation'\uff08BAGG\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u6587\u672c\u589e\u5f3a\u6280\u672f\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u5ffd\u7565\u4e86\u589e\u5f3a\u6587\u672c\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5c06\u5176\u89c6\u4e3a\u72ec\u7acb\u6837\u672c\uff0c\u53ef\u80fd\u5bfc\u81f4\u5206\u7c7b\u9519\u8bef\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u989d\u5916\u7684\u5c42\u6765\u805a\u5408\u76f8\u5173\u589e\u5f3a\u6587\u672c\u7684\u7ed3\u679c\uff0c\u663e\u5f0f\u5efa\u6a21\u589e\u5f3a\u6587\u672c\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cBAGG\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u663e\u8457\uff0c\u51c6\u786e\u6027\u63d0\u5347\u8fbe10-29%\u3002", "conclusion": "BAGG\u65b9\u6cd5\u514b\u670d\u4e86\u4f20\u7edf\u6280\u672f\u7684\u5c40\u9650\u6027\uff0c\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u7ed3\u679c\u3002"}}
{"id": "2504.04658", "pdf": "https://arxiv.org/pdf/2504.04658", "abs": "https://arxiv.org/abs/2504.04658", "authors": ["Haisheng Fu", "Jie Liang", "Feng Liang", "Zhenman Fang", "Guohe Zhang", "Jingning Han"], "title": "3DM-WeConvene: Learned Image Compression with 3D Multi-Level Wavelet-Domain Convolution and Entropy Model", "categories": ["cs.CV", "stat.AP"], "comment": "13 pages", "summary": "Learned image compression (LIC) has recently made significant progress,\nsurpassing traditional methods. However, most LIC approaches operate mainly in\nthe spatial domain and lack mechanisms for reducing frequency-domain\ncorrelations. To address this, we propose a novel framework that integrates\nlow-complexity 3D multi-level Discrete Wavelet Transform (DWT) into\nconvolutional layers and entropy coding, reducing both spatial and channel\ncorrelations to improve frequency selectivity and rate-distortion (R-D)\nperformance.\n  Our proposed 3D multi-level wavelet-domain convolution (3DM-WeConv) layer\nfirst applies 3D multi-level DWT (e.g., 5/3 and 9/7 wavelets from JPEG 2000) to\ntransform data into the wavelet domain. Then, different-sized convolutions are\napplied to different frequency subbands, followed by inverse 3D DWT to restore\nthe spatial domain. The 3DM-WeConv layer can be flexibly used within existing\nCNN-based LIC models.\n  We also introduce a 3D wavelet-domain channel-wise autoregressive entropy\nmodel (3DWeChARM), which performs slice-based entropy coding in the 3D DWT\ndomain. Low-frequency (LF) slices are encoded first to provide priors for\nhigh-frequency (HF) slices.\n  A two-step training strategy is adopted: first balancing LF and HF rates,\nthen fine-tuning with separate weights.\n  Extensive experiments demonstrate that our framework consistently outperforms\nstate-of-the-art CNN-based LIC methods in R-D performance and computational\ncomplexity, with larger gains for high-resolution images. On the Kodak, Tecnick\n100, and CLIC test sets, our method achieves BD-Rate reductions of -12.24%,\n-15.51%, and -12.97%, respectively, compared to H.266/VVC.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u96c6\u62103D\u591a\u7ea7\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\uff08DWT\uff09\u7684\u6846\u67b6\uff0c\u4ee5\u6539\u8fdb\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u56fe\u50cf\u538b\u7f29\uff08LIC\uff09\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570LIC\u65b9\u6cd5\u4e3b\u8981\u5728\u7a7a\u95f4\u57df\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u51cf\u5c11\u9891\u57df\u76f8\u5173\u6027\u7684\u673a\u5236\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51fa3D\u591a\u7ea7\u5c0f\u6ce2\u57df\u5377\u79ef\u5c42\uff083DM-WeConv\uff09\u548c3D\u5c0f\u6ce2\u57df\u901a\u9053\u81ea\u56de\u5f52\u71b5\u6a21\u578b\uff083DWeChARM\uff09\uff0c\u7ed3\u5408\u4e24\u6b65\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728Kodak\u3001Tecnick 100\u548cCLIC\u6d4b\u8bd5\u96c6\u4e0a\uff0cBD-Rate\u5206\u522b\u964d\u4f4e-12.24%\u3001-15.51%\u548c-12.97%\uff0c\u4f18\u4e8eH.266/VVC\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86LIC\u7684\u7387\u5931\u771f\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3002"}}
{"id": "2504.05050", "pdf": "https://arxiv.org/pdf/2504.05050", "abs": "https://arxiv.org/abs/2504.05050", "authors": ["Jiawei Lian", "Jianhong Pan", "Lefan Wang", "Yi Wang", "Shaohui Mei", "Lap-Pui Chau"], "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities.", "AI": {"task": "\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9884\u8bad\u7ec3\u548c\u6307\u4ee4\u8c03\u4f18\u540e\u4ecd\u5b58\u5728\u7684\u4f26\u7406\u6f0f\u6d1e\u53ca\u5176\u5bf9\u6297\u6027\u8bf1\u5bfc\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1\u901a\u8fc7\u6307\u4ee4\u8c03\u4f18\u548c\u504f\u597d\u5b66\u4e60\u4f7fLLMs\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\uff0c\u4f46\u5176\u9884\u8bad\u7ec3\u9636\u6bb5\u5d4c\u5165\u7684\u6709\u5bb3\u77e5\u8bc6\u4ecd\u4ee5\u201c\u6697\u6a21\u5f0f\u201d\u5f62\u5f0f\u5b58\u5728\uff0c\u5bfc\u81f4\u5bf9\u9f50\u63aa\u65bd\u5931\u6548\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u5f53\u524d\u5bf9\u9f50\u65b9\u6cd5\u4ec5\u4ea7\u751f\u5c40\u90e8\u7684\u201c\u5b89\u5168\u533a\u57df\u201d\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u4e00\u81f4\u6027\u8bf1\u5bfc\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u9a8c\u8bc1\u6a21\u578b\u7684\u8106\u5f31\u6027\u3002", "result": "\u572823\u4e2a\u5148\u8fdb\u5bf9\u9f50LLMs\u4e2d\uff0c19\u4e2a\u6a21\u578b\u7684\u653b\u51fb\u6210\u529f\u7387\u8fbe\u5230100%\uff0c\u63ed\u793a\u4e86\u5176\u666e\u904d\u5b58\u5728\u7684\u6f0f\u6d1e\u3002", "conclusion": "\u5f53\u524d\u7684\u5bf9\u9f50\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\u9884\u8bad\u7ec3\u77e5\u8bc6\u4e2d\u7684\u6709\u5bb3\u5185\u5bb9\uff0c\u9700\u8981\u66f4\u5168\u5c40\u5316\u7684\u5b89\u5168\u7b56\u7565\u3002"}}
{"id": "2504.04676", "pdf": "https://arxiv.org/pdf/2504.04676", "abs": "https://arxiv.org/abs/2504.04676", "authors": ["Bo Li", "Jing Yun"], "title": "Dual Consistent Constraint via Disentangled Consistency and Complementarity for Multi-view Clustering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-view clustering can explore common semantics from multiple views and\nhas received increasing attention in recent years. However, current methods\nfocus on learning consistency in representation, neglecting the contribution of\neach view's complementarity aspect in representation learning. This limit poses\na significant challenge in multi-view representation learning. This paper\nproposes a novel multi-view clustering framework that introduces a disentangled\nvariational autoencoder that separates multi-view into shared and private\ninformation, i.e., consistency and complementarity information. We first learn\ninformative and consistent representations by maximizing mutual information\nacross different views through contrastive learning. This process will ignore\ncomplementary information. Then, we employ consistency inference constraints to\nexplicitly utilize complementary information when attempting to seek the\nconsistency of shared information across all views. Specifically, we perform a\nwithin-reconstruction using the private and shared information of each view and\na cross-reconstruction using the shared information of all views. The dual\nconsistency constraints are not only effective in improving the representation\nquality of data but also easy to extend to other scenarios, especially in\ncomplex multi-view scenes. This could be the first attempt to employ dual\nconsistent constraint in a unified MVC theoretical framework. During the\ntraining procedure, the consistency and complementarity features are jointly\noptimized. Extensive experiments show that our method outperforms baseline\nmethods.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u591a\u89c6\u56fe\u805a\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5206\u79bb\u5171\u4eab\u548c\u79c1\u6709\u4fe1\u606f\uff0c\u4ee5\u5229\u7528\u4e00\u81f4\u6027\u548c\u4e92\u8865\u6027\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u4e00\u81f4\u6027\uff0c\u5ffd\u89c6\u4e86\u591a\u89c6\u56fe\u4e2d\u4e92\u8865\u6027\u4fe1\u606f\u7684\u8d21\u732e\uff0c\u9650\u5236\u4e86\u591a\u89c6\u56fe\u8868\u793a\u5b66\u4e60\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528\u89e3\u8026\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6700\u5927\u5316\u4e0d\u540c\u89c6\u56fe\u95f4\u7684\u4e92\u4fe1\u606f\uff0c\u5e76\u5229\u7528\u4e00\u81f4\u6027\u63a8\u7406\u7ea6\u675f\u663e\u5f0f\u5229\u7528\u4e92\u8865\u6027\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u6709\u6548\u63d0\u5347\u6570\u636e\u8868\u793a\u8d28\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5728\u7edf\u4e00\u7684\u591a\u89c6\u56fe\u805a\u7c7b\u7406\u8bba\u4e2d\u5f15\u5165\u53cc\u91cd\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u4e3a\u590d\u6742\u591a\u89c6\u56fe\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.05058", "pdf": "https://arxiv.org/pdf/2504.05058", "abs": "https://arxiv.org/abs/2504.05058", "authors": ["Aravind Krishnan", "Siva Reddy", "Marius Mosbach"], "title": "Not All Data Are Unlearned Equally", "categories": ["cs.CL"], "comment": null, "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.", "AI": {"task": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u77e5\u8bc6\u9057\u5fd8\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u77e5\u8bc6\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u9891\u7387\u5bf9\u9057\u5fd8\u6210\u529f\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u6240\u6709\u9700\u8981\u9057\u5fd8\u7684\u6570\u636e\u70b9\u4e00\u89c6\u540c\u4ec1\uff0c\u4f46\u5b9e\u9645\u4e2d\u4e0d\u540c\u77e5\u8bc6\u7684\u9057\u5fd8\u96be\u5ea6\u53ef\u80fd\u4e0d\u540c\uff0c\u5c24\u5176\u662f\u9690\u79c1\u76f8\u5173\u6570\u636e\u3002", "method": "\u5206\u6790\u77e5\u8bc6\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u9891\u7387\u5bf9\u9057\u5fd8\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u6bd4\u8f83\u57fa\u4e8e\u6982\u7387\u548c\u751f\u6210\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u9ad8\u9891\u77e5\u8bc6\u66f4\u96be\u9057\u5fd8\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u589e\u5927\u4f1a\u52a0\u5267\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "\u9700\u8981\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u5e76\u5f00\u53d1\u8003\u8651\u8bad\u7ec3\u6570\u636e\u7684\u65b0\u578b\u9057\u5fd8\u65b9\u6cd5\u3002"}}
{"id": "2504.04679", "pdf": "https://arxiv.org/pdf/2504.04679", "abs": "https://arxiv.org/abs/2504.04679", "authors": ["Wanzhou Liu", "Zhexiao Xiong", "Xinyu Li", "Nathan Jacobs"], "title": "DeclutterNeRF: Generative-Free 3D Scene Recovery for Occlusion Removal", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 4th CV4Metaverse Workshop. 15 pages, 10\n  figures. Code and data at: https://github.com/wanzhouliu/declutter-nerf", "summary": "Recent novel view synthesis (NVS) techniques, including Neural Radiance\nFields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly advanced 3D scene\nreconstruction with high-quality rendering and realistic detail recovery.\nEffectively removing occlusions while preserving scene details can further\nenhance the robustness and applicability of these techniques. However, existing\napproaches for object and occlusion removal predominantly rely on generative\npriors, which, despite filling the resulting holes, introduce new artifacts and\nblurriness. Moreover, existing benchmark datasets for evaluating occlusion\nremoval methods lack realistic complexity and viewpoint variations. To address\nthese issues, we introduce DeclutterSet, a novel dataset featuring diverse\nscenes with pronounced occlusions distributed across foreground, midground, and\nbackground, exhibiting substantial relative motion across viewpoints. We\nfurther introduce DeclutterNeRF, an occlusion removal method free from\ngenerative priors. DeclutterNeRF introduces joint multi-view optimization of\nlearnable camera parameters, occlusion annealing regularization, and employs an\nexplainable stochastic structural similarity loss, ensuring high-quality,\nartifact-free reconstructions from incomplete images. Experiments demonstrate\nthat DeclutterNeRF significantly outperforms state-of-the-art methods on our\nproposed DeclutterSet, establishing a strong baseline for future research.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u751f\u6210\u5148\u9a8c\u7684\u906e\u6321\u7269\u53bb\u9664\u65b9\u6cd5DeclutterNeRF\uff0c\u5e76\u6784\u5efa\u5305\u542b\u590d\u6742\u906e\u6321\u573a\u666f\u7684\u6570\u636e\u96c6DeclutterSet\u3002", "motivation": "\u73b0\u6709\u906e\u6321\u7269\u53bb\u9664\u65b9\u6cd5\u4f9d\u8d56\u751f\u6210\u5148\u9a8c\uff0c\u5bfc\u81f4\u65b0\u4f2a\u5f71\u548c\u6a21\u7cca\uff1b\u4e14\u73b0\u6709\u8bc4\u4f30\u6570\u636e\u96c6\u7f3a\u4e4f\u771f\u5b9e\u590d\u6742\u6027\u548c\u89c6\u89d2\u53d8\u5316\u3002", "method": "DeclutterNeRF\u901a\u8fc7\u8054\u5408\u591a\u89c6\u89d2\u4f18\u5316\u53ef\u5b66\u4e60\u76f8\u673a\u53c2\u6570\u3001\u906e\u6321\u9000\u706b\u6b63\u5219\u5316\u548c\u53ef\u89e3\u91ca\u968f\u673a\u7ed3\u6784\u76f8\u4f3c\u6027\u635f\u5931\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "result": "DeclutterNeRF\u5728DeclutterSet\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "DeclutterNeRF\u548cDeclutterSet\u89e3\u51b3\u4e86\u73b0\u6709\u906e\u6321\u7269\u53bb\u9664\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u4e14\u65e0\u4f2a\u5f71\u7684\u91cd\u5efa\u7ed3\u679c\u3002"}}
{"id": "2504.05074", "pdf": "https://arxiv.org/pdf/2504.05074", "abs": "https://arxiv.org/abs/2504.05074", "authors": ["Venkat Srinivasan", "Vishaal Jatav", "Anushka Chandrababu", "Geetika Sharma"], "title": "On the Performance of an Explainable Language Model on PubMedQA", "categories": ["cs.CL"], "comment": "Working Paper", "summary": "Large language models (LLMs) have shown significant abilities in retrieving\nmedical knowledge, reasoning over it and answering medical questions comparably\nto physicians. However, these models are not interpretable, hallucinate, are\ndifficult to maintain and require enormous compute resources for training and\ninference. In this paper, we report results from Gyan, an explainable language\nmodel based on an alternative architecture, on the PubmedQA data set. The Gyan\nLLM is a compositional language model and the model is decoupled from\nknowledge. Gyan is trustable, transparent, does not hallucinate and does not\nrequire significant training or compute resources. Gyan is easily transferable\nacross domains. Gyan-4.3 achieves SOTA results on PubmedQA with 87.1% accuracy\ncompared to 82% by MedPrompt based on GPT-4 and 81.8% by Med-PaLM 2 (Google and\nDeepMind). We will be reporting results for other medical data sets - MedQA,\nMedMCQA, MMLU - Medicine in the future.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u8bed\u8a00\u6a21\u578bGyan\uff0c\u7528\u4e8e\u5728\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u5b66\u77e5\u8bc6\u68c0\u7d22\u548c\u95ee\u7b54\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u4e0d\u53ef\u89e3\u91ca\u6027\u3001\u5e7b\u89c9\u95ee\u9898\u3001\u7ef4\u62a4\u56f0\u96be\u4ee5\u53ca\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u57fa\u4e8e\u66ff\u4ee3\u67b6\u6784\u7684\u7ec4\u5408\u5f0f\u8bed\u8a00\u6a21\u578bGyan\uff0c\u5c06\u6a21\u578b\u4e0e\u77e5\u8bc6\u89e3\u8026\uff0c\u4f7f\u5176\u900f\u660e\u3001\u53ef\u4fe1\u4e14\u65e0\u9700\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "Gyan-4.3\u5728PubmedQA\u6570\u636e\u96c6\u4e0a\u8fbe\u523087.1%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8eGPT-4\u548cMed-PaLM 2\u3002", "conclusion": "Gyan\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u6613\u4e8e\u8de8\u9886\u57df\u8fc1\u79fb\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u533b\u5b66\u95ee\u7b54\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.04687", "pdf": "https://arxiv.org/pdf/2504.04687", "abs": "https://arxiv.org/abs/2504.04687", "authors": ["Yicheng Leng", "Chaowei Fang", "Junye Chen", "Yixiang Fang", "Sheng Li", "Guanbin Li"], "title": "Bridging Knowledge Gap Between Image Inpainting and Large-Area Visible Watermark Removal", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV", "I.2.10; I.4.4; I.4.5"], "comment": "To be published in AAAI 2025", "summary": "Visible watermark removal which involves watermark cleaning and background\ncontent restoration is pivotal to evaluate the resilience of watermarks.\nExisting deep neural network (DNN)-based models still struggle with large-area\nwatermarks and are overly dependent on the quality of watermark mask\nprediction. To overcome these challenges, we introduce a novel feature adapting\nframework that leverages the representation modeling capacity of a pre-trained\nimage inpainting model. Our approach bridges the knowledge gap between image\ninpainting and watermark removal by fusing information of the residual\nbackground content beneath watermarks into the inpainting backbone model. We\nestablish a dual-branch system to capture and embed features from the residual\nbackground content, which are merged into intermediate features of the\ninpainting backbone model via gated feature fusion modules. Moreover, for\nrelieving the dependence on high-quality watermark masks, we introduce a new\ntraining paradigm by utilizing coarse watermark masks to guide the inference\nprocess. This contributes to a visible image removal model which is insensitive\nto the quality of watermark mask during testing. Extensive experiments on both\na large-scale synthesized dataset and a real-world dataset demonstrate that our\napproach significantly outperforms existing state-of-the-art methods. The\nsource code is available in the supplementary materials.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u7279\u5f81\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u89c1\u6c34\u5370\u53bb\u9664\uff0c\u5305\u62ec\u6c34\u5370\u6e05\u6d01\u548c\u80cc\u666f\u5185\u5bb9\u6062\u590d\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\u5728\u5904\u7406\u5927\u9762\u79ef\u6c34\u5370\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6c34\u5370\u63a9\u7801\u9884\u6d4b\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u56fe\u50cf\u4fee\u590d\u6a21\u578b\u7684\u8868\u793a\u5efa\u6a21\u80fd\u529b\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u7cfb\u7edf\u6355\u83b7\u548c\u5d4c\u5165\u6b8b\u5dee\u80cc\u666f\u5185\u5bb9\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u95e8\u63a7\u7279\u5f81\u878d\u5408\u6a21\u5757\u5c06\u5176\u5408\u5e76\u5230\u4fee\u590d\u4e3b\u5e72\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u51cf\u5c11\u4e86\u5bf9\u9ad8\u8d28\u91cf\u6c34\u5370\u63a9\u7801\u7684\u4f9d\u8d56\uff0c\u8fd8\u63d0\u5347\u4e86\u6c34\u5370\u53bb\u9664\u7684\u6548\u679c\u3002"}}
{"id": "2504.05081", "pdf": "https://arxiv.org/pdf/2504.05081", "abs": "https://arxiv.org/abs/2504.05081", "authors": ["Tianshi Zheng", "Yixiang Chen", "Chengxi Li", "Chunyang Li", "Qing Zong", "Haochen Shi", "Baixuan Xu", "Yangqiu Song", "Ginny Y. Wong", "Simon See"], "title": "The Curse of CoT: On the Limitations of Chain-of-Thought in In-Context Learning", "categories": ["cs.CL"], "comment": "30 pages, 12 tables, 6 figures", "summary": "Chain-of-Thought (CoT) prompting has been widely recognized for its ability\nto enhance reasoning capabilities in large language models (LLMs) through the\ngeneration of explicit explanatory rationales. However, our study reveals a\nsurprising contradiction to this prevailing perspective. Through extensive\nexperiments involving 16 state-of-the-art LLMs and nine diverse pattern-based\nin-context learning (ICL) datasets, we demonstrate that CoT and its reasoning\nvariants consistently underperform direct answering across varying model scales\nand benchmark complexities. To systematically investigate this unexpected\nphenomenon, we designed extensive experiments to validate several hypothetical\nexplanations. Our analysis uncovers a fundamental explicit-implicit duality\ndriving CoT's performance in pattern-based ICL: while explicit reasoning\nfalters due to LLMs' struggles to infer underlying patterns from\ndemonstrations, implicit reasoning-disrupted by the increased contextual\ndistance of CoT rationales-often compensates, delivering correct answers\ndespite flawed rationales. This duality explains CoT's relative\nunderperformance, as noise from weak explicit inference undermines the process,\neven as implicit mechanisms partially salvage outcomes. Notably, even long-CoT\nreasoning models, which excel in abstract and symbolic reasoning, fail to fully\novercome these limitations despite higher computational costs. Our findings\nchallenge existing assumptions regarding the universal efficacy of CoT,\nyielding novel insights into its limitations and guiding future research toward\nmore nuanced and effective reasoning methodologies for LLMs.", "AI": {"task": "\u7814\u7a76Chain-of-Thought (CoT)\u63d0\u793a\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u4e2d\u7684\u8868\u73b0\u53ca\u5176\u5c40\u9650\u6027\u3002", "motivation": "\u53d1\u73b0CoT\u53ca\u5176\u63a8\u7406\u53d8\u4f53\u5728\u6a21\u5f0f\u5316\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6311\u6218\u4e86\u5176\u666e\u904d\u6709\u6548\u7684\u5047\u8bbe\u3002", "method": "\u901a\u8fc716\u79cd\u5148\u8fdbLLMs\u548c9\u79cd\u591a\u6837\u5316\u6a21\u5f0f\u5316ICL\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1CoT\u8868\u73b0\u4e0d\u4f73\u7684\u539f\u56e0\u3002", "result": "\u63ed\u793a\u4e86CoT\u5728\u6a21\u5f0f\u5316ICL\u4e2d\u7684\u663e\u9690\u53cc\u91cd\u6027\uff1a\u663e\u5f0f\u63a8\u7406\u56e0LLMs\u96be\u4ee5\u63a8\u65ad\u6a21\u5f0f\u800c\u5931\u8d25\uff0c\u9690\u5f0f\u63a8\u7406\u90e8\u5206\u5f25\u8865\u4f46\u65e0\u6cd5\u5b8c\u5168\u514b\u670d\u9650\u5236\u3002", "conclusion": "CoT\u5e76\u975e\u666e\u904d\u6709\u6548\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u8bbe\u8ba1\u66f4\u6709\u6548\u7684LLM\u63a8\u7406\u65b9\u6cd5\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2504.04701", "pdf": "https://arxiv.org/pdf/2504.04701", "abs": "https://arxiv.org/abs/2504.04701", "authors": ["Bo-Wen Yin", "Jiao-Long Cao", "Ming-Ming Cheng", "Qibin Hou"], "title": "DFormerv2: Geometry Self-Attention for RGBD Semantic Segmentation", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Recent advances in scene understanding benefit a lot from depth maps because\nof the 3D geometry information, especially in complex conditions (e.g., low\nlight and overexposed). Existing approaches encode depth maps along with RGB\nimages and perform feature fusion between them to enable more robust\npredictions. Taking into account that depth can be regarded as a geometry\nsupplement for RGB images, a straightforward question arises: Do we really need\nto explicitly encode depth information with neural networks as done for RGB\nimages? Based on this insight, in this paper, we investigate a new way to learn\nRGBD feature representations and present DFormerv2, a strong RGBD encoder that\nexplicitly uses depth maps as geometry priors rather than encoding depth\ninformation with neural networks. Our goal is to extract the geometry clues\nfrom the depth and spatial distances among all the image patch tokens, which\nwill then be used as geometry priors to allocate attention weights in\nself-attention. Extensive experiments demonstrate that DFormerv2 exhibits\nexceptional performance in various RGBD semantic segmentation benchmarks. Code\nis available at: https://github.com/VCIP-RGBD/DFormer.", "AI": {"task": "\u63a2\u7d22\u4e00\u79cd\u65b0\u7684RGBD\u7279\u5f81\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u63d0\u51faDFormerv2\uff0c\u5c06\u6df1\u5ea6\u56fe\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\u800c\u975e\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u7f16\u7801\u3002", "motivation": "\u6df1\u5ea6\u56fe\u63d0\u4f9b\u4e863D\u51e0\u4f55\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4e0eRGB\u56fe\u50cf\u4e00\u8d77\u7f16\u7801\uff0c\u662f\u5426\u5fc5\u987b\u50cfRGB\u56fe\u50cf\u90a3\u6837\u663e\u5f0f\u7f16\u7801\u6df1\u5ea6\u4fe1\u606f\uff1f", "method": "DFormerv2\u5229\u7528\u6df1\u5ea6\u56fe\u63d0\u53d6\u51e0\u4f55\u7ebf\u7d22\u548c\u7a7a\u95f4\u8ddd\u79bb\uff0c\u4f5c\u4e3a\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u51e0\u4f55\u5148\u9a8c\u5206\u914d\u6743\u91cd\u3002", "result": "DFormerv2\u5728\u591a\u4e2aRGBD\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "DFormerv2\u901a\u8fc7\u5c06\u6df1\u5ea6\u56fe\u4f5c\u4e3a\u51e0\u4f55\u5148\u9a8c\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684RGBD\u7279\u5f81\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2504.05097", "pdf": "https://arxiv.org/pdf/2504.05097", "abs": "https://arxiv.org/abs/2504.05097", "authors": ["Liu Xiao", "Li Zhiyuan", "Lin Yueyu"], "title": "State Tuning: State-based Test-Time Scaling on RWKV-7", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u8c03\u6574\uff08state tuning\uff09\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u9488\u5bf9RNN-based RWKV-7\u6a21\u578b\u3002", "motivation": "\u5229\u7528RWKV-7\u6a21\u578b\u7684\u72ec\u7279\u4f18\u52bf\uff0c\u5728\u4e0d\u6539\u53d8\u9884\u8bad\u7ec3\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u89c2\u5bdf\u8005\u6846\u67b6\u3001\u52a8\u6001\u6269\u5c55\u72b6\u6001\u5927\u5c0f\u7684\u6838\u65b9\u6cd5\u3001\u96c6\u6210\u53bb\u76f8\u5173\u53cd\u5411\u4f20\u64ad\uff08DBP\uff09\u4f18\u5316\u72b6\u6001\u77e9\u9635\u3002", "result": "\u5c0f\u6a21\u578b\u5728\u76ee\u6807\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u5927\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301RWKV-7\u67b6\u6784\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u72b6\u6001\u8c03\u6574\u662f\u4e00\u79cd\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2504.04708", "pdf": "https://arxiv.org/pdf/2504.04708", "abs": "https://arxiv.org/abs/2504.04708", "authors": ["Minchul Kim", "Dingqiang Ye", "Yiyang Su", "Feng Liu", "Xiaoming Liu"], "title": "SapiensID: Foundation for Human Recognition", "categories": ["cs.CV"], "comment": "To appear in CVPR2025", "summary": "Existing human recognition systems often rely on separate, specialized models\nfor face and body analysis, limiting their effectiveness in real-world\nscenarios where pose, visibility, and context vary widely. This paper\nintroduces SapiensID, a unified model that bridges this gap, achieving robust\nperformance across diverse settings. SapiensID introduces (i) Retina Patch\n(RP), a dynamic patch generation scheme that adapts to subject scale and\nensures consistent tokenization of regions of interest, (ii) a masked\nrecognition model (MRM) that learns from variable token length, and (iii)\nSemantic Attention Head (SAH), an module that learns pose-invariant\nrepresentations by pooling features around key body parts. To facilitate\ntraining, we introduce WebBody4M, a large-scale dataset capturing diverse poses\nand scale variations. Extensive experiments demonstrate that SapiensID achieves\nstate-of-the-art results on various body ReID benchmarks, outperforming\nspecialized models in both short-term and long-term scenarios while remaining\ncompetitive with dedicated face recognition systems. Furthermore, SapiensID\nestablishes a strong baseline for the newly introduced challenge of Cross\nPose-Scale ReID, demonstrating its ability to generalize to complex, real-world\nconditions.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u4eba\u4f53\u8bc6\u522b\u6a21\u578bSapiensID\uff0c\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u56e0\u4f9d\u8d56\u72ec\u7acb\u6a21\u578b\u800c\u6548\u679c\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u548c\u8eab\u4f53\u5206\u6790\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u72ec\u7acb\u7684\u4e13\u95e8\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5728\u59ff\u6001\u3001\u53ef\u89c1\u6027\u548c\u4e0a\u4e0b\u6587\u53d8\u5316\u5e7f\u6cdb\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "SapiensID\u5f15\u5165Retina Patch\uff08RP\uff09\u52a8\u6001\u8865\u4e01\u751f\u6210\u65b9\u6848\u3001\u63a9\u7801\u8bc6\u522b\u6a21\u578b\uff08MRM\uff09\u548c\u8bed\u4e49\u6ce8\u610f\u529b\u5934\uff08SAH\uff09\uff0c\u5e76\u5229\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6WebBody4M\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "SapiensID\u5728\u591a\u79cd\u4eba\u4f53ReID\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4f18\u4e8e\u4e13\u95e8\u6a21\u578b\uff0c\u5e76\u5728\u8de8\u59ff\u6001\u5c3a\u5ea6ReID\u65b0\u6311\u6218\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "SapiensID\u4e3a\u590d\u6742\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u7edf\u4e00\u8bc6\u522b\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.05104", "pdf": "https://arxiv.org/pdf/2504.05104", "abs": "https://arxiv.org/abs/2504.05104", "authors": ["Saeid Ario Vaghefi", "Aymane Hachcham", "Veronica Grasso", "Jiska Manicus", "Nakiete Msemo", "Chiara Colesanti Senni", "Markus Leippold"], "title": "AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments", "categories": ["cs.CL"], "comment": null, "summary": "Tracking financial investments in climate adaptation is a complex and\nexpertise-intensive task, particularly for Early Warning Systems (EWS), which\nlack standardized financial reporting across multilateral development banks\n(MDBs) and funds. To address this challenge, we introduce an LLM-based agentic\nAI system that integrates contextual retrieval, fine-tuning, and multi-step\nreasoning to extract relevant financial data, classify investments, and ensure\ncompliance with funding guidelines. Our study focuses on a real-world\napplication: tracking EWS investments in the Climate Risk and Early Warning\nSystems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple\nAI-driven classification methods, including zero-shot and few-shot learning,\nfine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and\nan agent-based retrieval-augmented generation (RAG) approach. Our results show\nthat the agent-based RAG approach significantly outperforms other methods,\nachieving 87\\% accuracy, 89\\% precision, and 83\\% recall. Additionally, we\ncontribute a benchmark dataset and expert-annotated corpus, providing a\nvaluable resource for future research in AI-driven financial tracking and\nclimate finance transparency.", "AI": {"task": "\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8eLLM\u7684AI\u7cfb\u7edf\uff0c\u7528\u4e8e\u8ffd\u8e2a\u548c\u5206\u7c7b\u6c14\u5019\u9002\u5e94\u6295\u8d44\uff0c\u7279\u522b\u662f\u5728\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\uff08EWS\uff09\u9886\u57df\u3002", "motivation": "\u7531\u4e8e\u591a\u8fb9\u5f00\u53d1\u94f6\u884c\uff08MDBs\uff09\u548c\u57fa\u91d1\u5728\u6c14\u5019\u9002\u5e94\u6295\u8d44\u4e2d\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8d22\u52a1\u62a5\u544a\uff0c\u8ffd\u8e2a\u8fd9\u4e9b\u6295\u8d44\u53d8\u5f97\u590d\u6742\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "\u7ed3\u5408\u4e0a\u4e0b\u6587\u68c0\u7d22\u3001\u5fae\u8c03\u548c\u591a\u6b65\u63a8\u7406\u7684LLM\u4ee3\u7406AI\u7cfb\u7edf\uff0c\u91c7\u7528\u96f6\u6837\u672c\u5b66\u4e60\u3001\u5c11\u6837\u672c\u5b66\u4e60\u3001\u5fae\u8c03\u53d8\u538b\u5668\u5206\u7c7b\u5668\u3001\u601d\u7ef4\u94fe\u63d0\u793a\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u3002", "result": "\u57fa\u4e8e\u4ee3\u7406\u7684RAG\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u8fbe\u523087%\u51c6\u786e\u7387\u300189%\u7cbe\u786e\u7387\u548c83%\u53ec\u56de\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684AI\u9a71\u52a8\u8d22\u52a1\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u8fd8\u8d21\u732e\u4e86\u4e00\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e13\u5bb6\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2504.04716", "pdf": "https://arxiv.org/pdf/2504.04716", "abs": "https://arxiv.org/abs/2504.04716", "authors": ["Haoren Zhao", "Tianyi Chen", "Zhen Wang"], "title": "On the Robustness of GUI Grounding Models Against Image Attacks", "categories": ["cs.CV"], "comment": null, "summary": "Graphical User Interface (GUI) grounding models are crucial for enabling\nintelligent agents to understand and interact with complex visual interfaces.\nHowever, these models face significant robustness challenges in real-world\nscenarios due to natural noise and adversarial perturbations, and their\nrobustness remains underexplored. In this study, we systematically evaluate the\nrobustness of state-of-the-art GUI grounding models, such as UGround, under\nthree conditions: natural noise, untargeted adversarial attacks, and targeted\nadversarial attacks. Our experiments, which were conducted across a wide range\nof GUI environments, including mobile, desktop, and web interfaces, have\nclearly demonstrated that GUI grounding models exhibit a high degree of\nsensitivity to adversarial perturbations and low-resolution conditions. These\nfindings provide valuable insights into the vulnerabilities of GUI grounding\nmodels and establish a strong benchmark for future research aimed at enhancing\ntheir robustness in practical applications. Our code is available at\nhttps://github.com/ZZZhr-1/Robust_GUI_Grounding.", "AI": {"task": "\u7cfb\u7edf\u5730\u8bc4\u4f30\u6700\u5148\u8fdb\u7684GUI\u63a5\u5730\u6a21\u578b\u5728\u81ea\u7136\u566a\u58f0\u3001\u65e0\u76ee\u6807\u5bf9\u6297\u653b\u51fb\u548c\u76ee\u6807\u5bf9\u6297\u653b\u51fb\u4e09\u79cd\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "GUI\u63a5\u5730\u6a21\u578b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u9762\u4e34\u81ea\u7136\u566a\u58f0\u548c\u5bf9\u6297\u6270\u52a8\u7684\u9c81\u68d2\u6027\u6311\u6218\uff0c\u5176\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u5728\u591a\u79cdGUI\u73af\u5883\uff08\u79fb\u52a8\u3001\u684c\u9762\u548c\u7f51\u9875\u754c\u9762\uff09\u4e0b\u5bf9UGround\u7b49\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGUI\u63a5\u5730\u6a21\u578b\u5bf9\u5bf9\u6297\u6270\u52a8\u548c\u4f4e\u5206\u8fa8\u7387\u6761\u4ef6\u9ad8\u5ea6\u654f\u611f\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86GUI\u63a5\u5730\u6a21\u578b\u7684\u8106\u5f31\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u63d0\u5347\u5176\u9c81\u68d2\u6027\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2504.05122", "pdf": "https://arxiv.org/pdf/2504.05122", "abs": "https://arxiv.org/abs/2504.05122", "authors": ["Xinglin Lyu", "Wei Tang", "Yuang Li", "Xiaofeng Zhao", "Ming Zhu", "Junhui Li", "Yunfei Lu", "Min Zhang", "Daimeng Wei", "Hao Yang", "Min Zhang"], "title": "DoCIA: An Online Document-Level Context Incorporation Agent for Speech Translation", "categories": ["cs.CL"], "comment": null, "summary": "Document-level context is crucial for handling discourse challenges in\ntext-to-text document-level machine translation (MT). Despite the increased\ndiscourse challenges introduced by noise from automatic speech recognition\n(ASR), the integration of document-level context in speech translation (ST)\nremains insufficiently explored. In this paper, we develop DoCIA, an online\nframework that enhances ST performance by incorporating document-level context.\nDoCIA decomposes the ST pipeline into four stages. Document-level context is\nintegrated into the ASR refinement, MT, and MT refinement stages through\nauxiliary LLM (large language model)-based modules. Furthermore, DoCIA\nleverages document-level information in a multi-level manner while minimizing\ncomputational overhead. Additionally, a simple yet effective determination\nmechanism is introduced to prevent hallucinations from excessive refinement,\nensuring the reliability of the final results. Experimental results show that\nDoCIA significantly outperforms traditional ST baselines in both sentence and\ndiscourse metrics across four LLMs, demonstrating its effectiveness in\nimproving ST performance.", "AI": {"task": "\u5f00\u53d1\u4e00\u4e2a\u5728\u7ebf\u6846\u67b6\uff08DoCIA\uff09\u4ee5\u901a\u8fc7\u5f15\u5165\u6587\u6863\u7ea7\u4e0a\u4e0b\u6587\u63d0\u5347\u8bed\u97f3\u7ffb\u8bd1\uff08ST\uff09\u6027\u80fd\u3002", "motivation": "\u6587\u6863\u7ea7\u4e0a\u4e0b\u6587\u5bf9\u5904\u7406\u6587\u672c\u5230\u6587\u672c\u7684\u6587\u6863\u7ea7\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u4e2d\u7684\u8bed\u7bc7\u6311\u6218\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u8bed\u97f3\u7ffb\u8bd1\uff08ST\uff09\u4e2d\uff0c\u7531\u4e8e\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u566a\u58f0\u7684\u5f15\u5165\uff0c\u6587\u6863\u7ea7\u4e0a\u4e0b\u6587\u7684\u6574\u5408\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "DoCIA\u5c06ST\u6d41\u7a0b\u5206\u89e3\u4e3a\u56db\u4e2a\u9636\u6bb5\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8f85\u52a9\u6a21\u5757\u5728ASR\u7ec6\u5316\u3001MT\u548cMT\u7ec6\u5316\u9636\u6bb5\u5f15\u5165\u6587\u6863\u7ea7\u4e0a\u4e0b\u6587\uff0c\u540c\u65f6\u4ee5\u591a\u7ea7\u65b9\u5f0f\u5229\u7528\u6587\u6863\u7ea7\u4fe1\u606f\u5e76\u6700\u5c0f\u5316\u8ba1\u7b97\u5f00\u9500\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u786e\u5b9a\u673a\u5236\u4ee5\u9632\u6b62\u8fc7\u5ea6\u7ec6\u5316\u5bfc\u81f4\u7684\u5e7b\u89c9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDoCIA\u5728\u56db\u4e2aLLM\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edfST\u57fa\u7ebf\uff0c\u5728\u53e5\u5b50\u548c\u8bed\u7bc7\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "DoCIA\u901a\u8fc7\u6709\u6548\u6574\u5408\u6587\u6863\u7ea7\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u7ffb\u8bd1\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2504.04722", "pdf": "https://arxiv.org/pdf/2504.04722", "abs": "https://arxiv.org/abs/2504.04722", "authors": ["Adnan Khan", "Alireza Choubineh", "Mai A. Shaaban", "Abbas Akkasi", "Majid Komeili"], "title": "TactileNet: Bridging the Accessibility Gap with AI-Generated Tactile Graphics for Individuals with Vision Impairment", "categories": ["cs.CV"], "comment": null, "summary": "Tactile graphics are essential for providing access to visual information for\nthe 43 million people globally living with vision loss, as estimated by global\nprevalence data. However, traditional methods for creating these tactile\ngraphics are labor-intensive and struggle to meet demand. We introduce\nTactileNet, the first comprehensive dataset and AI-driven framework for\ngenerating tactile graphics using text-to-image Stable Diffusion (SD) models.\nBy integrating Low-Rank Adaptation (LoRA) and DreamBooth, our method fine-tunes\nSD models to produce high-fidelity, guideline-compliant tactile graphics while\nreducing computational costs. Evaluations involving tactile experts show that\ngenerated graphics achieve 92.86% adherence to tactile standards and 100%\nalignment with natural images in posture and features. Our framework also\ndemonstrates scalability, generating 32,000 images (7,050 filtered for quality)\nacross 66 classes, with prompt editing enabling customizable outputs (e.g.,\nadding/removing details). Our work empowers designers to focus on refinement,\nsignificantly accelerating accessibility efforts. It underscores the\ntransformative potential of AI for social good, offering a scalable solution to\nbridge the accessibility gap in education and beyond.", "AI": {"task": "\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u7684Stable Diffusion\u6a21\u578b\u751f\u6210\u7b26\u5408\u89e6\u89c9\u6807\u51c6\u7684\u89e6\u89c9\u56fe\u5f62\u3002", "motivation": "\u4f20\u7edf\u89e6\u89c9\u56fe\u5f62\u5236\u4f5c\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u4e14\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\uff0c\u4e9f\u9700\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u96c6\u6210Low-Rank Adaptation (LoRA)\u548cDreamBooth\u6280\u672f\uff0c\u5fae\u8c03Stable Diffusion\u6a21\u578b\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u89e6\u89c9\u56fe\u5f62\u3002", "result": "\u751f\u6210\u7684\u89e6\u89c9\u56fe\u5f62\u5728\u89e6\u89c9\u6807\u51c6\u4e0a\u8fbe\u523092.86%\u7684\u7b26\u5408\u7387\uff0c\u5e76\u5728\u59ff\u52bf\u548c\u7279\u5f81\u4e0a\u4e0e\u81ea\u7136\u56fe\u50cf100%\u5bf9\u9f50\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u89e6\u89c9\u56fe\u5f62\u7684\u751f\u6210\u6548\u7387\uff0c\u4e3a\u6559\u80b2\u7b49\u9886\u57df\u7684\u65e0\u969c\u788d\u9700\u6c42\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.05154", "pdf": "https://arxiv.org/pdf/2504.05154", "abs": "https://arxiv.org/abs/2504.05154", "authors": ["Geyang Guo", "Tarek Naous", "Hiromi Wakaki", "Yukiko Nishimura", "Yuki Mitsufuji", "Alan Ritter", "Wei Xu"], "title": "CARE: Aligning Language Models for Regional Cultural Awareness", "categories": ["cs.CL"], "comment": "24 pages", "summary": "Existing language models (LMs) often exhibit a Western-centric bias and\nstruggle to represent diverse cultural knowledge. Previous attempts to address\nthis rely on synthetic data and express cultural knowledge only in English. In\nthis work, we study whether a small amount of human-written, multilingual\ncultural preference data can improve LMs across various model families and\nsizes. We first introduce CARE, a multilingual resource of 24.1k responses with\nhuman preferences on 2,580 questions about Chinese and Arab cultures, all\ncarefully annotated by native speakers and offering more balanced coverage.\nUsing CARE, we demonstrate that cultural alignment improves existing LMs beyond\ngeneric resources without compromising general capabilities. Moreover, we\nevaluate the cultural awareness of LMs, native speakers, and retrieved web\ncontent when queried in different languages. Our experiment reveals regional\ndisparities among LMs, which may also be reflected in the documentation gap:\nnative speakers often take everyday cultural commonsense and social norms for\ngranted, while non-natives are more likely to actively seek out and document\nthem. CARE is publicly available at https://github.com/Guochry/CARE (we plan to\nadd Japanese data in the near future).", "AI": {"task": "\u7814\u7a76\u5c11\u91cf\u4eba\u7c7b\u7f16\u5199\u7684\u591a\u8bed\u8a00\u6587\u5316\u504f\u597d\u6570\u636e\u662f\u5426\u80fd\u6539\u5584\u8bed\u8a00\u6a21\u578b\u5bf9\u4e0d\u540c\u6587\u5316\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u897f\u65b9\u4e2d\u5fc3\u504f\u89c1\uff0c\u4e14\u96be\u4ee5\u8868\u8fbe\u591a\u6837\u6587\u5316\u77e5\u8bc6\uff0c\u4e4b\u524d\u7684\u65b9\u6cd5\u4f9d\u8d56\u5408\u6210\u6570\u636e\u4e14\u4ec5\u7528\u82f1\u8bed\u8868\u8fbe\u6587\u5316\u77e5\u8bc6\u3002", "method": "\u5f15\u5165CARE\u8d44\u6e90\uff08\u5305\u542b24.1k\u6761\u4eba\u7c7b\u504f\u597d\u54cd\u5e94\uff0c\u6db5\u76d6\u4e2d\u963f\u6587\u5316\u76842,580\u4e2a\u95ee\u9898\uff09\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u8bed\u8a00\u6a21\u578b\u7684\u6539\u8fdb\u6548\u679c\u3002", "result": "CARE\u80fd\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u5bf9\u9f50\u6027\u800c\u4e0d\u635f\u5bb3\u901a\u7528\u80fd\u529b\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5316\u610f\u8bc6\u4e0a\u7684\u533a\u57df\u5dee\u5f02\u3002", "conclusion": "\u5c11\u91cf\u9ad8\u8d28\u91cf\u591a\u8bed\u8a00\u6587\u5316\u6570\u636e\u53ef\u663e\u8457\u6539\u5584\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u8868\u73b0\uff0c\u4e14CARE\u8d44\u6e90\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2504.04728", "pdf": "https://arxiv.org/pdf/2504.04728", "abs": "https://arxiv.org/abs/2504.04728", "authors": ["Sheng Zheng", "Chaoning Zhang", "Dongshen Han", "Fachrina Dewi Puspitasari", "Xinhong Hao", "Yang Yang", "Heng Tao Shen"], "title": "Exploring Kernel Transformations for Implicit Neural Representations", "categories": ["cs.CV"], "comment": "Accepted at IEEE Transactions on Multimedia (TMM) on December 20,\n  2024 (To appear on IEEE Website soon)", "summary": "Implicit neural representations (INRs), which leverage neural networks to\nrepresent signals by mapping coordinates to their corresponding attributes,\nhave garnered significant attention. They are extensively utilized for image\nrepresentation, with pixel coordinates as input and pixel values as output. In\ncontrast to prior works focusing on investigating the effect of the model's\ninside components (activation function, for instance), this work pioneers the\nexploration of the effect of kernel transformation of input/output while\nkeeping the model itself unchanged. A byproduct of our findings is a simple yet\neffective method that combines scale and shift to significantly boost INR with\nnegligible computation overhead. Moreover, we present two perspectives, depth\nand normalization, to interpret the performance benefits caused by scale and\nshift transformation. Overall, our work provides a new avenue for future works\nto understand and improve INR through the lens of kernel transformation.", "AI": {"task": "\u63a2\u7d22\u8f93\u5165/\u8f93\u51fa\u7684\u6838\u53d8\u6362\u5bf9\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INRs\uff09\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u5185\u90e8\u7ec4\u4ef6\uff08\u5982\u6fc0\u6d3b\u51fd\u6570\uff09\u7684\u5f71\u54cd\uff0c\u800c\u5ffd\u7565\u4e86\u8f93\u5165/\u8f93\u51fa\u7684\u6838\u53d8\u6362\u5bf9INRs\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u4fdd\u6301\u6a21\u578b\u4e0d\u53d8\uff0c\u7814\u7a76\u8f93\u5165/\u8f93\u51fa\u7684\u6838\u53d8\u6362\uff08\u5982\u5c3a\u5ea6\u548c\u4f4d\u79fb\uff09\u5bf9INRs\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u5c3a\u5ea6\u548c\u4f4d\u79fb\u53d8\u6362\u80fd\u663e\u8457\u63d0\u5347INRs\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u53ef\u5ffd\u7565\uff1b\u540c\u65f6\u4ece\u6df1\u5ea6\u548c\u5f52\u4e00\u5316\u89d2\u5ea6\u89e3\u91ca\u4e86\u6027\u80fd\u63d0\u5347\u7684\u539f\u56e0\u3002", "conclusion": "\u4e3a\u672a\u6765\u901a\u8fc7\u6838\u53d8\u6362\u7406\u89e3\u548c\u6539\u8fdbINRs\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.05185", "pdf": "https://arxiv.org/pdf/2504.05185", "abs": "https://arxiv.org/abs/2504.05185", "authors": ["Mehdi Fatemi", "Banafsheh Rafiee", "Mingjie Tang", "Kartik Talamadupula"], "title": "Concise Reasoning via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advancements in large language models (LLMs), a major\ndrawback of reasoning models is their enormous token usage, which increases\ncomputational cost, resource requirements, and response time. In this work, we\nrevisit the core principles of reinforcement learning (RL) and, through\nmathematical analysis, demonstrate that the tendency to generate lengthy\nresponses arises inherently from RL-based optimization during training. This\nfinding questions the prevailing assumption that longer responses inherently\nimprove reasoning accuracy. Instead, we uncover a natural correlation between\nconciseness and accuracy that has been largely overlooked. Moreover, we show\nthat introducing a secondary phase of RL post-training, using a small set of\nproblems and limited resources, can significantly reduce a model's chain of\nthought while maintaining or even enhancing accuracy. Finally, we validate our\nconclusions through extensive experimental results.", "AI": {"task": "\u7814\u7a76\u5982\u4f55\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4ee4\u724c\u4f7f\u7528\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u7684\u63a8\u7406\u6a21\u578b\u503e\u5411\u4e8e\u751f\u6210\u957f\u54cd\u5e94\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u8d44\u6e90\u9700\u6c42\u548c\u54cd\u5e94\u65f6\u95f4\uff0c\u4f46\u957f\u54cd\u5e94\u5e76\u4e0d\u4e00\u5b9a\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u5206\u6790\u63ed\u793aRL\u8bad\u7ec3\u5bfc\u81f4\u957f\u54cd\u5e94\u7684\u503e\u5411\uff0c\u5e76\u63d0\u51fa\u5728\u8bad\u7ec3\u540e\u5f15\u5165\u7b2c\u4e8c\u9636\u6bb5RL\u4f18\u5316\u4ee5\u51cf\u5c11\u4ee4\u724c\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u51cf\u5c11\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u7b80\u6d01\u6027\u4e0e\u51c6\u786e\u6027\u5b58\u5728\u81ea\u7136\u76f8\u5173\u6027\uff0c\u901a\u8fc7RL\u4f18\u5316\u53ef\u4ee5\u9ad8\u6548\u51cf\u5c11\u4ee4\u724c\u4f7f\u7528\u800c\u4e0d\u727a\u7272\u6027\u80fd\u3002"}}
{"id": "2504.04732", "pdf": "https://arxiv.org/pdf/2504.04732", "abs": "https://arxiv.org/abs/2504.04732", "authors": ["Zhenxing Ming", "Julie Stephany Berrio", "Mao Shan", "Stewart Worrall"], "title": "Inverse++: Vision-Centric 3D Semantic Occupancy Prediction Assisted with 3D Object Detection", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "3D semantic occupancy prediction aims to forecast detailed geometric and\nsemantic information of the surrounding environment for autonomous vehicles\n(AVs) using onboard surround-view cameras. Existing methods primarily focus on\nintricate inner structure module designs to improve model performance, such as\nefficient feature sampling and aggregation processes or intermediate feature\nrepresentation formats. In this paper, we explore multitask learning by\nintroducing an additional 3D supervision signal by incorporating an additional\n3D object detection auxiliary branch. This extra 3D supervision signal enhances\nthe model's overall performance by strengthening the capability of the\nintermediate features to capture small dynamic objects in the scene, and these\nsmall dynamic objects often include vulnerable road users, i.e. bicycles,\nmotorcycles, and pedestrians, whose detection is crucial for ensuring driving\nsafety in autonomous vehicles. Extensive experiments conducted on the nuScenes\ndatasets, including challenging rainy and nighttime scenarios, showcase that\nour approach attains state-of-the-art results, achieving an IoU score of 31.73%\nand a mIoU score of 20.91% and excels at detecting vulnerable road users (VRU).\nThe code will be made available at:https://github.com/DanielMing123/Inverse++", "AI": {"task": "\u901a\u8fc7\u5f15\u5165\u989d\u5916\u76843D\u7269\u4f53\u68c0\u6d4b\u8f85\u52a9\u5206\u652f\uff0c\u63a2\u7d22\u591a\u4efb\u52a1\u5b66\u4e60\u4ee5\u63d0\u9ad83D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u590d\u6742\u7684\u5185\u90e8\u7ed3\u6784\u6a21\u5757\u8bbe\u8ba1\uff0c\u800c\u672c\u6587\u901a\u8fc7\u5f15\u5165\u989d\u5916\u76843D\u76d1\u7763\u4fe1\u53f7\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u573a\u666f\u4e2d\u5c0f\u52a8\u6001\u7269\u4f53\u7684\u6355\u6349\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5bf9\u6613\u53d7\u4f24\u5bb3\u7684\u9053\u8def\u4f7f\u7528\u8005\uff08\u5982\u81ea\u884c\u8f66\u3001\u6469\u6258\u8f66\u548c\u884c\u4eba\uff09\u7684\u68c0\u6d4b\uff0c\u8fd9\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u989d\u5916\u76843D\u7269\u4f53\u68c0\u6d4b\u8f85\u52a9\u5206\u652f\uff0c\u63d0\u4f9b\u989d\u5916\u76843D\u76d1\u7763\u4fe1\u53f7\uff0c\u4ece\u800c\u589e\u5f3a\u4e2d\u95f4\u7279\u5f81\u5bf9\u5c0f\u52a8\u6001\u7269\u4f53\u7684\u6355\u6349\u80fd\u529b\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cIoU\u5f97\u5206\u4e3a31.73%\uff0cmIoU\u5f97\u5206\u4e3a20.91%\uff0c\u5e76\u4e14\u5728\u68c0\u6d4b\u6613\u53d7\u4f24\u5bb3\u7684\u9053\u8def\u4f7f\u7528\u8005\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u989d\u5916\u76843D\u76d1\u7763\u4fe1\u53f7\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u672c\u6587\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e863D\u8bed\u4e49\u5360\u7528\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u68c0\u6d4b\u5c0f\u52a8\u6001\u7269\u4f53\u65b9\u9762\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2504.05211", "pdf": "https://arxiv.org/pdf/2504.05211", "abs": "https://arxiv.org/abs/2504.05211", "authors": ["Richard A. Blythe", "Casimir Fisch"], "title": "Exploiting individual differences to bootstrap communication", "categories": ["cs.CL", "physics.soc-ph", "q-bio.PE"], "comment": "13 pages including supplementary information, 3 figures", "summary": "Establishing a communication system is hard because the intended meaning of a\nsignal is unknown to its receiver when first produced, and the signaller also\nhas no idea how that signal will be interpreted. Most theoretical accounts of\nthe emergence of communication systems rely on feedback to reinforce behaviours\nthat have led to successful communication in the past. However, providing such\nfeedback requires already being able to communicate the meaning that was\nintended or interpreted. Therefore these accounts cannot explain how\ncommunication can be bootstrapped from non-communicative behaviours. Here we\npresent a model that shows how a communication system, capable of expressing an\nunbounded number of meanings, can emerge as a result of individual behavioural\ndifferences in a large population without any pre-existing means to determine\ncommunicative success. The two key cognitive capabilities responsible for this\noutcome are behaving predictably in a given situation, and an alignment of\npsychological states ahead of signal production that derives from shared\nintentionality. Since both capabilities can exist independently of\ncommunication, our results are compatible with theories in which large flexible\nsocially-learned communication systems like language are the product of a\ngeneral but well-developed capacity for social cognition.", "AI": {"task": "\u7814\u7a76\u5982\u4f55\u5728\u6ca1\u6709\u9884\u5148\u5b58\u5728\u7684\u6c9f\u901a\u624b\u6bb5\u7684\u60c5\u51b5\u4e0b\uff0c\u4ece\u975e\u6c9f\u901a\u884c\u4e3a\u4e2d\u5f15\u5bfc\u51fa\u6c9f\u901a\u7cfb\u7edf\u7684\u51fa\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u7406\u8bba\u4f9d\u8d56\u4e8e\u53cd\u9988\u673a\u5236\u6765\u89e3\u91ca\u6c9f\u901a\u7cfb\u7edf\u7684\u5f62\u6210\uff0c\u4f46\u53cd\u9988\u672c\u8eab\u9700\u8981\u9884\u5148\u5b58\u5728\u7684\u6c9f\u901a\u80fd\u529b\uff0c\u65e0\u6cd5\u89e3\u91ca\u6c9f\u901a\u7cfb\u7edf\u7684\u521d\u59cb\u5f15\u5bfc\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u578b\uff0c\u5c55\u793a\u5728\u5927\u7fa4\u4f53\u4e2d\uff0c\u7531\u4e8e\u4e2a\u4f53\u884c\u4e3a\u5dee\u5f02\u548c\u5171\u4eab\u610f\u5411\u6027\uff0c\u80fd\u591f\u8868\u8fbe\u65e0\u9650\u6570\u91cf\u610f\u4e49\u7684\u6c9f\u901a\u7cfb\u7edf\u5982\u4f55\u81ea\u53d1\u5f62\u6210\u3002", "result": "\u6a21\u578b\u8868\u660e\uff0c\u901a\u8fc7\u4e2a\u4f53\u884c\u4e3a\u7684\u53ef\u9884\u6d4b\u6027\u548c\u5171\u4eab\u610f\u5411\u6027\uff0c\u53ef\u4ee5\u5728\u6ca1\u6709\u9884\u5148\u6c9f\u901a\u624b\u6bb5\u7684\u60c5\u51b5\u4e0b\u5f15\u5bfc\u51fa\u6c9f\u901a\u7cfb\u7edf\u3002", "conclusion": "\u6c9f\u901a\u7cfb\u7edf\u7684\u5f62\u6210\u4f9d\u8d56\u4e8e\u793e\u4f1a\u8ba4\u77e5\u7684\u4e00\u822c\u80fd\u529b\uff0c\u652f\u6301\u8bed\u8a00\u7b49\u5927\u578b\u7075\u6d3b\u6c9f\u901a\u7cfb\u7edf\u662f\u793e\u4f1a\u8ba4\u77e5\u80fd\u529b\u7684\u4ea7\u7269\u3002"}}
{"id": "2504.04740", "pdf": "https://arxiv.org/pdf/2504.04740", "abs": "https://arxiv.org/abs/2504.04740", "authors": ["Samarth Mishra", "Kate Saenko", "Venkatesh Saligrama"], "title": "Enhancing Compositional Reasoning in Vision-Language Models with Synthetic Preference Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Compositionality, or correctly recognizing scenes as compositions of atomic\nvisual concepts, remains difficult for multimodal large language models\n(MLLMs). Even state of the art MLLMs such as GPT-4o can make mistakes in\ndistinguishing compositions like \"dog chasing cat\" vs \"cat chasing dog\". While\non Winoground, a benchmark for measuring such reasoning, MLLMs have made\nsignificant progress, they are still far from a human's performance. We show\nthat compositional reasoning in these models can be improved by elucidating\nsuch concepts via data, where a model is trained to prefer the correct caption\nfor an image over a close but incorrect one. We introduce SCRAMBLe: Synthetic\nCompositional Reasoning Augmentation of MLLMs with Binary preference Learning,\nan approach for preference tuning open-weight MLLMs on synthetic preference\ndata generated in a fully automated manner from existing image-caption data.\nSCRAMBLe holistically improves these MLLMs' compositional reasoning\ncapabilities which we can see through significant improvements across multiple\nvision language compositionality benchmarks, as well as smaller but significant\nimprovements on general question answering tasks. As a sneak peek, SCRAMBLe\ntuned Molmo-7B model improves on Winoground from 49.5% to 54.8% (best reported\nto date), while improving by ~1% on more general visual question answering\ntasks. Code for SCRAMBLe along with tuned models and our synthetic training\ndataset is available at https://github.com/samarth4149/SCRAMBLe.", "AI": {"task": "\u901a\u8fc7\u6570\u636e\u9610\u660e\u89c6\u89c9\u6982\u5ff5\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524dMLLMs\u5728\u7ec4\u5408\u6027\u63a8\u7406\uff08\u5982\u533a\u5206\u201c\u72d7\u8ffd\u732b\u201d\u548c\u201c\u732b\u8ffd\u72d7\u201d\uff09\u4e0a\u4ecd\u843d\u540e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faSCRAMBLe\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u504f\u597d\u6570\u636e\u5bf9MLLMs\u8fdb\u884c\u504f\u597d\u8c03\u4f18\uff0c\u63d0\u5347\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u3002", "result": "SCRAMBLe\u663e\u8457\u63d0\u5347MLLMs\u5728\u7ec4\u5408\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5728\u901a\u7528\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u7565\u6709\u63d0\u5347\u3002", "conclusion": "SCRAMBLe\u662f\u4e00\u79cd\u6709\u6548\u63d0\u5347MLLMs\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4e14\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2504.05214", "pdf": "https://arxiv.org/pdf/2504.05214", "abs": "https://arxiv.org/abs/2504.05214", "authors": ["Sefika Efeoglu", "Adrian Paschke", "Sonja Schimmler"], "title": "Post-Training Language Models for Continual Relation Extraction", "categories": ["cs.CL"], "comment": "17 pages", "summary": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction.", "AI": {"task": "\u7814\u7a76\u5982\u4f55\u5c06\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLMs\uff09\u5e94\u7528\u4e8e\u6301\u7eed\u5173\u7cfb\u62bd\u53d6\uff08CRE\uff09\uff0c\u4ee5\u89e3\u51b3\u52a8\u6001\u6570\u636e\u4e2d\u7684\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u52a8\u6001\u4e14\u975e\u5e73\u7a33\uff0c\u4f20\u7edf\u5173\u7cfb\u62bd\u53d6\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u9002\u5e94\u6570\u636e\u53d8\u5316\uff0c\u9700\u8981\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u89e3\u7801\u5668\u6a21\u578b\uff08\u5982Mistral-7B\u3001Llama2-7B\uff09\u548c\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\uff08\u5982Flan-T5 Base\uff09\uff0c\u901a\u8fc7\u4efb\u52a1\u589e\u91cf\u5fae\u8c03\u548c\u8bb0\u5fc6\u56de\u653e\u6280\u672f\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728TACRED\u548cFewRel\u6570\u636e\u96c6\u4e0a\uff0c\u89e3\u7801\u5668\u548c\u7f16\u7801\u5668-\u89e3\u7801\u5668\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728Mistral\u548cFlan-T5\u6a21\u578b\u4e0a\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "conclusion": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u8bb0\u5fc6\u56de\u653e\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347\u6301\u7eed\u5173\u7cfb\u62bd\u53d6\u6027\u80fd\uff0c\u4e3a\u52a8\u6001\u5b9e\u65f6\u5173\u7cfb\u62bd\u53d6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2504.04743", "pdf": "https://arxiv.org/pdf/2504.04743", "abs": "https://arxiv.org/abs/2504.04743", "authors": ["Xiongbo Lu", "Yaxiong Chen", "Shengwu Xiong"], "title": "AnyArtisticGlyph: Multilingual Controllable Artistic Glyph Generation", "categories": ["cs.CV"], "comment": null, "summary": "Artistic Glyph Image Generation (AGIG) differs from current\ncreativity-focused generation models by offering finely controllable\ndeterministic generation. It transfers the style of a reference image to a\nsource while preserving its content. Although advanced and promising, current\nmethods may reveal flaws when scrutinizing synthesized image details, often\nproducing blurred or incorrect textures, posing a significant challenge. Hence,\nwe introduce AnyArtisticGlyph, a diffusion-based, multilingual controllable\nartistic glyph generation model. It includes a font fusion and embedding\nmodule, which generates latent features for detailed structure creation, and a\nvision-text fusion and embedding module that uses the CLIP model to encode\nreferences and blends them with transformation caption embeddings for seamless\nglobal image generation. Moreover, we incorporate a coarse-grained\nfeature-level loss to enhance generation accuracy. Experiments show that it\nproduces natural, detailed artistic glyph images with state-of-the-art\nperformance. Our project will be open-sourced on\nhttps://github.com/jiean001/AnyArtisticGlyph to advance text generation\ntechnology.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u8bed\u8a00\u53ef\u63a7\u827a\u672f\u5b57\u5f62\u751f\u6210\u65b9\u6cd5\uff08AnyArtisticGlyph\uff09\u3002", "motivation": "\u73b0\u6709\u827a\u672f\u5b57\u5f62\u751f\u6210\u65b9\u6cd5\u5728\u7ec6\u8282\u4e0a\u5b58\u5728\u6a21\u7cca\u6216\u9519\u8bef\u7eb9\u7406\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u548c\u751f\u6210\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u5b57\u4f53\u878d\u5408\u4e0e\u5d4c\u5165\u6a21\u5757\u751f\u6210\u6f5c\u5728\u7279\u5f81\uff0c\u5229\u7528CLIP\u6a21\u578b\u7f16\u7801\u53c2\u8003\u56fe\u50cf\u5e76\u4e0e\u6587\u672c\u5d4c\u5165\u878d\u5408\uff0c\u540c\u65f6\u5f15\u5165\u7c97\u7c92\u5ea6\u7279\u5f81\u7ea7\u635f\u5931\u63d0\u5347\u751f\u6210\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u81ea\u7136\u4e14\u7ec6\u8282\u4e30\u5bcc\u7684\u827a\u672f\u5b57\u5f62\u56fe\u50cf\uff0c\u6027\u80fd\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "AnyArtisticGlyph\u901a\u8fc7\u6539\u8fdb\u751f\u6210\u7ec6\u8282\u548c\u63a7\u5236\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u827a\u672f\u5b57\u5f62\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5e76\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u6587\u672c\u751f\u6210\u6280\u672f\u7684\u8fdb\u6b65\u3002"}}
{"id": "2504.05226", "pdf": "https://arxiv.org/pdf/2504.05226", "abs": "https://arxiv.org/abs/2504.05226", "authors": ["Jungyeul Park"], "title": "Proposing TAGbank as a Corpus of Tree-Adjoining Grammar Derivations", "categories": ["cs.CL"], "comment": null, "summary": "The development of lexicalized grammars, particularly Tree-Adjoining Grammar\n(TAG), has significantly advanced our understanding of syntax and semantics in\nnatural language processing (NLP). While existing syntactic resources like the\nPenn Treebank and Universal Dependencies offer extensive annotations for\nphrase-structure and dependency parsing, there is a lack of large-scale corpora\ngrounded in lexicalized grammar formalisms. To address this gap, we introduce\nTAGbank, a corpus of TAG derivations automatically extracted from existing\nsyntactic treebanks. This paper outlines a methodology for mapping\nphrase-structure annotations to TAG derivations, leveraging the generative\npower of TAG to support parsing, grammar induction, and semantic analysis. Our\napproach builds on the work of CCGbank, extending it to incorporate the unique\nstructural properties of TAG, including its transparent derivation trees and\nits ability to capture long-distance dependencies. We also discuss the\nchallenges involved in the extraction process, including ensuring consistency\nacross treebank schemes and dealing with language-specific syntactic\nidiosyncrasies. Finally, we propose the future extension of TAGbank to include\nmultilingual corpora, focusing on the Penn Korean and Penn Chinese Treebanks,\nto explore the cross-linguistic application of TAG's formalism. By providing a\nrobust, derivation-based resource, TAGbank aims to support a wide range of\ncomputational tasks and contribute to the theoretical understanding of TAG's\ngenerative capacity.", "AI": {"task": "\u6784\u5efaTAGbank\uff0c\u4e00\u4e2a\u57fa\u4e8e\u6811\u90bb\u63a5\u8bed\u6cd5\uff08TAG\uff09\u7684\u5927\u89c4\u6a21\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u652f\u6301\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u8bed\u6cd5\u5206\u6790\u548c\u8bed\u4e49\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u8bed\u6599\u5e93\uff08\u5982Penn Treebank\u548cUniversal Dependencies\uff09\u4e3b\u8981\u57fa\u4e8e\u77ed\u8bed\u7ed3\u6784\u548c\u4f9d\u5b58\u8bed\u6cd5\uff0c\u7f3a\u4e4f\u57fa\u4e8e\u8bcd\u6c47\u5316\u8bed\u6cd5\u5f62\u5f0f\uff08\u5982TAG\uff09\u7684\u8bed\u6599\u8d44\u6e90\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u5c06\u77ed\u8bed\u7ed3\u6784\u6807\u6ce8\u81ea\u52a8\u6620\u5c04\u5230TAG\u63a8\u5bfc\uff0c\u5229\u7528TAG\u7684\u751f\u6210\u80fd\u529b\u652f\u6301\u89e3\u6790\u3001\u8bed\u6cd5\u5f52\u7eb3\u548c\u8bed\u4e49\u5206\u6790\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86TAGbank\uff0c\u5e76\u8ba8\u8bba\u4e86\u63d0\u53d6\u8fc7\u7a0b\u4e2d\u7684\u6311\u6218\uff0c\u5982\u786e\u4fdd\u6811\u5e93\u65b9\u6848\u7684\u4e00\u81f4\u6027\u548c\u5904\u7406\u8bed\u8a00\u7279\u5b9a\u7684\u53e5\u6cd5\u7279\u6027\u3002", "conclusion": "TAGbank\u4e3a\u8ba1\u7b97\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u8d44\u6e90\uff0c\u5e76\u6709\u52a9\u4e8e\u7406\u8bba\u7406\u89e3TAG\u7684\u751f\u6210\u80fd\u529b\uff0c\u672a\u6765\u8ba1\u5212\u6269\u5c55\u5230\u591a\u8bed\u8a00\u8bed\u6599\u5e93\u3002"}}
{"id": "2504.04744", "pdf": "https://arxiv.org/pdf/2504.04744", "abs": "https://arxiv.org/abs/2504.04744", "authors": ["He Zhu", "Quyu Kong", "Kechun Xu", "Xunlong Xia", "Bing Deng", "Jieping Ye", "Rong Xiong", "Yue Wang"], "title": "Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "CVPR 2025", "summary": "Grounding 3D object affordance is a task that locates objects in 3D space\nwhere they can be manipulated, which links perception and action for embodied\nintelligence. For example, for an intelligent robot, it is necessary to\naccurately ground the affordance of an object and grasp it according to human\ninstructions. In this paper, we introduce a novel task that grounds 3D object\naffordance based on language instructions, visual observations and\ninteractions, which is inspired by cognitive science. We collect an Affordance\nGrounding dataset with Points, Images and Language instructions (AGPIL) to\nsupport the proposed task. In the 3D physical world, due to observation\norientation, object rotation, or spatial occlusion, we can only get a partial\nobservation of the object. So this dataset includes affordance estimations of\nobjects from full-view, partial-view, and rotation-view perspectives. To\naccomplish this task, we propose LMAffordance3D, the first multi-modal,\nlanguage-guided 3D affordance grounding network, which applies a\nvision-language model to fuse 2D and 3D spatial features with semantic\nfeatures. Comprehensive experiments on AGPIL demonstrate the effectiveness and\nsuperiority of our method on this task, even in unseen experimental settings.\nOur project is available at https://sites.google.com/view/lmaffordance3d.", "AI": {"task": "\u57fa\u4e8e\u8bed\u8a00\u6307\u4ee4\u3001\u89c6\u89c9\u89c2\u5bdf\u548c\u4ea4\u4e92\uff0c\u5b9a\u4f4d3D\u7a7a\u95f4\u4e2d\u53ef\u64cd\u4f5c\u7269\u4f53\u7684\u4efb\u52a1\u3002", "motivation": "\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\u94fe\u63a5\u611f\u77e5\u4e0e\u52a8\u4f5c\uff0c\u4f8b\u5982\u667a\u80fd\u673a\u5668\u4eba\u9700\u8981\u6839\u636e\u4eba\u7c7b\u6307\u4ee4\u51c6\u786e\u8bc6\u522b\u7269\u4f53\u53ef\u64cd\u4f5c\u6027\u5e76\u6293\u53d6\u3002", "method": "\u63d0\u51faLMAffordance3D\uff0c\u4e00\u79cd\u591a\u6a21\u6001\u3001\u8bed\u8a00\u5f15\u5bfc\u76843D\u53ef\u64cd\u4f5c\u6027\u5b9a\u4f4d\u7f51\u7edc\uff0c\u878d\u54082D/3D\u7a7a\u95f4\u7279\u5f81\u4e0e\u8bed\u4e49\u7279\u5f81\u3002", "result": "\u5728AGPIL\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u5305\u62ec\u672a\u89c1\u8fc7\u7684\u5b9e\u9a8c\u573a\u666f\u3002", "conclusion": "\u63d0\u51fa\u7684\u4efb\u52a1\u548c\u65b9\u6cd5\u4e3a3D\u7269\u4f53\u53ef\u64cd\u4f5c\u6027\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2504.05228", "pdf": "https://arxiv.org/pdf/2504.05228", "abs": "https://arxiv.org/abs/2504.05228", "authors": ["Yiming Zhang", "Harshita Diddee", "Susan Holm", "Hanchen Liu", "Xinyue Liu", "Vinay Samuel", "Barry Wang", "Daphne Ippolito"], "title": "NoveltyBench: Evaluating Creativity and Diversity in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Language models have demonstrated remarkable capabilities on standard\nbenchmarks, yet they struggle increasingly from mode collapse, the inability to\ngenerate diverse and novel outputs. Our work introduces NoveltyBench, a\nbenchmark specifically designed to evaluate the ability of language models to\nproduce multiple distinct and high-quality outputs. NoveltyBench utilizes\nprompts curated to elicit diverse answers and filtered real-world user queries.\nEvaluating 20 leading language models, we find that current state-of-the-art\nsystems generate significantly less diversity than human writers. Notably,\nlarger models within a family often exhibit less diversity than their smaller\ncounterparts, challenging the notion that capability on standard benchmarks\ntranslates directly to generative utility. While prompting strategies like\nin-context regeneration can elicit diversity, our findings highlight a\nfundamental lack of distributional diversity in current models, reducing their\nutility for users seeking varied responses and suggesting the need for new\ntraining and evaluation paradigms that prioritize creativity alongside quality.", "AI": {"task": "\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u548c\u65b0\u9896\u8f93\u51fa\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u751f\u6210\u591a\u6837\u5316\u548c\u65b0\u9896\u8f93\u51fa\u65b9\u9762\u5b58\u5728\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f15\u5165NoveltyBench\u57fa\u51c6\uff0c\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u8bc4\u4f3020\u79cd\u9886\u5148\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6837\u6027\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u751f\u6210\u7684\u591a\u6837\u6027\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u4f5c\u8005\uff0c\u4e14\u540c\u4e00\u5bb6\u65cf\u4e2d\u66f4\u5927\u7684\u6a21\u578b\u901a\u5e38\u6bd4\u5c0f\u6a21\u578b\u591a\u6837\u6027\u66f4\u4f4e\u3002", "conclusion": "\u9700\u8981\u65b0\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u8303\u5f0f\uff0c\u4ee5\u5728\u4fdd\u8bc1\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u7684\u521b\u9020\u6027\u3002"}}
{"id": "2504.04747", "pdf": "https://arxiv.org/pdf/2504.04747", "abs": "https://arxiv.org/abs/2504.04747", "authors": ["Yoojin Jung", "Byung Cheol Song"], "title": "Two is Better than One: Efficient Ensemble Defense for Robust and Compact Models", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Deep learning-based computer vision systems adopt complex and large\narchitectures to improve performance, yet they face challenges in deployment on\nresource-constrained mobile and edge devices. To address this issue, model\ncompression techniques such as pruning, quantization, and matrix factorization\nhave been proposed; however, these compressed models are often highly\nvulnerable to adversarial attacks. We introduce the \\textbf{Efficient Ensemble\nDefense (EED)} technique, which diversifies the compression of a single base\nmodel based on different pruning importance scores and enhances ensemble\ndiversity to achieve high adversarial robustness and resource efficiency. EED\ndynamically determines the number of necessary sub-models during the inference\nstage, minimizing unnecessary computations while maintaining high robustness.\nOn the CIFAR-10 and SVHN datasets, EED demonstrated state-of-the-art robustness\nperformance compared to existing adversarial pruning techniques, along with an\ninference speed improvement of up to 1.86 times. This proves that EED is a\npowerful defense solution in resource-constrained environments.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aEfficient Ensemble Defense (EED)\u7684\u6280\u672f\uff0c\u4ee5\u589e\u5f3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u5bf9\u6297\u653b\u51fb\u8106\u5f31\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u4e0d\u540c\u526a\u679d\u91cd\u8981\u6027\u5206\u6570\u5bf9\u5355\u4e00\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u591a\u6837\u5316\u538b\u7f29\uff0c\u5e76\u589e\u5f3a\u96c6\u6210\u591a\u6837\u6027\uff0c\u52a8\u6001\u786e\u5b9a\u63a8\u7406\u9636\u6bb5\u6240\u9700\u7684\u5b50\u6a21\u578b\u6570\u91cf\u3002", "result": "\u5728CIFAR-10\u548cSVHN\u6570\u636e\u96c6\u4e0a\uff0cEED\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u5bf9\u6297\u526a\u679d\u6280\u672f\u7684\u9c81\u68d2\u6027\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe1.86\u500d\u3002", "conclusion": "EED\u662f\u4e00\u79cd\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u9ad8\u6548\u7684\u9632\u5fa1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.05239", "pdf": "https://arxiv.org/pdf/2504.05239", "abs": "https://arxiv.org/abs/2504.05239", "authors": ["Hang Li", "Yucheng Chu", "Kaiqi Yang", "Yasemin Copur-Gencturk", "Jiliang Tang"], "title": "LLM-based Automated Grading with Human-in-the-Loop", "categories": ["cs.CL"], "comment": null, "summary": "The rise of artificial intelligence (AI) technologies, particularly large\nlanguage models (LLMs), has brought significant advancements to the field of\neducation. Among various applications, automatic short answer grading (ASAG),\nwhich focuses on evaluating open-ended textual responses, has seen remarkable\nprogress with the introduction of LLMs. These models not only enhance grading\nperformance compared to traditional ASAG approaches but also move beyond simple\ncomparisons with predefined \"golden\" answers, enabling more sophisticated\ngrading scenarios, such as rubric-based evaluation. However, existing\nLLM-powered methods still face challenges in achieving human-level grading\nperformance in rubric-based assessments due to their reliance on fully\nautomated approaches. In this work, we explore the potential of LLMs in ASAG\ntasks by leveraging their interactive capabilities through a human-in-the-loop\n(HITL) approach. Our proposed framework, GradeHITL, utilizes the generative\nproperties of LLMs to pose questions to human experts, incorporating their\ninsights to refine grading rubrics dynamically. This adaptive process\nsignificantly improves grading accuracy, outperforming existing methods and\nbringing ASAG closer to human-level evaluation.", "AI": {"task": "\u63a2\u7d22\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u540c\uff08HITL\uff09\u65b9\u6cd5\u63d0\u5347\u81ea\u52a8\u77ed\u7b54\u6848\u8bc4\u5206\uff08ASAG\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLMs\u7684ASAG\u65b9\u6cd5\u5728\u57fa\u4e8e\u91cf\u89c4\u7684\u8bc4\u4f30\u4e2d\u96be\u4ee5\u8fbe\u5230\u4eba\u7c7b\u8bc4\u5206\u6c34\u5e73\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGradeHITL\u6846\u67b6\uff0c\u5229\u7528LLMs\u7684\u751f\u6210\u80fd\u529b\u5411\u4eba\u7c7b\u4e13\u5bb6\u63d0\u95ee\uff0c\u52a8\u6001\u4f18\u5316\u8bc4\u5206\u91cf\u89c4\u3002", "result": "GradeHITL\u663e\u8457\u63d0\u9ad8\u4e86\u8bc4\u5206\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63a5\u8fd1\u4eba\u7c7b\u8bc4\u5206\u6c34\u5e73\u3002", "conclusion": "\u4eba\u673a\u534f\u540c\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347ASAG\u6027\u80fd\uff0c\u4e3a\u6559\u80b2\u9886\u57df\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u8bc4\u5206\u5de5\u5177\u3002"}}
{"id": "2504.04753", "pdf": "https://arxiv.org/pdf/2504.04753", "abs": "https://arxiv.org/abs/2504.04753", "authors": ["Cheng Chen", "Jiacheng Wei", "Tianrun Chen", "Chi Zhang", "Xiaofeng Yang", "Shangzhan Zhang", "Bingchen Yang", "Chuan-Sheng Foo", "Guosheng Lin", "Qixing Huang", "Fayao Liu"], "title": "CADCrafter: Generating Computer-Aided Design Models from Unconstrained Images", "categories": ["cs.CV"], "comment": "Accepted to CVPR2025", "summary": "Creating CAD digital twins from the physical world is crucial for\nmanufacturing, design, and simulation. However, current methods typically rely\non costly 3D scanning with labor-intensive post-processing. To provide a\nuser-friendly design process, we explore the problem of reverse engineering\nfrom unconstrained real-world CAD images that can be easily captured by users\nof all experiences. However, the scarcity of real-world CAD data poses\nchallenges in directly training such models. To tackle these challenges, we\npropose CADCrafter, an image-to-parametric CAD model generation framework that\ntrains solely on synthetic textureless CAD data while testing on real-world\nimages. To bridge the significant representation disparity between images and\nparametric CAD models, we introduce a geometry encoder to accurately capture\ndiverse geometric features. Moreover, the texture-invariant properties of the\ngeometric features can also facilitate the generalization to real-world\nscenarios. Since compiling CAD parameter sequences into explicit CAD models is\na non-differentiable process, the network training inherently lacks explicit\ngeometric supervision. To impose geometric validity constraints, we employ\ndirect preference optimization (DPO) to fine-tune our model with the automatic\ncode checker feedback on CAD sequence quality. Furthermore, we collected a\nreal-world dataset, comprised of multi-view images and corresponding CAD\ncommand sequence pairs, to evaluate our method. Experimental results\ndemonstrate that our approach can robustly handle real unconstrained CAD\nimages, and even generalize to unseen general objects.", "AI": {"task": "\u4ece\u65e0\u7ea6\u675f\u7684\u771f\u5b9e\u4e16\u754cCAD\u56fe\u50cf\u4e2d\u9006\u5411\u5de5\u7a0b\u751f\u6210\u53c2\u6570\u5316CAD\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4e14\u52b3\u52a8\u5bc6\u96c6\u76843D\u626b\u63cf\u548c\u540e\u5904\u7406\uff0c\u9700\u8981\u4e00\u79cd\u7528\u6237\u53cb\u597d\u4e14\u80fd\u4ece\u6613\u83b7\u53d6\u56fe\u50cf\u751f\u6210CAD\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCADCrafter\u6846\u67b6\uff0c\u5229\u7528\u5408\u6210\u65e0\u7eb9\u7406CAD\u6570\u636e\u8bad\u7ec3\uff0c\u901a\u8fc7\u51e0\u4f55\u7f16\u7801\u5668\u6355\u6349\u51e0\u4f55\u7279\u5f81\uff0c\u5e76\u91c7\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u786e\u4fdd\u51e0\u4f55\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u7a33\u5065\u5904\u7406\u771f\u5b9e\u65e0\u7ea6\u675fCAD\u56fe\u50cf\uff0c\u5e76\u53ef\u6cdb\u5316\u81f3\u672a\u89c1\u8fc7\u7684\u901a\u7528\u7269\u4f53\u3002", "conclusion": "CADCrafter\u4e3a\u4ece\u771f\u5b9e\u56fe\u50cf\u751f\u6210CAD\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2504.05262", "pdf": "https://arxiv.org/pdf/2504.05262", "abs": "https://arxiv.org/abs/2504.05262", "authors": ["Yang Yan", "Yu Lu", "Renjun Xu", "Zhenzhong Lan"], "title": "Do PhD-level LLMs Truly Grasp Elementary Addition? Probing Rule Learning vs. Memorization in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Despite high benchmark scores, Large Language Models (LLMs) often fail simple\nproblem, raising a critical question: Do LLMs learn mathematical principles or\nmerely memorize patterns? Rather than designing increasingly complex benchmarks\nlike recent works, we investigate this using elementary two-integer addition\n($0$ to $2^{64}$), probing two core properties: commutativity ($A+B=B+A$) and\ncompositional generalization (via isomorphic symbolic mappings, e.g., $7\n\\rightarrow y$). While state-of-the-art LLMs achieve 73.8-99.8\\% accuracy on\nnumerical addition, performance collapses to $\\leq$7.5\\% under symbolic\nmapping, indicating failure to generalize learned rules. Non-monotonic\nperformance scaling with digit count and frequent commutativity violations\n(over 1,700 cases of $A+B \\neq B+A$) further support this. Explicitly providing\naddition rules degrades performance by 81.2\\% on average, while\nself-explanation maintains baseline accuracy, suggesting LLM arithmetic\nprocessing is misaligned with human-defined principles. Our findings indicate\ncurrent LLMs rely on memory pattern over genuine rule learning, highlighting\narchitectural limitations and the need for new approaches to achieve true\nmathematical reasoning.", "AI": {"task": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u771f\u6b63\u5b66\u4e60\u6570\u5b66\u539f\u7406\uff0c\u8fd8\u662f\u4ec5\u8bb0\u5fc6\u6a21\u5f0f\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5f15\u53d1\u5bf9\u5176\u5b66\u4e60\u672c\u8d28\u7684\u8d28\u7591\u3002", "method": "\u901a\u8fc7\u57fa\u672c\u7684\u4e24\u6574\u6570\u52a0\u6cd5\uff080\u52302^64\u8303\u56f4\uff09\u6d4b\u8bd5\u4ea4\u6362\u5f8b\u548c\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u7b26\u53f7\u6620\u5c04\u3002", "result": "LLMs\u5728\u6570\u503c\u52a0\u6cd5\u4e0a\u8868\u73b0\u826f\u597d\uff0873.8-99.8%\u51c6\u786e\u7387\uff09\uff0c\u4f46\u5728\u7b26\u53f7\u6620\u5c04\u4e0b\u51c6\u786e\u7387\u9aa4\u964d\u81f3\u22647.5%\uff0c\u4e14\u5b58\u5728\u5927\u91cf\u4ea4\u6362\u5f8b\u8fdd\u53cd\u6848\u4f8b\u3002", "conclusion": "\u5f53\u524dLLMs\u4f9d\u8d56\u8bb0\u5fc6\u6a21\u5f0f\u800c\u975e\u771f\u6b63\u5b66\u4e60\u89c4\u5219\uff0c\u9700\u65b0\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u6570\u5b66\u63a8\u7406\u3002"}}
{"id": "2504.04756", "pdf": "https://arxiv.org/pdf/2504.04756", "abs": "https://arxiv.org/abs/2504.04756", "authors": ["Inhwan Bae", "Junoh Lee", "Hae-Gon Jeon"], "title": "Continuous Locomotive Crowd Behavior Generation", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Accepted at CVPR 2025. Project page:\n  https://ihbae.com/publication/crowdes/", "summary": "Modeling and reproducing crowd behaviors are important in various domains\nincluding psychology, robotics, transport engineering and virtual environments.\nConventional methods have focused on synthesizing momentary scenes, which have\ndifficulty in replicating the continuous nature of real-world crowds. In this\npaper, we introduce a novel method for automatically generating continuous,\nrealistic crowd trajectories with heterogeneous behaviors and interactions\namong individuals. We first design a crowd emitter model. To do this, we obtain\nspatial layouts from single input images, including a segmentation map,\nappearance map, population density map and population probability, prior to\ncrowd generation. The emitter then continually places individuals on the\ntimeline by assigning independent behavior characteristics such as agents'\ntype, pace, and start/end positions using diffusion models. Next, our crowd\nsimulator produces their long-term locomotions. To simulate diverse actions, it\ncan augment their behaviors based on a Markov chain. As a result, our overall\nframework populates the scenes with heterogeneous crowd behaviors by\nalternating between the proposed emitter and simulator. Note that all the\ncomponents in the proposed framework are user-controllable. Lastly, we propose\na benchmark protocol to evaluate the realism and quality of the generated\ncrowds in terms of the scene-level population dynamics and the individual-level\ntrajectory accuracy. We demonstrate that our approach effectively models\ndiverse crowd behavior patterns and generalizes well across different\ngeographical environments. Code is publicly available at\nhttps://github.com/InhwanBae/CrowdES .", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u8fde\u7eed\u3001\u771f\u5b9e\u7684\u7fa4\u4f53\u8f68\u8ff9\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u6a21\u62df\u5f02\u8d28\u884c\u4e3a\u548c\u4e2a\u4f53\u95f4\u7684\u4ea4\u4e92\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u590d\u73b0\u771f\u5b9e\u4e16\u754c\u4e2d\u7fa4\u4f53\u7684\u8fde\u7eed\u6027\u884c\u4e3a\uff0c\u800c\u6a21\u62df\u7fa4\u4f53\u884c\u4e3a\u5728\u5fc3\u7406\u5b66\u3001\u673a\u5668\u4eba\u5b66\u3001\u4ea4\u901a\u5de5\u7a0b\u548c\u865a\u62df\u73af\u5883\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7fa4\u4f53\u53d1\u5c04\u5668\u6a21\u578b\u548c\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u751f\u6210\u5f02\u8d28\u884c\u4e3a\u548c\u957f\u671f\u8fd0\u52a8\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6a21\u62df\u591a\u6837\u5316\u7684\u7fa4\u4f53\u884c\u4e3a\u6a21\u5f0f\uff0c\u5e76\u5728\u4e0d\u540c\u5730\u7406\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7fa4\u4f53\u884c\u4e3a\uff0c\u4e14\u6240\u6709\u7ec4\u4ef6\u5747\u53ef\u7531\u7528\u6237\u63a7\u5236\uff0c\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2504.05276", "pdf": "https://arxiv.org/pdf/2504.05276", "abs": "https://arxiv.org/abs/2504.05276", "authors": ["Yucheng Chu", "Peng He", "Hang Li", "Haoyu Han", "Kaiqi Yang", "Yu Xue", "Tingting Li", "Joseph Krajcik", "Jiliang Tang"], "title": "Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Short answer assessment is a vital component of science education, allowing\nevaluation of students' complex three-dimensional understanding. Large language\nmodels (LLMs) that possess human-like ability in linguistic tasks are\nincreasingly popular in assisting human graders to reduce their workload.\nHowever, LLMs' limitations in domain knowledge restrict their understanding in\ntask-specific requirements and hinder their ability to achieve satisfactory\nperformance. Retrieval-augmented generation (RAG) emerges as a promising\nsolution by enabling LLMs to access relevant domain-specific knowledge during\nassessment. In this work, we propose an adaptive RAG framework for automated\ngrading that dynamically retrieves and incorporates domain-specific knowledge\nbased on the question and student answer context. Our approach combines\nsemantic search and curated educational sources to retrieve valuable reference\nmaterials. Experimental results in a science education dataset demonstrate that\nour system achieves an improvement in grading accuracy compared to baseline LLM\napproaches. The findings suggest that RAG-enhanced grading systems can serve as\nreliable support with efficient performance gains.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u5206\uff0c\u4ee5\u52a8\u6001\u68c0\u7d22\u548c\u6574\u5408\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9886\u57df\u77e5\u8bc6\u4e0a\u7684\u5c40\u9650\u6027\u9650\u5236\u4e86\u5176\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u7406\u89e3\u80fd\u529b\uff0c\u65e0\u6cd5\u8fbe\u5230\u6ee1\u610f\u7684\u8bc4\u5206\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u8bed\u4e49\u641c\u7d22\u548c\u7cbe\u9009\u6559\u80b2\u8d44\u6e90\uff0c\u52a8\u6001\u68c0\u7d22\u6709\u4ef7\u503c\u7684\u53c2\u8003\u6750\u6599\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u8bc4\u5206\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728\u79d1\u5b66\u6559\u80b2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u76f8\u6bd4\u57fa\u7ebfLLM\u65b9\u6cd5\u5728\u8bc4\u5206\u51c6\u786e\u6027\u4e0a\u6709\u6240\u63d0\u5347\u3002", "conclusion": "RAG\u589e\u5f3a\u7684\u8bc4\u5206\u7cfb\u7edf\u53ef\u4ee5\u4f5c\u4e3a\u53ef\u9760\u7684\u652f\u6301\u5de5\u5177\uff0c\u5e76\u5e26\u6765\u9ad8\u6548\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2504.04764", "pdf": "https://arxiv.org/pdf/2504.04764", "abs": "https://arxiv.org/abs/2504.04764", "authors": ["Shyam Sundhar", "Riya Sharma", "Priyansh Maheshwari", "Suvidha Rupesh Kumar", "T. Sunil Kumar"], "title": "Enhancing Leaf Disease Classification Using GAT-GCN Hybrid Model", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Agriculture plays a critical role in the global economy, providing\nlivelihoods and ensuring food security for billions. As innovative agricultural\npractices become more widespread, the risk of crop diseases has increased,\nhighlighting the urgent need for efficient, low-intervention disease\nidentification methods. This research presents a hybrid model combining Graph\nAttention Networks (GATs) and Graph Convolution Networks (GCNs) for leaf\ndisease classification. GCNs have been widely used for learning from\ngraph-structured data, and GATs enhance this by incorporating attention\nmechanisms to focus on the most important neighbors. The methodology integrates\nsuperpixel segmentation for efficient feature extraction, partitioning images\ninto meaningful, homogeneous regions that better capture localized features.\nThe authors have employed an edge augmentation technique to enhance the\nrobustness of the model. The edge augmentation technique has introduced a\nsignificant degree of generalization in the detection capabilities of the\nmodel. To further optimize training, weight initialization techniques are\napplied. The hybrid model is evaluated against the individual performance of\nthe GCN and GAT models and the hybrid model achieved a precision of 0.9822,\nrecall of 0.9818, and F1-score of 0.9818 in apple leaf disease classification,\na precision of 0.9746, recall of 0.9744, and F1-score of 0.9743 in potato leaf\ndisease classification, and a precision of 0.8801, recall of 0.8801, and\nF1-score of 0.8799 in sugarcane leaf disease classification. These results\ndemonstrate the robustness and performance of the model, suggesting its\npotential to support sustainable agricultural practices through precise and\neffective disease detection. This work is a small step towards reducing the\nloss of crops and hence supporting sustainable goals of zero hunger and life on\nland.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GATs\uff09\u548c\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCNs\uff09\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u53f6\u7247\u75c5\u5bb3\u5206\u7c7b\u3002", "motivation": "\u519c\u4e1a\u75c5\u5bb3\u98ce\u9669\u589e\u52a0\uff0c\u4e9f\u9700\u9ad8\u6548\u3001\u4f4e\u5e72\u9884\u7684\u75c5\u5bb3\u8bc6\u522b\u65b9\u6cd5\u4ee5\u652f\u6301\u53ef\u6301\u7eed\u519c\u4e1a\u3002", "method": "\u91c7\u7528\u8d85\u50cf\u7d20\u5206\u5272\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u7ed3\u5408GCNs\u548cGATs\uff0c\u5e76\u5e94\u7528\u8fb9\u7f18\u589e\u5f3a\u6280\u672f\u548c\u6743\u91cd\u521d\u59cb\u5316\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5728\u82f9\u679c\u3001\u9a6c\u94c3\u85af\u548c\u7518\u8517\u53f6\u7247\u75c5\u5bb3\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0cF1\u5206\u6570\u5206\u522b\u4e3a0.9818\u30010.9743\u548c0.8799\u3002", "conclusion": "\u8be5\u6a21\u578b\u5177\u6709\u9c81\u68d2\u6027\u548c\u9ad8\u6027\u80fd\uff0c\u6709\u671b\u901a\u8fc7\u7cbe\u51c6\u75c5\u5bb3\u68c0\u6d4b\u652f\u6301\u53ef\u6301\u7eed\u519c\u4e1a\u76ee\u6807\u3002"}}
{"id": "2504.05294", "pdf": "https://arxiv.org/pdf/2504.05294", "abs": "https://arxiv.org/abs/2504.05294", "authors": ["Pedro Ferreira", "Wilker Aziz", "Ivan Titov"], "title": "Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations", "categories": ["cs.CL"], "comment": "22 pages, 10 figures, 5 tables", "summary": "Chain-of-thought explanations are widely used to inspect the decision process\nof large language models (LLMs) and to evaluate the trustworthiness of model\noutputs, making them important for effective collaboration between LLMs and\nhumans. We demonstrate that preference optimization - a key step in the\nalignment phase - can inadvertently reduce the faithfulness of these\nexplanations. This occurs because the reward model (RM), which guides\nalignment, is tasked with optimizing both the expected quality of the response\nand the appropriateness of the explanations (e.g., minimizing bias or adhering\nto safety standards), creating potential conflicts. The RM lacks a mechanism to\nassess the consistency between the model's internal decision process and the\ngenerated explanation. Consequently, the LLM may engage in \"reward hacking\" by\nproducing a final response that scores highly while giving an explanation\ntailored to maximize reward rather than accurately reflecting its reasoning. To\naddress this issue, we propose enriching the RM's input with a causal\nattribution of the prediction, allowing the RM to detect discrepancies between\nthe generated self-explanation and the model's decision process. In controlled\nsettings, we show that this approach reduces the tendency of the LLM to\ngenerate misleading explanations.", "AI": {"task": "\u7814\u7a76\u504f\u597d\u4f18\u5316\u5982\u4f55\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u3002", "motivation": "\u504f\u597d\u4f18\u5316\u53ef\u80fd\u5bfc\u81f4LLM\u751f\u6210\u4e0d\u5fe0\u5b9e\u7684\u89e3\u91ca\uff0c\u5f71\u54cd\u6a21\u578b\u4e0e\u4eba\u7c7b\u534f\u4f5c\u7684\u4fe1\u4efb\u5ea6\u3002", "method": "\u63d0\u51fa\u5728\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u8f93\u5165\u4e2d\u52a0\u5165\u56e0\u679c\u5f52\u56e0\uff0c\u4ee5\u68c0\u6d4b\u89e3\u91ca\u4e0e\u51b3\u7b56\u8fc7\u7a0b\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "result": "\u5728\u53d7\u63a7\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86LLM\u751f\u6210\u8bef\u5bfc\u6027\u89e3\u91ca\u7684\u503e\u5411\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3aRM\u7684\u8f93\u5165\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347LLM\u751f\u6210\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u3002"}}
{"id": "2504.04780", "pdf": "https://arxiv.org/pdf/2504.04780", "abs": "https://arxiv.org/abs/2504.04780", "authors": ["Chenxi Zhao", "Daochang Wang", "Siqian Zhang", "Gangyao Kuang"], "title": "Bottom-Up Scattering Information Perception Network for SAR target recognition", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning methods based synthetic aperture radar (SAR) image target\nrecognition tasks have been widely studied currently. The existing deep methods\nare insufficient to perceive and mine the scattering information of SAR images,\nresulting in performance bottlenecks and poor robustness of the algorithms. To\nthis end, this paper proposes a novel bottom-up scattering information\nperception network for more interpretable target recognition by constructing\nthe proprietary interpretation network for SAR images. Firstly, the localized\nscattering perceptron is proposed to replace the backbone feature extractor\nbased on CNN networks to deeply mine the underlying scattering information of\nthe target. Then, an unsupervised scattering part feature extraction model is\nproposed to robustly characterize the target scattering part information and\nprovide fine-grained target representation. Finally, by aggregating the\nknowledge of target parts to form the complete target description, the\ninterpretability and discriminative ability of the model is improved. We\nperform experiments on the FAST-Vehicle dataset and the SAR-ACD dataset to\nvalidate the performance of the proposed method.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u5e95\u5411\u4e0a\u6563\u5c04\u4fe1\u606f\u611f\u77e5\u7f51\u7edc\uff0c\u7528\u4e8e\u66f4\u53ef\u89e3\u91ca\u7684SAR\u56fe\u50cf\u76ee\u6807\u8bc6\u522b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5bf9SAR\u56fe\u50cf\u7684\u6563\u5c04\u4fe1\u606f\u611f\u77e5\u548c\u6316\u6398\u4e0d\u8db3\uff0c\u5bfc\u81f4\u7b97\u6cd5\u6027\u80fd\u74f6\u9888\u548c\u9c81\u68d2\u6027\u5dee\u3002", "method": "\u6784\u5efaSAR\u56fe\u50cf\u7684\u4e13\u6709\u89e3\u91ca\u7f51\u7edc\uff0c\u5305\u62ec\u5c40\u90e8\u6563\u5c04\u611f\u77e5\u5668\u66ff\u4ee3CNN\u4e3b\u5e72\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u4ee5\u53ca\u65e0\u76d1\u7763\u6563\u5c04\u90e8\u5206\u7279\u5f81\u63d0\u53d6\u6a21\u578b\u3002", "result": "\u5728FAST-Vehicle\u548cSAR-ACD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u805a\u5408\u76ee\u6807\u90e8\u5206\u77e5\u8bc6\u5f62\u6210\u5b8c\u6574\u76ee\u6807\u63cf\u8ff0\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5224\u522b\u80fd\u529b\u3002"}}
{"id": "2504.03714", "pdf": "https://arxiv.org/pdf/2504.03714", "abs": "https://arxiv.org/abs/2504.03714", "authors": ["Runpeng Dai", "Run Yang", "Fan Zhou", "Hongtu Zhu"], "title": "Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have become\nessential to general artificial intelligence, exhibiting remarkable\ncapabilities in task understanding and problem-solving. However, the real-world\nreliability of these models critically depends on their stability, which\nremains an underexplored area. Despite their widespread use, rigorous studies\nexamining the stability of LLMs under various perturbations are still lacking.\nIn this paper, we address this gap by proposing a novel stability measure for\nLLMs, inspired by statistical methods rooted in information geometry. Our\nmeasure possesses desirable invariance properties, making it well-suited for\nanalyzing model sensitivity to both parameter and input perturbations. To\nassess the effectiveness of our approach, we conduct extensive experiments on\nmodels ranging in size from 1.5B to 13B parameters. Our results demonstrate the\nutility of our measure in identifying salient parameters and detecting\nvulnerable regions in input images or critical dimensions in token embeddings.\nFurthermore, leveraging our stability framework, we enhance model robustness\nduring model merging, leading to improved performance.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u7a33\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e0d\u540c\u6270\u52a8\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u548cVLMs\u5728\u4efb\u52a1\u7406\u89e3\u548c\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u53ef\u9760\u6027\u4f9d\u8d56\u4e8e\u7a33\u5b9a\u6027\uff0c\u800c\u8fd9\u4e00\u9886\u57df\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u53d7\u4fe1\u606f\u51e0\u4f55\u4e2d\u7684\u7edf\u8ba1\u65b9\u6cd5\u542f\u53d1\uff0c\u63d0\u51fa\u4e00\u79cd\u5177\u6709\u4e0d\u53d8\u6027\u7279\u6027\u7684\u7a33\u5b9a\u6027\u5ea6\u91cf\uff0c\u7528\u4e8e\u5206\u6790\u6a21\u578b\u5bf9\u53c2\u6570\u548c\u8f93\u5165\u6270\u52a8\u7684\u654f\u611f\u6027\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u5ea6\u91cf\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u8bc6\u522b\u5173\u952e\u53c2\u6570\u548c\u8f93\u5165\u4e2d\u7684\u8106\u5f31\u533a\u57df\uff0c\u5e76\u5728\u6a21\u578b\u5408\u5e76\u4e2d\u63d0\u5347\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7a33\u5b9a\u6027\u6846\u67b6\u4e0d\u4ec5\u80fd\u591f\u8bc4\u4f30LLMs\u7684\u7a33\u5b9a\u6027\uff0c\u8fd8\u80fd\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2504.04781", "pdf": "https://arxiv.org/pdf/2504.04781", "abs": "https://arxiv.org/abs/2504.04781", "authors": ["Chaoyi Wang", "Baoqing Li", "Xinhan Di"], "title": "OCC-MLLM-CoT-Alpha: Towards Multi-stage Occlusion Recognition Based on Large Language Models via 3D-Aware Supervision and Chain-of-Thoughts Guidance", "categories": ["cs.CV", "I.2.10; I.4.8"], "comment": "This work has been accepted to the Multimodal Algorithmic Reasoning\n  (MAR) Workshop at CVPR 2025", "summary": "Comprehending occluded objects are not well studied in existing large-scale\nvisual-language multi-modal models. Current state-of-the-art multi-modal large\nmodels struggles to provide satisfactory results in understanding occluded\nobjects through universal visual encoders and supervised learning strategies.\nTherefore, we propose OCC-MLLM-CoT-Alpha, a multi-modal large vision language\nframework that integrates 3D-aware supervision and Chain-of-Thoughts guidance.\nParticularly, (1) we build a multi-modal large vision-language model framework\nwhich is consisted of a large multi-modal vision-language model and a 3D\nreconstruction expert model. (2) the corresponding multi-modal\nChain-of-Thoughts is learned through a combination of supervised and\nreinforcement training strategies, allowing the multi-modal vision-language\nmodel to enhance the recognition ability with learned multi-modal\nchain-of-thoughts guidance. (3) A large-scale multi-modal chain-of-thoughts\nreasoning dataset, consisting of $110k$ samples of occluded objects held in\nhand, is built. In the evaluation, the proposed methods demonstrate decision\nscore improvement of 15.75%,15.30%,16.98%,14.62%, and 4.42%,3.63%,6.94%,10.70%\nfor two settings of a variety of state-of-the-art models.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u591a\u6a21\u6001\u5927\u89c6\u89c9\u8bed\u8a00\u6846\u67b6OCC-MLLM-CoT-Alpha\uff0c\u7528\u4e8e\u7406\u89e3\u906e\u6321\u7269\u4f53\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00\u591a\u6a21\u6001\u6a21\u578b\u5728\u7406\u89e3\u906e\u6321\u7269\u4f53\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u7ed3\u54083D\u611f\u77e5\u76d1\u7763\u548c\u601d\u7ef4\u94fe\uff08Chain-of-Thoughts\uff09\u5f15\u5bfc\uff0c\u6784\u5efa\u591a\u6a21\u6001\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u5e76\u91c7\u7528\u76d1\u7763\u4e0e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u8bad\u7ec3\u3002", "result": "\u5728\u8bc4\u4f30\u4e2d\uff0c\u65b9\u6cd5\u5728\u591a\u79cd\u5148\u8fdb\u6a21\u578b\u7684\u4e24\u4e2a\u8bbe\u7f6e\u4e0b\u5206\u522b\u63d0\u5347\u4e8615.75%\u81f316.98%\u548c4.42%\u81f310.70%\u7684\u51b3\u7b56\u5206\u6570\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u906e\u6321\u7269\u4f53\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2504.03724", "pdf": "https://arxiv.org/pdf/2504.03724", "abs": "https://arxiv.org/abs/2504.03724", "authors": ["Zhiqiang Wang", "Pengbin Feng", "Yanbin Lin", "Shuzhang Cai", "Zongao Bian", "Jinghua Yan", "Xingquan Zhu"], "title": "CrowdVLM-R1: Expanding R1 Ability to Vision Language Model for Crowd Counting using Fuzzy Group Relative Policy Reward", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 6 figures and 4 tables", "summary": "We propose Fuzzy Group Relative Policy Reward (FGRPR), a novel framework that\nintegrates Group Relative Policy Optimization (GRPO) with a fuzzy reward\nfunction to enhance learning efficiency. Unlike the conventional binary 0/1\naccuracy reward, our fuzzy reward model provides nuanced incentives,\nencouraging more precise outputs. Experimental results demonstrate that GRPO\nwith a standard 0/1 accuracy reward underperforms compared to supervised\nfine-tuning (SFT). In contrast, FGRPR, applied to Qwen2.5-VL(3B and 7B),\nsurpasses all baseline models, including GPT4o, LLaMA2(90B), and SFT, across\nfive in-domain datasets. On an out-of-domain dataset, FGRPR achieves\nperformance comparable to SFT but excels when target values are larger, as its\nfuzzy reward function assigns higher rewards to closer approximations. This\napproach is broadly applicable to tasks where the precision of the answer is\ncritical. Code and data: https://github.com/yeyimilk/CrowdVLM-R1", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aFuzzy Group Relative Policy Reward (FGRPR)\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408Group Relative Policy Optimization (GRPO)\u4e0e\u6a21\u7cca\u5956\u52b1\u51fd\u6570\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u4e8c\u51430/1\u51c6\u786e\u5ea6\u5956\u52b1\u7f3a\u4e4f\u5bf9\u7cbe\u786e\u8f93\u51fa\u7684\u6fc0\u52b1\uff0c\u800c\u6a21\u7cca\u5956\u52b1\u6a21\u578b\u80fd\u63d0\u4f9b\u66f4\u7ec6\u81f4\u7684\u6fc0\u52b1\u3002", "method": "\u5c06GRPO\u4e0e\u6a21\u7cca\u5956\u52b1\u51fd\u6570\u7ed3\u5408\uff0c\u5f62\u6210FGRPR\u6846\u67b6\uff0c\u5e94\u7528\u4e8eQwen2.5-VL\u6a21\u578b\u3002", "result": "FGRPR\u5728\u4e94\u4e2a\u9886\u57df\u5185\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\u6a21\u578b\uff08\u5305\u62ecGPT4o\u3001LLaMA2\u548cSFT\uff09\uff0c\u5728\u9886\u57df\u5916\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0eSFT\u76f8\u5f53\uff0c\u4f46\u5bf9\u66f4\u63a5\u8fd1\u76ee\u6807\u503c\u7684\u60c5\u51b5\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "FGRPR\u9002\u7528\u4e8e\u9700\u8981\u7b54\u6848\u7cbe\u786e\u6027\u7684\u4efb\u52a1\uff0c\u5176\u6a21\u7cca\u5956\u52b1\u51fd\u6570\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2504.04784", "pdf": "https://arxiv.org/pdf/2504.04784", "abs": "https://arxiv.org/abs/2504.04784", "authors": ["Hui Liu", "Bin Zou", "Suiyun Zhang", "Kecheng Chen", "Rui Liu", "Haoliang Li"], "title": "Disentangling Instruction Influence in Diffusion Transformers for Parallel Multi-Instruction-Guided Image Editing", "categories": ["cs.CV"], "comment": "14 pages, 8 figures", "summary": "Instruction-guided image editing enables users to specify modifications using\nnatural language, offering more flexibility and control. Among existing\nframeworks, Diffusion Transformers (DiTs) outperform U-Net-based diffusion\nmodels in scalability and performance. However, while real-world scenarios\noften require concurrent execution of multiple instructions, step-by-step\nediting suffers from accumulated errors and degraded quality, and integrating\nmultiple instructions with a single prompt usually results in incomplete edits\ndue to instruction conflicts. We propose Instruction Influence Disentanglement\n(IID), a novel framework enabling parallel execution of multiple instructions\nin a single denoising process, designed for DiT-based models. By analyzing\nself-attention mechanisms in DiTs, we identify distinctive attention patterns\nin multi-instruction settings and derive instruction-specific attention masks\nto disentangle each instruction's influence. These masks guide the editing\nprocess to ensure localized modifications while preserving consistency in\nnon-edited regions. Extensive experiments on open-source and custom datasets\ndemonstrate that IID reduces diffusion steps while improving fidelity and\ninstruction completion compared to existing baselines. The codes will be\npublicly released upon the acceptance of the paper.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aInstruction Influence Disentanglement (IID)\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5355\u4e2a\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5e76\u884c\u6267\u884c\u591a\u4e2a\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6307\u4ee4\u5e76\u884c\u6267\u884c\u65f6\u5b58\u5728\u7d2f\u79ef\u9519\u8bef\u3001\u8d28\u91cf\u4e0b\u964d\u6216\u6307\u4ee4\u51b2\u7a81\u5bfc\u81f4\u7f16\u8f91\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790DiT\u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8bc6\u522b\u591a\u6307\u4ee4\u8bbe\u7f6e\u4e0b\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u5e76\u751f\u6210\u6307\u4ee4\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u63a9\u7801\u4ee5\u5206\u79bb\u6bcf\u4e2a\u6307\u4ee4\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIID\u5728\u51cf\u5c11\u6269\u6563\u6b65\u9aa4\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u4fdd\u771f\u5ea6\u548c\u6307\u4ee4\u5b8c\u6210\u5ea6\u3002", "conclusion": "IID\u6846\u67b6\u5728\u591a\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u63a5\u53d7\u540e\u516c\u5f00\u3002"}}
{"id": "2504.03735", "pdf": "https://arxiv.org/pdf/2504.03735", "abs": "https://arxiv.org/abs/2504.03735", "authors": ["Erfan Shayegani", "G M Shahariar", "Sara Abdali", "Lei Yu", "Nael Abu-Ghazaleh", "Yue Dong"], "title": "Misaligned Roles, Misplaced Images: Structural Input Perturbations Expose Multimodal Alignment Blind Spots", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Multimodal Language Models (MMLMs) typically undergo post-training alignment\nto prevent harmful content generation. However, these alignment stages focus\nprimarily on the assistant role, leaving the user role unaligned, and stick to\na fixed input prompt structure of special tokens, leaving the model vulnerable\nwhen inputs deviate from these expectations. We introduce Role-Modality Attacks\n(RMA), a novel class of adversarial attacks that exploit role confusion between\nthe user and assistant and alter the position of the image token to elicit\nharmful outputs. Unlike existing attacks that modify query content, RMAs\nmanipulate the input structure without altering the query itself. We\nsystematically evaluate these attacks across multiple Vision Language Models\n(VLMs) on eight distinct settings, showing that they can be composed to create\nstronger adversarial prompts, as also evidenced by their increased projection\nin the negative refusal direction in the residual stream, a property observed\nin prior successful attacks. Finally, for mitigation, we propose an adversarial\ntraining approach that makes the model robust against input prompt\nperturbations. By training the model on a range of harmful and benign prompts\nall perturbed with different RMA settings, it loses its sensitivity to Role\nConfusion and Modality Manipulation attacks and is trained to only pay\nattention to the content of the query in the input prompt structure,\neffectively reducing Attack Success Rate (ASR) while preserving the model's\ngeneral utility.", "AI": {"task": "\u7814\u7a76\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08MMLMs\uff09\u4e2d\u7684\u89d2\u8272-\u6a21\u6001\u653b\u51fb\uff08RMA\uff09\u53ca\u5176\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u52a9\u624b\u89d2\u8272\uff0c\u5ffd\u7565\u7528\u6237\u89d2\u8272\uff0c\u4e14\u8f93\u5165\u63d0\u793a\u7ed3\u6784\u56fa\u5b9a\uff0c\u5bfc\u81f4\u6a21\u578b\u6613\u53d7\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u89d2\u8272-\u6a21\u6001\u653b\u51fb\uff08RMA\uff09\uff0c\u901a\u8fc7\u89d2\u8272\u6df7\u6dc6\u548c\u56fe\u50cf\u4ee4\u724c\u4f4d\u7f6e\u4fee\u6539\u5f15\u53d1\u6709\u5bb3\u8f93\u51fa\uff0c\u5e76\u8bbe\u8ba1\u5bf9\u6297\u8bad\u7ec3\u9632\u5fa1\u65b9\u6cd5\u3002", "result": "RMA\u653b\u51fb\u5728\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u6709\u6548\uff0c\u5bf9\u6297\u8bad\u7ec3\u80fd\u663e\u8457\u964d\u4f4e\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u5e76\u4fdd\u6301\u6a21\u578b\u901a\u7528\u6027\u3002", "conclusion": "RMA\u653b\u51fb\u63ed\u793a\u4e86MMLMs\u7684\u8106\u5f31\u6027\uff0c\u5bf9\u6297\u8bad\u7ec3\u662f\u6709\u6548\u7684\u9632\u5fa1\u624b\u6bb5\u3002"}}
{"id": "2504.04787", "pdf": "https://arxiv.org/pdf/2504.04787", "abs": "https://arxiv.org/abs/2504.04787", "authors": ["Mengxuan Wu", "Zekai Li", "Zhiyuan Liang", "Moyang Li", "Xuanlei Zhao", "Samir Khaki", "Zheng Zhu", "Xiaojiang Peng", "Konstantinos N. Plataniotis", "Kai Wang", "Wangbo Zhao", "Yang You"], "title": "Dynamic Vision Mamba", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mamba-based vision models have gained extensive attention as a result of\nbeing computationally more efficient than attention-based models. However,\nspatial redundancy still exists in these models, represented by token and block\nredundancy. For token redundancy, we analytically find that early token pruning\nmethods will result in inconsistency between training and inference or\nintroduce extra computation for inference. Therefore, we customize token\npruning to fit the Mamba structure by rearranging the pruned sequence before\nfeeding it into the next Mamba block. For block redundancy, we allow each image\nto select SSM blocks dynamically based on an empirical observation that the\ninference speed of Mamba-based vision models is largely affected by the number\nof SSM blocks. Our proposed method, Dynamic Vision Mamba (DyVM), effectively\nreduces FLOPs with minor performance drops. We achieve a reduction of 35.2\\%\nFLOPs with only a loss of accuracy of 1.7\\% on Vim-S. It also generalizes well\nacross different Mamba vision model architectures and different vision tasks.\nOur code will be made public.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u89c6\u89c9Mamba\uff08DyVM\uff09\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11Mamba\u89c6\u89c9\u6a21\u578b\u4e2d\u7684\u7a7a\u95f4\u5197\u4f59\uff08\u5305\u62ectoken\u548cblock\u5197\u4f59\uff09\u3002", "motivation": "Mamba\u89c6\u89c9\u6a21\u578b\u867d\u7136\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4f46\u4ecd\u5b58\u5728\u7a7a\u95f4\u5197\u4f59\u95ee\u9898\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u901a\u8fc7\u5b9a\u5236\u5316token\u526a\u679d\u548c\u52a8\u6001\u9009\u62e9SSM\u5757\u6765\u51cf\u5c11\u5197\u4f59\u3002", "result": "\u5728Vim-S\u4e0a\u51cf\u5c11\u4e8635.2%\u7684FLOPs\uff0c\u4ec5\u635f\u59311.7%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "DyVM\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u4e14\u9002\u7528\u4e8e\u591a\u79cdMamba\u89c6\u89c9\u6a21\u578b\u548c\u4efb\u52a1\u3002"}}
{"id": "2504.03748", "pdf": "https://arxiv.org/pdf/2504.03748", "abs": "https://arxiv.org/abs/2504.03748", "authors": ["Kaiyuan Hou", "Minghui Zhao", "Lilin Xu", "Yuang Fan", "Xiaofan Jiang"], "title": "TDBench: Benchmarking Vision-Language Models in Understanding Top-Down Images", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid emergence of Vision-Language Models (VLMs) has significantly\nadvanced multimodal understanding, enabling applications in scene comprehension\nand visual reasoning. While these models have been primarily evaluated and\ndeveloped for front-view image understanding, their capabilities in\ninterpreting top-down images have received limited attention, partly due to the\nscarcity of diverse top-down datasets and the challenges in collecting such\ndata. In contrast, top-down vision provides explicit spatial overviews and\nimproved contextual understanding of scenes, making it particularly valuable\nfor tasks like autonomous navigation, aerial imaging, and spatial planning. In\nthis work, we address this gap by introducing TDBench, a comprehensive\nbenchmark for VLMs in top-down image understanding. TDBench is constructed from\npublic top-down view datasets and high-quality simulated images, including\ndiverse real-world and synthetic scenarios. TDBench consists of visual\nquestion-answer pairs across ten evaluation dimensions of image understanding.\nMoreover, we conduct four case studies that commonly happen in real-world\nscenarios but are less explored. By revealing the strengths and limitations of\nexisting VLM through evaluation results, we hope TDBench to provide insights\nfor motivating future research. Project homepage:\nhttps://github.com/Columbia-ICSL/TDBench", "AI": {"task": "Introduce TDBench, a benchmark for evaluating Vision-Language Models (VLMs) in top-down image understanding.", "motivation": "Address the gap in evaluating VLMs for top-down images, which are valuable for tasks like autonomous navigation and spatial planning, but lack diverse datasets.", "method": "Construct TDBench using public top-down datasets and simulated images, including diverse real-world and synthetic scenarios, with visual question-answer pairs across ten evaluation dimensions.", "result": "TDBench reveals the strengths and limitations of existing VLMs through evaluation results and case studies.", "conclusion": "TDBench provides insights to motivate future research in top-down image understanding for VLMs."}}
{"id": "2504.04801", "pdf": "https://arxiv.org/pdf/2504.04801", "abs": "https://arxiv.org/abs/2504.04801", "authors": ["Jinhong Wang", "Shuo Tong", "Jian liu", "Dongqi Tang", "Weiqiang Wang", "Wentong Li", "Hongxia Xu", "Danny Chen", "Jintai Chen", "Jian Wu"], "title": "OrderChain: A General Prompting Paradigm to Improve Ordinal Understanding Ability of MLLM", "categories": ["cs.CV"], "comment": null, "summary": "Despite the remarkable progress of multimodal large language models (MLLMs),\nthey continue to face challenges in achieving competitive performance on\nordinal regression (OR; a.k.a. ordinal classification). To address this issue,\nthis paper presents OrderChain, a novel and general prompting paradigm that\nimproves the ordinal understanding ability of MLLMs by specificity and\ncommonality modeling. Specifically, our OrderChain consists of a set of\ntask-aware prompts to facilitate the specificity modeling of diverse OR tasks\nand a new range optimization Chain-of-Thought (RO-CoT), which learns a\ncommonality way of thinking about OR tasks by uniformly decomposing them into\nmultiple small-range optimization subtasks. Further, we propose a category\nrecursive division (CRD) method to generate instruction candidate category\nprompts to support RO-CoT automatic optimization. Comprehensive experiments\nshow that a Large Language and Vision Assistant (LLaVA) model with our\nOrderChain improves baseline LLaVA significantly on diverse OR datasets, e.g.,\nfrom 47.5% to 93.2% accuracy on the Adience dataset for age estimation, and\nfrom 30.0% to 85.7% accuracy on the Diabetic Retinopathy dataset. Notably,\nLLaVA with our OrderChain also remarkably outperforms state-of-the-art methods\nby 27% on accuracy and 0.24 on MAE on the Adience dataset. To our best\nknowledge, our OrderChain is the first work that augments MLLMs for OR tasks,\nand the effectiveness is witnessed across a spectrum of OR datasets.", "AI": {"task": "\u901a\u8fc7OrderChain\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5e8f\u6570\u56de\u5f52\uff08OR\uff09\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1MLLMs\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5e8f\u6570\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4ecd\u6709\u5f85\u63d0\u5347\u3002", "method": "\u63d0\u51faOrderChain\uff0c\u5305\u62ec\u4efb\u52a1\u611f\u77e5\u63d0\u793a\u548c\u8303\u56f4\u4f18\u5316\u601d\u7ef4\u94fe\uff08RO-CoT\uff09\uff0c\u5e76\u901a\u8fc7\u7c7b\u522b\u9012\u5f52\u5212\u5206\uff08CRD\uff09\u751f\u6210\u6307\u4ee4\u5019\u9009\u7c7b\u522b\u63d0\u793a\u3002", "result": "\u5728\u591a\u4e2aOR\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f8b\u5982Adience\u6570\u636e\u96c6\u51c6\u786e\u7387\u4ece47.5%\u63d0\u5347\u81f393.2%\uff0cDiabetic Retinopathy\u6570\u636e\u96c6\u4ece30.0%\u63d0\u5347\u81f385.7%\u3002", "conclusion": "OrderChain\u662f\u9996\u4e2a\u589e\u5f3aMLLMs\u5728OR\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u5de5\u4f5c\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2504.03775", "pdf": "https://arxiv.org/pdf/2504.03775", "abs": "https://arxiv.org/abs/2504.03775", "authors": ["Weiqing Li", "Guochao Jiang", "Xiangyong Ding", "Zhangcheng Tao", "Chuzhan Hao", "Chenfeng Xu", "Yuewei Zhang", "Hao Wang"], "title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache Transfer and Load-Aware Scheduling", "categories": ["cs.DC", "cs.AI", "cs.CL"], "comment": null, "summary": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs.", "AI": {"task": "\u63d0\u51faFlowKV\uff0c\u4e00\u79cd\u65b0\u578b\u7684\u89e3\u8026\u63a8\u7406\u6846\u67b6\uff0c\u4f18\u5316KV\u7f13\u5b58\u4f20\u8f93\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u89e3\u8026\u63a8\u7406\u6846\u67b6\u4e2dKV\u7f13\u5b58\u4f20\u8f93\u5ef6\u8fdf\u9ad8\uff0c\u8ba1\u7b97\u8282\u70b9\u89d2\u8272\u56fa\u5b9a\u5bfc\u81f4\u8ba1\u7b97\u4e0d\u5e73\u8861\u3002", "method": "\u901a\u8fc7\u4f18\u5316KV\u7f13\u5b58\u4f20\u8f93\u3001\u5f15\u5165\u8d1f\u8f7d\u611f\u77e5\u8c03\u5ea6\u5668\u548c\u7075\u6d3b\u7684PD\u8282\u70b9\u5206\u914d\u3002", "result": "\u5c06KV\u7f13\u5b58\u5e73\u5747\u4f20\u8f93\u5ef6\u8fdf\u964d\u4f4e96%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534715.2%-48.9%\u3002", "conclusion": "FlowKV\u663e\u8457\u4f18\u5316\u4e86\u89e3\u8026\u63a8\u7406\u7684\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2504.04804", "pdf": "https://arxiv.org/pdf/2504.04804", "abs": "https://arxiv.org/abs/2504.04804", "authors": ["Yuanpei Liu", "Kai Han"], "title": "DebGCD: Debiased Learning with Distribution Guidance for Generalized Category Discovery", "categories": ["cs.CV"], "comment": "Accepted as a conference paper at ICLR 2025", "summary": "In this paper, we tackle the problem of Generalized Category Discovery (GCD).\nGiven a dataset containing both labelled and unlabelled images, the objective\nis to categorize all images in the unlabelled subset, irrespective of whether\nthey are from known or unknown classes. In GCD, an inherent label bias exists\nbetween known and unknown classes due to the lack of ground-truth labels for\nthe latter. State-of-the-art methods in GCD leverage parametric classifiers\ntrained through self-distillation with soft labels, leaving the bias issue\nunattended. Besides, they treat all unlabelled samples uniformly, neglecting\nvariations in certainty levels and resulting in suboptimal learning. Moreover,\nthe explicit identification of semantic distribution shifts between known and\nunknown classes, a vital aspect for effective GCD, has been neglected. To\naddress these challenges, we introduce DebGCD, a \\underline{Deb}iased learning\nwith distribution guidance framework for \\underline{GCD}. Initially, DebGCD\nco-trains an auxiliary debiased classifier in the same feature space as the GCD\nclassifier, progressively enhancing the GCD features. Moreover, we introduce a\nsemantic distribution detector in a separate feature space to implicitly boost\nthe learning efficacy of GCD. Additionally, we employ a curriculum learning\nstrategy based on semantic distribution certainty to steer the debiased\nlearning at an optimized pace. Thorough evaluations on GCD benchmarks\ndemonstrate the consistent state-of-the-art performance of our framework,\nhighlighting its superiority. Project page: https://visual-ai.github.io/debgcd/", "AI": {"task": "\u89e3\u51b3\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\uff08GCD\uff09\u95ee\u9898\uff0c\u65e8\u5728\u5bf9\u672a\u6807\u8bb0\u5b50\u96c6\u4e2d\u7684\u6240\u6709\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\uff0c\u65e0\u8bba\u5b83\u4eec\u6765\u81ea\u5df2\u77e5\u8fd8\u662f\u672a\u77e5\u7c7b\u522b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728GCD\u4e2d\u5b58\u5728\u6807\u7b7e\u504f\u5dee\u95ee\u9898\uff0c\u4e14\u5ffd\u7565\u4e86\u672a\u6807\u8bb0\u6837\u672c\u7684\u786e\u5b9a\u6027\u5dee\u5f02\u548c\u8bed\u4e49\u5206\u5e03\u53d8\u5316\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faDebGCD\u6846\u67b6\uff0c\u5305\u62ec\u5171\u540c\u8bad\u7ec3\u53bb\u504f\u5206\u7c7b\u5668\u3001\u5f15\u5165\u8bed\u4e49\u5206\u5e03\u68c0\u6d4b\u5668\u4ee5\u53ca\u57fa\u4e8e\u786e\u5b9a\u6027\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728GCD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DebGCD\u901a\u8fc7\u53bb\u504f\u5b66\u4e60\u548c\u5206\u5e03\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86GCD\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.03814", "pdf": "https://arxiv.org/pdf/2504.03814", "abs": "https://arxiv.org/abs/2504.03814", "authors": ["Grgur Kova\u010d", "J\u00e9r\u00e9my Perez", "R\u00e9my Portelas", "Peter Ford Dominey", "Pierre-Yves Oudeyer"], "title": "Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "Large language models (LLMs) are increasingly contributing to the creation of\ncontent on the Internet. This creates a feedback loop as subsequent generations\nof models will be trained on this generated, synthetic data. This phenomenon is\nreceiving increasing interest, in particular because previous studies have\nshown that it may lead to distribution shift - models misrepresent and forget\nthe true underlying distributions of human data they are expected to\napproximate (e.g. resulting in a drastic loss of quality). In this study, we\nstudy the impact of human data properties on distribution shift dynamics in\niterated training loops. We first confirm that the distribution shift dynamics\ngreatly vary depending on the human data by comparing four datasets (two based\non Twitter and two on Reddit). We then test whether data quality may influence\nthe rate of this shift. We find that it does on the twitter, but not on the\nReddit datasets. We then focus on a Reddit dataset and conduct a more\nexhaustive evaluation of a large set of dataset properties. This experiment\nassociated lexical diversity with larger, and semantic diversity with smaller\ndetrimental shifts, suggesting that incorporating text with high lexical (but\nlimited semantic) diversity could exacerbate the degradation of generated text.\nWe then focus on the evolution of political bias, and find that the type of\nshift observed (bias reduction, amplification or inversion) depends on the\npolitical lean of the human (true) distribution. Overall, our work extends the\nexisting literature on the consequences of recursive fine-tuning by showing\nthat this phenomenon is highly dependent on features of the human data on which\ntraining occurs. This suggests that different parts of internet (e.g. GitHub,\nReddit) may undergo different types of shift depending on their properties.", "AI": {"task": "\u7814\u7a76\u4eba\u7c7b\u6570\u636e\u7279\u6027\u5bf9\u8fed\u4ee3\u8bad\u7ec3\u5faa\u73af\u4e2d\u5206\u5e03\u504f\u79fb\u52a8\u6001\u7684\u5f71\u54cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u7684\u5185\u5bb9\u8d8a\u6765\u8d8a\u591a\u5730\u51fa\u73b0\u5728\u4e92\u8054\u7f51\u4e0a\uff0c\u5f62\u6210\u53cd\u9988\u5faa\u73af\uff0c\u53ef\u80fd\u5bfc\u81f4\u5206\u5e03\u504f\u79fb\uff0c\u5f71\u54cd\u6a21\u578b\u8d28\u91cf\u3002", "method": "\u6bd4\u8f83\u56db\u4e2a\u6570\u636e\u96c6\uff08\u4e24\u4e2a\u57fa\u4e8eTwitter\uff0c\u4e24\u4e2a\u57fa\u4e8eReddit\uff09\uff0c\u6d4b\u8bd5\u6570\u636e\u8d28\u91cf\u5bf9\u504f\u79fb\u901f\u7387\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u4e00\u6b65\u8bc4\u4f30Reddit\u6570\u636e\u96c6\u7684\u591a\u79cd\u7279\u6027\u3002", "result": "\u6570\u636e\u8d28\u91cf\u5f71\u54cdTwitter\u6570\u636e\u96c6\u7684\u504f\u79fb\u901f\u7387\uff0c\u4f46\u5bf9Reddit\u6570\u636e\u96c6\u65e0\u5f71\u54cd\uff1b\u8bcd\u6c47\u591a\u6837\u6027\u4e0e\u66f4\u5927\u7684\u6709\u5bb3\u504f\u79fb\u76f8\u5173\uff0c\u8bed\u4e49\u591a\u6837\u6027\u4e0e\u66f4\u5c0f\u7684\u504f\u79fb\u76f8\u5173\uff1b\u653f\u6cbb\u504f\u89c1\u7684\u6f14\u53d8\u53d6\u51b3\u4e8e\u4eba\u7c7b\u6570\u636e\u7684\u653f\u6cbb\u503e\u5411\u3002", "conclusion": "\u9012\u5f52\u5fae\u8c03\u7684\u5f71\u54cd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u4eba\u7c7b\u6570\u636e\u7684\u7279\u6027\uff0c\u4e0d\u540c\u4e92\u8054\u7f51\u5e73\u53f0\u53ef\u80fd\u56e0\u7279\u6027\u4e0d\u540c\u800c\u7ecf\u5386\u4e0d\u540c\u7c7b\u578b\u7684\u504f\u79fb\u3002"}}
{"id": "2504.04818", "pdf": "https://arxiv.org/pdf/2504.04818", "abs": "https://arxiv.org/abs/2504.04818", "authors": ["Zuying Xie", "Changtao Miao", "Ajian Liu", "Jiabao Guo", "Feng Li", "Dan Guo", "Yunfeng Diao"], "title": "SUEDE:Shared Unified Experts for Physical-Digital Face Attack Detection Enhancement", "categories": ["cs.CV"], "comment": "Accepted in ICME 2025", "summary": "Face recognition systems are vulnerable to physical attacks (e.g., printed\nphotos) and digital threats (e.g., DeepFake), which are currently being studied\nas independent visual tasks, such as Face Anti-Spoofing and Forgery Detection.\nThe inherent differences among various attack types present significant\nchallenges in identifying a common feature space, making it difficult to\ndevelop a unified framework for detecting data from both attack modalities\nsimultaneously. Inspired by the efficacy of Mixture-of-Experts (MoE) in\nlearning across diverse domains, we explore utilizing multiple experts to learn\nthe distinct features of various attack types. However, the feature\ndistributions of physical and digital attacks overlap and differ. This suggests\nthat relying solely on distinct experts to learn the unique features of each\nattack type may overlook shared knowledge between them. To address these\nissues, we propose SUEDE, the Shared Unified Experts for Physical-Digital Face\nAttack Detection Enhancement. SUEDE combines a shared expert (always activated)\nto capture common features for both attack types and multiple routed experts\n(selectively activated) for specific attack types. Further, we integrate CLIP\nas the base network to ensure the shared expert benefits from prior visual\nknowledge and align visual-text representations in a unified space. Extensive\nresults demonstrate SUEDE achieves superior performance compared to\nstate-of-the-art unified detection methods.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u6846\u67b6SUEDE\uff0c\u7528\u4e8e\u540c\u65f6\u68c0\u6d4b\u7269\u7406\u548c\u6570\u5b57\u4eba\u8138\u653b\u51fb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u7269\u7406\u653b\u51fb\uff08\u5982\u6253\u5370\u7167\u7247\uff09\u548c\u6570\u5b57\u653b\u51fb\uff08\u5982DeepFake\uff09\u4f5c\u4e3a\u72ec\u7acb\u4efb\u52a1\u7814\u7a76\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7279\u5f81\u7a7a\u95f4\uff0c\u96be\u4ee5\u540c\u65f6\u68c0\u6d4b\u4e24\u79cd\u653b\u51fb\u3002", "method": "\u7ed3\u5408\u5171\u4eab\u4e13\u5bb6\uff08\u6355\u83b7\u5171\u540c\u7279\u5f81\uff09\u548c\u8def\u7531\u4e13\u5bb6\uff08\u6355\u83b7\u7279\u5b9a\u653b\u51fb\u7279\u5f81\uff09\uff0c\u5e76\u5229\u7528CLIP\u4f5c\u4e3a\u57fa\u7840\u7f51\u7edc\u3002", "result": "SUEDE\u5728\u7edf\u4e00\u68c0\u6d4b\u65b9\u6cd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "SUEDE\u901a\u8fc7\u5171\u4eab\u548c\u8def\u7531\u4e13\u5bb6\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7269\u7406\u548c\u6570\u5b57\u653b\u51fb\u68c0\u6d4b\u7684\u7edf\u4e00\u95ee\u9898\u3002"}}
{"id": "2504.03947", "pdf": "https://arxiv.org/pdf/2504.03947", "abs": "https://arxiv.org/abs/2504.03947", "authors": ["Chris Samarinas", "Hamed Zamani"], "title": "Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "We present a novel approach for training small language models for\nreasoning-intensive document ranking that combines knowledge distillation with\nreinforcement learning optimization. While existing methods often rely on\nexpensive human annotations or large black-box language models, our methodology\nleverages web data and a teacher LLM to automatically generate high-quality\ntraining examples with relevance explanations. By framing document ranking as a\nreinforcement learning problem and incentivizing explicit reasoning\ncapabilities, we train a compact 3B parameter language model that achieves\nstate-of-the-art performance on the BRIGHT benchmark. Our model ranks third on\nthe leaderboard while using substantially fewer parameters than other\napproaches, outperforming models that are over 20 times larger. Through\nextensive experiments, we demonstrate that generating explanations during\ninference, rather than directly predicting relevance scores, enables more\neffective reasoning with smaller language models. The self-supervised nature of\nour method offers a scalable and interpretable solution for modern information\nretrieval systems.", "AI": {"task": "\u8bad\u7ec3\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u63a8\u7406\u5bc6\u96c6\u578b\u6587\u6863\u6392\u5e8f\uff0c\u7ed3\u5408\u77e5\u8bc6\u84b8\u998f\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6216\u5927\u578b\u9ed1\u76d2\u8bed\u8a00\u6a21\u578b\uff0c\u800c\u672c\u65b9\u6cd5\u5229\u7528\u7f51\u7edc\u6570\u636e\u548c\u6559\u5e08LLM\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6837\u672c\u53ca\u76f8\u5173\u6027\u89e3\u91ca\u3002", "method": "\u5c06\u6587\u6863\u6392\u5e8f\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u6fc0\u52b1\u663e\u5f0f\u63a8\u7406\u80fd\u529b\uff0c\u8bad\u7ec3\u4e00\u4e2a3B\u53c2\u6570\u7684\u7d27\u51d1\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728BRIGHT\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6392\u540d\u7b2c\u4e09\uff0c\u4e14\u53c2\u6570\u8fdc\u5c11\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4f18\u4e8e\u53c2\u6570\u592720\u500d\u7684\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u63a8\u7406\u65f6\u751f\u6210\u89e3\u91ca\u800c\u975e\u76f4\u63a5\u9884\u6d4b\u76f8\u5173\u6027\u5206\u6570\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u66f4\u6709\u6548\u63a8\u7406\uff0c\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2504.04827", "pdf": "https://arxiv.org/pdf/2504.04827", "abs": "https://arxiv.org/abs/2504.04827", "authors": ["Long Ma", "Zhiyuan Yan", "Yize Chen", "Jin Xu", "Qinglang Guo", "Hu Huang", "Yong Liao", "Hui Lin"], "title": "From Specificity to Generality: Revisiting Generalizable Artifacts in Detecting Face Deepfakes", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Detecting deepfakes has been an increasingly important topic, especially\ngiven the rapid development of AI generation techniques. In this paper, we ask:\nHow can we build a universal detection framework that is effective for most\nfacial deepfakes? One significant challenge is the wide variety of deepfake\ngenerators available, resulting in varying forgery artifacts (e.g., lighting\ninconsistency, color mismatch, etc). But should we ``teach\" the detector to\nlearn all these artifacts separately? It is impossible and impractical to\nelaborate on them all. So the core idea is to pinpoint the more common and\ngeneral artifacts across different deepfakes. Accordingly, we categorize\ndeepfake artifacts into two distinct yet complementary types: Face\nInconsistency Artifacts (FIA) and Up-Sampling Artifacts (USA). FIA arise from\nthe challenge of generating all intricate details, inevitably causing\ninconsistencies between the complex facial features and relatively uniform\nsurrounding areas. USA, on the other hand, are the inevitable traces left by\nthe generator's decoder during the up-sampling process. This categorization\nstems from the observation that all existing deepfakes typically exhibit one or\nboth of these artifacts. To achieve this, we propose a new data-level\npseudo-fake creation framework that constructs fake samples with only the FIA\nand USA, without introducing extra less-general artifacts. Specifically, we\nemploy a super-resolution to simulate the USA, while design a Blender module\nthat uses image-level self-blending on diverse facial regions to create the\nFIA. We surprisingly found that, with this intuitive design, a standard image\nclassifier trained only with our pseudo-fake data can non-trivially generalize\nwell to unseen deepfakes.", "AI": {"task": "\u6784\u5efa\u4e00\u4e2a\u901a\u7528\u7684\u9762\u90e8\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u8bc6\u522b\u5927\u591a\u6570\u9762\u90e8\u6df1\u5ea6\u4f2a\u9020\u3002", "motivation": "\u7531\u4e8e\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u8986\u76d6\u6240\u6709\u751f\u6210\u5668\u4ea7\u751f\u7684\u4f2a\u9020\u75d5\u8ff9\u3002", "method": "\u5c06\u6df1\u5ea6\u4f2a\u9020\u75d5\u8ff9\u5206\u4e3a\u4e24\u7c7b\uff08FIA\u548cUSA\uff09\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u7ea7\u7684\u4f2a\u4f2a\u9020\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8d85\u5206\u8fa8\u7387\u6a21\u62dfUSA\u548c\u56fe\u50cf\u81ea\u6df7\u5408\u6a21\u5757\u751f\u6210FIA\u3002", "result": "\u4ec5\u4f7f\u7528\u4f2a\u4f2a\u9020\u6570\u636e\u8bad\u7ec3\u7684\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u5668\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6df1\u5ea6\u4f2a\u9020\u3002", "conclusion": "\u901a\u8fc7\u805a\u7126\u4e8e\u901a\u7528\u4f2a\u9020\u75d5\u8ff9\uff08FIA\u548cUSA\uff09\uff0c\u53ef\u4ee5\u6784\u5efa\u4e00\u4e2a\u9ad8\u6548\u7684\u901a\u7528\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\u3002"}}
{"id": "2504.03970", "pdf": "https://arxiv.org/pdf/2504.03970", "abs": "https://arxiv.org/abs/2504.03970", "authors": ["Dahun Kim", "AJ Piergiovanni", "Ganesh Mallya", "Anelia Angelova"], "title": "VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR"], "comment": "CVPR 2025, project page at\n  https://github.com/google-deepmind/video_comp", "summary": "We introduce VideoComp, a benchmark and learning framework for advancing\nvideo-text compositionality understanding, aimed at improving vision-language\nmodels (VLMs) in fine-grained temporal alignment. Unlike existing benchmarks\nfocused on static image-text compositionality or isolated single-event videos,\nour benchmark targets alignment in continuous multi-event videos. Leveraging\nvideo-text datasets with temporally localized event captions (e.g.\nActivityNet-Captions, YouCook2), we construct two compositional benchmarks,\nActivityNet-Comp and YouCook2-Comp. We create challenging negative samples with\nsubtle temporal disruptions such as reordering, action word replacement,\npartial captioning, and combined disruptions. These benchmarks comprehensively\ntest models' compositional sensitivity across extended, cohesive video-text\nsequences. To improve model performance, we propose a hierarchical pairwise\npreference loss that strengthens alignment with temporally accurate pairs and\ngradually penalizes increasingly disrupted ones, encouraging fine-grained\ncompositional learning. To mitigate the limited availability of densely\nannotated video data, we introduce a pretraining strategy that concatenates\nshort video-caption pairs to simulate multi-event sequences. We evaluate\nvideo-text foundational models and large multimodal models (LMMs) on our\nbenchmark, identifying both strengths and areas for improvement in\ncompositionality. Overall, our work provides a comprehensive framework for\nevaluating and enhancing model capabilities in achieving fine-grained,\ntemporally coherent video-text alignment.", "AI": {"task": "\u63d0\u51faVideoComp\uff0c\u4e00\u4e2a\u7528\u4e8e\u63d0\u5347\u89c6\u9891-\u6587\u672c\u7ec4\u5408\u6027\u7406\u89e3\u7684\u57fa\u51c6\u548c\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u6539\u8fdb\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u65f6\u95f4\u5bf9\u9f50\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u56fe\u50cf-\u6587\u672c\u7ec4\u5408\u6027\u6216\u5b64\u7acb\u5355\u4e8b\u4ef6\u89c6\u9891\uff0c\u7f3a\u4e4f\u5bf9\u8fde\u7eed\u591a\u4e8b\u4ef6\u89c6\u9891\u5bf9\u9f50\u7684\u7814\u7a76\u3002", "method": "\u5229\u7528\u5177\u6709\u65f6\u95f4\u5c40\u90e8\u5316\u4e8b\u4ef6\u63cf\u8ff0\u7684\u89c6\u9891-\u6587\u672c\u6570\u636e\u96c6\uff08\u5982ActivityNet-Captions\u3001YouCook2\uff09\uff0c\u6784\u5efa\u4e24\u4e2a\u7ec4\u5408\u6027\u57fa\u51c6\uff08ActivityNet-Comp\u548cYouCook2-Comp\uff09\uff0c\u5e76\u63d0\u51fa\u5206\u5c42\u6210\u5bf9\u504f\u597d\u635f\u5931\u548c\u9884\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u8bc4\u4f30\u4e86\u89c6\u9891-\u6587\u672c\u57fa\u7840\u6a21\u578b\u548c\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5176\u5728\u7ec4\u5408\u6027\u65b9\u9762\u7684\u4f18\u52bf\u548c\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u3001\u65f6\u95f4\u8fde\u8d2f\u7684\u89c6\u9891-\u6587\u672c\u5bf9\u9f50\u80fd\u529b\u65b9\u9762\u63d0\u4f9b\u4e86\u5168\u9762\u6846\u67b6\u3002"}}
{"id": "2504.04834", "pdf": "https://arxiv.org/pdf/2504.04834", "abs": "https://arxiv.org/abs/2504.04834", "authors": ["Pengju Sun", "Banglei Guan", "Zhenbao Yu", "Yang Shang", "Qifeng Yu", "Daniel Barath"], "title": "Learning Affine Correspondences by Integrating Geometric Constraints", "categories": ["cs.CV"], "comment": null, "summary": "Affine correspondences have received significant attention due to their\nbenefits in tasks like image matching and pose estimation. Existing methods for\nextracting affine correspondences still have many limitations in terms of\nperformance; thus, exploring a new paradigm is crucial. In this paper, we\npresent a new pipeline designed for extracting accurate affine correspondences\nby integrating dense matching and geometric constraints. Specifically, a novel\nextraction framework is introduced, with the aid of dense matching and a novel\nkeypoint scale and orientation estimator. For this purpose, we propose loss\nfunctions based on geometric constraints, which can effectively improve\naccuracy by supervising neural networks to learn feature geometry. The\nexperimental show that the accuracy and robustness of our method outperform the\nexisting ones in image matching tasks. To further demonstrate the effectiveness\nof the proposed method, we applied it to relative pose estimation. Affine\ncorrespondences extracted by our method lead to more accurate poses than the\nbaselines on a range of real-world datasets. The code is available at\nhttps://github.com/stilcrad/DenseAffine.", "AI": {"task": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u6d41\u7a0b\uff0c\u901a\u8fc7\u7ed3\u5408\u5bc6\u96c6\u5339\u914d\u548c\u51e0\u4f55\u7ea6\u675f\u6765\u63d0\u53d6\u7cbe\u786e\u7684\u4eff\u5c04\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u63d0\u53d6\u4eff\u5c04\u5bf9\u5e94\u5173\u7cfb\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u5b58\u5728\u8bb8\u591a\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u63a2\u7d22\u65b0\u8303\u5f0f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u63d0\u53d6\u6846\u67b6\uff0c\u7ed3\u5408\u5bc6\u96c6\u5339\u914d\u548c\u65b0\u578b\u5173\u952e\u70b9\u5c3a\u5ea6\u4e0e\u65b9\u5411\u4f30\u8ba1\u5668\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u51e0\u4f55\u7ea6\u675f\u7684\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u5339\u914d\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u63d0\u53d6\u4eff\u5c04\u5bf9\u5e94\u5173\u7cfb\u548c\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2504.04030", "pdf": "https://arxiv.org/pdf/2504.04030", "abs": "https://arxiv.org/abs/2504.04030", "authors": ["Wasi Uddin Ahmad", "Aleksander Ficek", "Mehrzad Samadi", "Jocelyn Huang", "Vahid Noroozi", "Somshubra Majumdar", "Boris Ginsburg"], "title": "OpenCodeInstruct: A Large-scale Instruction Tuning Dataset for Code LLMs", "categories": ["cs.SE", "cs.CL"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) have transformed software development by\nenabling code generation, automated debugging, and complex reasoning. However,\ntheir continued advancement is constrained by the scarcity of high-quality,\npublicly available supervised fine-tuning (SFT) datasets tailored for coding\ntasks. To bridge this gap, we introduce OpenCodeInstruct, the largest\nopen-access instruction tuning dataset, comprising 5 million diverse samples.\nEach sample includes a programming question, solution, test cases, execution\nfeedback, and LLM-generated quality assessments. We fine-tune various base\nmodels, including LLaMA and Qwen, across multiple scales (1B+, 3B+, and 7B+)\nusing our dataset. Comprehensive evaluations on popular benchmarks (HumanEval,\nMBPP, LiveCodeBench, and BigCodeBench) demonstrate substantial performance\nimprovements achieved by SFT with OpenCodeInstruct. We also present a detailed\nmethodology encompassing seed data curation, synthetic instruction and solution\ngeneration, and filtering.", "AI": {"task": "\u4ecb\u7ecd\u5e76\u8bc4\u4f30OpenCodeInstruct\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u9ad8\u8d28\u91cf\u3001\u516c\u5f00\u53ef\u7528\u7684\u76d1\u7763\u5fae\u8c03\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u9650\u5236\u4e86LLMs\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u6784\u5efaOpenCodeInstruct\u6570\u636e\u96c6\uff08500\u4e07\u6837\u672c\uff09\uff0c\u5305\u542b\u7f16\u7a0b\u95ee\u9898\u3001\u89e3\u51b3\u65b9\u6848\u3001\u6d4b\u8bd5\u7528\u4f8b\u7b49\uff0c\u5e76\u5bf9\u591a\u79cd\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff08\u5982HumanEval\u3001MBPP\u7b49\uff09\uff0c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "OpenCodeInstruct\u586b\u8865\u4e86\u9ad8\u8d28\u91cf\u7f16\u7a0b\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2504.04835", "pdf": "https://arxiv.org/pdf/2504.04835", "abs": "https://arxiv.org/abs/2504.04835", "authors": ["Shanshan Wang", "Haixiang Xu", "Hui Feng", "Xiaoqian Wang", "Pei Song", "Sijie Liu", "Jianhua He"], "title": "Inland Waterway Object Detection in Multi-environment: Dataset and Approach", "categories": ["cs.CV"], "comment": "37 pages,11 figures,5 tables", "summary": "The success of deep learning in intelligent ship visual perception relies\nheavily on rich image data. However, dedicated datasets for inland waterway\nvessels remain scarce, limiting the adaptability of visual perception systems\nin complex environments. Inland waterways, characterized by narrow channels,\nvariable weather, and urban interference, pose significant challenges to object\ndetection systems based on existing datasets. To address these issues, this\npaper introduces the Multi-environment Inland Waterway Vessel Dataset (MEIWVD),\ncomprising 32,478 high-quality images from diverse scenarios, including sunny,\nrainy, foggy, and artificial lighting conditions. MEIWVD covers common vessel\ntypes in the Yangtze River Basin, emphasizing diversity, sample independence,\nenvironmental complexity, and multi-scale characteristics, making it a robust\nbenchmark for vessel detection. Leveraging MEIWVD, this paper proposes a\nscene-guided image enhancement module to improve water surface images based on\nenvironmental conditions adaptively. Additionally, a parameter-limited dilated\nconvolution enhances the representation of vessel features, while a multi-scale\ndilated residual fusion method integrates multi-scale features for better\ndetection. Experiments show that MEIWVD provides a more rigorous benchmark for\nobject detection algorithms, and the proposed methods significantly improve\ndetector performance, especially in complex multi-environment scenarios.", "AI": {"task": "\u63d0\u51fa\u4e00\u4e2a\u591a\u73af\u5883\u5185\u6cb3\u8239\u8236\u6570\u636e\u96c6\uff08MEIWVD\uff09\u5e76\u5f00\u53d1\u4e00\u79cd\u573a\u666f\u5f15\u5bfc\u7684\u56fe\u50cf\u589e\u5f3a\u6a21\u5757\u548c\u53c2\u6570\u53d7\u9650\u7684\u6269\u5f20\u5377\u79ef\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u590d\u6742\u73af\u5883\u4e0b\u7684\u8239\u8236\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5185\u6cb3\u822a\u9053\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u73b0\u6709\u6570\u636e\u96c6\u96be\u4ee5\u9002\u5e94\u590d\u6742\u73af\u5883\uff08\u5982\u72ed\u7a84\u822a\u9053\u3001\u591a\u53d8\u5929\u6c14\u548c\u57ce\u5e02\u5e72\u6270\uff09\u4e0b\u7684\u8239\u8236\u68c0\u6d4b\u9700\u6c42\u3002", "method": "\u5f15\u5165MEIWVD\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u573a\u666f\u5f15\u5bfc\u7684\u56fe\u50cf\u589e\u5f3a\u6a21\u5757\u548c\u53c2\u6570\u53d7\u9650\u7684\u6269\u5f20\u5377\u79ef\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u6269\u5f20\u6b8b\u5dee\u878d\u5408\u6280\u672f\u3002", "result": "MEIWVD\u4e3a\u8239\u8236\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u4e25\u683c\u7684\u57fa\u51c6\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u5668\u5728\u590d\u6742\u591a\u73af\u5883\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "MEIWVD\u548c\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5185\u6cb3\u8239\u8236\u68c0\u6d4b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002"}}
